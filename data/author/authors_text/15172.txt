Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 251?261,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Translingual Document Representations from Discriminative Projections
John C. Platt Kristina Toutanova
Microsoft Research
1 Microsoft Way
Redmond, WA 98005, USA
{jplatt,kristout,scottyih}@microsoft.com
Wen-tau Yih
Abstract
Representing documents by vectors that are
independent of language enhances machine
translation and multilingual text categoriza-
tion. We use discriminative training to create
a projection of documents from multiple lan-
guages into a single translingual vector space.
We explore two variants to create these pro-
jections: Oriented Principal Component Anal-
ysis (OPCA) and Coupled Probabilistic Latent
Semantic Analysis (CPLSA). Both of these
variants start with a basic model of docu-
ments (PCA and PLSA). Each model is then
made discriminative by encouraging compa-
rable document pairs to have similar vector
representations. We evaluate these algorithms
on two tasks: parallel document retrieval
for Wikipedia and Europarl documents, and
cross-lingual text classification on Reuters.
The two discriminative variants, OPCA and
CPLSA, significantly outperform their corre-
sponding baselines. The largest differences in
performance are observed on the task of re-
trieval when the documents are only compa-
rable and not parallel. The OPCA method is
shown to perform best.
1 Introduction
Given the growth of multiple languages on the In-
ternet, Natural Language Processing must operate
on dozens of languages. It is becoming critical that
computers reach high performance on the following
two tasks:
? Comparable and parallel document re-
trieval ? Cross-language information retrieval
and text categorization have become impor-
tant with the growth of the Web (Oard and
Diekema, 1998). In addition, machine trans-
lation (MT) systems can be improved by
training on sentences extracted from paral-
lel or comparable documents mined from the
Web (Munteanu and Marcu, 2005). Compa-
rable documents can also be used for learning
word-level translation lexicons (Fung and Yee,
1998; Rapp, 1999).
? Cross-language text categorization ? Appli-
cations of text categorization, such as sentiment
classification (Pang et al, 2002), are now re-
quired to run on multiple languages. Catego-
rization is usually trained on the language of
the developer: it needs to be easily extended to
other languages.
There are two broad approaches to comparable
document retrieval and cross-language text catego-
rization. One approach is to translate queries or a
training set from different languages into a single
target language. Standard monolingual retrieval and
classification algorithms can then be applied in the
target language.
Alternatively, a cross-language system can project
a bag-of-words vector into a translingual lower-
dimensional vector space. Ideally, vectors in this
space represent the semantics of a document, inde-
pendent of the language.
The advantage of pre-translation is that MT sys-
tems tend to preserve the meaning of documents.
However, MT can be very slow (more than 1 second
per document), preventing its use on large training
sets. When full MT is not practical, a fast word-by-
word translation model can be used instead, (Balles-
teros and Croft, 1996) but may be less accurate.
Conversely, applying a projection into a low-
dimensional space is quick. Linear projection al-
gorithms use matrix-sparse vector multiplication,
which can be easily parallelized. However, as seen
in section 3, the accuracies of previous projection
251
techniques are not as high as machine translation.
This paper presents two techniques: Oriented
PCA and Coupled PLSA. These techniques retain
the high speed of projection, while approaching or
exceeding the quality level of word glossing. We im-
prove the quality of the projections by the use of dis-
criminative training: we minimize the difference be-
tween comparable documents in the projected vec-
tor space. Oriented PCA minimizes the difference
by modifying the eigensystem of PCA (Diamantaras
and Kung, 1996), while Coupled PLSA uses poste-
rior regularization (Graca et al, 2008; Ganchev et
al., 2009) on the topic assignments of the compara-
ble documents.
1.1 Previous work
There has been extensive work in projecting mono-
lingual documents into a vector space. The ini-
tial algorithm for projecting documents was Latent
Semantic Analysis (LSA), which modeled bag-of-
word vectors as low-rank Gaussians (Deerwester et
al., 1990). Subsequent projection algorithms were
based on generative models of individual terms in
the documents, including Probabilistic Latent Se-
mantic Analysis (PLSA) (Hofmann, 1999) and La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003).
Work on cross-lingual projections followed a sim-
ilar pattern of moving from Gaussian models to
term-wise generative models. Cross-language La-
tent Semantic Indexing (CL-LSI) (Dumais et al,
1997) applied LSA to concatenated comparable doc-
uments from multiple languages. Similarly, Polylin-
gual Topic Models (PLTM) (Mimno et al, 2009)
generalized LDA to tuples of documents from mul-
tiple languages. The experiments in section 3 use
CL-LSI and an algorithm similar to PLTM as bench-
marks.
The closest previous work to this paper is the
use of Canonical Correlation Analysis (CCA) to find
projections for multiple languages whose results are
maximally correlated with each other (Vinokourov
et al, 2003).
PLSA-, LDA-, and CCA-based cross-lingual
models have also been trained without the use of par-
allel or comparable documents, using only knowl-
edge from a translation dictionary to achieve sharing
of topics across languages (Haghighi et al, 2008; Ja-
garlamudi and Daume?, 2010; Zhang et al, 2010).
Such work is complementary to ours and can be
used to extend the models to domains lacking par-
allel documents.
Outside of NLP, researchers have designed al-
gorithms to find discriminative projections. We
build on the Oriented Principal Component Analysis
(OPCA) algorithm (Diamantaras and Kung, 1996),
which finds projections that maximize a signal-to-
noise ratio (as defined by the user). OPCA has been
used to create discriminative features for audio fin-
gerprinting (Burges et al, 2003).
1.2 Structure of paper
This paper now presents two algorithms for translin-
gual document projection (in section 2): OPCA and
Coupled PLSA (CPLSA). To explain OPCA, we
first review CL-LSI in section 2.1, then discuss the
details of OPCA (section 2.2), and compare it to
CCA (section 2.3). To explain CPLSA, we first
introduce Joint PLSA (JPLSA), analogous to CL-
LSI, in section 2.4, and then describe the details of
CPLSA (section 2.5).
We have evaluated these algorithms on two dif-
ferent tasks: comparable document retrieval (sec-
tion 3.2) and cross-language text categorization
(section 3.3). We discuss the findings of the evalua-
tions and extensions to the algorithms in section 4.
2 Algorithms for translingual document
projection
2.1 Cross-language Latent Semantic Indexing
Cross-language Latent Semantic Indexing (CL-LSI)
is Latent Semantic Analysis (LSA) applied to multi-
ple languages. First, we review the mathematics of
LSA.
LSA models an n ? k document-term matrix D,
where n is the number of documents and k is the
number of terms. The model of the document-term
matrix is a low-rank Gaussian. Originally, LSA was
presented as performing a Singular Value Decompo-
sition (Deerwester et al, 1990), but here we present
it as eigendecomposition, to clarify its relationship
with OPCA.
LSA first computes the correlation matrix be-
tween terms:
C = DTD. (1)
252
The Rayleigh quotient for a vector ~v with the matrix
C is
~vTC~v
~vT~v
, (2)
and is equal to the variance of the data projected us-
ing the vector ~v, normalized by the length of ~v, if D
has columns that are zero mean. Good projections
retain a large amount of variance. LSA maximizes
the Rayleigh ratio by taking its derivative against ~v
and setting it to zero. This yields a set of projections
that are eigenvectors of C,
C~vj = ?j~vj , (3)
where ?j is the jth-largest eigenvalue. Each eigen-
value is also the variance of the data when projected
by the corresponding eigenvector ~vj . LSA simply
uses top d eigenvectors as projections.
LSA is very similar to Principal Components
Analysis (PCA). The only difference is that the cor-
relation matrix C is used, instead of the covariance
matrix. In practice, the document-term matrix D is
sparse, so the column means are close to zero, and
the correlation matrix is close to the covariance ma-
trix.
There are a number of methods to form the
document-term matrix D. One method that works
well in practice is to compute the log(tf)-idf weight-
ing: (Dumais, 1990; Wild et al, 2005)
Dij = log2(fij + 1) log2(n/dj), (4)
where fij is the number of times term j occurs in
document i, n is the total number of documents,
and dj is the total number of documents that con-
tain term j. Applying a logarthm to the term counts
makes the distribution of matrix entries approach
Gaussian, which makes the LSA model more valid.
Cross-language LSI is an application of LSA
where each row of D is formed by concatenating
comparable or parallel documents in multiple lan-
guages. If a single term occurs in multiple lan-
guages, the term only has one slot in the concate-
nation, and the term count accumulates for all lan-
guages. Such terms could be proper nouns, such as
?Smith? or ?Merkel?.
In general, the elements of D are computed via
Dij = log2
(
?
m
fmij + 1
)
log2(n/dj), (5)
where fmij is the number of times term j occurs in
document i in language m. Here, dj is the number
of documents term j appears in, and n is the total
number of documents across all languages.
Because CL-LSI is simply LSA applied to con-
catenated documents, it models terms in document
vectors jointly across languages as a single low-rank
Gaussian.
2.2 Oriented Principal Component Analysis
The limitations of CL-LSI can be illustrated by con-
sidering Oriented Principal Components Analysis
(OPCA), a generalization of PCA. A user of OPCA
computes a signal covariance matrix S and a noise
covariance matrix N. OPCA projections ~vj max-
imize the ratio of the variance of the signal pro-
jected by ~vj to the variance of the noise projected
by ~vj . This signal-to-noise ratio is the generalized
Rayleigh quotient: (Diamantaras and Kung, 1996)
~vTS~v
~vTN~v
. (6)
Taking the derivative of the Rayleigh quotient with
respect to the projections ~v and setting it to zero
yields the generalized eigenproblem
S~vj = ?jN~vj . (7)
This eigenproblem has no local minima, and can be
solved with commonly available parallel code.
PCA is a specialization of OPCA, where the noise
covariance matrix is assumed to be the identity (i.e.,
uncorrelated noise). PCA projections maximize the
signal-to-noise ratio where the signal is the empiri-
cal covariance of the data, and the noise is spherical
white noise. PCA projections are not truly appropri-
ate for forming multilingual document projections.
Instead, we want multilingual document projec-
tions to maximize the projected covariance of doc-
ument vectors across all languages, while simulta-
neously minimizing the projected distance between
comparable documents (see Figure 1). OPCA gives
us a framework for finding such discriminative pro-
jections. The covariance matrix for all documents
is the signal covariance in OPCA, and captures the
meaning of documents across all languages. The
projection of this covariance matrix should be max-
imized. The covariance matrix formed from differ-
ences between comparable documents is the noise
253
covariance in OPCA: we wish to minimize the lat-
ter covariance, to make the projection language-
independent.
Specifically, we create the weighted document-
term matrix Dm for each language:
Dij,m = log2(f
m
ij + 1)log2(n/dj). (8)
We then derive a signal covariance matrix over all
languages:
S =
?
m
DTmDm/n? ~?
T
m~?m, (9)
where ~?m is the mean of each Dm over its columns,
and a noise covariance matrix,
N =
?
m
(Dm ?D)T (Dm ?D)/n+ ?I, (10)
where D is the mean across all languages of the
document-term matrix,
D =
1
M
?
m
Dm, (11)
and M is the number of languages. Applying equa-
tion (7) to these matrices and taking the top gener-
alized eigenvectors yields the projection matrix for
OPCA.
Note the regularization term of ?I in equation
(10). The empirical sample of comparable docu-
ments may not cover the entire space of translation
noise the system will encounter in the test set. For
safety, we add a regularizer that prevents the vari-
ance of a term from getting too small. We tuned ?
on the development sets in section 3.2: for log(tf)-
idf weighted vectors, C = 0.1 works well for the
data sets and dimensionalities that we tried. We use
C = 0.1 for all final tests.
2.3 Canonical Correlation Analysis
Canonical Correlation Analysis (CCA) is a tech-
nique that is related to OPCA. CCA was kernelized
and applied to creating cross-language document
models by (Vinokourov et al, 2003). In CCA, a lin-
ear projection is found for each language, such that
the projections of the corpus from each language are
maximally correlated with each other. Similar to
OPCA, this linear projection can be found by find-
ing the top generalized eigenvectors of the system
en
esen e
n
enes es
es
Maxim
izes ov
erall v
arianc
e
? whi
le min
imizin
g dista
nce 
betwe
en com
parab
le pair
s
Figure 1: OPCA finds a projection that maximizes the
variance of all documents, while minimizing distance be-
tween comparable documents
(7), where S is now a matrix of cross-correlations
that the projection maximizes,
S =
[
0 C12
C21 0
]
, (12)
and N is a matrix of autocorrelations that the projec-
tion minimizes,
N =
[
C11 + ?I 0
0 C22 + ?I
]
. (13)
Here, Cij is the (cross-)covariance matrix, with di-
mension equal to the vocabulary size, that is com-
puted between the document vectors for languages
i and j. Analogous to OPCA, ? is a regularization
term, set by optimizing performance on a validation
set. Like OPCA, these matrices can be generalized
to more than two languages. Unlike OPCA, CCA
finds projections that maximize the cross-covariance
between the projected vectors, instead of minimiz-
ing Euclidean distance.1
By definition, CCA cannot take advantage of the
information that same term occurs simultaneously in
comparable documents. As shown in section 3, this
1Note that the eigenvectors have length equal to the sum of
the length of the vocabularies of each language. The projections
for each language are created by splitting the eigenvectors into
sections, each with length equal to the vocabulary size for each
language.
254
information is useful and helps OPCA perform bet-
ter then CCA. In addition, CCA encourages compa-
rable documents to be projected to vectors that are
mutually linearly predictable. This is not the same
OPCA?s projected vectors that have low Euclidean
distance: the latter may be preferred by algorithms
that consume the projections.
2.4 Cross-language Topic Models
We now turn to a baseline generative model that
is analogous to CL-LSI. Our baseline joint PLSA
model (JPLSA) is closely related to the poly-lingual
LDA model of (Mimno et al, 2009). The graphical
model for JPLSA is shown at the top in Figure 2.
We describe the model for two languages, but it is
straightforward to generalize to more than two lan-
guages, as in (Mimno et al, 2009).
z
z
??
w
w
?
TD
N1
N2
z
z
?1
?
w
w
?
TD
N1
N2
?2
?
?
Figure 2: Graphical models for JPLSA (top) and CPLSA
(bottom)
The model sees documents di as sequences of
words w1, w2, . . . , wni from a vocabulary V . There
are T cross-language topics, each of which has a dis-
tribution ?t over words in V . In the case of mod-
els for two languages, we define the vocabulary V
to contain word types from both languages. In this
way, each topic is shared across languages.
Each topic-specific distribution ?t, for t =
1 . . . T , is drawn from a symmetric Dirichlet prior
with concentration parameter ?. Given the topic-
specific word distributions, the generative process
for a corpus of paired documents [d1i , d
2
i ] in two lan-
guages L1 and L2 is described in the next paragraph.
For each pair of documents, pick a distribution
over topics ?i, from a symmetric Dirichlet prior with
concentration parameter ?. Then generate the doc-
uments d1i and d
2
i in turn. Each word token in each
document is generated independently by first pick-
ing a topic z from a multinomial distribution with
parameter ?i (MULTI(?i)), and then generating the
word token from the topic-specific word distribution
for the chosen topic MULTI(?z).
The probability of a document pair [d1, d2] with
words [w11, w
1
2, . . . , w
1
n1 ], [w
2
1, w
2
2, . . . , w
2
n2 ], topic
assignments [z11 , . . . , z
1
n1 ], [z
2
1 , . . . , z
2
n2 ], and a com-
mon topic vector ? is given by:
P (?|?)
n1?
j=1
P (z1j |?)P (w
1
j |?z1j )
n2?
j=1
P (z2j |?)P (w
2
j |?z2j )
The difference between the JPLSA model and the
poly-lingual topic model of (Mimno et al, 2009)
is that we merge the vocabularies in the two lan-
guages and learn topic-specific word distributions
over these merged vocabularies, instead of having
pairs of topic-specific word distributions, one for
each language, like in (Mimno et al, 2009). Thus
our model is more similar to the CL-LSI model, be-
cause it can be seen as viewing a pair of documents
in two languages as one bigger document containing
the words in both documents.
Another difference between our model and the
poly-lingual LDA model of (Mimno et al, 2009)
is that we use maximum aposteriori (MAP) instead
of Bayesian inference. Recently, MAP inference
was shown to perform comparably to the best in-
ference method for LDA (Asuncion et al, 2009),
if the hyper-parameters are chosen optimally for
the inference method. Our initial experiments with
Bayesian versus MAP inference for parallel docu-
ment retrieval using JPLSA confirmed this result.
In practice our baseline model outperforms poly-
lingual LDA as mentioned in our experiments.
2.5 Coupled Probabilistic Latent Semantic
Analysis
The JPLSA model assumes that a pair of translated
or comparable documents have a common topic dis-
tribution ?. JPLSA fits its parameters to optimize the
probability of the data, given this assumption.
For the task of comparable document retrieval, we
want our topic model to assign similar topic distri-
butions ? to a pair of corresponding documents. But
255
this is not exactly what the JPLSA model is doing.
Instead, it derives a common topic vector ? which
explains the union of all tokens in the English and
foreign documents, instead of making sure that the
best topic assignment for the English document is
close to the best topic assignment of the foreign doc-
ument. This difference becomes especially appar-
ent when corresponding documents have different
lengths. In this case, the model will tend to derive
a topic vector ? which explains the longer document
best, making the sum of the two documents? log-
likelihoods higher. Modeling the shorter document?s
best topic carries little weight.
Modeling both documents equally is what Cou-
pled PLSA (CPLSA) is designed to do. The graphi-
cal model for CPLSA is shown at the bottom of Fig-
ure 2. In this figure, the topic vectors of a pair of
documents in two languages are shown completely
independent. We use the log-likelihood according to
this model, but also add a regularization term, which
tries to make the topic assignments of correspond-
ing documents close. In particular, we use poste-
rior regularization (Graca et al, 2008; Ganchev et
al., 2009) to place linear constraints on the expec-
tations of topic assignments to two corresponding
documents.
For two linked documents d1 and d2, we would
like our model to be such that the expected fraction
of tokens in d1 that get assigned topic t is approxi-
mately the same as the expected fraction of tokens in
d2 that get assigned the same topic t, for each topic
t = 1 . . . T . This is exactly what we need to make
each pair of corresponding documents close.
Let z1 and z2 denote vectors of topic assignments
to the tokens in document d1 and d2, respectively.
Their dimensionality is equal to the lengths of the
two documents, n1 and n2. We define a space of
posterior distributions Q over hidden topic assign-
ments to the tokens in d1 and d2, that has the desired
property: the expected fraction of each topic is ap-
proximately equal in d1 and d2. We can formulate
this constrained space Q as follows:
Q = {q1(z1), q2(z2)}
such that
Eq1 [
?n1
j=1 1(z
1
j = t)
n1
]?Eq2 [
?n2
j=1 1(z
2
j = t)
n2
] ? t
Eq2 [
?n2
j=1 1(z
2
j = t)
n2
]?Eq1 [
?n1
j=1 1(z
1
j = t)
n1
] ? t
We then formulate an objective function that max-
imizes the log-likelihood of the data while simulta-
neously minimizing the KL-divergence between the
desired distribution set Q and the posterior distri-
bution according to the model: P (z1|d1, ?1, ?) and
P (z2|d2, ?2, ?).
The objective function for a single document pair
is as follows:
logP (d1|?1, ?) + logP (d2|?2, ?)
?KL(Q||P (z1|d1, ?1, ?), P (z2|d2, ?2, ?))
?||||
The final corpus-wide objective is summed over
document-pairs, and also contains terms for the
probabilities of the parameters ? and ? given the
Dirichlet priors. The norm of  is minimized, which
makes the expected proportions of topics in two doc-
uments as close as possible.
Following (Ganchev et al, 2009), we fit the pa-
rameters by an EM-like algorithm, where for each
document pair, after finding the posterior distri-
bution of the hidden variables, we find the KL-
projection of this posterior onto the constraint set,
and take expected counts with respect to this projec-
tion; these expected counts are used in the M-step.
The projection is found using a simple projected gra-
dient algorithm.2
For both the baseline JPLSA and the CPLSA
models, we performed learning through MAP infer-
ence using EM (with a projection step for CPLSA).
We did up to 500 iterations for each model, and did
early stopping based on task performance on the de-
velopment set. The JPLSA model required more it-
erations before reaching its peak accuracy, tending
to require around 300 to 450 iterations for conver-
gence. CPLSA required fewer iterations, but each
iteration was slower due to the projection step.
2We initialized the models deterministically by assigning
each word to exactly one topic to begin with, such that all topics
have roughly the same number of words. Words were sorted by
frequency and thus words of similar frequency are more likely
to be assigned to the same topic.This initialization method out-
performed random initialization and we use it for all models.
256
All models use ? = 1.1 and ? = 1.01 for the
values of the concentration parameters. We found
that the performance of the models was not very sen-
sitive to these values, in the region that we tested
(?, ? ? [1.001, 1.1]). Higher hyper-parameter val-
ues resulted in faster convergence, but the final per-
formance was similar across these different values.
3 Experimental validation
We test the proposed discriminative projections ver-
sus more established cross-language models on the
two tasks described in the introduction: retrieving
comparable documents from a corpus, and training
a classifier in one language and using it in another.
We measure accuracy on a test set, and also examine
the sensitivity to dimensionality of the projection on
development sets.
3.1 Speed of training and evaluation
We first test the speed of the various algorithms dis-
cussed in this paper, compared to a full machine
translation system. When finding document projec-
tions, CL-LSI, OPCA, CCA, JPLSA, and CPLSA
are equally fast: they perform a matrix multiplica-
tion and require O(nk) operations, where n is the
number of distinct words in the documents and k is
the dimensionality of the projection.3 A single CPU
core can read the indexed documents into memory
and take logarithms at 216K words per second. Pro-
jecting into a 2000-dimensional space operates at
41K words per second. Translating word-by-word
operates at 274K words per second. In contrast, ma-
chine translation processes 50 words per second, ap-
proximately 3 orders of magnitude slower.
Total training time for OPCA on 43,380 pairs of
comparable documents was 90 minutes, running on
an 8-core CPU for 2000 dimensions. On the same
corpus, JPLSA requires 31 minutes per iteration and
CPLSA requires 377 minutes per iteration. CPLSA
requires a factor of five times fewer iterations: over-
all, it is twice as slow as JPLSA.
3.2 Retrieval of comparable documents
In comparable document retrieval, a query is a doc-
ument in one language, which is compared to a cor-
3For JPLSA and CPLSA this is the case only when perform-
ing a single EM iteration at test time, which we found to per-
form best.
pus of documents in another language. By mapping
all documents into the same vector space, the com-
parison is a vector comparison. For our experiments
with CL-LSI, OPCA, and CCA, we use cosine sim-
ilarity between vectors to rank the documents.
For the JPLSA and CPLSA models, we map the
documents to corresponding topic vectors ?, and
compute distance between these probability vectors.
The mapping to topic vectors requires EM iterations,
or folding-in (Hofmann, 1999). We found that per-
forming a single EM iteration resulted in best per-
formance so we used this for all models. For com-
puting distance we used the L1-norm of the differ-
ence, which worked a bit better than the Jensen-
Shannon divergence between the topic vectors used
in (Mimno et al, 2009).
We test all algorithms on the Europarl data set
of documents in English and Spanish, and a set of
Wikipedia articles in English and Spanish that con-
tain interlanguage links between them (i.e., articles
that the Wikipedia community have identified as
comparable across languages).
For the Europarl data set, we use 52,685 doc-
uments as training, 11,933 documents as a devel-
opment set, and 18,415 documents as a final test
set. Documents are defined as speeches by a sin-
gle speaker, as in (Mimno et al, 2009).4 For the
Wikipedia set, we use 43,380 training documents,
8,675 development documents, and 8,675 final test
set documents.
For both corpora, the terms are extracted by word-
breaking all documents, removing the top 50 most
frequent terms and keeping the next 20,000 most fre-
quent terms. No stemming or folding is applied.
We assess performance by testing each document
in English against all possible documents in Span-
ish, and vice versa. We measure the Top-1 accu-
racy (i.e., whether the true comparable is the clos-
est in the test set), and the Mean Reciprocal Rank
of the true comparable, and report the average per-
formance over the two retrieval directions. Ties are
counted as errors.
We tuned the dimensionality of the projections on
the development set, as shown in Figures 3 and 4.
4The training section contains documents from the years 96
through 99 and the year 02; the dev section contains documents
from 01, and the test section contains documents from 00 plus
the first 9 months of 03.
257
We chose the best dimension on the development set
for each algorithm, and used it on the final test set.
The regularization ? was tuned for CCA: ? = 10 for
Europarl, and ? = 3 for Wikipedia.
Figure 3: Mean reciprocal rank versus dimension for Eu-
roparl
Figure 4: Mean reciprocal rank versus dimension for
Wikipedia
In the two figures, we evaluate the five projec-
tion methods, as well as a word-by-word transla-
tion method (denoted by WbW in the graphs). Here
?word-by-word? refers to using cosine distance after
applying a word-by-word translation model to the
Spanish documents.
The word-by-word translation model was trained
on the Europarl training set, using the WDHMM
model (He, 2007), which performs similarly to IBM
Model 4. The probability matrix of generating
English words from Spanish words was multiplied
by each document?s log(tf)-idf vector to produce a
translated document vector. We found that multi-
plying the probability matrix to the log(tf)-idf vector
was more accurate on the development set than mul-
tiplying the tf vector directly. This vector was either
tested as-is, or mapped through LSA learned from
the English training set of the corpus. In the figures,
the dimensionality of WbW translation refers to the
dimensionality of monolingual LSA.
The overall ordering of the six models is dif-
ferent for the Europarl and Wikipedia development
datasets. The discriminative models outperform
the corresponding generative ones (OPCA vs CL-
LSI) and (CPLSA vs JPLSA) for both datasets, and
OPCA performs best overall, dominating the best
fast-translation based model, as well as the other
projection methods, including CCA.
On Europarl, JPLSA and CPLSA outperform CL-
LSI, with the best dimension or JPLSA also slightly
outperforming the best setting for the word-by-word
translation model, whereas on Wikipedia the PLSA-
based models are significantly worse than the other
models.
The results on the final test set, evaluating each
model using its best dimensionality setting, confirm
the trends observed on the development set. The fi-
nal results are shown in Tables 1 and 2. For these
experiments, we use the unpaired t-test with Bon-
ferroni correction to determine the smallest set of
algorithms that have statistically significantly better
accuracy than the rest. The p-value threshold for sig-
nificance is chosen to be 0.05. The accuracies for
these significantly superior algorithms are shown in
boldface.
For Wikipedia and Europarl, we include an ad-
ditional baseline model,?Untranslated?: this refers
to applying cosine distance to both the Spanish and
English documents directly (since they share some
vocabulary terms). For Wikipedia, comparable doc-
uments seem to share many common terms, so co-
sine distance between untranslated documents is a
reasonable benchmark.
From the final Europarl results we can see that the
best models can learn to retrieve parallel documents
from the narrow Europarl domain very well. All
dimensionality reduction methods can learn from
258
cleanly parallel data, but discriminative training can
bring additional error reduction.
In previously reported work, (Mimno et al, 2009)
evaluate parallel document retrieval using PLTM on
Europarl speeches in English and Spanish, using
training and test sets of size similar to ours. They
report an accuracy of 81.2% when restricting to test
documents of length at least 100 and using 50 topics.
JPLSA with 50 topics obtains accuracy of 98.9% for
documents of that length.
The final Wikipedia results are also similar to the
the development set results. The problem setting for
Wikipedia is different, because corresponding doc-
uments linked in Wikipedia may have widely vary-
ing degrees of parallelism. While most linked doc-
uments share some main topics, they could cover
different numbers of sub-topics at varying depths.
Thus the training data of linked documents is noisy,
which makes it hard for projection methods to learn.
The word-by-word translation model in this setting
is trained on clean, but out-of-domain parallel data
(Europarl), so it has the disadvantage that it may not
have a good coverage of the vocabulary; however,
it is not able to make use of the Wikipedia train-
ing data since it requires sentence-aligned transla-
tions. We find it encouraging that the best projection
method OPCA outperformed word-by-word trans-
lation. This means that OPCA is able to uncover
topic correspondence given only comparable docu-
ment pairs, and to learn well in this noisy setting.
The PLSA-based models fare worse on Wikipedia
document retrieval. CPLSA outperforms JPLSA
more strongly, but both are worse than CL-LSI and
even the Untranslated baseline. We think this is
partly explained by the diverse vocabulary in the het-
erogenous Wikipedia collection. All other models
use log(tf)-idf weighting, which automatically as-
signs importance weights to terms, whereas the topic
models use word counts. This weighting is very use-
ful for Wikipedia. For example, if we apply the
untranslated matching using raw word counts, the
MRR is 0.1024 on the test set, compared to 0.5383
for log(tf)-idf. We hypothesize that using a hierar-
chical topic model that automatically learns about
more general and more topic-specific words would
be helpful in this case. It is also possible that PLSA-
based models require cleaner data to learn well.
The overall conclusion is that OPCA outper-
Algorithm Dimension Accuracy MRR
OPCA 1000 0.9742 0.9806
CPLSA 1000 0.9716 0.9782
Word-by-word N/A 0.9707 0.9779
Word-by-word 5000 0.9706 0.9778
JPLSA 1000 0.9645 0.9726
CCA 1500 0.9613 0.9705
CL-LSI 3000 0.9457 0.9595
Untranslated N/A 0.1595 0.2564
Table 1: Test results for comparable document retrieval
in Europarl. Boldface indicates statistically significant
superior results.
Algorithm Dimension Accuracy MRR
OPCA 2000 0.7255 0.7734
Word-by-word N/A 0.7033 0.7467
CCA 1500 0.6894 0.7378
Word-by-word 5000 0.6786 0.7236
CL-LSI 5000 0.5302 0.6130
Untranslated N/A 0.4692 0.5383
CPLSA 200 0.4579 0.5130
JPLSA 1000 0.3322 0.3619
Table 2: Test results for comparable document retrieval
in Wikipedia. Boldface indicates statistically significant
best result.
formed all other document retrieval methods we
tested, including fast machine translation of docu-
ments. Additionally, both discriminative projection
methods outperformed their generative counterparts.
3.3 Cross-language text classification
The second task is to train a text categorization sys-
tem in one language, and test it with documents in
another. To evaluate on this task, we use the Mul-
tilingual Reuters Collection, defined and provided
by (Amini et al, 2009). We test the English/Spanish
language pair. The collection has news articles in
English and Spanish, each of which has been trans-
lated to the other by the Portage translation sys-
tem (Ueffing et al, 2007).
From the English news corpus, we take 13,131
documents as training, 1,875 documents as develop-
ment, and 1,875 documents as test. We take the En-
glish training documents translated into Spanish as
our comparable training data. For testing, we use the
entire Spanish news corpus of 12,342 documents, ei-
259
ther mapped with cross-lingual projection, or trans-
lated by Portage.
The data set was provided by (Amini et al,
2009) as already-processed document vectors, using
BM25 weighting. Thus, we only test OPCA, CL-
LSI, and related methods: JPLSA and CPLSA re-
quire modeling the term counts directly.
The performance on the task is measured by clas-
sification accuracy on the six disjoint category la-
bels defined by (Amini et al, 2009). To introduce
minimal bias due to the classifier model, we use 1-
nearest neighbor on top of the cosine distance be-
tween vectors as a classifier. For all of the tech-
niques, we treated the vocabulary in each language
as completely separate, using the top 10,000 terms
from each language.
Note that no Spanish labeled data is provided
for training any of these algorithms: only English
and translated English news is labeled. The op-
timal dimension (and ? for CCA) on the devel-
opment set was chosen to maximize the accuracy
of English classification and translated English-to-
Spanish classification.
Algorithm Dim. English Spanish
Accuracy Accuracy
Full MT 50 0.8483 0.6484
OPCA 100 0.8412 0.5954
Word-by-word 50 0.8483 0.5780
CCA 150 0.8388 0.5384
Full MT N/A 0.8046 0.5323
CL-LSI 150 0.8401 0.5105
Word-by-word N/A 0.8046 0.4481
Table 3: Test results for cross-language text categoriza-
tion
The test classification accuracy is shown in Ta-
ble 3. As above, the smallest set of superior al-
gorithms as determined by Bonferroni-corrected t-
tests are shown in boldface. The results for MT and
word-by-word translation use the log(tf)-idf vector
directly for documents that were written in English,
and use a Spanish-to-English translated vector if the
document was written in Spanish. As in section 3.2,
word-by-word translation multiplied each log(tf)-idf
vector by the translation probability matrix trained
on Europarl.
The tests show that OPCA is better than CCA,
CL-LSI, plain word-by-word translation, and even
full translation for Spanish documents. However,
if we post-process full translation by an LSI model
trained on the English training set, full translation
is the most accurate. If full translation is time-
prohibitive, then OPCA is the best method: it is sig-
nificantly better than word-by-word translation fol-
lowed by LSI.
4 Discussion and Extensions
OPCA extends naturally to multiple languages.
However, it requires memory and computation time
that scales quadratically with the size of the vocab-
ulary. As the number of languages goes up, it may
become impractical to perform OPCA directly on a
large vocabulary.
Researchers have solved the problem of scaling
OPCA by using Distortion Discriminant Analysis
(DDA) (Burges et al, 2003). DDA performs OPCA
in two stages which avoids the need for solving a
very large generalized eigensystem. As future work,
DDA could be applied to mapping documents in
many languages simultaneously.
Spherical Admixture Models (Reisinger et al,
2010) have recently been proposed that combine an
LDA-like hierarchical generative model with the use
of tf-idf representations. A similar model could be
used for CPLSA: future work will show whether
such a model can outperform OPCA.
5 Conclusions
This paper presents two different methods for creat-
ing discriminative projections: OPCA and CPLSA.
Both of these methods avoid the use of artificial
concatenated documents. Instead, they model docu-
ments in multiple languages, with the constraint that
comparable documents should map to similar loca-
tions in the projected space.
When compared to other techniques, OPCA had
the highest accuracy while still having a run-time
that allowed scaling to large data sets. We therefore
recommend the use of OPCA as a pre-processing
step for large-scale comparable document retrieval
or cross-language text categorization.
260
References
Massih-Reza Amini, Nicolas Usunier, and Cyril Goutte.
2009. Learning from multiple partially observed
views - an application to multilingual text categoriza-
tion. In Advances in Neural Information Processing
Systems 22 (NIPS 2009), pages 28?36.
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference
for topic models. In Proceedings of Uncertainty in Ar-
tificial Intelligence, pages 27?34.
Lisa Ballesteros and Bruce Croft. 1996. Dictionary
methods for cross-lingual information retrieval. In
Proceedings of the 7th International DEXA Confer-
ence on Database and Expert Systems Applications,
pages 791?801.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent Dirichlet alocation.
Journal of Machine Learning Research, 3:993?1022.
Christopher J.C. Burges, John C. Platt, and Soumya Jana.
2003. Distortion discriminant analysis for audio fin-
gerprinting. IEEE Transactions on Speech and Audio
Processing, 11(3):165?174.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Konstantinos I. Diamantaras and S.Y. Kung. 1996. Prin-
cipal Component Neural Networks: Theory and Appli-
cations. Wiley-Interscience.
Susan T. Dumais, Todd A. Letsche, Michael L. Littman,
and Thomas K. Landauer. 1997. Automatic cross-
language retrieval using latent semantic indexing. In
AAAI-97 Spring Symposium Series: Cross-Language
Text and Speech Retrieval.
Susan T. Dumais. 1990. Enhancing performance in la-
tent semantic indexing (LSI) retrieval. Technical Re-
port TM-ARH-017527, Bellcore.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of COLING-ACL, pages
414?420.
Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and
Ben Taskar. 2009. Posterior regularization for struc-
tured latent variable models. Technical Report MS-
CIS-09-16, University of Pennsylvania.
Joao Graca, Kuzman Ganchev, and Ben Taskar. 2008.
Expectation maximization and posterior constraints.
In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, edi-
tors, Advances in Neural Information Processing Sys-
tems 20, pages 569?576. MIT Press, Cambridge, MA.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proc. ACL, pages 771?
779.
Xiaodong He. 2007. Using word-dependent transition
models in HMM based word alignment for statistical
machine translation. In ACL 2nd Statistical MT work-
shop, pages 80?87.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of Uncertainty in Artificial
Intelligence, pages 289?296.
Jagadeesh Jagarlamudi and Hal Daume?, III. 2010. Ex-
tracting multilingual topics from unaligned compara-
ble corpora. In ECIR.
David Mimno, Hanna W. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of Empir-
ical Methods in Natural Language Processing, pages
880?889.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31:477?504.
Douglas W. Oard and Anne R. Diekema. 1998. Cross-
language information retrieval. In Martha Williams,
editor, Annual Review of Information Science (ARIST),
volume 33, pages 223?256.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proc. EMNLP, pages
79?86.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of the ACL, pages 519?526.
Joseph Reisinger, Austin Waters, Bryan Silverthorn, and
Raymond J. Mooney. 2010. Spherical topic models.
In Proc. ICML.
Nicola Ueffing, Michel Simard, Samuel Larkin, and
J. Howard Johnson. 2007. NRC?s PORTAGE system
for WMT 2007. In ACL-2007 2nd Workshop on SMT,
pages 185?188.
Alexei Vinokourov, John Shawe-Taylor, and Nello Cris-
tianini. 2003. Inferring a semantic representation
of text via cross-language correlation analysis. In
S. Thrun S. Becker and K. Obermayer, editors, Ad-
vances in Neural Information Processing Systems 15,
pages 1473?1480, Cambridge, MA. MIT Press.
Fridolin Wild, Christina Stahl, Gerald Stermsek, and
Gustaf Neumann. 2005. Parameters driving effective-
ness of automated essay scoring with LSA. In Pro-
ceedings 9th Internaional Computer-Assisted Assess-
ment Conference, pages 485?494.
Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai. 2010.
Cross-lingual latent topic extraction. In Proc. ACL,
pages 1128?1137, Uppsala, Sweden. Association for
Computational Linguistics.
261
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1212?1222, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Polarity Inducing Latent Semantic Analysis
Wen-tau Yih Geoffrey Zweig John C. Platt
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
{scottyih,gzweig,jplatt}@microsoft.com
Abstract
Existing vector space models typically map
synonyms and antonyms to similar word vec-
tors, and thus fail to represent antonymy. We
introduce a new vector space representation
where antonyms lie on opposite sides of a
sphere: in the word vector space, synonyms
have cosine similarities close to one, while
antonyms are close to minus one.
We derive this representation with the aid of a
thesaurus and latent semantic analysis (LSA).
Each entry in the thesaurus ? a word sense
along with its synonyms and antonyms ? is
treated as a ?document,? and the resulting doc-
ument collection is subjected to LSA. The key
contribution of this work is to show how to as-
sign signs to the entries in the co-occurrence
matrix on which LSA operates, so as to induce
a subspace with the desired property.
We evaluate this procedure with the Grad-
uate Record Examination questions of (Mo-
hammed et al 2008) and find that the method
improves on the results of that study. Further
improvements result from refining the sub-
space representation with discriminative train-
ing, and augmenting the training data with
general newspaper text. Altogether, we im-
prove on the best previous results by 11 points
absolute in F measure.
1 Introduction
Vector space representations have proven useful
across a wide variety of text processing applications
ranging from document clustering to search rele-
vance measurement. In these applications, text is
represented as a vector in a multi-dimensional con-
tinuous space, and a similarity metric such as co-
sine similarity can be used to measure the related-
ness of different items. Vector space representations
have been used both at the document and word lev-
els. At the document level, they are effective for
applications including information retrieval (Salton
and McGill, 1983; Deerwester et al 1990), docu-
ment clustering (Deerwester et al 1990; Xu et al
2003), search relevance measurement (Baeza-Yates
and Ribiero-Neto, 1999) and cross-lingual docu-
ment retrieval (Platt et al 2010). At the word level,
vector representations have been used to measure
word similarity (Deerwester et al 1990; Turney and
Littman, 2005; Turney, 2006; Turney, 2001; Lin,
1998; Agirre et al 2009; Reisinger and Mooney,
2010) and for language modeling (Bellegarda, 2000;
Coccaro and Jurafsky, 1998). While quite success-
ful, these applications have typically been consistent
with a very general notion of similarity in which
basic association is measured, and finer shades of
meaning need not be distinguished. For example,
latent semantic analysis might assign a high degree
of similarity to opposites as well as synonyms (Lan-
dauer and Laham, 1998; Landauer, 2002).
Independent of vector-space representations, a
number of authors have focused on identifying dif-
ferent kinds of relatedness. At the simplest level,
we may wish to distinguish between synonyms and
antonyms, which can be further differentiated. For
example, in synonymy, we may wish to distinguish
hyponyms and hypernyms. Moreover, Cruse (1986)
notes that numerous kinds of antonymy are possible,
for example antipodal pairs like ?top-bottom? or
1212
gradable opposites like ?light-heavy.? Work in this
area includes (Turney, 2001; Lin et al 2003; Tur-
ney and Littman, 2005; Turney, 2006; Curran and
Moens, 2002; van der Plas and Tiedemann, 2006;
Mohammed et al 2008; Mohammed et al 2011).
Despite the existence of a large amount of related
work in the literature, distinguishing synonyms and
antonyms is still considered as a difficult open prob-
lem in general (Poon and Domingos, 2009).
In this paper, we fuse these two strands of re-
search in an attempt to develop a vector space rep-
resentation in which the synonymy and antonymy
are naturally differentiated. We follow Schwab et
al. (2002) in requiring a representation in which
two lexical items in an antonymy relation should lie
at opposite ends of an axis. However, in contrast
to the logical axes used previously, we desire that
antonyms should lie at the opposite ends of a sphere
lying in a continuous and automatically induced vec-
tor space. To generate this vector space, we present
a novel method for assigning both negative and pos-
itive values to the TF-IDF weights used in latent se-
mantic analysis.
To determine these signed values, we exploit the
information present in a thesaurus. The result is a
vector space representation in which synonyms clus-
ter together, and the opposites of a word tend to clus-
ter together at the opposite end of a sphere.
This representation provides several advantages
over the raw thesaurus. First, by finding the items
most and least similar to a word, we are able to dis-
cover new synonyms and antonyms. Second, as dis-
cussed in Section 5, the representation provides a
natural starting point for gradient-descent based op-
timization. Thirdly, as we discuss in Section 6, it is
straightforward to embed new words into the derived
subspace by using information from a large unsuper-
vised text corpus such as Wikipedia.
The remainder of this paper is organized as fol-
lows. Section 2 describes previous work. Section 3
presents the classical LSA approach and analyzes
some of its limitations. In Section 4 we present our
polarity inducing extension to LSA. Section 5 fur-
ther extends the approach by optimizing the vector
space representation with supervised discriminative
training. Section 6 describes the proposed method of
embedding new words in the thesaurus-derived sub-
space. The experimental results of Section 7 indi-
cate that the proposed method outperforms previous
approaches on a GRE test of closest-opposites (Mo-
hammed et al 2008). Finally, Section 8 concludes
the paper.
2 Related Work
The detection of antonymy has been studied in a
number of previous papers. Mohammed et al(2008)
approach the problem by combining information
from a published thesaurus with corpus statistics de-
rived from the Google n-gram corpus (Brants and
Franz, 2006). Their method consists of two main
steps: first, detecting contrasting word categories
(e.g. ?WORK? vs. ?ACTIVITY FOR FUN?) and
then determining the degree of antonymy. Cate-
gories are defined by a thesaurus; contrasting cat-
egories are found by using affix rules (e.g., un- &
dis-) and WordNet antonymy links. Words belong-
ing to contrasting categories are treated as antonyms
and the degree of contrast is determined by distri-
butional similarity. Mohammed et al(2008) also
provides a publicly available dataset for detection of
antonymy, which we have adopted. This work has
been extended in (Mohammed et al 2011) to in-
clude a study of antonymy based on crowd-sourcing
experiments.
Turney (2008) proposes a unified approach to
handling analogies, synonyms, antonyms and asso-
ciations by transforming the last three cases into
cases of analogy. A supervised learning method
is then used to solve the resulting analogical prob-
lems. This is evaluated on a set of 136 ESL ques-
tions. Lin et al(2003) builds on (Lin, 1998) and
identifies antonyms as semantically related words
which also happen to be found together in a database
in pre-identified phrases indicating opposition. Lin
et al(2003) further note that whereas synonyms
will tend to translate to the same word in another
language, antonyms will not. This observation is
used to select antonyms from amongst distribution-
ally similar words. Antonymy is used in (de Si-
mone and Kazakov, 2005) for document clustering
and (Harabagiu et al 2006) to find contradiction.
The automatic detection of synonyms has been
more extensively studied. Lin (1998) presents
a thorough comparison of word-similarity metrics
based on distributional similarity, where this is de-
1213
termined from co-occurrence statistics in depen-
dency triples extracted by parsing a large dataset.
Related studies are described in (Curran and Moens,
2002; van der Plas and Bouma, 2005). Later, van
der Plas and Tiedemann (2006) extend the use of
multilingual data present in Lin et al(2003) by mea-
suring distributional similarity based on the contexts
that a word occurs in once translated into a new lan-
guage. This is used to improve the precision/recall
characteristics on synonym pairs. Structured infor-
mation can be important in determining relatedness,
and thesauri and Wikipedia links have been studied
in (Milne and Witten, 2008; Jarmasz and Szpakow-
icz, 2003). Combinations of approaches are studied
in (Turney et al 2003).
Vector-space models and latent semantic analysis
in particular have a long history of use in synonym
detection, which in fact was suggested in some of
the earliest LSA papers. Deerwester et al(1990)
defines a metric for measuring word similarity based
on LSA, and it has been used in (Landauer and Du-
mais, 1997; Landauer et al 1998) to answer word
similarity questions derived from the Test of English
as a Foreign Language (TOEFL). Turney (2001)
proposes the use of point-wise mutual information in
conjunction with LSA, and again presents results on
synonym questions derived from the TOEFL. Vari-
ants of vector space models are further analyzed
in (Turney and Littman, 2005; Turney, 2006; Tur-
ney and Pantel, 2010).
3 Latent Semantic Analysis
Latent Semantic Analysis (Deerwester et al 1990)
is a widely used method for representing words and
documents in a low dimensional vector space. The
method is based on applying singular value decom-
position (SVD) to a matrix W which indicates the
occurrence of words in documents. To perform
LSA, one proceeds as follows. The input is a col-
lection of d documents which are expressed in terms
of words from a vocabulary of size n. These docu-
ments may be actual documents such as newspaper
articles, or simply notional documents such as sen-
tences, or any other collection in which words are
grouped together. Next, a d?n document-term ma-
trix W is formed1. At its simplest form, the ijth
entry contains the number of times word j has oc-
curred in document i ? its term frequency or TF
value. More conventionally, the entry is weighted
by some notion of the importance of word j, for ex-
ample the negative logarithm of the fraction of doc-
uments that contain it, resulting in a TF-IDF weight-
ing (Salton et al 1975). The similarity between two
documents can be computed using the cosine simi-
larity of their corresponding row vectors:
sim(x,y) =
x ? y
? x ?? y ?
Similarly, the cosine similarity of two column vec-
tors can be used to judge the similarity of the corre-
sponding words. Finally, to obtain a subspace repre-
sentation of dimension k, W is decomposed as
W ? USV T
where U is d ? k, V T is k ? n, and S is a k ? k
diagonal matrix. In applications, k  n and k  d;
for example one might have a 50, 000 word vocab-
ulary and 1, 000, 000 documents and use a 300 di-
mensional subspace representation.
An important property of SVD is that the columns
of SV T ? which now represent the words ? behave
similarly to the original columns of W , in the sense
that the cosine similarity between two columns in
SV T approximates the cosine similarity between the
corresponding columns in W . This follows from
the observation that W TW = V S2V T , and the fact
that the ijth entry of W TW is the dot product of
the ith and jth columns (words) in W . We will
use this observation subsequently in the derivation
of polarity-inducing LSA. For efficiency, we nor-
malize the columns of SV T to unit length, allow-
ing the cosine similarity between two words to be
computed with a single dot-product; this also has the
property of mapping each word to a point on a multi-
dimensional sphere.
A second important property of LSA is that in the
word representations which result can by viewed as
the result of applying a projection matrix U to the
original vectors as:
UTW = SV T
1(Bellegarda, 2000) constructs the transpose of this, but we
have found it convenient in data processing for documents to
represent rows.
1214
In Section 5, we will viewU simply as a d?k matrix
learned through gradient descent so as to optimize
an objective function.
3.1 Limitation of LSA
Word similarity as determined by LSA assigns high
values to words which tend to co-occur in doc-
uments. However, as noted by (Landauer and
Laham, 1998; Landauer, 2002), there is no no-
tion of antonymy; words with low or negative co-
sine scores are simply unrelated. In comparison,
words with high cosine similarity scores are typi-
cally semantically related, which includes both syn-
onyms and antonyms, as contrasting words often co-
occur (Murphy and Andrew, 1993; Mohammed et
al., 2008). To illustrate this, we have performed
SVD with the aid of the Encarta thesaurus developed
by Bloomsbury Publishing Plc. This thesaurus con-
tains approximately 47k word senses and a vocab-
ulary of 50k words and phrases. Each ?document?
is taken to be the thesaurus entry for a word-sense,
including synonyms and antonyms. For example,
the word ?admirable? induces a document consist-
ing of {admirable, estimable, commendable, vener-
able, good, splendid, worthy, marvelous, excellent,
unworthy}. Note that the last word in this set is its
antonym. Performing SVD on this set of thesaurus
derived ?meaning-documents? results in a subspace
representation for each word. This form of LSA is
similar to the use of Wikipedia in (Gabrilovich and
Markovitch, 2007).
Table 1 shows some words, their original the-
saurus documents, and the most and least similar
words in the LSA subspace. Several properties are
apparent:
? The vector-space representation of words is
able to identify related words that are not ex-
plicitly present in the original thesaurus. For
example, ?meritorious? for ?admirable? ? ar-
guably better than any of the words given in the
thesaurus itself.
? Similarity is based on co-occurrence, so the
co-occurrence of antonyms in the thesaurus-
derived documents induces their presence as
LSA-similar words. For example, ?con-
temptible? is identified as similar to ?ad-
mirable.? In the case of ?mourning,? opposites
acrimony rancor goodwill affection
acrimony 1 1 1 1
affection 1 1 1 1
Table 2: The W matrix for two thesaurus entries in
its original form. Rows represent documents; columns
words.
acrimony rancor goodwill affection
acrimony 1 1 -1 -1
affection -1 -1 1 1
Table 3: The W matrix for two thesaurus entries in its
polarity-inducing form.
such as ?joy? and ?elation? actually dominate
the list of LSA-similar words.
? The LSA-least-similar words have no relation-
ship at all to the word they are least-similar to.
For example, the least-similar word to ?consid-
ered? is ?ready-made-meal.?
In the next section, we will present a method for
inducing polarity in LSA subspaces, where opposite
words will tend to have negative cosine similarities,
analogous to the positive similarities of synonyms.
Thus, the least-similar words to a given word will be
its opposites.
4 Polarity Inducing LSA
We modify LSA so that we may exploit a thesaurus
to embed meaningful axes in the induced subspace
representation. Words with opposite meaning will
lie at opposite positions on a sphere. Recall that the
cosine similarity between word-vectors in the orig-
inal matrix W are preserved in the subspace repre-
sentation of words. Thus, if we construct the original
matrix so that the columns representing antonyms
will tend to have negative cosine similarities while
columns representing synonyms will tend to have
positive similarities, we will achieve the desired be-
havior.
This can be achieved by negating the TF-IDF en-
tries for the antonyms of a word when constructing
W from the thesaurus, which is illustrated in Ta-
bles 2 and 3. The two rows in these tables corre-
spond to thesaurus entries for the sense-categories
1215
Word Thesaurus Entry LSA Most-Similar Words LSA Least-Similar Words
admirable estimable, commendable,
venerable, good, splen-
did, worthy, marvelous,
excellent, unworthy
commendable, creditable,
laudable, praiseworthy,
worthy, meritorious,
scurvy, contemptible,
despicable, estimable
easy-on-the-eye, peace-
keeper, peace-lover,
conscientious-objector,
uninviting, dishy, dessert,
pudding, seductive
considered careful, measured, well-
thought-out, painstaking,
rash
calculated, premeditated,
planned, tactical, strate-
gic, thought-through, in-
tentional, fortuitous, pur-
poseful, unpremeditated
ready-made-meal, ready-
meal, disposed-to, apt-to,
wild-animals, big-game,
game-birds, game-fish,
rugger, rugby
mourning grief, bereavement, sor-
row, sadness, lamenta-
tion, woe, grieving, exul-
tation
sorrowfulness, anguish,
exultation, rejoicing, ju-
bilation, glee, heartache,
travail, joy, elation
muckiness, turn-the-
corner, impassibility,
filminess, pellucidity,
limpidity, sheerness
Table 1: LSA on a thesaurus. Thesaurus entries include antonyms in italics.
?acrimony,? and ?affection.? The thesaurus entries
induce two ?documents? containing the words and
their synonyms and antonyms. The complete set of
words is acrimony, rancor, goodwill, affection. For
simplicity, we assume that all TF-IDF weights are
1. In the original LSA formulation, we have the rep-
resentation of Table 2. ?Rancor? is listed as a syn-
onym of ?acrimony,? which has ?goodwill? and ?af-
fection? as its antonyms. This results in the first row.
Note that the cosine similarity between every pair of
words (columns) is 1.
Table 3 shows the polarity-inducing representa-
tion. Here, the cosine similarity between synony-
mous words (columns) is 1, and the cosine similarity
between antonymous words is -1. Since LSA tends
to preserve cosine similarities between words, in the
resulting subspace we may expect to find meaning-
ful axes, where opposite senses map to opposite ex-
tremes. We refer to this as polarity-inducing LSA or
PILSA.
In Table 4, we show the PILSA-similar and
PILSA-least-similar words for the same words as in
Table 1. We see now that words which are least
similar in the sense of having the lowest cosine-
similarity are indeed opposites. In this table gen-
erally the most similar words have similarities in the
range of 0.7 to 1.0 and the least similar in the range
of -0.7 to -1.0.
5 Discriminative Training
Although the cosine similarity of LSA-derived word
vectors are generally very effective in applications
such as judging the relevance of words or docu-
ments, or detecting antonyms as in our construction,
the process of singular value decomposition in LSA
does not explicitly try to achieve such goals. In this
section, we see that when supervised training data is
available, the projection matrix of LSA can be en-
hanced through a discriminative training technique
explicitly designed to create a representation suited
to a specific task.
Because LSA is closely related to principle com-
ponent analysis (PCA), extensions of PCA such as
canonical correlation analysis (CCA) and oriented
principle component analysis (OPCA) can leverage
the labeled data and produce the projection matrix
through general eigen-decomposition (Platt et al
2010). Along this line of work, Yih et al(2011)
proposed a Siamese neural network approach called
S2Net, which tunes the projection matrix directly
through gradient descent, and has shown to outper-
form other methods in several tasks. Below we de-
scribe briefly this technique and explain how we
adopt it for the task of antonym detection.
The goal of S2Net is to learn a concept vector
representation of the original sparse term vectors.
Although such transformation can be non-linear in
general, its current design chooses the model form
to be a linear projection matrix, which is identical to
1216
Word PILSA-Similar Words PILSA-Least-Similar Words
admirable commendable, creditable, laudable,
praiseworthy, worthy, meritorious, es-
timable, deserving, tiptop, valued
scurvy, contemptible, despicable,
lamentable, shameful, reprehensible,
unworthy, disgraceful, discreditable,
undeserving
considered calculated, premeditated, planned, tac-
tical, strategic, thought-through, inten-
tional, purposeful, intended, psycho-
logical
fortuitous, unpremeditated, unconsid-
ered, off-your-own-bat, unintended,
undirected, objectiveless, hit-and-miss,
unforced, involuntary
mourning sorrowful, doleful, sad, miserable,
wistful, pitiful, wailing, sobbing,
heavy-hearted, forlorn
smiley, happy, blissful, wooden, mirth-
ful, joyful, deadpan, fulfilled, straight-
faced, content
Table 4: PILSA on a thesaurus. Thesaurus entries are as in Table 1.
that of LSA, PCA, OPCA or CCA. Given a d-by-1
input vector f , the model of S2Net is a d-by-k ma-
trix A = [aij ]d?k, which maps f to a k-by-1 output
vector g = AT f . The fact that the transformation
can be viewed as a two-layer neural network leads
to the method?s name.
What differentiates S2Net from other approaches
is its loss function and optimization process. In
the ?parallel text? setting, the labeled data con-
sists of pairs of similar text objects such as doc-
uments. The objective of the training process is
to assign higher cosine similarities to these pairs
compared to others. More specifically, suppose the
training set consists of m pairs of raw input vectors
{(fp1 , fq1), (fp2 , fq2), ? ? ? , (fpm , fqm)}. Given a pro-
jection matrix A, the similarity score of any pair of
objects is simA(fpi , fqj ) = cosine(A
T fpi ,A
T fqj ).
Let ?ij = simA(fpi , fqi) ? simA(fpi , fqj ) be the
difference of the similarity scores of (fpi , fqi) and
(fpi , fqj ). The learning procedure tries to increase
?ij by using the following logistic loss:
L(?ij ;A) = log(1 + exp(???ij)),
where ? is a scaling factor that adjusts the loss func-
tion2. The loss of the whole training set is thus:
1
m(m? 1)
?
1?i,j?m,i 6=j
L(?ij ;A)
Parameter learning (i.e., tuning A) can be done
2As suggested in (Yih et al 2011), ? is set to 10 in our
experiments.
by standard gradient-based methods, such as L-
BFGS (Nocedal and Wright, 2006).
The original setting of S2Net can be directly ap-
plied to finding synonymous words, where the train-
ing data consists of pairs of vectors representing
two synonyms. It is also easy to modify the loss
function to apply it to the antonym detection prob-
lem. We first sample pairs of antonyms from the
thesaurus to create the training data. The raw input
vector f of a selected word is its corresponding col-
umn vector of the document-term matrix W (Sec-
tion 3) after inducing polarity (Section 4). When
each pair of vectors in the training data represents
two antonyms, we can redefine ?ij by flipping the
sign: ?ij = simA(fpi , fqj ) ? simA(fpi , fqi), and
leave others unchanged. As the loss function encour-
ages ?ij to be larger, an antonym pair will tend to
have a smaller cosine similarity than other pairs. Be-
cause S2Net uses a gradient descent technique and a
non-convex objective function, it is sensitive to ini-
tialization, and we have found that the PILSA pro-
jection matrix U (Section 3) provides an excellent
starting point. As illustrated in Section 7, learning
the word vectors with S2Net produces a significant
improvement over PILSA alone.
6 Extending PILSA to Out-of-thesaurus
Words
While PILSA is effective at representing synonym
and antonym information, in its pure form, it is lim-
ited to the vocabulary of the thesaurus. In order to
extend PILSA to operate on out-of-thesaurus words,
1217
we employ a two-stage strategy. We first conduct
some lexical analysis and try to match an unknown
word to one or more in-thesaurus words in their lem-
matized forms. If no such match can be found,
we then attempt to find semantically related in-
thesaurus words by leveraging co-occurrence statis-
tics from general text data. These two steps are de-
scribed in detail below.
6.1 Matching via Lexical Analysis
When a target word is not included in a thesaurus, it
is quite often that some of its morphological varia-
tions are covered. For example, although the Encarta
thesaurus does not have the word ?corruptibility,?
it does contain other forms like ?corruptible? and
?corruption.? Replacing the out-of-thesaurus target
word with these morphological variations may alter
the part-of-speech but typically does not change the
meaning3.
Given an out-of-thesaurus target word, we first
apply a morphological analyzer for English devel-
oped by Minnen et al(2001), which removes the
inflectional affixes and returns the lemma. If the
lemma still does not exist in the thesaurus, we then
apply Porter?s stemmer (Porter, 1980) and check
whether the target word can match any of the in-
thesaurus words in their stemmed forms. A sim-
ple rule that checks whether removing hyphens from
words can lead to a match and whether the target
word occurs as part of a compound word in the the-
saurus is applied when both morphological analysis
and stemming fail to find a match. When there are
more than one matched words, the centroid of their
PILSA vectors is used to represent the target word.
When there is only one matched word, the matched
word is treated as the target word.
6.2 Leveraging General Text Data
If no words in the thesaurus can be linked to the
target word through the simple lexical analysis pro-
cedure, we try to find matched words by creating
a context vector space model from a large docu-
ment collection, and then mapping from this space
to the PILSA space. We use contexts because of the
distributional hypothesis ? words that occur in the
same contexts tend to have similar meaning (Harris,
3The rules we use based on lexical analysis are moderately
conservative to avoid mistakes like mapping hopeless to hope.
1954). When a word is not in the thesaurus but ap-
pears in the corpus, we predict its PILSA vector rep-
resentation from the context vector space model by
using its k-nearest neighbors which are in the the-
saurus and consistent with each other.
6.2.1 Context Vector Space Model
Given a corpus of documents, we construct the
raw context vectors as follows. For each target word,
we first create a bag of words by collecting all the
terms within a window of [-10,+10] centered at each
occurrence of the target word in the corpus. The
non-identical terms form a term-vector, where each
term is weighted using its TF-IDF value. We then
perform LSA on the context-word matrix. The se-
mantic similarity/relatedness of two words can then
be determined using the cosine similarity of their
corresponding LSA word vectors. In the following
text, we refer to this LSA context vector space model
as the corpus space, in contrast to the PILSA the-
saurus space.
6.2.2 Embedding Out-of-Vocabulary Words
Given the context space model, we may use a
linear regression or a k-nearest neighbors approach
to embed out-of-thesaurus words into the thesaurus-
space representation. However, as near words in the
context space may be synonyms in addition to other
semantically related words (including antonyms),
such approaches can potentially be noisy. For ex-
ample, words like ?hot? and ?cold? may be close
to each other in the context space due to their sim-
ilar usage in text. An affine transform cannot ?tear
space? and map them to opposite poles in the the-
saurus space.
Therefore, we propose a revised k-nearest neigh-
bors approach. Suppose we are interested in an out-
of-thesaurus word w. We first find K-nearest in-
thesaurus neighbors to w in the context space. We
then select a subset of k members of these K words
such that the pairwise similarity of each of the k
members with every other member is positive. The
thesaurus-space centroid of these k items is com-
puted as w?s representation. This procedure has the
property that the k nearby words used to form the
embedding of a non-thesaurus word are selected to
be consistent with each other. In practice, we used
K = 10 and k = 3, which requires only around
1218
1000 pairwise computations even done in a brute-
force way. To provide a concrete example, if we
had the out-of-thesaurus word ?sweltering? with in-
thesaurus neighbors ?hot, cold, burning, scorching,
...? the procedure would return the centroid of ?hot,
burning, scorching? and exclude ?cold.?
7 Experimental Validation
In this section, we present our experimental results
on applying PILSA and its extensions to answering
the closest-opposite GRE questions.
7.1 Data Resources
The primary thesaurus we use is the Encarta The-
saurus developed by Bloomsbury Publishing Plc4.
Our version of this has approximately 47k word
senses and a vocabulary of 50k words, and con-
tains 125,724 pairs of antonyms. To experiment with
the effect of using a different thesaurus, we used
WordNet as an information source. Each synset in
WordNet maps to a row in the document-term ma-
trix; synonyms in a synset are weighted with posi-
tive TFIDF values, and antonyms are weighted neg-
ative TFIDF values. Entries corresponding to other
words in the vocabulary are 0. WordNet provides
significantly greater coverage with approximately
227k synsets involving multiple words, and a vo-
cabulary of about 190k words. However, it is also
much sparser, with 5.3 words per sense on average
as opposed to 10.3 in the thesaurus, and has only
62,821 pairs of antonyms. As general text data for
use in embedding out-of-vocabulary words, we used
a Nov-2010 dump of English Wikipedia, which con-
tains approximately 917M words.
7.2 Development and Test Data
For testing, we use the closest-opposite questions
from GRE tests provided by (Mohammed et al
2008). Each question contains a target word and
five choices, and asks which of the choice words has
the most opposite meaning to the target word. Two
datasets are made publicly available by Mohammad
et al(2008): the development set, which consists of
162 questions, and the test set, which has 950 ques-
tions5. We considered making our own, more exten-
4http://www.bloomsbury.com/
5http://www.umiacs.umd.edu/?saif/WebDocs/LC-
data/{devset,testset}.txt
Dimensions Bloomsbury Prec. WordNet Prec.
50 0.778 0.475
100 0.850 0.563
200 0.856 0.569
300 0.863 0.625
400 0.843 0.625
500 0.843 0.613
750 0.830 0.613
1000 0.837 0.544
2000 0.784 0.519
3000 0.778 0.494
Table 5: The performance of PILSA vs. the number of di-
mensions when applied to the closest-opposite questions
from the GRE development set. Out of the 162 ques-
tions, using the Bloomsbury thesaurus data we are able
to answer 153 of them. Using 300 dimensions gives the
best precision (132/153 = 0.863). This dimension set-
ting is also optimal when using the WordNet data, which
answers 100 questions correctly out of the 160 attempts
(100/160 = 0.625).
sive, test ? for example one which would require the
use of sentence context to choose between related
yet distributionally different antonyms (e.g. ?little,
small? as antonyms of ?big?) but chose to stick to a
previously used benchmark. This allows the direct
comparison with previously reported methods.
Some of these questions contain very rarely used
target or choice words, which are not included in
the thesaurus vocabulary. In order to provide a fair
comparison to existing methods, we do not try to
randomly answer these questions. Instead, when the
target word is out of vocabulary, we skip the whole
question. When the target word is in vocabulary but
one or more choices are unknown words, we ignore
those unknown words and pick the word with the
lowest cosine similarity from the rest as the answer.
The results of our methods are reported in precision
(the number of questions answered correctly divided
by the number of questions attempted), recall (the
number of questions answered correctly divided by
the number of all questions) and F1 (the harmonic
mean of precision and recall)6. We now turn to an
in-depth evaluation.
6Precision/recall/F1 were used in (Mohammed et al 2008)
as when their system ?did not find any evidence of antonymy
between the target and any of its alternatives, then it refrained
from attempting that question.? We adopt this convention to
provide a fair comparison to their system.
1219
Dev. Set Test Set
Prec Rec F1 Prec Rec F1
WordNet lookup 0.40 0.40 0.40 0.42 0.41 0.42
WordNet signed-TFIDF w/o LSA 0.41 0.41 0.41 0.43 0.42 0.43
WordNet PILSA 0.63 0.62 0.62 0.60 0.60 0.60
Bloomsbury lookup 0.65 0.61 0.63 0.61 0.56 0.59
Bloomsbury signed TFIDF w/o LSA 0.68 0.64 0.66 0.63 0.57 0.60
Bloomsbury PILSA 0.86 0.81 0.84 0.81 0.74 0.77
Bloomsbury PILSA + S2Net 0.89 0.84 0.86 0.84 0.77 0.80
Bloomsbury PILSA + S2Net + Embedding 0.88 0.87 0.87 0.81 0.80 0.81
(Mohammed et al 2008) 0.76 0.66 0.70 0.76 0.64 0.70
Table 6: The overall results. PILSA performs LSA on the signed TF-IDF vectors.
7.3 Basic PILSA
When applying PILSA, we need to determine the
number of dimensions in the projected space. Eval-
uated on the GRE development set, Table 5 shows
the precision of PILSA, using two different training
datasets, Bloomsbury and WordNet, at different di-
mensions.
The Bloomsbury-based system is able to answer
153 questions, and the best dimension setting is
300, which answers 132 questions correctly and thus
archives 0.863 in precision. In contrast, the larger
vocabulary in WordNet helps the system answer 160
questions but the quality is not as good. We find
dimensions 300 and 400 are equally good, where
both answer 100 questions correctly (0.625 in pre-
cision)7. Because a lower number of dimensions
is preferred for saving storage space and computing
time, we choose 300 as the number of dimensions in
PILSA.
We now compare the proposed methods. All re-
sults are summarized in Table 6. When evaluated on
the GRE test set, the Bloomsbury thesaurus-based
methods (Lines 4?7) attempted 865 questions. The
precision, recall and F1 of the Bloomsbury-based
PILSA model (Line 6) are 0.81, 0.74 and 0.77,
which are all better than the best reported method
in (Mohammed et al 2008)8. In contrast, the
WordNet-based methods (Lines 1?3) attempted 936
7Note that the number of questions attempted is not a func-
tion of the number of dimensions.
8We take a conservative approach and assume that skipped
questions are answered incorrectly. The difference is statisti-
cally significant at 99% confidence level using a binomial test.
questions. However, consistent with what we ob-
served on the development set, the WordNet-based
model is inferior. Its precision, recall and F1 on
the test set are 0.60, 0.60 and 0.60 (Line 3). Al-
though the quality of the data source plays an im-
portant role, we need to emphasize that performing
LSA using our polarity inducing construction is in
fact a critical step in enhancing the model perfor-
mance. For example, directly using the antonym sets
in the Bloomsbury thesaurus gives 0.59 in F1 (Line
4), while using cosine similarity on the signed vec-
tors prior to LSA only reaches 0.60 in F1 (Line 5).
7.4 Improving Precision with Discriminative
Training
Building on the success of the unsupervised PILSA
model, we refine the projection matrix. As described
in Section 5, we take the PILSA projection matrix
as the initial model in S2Net and train the model
using 20,517 pairs of antonyms sampled from the
Bloomsbury thesaurus. A separate sample of 5,000
antonym pairs is used as the validation set for hyper-
parameter tuning in regularization. Encouragingly,
we found that the already strong results of PILSA
can indeed be improved, which gives 3 more points
in both precision (0.84), recall (0.77) and F1 (0.80).
7.5 Improving Recall with Unsupervised Data
We next evaluate our approach of extending the
word coverage with the help of an external text cor-
pus, as well as the lexical analysis procedure. Using
the Bloomsbury PILSA-S2Net thesaurus space and
the Wikipedia corpus space, our method increases
1220
recall by 3 points on the test set. Compared to the in-
vocabulary only setting, it attempted 75 more ques-
tions (865? 940) and had 33 of them correctly an-
swered.
While the accuracy on these questions is much
higher than random, the fact that it is substantially
below the precision of the original indicates some
room for improvement. We notice that the out-of-
thesaurus words are either offensive words excluded
in the thesaurus (e.g., moronic) or some very rarely
used words (e.g., froward). When the lexical analy-
sis procedure fails to match the target word to some
in-thesaurus words, the context vector embedding
approach solves the former case, but has difficulty
in handling the latter. The main reason is that such
words occur very infrequently in a general corpus,
which result in significant uncertainty in their se-
mantic vectors. Other than using a much larger
corpus, approaches that leverage character n-grams
may help. We leave this as future work.
8 Conclusion
In this paper we have tackled the problem of find-
ing a vector-space representation of words where,
by construction, synonyms and antonyms are easy
to distinguish. Specifically, we have defined a way
of assigning sign to the entries in the co-occurrence
matrix on which LSA operates, such that synonyms
will tend to have positive cosine similarity, and
antonyms will tend to have negative similarities. To
the best of our knowledge, our method of inducing
polarity to the document-term matrix before apply-
ing LSA is novel and has shown to effectively pre-
serve and generalize the synonymous/antonymous
information in the projected space. With this vector
space representation, we were able to bring to bear
the machinery of discriminative training in order to
further optimize the word representations. Finally,
by using the notion of closeness in this space, we
were able to embed new out-of-vocabulary words
into the space. On a standard test set, the proposed
methods improved the F measure by 11 points abso-
lute over previous results.
Acknowledgments
We thank Susan Dumais for her thoughtful com-
ments, Silviu-Petru Cucerzan for preparing the
Wikipedia data, and Saif Mohammed for sharing the
GRE datasets. We are also grateful to the anony-
mous reviewers for their useful suggestions.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceedings
of HLT-NAACL, pages 19?27.
Ricardo Baeza-Yates and Berthier Ribiero-Neto. 1999.
Modern Information Retrieval. Addison-Wesley.
J. Bellegarda. 2000. Exploiting latent semantic informa-
tion in statistical language modeling. Proceedings of
the IEEE, 88(8).
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium.
N. Coccaro and D. Jurafsky. 1998. Towards better in-
tegration of semantic predictors in statistical language
modeling. In Proceedings, International Conference
on Spoken Language Processing (ICSLP-98).
D. A. Cruse. 1986. Lexical Semantics. Cambridge Uni-
versity Press.
James R. Curran and Marc Moens. 2002. Improvements
in automatic thesaurus extraction. In Proceedings of
the ACL-02 workshop on Unsupervised lexical acqui-
sition - Volume 9, pages 59?66. Association for Com-
putational Linguistics.
Thomas de Simone and Dimitar Kazakov. 2005. Using
wordnet similarity and antonymy relations to aid doc-
ument retrieval. In Recent Advances in Natural Lan-
guage Processing (RANLP).
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(96).
E. Gabrilovich and S. Markovitch. 2007. Computing se-
mantic relatedness using wikipedia-based explicit se-
mantic analysis. In AAAI Conference on Artificial In-
telligence (AAAI).
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text pro-
cessing. In AAAI Conference on Artificial Intelligence
(AAAI).
Zelig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Mario Jarmasz and Stan Szpakowicz. 2003. Rogets the-
saurus and semantic similarity. In Proceedings of the
International Conference on Recent Advances in Nat-
ural Language Processing (RANLP-2003).
1221
Thomas Landauer and Susan Dumais. 1997. A solution
to plato?s problem: The latent semantic analysis the-
ory of the acquisition, induction, and representation of
knowledge. Psychological Review, 104(2), pages 211?
240.
T.K. Landauer and D. Laham. 1998. Learning human-
like knowledge by singular value decomposition: A
progress report. In Neural Information Processing
Systems (NIPS).
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25, pages 259?284.
T.K. Landauer. 2002. On the computational basis of
learning and cognition: Arguments from lsa. Psychol-
ogy of Learning and Motivation, 41:43?84.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In International Joint Conference on
Artificial Intelligence (IJCAI).
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 2, ACL ?98, pages 768?
774, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
David Milne and Ian H. Witten. 2008. An effective low-
cost measure of semantic relatedness obtained from
wikipedia links. In Proceedings of the AAAI 2008
Workshop on Wikipedia and Artificial Intelligence.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
morphological processing of english. Natural Lan-
guage Engineering, 7(3):207?223.
Saif Mohammed, Bonnie Dorr, and Graeme Hirst. 2008.
Computing word pair antonymy. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
Saif M. Mohammed, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2011. Measuring degrees of seman-
tic opposition. Technical report, National Research
Council Canada.
Gregory L. Murphy and Jane M. Andrew. 1993. The
conceptual basis of antonymy and synonymy in adjec-
tives. Journal of Memory and Language, 32(3):1?19.
Jorge Nocedal and Stephen Wright. 2006. Numerical
Optimization. Springer, 2nd edition.
John Platt, Kristina Toutanova, and Wen-tau Yih. 2010.
Translingual document representations from discrimi-
native projections. In Proceedings of EMNLP, pages
251?261.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Empirical Methods in Nat-
ural Language Processing (EMNLP).
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Joseph Reisinger and Raymond J. Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Proceedings of HLT-NAACL, pages 109?117.
Gerard Salton and Michael J. McGill. 1983. Introduction
to Modern Information Retrieval. McGraw Hill.
G. Salton, A. Wong, and C. S. Yang. 1975. A Vector
Space Model for Automatic Indexing. Communica-
tions of the ACM, 18(11).
D. Schwab, M. Lafourcade, and V. Prince. 2002.
Antonymy and conceptual vectors. In International
Conference on Computational Linguistics (COLING).
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60 (1-3), pages 251?278.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, (37).
Peter D. Turney, Michael L. Littman, Jeffrey Bigham,
and Victor Shnayder. 2003. Combining independent
modules to solve multiple-choice synonym and anal-
ogy problems. In Recent Advances in Natural Lan-
guage Processing (RANLP).
Peter D. Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In European Conference on
Machine Learning (ECML).
P. D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
Lonneke van der Plas and Gosse Bouma. 2005. Syntac-
tic contexts for finding semantically similar words. In
Proceedings of the Meeting of Computational Linguis-
tics in the Netherlands 2004 (CLIN).
Lonneke van der Plas and Jo?rg Tiedemann. 2006. Find-
ing synonyms using automatic word alignment and
measures of distributional similarity. In Proceedings
of the COLING/ACL on Main conference poster ses-
sions, COLING-ACL ?06, pages 866?873. Associa-
tion for Computational Linguistics.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Document
clustering based on non-negative matrix factorization.
In Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in in-
formaion retrieval, pages 267?273, New York, NY,
USA. ACM.
Wen-tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning discriminative
projections for text similarity measures. In Proceed-
ings of the Fifteen Conference on Computational Nat-
ural Language Learning (CoNLL), pages 247?256,
Portland, Oregon, USA.
1222
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 601?610,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Computational Approaches to Sentence Completion
Geoffrey Zweig, John C. Platt
Christopher Meek
Christopher J.C. Burges
Microsoft Research
Redmond, WA 98052
Ainur Yessenalina
Cornell University
Computer Science Dept.
Ithaca, NY 14853
Qiang Liu
Univ. of California, Irvine
Info. & Comp. Sci.
Irvine, California 92697
Abstract
This paper studies the problem of sentence-
level semantic coherence by answering SAT-
style sentence completion questions. These
questions test the ability of algorithms to dis-
tinguish sense from nonsense based on a vari-
ety of sentence-level phenomena. We tackle
the problem with two approaches: methods
that use local lexical information, such as the
n-grams of a classical language model; and
methods that evaluate global coherence, such
as latent semantic analysis. We evaluate these
methods on a suite of practice SAT questions,
and on a recently released sentence comple-
tion task based on data taken from five Conan
Doyle novels. We find that by fusing local
and global information, we can exceed 50%
on this task (chance baseline is 20%), and we
suggest some avenues for further research.
1 Introduction
In recent years, standardized examinations have
proved a fertile source of evaluation data for lan-
guage processing tasks. They are valuable for many
reasons: they represent facets of language under-
standing recognized as important by educational ex-
perts; they are organized in various formats designed
to evaluate specific capabilities; they are yardsticks
by which society measures educational progress;
and they affect a large number of people.
Previous researchers have taken advantage of this
material to test both narrow and general language
processing capabilities. Among the narrower tasks,
the identification of synonyms and antonyms has
been studied by (Landauer and Dumais, 1997; Mo-
hammed et al, 2008; Mohammed et al, 2011; Tur-
ney et al, 2003; Turney, 2008), who used ques-
tions from the Test of English as a Foreign Lan-
guage (TOEFL), Graduate Record Exams (GRE)
and English as a Second Language (ESL) exams.
Tasks requiring broader competencies include logic
puzzles and reading comprehension. Logic puzzles
drawn from the Law School Administration Test
(LSAT) and the GRE were studied in (Lev et al,
2004), which combined an extensive array of tech-
niques to solve the problems. The DeepRead sys-
tem (Hirschman et al, 1999) initiated a long line of
research into reading comprehension based on test
prep material (Charniak et al, 2000; Riloff and The-
len, 2000; Wang et al, 2000; Ng et al, 2000).
In this paper, we study a new class of problems
intermediate in difficulty between the extremes of
synonym detection and general question answer-
ing - the sentence completion questions found on
the Scholastic Aptitude Test (SAT). These questions
present a sentence with one or two blanks that need
to be filled in. Five possible words (or short phrases)
are given as options for each blank. All possible an-
swers except one result in a nonsense sentence. Two
examples are shown in Figure 1.
The questions are highly constrained in the sense
that all the information necessary is present in the
sentence itself without any other context. Neverthe-
less, they vary widely in difficulty. The first of these
examples is relatively simple: the second half of the
sentence is a clear description of the type of behavior
characterized by the desired adjective. The second
example is more sophisticated; one must infer from
601
1. One of the characters in Milton Murayama?s
novel is considered because he deliber-
ately defies an oppressive hierarchical society.
(A) rebellious (B) impulsive (C) artistic (D)
industrious (E) tyrannical
2. Whether substances are medicines or poisons
often depends on dosage, for substances that are
in small doses can be in large.
(A) useless .. effective
(B) mild .. benign
(C) curative .. toxic
(D) harmful .. fatal
(E) beneficial .. miraculous
Figure 1: Sample sentence completion questions
(Educational-Testing-Service, 2011).
the contrast between medicine and poison that the
correct answer involves a contrast, either useless vs.
effective or curative vs. toxic. Moreover, the first, in-
correct, possibility is perfectly acceptable in the con-
text of the second clause alone; only irrelevance to
the contrast between medicine and poison eliminates
it. In general, the questions require a combination of
semantic and world knowledge as well as occasional
logical reasoning. We study the sentence comple-
tion task because we believe it is complex enough to
pose a significant challenge, yet structured enough
that progress may be possible.
As a first step, we have approached the prob-
lem from two points-of-view: first by exploiting lo-
cal sentence structure, and secondly by measuring
a novel form of global sentence coherence based
on latent semantic analysis. To investigate the use-
fulness of local information, we evaluated n-gram
language model scores, from both a conventional
model with Good-Turing smoothing, and with a re-
cently proposed maximum-entropy class-based n-
gram model (Chen, 2009a; Chen, 2009b). Also
in the language modeling vein, but with potentially
global context, we evaluate the use of a recurrent
neural network language model. In all the language
modeling approaches, a model is used to compute a
sentence probability with each of the potential com-
pletions. To measure global coherence, we propose
a novel method based on latent semantic analysis
(LSA). We find that the LSA based method performs
best, and that both local and global information can
be combined to exceed 50% accuracy. We report re-
sults on a set of questions taken from a collection
of SAT practice exams (Princeton-Review, 2010),
and further validate the methods with the recently
proposed MSR Sentence Completion Challenge set
(Zweig and Burges, 2011).
Our paper thus makes the following contributions:
First, we present the first published results on the
SAT sentence completion task. Secondly, we eval-
uate the effectiveness of both local n-gram informa-
tion, and global coherence in the form of a novel
LSA-based metric. Finally, we illustrate that the lo-
cal and global information can be effectively fused.
The remainder of this paper is organized as fol-
lows. In Section 2 we discuss related work. Section
3 describes the language modeling methods we have
evaluated. Section 4 outlines the LSA-based meth-
ods. Section 5 presents our experimental results. We
conclude with a discussion in Section 6.
2 Related Work
The past work which is most similar to ours is de-
rived from the lexical substitution track of SemEval-
2007 (McCarthy and Navigli, 2007). In this task,
the challenge is to find a replacement for a word or
phrase removed from a sentence. In contrast to our
SAT-inspired task, the original answer is indicated.
For example, one might be asked to find alternates
for match in ?After the match, replace any remain-
ing fluid deficit to prevent problems of chronic de-
hydration throughout the tournament.? Two consis-
tently high-performing systems for this task are the
KU (Yuret, 2007) and UNT (Hassan et al, 2007)
systems. These operate in two phases: first they find
a set of potential replacement words, and then they
rank them. The KU system uses just an N-gram lan-
guage model to do this ranking. The UNT system
uses a large variety of information sources, and a
language model score receives the highest weight.
N-gram statistics were also very effective in (Giu-
liano et al, 2007). That paper also explores the use
of Latent Semantic Analysis to measure the degree
of similarity between a potential replacement and its
context, but the results are poorer than others. Since
the original word provides a strong hint as to the pos-
602
sible meanings of the replacements, we hypothesize
that N-gram statistics are largely able to resolve the
remaining ambiguities. The SAT sentence comple-
tion sentences do not have this property and thus are
more challenging.
Related to, but predating the Semeval lexical sub-
stitution task are the ESL synonym questions pro-
posed by Turney (2001), and subsequently consid-
ered by numerous research groups including Terra
and Clarke (2003) and Pado and Lapata (2007).
These questions are similar to the SemEval task, but
in addition to the original word and the sentence
context, the list of options is provided. Jarmasz and
Szpakowicz (2003) used a sophisticated thesaurus-
based method and achieved state-of-the art perfor-
mance, which is 82%.
Other work on standardized tests includes the syn-
onym and antonym tasks mentioned in Section 1,
and more recent work on a SAT analogy task in-
troduced by (Turney et al, 2003) and extensively
used by other researchers (Veale, 2004; Turney and
Littman, 2005; D. et al, 2009).
3 Sentence Completion via Language
Modeling
Perhaps the most straightforward approach to solv-
ing the sentence completion task is to form the com-
plete sentence with each option in turn, and to eval-
uate its likelihood under a language model. As
discussed in Section 2, this was found be be very
effective in the ranking phase of several SemEval
systems. In this section, we describe the suite of
state-of-the-art language modeling techniques for
which we will present results. We begin with n-
gram models; first a classical n-gram backoff model
(Chen and Goodman, 1999), and then a recently pro-
posed class-based maximum-entropy n-gram model
(Chen, 2009a; Chen, 2009b). N-gram models have
the obvious disadvantage of using a very limited
context in predicting word probabilities. There-
fore we evaluate the recurrent neural net model of
(Mikolov et al, 2010; Mikolov et al, 2011b). This
model has produced record-breaking perplexity re-
sults in several tasks (Mikolov et al, 2011a), and has
the potential to encode sentence-span information in
the network hidden-layer activations. We have also
evaluated the use of parse scores, using an off-the-
shelf stochastic context free grammar parser. How-
ever, the grammatical structure of the alternatives is
often identical. With scores differing only in the fi-
nal non-terminal/terminal rewrites, this did little bet-
ter than chance. The use of other syntactically de-
rived features, for example based on a dependency
parse, are likely to be more effective, but we leave
this for future work.
3.1 Backoff N-gram Language Model
Our baseline model is a Good-Turing smoothed
model trained with the CMU language modeling
toolkit (Clarkson and Rosenfeld, 1997). For the SAT
task, we used a trigram language model trained on
1.1B words of newspaper data, described in Section
5.1. All bigrams occurring at least twice were re-
tained in the model, along with all trigrams occur-
ring at least three times. The vocabulary consisted
of all words occurring at least 100 times in the data,
along with every word in the development or test
sets. This resulted in a 124k word vocabulary and
59M n-grams. For the Conan Doyle data, which we
henceforth refer to as the Holmes data (see Section
5.1), the smaller amount of training data allowed us
to use 4-grams and a vocabulary cutoff of 3. This re-
sulted in 26M n-grams and a 126k word vocabulary.
3.2 Maximum Entropy Class-Based N-gram
Language Model
Word-class information provides a level of abstrac-
tion which is not available in a word-level lan-
guage model; therefore we evaluated a state-of-the-
art class based language model. Model M (Chen,
2009a; Chen, 2009b) is a recently proposed class
based exponential n-gram language model which
has shown improvements across a variety of tasks
(Chen, 2009b; Chen et al, 2009; Emami et al,
2010). The key ideas are the modeling of word n-
gram probabilities with a maximum entropy model,
and the use of word-class information in the defini-
tion of the features. In particular, each word w is
assigned deterministically to a class c, allowing the
n-gram probabilities to be estimated as the product
of class and word parts
P (wi|wi?n+1 . . . wi?2wi?1) =
P (ci|ci?n+1 . . . ci?2ci?1, wi?n+1 . . . wi?2wi?1)
P (wi|wi?n+1 . . . wi?2wi?1, ci).
603
Both components are themselves maximum entropy
n-gram models in which the probability of a word
or class label l given history h is determined by
1
Z exp(
?
k fk(h, l)). The features fk(h, l) used are
the presence of various patterns in the concatena-
tion of hl, for example whether a particular suffix
is present in hl.
3.3 Recurrent Neural Net Language Model
Many of the questions involve long-range depen-
dencies between words. While n-gram models have
no ability to explicitly maintain long-span context,
the recently proposed recurrent neural-net model of
(Mikolov et al, 2010) does. Related approaches
have been proposed by (Sutskever et al, 2011;
Socher et al, 2011). In this model, a set of neu-
ral net activations s(t) is maintained and updated at
each sentence position t. These activations encapsu-
late the sentence history up to the tth word in a real-
valued vector which typically has several hundred
dimensions. The word at position t is represented as
a binary vector w(t) whose length is the vocabulary
size, and with a ?1? in a position uniquely associated
with the word, and ?0? elsewhere. w(t) and s(t) are
concatenated to predict an output distribution over
words, y(t). Updating is done with two weight ma-
trices u and v and nonlinear functions f() and g()
(Mikolov et al, 2011b):
x(t) = [w(t)T s(t ? 1)T ]T
sj(t) = f(
?
i
xi(t)uji)
yk(t) = g(
?
j
sj(t)vkj)
with f() being a sigmoid and g() a softmax:
f(x) =
1
1 + exp(?z)
, g(zm) =
exp(zm)
?
k exp(zk)
The output y(t) is a probability distribution over
words, and the parameters u and v are trained with
back-propagation to minimize the Kullback-Leibler
(KL) divergence between the predicted and observed
distributions. Because of the recurrent connections,
this model is similar to a nonlinear infinite impulse
response (IIR) filter, and has the potential to model
long span dependencies. Theoretical considerations
(Bengio et al, 1994) indicate that for many prob-
lems, this may not be possible, but in practice it is
an empirical question.
4 Sentence Completion via Latent
Semantic Analysis
Latent Semantic Analysis (LSA) (Deerwester et al,
1990) is a widely used method for representing
words and documents in a low dimensional vector
space. The method is based on applying singular
value decomposition (SVD) to a matrix W repre-
senting the occurrence of words in documents. SVD
results in an approximation of W by the product
of three matrices, one in which each word is rep-
resented as a low-dimensional vector, one in which
each document is represented as a low dimensional
vector, and a diagonal scaling matrix. The simi-
larity between two words can then be quantified as
the cosine-similarity between their respective scaled
vectors, and document similarity can be measured
likewise. It has been used in numerous tasks, rang-
ing from information retrieval (Deerwester et al,
1990) to speech recognition (Bellegarda, 2000; Coc-
caro and Jurafsky, 1998).
To perform LSA, one proceeds as follows. The
input is a collection of n documents which are ex-
pressed in terms of words from a vocabulary of size
m. These documents may be actual documents such
as newspaper articles, or simply as in our case no-
tional documents such as sentences. Next, a m x n
matrix W is formed. At its simplest, the ijth entry
contains the number of times word i has occurred in
document j - its term frequency or TF value. More
conventionally, the entry is weighted by some no-
tion of the importance of word i, for example the
negative logarithm of the fraction of documents that
contain it, resulting in a TF-IDF weighting (Salton
et al, 1975). Finally, to obtain a subspace represen-
tation of dimension d, W is decomposed as
W ? USV T
where U is m x d, V T is d x n, and S is a d x d diag-
onal matrix. In applications, d << n and d << m;
for example one might have a 50, 000 word vocab-
ulary and 1, 000, 000 documents and use a 300 di-
mensional subspace representation.
An important property of SVD is that the rows
of US - which represents the words - behave sim-
ilarly to the original rows of W , in the sense that
the cosine similarity between two rows in US ap-
proximates the cosine similarity between the corre-
604
sponding rows in W . Cosine similarity is defined as
sim(x,y) = x?y?x??y? .
4.1 Total Word Similarity
Perhaps the simplest way of doing sentence comple-
tion with LSA is to compute the total similarity of a
potential answer a with the rest of the words in the
sentence S, and to choose the most related option.
We define the total similarity as:
totsim(a,S) =
?
w?S
sim(a,w)
When the completion requires two words, total sim-
ilarity is the sum of the contributions for both words.
This is our baseline method for using LSA, and one
of the best methods we have found.
4.2 Sentence Reconstruction
Recall that LSA approximates a weighted word-
document matrix W as the product of low rank
matrices U and V along with a scaling matrix S:
W ? USV T . Using singular value decomposition,
this is done so as to minimize the mean square re-
construction error
?
ij Q
2
ij whereQ = W?USV
T .
From the basic definition of LSA, each column ofW
(representing a document) is represented as
Wj = USV Tj , (1)
that is, as a linear combination of the set of basis
functions formed by the columns of US, with the
combination weights specified in V Tj . When a new
document is presented, it is also possible to repre-
sent it in terms of the same basis vectors. Moreover,
we may take the reconstruction error induced by this
representation to be a measure of how consistent the
new document is with the original set of documents
used to determine U S and V (Bellegarda, 2000).
It remains to represent a new document in terms
of the LSA bases. This is done as follows (Deer-
wester et al, 1990; Bellegarda, 2000), again with
the objective of minimizing the reconstruction error.
First, note that since U is column-orthonormal, (1)
implies that
Vj = W Tj US
?1 (2)
Thus, if we notionally index a new document by p,
we proceed by forming a new column (document)
vector Wp using the standard term-weighting, and
then find its LSA-space representation Vp using (2).
We can evaluate the reconstruction quality by insert-
ing the result in (1). The reconstruction error is then
||(UUT ? I)Wp||2
Note that if all the dimensions are retained, the re-
construction error is zero; in the case that only the
highest singular vectors are used, however, it is not.
Due to the fact that the sentences vary in length we
choose the number of retained singular vectors as a
fraction f of the sentence length. If the answer has
n words we use the top nf components. In practice,
a f of 1.2 was selected on the basis of development
set results.
4.3 A LSA N-gram Language Model
In the context of speech recognition, LSA has been
combined with classical n-gram language models
in (Coccaro and Jurafsky, 1998; Bellegarda, 2000).
The crux of this idea is to interpolate an n-gram lan-
guage model probability with one based on LSA,
with the intuition that the standard n-gram model
will do a good job predicting function words, and
the LSA model will do a good job on words pre-
dicted by their long-span context. This logic makes
sense for the sentence completion task as well, mo-
tivating us to evaluate it.
To do this, we adopt the procedure of (Coccaro
and Jurafsky, 1998), using linear interpolation be-
tween the n-gram and LSA probabilities:
p(w|history) =
?png(w|history) + (1 ? ?)plsa(w|history)
The probability of a word given its history is com-
puted by the LSA model in the following way. Let h
be the sum of all the LSA word vectors in the his-
tory. Let m be the smallest cosine similarity be-
tween h and any word in the vocabulary V : m =
minw?V sim(h,w). The probability of a word w in
the context of history h is given by
Plsa(w|h) =
sim(h,w) ? m
?
q?V (sim(h, q) ? m)
Since similarity can be negative, subtracting the
minimum (m) ensures that all the estimated prob-
abilities are between 0 and 1.
605
4.4 Improving Efficiency and Expressiveness
Given the basic framework described above, a num-
ber of enhancements are possible. In terms of ef-
ficiency, recall that it is necessary to perform SVD
on a term-document matrix. The data we used was
grouped into paragraph ?documents,? of which there
were over 27 million, with 2.6 million unique words.
While the resulting matrix is highly sparse, it is nev-
ertheless impractical to perform SVD. We overcome
this difficulty in two ways. First, we restrict the set
of documents used to those which are ?relevant? to
a given test set. This is done by requiring that a doc-
ument contain at least one of the potential answer-
words. Secondly, we restrict the vocabulary to the
set of words present in the test set. For the sentence-
reconstruction method of Section 4.2, we have found
it convenient to do data selection per-sentence.
To enhance the expressive power of LSA, the term
vocabulary can be expanded from unigrams to bi-
grams or trigrams of words, thus adding information
about word ordering. This was also used in the re-
construction technique.
5 Experimental Results
5.1 Data Resources
We present results with two datasets. The first is
taken from 11 Practice Tests for the SAT & PSAT
2011 Edition (Princeton-Review, 2010). This book
contains eleven practice tests, and we used all the
sentence completion questions in the first five tests
as a development set, and all the questions in the last
six tests as the test set. This resulted in sets with 95
and 108 questions respectively. Additionally, we re-
port results on the recently released MSR Sentence
Completion Challenge (Zweig and Burges, 2011).
This consists of a set of 1, 040 sentence completion
questions based on sentences occurring in five Co-
nan Doyle Sherlock Holmes novels, and is identical
in format to the SAT questions. Due to the source of
this data, we refer to it as the Holmes data.
To train models, we have experimented with a
variety of data sources. Since there is no publi-
cally available collection of SAT questions suitable
to training, our methods have all relied on unsu-
pervised data. Early on, we ran a set of experi-
ments to determine the relevance of different types
of data. Thinking that data from an encyclopedia
Data Dev % Correct Test % Correct
Encarta 26 33
Wikipedia 32 31
LA Times 39 42
Table 1: Effectiveness of different types of training data.
might be useful, we evaluated an electronic version
of the 2003 Encarta encyclopedia, which has ap-
proximately 29M words. Along similar lines, we
used a collection of Wikipedia articles consisting of
709M words. This data is the entire Wikipedia as of
January 2011, broken down into sentences, with fil-
tering to remove sentences consisting of URLs and
Wiki author comments. Finally, we used a com-
mercial newspaper dataset consisting of all the Los
Angeles Times data from 1985 to 2002, containing
about 1.1B words. These data sources were evalu-
ated using the baseline n-gram LM approach of Sec-
tion 3.1. Initial experiments indicated that that the
Los Angeles Times data is best suited to this task
(see Table 1), and our SAT experiments use this
source. For the MSR Sentence Completion data,
we obtained the training data specified in (Zweig
and Burges, 2011), consisting of approximately 500
19th-century novels available from Project Guten-
berg, and comprising 48M words.
5.2 Human Performance
To provide human benchmark performance, we
asked six native speaking high school students and
five graduate students to answer the questions on the
development set. The high-schoolers attained 87%
accuracy and the graduate students 95%. Zweig and
Burges (2011) cite a human performance of 91%
on the Holmes data. Statistics from a large cross-
section of the population are not available. As a fur-
ther point of comparison, we note that chance per-
formance is 20%.
5.3 Language Modeling Results
Table 2 summarizes our language modeling results
on the SAT data. With the exception of the base-
line backoff n-gram model, these techniques were
too computationally expensive to utilize the full Los
Angeles Times corpus. Instead, as with LSA, a ?rel-
evant? corpus was selected of the sentences which
contain at least one answer option from either the
606
Method Data (Dev / Test) Dev Test
3-gram GT 1.1B / 1.1B 39% 42%
Model M 193M / 236M 35 41
RNN 36M / 44M 37 42
LSA-LM 293M / 358 M 48 44
Table 2: Performance of language modeling methods on
SAT questions.
Method Dev ppl Dev Test ppl Test
3-gram GT 195 36% 190 44%
Model M 178 36 175 42
RNN 147 37 144 42
Table 3: Performance of language modeling methods us-
ing identical training data and vocabularies.
development or test set. Separate subsets were made
for development and test data. This data was further
sub-sampled to obtain the training set sizes indicated
in the second column. For the LSA-LM, an interpo-
lation weight of 0.1 was used for the LSA score, de-
termined through optimization on the development
set. We see from this table that the language models
perform similarly and achieve just above 40% on the
test set.
To make a more controlled comparison that nor-
malizes for the amount of training data, we have
trained Model M, and the Good-Turing model on
the same data subset as the RNN, and with the same
vocabulary. In Table 3, we present perplexity re-
sults on a held-out set of dev/test-relevant Los Ange-
les Times data, and performance on the actual SAT
questions. Two things are notable. First, the re-
current neural net has dramatically lower perplexity
than the other methods. This is consistent with re-
sults in (Mikolov et al, 2011a). Secondly, despite
the differences in perplexity, the methods show little
difference on SAT performance. Because Model M
was not better, only uses n-gram context, and was
used in the construction of the Holmes data (Zweig
and Burges, 2011), we do not consider it further.
5.4 LSA Results
Table 4 presents results for the methods of Sections
4.1 and 4.2. Of all the methods in isolation, the sim-
ple approach of Section 4.1 - to use the total cosine
similarity between a potential answer and the other
words in the sentence - has performed best. The ap-
Method Dev Test
Total Word Similarity 46% 46%
Reconstruction Error 53 41
Table 4: SAT performance of LSA based methods.
Method Test
3-input LSA 46%
LSA + Good-Turing LM 53
LSA + Good-Turing LM + RNN 52
Table 5: SAT test set accuracy with combined methods.
proach of using reconstruction error performed very
well on the development set, but unremarkably on
the test set.
5.5 Combination Results
A well-known trick for obtaining best results from
a machine learning system is to combine a set of
diverse methods into a single ensemble (Dietterich,
2000). We use ensembles to get the highest accuracy
on both of our data sets.
We use a simple linear combination of the out-
puts of the other models discussed in this paper. For
the LSA model, the linear combination has three in-
puts: the total word similarity, the cosine similarity
between the sum of the answer word vectors and the
sum of the rest of sentence?s word vectors, and the
number of out-of-vocabulary terms in the answer.
Each additional language model beyond LSA con-
tributes an additional input: the probability of the
sentence under that language model.
We train the parameters of the linear combination
on the SAT development set. The training minimizes
a loss function of pairs of answers: one correct and
one incorrect fill-in from the same question. We use
the RankNet loss function (Burges et al, 2005):
min
~w
f(~w ? (~x ? ~y)) + ?||~w||2
where ~x are the input features for the incorrect an-
swer, ~y are the features for the correct answer, ~w
are the weights for the combination, and f(z) =
log(1 + exp(z)). We tune the regularizer via 5-
fold cross validation, and minimize the loss using
L-BFGS (Nocedal and Wright, 2006). The results
on the SAT test set for combining various models
are shown in Table 5.
607
5.6 Holmes Data Results
To measure the robustness of our approaches, we
have applied them to the MSR Sentence Completion
set (Zweig and Burges, 2011), termed the Holmes
data. In Table 6, we present the results on this set,
along with the comparable SAT results. Note that
the latter are derived from models trained with the
Los Angeles Times data, while the Holmes results
are derived from models trained with 19th-century
novels. We see from this table that the results are
similar across the two tasks. The best performing
single model is LSA total word similarity.
For the Holmes data, combining the models out-
performs any single model. We train the linear com-
bination function via 5-fold cross-validation: the
model is trained five times, each time on 3/5 of the
data, the regularization tuned on 1/5 of the data, and
tested on 1/5. The test results are pooled across all
5 folds and are shown in Table 6. In this case, the
best combination is to blend LSA, the Good-Turing
language model, and the recurrent neural network.
6 Discussion
To verify that the differences in accuracy between
the different algorithms are not statistical flukes, we
perform a statistical significance test on the out-
puts of each algorithm. We use McNemar?s test,
which is a matched test between two classifiers (Di-
etterich, 1998). We use the False Discovery Rate
method (Benjamini and Hochberg, 1995) to control
the false positive rate caused by multiple tests. If
we allow 2% of our tests to yield incorrectly false
results, then for the SAT data, the combination of
the Good-Turing smoothed language model with an
LSA-based global similarity model (52% accuracy)
is better that the baseline alone (42% accuracy).
Secondly, for the Holmes data, we can state that
LSA total similarity beats the recurrent neural net-
work, which in turn is better than the baseline n-
gram model. The combination of all three is sig-
nificantly better than any of the individual models.
To better understand the system performance and
gain insight into ways of improving it, we have ex-
amined the system?s errors. Encouragingly, one-
third of the errors involve single-word questions
which test the dictionary definition of a word. This
is done either by stating the definition, or provid-
Method SAT Holmes
Chance 20% 20%
GT N-gram LM 42 39
RNN 42 45
LSA Total Similarity 46 49
Reconstruction Error 41 41
LSA-LM 44 42
Combination 53 52
Human 87 to 95 91
Table 6: Performance of methods on the MSR Sentence
Completion Challenge, contrasted with SAT test set.
ing a stereotypical use of the word. An example of
the first case is: ?Great artists are often prophetic
(visual): they perceive what we cannot and antici-
pate the future long before we do.? (The system?s
incorrect answer is in parentheses.) An example
of the second is: ?One cannot help but be moved
by Theresa?s heartrending (therapeutic) struggle to
overcome a devastating and debilitating accident.?
At the other end of the difficulty spectrum are
questions involving world knowledge and/or logical
implications. An example requiring both is, ?Many
fear that the ratification (withdrawal) of more le-
nient tobacco advertising could be detrimental to
public health.? About 40% of the errors require this
sort of general knowledge to resolve. Based on our
analysis, we believe that future research could prof-
itably exploit the structured information present in
a dictionary. However, the ability to identify and
manipulate logical relationships and embed world
knowledge in a manner amenable to logical manip-
ulation may be necessary for a full solution. It is
an interesting research question if this could be done
implicitly with a machine learning technique, for ex-
ample recurrent or recursive neural networks.
7 Conclusion
In this paper we have investigated methods for
answering sentence-completion questions. These
questions are intriguing because they probe the abil-
ity to distinguish semantically coherent sentences
from incoherent ones, and yet involve no more con-
text than the single sentence. We find that both local
n-gram information and an LSA-based global coher-
ence model do significantly better than chance, and
that they can be effectively combined.
608
References
J. Bellegarda. 2000. Exploiting latent semantic informa-
tion in statistical language modeling. Proceedings of
the IEEE, 88(8).
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE Transactions on Neural
Networks, 5(2):157 ?166.
Y. Benjamini and Y. Hochberg. 1995. Controlling the
fase discovery rate: a practical and powerful approach
to multiple testing. J. Royal Statistical Society B,
53(1):289?300.
C. Burges, T. Shaked., E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. 2005. Learning to
rank using gradient descent. In Proc. ICML, pages 89?
96.
Eugene Charniak, Yasemin Altun, Rodrigo de Salvo
Braz, Benjamin Garrett, Margaret Kosmala, Tomer
Moscovich, Lixin Pang, Changhee Pyo, Ye Sun,
Wei Wy, Zhongfa Yang, Shawn Zeller, and Lisa
Zorn. 2000. Reading comprehension programs in
a statistical-language-processing class. In Proceed-
ings of the 2000 ANLP/NAACL Workshop on Read-
ing comprehension tests as evaluation for computer-
based language understanding sytems - Volume 6,
ANLP/NAACL-ReadingComp ?00, pages 1?5. Asso-
ciation for Computational Linguistics.
Stanley Chen and Joshua Goodman. 1999. An empirical
study of smoothing techniques for language modeling.
Computer Speech and Language, 13(4):359?393.
S. Chen, L. Mangu, B. Ramabhadran, R. Sarikaya, and
A. Sethy. 2009. Scaling shrinkage-based language
models. In ASRU.
S. Chen. 2009a. Performance prediction for exponential
language models. In NAACL-HLT.
S. Chen. 2009b. Shrinking exponential language models.
In NAACL-HLT.
P.R. Clarkson and R. Rosenfeld. 1997. Statistical
language modeling using the CMU-Cambridge
Toolkit. In Proceedings ESCA Eurospeech,
http://www.speech.cs.cmu.edu/SLM/toolkit.html.
N. Coccaro and D. Jurafsky. 1998. Towards better in-
tegration of semantic predictors in statistical language
modeling. In Proceedings, ICSLP.
Bollegala D., Matsuo Y., and Ishizuka M. 2009. Measur-
ing the similarity between implicit semantic relations
from the web. InWorldWideWeb Conference (WWW).
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(96).
T.G. Dietterich. 1998. Approximate statistical tests
for comparing supervised classification learning algo-
rithms. Neural Computation, 10:1895?1923.
T.G. Dietterich. 2000. Ensemble methods in machine
learning. In International Workshop on Multiple Clas-
sifier Systems, pages 1?15. Springer-Verlag.
Educational-Testing-Service. 2011.
https://satonlinecourse.collegeboard.com/sr/digital assets/
assessment/pdf/0833a611-0a43-10c2-0148-
cc8c0087fb06-f.pdf.
A. Emami, S. Chen, A. Ittycheriah, H. Soltau, and
B. Zhao. 2010. Decoding with shrinkage-based lan-
guage models. In Interspeech.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.
2007. Fbk-irst: Lexical substitution task exploiting
domain and syntagmatic coherence. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations, SemEval ?07, pages 145?148, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Samer Hassan, Andras Csomai, Carmen Banea, Ravi
Sinha, and Rada Mihalcea. 2007. Unt: Subfinder:
Combining knowledge sources for automatic lexical
substitution. In Proceedings of the 4th International
Workshop on Semantic Evaluations, SemEval ?07,
pages 410?413, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Lynette Hirschman, Mark Light, Eric Breck, and John D.
Burger. 1999. Deep read: A reading comprehension
system. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics.
Thomas Landauer and Susan Dumais. 1997. A solution
to Plato?s problem: The latent semantic analysis the-
ory of the acquisition, induction, and representation of
knowledge. Psychological Review, 104(2), pages 211?
240.
Iddo Lev, Bill MacCartney, Christopher D. Manning, and
Roger Levy. 2004. Solving logic puzzles: from ro-
bust processing to precise semantics. In Proceedings
of the 2nd Workshop on Text Meaning and Interpreta-
tion, pages 9?16. Association for Computational Lin-
guistics.
Jarmasz M. and Szpakowicz S. 2003. Roget?s thesaurus
and semantic similarity. In Recent Advances in Natu-
ral Language Processing (RANLP).
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 48?53.
Tomas Mikolov, Martin Karafiat, Jan Cernocky, and San-
jeev Khudanpur. 2010. Recurrent neural network
based language model. In Proceedings of Interspeech
2010.
609
Tomas Mikolov, Anoop Deoras, Stefan Kombrink, Lukas
Burget, and Jan Cernocky. 2011a. Empirical evalua-
tion and combination of advanced language modeling
techniques. In Proceedings of Interspeech 2011.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky, and Sanjeev Khudanpur. 2011b. Ex-
tensions of recurrent neural network based language
model. In Proceedings of ICASSP 2011.
Saif Mohammed, Bonnie Dorr, and Graeme Hirst. 2008.
Computing word pair antonymy. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
Saif M. Mohammed, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2011. Measuring degrees of seman-
tic opposition. Technical report, National Research
Council Canada.
Hwee Tou Ng, Leong Hwee Teo, and Jennifer Lai Pheng
Kwan. 2000. A machine learning approach to answer-
ing questions for reading comprehension tests. In Pro-
ceedings of the 2000 Joint SIGDAT conference on Em-
pirical methods in natural language processing and
very large corpora: held in conjunction with the 38th
Annual Meeting of the Association for Computational
Linguistics - Volume 13, EMNLP ?00, pages 124?132.
J. Nocedal and S. Wright. 2006. Numerical Optimiza-
tion. Springer-Verlag.
Sebastian Pado and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33 (2), pages 161?199.
Princeton-Review. 2010. 11 Practice Tests for the SAT
& PSAT, 2011 Edition. The Princeton Review.
Ellen Riloff and Michael Thelen. 2000. A rule-based
question answering system for reading comprehension
tests. In Proceedings of the 2000 ANLP/NAACL Work-
shop on Reading comprehension tests as evaluation for
computer-based language understanding sytems - Vol-
ume 6, ANLP/NAACL-ReadingComp ?00, pages 13?
19.
G. Salton, A. Wong, and C. S. Yang. 1975. A Vector
Space Model for Automatic Indexing. Communica-
tions of the ACM, 18(11).
Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,
and Christopher D. Manning. 2011. Parsing natural
scenes and natural language with recursive neural net-
works. In Proceedings of the 2011 International Con-
ference on Machine Learning (ICML-2011).
Ilya Sutskever, James Martens, and Geoffrey Hinton.
2011. Generating text with recurrent neural networks.
In Proceedings of the 2011 International Conference
on Machine Learning (ICML-2011).
E. Terra and C. Clarke. 2003. Frequency estimates for
statistical word similarity measures. In Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL).
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60 (1-3), pages 251?278.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham,
and Victor Shnayder. 2003. Combining independent
modules to solve multiple-choice synonym and anal-
ogy problems. In Recent Advances in Natural Lan-
guage Processing (RANLP).
Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In European Confer-
ence on Machine Learning (ECML).
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
T. Veale. 2004. Wordnet sits the sat: A knowledge-based
approach to lexical analogy. In European Conference
on Artificial Intelligence (ECAI).
W. Wang, J. Auer, R. Parasuraman, I. Zubarev,
D. Brandyberry, and M. P. Harper. 2000. A ques-
tion answering system developed as a project in a
natural language processing course. In Proceed-
ings of the 2000 ANLP/NAACL Workshop on Read-
ing comprehension tests as evaluation for computer-
based language understanding sytems - Volume 6,
ANLP/NAACL-ReadingComp ?00, pages 28?35.
Deniz Yuret. 2007. Ku: word sense disambiguation
by substitution. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, SemEval
?07, pages 207?213, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Geoffrey Zweig and Christopher J.C. Burges. 2011. The
Microsoft Research sentence completion challenge.
Technical Report MSR-TR-2011-129, Microsoft.
610
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 247?256,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Learning Discriminative Projections for Text Similarity Measures
Wen-tau Yih Kristina Toutanova John C. Platt Christopher Meek
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
{scottyih,kristout,jplatt,meek}@microsoft.com
Abstract
Traditional text similarity measures consider
each term similar only to itself and do not
model semantic relatedness of terms. We pro-
pose a novel discriminative training method
that projects the raw term vectors into a com-
mon, low-dimensional vector space. Our ap-
proach operates by finding the optimal matrix
to minimize the loss of the pre-selected sim-
ilarity function (e.g., cosine) of the projected
vectors, and is able to efficiently handle a
large number of training examples in the high-
dimensional space. Evaluated on two very dif-
ferent tasks, cross-lingual document retrieval
and ad relevance measure, our method not
only outperforms existing state-of-the-art ap-
proaches, but also achieves high accuracy at
low dimensions and is thus more efficient.
1 Introduction
Measures of text similarity have many applications
and have been studied extensively in both the NLP
and IR communities. For example, a combination
of corpus and knowledge based methods have been
invented for judging word similarity (Lin, 1998;
Agirre et al, 2009). Similarity derived from a large-
scale Web corpus has been used for automatically
extending lists of typed entities (Vyas and Pantel,
2009). Judging the degree of similarity between
documents is also fundamental to classical IR prob-
lems such as document retrieval (Manning et al,
2008). In all these applications, the vector-based
similarity method is the most widely used. Term
vectors are first constructed to represent the origi-
nal text objects, where each term is associated with
a weight indicating its importance. A pre-selected
function operating on these vectors, such as cosine,
is used to output the final similarity score. This ap-
proach has not only proved to be effective, but is also
efficient. For instance, only the term vectors rather
than the raw data need to be stored. A pruned inverse
index can be built to support fast similarity search.
However, the main weakness of this term-vector
representation is that different but semantically re-
lated terms are not matched and cannot influence
the final similarity score. As an illustrative ex-
ample, suppose the two compared term-vectors
are: {purchase:0.4, used:0.3, automobile:0.2} and
{buy:0.3, pre-owned: 0.5, car: 0.4}. Even though
the two vectors represent very similar concepts, their
similarity score will be 0, for functions like cosine,
overlap or Jaccard. Such an issue is more severe
in cross-lingual settings. Because language vocab-
ularies typically have little overlap, term-vector rep-
resentations are completely inapplicable to measur-
ing similarity between documents in different lan-
guages. The general strategy to handle this prob-
lem is to map the raw representation to a common
concept space, where extensive approaches have
been proposed. Existing methods roughly fall into
three categories. Generative topic models like La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003)
assume that the terms are sampled by probabil-
ity distributions governed by hidden topics. Lin-
ear projection methods like Latent Semantic Anal-
ysis (LSA) (Deerwester et al, 1990) learn a projec-
tion matrix and map the original term-vectors to the
dense low-dimensional space. Finally, metric learn-
ing approaches for high-dimensional spaces have
247
also been proposed (Davis and Dhillon, 2008).
In this paper, we propose a new projection learn-
ing framework, Similarity Learning via Siamese
Neural Network (S2Net), to discriminatively learn
the concept vector representations of input text ob-
jects. Following the general Siamese neural network
architecture (Bromley et al, 1993), our approach
trains two identical networks concurrently. The in-
put layer corresponds to the original term vector
and the output layer is the projected concept vector.
Model parameters (i.e., the weights on the edges)
are equivalently the projection matrix. Given pairs
of raw term vectors and their labels (e.g., similar or
not), the model is trained by minimizing the loss of
the similarity scores of the output vectors. S2Net
is closely related to the linear projection and met-
ric learning approaches, but enjoys additional ad-
vantages over existing methods. While its model
form is identical to that of LSA, CCA and OPCA, its
objective function can be easily designed to match
the true evaluation metric of interest for the target
task, which leads to better performance. Compared
to existing high-dimensional metric learning meth-
ods, S2Net can learn from a much larger number
of labeled examples. These two properties are cru-
cial in helping S2Net outperform existing methods.
For retrieving comparable cross-lingual documents,
S2Net achieves higher accuracy than the best ap-
proach (OPCA) at a much lower dimension of the
concept space (500 vs. 2,000). In a monolingual
setting, where the task is to judge the relevance of
an ad landing page to a query, S2Net alo has the
best performance when compared to a number of ap-
proaches, including the raw TFIDF cosine baseline.
In the rest of the paper, we first survey some
existing work in Sec. 2, with an emphasis on ap-
proaches included in our experimental comparison.
We present our method in Sec. 3 and report on an
extensive experimental study in Sec. 4. Other re-
lated work is discussed in Sec. 5 and finally Sec. 6
concludes the paper.
2 Previous Work
In this section, we briefly review existing ap-
proaches for mapping high-dimensional term-
vectors to a low-dimensional concept space.
2.1 Generative Topic Models
Probabilistic Latent Semantic Analysis
(PLSA) (Hofmann, 1999) assumes that each
document has a document-specific distribution ?
over some finite number K of topics, where each
token in a document is independently generated
by first selecting a topic z from a multinomial
distribution MULTI(?), and then sampling a word
token from the topic-specific word distribution
for the chosen topic MULTI(?z). Latent Dirichlet
Allocation (LDA) (Blei et al, 2003) generalizes
PLSA to a proper generative model for documents
and places Dirichlet priors over the parameters
? and ?. In the experiments in this paper, our
implementation of PLSA is LDA with maximum a
posteriori (MAP) inference, which was shown to be
comparable to the current best Bayesian inference
methods for LDA (Asuncion et al, 2009).
Recently, these topic models have been general-
ized to handle pairs or tuples of corresponding doc-
uments, which could be translations in multiple lan-
guages, or documents in the same language that are
considered similar. For instance, the Poly-lingual
Topic Model (PLTM) (Mimno et al, 2009) is an
extension to LDA that views documents in a tu-
ple as having a shared topic vector ?. Each of the
documents in the tuple uses ? to select the topics
z of tokens, but could use a different (language-
specific) word-topic-distribution MULTI(?Lz ). Two
additional models, Joint PLSA (JPLSA) and Cou-
pled PLSA (CPLSA) were introduced in (Platt et al,
2010). JPLSA is a close variant of PLTM when doc-
uments of all languages share the same word-topic
distribution parameters, and MAP inference is per-
formed instead of Bayesian. CPLSA extends JPLSA
by constraining paired documents to not only share
the same prior topic distribution ?, but to also have
similar fractions of tokens assigned to each topic.
This constraint is enforced on expectation using pos-
terior regularization (Ganchev et al, 2009).
2.2 Linear Projection Methods
The earliest method for projecting term vectors into
a low-dimensional concept space is Latent Seman-
tic Analysis (LSA) (Deerwester et al, 1990). LSA
models all documents in a corpus using a n ?
d document-term matrix D and performs singular
248
value decomposition (SVD) on D. The k biggest
singular values are then used to find the d ? k pro-
jection matrix. Instead of SVD, LSA can be done
by applying eigen-decomposition on the correlation
matrix between terms C = DTD. This is very sim-
ilar to principal component analysis (PCA), where a
covariance matrix between terms is used. In prac-
tice, term vectors are very sparse and their means
are close to 0. Therefore, the correlation matrix is in
fact close to the covariance matrix.
To model pairs of comparable documents,
LSA/PCA has been extended in different ways. For
instance, Cross-language Latent Semantic Indexing
(CL-LSI) (Dumais et al, 1997) applies LSA to con-
catenated comparable documents from different lan-
guages. Oriented Principal Component Analysis
(OPCA) (Diamantaras and Kung, 1996; Platt et al,
2010) solves a generalized eigen problem by intro-
ducing a noise covariance matrix to ensure that com-
parable documents can be projected closely. Canon-
ical Correlation Analysis (CCA) (Vinokourov et al,
2003) finds projections that maximize the cross-
covariance between the projected vectors.
2.3 Distance Metric Learning
Measuring the similarity between two vectors can be
viewed as equivalent to measuring their distance, as
the cosine score has a bijection mapping to the Eu-
clidean distance of unit vectors. Most work on met-
ric learning learns a Mahalanobis distance, which
generalizes the standard squared Euclidean distance
by modeling the similarity of elements in different
dimensions using a positive semi-definite matrix A.
Given two vectors x and y, their squared Maha-
lanobis distance is: dA = (x ? y)TA(x ? y).
However, the computational complexity of learn-
ing a general Mahalanobis matrix is at least O(n2),
where n is the dimensionality of the input vectors.
Therefore, such methods are not practical for high
dimensional problems in the text domain.
In order to tackle this issue, special metric
learning approaches for high-dimensional spaces
have been proposed. For example, high dimen-
sion low-rank (HDLR) metric learning (Davis and
Dhillon, 2008) constrains the form of A = UUT ,
where U is similar to the regular projection ma-
trix, and adapts information-theoretic metric learn-
ing (ITML) (Davis et al, 2007) to learn U.
sim(vp,vq) 
1t
dtvp vq 
it
1c
kcjc
'tw
tw
Figure 1: Learning concept vectors. The output layer
consists of a small number of concept nodes, where the
weight of each node is a linear combination of all the
original term weights.
3 Similarity Learning via Siamese Neural
Network (S2Net)
Given pairs of documents with their labels, such as
binary or real-valued similarity scores, our goal is
to construct a projection matrix that maps the corre-
sponding term-vectors into a low-dimensional con-
cept space such that similar documents are close
when projected into this space. We propose a sim-
ilarity learning framework via Siamese neural net-
work (S2Net) to learn the projection matrix directly
from labeled data. In this section, we introduce its
model design and describe the training process.
3.1 Model Design
The network structure of S2Net consists of two lay-
ers. The input layer corresponds to the raw term vec-
tor, where each node represents a term in the original
vocabulary and its associated value is determined by
a term-weighting function such as TFIDF. The out-
put layer is the learned low-dimensional vector rep-
resentation that captures relationships among terms.
Similarly, each node of the output layer is an ele-
ment in the new concept vector. In this work, the
final similarity score is calculated using the cosine
function, which is the standard choice for document
similarity (Manning et al, 2008). Our framework
can be easily extended to other similarity functions
as long as they are differentiable.
The output of each concept node is a linear com-
249
bination of the weights of all the terms in the orig-
inal term vector. In other words, these two layers
of nodes form a complete bipartite graph as shown
in Fig. 1. The output of a concept node cj is thus
defined as:
tw?(cj) =
?
ti?V
?ij ? tw(ti) (1)
Notice that it is straightforward to add a non-linear
activation function (e.g., sigmoid) in Eq. (1), which
can potentially lead to better results. However, in
the current design, the model form is exactly the
same as the low-rank projection matrix derived by
PCA, OPCA or CCA, which facilitates comparison
to alternative projection methods. Using concise
matrix notation, let f be a raw d-by-1 term vector,
A = [?ij ]d?k the projection matrix. g = AT f is
thus the k-by-1 projected concept vector.
3.2 Loss Function and Training Procedure
For a pair of term vectors fp and fq, their similar-
ity score is defined by the cosine value of the corre-
sponding concept vectors gp and gq according to the
projection matrix A.
simA(fp, fq) =
gTp gq
||gp||||gq||
,
where gp = AT fp and gq = AT fq. Let ypq be
the true label of this pair. The loss function can
be as simple as the mean-squared error 12(ypq ?
simA(fp, fq))2. However, in many applications, the
similarity scores are used to select the closest text
objects given the query. For example, given a query
document, we only need to have the comparable
document in the target language ranked higher than
any other documents. In this scenario, it is more
important for the similarity measure to yield a good
ordering than to match the target similarity scores.
Therefore, we use a pairwise learning setting by con-
sidering a pair of similarity scores (i.e., from two
vector pairs) in our learning objective.
Consider two pairs of term vectors (fp1 , fq1) and
(fp2 , fq2), where the first pair has higher similarity.
Let ? be the difference of their similarity scores.
Namely, ? = simA(fp1 , fq1)? simA(fp2 , fq2). We
use the following logistic loss over ?, which upper-
bounds the pairwise accuracy (i.e., 0-1 loss):
L(?;A) = log(1 + exp(???)) (2)
Because of the cosine function, we add a scaling
factor ? that magnifies ? from [?2, 2] to a larger
range, which helps penalize more on the prediction
errors. Empirically, the value of ? makes no dif-
ference as long as it is large enough1. In the ex-
periments, we set the value of ? to 10. Optimizing
the model parameters A can be done using gradi-
ent based methods. We derive the gradient of the
whole batch and apply the quasi-Newton optimiza-
tion method L-BFGS (Nocedal and Wright, 2006)
directly. For a cleaner presentation, we detail the
gradient derivation in Appendix A. Given that the
optimization problem is not convex, initializing the
model from a good projection matrix often helps re-
duce training time and may lead to convergence to
a better local minimum. Regularization can be done
by adding a term ?2 ||A ? A0||
2 in Eq. (2), which
forces the learned model not to deviate too much
from the starting point (A0), or simply by early stop-
ping. Empirically we found that the latter is more
effective and it is used in the experiments.
4 Experiments
We compare S2Net experimentally with existing ap-
proaches on two very different tasks: cross-lingual
document retrieval and ad relevance measures.
4.1 Comparable Document Retrieval
With the growth of multiple languages on the Web,
there is an increasing demand of processing cross-
lingual documents. For instance, machine trans-
lation (MT) systems can benefit from training on
sentences extracted from parallel or comparable
documents retrieved from the Web (Munteanu and
Marcu, 2005). Word-level translation lexicons can
also be learned from comparable documents (Fung
and Yee, 1998; Rapp, 1999). In this cross-lingual
document retrieval task, given a query document in
one language, the goal is to find the most similar
document from the corpus in another language.
4.1.1 Data & Setting
We followed the comparable document retrieval
setting described in (Platt et al, 2010) and evalu-
ated S2Net on the Wikipedia dataset used in that pa-
per. This data set consists of Wikipedia documents
1Without the ? parameter, the model still outperforms other
baselines in our experiments, but with a much smaller gain.
250
in two languages, English and Spanish. An article
in English is paired with a Spanish article if they
are identified as comparable across languages by the
Wikipedia community. To conduct a fair compari-
son, we use the same term vectors and data split as in
the previous study. The numbers of document pairs
in the training/development/testing sets are 43,380,
8,675 and 8,675, respectively. The dimensionality
of the raw term vectors is 20,000.
The models are evaluated by using each English
document as query against all documents in Span-
ish and vice versa; the results from the two direc-
tions are averaged. Performance is evaluated by two
metrics: the Top-1 accuracy, which tests whether
the document with the highest similarity score is the
true comparable document, and the Mean Recipro-
cal Rank (MRR) of the true comparable.
When training the S2Net model, all the compara-
ble document pairs are treated as positive examples
and all other pairs are used as negative examples.
Naively treating these 1.8 billion pairs (i.e., 433802)
as independent examples would make the training
very inefficient. Fortunately, most computation in
deriving the batch gradient can be reused via com-
pact matrix operations and training can still be done
efficiently. We initialized the S2Net model using the
matrix learned by OPCA, which gave us the best per-
formance on the development set2.
Our approach is compared with most methods
studied in (Platt et al, 2010), including the best per-
forming one. For CL-LSI, OPCA, and CCA, we in-
clude results from that work directly. In addition, we
re-implemented and improved JPLSA and CPLSA
by changing three settings: we used separate vocab-
ularies for the two languages as in the Poly-lingual
topic model (Mimno et al, 2009), we performed 10
EM iterations for folding-in instead of only one, and
we used the Jensen-Shannon distance instead of the
L1 distance. We also attempted to apply the HDLR
algorithm. Because this algorithm does not scale
well as the number of training examples increases,
we used 2,500 positive and 2,500 negative docu-
ment pairs for training. Unfortunately, among all the
2S2Net outperforms OPCA when initialized from a random
or CL-LSI matrix, but with a smaller gain. For example, when
the number of dimensions is 1000, the MRR score of OPCA
is 0.7660. Starting from the CL-LSI and OPCA matrices, the
MRR scores of S2Net are 0.7745 and 0.7855, respectively.
Figure 2: Mean reciprocal rank versus dimension for
Wikipedia. Results of OPCA, CCA and CL-LSI are
from (Platt et al, 2010).
hyper-parameter settings we tested, HDLR could not
outperform its initial model, which was the OPCA
matrix. Therefore we omit these results.
4.1.2 Results
Fig. 2 shows the MRR performance of all meth-
ods on the development set, across different dimen-
sionality settings of the concept space. As can be
observed from the figure, higher dimensions usually
lead to better results. In addition, S2Net consistently
performs better than all other methods across differ-
ent dimensions. The gap is especially large when
projecting input vectors to a low-dimensional space,
which is preferable for efficiency. For instance, us-
ing 500 dimensions, S2Net aleady performs as well
as OPCA with 2000 dimensions.
Table 1 shows the averaged Top-1 accuracy and
MRR scores of all methods on the test set, where
the dimensionality for each method is optimized on
the development set (Fig. 2). S2Net clearly outper-
forms all other methods and the difference in terms
of accuracy is statistically significant3.
4.2 Ad Relevance
Paid search advertising is the main revenue source
that supports modern commercial search engines.
To ensure satisfactory user experience, it is impor-
tant to provide both relevant ads and regular search
3We use the unpaired t-test with Bonferroni correction and
the difference is considered statistically significant when the p-
value is less than 0.01.
251
Algorithm Dimension Accuracy MRR
S2Net 2000 0.7447 0.7973
OPCA 2000 0.7255 0.7734
CCA 1500 0.6894 0.7378
CPLSA 1000 0.6329 0.6842
JPLSA 1000 0.6079 0.6604
CL-LSI 5000 0.5302 0.6130
Table 1: Test results for comparable document retrieval
in Wikipedia. Results of OPCA, CCA and CL-LSI are
from (Platt et al, 2010).
results. Previous work on ad relevance focuses on
constructing appropriate term-vectors to represent
queries and ad-text (Broder et al, 2008; Choi et al,
2010). In this section, we extend the work in (Yih
and Jiang, 2010) and show how S2Net can exploit
annotated query?ad pairs to improve the vector rep-
resentation in this monolingual setting.
4.2.1 Data & Tasks
The ad relevance dataset we used consists of
12,481 unique queries randomly sampled from the
logs of the Bing search engine. For each query, a
number of top ranked ads are selected, which results
in a total number of 567,744 query-ad pairs in the
dataset. Each query-ad pair is manually labeled as
same, subset, superset or disjoint. In our experi-
ment, when the task is a binary classification prob-
lem, pairs labeled as same, subset, or superset are
considered relevant, and pairs labeled as disjoint are
considered irrelevant. When pairwise comparisons
are needed in either training or evaluation, the rele-
vance order is same > subset = superset > disjoint.
The dataset is split into training (40%), validation
(30%) and test (30%) sets by queries.
Because a query string usually contains only a few
words and thus provides very little content, we ap-
plied the same web relevance feedback technique
used in (Broder et al, 2008) to create ?pseudo-
documents? to represent queries. Each query in our
data set was first issued to the search engine. The
result page with up to 100 snippets was used as the
pseudo-document to create the raw term vectors. On
the ad side, we used the ad landing pages instead
of the short ad-text. Our vocabulary set contains
29,854 words and is determined using a document
frequency table derived from a large collection of
Web documents. Only words with counts larger than
a pre-selected threshold are retained.
How the data is used in training depends on the
model. For S2Net, we constructed preference pairs
in the following way. For the same query, each rel-
evant ad is paired with a less relevant ad. The loss
function from Eq. (2) encourages achieving a higher
similarity score for the more relevant ad. For HDLR,
we used a sample of 5,000 training pairs of queries
and ads, as it was not able to scale to more train-
ing examples. For OPCA, CCA, PLSA and JPLSA,
we constructed a parallel corpus using only rele-
vant pairs of queries and ads, as the negative exam-
ples (irrelevant pairs of queries and ads) cannot be
used by these models. Finally, PCA and PLSA learn
the models from all training queries and documents
without using any relevance information.
We tested S2Net and other methods in two differ-
ent application scenarios. The first is to use the ad
relevance measure as an ad filter. When the similar-
ity score between a query and an ad is below a pre-
selected decision threshold, this ad is considered ir-
relevant to the query and will be filtered. Evaluation
metrics used for this scenario are the ROC analysis
and the area under the curve (AUC). The second one
is the ranking scenario, where the ads are selected
and ranked by their relevance scores. In this sce-
nario, the performance is evaluated by the standard
ranking metric, Normalized Discounted Cumulative
Gain (NDCG) (Jarvelin and Kekalainen, 2000).
4.2.2 Results
We first compare different methods in their AUC
and NDCG scores. TFIDF is the basic term vec-
tor representation with the TFIDF weighting (tf ?
log(N/df)). It is used as our baseline and also as
the raw input for S2Net, HDLR and other linear pro-
jection methods. Based on the results on the devel-
opment set, we found that PCA performs better than
OPCA and CCA. Therefore, we initialized the mod-
els of S2Net and HDLR using the PCA matrix. Ta-
ble 2 summarizes results on the test set. All models,
except TFIDF, use 1000 dimensions and their best
configuration settings selected on the validation set.
TFIDF is a very strong baseline on this monolin-
gual ad relevance dataset. Among all the methods
we tested, at dimension 1000, only S2Net outper-
forms the raw TFIDF cosine measure in every eval-
uation metric, and the difference is statistically sig-
252
AUC NDCG@1 NDCG@3 NDCG@5
S2Net 0.892 0.855 0.883 0.901
TFIDF 0.861 0.825 0.854 0.876
HDLR 0.855 0.826 0.856 0.877
CPLSA 0.853 0.845 0.872 0.890
PCA 0.848 0.815 0.847 0.870
OPCA 0.844 0.817 0.850 0.872
JPLSA 0.840 0.838 0.864 0.883
CCA 0.836 0.820 0.852 0.874
PLSA 0.835 0.831 0.860 0.879
Table 2: The AUC and NDCG scores of the cosine sim-
ilarity scores on different vector representations. The di-
mension for all models except TFIDF is 1000.
 0.3 0.4 0.5 0.6 0.7 0.8 0.9
 0.05  0.1  0.15  0.2  0.25True-Positive Rate False-Positive RateThe ROC Curves S2NetTFIDFHDLRCPLSA
Figure 3: The ROC curves of S2Net, TFIDF, HDLR and
CPLSA when the similarity scores are used as ad filters.
nificant4. In contrast, both CPLSA and HDLR have
higher NDCG scores but lower AUC values, and
OPCA/CCA perform roughly the same as PCA.
When the cosine scores of these vector represen-
tations are used as ad filters, their ROC curves (fo-
cusing on the low false-positive region) are shown
in Fig. 3. It can be clearly observed that the similar-
ity score computed based on vectors derived from
S2Net indeed has better quality, compared to the
raw TFIDF representation. Unfortunately, other ap-
proaches perform worse than TFIDF and their per-
formance in the low false-positive region is consis-
tent with the AUC scores.
Although ideally we would like the dimensional-
ity of the projected concept vectors to be as small
4For AUC, we randomly split the data into 50 subsets and
ran a paired-t test between the corresponding AUC scores. For
NDCG, we compared the DCG scores per query of the com-
pared models using the paired-t test. The difference is consid-
ered statistically significant when the p-value is less than 0.01.
as possible for efficient processing, the quality of
the concept vector representation usually degrades
as well. It is thus interesting to know the best trade-
off point between these two variables. Table 3 shows
the AUC and NDCG scores of S2Net at different di-
mensions, as well as the results achieved by TFIDF
and PCA, HDLR and CPLSA at 1000 dimensions.
As can be seen, S2Net surpasses TFIDF in AUC at
dimension 300 and keeps improving as the dimen-
sionality increases. Its NDCG scores are also con-
sistently higher across all dimensions.
4.3 Discussion
It is encouraging to find that S2Net achieves strong
performance in two very different tasks, given that
it is a conceptually simple model. Its empirical suc-
cess can be attributed to two factors. First, it is flex-
ible in choosing the loss function and constructing
training examples and is thus able to optimize the
model directly for the target task. Second, it can
be trained on a large number of examples. For ex-
ample, HDLR can only use a few thousand exam-
ples and is not able to learn a matrix better than its
initial model for the task of cross-lingual document
retrieval. The fact that linear projection methods
like OPCA/CCA and generative topic models like
JPLSA/CPLSA cannot use negative examples more
effectively also limits their potential.
In terms of scalability, we found that methods
based on eigen decomposition, such as PCA, OPCA
and CCA, take the least training time. The complex-
ity is decided by the size of the covariance matrix,
which is quadratic in the number of dimensions. On
a regular eight-core server, it takes roughly 2 to 3
hours to train the projection matrix in both experi-
ments. The training time of S2Net scales roughly
linearly to the number of dimensions and training
examples. In each iteration, performing the projec-
tion takes the most time in gradient derivation, and
the complexity is O(mnk), where m is the num-
ber of distinct term-vectors, n is the largest number
of non-zero elements in the sparse term-vectors and
k is the dimensionality of the concept space. For
cross-lingual document retrieval, when k = 1000,
each iteration takes roughly 48 minutes and about 80
iterations are required to convergence. Fortunately,
the gradient computation is easily parallelizable and
further speed-up can be achieved using a cluster.
253
TFIDF HDLR CPLSA PCA S2Net100 S2Net300 S2Net500 S2Net750 S2Net1000
AUC 0.861 0.855 0.853 0.848 0.855 0.879 0.880 0.888 0.892
NDCG@1 0.825 0.826 0.845 0.815 0.843 0.852 0.856 0.860 0.855
NDCG@3 0.854 0.856 0.872 0.847 0.871 0.879 0.881 0.884 0.883
NDCG@5 0.876 0.877 0.890 0.870 0.890 0.897 0.899 0.902 0.901
Table 3: The AUC and NDCG scores of S2Net at different dimensions. PCA, HDLR & CPLSA (at dimension 1000)
along with the raw TFIDF representation are used for reference.
5 Related Work
Although the high-level design of S2Net follows the
Siamese architecture (Bromley et al, 1993; Chopra
et al, 2005), the network construction, loss func-
tion and training process of S2Net are all differ-
ent compared to previous work. For example, tar-
geting the application of face verification, Chopra
et al (2005) used a convolutional network and de-
signed a contrastive loss function for optimizing a
Eucliden distance metric. In contrast, the network
of S2Net is equivalent to a linear projection ma-
trix and has a pairwise loss function. In terms of
the learning framework, S2Net is closely related to
several neural network based approaches, including
autoencoders (Hinton and Salakhutdinov, 2006) and
finding low-dimensional word representations (Col-
lobert and Weston, 2008; Turian et al, 2010). Archi-
tecturally, S2Net is also similar to RankNet (Burges
et al, 2005), which can be viewed as a Siamese neu-
ral network that learns a ranking function.
The strategy that S2Net takes to learn from la-
beled pairs of documents can be analogous to the
work of distance metric learning. Although high
dimensionality is not a problem to algorithms like
HDLR, it suffers from a different scalability issue.
As we have observed in our experiments, the al-
gorithm can only handle a small number of simi-
larity/dissimilarity constraints (i.e., the labeled ex-
amples), and is not able to use a large number of
examples to learn a better model. Empirically, we
also found that HDLR is very sensitive to the hyper-
parameter settings and its performance can vary sub-
stantially from iteration to iteration.
Other than the applications presented in this pa-
per, concept vectors have shown useful in traditional
IR tasks. For instance, Egozi et al (2008) use ex-
plicit semantic analysis to improve the retrieval re-
call by leveraging Wikipedia. In a companion pa-
per, we also demonstrated that various topic mod-
els including S2Net can enhance the ranking func-
tion (Gao et al, 2011). For text categorization, simi-
larity between terms is often encoded as kernel func-
tions embedded in the learning algorithms, and thus
increase the classification accuracy. Representative
approaches include latent semantic kernels (Cris-
tianini et al, 2002), which learns an LSA-based ker-
nel function from a document collection, and work
that computes term-similarity based on the linguis-
tic knowledge provided by WordNet (Basili et al,
2005; Bloehdorn and Moschitti, 2007).
6 Conclusions
In this paper, we presented S2Net, a discrimina-
tive approach for learning a projection matrix that
maps raw term-vectors to a low-dimensional space.
Our learning method directly optimizes the model
so that the cosine score of the projected vectors can
become a reliable similarity measure. The strength
of this model design has been shown empirically in
two very different tasks. For cross-lingual document
retrieval, S2Net significantly outperforms OPCA,
which is the best prior approach. For ad selection
and filtering, S2Net alo outperforms all methods we
compared it with and is the only technique that beats
the raw TFIDF vectors in both AUC and NDCG.
The success of S2Net is truly encouraging, and
we would like to explore different directions to fur-
ther enhance the model in the future. For instance, it
will be interesting to extend the model to learn non-
linear transformations. In addition, since the pairs of
text objects being compared often come from differ-
ent distributions (e.g., English documents vs. Span-
ish documents or queries vs. pages), learning two
different matrices instead of one could increase the
model expressivity. Finally, we would like to apply
S2Net to more text similarity tasks, such as word
similarity and entity recognition and discovery.
254
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and WordNet-based approaches. In Proceedings of
HLT-NAACL, pages 19?27, June.
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference
for topic models. In UAI.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005. Effective use of WordNet semantics via
kernel-based learning. In CoNLL.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet alocation. Jour-
nal of Machine Learning Research, 3:993?1022.
Stephan Bloehdorn and Alessandro Moschitti. 2007.
Combined syntactic and semantic kernels for text clas-
sification. In ECIR, pages 307?318.
Andrei Z. Broder, Peter Ciccolo, Marcus Fontoura,
Evgeniy Gabrilovich, Vanja Josifovski, and Lance
Riedel. 2008. Search advertising using web relevance
feedback. In CIKM, pages 1013?1022.
Jane Bromley, James W. Bentz, Le?on Bottou, Isabelle
Guyon, Yann LeCun, Cliff Moore, Eduard Sa?ckinger,
and Roopak Shah. 1993. Signature verification us-
ing a ?Siamese? time delay neural network. Interna-
tional Journal Pattern Recognition and Artificial Intel-
ligence, 7(4):669?688.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to rank using gradient descent. In
ICML.
Y. Choi, M. Fontoura, E. Gabrilovich, V. Josifovski,
M. Mediano, and B. Pang. 2010. Using landing pages
for sponsored search ad selection. In WWW.
Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005.
Learning a similarity metric discriminatively, with ap-
plication to face verification. In Proceedings of CVPR-
2005, pages 539?546.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neural
networks with multitask learning. In ICML.
Nello Cristianini, John Shawe-Taylor, and Huma Lodhi.
2002. Latent semantic kernels. Journal of Intelligent
Information Systems, 18(2?3):127?152.
Jason V. Davis and Inderjit S. Dhillon. 2008. Struc-
tured metric learning for high dimensional problems.
In KDD, pages 195?203.
Jason V. Davis, Brian Kulis, Prateek Jain, Suvrit Sra, and
Inderjit S. Dhillon. 2007. Information-theoretic met-
ric learning. In ICML.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1990. In-
dexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Konstantinos I. Diamantaras and S.Y. Kung. 1996. Prin-
cipal Component Neural Networks: Theory and Appli-
cations. Wiley-Interscience.
Susan T. Dumais, Todd A. Letsche, Michael L. Littman,
and Thomas K. Landauer. 1997. Automatic cross-
linguistic information retrieval using latent seman-
tic indexing. In AAAI-97 Spring Symposium Series:
Cross-Language Text and Speech Retrieval.
Ofer Egozi, Evgeniy Gabrilovich, and Shaul Markovitch.
2008. Concept-based feature generation and selection
for information retrieval. In AAAI.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of COLING-ACL.
Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and
Ben Taskar. 2009. Posterior regularization for struc-
tured latent variable models. Technical Report MS-
CIS-09-16, University of Pennsylvania.
Jianfeng Gao, Kristina Toutanova, and Wen-tau Yih.
2011. Clickthrough-based latent semantic models for
web search. In SIGIR.
G. E. Hinton and R. R. Salakhutdinov. 2006. Reducing
the dimensionality of data with neural networks. Sci-
ence, 313(5786):504?507, July.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In SIGIR ?99, pages 50?57.
K. Jarvelin and J. Kekalainen. 2000. Ir evaluation meth-
ods for retrieving highly relevant documents. In SI-
GIR, pages 41?48.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. of COLING-ACL 98.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Pres.
David Mimno, Hanna W. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In EMNLP.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31:477?504.
Jorge Nocedal and Stephen Wright. 2006. Numerical
Optimization. Springer, 2nd edition.
John Platt, Kristina Toutanova, and Wen-tau Yih. 2010.
Translingual document representations from discrimi-
native projections. In EMNLP.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of the ACL, pages 519?526.
255
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In ACL.
Alexei Vinokourov, John Shawe-taylor, and Nello Cris-
tianini. 2003. Inferring a semantic representation of
text via cross-language correlation analysis. In NIPS-
15.
Vishnu Vyas and Patrick Pantel. 2009. Semi-automatic
entity set refinement. In NAACL ?09, pages 290?298.
Wen-tau Yih and Ning Jiang. 2010. Similarity models
for ad relevance measures. In MLOAD - NIPS 2010
Workshop on online advertising.
Appendix A. Gradient Derivation
The gradient of the loss function in Eq. (2) can be
derived as follows.
?L(?,A)
?A
=
??
1 + exp(???)
??
?A
??
?A
=
?
?A
simA(fp1 , fq1)?
?
?A
simA(fp2 , fq2)
?
?A
simA(fp, fq) =
?
?A
cos(gp,gq),
where gp = AT fp and gq = AT fq are the projected
concept vectors of fq and fq. The gradient of the
cosine score can be further derived in the following
steps.
cos(gp,gq) =
gTp gq
?gp??gq?
?Ag
T
p gq = (?AA
T fp)gq + (?AA
T fq)gp
= fpgTq + fqg
T
p
?A
1
?gp?
= ?A(g
T
p gp)
? 12
= ?
1
2
(gTp gp)
? 32?A(g
T
p gp)
= ?(gTp gp)
? 32 fpgTp
?A
1
?gq?
= ?(gTq gq)
? 32 fqgTq
Let a, b, c be gTp gq, 1/?gp? and 1/?gq?, respec-
tively.
?A
gTp gq
?gp??gq?
= ? abc3fqgTq ? acb
3fpgTp
+ bc(fpgTq + fqg
T
p )
256
