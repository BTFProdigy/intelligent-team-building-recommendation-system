A Max imum-Ent ropy- Insp i red  Parser  * 
Eugene Charniak 
Brown Laboratory for Linguistic Information Processing 
Department  of Computer  Science 
Brown University, Box 1910, Providence, RI  02912 
ec@cs.brown.edu 
Abstract 
We present a new parser for parsing down to 
Penn tree-bank style parse trees that achieves 
90.1% average precision/recall for sentences of 
length 40 and less, and 89.5% for sentences of 
length 100 and less when trMned and tested on 
the previously established \[5,9,10,15,17\] "stan- 
dard" sections of the Wall Street Journal tree- 
bank. This represents a 13% decrease in er- 
ror rate over the best single-parser results on 
this corpus \[9\]. The major technical innova- 
tion is tire use of a "ma~ximum-entropy-inspired" 
model for conditioning and smoothing that let 
us successfully to test and combine many differ- 
ent conditioning events. We also present some 
partial results showing the effects of different 
conditioning information, including a surpris- 
ing 2% improvement due to guessing the lexical 
head's pre-terminal before guessing the lexical 
head. 
1 Introduction 
We present a new parser for parsing down 
to Penn tree-bank style parse trees \[16\] that 
achieves 90.1~ average precision/recall for sen- 
tences of length < 40, and 89.5% for sentences 
of length < 100, when trained and tested on the 
previously established \[5,9,10,15,17\] "standard" 
sections of the Wall Street Journal tree-bank. 
This represents a 13% decrease in error rate over 
the best single-parser results on this corpus \[9\]. 
Following \[5,10\], our parser is based upon a 
probabilistic generative model. That is, for all 
sentences s and MI parses 7r, the parser assigns a
probability p(s, ~) = p(Tr), the equality holding 
when we restrict consideration to ~r whose yield 
* This research was supported in part by NSF grant 
LIS SBR 9720368. The author would like to thank Mark 
Johnson and all the rest of the Brown Laboratory for 
Linguistic Information Processing. 
is s. Then for any s the parser eturns the parse 
7r that maximizes this probability. That is, the 
parser implements the function 
arg. a= p(  I = 
= arg maxTrp(lr). 
What fundamentally distinguishes probabilis- 
tic generative parsers is how they compute p(~r), 
and it is to that topic we turn next. 
2 The Generative Model 
The model assigns a probability to a parse by 
a top-down process of considering each con- 
stituent c in ~r and for each c first guessing the 
pre-terminal of c, t(c) (t for "tag"), then the 
lexical head of c, h(c), and then the expansion 
of c into further constituents c(c). Thus the 
probability of a parse is given by the equation 
1"I P(t(c) l l(c),H(c)) 
cE;? 
.v(h(c) l t(c),l(c),H(c)) 
.p(e(c) i l(c),t(c),h(c),H(c)) 
where l(c) is the label of c (e.g., whether it is a 
noun phrase (np), verb-phrase, tc.) and H(c)is 
the relevant history of c - -  information outside c
that our probability model deems important in 
determining the probability in question. Much 
of the interesting work is determining what goes 
into H(c). Whenever it is clear to which con- 
stituent we are referring we omit the (c) in, e.g., 
h(c). In this notation the above equation takes 
the following form: 
= 1-\[ v(t I l , z ) .v (h  I t , l ,H) .v(?  I 
cE;? 
132 
Next we describe how we assign a probability 
to the expansion e of a constituent. In Sec- 
tion 5 we present some results in which the 
possible expansions of a constituent are fixed 
in advanced by extracting a tree-bank grammar 
\[3\] from the training corpus. The method that 
gives the best results, however, uses a Markov 
grammar - -  a method for assigning probabil- 
ities to any possible expansion using statistics 
gathered from the training corpus \[6,10,15\]. The 
method we use follows that of \[10\]. In this 
scheme a traditional probabilistic context-free 
grammar (PCFG) rule can be thought of as con- 
sisting of a left-hand side with a label l(e) drawn 
from the non-terminal symbols of our grammar, 
and a right-hand side that is a sequence of one or 
more such symbols. (We assume that all termi- 
nal symbols are generated by rules of the form 
"preterm -+ word' and we treat these as a spe- 
cial case.) For us the non-terminal symbols are 
those of the tree-bank, augmented by the sym- 
bols aux and auxg, which have been assigned e- 
terministically to certain auxiliary verbs such as 
"have" or "having". For each expansion we dis- 
tinguish one of the right-hand side labels as the 
"middle" or "head" symbol M(c). M(c) is the 
constituent from which the head lexical item h 
is obtained according to deterministic rules that 
pick the head of a constituent from among the 
heads of its children. To the left of M is a se- 
quence of one or more left labels Li (c) including 
the special termination symbol A, which indi- 
cates that there are no more symbols to the left, 
and similarly for the labels to the right, Ri(c). 
Thus an expansion e(c) looks like: 
1 --~ ALm...L1MRI...RnA. (2) 
The expansion is generated by guessing first M, 
then in order L1 through Lm+t (= A), and sim- 
ilarly for R1 through R,+~. 
In a pure Markov PCFG we are given the 
left-hand side label l and then probabilisticaily 
generate the right-hand side conditioning on no 
information other than I and (possibly) previ- 
ously generated pieces of the right-hand side 
itself. In the simplest of such models, a zero- 
order Markov grammar, each label on the right- 
hand side is generated conditioned only on l - -  
that is, according to the distributions p(Li I l), 
p(M l I), and p(Ri l l). 
More generally, one can condition on the m 
previously generated labels, thereby obtaining 
an mth-order Markov grammar. So, for ex- 
ample, in a second-order Markov PCFG, L2 
would be conditioned on L1 and M. In our 
complete model, of course, the probability of 
each label in the expansions i  also conditioned 
on other material as specified in Equation 1, 
e.g., p(e I l, t, h, H). Thus we would use p(L2 I 
L1, M, l, t, h, H). Note that the As on both ends 
of the expansion in Expression 2 are conditioned 
just like any other label in the expansion. 
3 Max imum-Ent ropy- Insp i red  
Parsing 
The major problem confronting the author of 
a generative parser is what information to use 
to condition the probabilities required in the 
model, and how to smooth the empirically ob- 
tained probabilities to take the sting out of the 
sparse data problems that are inevitable with 
even the most modest conditioning. For exam- 
ple, in a second-order Markov grammar we con- 
ditioned the L2 label according to the distribu- 
tion p(L2 I Lt ,M, I , t ,h ,H) .  Also, remember 
that H is a placeholder for any other informa- 
tion beyond the constituent e that may be useful 
in assigning c a probability. 
In the past few years the maximum entropy, 
or log-linear, approach as recommended itself 
to probabilistic model builders for its flexibility 
and its novel approach to smoothing \[1,17\]. A
complete review of log-linear models is beyond 
the scope of this paper. Rather, we concentrate 
on the aspects of these models that most di- 
rectly influenced the model presented here. 
To compute a probability in a log-linear 
model one first defines a set of "features", 
functions from the space of configurations over 
which one is trying to compute probabilities to 
integers that denote the number of times some 
pattern occurs in the input. In our work we as- 
sume that any feature can occur at most once, 
so features are boolean-valued: 0 if the pattern 
does not occur, 1 if it does. 
In the parser we further assume that fea- 
tures are chosen from certain feature schemata 
and that every feature is a boolean conjunc- 
tion of sub-features. For example, in computing 
the probability of the head's pre-terminal t we 
might want a feature schema f(t ,  l) that returns 
1 if the observed pre-terminal of c = t and the 
133 
label of c = l, and zero otherwise. This feature 
is obviously composed of two sub-features, one 
recognizing t, the other 1. If both return 1, then 
the feature returns 1. 
Now consider computing a conditional prob- 
ability p(a I H) with a set of features f l . . .  f j  
that connect a to the history H. In a log-linear 
model the probability function takes the follow- 
ing form: 
1 eXl(a,H)fl(a,H)+...+,km(a,H).fm(a,H) p(a I H )  - Z(H) 
(3) 
Here the Ai are weights between negative and 
positive infinity that indicate the relative impor- 
tance of a feature: the more relevant he feature 
to the value of the probability, the higher the ab- 
solute value of the associated X. The function 
Z(H), called the partition function, is a normal- 
izing constant (for fixed H), so the probabilities 
over all a sum to one. 
Now for our purposes it is useful to rewrite 
this as a sequence of multiplicative functions 
gi(a,H) for 0 < i < j :  
p(a I H)= go(a,H)gl(a,H) .. .gj(a,H). (4) 
Here go(a,H) = 1/Z(H) and gi(a,H) = 
e'~(a'n)f~(a'H). The intuitive idea is that each 
factor gi is larger than one if the feature in ques- 
tion makes the probability more likely, one if the 
feature has no effect, and smaller than one if it 
makes the probability less likely. 
Maximum-entropy models have two benefits 
for a parser builder. First, as already implicit in 
our discussion, factoring the probability compu- 
tation into a sequence of values corresponding 
to various 'tfeatures" suggests that the proba- 
bility model should be easily changeable - - just 
change the set of features used. This point is 
emphasized by Ratnaparkhi n discussing his 
parser \[17\]. Second, and this is a point we have 
not yet mentioned, the features used in these 
models need have no particular independence of
one another. This is useful if one is using a log- 
linear model for smoothing. That is, suppose 
we want to compute a conditional probability 
p(a \] b,c), but we are not sure that we have 
enough examples of the conditioning event b, c 
in the training corpus to ensure that the empiri- 
cally obtained probability/~(a \[ b, c) is accurate. 
The traditional way to handle this is also to 
compute/~(a I b), and perhaps iS(a I c) as well, 
and take some combination of these values as 
one's best estimate for p(a I b, c). This method 
is known as "deleted interpolation" smoothing. 
In max-entropy models one can simply include 
features for all three events fl(a, b, c), f2(a, b), 
and f3(a, c) and combine them in the model ac- 
cording to Equation 3, or equivalently, Equation 
4. The fact that the features are very far from 
independent is not a concern. 
Now let us note that we can get an equation 
of exactly the same form as Equation 4 in the 
following fashion: 
p(alb, c)p(alb, c,d) 
p(alb, c ,d )=p(a lb ) -~a lb )  p(alb, c) 
(5) 
Note that the first term of the equation gives a 
probability based upon little conditioning infor- 
mation and that each subsequent term is a num- 
ber from zero to positive infinity that is greater 
or smaller than one if the new information be- 
ing considered makes the probability greater or 
smaller than the previous estimate. 
As it stands, this last equation is pretty much 
content-free. But let us look at how it works for 
a particular case in our parsing scheme. Con- 
sider the probability distribution for choosing 
the pre-terminal for the head of a constituent. 
In Equation I we wrote this as p(t I l, H). As 
we discuss in more detail in Section 5, several 
different features in the context surrounding c 
are useful to include in H: the label, head 
pre-terminal and head of the parent of c (de- 
noted as lv, tv, hp), the label of c's left sibling 
(lb for "before"), and the label of the grand- 
parent of c (la). That is, we wish to compute 
p(t I l, lv, tv, lb, lg, by). We can now rewrite this 
in the form of Equation 5 as follows: 
p(t I 1, Iv, tv, lb, IQ, hv) = 
p(t l t)P(t l t, tv) P(t l t, tv, tv) p(t l t, tp, tv, tb) 
p(t l l) p(t l l, lp) p(t l t, tp, tp) 
P(t l t'Iv'tv'Ib'Ig)p(t l t'Ip'tv'Ib'Ig'hP). (6) 
p(t I z, t,, t,, lb) p(t I t, l,, t,, lb, t,) 
Here we have sequentially conditioned on 
steadily increasing portions of c's history. In 
many cases this is clearly warranted. For ex- 
ample, it does not seem to make much sense 
to condition on, say, h v without first condition- 
ing on tp. In other cases, however, we seem 
134 
to be conditioning on apples and oranges, so 
to speak. For example, one can well imagine 
that one might want to condition on the par- 
ent's lexical head without conditioning on the 
left sibling, or the grandparent label. One way 
to do this is to modify the simple version shown 
in Equation 6 to allow this: 
p(t I l, l., b, h,) = 
p(t t l)P(t l l, lv) P(t l l, lp, tv) P(t l l, lv, tp, lb) 
p(t i l ) p(t l l ,lp) p(t l l ,lv,tv) 
p(t I l, lp, tp, p(t I l, t,,, 
p(t I l, lp, tp) p(t I l, tp, (7) 
Note the changes to the last three terms in 
Equation 7. Rather than conditioning each 
term on the previous ones, they are now condi- 
tioned only on those aspects of the history that 
seem most relevant. The hope is that by doing 
this we will have less difficulty with the splitting 
of conditioning events, and thus somewhat less 
difficulty with sparse data. 
We make one more point on the connec- 
tion of Equation 7 to a maximum entropy for- 
mulation. Suppose we were, in fact, going 
to compute a true maximum entropy model 
based upon the features used in Equation 7, 
f l(t, l),f2(t, l ,  lp),f3(t,l, lv) .... This requires 
finding the appropriate his for Equation 3, 
which is accomplished using an algorithm such 
as iterative scaling \[11\] in which values for the Ai 
are initially "guessed" and then modified until 
they converge on stable values. With no prior 
knowledge of values for the )q one traditionally 
starts with )~i = 0, this being a neutral assump- 
tion that the feature has neither a positive nor 
negative impact on the probability in question. 
With some prior knowledge, non-zero values can 
greatly speed up this process because fewer it- 
erations are required for convergence. We com- 
ment on this because in our example we can sub- 
stantially speed up the process by choosing val- 
ues picked so that, when the maximum-entropy 
equation is expressed in the form of Equation 
4, the gi have as their initial values the values 
of the corresponding terms in Equation 7. (Our 
experience is that rather than requiring 50 or so 
iterations, three suffice.) Now we observe that 
if we were to use a maximum-entropy approach 
but run iterative scaling zero times, we would, 
in fact, just have Equation 7. 
The major advantage of using Equation 7 is 
that one can generally get away without com- 
puting the partition function Z(H). In the sim- 
ple (content-free) form (Equation 6), it is clear 
that Z(H) = 1. In the more interesting version, 
Equation 7, this is not true in general, but one 
would not expect it to differ much from one, 
and we assume that as long as we are not pub- 
lishing the raw probabilities (as we would be 
doing, for example, in publishing perplexity re- 
sults) the difference from one should be unim- 
portant. As partition-function calculation is 
typically the major on-line computational prob- 
lem for maximum-entropy models, this simpli- 
fies the model significantly. 
Naturally, the distributions required by 
Equation 7 cannot be used without smooth- 
ing. In a pure maximum-entropy model this is 
done by feature selection, as in Ratnaparkhi's 
maximum-entropy arser \[17\]. While we could 
have smoothed in the same fashion, we choose 
instead to use standard deleted interpolation. 
(Actually, we use a minor variant described in 
\[4\].) 
4 The Exper iment 
We created a parser based upon the maximum- 
entropy-inspired model of the last section, 
smoothed using standard eleted interpolation. 
As the generative model is top-down and we 
use a standard bottom-up best-first probabilis- 
tic chart parser \[2,7\], we use the chart parser as 
a first pass to generate candidate possible parses 
to be evaluated in the second pass by our prob- 
abilistic model. For runs with the generative 
model based upon Markov grammar statistics, 
the first pass uses the same statistics, but con- 
ditioned only on standard PCFG information. 
This allows the second pass to see expansions 
not present in the training corpus. 
We use the gathered statistics for all observed 
words, even those with very low counts, though 
obviously our deleted interpolation smoothing 
gives less emphasis to observed probabilities for 
rare words. We guess the preterminals of words 
that are not observed in the training data using 
statistics on capitalization, hyphenation, word 
endings (the last two letters), and the probabil- 
ity that a given pre-terminal is realized using a 
previously unobserved word. 
As noted above, the probability model uses 
135 
Parser LR LP CB 0CB 2CB 
< 40 words (2245 sentences) 
Char97 87.5 87.4 1.00 62.1 86.1 
Co1199 88.5 88.7 0.92 66.7 87.1 
Char00 90.1 90.1 0.74 70.1 89.6 
< 100 words (2416 sentences) 
Char97 86.7 86.6 1.20 59.9 83.2 
Coll99 88.1 88.3 1.06 64.0 85.1 
Ratna99 86.3 87.5 
Char00 89.6 89.5 0.88 67.6 87.7 
Figure 1: Parsing results compared with previ- 
ous work 
five smoothed probability distributions, one 
each for L~, M, Ri, t, and h. The equation for 
the (unsmoothed) conditional probability distri- 
bution for t is given in Equation 7. The other 
four equations can be found in a longer version 
of this paper available on the author's website 
(www.cs.brown.edu/~.,ec). L and R are condi- 
tioned on three previous labels so we are using 
a third-order Markov grammar. Also, the label 
of the parent constituent Ip is conditioned upon 
even when it is not obviously related to the fur- 
ther conditioning events. This is due to the im- 
portance of this factor in parsing, as noted in, 
e.g., \[14\]. 
In keeping with the standard methodology \[5, 
9,10,15,17\], we used the Penn Wall Street Jour- 
nal tree-bank \[16\] with sections 2-21 for train- 
ing, section 23 for testing, and section 24 for 
development (debugging and tuning). 
Performance on the test corpus is measured 
using the standard measures from \[5,9,10,17\]. 
In particular, we measure labeled precision 
(LP) and recall (LR), average number of cross- 
brackets per sentence (CB), percentage of sen- 
tences with zero cross brackets (0CB), and per- 
centage of sentences with < 2 cross brackets 
(2CB). Again as standard, we take separate 
measurements for all sentences of length <_ 40 
and all sentences of length < 100. Note that 
the definitions of labeled precision and recall are 
those given in \[9\] and used in all of the previous 
work. As noted in \[5\], these definitions typically 
give results about 0.4% higher than the more 
obvious ones. The results for the new parser 
as well as for the previous top-three individual 
parsers on this corpus are given in Figure 1. 
As is typical, all of the standard measures tell 
pretty much the same story, with the new parser 
outperforming the other three parsers. Looking 
in particular at the precision and recall figures, 
the new parser's give us a 13% error reduction 
over the best of the previous work, Co1199 \[9\]. 
5 Discussion 
In the previous sections we have concentrated 
on the relation of the parser to a maximum- 
entropy approach, the aspect of the parser that 
is most novel. However, we do not think this 
aspect is the sole or even the most important 
reason for its comparative success. Here we list 
what we believe to be the most significant con- 
tributions and give some experimental results 
on how well the program behaves without them. 
We take as our starting point the parser 
labled Char97 in Figure 1 \[5\], as that is the 
program from which our current parser derives. 
That parser, as stated in Figure 1, achieves an 
average precision/recall of 87.5. As noted in \[5\], 
that system is based upon a "tree-bank gram- 
mar" - -  a grammar ead directly off the train- 
ing corpus. This is as opposed to the "Markov- 
grammar" approach used in the current parser. 
Also, the earlier parser uses two techniques not 
employed in the current parser. First, it uses 
a clustering scheme on words to give the sys- 
tem a "soft" clustering of heads and sub-heads. 
(It is "soft" clustering in that a word can be- 
long to more than one cluster with different 
weights - -  the weights express the probability 
of producing the word given that one is going 
to produce a word from that cluster.) Second, 
Char97 uses unsupervised learning in that the 
original system was run on about thirty million 
words of unparsed text, the output was taken 
as "correct", and statistics were collected on 
the resulting parses. Without these enhance- 
ments Char97 performs at the 86.6% level for 
sentences of length < 40. 
In this section we evaluate the effects of the 
various changes we have made by running var- 
ious versions of our current program. To avoid 
repeated evaluations based upon the testing cor- 
pus, here our evaluation is based upon sen- 
tences of length < 40 from the development cor- 
pus. We note here that this corpus is somewhat 
more difficult than the "official" test corpus. 
For example, the final version of our system 
136 
System Precision Recall 
Old 86.3 86.1 
Explicit Pre-Term 88.0 88.1 
Marked Coordination 88.6 88.7 
Standard Interpolation 88.2 88.3 
MaxEnt-Inspired 89.0 89.2 
First-order Markov 88.6 87.4 
Second-order Markov 89.5 89.3 
Best 89.8 89.6 
Figure 2: Labeled precision/recall for length < 
40, development corpus 
achieves an average precision/recall of 90.1% on 
the test corpus but an average precision/recall 
of only 89.7% on the development corpus. This 
is indicated in Figure 2, where the model la- 
beled "Best" has precision of 89.8% and recall of 
89.6% for an average of 89.7%, 0.4% lower than 
the results on the official test corpus. This is in 
accord with our experience that development- 
corpus results are from 0.3% to 0.5% lower than 
those obtained on the test corpus. 
The model abeled "Old" attempts to recreate 
the Char97 system using the current program. 
It makes no use of special maximum-entropy- 
inspired features (though their presence made 
it much easier to perform these experiments), it 
does not guess the pre-terminal before guess- 
ing the lexical head, and it uses a tree-bank 
grammar rather than a Markov grammar. This 
parser achieves an average precision/recall of 
86.2%. This is consistent with the average pre- 
cision/recall of 86.6% for \[5\] mentioned above, 
as the latter was on the test corpus and the for- 
mer on the development corpus. 
Between the Old model and the Best model, 
Figure 2 gives precision/recall measurements for 
several different versions of our parser. One of 
the first and without doubt the most signifi- 
cant change we made in the current parser is to 
move from two stages of probabilistic decisions 
at each node to three. As already noted, Char97 
first guesses the lexical head of a constituent 
and then, given the head, guesses the PCFG 
rule used to expand the constituent in question. 
In contrast, the current parser first guesses the 
head's pre~terminal, then the head, and then the 
expansion. It turns out that usefulness of this 
process had a/ready been discovered by Collins 
\[10\], who in turn notes (personal communica- 
tion) that it was previously used by Eisner \[12\]. 
However, Collins in \[10\] does not stress the de- 
cision to guess the head's pre-terminal first, and 
it might be lost on the casual reader. Indeed, 
it was lost on the present author until he went 
back after the fact and found it there. In Figure 
2 we show that this one factor improves perfor- 
mance by nearly 2%. 
It may not be obvious why this should make 
so great a difference, since most words are ef- 
fectively unambiguous. (For example, part-of- 
speech tagging using the most probable pre- 
terminal for each word is 90% accurate \[8\].) We 
believe that two factors contribute to this per- 
formance gain. The first is simply that if we first 
guess the pre~terminal, when we go to guess the 
head the first thing we can condition upon is 
the pre-terminal, i.e., we compute p(h I t). This 
quantity is a relatively intuitive one (as, for ex- 
ample, it is the quantity used in a PCFG to re- 
late words to their pre-terminals) and it seems 
particularly good to condition upon here since 
we use it, in effect, as the unsmoothed probabil- 
ity upon which all smoothing of p(h) is based. 
This one '~fix" makes lightly over a percent dif- 
ference in the results. 
The second major reason why first guessing 
the pre-terminal makes so much difference is 
that it can be used when backing off the lexical 
head in computing the probability of the rule 
expansion. For example, when we first guess 
the lexical head we can move from computing 
p(r I 1, lp, h) to p(r I l,t, lp, h). So, e.g., even 
if the word "conflating" does not appear in the 
training corpus (and it does not)~ the "ng" end- 
ing allows our program to guess with relative 
security that the word has the vbg pre-terminal, 
and thus the probability of various rule expan- 
sions can be considerable sharpened. For exam- 
ple, the tree-bank PCFG probability of the rule 
"vp --+ vbg np" is 0.0145, whereas once we con- 
dition on the fact that the lexical head is a vbg 
we get a probability of 0.214. 
The second modification is the explicit mark- 
ing of noun and verb-phrase coordination. We 
have already noted the importance of condition- 
ing on the parent label l v. So, for example, 
information about an np is conditioned on the 
parent - -  e.g., an s, vp, pp, etc. Note that when 
an np is part of an np coordinate structure the 
137 
vp 
aux vp 
vbd np 
Figure 3: Verb phrase with both main and aux- 
iliary verbs 
parent will itself be an np, and similarly for a 
vp. But nps and vps can occur with np and 
vp parents in non-coordinate structures as well. 
For example, in the Penn Treebank a vp with 
both main and auxiliary verbs has the structure 
shown in Figure 3. Note that the subordinate 
vp has a vp parent. 
Thus np and vp parents of constituents are 
marked to indicate if the parents are a coor- 
dinate structure. A vp coordinate structure 
is defined here as a constituent with two or 
more vp children, one or more of the con- 
stituents comma, cc, conjp (conjunctive phrase), 
and nothing else; coordinate np phrases are de- 
fined similarly. Something very much like this is 
done in \[15\]. As shown in Figure 2, condition- 
ing on this information gives a 0.6% improve- 
ment. We believe that this is mostly due to 
improvements in guessing the sub-constituent's 
pre-terminai and head. Given we are already 
at the 88% level of accuracy, we judge a 0.6% 
improvement to be very much worth while. 
Next we add the less obvious conditioning 
events noted in our previous discussion of the 
final model - -  grandparent label I a and left 
sibling label lb. When we do so using our 
maximum-entropy-inspired conditioning, we get 
another 0.45% improvement in average preci- 
sion/recall, as indicated in Figure 2 on the line 
labeled "MaocEnt-Inspired'. Note that we also 
tried including this information using a stan- 
dard deleted-interpolation model. The results 
here are shown in the line "Standard Interpola- 
tion". Including this information within a stan- 
dard deleted-interpolation model causes a 0.6% 
decrease from the results using the less conven- 
tional model. Indeed, the resulting performance 
is worse than not using this information at all. 
Up to this point all the models considered 
in this section are tree-bank grammar models. 
That is, the PCFG grammar ules are read di- 
rectly off the training corpus. As already noted, 
our best model uses a Markov-grammar ap- 
proach. As one can see in Figure 2, a first- 
order Markov grammar (with all the aforemen- 
tioned improvements) performs slightly worse 
than the equivalent tree-bank-grammar parser. 
However, a second-order grammar does slightly 
better and a third-order grammar does signifi- 
cantly better than the tree-bank parser. 
6 Conc lus ion  
We have presented a lexicalized Markov gram- 
mar parsing model that achieves (using the now 
standard training/testing/development sections 
of the Penn treebank) an average preci- 
sion/recall of 91.1% on sentences of length < 
40 and 89.5% on sentences of length < 100. 
This corresponds to an error reduction of 13% 
over the best previously published single parser 
results on this test set, those of Collins \[9\]. 
That the previous three best parsers on this 
test \[5,9,17\] all perform within a percentage 
point of each other, despite quite different ba- 
sic mechanisms, led some researchers to won- 
der if there might be some maximum level of 
parsing performance that could be obtained us- 
ing the treebank for training, and to conjec- 
ture that perhaps we were at it. The results 
reported here disprove this conjecture. The re- 
sults of \[13\] achieved by combining the afore- 
mentioned three-best parsers also suggest hat 
the limit on tree-bank trained parsers is much 
higher than previously thought. Indeed, it may 
be that adding this new parser to the mix may 
yield still higher results. 
From our perspective, perhaps the two most 
important numbers to come out of this re- 
search are the overall error reduction of 13% 
over the results in \[9\] and the intermediate- 
result improvement ofnearly 2% on labeled pre- 
cision/recall due to the simple idea of guess- 
ing the bead's pre-terminal before guessing the 
head. Neither of these results were anticipated 
at the start of this research. 
As noted above, the main methodological 
innovation presented here is our "maximum- 
entropy-inspired" model for conditioning and 
smoothing. Two aspects of this model deserve 
some comment. The first is the slight, but im- 
portant, improvement achieved by using this 
model over conventional deleted interpolation, 
as indicated in Figure 2. We expect that as 
138 
we experiment with other, more semantic on- 
ditioning information, the importance of this as- 
pect of the model will increase. 
More important in our eyes, though, is 
the flexibility of the maximum-entropy-inspired 
model. Though in some respects not quite as 
flexible as true maximum entropy, it is much 
simpler and, in our estimation, has benefits 
when it comes to smoothing. Ultimately it is 
this flexibility that let us try the various condi- 
tioning events, to move on to a Markov gram- 
mar approach, and to try several Markov gram- 
mars of different orders, without significant pro- 
gramming. Indeed, we initiated this line of work 
in an attempt o create a parser that would be 
flexible enough to allow modifications for pars- 
ing down to more semantic levels of detail. It is 
to this project hat our future parsing work will 
be devoted. 
References 
1. BERGER, A. L., PIETRA, S. A. D. AND 
PIETRA, V. J. D. A maximum entropy ap- 
proach to natural anguage processing. Com- 
putational Linguistics 22 1 (1996), 39-71. 
2. CARABALLO, S. AND CHARNIAK, E. New 
figures of merit for best-first probabilistic 
chart parsing. Computational Linguistics 24 
(1998), 275-298. 
3. CHARNIAK, E. Tree-bank grammars. In 
Proceedings of the Thirteenth National 
Conference on Artificial Intelligence. AAAI 
Press/MIT Press, Menlo Park, 1996, 1031- 
1036. 
4. CHARNIAK, E. Expected-frequency interpo- 
lation. Department of Computer Science, 
Brown University, Technical Report CS96-37, 
1996. 
5. CHARNIAK, E. Statistical parsing with a 
context-free grammar and word statistics. 
In Proceedings of the Fourteenth National 
Conference on Artificial Intelligence. AAAI 
Press/MIT Press, Menlo Park, CA, 1997, 
598-603. 
6. CHARNIAK, E. Statistical techniques for 
natural anguage parsing. AI Magazine 18 4 
(1997), 33-43. 
7. CHARNIAK, E., GOLDWATER, S. AND JOHN- 
SON, M. Edge-based best-first chart pars- 
ing. In Proceedings of the Sixth Workshop 
on Very Large Corpora. 1998, 127-133. 
8. CHARNIAK, E., HENDRICKSON, C., JACOB- 
SON, N. AND PERKOWITZ, M. Equations 
for part-of-speech tagging. In Proceedings of 
the Eleventh National Conference on Arti- 
ficial Intelligence. AAAI Press/MIT Press, 
Menlo Park, 1993, 784-789. 
9. COLLINS, M. Head-Driven Statistical Mod- 
els for Natural Language Parsing. University 
of Pennsylvania, Ph.D. Disseration, 1999. 
10. COLLINS, M. J. Three generative l xicalised 
models for statistical parsing. In Proceedings 
of the 35th Annual Meeting of the ACL. 1997, 
16-23. 
11. DARROCH, J. N. AND RATCLIFF, D. Gener- 
alized iterative scaling for log-linear models. 
Annals of Mathematical Statistics 33 (1972), 
1470-1480. 
12. EISNER~ J. M. An empirical comparison of 
probability models for dependency grammar. 
Institute for Research in Cognitive Science, 
University of Pennsylvania, Technical Report 
IRCS-96-11, 1996. 
13. HENDERSON, J. C. AND BRILL, E. Exploit- 
ing diversity in natural language process- 
ing: combining parsers. In 1999 Joint Sigdat 
Conference on Empirical Methods in Natu- 
red Language Processing and Very Large Cor- 
pora. ACL, New Brunswick N J, 1999, 187- 
194. 
14. JOHNSON, M. PCFG models of linguistic 
tree representations. Computational Linguis- 
tics 24 4 (1998), 613-632. 
15. MAGERMAN, D.M.  Statistical decision-tree 
models for parsing. In Proceedings of the 33rd 
Annual Meeting of the Association for Com- 
putational Linguistics. 1995, 276-283. 
16. MARCUS, M. P., SANTORINI, B. AND 
MARCINKIEWICZ, M. A. Building a large 
annotated corpus of English: the Penn tree- 
bank. Computational Linguistics 19 (1993), 
313-330. 
17. RATNAPARKHI, A. Learning to parse natu- 
ral language with maximum entropy models. 
Machine Learning 341/2/3 (1999), 151-176. 
139 
Assigning Function Tags to Parsed Text* 
Don B laheta  and  Eugene Charn iak  
{dpb, ec}@cs, brown, edu 
Department of Computer Science 
Box 1910 / 115 Waterman St . - -4th  floor 
Brown University 
Providence, RI 02912 
Abst rac t  
It is generally recognized that the common on- 
terminal abels for syntactic constituents (NP, 
VP, etc.) do not exhaust he syntactic and se- 
mantic information one would like about parts 
of a syntactic tree. For example, the Penn Tree- 
bank gives each constituent zero or more 'func- 
tion tags' indicating semantic roles and other 
related information ot easily encapsulated in 
the simple constituent labels. We present a sta- 
tistical algorithm for assigning these function 
tags that, on text already parsed to a simple- 
label level, achieves an F-measure of 87%, which 
rises to 99% when considering 'no tag' as a valid 
choice. 
1 In t roduct ion  
Parsing sentences using statistical information 
gathered from a treebank was first examined a 
decade ago in (Chitrao and Grishman, 1990) 
and is by now a fairly well-studied problem 
((Charniak, 1997), (Collins, 1997), (Ratna- 
parkhi, 1997)). But to date, the end product of 
the parsing process has for the most part been 
a bracketing with simple constituent labels like 
NP, VP, or SBAR. The Penn treebank contains a 
great deal of additional syntactic and seman- 
tic information from which to gather statistics; 
reproducing more of this information automat- 
ically is a goal which has so far been mostly 
ignored. This paper details a process by which 
some of this information--the function tags-- 
may be recovered automatically. 
In the Penn treebank, there are 20 tags (fig- 
ure 1) that can be appended to constituent la- 
bels in order to indicate additional information 
about the syntactic or semantic role of the con- 
* This research was funded in part by NSF grants LIS- 
SBR-9720368 and IGERT-9870676. 
stituent. We have divided them into four cate- 
gories (given in figure 2) based on those in the 
bracketing uidelines (Bies et al, 1995). A con- 
stituent can be tagged with multiple tags, but 
never with two tags from the same category. 1 
In actuality, the case where a constituent has 
tags from all four categories never happens, but 
constituents with three tags do occur (rarely). 
At a high level, we can simply say that hav- 
ing the function tag information for a given text 
is useful just because any further information 
would help. But specifically, there are distinct 
advantages for each of the various categories. 
Grammatical tags are useful for any application 
trying to follow the thread of the text--they find 
the 'who does what' of each clause, which can 
be useful to gain information about the situa- 
tion or to learn more about the behaviour of 
the words in the sentence. The form/function 
tags help to find those constituents behaving in 
ways not conforming to their labelled type, as 
well as further clarifying the behaviour of ad- 
verbial phrases. Information retrieval applica- 
tions specialising in describing events, as with a 
number of the MUC applications, could greatly 
benefit from some of these in determining the 
where-when-why of things. Noting a topicalised 
constituent could also prove useful to these ap- 
plications, and it might also help in discourse 
analysis, or pronoun resolution. Finally, the 
'miscellaneous' tags are convenient at various 
times; particularly the CLI~ 'closely related' tag, 
which among other things marks phrasal verbs 
and prepositional ditransitives. 
To our knowledge, there has been no attempt 
so far to recover the function tags in pars- 
ing treebank text. In fact, we know of only 
1There is a single exception i the corpus: one con- 
stituent is tagged with -LOC-I~R. This appears to be an 
error. 
234 
ADV Non-specific adverbial 
BNF Benefemtive 
CLF It-cleft 
CLR 'Closely related' 
DIR Direction 
DTV Dative 
EXT Extent 
HLN Headline 
LGS Logical subject 
L0C Location 
MNI~ Manner 
N0M Nominal 
PRD Predicate 
PRP Purpose 
PUT Locative complement of 'put' 
SBJ Subject 
TMP Temporal 
TPC Topic 
TTL Title 
V0C Vocative 
Grammatical 
DTV 0.48% 
LGS 3.0% 
PRD 18.% 
PUT 0.26% 
SBJ 78.% 
v0c 0.025% 
Figure 1: Penn treebank function tags 
53.% Form/Function 37.% Topicalisation 2.2% 
0.25% NOM 6.8% 2.5% TPC 100% 2.2% 
1.5% ADV 11.% 4.2% 
9.3% BN'F 0.072% 0.026% 
0.13% DIR 8.3% 3.0% 
41.% EXT 3.2% 1.2% 
0.013% LOC 25.% 9.2% 
MNR 6.2% 2.3% 
PI~ 5.2% 1.9% 
33.% 12.% 
Miscellaneous 9.5% 
CLR 94.% 8.8% 
CLF 0 .34% 0.03% 
HLN 2.6% 0.25% 
TTL 3.1% 0.29% 
Figure 2: Categories of function tags and their relative frequencies 
one project that used them at all: (Collins, 
1997) defines certain constituents as comple- 
ments based on a combination of label and func- 
tion tag information. This boolean condition is 
then used to train an improved parser. 
2 Features  
We have found it useful to define our statisti- 
cal model in terms of features. A 'feature', in 
this context, is a boolean-valued function, gen- 
erally over parse tree nodes and either node la- 
bels or lexical items. Features can be fairly sim- 
ple and easily read off the tree (e.g. 'this node's 
label is X', 'this node's parent's label is Y'), or 
slightly more complex ('this node's head's part- 
of-speech is Z'). This is concordant with the us- 
age in the maximum entropy literature (Berger 
et al, 1996). 
When using a number of known features to 
guess an unknown one, the usual procedure is
to calculate the value of each feature, and then 
essentially look up the empirically most proba- 
ble value for the feature to be guessed based on 
those known values. Due to sparse data, some 
of the features later in the list may need to be 
ignored; thus the probability of an unknown fea- 
ture value would be estimated as 
P(flYl, ? ?, Y,) 
P ( f l f l ,  f2 , . . . , f j ) ,  j < n ,  (1) 
where/3 refers to an empirically observed prob- 
ability. Of course, if features 1 through i only 
co-occur a few times in the training, this value 
may not be reliable, so the empirical probability 
is usually smoothed: 
P(flf l ,  Ii) 
AiP(flfl, fa , . . . ,  fi) 
+ (2) 
The values for )~i can then be determined ac- 
cording to the number of occurrences of features 
1 through i together in the training. 
One way to think about equation 1 (and 
specifically, the notion that j will depend on 
the values of f l . . .  fn) is as follows: We begin 
with the prior probability of f .  If we have data 
indicating P(flfl), we multiply in that likeli- 
hood, while dividing out the original prior. If 
we have data for /3(f l f l ,  f2), we multiply that 
in while dividing out the P(flfl) term. This is 
repeated for each piece of feature data we have; 
at each point, we are adjusting the probability 
235 
P(flfl,f2,... ,fn) p(/) P(SlA) P(SlSl, S:) 
P(f) P(f lf l)  
P(flfl,..., Yi-1, A) 
-,_-o " p- ff, 
P(flft, $2,..., f~) 
P(flA, A,... ,f?-x) 
j<n  
(3) 
we already have estimated. If knowledge about 
feature fi makes S more likely than with just 
f l . . .  fi-1, the term where fi is added will be 
greater than one and the running probability 
will be adjusted upward. This gives us the new 
probability shown in equation 3, which is ex- 
actly equivalent to equation 1 since everything 
except the last numerator cancels out of the 
equation. The value of j is chosen such that 
features f l . . - f j  are sufficiently represented in 
the training data; sometimes all n features are 
used, but often that would cause sparse data 
problems. Smoothing isperformed on this equa- 
tion exactly as before: each term is interpolated 
between the empirical value and the prior esti- 
mated probability, according to a value of Ai 
that estimates confidence. But aside from per- 
haps providing a new way to think about the 
problem, equation 3 is not particularly useful 
as it is--it is exactly the same as what we had 
before. Its real usefulness comes, as shown in 
(Charniak, 1999), when we move from the no- 
tion of a feature chain to a feature tree. 
These feature chains don't capture verything 
we'd like them to. If there are two independent 
features that are each relatively sparse but occa- 
sionally carry a lot of information, then putting 
one before the other in a chain will effectively 
block the second from having any effect, since 
its information is (uselessly) conditioned on the 
first one, whose sparseness will completely di- 
lute any gain. What we'd really like is to be able 
to have a feature tree, whereby we can condition 
those two sparse features independently on one 
common predecessor feature. As we said be- 
fore, equation 3 represents, for each feature fi, 
the probability of f based on fi and all its pre- 
decessors, divided by the probability of f based 
only on the predecessors. In the chain case, this 
means that the denominator is conditioned on 
every feature from 1 to i - 1, but if we use a 
feature tree, it is conditioned only on those fea- 
tures along the path to the root of the tree. 
A notable issue with feature trees as opposed 
to feature chains is that the terms do not all 
cancel out. Every leaf on the tree will be repre- 
target ~ 
feature 
Figure 3: A small example feature tree 
sented in the numerator, and every fork in the 
tree (from which multiple nodes depend) will 
be represented at least once in the denomina- 
tor. For example: in figure 3 we have a small 
feature tree that has one target feature and four 
conditioning features. Features b and d are in- 
dependent ofeach other, but each depends on a; 
c depends directly only on b. The unsmoothed 
version of the corresponding equation would be 
P(fla, b, c, d) ,~ 
p ,~ P(fla) ~)(f\]a, b) P(f\[a, b, c) P(fla, d) 
which, after cancelling of terms and smoothing, 
results in 
P(fla, b, c, d) P(fla, b, c)P(fla, d) 
P(fla) (4) 
Note that strictly speaking the result is not a 
probability distribution. It could be made into 
one with an appropriate normalisation--the 
so-called partition function in the maximum- 
entropy literature. However, if the indepen- 
dence assumptions made in the derivation of 
equation 4 are good ones, the partition func- 
tion will be close to 1.0. We assume this to be 
the case for our feature trees. 
Now we return the discussion to function tag- 
ging. There are a number of features that seem 
236 
function 
tag label 
succeeding preceding 
, , . / -d~e l  laf)el 
pare_p~ 
gra-'n~arent's parent's 
label head's POS 
grandparent's 
h ~ P O S  
headS~ parent's 
P ~ e a d  
head 
alt-head's 
POs alt-~ead 
Figure 4: The feature tree used to guess function tags 
to condition strongly for one function tag or an- 
other; we have assembled them into the feature 
tree shown in figure 4. 2 This figure should be 
relatively self-explanatory, except for the notion 
of an 'alternate head'; currently, an alternate 
head is only defined for prepositional phrases, 
and is the head of the object of the preposi- 
tional phrase. This data is very important in 
distinguishing, for example, 'by John' (where 
John might be a logical subject) from 'by next 
year' (a temporal modifier) and 'by selling it' 
(an adverbial indicating manner). 
3 Exper iment  
In the training phase of our experiment, we 
gathered statistics on the occurrence of func- 
tion tags in sections 2-21 of the Penn treebank. 
Specifically, for every constituent in the tree- 
bank, we recorded the presence of its function 
tags (or lack thereof) along with its condition- 
ing information. From this we calculated the 
empirical probabilities of each function tag ref- 
erenced in section 2 of this paper. Values of )~ 
were determined using EM on the development 
corpus (treebank section 24). 
To test, then, we simply took the output of 
our parser on the test corpus (treebank section 
23), and applied a postprocessing step to add 
function tags. For each constituent in the tree, 
we calculated the likelihood of each function tag 
according to the feature tree in figure 4, and 
for each category (see figure 2) we assigned the 
most likely function tag (which might be the 
null tag). 
2The reader will note that  the ' features'  l isted in the 
tree are in fact not  boolean-valued; each node in the 
given tree can be assumed to s tand for a chain of boolean 
features, one per potent ia l  value at  that  node, exact ly 
one of which will be true. 
4 Eva luat ion  
To evaluate our results, we first need to deter- 
mine what is 'correct'. The definition we chose 
is to call a constituent correct if there exists in 
the correct parse a constituent with the same 
start and end points, label, and function tag 
(or lack thereof). Since we treated each of the 
four function tag categories as a separate fea- 
ture for the purpose of tagging, evaluation was 
also done on a per-category basis. 
The denominator of the accuracy measure 
should be the maximum possible number we 
could get correct. In this case, that means 
excluding those constituents hat were already 
wrong in the parser output; the parser we used 
attains 89% labelled precision-recall, so roughly 
11% of the constituents are excluded from the 
function tag accuracy evaluation. (For refer- 
ence, we have also included the performance of
our function tagger directly on treebank parses; 
the slight gain that resulted is discussed below.) 
Another consideration is whether to count 
non-tagged constituents in our evaluation. On 
the one hand, we could count as correct any 
constituent with the correct tag as well as any 
correctly non-tagged constituent, and use as 
our denominator the number of all correctly- 
labelled constituents. (We will henceforth refer 
to this as the 'with-null' measure.) On the other 
hand, we could just count constituents with the 
correct tag, and use as our denominators the 
total number of tagged, correctly-labelled con- 
stituents. We believe the latter number ('no- 
null') to be a better performance metric, as it 
is not overwhelmed by the large number of un- 
tagged constituents. Both are reported below. 
237 
Category 
Grammatical 
Form/Function 
Topicalisation 
Miscellaneous 
Overall 
Table 1: Baseline performance 
Baseline 1 
(never tag) Tag Precision 
86.935% SBJ 10.534% 
91.786% THP 3.105% 
99.406% TPC 0.594% 
98.436% CLR 1.317% 
94.141% - -  3.887% 
Baseline 2 (always choose most likely tag) 
Recall F-measure 
80.626% 18.633% 
37.795% 5.738% 
100.00% 1.181% 
84.211% 2.594% 
66.345% 7.344% 
Table 2: Performance within each category 
With-null - - -No-nu l l - -  
Category Accuracy Precision Recall F-measure 
Grammatical 98.909% 95.472% 95.837% 95.654% 
Form/Function 97.104% 80.415% 77.595% 78.980% 
Topicalisation 99.915% 92.195% 93.564% 92.875% 
Miscellaneous 98.645% 55.644% 65.789% 60.293% 
5 Resu l ts  
5.1 Base l ines  
There are, it seems, two reasonable baselines 
for this and future work. First of all, most con- 
stituents in the corpus have no tags at all, so 
obviously one baseline is to simply guess no tag 
for any constituent. Even for the most com- 
mon type of function tag (grammatical), this 
method performs with 87% accuracy. Thus the 
with-null accuracy of a function tagger needs to 
be very high to be significant here. 
The second baseline might be useful in ex- 
amining the no-null accuracy values (particu- 
larly the recall): always guess the most common 
tag in a category. This means that every con- 
stituent gets labelled with '-SBJ-THP-TPC-CLR' 
(meaning that it is a topicalised temporal sub- 
ject that is 'closely related' to its verb). This 
combination of tags is in fact entirely illegal 
by the treebank guidelines, but performs ad- 
equately for a baseline. The precision is, of 
course, abysmal, for the same reasons the first 
baseline did so well; but the recall is (as one 
might expect) substantial. The performances 
of the two baseline measures are given in Table 
1. 
5.2 Per fo rmance  in ind iv idua l  
categor ies 
In table 2, we give the results for each category. 
The first column is the with-null accuracy, and 
the precision and recall values given are the no- 
null accuracy, as noted in section 4. 
Grammatical tagging performs the best of the 
four categories. Even using the more difficult 
no-null accuracy measure, it has a 96% accu- 
racy. This seems to reflect the fact that gram- 
matical relations can often be guessed based on 
constituent labels, parts of speech, and high- 
frequency lexical items, largely avoiding sparse- 
data problems. Topicalisation can similarly be 
guessed largely on high-frequency information, 
and performed almost as well (93%). 
On the other hand, we have the 
form/function tags and the 'miscellaneous' 
tags. These are characterised by much more 
semantic information, and the relationships 
between lexical items are very important, 
making sparse data a real problem. All the 
same, it should be noted that the performance 
is still far better than the baselines. 
5.3 Per fo rmance  w i th  o ther  feature  
trees 
The feature tree given in figure 4 is by no means 
the only feature tree we could have used. In- 
238 
Table 3: Overall performance on different inputs 
With-null - -No-nu l l -  
Category Accuracy Precision Recall F-measure 
Parsed 98.643% 87.173% 87.381% 87.277% 
Treebank 98.805% 88.450% 88.493% 88.472% 
deed, we tried a number of different rees on the 
development corpus; this tree gave among the 
best overall results, with no category perform- 
ing too badly. However, there is no reason to 
use only one feature tree for all four categories; 
the best results can be got by using a separate 
tree for each one. One can thus achieve slight 
(one to three point) gains in each category. 
5.4 Overal l  per fo rmance  
The overall performance, given in table 3, ap- 
pears promising. With a tagging accuracy of 
about 87%, various information retrieval and 
knowledge base applications can reasonably ex- 
pect to extract useful information. 
The performance given in the first row is (like 
all previously given performance values) the 
function-tagger's performance on the correctly- 
labelled constituents output by our parser. For 
comparison, we also give its performance when 
run directly on the original treebank parse; since 
the parser's accuracy is about 89%, working di- 
rectly with the treebank means our statistics 
are over roughly 12% more constituents. This 
second version does slightly better. 
The main reason that tagging does worse on 
the parsed version is that although the con- 
stituent itself may be correctly bracketed and la- 
belled, its exterior conditioning information can 
still be incorrect. An example of this that ac- 
tually occurred in the development corpus (sec- 
tion 24 of the treebank) is the 'that' clause in 
the phrase 'can swallow the premise that the re- 
wards for such ineptitude are six-figure salaries', 
correctly diagrammed in figure 5. The function 
tagger gave this SBAR an ADV tag, indicating an 
unspecified adverbial function. This seems ex- 
tremely odd, given that its conditioning infor- 
mation (nodes circled in the figure) clearly show 
that it is part of an NP, and hence probably mod- 
ifies the preceding NN. Indeed, the statistics give 
the probability of an ADV tag in this condition- 
ing environment as vanishingly small. 
vP 
the ( premise ) ~  
Figure 5: SBAR and conditioning info 
the premise ~ ... 
Figure 6: SBAR and conditioning info, as parsed 
However, this was not the conditioning infor- 
mation that the tagger received. The parser 
had instead decided on the (incorrect) parse in 
figure 6. As such, the tagger's decision makes 
much more sense, since an SBAR under two VPs 
whose heads are VB and MD is rather likely to be 
an ADV. (For instance, the 'although' clause of 
the sentence 'he can help, although he doesn't 
want to.' has exactly the conditioning environ- 
ment given in figure 6, except that its prede- 
cessor is a comma; and this SBAR would be cor- 
rectly tagged ADV.) The SBAR itself is correctly 
bracketed and labelled, so it still gets counted 
in the statistics. Happily, this sort of case seems 
to be relatively rare. 
239 
Another thing that lowers the overall perfor- 
mance somewhat is the existence of error and in- 
consistency in the treebank tagging. Some tags 
seem to have been relatively easy for the human 
treebank taggers, and have few errors. Other 
tags have explicit caveats that, however well- 
justified, proved difficult to remember for the 
taggers--for instance, there are 37 instances of 
a PP being tagged with LGS (logical subject) in 
spite of the guidelines specifically saying, '\[LGS\] 
attaches to the NP object of by and not to the 
PP node itself.' (Bies et al, 1995) Each mistag- 
ging in the test corpus can cause up to two spu- 
rious errors, one in precision and one in recall. 
Still another source of difficulty comes when the 
guidelines are vague or silent on a specific issue. 
To return to logical subjects, it is clear that 'the 
loss' is a logical subject in 'The company was 
hurt by the loss', but what about in 'The com- 
pany was unperturbed by the loss' ? In addition, 
a number of the function tags are authorised for 
'metaphorical use', but what exactly constitutes 
such a use is somewhat inconsistently marked. 
It is as yet unclear just to what degree these 
tagging errors in the corpus are affecting our 
results. 
6 Conc lus ion  
This work presents a method for assigning func- 
tion tags to text that has been parsed to the 
simple label level. Because of the lack of prior 
research on this task, we are unable to com- 
pare our results to those of other researchers; 
but the results do seem promising. However, a 
great deal of future work immediately suggests 
itself: 
? Although we tested twenty or so feature 
trees besides the one given in figure 4, the 
space of possible trees is still rather un- 
explored. A more systematic investiga- 
tion into the advantages ofdifferent feature 
trees would be useful. 
? We could add to the feature tree the val- 
ues of other categories of function tag, or 
the function tags of various tree-relatives 
(parent, sibling). 
? One of the weaknesses of the lexical fea- 
tures is sparse data; whereas the part of 
speech is too coarse to distinguish 'by John' 
(LGS) from 'by Monday' (TMP), the lexi- 
cal information may be too sparse. This 
could be assisted by clustering the lexical 
items into useful categories (names, dates, 
etc.), and adding those categories as an ad- 
ditional feature type. 
? There is no reason to think that this work 
could not be integrated irectly into the 
parsing process, particularly if one's parser 
is already geared partially or entirely to- 
wards feature-based statistics; the func- 
tion tag information could prove quite use- 
ful within the parse itself, to rank several 
parses to find the most plausible. 
Re ferences  
Adam L. Berger, Stephen A. Della Pietra, 
and Vincent J. Della Pietra. 1996. A 
maximum entropy approach to natural lan- 
guage processing. Computational Linguistics, 
22(1):39-71. 
Ann Bies, Mark Ferguson, Karen Katz, and 
Robert MacIntyre, 1995. Bracketing Guide- 
lines for Treebank H Style Penn Treebank 
Project, January. 
Eugene Charniak. 1997. Statistical pars- 
ing with a context-free grammar and word 
statistics. In Proceedings of the Fourteenth 
National Conference on Artificial Intelli- 
gence, pages 598-603, Menlo Park. AAAI 
Press/MIT Press. 
Eugene Charniak. 1999. A maximum-entropy- 
inspired parser. Technical Report CS-99-12, 
Brown University, August. 
Mahesh V. Chitrao and Ralph Grishman. 1990. 
Statistical parsing of messages. In DARPA 
Speech and Language Workshop, pages 263- 
266. 
Michael Collins. 1997. Three generative, lexi- 
calised models for statistical parsing. In Pro- 
ceedings of the 35th Annual Meeting of the 
Association for Computational Linguistics, 
pages 16-23. 
Adwait Ratnaparkhi. 1997. A linear observed 
time statistical parser based on maximum en- 
tropy models. In Proceedings of the Second 
Annual Conference on Empirical Methods in 
Natural Language Processing, pages 1-10. 
240 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 329?336
Manchester, August 2008
Evaluating Unsupervised Part-of-Speech Tagging for Grammar Induction
William P. Headden III, David McClosky, Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{headdenw,dmcc,ec}@cs.brown.edu
Abstract
This paper explores the relationship be-
tween various measures of unsupervised
part-of-speech tag induction and the per-
formance of both supervised and unsuper-
vised parsing models trained on induced
tags. We find that no standard tagging
metrics correlate well with unsupervised
parsing performance, and several metrics
grounded in information theory have no
strong relationship with even supervised
parsing performance.
1 Introduction
There has been a great deal of recent interest in
the unsupervised discovery of syntactic structure
from text, both parts-of-speech (Johnson, 2007;
Goldwater and Griffiths, 2007; Biemann, 2006;
Dasgupta and Ng, 2007) and deeper grammatical
structure like constituency and dependency trees
(Klein and Manning, 2004; Smith, 2006; Bod,
2006; Seginer, 2007; Van Zaanen, 2001). While
some grammar induction systems operate on raw
text, many of the most successful ones presume
prior part-of-speech tagging. Meanwhile, most re-
cent work in part-of-speech induction focuses on
increasing the degree to which their tags match
hand-annotated ones such as those in the Penn
Treebank.
In this work our goal is to evaluate how im-
provements in part-of-speech tag induction affects
grammar induction. Using several different unsu-
pervised taggers, we induce tags and train three
grammar induction systems on the results.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
We then explore the relationship between the
performance on common unsupervised tagging
metrics and the performance of resulting grammar
induction systems. Disconcertingly we find that
they bear little to no relationship.
This paper is organized as follows. In Section 2
we discuss unsupervised part-of-speech induction
systems and common methods of evaluation. In
Section 3, we describe grammar induction in gen-
eral and discuss the systems with which we evalu-
ate taggings. We present our experiments in Sec-
tion 4, and finally conclude in Section 5.
2 Part-of-speech Tag Induction
Part-of-speech tag induction can be thought of as a
clustering problem where, given a corpus of words,
we aim to group word tokens into syntactic classes.
Two tasks are commonly labeled unsupervised
part-of-speech induction. In the first, tag induction
systems are allowed the use of a tagging dictionary,
which specifies for each word a set of possible
parts-of-speech (Merialdo, 1994; Smith and Eis-
ner, 2005; Goldwater and Griffiths, 2007). In the
second, only the word tokens and sentence bound-
aries are given. In this work we focus on this latter
task to explore grammar induction in a maximally
unsupervised context.
Tag induction systems typically focus on two
sorts of features: distributional and morphologi-
cal. Distributional refers to what sorts of words
appear in close proximity to the word in question,
while morphological refers to modeling the inter-
nal structure of a word. All the systems below
make use of distributional information, whereas
only two use morphological features.
We primarily focus on the metrics used to evalu-
ate induced taggings. The catalogue of recent part-
of-speech systems is large, and we can only test
329
the tagging metrics using a few systems. Recent
work that we do not explore explicitly includes
(Biemann, 2006; Dasgupta and Ng, 2007; Freitag,
2004; Smith and Eisner, 2005). We have selected
a few systems, described below, that represent a
broad range of features and techniques to make our
evaluation of the metrics as broad as possible.
2.1 Clustering using SVD and K-means
Schu?tze (1995) presents a series of part-of-speech
inducers based on distributional clustering. We
implement the baseline system, which Klein and
Manning (2002) use for their grammar induction
experiments with induced part-of-speech tags. For
each word type w in the vocabulary V , the system
forms a feature row vector consisting of the num-
ber of times each of the F most frequent words oc-
cur to the left of w and to the right of w. It normal-
izes these row vectors and assembles them into a
|V |?2F matrix. It then performs a Singular Value
Decomposition on the matrix and rank reduces it to
decrease its dimensionality to d principle compo-
nents (d < 2F ). This results in a representation
of each word as a point in a d dimensional space.
We follow Klein and Manning (2002) in using K-
means to cluster the d dimensional word vectors
into parts-of-speech. We use the F = 500 most
frequent words as left and right context features,
and reduce to a dimensionality of d = 50. We re-
fer to this system as SVD in our experiments.
The other systems described in Schu?tze (1995)
make use of more complicated feature models. We
chose the baseline system primarily to match pre-
vious evaluations of grammar induction using in-
duced tags (Klein and Manning, 2002).
2.2 Hidden Markov Models
One simple family of models for part-of-speech in-
duction are the Hidden Markov Models (HMMs),
in which there is a sequence of hidden state vari-
ables t
1
...t
n
(for us, the part-of-speech tags). Each
state t
i
is conditioned on the previous n ? 1 states
t
i?1
...t
i?n+1
, and every t
i
emits an observed word
w
i
conditioned on t
i
. There is a single start state
that emits nothing, as well as a single stop state,
which emits an end-of-sentence marker with prob-
ability 1 and does not transition further. In our ex-
periments we use the bitag HMM, in which each
state t
i
depends only on state t
i?1
.
The classic method of training HMMs for part-
of-speech induction is the Baum-Welch (Baum,
1972) variant of the Expectation-Maximization
(EM) algorithm, which searches for a local max-
imum in the likelihood of the observed words.
Other methods approach the problem from
a Bayesian perspective. These methods place
Dirichlet priors over the parameters of each transi-
tion and emission multinomial. For an HMM with
a set of states T and a set of output symbols V :
?t ? T ?
t
? Dir(?
1
, ...?
|T |
) (1)
?t ? T ?
t
? Dir(?
1
, ...?
|V |
) (2)
t
i
|t
i?1
, ?
t
i
?1
? Multi(?
t
i?1
) (3)
w
i
|t
i
, ?
t
i
? Multi(?
t
i
) (4)
One advantage of the Bayesian approach is that
the prior allows us to bias learning toward sparser
structures, by setting the Dirichlet hyperparame-
ters ?, ? to a value less than one (Johnson, 2007;
Goldwater and Griffiths, 2007). This increases the
probability of multinomial distributions which put
most of their mass on a few events, instead of dis-
tributing them broadly across many events. There
is evidence that this leads to better performance
on some part-of-speech induction metrics (John-
son, 2007; Goldwater and Griffiths, 2007).
There are both MCMC and variational ap-
proaches to estimating HMMs with sparse Dirich-
let priors; we chose the latter (Variational Bayes
or VB) due to its simple implementation as a
minor modification to Baum-Welch. Johnson
(2007) evaluates both estimation techniques on the
Bayesian bitag model; Goldwater and Griffiths
(2007) emphasize the advantage in the MCMC ap-
proach of integrating out the HMM parameters in a
tritag model, yielding a tagging supported by many
different parameter settings.
Following the setup in Johnson (2007), we ini-
tialize the transition and emission distributions to
be uniform with a small amount of noise, and run
EM and VB for 1000 iterations. We label these
systems as HMM-EM and HMM-VB respectively
in our experiments. In our VB experiments we set
?
i
= ?
j
= 0.1,?i ? {1, ..., |T |} , j ? {1, ..., |V |},
which yielded the best performance on most re-
ported metrics in Johnson (2007). We use max-
imum marginal decoding, which Johnson (2007)
reports performs better than Viterbi decoding.
2.3 Systems with Morphology
Clark (2003) presents several part-of-speech in-
duction systems which incorporate morphological
as well as distributional information. We use the
330
implementation found on his website.1
2.3.1 Ney-Essen with Morphology
The simplest model is based on work by (Ney et
al., 1994). It uses a bitag HMM, with the restric-
tion that each word type in the vocabulary can only
be generated by a single part-of-speech. Thus the
tag induction task here reduces to finding a multi-
way partition of the vocabulary. The learning al-
gorithm greedily reassigns each word type to the
part-of-speech that results in the greatest increase
in likelihood.
In order to incorporate morphology, Clark
(2003) associates with each part-of-speech a HMM
with letter emissions. The vocabulary is gener-
ated by generating a series of word types from
the letter HMM of each part-of-speech. These can
model very basic concatenative morphology. The
parameters of the HMMs are estimated by running
a single iteration of Forward-Backward after each
round of reassigning words to tags. In our exper-
iments we evaluate both the model without mor-
phology (NE in our experiments), and the morpho-
logical model, trying both 5 and 10 states in the let-
ter HMM (NEMorph5, NEMorph10 respectively).
2.3.2 Two-Level HMM
The final part-of-speech inducer we try from
Clark (2003) is a two-level HMM. This is similar
to the previous model, except it lifts the restriction
that a word appear under only one part-of-speech.
Alternatively, one could think of this model as a
standard HMM, whose emission distributions in-
corporate a mixture of a letter HMM and a stan-
dard multinomial. Training uses a simple variation
of Forward-Backward. In the experiments in this
paper, we initialize the mixture parameters to .5,
and try 5 states in the letter HMM. We refer to this
model as 2HMM.
2.4 Tag Evaluation
Objective evaluation in any clustering task is al-
ways difficult, since there are many ways to de-
fine good clusters. Typically it involves a mix-
ture of subjective evaluation and a comparison of
the clusters to those found by human annotators.
In the realm of part-of-speech induction, there are
several common ways of doing the latter. These
split into two groups: accuracy and information-
theoretic criteria.
1http://www.cs.rhul.ac.uk/home/alexc/pos.tar.gz
Accuracy, given some mapping between the set
of induced classes and the gold standard labels, is
the number of words in the corpus that have been
marked with the correct gold label divided by the
total number of word tokens. The main challenge
facing these metrics is deciding how to to map each
induced part-of-speech class to a gold tag. One
option is what Johnson (2007) calls ?many-to-one?
(M-to-1) accuracy, in which each induced tag is
labeled with its most frequent gold tag. Although
this results in a situation where multiple induced
tags may share a single gold tag, it does not punish
a system for providing tags of a finer granularity
than the gold standard.
In contrast, ?one-to-one? (1-to-1) accuracy re-
stricts each gold tag to having a single induced
tag. The mapping typically is made to try to give
the most favorable mapping in terms of accuracy,
typically using a greedy assignment (Haghighi and
Klein, 2006). In cases where the number of gold
tags is different than the number of induced tags,
some must necessarily remain unassigned (John-
son, 2007).
In addition to accuracy, there are several infor-
mation theoretic criteria presented in the literature.
These escape the problem of trying to find an ap-
propriate mapping between induced and gold tags,
at the expense of perhaps being less intuitive.
Let T
I
be the tag assignments to the words
in the corpus created by an unsupervised tag-
ger, and let T
G
be the gold standard tag as-
signments. Clark (2003) uses Shannon?s condi-
tional entropy of the gold tagging given the in-
duced tagging H(T
G
|T
I
). Lower entropy indi-
cates less uncertainty in the gold tagging if we al-
ready know the induced tagging. Freitag (2004)
uses the similar ?cluster-conditional tag perplex-
ity? which is merely exp(H(T
G
|T
I
))
2
. Since
cluster-conditional tag perplexity is a monotonic
function of H(T
G
|T
I
), we only report the latter.
Goldwater and Griffiths (2007) propose using
the Variation of Information of Meila? (2003):
V I(T
G
;T
I
) = H(T
G
|T
I
) + H(T
I
|T
G
)
VI represents the change in information when go-
ing from one clustering to another. It holds the
nice properties of being nonnegative, symmetric,
as well as fulfilling the triangle inequality.
2Freitag (2004) measures entropy in nats, while we use
bits. The difference is a constant factor.
331
3 Grammar Induction
In addition to parts-of-speech, we also want to dis-
cover deeper syntactic relationships. Grammar in-
duction is the problem of determining these re-
lationships in an unsupervised fashion. This can
be thought of more concretely as an unsupervised
parsing task. As there are many languages and do-
mains with few treebank resources, systems that
can learn syntactic structure from unlabeled data
would be valuable. Most work on this problem has
focused on either dependency induction, which we
discuss in Section 3.2, or on constituent induction,
which we examine in the next section.
The Grammar Induction systems we use to eval-
uate the above taggers are the Constituent-Context
Model (CCM), the Dependency Model with Va-
lence (DMV), and a model which combines the
two (CCM+DMV) outlined in (Klein and Man-
ning, 2002; Klein and Manning, 2004).
3.1 Constituent Grammar Induction
Klein and Manning (2002) present a generative
model for inducing constituent boundaries from
part-of-speech tagged text. The model first gener-
ates a bracketing B = {B
ij
}
1?i?j?n
, which spec-
ifies whether each span (i, j) in the sentence is a
constituent or a distituent. Next, given the con-
stituency or distituency of the span B
ij
, the model
generates the part-of-speech yield of the span
t
i
...t
j
, and the one-tag context window of the span
t
i?1
, t
j+1
. P (t
i
...t
j
|B
ij
) and P (t
i?1
, t
j+1
|B
ij
)
are multinomial distributions. The model is trained
using EM.
We evaluate induced constituency trees against
those of the Penn Treebank using the versions of
unlabeled precision, recall, and F-score used by
Klein and Manning (2002). These ignore triv-
ial brackets and multiple constituents spanning the
same bracket. They evaluate their CCM system
on the Penn Treebank WSJ sentences of length 10
or less, using part-of-speech tags induced by the
baseline system of Schu?tze (1995). They report
that switching to induced tags decreases the overall
bracketing F-score from 71.1 to 63.2, although the
recall of VP and S constituents actually improves.
Additionally, they find that NP and PP recall de-
creases substantially with induced tags. They at-
tribute this to the fact that nouns end up in many
induced tags.
There has been quite a bit of other work on
constituency induction. Smith and Eisner (2004)
present an alternative estimation technique for
CCM which uses annealing to try to escape local
maxima. Bod (2006) describes an unsupervised
system within the Data-Oriented-Parsing frame-
work. Several approaches try to learn structure
directly from raw text. Seginer (2007) has an in-
cremental parsing approach using a novel repre-
sentation called common-cover-links, which can
be converted to constituent brackets. Van Zaanen
(2001)?s ABL attempts to align sentences to deter-
mine what sequences of words are substitutable.
The work closest in spirit to this paper is Cramer
(2007), who evaluates several grammar induction
systems on the Eindhoven corpus (Dutch). One
of his experiments compares the grammar induc-
tion performance of these systems starting with
tags induced using the system described by Bie-
mann (2006), to the performance of the systems
on manually-marked tags. However he does not
evaluate to what degree better tagging performance
leads to improvement in these systems.
3.2 Dependency Grammar Induction
A dependency tree is a directed graph whose nodes
are words in the sentence. A directed edge exists
between two words if the target word (argument) is
a dependent of the source word (head). Each word
token may be the argument of only one head, but a
head may have several arguments. One word is the
head of the sentence, and is often thought of as the
argument of a virtual ?Root? node.
Klein and Manning (2004) present their Depen-
dency Model with Valence (DMV) for the un-
supervised induction of dependencies. Like the
constituency model, DMV works from parts-of-
speech. Under this model, for a given head, h,
they first generate the parts-of-speech of the argu-
ments to the right of h, and then those to the left.
Generating the arguments in a particular direction
breaks down into two parts: deciding whether to
stop generating in this direction, and if not, what
part-of-speech to generate as the argument. The ar-
gument decision conditions on h and the direction.
The stopping decision conditions on this and also
on whether h has already generated an argument in
this direction, thereby capturing the limited notion
of valence from which the model takes its name.
It is worth noting that this model can only repre-
sent projective dependency trees, i.e. those without
crossing edges.
Dependencies are typically evaluated using di-
332
Tagging Metrics Grammar Induction Metrics
Tagger No. Tags CCM CCM+DMV DMV
1-to-1 H(T
G
|T
I
) M-to-1 VI UF1 DA UA UF1 DA UA
Gold 1.00 0.00 1.00 0.00 71.50 52.90 67.60 56.50 45.40 63.80
HMM-EM 10 0.39 2.67 0.41 4.39 58.89 40.12 59.26 59.43 36.77 57.37
HMM-EM 20 0.43 2.28 0.48 4.54 57.31 51.16 64.66 61.33 38.65 58.57
HMM-EM 50 0.36 1.83 0.58 4.92 56.56 48.03 63.84 58.02 39.30 58.84
HMM-VB 10 0.40 2.75 0.41 4.42 39.05 27.72 52.84 58.64 23.94 51.64
HMM-VB 20 0.40 2.63 0.43 4.65 37.60 33.77 55.97 40.30 30.36 51.53
HMM-VB 50 0.38 2.70 0.42 5.01 34.68 37.29 57.72 39.82 29.03 50.50
NE 10 0.34 2.74 0.40 4.32 28.80 20.70 50.60 32.70 26.20 48.90
NE 20 0.48 2.02 0.55 3.76 32.50 36.00 59.30 40.60 32.80 54.00
NEMorph10 10 0.44 2.46 0.47 3.74 29.03 25.99 53.80 34.58 26.98 48.72
NEMorph10 20 0.48 1.94 0.56 3.65 31.95 35.85 57.93 38.22 30.45 50.72
NEMorph10 50 0.47 1.24 0.72 3.60 31.07 36.29 57.76 39.28 31.50 52.83
NEMorph5 10 0.45 2.50 0.47 3.76 29.04 22.72 51.58 32.67 23.62 47.89
NEMorph5 20 0.44 2.02 0.56 3.80 31.94 24.17 52.43 32.90 22.41 47.17
NEMorph5 50 0.47 1.27 0.72 3.64 31.39 38.63 59.44 40.23 34.26 54.63
2HMM 10 0.38 2.78 0.41 4.55 31.63 36.35 58.87 44.97 28.43 49.32
2HMM 20 0.41 2.35 0.48 4.71 42.39 43.91 60.74 50.85 29.32 50.69
2HMM 50 0.37 1.92 0.58 5.11 41.18 49.94 64.87 57.84 39.24 59.14
SVD 10 0.31 3.07 0.34 4.99 37.77 27.64 49.56 36.46 20.74 45.52
SVD 20 0.33 2.73 0.40 4.99 37.17 30.14 51.66 37.66 22.24 46.25
SVD 50 0.34 2.37 0.47 5.18 36.87 37.66 56.49 52.83 22.50 46.52
SVD 100 0.34 2.03 0.53 5.37 45.46 41.68 58.83 64.20 20.81 44.36
SVD 200 0.32 1.72 0.59 5.59 61.90 34.79 52.25 59.93 22.66 42.30
Table 1: The performance of the taggers regarding both tag and grammar induction metrics on WSJ
sections 0-10, averaged over 10 runs. Bold indicates the result was within 10 percent of the best-scoring
induced system for a given metric.
rected and undirected accuracy. These are the to-
tal number of proposed edges that appear in the
gold tree divided by the total number of edges (the
number of words in the sentence). Directed accu-
racy gives credit to a proposed edge if it is in the
gold tree and is in the correct direction, while undi-
rected accuracy ignores the direction.
Klein and Manning (2004) also present a model
which combines CCM and DMV into a single
model, which we show as CCM+DMV. In their
experiments, this model performed better on both
the constituency and dependency induction tasks.
As with CCM, Klein and Manning (2004) simi-
larly evaluate the combined CCM+DMV system
using tags induced with the same method. Again
they find that overall bracketing F-score decreases
from 77.6 to 72.9 and directed dependency accu-
racy measures decreases from 47.5 to 42.3 when
switching to induced tags from gold. However for
each metric, the systems still do quite well with
induced tags.
As in the constituency case, Smith (2006)
presents several alternative estimation procedures
for DMV, which try to minimize the local maxi-
mum problems inherent in EM. It is thus possible
these methods might yield better performance for
the models when run off of induced tags.
4 Experiments
We induce tags with each system on the Penn Tree-
bank Wall Street Journal (Marcus et al, 1994), sec-
tions 0-10, which contain 20,260 sentences. We
vary the number of tags (10, 20, 50) and run each
system 10 times for a given setting. The result of
each run is used as the input to the CCM, DMV,
and CCM+DMV systems. While the tags are in-
duced from all sentences in the section, following
the practice in (Klein and Manning, 2002; Klein
and Manning, 2004), we remove punctuation, and
consider only sentences of length not greater than
10 in our grammar induction experiments. Tag-
gings are evaluated after punctuation is removed,
but before filtering for length.
To explore the relationship between tagging
metrics and the resulting performance of grammar
induction systems, we examine each pair of tag-
ging and grammar induction metrics. Consider the
following two examples: DMV directed accuracy
vs. H(T
G
|T
I
) (Figure 1), and CCM f-score vs.
variation of information (Figure 2). These were se-
lected because they have relatively high magnitude
?s. From these plots it is clear that although there
333
may be a slight correspondence, the relationships
are weak at best.
Each tagging and grammar induction metric
gives us a ranking over the set of taggings of the
data generated over the course of our experiments.
These are ordered from best to worst according to
the metric, so for instance H(T
G
|T
I
) would give
highest rank to its lowest value. We can com-
pare the two rankings using Kendall?s ? (see Lap-
ata (2006) for an overview), a nonparametric mea-
sure of correspondence for rankings. ? measures
the difference between the number of concordant
pairs (items the two rankings place in the same or-
der) and discordant pairs (those the rankings place
in opposite order), divided by the total number of
pairs. A value of 1 indicates the rankings have per-
fect correspondence, -1 indicates they are in the
opposite order, and 0 indicates they are indepen-
dent. The ? values are shown in Table 2. The
scatter-plot in Figure 1 shows the ? with the great-
est magnitude. However, we can see that even
these rankings have barely any relationship.
An objection one might raise is that the lack of
correspondence reflects poorly not on these met-
rics, but upon the grammar induction systems we
use to evaluate them. There might be something
about these models in particular which yields these
low correlations. For instance these grammar in-
ducers all estimate their models using EM, which
can get caught easily in a local maximum.
To this possibility, we respond by pointing to
performance on gold tags, which is consistently
high for all grammar induction metrics. There is
clearly some property of the gold tags which is ex-
ploited by the grammar induction systems even in
the absence of better estimation procedures. This
property is not reflected in the tagging metrics.
The scores for each system for tagging and
grammar induction, averaged over the 10 runs, are
shown in Table 1. Additionally, we included runs
of the SVD-tagger for 100 and 200 tags, since run-
ning this system is still practical with these num-
bers of tags. The Ney-Essen with Morphology tag-
gers perform at or near the top on the various tag-
ging metrics, but not well on the grammar induc-
tion tasks on average. HMM-EM seems to perform
on average quite well on all the grammar induction
tasks, while the SVD-based systems yield the top
bracketing F-scores, making use of larger numbers
of tags.
Grammar Induction Metrics
Tagging CCM CCM+DMV DMV
Metrics UF1 DA UA UF1 DA UA
1-to-1 -0.22 -0.04 0.05 -0.13 0.13 0.12
M-to-1 -0.09 0.17 0.24 0.03 0.26 0.25
H(T
G
|T
I
) 0.01 0.21 0.27 0.07 0.29 0.28
VI -0.25 -0.17 -0.06 -0.20 0.07 0.07
Table 2: Kendall?s ? , between tag and grammar
induction criteria.
4.1 Supervised Experiments
One question we might ask is whether these tag-
ging metrics capture information relevant to any
parsing task. We explored this by experimenting
with a supervised parser, training off trees where
the gold parts-of-speech have been removed and
replaced with induced tags. Our expectation was
that the brackets, the head propagation paths, and
the phrasal categories in the training trees would
be sufficient to overcome any loss in information
that the gold tags might provide. Additionally it
was possible the induced tags would ignore rare
parts-of-speech such as FW, and make better use
of the available tags, perhaps using new distribu-
tional clues not in the original tags.
To this end we modified the Charniak Parser
(Charniak, 2000) to train off induced parts-of-
speech. The Charniak parser is a lexicalized PCFG
parser for which the part-of-speech of a head word
is a key aspect of its model. During training, the
head-paths from the gold part-of-speech tags are
retained, but we replace the tags themselves.
We ran experiments using the bitag HMM from
Section 2.2 trained using EM, as well as with the
Schu?tze SVD tagger from Section 2.1. The parser
was trained on sections 2-21 of the Penn Treebank
for training and section 24 was used for evaluation.
As before we calculated ? scores between each
tagging metric and supervised f-score. Unlike the
unsupervised evaluation where we used the metric
UF1, we use the standard EVALB calculation of
unlabeled f-score. The results are shown in Table
3.
The contrast with the unsupervised case is vast,
with very high ?s for both accuracy metrics. Con-
sider f-score vs. many-to-one, plotted in Figure 3.
The correspondence here is very clear: taggings
with high accuracy do actually reflect on better
parser performance. Note, however, that the corre-
spondence between the information theoretic mea-
sures and parsing performance is still rather weak.
334
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5
 
15
20
25
30
35
40
45
50
55
D
M
V
 
D
A
NEMorph
HMM-VB
2HMM
NE
SVD
HMM-EM
Gold
Figure 1: DMV Directed Accuracy vs. H(T
G
|T
I
)
0 1 2 3 4 5 6
 VI
20
30
40
50
60
70
80
C
C
M
 
U
F
1
NEMorph
HMM-VB
2HMM
NE
SVD
HMM-EM
Gold
Figure 2: CCM fscore vs. tagging variation of in-
formation.
0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
 M-to-1
0.78
0.80
0.82
0.84
0.86
0.88
0.90
C
h
a
r
n
i
a
k
 
p
a
r
s
e
r
 
F
1
SVD 10 tags
SVD 50 tags
Gold
HMM-EMbitag 10 tags
HMM-EMbitag 50 tags
Figure 3: Supervised parsing f-score vs. tagging
many-to-one accuracy.
Tagging Metric Supervised F1
1-to-1 0.62
M-to-1 0.83
H(T
G
|T
I
) -0.19
VI 0.25
Table 3: Kendall?s ? , between tag induction cri-
teria and supervised parsing unlabeled bracketing
F-score.
Interestingly, parsing performance and speed
does degrade considerably when training off in-
duced tags. We are not sure what causes this. One
possibility is in the lexicalized stage of the parser,
where the probability of a head word is smoothed
primarily by its part-of-speech tag. This requires
that the tag be a good proxy for the syntactic role
of the head. In any case this warrants further in-
vestigation.
5 Conclusion and Future Work
In this work, we found that none of the most com-
mon part-of-speech tagging metrics bear a strong
relationship to good grammar induction perfor-
mance. Although our experiments only involve
English, the poor correspondence we find between
the various tagging metrics and grammar induc-
tion performance raises concerns about their re-
lationship more broadly. We additionally found
that while tagging accuracy measures do corre-
late with better supervised parsing, common infor-
mation theoretic ones do not strongly predict bet-
ter performance on either task. Furthermore, the
supervised experiments indicate that informative
part-of-speech tags are important for good parsing.
The next step is to explore better tagging met-
rics that correspond more strongly to better gram-
mar induction performance. A good metric should
use all the information we have, including the gold
trees, to evaluate. Finally, we should explore gram-
mar induction schemes that do not rely on prior
parts-of-speech, instead learning them from raw
text at the same time as deeper structure.
Acknowledgments
We thank Dan Klein for his grammar induction
code, as well as Matt Lease and other members of
BLLIP for their feedback. This work was partially
supported by DARPA GALE contract HR0011-06-
2-0001 and NSF award 0631667.
335
References
Baum, L.E. 1972. An inequality and associated maxi-
mization techniques in statistical estimation of prob-
abilistic functions of Markov processes. Inequali-
ties, 3:1?8.
Biemann, Chris. 2006. Unsupervised part-of-speech
tagging employing efficient graph clustering. In
Proceedings of the COLING/ACL 2006 Student Re-
search Workshop, pages 7?12, Sydney, Australia.
Bod, Rens. 2006. An all-subtrees approach to unsuper-
vised parsing. In Proceedings of Coling/ACL 2006,
pages 865?872.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proceedings of the North Ameri-
can Chapter of the ACL 2000, pages 132?139.
Clark, Alexander. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of EACL 2003, pages 59?66,
Budapest, Hungary.
Cramer, Bart. 2007. Limitations of current grammar
induction algorithms. In Proceedings of the ACL
2007 Student Research Workshop, pages 43?48.
Dasgupta, Sajib and Vincent Ng. 2007. Unsupervised
part-of-speech acquisition for resource-scarce lan-
guages. In Proceedings of the EMNLP/CoNLL 2007,
pages 218?227.
Freitag, Dayne. 2004. Toward unsupervised whole-
corpus tagging. In Proceedings of Coling 2004,
pages 357?363, Aug 23?Aug 27.
Goldwater, Sharon and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the ACL 2007, pages
744?751.
Haghighi, Aria and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
HLT/NAACL 2006, pages 320?327, New York, USA.
Johnson, Mark. 2007. Why doesn?t EM find
good HMM POS-taggers? In Proceedings of the
EMNLP/CoNLL 2007, pages 296?305.
Klein, Dan and Christopher Manning. 2002. A gener-
ative constituent-context model for improved gram-
mar induction. In Proceedings of ACL 2002.
Klein, Dan and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL
2004, pages 478?485, Barcelona, Spain, July.
Lapata, Mirella. 2006. Automatic evaluation of infor-
mation ordering: Kendall?s tau. Computational Lin-
guistics, 32(4):1?14.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schasberger.
1994. The Penn Treebank: Annotating Predicate
Argument Structure. In Proceedings of the 1994
ARPA Human Language Technology Workshop.
Meila?, Marina. 2003. Comparing clusterings. Pro-
ceedings of the Conference on Computational Learn-
ing Theory (COLT).
Merialdo, Bernard. 1994. Tagging english text with
a probabilistic model. Computational Linguistics,
20(2):154?172.
Ney, Herman, Ute Essen, and Renhard Knesser. 1994.
On structuring probabilistic dependencies in stochas-
tic language modelling. Computer Speech and Lan-
guage, 8:1?38.
Schu?tze, Hinrich. 1995. Distributional part-of-speech
tagging. In Proceedings of the 7th conference of the
EACL, pages 141?148.
Seginer, Yoav. 2007. Fast unsupervised incremental
parsing. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 384?391, Prague, Czech Republic.
Smith, Noah A. and Jason Eisner. 2004. Anneal-
ing techniques for unsupervised statistical language
learning. In Proceedings of ACL 2004, pages 487?
494, Barcelona, Spain.
Smith, Noah A. and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of ACL 2005, pages 354?362,
Ann Arbor, Michigan.
Smith, Noah A. 2006. Novel Estimation Methods for
Unsupervised Discovery of Latent Structure in Nat-
ural Language Text. Ph.D. thesis, Department of
Computer Science, Johns Hopkins University, Octo-
ber.
Van Zaanen, Menno M. 2001. Bootstrapping Structure
into Language: Alignment-Based Learning. Ph.D.
thesis, University of Leeds, September.
336
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 561?568
Manchester, August 2008
When is Self-Training Effective for Parsing?
David McClosky, Eugene Charniak, and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{dmcc|ec|mj}@cs.brown.edu
Abstract
Self-training has been shown capable of
improving on state-of-the-art parser per-
formance (McClosky et al, 2006) despite
the conventional wisdom on the matter and
several studies to the contrary (Charniak,
1997; Steedman et al, 2003). However, it
has remained unclear when and why self-
training is helpful. In this paper, we test
four hypotheses (namely, presence of a
phase transition, impact of search errors,
value of non-generative reranker features,
and effects of unknown words). From
these experiments, we gain a better un-
derstanding of why self-training works for
parsing. Since improvements from self-
training are correlated with unknown bi-
grams and biheads but not unknown words,
the benefit of self-training appears most in-
fluenced by seeing known words in new
combinations.
1 Introduction
Supervised statistical parsers attempt to capture
patterns of syntactic structure from a labeled set of
examples for the purpose of annotating new sen-
tences with their structure (Bod, 2003; Charniak
and Johnson, 2005; Collins and Koo, 2005; Petrov
et al, 2006; Titov and Henderson, 2007). These
annotations can be used by various higher-level ap-
plications such as semantic role labeling (Pradhan
et al, 2007) and machine translation (Yamada and
Knight, 2001).
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
However, labeled training data is expensive to
annotate. Given the large amount of unlabeled text
available for many domains and languages, tech-
niques which allow us to use both labeled and
unlabeled text to train our models are desirable.
These methods are called semi-supervised. Self-
training is a specific type of semi-supervised learn-
ing. In self-training, first we train a model on the
labeled data and use that model to label the unla-
beled data. From the combination of our original
labeled data and the newly labeled data, we train a
second model ? our self-trained model. The pro-
cess can be iterated, where the self-trained model
is used to label new data in the next iteration. One
can think of self-training as a simple case of co-
training (Blum and Mitchell, 1998) using a single
learner instead of several. Alternatively, one can
think of it as one step of the Viterbi EM algorithm.
Studies prior to McClosky et al (2006) failed to
show a benefit to parsing from self-training (Char-
niak, 1997; Steedman et al, 2003). While the re-
cent success of self-training has demonstrated its
merit, it remains unclear why self-training helps in
some cases but not others. Our goal is to better un-
derstand when and why self-training is beneficial.
In Section 2, we discuss the previous applica-
tions of self-training to parsing. Section 3 de-
scribes our experimental setup. We present and
test four hypotheses of why self-training helps in
Section 4 and conclude with discussion and future
work in Section 5.
2 Previous Work
To our knowledge, the first reported uses of self-
training for parsing are by Charniak (1997). He
used his parser trained on the Wall Street Journal
(WSJ, Mitch Marcus et al (1993)) to parse 30 mil-
lion words of unparsed WSJ text. He then trained
561
a self-trained model from the combination of the
newly parsed text with WSJ training data. How-
ever, the self-trained model did not improve on the
original model.
Self-training and co-training were subsequently
investigated in the 2002 CLSP Summer Work-
shop at Johns Hopkins University (Steedman et
al., 2003). They considered several different pa-
rameter settings, but in all cases, the number of
sentences parsed per iteration of self-training was
relatively small (30 sentences). They performed
many iterations of self-training. The largest seed
size (amount of labeled training data) they used
was 10,000 sentences from WSJ, though many ex-
periments used only 500 or 1,000 sentences. They
found that under these parameters, self-training did
not yield a significant gain.
Reichart and Rappoport (2007) showed that one
can self-train with only a generative parser if the
seed size is small. The conditions are similar to
Steedman et al (2003), but only one iteration of
self-training is performed (i.e. all unlabeled data is
labeled at once).1 In this scenario, unknown words
(words seen in the unlabeled data but not in train-
ing) were a useful predictor of when self-training
improves performance.
McClosky et al (2006) showed that self-training
improves parsing accuracy when the two-stage
Charniak and Johnson (2005) reranking parser is
used. Using both stages (a generative parser and
discriminative reranker) to label the unlabeled data
set is necessary to improve performance. Only re-
training the first stage had a positive effect. How-
ever, after retraining the first stage, both stages pro-
duced better parses. Unlike Steedman et al (2003),
the training seed size is large and only one itera-
tion of self-training is performed. Error analysis
revealed that most improvement comes from sen-
tences with lengths between 20 and 40 words. Sur-
prisingly, improvements were also correlated with
the number of conjunctions but not with the num-
ber of unknown words in the sentence.
To summarize, several factors have been iden-
tified as good predictors of when self-training im-
proves performance, but a full explanation of why
self-training works is lacking. Previous work es-
tablishes that parsing all unlabeled sentences at
once (rather than over many iterations) is impor-
tant for successful self-training. The full effect of
1Performing multiple iterations presumably fails because
the parsing models become increasingly biased. However,
this remains untested in the large seed case.
seed size and the reranker on self-training is not
well understood.
3 Experimental Setup
We use the Charniak and Johnson reranking parser
(outlined below), though we expect many of these
results to generalize to other generative parsers
and discriminative rerankers. Our corpora consist
of WSJ for labeled data and NANC (North Amer-
ican News Text Corpus, Graff (1995)) for unla-
beled data. We use the standard WSJ division for
parsing: sections 2-21 for training (39,382 sen-
tences) and section 24 for development (1,346 sen-
tences). Given self-training?s varied performance
in the past, many of our experiments use the con-
catenation of sections 1, 22, and 24 (5,039 sen-
tences) rather than the standard development set
for more robust testing.
A full description of the reranking parser can be
found in Charniak and Johnson (2005). Briefly
put, the reranking parser consists of two stages:
A generative lexicalized PCFG parser which pro-
poses a list of the n most probable parses (n-best
list) followed by a discriminative reranker which
reorders the n-best list. The reranker uses about
1.3 million features to help score the trees, the
most important of which is the first stage parser?s
probability. In Section 4.3, we mention two classes
of reranker features in more depth. Since some of
experiments rely on details of the first stage parser,
we present a summary of the parsing model.
3.1 The Parsing Model
The parsing model assigns a probability to a parse
? by a top-down process of considering each con-
stituent c in ? and, for each c, first guessing the
preterminal of c, t(c) then the lexical head of c,
h(c), and then the expansion of c into further con-
stituents e(c). Thus the probability of a parse is
given by the equation
p(?) =
?
c??
p(t(c) | l(c),R(c))
?p(h(c) | t(c), l(c),R(c))
?p(e(c) | l(c), t(c), h(c),R(c))
where l(c) is the label of c (e.g., whether it is a
noun phrase (np), verb phrase, etc.) and R(c) is
the relevant history of c ?- information outside c
that the probability model deems important in de-
termining the probability in question.
562
For each expansion e(c) we distinguish one of
the children as the ?middle? child M(c). M(c) is
the constituent from which the head lexical item
h is obtained according to deterministic rules that
pick the head of a constituent from among the
heads of its children. To the left of M is a sequence
of one or more left labels L
i
(c) including the spe-
cial termination symbol ? and similarly for the la-
bels to the right, R
i
(c). Thus an expansion e(c)
looks like:
l ? ?L
m
...L
1
MR
1
...R
n
?. (1)
The expansion is generated by guessing first M ,
then in order L
1
through L
m+1
(= ?), and simi-
larly for R
1
through R
n+1
.
So the parser assigns a probability to the parse
based upon five probability distributions, T (the
part of speech of the head), H (the head), M (the
child constituent which includes the head), L (chil-
dren to the left of M ), and R (children to the right
of M ).
4 Testing the Four Hypotheses
The question of why self-training helps in some
cases (McClosky et al, 2006; Reichart and Rap-
poport, 2007) but not others (Charniak, 1997;
Steedman et al, 2003) has inspired various the-
ories. We investigate four of these to better un-
derstand when and why self-training helps. At
a high level, the hypotheses are (1) self-training
helps after a phase transition, (2) self-training re-
duces search errors, (3) specific classes of reranker
features are needed for self-training, and (4) self-
training improves because we see new combina-
tions of words.
4.1 Phase Transition
The phase transition hypothesis is that once a
parser has achieved a certain threshold of perfor-
mance, it can label data sufficiently accurately.
Once this happens, the labels will be ?good
enough? for self-training.
To test the phase transition hypothesis, we use
the same parser as McClosky et al (2006) but train
on only a fraction of WSJ to see if self-training is
still helpful. This is similar to some of the ex-
periments by Reichart and Rappoport (2007) but
with the use of a reranker and slightly larger seed
sizes. The self-training protocol is the same as
in (Charniak, 1997; McClosky et al, 2006; Re-
ichart and Rappoport, 2007): we parse the entire
unlabeled corpus in one iteration. We start by tak-
ing a random subset of the WSJ training sections
(2-21), accepting each sentence with 10% proba-
bility. With the sampled training section and the
standard development data, we train a parser and a
reranker. In Table 1, we show the performance of
the parser with and without the reranker. For ref-
erence, we show the performance when using the
complete training division as well. Unsurprisingly,
both metrics drop as we decrease the amount of
training data. These scores represent our baselines
for this experiment.
Using this parser model, we parse one million
sentences from NANC, both with and without the
reranker. We combine these one million sentences
with the sampled subsets of WSJ training and train
new parser models from them.2
Finally, we evaluate these self-trained models
(Table 2). The numbers in parentheses indicate the
change from the corresponding non-self-trained
model. As in Reichart and Rappoport (2007), we
see large improvements when self-training on a
small seed size (10%) without using the reranker.
However, using the reranker to parse the self-
training and/or evaluation sentences further im-
proves results. From McClosky et al (2006), we
know that when 100% of the training data is used,
self-training does not improve performance with-
out a reranker.
From this we conclude that there is no such
threshold phase transition in this case. High per-
formance is not a requirement to successfully use
self-training for parsing, since there are lower per-
forming parsers which can self-train and higher
performing parsers which cannot. The higher per-
forming Charniak and Johnson (2005) parser with-
out reranker achieves an f -score of 89.0 on section
24 when trained on all of WSJ. This parser does
not benefit from self-training unless paired with a
reranker. Contrast this with the same parser trained
on only 10% of WSJ, where it gets an f -score of
85.8 (Table 2) or the small seed models of Reichart
and Rappoport (2007). Both of these lower per-
forming parsers can successfully self-train. Ad-
ditionally, we now know that while a reranker is
not required for self-training when the seed size is
small, it still helps performance considerably (f -
score improves from 87.7 to 89.0 in the 10% case).
2We do not weight the original WSJ data, though our ex-
pectation is that performance would improve if WSJ were
given a higher relative weight. This is left as future work.
563
% WSJ # sentences Parser f -score Reranking parser f -score
10 3,995 85.8 87.0
100 39,832 89.9 91.5
Table 1: Parser and reranking parser performance on sentences ? 100 words in sections 1, 22, and 24
when trained on different amounts of training data. % WSJ is the percentage of WSJ training data trained
on (sampled randomly). Note that the full amount of development data is still used as held out data.
Parsed NANC with reranker? Parser f -score Reranking parser f -score
No 87.7 (+1.9) 88.7 (+1.7)
Yes 88.4 (+2.6) 89.0 (+2.0)
Table 2: Effect of self-training using only 10% of WSJ as labeled data. The parser model is trained from
one million parsed sentences from NANC + WSJ training. The first column indicates whether the million
NANC sentences were parsed by the parser or reranking parser. The second and third columns differ in
whether the reranker is used to parse the test sentences (WSJ sections 1, 22, and 24, sentences 100 words
and shorter). Numbers in parentheses are the improvements over the corresponding non-self-trained
parser.
4.2 Search Errors
Another possible explanation of self-training?s im-
provements is that seeing newly labeled data re-
sults in fewer search errors (Daniel Marcu, per-
sonal communication). A search error would in-
dicate that the parsing model could have produced
better (more probable) parses if not for heuristics
in the search procedure. The additional parse trees
may help produce sharper distributions and reduce
data sparsity, making the search process easier. To
test this, first we present some statistics on the n-
best lists (n = 50) from the baseline WSJ trained
parser and self-trained model3 from McClosky et
al. (2006). We use each model to parse sentences
from held-out data (sections 1, 22, and 24) and ex-
amine the n-best lists.
We compute statistics of the WSJ and self-
trained n-best lists with the goal of understand-
ing how much they intersect and whether there are
search errors. On average, the n-best lists over-
lap by 66.0%. Put another way, this means that
about a third of the parses from each model are
unique, so the parsers do find a fair number of dif-
ferent parses in their search. The next question
is where the differences in the n-best lists lie ?
if all the differences were near the bottom, this
would be less meaningful. Let W and S repre-
sent the n-best lists from the baseline WSJ and self-
trained parsers, respectively. The top
m
(?) func-
tion returns the highest scoring parse in the n-best
list ? according to the reranker and parser model
3http://bllip.cs.brown.edu/selftraining/
m.
4 Almost 40% of the time, the top parse in
the self-trained model is not in the WSJ model?s
n-best list, (top
s
(S) /? W ) though the two mod-
els agree on the top parse roughly 42.4% of the
time (top
s
(S) = top
w
(W )). Search errors can
be formulated as top
s
(S) /? W ? top
s
(S) =
top
w
(W ? S). This captures sentences where the
parse that the reranker chose in the self-trained
model is not present in the WSJ model?s n-best list,
but if the parse were added to the WSJ model?s list,
the parse?s probability in the WSJ model and other
reranker features would have caused it to be cho-
sen. These search errors occur in only 2.5% of
the n-best lists. At first glance, one might think
that this could be enough to account for the differ-
ences, since the self-trained model is only several
tenths better in f -score. However, we know from
McClosky et al (2006) that on average, parses do
not change between the WSJ and self-trained mod-
els and when they do, they only improve slightly
more than half the time. For this reason, we run a
second test more focused on performance.
For our second test we help the WSJ trained
model find the parses that the self-trained model
found. For each sentence, we start with the n-best
list (n = 500 here) from the WSJ trained parser,
W . We then consider parses in the self-trained
parser?s n-best list, S, that are not present in W
(S ? W ). For each of these parses, we deter-
mine its probability under the WSJ trained parsing
4Recall that the parser?s probability is a reranker feature
so the parsing model influences the ranking.
564
Model f -score
WSJ 91.5
WSJ & search help 91.7
Self-trained 92.0
Table 3: Test of whether ?search help? from the
self-trained model impacts the WSJ trained model.
WSJ + search help is made by adding self-trained
parses not proposed by the WSJ trained parser but
to which the parser assigns a positive probability.
The WSJ reranker is used in all cases to select the
best parse for sections 1, 22, and 24.
model. If the probability is non-zero, we add the
parse to the n-best list W , otherwise we ignore the
parse. In other words, we find parses that the WSJ
trained model could have produced but didn?t due
to search heuristics. In Table 3, we show the per-
formance of the WSJ trained model, the model with
?search help? as described above, and the self-
trained model on WSJ sections 1, 22, and 24. The
WSJ reranker is used to pick the best parse from
each n-best list. WSJ with search help performs
slightly better than WSJ alone but does not reach
the level of the self-trained model. From these ex-
periments, we conclude that reduced search errors
can only explain a small amount of self-training?s
improvements.
4.3 Non-generative reranker features
We examine the role of specific reranker features
by training rerankers using only subsets of the fea-
tures. Our goal is to determine whether some
classes of reranker features benefit self-training
more than others. We hypothesize that features
which are not easily captured by the generative
first-stage parser are the most beneficial for self-
training. If we treat the parser and reranking parser
as different (but clearly dependent) views, this is a
bit like co-training. If the reranker uses features
which are captured by the first-stage, the views
may be too similar for there to be an improvement.
We consider two classes of features (GEN and
EDGE) and their complements (NON-GEN and
NON-EDGE).5 GEN consists of features that
are roughly captured by the first-stage generative
parser: rule rewrites, head-child dependencies, etc.
EDGE features describe items across constituent
boundaries. This includes the words and parts of
5A small number of features overlap hence these sizes do
not add up.
Feature set # features f -score
GEN 448,349 90.4
NON-GEN 885,492 91.1
EDGE 601,578 91.0
NON-EDGE 732,263 91.1
ALL 1,333,519 91.3
Table 4: Sizes and performance of reranker feature
subsets. Reranking parser f -scores are on all sen-
tences in section 24.
speech of the tokens on the edges between con-
stituents and the labels of these constituents. This
represents a specific class of features not captured
by the first-stage. These subsets and their sizes are
shown in Table 4. For comparison, we also include
the results of experiments using the full feature set,
as in McClosky et al (2006), labeled ALL. The
GEN features are roughly one third the size of the
full feature set.
We evaluate the effect of these new reranker
models on self-training (Table 4). For each fea-
ture set, we do the following: We parse one million
NANC sentences with the reranking parser. Com-
bining the parses with WSJ training data, we train
a new first-stage model. Using this new first-stage
model and the reranker subset, we evaluate on sec-
tion 24 of WSJ. GEN?s performance is weaker
while the other three subsets achieve almost the
same score as the full feature set. This confirms
our hypothesis that when the reranker helps in self-
training it is due to features which are not captured
by the generative first-stage model.
4.4 Unknown Words
Given the large size of the parsed self-training cor-
pus, it contains an immense number of parsing
events which never occur in the training corpus.
The most obvious of these events is words ? the
vocabulary grows from 39,548 to 265,926 words
as we transition from the WSJ trained model to
the self-trained model. Slightly less obvious is bi-
grams. There are roughly 330,000 bigrams in WSJ
training data and approximately 4.8 million new
bigrams in the self-training corpus.
One hypothesis (Mitch Marcus, personal com-
munication) is that the parser is able to learn a lot
of new bilexical head-to-head dependencies (bi-
heads) from self-training. The reasoning is as fol-
lows: Assume the self-training corpus is parsed in
a mostly correct manner. If there are not too many
565
new pairs of words in a sentence, there is a de-
cent chance that we can tag these words correctly
and bracket them in a reasonable fashion from con-
text. Thus, using these parses as part of the train-
ing data improves parsing because should we see
these pairs of words together in the future, we will
be more likely to connect them together properly.
We test this hypothesis in two ways. First, we
perform an extension of the factor analysis simi-
lar to that in McClosky et al (2006). This is done
via a generalized linear regression model intended
to determine which features of parse trees can pre-
dict when the self-training model will perform bet-
ter. We consider many of the same features (e.g.
bucketed sentence length, number of conjunctions,
and number of unknown words) but also consider
two new features: unknown bigrams and unknown
biheads. Unknown items (words, bigrams, bi-
heads) are calculated by counting the number of
items which have never been seen in WSJ train-
ing but have been seen in the parsed NANC data.
Given these features, we take the f -scores for each
sentence when parsed by the WSJ and self-trained
models and look at the differences. Our goal is to
find out which features, if any, can predict these f -
score differences. Specifically, we ask the question
of whether seeing more unknown items indicates
whether we are more likely to see improvements
when self-training.
The effect of unknown items on self-training?s
relative performance is summarized in Figure 1.
For each item, we show the total number of incor-
rect parse nodes in sentences that contain the item.
We also show the change in the number of correct
parse nodes in these sentences between the WSJ
and self-trained models. A positive change means
that performance improved under self-training. In
other words, looking at Figure 1a, the greatest per-
formance improvement occurs, perhaps surpris-
ingly, when we have seen no unknown words.
As we see more unknown words, the improve-
ment from self-training decreases. This is a pretty
clear indication that unknown words are not a good
predictor of when self-training improves perfor-
mance.
A possible objection that one might raise is that
using unknown biheads as a regression feature will
bias our results if they are counted from gold trees
instead of parsed trees. Seeing a bihead in train-
ing will cause the otherwise sparse biheads dis-
tribution to be extremely peaked around that bi-
f -score Model
89.8 ? WSJ (baseline)
89.8 ? WSJ+NANC M
89.9 ? WSJ+NANC T
89.9 ? WSJ+NANC L
90.0 ? WSJ+NANC R
90.0 WSJ+NANC MT
90.1 WSJ+NANC H
90.2 WSJ+NANC LR
90.3 WSJ+NANC LRT
90.4 WSJ+NANC LMRT
90.4 WSJ+NANC LMR
90.5 WSJ+NANC LRH
90.7 ? WSJ+NANC LMRH
90.8 ? WSJ+NANC (fully self-trained)
Table 5: Performance of the first-stage parser
on various combinations of distributions WSJ and
WSJ+NANC (self-trained) models on sections 1,
22, and 24. Distributions are L (left expansion), R
(right expansion), H (head word), M (head phrasal
category), and T (head POS tag). ? and ? indicate
the model is not significantly different from base-
line and self-trained model, respectively.
head. If we see the same pair of words in testing,
we are likely to connect them in the same fash-
ion. Thus, if we count unknown biheads from gold
trees, this feature may explain away other improve-
ments: When gold trees contain a bihead found in
our self-training data, we will almost always see an
improvement. However, given the similar trends in
Figures 1b and 1c, we propose that unknown bi-
grams can be thought of as a rough approximation
of unknown biheads.
The regression analysis reveals that unknown bi-
grams and unknown biheads are good predictors of
f -score improvements. The significant predictors
from McClosky et al (2006) such as the number
of conjunctions or sentence length continue to be
helpful whereas unknown words are a weak pre-
dictor at best. These results are apparent in Figure
1: as stated before, seeing more unknown words
does not correlate with improvements. However,
seeing more unknown bigrams and biheads does
predict these changes fairly well. When we have
seen zero or one new bigrams or biheads, self-
training negatively impacts performance. After
seeing two or more, we see positive effects until
about six to ten after which improvements taper
off.
566
To see the effect of biheads on performance
more directly, we also experiment by interpolat-
ing between the WSJ and self-trained models on a
distribution level. To do this, we take specific dis-
tributions (see Section 3.1) from the self-trained
model and have them override the corresponding
distributions in a compatible WSJ trained model.
From this we hope to show which distributions
self-training boosts. According to the biheads hy-
pothesis, the H distribution (which captures infor-
mation about head-to-head dependencies) should
account for most of the improvement.
The results of moving these distributions is
shown in Table 5. For each new model, we show
whether the model?s performance is not signifi-
cantly different than the baseline model (indicated
by ?) or not significantly different than the self-
trained model (?). H (biheads) is the strongest sin-
gle feature and the only one to be significantly bet-
ter than the baseline. Nevertheless, it is only 0.3%
higher, accounting for 30% of the full self-training
improvement. In general, the performance im-
provements from distributions are additive (+/?
0.1%). Self-training improves all distributions, so
biheads are not the full picture. Nevertheless, they
remain the strongest single feature.
5 Discussion
The experiments in this paper have clarified many
details about the nature of self-training for parsing.
We have ruled out the phase transition hypothe-
sis entirely. Reduced search errors are responsible
for some, but not all, of the improvements in self-
training. We have confirmed that non-generative
reranker features are more beneficial than genera-
tive reranker features since they make the rerank-
ing parser more different from the base parser. Fi-
nally, we have found that while unknown bigrams
and biheads are a significant source of improve-
ment, they are not the sole source of it. Since
unknown words do not correlate well with self-
training improvements, there must be something
about the unknown bigrams and biheads which are
aid the parser. Our belief is that new combinations
of words we have already seen guides the parser in
the right direction. Additionally, these new combi-
nations result in more peaked distributions which
decreases the number of search errors.
However, while these experiments and others
get us closer to understanding self-training, we still
lack a complete explanation. Naturally, the hy-
0 1 2 3 4 5 6 7 10 11 12
To
ta
l n
um
be
r o
f
in
co
rre
ct
 n
od
es
0
60
00
0 1 2 3 4 5 6 7 10 11 12
Number of unknown words in tree
R
ed
uc
tio
n 
in
in
co
rre
ct
 n
od
es
0
30
0
(a) Effect of unknown words on performance
0 2 4 6 8 10 12 14 16 18 20
To
ta
l n
um
be
r o
f
in
co
rre
ct
 n
od
es
0
10
00
0 2 4 6 8 10 12 14 16 18 20
Number of unknown bigrams in tree
R
ed
uc
tio
n 
in
in
co
rre
ct
 n
od
es
?
10
0
10
0
(b) Effect of unknown bigrams on performance
0 2 4 6 8 10 12 14 16 18 20 25
To
ta
l n
um
be
r o
f
in
co
rre
ct
 n
od
es
0
10
00
0 2 4 6 8 10 12 14 16 18 20 25
Number of unknown biheads in tree
R
ed
uc
tio
n 
in
in
co
rre
ct
 n
od
es
?
50
10
0
(c) Effect of unknown biheads on performance
Figure 1: Change in the number of incorrect parse
tree nodes between WSJ and self-trained models
as a function of number of unknown items. See-
ing any number of unknown words results in fewer
errors on average whereas seeing zero or one un-
known bigrams or biheads is likely to hurt perfor-
mance.
567
potheses tested are by no means exhaustive. Addi-
tionally, we have only considered generative con-
stituency parsers here and a good direction for fu-
ture research would be to see if self-training gener-
alizes to a broader class of parsers. We suspect that
using a generative parser/discriminative reranker
paradigm should allow self-training to extend to
other parsing formalisms.
Recall that in Reichart and Rappoport (2007)
where only a small amount of labeled data was
used, the number of unknown words in a sen-
tence was a strong predictor of self-training ben-
efits. When a large amount of labeled data is avail-
able, unknown words are no longer correlated with
these gains, but unknown bigrams and biheads are.
When using a small amount of training data, un-
known words are useful since we have not seen
very many words yet. As the amount of train-
ing data increases, we see fewer new words but
the number of new bigrams and biheads remains
high. We postulate that this difference may help
explain the shift from unknown words to unknown
bigrams and biheads. We hope to further inves-
tigate the role of these unknown items by seeing
how our analyses change under different amounts
of labeled data relative to unknown item rates.
Acknowledgments
This work was supported by DARPA GALE contract
HR0011-06-2-0001. We would like to thank Matt Lease, the
rest of the BLLIP team, and our anonymous reviewers for
their comments. Any opinions, findings, and conclusions or
recommendations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of DARPA.
References
Blum, Avrim and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of the 11th Annual Conference on Compu-
tational Learning Theory (COLT-98).
Bod, Rens. 2003. An efficient implementation of a
new DOP model. In 10th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, Budapest, Hungary.
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. of the 2005 Meeting of the Assoc.
for Computational Linguistics (ACL), pages 173?
180.
Charniak, Eugene. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proc.
AAAI, pages 598?603.
Collins, Michael and Terry Koo. 2005. Discriminative
Reranking for Natural Language Parsing. Computa-
tional Linguistics, 31(1):25?69.
Graff, David. 1995. North American News Text Cor-
pus. Linguistic Data Consortium. LDC95T21.
McClosky, David, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152?159.
Mitch Marcus et al 1993. Building a large annotated
corpus of English: The Penn Treebank. Comp. Lin-
guistics, 19(2):313?330.
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 433?440, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Pradhan, Sameer, Wayne Ward, and James Martin.
2007. Towards robust semantic role labeling. In Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference, pages 556?563, Rochester, New York,
April. Association for Computational Linguistics.
Reichart, Roi and Ari Rappoport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 616?623.
Steedman, Mark, Steven Baker, Jeremiah Crim,
Stephen Clark, Julia Hockenmaier, Rebecca Hwa,
Miles Osborne, Paul Ruhlen, and Anoop Sarkar.
2003. CLSP WS-02 Final Report: Semi-Supervised
Training for Statistical Parsing. Technical report,
Johns Hopkins University.
Titov, Ivan and James Henderson. 2007. Constituent
parsing with incremental sigmoid belief networks.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 632?
639, Prague, Czech Republic, June. Association for
Computational Linguistics.
Yamada, Kenji and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of the 39th Annual Meeting of the Association for
Computational Linguistics, pages 523?529.
568
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 148?156,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
EM Works for Pronoun Anaphora Resolution
Eugene Charniak and Micha Elsner
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{ec,melsner}@cs.brown.edu
Abstract
We present an algorithm for pronoun-
anaphora (in English) that uses Expecta-
tion Maximization (EM) to learn virtually
all of its parameters in an unsupervised
fashion. While EM frequently fails to find
good models for the tasks to which it is
set, in this case it works quite well. We
have compared it to several systems avail-
able on the web (all we have found so far).
Our program significantly outperforms all
of them. The algorithm is fast and robust,
and has been made publically available for
downloading.
1 Introduction
We present a new system for resolving (per-
sonal) pronoun anaphora1. We believe it is of
interest for two reasons. First, virtually all of
its parameters are learned via the expectation-
maximization algorithm (EM). While EM has
worked quite well for a few tasks, notably ma-
chine translations (starting with the IBM models
1-5 (Brown et al, 1993), it has not had success in
most others, such as part-of-speech tagging (Meri-
aldo, 1991), named-entity recognition (Collins
and Singer, 1999) and context-free-grammar in-
duction (numerous attempts, too many to men-
tion). Thus understanding the abilities and limi-
tations of EM is very much a topic of interest. We
present this work as a positive data-point in this
ongoing discussion.
Secondly, and perhaps more importantly, is the
system?s performance. Remarkably, there are very
few systems for actually doing pronoun anaphora
available on the web. By emailing the corpora-
list the other members of the list pointed us to
1The system, the Ge corpus, and the
model described here can be downloaded from
http://bllip.cs.brown.edu/download/emPronoun.tar.gz.
four. We present a head to head evaluation and find
that our performance is significantly better than
the competition.
2 Previous Work
The literature on pronominal anaphora is quite
large, and we cannot hope to do justice to it here.
Rather we limit ourselves to particular papers and
systems that have had the greatest impact on, and
similarity to, ours.
Probably the closest approach to our own is
Cherry and Bergsma (2005), which also presents
an EM approach to pronoun resolution, and ob-
tains quite successful results. Our work improves
upon theirs in several dimensions. Firstly, they
do not distinguish antecedents of non-reflexive
pronouns based on syntax (for instance, subjects
and objects). Both previous work (cf. Tetreault
(2001) discussed below) and our present results
find these distinctions extremely helpful. Sec-
ondly, their system relies on a separate prepro-
cessing stage to classify non-anaphoric pronouns,
and mark the gender of certain NPs (Mr., Mrs.
and some first names). This allows the incorpo-
ration of external data and learning systems, but
conversely, it requires these decisions to be made
sequentially. Our system classifies non-anaphoric
pronouns jointly, and learns gender without an
external database. Next, they only handle third-
person pronouns, while we handle first and sec-
ond as well. Finally, as a demonstration of EM?s
capabilities, its evidence is equivocal. Their EM
requires careful initialization ? sufficiently care-
ful that the EM version only performs 0.4% better
than the initialized program alone. (We can say
nothing about relative performance of their system
vs. ours since we have been able to access neither
their data nor code.)
A quite different unsupervised approach is
Kehler et al (2004a), which uses self-training of a
discriminative system, initialized with some con-
148
servative number and gender heuristics. The sys-
tem uses the conventional ranking approach, ap-
plying a maximum-entropy classifier to pairs of
pronoun and potential antecedent and selecting the
best antecedent. In each iteration of self-training,
the system labels the training corpus and its de-
cisions are treated as input for the next training
phase. The system improves substantially over a
Hobbs baseline. In comparison to ours, their fea-
ture set is quite similar, while their learning ap-
proach is rather different. In addition, their system
does not classify non-anaphoric pronouns,
A third paper that has significantly influenced
our work is that of (Haghighi and Klein, 2007).
This is the first paper to treat all noun phrase (NP)
anaphora using a generative model. The success
they achieve directly inspired our work. There are,
however, many differences between their approach
and ours. The most obvious is our use of EM
rather than theirs of Gibbs sampling. However, the
most important difference is the choice of training
data. In our case it is a very large corpus of parsed,
but otherwise unannotated text. Their system is
trained on the ACE corpus, and requires explicit
annotation of all ?markables? ? things that are or
have antecedents. For pronouns, only anaphoric
pronouns are so marked. Thus the system does
not learn to recognize non-anaphoric pronouns ?
a significant problem. More generally it follows
from this that the system only works (or at least
works with the accuracy they achieve) when the
input data is so marked. These markings not only
render the non-anaphoric pronoun situation moot,
but also significantly restrict the choice of possible
antecedent. Only perhaps one in four or five NPs
are markable (Poesio and Vieira, 1998).
There are also several papers which treat
coference as an unsupervised clustering problem
(Cardie and Wagstaff, 1999; Angheluta et al,
2004). In this literature there is no generative
model at all, and thus this work is only loosely
connected to the above models.
Another key paper is (Ge et al, 1998). The data
annotated for the Ge research is used here for test-
ing and development data. Also, there are many
overlaps between their formulation of the problem
and ours. For one thing, their model is genera-
tive, although they do not note this fact, and (with
the partial exception we are about to mention) they
obtain their probabilities from hand annotated data
rather than using EM. Lastly, they learn their gen-
der information (the probability of that a pronoun
will have a particular gender given its antecedent)
using a truncated EM procedure. Once they have
derived all of the other parameters from the train-
ing data, they go through a larger corpus of unla-
beled data collecting estimated counts of how of-
ten each word generates a pronoun of a particular
gender. They then normalize these probabilities
and the result is used in the final program. This is,
in fact, a single iteration of EM.
Tetreault (2001) is one of the few papers that
use the (Ge et al, 1998) corpus used here. They
achieve a very high 80% correct, but this is
given hand-annotated number, gender and syntac-
tic binding features to filter candidate antecedents
and also ignores non-anaphoric pronouns.
We defer discussion of the systems against
which we were able to compare to Section 7 on
evaluation.
3 Pronouns
We briefly review English pronouns and their
properties. First we only concern ourselves with
?personal? pronouns: ?I?, ?you?, ?he?, ?she?, ?it?,
and their variants. We ignore, e.g., relative pro-
nouns (?who?, ?which?, etc.), deictic pronouns
(?this?, ?that?) and others.
Personal pronouns come in four basic types:
subject ?I?, ?she?, etc. Used in subject position.
object ?me?, ?her? etc. Used in non-subject po-
sition.
possessive ?my? ?her?, and
reflexive ?myself?, ?herself? etc. Required by
English grammar in certain constructions ?
e.g., ?I kicked myself.?
The system described here handles all of these
cases.
Note that the type of a pronoun is not connected
with its antecedent, but rather is completely deter-
mined by the role it plays in it?s sentence.
Personal pronouns are either anaphoric or non-
anaphoric. We say that a pronoun is anaphoric
when it is coreferent with another piece of text in
the same discourse. As is standard in the field we
distinguish between a referent and an antecedent.
The referent is the thing in the world that the pro-
noun, or, more generally, noun phrase (NP), de-
notes. Anaphora on the other hand is a relation be-
149
tween pieces of text. It follows from this that non-
anaphoric pronouns come in two basic varieties ?
some have a referent, but because the referent is
not mentioned in the text2 there is no anaphoric
relation to other text. Others have no referent (ex-
pletive or pleonastic pronouns, as in ?It seems that
. . . ?). For the purposes of this article we do not
distinguish the two.
Personal pronouns have three properties other
than their type:
person first (?I?,?we?), second (?you?) or third
(?she?,?they?) person,
number singular (?I?,?he?) or plural (?we?,
?they?), and
gender masculine (?he?), feminine (?she?) or
neuter (?they?).
These are critical because it is these properties
that our generative model generates.
4 The Generative Model
Our generative model ignores the generation of
most of the discourse, only generating a pronoun?s
person, number,and gender features along with the
governor of the pronoun and the syntactic relation
between the pronoun and the governor. (Infor-
mally, a word?s governor is the head of the phrase
above it. So the governor of both ?I? and ?her? in
?I saw her? is ?saw?.
We first decide if the pronoun is anaphoric
based upon a distribution p(anaphoric). (Actu-
ally this is a bit more complex, see the discus-
sion in Section 5.3.) If the pronoun is anaphoric
we then select a possible antecedent. Any NP
in the current or two previous sentences is con-
sidered. We select the antecedent based upon a
distribution p(anaphora|context). The nature of
the ?context? is discussed below. Then given
the antecedent we generative the pronoun?s person
according to p(person|antecedent), the pronoun?s
gender according to p(gender|antecedent), num-
ber, p(number|antecedent) and governor/relation-
to-governor from p(governor/relation|antecedent).
To generate a non-anaphoric third person singu-
lar ?it? we first guess that the non-anaphoric pro-
nouns is ?it? according to p(?it?|non-anaphoric).
2Actually, as in most previous work, we only consider ref-
erents realized by NPs. For more general approaches see By-
ron (2002).
and then generate the governor/relation according
to p(governor/relation|non-anaphoric-it);
Lastly we generate any other non-anaphoric
pronouns and their governor with a fixed probabil-
ity p(other). (Strictly speaking, this is mathemati-
cally invalid, since we do not bother to normalize
over all the alternatives; a good topic for future re-
search would be exploring what happens when we
make this part of the model truly generative.)
One inelegant part of the model is the need
to scale the p(governor/rel|antecedent) probabili-
ties. We smooth them using Kneser-Ney smooth-
ing, but even then their dynamic range (a factor of
106) greatly exceeds those of the other parameters.
Thus we take their nth root. This n is the last of
the model parameters.
5 Model Parameters
5.1 Intuitions
All of our distributions start with uniform val-
ues. For example, gender distributions start with
the probability of each gender equal to one-third.
From this it follows that on the first EM iteration
all antecedents will have the same probability of
generating a pronoun. At first glance then, the EM
process might seem to be futile. In this section we
hope to give some intuitions as to why this is not
the case.
As is typically done in EM learning, we start
the process with a much simpler generative model,
use a few EM iterations to learn its parameters,
and gradually expose the data to more and more
complex models, and thus larger and larger sets of
parameters.
The first model only learns the probability of
an antecedent generating the pronoun given what
sentence it is in. We train this model through four
iterations before moving on to more complex ones.
As noted above, all antecedents initially have
the same probability, but this is not true after the
first iteration. To see how the probabilities diverge,
and diverge correctly, consider the first sentence of
a news article. Suppose it starts ?President Bush
announced that he ...? In this situation there is
only one possible antecedent, so the expectation
that ?he? is generated by the NP in the same sen-
tence is 1.0. Contrast this with the situation in the
third and subsequent sentences. It is only then that
we have expectation for sentences two back gener-
ating the pronoun. Furthermore, typically by this
point there will be, say, twenty NPs to share the
150
probability mass, so each one will only get an in-
crease of 0.05. Thus on the first iteration only the
first two sentences have the power to move the dis-
tributions, but they do, and they make NPs in the
current sentence very slightly more likely to gener-
ate the pronoun than the sentence one back, which
in turn is more likely than the ones two back.
This slight imbalance is reflected when EM
readjusts the probability distribution at the end of
the first iteration. Thus for the second iteration ev-
eryone contributes to subsequent imbalances, be-
cause it is no longer the case the all antecedents are
equally likely. Now the closer ones have higher
probability so forth and so on.
To take another example, consider how EM
comes to assign gender to various words. By the
time we start training the gender assignment prob-
abilities the model has learned to prefer nearer
antecedents as well as ones with other desirable
properties. Now suppose we consider a sentence,
the first half of which has no pronouns. Consider
the gender of the NPs in this half. Given no fur-
ther information we would expect these genders to
distribute themselves accord to the prior probabil-
ity that any NP will be masculine, feminine, etc.
But suppose that the second half of the sentence
has a feminine pronoun. Now the genders will be
skewed with the probability of one of them being
feminine being much larger. Thus in the same way
these probabilities will be moved from equality,
and should, in general be moved correctly.
5.2 Parameters Learned by EM
Virtually all model parameters are learned by EM.
We use the parsed version of the North-American
News Corpus. This is available from the (Mc-
Closky et al, 2008). It has about 800,000 articles,
and 500,000,000 words.
The least complicated parameter is the proba-
bility of gender given word. Most words that have
a clear gender have this reflected in their probabil-
ities. Some examples are shown in Table 1. We
can see there that EM gets ?Paul?, ?Paula?, and
?Wal-mart? correct. ?Pig? has no obvious gender
in English, and the probabilities reflect this. On
the other hand ?Piggy? gets feminine gender. This
is no doubt because of ?Miss Piggy? the puppet
character. ?Waist? the program gets wrong. Here
the probabilities are close to gender-of-pronoun
priors. This happens for a (comparatively small)
class of pronouns that, in fact, are probably never
Word Male Female Neuter
paul 0.962 0.002 0.035
paula 0.003 0.915 0.082
pig 0.445 0.170 0.385
piggy 0.001 0.853 0.146
wal-mart 0.016 0.007 0.976
waist 0.380 0.155 0.465
Table 1: Words and their probabilities of generat-
ing masculine, feminine and neuter pronouns
antecedent p(singular|antecedent)
Singular 0.939048
Plural 0.0409721
Not NN or NNP 0.746885
Table 2: The probability of an antecedent genera-
tion a singular pronoun as a function of its number
an antecedent, but are nearby random pronouns.
Because of their non-antecedent proclivities, this
sort of mistake has little effect.
Next consider p(number|antecedent), that is the
probability that a given antecedent will generate a
singular or plural pronoun. This is shown in Table
2. Since we are dealing with parsed text, we have
the antecedent?s part-of-speech, so rather than the
antecedent we get the number from the part of
speech: ?NN? and ?NNP? are singular, ?NNS?
and ?NNPS? are plural. Lastly, we have the prob-
ability that an antecedent which is not a noun will
have a singular pronoun associated with it. Note
that the probability that a singular antecedent will
generate a singular pronoun is not one. This is
correct, although the exact number probably is too
low. For example, ?IBM? may be the antecedent
of both ?we? and ?they?, and vice versa.
Next we turn to p(person|antecedent), predict-
ing whether the pronoun is first, second or third
person given its antecedent. We simplify this
by noting that we know the person of the an-
tecedent (everything except ?I? and ?you? and
their variants are third person), so we compute
p(person|person). Actually we condition on one
further piece of information, if either the pronoun
or the antecedent is being quoted. The idea is that
an ?I? in quoted material may be the same person
as ?John Doe? outside of quotes, if Mr. Doe is
speaking. Indeed, EM picks up on this as is il-
lustrated in Tables 3 and 4. The first gives the
situation when neither antecedent nor pronoun is
within a quotation. The high numbers along the
151
Person of Pronoun
Person of Ante First Second Third
First 0.923 0.076 0.001
Second 0.114 0.885 0.001
Third 0.018 0.015 0.967
Table 3: Probability of an antecedent generating a
first,second or third person pronoun as a function
of the antecedents person
Person of Pronoun
Person of Ante First Second Third
First 0.089 0.021 0.889
Second 0.163 0.132 0.705
Third 0.025 0.011 0.964
Table 4: Same, but when the antecedent is in
quoted material but the pronoun is not
diagonal (0.923, 0.885, and 0.967) show the ex-
pected like-goes-to-like preferences. Contrast this
with Table 4 which gives the probabilities when
the antecedent is in quotes but the pronoun is not.
Here we see all antecedents being preferentially
mapped to third person (0.889, 0.705, and 0.964).
We save p(antecedent|context) till last because
it is the most complicated. Given what we know
about the context of the pronoun not all antecedent
positions are equally likely. Some important con-
ditioning events are:
? the exact position of the sentence relative to
the pronoun (0, 1, or 2 sentences back),
? the position of the head of the antecedent
within the sentence (bucketed into 6 bins).
For the current sentence position is measured
backward from the pronoun. For the two pre-
vious sentences it is measure forward from
the start of the sentence.
? syntactic positions ? generally we expect
NPs in subject position to be more likely an-
tecedents than those in object position, and
those more likely than other positions (e.g.,
object of a preposition).
? position of the pronoun ? for example the
subject of the previous sentence is very likely
to be the antecedent if the pronoun is very
early in the sentence, much less likely if it is
at the end.
? type of pronoun ? reflexives can only be
bound within the same sentence, while sub-
Part of Speech pron proper common
0.094 0.057 0.032
Word Position bin 0 bin 2 bin 5
0.111 0.007 0.0004
Syntactic Type subj other object
0.068 0.045 0.037
Table 5: Geometric mean of the probability of
the antecedent when holding everything expect the
stated feature of the antecedent constant
ject and object pronouns may be anywhere.
Possessives may be in previous sentences but
this is not as common.
? type of antecedent. Intuitively other pro-
nouns and proper nouns are more likely to
be antecedents than common nouns and NPs
headed up by things other than nouns.
All told this comes to 2592 parameters (3 sen-
tences, 6 antecedent word positions, 3 syntactic
positions, 4 pronoun positions, 3 pronoun types,
and 4 antecedent types). It is impossible to say
if EM is setting all of these correctly. There are
too many of them and we do not have knowledge
or intuitions about most all of them. However, all
help performance on the development set, and we
can look at a few where we do have strong intu-
itions. Table 5 gives some examples. The first two
rows are devoted to the probabilities of particular
kind of antecedent (pronouns, proper nouns, and
common nouns) generating a pronoun, holding ev-
erything constant except the type of antecedent.
The numbers are the geometric mean of the prob-
abilities in each case. The probabilities are or-
dered according to, at least my, intuition with pro-
noun being the most likely (0.094), followed by
proper nouns (0.057), followed by common nouns
(0.032), a fact also noted by (Haghighi and Klein,
2007). When looking at the probabilities as a func-
tion of word position again the EM derived proba-
bilities accord with intuition, with bin 0 (the clos-
est) more likely than bin 2 more likely than bin
5. The last two lines have the only case where we
have found the EM probability not in accord with
our intuitions. We would have expected objects
of verbs to be more likely to generate a pronoun
than the catch-all ?other? case. This proved not to
be the case. On the other hand, the two are much
closer in probabilities than any of the other, more
intuitive, cases.
152
5.3 Parameters Not Set by EM
There are a few parameters not set by EM.
Several are connected with the well known syn-
tactic constraints on the use of reflexives. A simple
version of this is built in. Reflexives must have an
antecedent in same sentence, and generally cannot
be coreferent-referent with the subject of the sen-
tence.
There are three system parameters that we set
by hand to optimize performance on the develop-
ment set. The first is n. As noted above, the distri-
bution p(governor/relation|antecedent) has a much
greater dynamic range than the other probability
distributions and to prevent it from, in essence,
completely determining the answer, we take its
nth root. Secondly, there is a probability of gen-
erating a non-anaphoric ?it?. Lastly we have a
probability of generating each of the other non-
monotonic pronouns along with (the nth root of)
their governor. These parameters are 6, 0.1, and
0.0004 respectively.
6 Definition of Correctness
We evaluate all programs according to Mitkov?s
?resolution etiquette? scoring metric (also used
in Cherry and Bergsma (2005)), which is defined
as follows: if N is the number of non-anaphoric
pronouns correctly identified, A the number of
anaphoric pronouns correctly linked to their an-
tecedent, and P the total number of pronouns, then
a pronoun-anaphora program?s percentage correct
is N+AP .
Most papers dealing with pronoun coreference
use this simple ratio, or the variant that ignores
non-anaphoric pronouns. It has appeared under
a number of names: success (Yang et al, 2006),
accuracy (Kehler et al, 2004a; Angheluta et al,
2004) and success rate (Tetreault, 2001). The
other occasionally-used metric is the MUC score
restricted to pronouns, but this has well-known
problems (Bagga and Baldwin, 1998).
To make the definition perfectly concrete, how-
ever, we must resolve a few special cases. One
is the case in which a pronoun x correctly says
that it is coreferent with another pronoun y. How-
ever, the program misidentifies the antecedent of
y. In this case (sometimes called error chaining
(Walker, 1989)), both x and y are to be scored as
wrong, as they both end up in the wrong corefer-
ential chain. We believe this is, in fact, the stan-
dard (Mitkov, personal communication), although
there are a few papers (Tetreault, 2001; Yang et
al., 2006) which do the opposite and many which
simply do not discuss this case.
One more issue arises in the case of a system
attempting to perform complete NP anaphora3. In
these cases the coreferential chains they create
may not correspond to any of the original chains.
In these cases, we call a pronoun correctly re-
solved if it is put in a chain including at least one
correct non-pronominal antecedent. This defini-
tion cannot be used in general, as putting all NPs
into the same set would give a perfect score. For-
tunately, the systems we compare against do not
do this ? they seem more likely to over-split than
under-split. Furthermore, if they do take some
inadvertent advantage of this definition, it helps
them and puts our program at a possible disadvan-
tage, so it is a more-than-fair comparison.
7 Evaluation
To develop and test our program we use the dataset
annotated by Niyu Ge (Ge et al, 1998). This
consists of sections 0 and 1 of the Penn tree-
bank. Ge marked every personal pronoun and all
noun phrases that were coreferent with these pro-
nouns. We used section 0 as our development
set, and section 1 for testing. We reparsed the
sentences using the Charniak and Johnson parser
(Charniak and Johnson, 2005) rather than using
the gold-parses that Ge marked up. We hope
thereby to make the results closer to those a user
will experience. (Generally the gold trees perform
about 0.005 higher than the machine parsed ver-
sion.) The test set has 1119 personal pronouns
of which 246 are non-anaphoric. Our selection of
this dataset, rather than the widely used MUC-6
corpus, is motivated by this large number of pro-
nouns.
We compared our results to four currently-
available anaphora programs from the web. These
four were selected by sending a request to a com-
monly used mailing list (the ?corpora-list?) ask-
ing for such programs. We received four leads:
JavaRAP, Open-NLP, BART and GuiTAR. Of
course, these systems represent the best available
work, not the state of the art. We presume that
more recent supervised systems (Kehler et al,
2004b; Yang et al, 2004; Yang et al, 2006) per-
3Of course our system does not attempt NP coreference
resolution, nor does JavaRAP. The other three comparison
systems do.
153
form better. Unfortunately, we were unable to ob-
tain a comparison unsupervised learning system at
all.
Only one of the four is explicitly aimed
at personal-pronoun anaphora ? RAP (Resolu-
tion of Anaphora Procedure) (Lappin and Le-
ass, 1994). It is a non-statistical system orig-
inally implemented in Prolog. The version we
used is JavaRAP, a later reimplementation in Java
(Long Qiu and Chua, 2004). It only handles third
person pronouns.
The other three are more general in that they
handle all NP anaphora. The GuiTAR system
(Poesio and Kabadjov, 2004) is designed to work
in an ?off the shelf? fashion on general text GUI-
TAR resolves pronouns using the algorithm of
(Mitkov et al, 2002), which filters candidate an-
tecedents and then ranks them using morphosyn-
tactic features. Due to a bug in version 3, GUI-
TAR does not currently handle possessive pro-
nouns.GUITAR also has an optional discourse-
new classification step, which cannot be used as
it requires a discontinued Google search API.
OpenNLP (Morton et al, 2005) uses a
maximum-entropy classifier to rank potential an-
tecedents for pronouns. However despite being
the best-performing (on pronouns) of the existing
systems, there is a remarkable lack of published
information on its innards.
BART (Versley et al, 2008) also uses a
maximum-entropy model, based on Soon et al
(2001). The BART system also provides a more
sophisticated feature set than is available in the
basic model, including tree-kernel features and a
variety of web-based knowledge sources. Unfor-
tunately we were not able to get the basic version
working. More precisely we were able to run the
program, but the results we got were substantially
lower than any of the other models and we believe
that the program as shipped is not working prop-
erly.
Some of these systems provide their own pre-
processing tools. However, these were bypassed,
so that all systems ran on the Charniak parse trees
(with gold sentence segmentation). Systems with
named-entity detectors were allowed to run them
as a preprocess. All systems were run using the
models included in their standard distribution; typ-
ically these models are trained on annotated news
articles (like MUC-6), which should be relatively
similar to our WSJ documents.
System Restrictions Performance
GuiTAR No Possessives 0.534
JavaRap Third Person 0.529
Open-NLP None 0.593
Our System None 0.686
Table 6: Performance of Evaluated Systems on
Test Data
The performance of the remaining systems is
given in Table 6. The two programs with restric-
tions were only evaluated on the pronouns the sys-
tem was capable of handling.
These results should be approached with some
caution. In particular it is possible that the re-
sults for the systems other than ours are underes-
timated due to errors in the evaluation. Compli-
cations include the fact all of the four programs
all have different output conventions. The better
to catch such problems the authors independently
wrote two scoring programs.
Nevertheless, given the size of the difference
between the results of our system and the others,
the conclusion that ours has the best performance
is probably solid.
8 Conclusion
We have presented a generative model of pronoun-
anaphora in which virtually all of the parameters
are learned by expectation maximization. We find
it of interest first as an example of one of the few
tasks for which EM has been shown to be effec-
tive, and second as a useful program to be put in
general use. It is, to the best of our knowledge, the
best-performing system available on the web. To
down-load it, go to (to be announced).
The current system has several obvious limita-
tion. It does not handle cataphora (antecedents
occurring after the pronoun), only allows an-
tecedents to be at most two sentences back, does
not recognize that a conjoined NP can be the an-
tecedent of a plural pronoun, and has a very lim-
ited grasp of pronominal syntax. Perhaps the
largest limitation is the programs inability to rec-
ognize the speaker of a quoted segment. The result
is a very large fraction of first person pronouns are
given incorrect antecedents. Fixing these prob-
lems would no doubt push the system?s perfor-
mance up several percent.
However the most critical direction for future
research is to push the approach to handle full NP
154
anaphora. Besides being of the greatest impor-
tance in its own right, it would also allow us to
add one piece of information we currently neglect
in our pronominal system ? the more times a doc-
ument refers to an entity the more likely it is to do
so again.
9 Acknowledgements
We would like to thank the authors and main-
tainers of the four systems against which we did
our comparison, especially Tom Morton, Mijail
Kabadjov and Yannick Versley. Making your sys-
tem freely available to other researchers is one of
the best ways to push the field forward. In addi-
tion, we thank three anonymous reviewers.
References
Roxana Angheluta, Patrick Jeuniaux, Rudradeb Mi-
tra, and Marie-Francine Moens. 2004. Clustering
algorithms for noun phrase coreference resolution.
In Proceedings of the 7es Journes internationales
d?Analyse statistique des Donnes Textuelles, pages
60?70, Louvain La Neuve, Belgium, March 10?12.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563?566.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
Linguistics, 19(2).
Donna K. Byron. 2002. Resolving pronominal
reference to abstract entities. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics (ACL2002), pages 80?
87, Philadelphia, PA, USA, July 6?12.
Claire Cardie and Kiri Wagstaff. 1999. Noun phrase
coreference as clustering. In In Proceedings of
EMNLP, pages 82?89.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. of the 2005 Meeting of the
Assoc. for Computational Linguistics (ACL), pages
173?180.
Colin Cherry and Shane Bergsma. 2005. An Expecta-
tion Maximization approach to pronoun resolution.
In Proceedings of the Ninth Conference on Compu-
tational Natural Language Learning (CoNLL-2005),
pages 88?95, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Michael Collins and Yorav Singer. 1999. Unsuper-
vised models for named entity classification. In Pro-
ceedings of the Joint SIGDAT Conference on Empir-
ical Methods in Natural Language Processing and
Very Large Corpora (EMNLP 99).
Niyu Ge, John Hale, and Eugene Charniak. 1998. A
statistical approach to anaphora resolution. In Pro-
ceedings of the Sixth Workshop on Very Large Cor-
pora, pages 161?171, Orlando, Florida. Harcourt
Brace.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 848?855. Association for Computational
Linguistics.
Andrew Kehler, Douglas Appelt, Lara Taylor, and
Aleksandr Simma. 2004a. Competitive self-trained
pronoun interpretation. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL 2004:
Short Papers, pages 33?36, Boston, Massachusetts,
USA, May 2 - May 7. Association for Computa-
tional Linguistics.
Andrew Kehler, Douglas E. Appelt, Lara Taylor, and
Aleksandr Simma. 2004b. The (non)utility of
predicate-argument frequencies for pronoun inter-
pretation. In Proceedings of the 2004 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 289?296.
Shalom Lappin and Herber J. Leass. 1994. An algo-
rithm for pronouminal anaphora resolution. Compu-
tational Linguistics, 20(4):535?561.
Min-Yen Kan Long Qiu and Tat-Seng Chua. 2004.
A public reference implementation of the RAP
anaphora resolution algorithm. In Proceedings of
the Fourth International Conference on Language
Resources and Evaluation, volume I, pages 291?
294.
David McClosky, Eugene Charniak, and MarkJohnson.
2008. BLLIP North American News Text, Complete.
Linguistic Data Consortium. LDC2008T13.
Bernard Merialdo. 1991. Tagging text with a prob-
abilistic model. In International Conference on
Speech and Signal Processing, volume 2, pages
801?818.
Ruslan Mitkov, Richard Evans, and Constantin Ora?san.
2002. A new, fully automatic version of Mitkov?s
knowledge-poor pronoun resolution method. In
Proceedings of the Third International Conference
on Intelligent Text Processing and Computational
Linguistics (CICLing-2002), Mexico City, Mexico,
February, 17 ? 23.
Thomas Morton, Joern Kottmann, Jason Baldridge, and
Gann Bierner. 2005. Opennlp: A java-based nlp
toolkit. http://opennlp.sourceforge.net.
155
Massimo Poesio and Mijail A. Kabadjov. 2004.
A general-purpos, of-the-shelf anaphora resolution
module: implementataion and preliminary evalu-
ation. In Proceedings of the 2004 international
Conference on Language Evaluation and Resources,
pages 663,668.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183?216.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Joel R. Tetreault. 2001. A corpus-based evaluation
of centering and pronoun resolution. Computational
Linguistics, 27(4):507?520.
Yannick Versley, Simone Ponzetto, Massimo Poesio,
Vladimir Eidelman, Alan Jern, Jason Smith, Xi-
aofeng Yang, and Alessandro Moschitti. 2008.
Bart: A modular toolkit for coreference resolution.
In Companion Volume of the Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 9?12.
Marilyn A. Walker. 1989. Evaluating discourse pro-
cessing algorithms. In ACL, pages 251?261.
Xiaofeng Yang, Jian Su, Guodong Zhou, and
Chew Lim Tan. 2004. Improving pronoun res-
olution by incorporating coreferential information
of candidates. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL2004), pages 127?134, Barcelona,
Spain, July 21?26.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
Kernel-based pronoun resolution with structured
syntactic knowledge. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 41?48, Sydney,
Australia, July. Association for Computational Lin-
guistics.
156
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 233?240, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Effective Use of Prosody in Parsing Conversational Speech
Jeremy G. Kahn? Matthew Lease?
Eugene Charniak? Mark Johnson? Mari Ostendorf?
University of Washington, SSLI? Brown University?
{jgk,mo}@ssli.ee.washington.edu {mlease,ec,mj}@cs.brown.edu
Abstract
We identify a set of prosodic cues for parsing con-
versational speech and show how such features can
be effectively incorporated into a statistical parsing
model. On the Switchboard corpus of conversa-
tional speech, the system achieves improved parse
accuracy over a state-of-the-art system which uses
only lexical and syntactic features. Since removal
of edit regions is known to improve downstream
parse accuracy, we explore alternatives for edit de-
tection and show that PCFGs are not competitive
with more specialized techniques.
1 Introduction
For more than a decade, the Penn Treebank?s Wall
Street Journal corpus has served as a benchmark for
developing and evaluating statistical parsing tech-
niques (Collins, 2000; Charniak and Johnson, 2005).
While this common benchmark has served as a valu-
able shared task for focusing community effort, it
has unfortunately led to the relative neglect of other
genres, particularly speech. Parsed speech stands to
benefit from practically every application envisioned
for parsed text, including machine translation, infor-
mation extraction, and language modeling. In con-
trast to text, however, speech (in particular, conver-
sational speech) presents a distinct set of opportu-
nities and challenges. While new obstacles arise
from the presence of speech repairs, the possibility
of word errors, and the absence of punctuation and
sentence boundaries, speech also presents a tremen-
dous opportunity to leverage multi-modal input, in
the form of acoustic or even visual cues.
As a step in this direction, this paper identifies a
set of useful prosodic features and describes how
they can be effectively incorporated into a statisti-
cal parsing model, ignoring for now the problem
of word errors. Evaluated on the Switchboard cor-
pus of conversational telephone speech (Graff and
Bird, 2000), our prosody-aware parser out-performs
a state-of-the-art system that uses lexical and syntac-
tic features only. While we are not the first to employ
prosodic cues in a statistical parsing model, previous
efforts (Gregory et al, 2004; Kahn et al, 2004) in-
corporated these features as word tokens and thereby
suffered from the side-effect of displacing words in
the n-gram models by the parser. To avoid this prob-
lem, we generate a set of candidate parses using an
off-the-shelf, k-best parser, and use prosodic (and
other) features to rescore the candidate parses.
Our system architecture combines earlier models
proposed for parse reranking (Collins, 2000) and
filtering out edit regions (Charniak and Johnson,
2001). Detecting and removing edits prior to parsing
is motivated by the claim that probabilistic context-
free grammars (PCFGs) perform poorly at detect-
ing edit regions. We validate this claim empirically:
two state-of-the-art PCFGs (Bikel, 2004; Charniak
and Johnson, 2005) are both shown to perform sig-
nificantly below a state-of-the-art edit detection sys-
tem (Johnson et al, 2004).
2 Previous Work
As mentioned earlier, conversational speech
presents a different set of challenges and opportu-
nities than encountered in parsing text. This paper
focuses on the challenges associated with disfluen-
cies (Sec. 2.1) and the opportunity of leveraging
acoustic-prosodic cues at the sub-sentence level
(Sec. 2.2). Here, sentence segmentation is assumed
to be known (though punctuation is not available);
233
. . . while I think,
? ?? ?
Reparandum
+ uh, I mean,
? ?? ?
Editing phrase
I know
? ?? ?
Repair
that. . .
Figure 1: The structure of a typical repair, with ?+? indicating the interruption point.
the impact of automatic segmentation is addressed
in other work (Kahn et al, 2004).
2.1 Speech Repairs and Parsing
Spontaneous speech abounds with disfluencies such
as partial words, filled pauses (e.g., ?uh?, ?um?),
conversational fillers (e.g., ?you know?), and par-
enthetical asides. One type of disfluency that has
proven particularly problematic for parsing is speech
repairs: when a speaker amends what he is saying
mid-sentence (see Figure 1). Following the analy-
sis of (Shriberg, 1994), a speech repair can be un-
derstood as consisting of three parts: the reparan-
dum (the material repaired), the editing phrase (that
is typically either empty or consists of a filler), and
the repair. The point between the reparandum and
the editing phrase is referred to as the interruption
point (IP), and it is the point that may be acousti-
cally marked. We refer to the reparandum and edit-
ing phrase together as an edit or edit region. Speech
repairs are difficult to model with HMM or PCFG
models, because these models can induce only linear
or tree-structured dependencies between words. The
relationship between reparandum and repair is quite
different: the repair is often a ?rough copy? of the
reparandum, using the same or very similar words
in roughly the same order. A language model char-
acterizing this dependency with hidden stack opera-
tions is proposed in (Heeman and Allen, 1999).
Several parsing models have been proposed which
accord special treatment to speech repairs. Most
prior work has focused on handling disfluencies
and continued to rely on hand-annotated transcripts
that include punctuation, case, and known sentence
boundaries (Hindle, 1983; Core and Schubert, 1999;
Charniak and Johnson, 2001; Engel et al, 2002).
Of particular mention is the analysis of the rela-
tionship between speech repairs and parsing accu-
racy presented by Charniak and Johnson (2001), as
this directly influenced our work. They presented
evidence that improved edit detection (i.e. detect-
ing the reparandum and edit phrase) leads to better
parsing accuracy, showing a relative reduction in F -
score error of 14% (2% absolute) between oracle and
automatic edit removal. Thus, this work adopts their
edit detection preprocessing approach. They have
subsequently presented an improved model for de-
tecting edits (Johnson et al, 2004), and our results
here complement their analysis of the edit detection
and parsing relationship, particularly with respect to
the limitations of PCFGs in edit detection.
2.2 Prosody and parsing
While spontaneous speech poses problems for pars-
ing due to the presence of disfluencies and lack of
punctuation, there is information in speech associ-
ated with prosodic cues that can be taken advantage
of in parsing. Certainly, prosodic cues are useful
for sentence segmentation (Liu et al, 2004), and
the quality of automatic segmentation can have a
significant impact on parser performance (Kahn et
al., 2004). There is also perceptual evidence that
prosody provides cues to human listeners that aid
in syntactic disambiguation (Price et al, 1991), and
the most important of these cues seems to be the
prosodic phrases (perceived groupings of words) or
the boundary events marking them. However, the
utility of sentence-internal prosody in parsing con-
versational speech is not well established.
Most early work on integrating prosody in parsing
was in the context of human-computer dialog sys-
tems, where parsers typically operated on isolated
utterances. The primary use of prosody was to rule
out candidate parses (Bear and Price, 1990; Batliner
et al, 1996). Since then, parsing has advanced con-
siderably, and the use of statistical parsers makes the
candidate pruning benefits of prosody less impor-
tant. This raises the question of whether prosody
is useful for improving parsing accuracy for con-
versational speech, apart from its use in sentence
234
Figure 2: System architecture
boundary detection. Extensions of Charniak and
Johnson (2001) look at using quantized combina-
tions of prosodic features as additional ?words?,
similar to the use of punctuation in parsing written
text (Gregory et al, 2004), but do not find that the
prosodic features are useful. It may be that with the
short ?sentences? in spontaneous speech, sentence-
internal prosody is rarely of use in parsing. How-
ever, in edit detection using a parsing model (John-
son et al, 2004), posterior probabilities of automati-
cally detected IPs based on prosodic cues (Liu et al,
2004) are found to be useful. The seeming discrep-
ancy between results could be explained if prosodic
cues to IPs are useful but not other sub-sentence
prosodic constituents. Alternatively, it could be that
including a representation of prosodic features as
terminals in (Gregory et al, 2004) displaces words
in the parser n-gram model history. Here, prosodic
event posteriors are used, with the goal of providing
a more effective way of incorporating prosody than
a word-like representation.
3 Approach
3.1 Overall architecture
Our architecture, shown in Figure 2, combines the
parse reranking framework of (Collins, 2000) with
the edit detection and parsing approach of (Charniak
and Johnson, 2001). The system operates as follows:
1. Edit words are identified and removed.
2. Each resulting string is parsed to produce a set
of k candidate parses.
3. Edit words reinserted into the candidates with
a new part-of-speech tag EW. Consecutive se-
quences of edit words are inserted as single, flat
EDITED constituents.
4. Features (syntactic and/or prosodic) are ex-
tracted for each candidate, i.e. candidates are
converted to feature vector representation.
5. The candidates are rescored by the reranker to
identify the best parse.
Use of Collins? parse reranking model has several
advantages for our work. In addition to allowing us
to incorporate prosody without blocking lexical de-
pendencies, the discriminative model makes it rela-
tively easy to experiment with a variety of prosodic
features, something which is considerably more dif-
ficult to do directly with a typical PCFG parser.
Our use of the Charniak-Johnson approach of sep-
arately detecting disfluencies is motivated by their
result that edit detection error degrades parser accu-
racy, but we also include experiments that omit this
step (forcing the PCFG to model the edits) and con-
firm the practical benefit of separating responsibili-
ties between the edit detection and parsing tasks.
3.2 Baseline system
We adopt an existing parser-reranker as our base-
line (Charniak and Johnson, 2005). The parser
component supports k-best parse generation, and
the reranker component is used to rescore candi-
date parses proposed by the parser. In detail, the
reranker selects from the set of k candidates T =
{t1, . . . tk} the parse t? ? T with the highest bracket
F -score (in comparison with a hand-annotated ref-
erence). To accomplish this, a feature-extractor con-
verts each candidate parse t ? T into a vector of
real-valued features f(t) = (f1(t), . . . , fm(t)) (e.g.,
the value fj(t) of the feature fj might be the num-
ber of times a certain syntactic structure appears in
t). The reranker training procedure associates each
feature fj with a real-valued weight ?j , and ??f(t)
(the dot product of the feature vector and the weight
vector ?) is a single scalar weight for each parse can-
didate. The reranker employs a maximum-entropy
estimator that selects the ? that minimizes the log
loss of the highest bracket F -score parse t? condi-
tioned on T (together with a Gaussian regularizer
to prevent overtraining). Informally, ? is chosen to
235
make high F -score parses as likely as possible un-
der the (conditional) distribution defined by f and ?.
As in (Collins, 2000), we generate training data for
the reranker by reparsing the training corpus, using
n ? 1 folds as training data to parse the n-th fold.
The existing system also includes a feature extrac-
tor that identifies interesting syntactic relationships
not included in the PCFG parsing model (but used
in the reranker). These features are primarily related
to non-local dependencies, including parallelism of
conjunctions, the number of terminals dominated by
coordinated structures, right-branching root-to-leaf
length, lexical/functional head pairs, n-gram style
sibling relationships, etc.
3.3 Prosodic Features
Most theories of prosody have a symbolic represen-
tation for prosodic phrasing, where different combi-
nations of acoustic cues (fundamental frequency, en-
ergy, timing) combine to give categorical perceptual
differences. Our approach to integrating prosody in
parsing is to use such symbolic boundary events, in-
cluding prosodic break labels that build on linguistic
notions of intonational phrases and hesitation phe-
nomena. These events are predicted from a com-
bination of continuous acoustic correlates, rather
than using the acoustic features directly, because
the intermediate representation simplifies training
with high-level (sparse) structures. Just as phone-
based acoustic models are useful in speech recogni-
tion systems as an intermediate level between words
and acoustic features (especially for characterizing
unseen words), the small set of prosodic boundary
events are used here to simplify modeling the inter-
dependent set of continuous-valued acoustic cues re-
lated to prosody. However, also as in speech recog-
nition, we use posterior probabilities of these events
as features rather than making hard decisions about
presence vs. absence of a constituent boundary.
In the past, the idea of using perceptual categories
has been dismissed as impractical due to the high
cost of hand annotation. However, with advances
in weakly supervised learning, it is possible to train
prosodic event classifiers with only a small amount
of hand-labeled data by leveraging information in
syntactic parses of unlabeled data. Our strategy is
similar to that proposed in (No?th et al, 2000), which
uses categorical labels defined in terms of syntactic
structure and pause duration. However, their sys-
tem?s category definitions are without reference to
human perception, while we leverage learned re-
lations between perceptual events and syntax with
other acoustic cues, without predetermining the re-
lation or requiring a direct coupling to syntax.
More specifically, we represent three classes of
prosodic boundaries (or, breaks): major intonational
phrase, hesitation, and all other word boundaries.1
A small set of hand-labeled data from the treebanked
portion of the Switchboard corpus (Ostendorf et al,
2001) was used to train initial break prediction mod-
els based on both parse and acoustic cues. Next, the
full set of treebanked Switchboard data is used with
an EM algorithm that iterates between: i) finding
probabilities of prosodic breaks in unlabeled data
based on the current model, again using parse and
acoustic features, and ii) re-estimating the model us-
ing the probabilities as weighted counts. Finally, a
new acoustic-only break prediction model was de-
signed from this larger data set for use in the parsing
experiments.
In each stage, we use decision trees for models, in
part because of an interest in analyzing the prosody-
syntax relationships learned. The baseline system
trained on hand-labeled data has error rates of 9.6%
when all available cues are used (both syntax and
prosody) and 16.7% when just acoustic and part-of-
speech cues are provided (our target environment).
Using weakly supervised (EM) training to incorpo-
rate unannotated data led to a 15% reduction in error
rate to 14.2% for the target trees. The final decision
tree was used to generate posteriors for each of the
three classes, one for each word in a sentence.
?From perceptual studies and decision tree analy-
ses, we know that major prosodic breaks tend to co-
occur with major clauses, and that hesitations often
occur in edit regions or at high perplexity points in
the word sequence. To represent the co-occurrence
as a feature for use in parse reranking, we treat
the prosodic break posteriors as weighted counts in
accumulating the number of constituents in parse
t of type i with break type j at their right edge,
which (with some normalization and binning) be-
comes feature fij . Note that the unweighted count
1The intonational phrase corresponds to a break of ?4? in the
ToBI labeling system (Pitrelli et al, 1994), and a hesitation is
marked with the ?p? diacritic.
236
for constituent i corresponds directly to a feature
in the baseline set, but the baseline set of features
also includes semantic information via association
with specific words. Here, we simply use syntactic
constituents. It is also known that major prosodic
breaks tend to be associated with longer syntactic
constituents, so we used the weighted count strategy
with length-related features as well. In all, the vari-
ous attributes associated with prosodic break counts
were the constituent label of the subtree, its length
(in words), its height (maximal distance from the
constituent root to any leaf), and the depth of the
rightmost word (distance from the right word to the
subtree root). For each type in each of these cate-
gories, there are three prosodic features, correspond-
ing to the three break types.
3.4 Edit detection
To provide a competitive baseline for our parsing
experiments, we used an off-the-shelf, state-of-the-
art TAG-based model as our primary edit detec-
tor (Johnson et al, 2004).2 This also provided us a
competitive benchmark for contrasting the accuracy
of PCFGs on the edit detection task (Section 4.2).
Whereas the crossing-dependencies inherent in
speech repairs makes them difficult to model us-
ing HMM or PCFG approaches (Section 2.1), Tree
Adjoining Grammars (TAGs) are capable of cap-
turing these dependencies. To model both the
crossed-dependencies of speech repairs and the lin-
ear or tree-structured dependencies of non-repaired
speech, Johnson et al?s system applies the noisy
channel paradigm: a PCFG language model defines
a probability distribution over non-repaired speech,
and a TAG is used to model the optional insertion of
edits. The output of this noisy channel model is a
set of candidate edits which are then reranked using
a max-ent model (similar to what is done here for
parse reranking). This reranking step enables incor-
poration of features based on an earlier word-based
classifier (Charniak and Johnson, 2001) in addition
to output features of the TAG model. Acoustic fea-
tures are not yet incorporated.
2We also evaluated another state-of-the-art edit detection
system (Liu et al, 2004) but found that it suffered from a mis-
match between the current LDC specification of edits (LDC,
2004) and that used in the treebank.
4 Experimental design
4.1 Corpus
Experiments were carried out on conversational
speech using the hand-annotated transcripts associ-
ated with the Switchboard treebank (Graff and Bird,
2000). As was done in (Kahn et al, 2004), we
resegmented the treebank?s sentences into V5-style
sentence-like units (SUs) (LDC, 2004), since our ul-
timate goal was to be able to parse speech given au-
tomatically detected boundaries. Unfortunately, the
original transcription effort did not provide punctu-
ation guidelines, and the Switchboard treebanking
was performed on the transcript unchanged, with-
out reference to the audio. As a result, the sentence
boundaries sometimes do not match human listener
decisions using SU annotation guidelines, with dif-
ferences mainly corresponding to treatment of dis-
course markers and backchannels. In the years since
the original Switchboard annotation was performed,
LDC has iteratively refined guidelines for annotating
SUs, and significant progress has been made in au-
tomatically recovering SU boundaries annotated ac-
cording to this standard (Liu et al, 2004). To even-
tually leverage this work, we have taken the Meteer-
annotated SUs (Meteer et al, 1995), for which there
exists treebanked training data, and automatically
adjusted them to be more like the V5 LDC stan-
dard, and resegmented the Switchboard treebank ac-
cordingly. In cases where the original syntactic con-
stituents span multiple SUs, we discard any con-
stituents violating the SU boundary, and in the event
that an SU spans a treebank sentence boundary, a
new top-level constituent SUGROUP is inserted to
produce a proper tree (and evaluated like any other
constituent in the gold tree).3 While this SU reseg-
mentation makes it difficult to compare our experi-
mental results to past work, we believe this is a nec-
essary step towards developing a more realistic base-
line for fully automated parsing of speech.
In addition to resegmention, we removed all punc-
tuation and case from the corpus to more closely
reflect the form of output available from a speech
recognizer. We retained partial words for consis-
3SU and treebank segments disagree at about 5% in each di-
rection, due mostly to the analysis of discourse markers as con-
junctions (sentences of >1 SU) and the separation of backchan-
nels into separate treebank sentences (SUs of >1 sentence).
237
Table 1: Statistics on the Switchboard division used.
Section Sides SUs Words
Train 1,031 87,599 659,437
Tune 126 13,147 103,500
Test 128 8,726 61,313
Total 1,285 109,472 824,250
tency with other work (Liu et al, 2004; Johnson et
al., 2004), although word fragments would not typ-
ically be available from ASR. Finally, of the 1300
total conversation sides, we discarded 15 for which
we did not have prosodic data. Our corpus division
statistics are given in Table 1. During development,
experiments were carried out on the tune section; the
test section was reserved for a final test run.
4.2 Experimental Variables
Our primary goal is to evaluate the extent to which
prosodic cues could augment and/or stand-in for lex-
ical and syntactic features. Correspondingly, we
report on using three flavors of feature extraction:
syntactic and lexical features (Section 3.2), prosodic
features (Section 3.3), and both sets of features com-
bined. For all three conditions, the first-stage score
for each parse (generated by the off-the-shelf k-best
parser) was always included as a feature.
A second parameter varied in the experiments was
the method of upstream edit detection employed
prior to parsing: PCFG, TAG-based, and oracle
knowledge of treebank edit annotations. While it
had been claimed that PCFGs perform poorly as edit
detectors (Charniak and Johnson, 2001), we could
not find empirical evidence in the literature quan-
tifying the severity of the problem. Therefore, we
evaluated two PCFGs (Bikel, 2004; Charniak and
Johnson, 2005) on edit detection and compared their
performance to a state-of-the-art TAG-based edit de-
tection system (Johnson et al, 2004). For this ex-
periment, we evaluated edit detection accuracy on a
per-word basis, where any tree terminal is consid-
ered an edit word if and only if it is dominated by
an EDITED constituent in the gold tree. The PCFGs
were trained on the train section of the treebank data
with the flattened edit regions included4 and then
4Training on flattened EDITED nodes improved detection ac-
curacy for both PCFGs: as much as 15% for Bikel-Collins.
Table 2: Edit word detection performance for two
word-based PCFGs and the TAG-based edit detec-
tor. F -score and error are word-based measures.
Edit Detector Edit F -score Edit Error
Bikel-Collins PCFG 65.3 62.1
Charniak PCFG 65.8 59.9
TAG-based 78.2 42.2
Table 3: Parsing F -score of various feature and edit-
detector combinations.
PCFG TAG Oracle
Edit F (Table 2) 65.8 78.2 100.0
Parser 1-best 84.4 85.0 86.9
Prosodic feats 85.0 85.6 87.6
Syntactic feats 85.9 86.4 88.4
Combined feats 86.0 86.6 88.6
Oracle-rate 92.6 93.2 95.2
used to parse the test data.5 The TAG-based de-
tector was trained on the same conversation sides,
with its channel model trained on the Penn Treebank
disfluency-annotated files and its language model
trained on trees with the EDITED nodes excised. As
shown in Table 2, we did find that both PCFGs per-
formed significantly below the TAG-based detector.
5 Results
In evaluating parse accuracy, we adopt the relaxed
edited revision (Charniak and Johnson, 2001) to the
standard PARSEVAL metric, which penalizes sys-
tems that get EDITED spans wrong, but does not pe-
nalize disagreements in the attachment or internal
structure of edit regions. This metric is based on the
assumption that there is little reason to recover syn-
tactic structure in regions of speech that have been
repaired or restarted by the speaker.
Table 3 shows the F -scores for the top-ranked
parses after reranking, where the first-stage PCFG
parser was run with a beam-size of 50. The first
and last rows show lower and upper bounds, respec-
tively, for reranked parsing accuracy on each edit
condition. As the oracle rate6 shows, there is con-
5For the Charniak parser, edits were detected using only its
PCFG component in 1-best mode, not its 2nd stage reranker.
6Oracle F uses the best parse in the 50-best list.
238
siderable room for improvement. Statistical signif-
icance was computed using a non-parametric shuf-
fle test similar to that in (Bikel, 2004). For TAG
and oracle edit detection conditions, the improve-
ment from using the combined features over either
prosodic or syntactic features in isolation was sig-
nificant (p < 0.005). (For PCFG edit detection,
p < 0.04.) Similarly, for all three feature extraction
conditions, the improvement from using the TAG-
based edit detector instead of the PCFG edit detector
was also significant (p < 0.001). Interestingly, the
TAG?s 34% reduction in edit detection error relative
to the PCFG yielded only about 23% of the parse
accuracy differential between the PCFG and oracle
conditions. Nevertheless, there remains a promising
2.0% difference in parse F -score between the TAG
and oracle detection conditions to be realized by fur-
ther improvements in edit detection. Training for
the syntactic feature condition resulted in a learned
weight ? with approximately 50K features, while
the prosodic features used only about 1300 features.
Despite this difference in the length of the ? vectors,
the prosodic feature condition achieved 40?50% of
the improvement of the syntactic features.
In some situations, e.g. for language modeling,
improving the ordering and weights of the entire
parse set (an not just the top ranked parse) is im-
portant. To illustrate the overall improvement of the
reranked order, in Table 4 we report the reranked-
oracle rate over the top s parses, varying the beam s.
The error for each feature condition, relative to using
the PCFG parser in isolation, is shown in Figure 3.
Both the table and figure show that the reranked
beam achieves a consistent trend in parse accuracy
improvement relative to the PCFG beam, similar to
what is demonstrated by the 1-best scores (Table 3).
Table 4: Reranked-oracle rate parse F -score for the
top s parses with reference edit detection.
s 1 2 3 5 10 25
PCFG 86.9 89.8 91.0 92.2 93.4 94.6
Pros. 87.6 90.3 91.5 92.7 93.9 94.8
Syn. 88.4 91.3 92.4 93.4 94.3 95.0
Comb. 88.6 91.5 92.5 93.5 94.4 95.0
Figure 3: Reduction in error (Error = 1?F ) for the
s-best reranked-oracle relative to the parser-only or-
acle, for different feature rerankings (reference edit
detection).
6 Conclusion
This study shows that incorporating prosodic infor-
mation into the parse selection process, along with
non-local syntactic information, leads to improved
parsing accuracy on accurate transcripts of conver-
sational speech. Gains are shown to be robust to dif-
ficulties introduced by automatic edit detection and,
in addition to improving the one-best performance,
the overall ordering of the parse candidates is im-
proved. While the gains from combining prosodic
and syntactic features are not additive, since the
prosodic features incorporates some constituent-
structure information, the combined gains are sig-
nificant. These results are consistent with related ex-
periments with a different type of prosodically cued
event, which showed that automatically detected IPs
based on prosodic cues (Liu et al, 2004) are useful
in the reranking stage of a TAG-based speech repair
detection system (Johnson et al, 2004).
The experiments described here used automat-
ically extracted prosodic features in combination
with human-produced transcripts. It is an open ques-
tion as to whether the conclusions will hold for er-
rorful ASR transcripts and automatically detected
SU boundaries. However, there is reason to believe
that relative gains from using prosody may be larger
than those observed here for reference transcripts
239
(though overall performance will degrade), based on
prior work combining prosody and lexical cues to
detect other language structures (Shriberg and Stol-
cke, 2004). While the prosody feature extraction de-
pends on timing of the hypothesized word sequence,
the acoustic cues are relatively robust to word errors
and the break model can be retrained on recognizer
output to automatically learn to discount the lexical
evidence. Furthermore, if parse reranking operates
on the top N ASR hypotheses, the reranking pro-
cedure can improve recognition outputs, as demon-
strated in (Kahn, 2005) for syntactic features alone.
Allowing for alternative SU hypotheses in reranking
may also lead to improved SU segmentation.
In addition to assessing the impact of prosody
in a fully automatic system, other avenues for fu-
ture work include improving feature extraction. One
could combine IP and prosodic break features (so
far explored separately), find new combinations of
prosody and syntactic structure, and/or incorporate
other prosodic events. Finally, it may also be use-
ful to integrate the prosodic events directly into the
PCFG, in addition to their use in reranking.
This work was supported by the NSF under grants DMS-
0074276, IIS-0085940, IIS-0112432, IIS-0326276, and LIS-
9721276. Conclusions are those of the authors and do not nec-
essarily reflect the views of the NSF.
References
A. Batliner et al 1996. Prosody, empty categories and
parsing - a success story. Proc. ICSLP, pp. 1169-1172.
J. Bear and P. Price. 1990. Prosody, syntax and parsing.
Proc. ACL, pp. 17-22.
D. Bikel. 2004. On the Parameter Space of Lexicalized
Statistical Parsing Models. Ph.D. thesis, U. Penn.
E. Charniak and M. Johnson. 2001. Edit detection and
parsing for transcribed speech. NAACL, pp. 118-126.
E. Charniak and M. Johnson. 2005. Coarse-to-fine
n-best parsing and MaxEnt discriminative reranking.
Proc. ACL.
M. Collins. 2000. Discriminative reranking for natural
language parsing. Proc. ICML, pp. 175-182.
M. Core and L. Schubert. 1999. A syntactic framework
for speech repairs and other disruptions. Proc. ACL,
pp. 413-420.
D. Engel, E. Charniak, and M. Johnson. 2002. Parsing
and disfluency placement. Proc. EMNLP, pp. 49-54.
D. Graff and S. Bird. 2000. Many uses, many annota-
tions for large speech corpora: Switchboard and TDT
as case studies. Proc. LREC, pp. 427-433.
M. Gregory, M. Johnson, and E. Charniak. 2004.
Sentence-internal prosody does not help parsing the
way punctuation does. Proc. NAACL, pp. 81-88.
P. A. Heeman and J. F. Allen. 1999. Speech repairs,
intonational phrases, and discourse markers: Model-
ing speaker?s utterances in spoken dialogue. Compu-
tational Linguistics, 25(4):527-571.
D. Hindle. 1983. Deterministic parsing of syntactic non-
fluencies. Proc. ACL, pp. 123-128.
M. Johnson, E. Charniak, and M. Lease. 2004. An im-
proved model for recognizing disfluencies in conver-
sational speech. Proc. Rich Transcription Workshop.
J. G. Kahn, M. Ostendorf, and C. Chelba. 2004. Pars-
ing conversational speech using enhanced segmenta-
tion. Proc. HLT-NAACL 2004, pp. 125-128.
J. G. Kahn. 2005. Moving beyond the lexical layer in
parsing conversational speech. M.A. thesis, U. Wash.
LDC. 2004. Simple metadata annotation specification.
Tech. report, Linguistic Data Consortium. Available
at http://www.ldc.upenn.edu/Projects/MDE.
Y. Liu et al 2004. The ICSI-SRI-UW metadata extrac-
tion system. Proc. ICSLP, pp. 577-580.
M. Meteer, A. Taylor, R. MacIntyre, and R. Iyer. 1995.
Dysfluency annotation stylebook for the switchboard
corpus. Tech. report, Linguistic Data Consortium.
E. No?th et al 2000. Verbmobil: The use of prosody in
the linguistic components of a speech understanding
system. IEEE Trans. SAP, 8(5):519-532.
M. Ostendorf et al 2001. A prosodically labeled
database of spontaneous speech. ISCA Workshop on
Prosody in Speech Recognition and Understanding,
pp. 119-121, 10.
J. Pitrelli, M. Beckman, and J. Hirschberg. 1994. Eval-
uation of prosodic transcription labeling reliability in
the ToBI framework. Proc. ICSLP, pp. 123-126.
P. J. Price et al 1991. The use of prosody in syntactic
disambiguation. JASA, 90(6):2956-2970, 12.
E. Shriberg. 1994. Preliminaries to a Theory of Speech
Disfluencies. Ph.D. thesis, U.C. Berkeley.
E. Shriberg and A. Stolcke. 2004. Prosody modeling
for automatic speech recognition and understanding.
Mathematical Foundations of Speech and Language
Processing. Springer-Verlag, pp. 105-114.
240
Parsing Biomedical Literature
Matthew Lease and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP),
Brown University, Providence, RI USA
{mlease, ec}@cs.brown.edu
Abstract. We present a preliminary study of several parser adaptation
techniques evaluated on the GENIA corpus of MEDLINE abstracts [1,2].
We begin by observing that the Penn Treebank (PTB) is lexically im-
poverished when measured on various genres of scientific and techni-
cal writing, and that this significantly impacts parse accuracy. To re-
solve this without requiring in-domain treebank data, we show how ex-
isting domain-specific lexical resources may be leveraged to augment
PTB-training: part-of-speech tags, dictionary collocations, and named-
entities. Using a state-of-the-art statistical parser [3] as our baseline, our
lexically-adapted parser achieves a 14.2% reduction in error. With oracle-
knowledge of named-entities, this error reduction improves to 21.2%.
1 Introduction
Since the advent of the Penn Treebank (PTB) [4], statistical approaches to nat-
ural language parsing have quickly matured [3,5]. By providing a very large
corpus of manually labeled parsing examples, PTB has played an invaluable
role in enabling the broad analysis, automatic training, and quantitative evalu-
ation of parsing techniques. However, while PTB?s Wall Street Journal (WSJ)
corpus has historically served as the canonical benchmark for evaluating statis-
tical parsing, the need for broader evaluation has been increasingly recognized
in recent years. Furthermore, since it is impractical to create a large treebank
like PTB for every genre of interest, significant attention has been directed to-
wards maximally reusing existing training data in order to mitigate the need
for domain-specific training examples. These issues have been most notably ex-
plored in parser adaptation studies conducted between PTB?s WSJ and Brown
corpora [6,7,8,9].
As part of our own exploration of these issues, we have been investigating
statistical parser adaptation to a novel domain: biomedical literature. This lit-
erature presents a stark contrast to WSJ and Brown: it is suffused with domain-
specific vocabulary, has markedly different stylistic constraints, and is often writ-
ten by non-native speakers. Moreover, broader consideration of technical litera-
ture shows this challenge and opportunity is not confined to biomedical literature
 We would like to thank the National Science Foundation for their support of this work
(IIS-0112432, LIS-9721276, and DMS-0074276), as well as thank Sharon Goldwater
and our anonymous reviewers for their valuable feeback.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 58?69, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Parsing Biomedical Literature 59
alone, but is also demonstrated by patent literature, engineering manuals, and
field-specific scientific discourse. Through our work with biomedical literature,
we hope to gain insights into effective techniques for adapting statistical parsing
to technical literature in general.
Our interest in biomedical literature is also motivated by a real need to im-
prove information extraction in this domain. With over 15 million citations in
PubMed today, biomedical literature is the largest and fastest growing knowl-
edge domain of any science. As such, simply managing the sheer volume of its
accumulated information has become a significant problem. In response to this,
a large research community has formed around the challenge of enabling auto-
mated mining of the literature [10,11]. While the potential value of parsing has
often been discussed by this community, attempts to employ it thus far appear
to have been limited by the parsing technologies employed. Reported difficul-
ties include poor coverage, inability to resolve syntactic ambiguity, unacceptable
memory and speed, and difficulty in hand-crafting rules of grammar [12,13].
Perhaps the most telling indicator of community perspective came in a recent
survey?s bleak observation that efficient and accurate parsing of unrestricted text
appears to be out of reach of current techniques [14].
In this paper, we show that broad, accurate parsing of biomedical literature
is indeed possible. Using an off-the-shelf WSJ-trained statistical parser [3] as our
baseline, we provide the first full-coverage parse accuracy results for biomedi-
cal literature, as measured on the GENIA corpus of MEDLINE abstracts [1,2].
Furthermore, after showing that PTB is lexically impoverished when measured
on various genres of scientific and technical writing, we describe three methods
for improving parse accuracy by leveraging lexical resources from the domain:
part-of-speech (POS) tags, dictionary collocations, and named-entities. Our gen-
eral hope is that lexically-based techniques such as these can provide alternative
and complementary value to treebank-based adaptation methods such as co-
training [9] and sample selection [15]. Our lexically-adapted parser achieves a
14.2% reduction in error over the baseline, and in the case of oracle-knowledge
of named-entities, this reduction improves to 21.2%.
Section 2 describes the GENIA corpus in detail. In Section 3, we present
unknown word rate experiments which measure the coverage of PTB?s gram-
mar on various genres of scientific and technical writing. Section 4 describes
our methods for lexical adaptation and their corresponding effects on parse ac-
curacy. Section 5 concludes with a discussion challenges and opportunities for
future work.
2 The GENIA Corpus
The GENIA corpus [1,2] consists of MEDLINE abstracts related to transcription
factors in human blood cells. Version 3.02p of the corpus includes 19991 ab-
stracts (18,545 sentences, 436,947 words) annotated with part-of-speech (POS)
1 The reported total of 2000 abstracts includes repetition of article ID 97218353.
60 M. Lease and E. Charniak
tags and named-entities. Named-entities were labelled according to a corpus-
defined ontology, and the POS-tagging scheme employed is very similar to that
used in PTB (see Section 4.1).
Using these POS annotations and PTB guidelines [16], we hand-parsed 21
of these abstracts (215 sentences) to create a pilot treebank for measuring parse
accuracy. We performed the treebanking using the GRAPH2 tool developed for
the Prague Dependency Treebank. Initial bracketing was performed without any
form of automation. Following this, our baseline parser [3] was used to propose
alternative parses. In cases where hand-generated parses conflicted with those
proposed by the parser, hand-parses were manually corrected, or not corrected,
according to PTB bracketing guidelines. Our pilot treebank is publicly available3.
Subsequent to this, the Tsujii lab released its own beta version treebank,
which includes 200 abstracts (1761 sentences) from the original corpus. This
treebanking was performed largely in accordance with PTB guidelines (perhaps
the most significant difference being constituent labels NAC and NX were excluded
in favor of NP). Because there is no redundancy in the coverage of the Tsuijii lab?s
treebank and our own pilot treebank (and by chance, NAC and NX do not occur
in our pilot treebank either), we have combined the two treebanks to maximize
our evaluation treebank (see Table 3).
An additional note is required regarding our use of named-entities (Sec-
tion 4.3). Entity annotations (not available in the treebank) were obtained from
the earlier 3.02p version of the corpus. Any sentences that did not match be-
tween the two versions of the corpus (due to differences in tokenization or other
variations) were discarded. The practical impact of this was negligible, as only
25 sentences had to be discarded4.
3 Unknown Words
Casual reading of technical literature quickly reveals a rich, field-specific vocab-
ulary. For example, consider the following sentence taken from GENIA:
The study of NF-kappaB showed that oxLDLs led to a decrease of
activation-induced p65/p50 NF-kappaB heterodimer binding to DNA,
whereas the presence of the constitutive nuclear form of p50 dimer was
unchanged.
To quantitatively measure the size and field-specificity of domain vocabulary, we
extracted the lexicon contained in WSJ sections 2-21 and evaluated the unknown
word rate (by token) for various genres of technical literature. Results are given
in Table 1.
2 http://quest.ms.mff.cuni.cz/pdt/Tools/Tree Editors/Graph
3 http://www.cog.brown.edu/Research/nlp
4 Because our preliminary use of named-entities assumes oracle-knowledge, this exper-
iment was carried out on the development section only, thus only the development
section was reduced in this way.
Parsing Biomedical Literature 61
Table 1. Unknown word rate on various technical corpora given WSJ 2-21 lexion
Corpus Unknown Word Rate
WSJ sect. 24 2.7
Brown-DEV 5.8
Brown sect. J 7.3
CRAN 10.0
CACM 10.7
DOE 16.7
GENIA 25.5
Brown-DEV corresponds to a balanced sampling of the Brown corpus (see Ta-
ble 4). Section J of Brown contains ?Learned? writing samples and demonstrated
the highest rate of any single Brown section. CRAN contains 1400 abstracts in
the field of aerodynamics, and CACM includes 3200 abstracts from Communica-
tions of the ACM [17]. DOE contains abstracts from the Department of Energy,
released as part of PTB. GENIA here refers to 333 abstracts (IDs 97449161-
99101008) not overlapping our treebank. As this table shows, unknown word
rate clearly increases as we move to increasingly technical domains. Annecdotal
evaluation on patent literature suggests its unknown rate lies somewhere between
that of DOE and GENIA.
While these results appear to indicate WSJ is lexically impoverished with
respect to increasingly technical domains, it was also necessary to consider the
possibility that the results were simply symptomatic of technical domains having
very large lexicons. If such were the case, we would expect to see these domains
demonstrate high unknown word rates even in the presence of a domain-specific
lexicon. To test this hypothesis, we contrasted unknown word rates on GENIA
using lexicons extracted from WSJ sections 2-21, Brown (training section from
Table 4), and from GENIA itself (1,333 abstracts: IDs 90110496-97445684)5.
Results are presented in Table 2.
Table 2. Unknown word rate on GENIA using lexicons extracted from WSJ, Brown,
and GENIA
Lexicon Size Unknown Word Rate
Brown 25K 28.2
WSJ 40K 25.5
Brown+WSJ 50K 22.4
GENIA 15K 5.3
Brown+WSJ+GENIA 60K 4.6
5 While this set of abstracts does overlap the Tsujii treebank, this experiment was run
prior to the treebank?s release.
62 M. Lease and E. Charniak
Although the unknown word rate in the presence of in-domain training for
GENIA (5.3%, Table 2) is nearly twice that of out-of-domain training (2.7%,
Table 1), suggesting a larger lexicon does indeed exist, it is also strikingly clear
that WSJ and Brown provide almost no lexical value to the domain: expanding
GENIAs lexicon by 45,000 new terms found in WSJ and Brown produced only a
meager 0.7% reduction in unknown word rate. Contrast this with the enormous
reduction achieved through using GENIA?s lexicon instead of the WSJ or Brown
lexicons (Table 2).
4 Parser Adaptation
In this section, we present three methods for parser adaptation motivated by
the results of our unknown word rate experiments (Section 3). The goal of
these adaptations is to help an off-the-shelf PTB-trained parser compensate
for the large amount of domain-specific vocabulary found in technical liter-
ature, specifically biomedical text. To accomplish this without depending on
in-domain treebank data, we consider three alternative (and less expensive)
domain-specific knowledge sources: part-of-speech tags, dictionary collocations,
and named-entities. We report on the results of each technique both in isolation
and in combination.
We adopt as our baseline for these experiments the publicly available Charniak
parser [3] trained on WSJ sections 2-21 of the Penn Treebank. Our division of the
GENIA corpus into development and test sets is shown in Table 3. Analysis was
carried out on the development section, and the test section was reserved for final
evaluation. Parse accuracy was measured using the standard PARSEVAL met-
ric of bracket-bracket scoring, assuming the usual conventions regarding punctua-
tion [18]. Statistical significance for eachexperimentwas assessedusing a two-tailed
paired t-test on sentence-averaged f-measure scores. Since our evaluation treebank
excludes NX and NAC constituent labels in favor ofNP (Section 2), for all experiments
Table 3. Division of the GENIA combined treebank into development and test sections
Source Section Abstract IDs Sentences
Pilot Development 99101510-99120900 215
Tsujii Development 91079577-92060325 732
Tsujii Test 92062170-94051535 1004
Table 4. Brown corpus division. Training and evaluation sections were obtained from
Gildea [7]. The development (and final training) section was created by extracting
every tenth sentence from Gildea?s training corpus.
POS-Train Development Test
Sentences 19637 2181 2425
Parsing Biomedical Literature 63
Table 5. PARSEVAL f-measure scores on the GENIA development section using the
adaptation methods described in Section 4. Statistical significance of individual adap-
tations are compared against no adaptation, and combined adaptations are compared
against the best prior adaptation. As the p values indicate, all of the adaptions listed
here produced a significant improvement in parse accuracy.
Adaptation F-measure Error reduction Significance
none 78.3 ? ?
lexicon 78.6 1.4 p = 0.002
no NNP 79.1 3.7 p = 0.002
train POS 80.8 11.5 p < 0.001
entities 80.9 12.0 p < 0.001
no NNP, train POS 81.5 14.7 p = 0.043
no NNP, train POS, entities 82.9 21.2 p < 0.001
Table 6. Final PARSEVAL f-measure results on GENIA compared with scores on
Brown and WSJ sect. 23. In all cases, the parser was trained on WSJ sect. 2-21 with
the over-parsing parameter set to 21x over-parsing. Adapted GENIA results includes
POS adaptations only (oracle-type entity adaptation was not used). Adapted Brown
results use POS re-training on Brown train section.
Corpus F-measure Error reduction Significance
GENIA-unadapted 76.3 ? ?
GENIA-adapted 79.6 14.2 p < 0.001
Brown-unadapted 83.4 ? ?
Brown-adapted 84.1 4.1 p = 0.002
WSJ 89.5 ? ?
(including baseline)wepost-processedparser output to collapse these label distinc-
tions6. Results from our various experiments are summarized in Table 5.
Final results of our adapted parser are given in Table 6. For comparison with
standard benchmarks, parser performance was also evaluated on WSJ section
23 and on Brown. Table 4 shows our division of the Brown corpus.
4.1 Using POS Tags
Part-of-speech tags provide an important data feature to statistical parsers [3,5].
Since technical and scientific texts introduce a significant amount of domain-
specific vocabulary (Section 3), a POS-tagger trained only on everyday
6 While PTB examples could be similarly pre-processed prior to training, thereby reduc-
ing the search spacewhile parsing, the reductionwould beminor andwouldmean giving
up a potentially useful distinction in syntactic contexts.
64 M. Lease and E. Charniak
English is immediately at a disadvantage for tagging such text. Indeed, our
off-the-shelf PTB-trained parser achieves only 84.6% tagging accuracy on GE-
NIA. Consequently, our simple first adaptation step was to retrain the parser?s
POS-tagger on the 1,778 GENIA abstracts not present in the combined tree-
bank (in addition to WSJ sections 2-21). This simple fix raised tagging accuracy
to 95.9%. Correspondingly, parsing accuracy improved from 78.3% to 80.8%
(Table 5).
While such POS-retraining is a direct remedy to learning appropriate tags
for new vocabulary, it is only a partial fix to a larger problem. In particular, the
trees found in PTB codify a relationship between PTB POS tags and constituent
structure, and any mismatch between the tagging schemata used in PTB and
that used by our new corpus could result in misapplication or underutilization of
the bracketing rules acquired by the parser during training. To overcome this, it
is necessary to introduce an additional mapping step which converts between the
two POS tagging schemata. For closely related schemata, this mapping may be
trivial, but this cannot be assumed without a carefully analysis of tag distribution
and usage across the two corpora.
In the case of GENIA, the tagging guidelines used were based on PTB and
only subsequently revised (to improve inter-annotator agreement), so while dif-
ferences do exist, the problem is much less significant than the general case
of arbitrarily different schemata. Reported differences include treatment of hy-
phenated, partial, and foreign terms, and most notably, the distinction between
proper (NNP) and common (NN) nouns [2]. In order to quantitatively assess
the degree to which these and other revisions were made to the tagging scheme,
we extracted the POS distribution for 333 GENIA abstracts (as used in our
unknown word rate experiments from Section 3). From this distribution, we
learned that NNP almost never occurs in GENIA. This meant that our PTB-
trained parser would be unable to leverage PTB?s constituent structure examples
examples that involved proper nouns.
As a preliminary remedy, we simply relabeled all proper nouns as common in
PTBand re-trained the parser.This improved tagging accuracy to 96.4%and pars-
ing accuracy to 81.5% (Table 5). We should note, however, that this solution is
not ideal. While it does allow use of PTB?s NNP-examples, it does so at the cost
of confusing legitimate differences in the syntactic distribution of common and
proper nouns in English (as reflected by a 0.7% loss in accuracy on WSJ evalua-
tion when using this NN-NNP conflated training data). Clearly it would be better
if GENIA?s nouns could be re-tagged to preserve this distinction while preserving
inter-annotator agreement. A first step in this direction would be to perform this
re-tagging automatically based on determiner usage and GENIA?s entity annota-
tions, with success measured by the corresponding impact on parse accuracy. This,
along with a more careful analysis of tagging differences, remains for future work.
Wehavealso evaluatedparser performanceunder the oracle conditionofperfect
tags. This was implemented as a soft constraint so that the parser?s joint probabil-
ity model could overrule the oracle tag for cases in which no parse could be found
using it (cases of annotator error or data sparsity). Using the oracle tag 99.8% of
Parsing Biomedical Literature 65
the time (in addition to other POS adaptations) had almost no impact on parse ac-
curacy, suggesting that further POS-related improvements in parse accuracy will
only come from the sort of careful analysis of the tagging schemata discussed above.
4.2 Using a Domain-Specific Lexicon
Another strategy we employed for lexical adaptation was the use of a domain-
specific dictionary. For biomedicine, such a dictionary is available from the Na-
tional Library of Medicine: the Unified Medical Language System (UMLS) SPE-
CIALIST lexicon [19]. Covering both general English as well as biomedical vo-
cabulary, the SPECIALIST lexicon contains over 415,000 entries (including or-
thographic and morphological variants). Entries are also assigned one of eleven
POS categories specified as part of the lexicon.
Given our finding from Section 4.1 that even oracle POS tags would do little
to improve upon our re-trained POS tagger, we did not make use of lexicon POS
tags. Instead, we restricted our use of the lexicon to extracting collocations. We
then added a hard-constraint to the parser that these collocations could not be
cross-bracketed and that each collocation must represent a flat phrase with no
internal sub-constituents. This approach was motivated by a couple of observa-
tions. On one hand, we observed cases where the parser would be confused by
long compound nouns; in desperation to find the start of a verb phrase, it would
sometimes use part of the compound to head a new verb phrase. Unfortunately,
WSJ sections 2-21 contain approximately 500 verb phrases headed by present-
participle verbs mistagged as nouns, thus making this bizarre bracketing rule
statistically viable. A second observation was the frequency with which we saw
the terms ?in vivo? and ?in vitro? (treebanked as foreign adjverbial or adjecti-
val collocations) mis-analyzed. Even in biomedical texts, ?in? appears far more
often as a preposition than as part of such collocations, and as such, is almost
always mis-parsed in these collocational contexts to head a prepositional phrase.
Our hope was that by preventing such collocations from being cross-bracketted,
we could prevent this class of parsing mistakes.
We found use of lexical collocations did yield a small (0.3%) but statistically
significant improvement in performance over the unmodified parser (Table 5).
However, when combined with either POS or entity adaptations, the lexicon?s
impact on parsing accuracy was statistically insignificant. Our interpretation of
this latter result is that the primary limitation of the lexicon is coverage, despite
its size. That is, when either of the other adaptations were used, the lexicon
did not offer much beyond them. It is not surprising that oracle-knowledge of
entities (Section 4.3) provided greater coverage than the generic dictionary, and
the improvement in tagging from POS adaptation (sharper tag probabilities)
helped somewhat in preventing the verb-ification of some of the long compound
nouns. While the lexicon was the only adaptation to correctly fix ?in vivo? type
mistakes, these phrases alone were not sufficiently frequent to provide a statisti-
cally significant improvement in parse accuracy on top of other adaptations. As
such, the primary value of this method would be in cases where such a lexicon
is available but POS tags and labelled entities are not.
66 M. Lease and E. Charniak
4.3 Using Named-Entities
The primary focus of the GENIA corpus is to support training and evaluation of
automatic named-entity recognition. As such, a variety of biologically meaningful
terms have been annotated in the corpus according to a corpus-defined ontology.
Given the availability of these annotations, we were interested in considering
the extent to which they could be used as a source of lexical information for
parser adaptation.
Given the problems described earlier with regard to lexical collocations being
cross-bracketted by our off-the-shelf PTB-trained parser (Section 4.2), our hope
was that named-entities could be used similarly to lexical collocations in helping
to prevent this class of mistakes. To put it another way, we hoped to exploit
the correlation between named-entities and noun phrase (NP) boundaries. A
common preprocessing step in detecting named-entities is to use a chunker to
find NPs. Our approach was to do the reverse: to use named-entities as a feature
for finding NP boundaries.
Our initial plan was to use the same strategy we had used with dictionary
collocations: to add a hard-constraint to the parser that a named-entity could
not be cross-bracketed and had to represent a flat phrase with no internal sub-
constituents. However, we found upon closer inspection that the entities often
did contain substructure (primarily parenthetical acronyms), and so we relaxed
the flat-constituent constraint and enforced only the cross-bracketing constraint.
As a preliminary step, we evaluated the utility of this method using oracle-
knowledge of named-entities. By itself, this method was roughly equivalent to POS
re-training in improving parsing accuracy from78.3%to 80.9%(Table 5).Butwhen
combined with POS adaptations, use of named-entities provided another signifi-
cant improvement in performance, from81.5%to 82.9%.Clearly this is a promising
avenue for further work, and it will be interesting to see how much of this benefit
from the oracle case can be realized when using automatically detected entities.
5 Discussion
We have found only limited use of parsing reported to date for biomedical liter-
ature, thus it is difficult to compare our parsing results against previous work in
parsing this domain. To the best of our knowledge, only one other wide-coverage
parser has been applied to biomedical literature: Grover et al report 99% cov-
erage using a hand-written grammar with a statistical ranking component [20].
We do not know of any quantitative accuracy figures reported for this domain
other than those described here.
For those interested in mining the biomedical literature, the next important
step will be assessing the utility of PTB-style parsing compared to other pars-
ing models that have been employed for information extraction. There has been
promising work in using PTB-style parses for information extraction by inducing
predicate-argument structures from the output parses [21]. It will be interesting to
see for the biomedical domain how these predicate-argument structures compare
to those induced by other grammar formalisms currently in use, such as HPSG [22].
Parsing Biomedical Literature 67
The next immediate extension of our work is to evaluate use of detected
named-entities in place of the oracle case described in Section 4.3, replacing the
current hard-constraint with a soft-constraint confidence term to be incorporated
into the parser?s generative model. Performance of named-entity recognition on
GENIA was recently studied as part of a shared task at BioNLP/NLPBA 2004.
The best system achieved 72.6% f-measure [23], though note that this task re-
quired both detection and classification of named-entities. As our usage of en-
tities does not require classification, this number should be considered a lower-
bound in the context of our usage model. We expect this level of accuracy should
be sufficient to improve parse scores, though how much of the oracle benefit we
can realize remains to be seen.
There are also interesting POS issues meriting further investigation. As dis-
cussed in Section 4.1, we would like to find a better solution to the lack of
proper noun annotations in GENIA, perhaps by detecting proper nouns using
determiners and labelled entities. More careful analysis of the differences be-
tween the PTB and GENIA tagging schemata is also needed. Additionally, there
are interesting issues regarding how POS tags are used by the parsing model.
Whereas the Collins? model [5] treats POS tagging as an external preprocessing
step (a single best tag is input to the parsing model), the Charniak model [3]
generates tag hypotheses as part of its combined generative model, and thus
considers multiple hypotheses in searching for the best parse. The significance
of this is that other components of the generative model can influence tag se-
lection, and Charniak has reported adding this feature to his simulated version
of the Collins model improved its accuracy by 0.6% [24]. However, this result
was for in-domain evaluation; the picture becomes more complicated when we
begin parsing out-of-domain. If we have an in-domain trained POS-tagger, we
might not want a combined model trained on out-of-domain data overruling our
tagger?s predictions. One option may be introducing a weighting factor into the
generative model to indicate the degree of confidence assigned to our tagger
relative to the other components of the combined model.
Another issue for further work is the parsing of paper titles. In the GENIA
development section, only 28% of the titles are sentences whereas 71% are noun
phrases. This distribution is radically different than the rest of the corpus, which
is heavily dominated by sentence-type utterances. As headlines are even more
rare in our WSJ training data than titles are in GENIA (since WSJ contains
full article text), our parser performs miserably at utterance-type detection (i.e.
correctly labelling the top-most node in the parse tree): 58.6%. Correspondingly,
parse accuracy on titles is only 69.1%, which represents a statistically significant
decrease in accuracy in comparison to the entire development section (p = 0.038).
In investigating this, we noticed an oddity in GENIA in that most titles were
encoded in the corpus with an ending period that did not exist in the original
papers the corpus was derived from. By removing these periods, we improved
utterance-type detection to 77.9%. While parse accuracy rose to 72.0%, this
was statistically insignificant (p = 0.082). The solution we would like to move
towards is to respect the legitimate distributional differences between title and
68 M. Lease and E. Charniak
non-title utterances and parameterize the parser differently for the two cases.
Generally speaking, such ?contextual parsing? might allow us to improve parsing
accuracy more widely by parameterizing our parser differently based on where
the current utterance fits in the larger discourse. This example of period usage
in titles also highlights a broader issue that seemingly innocuous issues in corpus
preparation can have significant impact when parsing. As a further example of
this, the choice to (at times) separately tokenize term-embedded parentheses in
GENIA creates unnecessary attachment ambiguity in the resulting parenthetical
phrases. For example, in the phrase ?C3a and C3a(desArg)?, ?C3a(desArg)? is
tokenized as ?C3a ( desArg )?, which produces ambiguity as to whether the
parenthetical should attach low (to the latter ?C3a?) or high (to the compound
?C3a and C3a?). Issues such as these remind us to be mindful of the relationship
between corpus preparation and parsing, as well as downstream processing, and
that some issues which appear difficult to resolve while parsing might be handled
more easily at another stage in the processing pipeline.
We view biomedical and other technical texts as providing an interesting set
of challenges and questions for future parsing research. An interesting introduc-
tion to some of these challenges, supported by examples drawn from the domain,
can be found in [25]. A significant question for consideration is the degree to
which these challenges are related to domain knowledge vs. stylistic norms of
the genre. For example, [2] reports that whereas POS determination required
domain expertise, prepositional phrase (PP)-attachment could be largely deter-
mined even by non-biologists. Our own treebanking experience left us with the
opposite impression. For example, in the phrase ?gene expression and protein
secretion of IL-6?, should the PP attach high (IL-6 gene expression and protein
secretion) or low (gene expression and IL-6 protein secretion)? Domain knowl-
edge appears to be necessary here for correct resolution. In contrast to this, POS
tags appear to be a distributional rather than a semantic concern. Issues like
this highlight how little we really understand currently about the parameters
of corpus variation. How do the frequencies of different syntactic constructions
vary by genre, and are there key structural variations at work? How do we ef-
fectively adapt parsers in response? These issues remain important topics for
future investigation.
References
1. Kim, J.d., Ohta, T., Tateisi, Y., Tsujii, J.: Genia corpus - a semantically annotated
corpus for bio-textmining. Bioinformatics (Supplement: Eleventh International
Conference on Intelligent Systems for Molecular Biology) 19 (2003) i180?i182
2. Tateisi, Y., Ohta, T., dong Kim, J., Hong, H., Jian, S., Tsujii, J.: The genia corpus:
Medline abstracts annotated with linguistic information. In: Third meeting of SIG
on Text Mining, Intelligent Systems for Molecular Biology (ISMB). (2003)
3. Charniak, E.: A maximum-entropy-inspired parser. In: Proc. NAACL. (2000)
132?139
4. Marcus, M., Santorini, B., Marcinkiewicz, M.A.: Building a large annotated corpus
of English: The Penn Treebank. Computational Linguistics 19 (1993) 313?330
Parsing Biomedical Literature 69
5. Collins, M.: Discriminative reranking for natural language parsing. In: Proc. ICML.
(2000) 175?182
6. Ratnaparkhi, A.: Learning to parse natural language with maximum entropy mod-
els. Machine Learning 34 (1999) 151?175
7. Gildea, D.: Corpus variation and parser performance. In: Proceedings of the 2001
Conference on Empirical Methods in Natural Language Processing. (2001) 167?202
8. Roark, B., Bacchiani, M.: Supervised and unsupervised pcfg adaptation to novel
domains. In: Proceedings of HLT-NAACL. (2003) 205?212
9. Steedman, M., Hwa, R., Clark, S., Osborne, M., Sarkar, A., Hockenmaier, J.,
Ruhlen, P., Baker, S., Crim, J.: Example selection for bootstrapping statistical
parsers. In: Proceedings of HLT-NAACL. (2003) 331?338
10. de Bruijn, B., Martin, J.: Literature mining in molecular biology. In: Proceedings
of the European Federation for Medical Informatics (EFMI) Workshop on Natural
Language Processing in Biomedical Applications. (2002)
11. Hirschman, L., Park, J., Tsujii, J., Wong, L., Wu, C.: Accomplishments and chal-
lenges in literature data mining for biology. Bioinformatics 18 (2002) 1553?1561
12. Yakushiji, A., Tateisi, Y., Miyao, Y., Tsujii, J.: Event extraction from biomedical
papers using a full parser. In: Pacific Symposium on Biocomputing. (2001) 408?419
13. Daraselia, N., Yuryev, A., Egorov, S., Novichkova, S., Nikitin, A., Mazo, I.: Extract-
ing human protein interactions from medline using a full-sentence parser. Bioin-
formatics 20 (2004) 604?611
14. Shatkay, H., Feldman, R.: Mining the biomedical literature in the genomic era: An
overview. Journal of Computational Biology 10 (2003) 821?855
15. Hwa, R.: Learning Probabilistic Lexicalized Grammars for Natural Language Pro-
cessing. PhD thesis, Harvard University (2001)
16. Bies, A., Ferguson, M., Katz, K., MacIntyre, R.: Bracketting Guideliness for Tree-
bank II style Penn Treebank Project. Linguistic Data Consortium. (1995)
17. Buckley, C.: Implementation of the smart information retrieval system. Technical
Report 85-686, Cornell University (1985)
18. Goodman, J.: Parsing inside-out. PhD thesis, Harvard University (1998)
19. McCray, A.T., Srinivasan, S., Browne, A.C.: Lexical methods for managing varia-
tion in biomedical terminologies. In: Proceedings of the 18th Annual Symposium
on Computer Applications in Medical Care (SCAMC). (1994) 235?239
20. Grover, C., Lapata, M., Lascarides, A.: A comparison of parsing technologies for
the biomedical domain. Journal of Natural Language Engineering (2002)
21. Surdeanu, M., Harabagiu, S., Williams, J., Aarseth, P.: Using predicate-argument
structures for information extraction. In: Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics (ACL-03). (2003) 8?15
22. Miyao, Y., Ninomiya, T., Tsujii, J.: Corpus-oriented grammar development for
acquiring a head-driven phrase structure grammar from the penn treebank. In:
Proc. of IJCNLP-04. (2004) 684?693
23. Zhou, G., Su, J.: Exploring deep knowledge resources in biomedical name recog-
nition. In: Proceedings of the Joint Workshop on Natural Language Processing in
Biomedicine and its Applications (JNLPBA-04). (2004)
24. Charniak, E.: Statistical parsing with a context-free grammar and word statistics.
In: Proceedings of the Fourteenth National Conference on Artificial Intelligence,
Menlo Park, AAAI Press/MIT Press (1997)
25. Park, J.C.: Using combinatory categorical grammar to extract biomedical infor-
mation. IEEE Intelligent Systems 16 (2001) 62?67
Unsupervised Learning of Name Structure
From Coreference Data?
Eugene Charniak
Brown Laboratory for Linguistic Information Processing
Department of Computer Science
Brown University, Box 1910, Providence, RI
ec@cs.brown.edu
Abstract
We present two methods for learning the struc-
ture of personal names from unlabeled data.
The rst simply uses a few implicit constraints
governing this structure to gain a toehold on the
problem | e.g., descriptors come before rst
names, which come before middle names, etc.
The second model also uses possible coreference
information. We found that coreference con-
straints on names improve the performance of
the model from 92.6% to 97.0%. We are in-
terested in this problem in its own right, but
also as a possible way to improve named entity
recognition (by recognizing the structure of dif-
ferent kinds of names) and as a way to improve
noun-phrase coreference determination.
1 Introduction
We present two methods for the unsupervised
learning of the structure of personal names as
found in Wall Street Journal text. More specif-
ically, we consider a \name" to be a sequence of
proper nouns from a single noun-phrase (as in-
dicated by Penn treebank-style parse trees). For
example, \Defense Secretary George W. Smith"
would be a name and we would analyze it into
the components \Defense Secretary" (a descrip-
tor), \George" (a rst name), \W." (a middle
name, we do not distinguish between initials
and \true" names), and \Smith" (a last name).
We consider two unsupervised models for
learning this information. The rst simply uses
a few implicit constraints governing this struc-
ture to gain a toehold on the problem | e.g.,
descriptors come before rst names, which come
? This research was supported in part by NSF grant
LIS SBR 9720368. The author would like to thank Mark
Johnson and the rest of the Brown Laboratory for Lin-
guistic Information Processing (BLLIP) for general ad-
vice and encouragement.
before middle names, etc. We henceforth call
this the \name" model. The second model also
uses possible coreference information. Typically
the same individual is mentioned several times
in the same article (e.g., we might later en-
counter \Mr. Smith"), and the pattern of such
references, and the mutual constraints among
them, could very well help our unsupervised
methods determine the correct structure. We
call this the \coreference" model. We were at-
tracted to this second model as it might oer a
small example of how semantic information like
coreference could help in learning structural in-
formation.
To the best of our knowledge there has not
been any previous work on learning personal
structure. We are aware of one previous case
of unsupervised learning of lexical information
from possible coreference, namely that of Ge et.
al. [5] where possible pronoun coreference was
used to learn the gender of nouns. In this case a
program with an approximately 65% accuracy
in determining the correct antecedent was used
to collect information on pronouns and their
possible antecedents. The gender of the pro-
noun was then used to suggest the gender of
the noun-phrase that was proposed as the an-
tecedent. The current work is quite dierent
in both goal and methods, but similar in spirit.
More generally this work is part of a growing
body of work on learning language-related in-
formation from unlabeled corpora [1,2,3,8,9,10,
11].
2 Problem Definition and Data
Preparation
We assume that people?s names have six (op-
tional) components as exemplied in the follow-
ing somewhat contrived example:
Word Label Label Number
Defense descriptor 0
Secretary descriptor 0
Mr. honorific 1
John first-name 2
W. middle-name 3
Smith last-name 4
Jr. close 5
Our models make the following assumptions
about personal names:
? all words of label l (the label number) must
occur before all words of label l + 1
? with the exception of descriptors, a maxi-
mum of one word may appear for each label
? every name must include either a rst name
or a last name
? in a loose sense, honorifics and closes are
\closed classes", even if we do not know
which words are in the classes. We im-
plement this by requiring that words given
these labels must appear in our dictionary
only as proper names, and that they must
have appeared at least three times in the
training set used for the lexicon (sections
2-21 of the Penn Wall Street Journal tree-
bank)
Section 5 discusses these constraints and as-
sumptions.
The input to the name model is a noisy list
of personal names. This list is approximately
85% correct; that is, about 15% of the word
sequences are not personal names, but rather
non-names, or the names of other types of en-
tities. We obtained these names by running a
program inspired by that of Collins and Singer
[4] for unsupervised learning of named entity
recognition. This program takes as input pos-
sible names plus contextual information about
their occurrences. It then categorizes each name
as one of person, place, or organization. A possi-
ble name is considered to be a sequence of one
or more proper nouns immediately dominated
by a noun-phrase where the last of the proper
nouns is the head (rightmost noun) of the noun
phrase. We used as input to this program the
parsed text found in the BLLIP WSJ 1987-89
WSJ Corpus | Release 1 [6]. Because of a mi-
nor error, the parser used in producing this cor-
pus had a unwarranted propensity to label un-
capitalized words as proper nouns. To correct
for this we only allowed capitalized words to be
considered proper nouns. In section 5 we note
an unintended consequence of this decision.
The coreference model for our tasks is also
given a list of all personal names (as de-
ned above) in each Wall Street Journal arti-
cle. Although the BLLIP corpus has machine-
generated coreference markers, these are ig-
nored.
The output of both programs is an assign-
ment from each name to a sequence of labels,
one for each word in the name. Performance is
measured by the percent of words labeled cor-
rectly and percent of names for which all of the
labels are correct.
3 The Probability Models
We now consider the probability models that
underlie our learning mechanisms. Both models
are generative in that they assign probabilities
to all possible labelings of the names. (For the
coreference model the model generates all pos-
sible labelings given the proposed antecedent.)
Let ~l be a sequence of label assignments to the
name ~n (a sequence of words). For the name
model we estimate
arg max
~
l
p(~l | ~n) = arg max
~
l
p(~l, ~n) (1)
We estimate this latter probability by assum-
ing that the number of words assigned label l,
n(l), is independent of which other labels have
appeared. Our assumptions imply that with the
exception of descriptor, all labels may occur zero
or one times. We arbitrarily assume that there
may be zero to fourteen descriptors. We then
assume that the words in the name are inde-
pendent of one another given their label. Thus
we get the following equation:
p(~l, ~n) =
?
l=0,5
p(N (l) = n(l))
?
i=0,n(l)
p(w(i) | l)
(2)
Here w(i) is the ith word from ~n assigned the
label l in ~l and N (l) is a random variable whose
value is the number of words in the name with
label l. To put this slightly dierently, we rst
guess the number of words with each label l
according to the distribution p(N (l) = n(l)).
Given the ordering constraints, this completely
determines which words in ~n get which label.
We then guess each of the words according to
the distribution p(w(i) | l). The name model
does not use information concerning how often
each name occurs. (That is, it implicitly as-
sumes that all names occur equally often.)
We have also considered somewhat more com-
plex approximations to p(~l, ~n). See section 5.
The coreference model is more complicated.
Here we estimate
arg max
~
l
p(~l | ~n,~c) = arg max
~
l
p(~l, ~n | ~c) (3)
That is, for each name the program identies
zero or one possible antecedent name. It does
this using a very crude lter. The last word
of the proposed antecedent (unless that word is
\Jr.", in which case it looks at the second to last
word) must also appear in ~n as well. If no such
name exists, then ~c = 4 and we estimate the
distribution according to equation 2. If more
than one such name exists, we choose the rst
one appearing in the article.
Even if there is such a name, the program
does not assume that the two names are, in fact,
coreferent. Rather, a hidden random variable R
determines how the two names relate. There are
three possibilities:
? ~c is not coreferent (R = 4), in which case
the probability is estimated according to
the ~c = 4 case.
? ~c is not coreferent but is a member of
the same family as ~n (e.g., \John Sebas-
tian Bach" and \Carl Philipp Emmanuel
Bach"). This case (R = f) is computed
as the non-coreference case, but the sec-
ond occurrence is given \credit" for the last
name.
? ~c is coreferent to ~n, in which case we com-
pute the probability as described below. In
this case we assume that any words shared
by the two names must appear with the
same label, and except for descriptions, la-
bels may not change between them (e.g.,
if ~c has a rst name, then ~n can be given
a rst name only if it is the same word as
that in ~c). This does not allow for nick-
names and other such cases, but they are
rare in the Wall Street Journal.
More formally, we have
p(~n,~l | ~c) =
?
R
p(R)p(~n,~l | R,~c) (4)
We then estimate p(~n,~l | R,~c) as follows:
? if R = 4, compute p(~n,~l) as estimated
from equation 2.
? if R = f , then p(~n,~l)/p(s | ~l(s)) where s is
the word in common that caused the previ-
ous name to be selected as a possible coref-
erent and ~l(s) is the label assigned to s ac-
cording to ~l.
? if R = ~c, use equation 8 below.
In equation 4 the R = 4 case is reasonably
straight-forward: we simply use equation 2 as
the non-coreferent distribution. For R = f , as
we noted earlier, we want to claim that the new
name is a member of the same family as that of
the earlier name. Thus, as we said earlier, we
get \credit" for the repeated family name. This
is why we take the non-coreferent probability
and divide by the probability of what we take
to be the family name.
This leaves the coreferent case. The basic
idea is that we view the labeling of new name
(~l) as a transformation of the labeling of the
old one (~l0). However, we do not know ~l0 so we
have to sum over all possible former labelings
L
?
. This is expressed as
p(~n,~l | ~c) =
?
~
l
?
2L
?
p(~l0 | ~c)p(~n,~l | ~l0,~c) (5)
The rst term, p(~l0 | ~c), is easy to compute
from equation 2 using Bayes law. We now turn
our attention to the second term.
To establish a more detailed relationship be-
tween the old and new names we compute pos-
sible correspondences between the two names,
where a correspondence species for each word
in the old name if it is retained in the new name,
and if so, the word in the new name to which it
corresponds. Two words may correspond only
if they are the same lexical item. (The converse
does not hold.) Since in principle there can be
multiple correspondences, we introduce the cor-
respondences ? by summing the probability over
all of them:
p(~n,~l | ~l0,~c) =
?
?
p(~n,~l, ? | ~l0,~c) (6)
? max
?
p(~n,~l, ? | ~l0,~c) (7)
In the second equation we simplify by making
the assumption that the sum will be dominated
by one of the correspondences, a very good as-
sumption. Furthermore, as is intuitively plau-
sible, one can identify the maximum ? with-
out actually computing the probabilities: it is
the ? with the maximum number of words re-
tained from ~c. Henceforth we use ? to denote
this maximum-probability correspondence.
By specifying ? we divide the words of the
old name into two groups, those (R) that are
retained in the new name and those (S) that are
subtracted when going to the new name. Simi-
larly we have divided the words of the new name
into two classes, those retained and those added
(A). We then assume that the probability of a
word being subtracted or retained is indepen-
dent of the word and depends only on its label
(e.g., the probability of a subtraction given the
label l is p(s | l)). Furthermore, we assume that
the labels of words in R do not change between
~l0 and ~l. Once we have pinned down R and S,
any words left in ~l must be added. However,
we do not yet \know" the labels of those, so
we need a probability term p(l | a). Lastly, for
words that are added in the new name, we need
to guess the particular word corresponding to
the label type. This gives us the following dis-
tribution:
p(~n,~l, ? | ~c,~l0) =
?
w2S
p(s | ~l0(w))
?
w2R
p(r | ~l0(w))
?
w2A
p(l | a)
p(w | l) (8)
Taken together, equations 2 | 8 dene our
probability model.
4 Experiments
From the work on named entity recognition we
obtained a list of 145,670 names, of which 87,809
were marked as personal names. A second pro-
gram creates an ordered list of names that ap-
pear in each article in the corpus. The two les,
names and article-name occurrences, are the in-
put to our procedures.
With one exception, all the probabilities re-
quired by the two models are initialized with
flat distributions | i.e., if a random variable
can take n possible values, each value is 1/n.
The probabilites so set are:
1. p(N (l) = n(l)) from equation 2 (the prob-
ability that label l appears n(l) times),
2. p(w(i) | l) from equation 2 (the probability
of generating w(i) given it has label l),
3. p(s | ~l0(w)), p(r | ~l0(w)), and p(a | ~l(w))
from equation 8, the probabilities that a
the label ~l(w) will be subtracted, retained,
or added when going from the old name to
the new name.
We then used the expectation-maximization
(EM) algorithm to re-estimate the values. We
initially decided to run EM for 100 iterations as
our benchmark. In practice no change in perfor-
mance was observed after about 15 iterations.
The one exception to the flat probability dis-
tribution rule is the probability distribution
p(R), the probability of an antecedent being
coreferent, a family relation, or non-coreferent.
This distribution was set at .993, .002, and .005
respectively for the three alternatives and the
values were not re-estimated by EM.1 Figure
1 show some of the probabilities for individual
words given the possible labels.
The result shown in Figure 1 are basically
correct, with \Director" having a high proba-
bility as a descriptor, (0.0059), \Ms." having a
high probability as honorific (0.058), etc. Some
of the small non-zero probabilities are due to
genuine ambiguity (e.g., Fisher does occur as a
rst name as well as a last name) but more of
it is due to small confusions in particular cases
(e.g., \Director" as a last-name, or \John" as
descriptor).
After EM training we evaluated the program
on 309 personal names from our names list that
we had annotated by hand. These names were
obtained by random selection of names labeled
1These were the first values we tried and, as they
worked satisfactorily, we simply left them alone.
Word p(w | 0) p(w | 1) p(w | 2) p(w | 3) p(w | 4) p(w | 5)
Director 0.0059 0 2.0 10?5 0 0.00016 0
Ms. 1.2 10?7 0.058 0.0041 2.9 10?14 0 0
John 0.0037 2.9 10?6 0.038 9.7 10?6 0 0
T. 0.0018 1.2 10?12 .00032 0.02 0 0
Fisher 0 0 4.5 10?5 7.4 10?5 0.00073 0
III 0 0 0 0 6.8 10?5 0.16
Figure 1: Some example probabilities of words given labels after 15 EM iterations
as personal names by the name-entity recog-
nizer. If the named entity recognizer had mis-
takenly classied something as a personal name
it was not used in our test data.
For the name model we straightforwardly
used equation 2 to determine the most probable
label sequence ~l for each name. Note, however,
that the testing data does not itself include any
information on whether or not the test name
was a rst or subsequent occurrence of an indi-
vidual in the text. To evaluate the coreference
model we looked at the possible coreference data
to nd if the test-data name was most common
as a rst occurrence, or if not, which possible
antecedent was the most common. If rst occur-
rence prevailed, ~l was determined from equation
2, and otherwise it was determined using equa-
tion 3 with ~c set to the most common possible
coreferent for this name.
We compare the most probable labels ~l for a
test example with the hand-labeled test data.
We report percentage of words that are given
the correct label and percentage of names that
are completely correct. The results of our ex-
periments are as follows:
Model Label% Name%
Name 92.6 85.1
Coreference 97.0 94.5
As can be seen, information about possible
coreference was a decided help in this task, lead-
ing to an error reduction of 59% for the number
of labels correct and 63% for names correct.
5 Error Analysis
The errors tend to arise from three situations:
the name disobeys the name structure assump-
tions upon which the program is based, the
name is anomalous in some way, or sparse data.
We consider each of these in turn.
Many of the names we encounter do not obey
our assumptions. Probably the most common
situation is last names that, contrary to our as-
sumption, are composed of more than word e.g.,
\Van Dam". Actually, a detail of our process-
ing has caused this case to be under-represented
in our data and testing examples. As noted in
Section 2, uncapitalized proper nouns were not
allowed. The most common extra last name is
probably \van," but all of these names were ei-
ther truncated or ignored because of our pro-
cessing step.
In principle, it should be possible to allow for
multiple last names, or alternatively have a new
label for \rst of two last names". In practice,
it is of course the case that the more parameters
we give EM to ddle with, the more mischief it
can get into. However, for a practical program
this is probably the most important extension
we envision.
Names may be anomalous while obeying our
restrictions at least in the letter if not the spirit.
Chinese names have something very much like
the rst-middle-last name structure we assume,
but the family name comes rst. This is partic-
ularly relevant for the coreferent model, since it
will be the family name that is repeated. There
is nothing in our model that prevents this, but
it is suciently rare that the program gets \con-
fused". In a similar way, we marked both \Dr."
and \Sir" as honorics in our test data. How-
ever, the Wall Street Journal treats them very
dierently from \Mr." in that the former tend
to be included even in the rst mention of a
name, while the latter is not. Thus in some
cases our program labeled \Dr." and \Sir" as
descriptors.
Lastly, there are situations where we imag-
ine that if the program had more data (or if the
learning mechanisms were somehow \better") it
would get the example right. For example, the
name \Mikio Suzuki" appears only once in our
corpus, as does the word \Mikio". \Suzuki"
appears two times, the rst being in \Yotaro
Suzuki" who is mentioned earlier in the same
article. Unfortunately, because \Mikio" does
not appear elsewhere, the program is at a loss
to decide which label to give it. However, be-
cause Yotaro is assumed to be a rst name, the
program makes \Mikio Suzuki" coreferent with
\Yotaro Suzuki" by labeling \Mikio" descriptor.
As noted briefly in section 3, we have consid-
ered more complicated probabilities models to
replace equation 2. The most obvious of these is
to allow the distribution over numbers of words
for each label to be conditioned on the previ-
ous label | e.g., a bi-label model. This model
generally performed poorly, although the coref-
erence versions often performed as well as the
coreference model reported here. Our hypoth-
esis is that we are seeing problems similar to
those that have bedeviled applying EM to tasks
like part-of-speech tagging [7]. In such cases EM
typically tries to lower probabilities of the cor-
pus by using the tags to encode common word-
word combinations. As the models correspond-
ing to equations 2 and 8 do not include any
label-label probabilities, this problem does not
appear in these models.
6 Other Applications
It is probably clear to most readers that the
structure and probabilities learned by these
models, particularly the coreferent model, could
be used for tasks other than assigning structure
to names. For starters, we would imagine that
a named entity recognition program that used
information about name structure could do a
better job. The named entity recognition pro-
gram used to create the input looks at only a
few features of the context in which the name
appears, the complete name, and the individ-
ual words that appear in the name irrespective
of the other words. Since the dierent kinds
of names (person, company and location) dier
in structure from one another, a program that
simultaneously establishes both structure and
type would have an extra source of information,
thus enabling it to do a better job.
Our name-structure coreferent model is also
learning a lot of information that would be use-
ful for a program whose primary purpose is to
detect coreference. One way to see this is to
look at some of the probabilities that the pro-
gram learned. Consider the probability that we
will have an honorific in a rst occurrence of a
name:
p(n(1) = 1) = .000044 (9)
This is very low. Contrast this with the prob-
ability that we add an honorific in the second
occurrence:
p(a | honorific) = 1 (10)
These dramatic probabilities are not, in fact,
accurate, as EM tends to exaggerate the eects
by moving words that do not obey the trend out
of the honorific category. They are, however, in-
dicative of the fact that in the Wall Street Jour-
nal names are introduced without honorics,
but subsequent occurrences tend to have them
(a fact we were not aware of at the start of this
research).
Another way to suggest the usefulness of
this research for coreference and named-entity
recognition is to consider the cases where our
program?s crude lter suggests a possible an-
tecedent, but the probabilistic model of equa-
tion 4 rejects this analysis. The rst 15 cases
are given in gure 2. As can be seen, except for
\Mr. President" and \President Reagan", all
of the examples are either not coreferent or are
not people at all.
7 Conclusion
We have presented two methods for the un-
supervised discovery of personal-name struc-
ture. The two methods dier in that the second
uses multiple, possibly coreferent, occurrences
of names to constrain the problem. The meth-
ods perform at a level of 92.6 and 97.0 percent
accuracy respectively.
The methods are of potential interest in their
own right, e.g., to improve the level of detail
provided by Penn Treebank style parses. As
we have also noted, we should also be able to
use this research in the quest for better unsu-
pervised learning of named-entity recognition,
and the model that attends to coreference in-
formation can potentially be useful for programs
aimed directly at this latter problem.
Finally, many of us believe that the power
of unsupervised learning methods for linguistic
Vice President President Reagan
Mr. President President Reagan
Dean P. Guerin Guerin & Turner
Ronald Reagan White House Speaker
House James Wright
Rev. Leon H. Sullivan Sullivan Principles
Mr. Sullivan Sullivan Principles
Rev. Leon Sullivan Sullivan Principles
South Dakota Republican Party
Republican
Kansas Republican Republican Party
Cyril Wagner Jr. Wagner & Brown
General Preston R. Laurence A. Tisch
Tisch
Lt. Col. Oliver Whitney North
North Seymour Jr.
Republican White House Republicans
House
J. Seward Johnson Johnson & Johnson
Vice President President Nixon
Figure 2: The rst 15 cases in which the coref-
erent model rejected the coreferent hypothesis
information will be proportional to the depth of
semantic or pragmatic analysis that goes into
the features they consider. The vastly superior
performance of the coreference model over the
basic name model moves this believe some small
distance from hope to hypothesis.
References
1. Berland, M. and Charniak, E. Finding
parts in very large corpora. In Proceedings
of the ACL 1999. ACL, New Brunswick NJ,
1999, 57{64.
2. Brill, E. Unsupervised learning of disam-
biguation rules for part of speech tagging. In
Proceedings of the Third Workshop on Very
Large Corpora. 1995, 1{13.
3. Caraballo, S. A. Automatic construction
of a hypernym-labeled noun hierarchy from
text. In Proceedings of the ACL 1999. ACL,
New Brunswick NJ, 1999.
4. Collins, M. and Singer, Y. Unsuper-
vised models for named entity classification.
In Proceedings of the 1999 Joint Sigdat Con-
ference on Empirical Methods in Natural
Language Processing and Very Large Cor-
pora. Association for Computational Linguis-
tics, 1999.
5. Ge, N., Hale, J. and Charniak, E. A
statistical approach to anaphora resolution.
In Proceedings of the Sixth Workshop on
Very Large Corpora. 1998, 161{171.
6. Linguistic Data Consortium. BLLIP
1987-1989 WSJ Corpus Release 1. 2000.
7. Merialdo, B. Tagging English text with a
probabilistic model. Computational Linguis-
tics 20 (1994), 155{172.
8. Riloff, E. Automatically generating ex-
traction patterns from untagged text. In
Proceedings of the Thirteenth National
Conference on Artificial Intelligence. AAAI
Press/MIT Press, Menlo Park, 1996, 1044{
1049.
9. Riloff, E. and Shepherd, J. A corpus-
based approach for building semantic lexi-
cons. In Proceedings of the Second Confer-
ence on Empirical Methods in Natural Lan-
guage Processing. 1997, 117{124.
10. Roark, B. and Charniak, E. Noun-
phrase co-occurrence statistics for semi-
automatic semantic lexicon construction. In
36th Annual Meeting of the Association for
Computational Linguistics and 17th Interna-
tional Conference on Computational Linguis-
tics. 1998, 1110{1116.
11. Yarowsky, D. Unsupervised word sense
disambiguation rivaling supervised methods.
In Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguis-
tics. 1995, 189{196.
Edit Detection and Parsing for Transcribed Speech
Eugene Charniak and Mark Johnson
Deparments of Computer Science and Cognitive and Linguistic Sciences
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University, Providence, RI 02912
ec,mj@cs.brown.edu 
Abstract
We present a simple architecture for parsing
transcribed speech in which an edited-word de-
tector rst removes such words from the sen-
tence string, and then a standard statistical
parser trained on transcribed speech parses the
remaining words. The edit detector achieves a
misclassication rate on edited words of 2.2%.
(The NULL-model, which marks everything as
not edited, has an error rate of 5.9%.) To evalu-
ate our parsing results we introduce a new eval-
uation metric, the purpose of which is to make
evaluation of a parse tree relatively indierent
to the exact tree position of EDITED nodes. By
this metric the parser achieves 85.3% precision
and 86.5% recall.
1 Introduction
While signicant eort has been expended on
the parsing of written text, parsing speech
has received relatively little attention. The
comparative neglect of speech (or transcribed
speech) is understandable, since parsing tran-
scribed speech presents several problems absent
in regular text: \um"s and \ah"s (or more
formally, lled pauses), frequent use of par-
entheticals (e.g., \you know"), ungrammatical
constructions, and speech repairs (e.g., \Why
didn?t he, why didn?t she stay home?").
In this paper we present and evaluate a simple
two-pass architecture for handling the problems
of parsing transcribed speech. The rst pass
tries to identify which of the words in the string
are edited (\why didn?t he," in the above exam-
ple). These words are removed from the string
given to the second pass, an already existing sta-
tistical parser trained on a transcribed speech
? This research was supported in part by NSF grant LIS
SBR 9720368 and by NSF ITR grant 20100203.
corpus. (In particular, all of the research in this
paper was performed on the parsed \Switch-
board" corpus as provided by the Linguistic
Data Consortium.)
This architecture is based upon a fundamen-
tal assumption: that the semantic and prag-
matic content of an utterance is based solely
on the unedited words in the word sequence.
This assumption is not completely true. For
example, Core and Schubert [8] point to coun-
terexamples such as \have the engine take the
oranges to Elmira, um, I mean, take them to
Corning" where the antecedent of \them" is
found in the EDITED words. However, we be-
lieve that the assumption is so close to true that
the number of errors introduced by this assump-
tion is small compared to the total number of
errors made by the system.
In order to evaluate the parser?s output we
compare it with the gold-standard parse trees.
For this purpose a very simple third pass is
added to the architecture: the hypothesized
edited words are inserted into the parser output
(see Section 3 for details). To the degree that
our fundamental assumption holds, a \real" ap-
plication would ignore this last step.
This architecture has several things to recom-
mend it. First, it allows us to treat the editing
problem as a pre-process, keeping the parser un-
changed. Second, the major clues in detecting
edited words in transcribed speech seem to be
relatively shallow phenomena, such as repeated
word and part-of-speech sequences. The kind
of information that a parser would add, e.g.,
the node dominating the EDITED node, seems
much less critical.
Note that of the major problems associated
with transcribed speech, we choose to deal with
only one of them, speech repairs, in a special
fashion. Our reasoning here is based upon what
one might and might not expect from a second-
pass statistical parser. For example, ungram-
maticality in some sense is relative, so if the
training corpus contains the same kind of un-
grammatical examples as the testing corpus,
one would not expect ungrammaticality itself
to be a show stopper. Furthermore, the best
statistical parsers [3,5] do not use grammatical
rules, but rather dene probability distributions
over all possible rules.
Similarly, parentheticals and lled pauses ex-
ist in the newspaper text these parsers currently
handle, albeit at a much lower rate. Thus there
is no particular reason to expect these construc-
tions to have a major impact.1 This leaves
speech repairs as the one major phenomenon
not present in written text that might pose a
major problem for our parser. It is for that rea-
son that we have chosen to handle it separately.
The organization of this paper follows the ar-
chitecture just described. Section 2 describes
the rst pass. We present therein a boosting
model for learning to detect edited nodes (Sec-
tions 2.1 { 2.2) and an evaluation of the model
as a stand-alone edit detector (Section 2.3).
Section 3 describes the parser. Since the parser
is that already reported in [3], this section sim-
ply describes the parsing metrics used (Section
3.1), the details of the experimental setup (Sec-
tion 3.2), and the results (Section 3.3).
2 Identifying EDITED words
The Switchboard corpus annotates disfluencies
such as restarts and repairs using the terminol-
ogy of Shriberg [15]. The disfluencies include
repetitions and substitutions, italicized in (1a)
and (1b) respectively.
(1) a. I really, I really like pizza.
b. Why didn?t he, why didn?t she stay
home?
Restarts and repairs are indicated by disfluency
tags ?[?, ?+? and ?]? in the disfluency POS-tagged
Switchboard corpus, and by EDITED nodes in
the tree-tagged corpus. This section describes
a procedure for automatically identifying words
corrected by a restart or repair, i.e., words that
1Indeed, [17] suggests that filled pauses tend to indi-
cate clause boundaries, and thus may be a help in pars-
ing.
are dominated by an EDITED node in the tree-
tagged corpus.
This method treats the problem of identify-
ing EDITED nodes as a word-token classication
problem, where each word token is classied as
either edited or not. The classier applies to
words only; punctuation inherits the classica-
tion of the preceding word. A linear classier
trained by a greedy boosting algorithm [16] is
used to predict whether a word token is edited.
Our boosting classier is directly based on the
greedy boosting algorithm described by Collins
[7]. This paper contains important implemen-
tation details that are not repeated here. We
chose Collins? algorithm because it oers good
performance and scales to hundreds of thou-
sands of possible feature combinations.
2.1 Boosting estimates of linear
classifiers
This section describes the kinds of linear clas-
siers that the boosting algorithm infers. Ab-
stractly, we regard each word token as an event
characterized by a nite tuple of random vari-
ables
(Y;X1; : : : ;Xm):
Y is the the conditioned variable and ranges
over f?1;+1g, with Y = +1 indicating that
the word is not edited. X1; : : : ;Xm are the con-
ditioning variables; each Xj ranges over a nite
set Xj . For example, X1 is the orthographic
form of the word and X1 is the set of all words
observed in the training section of the corpus.
Our classiers use m = 18 conditioning vari-
ables. The following subsection describes the
conditioning variables in more detail; they in-
clude variables indicating the POS tag of the
preceding word, the tag of the following word,
whether or not the word token appears in a
\rough copy" as explained below, etc.
The goal of the classier is to predict the
value of Y given values for X1; : : : ;Xm. The
classier makes its predictions based on the oc-
curence of combinations of conditioning vari-
able/value pairs called features. A feature F
is a set of variable-value pairs hXj ; xji, with
xj 2 Xj. Our classier is dened in terms of
a nite number n of features F1; : : : ;Fn, where
n  106 in our classiers.2 Each feature Fi de-
2It turns out that many pairs of features are exten-
sionally equivalent, i.e., take the same values on each
nes an associated random boolean variable
Fi =
?
hX
j
,x
j
i2F
i
(Xj=xj);
where (X=x) takes the value 1 if X = x and 0
otherwise. That is, Fi = 1 i Xj = xj for all
hXj ; xji 2 Fi.
Our classier estimates a feature weight i for
each feature Fi, that is used to dene the pre-
diction variable Z:
Z =
n
?
i=1
iFi:
The prediction made by the classier is
sign(Z) = Z=jZj, i.e., ?1 or +1 depending on
the sign of Z.
Intuitively, our goal is to adjust the vector
of feature weights ~ = (1; : : : ; n) to minimize
the expected misclassification rate E[(sign(Z) 6=
Y )]. This function is dicult to minimize,
so our boosting classier minimizes the ex-
pected Boost loss E[exp(?Y Z)]. As Singer and
Schapire [16] point out, the misclassication
rate is bounded above by the Boost loss, so a
low value for the Boost loss implies a low mis-
classication rate.
Our classier estimates the Boost loss as
?Et[exp(?Y Z)], where ?Et[] is the expectation
on the empirical training corpus distribution.
The feature weights are adjusted iteratively;
one weight is changed per iteration. The fea-
ture whose weight is to be changed is selected
greedily to minimize the Boost loss using the
algorithm described in [7]. Training contin-
ues for 25,000 iterations. After each iteration
the misclassication rate on the development
corpus ?Ed[(sign(Z) 6= Y )] is estimated, where
?Ed[] is the expectation on empirical develop-
ment corpus distribution. While each iteration
lowers the Boost loss on the training corpus, a
graph of the misclassication rate on the de-
velopment corpus versus iteration number is a
noisy U-shaped curve, rising at later iterations
due to overlearning. The value of ~ returned
word token in our training data. We developed a method
for quickly identifying such extensionally equivalent fea-
ture pairs based on hashing XORed random bitmaps,
and deleted all but one of each set of extensionally equiv-
alent features (we kept a feature with the smallest num-
ber of conditioning variables).
by the estimator is the one that minimizes the
misclassciation rate on the development cor-
pus; typically the minimum is obtained after
about 12,000 iterations, and the feature weight
vector ~ contains around 8000 nonzero feature
weights (since some weights are adjusted more
than once).3
2.2 Conditioning variables and features
This subsection describes the conditioning vari-
ables used in the EDITED classier. Many of the
variables are dened in terms of what we call
a rough copy. Intuitively, a rough copy iden-
ties repeated sequences of words that might
be restarts or repairs. Punctuation is ignored
for the purposes of dening a rough copy, al-
though conditioning variables indicate whether
the rough copy includes punctuation. A rough
copy in a tagged string of words is a substring
of the form 1?2, where:
1. 1 (the source) and 2 (the copy) both be-
gin with non-punctuation,
2. the strings of non-punctuation POS tags of
1 and 2 are identical,
3.  (the free final) consists of zero or more
sequences of a free nal word (see below)
followed by optional punctuation, and
4. ? (the interregnum) consists of sequences of
an interregnum string (see below) followed
by optional punctuation.
The set of free-final words includes all partial
words (i.e., ending in a hyphen) and a small set
of conjunctions, adverbs and miscellanea, such
as and, or, actually, so, etc. The set of interreg-
num strings consists of a small set of expressions
such as uh, you know, I guess, I mean, etc. We
search for rough copies in each sentence start-
ing from left to right, searching for longer copies
rst. After we nd a rough copy, we restart
searching for additional rough copies following
the free nal string of the previous copy. We
say that a word token is in a rough copy i it
appears in either the source or the free nal.4
(2) is an example of a rough copy.
3We used a smoothing parameter  as described in
[7], which we estimate by using a line-minimization rou-
tine to minimize the classifier?s minimum misclassifica-
tion rate on the development corpus.
4In fact, our definition of rough copy is more complex.
For example, if a word token appears in an interregnum
(2) I thought I
????
1
cou-,
? ?? ?

I mean,
? ?? ?
?
I
????
2
would n-
ish the work
Table 1 lists the conditioning variables used
in our classier. In that table, subscript inte-
gers refer to the relative position of word to-
kens relative to the current word; e.g. T1 is
the POS tag of the following word. The sub-
script f refers to the tag of the rst word of the
free nal match. If a variable is not dened for
a particular word it is given the special value
?NULL?; e.g., if a word is not in a rough copy
then variables such as Nm, Nu, Ni, Nl, Nr and
Tf all take the value NULL. Flags are boolean-
valued variables, while numeric-valued variables
are bounded to a value between 0 and 4 (as well
as NULL, if appropriate). The three variables
Ct, Cw and Ti are intended to help the classier
capture very short restarts or repairs that may
not involve a rough copy. The flags Ct and Ci
indicate whether the orthographic form and/or
tag of the next word (ignoring punctuation) are
the same as those of the current word. Ti has
a non-NULL value only if the current word is
followed by an interregnum string; in that case
Ti is the POS tag of the word following that
interregnum.
As described above, the classier?s features
are sets of variable-value pairs. Given a tuple of
variables, we generate a feature for each tuple
of values that the variable tuple assumes in the
training data. In order to keep the feature set
managable, the tuples of variables we consider
are restricted in various ways. The most impor-
tant of these are constraints of the form ?if Xj
is included among feature?s variables, then so
is Xk?. For example, we require that if a fea-
ture contains Pi+1 then it also contains Pi for
i  0, and we impose a similiar constraint on
POS tags.
2.3 Empirical evaluation
For the purposes of this research the Switch-
board corpus, as distributed by the Linguistic
Data Consortium, was divided into four sections
and the word immediately following the interregnum also
appears in a (different) rough copy, then we say that the
interregnum word token appears in a rough copy. This
permits us to approximate the Switchboard annotation
convention of annotating interregna as EDITED if they
appear in iterated edits.
(or subcorpora). The training subcorpus con-
sists of all les in the directories 2 and 3 of the
parsed/merged Switchboard corpus. Directory
4 is split into three approximately equal-size sec-
tions. (Note that the les are not consecutively
numbered.) The rst of these (les sw4004.mrg
to sw4153.mrg) is the testing corpus. All edit
detection and parsing results reported herein
are from this subcorpus. The les sw4154.mrg
to sw4483.mrg are reserved for future use. The
les sw4519.mrg to sw4936.mrg are the devel-
opment corpus. In the complete corpus three
parse trees were suciently ill formed in that
our tree-reader failed to read them. These trees
received trivial modications to allow them to
be read, e.g., adding the missing extra set of
parentheses around the complete tree.
We trained our classier on the parsed data
les in the training and development sections,
and evaluated the classifer on the test section.
Section 3 evaluates the parser?s output in con-
junction with this classier; this section focuses
on the classier?s performance at the individual
word token level. In our complete application,
the classier uses a bitag tagger to assign each
word a POS tag. Like all such taggers, our tag-
ger has a nonnegligible error rate, and these tag-
ging could conceivably aect the performance of
the classier. To determine if this is the case,
we report classier performance when trained
both on \Gold Tags" (the tags assigned by the
human annotators of the Switchboard corpus)
and on \Machine Tags" (the tags assigned by
our bitag tagger). We compare these results to
a baseline \null" classier, which never identi-
es a word as EDITED. Our basic measure of
performance is the word misclassication rate
(see Section 2.1). However, we also report pre-
cision and recall scores for EDITED words alone.
All words are assigned one of the two possible
labels, EDITED or not. However, in our evalua-
tion we report the accuracy of only words other
than punctuation and lled pauses. Our logic
here is much the same as that in the statistical
parsing community which ignores the location
of punctuation for purposes of evaluation [3,5,
6] on the grounds that its placement is entirely
conventional. The same can be said for lled
pauses in the switchboard corpus.
Our results are given in Table 2. They show
that our classier makes only approximately 1/3
W0 Orthographic word
P0; P1; P2; Pf Partial word flags
T
?1; T0; T1; T2; Tf POS tags
Nm Number of words in common in source and copy
Nu Number of words in source that do not appear in copy
Ni Number of words in interregnum
Nl Number of words to left edge of source
Nr Number of words to right edge of source
Ct Followed by identical tag flag
Cw Followed by identical word flag
Ti Post-interregnum tag flag
Table 1: Conditioning variables used in the EDITED classier.
of the misclassication errors made by the null
classier (0.022 vs. 0.059), and that using the
POS tags produced by the bitag tagger does
not have much eect on the classier?s perfor-
mance (e.g., EDITED recall decreases from 0.678
to 0.668).
3 Parsing transcribed speech
We now turn to the second pass of our two-pass
architecture, using an \o-the-shelf" statistical
parser to parse the transcribed speech after hav-
ing removed the words identied as edited by
the rst pass. We rst dene the evaluation
metric we use and then describe the results of
our experiments.
3.1 Parsing metrics
In this section we describe the metric we use
to grade the parser output. As a rst desider-
atum we want a metric that is a logical exten-
sion of that used to grade previous statistical
parsing work. We have taken as our starting
point what we call the \relaxed labeled preci-
sion/recall" metric from previous research (e.g.
[3,5]). This metric is characterized as follows.
For a particular test corpus let N be the total
number of nonterminal (and non-preterminal)
constituents in the gold standard parses. Let
M be the number of such constituents returned
by the parser, and let C be the number of these
that are correct (as dened below). Then pre-
cision = C=M and recall = C=N .
A constituent c is correct if there exists a con-
stituent d in the gold standard such that:
1. label(c) = label(d)5
5For some reason, starting with [12] the labels ADVP
2. begin(c) r begin(d)
3. end(c) r end(d)
In 2 and 3 above we introduce an equivalence
relation r between string positions. We dene
r to be the smallest equivalence relation sat-
isfying a r b for all pairs of string positions a
and b separated solely by punctuation symbols.
The parsing literature uses r rather than =
because it is felt that two constituents should
be considered equal if they disagree only in the
placement of, say, a comma (or any other se-
quence of punctuation), where one constituent
includes the punctuation and the other excludes
it.
Our new metric, \relaxed edited labeled preci-
sion/recall" is identical to relaxed labeled preci-
sion/recall except for two modications. First,
in the gold standard all non-terminal subcon-
stituents of an EDITED node are removed and
the terminal constituents are made immediate
children of a single EDITED node. Furthermore,
two or more EDITED nodes with no separating
non-edited material between them are merged
into a single EDITED node. We call this version
a \simplied gold standard parse." All precision
recall measurements are taken with respected to
the simplied gold standard.
Second, we replace r with a new equiva-
lence relation e which we dene as the smallest
equivalence relation containing r and satisfy-
ing begin(c) e end(c) for each EDITED node c
in the gold standard parse.6
and PRT are considered to be identical as well.
6We considered but ultimately rejected defining ?
e
using the EDITED nodes in the returned parse rather
Classifer
Null Gold Tags Machine Tags
Misclassication rate 0.059 0.021 0.022
EDITED precision { 0.952 0.944
EDITED recall 0 0.678 0.668
Table 2: Performance of the \null" classier (which never marks a word as EDITED) and boosting
classiers trained on \Gold Tags" and \Machine Tags".
1 2 3 4 5 6 7 8
E E E E
the , bagel with uh , doughnut
1 2 2 4 5 2 2 8
Figure 1: Equivalent string positions as dened by e.
We give a concrete example in Figure 1. The
rst row indicates string position (as usual in
parsing work, position indicators are between
words). The second row gives the words of the
sentence. Words that are edited out have an
\E" above them. The third row indicates the
equivalence relation by labeling each string posi-
tion with the smallest such position with which
it is equivalent.
There are two basic ideas behind this deni-
tion. First, we do not care where the EDITED
nodes appear in the tree structure produced by
the parser. Second, we are not interested in the
ne structure of EDITED sections of the string,
just the fact that they are EDITED. That we
do care which words are EDITED comes into
our gure of merit in two ways. First, (non-
contiguous) EDITED nodes remain, even though
their substructure does not, and thus they are
counted in the precision and recall numbers.
Secondly (and probably more importantly), fail-
ure to decide on the correct positions of edited
nodes can cause collateral damage to neighbor-
ing constituents by causing them to start or stop
in the wrong place. This is particularly rele-
vant because according to our denition, while
the positions at the beginning and ending of an
edit node are equivalent, the interior positions
are not (unless related by the punctuation rule).
than the simplified gold standard. We rejected this be-
cause the ?
e
relation would then itself be dependent
on the parser?s output, a state of affairs that might al-
low complicated schemes to improve the parser?s perfor-
mance as measured by the metric.
See Figure 1.
3.2 Parsing experiments
The parser described in [3] was trained on the
Switchboard training corpus as specied in sec-
tion 2.1. The input to the training algorithm
was the gold standard parses minus all EDITED
nodes and their children.
We tested on the Switchboard testing sub-
corpus (again as specied in Section 2.1). All
parsing results reported herein are from all sen-
tences of length less than or equal to 100 words
and punctuation. When parsing the test corpus
we carried out the following operations:
1. create the simplied gold standard parse
by removing non-terminal children of an
EDITED node and merging consecutive
EDITED nodes.
2. remove from the sentence to be fed to the
parser all words marked as edited by an
edit detector (see below).
3. parse the resulting sentence.
4. add to the resulting parse EDITED nodes
containing the non-terminal symbols re-
moved in step 2. The nodes are added as
high as possible (though the denition of
equivalence from Section 3.1 should make
the placement of this node largely irrele-
vant).
5. evaluate the parse from step 4 against the
simplied gold standard parse from step 1.
We ran the parser in three experimental sit-
uations, each using a dierent edit detector in
step 2. In the rst of the experiments (labeled
\Gold Edits") the \edit detector" was simply
the simplied gold standard itself. This was to
see how well the parser would do it if had perfect
information about the edit locations.
In the second experiment (labeled \Gold
Tags"), the edit detector was the one described
in Section 2 trained and tested on the part-of-
speech tags as specied in the gold standard
trees. Note that the parser was not given the
gold standard part-of-speech tags. We were in-
terested in contrasting the results of this experi-
ment with that of the third experiment to gauge
what improvement one could expect from using
a more sophisticated tagger as input to the edit
detector.
In the third experiment (\Machine Tags") we
used the edit detector based upon the machine
generated tags.
The results of the experiments are given in
Table 3. The last line in the gure indicates
the performance of this parser when trained and
tested on Wall Street Journal text [3]. It is
the \Machine Tags" results that we consider the
\true" capability of the detector/parser combi-
nation: 85.3% precision and 86.5% recall.
3.3 Discussion
The general trends of Table 3 are much as one
might expect. Parsing the Switchboard data is
much easier given the correct positions of the
EDITED nodes than without this information.
The dierence between the Gold-tags and the
Machine-tags parses is small, as would be ex-
pected from the relatively small dierence in
the performance of the edit detector reported in
Section 2. This suggests that putting signicant
eort into a tagger for use by the edit detec-
tor is unlikely to produce much improvement.
Also, as one might expect, parsing conversa-
tional speech is harder than Wall Street Jour-
nal text, even given the gold-standard EDITED
nodes.
Probably the only aspect of the above num-
bers likely to raise any comment in the pars-
ing community is the degree to which pre-
cision numbers are lower than recall. With
the exception of the single pair reported in [3]
and repeated above, no precision values in the
recent statistical-parsing literature [2,3,4,5,14]
have ever been lower than recall values. Even
this one exception is by only 0.1% and not sta-
tistically signicant.
We attribute the dominance of recall over pre-
cision primarily to the influence of edit-detector
mistakes. First, note that when given the
gold standard edits the dierence is quite small
(0.3%). When using the edit detector edits the
dierence increases to 1.2%. Our best guess is
that because the edit detector has high preci-
sion, and lower recall, many more words are left
in the sentence to be parsed. Thus one nds
more nonterminal constituents in the machine
parses than in the gold parses and the precision
is lower than the recall.
4 Previous research
While there is a signicant body of work on nd-
ing edit positions [1,9,10,13,17,18], it is dicult
to make meaningful comparisons between the
various research eorts as they dier in (a) the
corpora used for training and testing, (b) the
information available to the edit detector, and
(c) the evaluation metrics used. For example,
[13] uses a subsection of the ATIS corpus, takes
as input the actual speech signal (and thus has
access to silence duration but not to words), and
uses as its evaluation metric the percentage of
time the program identies the start of the in-
terregnum (see Section 2.2). On the other hand,
[9,10] use an internally developed corpus of sen-
tences, work from a transcript enhanced with
information from the speech signal (and thus
use words), but do use a metric that seems to be
similar to ours. Undoubtedly the work closest
to ours is that of Stolcke et al [18], which also
uses the transcribed Switchboard corpus. (How-
ever, they use information on pause length, etc.,
that goes beyond the transcript.) They cate-
gorize the transitions between words into more
categories than we do. At rst glance there
might be a mapping between their six categories
and our two, with three of theirs corresponding
to EDITED words and three to not edited. If
one accepts this mapping they achieve an er-
ror rate of 2.6%, down from their NULL rate of
4.5%, as contrasted with our error rate of 2.2%
down from our NULL rate of 5.9%. The dier-
ence in NULL rates, however, raises some doubts
that the numbers are truly measuring the same
thing.
Experiment Labeled Precision Labeled Recall F-measure
Gold Edits 87.8 88.1 88.0
Gold Tags 85.4 86.6 86.0
Machine Tags 85.3 86.5 85.9
WSJ 89.5 89.6
Table 3: Results of Switchboard parsing, sentence length  100.
There is also a small body of work on parsing
disfluent sentences [8,11]. Hindle?s early work
[11] does not give a formal evaluation of the
parser?s accuracy. The recent work of Schubert
and Core [8] does give such an evaluation, but
on a dierent corpus (from Rochester Trains
project). Also, their parser is not statistical
and returns parses on only 62% of the strings,
and 32% of the strings that constitute sentences.
Our statistical parser naturally parses all of our
corpus. Thus it does not seem possible to make
a meaningful comparison between the two sys-
tems.
5 Conclusion
We have presented a simple architecture for
parsing transcribed speech in which an edited
word detector is rst used to remove such words
from the sentence string, and then a statistical
parser trained on edited speech (with the edited
nodes removed) is used to parse the text. The
edit detector reduces the misclassication rate
on edited words from the null-model (marking
everything as not edited) rate of 5.9% to 2.2%.
To evaluate our parsing results we have intro-
duced a new evaluation metric, relaxed edited
labeled precision/recall. The purpose of this
metric is to make evaluation of a parse tree
relatively indierent to the exact tree posi-
tion of EDITED nodes, in much the same way
that the previous metric, relaxed labeled pre-
cision/recall, make it indierent to the attach-
ment of punctuation. By this metric the parser
achieved 85.3% precision and 86.5% recall.
There is, of course, great room for improve-
ment, both in stand-alone edit detectors, and
their combination with parsers. Also of interest
are models that compute the joint probabilities
of the edit detection and parsing decisions |
that is, do both in a single integrated statistical
process.
References
1. Bear, J., Dowding, J. and Shriberg, E.
Integrating multiple knowledge sources for
detection and correction of repairs in human-
computer dialog. In Proceedings of the 30th
Annual Meeting of the Association for Com-
putational Linguistics. 56{63.
2. Charniak, E. Statistical parsing with a
context-free grammar and word statistics.
In Proceedings of the Fourteenth National
Conference on Articial Intelligence. AAAI
Press/MIT Press, Menlo Park, CA, 1997,
598{603.
3. Charniak, E. A maximum-entropy-
inspired parser. In Proceedings of the 2000
Conference of the North American Chap-
ter of the Association for Computational
Linguistics. ACL, New Brunswick NJ, 2000.
4. Collins, M. J. A new statistical parser
based on bigram lexical dependencies. In Pro-
ceedings of the 34th Annual Meeting of the
ACL. 1996.
5. Collins, M. J. Three generative lexical-
ized models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the
ACL. 1997, 16{23.
6. Collins, M. J. Head-Driven Statistical
Models for Natural Language Parsing. Uni-
versity of Pennsylvania, Ph.D. Dissertation,
1999.
7. Collins, M. J. Discriminative reranking
for natural language parsing. In Proceedings
of the International Conference on Machine
Learning (ICML 2000). 2000.
8. Core, M. G. and Schubert, L. K. A syn-
tactic framework for speech repairs and other
disruptions. In Proceedings of the 37th An-
nual Meeting of the Association for Compu-
tational Linguistics. 1999, 413{420.
9. Heeman, P. A. and Allen, J. F. Into-
national boundaries, speech repairs and dis-
course markers: modeling spoken dialog. In
35th Annual Meeting of the Association for
Computational Linguistics and 17th Interna-
tional Conference on Computational Linguis-
tics. 1997, 254{261.
10. Heeman, P. A. and Allen, J. F. Speech
repairs, intonational phrases and discourse
markers: modeling speakers? utterances in
spoken dialogue. Computational Linguistics
254 (1999).
11. Hindle, D. Deterministic parsing of syn-
tactic non-fluencies. In Proceedings of the
21st Annual Meeting of the Association for
Computational Linguistics. 1983, 123{128.
12. Magerman, D. M. Statistical decision-tree
models for parsing. In Proceedings of the 33rd
Annual Meeting of the Association for Com-
putational Linguistics. 1995, 276{283.
13. Nakatani, C. H. and Hirschberg, J. A
corpus-based study of repair cues in sponta-
neous speech. Journal of the Acoustical Soci-
ety of America 953 (1994), 1603{1616.
14. Ratnaparkhi, A. Learning to parse natu-
ral language with maximum entropy models.
Machine Learning 34 1/2/3 (1999), 151{176.
15. Shriberg, E. E. Preliminaries to a The-
ory of Speech Disfluencies. In PhD Disserta-
tion. Department of Psychology, University
of California-Berkeley, 1994.
16. Singer, Y. and Schapire, R. E. Im-
proved boosting algorithms using condence-
based predictions. In Proceedings of the
Eleventh Annual Conference on Computa-
tional Learning Theory. 1998, 80{91.
17. Stolcke, A. and Shriberg, E. Auto-
matic linguistic segmantation of conversa-
tional speech. In Proceedings of the 4th In-
ternational Conference on Spoken Language
Processing (ICSLP-96). 1996.
18. Stolcke, A., Shriberg, E., Bates, R.,
Ostendorf, M., Hakkani, D., Plauche,
M., Tu?r, G. and Lu, Y. Automatic detec-
tion of sentence boundaries and disfluencies
based on recognized words. Proceedings of
the International Conference on Spoken Lan-
guage Processing 5 (1998), 2247{2250.
Sentence-Internal Prosody Does not Help Parsing the Way Punctuation Does
Michelle L Gregory
Brown University
mgregory@cog.brown.edu
Mark Johnson
Brown University
Mark Johnson@Brown.edu
Eugene Charniak
Brown University
ec@cs.brown.edu
Abstract
This paper investigates the usefulness of
sentence-internal prosodic cues in syntac-
tic parsing of transcribed speech. Intu-
itively, prosodic cues would seem to pro-
vide much the same information in speech
as punctuation does in text, so we tried to
incorporate them into our parser in much
the same way as punctuation is. We com-
pared the accuracy of a statistical parser
on the LDC Switchboard treebank corpus
of transcribed sentence-segmented speech
using various combinations of punctua-
tion and sentence-internal prosodic infor-
mation (duration, pausing, and f0 cues).
With no prosodic or punctuation informa-
tion the parser?s accuracy (as measured by
F-score) is 86.9%, and adding punctuation
increases its F-score to 88.2%. However,
all of the ways we have tried of adding
prosodic information decrease the parser?s
F-score to between 84.8% to 86.8%, de-
pending on exactly which prosodic infor-
mation is added. This suggests that for
sentence-internal prosodic information to
improve speech transcript parsing, either
different prosodic cues will have to used
or they will have be exploited in the parser
in a way different to that used currently.
1 Introduction
Acoustic cues, generally duration, pausing, and
f0, have been demonstrated to be useful for auto-
S
INTJ
UH
Oh
,
,
NP
PRP
I
VP
VBD
loved
NP
PRP
it
.
.
Figure 1: A treebank style tree in which punctuation
is coded with terminal and preterminal nodes.
matic segmentation of natural speech (Baron et al,
2002; Hirschberg and Nakatani, 1998; Neiman et
al., 1998). In fact, it is generally accepted that
prosodic information is a reliable tool in predict-
ing topic shifts and sentence boundaries (Shriberg
et al, 2000). Sentences are generally demarcated
by a major fall (or rise) in f0, lengthening of the
final syllable, and following pauses. However,
the usefulness of prosodic information in sentence-
internal parsing is less clear. While assumed not
to be a one-to-one mapping, there is evidence
that there is a strong correlation between prosodic
boundaries and sentence-internal syntactic bound-
aries (Altenberg, 1987; Croft, 1995). For exam-
ple, Schepman and Rodway (2000) have shown that
prosodic cues reliably predict ambiguous attach-
ment of relative clauses within coordination con-
structions. Jansen et al (2001) have demonstrated
that prosodic breaks and an increase in pitch range
can distinguish direct quotes from indirect quotes in
a corpus of natural speech.
This paper evaluates the accuracy of a statistical
parser whose input includes prosodic cues. The pur-
pose of this study to determine if prosodic cues im-
prove parsing accuracy in the same way that punc-
tuation does. Punctuation is represented in the vari-
ous Penn treebank corpora as independent word-like
tokens, with corresponding terminal and pretermi-
nal nodes, as shown in Figure 1 (Bies et al, 1995).
Even though this seems linguistically highly un-
natural (e.g., punctuation might indicate supraseg-
mental prosodic properties), statistical parsers gen-
erally perform significantly better when their train-
ing and test data contains punctuation represented
in this way than if the punctuation is stripped out
of the training and test data (Charniak, 2000; En-
gel et al, 2002; Johnson, 1998). On the Switch-
board treebank data set using the experimental setup
described below we obtained an F-score of 0.882
when using punctuation and 0.869 when punctua-
tion was stripped out, replicating previous experi-
ments demonstrating the importance of punctuation.
(F-score is a standard measure of parse accuracy, see
e.g., Manning and Schu?tze (1999) for details).
This paper investigates how prosodic cues, when
encoded in the parser?s input in a manner similar to
the way the Penn treebanks encode punctuation, af-
fect parser accuracy. Our starting point is the ob-
servation that the Penn treebank annotation of punc-
tuation does significantly improve parsing accuracy.
Coupled with the assumption that punctuation and
prosody are encoding similar information, this led
us to try to encode prosodic information in a man-
ner that was as similar as possible to the way that
punctuation is encoded in the Penn treebanks.
For example, commas in text and pauses in speech
seem to convey similar information. In fact, when
transcribing speech, commas are often used to de-
note a pause. Thus, given the correlation between
the two, and the fact that sentence-internal punctu-
ation tends to be commas, we expected that pause
duration, coded in a way similar to punctuation,
would improve parsing accuracy in the same way
that punctuation does.
While it may be the case that the encoding of
prosodic information used in the experiments be-
low is perhaps not optimal and the parser has not
been tuned to use this information, note that exactly
the same objections could be made to the way that
punctuation is encoded and used in modern statis-
tical parsers, and punctuation does in fact dramati-
cally improve parsing accuracy.
We focus in this paper on parsing accuracy in a
modern statistical parsing framework, but it is im-
portant to remember that prosodic cues might help
parsing in other ways as well, even if they do not im-
prove parsing accuracy. No?th et al (2000) point out
that prosodic cues reduce parsing time and increase
recognition accuracy when parsing speech lattices
with the hand-crafted Verbmobil grammar. Page 266
of Kompe (1997) discusses the effect that incorpo-
rating prosodic information has on parse quality in
the Verbmobil system using the TUG unification
grammar parser: out of the 54 parses affected by
the addition of prosodic information, 33 were judged
?better with prosody?, 14 were judged ?better with-
out prosody? and 7 were judged ?unclear?. Our
experiments below differ from the experiments of
No?th and Kompe in many ways. First, we used
speech transcripts rather than speech recognizer lat-
tices. Second, we used a general-purpose broad-
coverage statistical parser rather than a unification
grammar parser with a hand-constructed grammar.
2 Method
The data used for this study is the transcribed ver-
sion of the Switchboard Corpus as released by
the Linguistic Data Consortium. The Switchboard
Corpus is a corpus of telephone conversations be-
tween adult speakers of varying dialects. The cor-
pus was split into training and test data as de-
scribed in Charniak and Johnson (2001). The train-
ing data consisted of all files in sections 2 and 3 of
the Switchboard treebank. The testing corpus con-
sists of files sw4004.mrg to sw4153.mrg, while files
sw4519.mrg to sw4936.mrg were used as develop-
ment corpus.
2.1 Prosodic variables
Prosodic information for the corpus was ob-
tained from forced alignments provided by
Hamaker et al (2003) and Ferrer et al (2002).
Hamaker et al (2003) provided word alignments
between the LDC parsed corpus and new alignments
of the Switchboard Coprus. Most of the differences
between the two alignments were individual lexical
items. In cases of differences, we kept the lexical
item from the LDC version. Ferrer et al (2002)
provided very rich prosodic information including
duration, pausing, f0 information, and individual
speaker statistics for each word in the corpus. The
information obtained from this corpus was aligned
to the LDC corpus.
It is not known exactly which prosodic vari-
ables convey the information about syntactic bound-
aries that is most useful to a modern syntactic
parser, so we investigated many different com-
binations of these variables. We looked for
changes in pitch and duration that we expected
would correspond to syntactic boundaries. While
we tested many combinations of variables, they
were mainly based on the variables PAU DUR N,
NORM LAST RHYME DUR, FOK WRD DIFF MNMN N,
FOK LR MEAN KBASELN and SLOPE MEAN DIFF N in
the data provided by Ferrer et al (2002).
While Ferrer (2002) should be consulted for full
details, PAU DUR N is pause duration normalized by
the speaker?s mean sentence-internal pause dura-
tion, NORM LAST RHYME DUR is the duration of the
phone minus the mean phone duration normalized
by the standard deviation of the phone duration for
each phone in the rhyme, FOK WRD DIFF MNMN NG
is the log of the mean f0 of the current word,
divided by the log mean f0 of the following
word, normalized by the speakers mean range,
FOK LR MEAN KBASELN is the log of the mean f0
of the word normalized by speaker?s baseline, and
SLOPE MEAN DIFF N is the difference in the f0 slope
normalized by the speaker?s mean f0 slope.
These variables all range over continuous values.
Modern statistical parsing technology has been de-
veloped assuming that all of the input variables are
categorical, and currently our parser can only use
categorical inputs. Given the complexity of the dy-
namic programming algorithms used by the parser,
it would be a major research undertaking to develop
a statistical parser of the same quality as the one
used here that is capable of using both categorical
and continuous variables as input.
In the experiments below we binned the contin-
uous prosodic variables to produce the actual cate-
gorical values used in our experiments. Binning in-
volves a trade-off, as fewer bins involve a loss of
information, whereas a large number of bins splits
the data so finely that the statistical models used in
the parser fail to generalize. We binned by first con-
structing a histogram of each feature?s values, and
divided these values into bins in such a way that each
bin contained the same number of samples. In runs
in which a single feature is the sole prosodic feature
we divided that feature?s values into 10 bins, while
runs in which two or more prosodic features were
conjoined we divided each feature into 5 bins.
While not reported here, we experimented with a
wide variety of different binning strategies, includ-
ing using the bins proposed by Ferrer et al (2002).
In fact the number of bins used does not affect the
results markedly; we obtained virtually the same re-
sults with only two bins.
We generated and inserted ?pseudo-punctuation?
symbols based on these binned values that were in-
serted into the parse input as described below. In
general, a pseudo-punctuation symbol is the con-
junction of the binned values of all of the prosodic
features used in a particular run. When map-
ping from binned prosodic variables to pseudo-
punctuation symbols, some of the binned values
can be represented by the absence of a pseudo-
punctuation symbol.
Because we intend these pseudo-punctuation
symbols to be as similar as possible to normal punc-
tuation, we generated pseudo-punctuation symbols
only when the corresponding prosodic variable falls
outside of its typical values. The ranges are given
below, and were chosen so that they align with
bin boundaries and result in each type of pseudo-
punctuation symbol occuring on 40% of words.
Thus when a prosodic feature is used alone only 4 of
its 10 bins are represented by a pseudo-punctuation
symbol.
However, when two or more types of the prosodic
pseudo-punctuation symbols are used at once there
is a larger number of different pseudo-punctuation
symbols and a greater number of words appear-
ing with a following pseudo-punctuation symbol.
For example, when P, R and S prosodic annota-
tions are used together there are 89 distinct types
of prosodic pseudo-punctuation symbols in our cor-
pus, and 54% of words are followed by a prosodic
pseudo-punctuation symbol.
The experiments below make use of the following
types of pseudo-punctuation symbols, either alone
or concatenated in combination. See Figure 2 for
an example tree with pseudo-punctuation symbols
inserted.
Pb This is based on the bin b of the binned
PAU DUR N value, and is only generated when
the PAU DUR N value is greater than 0.285.
Rb This is based on the bin b of the binned
NORM LAST RHYME DUR value, and is only
generated that value is greater than -0.061.
Wb This is based on the bin b of the binned
FOK WRD DIFF MNMN N value, and is only gen-
erated when that value is less than -0.071 or
greater than 0.0814.
Lb This is based on the bin b of the
FOK LR MEAN KBASELN value, and is only
generated when that value is less than 0.157 or
greater than 0.391.
Sb This is based on the bin b of the
SLOPE MEAN DIFF N value, and is only
generated whenever that value is non-zero.
In addition, we also created a binary version of
the P feature in order to evaluate the effect of bina-
rization.
NP This is based on the PAU DUR N value, and is
only generated when that value is greater than
0.285.
We actually experimented with a much wider
range of binned variables, but they all produced re-
sults similar to those described below.
2.2 Parse corpus construction
We tried to incorporate the binned prosodic informa-
tion described in the previous subsection in a manner
that corresponds as closely as possible to the way
that punctuation is represented in this corpus, be-
cause previous experiments have shown that punc-
tuation improves parser performance (Charniak and
Johnson, 2001; Engel et al, 2002). We deleted dis-
fluency tags and EDITED subtrees from our training
and test corpora.
We investigated several combinations of prosodic
pseudo-punctuation symbols. For each of these we
generated a training and test corpus. The pseudo-
punctuation symbols are dominated by a new preter-
minal PROSODY to produce a well-formed tree.
These prosodic local trees are introduced into the
tree following the word they described, and are at-
tached as high as possible in the tree, just as punc-
tuation is in the Penn treebank. Figure 2 depicts
a typical tree that contains P R S prosodic pseudo-
punctuation symbols inserted following the word
they describe.
We experimented with several other ways of in-
corporating prosody into parse trees, none of which
greatly affected the results. For example, we also ex-
perimented with a ?raised? representation in which
the prosodic pseudo-punctuation symbol also serves
as the preterminal label. The corresponding ?raised?
version of the example tree is depicted in Figure 3.
The motivation for raising is as follows. The sta-
tistical parser used for this research generates the
siblings of a head in a sequential fashion, first pre-
dicting the category label of a sibling and later con-
ditioning on that label to predict the remaining sib-
lings. ?Raising? should permit the generative model
to condition not just on the presence of a prosodic
pseudo-punctuation symbol but also on its actual
identity. If some but not all of the prosodic pseudo-
punctuation symbols were especially indicative of
some aspect of phrase structure, then the ?raising?
structures should permit the parsing model to detect
this and condition on just those symbols. Note that
in the Penn treebank annotation scheme, different
types of punctuation are given different preterminal
categories, so punctuation is encoded in the treebank
using a ?raised? representation.
The resulting corpora contain both prosodic and
punctuation information. We prepared our actual
training and testing corpora by selectively remov-
ing subtrees from these corpora. By removing all
punctuation subtrees we obtain corpora that contain
prosodic information but no punctuation, by remov-
ing all prosodic information we obtain the original
treebank data, and by removing both prosodic and
punctuation subtrees we obtain corpora that contain
neither type of information.
2.3 Evaluation
We trained and evaluated the parser on the various
types of corpora described in the previous section.
S
INTJ
UH
Uh
PROSODY
*R4*
,
,
NP
PRP
I
PROSODY
*R4*
VP
VBP
do
RB
nt
VP
VB
live
PP
IN
in
NP
DT
a
PROSODY
*R3*S2*
NN
house
PROSODY
*S4*
,
,
Figure 2: A tree with P R S prosodic pseudo-punctuation symbols inserted following the words they corre-
spond to. (No P prosodic features occured in this utterance).
S
INTJ
UH
Uh
*R4*
*R4*
,
,
NP
PRP
I
*R4*
*R4*
VP
VBP
do
RB
nt
VP
VB
live
PP
IN
in
NP
DT
a
*R3*S2*
*R3*S2*
NN
house
*S4*
*S4*
,
,
Figure 3: The same sentence as in Figure 2, but with prosodic pseudo-punctuation raised to the preterminal
level.
Annotation unraised raised
punctuation 88.212
none 86.891
L 85.632 85.361
NP 86.633 86.633
P 86.754 86.594
R 86.407 86.288
S 86.424 85.75
W 86.031 85.681
P R 86.405 86.282
P W 86.175 85.713
P S 86.328 85.922
P R S 85.64 84.832
Table 1: The F-score of the parser?s output when
trained and tested on corpora with varying prosodic
pseudo-punctuation symbols. The entry ?punc-
tuation? gives the parser?s performance on input
with standard punctuation, while ?none? gives the
parser?s performance on input without any punctua-
tion or prosodic pseudo-punctuation whatsoever.
(We always tested on the type of corpora that corre-
sponded to the training data). We evaluated parser
performance using the methodology described in
Engel et al (2002), which is a simple adaptation of
the well-known PARSEVAL measures in which punc-
tuation and prosody preterminals are ignored. This
evaluation yields precision, recall and F-score values
for each type of training and test corpora.
3 Results
Table 1 presents the results of our experiments. The
RAISED prosody entry corresponds to the raised ver-
sion of the COMBINED corpora, as described above.
We replicated previous results and showed that
punctuation information does help parsing. How-
ever, none of the experiments with prosodic infor-
mation resulted in improved parsing performance;
indeed, adding prosodic information reduced perfor-
mance by 2 percentage points in some cases. This is
a very large amount by the standards of modern sta-
tistical parsers. Notice that the general trend is that
performance decreases as the amount and complex-
ity of the prosodic annotation increased.
4 Discussion and Conclusion
Simple statistical tests show that there is in fact
a significant correlation between the location of
opening and closing phrase boundaries and all of
the prosodic pseudo-punctuation symbols described
above, so there is no doubt that these do con-
vey information about syntactic structure. How-
ever, adding the prosodic pseudo-punctuation sym-
bols uniformly decreased parsing accuracy relative
to input with no prosodic information. There are a
number of reasons why this might be the case.
While we investigated a wide range of prosodic
features, it is possible that different prosodic features
might improve parsing performance, and it would be
interesting to see if improved prosodic feature ex-
traction would improve parsing accuracy.
We suspect that the decrease in accuracy is due
to the fact that the addition of prosodic pseudo-
punctuation symbols effectively excluded other
sources of information from the parser?s statisti-
cal models. For example, as mentioned earlier the
parser uses a mixture of n-gram models to predict
the sequence of categories on the right-hand side
of syntactic rules, backing off ultimately to a dis-
tribution that includes just the head and the preced-
ing sibling?s category. Consider the effect of insert-
ing a prosodic pseudo-punctuation symbol on such
a model. The prosodic pseudo-punctuation symbol
would replace the true preceding sibling?s category
in the model, thus possibly resulting in poorer over-
all performance (note however that the parser also
includes a higher-order backoff distribution in which
the next category is predicted using the preceding
two sibling?s categories, so the true sibling?s cate-
gory would still have some predictive value).
The basic point is that inserting additional in-
formation into the parse tree effectively splits the
conditioning contexts, exacerbating the sparse data
problems that are arguably the bane of all statisti-
cal parsers. Additional information only improves
parsing accuracy if the information it conveys is suf-
ficient to overcome the loss in accuracy incurred by
the increase in data sparseness. It seems that punctu-
ation carries sufficient information to overcome this
loss, but that the prosodic categories we introduced
do not.
It could be that our results reflect the fact that we
are parsing speech transcripts in which the words
(and hence their parts of speech) are very reliably
identified, whereas our prosodic features were auto-
matically extracted directly from the speech signal
and hence might be noisier. If the explanation pro-
posed above is correct, it is perhaps not surprising
that an accurate part of speech label would prove
more useful in a conditioning context used by the
parser than a noisy prosodic feature. Note that this
would not be the case when parsing from speech rec-
ognizer output (since word identity would itself be
uncertain), and it is possible that in such applications
prosodic information would be more useful.
Of course, there are many other ways prosodic in-
formation might be exploited in a parser, and one
of those may yield improved parser performance.
We chose to incorporate prosodic information into
our parser in a way that was similar to the way
that punctuation is annotated in the Penn treebanks
because we assumed that punctuation carries infor-
mation similar to prosody, and it had already been
demonstrated that punctuation annotated in the Penn
treebank fashion does systematically improve pars-
ing accuracy.
But the assumption that prosody conveys infor-
mation about syntactic structure in the same way
that punctuation does could be false. It could also be
that even though prosody encodes information about
syntactic structure, this information is encoded in
a manner that is too complicated for our parser to
utilize. For example, even though commas are of-
ten used to indicate pauses, pauses have many other
functions in fluent speech. Pauses of greater than
200 ms are often associated with planning problems,
which might be correlated with syntactic structure
in ways too complex for the parser to exploit. While
not reported here, we tried various techniques to iso-
late different functions of pauses, such as exclud-
ing pauses of greater than 200 ms. However, all of
these experiments produced results similar to those
reported here.
Finally, there is another possible reason why our
assumption that prosody and punctuation are similar
in their information content could be wrong. Our
prosodic information was automatically extracted
from the speech stream, while punctuation was pro-
duced by human annotators who presumably com-
prehended the utterances being annotated. Given
this, it is perhaps no surprise that our automatically
extracted prosodic annotations proved less useful
than human-produced punctuation.
References
Bengt Altenberg. 1987. Prosodic patterns in spoken En-
glish: studies in the correlation between prosody and
grammar. Lund University Press, Lund.
Don Baron, Elizabeth Shriberg, and Andreas Stolcke.
2002. Automatic punctuation and disfluency detec-
tion in multi-party meetings using prosodic and lex-
ical cues. In Proc. Intl. Conf. on Spoken Language
Processing, volume 2, pages 949?952, Denver.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre, 1995. Bracketting Guideliness for Treebank II
style Penn Treebank Project. Linguistic Data Consor-
tium.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In Proceed-
ings of the 2nd Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 118?126.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In The Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics, pages 132?139.
William Croft. 1995. Intonation units and grammatical
structure. Linguistics, 33:839?882.
Donald Engel, Eugene Charniak, and Mark Johnson.
2002. Parsing and disfluency placement. In Proceed-
ings of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 49?54.
Luciana Ferrer, Elizabeth Shriberg, and Andreas Stol-
cke. 2002. Is the speaker done yet? faster and more
accurate end-of-utterance detection using prosody in
human-computer dialog. In Proc. Intl. Conf. on Spo-
ken Language Processing, volume 3, pages 2061?
2064, Denver.
Luciana Ferrer. 2002. Prosodic features for the switch-
board database. Technical report, SRI International,
Menlo Park.
Jon Hamaker, Dan Harkins, and Joe Picone. 2003. Man-
ually corrected switchboard word alignments.
Julia Hirschberg and Christine Nakatani. 1998. Acoustic
indicators of topic segmentation. In Proc. Intl. Conf.
on Spoken Language Processing, volume 4, pages
1255?1258, Philadelphia.
Wouter Jansen, Michelle L. Gregory, and Jason M. Bre-
nier. 2001. Prosodic correlates of directly reported
speech: Evidence from conversational speech. In Pro-
ceedings of the ISCA Workshop on Prosody in Speech
Recognition and Understanding, pages 77?80, Red
Banks, NJ.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Ralf Kompe. 1997. Prosody in speech understanding
systems. Springer, Berlin.
Chris Manning and Hinrich Schu?tze. 1999. Foundations
of Statistical Natural Language Processing. The MIT
Press, Cambridge, Massachusetts.
Heinrich Neiman, Elmar Noth, Anton Batliner, Jan
Buckow, Florian Gallwitz, Richard Huber, and Volkar
Warnke. 1998. Using prosodic cues in spoken dialog
systems. In Proceedings of the International Work-
shop on Speech and Computer, pages 17?28, St. Pe-
tersburg.
Elmar No?th, Anton Batliner, Andreas Kie?ling, Ralf
Kompe, and Heinrich Niemann. 2000. Verbmobil:
The use of prosody in the linguistic components of a
speech understanding system. IEEE Transactions on
Speech and Auditory Processing, 8(5):519?532.
Astrid Schepman and Paul Rodway. 2000. Prosody
and on-line parsing in coordination structures. The
Quarterly Journal of Experimental Psychology: A,
53(2):377?396.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-
Tur, and Gorkhan Tur. 2000. Prosody-based auto-
matic segmentation of speech into sentences and top-
ics. Speech Communication, 32(1-2):127?154.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 152?159,
New York, June 2006. c?2006 Association for Computational Linguistics
Effective Self-Training for Parsing
David McClosky, Eugene Charniak, and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{dmcc|ec|mj}@cs.brown.edu
Abstract
We present a simple, but surprisingly ef-
fective, method of self-training a two-
phase parser-reranker system using read-
ily available unlabeled data. We show
that this type of bootstrapping is possible
for parsing when the bootstrapped parses
are processed by a discriminative reranker.
Our improved model achieves an f -score
of 92.1%, an absolute 1.1% improvement
(12% error reduction) over the previous
best result for Wall Street Journal parsing.
Finally, we provide some analysis to bet-
ter understand the phenomenon.
1 Introduction
In parsing, we attempt to uncover the syntactic struc-
ture from a string of words. Much of the challenge
of this lies in extracting the appropriate parsing
decisions from textual examples. Given sufficient
labelled data, there are several ?supervised? tech-
niques of training high-performance parsers (Char-
niak and Johnson, 2005; Collins, 2000; Henderson,
2004). Other methods are ?semi-supervised? where
they use some labelled data to annotate unlabeled
data. Examples of this include self-training (Char-
niak, 1997) and co-training (Blum and Mitchell,
1998; Steedman et al, 2003). Finally, there are ?un-
supervised? strategies where no data is labeled and
all annotations (including the grammar itself) must
be discovered (Klein and Manning, 2002).
Semi-supervised and unsupervised methods are
important because good labeled data is expensive,
whereas there is no shortage of unlabeled data.
While some domain-language pairs have quite a bit
of labelled data (e.g. news text in English), many
other categories are not as fortunate. Less unsuper-
vised methods are more likely to be portable to these
new domains, since they do not rely as much on ex-
isting annotations.
2 Previous work
A simple method of incorporating unlabeled data
into a new model is self-training. In self-training,
the existing model first labels unlabeled data. The
newly labeled data is then treated as truth and com-
bined with the actual labeled data to train a new
model. This process can be iterated over different
sets of unlabeled data if desired. It is not surprising
that self-training is not normally effective: Charniak
(1997) and Steedman et al (2003) report either mi-
nor improvements or significant damage from using
self-training for parsing. Clark et al (2003) applies
self-training to POS-tagging and reports the same
outcomes. One would assume that errors in the orig-
inal model would be amplified in the new model.
Parser adaptation can be framed as a semi-
supervised or unsupervised learning problem. In
parser adaptation, one is given annotated training
data from a source domain and unannotated data
from a target. In some cases, some annotated data
from the target domain is available as well. The goal
is to use the various data sets to produce a model
that accurately parses the target domain data despite
seeing little or no annotated data from that domain.
Gildea (2001) and Bacchiani et al (2006) show that
out-of-domain training data can improve parsing ac-
152
curacy. The unsupervised adaptation experiment by
Bacchiani et al (2006) is the only successful in-
stance of parsing self-training that we have found.
Our work differs in that all our data is in-domain
while Bacchiani et al uses the Brown corpus as la-
belled data. These correspond to different scenarios.
Additionally, we explore the use of a reranker.
Co-training is another way to train models from
unlabeled data (Blum and Mitchell, 1998). Unlike
self-training, co-training requires multiple learners,
each with a different ?view? of the data. When one
learner is confident of its predictions about the data,
we apply the predicted label of the data to the train-
ing set of the other learners. A variation suggested
by Dasgupta et al (2001) is to add data to the train-
ing set when multiple learners agree on the label. If
this is the case, we can be more confident that the
data was labelled correctly than if only one learner
had labelled it.
Sarkar (2001) and Steedman et al (2003) inves-
tigated using co-training for parsing. These studies
suggest that this type of co-training is most effec-
tive when small amounts of labelled training data is
available. Additionally, co-training for parsing can
be helpful for parser adaptation.
3 Experimental Setup
Our parsing model consists of two phases. First, we
use a generative parser to produce a list of the top n
parses. Next, a discriminative reranker reorders the
n-best list. These components constitute two views
of the data, though the reranker?s view is restricted
to the parses suggested by the first-stage parser. The
reranker is not able to suggest new parses and, more-
over, uses the probability of each parse tree accord-
ing to the parser as a feature to perform the rerank-
ing. Nevertheless, the reranker?s value comes from
its ability to make use of more powerful features.
3.1 The first-stage 50-best parser
The first stage of our parser is the lexicalized proba-
bilistic context-free parser described in (Charniak,
2000) and (Charniak and Johnson, 2005). The
parser?s grammar is a smoothed third-order Markov
grammar, enhanced with lexical heads, their parts
of speech, and parent and grandparent informa-
tion. The parser uses five probability distributions,
one each for heads, their parts-of-speech, head-
constituent, left-of-head constituents, and right-of-
head constituents. As all distributions are condi-
tioned with five or more features, they are all heavily
backed off using Chen back-off (the average-count
method from Chen and Goodman (1996)). Also,
the statistics are lightly pruned to remove those that
are statistically less reliable/useful. As in (Char-
niak and Johnson, 2005) the parser has been mod-
ified to produce n-best parses. However, the n-best
parsing algorithm described in that paper has been
replaced by the much more efficient algorithm de-
scribed in (Jimenez and Marzal, 2000; Huang and
Chang, 2005).
3.2 The MaxEnt Reranker
The second stage of our parser is a Maximum En-
tropy reranker, as described in (Charniak and John-
son, 2005). The reranker takes the 50-best parses
for each sentence produced by the first-stage 50-
best parser and selects the best parse from those
50 parses. It does this using the reranking method-
ology described in Collins (2000), using a Maxi-
mum Entropy model with Gaussian regularization
as described in Johnson et al (1999). Our reranker
classifies each parse with respect to 1,333,519 fea-
tures (most of which only occur on few parses).
The features consist of those described in (Char-
niak and Johnson, 2005), together with an additional
601,577 features. These features consist of the parts-
of-speech, possibly together with the words, that
surround (i.e., precede or follow) the left and right
edges of each constituent. The features actually used
in the parser consist of all singletons and pairs of
such features that have different values for at least
one of the best and non-best parses of at least 5 sen-
tences in the training data. There are 147,456 such
features involving only parts-of-speech and 454,101
features involving parts-of-speech and words. These
additional features are largely responsible for im-
proving the reranker?s performance on section 23
to 91.3% f -score (Charniak and Johnson (2005) re-
ported an f -score of 91.0% on section 23).
3.3 Corpora
Our labeled data comes from the Penn Treebank
(Marcus et al, 1993) and consists of about 40,000
sentences from Wall Street Journal (WSJ) articles
153
annotated with syntactic information. We use the
standard divisions: Sections 2 through 21 are used
for training, section 24 is held-out development, and
section 23 is used for final testing. Our unlabeled
data is the North American News Text corpus, NANC
(Graff, 1995), which is approximately 24 million un-
labeled sentences from various news sources. NANC
contains no syntactic information. Sentence bound-
aries in NANC are induced by a simple discrimina-
tive model. We also perform some basic cleanups on
NANC to ease parsing. NANC contains news articles
from various news sources including the Wall Street
Journal, though for this paper, we only use articles
from the LA Times.
4 Experimental Results
We use the reranking parser to produce 50-best
parses of unlabeled news articles from NANC. Next,
we produce two sets of one-best lists from these 50-
best lists. The parser-best and reranker-best lists
represent the best parse for each sentence accord-
ing to the parser and reranker, respectively. Fi-
nally, we mix a portion of parser-best or reranker-
best lists with the standard Wall Street Journal train-
ing data (sections 2-21) to retrain a new parser (but
not reranker1) model. The Wall Street Journal train-
ing data is combined with the NANC data in the
following way: The count of each parsing event is
the (optionally weighted) sum of the counts of that
event in Wall Street Journal and NANC. Bacchiani
et al (2006) show that count merging is more effec-
tive than creating multiple models and calculating
weights for each model (model interpolation). Intu-
itively, this corresponds to concatenating our train-
ing sets, possibly with multiple copies of each to ac-
count for weighting.
Some notes regarding evaluations: All numbers
reported are f -scores2. In some cases, we evaluate
only the parser?s performance to isolate it from the
reranker. In other cases, we evaluate the reranking
parser as a whole. In these cases, we will use the
term reranking parser.
Table 1 shows the difference in parser?s (not
reranker?s) performance when trained on parser-best
1We attempted to retrain the reranker using the self-trained
sentences, but found no significant improvement.
2The harmonic mean of labeled precision (P) and labeled
recall (R), i.e. f = 2?P?RP+R
Sentences added Parser-best Reranker-best
0 (baseline) 90.3
50k 90.1 90.7
250k 90.1 90.7
500k 90.0 90.9
750k 89.9 91.0
1,000k 90.0 90.8
1,500k 90.0 90.8
2,000k ? 91.0
Table 1: f -scores after adding either parser-best or
reranker-best sentences from NANC to WSJ training
data. While the reranker was used to produce the
reranker-best sentences, we performed this evalua-
tion using only the first-stage parser to parse all sen-
tences from section 22. We did not train a model
where we added 2,000k parser-best sentences.
output versus reranker-best output. Adding parser-
best sentences recreates previous self-training ex-
periments and confirms that it is not beneficial.
However, we see a large improvement from adding
reranker-best sentences. One may expect to see a
monotonic improvement from this technique, but
this is not quite the case, as seen when we add
1,000k sentences. This may be due to some sec-
tions of NANC being less similar to WSJ or contain-
ing more noise. Another possibility is that these
sections contains harder sentences which we can-
not parse as accurately and thus are not as useful
for self-training. For our remaining experiments, we
will only use reranker-best lists.
We also attempt to discover the optimal number
of sentences to add from NANC. Much of the im-
provement comes from the addition of the initial
50,000 sentences, showing that even small amounts
of new data can have a significant effect. As we add
more data, it becomes clear that the maximum ben-
efit to parsing accuracy by strictly adding reranker-
best sentences is about 0.7% and that f -scores will
asymptote around 91.0%. We will return to this
when we consider the relative weightings of WSJ and
NANC data.
One hypothesis we consider is that the reranked
NANC data incorporated some of the features from
the reranker. If this were the case, we would not see
an improvement when evaluating a reranking parser
154
Sentences added 1 22 24
0 (baseline) 91.8 92.1 90.5
50k 91.8 92.4 90.8
250k 91.8 92.3 91.0
500k 92.0 92.4 90.9
750k 92.0 92.4 91.1
1,000k 92.1 92.2 91.3
1,500k 92.1 92.1 91.2
1,750k 92.1 92.0 91.3
2,000k 92.2 92.0 91.3
Table 2: f -scores from evaluating the rerank-
ing parser on three held-out sections after adding
reranked sentences from NANC to WSJ training.
These evaluations were performed on all sentences.
on the same models. In Table 2, we see that the new
NANC data contains some information orthogonal to
the reranker and improves parsing accuracy of the
reranking parser.
Up to this point, we have only considered giving
our true training data a relative weight of one. In-
creasing the weight of the Wall Street Journal data
should improve, or at least not hurt, parsing perfor-
mance. Indeed, this is the case for both the parser
(figure not shown) and reranking parser (Figure 1).
Adding more weight to the Wall Street Journal data
ensures that the counts of our events will be closer
to our more accurate data source while still incorpo-
rating new data from NANC. While it appears that
the performance still levels off after adding about
one million sentences from NANC, the curves cor-
responding to higher WSJ weights achieve a higher
asymptote. Looking at the performance of various
weights across sections 1, 22, and 24, we decided
that the best combination of training data is to give
WSJ a relative weight of 5 and use the first 1,750k
reranker-best sentences from NANC.
Finally, we evaluate our new model on the test
section of Wall Street Journal. In Table 3, we note
that baseline system (i.e. the parser and reranker
trained purely on Wall Street Journal) has improved
by 0.3% over Charniak and Johnson (2005). The
92.1% f -score is the 1.1% absolute improvement
mentioned in the abstract. The improvement from
self-training is significant in both macro and micro
tests (p < 10?5).
 91.7
 91.8
 91.9
 92
 92.1
 92.2
 92.3
 92.4
 0  5  10  15  20  25  30  35  40
f-s
co
re
NANC sentences added (units of 50k sentences)
WSJ x1
WSJ x3
WSJ x5
Figure 1: Effect of giving more relative weight to
WSJ training data on reranking parser f -score. Eval-
uations were done from all sentences from section
1.
Model fparser freranker
Charniak and Johnson (2005) ? 91.0
Current baseline 89.7 91.3
WSJ + NANC 91.0 92.1
Table 3: f -scores on WSJ section 23. fparser and
freranker are the evaluation of the parser and rerank-
ing parser on all sentences, respectively. ?WSJ +
NANC? represents the system trained on WSJ train-
ing (with a relative weight of 5) and 1,750k sen-
tences from the reranker-best list of NANC.
5 Analysis
We performed several types of analysis to better un-
derstand why the new model performs better. We
first look at global changes, and then at changes at
the sentence level.
5.1 Global Changes
It is important to keep in mind that while the
reranker seems to be key to our performance im-
provement, the reranker per se never sees the extra
data. It only sees the 50-best lists produced by the
first-stage parser. Thus, the nature of the changes to
this output is important.
We have already noted that the first-stage parser?s
one-best has significantly improved (see Table 1). In
Table 4, we see that the 50-best oracle rate also im-
155
Model 1-best 10-best 50-best
Baseline 89.0 94.0 95.9
WSJ?1 + 250k 89.8 94.6 96.2
WSJ?5 + 1,750k 90.4 94.8 96.4
Table 4: Oracle f -scores of top n parses produced
by baseline, a small self-trained parser, and the
?best? parser
proves from 95.5% for the original first-stage parser,
to 96.4% for our final model. We do not show it here,
but if we self-train using first-stage one-best, there is
no change in oracle rate.
The first-stage parser also becomes more ?deci-
sive.? The average (geometric mean) of log2(Pr(1-
best) / Pr(50th-best)) (i.e. the ratios between the
probabilities in log space) increases from 11.959 for
the baseline parser, to 14.104 for the final parser. We
have seen earlier that this ?confidence? is deserved,
as the first-stage one-best is so much better.
5.2 Sentence-level Analysis
To this point we have looked at bulk properties of the
data fed to the reranker. It has higher one best and
50-best-oracle rates, and the probabilities are more
skewed (the higher probabilities get higher, the lows
get lower). We now look at sentence-level proper-
ties. In particular, we analyzed the parsers? behav-
ior on 5,039 sentences in sections 1, 22 and 24 of
the Penn treebank. Specifically, we classified each
sentence into one of three classes: those where the
self-trained parser?s f -score increased relative to the
baseline parser?s f -score, those where the f -score
remained the same, and those where the self-trained
parser?s f -score decreased relative to the baseline
parser?s f -score. We analyzed the distribution of
sentences into these classes with respect to four fac-
tors: sentence length, the number of unknown words
(i.e., words not appearing in sections 2?21 of the
Penn treebank) in the sentence, the number of coor-
dinating conjunctions (CC) in the sentence, and the
number of prepositions (IN) in the sentence. The
distributions of classes (better, worse, no change)
with respect to each of these factors individually are
graphed in Figures 2 to 5.
Figure 2 shows how the self-training affects f -
score as a function of sentence length. The top line
0 10 20 30 40 50 60
20
40
60
80
10
0
Sentence length
N
um
be
r o
f s
en
te
nc
es
 (s
mo
oth
ed
)
Better
No change
Worse
Figure 2: How self-training improves performance
as a function of sentence length
shows that the f -score of most sentences remain un-
changed. The middle line is the number of sentences
that improved their f -score, and the bottom are those
which got worse. So, for example, for sentences of
length 30, about 80 were unchanged, 25 improved,
and 22 worsened. It seems clear that there is no
improvement for either very short sentences, or for
very long ones. (For long ones the graph is hard
to read. We show a regression analysis later in this
section that confirms this statement.) While we did
not predict this effect, in retrospect it seems reason-
able. The parser was already doing very well on
short sentences. The very long ones are hopeless,
and the middle ones are just right. We call this the
Goldilocks effect.
As for the other three of these graphs, their stories
are by no means clear. Figure 3 seems to indicate
that the number of unknown words in the sentence
does not predict that the reranker will help. Figure 4
might indicate that the self-training parser improves
prepositional-phrase attachment, but the graph looks
suspiciously like that for sentence length, so the im-
provements might just be due to the Goldilocks ef-
fect. Finally, the improvement in Figure 5 is hard to
judge.
To get a better handle on these effects we did a
factor analysis. The factors we consider are number
of CCs, INs, and unknowns, plus sentence length.
As Figure 2 makes clear, the relative performance
of the self-trained and baseline parsers does not
156
0 1 2 3 4 5
0
50
0
10
00
15
00
20
00
Unknown words
N
um
be
r o
f s
en
te
nc
es
Better
No change
Worse
Figure 3: How self-training improves performance
as a function of number of unknown words
Estimate Pr(> 0)
(Intercept) -0.25328 0.3649
BinnedLength(10,20] 0.02901 0.9228
BinnedLength(20,30] 0.45556 0.1201
BinnedLength(30,40] 0.40206 0.1808
BinnedLength(40,50] 0.26585 0.4084
BinnedLength(50,200] -0.06507 0.8671
CCs 0.12333 0.0541
Table 5: Factor analysis for the question: does the
self-trained parser improve the parse with the high-
est probability
vary linearly with sentence length, so we introduced
binned sentence length (with each bin of length 10)
as a factor.
Because the self-trained and baseline parsers pro-
duced equivalent output on 3,346 (66%) of the sen-
tences, we restricted attention to the 1,693 sentences
on which the self-trained and baseline parsers? f -
scores differ. We asked the program to consider the
following factors: binned sentence length, number
of PPs, number of unknown words, and number of
CCs. The results are shown in Table 5. The factor
analysis is trying to model the log odds as a sum of
linearly weighted factors. I.e,
log(P (1|x)/(1 ? P (1|x))) = ?0 +
m
?
j=1
?jfj(x)
In Table 5 the first column gives the name of the fac-
0 2 4 6 8 10
20
0
40
0
60
0
Number of INs
N
um
be
r o
f s
en
te
nc
es
Better
No change
Worse
Figure 4: How self-training improves performance
as a function of number of prepositions
tor. The second the change in the log-odds resulting
from this factor being present (in the case of CCs
and INs, multiplied by the number of them) and the
last column is the probability that this factor is really
non-zero.
Note that there is no row for either PPs or un-
known words. This is because we also asked the pro-
gram to do a model search using the Akaike Infor-
mation Criterion (AIC) over all single and pairwise
factors. The model it chooses predicts that the self-
trained parser is likely produce a better parse than
the baseline only for sentences of length 20?40 or
sentences containing several CCs. It did not include
the number of unknown words and the number of
INs as factors because they did not receive a weight
significantly different from zero, and the AIC model
search dropped them as factors from the model.
In other words, the self-trained parser is more
likely to be correct for sentences of length 20?
40 and as the number of CCs in the sentence in-
creases. The self-trained parser does not improve
prepositional-phrase attachment or the handling of
unknown words.
This result is mildly perplexing. It is fair to say
that neither we, nor anyone we talked to, thought
conjunction handling would be improved. Conjunc-
tions are about the hardest things in parsing, and we
have no grip on exactly what it takes to help parse
them. Conversely, everyone expected improvements
on unknown words, as the self-training should dras-
157
0 1 2 3 4 5
0
50
0
10
00
15
00
20
00
Number of CCs
N
um
be
r o
f s
en
te
nc
es
Better
No change
Worse
Figure 5: How self-training improves performance
as a function of number of conjunctions
tically reduce the number of them. It is also the case
that we thought PP attachment might be improved
because of the increased coverage of preposition-
noun and preposition-verb combinations that work
such as (Hindle and Rooth, 1993) show to be so im-
portant.
Currently, our best conjecture is that unknowns
are not improved because the words that are un-
known in the WSJ are not significantly represented
in the LA Times we used for self-training. CCs
are difficult for parsers because each conjunct has
only one secure boundary. This is particularly the
case with longer conjunctions, those of VPs and Ss.
One thing we know is that self-training always im-
proves performance of the parsing model when used
as a language model. We think CC improvement is
connected with this fact and our earlier point that
the probabilities of the 50-best parses are becoming
more skewed. In essence the model is learning, in
general, what VPs and Ss look like so it is becom-
ing easier to pull them out of the stew surrounding
the conjunct. Conversely, language modeling has
comparatively less reason to help PP attachment. As
long as the parser is doing it consistently, attaching
the PP either way will work almost as well.
6 Conclusion
Contrary to received wisdom, self-training can im-
prove parsing. In particular we have achieved an ab-
solute improvement of 0.8% over the baseline per-
formance. Together with a 0.3% improvement due
to superior reranking features, this is a 1.1% im-
provement over the previous best parser results for
section 23 of the Penn Treebank (from 91.0% to
92.1%). This corresponds to a 12% error reduc-
tion assuming that a 100% performance is possible,
which it is not. The preponderance of evidence sug-
gests that it is somehow the reranking aspect of the
parser that makes this possible, but given no idea of
why this should be, so we reserve final judgement
on this matter.
Also contrary to expectations, the error analy-
sis suggests that there has been no improvement in
either the handing of unknown words, nor prepo-
sitional phrases. Rather, there is a general im-
provement in intermediate-length sentences (20-50
words), but no improvement at the extremes: a phe-
nomenon we call the Goldilocks effect. The only
specific syntactic phenomenon that seems to be af-
fected is conjunctions. However, this is good news
since conjunctions have long been considered the
hardest of parsing problems.
There are many ways in which this research
should be continued. First, the error analysis needs
to be improved. Our tentative guess for why sen-
tences with unknown words failed to improve should
be verified or disproven. Second, there are many
other ways to use self-trained information in pars-
ing. Indeed, the current research was undertaken
as the control experiment in a program to try much
more complicated methods. We still have them
to try: restricting consideration to more accurately
parsed sentences as training data (sentence selec-
tion), trying to learn grammatical generalizations di-
rectly rather than simply including the data for train-
ing, etc.
Next there is the question of practicality. In terms
of speed, once the data is loaded, the new parser is
pretty much the same speed as the old ? just un-
der a second a sentence on average for treebank sen-
tences. However, the memory requirements are lar-
gish, about half a gigabyte just to store the data. We
are making our current best self-trained parser avail-
able3 as machines with a gigabyte or more of RAM
are becoming commonplace. Nevertheless, it would
be interesting to know if the data can be pruned to
3ftp://ftp.cs.brown.edu/pub/nlparser
158
make the entire system less bulky.
Finally, there is also the nature of the self-trained
data themselves. The data we use are from the LA
Times. Those of us in parsing have learned to expect
significant decreases in parsing accuracy even when
moving the short distance from LA Times to Wall
Street Journal. This seemingly has not occurred.
Does this mean that the reranking parser somehow
overcomes at least small genre differences? On this
point, we have some pilot experiments that show
great promise.
Acknowledgments
This work was supported by NSF grants LIS9720368, and
IIS0095940, and DARPA GALE contract HR0011-06-2-0001.
We would like to thank Michael Collins, Brian Roark, James
Henderson, Miles Osborne, and the BLLIP team for their com-
ments.
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the 11th Annual Conference on Computational
Learning Theory (COLT-98).
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 173?180, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of the Fourteenth National Conference on Artifi-
cial Intelligence, Menlo Park. AAAI Press/MIT Press.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In 1st Annual Meeting of the NAACL.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Arivind Joshi and Martha Palmer, editors,
Proceedings of the Thirty-Fourth Annual Meeting of
the Association for Computational Linguistics.
Stephen Clark, James Curran, and Miles Osborne. 2003.
Bootstrapping POS-taggers using unlabelled data. In
Proceedings of CoNLL-2003.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Machine Learning: Pro-
ceedings of the 17th International Conference (ICML
2000), pages 175?182, Stanford, California.
Sanjoy Dasgupta, M.L. Littman, and D. McAllester.
2001. PAC generalization bounds for co-training. In
Advances in Neural Information Processing Systems
(NIPS), 2001.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP).
David Graff. 1995. North American News Text Corpus.
Linguistic Data Consortium. LDC95T21.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In Proc. 42nd Meet-
ing of Association for Computational Linguistics (ACL
2004), Barcelona, Spain.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
19(1):103?120.
Liang Huang and David Chang. 2005. Better k-best pars-
ing. Technical Report MS-CIS-05-08, Department of
Computer Science, University of Pennsylvania.
Victor M. Jimenez and Andres Marzal. 2000. Computa-
tion of the n best parse trees for weighted and stochas-
tic context-free grammars. In Proceedings of the Joint
IAPR International Workshops on Advances in Pattern
Recognition. Springer LNCS 1876.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochas-
tic ?unification-based? grammars. In The Proceedings
of the 37th Annual Conference of the Association for
Computational Linguistics, pages 535?541, San Fran-
cisco. Morgan Kaufmann.
Dan Klein and Christopher Manning. 2002. A genera-
tive constituent-context model for improved grammar
induction. In Proceedings of the 40th Annual Meeting
of the ACL.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Anoop Sarkar. 2001. Applying cotraining methods to
statistical parsing. In Proceedings of the 2001 NAACL
Conference.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In Pro-
ceedings of EACL 03.
159
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 168?175,
New York, June 2006. c?2006 Association for Computational Linguistics
Multilevel Coarse-to-fine PCFG Parsing
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph Austerweil,
David Ellis, Isaac Haxton, Catherine Hill, R. Shrivaths,
Jeremy Moore, Michael Pozar, and Theresa Vu
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
ec@cs.brown.edu
Abstract
We present a PCFG parsing algorithm
that uses a multilevel coarse-to-fine
(mlctf) scheme to improve the effi-
ciency of search for the best parse.
Our approach requires the user to spec-
ify a sequence of nested partitions or
equivalence classes of the PCFG non-
terminals. We define a sequence of
PCFGs corresponding to each parti-
tion, where the nonterminals of each
PCFG are clusters of nonterminals of
the original source PCFG. We use the
results of parsing at a coarser level
(i.e., grammar defined in terms of a
coarser partition) to prune the next
finer level. We present experiments
showing that with our algorithm the
work load (as measured by the total
number of constituents processed) is
decreased by a factor of ten with no de-
crease in parsing accuracy compared to
standard CKY parsing with the origi-
nal PCFG. We suggest that the search
space over mlctf algorithms is almost
totally unexplored so that future work
should be able to improve significantly
on these results.
1 Introduction
Reasonably accurate constituent-based parsing
is fairly quick these days, if fairly quick means
about a second per sentence. Unfortunately, this
is still too slow for many applications. In some
cases researchers need large quantities of parsed
data and do not have the hundreds of machines
necessary to parse gigaword corpora in a week
or two. More pressingly, in real-time applica-
tions such as speech recognition, a parser would
be only a part of a much larger system, and
the system builders are not keen on giving the
parser one of the ten seconds available to pro-
cess, say, a thirty-word sentence. Even worse,
some applications require the parsing of multi-
ple candidate strings per sentence (Johnson and
Charniak, 2004) or parsing from a lattice (Hall
and Johnson, 2004), and in these applications
parsing efficiency is even more important.
We present here a multilevel coarse-to-fine
(mlctf) PCFG parsing algorithm that reduces
the complexity of the search involved in find-
ing the best parse. It defines a sequence of in-
creasingly more complex PCFGs, and uses the
parse forest produced by one PCFG to prune
the search of the next more complex PCFG.
We currently use four levels of grammars in our
mlctf algorithm. The simplest PCFG, which we
call the level-0 grammar, contains only one non-
trivial nonterminal and is so simple that min-
imal time is needed to parse a sentence using
it. Nonetheless, we demonstrate that it identi-
fies the locations of correct constituents of the
parse tree (the ?gold constituents?) with high
recall. Our level-1 grammar distinguishes only
argument from modifier phrases (i.e., it has two
nontrivial nonterminals), while our level-2 gram-
mar distinguishes the four major phrasal cate-
gories (verbal, nominal, adjectival and preposi-
tional phrases), and level 3 distinguishes all of
the standard categories of the Penn treebank.
168
The nonterminal categories in these grammars
can be regarded as clusters or equivalence classes
of the original Penn treebank nonterminal cat-
egories. (In fact, we obtain these grammars by
relabeling the node labels in the treebank and
extracting a PCFG from this relabelled treebank
in the standard fashion, but we discuss other ap-
proaches below.) We require that the partition
of the nonterminals defined by the equivalence
classes at level l + 1 be a refinement of the par-
tition defined at level l. This means that each
nonterminal category at level l+1 is mapped to a
unique nonterminal category at level l (although
in general the mapping is many to one, i.e., each
nonterminal category at level l corresponds to
several nonterminal categories at level l + 1).
We use the correspondence between categories
at different levels to prune possible constituents.
A constituent is considered at level l + 1 only
if the corresponding constituent at level l has
a probability exceeding some threshold.. Thus
parsing a sentence proceeds as follows. We first
parse the sentence with the level-0 grammar to
produce a parse forest using the CKY parsing
algorithm. Then for each level l + 1 we reparse
the sentence with the level l + 1 grammar us-
ing the level l parse forest to prune as described
above. As we demonstrate, this leads to consid-
erable efficiency improvements.
The paper proceeds as follows. We next dis-
cuss previous work (Section 2). Section 3 out-
lines the algorithm in more detail. Section
4 presents some experiments showing that the
work load (as measured by the total number of
constituents processed) is decreased by a fac-
tor of ten over standard CKY parsing at the
final level. We also discuss some fine points of
the results therein. Finally in section 5 we sug-
gest that because the search space of mlctf al-
gorithms is, at this point, almost totally unex-
plored, future work should be able to improve
significantly on these results.
2 Previous Research
Coarse-to-fine search is an idea that has ap-
peared several times in the literature of com-
putational linguistics and related areas. The
first appearance of this idea we are aware of is
in Maxwell and Kaplan (1993), where a cover-
ing CFG is automatically extracted from a more
detailed unification grammar and used to iden-
tify the possible locations of constituents in the
more detailed parses of the sentence. Maxwell
and Kaplan use their covering CFG to prune the
search of their unification grammar parser in es-
sentially the same manner as we do here, and
demonstrate significant performance improve-
ments by using their coarse-to-fine approach.
The basic theory of coarse-to-fine approxima-
tions and dynamic programming in a stochastic
framework is laid out in Geman and Kochanek
(2001). This paper describes the multilevel
dynamic programming algorithm needed for
coarse-to-fine analysis (which they apply to de-
coding rather than parsing), and show how
to perform exact coarse-to-fine computation,
rather than the heuristic search we perform here.
A paper closely related to ours is Goodman
(1997). In our terminology, Goodman?s parser
is a two-stage ctf parser. The second stage is a
standard tree-bank parser while the first stage is
a regular-expression approximation of the gram-
mar. Again, the second stage is constrained by
the parses found in the first stage. Neither stage
is smoothed. The parser of Charniak (2000)
is also a two-stage ctf model, where the first
stage is a smoothed Markov grammar (it uses
up to three previous constituents as context),
and the second stage is a lexicalized Markov
grammar with extra annotations about parents
and grandparents. The second stage explores
all of the constituents not pruned out after the
first stage. Related approaches are used in Hall
(2004) and Charniak and Johnson (2005).
A quite different approach to parsing effi-
ciency is taken in Caraballo and Charniak (1998)
(and refined in Charniak et al (1998)). Here
efficiency is gained by using a standard chart-
parsing algorithm and pulling constituents off
the agenda according to (an estimate of) their
probability given the sentence. This probability
is computed by estimating Equation 1:
p(nki,j | s) =
?(nki,j)?(nki,j)
p(s) . (1)
169
It must be estimated because during the
bottom-up chart-parsing algorithm, the true
outside probability cannot be computed. The
results cited in Caraballo and Charniak (1998)
cannot be compared directly to ours, but are
roughly in the same equivalence class. Those
presented in Charniak et al (1998) are superior,
but in Section 5 below we suggest that a com-
bination of the techniques could yield better re-
sults still.
Klein and Manning (2003a) describe efficient
A? for the most likely parse, where pruning is
accomplished by using Equation 1 and a true
upper bound on the outside probability. While
their maximum is a looser estimate of the out-
side probability, it is an admissible heuristic and
together with an A? search is guaranteed to find
the best parse first. One question is if the guar-
antee is worth the extra search required by the
looser estimate of the true outside probability.
Tsuruoka and Tsujii (2004) explore the frame-
work developed in Klein and Manning (2003a),
and seek ways to minimize the time required
by the heap manipulations necessary in this
scheme. They describe an iterative deepening
algorithm that does not require a heap. They
also speed computation by precomputing more
accurate upper bounds on the outside proba-
bilities of various kinds of constituents. They
are able to reduce by half the number of con-
stituents required to find the best parse (com-
pared to CKY).
Most recently, McDonald et al (2005) have
implemented a dependency parser with good
accuracy (it is almost as good at dependency
parsing as Charniak (2000)) and very impres-
sive speed (it is about ten times faster than
Collins (1997) and four times faster than Char-
niak (2000)). It achieves its speed in part be-
cause it uses the Eisner and Satta (1999) algo-
rithm for n3 bilexical parsing, but also because
dependency parsing has a much lower grammar
constant than does standard PCFG parsing ?
after all, there are no phrasal constituents to
consider. The current paper can be thought of
as a way to take the sting out of the grammar
constant for PCFGs by parsing first with very
few phrasal constituents and adding them only
Level: 0 1 2 3
S1
{
S1
{
S1
{
S1
P
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
HP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
S
?
?
?
?
?
?
?
?
?
?
?
?
?
S
VP
UCP
SQ
SBAR
SBARQ
SINV
N
?
?
?
?
?
?
?
?
?
?
?
?
?
NP
NAC
NX
LST
X
UCP
FRAG
MP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
A
?
?
?
?
?
?
?
?
?
?
?
?
?
ADJP
QP
CONJP
ADVP
INTJ
PRN
PRT
P
?
?
?
?
?
?
?
?
?
?
?
?
?
PP
PRT
RRC
WHADJP
WHADVP
WHNP
WHPP
Figure 1: The levels of nonterminal labels
after most constituents have been pruned away.
3 Multilevel Course-to-fine Parsing
We use as the underlying parsing algorithm a
reasonably standard CKY parser, modified to
allow unary branching rules.
The complete nonterminal clustering is given
in Figure 1. We do not cluster preterminals.
These remain fixed at all levels to the standard
Penn-tree-bank set Marcus et al (1993).
Level-0 makes two distinctions, the root node
and everybody else. At level 1 we make one
further distinction, between phrases that tend
to be heads of constituents (NPs, VPs, and Ss)
and those that tend to be modifiers (ADJPs,
PPs, etc.). Level-2 has a total of five categories:
root, things that are typically headed by nouns,
those headed by verbs, things headed by prepo-
sitions, and things headed by classical modifiers
(adjectives, adverbs, etc.). Finally, level 3 is the
170
S1
P
P
PRP
He
P
VBD
ate
P
IN
at
P
DT
the
NN
mall
.
.
S1
HP
HP
PRP
He
HP
VBD
ate
MP
IN
at
HP
DT
the
NN
mall
.
.
S1
S_
N_
PRP
He
S_
VBD
ate
P_
IN
at
N_
DT
the
NN
mall
.
.
S1
S
NP
PRP
He
VP
VBD
ate
PP
IN
at
NP
DT
the
NN
mall
.
.
Figure 2: A tree represented at levels 0 to 3
classical tree-bank set. As an example, Figure 2
shows the parse for the sentence ?He ate at the
mall.? at levels 0 to 3.
During training we create four grammars, one
for each level of granularity. So, for example, at
level 1 the tree-bank rule
S ?NP VP .
would be translated into the rule
HP ?HP HP .
That is, each constituent type found in ?S ?NP
VP .? is mapped into its generalization at level 1.
The probabilities of all rules are computed us-
ing maximum likelihood for constituents at that
level.
The grammar used by the parser can best be
described as being influenced by four compo-
nents:
1. the nonterminals defined at that level of
parsing,
2. the binarization scheme,
3. the generalizations defined over the bina-
rization, and
4. extra annotation to improve parsing accu-
racy.
The first of these has already been covered. We
discuss the other three in turn.
In anticipation of eventually lexicalizing the
grammar we binarize from the head out. For
example, consider the rule
A ?a b c d e
where c is the head constituent. We binarize
this as follows:
A ?A1 e
A1 ?A2 d
A2 ?a A3
A3 ?b c
Grammars induced in this way tend to be
too specific, as the binarization introduce a very
large number of very specialized phrasal cat-
egories (the Ai). Following common practice
Johnson (1998; Klein and Manning (2003b) we
Markovize by replacing these nonterminals with
ones that remember less of the immediate rule
context. In our version we keep track of only the
parent, the head constituent and the constituent
immediately to the right or left, depending on
which side of the constituent we are processing.
With this scheme the above rules now look like
this:
A ?Ad,c e
Ad,c ?Aa,c d
Aa,c ?a Ab,c
Ab,c ?b c
So, for example, the rule ?A ?Ad,c e? would
have a high probability if constituents of type
A, with c as their head, often have d followed
by e at their end.
Lastly, we add parent annotation to phrasal
categories to improve parsing accuracy. If we
assume that in this case we are expanding a rule
for an A used as a child of Q, and a, b, c, d, and
e are all phrasal categories, then the above rules
become:
AQ ?Ad,c eA
Ad,c ?Aa,c dA
Aa,c ?aA Ab,c
Ab,c ?bA cA
171
10?8 10?7 10?6 10?5 10?4 10?3
0.0001
0.001
0.01
0.1
 
 
Level 0
Level 1
Level 2
Level 3
Figure 3: Probability of a gold constituent being
pruned as a function of pruning thresholds for
the first 100 sentences of the development corpus
Once we have parsed at a level, we evaluate
the probability of a constituent p(nki,j | s) ac-
cording to the standard inside-outside formula
of Equation 1. In this equation nki,j is a con-
stituent of type k spanning the words i to j, and
?(?) and ?(?) are the outside and inside proba-
bilities of the constituent, respectively. Because
we prune at the end each granularity level, we
can evaluate the equation exactly; no approxi-
mations are needed (as in, e.g., Charniak et al
(1998)).
During parsing, instead of building each con-
stituent allowed by the grammar, we first test
if the probability of the corresponding coarser
constituent (which we have from Equation 1 in
the previous round of parsing) is greater than
a threshold. (The threshold is set empirically
based upon the development data.) If it is below
the threshold, we do not put the constituent in
the chart. For example, before we can use a NP
and a VP to create a S (using the rule above),
we would first need to check that the probability
in the coarser grammar of using the same span
HP and HP to create a HP is above the thresh-
old. We use the standard inside-outside for-
mula to calculate this probability (Equation 1).
The empirical results below justify our conjec-
ture that there are thresholds that allow signifi-
cant pruning while leaving the gold constituents
untouched.
10?8 10?7 10?6 10?5 10?4 10?3
0.001
0.01
0.1
1
 
 
Level 0
Level 1
Level 2
Level 3
Figure 4: Fraction of incorrect constituents kept
as a function of pruning thresholds for the first
100 sentences of the development corpus
4 Results
In all experiments the system is trained on the
Penn tree-bank sections 2-21. Section 23 is used
for testing and section 24 for development. The
input to the parser are the gold-standard parts
of speech, not the words.
The point of parsing at multiple levels of gran-
ularity is to prune the results of rough levels be-
fore going on to finer levels. In particular, it is
necessary for any pruning scheme to retain the
true (gold-standard WSJ) constituents in the
face of the pruning. To gain an idea of what
is possible, consider Figure 3. According to the
graph, at the zeroth level of parsing and a the
pruning level 10?4 the probability that a gold
constituent is deleted due to pruning is slightly
more than 0.001 (or 0.1%). At level three it is
slightly more that 0.01 (or 1.0%).
The companion figure, Figure 4 shows the
retention rate of the non-gold (incorrect) con-
stituents. Again, at pruning level 10?4 and pars-
ing level 0 we retain about .3 (30%) of the bad
constituents (so we pruned 70%), whereas at
level 3 we retain about .004 (0.4%). Note that
in the current paper we do not actually prune
at level 3, instead return the Viterbi parse. We
include pruning results here in anticipation of
future work in which level 3 would be a precur-
sor to still more fine-grained parsing.
As noted in Section 2, there is some (implicit)
172
Level Constits Constits % Pruned
Produced Pruned
?106 ?106
0 8.82 7.55 86.5
1 9.18 6.51 70.8
2 11.2 9.48 84.4
3 11,8 0 0.0
total 40.4 ? ?
3-only 392.0 0 0
Figure 5: Total constituents pruned at all levels
for WSJ section 23, sentences of length ? 100
debate in the literature on using estimates of
the outside probability in Equation 1, or instead
computing the exact upper bound. The idea is
that an exact upper bound gives one an admis-
sible search heuristic but at a cost, since it is a
less accurate estimator of the true outside prob-
ability. (Note that even the upper bound does
not, in general, keep all of the gold constituents,
since a non-perfect model will assign some of
them low probability.) As is clear from Figure
3, the estimate works very well indeed.
On the basis of this graph, we set the lowest
allowable constituent probability at ? 5 ? 10?4,
? 10?5, and ? 10?4 for levels 0,1, and 2, re-
spectively. No pruning is done at level 3, since
there is no level 4. After setting the pruning
parameters on the development set we proceed
to parse the test set (WSJ section 23). Figure 5
shows the resulting pruning statistics. The to-
tal number of constituents created at level 0, for
all sentences combined, is 8.82 ? 106. Of those
7.55 ? 106 (or 86.5%) are pruned before going on
to level 1. At level 1, the 1.3 million left over
from level 0 expanded to a total of 9.18 ? 106.
70.8% of these in turn are pruned, and so forth.
The percent pruned at, e.g., level 1 in Figure 3
is much higher than that shown here because it
considers all of the possible level-1 constituents,
not just those left unpruned after level 0.
There is no pruning at level 3. There we sim-
ply return the Viterbi parse. We also show that
with pruning we generate a total of 40.4 ? 106
constituents. For comparison exhaustively pars-
ing using the tree-bank grammar yields a total
of 392 ? 106 constituents. This is the factor-of-10
Level Time for Level Running Total
0 1598 1598
1 2570 4168
2 4303 8471
3 1527 9998
3-only 114654 ?
Figure 6: Running times in seconds on WSJ sec-
tion 23, with and without pruning
workload reduction mentioned in Section 1.
There are two points of interest. The first is
that each level of pruning is worthwhile. We do
not get most of the effect from one or the other
level. The second point is that we get signif-
icant pruning at level 0. The reader may re-
member that level 0 distinguishes only between
the root node and the rest. We initially ex-
pected that it would be too coarse to distinguish
good from bad constituents at this level, but it
proved as useful as the other levels. The expla-
nation is that this level does use the full tree-
bank preterminal tags, and in many cases these
alone are sufficient to make certain constituents
very unlikely. For example, what is the proba-
bility of any constituent of length two or greater
ending in a preposition? The answer is: very
low. Similarly for constituents of length two or
greater ending in modal verbs, and determiners.
Not quite so improbable, but nevertheless less
likely than most, would be constituents ending
in verbs, or ending just short of the end of the
sentence.
Figure 6 shows how much time is spent at each
level of the algorithm, along with a running to-
tal of the time spent to that point. (This is for
all sentences in the test set, length ? 100.) The
number for the unpruned parser is again about
ten times that for the pruned version, but the
number for the standard CKY version is prob-
ably too high. Because our CKY implementa-
tion is quite slow, we ran the unpruned version
on many machines and summed the results. In
all likelihood at least some of these machines
were overloaded, a fact that our local job dis-
tributer would not notice. We suspect that the
real number is significantly lower, though still
173
No pruning 77.9
With pruning 77.9
Klein and Manning (2003b) 77.4
Figure 7: Labeled precision/recall f-measure,
WSJ section 23, all sentences of length ? 100
much higher than the pruned version.
Finally Figure 7 shows that our pruning is ac-
complished without loss of accuracy. The results
with pruning include four sentences that did not
receive any parses at all. These sentences re-
ceived zeros for both precision and recall and
presumably lowered the results somewhat. We
allowed ourselves to look at the first of these,
which turned out to contain the phrase:
(NP ... (INTJ (UH oh) (UH yes)) ...)
The training data does not include interjections
consisting of two ?UH?s, and thus a gold parse
cannot be constructed. Note that a different
binarization scheme (e.g. the one used in Klein
and Manning (2003b) would have smoothed over
this problem. In our case the unpruned version
is able to patch together a lot of very unlikely
constituents to produce a parse, but not a very
good one. Thus we attribute the problem not to
pruning, but to binarization.
We also show the results for the most similar
Klein and Manning (2003b) experiment. Our
results are slightly better. We attribute the dif-
ference to the fact that we have the gold tags
and they do not, but their binarization scheme
does not run into the problems that we encoun-
tered.
5 Conclusion and Future Research
We have presented a novel parsing algorithm
based upon the coarse-to-fine processing model.
Several aspects of the method recommend it.
First, unlike methods that depend on best-first
search, the method is ?holistic? in its evalua-
tion of constituents. For example, consider the
impact of parent labeling. It has been repeat-
edly shown to improve parsing accuracy (John-
son, 1998; Charniak, 2000; Klein and Manning,
2003b), but it is difficult if not impossible to
integrate with best-first search in bottom-up
chart-parsing (as in Charniak et al (1998)). The
reason is that when working bottom up it is diffi-
cult to determine if, say, ssbar is any more or less
likely than ss, as the evidence, working bottom
up, is negligible. Since our method computes
the exact outside probability of constituents (al-
beit at a coarser level) all of the top down in-
formation is available to the system. Or again,
another very useful feature in English parsing
is the knowledge that a constituent ends at the
right boundary (minus punctuation) of a string.
This can be included only in an ad-hoc way when
working bottom up, but could be easily added
here.
Many aspects of the current implementation
that are far from optimal. It seems clear to
us that extracting the maximum benefit from
our pruning would involve taking the unpruned
constituents and specifying them in all possible
ways allowed by the next level of granularity.
What we actually did is to propose all possi-
ble constituents at the next level, and immedi-
ately rule out those lacking a corresponding con-
stituent remaining at the previous level. This
was dictated by ease of implementation. Before
using mlctf parsing in a production parser, the
other method should be evaluated to see if our
intuitions of greater efficiency are correct.
It is also possible to combine mlctf parsing
with queue reordering methods. The best-first
search method of Charniak et al (1998) esti-
mates Equation 1. Working bottom up, estimat-
ing the inside probability is easy (we just sum
the probability of all the trees found to build
this constituent). All the cleverness goes into
estimating the outside probability. Quite clearly
the current method could be used to provide a
more accurate estimate of the outside probabil-
ity, namely the outside probability at the coarser
level of granularity.
There is one more future-research topic to add
before we stop, possibly the most interesting of
all. The particular tree of coarser to finer con-
stituents that governs our mlctf algorithm (Fig-
ure 1) was created by hand after about 15 min-
utes of reflection and survives, except for typos,
with only two modifications. There is no rea-
174
son to think it is anywhere close to optimal. It
should be possible to define ?optimal? formally
and search for the best mlctf constituent tree.
This would be a clustering problem, and, for-
tunately, one thing statistical NLP researchers
know how to do is cluster.
Acknowledgments
This paper is the class project for Computer
Science 241 at Brown University in fall 2005.
The faculty involved were supported in part
by DARPA GALE contract HR0011-06-2-0001.
The graduate students were mostly supported
by Brown University fellowships. The under-
graduates were mostly supported by their par-
ents. Our thanks to all.
References
Sharon Caraballo and Eugene Charniak. 1998. Fig-
ures of merit for best-first probabalistic parsing.
Computational Linguistics, 24(2):275?298.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 2005 Meeting of
the Association for Computational Linguistics.
Eugene Charniak, Sharon Goldwater, and Mark
Johnson. 1998. Edge-based best-first chart pars-
ing. In Proceedings of the Sixth Workshop on
Very Large Corpora, pages 127?133. Morgan Kauf-
mann.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 132?139.
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics, San Francisco. Mor-
gan Kaufmann.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head
automaton grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 457?464.
Stuart Geman and Kevin Kochanek. 2001. Dy-
namic programming and the representation of
soft-decodable codes. IEEE Transactions on In-
formation Theory, 47:549?568.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 1997).
Keith Hall and Mark Johnson. 2004. Attention shift-
ing for parsing speech. In The Proceedings of the
42th Annual Meeting of the Association for Com-
putational Linguistics, pages 40?46.
Keith Hall. 2004. Best-first Word-lattice Pars-
ing: Techniques for Integrated Syntactic Language
Modeling. Ph.D. thesis, Brown University.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy-channel model of speech repairs. In
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 33?
39.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Chris Manning. 2003a. A* parsing:
Fast exact viterbi parse selection. In Proceedings
of HLT-NAACL?03.
Dan Klein and Christopher Manning. 2003b. Accu-
rate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics.
Michell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?
330.
John T. Maxwell and Ronald M. Kaplan. 1993.
The interface between phrasal and functional con-
straints. Computational Linguistics, 19(4):571?
590.
Ryan McDonald, Toby Crammer, and Fernando
Pereira. 2005. Online large margin training of
dependency parsers. In Proceedings of the 43rd
Meeting of the Association for Computational Lin-
guistics.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2004. It-
erative cky parsing for probabilistic context-free
grammars. In International Joint Conference on
Natural-Language Processing.
175
Proceedings of NAACL HLT 2007, pages 436?443,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Unified Local and Global Model for Discourse Coherence
Micha Elsner, Joseph Austerweil, and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{melsner,ec}@cs.brown.edu, joseph.austerweil@gmail.com
Abstract
We present a model for discourse co-
herence which combines the local entity-
based approach of (Barzilay and Lapata,
2005) and the HMM-based content model
of (Barzilay and Lee, 2004). Unlike the
mixture model of (Soricut and Marcu,
2006), we learn local and global features
jointly, providing a better theoretical ex-
planation of how they are useful. As the
local component of our model we adapt
(Barzilay and Lapata, 2005) by relaxing
independence assumptions so that it is ef-
fective when estimated generatively. Our
model performs the ordering task compet-
itively with (Soricut and Marcu, 2006),
and significantly better than either of the
models it is based on.
1 Introduction
Models of coherent discourse are central to several
tasks in natural language processing: such mod-
els have been used in text generation (Kibble and
Power, 2004) and evaluation of human-produced
text in educational applications (Miltsakaki and Ku-
kich, 2004; Higgins et al, 2004). Moreover, an ac-
curate model can reveal information about document
structure, aiding in such tasks as supervised summa-
rization (Barzilay and Lapata, 2005).
Models of coherence tend to fall into two classes.
Local models (Lapata, 2003; Barzilay and Lapata,
2005; Foltz et al, 1998) attempt to capture the gen-
eralization that adjacent sentences often have similar
content, and therefore tend to contain related words.
Models of this type are good at finding sentences
that belong near one another in the document. How-
ever, they have trouble finding the beginning or end
of the document, or recovering from sudden shifts in
topic (such as occur at paragraph boundaries). Some
local models also have trouble deciding which of a
pair of related sentences ought to come first.
In contrast, the global HMM model of Barzilay
and Lee (2004) tries to track the predictable changes
in topic between sentences. This gives it a pro-
nounced advantage in ordering sentences, since it
can learn to represent beginnings, ends and bound-
aries as separate states. However, it has no local
features; the particular words in each sentence are
generated based only on the current state of the doc-
ument. Since information can pass from sentence
to sentence only in this restricted manner, the model
sometimes fails to place sentences next to the correct
neighbors.
We attempt here to unify the two approaches by
constructing a model with both sentence-to-sentence
dependencies providing local cues, and a hidden
topic variable for global structure. Our local fea-
tures are based on the entity grid model of (Barzilay
and Lapata, 2005; Lapata and Barzilay, 2005). This
model has previously been most successful in a con-
ditional setting; to integrate it into our model, we
first relax its independence assumptions to improve
its performance when used generatively. Our global
model is an HMM like that of Barzilay and Lee
(2004), but with emission probabilities drawn from
the entity grid. We present results for two tasks,
the ordering task, on which global models usually
do well, and the discrimination task, on which lo-
cal models tend to outperform them. Our model im-
proves on purely global or local approaches on both
436
tasks.
Previous work by Soricut and Marcu (2006) has
also attempted to integrate local and global fea-
tures using a mixture model, with promising results.
However, mixture models lack explanatory power;
since each of the individual component models is
known to be flawed, it is difficult to say that the com-
bination is theoretically more sound than the parts,
even if it usually works better. Moreover, since the
model we describe uses a strict subset of the fea-
tures used in the component models of (Soricut and
Marcu, 2006), we suspect that adding it to the mix-
ture would lead to still further improved results.
2 Naive Entity Grids
Entity grids, first described in (Lapata and Barzilay,
2005), are designed to capture some ideas of Cen-
tering Theory (Grosz et al, 1995), namely that ad-
jacent utterances in a locally coherent discourses are
likely to contain the same nouns, and that important
nouns often appear in syntactically important roles
such as subject or object. An entity grid represents
a document as a matrix with a column for each en-
tity, and a row for each sentence. The entry ri,j de-
scribes the syntactic role of entity j in sentence i:
these roles are subject (S), object (O), or some other
role (X)1. In addition there is a special marker (-)
for nouns which do not appear at all in a given sen-
tence. Each noun appears only once in a given row
of the grid; if a noun appears multiple times, its grid
symbol describes the most important of its syntac-
tic roles: subject if possible, then object, or finally
other. An example text is figure 1, whose grid is fig-
ure 2.
Nouns are also treated as salient or non-salient,
another important concern of Centering Theory. We
condition events involving a noun on the frequency
of that noun. Unfortunately, this way of representing
salience makes our model slightly deficient, since
the model conditions on a particular noun occurring
e.g. 2 times, but assigns nonzero probabilities to
documents where it occurs 3 times. This is theo-
1Roles are determined heuristically using trees produced by
the parser of (Charniak and Johnson, 2005). Following previous
work, we slightly conflate thematic and syntactic roles, marking
the subject of a passive verb as O.
2The numeric token ?1300? is removed in preprocessing,
and ?Nuevo Laredo? is marked as ?PROPER?.
0 [The commercial pilot]O , [sole occupant of [the airplane]X]X
, was not injured .
1 [The airplane]O was owned and operated by [a private
owner]X .
2 [Visual meteorological conditions]S prevailed for [the per-
sonal cross country flight for which [a VFR flight plan]O was
filed]X .
3 [The flight]S originated at [Nuevo Laredo , Mexico]X , at
[approximately 1300]X .
Figure 1: A section of a document, with syntactic
roles of noun phrases marked.
0 1 2 3
PLAN - - O -
AIRPLANE X O - -
CONDITION - - S -
FLIGHT - - X S
PILOT O - - -
PROPER - - - X
OWNER - X - -
OCCUPANT X - - -
Figure 2: The entity grid for figure 12.
retically quite unpleasant but in comparing different
orderings of the same document, it seems not to do
too much damage.
Properly speaking entities may be referents of
many different nouns and pronouns throughout the
discourse, and both (Lapata and Barzilay, 2005) and
(Barzilay and Lapata, 2005) present models which
use coreference resolution systems to group nouns.
We follow (Soricut and Marcu, 2006) in dropping
this component of the system, and treat each head
noun as having an individual single referent.
To model transitions in this entity grid model,
Lapata and Barzilay (2005) takes a generative ap-
proach. First, the probability of a document is de-
fined as P (D) = P (Si..Sn), the joint probability of
all the sentences. Sentences are generated in order
conditioned on all previous sentences:
P (D) =
?
i
P (Si|S0..(i?1)). (1)
We make a Markov assumption of order h (in our
experiments h = 2) to shorten the history. We repre-
sent the truncated history as ~Shi?1 = S(i?h)..S(i?1).
Each sentence Si can be split up into a set of
nouns representing entities, Ei, and their corre-
sponding syntactic roles Ri, plus a set of words
which are not entities, Wi. The model treats Wi as
independent of the previous sentences. For any fixed
437
set of sentences Si,
?
i P (Wi) is always constant,
and so cannot help in finding a coherent ordering.
The probability of a sentence is therefore dependent
only on the entities:
P (Si|~Sh(i?1)) = P (Ei, Ri|~Sh(i?1)). (2)
Next, the model assumes that each entity ej ap-
pears in sentences and takes on syntactic roles in-
dependent of all the other entities. As we show
in section 3, this assumption can be problem-
atic. Once we assume this, however, we can sim-
plify P (Ei, Ri|~Sh(i?1)) by calculating for each en-
tity whether it occurs in sentence i and if so, which
role it takes. This is equivalent to predicting ri,j .
We represent the history of the specific entity ej as
~r h(i?1),j = r(i?h),j ..r(i?1),j , and write:
P (Ei, Ri|~Sh(i?1)) ?
?
j
P (ri,j|~r h(i?1),j). (3)
For instance, in figure 2, the probability of S3 with
horizon 1 is the product of P (S|X) (for FLIGHT),
P (X|?) (for PROPER), and likewise for each other
entity, P (?|O), P (?|S), P (?|?)3.
Although this generative approach outperforms
several models in correlation with coherence ratings
assigned by human judges, it suffers in comparison
with later systems. Barzilay and Lapata (2005) uses
the same grid representation, but treats the transi-
tion probabilities P (ri,j |~ri,j) for each document as
features for input to an SVM classifier. Soricut and
Marcu (2006)?s implementation of the entity-based
model also uses discriminative training.
The generative model?s main weakness in com-
parison to these conditional models is its assump-
tion of independence between entities. In real doc-
uments, each sentence tends to contain only a few
nouns, and even fewer of them can fill roles like
subject and object. In other words, nouns compete
with each other for the available syntactic positions
in sentences; once one noun is chosen as the sub-
ject, the probability that any other will also become
a subject (of a different subclause of the same sen-
tence) is drastically lowered. Since the generative
entity grid does not take this into account, it learns
that in general, the probability of any given entity
appearing in a specific sentence is low. Thus it gen-
erates blank sentences (those without any nouns at
all) with overwhelmingly high probability.
It may not be obvious that this misallocation of
probability mass also reduces the effectiveness of
the generative entity grid in ordering fixed sets of
sentences. However, consider the case where an en-
tity has a history ~r h, and then does not appear in
the next sentence. The model treats this as evidence
that entities generally do not occur immediately af-
ter ~r h? but it may also happen that the entity was
outcompeted by some other word with even more
significance.
3 Relaxed Entity Grid
In this section, we relax the troublesome assump-
tion of independence between entities, thus mov-
ing the probability distribution over documents away
from blank sentences. We begin at the same point as
above: sequential generation of sentences: P (D) =
?
i P (Si|S0..(i?1)). We similarly separate the words
into entities and non-entities, treat the non-entities as
independent of the history ~S and omit them. We also
distinguish two types of entities. Let the known set
Ki = ej : ej ? ~S(i?1), the set of all entities which
have appeared before sentence i. Of the entities ap-
pearing in Si, those in Ki are known entities, and
those which are not are new entities. Since each en-
tity in the document is new precisely once, we treat
these as independent and omit them from our calcu-
lations as we did the non-entities. We return to both
groups of omitted words in section 4 below when
discussing our topic-based models.
To model a sentence, then, we generate the set of
known entities it contains along with their syntac-
tic roles, given the history and the known set Ki.
We truncate the history, as above, with horizon h;
note that this does not make the model Markovian,
since the known set has no horizon. Finally, we con-
sider only the portion of the history which relates to
known nouns (since all non-known nouns have the
same history - -). In all the equations below, we re-
strict Ei to known entities which actually appear in
sentence i, and Ri to roles filled by known entities.
The probability of a sentence is now:
P (Si|~Sh(i?1)) = P (Ei, Ri|~Rh(i?1)). (4)
We make one further simplification before begin-
ning to approximate: we first generate the set of syn-
tactic slots Ri which we intend to fill with known en-
tities, and then decide which entities from the known
438
set to select. Again, we assume independence from
the history, so that the contribution of P (Ri) for any
ordering of a fixed set of sentences is constant and
we omit it:
P (Ei, Ri|~Rh(i?1),j) = P (Ei|Ri, ~Rh(i?1),j). (5)
Estimating P (Ei|Ri, ~Rh(i?1),j) proves to be dif-
ficult, since the contexts are very sparse. To con-
tinue, we make a series of approximations. First let
each role be filled individually (where r ? e is the
boolean indicator function ?noun e fills role r?):
P (Ei|Ri, ~Rh(i?1),j) ?
?
r?Ri
P (r ? ej |r, ~Rh(i?1),j).
(6)
Notice that this process can select the same noun ej
to fill multiple roles r, while the entity grid cannot
represent such an occurrence. The resulting distri-
bution is therefore slightly deficient.
Unfortunately, we are still faced with the sparse
context ~Rh(i?1),j , the set of histories of all currently
known nouns. It is much easier to estimate P (r ?
ej |r,~r h(i?1),j), where we condition only on the his-
tory of the particular noun which is chosen to fill
slot r. However, in this case we do not have a proper
probability distribution: i.e. the probabilities do not
sum to 1. To overcome this difficulty we simply nor-
malize by force3:
P (r ? ej|r, ~Rh(i?1),j) ? (7)
P (r ? ej |r,~r h(i?1),j)
?
j?Ki P (r ? ej|r,~r h(i?1),j)
The individual probabilities P (r ? ej |r,~r h(i?1),j)
are calculated by counting situations in the train-
ing documents in which a known noun has his-
tory ~r h(i?1),j and fills slot r in the next sentence,
versus situations where the slot r exists but is
filled by some other noun. Some rare contexts are
still sparse, and so we smooth by adding a pseu-
docount of 1 for all events. Our model is ex-
pressed by equations (1),(4),(5),(6) and (7). In
3Unfortunately this estimator is not consistent (that is, given
infinite training data produced by the model, the estimated pa-
rameters do not converge to the true parameters). We are in-
vestigating maximum entropy estimation as a solution to this
problem.
figure 2, the probability of S3 with horizon 1 is
now calculated as follows: the known set con-
tains PLAN, AIRPLANE, CONDITION, FLIGHT,
PILOT, OWNER and OCCUPANT. There is one syn-
tactic role filled by a known noun, S. The proba-
bility is then calculated as P (+|S,X) (the proba-
bility of selecting a noun with history X to fill the
role of S) normalized by P (+|S,O)+P (+|S,S)+
P (+|S,X) + 4? P (+|S,?).
Like Lapata and Barzilay (2005), our relaxed
model assigns low probability to sentences where
nouns with important-seeming histories do not ap-
pear. However, in our model, the penalty is less
severe if there are many competitor nouns. On the
other hand, if the sentence contains many slots, giv-
ing the noun more opportunity to fill one of them,
the penalty is proportionally greater if it does not
appear.
4 Topic-Based Model
The model we describe above is a purely local one,
and moreover it relies on a particular set of local fea-
tures which capture the way adjacent sentences tend
to share lexical choices. Its lack of any global struc-
ture makes it impossible for the model to recover at
a paragraph boundary, or to accurately guess which
sentence should begin a document. Its lack of lexi-
calization, meanwhile, renders it incapable of learn-
ing dependences between pairs of words: for in-
stance, that a sentence discussing a crash is often
followed by a casualty report.
We remedy both these problems by extending our
model of document generation. Like Barzilay and
Lee (2004), we learn an HMM in which each sen-
tence has a hidden topic qi, which is chosen con-
ditioned on the previous state qi?1. The emission
model of each state is an instance of the relaxed en-
tity grid model as described above, but in addition
to conditioning on the role and history, we condi-
tion also on the state and on the particular set of
lexical items lex(Ki) which may be selected to fill
the role: P (r ? ej |r, ~Rh(i?1),j , qi, lex(Ki)). This
distribution is approximated as above by the nor-
malized value of P (r ? ej |r,~r h(i?1),j , qi, lex(ej)).
However, due to our use of lexical information,
even this may be too sparse for accurate estima-
tion, so we back off by interpolating with the pre-
439
Figure 3: A single time-slice of our HMM.
Wi ? PY (?|qi; ?LM , discountLM )
Ni ? PY (?|qi; ?NN , discountNN )
Ei ? EGrid(?|R, ~R2i?1, qi, lex(Ki); ?EG)
qi ? DP (?|qi?1)
In the equations above, only the manually set inter-
polation hyperparameters are indicated.
vious model. In each context, we introduce ?EG
pseudo-observations, split fractionally according to
the backoff distribution: if we abbreviate the context
in the relaxed entity grid as C and the event as e, this
smoothing corresponds to:
P (e|C, qi, ej) =
#(e,C, qi, ej) + ?EGP (e|C)
#(e,C, qi, ej) + ?EG
.
This is equivalent to defining the topic-based entity
grid as a Dirichlet process with parameter ?EG sam-
pling from the relaxed entity grid.
In addition, we are now in a position to gener-
ate the non-entity words Wi and new entities Ni in
an informative way, by conditioning on the sentence
topic qi. Since they are interrupted by the known
entities, they do not form contiguous sequences of
words, so we make a bag-of-words assumption. To
model these sets of words, we use unigram ver-
sions of the hierarchical Pitman-Yor processes of
(Teh, 2006), which implement a Bayesian version
of Kneser-Ney smoothing.
To represent the HMM itself, we adapt the non-
parametric HMM of (Beal et al, 2001). This is
a Bayesian alternative to the conventional HMM
model learned using EM, chosen mostly for conve-
nience. Our variant of it, unlike (Beal et al, 2001),
has no parameter ? to control self-transitions; our
emission model is complex enough to make it un-
necessary.
The actual number of states found by the model
depends mostly on the backoff constants, the ?s
(and, for Pitman-Yor processes, discounts) chosen
for the emission models (the entity grid, non-entity
word model and new noun model), and is relatively
insensitive to particular choices of prior for the other
hyperparameters. As the backoff constants decrease,
the emission models become more dependent on the
state variable q, which leads to more states (and
eventually to memorization of the training data). If
instead the backoff rate increases, the emission mod-
els all become close to the general distribution and
the model prefers relatively few states. We train with
interpolations which generally result in around 40
states.
Once the interpolation constants are set, the
model can be trained by Gibbs sampling. We also
do inference over the remaining hyperparameters of
the model by Metropolis sampling from uninforma-
tive priors. Convergence is generally very rapid; we
obtain good results after about 10 iterations. Unlike
Barzilay and Lee (2004), we do not initialize with
an informative starting distribution.
When finding the probability of a test document,
we do not do inference over the full Bayesian model,
because the number of states, and the probability of
different transitions, can change with every new ob-
servation, making dynamic programming impossi-
ble. Beal et al (2001) proposes an inference algo-
rithm based on particle filters, but we feel that in
this case, the effects are relatively minor, so we ap-
proximate by treating the model as a standard HMM,
using a fixed transition function based only on the
training data. This allows us to use the conventional
Viterbi algorithm. The backoff rates we choose at
training time are typically too small for optimal in-
ference in the ordering task. Before doing tests, we
set them to higher values (determined to optimize
ordering performance on held-out data) so that our
emission distributions are properly smoothed.
5 Experiments
Our experiments use the popular AIRPLANE cor-
pus, a collection of documents describing airplane
crashes taken from the database of the National
440
Transportation Safety Board, used in (Barzilay and
Lee, 2004; Barzilay and Lapata, 2005; Soricut and
Marcu, 2006). We use the standard division of
the corpus into 100 training and 100 test docu-
ments; for development purposes we did 10-fold
cross-validation on the training data. The AIRPLANE
documents have some advantages for coherence re-
search: they are short (11.5 sentences on average)
and quite formulaic, which makes it easy to find lex-
ical and structural patterns. On the other hand, they
do have some oddities. 46 of the training documents
begin with a standard preamble: ?This is prelimi-
nary information, subject to change, and may con-
tain errors. Any errors in this report will be corrected
when the final report has been completed,? which
essentially gives coherence models the first two sen-
tences for free. Others, however, begin abruptly with
no introductory material whatsoever, and sometimes
without even providing references for their definite
noun phrases; one document begins: ?At V1, the
DC-10-30?s number 1 engine, a General Electric
CF6-50C2, experienced a casing breach when the
2nd-stage low pressure turbine (LPT) anti-rotation
nozzle locks failed.? Even humans might have trou-
ble identifying this sentence as the beginning of a
document.
5.1 Sentence Ordering
In the sentence ordering task, (Lapata, 2003; Barzi-
lay and Lee, 2004; Barzilay and Lapata, 2005; Sori-
cut and Marcu, 2006), we view a document as an
unordered bag of sentences and try to find the or-
dering of the sentences which maximizes coherence
according to our model. This type of ordering pro-
cess has applications in natural language generation
and multi-document summarization. Unfortunately,
finding the optimal ordering according to a prob-
abilistic model with local features is NP-complete
and non-approximable (Althaus et al, 2004). More-
over, since our model is not Markovian, the relax-
ation used as a heuristic for A? search by Soricut
and Marcu (2006) is ineffective. We therefore use
simulated annealing to find a high-probability order-
ing, starting from a random permutation of the sen-
tences. Our search system has few Estimated Search
Errors as defined by Soricut and Marcu (2006); it
rarely proposes an ordering which has lower proba-
? Discr. (%)
(Barzilay and Lapata, 2005) - 90
(Barzilay and Lee, 2004) .44 745
(Soricut and Marcu, 2006) .50 -6
Topic-based (relaxed) .50 94
Table 1: Results for AIRPLANE test data.
bility than the original ordering4 .
To evaluate the quality of the orderings we predict
as optimal, we use Kendall?s ? , a measurement of
the number of pairwise swaps needed to transform
our proposed ordering into the original document,
normalized to lie between ?1 (reverse order) and 1
(original order). Lapata (2006) shows that it corre-
sponds well with human judgements of coherence
and reading times. A slight problem with ? is that
it does not always distinguish between proposed or-
derings of a document which disrupt local relation-
ships at random, and orderings in which paragraph-
like units move as a whole. In longer documents, it
may be worth taking this problem into account when
selecting a metric; however, the documents in the
AIRPLANE corpus are mostly short and have little
paragraph structure, so ? is an effective metric.
5.2 Discrimination
Our second task is the discriminative test used by
(Barzilay and Lapata, 2005). In this task we gen-
erate random permutations of a test document, and
measure how often the probability of a permutation
is higher than that of the original document. This
task bears some resemblance to the task of discrim-
inating coherent from incoherent essays in (Milt-
sakaki and Kukich, 2004), and is also equivalent
in the limit to the ranking metric of (Barzilay and
Lee, 2004), which we cannot calculate because our
model does not produce k-best output. As opposed
to the ordering task, which tries to measure how
close the model?s preferred orderings are to the orig-
inal, this measurement assesses how many orderings
the model prefers. We use 20 random permutations
per document, for 2000 total tests.
441
? Discr. (%)
Naive Entity Grid .17 81
Relaxed Entity Grid .02 87
Topic-based (naive) .39 85
Topic-based (relaxed) .54 96
Table 2: Results for 10-fold cross-validation on AIR-
PLANE training data.
6 Results
Since the ordering task requires a model to propose
the complete structure for a set of sentences, it is
very dependent on global features. To perform ad-
equately, a model must be able to locate the begin-
ning and end of the document, and place intermedi-
ate sentences relative to these two points. Without
any way of doing this, our relaxed entity grid model
has ? of approximately 0, meaning its optimal or-
derings are essentially uncorrelated with the correct
orderings7 . The HMM content model of (Barzilay
and Lee, 2004), which does have global structure,
performs much better on ordering, at ? of .44. How-
ever, local features can help substantially for this
task, since models which use them are better at plac-
ing related sentences next to one another. Using both
sets of features, our topic-based model achieves state
of the art performance (? = .5) on the ordering task,
comparable with the mixture model of (Soricut and
Marcu, 2006).
The need for good local coherence features is es-
pecially clear from the results on the discrimination
task (table 1). Permuting a document may leave ob-
vious ?signposts? like the introduction and conclu-
sion in place, but it almost always splits up many
pairs of neighboring sentences, reducing local co-
herence. (Barzilay and Lee, 2004), which lacks lo-
cal features, does quite poorly on this task (74%),
while our model performs extremely well (94%).
It is also clear from the results that our relaxed en-
tity grid model (87%) improves substantially on the
generative naive entity grid (81%). When used on
40 times on test data, 3 times in cross-validation.
5Calculated on our test permutations using the code at
http://people.csail.mit.edu/regina/code.html.
6Soricut and Marcu (2006) do not report results on this task,
except to say that their implementation of the entity grid per-
forms comparably to (Barzilay and Lapata, 2005).
7Barzilay and Lapata (2005) do not report ? scores.
its own, it performs much better on the discrimina-
tion task, which is the one for which it was designed.
(The naive entity grid has a higher ? score, .17, es-
sentially by accident. It slightly prefers to generate
infrequent nouns from the start context rather than
the context - -, which happens to produce the correct
placement for the ?preliminary information? pream-
ble.) When used as the emission model for known
entities in our topic-based system, the relaxed en-
tity grid shows its improved performance even more
strongly (table 2); its results are about 10% higher
than the naive version under both metrics.
Our combined model uses only entity-grid fea-
tures and unigram language models,a strict subset of
the feature set of (Soricut and Marcu, 2006). Their
mixture includes an entity grid model and a version
of the HMM of (Barzilay and Lee, 2004), which
uses n-gram language modeling. It also uses a model
of lexical generation based on the IBM-1 model for
machine translation, which produces all words in the
document conditioned on words from previous sen-
tences. In contrast, we generate only entities con-
ditioned on words from previous sentences; other
words are conditionally independent given the topic
variable. It seems likely therefore that using our
model as a component of a mixture might improve
on the state of the art result.
7 Future Work
Ordering in the AIRPLANE corpus and similar con-
strained sets of short documents is by no means a
solved problem, but the results so far show a good
deal of promise. Unfortunately, in longer and less
formulaic corpora, the models, inference algorithms
and even evaluation metrics used thus far may prove
extremely difficult to scale up. Domains with more
natural writing styles will make lexical prediction a
much more difficult problem. On the other hand,
the wider variety of grammatical constructions used
may motivate more complex syntactic features, for
instance as proposed by (Siddharthan et al, 2004) in
sentence clustering.
Finding optimal orderings is a difficult task even
for short documents, and will become exponen-
tially more challenging in longer ones. For multi-
paragraph documents, it is probably impractical to
use full-scale coherence models to find optimal or-
442
derings directly. A better approach may be a coarse-
to-fine or hierarchical system which cuts up longer
documents into more manageable chunks that can be
ordered as a unit.
Multi-paragraph documents also pose a problem
for the ? metric itself. In documents with clear the-
matic divisions between their different sections, a
good ordering metric should treat transposed para-
graphs differently than transposed sentences.
8 Acknowledgements
We are extremely grateful to Regina Barzilay, for her
code, data and extensive support, Mirella Lapata for
code and advice, and the BLLIP group, especially
Tom Griffiths, Sharon Goldwater and Mark Johnson,
for comments and criticism. We were supported by
DARPA GALE contract HR0011-06-2-0001 and the
Karen T. Romer Foundation. Finally we thank three
anonymous reviewers for their comments.
References
Ernst Althaus, Nikiforos Karamanis, and Alexander
Koller. 2004. Computing locally coherent discourses.
In Proceedings of the 42nd ACL, Barcelona.
Regina Barzilay and Mirella Lapata. 2005. Modeling lo-
cal coherence: an entity-based approach. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05).
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113?120.
Matthew J. Beal, Zoubin Ghahramani, and Carl Ed-
ward Rasmussen. 2001. The infinite Hidden Markov
Model. In NIPS, pages 577?584.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. of the 2005 Meeting of the Assoc. for
Computational Linguistics (ACL), pages 173?180.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
latent semantic analysis. Discourse Processes,
25(2&3):285?307.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects of co-
herence in student essays. In HLT-NAACL, pages 185?
192.
Roger Kibble and Richard Power. 2004. Optimising ref-
erential coherence in text generation. Computational
Linguistics, 30(4):401?416.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In IJCAI, pages 1085?1090.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the annual meeting of ACL, 2003.
Mirella Lapata. 2006. Automatic evaluation of informa-
tion ordering: Kendall?s tau. Computational Linguis-
tics, 32(4):1?14.
E. Miltsakaki and K. Kukich. 2004. Evaluation of text
coherence for electronic essay scoring systems. Nat.
Lang. Eng., 10(1):25?55.
Advaith Siddharthan, Ani Nenkova, and Kathleen McK-
eown. 2004. Syntactic simplification for improving
content selection in multi-document summarization.
In COLING04, pages 896?902.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the Association for Computational Lin-
guistics Conference (ACL-2006).
Y.W. Teh. 2006. A Bayesian interpretation of interpo-
lated Kneser-Ney. Technical Report TRA2/06, Na-
tional University of Singapore.
443
Proceedings of NAACL HLT 2007, Companion Volume, pages 177?180,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Language Modeling for Determiner Selection
Jenine Turner and Eugene Charniak
Department of Computer Science
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{jenine|ec}@cs.brown.edu
Abstract
We present a method for automatic deter-
miner selection, based on an existing lan-
guage model. We train on the Penn Tree-
bank and also use additional data from the
North American News Text Corpus. Our
results are a significant improvement over
previous best.
1 Introduction
Determiner placement (choosing if a noun phrase
needs a determiner, and if so, which one) is a
non-trivial problem in several language processing
tasks. While context beyond that of the current sen-
tence can sometimes be necessary, native speakers
of languages with determiners can select determin-
ers quite well for most NPs. Native speakers of lan-
guages without determiners have a much more diffi-
cult time.
Automating determiner selection is helpful in sev-
eral applications. A determiner selection program
can aid in Machine Translation of determiner-free
languages (by adding determiners after the text has
been translated), correct English text written by non-
native speakers (Lee, 2004), and choose determiners
for text generation programs.
Early work on determiner selection focuses on
rule-based systems (Gawronska, 1990; Murata and
Nagao, 1993; Bond and Ogura, 1994; Heine, 1998).
Knight and Chander (1994) use decision trees to
choose between the and a/an, ignoring NPs with no
determiner, and achieve 78% accuracy on their Wall
Street Journal corpus. (Deciding between a and an
is a trivial postprocessing step.)
Minnen et al (2000) use a memory-based learner
(Daelemans et al, 2000) to choose determiners of
base noun phrases. They choose between no deter-
miner (hencefore null), the, and a/an. They use syn-
tactic features (head of the NP, part-of-speech tag of
the head of the NP, functional tag of the head of the
NP, category of the constituent embedding the NP,
and functional tag of the constituent embedding the
NP), whether the head is a mass or count noun and
semantic classes of the head of the NP (Ikehara et
al., 1991). They report 83.58% accuracy.
In this paper, we use the Charniak language model
(Charniak, 2001) for determiner selection. Our ap-
proach significantly improves upon the work of Min-
nen et al (2000). We also use additional automat-
ically parsed data from the North American News
Text Corpus (Graff, 1995), further improving our re-
sults.
2 The Immediate-Head Parsing Model
The language model we use is described in (Char-
niak, 2001). It is based upon a parser that, for a
sentence s, tries to find the parse pi defined as:
argmaxpip(pi | s) = argmaxpip(pi, s) (1)
The parser can be turned into a language model p(s)
describing the probability distribution over all pos-
sible strings s in the language, by considering all
parses pi of s:
p(s) =
?
pi
p(pi, s) (2)
177
Here p(pi, s) is zero if the yield of pi 6= s.
The parsing model assigns a probability to a parse
pi by a top-down process. For each constituent c in
pi it first guesses the pre-terminal of c, t(c) (t for
?tag?), then the lexical head of c, h(c), and then the
expansion of c into further constituents e(c). Thus
the probability of a parse is given by the equation
p(pi) =
?
c?pi
p(t(c) | l(c),H(c))
? p(h(c) | t(c), l(c),H(c))
? p(e(c) | l(c), t(c), h(c),H(c))
where l(c) is the label of c (e.g., whether it is a noun
phrase NP, verb phrase, etc.) and H(c) is the rel-
evant history of c ? information outside c deemed
important in determining the probability in question.
H(c) approximately consists of the label, head, and
head-part-of-speech for the parent of c: m(c), i(c),
and u(c) respectively and also a secondary head
(e.g., in ?Monday Night Football? Monday would
be conditioned on both the head of the noun-phrase
?Football? and the secondary head ?Night?).
It is usually clear to which constituent we are re-
ferring and we omit the (c) in, e.g., h(c). In this no-
tation the above equation takes the following form:
p(pi) =
?
c?pi
p(t | l,m, u, i) ? p(h | t, l,m, u, i)
? p(e | l, t, h,m, u). (3)
Next we describe how we assign a probability to
the expansion e of a constituent. We break up a tra-
ditional probabilistic context-free grammar (PCFG)
rule into a left-hand side with a label l(c) drawn
from the non-terminal symbols of our grammar, and
a right-hand side that is a sequence of one or more
such symbols. For each expansion we distinguish
one of the right-hand side labels as the ?middle? or
?head? symbol M(c). M(c) is the constituent from
which the head lexical item h is obtained according
to deterministic rules that pick the head of a con-
stituent from among the heads of its children. To the
left of M is a sequence of one or more left labels
Li(c) including the special termination symbol ?,
which indicates that there are no more symbols to
the left. We do the same for the labels to the right,
Ri(c). Thus, an expansion e(c) looks like:
l ? ?Lm...L1MR1...Rn?. (4)
The expansion is generated first by guessing M ,
then in order L1 through Lm+1 (= ?), and then, R1
through Rn+1.
Let us turn to how this works in the case of de-
terminer recovery. Consider a noun-phrase, which,
missing a possible determiner, is simply ?FBI.? The
language model is interested in the probability of the
strings ?the FBI,? ?a/an FBI? and ?FBI.? The ver-
sion with the highest probability will dictate the de-
terminer, or lack thereof. So, consider (most of) the
probability calculation for the answer ?the FBI:?
p(NNP | H) ? p(FBI | NNP,H)
? p(det | FBI,NNP,H)
? p(? | det,FBI,NNP,H)
? p(the | det,FBI,NNP,H) (5)
Of these, the first two terms, the probability that
the head will be an NNP (a singular proper noun)
and the probability that it will be ?FBI?, are shared
by all three competitors, null, the, and a/an. These
terms can therefore be ignored when we only wish to
identify the competitor with the highest probability.
The next two probabilities state that the noun-phrase
contains a determiner to the left of ?FBI? and that
the determiner is the last constituent of the left-hand
side. The last of the probabilities states that the de-
terminer in question is the. Ignoring the first two
probabilities, the critical probabilities for ?the FBI?
are:
p(det | FBI,NNP,H)
? p(? | det,FBI,NNP,H)
? p(the | det,FBI,NNP,H) (6)
Conversely, to evaluate the probability of the noun-
phrase ?FBI? ? i.e., no determiner, we evaluate:
p(? | FBI,NNP,H) (7)
We ask the probability of the NP stopping immedi-
ately to the left of ?FBI.? For ?a/an FBI? we evalu-
ate:
p(det | FBI,NNP,H)
? p(? | det,FBI,NNP,H) (8)
? (p(a | det,FBI,NNP,H) +
p(an | det,FBI,NNP,H))
178
Test Data Method Accuracy
leave-one-out Minnen et al 83.58%
Language Model (LM) 86.74%
tenfold on development LM 84.72%
LM trained on WSJ + 3 million words of NANC 85.83%
LM trained on WSJ + 10 million words of NANC 86.36%
LM trained on WSJ + 20 million words of NANC 86.64%
tenfold on test LM trained on WSJ + 20 million words of NANC 86.63%
Table 1: Results of classification
This equation is very similar to Equation 6 (the
equation for ?the FBI?, except the term for the prob-
ability of the is replaced by the sum of the probabil-
ities for a and an.
To choose between null, the, or a/an, the language
model in effect constructs Equations 6, 7 and 8 and
we pick the one that has the highest probability.
2.1 Training the model
As with (Minnen et al, 2000), we train the lan-
guage model on the Penn Treebank (Marcus et al,
1993). As far as we know, language modeling
always improves with additional training data, so
we add data from the North American News Text
Corpus (NANC) (Graff, 1995) automatically parsed
with the Charniak parser (McClosky et al, 2006) to
train our language model on up to 20 million addi-
tional words.
3 Results and Discussion
The best results of Minnen et al (2000) are using
leave-one-out cross-validation. We also test our lan-
guage model using leave-one-out cross-validation
on the Penn Treebank (Marcus et al, 1993) (WSJ),
giving us 86.74% accuracy (see Table 1).
Leave-one-out cross-validation does not make
sense in this case. When choosing determiners, we
can train a language model on similar data, but not
on other NPs in the article. Therefore, for the rest
of our tests, we use tenfold cross-validation. The
difference between leave-one-out and tenfold cross-
validation is due to the co-occurrence of NPs within
an article. Church (2000) shows that a word appears
with much higher probability when seen elsewhere
in an article. Thus, a rare NP might be unseen in
tenfold cross-validation, but seen in leave-one-out.
For each of our sets in tenfold cross validation,
we use 80% of the Penn Treebank for training, 10%
for development, and 10% for testing. The divisions
occur at article boundaries. On our development set
with tenfold cross-validation, we get 84.72% accu-
racy using the language model (Table 1).
As expected, we achieve significant improvement
when adding NANC data over training on data from
the Penn Treebank alone (Table 1). With 20 mil-
lion additional words, we seem to be approaching
an upper bound on the language model features. We
obtain improvement despite the fact that the parses
were automatic, but there may have been errors in
determiner selection due to parsing error.
Table 2 gives ?error? examples. Some errors are
wrong (either grammatically or yielding a signifi-
cantly different interpretation), but some ?incorrect?
answers are reasonable possibilities. Furthermore,
even all the text of the article is not enough for clas-
sification at times. In particular note Example 5,
where unless you know whether IBM was the world
leader or simply one of the world leaders at the time
of the article, no additional context would help.
4 Conclusions and Future Work
With the Charniak (Charniak, 2001) language
model, our results exceed those of the previous best
(Minnen et al, 2000) on the determiner selection
task. This shows the benefits of the language model
features in determining the most grammatical deter-
miner to use in a noun phrase. Such a language
model looks at much of the structure in individual
sentences, but there may be additional features that
could improve performance. There is a high rate of
ambiguity for many of the misclassified sentences.
The success of using a state-of-the-art language
179
Guess Correct Sentence
the null (1) The computers were crude by today?s standards.
null the (2) In addition, the Apple II was an affordable $1,298.
(3) Highway officials insist the ornamental railings on older bridges aren?t strong enough
to prevent vehicles from crashing through.
a/an the (4) The new carrier can tote as many as four cups at once.
(5) IBM, the world leader in computers, didn?t offer its first PC
until August 1981 as many other companies entered the market.
the a/an (6) In addition, the Apple II was an affordable $1,298.
(7) ?The primary purpose of a railing is to contain a vehicle and not to provide
a scenic view,? says Jack White, a planner with the Indiana Highway Department.
a/an null (8) Crude as they were, these early PCs triggered explosive product development in
desktop models for the home and office.
Table 2: Examples of ?errors?
model in determiner selection also suggests that one
would be helpful in making other decisions in the
surface realization stage of text generation. This is
an avenue worth exploring.
Acknowledgements
This work was supported by NSF PIRE grant OISE-0530118.
We would also like to thank the BLLIP team for their comments.
References
Francis Bond and Kentaro Ogura. 1994. Countability
and number in Japanese-to-English machine transla-
tion. In 15th International Conference on Computa-
tional Linguistics, pages 32?38.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguis-
tics. The Association for Computational Linguistics.
Kenneth Church. 2000. Empirical estimates of adap-
tation: The chance of Two Noriegas is closer to p/2
than p2. In Proceedings of COLING-2000.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2000. TiMBL: Tilburg memory
based learner, version 3.0, reference guide. ILK Tech-
nical Report ILK-0001, ILK, Tilburg University, The
Netherlands.
Barbara Gawronska. 1990. Translation great problem.
In Proceedings of the 13th International Conference
on Computational Linguistics.
David Graff. 1995. North American News Text Corpus.
Linguistic Data Consortium. LDC95T21.
Julia E. Heine. 1998. Definiteness predictions for
Japanese noun phrases. In Proceedings of the 36th
Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference
on Computational Linguistics, pages 519?525.
Satoru Ikehara, Satoship Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-
editing - effects of new methods in ALT-J/E. In Third
Machine Translation Summit, pages 101?106.
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In Proceedings of the
Twelfth National Conference on Artificial Intelligence,
pages 779?784.
John Lee. 2004. Automatic article restoration. In Pro-
ceedings of the 2004 NAACL Conference Student Re-
search Workshop, pages 195?200.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT-NAACL 2006.
Guido Minnen, Francis Bond, and Ann Copestake. 2000.
Memory-based learning for article generation. In Pro-
ceedings of the Fourth Conference on Computational
Natural Language Learning and of the Second Learn-
ing Language in Logic Workshop, pages 43?48.
Masaki Murata and Makoto Nagao. 1993. Determina-
tion of referential property and number of nouns in
Japanese sentences for machine translation into En-
glish. In Fifth International Conference on Theoret-
ical and Methodological Issues in Machine Transla-
tion, pages 218?225.
180
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 164?172,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Structured Generative Models for Unsupervised Named-Entity Clustering
Micha Elsner, Eugene Charniak and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{melsner,ec,mj}@cs.brown.edu
Abstract
We describe a generative model for clustering
named entities which also models named en-
tity internal structure, clustering related words
by role. The model is entirely unsupervised;
it uses features from the named entity itself
and its syntactic context, and coreference in-
formation from an unsupervised pronoun re-
solver. The model scores 86% on the MUC-7
named-entity dataset. To our knowledge, this
is the best reported score for a fully unsuper-
vised model, and the best score for a genera-
tive model.
1 Introduction
Named entity clustering is a classic task in NLP, and
one for which both supervised and semi-supervised
systems have excellent performance (Mikheev et al,
1998; Chinchor, 1998). In this paper, we describe a
fully unsupervised system (using no ?seed rules? or
initial heuristics); to our knowledge this is the best
such system reported on the MUC-7 dataset. In ad-
dition, the model clusters the words which appear
in named entities, discovering groups of words with
similar roles such as first names and types of orga-
nization. Finally, the model defines a notion of con-
sistency between different references to the same en-
tity; this component of the model yields a significant
increase in performance.
The main motivation for our system is the re-
cent success of unsupervised generative models for
coreference resolution. The model of Haghighi
and Klein (2007) incorporated a latent variable for
named entity class. They report a named entity score
of 61.2 percent, well above the baseline of 46.4, but
still far behind existing named-entity systems.
We suspect that better models for named entities
could aid in the coreference task. The easiest way to
incorporate a better model is simply to run a super-
vised or semi-supervised system as a preprocess. To
perform joint inference, however, requires an unsu-
pervised generative model for named entities. As far
as we know, this work is the best such model.
Named entities also pose another problem with
the Haghighi and Klein (2007) coreference model;
since it models only the heads of NPs, it will fail to
resolve some references to named entities: (?Ford
Motor Co.?, ?Ford?), while erroneously merging
others: (?Ford Motor Co.?, ?Lockheed Martin Co.?).
Ng (2008) showed that better features for match-
ing named entities? exact string match and an ?alias
detector? looking for acronyms, abbreviations and
name variants? improve the model?s performance
substantially. Yet building an alias detector is non-
trivial (Uryupina, 2004). English speakers know that
?President Clinton? is the same person as ?Bill Clin-
ton? , not ?President Bush?. But this cannot be im-
plemented by simple substring matching. It requires
some concept of the role of each word in the string.
Our model attempts to learn this role information by
clustering the words within named entities.
2 Related Work
Supervised named entity recognition now performs
almost as well as human annotation in English
(Chinchor, 1998) and has excellent performance on
other languages (Tjong Kim Sang and De Meul-
der, 2003). For a survey of the state of the art,
164
see Nadeau and Sekine (2007). Of the features
we explore here, all but the pronoun information
were introduced in supervised work. Supervised ap-
proaches such as Black et al (1998) have used clus-
tering to group together different nominals referring
to the same entity in ways similar to the ?consis-
tency? approach outlined below in section 3.2.
Semi-supervised approaches have also achieved
notable success on the task. Co-training (Riloff and
Jones, 1999; Collins and Singer, 1999) begins with
a small set of labeling heuristics and gradually adds
examples to the training data. Various co-training
approaches presented in Collins and Singer (1999)
all score about 91% on a dataset of named entities;
the inital labels were assigned using 7 hand-written
seed rules. However, Collins and Singer (1999)
show that a mixture-of-naive-Bayes generative clus-
tering model (which they call an EM model), initial-
ized with the same seed rules, performs much more
poorly at 83%.
Much later work (Evans, 2003; Etzioni et al,
2005; Cucerzan, 2007; Pasca, 2004) relies on the
use of extremely large corpora which allow very
precise, but sparse features. For instance Etzioni
et al (2005) and Pasca (2004) use web queries to
count occurrences of ?cities such as X? and simi-
lar phrases. Although our research makes use of a
fairly large amount of data, our method is designed
to make better use of relatively common contextual
features, rather than searching for high-quality se-
mantic features elsewhere.
Models of the internal structure of names have
been used for cross-document coreference (Li et al,
2004; Bhattacharya and Getoor, 2006) and a goal in
their own right (Charniak, 2001). Li et al (2004)
take named entity classes as a given, and develops
both generative and discriminative models to detect
coreference between members of each class. Their
generative model designates a particular mention of
a name as a ?representative? and generates all other
mentions from it according to an editing process.
Bhattacharya and Getoor (2006) operates only on
authors of scientific papers. Their model accounts
for a wider variety of name variants than ours, in-
cluding misspellings and initials. In addition, they
confirm our intuition that Gibbs sampling for infer-
ence has insufficient mobility; rather than using a
heuristic algorithm as we do (see section 3.5), they
use a data-driven block sampler. Charniak (2001)
uses a Markov chain to generate 6 different com-
ponents of people?s names, again assuming that the
class of personal names can be pre-distinguished us-
ing a name list. He infers coreference relationships
between similar names appearing in the same docu-
ment, using the same notion of consistency between
names as our model. As with our model, the clusters
found are relatively good, although with some mis-
takes even on frequent items (for example, ?John? is
sometimes treated as a descriptor like ?Secretary?).
3 System Description
Like Collins and Singer (1999), we assume that the
named entities have already been correctly extracted
from the text, and our task is merely to label them.
We assume that all entities fit into one of the three
MUC-7 categories, LOC (locations), ORG (organi-
zations), and PER (people). This is an oversimplifi-
cation; Collins and Singer (1999) show that about
12% of examples do not fit into these categories.
However, while using the MUC-7 data, we have no
way to evaluate on such examples.
As a framework for our models, we adopt adap-
tor grammars (Johnson et al, 2007), a framework
for non-parametric Bayesian inference over context-
free grammars. Although our system does not re-
quire the full expressive power of PCFGs, the adap-
tor grammar framework allows for easy develop-
ment of structured priors, and supplies a flexible
generic inference algorithm. An adaptor grammar is
a hierarchical Pitman-Yor process (Pitman and Yor,
1997). The grammar has two parts: a base PCFG
and a set of adapted nonterminals. Each adapted
nonterminal is a Pitman-Yor process which expands
either to a previously used subtree or to a sample
from the base PCFG. The end result is a posterior
distribution over PCFGs and over parse trees for
each example in our dataset.
Each of our models is an adaptor grammar based
on a particular base PCFG where the top nonter-
minal of each parse tree represents a named entity
class.
3.1 Core NP Model
We begin our analysis by reducing each named-
entity reference to the contiguous substring of
165
ROOT ?NE 0|NE 1|NE 2
NE 0 ?(NE 00)(NE 10)(NE 20)(NE 30)(NE 40)
?NE 00 ?Words
?Words ?Word (Words)
Word ?Bill . . .
Figure 1: Part of the grammar for core phrases. (Paren-
theses) mark optional nonterminals. *Starred nontermi-
nals are adapted.
proper nouns which surrounds its head, which we
call the core (Figure 1). To analyze the core, we use
a grammar with three main symbols (NEx), one for
each named entity class x. Each class has an asso-
ciated set of lexical symbols, which occur in a strict
order (NE ix is the ith symbol for class x). We can
think of the NE i as the semantic parts of a proper
name; for people, NE 0PER might generate titles and
NE 1PER first names. Each NE i is adapted, and can
expand to any string of words; the ability to gen-
erate multiple words from a single symbol is use-
ful both because it can learn to group collocations
like ?New York? and because it allows the system to
handle entities longer than four words. However, we
set the prior on multi-word expansions very low, to
avoid degenerate solutions where most phrases are
analyzed with a single symbol. The system learns
a separate probability for each ordered subset of the
NE i (for instance the rule NE 0 ? NE 00 NE 20 NE 40),
so that it can represent constraints on possible refer-
ences; for instance, a last name can occur on its own,
but not a title.
3.2 Consistency Model
This system captures some of our intuitions about
core phrases, but not all: our representation for ?Bill
Clinton? does not share any information with ?Presi-
dent Bill Clinton? except the named-entity class. To
remedy this, we introduce a set of ?entity? nonter-
minals Ek, which enforce a weak notion of consis-
tency. We follow Charniak (2001) in assuming that
two names are consistent (can be references to the
same entity) if they do not have different expansions
for any lexical symbol. In other words, a particu-
lar entity EPER,Clinton has a title E0PER,Clinton =
ROOT ?NE 0|NE 1|NE 2
NE 0 ?E00|E01 . . . E0k
E00 ?(E000)(E100)(E200)(E300)(E400)
? ? E000 ?NE 00
?NE 00 ?Words . . .
Figure 2: Part of the consistency-enforcing grammar for
core phrases. There are an infinite number of entities
Exk, all with their own lexical symbols. Each lexical
symbol Eixk expands to a single NE ix.
?President?, a first name E1PER,Clinton = ?Bill? etc.
These are generated from the class-specific distribu-
tions, for instance E0PER,Clinton ? E0PER, which
we intend to be a distribution over titles in general.
The resulting grammar is shown in Figure 2; the
prior parameters for the entity-specific symbols Eixk
are fixed so that, with overwhelming probability,
only one expansion occurs. We can represent any
fixed number of entities Ek with a standard adap-
tor grammar, but since we do not know the correct
number, we must extend the adaptor model slightly
to allow for an unbounded number. We generate the
Ek from a Chinese Restaurant process prior. (Gen-
eral grammars with infinite numbers of nonterminals
were studied by (Liang et al, 2007b)).
3.3 Modifiers, Prepositions and Pronouns
Next, we introduce two types of context information
derived from Collins and Singer (1999): nominal
modifiers and prepositional information. A nominal
modifier is either the head of an appositive phrase
(?Maury Cooper, a vice president?) or a non-proper
prenominal (?spokesman John Smith?)1. If the en-
tity is the complement of a preposition, we extract
the preposition and the head of the governing NP (?a
federally funded sewage plant in Georgia?). These
are added to the grammar at the named-entity class
level (separated from the core by a special punctua-
tion symbol).
Finally, we add information about pronouns and
wh-complementizers (Figure 3). Our pronoun infor-
mation is derived from an unsupervised coreference
algorithm which does not use named entity informa-
1We stem modifiers with the Porter stemmer.
166
ROOT ?Modifiers0 # NE 0 #
Prepositions0 # Pronouns0 #
. . .
Pronouns0 ?Pronoun0 Pronouns0
Pronouns0 ?
Pronoun0 ?pers|loc|org |any
pers ?i |he|she|who|me . . .
loc ?where|which|it |its
org ?which|it |they |we . . .
Figure 3: A fragment of the full grammar. The symbol
# represents punctuation between different feature types.
The prior for class 0 is concentrated around personal pro-
nouns, although other types are possible.
tion (Charniak and Elsner, 2009). This algorithm
uses EM to learn a generative model with syntactic,
number and gender parameters. Like Haghighi and
Klein (2007), we give our model information about
the basic types of pronouns in English. By setting
up the base grammar so that each named-entity class
prefers to associate to a single type of pronoun, we
can also determine the correspondence between our
named-entity symbols and the actual named-entity
labels? for the models without pronoun information,
this matching is arbitrary and must be inferred dur-
ing the evaluation process.
3.4 Data Preparation
To prepare data for clustering with our system, we
first parse it with the parser of Charniak and Johnson
(2005). We then annotate pronouns with Charniak
and Elsner (2009). For the evaluation set, we use the
named entity data from MUC-7. Here, we extract
all strings in <ne> tags and determine their cores,
plus any relevant modifiers, governing prepositions
and pronouns, by examining the parse trees. In addi-
tion, we supply the system with additional data from
the North American News Corpus (NANC). Here
we extract all NPs headed by proper nouns.
We then process our data by merging all exam-
ples with the same core; some merged examples
from our dataset are shown in Figure 4. When two
examples are merged, we concatenate their lists of
attack airlift airlift rescu # wing # of-commander
of-command with-run # #
# air-india # # #
# abels # # it #
# gaudreau # # they he #
# priddy # # he #
spokesman bird bird bird director bird ford clin-
ton director bird # johnson # before-hearing
to-happened of-cartoon on-pressure under-medicare
to-according to-allied with-stuck of-government of-
photographs of-daughter of-photo for-embarrassing
under-instituted about-allegations for-worked
before-hearing to-secretary than-proposition of-
typical # he he his he my himself his he he he he i
he his his i i i he his #
Figure 4: Some merged examples from an input file. (#
separates different feature types.)
modifiers, prepositions and pronouns (capping the
length of each list at 20 to keep inference tractable).
For instance, ?air-india? has no features outside the
core, while ?wing? has some nominals (?attack?
&c.) and some prepositions (?commander-of? &c.).
This merging is useful because it allows us to do in-
ference based on types rather than tokens (Goldwa-
ter et al, 2006). It is well known that, to interpo-
late between types and tokens, Hierarchical Dirich-
let Processes (including adaptor grammars) require
a deeper hierarchy, which slows down inference and
reduces the mobility of sampling schemes. By merg-
ing examples, we avoid using this more complicated
model. Each merged example also represents many
examples from the training data, so we can summa-
rize features (such as modifiers) observed through-
out a large input corpus while keeping the size of
our input file small.
To create an input file, we first add all the MUC-
7 examples. We then draw additional examples
from NANC, ranking them by how many features
they have, until we reach a specified number (larger
datasets take longer, but without enough data, results
tend to be poor).
3.5 Inference
Our implementation of adaptor grammars is a mod-
ified version of the Pitman-Yor adaptor grammar
167
sampler2, altered to deal with the infinite number of
entities. It carries out inference using a Metropolis-
within-Gibbs algorithm (Johnson et al, 2007), in
which it repeatedly parses each input line using the
CYK algorithm, samples a parse, and proposes this
as the new tree.
To do Gibbs sampling for our consistency-
enforcing model, we would need to sample a parse
for an example from the posterior over every pos-
sible entity. However, since there are thousands of
entities (the number grows roughly linearly with the
number of merged examples in the data file), this is
not tractable. Instead, we perform a restricted Gibbs
sampling search, where we enumerate the posterior
only for entities which share a word in their core
with the example in question. In fact, if the shared
word is very common (occuring in more than .001 of
examples), we compute the posterior for that entity
only .05 of the time3. These restrictions mean that
we do not compute the exact posterior. In particular,
the actual model allows entities to contain examples
with no words in common, but our search procedure
does not explore these solutions.
For our model, inference with the Gibbs algo-
rithm seems to lack mobility, sometimes falling into
very poor local minima from which it does not seem
to escape. This is because, if there are several ref-
erences to the same named entity with slightly dif-
ferent core phrases, once they are all assigned to
the wrong class, it requires a low-probability se-
ries of individual Gibbs moves to pull them out.
Similarly, the consistency-enforcing model gener-
ally does not fully cluster references to common en-
tities; there are usually several ?Bill Clinton? clus-
ters which it would be best to combine, but the se-
quence of moves that does so is too improbable. The
data-merging process described above is one attempt
to improve mobility by reducing the number of du-
plicate examples. In addition, we found that it was a
better use of CPU time to run multiple samplers with
different initialization than to perform many itera-
tions. In the experiments below, we use 20 chains,
initializing with 50 iterations without using consis-
tency, then 50 more using the consistency model,
and evaluate the last sample from each. We discard
2Available at http://www.cog.brown.edu/ mj/Software.htm
3We ignore the corresponding Hastings correction, as in
practice it leads to too many rejections.
the 10 samples with worst log-likelihood and report
the average score for the other 10.
3.6 Parameters
In addition to the base PCFG itself, the system re-
quires a few hyperparameter settings: Dirichlet pri-
ors for the rule weights of rules in the base PCFG.
Pitman-Yor parameters for the adapted nonterminals
are sampled from vague priors using a slice sam-
pler (Neal, 2003). The prior over core words was
set to the uniform distribution (Dirichlet 1.0) and the
prior for all modifiers, prepositions and pronouns to
a sparse value of .01. Beyond setting these param-
eters to a priori reasonable values, we did not opti-
mize them. To encourage the system to learn that
some lexical symbols were more common than oth-
ers, we set a sparse prior over expansions to sym-
bols4. There are two really important hyperparame-
ters: an extremely biased prior on class-to-pronoun-
type probabilities (1000 for the desired class, .0001
for everything else), and a prior of .0001 for the
Word ?Word Words rule to discourage symbols
expanding to multiword strings.
4 Experiments
We performed experiments on the named entity
dataset from MUC-7 (Chinchor, 1998), using the
training set as development data and the formal test
set as test data. The development set has 4936
named entities, of which 1575 (31.9%) are locations,
2096 (42.5%) are organizations and 1265 (25.6%)
people. The test set has 4069 named entities, 1321
(32.5%) locations, 1862 (45.8%) organizations and
876 (21.5%) people5. We use a baseline which
gives all named entities the same label; this label is
mapped to ?organization?.
In most of our experiments, we use an input file of
40000 lines. For dev experiments, the labeled data
contributes 1585 merged examples; for test experi-
ments, only 1320. The remaining lines are derived
4Expansions that used only the middle three symbols
NE1,2,3x got a prior of .005, expansions whose outermost sym-
bol was NE0,4x got .0025, and so forth. This is not so impor-
tant for our final system, which has only 5 symbols, but was
designed during development to handle systems with up to 10
symbols.
510 entities are labeled location|organization; since this
fraction of the dataset is insignificant we score them as wrong.
168
Model Accuracy
Baseline (All Org) 42.5
Core NPs (no consistency) 45.5
Core NPs (consistency) 48.5
Context Features 83.3
Pronouns 87.1
Table 1: Accuracy of various models on development
data.
Model Accuracy
Baseline (All Org) 45.8
Pronouns 86.0
Table 2: Accuracy of the final model on test data.
using the process described in section 3.4 from 5
million words of NANC.
To evaluate our results, we map our three induced
labels to their corresponding gold label, then count
the overlap; as stated, this mapping is predictably
encoded in the prior when we use the pronoun fea-
tures. Our experimental results are shown in Table
1. All models perform above baseline, and all fea-
tures contribute significantly to the final result. Test
results for our final model are shown in Table 2.
A confusion matrix for our highest-likelihood test
solution is shown as Figure 5. The highest confusion
class is ?organization?, which is confused most often
with ?location? but also with ?person?. ?location? is
likewise confused with ?organization?. ?person? is
the easiest class to identify? we believe this explains
the slight decline in performance from dev to test,
since dev has proportionally more people.
Our mapping from grammar symbols to words ap-
pears in Table 3; the learned prepositional and mod-
ifier information is in Table 4. Overall the results
are good, but not perfect; for instance, the Pers
states are mostly interpretable as a sequence of ti-
tle - first name - middle name or initial - last name -
loc org per
LOC 1187 97 37
ORG 223 1517 122
PER 36 20 820
Figure 5: Confusion matrix for highest-likelihood test
run. Gold labels in CAPS, induced labels italicized. Or-
ganizations are most frequently confused.
last name or post-title (similar to (Charniak, 2001)).
The organization symbols tend to put nationalities
and other modifiers first, and end with institutional
types like ?inc.? or ?center?, although there is a sim-
ilar (but smaller) cluster of types at Org2, suggest-
ing the system has incorrectly found two analyses
for these names. Location symbols seem to put en-
tities with a single, non-analyzable name into Loc2,
and use symbols 0, 1 and 3 for compound names.
Loc4 has been recruited for time expressions, since
our NANC dataset includes many of these, but we
failed to account for them in the model. Since
they appear in a single class here, we are optimistic
that they could be clustered separately if another
class and some appropriate features were added to
the prior. Some errors do appear (?supreme court?
and ?house? as locations, ?minister? and ?chairman?
as middle names, ?newt gingrich? as a multiword
phrase). The table also reveals an unforeseen issue
with the parser: it tends to analyze the dateline be-
ginning a news story along with the following NP
(?WASHINGTON Bill Clinton said...?). Thus com-
mon datelines (?washington?, ?new york? and ?los
angeles?) appear in state 0 for each class.
5 Discussion
As stated above, we aim to build an unsupervised
generative model for named entity clustering, since
such a model could be integrated with unsupervised
coreference models like Haghighi and Klein (2007)
for joint inference. To our knowledge, the closest
existing system to such a model is the EM mix-
ture model used as a baseline in Collins and Singer
(1999). Our system improves on this EM system
in several ways. While they initialize with minimal
supervision in the form of 7 seed heuristics, ours is
fully unsupervised. Their results cover only exam-
ples which have a prepositional or modifier feature;
we adopt these features from their work, but label
all entities in the predefined test set, including those
that appear without these features. Finally, as dis-
cussed, we find the ?person? category to be the eas-
iest to label. 33% of the test items in Collins and
Singer (1999) were people, as opposed to 21% of
ours. However, even without the pronoun features,
that is, using the same feature set, our system scores
equivalently to the EM model, at 83% (this score is
169
Pers0 Pers1 Pers2 Pers3 Pers4
rep. john (767) minister brown jr.
sen. (256) robert (495) j. smith (97) a
washington david john (242) b smith (111)
dr. michael l. johnson iii
los angeles james chairman newt gingrich williams
senate president e. king wilson
house richard m. miller brown
new york william (317) william (173) kennedy clinton
president sen. (236) robert (155) martin simpson
republican george r. davis b
Org0 Org1 Org2 Org3 Org4
american (137) national university research association
washington american (182) inc. (166) medical center
washington the new york corp. (156) news inc. (257)
national international (136) college health corp. (252)
first public institute (87) services co.
los angeles united group communications committee
new house hospital development institute
royal federal museum policy council
british home press affairs fund
california world international (61) defense act
Loc0 Loc1 Loc2 Loc3 Loc4
washington (92) the texas county monday
los angeles st. new york city thursday
south new washington (22) beach river (57)
north national (69) united states valley tuesday
old east (65) baltimore island wednesday
grand mount california river (71) hotel
black fort capitol park friday
west (22) west (56) christmas bay hall
east (21) lake bosnia house center
haiti great san juan supreme court building
Table 3: 10 most common words for each grammar symbol. Words which appear in multiple places have observed
counts indicated in parentheses.
170
Pers-gov Pers-mod Org-gov Org-mod Loc-gov Loc-mod
according-to (1044) director president-of $ university-of calif.
played-by spokesman chairman-of giant city-of newspap[er]
directed-by leader director-of opposit[e] from-to state
led-by presid[ent] according-to (786) group town-of downtown
meeting-with attorney professor-at pp state-of n.y.
from-to candid[ate] head-of compan[y] center-in warrant
met-with lawyer department-of journal out-of va.
letter-to chairman member-of firm is-in fla.
secretary-of counsel members-of state house-of p.m.
known-as actor spokesman-for agenc[y] known-as itself
Table 4: 10 most common prepositional and modifier features for each named entity class. Modifiers were Porter-
stemmed; for clarity a reconstructed stem is shown in brackets.
on dev, 25% people). When the pronoun features are
added, our system?s performance increases to 86%,
significantly better than the EM system.
One motivation for our use of a structured model
which defined a notion of consistency between en-
tities was that it might allow the construction of
an unsupervised alias detector. According to the
model, two entities are consistent if they are in the
same class, and do not have conflicting assignments
of words to lexical symbols. Results here are at
best equivocal. The model is reasonable at pass-
ing basic tests? ?Dr. Seuss? is not consistent with
?Dr. Strangelove?, ?Dr. Quinn? etc, despite their
shared title, because the model identifies the sec-
ond element of each as a last name. Also correctly,
?Dr. William F. Gibson? is judged consistent with
?Dr. Gibson? and ?Gibson? despite the missing el-
ements. But mistakes are commonplace. In the
?Gibson? case, the string ?William F.? is misana-
lyzed as a multiword string, making the name in-
consistent with ?William Gibson?; this is probably
the result of a search error, which, as we explained,
Gibbs sampling is unlikely to correct. In other cases,
the system clusters a family group together under
a single ?entity? nonterminal by forcing their first
names into inappropriate states, for instance assign-
ing Pers1 Bruce, Pers2 Ellen, Pers3 Jarvis, where
Pers2 (usually a middle name) actually contains the
first name of a different individual. To improve this
aspect of our system, we might incorporate name-
specific features into the prior, such as abbreviations
and the concept of a family name. The most critical
improvement, however, would be integration with a
generative coreference system, since the document
context probably provides hints about which entities
are and are not coreferent.
The other key issue with our system is inference.
Currently we are extremely vulnerable to falling into
local minima, since the complex structure of the
model can easily lock a small group of examples
into a poor configuration. (The ?William F. Gibson?
case above seems to be one of these.) In addition to
the block sampler used by Bhattacharya and Getoor
(2006), we are investigating general-purpose split-
merge samplers (Jain and Neal, 2000) and the per-
mutation sampler (Liang et al, 2007a). One inter-
esting question is how well these samplers perform
when faced with thousands of clusters (entities).
Despite these issues, we clearly show that it is
possible to build a good model of named entity class
while retaining compatibility with generative sys-
tems and without supervision. In addition, we do a
reasonable job learning the latent structure of names
in each named entity class. Our system improves
over the latent named-entity tagging in Haghighi
and Klein (2007), from 61% to 87%. This sug-
gests that it should indeed be possible to improve
on their coreference results without using a super-
vised named-entity model. How much improvement
is possible in practice, and whether joint inference
can also improve named-entity performance, remain
interesting questions for future work.
Acknowledgements
We thank three reviewers for their comments, and
NSF for support via grants 0544127 and 0631667.
171
References
Indrajit Bhattacharya and Lise Getoor. 2006. A latent
dirichlet model for unsupervised entity resolution. In
The SIAM International Conference on Data Mining
(SIAM-SDM), Bethesda, MD, USA.
William J. Black, Fabio Rinaldi, and David Mowatt.
1998. Facile: Description of the ne system used for
muc-7. In In Proceedings of the 7th Message Under-
standing Conference.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
the Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-09),
Athens, Greece.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. of the 2005 Meeting of the Assoc. for
Computational Linguistics (ACL), pages 173?180.
Eugene Charniak. 2001. Unsupervised learning of name
structure from coreference data. In NAACL-01.
Nancy A. Chinchor. 1998. Proceedings of the Sev-
enth Message Understanding Conference (MUC-7)
named entity task definition. In Proceedings of the
Seventh Message Understanding Conference (MUC-
7), page 21 pages, Fairfax, VA, April. version 3.5,
http://www.itl.nist.gov/iaui/894.02/related projects/muc/.
Michael Collins and Yorav Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of EMNLP 99.
Silviu Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In Proceedings of
EMNLP-CoNLL, pages 708?716, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana
maria Popescu, Tal Shaked, Stephen Soderl, Daniel S.
Weld, and Er Yates. 2005. Unsupervised named-
entity extraction from the web: An experimental study.
Artificial Intelligence, 165:91?134.
Richard Evans. 2003. A framework for named en-
tity recognition in the open domain. In Proceedings
of Recent Advances in Natural Language Processing
(RANLP-2003), pages 137 ? 144, Borovetz, Bulgaria,
September.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006. Interpolating between types and tokens by es-
timating power-law generators. In Advances in Neural
Information Processing Systems (NIPS) 18.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
848?855. Association for Computational Linguistics.
Sonia Jain and Radford M. Neal. 2000. A split-merge
markov chain monte carlo procedure for the dirichlet
process mixture model. Journal of Computational and
Graphical Statistics, 13:158?182.
Mark Johnson, Tom L. Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Proceedings of NAACL 2007.
Xin Li, Paul Morie, and Dan Roth. 2004. Identification
and tracing of ambiguous names: Discriminative and
generative approaches. In AAAI, pages 419?424.
Percy Liang, Michael I. Jordan, and Ben Taskar. 2007a.
A permutation-augmented sampler for DP mixture
models. In Proceedings of ICML, pages 545?552,
New York, NY, USA. ACM.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007b. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of EMNLP-CoNLL, pages
688?697, Prague, Czech Republic, June. Association
for Computational Linguistics.
A. Mikheev, C. Grover, and M. Moens. 1998. Descrip-
tion of the LTG System Used for MUC-7. In Pro-
ceedings of the 7th Message Understanding Confer-
ence (MUC-7), Fairfax, Virginia.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Journal
of Linguisticae Investigationes, 30(1).
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705?767.
Vincent Ng. 2008. Unsupervised models for coreference
resolution. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 640?649, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In CIKM ?04: Proceedings of
the thirteenth ACM international conference on Infor-
mation and knowledge management, pages 137?145,
New York, NY, USA. ACM.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Ann. Probab., 25:855?900.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence, pages 472?479.
AAAI.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Walter
Daelemans and Miles Osborne, editors, Proceedings
of CoNLL-2003, pages 142?147. Edmonton, Canada.
Olga Uryupina. 2004. Evaluating name-matching for
coreference resolution. In Proceedings of LREC 04,
Lisbon.
172
Immediate-Head Parsing for Language Models
Eugene Charniak
Brown Laboratory for Linguistic Information Processing
Department of Computer Science
Brown University, Box 1910, Providence RI
ec@cs.brown.edu
Abstract
We present two language models based
upon an ?immediate-head? parser ?
our name for a parser that conditions
all events below a constituent c upon
the head of c. While all of the most
accurate statistical parsers are of the
immediate-head variety, no previous
grammatical language model uses this
technology. The perplexity for both
of these models significantly improve
upon the trigram model base-line as
well as the best previous grammar-
based language model. For the better
of our two models these improvements
are 24% and 14% respectively. We also
suggest that improvement of the un-
derlying parser should significantly im-
prove the model?s perplexity and that
even in the near term there is a lot of po-
tential for improvement in immediate-
head language models.
1 Introduction
All of the most accurate statistical parsers [1,3,
6,7,12,14] are lexicalized in that they condition
probabilities on the lexical content of the sen-
tences being parsed. Furthermore, all of these
This research was supported in part by NSF grant LIS
SBR 9720368 and by NSF grant 00100203 IIS0085980.
The author would like to thank the members of the Brown
Laboratory for Linguistic Information Processing (BLLIP)
and particularly Brian Roark who gave very useful tips on
conducting this research. Thanks also to Fred Jelinek and
Ciprian Chelba for the use of their data and for detailed com-
ments on earlier drafts of this paper.
parsers are what we will call immediate-head
parsers in that all of the properties of the imme-
diate descendants of a constituent c are assigned
probabilities that are conditioned on the lexical
head of c. For example, in Figure 1 the probability
that the vp expands into v np pp is conditioned on
the head of the vp, ?put?, as are the choices of the
sub-heads under the vp, i.e., ?ball? (the head of
the np) and ?in? (the head of the pp). It is the ex-
perience of the statistical parsing community that
immediate-head parsers are the most accurate we
can design.
It is also worthy of note that many of these
parsers [1,3,6,7] are generative ? that is, for a
sentence s they try to find the parse  defined by
Equation 1:
arg max

p( j s) = arg max

p(, s) (1)
This is interesting because insofar as they com-
pute p(, s) these parsers define a language-model
in that they can (in principle) assign a probability
to all possible sentences in the language by com-
puting the sum in Equation 2:
p(s) =
X

p(, s) (2)
where p(, s) is zero if the yield of  6= s. Lan-
guage models, of course, are of interest because
speech-recognition systems require them. These
systems determine the words that were spoken by
solving Equation 3:
arg maxsp(s j A) = arg maxsp(s)p(A j s) (3)
where A denotes the acoustic signal. The first
term on the right, p(s), is the language model, and
is what we compute via parsing in Equation 2.
put the ball in the box
verb/put det/the prep/in det/thenoun/ball noun/box
np/box
pp/innp/ball
vp/put
Figure 1: A tree showing head information
Virtually all current speech recognition sys-
tems use the so-called trigram language model in
which the probability of a string is broken down
into conditional probabilities on each word given
the two previous words. E.g.,
p(w0,n) =
Y
i=0,n 1
p(wi j wi 1, wi 2) (4)
On the other hand, in the last few years there
has been interest in designing language models
based upon parsing and Equation 2. We now turn
to this previous research.
2 Previous Work
There is, of course, a very large body of litera-
ture on language modeling (for an overview, see
[10]) and even the literature on grammatical lan-
guage models is becoming moderately large [4,
9,15,16,17]. The research presented in this pa-
per is most closely related to two previous efforts,
that by Chelba and Jelinek [4] (C&J) and that by
Roark [15], and this review concentrates on these
two papers. While these two works differ in many
particulars, we stress here the ways in which they
are similar, and similar in ways that differ from
the approach taken in this paper.
In both cases the grammar based language
model computes the probability of the next word
based upon the previous words of the sentence.
More specifically, these grammar-based models
compute a subset of all possible grammatical re-
lations for the prior words, and then compute
 the probability of the next grammatical situ-
ation, and
 the probability of seeing the next word given
each of these grammatical situations.
Also, when computing the probability of the next
word, both models condition on the two prior
heads of constituents. Thus, like a trigram model,
they use information about triples of words.
Neither of these models uses an immediate-
head parser. Rather they are both what we will
call strict left-to-right parsers. At each sentence
position in strict left-to-right parsing one com-
putes the probability of the next word given the
previous words (and does not go back to mod-
ify such probabilities). This is not possible in
immediate-head parsing. Sometimes the imme-
diate head of a constituent occurs after it (e.g,
in noun-phrases, where the head is typically the
rightmost noun) and thus is not available for con-
ditioning by a strict left-to-right parser.
There are two reasons why one might prefer
strict left-to-right parsing for a language model
(Roark [15] and Chelba, personal communica-
tion). First, the search procedures for guessing
the words that correspond to the acoustic signal
works left to right in the string. If the language
model is to offer guidance to the search procedure
it must do so as well.
The second benefit of strict left-to-right parsing
is that it is easily combined with the standard tri-
gram model. In both cases at every point in the
sentence we compute the probability of the next
word given the prior words. Thus one can inter-
polate the trigram and grammar probability esti-
mates for each word to get a more robust estimate.
It turns out that this is a good thing to do, as is
clear from Table 1, which gives perplexity results
for a trigram model of the data in column one, re-
sults for the grammar-model in column two, and
results for a model in which the two are interpo-
Model Perplexity
Trigram Grammar Interpolation
C&J 167.14 158.28 148.90
Roark 167.02 152.26 137.26
Table 1: Perplexity results for two previous
grammar-based language models
lated in column three.
Both the were trained and tested on the same
training and testing corpora, to be described in
Section 4.1. As indicated in the table, the trigram
model achieved a perplexity of 167 for the test-
ing corpus. The grammar models did slightly bet-
ter (e.g., 158.28 for the Chelba and Jelinek (C&J)
parser), but it is the interpolation of the two that
is clearly the winner (e.g., 137.26 for the Roark
parser/trigram combination). In both papers the
interpolation constants were 0.36 for the trigram
estimate and 0.64 for the grammar estimate.
While both of these reasons for strict-left-to-
right parsing (search and trigram interpolation)
are valid, they are not necessarily compelling.
The ability to combine easily with trigram models
is important only as long as trigram models can
improve grammar models. A sufficiently good
grammar model would obviate the need for tri-
grams. As for the search problem, we briefly re-
turn to this point at the end of the paper. Here
we simply note that while search requires that
a language model provide probabilities in a left
to right fashion, one can easily imagine proce-
dures where these probabilities are revised after
new information is found (i.e., the head of the
constituent). Note that already our search pro-
cedure needs to revise previous most-likely-word
hypotheses when the original guess makes the
subsequent words very unlikely. Revising the
associated language-model probabilities compli-
cates the search procedure, but not unimaginably
so. Thus it seems to us that it is worth finding
out whether the superior parsing performance of
immediate-head parsers translates into improved
language models.
3 The Immediate-Head Parsing Model
We have taken the immediate-head parser de-
scribed in [3] as our starting point. This parsing
model assigns a probability to a parse  by a top-
down process of considering each constituent c in
 and, for each c, first guessing the pre-terminal
of c, t(c) (t for ?tag?), then the lexical head of c,
h(c), and then the expansion of c into further con-
stituents e(c). Thus the probability of a parse is
given by the equation
p() =
Y
c2
p(t(c) j l(c), H(c))
p(h(c) j t(c), l(c), H(c))
p(e(c) j l(c), t(c), h(c), H(c))
where l(c) is the label of c (e.g., whether it is a
noun phrase (np), verb phrase, etc.) and H(c) is
the relevant history of c ? information outside c
that our probability model deems important in de-
termining the probability in question. In [3] H(c)
approximately consists of the label, head, and
head-part-of-speech for the parent of c: m(c), i(c),
and u(c) respectively. One exception is the distri-
bution p(e(c) j l(c), t(c), h(c), H(c)), where H only
includes m and u.1
Whenever it is clear to which constituent we
are referring we omit the (c) in, e.g., h(c). In this
notation the above equation takes the following
form:
p() =
Y
c2
p(t j l, m, u, i)  p(h j t, l, m, u, i)
p(e j l, t, h, m, u). (5)
Because this is a point of contrast with the parsers
described in the previous section, note that all
of the conditional distributions are conditioned
on one lexical item (either i or h). Thus only
p(h j t, l, m, u, i), the distribution for the head of c,
looks at two lexical items (i and h itself), and none
of the distributions look at three lexical items as
do the trigram distribution of Equation 4 and the
previously discussed parsing language models [4,
15].
Next we describe how we assign a probabil-
ity to the expansion e of a constituent. We break
up a traditional probabilistic context-free gram-
mar (PCFG) rule into a left-hand side with a label
l(c) drawn from the non-terminal symbols of our
grammar, and a right-hand side that is a sequence
1We simplify slightly in this section. See [3] for all the
details on the equations as well as the smoothing used.
of one or more such symbols. For each expansion
we distinguish one of the right-hand side labels as
the ?middle? or ?head? symbol M(c). M(c) is the
constituent from which the head lexical item h is
obtained according to deterministic rules that pick
the head of a constituent from among the heads of
its children. To the left of M is a sequence of one
or more left labels Li(c) including the special ter-
mination symbol 4, which indicates that there are
no more symbols to the left, and similarly for the
labels to the right, Ri(c). Thus an expansion e(c)
looks like:
l ! 4Lm. . . L1MR1. . . Rn4. (6)
The expansion is generated by guessing first M,
then in order L1 through Lm+1 (= 4), and similarly
for R1 through Rn+1.
In anticipation of our discussion in Section 4.2,
note that when we are expanding an Li we do not
know the lexical items to its left, but if we prop-
erly dovetail our ?guesses? we can be sure of what
word, if any, appears to its right and before M, and
similarly for the word to the left of Rj. This makes
such words available to be conditioned upon.
Finally, the parser of [3] deviates in two places
from the strict dictates of a language model. First,
as explicitly noted in [3], the parser does not com-
pute the partition function (normalization con-
stant) for its distributions so the numbers it re-
turns are not true probabilities. We noted there
that if we replaced the ?max-ent inspired? fea-
ture with standard deleted interpolation smooth-
ing, we took a significant hit in performance. We
have now found several ways to overcome this
problem, including some very efficient ways to
compute partition functions for this class of mod-
els. In the end, however, this was not neces-
sary, as we found that we could obtain equally
good performance by ?hand-crafting? our inter-
polation smoothing rather than using the ?obvi-
ous? method (which performs poorly).
Secondly, as noted in [2], the parser encourages
right branching with a ?bonus? multiplicative fac-
tor of 1.2 for constituents that end at the right
boundary of the sentence, and a penalty of 0.8
for those that do not. This is replaced by explic-
itly conditioning the events in the expansion of
Equation 6 on whether or not the constituent is at
the right boundary (barring sentence-final punctu-
ation). Again, with proper attention to details, this
can be known at the time the expansion is taking
place. This modification is much more complex
than the multiplicative ?hack,? and it is not quite
as good (we lose about 0.1% in precision/recall
figures), but it does allow us to compute true prob-
abilities.
The resulting parser strictly speaking defines
a PCFG in that all of the extra conditioning in-
formation could be included in the non-terminal-
node labels (as we did with the head information
in Figure 1). When a PCFG probability distribu-
tion is estimated from training data (in our case
the Penn tree-bank) PCFGs define a tight (sum-
ming to one) probability distribution over strings
[5], thus making them appropriate for language
models. We also empirically checked that our in-
dividual distributions (p(t j l, m, u, i), and p(h j
t, l, m, u, i) from Equation 5 and p(L j l, t, h, m, u),
p(M j l, t, h, m, u), and p(R j l, t, h, m, u) from
Equation 5) sum to one for a large, random, se-
lection of conditioning events2
As with [3], a subset of parses is computed with
a non-lexicalized PCFG, and the most probable
edges (using an empirically established thresh-
old) have their probabilities recomputed accord-
ing to the complete probability model of Equation
5. Both searches are conducted using dynamic
programming.
4 Experiments
4.1 The Immediate-Bihead Language Model
The parser as described in the previous section
was trained and tested on the data used in the pre-
viously described grammar-based language mod-
eling research [4,15]. This data is from the Penn
Wall Street Journal tree-bank [13], but modified
to make the text more ?speech-like?. In particu-
lar:
1. all punctuation is removed,
2. no capitalization is used,
3. all symbols and digits are replaced by the
symbol N, and
2They should sum to one. We are just checking that there
are no bugs in the code.
Model Perplexity
Trigram Grammar Interpolation
C&J 167.14 158.28 148.90
Roark 167.02 152.26 137.26
Bihead 167.89 144.98 133.15
Table 2: Perplexity results for the immediate-
bihead model
4. all words except for the 10,000 most com-
mon are replaced by the symbol UNK.
As in previous work, files F0 to F20 are used for
training, F21-F22 for development, and F23-F24
for testing.
The results are given in Table 2. We refer to
the current model as the bihead model. ?Bihead?
here emphasizes the already noted fact that in this
model probabilities involve at most two lexical
heads. As seen in Table 2, the immediate-bihead
model with a perplexity of 144.98 outperforms
both previous models, even though they use tri-
grams of words in their probability estimates.
We also interpolated our parsing model with
the trigram model (interpolation constant .36, as
with the other models) and this model outper-
forms the other interpolation models. Note, how-
ever, that because our parser does not define prob-
abilities for each word based upon previous words
(as with trigram) it is not possible to do the inte-
gration at the word level. Rather we interpolate
the probabilities of the entire sentences. This is a
much less powerful technique than the word-level
interpolation used by both C&J and Roark, but we
still observe a significant gain in performance.
4.2 The Immediate-Trihead Model
While the performance of the grammatical model
is good, a look at sentences for which the tri-
gram model outperforms it makes its limitations
apparent. The sentences in question have noun
phrases like ?monday night football? that trigram
models eats up but on which our bihead parsing
model performs less well. For example, consider
the sentence ?he watched monday night football?.
The trigram model assigns this a probability of
1. 9  10 5, while the grammar model gives it a
probability of 2. 77  10 7. To a first approxima-
tion, this is entirely due to the difference in prob-
monday night football
nbar
np
Figure 2: A noun-phrase with sub-structure
ability of the noun-phrase. For example, the tri-
gram probability p(football j monday, night) =
0. 366, and would have been 1.0 except that
smoothing saved some of the probability for other
things it might have seen but did not. Because the
grammar model conditions in a different order,
the closest equivalent probability would be that
for ?monday?, but in our model this is only con-
ditioned on ?football? so the probability is much
less biased, only 0. 0306. (Penn tree-bank base
noun-phrases are flat, thus the head above ?mon-
day? is ?football?.)
This immediately suggests creating a second
model that captures some of the trigram-like
probabilities that the immediate-bihead model
misses. The most obvious extension would be to
condition upon not just one?s parent?s head, but
one?s grandparent?s as well. This does capture
some of the information we would like, partic-
ularly the case heads of noun-phrases inside of
prepositional phrases. For example, in ?united
states of america?, the probability of ?america?
is now conditioned not just on ?of? (the head of
its parent) but also on ?states?.
Unfortunately, for most of the cases where tri-
gram really cleans up this revision would do lit-
tle. Thus, in ?he watched monday night football?
?monday? would now be conditioned upon ?foot-
ball? and ?watched.? The addition of ?watched?
is unlikely to make much difference, certainly
compared to the boost trigram models get by, in
effect, recognizing the complete name.
It is interesting to note, however, that virtu-
ally all linguists believe that a noun-phrase like
?monday night football? has significant substruc-
ture ? e.g., it would look something like Figure
2. If we assume this tree-structure the two heads
above ?monday? are ?night? and ?football? re-
spectively, thus giving our trihead model the same
power as the trigram for this case. Ignoring some
of the conditioning events, we now get a proba-
bility p(h = monday j i = night, j = football),
which is much higher than the corresponding bi-
head version p(h = monday j i = football). The
reader may remember that h is the head of the cur-
rent constituent, while i is the head of its parent.
We now define j to be the grandparent head.
We decided to adopt this structure, but to keep
things simple we only changed the definition of
?head? for the distribution p(h j t, l, m, u, i, j).
Thus we adopted the following revised definition
of head for constituents of base noun-phrases:
For a pre-terminal (e.g., noun) con-
stituent c of a base noun-phrase in
which it is not the standard head (h) and
which has as its right-sister another pre-
terminal constituent d which is not it-
self h, the head of c is the head of d. The
sole exceptions to this rule are phrase-
initial determiners and numbers which
retain h as their heads.
In effect this definition assumes that the sub-
structure of all base noun-phrases is left branch-
ing, as in Figure 2. This is not true, but Lauer
[11] shows that about two-thirds of all branching
in base-noun-phrases is leftward. We believe we
would get even better results if the parser could
determine the true branching structure.
We then adopt the following definition of a
grandparent-head feature j.
1. if c is a noun phrase under a prepositional
phrase, or is a pre-terminal which takes a
revised head as defined above, then j is the
grandparent head of c, else
2. if c is a pre-terminal and is not next (in the
production generating c) to the head of its
parent (i) then j(c) is the head of the con-
stituent next to c in the production in the di-
rection of the head of that production, else
3. j is a ?none-of-the-above? symbol.
Case 1 now covers both ?united states of amer-
ica? and ?monday night football? examples. Case
2 handles other flat constituents in Penn tree-bank
style (e.g., quantifier-phrases) for which we do
not have a good analysis. Case three says that this
feature is a no-op in all other situations.
Model Perplexity
Trigram Grammar Interpolation
C&J 167.14 158.28 148.90
Roark 167.02 152.26 137.26
Bihead 167.89 144.98 133.15
Trihead 167.89 130.20 126.07
Table 3: Perplexity results for the immediate-
trihead model
The results for this model, again trained on F0-
F20 and tested on F23-24, are given in Figure
3 under the heading ?Immediate-trihead model?.
We see that the grammar perplexity is reduced
to 130.20, a reduction of 10% over our first
model, 14% over the previous best grammar
model (152.26%), and 22% over the best of the
above trigram models for the task (167.02). When
we run the trigram and new grammar model in
tandem we get a perplexity of 126.07, a reduction
of 8% over the best previous tandem model and
24% over the best trigram model.
4.3 Discussion
One interesting fact about the immediate-trihead
model is that of the 3761 sentences in the test cor-
pus, on 2934, or about 75%, the grammar model
assigns a higher probability to the sentence than
does the trigram model. One might well ask what
went ?wrong? with the remaining 25%? Why
should the grammar model ever get beaten? Three
possible reasons come to mind:
1. The grammar model is better but only by a
small amount, and due to sparse data prob-
lems occasionally the worse model will luck
out and beat the better one.
2. The grammar model and the trigram model
capture different facts about the distribution
of words in the language, and for some set of
sentences one distribution will perform bet-
ter than the other.
3. The grammar model is, in some sense, al-
ways better than the trigram model, but if the
parser bungles the parse, then the grammar
model is impacted very badly. Obviously the
trigram model has no such Achilles? heel.
Sentence Group Num. Labeled Labeled
Precision Recall
All Sentences 3761 84.6% 83.7%
Grammar High 2934 85.7% 84.9%
Trigram High 827 80.1% 79.0%
Table 4: Precision/recall for sentences in which
trigram/grammar models performed best
We ask this question because what we should
do to improve performance of our grammar-based
language models depends critically on which of
these explanations is correct: if (1) we should col-
lect more data, if (2) we should just live with the
tandem grammar-trigram models, and if (3) we
should create better parsers.
Based upon a few observations on sentences
from the development corpus for which the tri-
gram model gave higher probabilities we hypoth-
esized that reason (3), bungled parses, is primary.
To test this we performed the following experi-
ment. We divide the sentences from the test cor-
pus into two groups, ones for which the trigram
model performs better, and the ones for which
the grammar model does better. We then collect
labeled precision and recall statistics (the stan-
dard parsing performance measures) separately
for each group. If our hypothesis is correct we ex-
pect the ?grammar higher? group to have more ac-
curate parses than the trigram-higher group as the
poor parse would cause poor grammar perplexity
for the sentence, which would then be worse than
the trigram perplexity. If either of the other two
explanations were correct one would not expect
much difference between the two groups. The re-
sults are shown in Table 4. We see there that, for
example, sentences for which the grammar model
has the superior perplexity have average recall 5.9
(= 84. 9 79. 0) percentage points higher than the
sentences for which the trigram model performed
better. The gap for precision is 5.6. This seems to
support our hypothesis.
5 Conclusion and Future Work
We have presented two grammar-based language
models, both of which significantly improve upon
both the trigram model baseline for the task (by
24% for the better of the two) and the best pre-
vious grammar-based language model (by 14%).
Furthermore we have suggested that improve-
ment of the underlying parser should improve the
model?s perplexity still further.
We should note, however, that if we were deal-
ing with standard Penn Tree-bank Wall-Street-
Journal text, asking for better parsers would be
easier said than done. While there is still some
progress, it is our opinion that substantial im-
provement in the state-of-the-art precision/recall
figures (around 90%) is unlikely in the near fu-
ture.3 However, we are not dealing with stan-
dard tree-bank text. As pointed out above, the
text in question has been ?speechified? by re-
moving punctuation and capitalization, and ?sim-
plified? by allowing only a fixed vocabulary of
10,000 words (replacing all the rest by the sym-
bol ?UNK?), and replacing all digits and symbols
by the symbol ?N?.
We believe that the resulting text grossly under-
represents the useful grammatical information
available to speech-recognition systems. First, we
believe that information about rare or even truly
unknown words would be useful. For example,
when run on standard text, the parser uses ending
information to guess parts of speech [3]. Even
if we had never encountered the word ?show-
boating?, the ?ing? ending tells us that this is
almost certainly a progressive verb. It is much
harder to determine this about UNK.4 Secondly,
while punctuation is not to be found in speech,
prosody should give us something like equiva-
lent information, perhaps even better. Thus sig-
nificantly better parser performance on speech-
derived data seems possible, suggesting that high-
performance trigram-less language models may
be within reach. We believe that the adaptation
of prosodic information to parsing use is a worthy
topic for future research.
Finally, we have noted two objections to
immediate-head language models: first, they
complicate left-to-right search (since heads are
often to the right of their children) and second,
3Furthermore, some of the newest wrinkles [8] use dis-
criminative methods and thus do not define language models
at all, seemingly making them ineligible for the competition
on a priori grounds.
4To give the reader some taste for the difficulties pre-
sented by UNKs, we encourage you to try parsing the fol-
lowing real example: ?its supposedly unk unk unk a unk that
makes one unk the unk of unk unk the unk radical unk of
unk and unk and what in unk even seems like unk in unk?.
they cannot be tightly integrated with trigram
models.
The possibility of trigram-less language mod-
els makes the second of these objections without
force. Nor do we believe the first to be a per-
manent disability. If one is willing to provide
sub-optimal probability estimates as one proceeds
left-to-right and then amend them upon seeing the
true head, left-to-right processing and immediate-
head parsing might be joined. Note that one of the
cases where this might be worrisome, early words
in a base noun-phrase could be conditioned upon
a head which comes several words later, has been
made significantly less problematic by our revised
definition of heads inside noun-phrases. We be-
lieve that other such situations can be brought into
line as well, thus again taming the search prob-
lem. However, this too is a topic for future re-
search.
References
1. BOD, R. What is the minimal set of frag-
ments that achieves maximal parse accuracy.
In Proceedings of Association for Computa-
tional Linguistics 2001. 2001.
2. CHARNIAK, E. Tree-bank grammars. In Pro-
ceedings of the Thirteenth National Con-
ference on Artificial Intelligence. AAAI
Press/MIT Press, Menlo Park, 1996, 1031?
1036.
3. CHARNIAK, E. A maximum-entropy-
inspired parser. In Proceedings of the 2000
Conference of the North American Chapter of
the Association for Computational Linguistics .
ACL, New Brunswick NJ, 2000.
4. CHELBA, C. AND JELINEK, F. Exploiting
syntactic structure for language modeling. In
Proceedings for COLING-ACL 98. ACL, New
Brunswick NJ, 1998, 225?231.
5. CHI, Z. AND GEMAN, S. Estimation of
probabilistic context-free grammars. Computa-
tional Linguistics 24 2 (1998), 299?306.
6. COLLINS, M. J. Three generative lexicalized
models for statistical parsing. In Proceedings
of the 35th Annual Meeting of the ACL. 1997,
16?23.
7. COLLINS, M. J. Head-Driven Statistical
Models for Natural Language Parsing. Univer-
sity of Pennsylvania, Ph.D. Dissertation, 1999.
8. COLLINS, M. J. Discriminative reranking for
natural language parsing. In Proceedings of the
International Conference on Machine Learning
(ICML 2000) . 2000.
9. GODDEAU, D. Using probabilistic shift-
reduce parsing in speech recognition systems.
In Proceedings of the 2nd International Confer-
ence on Spoken Language Processing. 1992,
321?324.
10. GOODMAN, J. Putting it all together: lan-
guage model combination. In ICASSP-2000.
2000.
11. LAUER, M. Corpus statistics meet the noun
compound: some empirical results. In Proceed-
ings of the 33rd Annual Meeting of the Associ-
ation for Computational Linguistics. 1995, 47?
55.
12. MAGERMAN, D. M. Statistical decision-tree
models for parsing. In Proceedings of the 33rd
Annual Meeting of the Association for Com-
putational Linguistics. 1995, 276?283.
13. MARCUS, M. P., SANTORINI, B. AND
MARCINKIEWICZ, M. A. Building a large
annotated corpus of English: the Penn tree-
bank. Computational Linguistics 19 (1993),
313?330.
14. RATNAPARKHI, A. Learning to parse natural
language with maximum entropy models. Ma-
chine Learning 34 1/2/3 (1999), 151?176.
15. ROARK, B. Probabilistic top-down parsing
and language modeling. Computational Lin-
guistics (forthcoming).
16. STOLCKE, A. An efficient probabilistic
context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics
21 (1995), 165?202.
17. STOLCKE, A. AND SEGAL, J. Precise n-
gram probabilities from stochastic context-free
grammars. In Proceedings of the 32th Annual
Meeting of the Association for Computational
Linguistics. 1994, 74?79.
Entropy Rate Constancy in Text
Dmitriy Genzel and Eugene Charniak
Brown Laboratory for Linguistic Information Processing
Department of Computer Science
Brown University
Providence, RI, USA, 02912
fdg,ecg@cs.brown.edu
Abstract
We present a constancy rate princi-
ple governing language generation. We
show that this principle implies that lo-
cal measures of entropy (ignoring con-
text) should increase with the sentence
number. We demonstrate that this is
indeed the case by measuring entropy
in three dierent ways. We also show
that this eect has both lexical (which
words are used) and non-lexical (how
the words are used) causes.
1 Introduction
It is well-known from Information Theory that
the most ecient way to send information
through noisy channels is at a constant rate. If
humans try to communicate in the most ecient
way, then they must obey this principle. The
communication medium we examine in this pa-
per is text, and we present some evidence that
this principle holds here.
Entropy is a measure of information rst pro-
posed by Shannon (1948). Informally, entropy
of a random variable is proportional to the di-
culty of correctly guessing the value of this vari-
able (when the distribution is known). Entropy
is the highest when all values are equally prob-
able, and is lowest (equal to 0) when one of the
choices has probability of 1, i.e. deterministi-
cally known in advance.
In this paper we are concerned with entropy
of English as exhibited through written text,
though these results can easily be extended to
speech as well. The random variable we deal
with is therefore a unit of text (a word, for our
purposes1) that a random person who has pro-
duced all the previous words in the text stream
is likely to produce next. We have as many ran-
dom variables as we have words in a text. The
distributions of these variables are obviously dif-
ferent and depend on all previous words pro-
duced. We claim, however, that the entropy of
these random variables is on average the same2.
2 Related Work
There has been work in the speech community
inspired by this constancy rate principle. In
speech, distortion of the audio signal is an extra
source of uncertainty, and this principle can by
applied in the following way:
A given word in one speech context might be
common, while in another context it might be
rare. To keep the entropy rate constant over
time, it would be necessary to take more time
(i.e., pronounce more carefully) in less common
situations. Aylett (1999) shows that this is in-
deed the case.
It has also been suggested that the principle
of constant entropy rate agrees with biological
evidence of how human language processing has
evolved (Plotkin and Nowak, 2000).
Kontoyiannis (1996) also reports results on 5
consecutive blocks of characters from the works
1It may seem like an arbitrary choice, but a word is a
natural unit of length, after all when one is asked to give
the length of an essay one typically chooses the number
of words as a measure.
2Strictly speaking, we want the cross-entropy between
all words in the sentences number n and the true model
of English to be the same for all n.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 199-206.
                         Proceedings of the 40th Annual Meeting of the Association for
of Jane Austen which are in agreement with our
principle and, in particular, with its corollary as
derived in the following section.
3 Problem Formulation
Let fXig, i = 1 . . . n be a sequence of random
variables, with Xi corresponding to word wi in
the corpus. Let us consider i to be xed. The
random variable we are interested in is Yi, a ran-
dom variable that has the same distribution as
XijX1 = w1, . . . ,Xi?1 = wi?1 for some xed
words w
1
. . . wi?1. For each word wi there will
be some word wj , (j  i) which is the start-
ing word of the sentence wi belongs to. We will
combine random variables X
1
. . . Xi?1 into two
sets. The rst, which we call Ci (for context),
contains X
1
through Xj?1, i.e. all the words
from the preceding sentences. The remaining
set, which we call Li (for local), will contain
words Xj through Xi?1 . Both Li and Ci could
be empty sets. We can now write our variable
Yi as XijCi, Li.
Our claim is that the entropy of Yi , H(Yi)
stays constant for all i. By the denition of rel-
ative mutual information between Xi and Ci,
H(Yi) = H(XijCi, Li)
= H(XijLi) ? I(XijCi, Li)
where the last term is the mutual information
between the word and context given the sen-
tence. As i increases, so does the set Ci. Li, on
the other hand, increases until we reach the end
of the sentence, and then becomes small again.
Intuitively, we expect the mutual information
at, say, word k of each sentence (where Li has
the same size for all i) to increase as the sen-
tence number is increasing. By our hypothesis
we then expect H(XijLi) to increase with the
sentence number as well.
Current techniques are not very good at es-
timating H(Yi), because we do not have a
very good model of context, since this model
must be mostly semantic in nature. We have
shown, however, that if we can instead estimate
H(XijLi) and show that it increases with the
sentence number, we will provide evidence to
support the constancy rate principle.
The latter expression is much easier to esti-
mate, because it involves only words from the
beginning of the sentence whose relationship
is largely local and can be successfully cap-
tured through something as simple as an n-gram
model.
We are only interested in the mean value of
the H(Xj jLj) for wj 2 Si, where Si is the ith
sentence. This number is equal to 1
jS
i
j
H(Si),
which reduces the problem to the one of esti-
mating the entropy of a sentence.
We use three dierent ways to estimate the
entropy:
 Estimate H(Si) using an n-gram probabilis-
tic model
 Estimate H(Si) using a probabilistic model
induced by a statistical parser
 Estimate H(Xi) directly, using a non-para-
metric estimator. We estimate the entropy
for the beginning of each sentence. This
approach estimates H(Xi), not H(XijLi),
i.e. ignores not only the context, but also
the local syntactic information.
4 Results
4.1 N-gram
N-gram models make the simplifying assump-
tion that the current word depends on a con-
stant number of the preceding words (we use
three). The probability model for sentence S
thus looks as follows:
P (S) = P (w
1
)P (w
2
jw
1
)P (w
3
jw
2
w
1
)

n
?
i=4
P (wnjwn?1wn?2wn?3)
To estimate the entropy of the sentence S, we
compute log P (S). This is in fact an estimate of
cross entropy between our model and true distri-
bution. Thus we are overestimating the entropy,
but if we assume that the overestimation error is
more or less uniform, we should still see our esti-
mate increase as the sentence number increases.
Penn Treebank corpus (Marcus et al, 1993)
sections 0-20 were used for training, sections 21-
24 for testing. Each article was treated as a sep-
arate text, results for each sentence number were
grouped together, and the mean value reported
on Figure 1 (dashed line). Since most articles
are short, there are fewer sentences available for
larger sentence numbers, thus results for large
sentence numbers are less reliable.
The trend is fairly obvious, especially for
small sentence numbers: sentences (with no con-
text used) get harder as sentence number in-
creases, i.e. the probability of the sentence given
the model decreases.
4.2 Parser Model
We also computed the log-likelihood of the sen-
tence using a statistical parser described in
Charniak (2001)3. The probability model for
sentence S with parse tree T is (roughly):
P (S) =
?
x2T
P (xjparents(x))
where parents(x) are words which are parents
of node x in the the tree T . This model takes
into account syntactic information present in
the sentence which the previous model does not.
The entropy estimate is again log P (S). Overall,
these estimates are lower (closer to the true en-
tropy) in this model because the model is closer
to the true probability distribution. The same
corpus, training and testing sets were used. The
results are reported on Figure 1 (solid line). The
estimates are lower (better), but follow the same
trend as the n-gram estimates.
4.3 Non-parametric Estimator
Finally we compute the entropy using the esti-
mator described in (Kontoyiannis et al, 1998).
The estimation is done as follows. Let T be our
training corpus. Let S = fw
1
. . . wng be the test
sentence. We nd the largest k  n, such that
sequence of words w
1
. . . wk occurs in T . Then
log S
k is an estimate of the entropy at the word
w
1
. We compute such estimates for many rst
sentences, second sentences, etc., and take the
average.
3This parser does not proceed in a strictly left-to-right
fashion, but this is not very important since we estimate
entropy for the whole sentence, rather than individual
words
For this experiment we used 3 million words of
the Wall Street Journal (year 1988) as the train-
ing set and 23 million words (full year 1987) as
the testing set4. The results are shown on Fig-
ure 2. They demonstrate the expected behavior,
except for the strong abnormality on the second
sentence. This abnormality is probably corpus-
specic. For example, 1.5% of the second sen-
tences in this corpus start with words \the terms
were not disclosed", which makes such sentences
easy to predict and decreases entropy.
4.4 Causes of Entropy Increase
We have shown that the entropy of a sentence
(taken without context) tends to increase with
the sentence number. We now examine the
causes of this eect.
These causes may be split into two categories:
lexical (which words are used) and non-lexical
(how the words are used). If the eects are en-
tirely lexical, we would expect the per-word en-
tropy of the closed-class words not to increase
with sentence number, since presumably the
same set of words gets used in each sentence.
For this experiment we use our n-gram estima-
tor as described in Section 4.2. We evaluate
the per-word entropy for nouns, verbs, deter-
miners, and prepositions. The results are given
in Figure 3 (solid lines). The results indicate
that entropy of the closed class words increases
with sentence number, which presumably means
that non-lexical eects (e.g. usage) are present.
We also want to check for presence of lexical
eects. It has been shown by Kuhn and Mohri
(1990) that lexical eects can be easily captured
by caching. In its simplest form, caching in-
volves keeping track of words occurring in the
previous sentences and assigning for each word
w a caching probability Pc(w) =
C(w)
?
w
C(w)
, where
C(w) is the number of times w occurs in the
previous sentences. This probability is then
mixed with the regular probability (in our case
- smoothed trigram) as follows:
Pmixed(w) = (1 ? ?)Pngram(w) + ?Pc(w)
4This is not the same training set as the one used in
two previous experiments. For this experiment we needed
a larger, but similar data set
0 5 10 15 20 25
6.8
7
7.2
7.4
7.6
7.8
8
8.2
8.4
sentence number
e
n
tro
py
 e
st
im
at
e
parser
n?gram
Figure 1: N-gram and parser estimates of entropy (in bits per word)
0 5 10 15 20 25
8
8.1
8.2
8.3
8.4
8.5
8.6
8.7
8.8
8.9
9
sentence number
e
n
tro
py
 e
st
im
at
e
Figure 2: Non-parametric estimate of entropy
where ? was picked to be 0.1. This new prob-
ability model is known to have lower entropy.
More complex caching techniques are possible
(Goodman, 2001), but are not necessary for this
experiment.
Thus, if lexical eects are present, we expect
the model that uses caching to provide lower
entropy estimates. The results are given in Fig-
ure 3 (dashed lines). We can see that caching
gives a signicant improvement for nouns and a
small one for verbs, and gives no improvement
for the closed-class parts of speech. This shows
that lexical eects are present for the open-class
parts of speech and (as we assumed in the previ-
ous experiment) are absent for the closed-class
parts of speech. Since we have proven the pres-
ence of the non-lexical eects in the previous
experiment, we can see that both lexical and
non-lexical eects are present.
5 Conclusion and Future Work
We have proposed a fundamental principle of
language generation, namely the entropy rate
constancy principle. We have shown that en-
tropy of the sentences taken without context in-
creases with the sentence number, which is in
agreement with the above principle. We have
also examined the causes of this increase and
shown that they are both lexical (primarily for
open-class parts of speech) and non-lexical.
These results are interesting in their own
right, and may have practical implications as
well. In particular, they suggest that language
modeling may be a fruitful way to approach is-
sues of contextual influence in text.
Of course, to some degree language-modeling
caching work has always recognized this, but
this is rather a crude use of context and does
not address the issues which one normally thinks
of when talking about context. We have seen,
however, that entropy measurements can pick
up much more subtle influences, as evidenced
by the results for determiners and prepositions
where we see no caching influence at all, but nev-
ertheless observe increasing entropy as a func-
tion of sentence number. This suggests that
such measurements may be able to pick up more
obviously semantic contextual influences than
simply the repeating words captured by caching
models. For example, sentences will dier in
how much useful contextual information they
carry. Are there useful generalizations to be
made? E.g., might the previous sentence always
be the most useful, or, perhaps, for newspa-
per articles, the rst sentence? Can these mea-
surements detect such already established con-
textual relations as the given-new distinction?
What about other pragmatic relations? All of
these deserve further study.
6 Acknowledgments
We would like to acknowledge the members of
the Brown Laboratory for Linguistic Informa-
tion Processing and particularly Mark Johnson
for many useful discussions. Also thanks to
Daniel Jurafsky who early on suggested the in-
terpretation of our data that we present here.
This research has been supported in part by
NSF grants IIS 0085940, IIS 0112435, and DGE
9870676.
References
M. P. Aylett. 1999. Stochastic suprasegmentals: Re-
lationships between redundancy, prosodic struc-
ture and syllabic duration. In Proceedings of
ICPhS?99, San Francisco.
E. Charniak. 2001. A maximum-entropy-inspired
parser. In Proceedings of ACL?2001, Toulouse.
J. T. Goodman. 2001. A bit of progress in lan-
guage modeling. Computer Speech and Language,
15:403{434.
I. Kontoyiannis, P. H. Algoet, Yu. M. Suhov, and
A.J. Wyner. 1998. Nonparametric entropy esti-
mation for stationary processes and random elds,
with applications to English text. IEEE Trans.
Inform. Theory, 44:1319{1327, May.
I. Kontoyiannis. 1996. The complexity and en-
tropy of literary styles. NSF Technical Report No.
97, Department of Statistics, Stanford University,
June. [unpublished, can be found at the author?s
web page].
R. Kuhn and R. De Mori. 1990. A cache-based
natural language model for speech reproduction.
IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 12(6):570{583.
2 4 6 8 10
8
8.5
9
9.5
Nouns
normal 
caching
2 4 6 8 10
9.5
10
10.5
11
Verbs
normal 
caching
2 4 6 8 10
4.6
4.8
5
5.2
5.4
Prepositions
normal 
caching
2 4 6 8 10
3.7
3.8
3.9
4
4.1
4.2
4.3
4.4
Determiners
normal 
caching
Figure 3: Comparing Parts of Speech
M. P. Marcus, B. Santorini, and M. A. Marcin-
kiewicz. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Computational
Linguistics, 19:313{330.
J. B. Plotkin and M. A. Nowak. 2000. Language
evolution and information theory. Journal of The-
oretical Biology, pages 147{159.
C. E. Shannon. 1948. A mathematical theory of
communication. The Bell System Technical Jour-
nal, 27:379{423, 623{656, July, October.
A TAG-based noisy channel model of speech repairs
Mark Johnson
Brown University
Providence, RI 02912
mj@cs.brown.edu
Eugene Charniak
Brown University
Providence, RI 02912
ec@cs.brown.edu
Abstract
This paper describes a noisy channel model of
speech repairs, which can identify and correct
repairs in speech transcripts. A syntactic parser
is used as the source model, and a novel type
of TAG-based transducer is the channel model.
The use of TAG is motivated by the intuition
that the reparandum is a ?rough copy? of the
repair. The model is trained and tested on the
Switchboard disfluency-annotated corpus.
1 Introduction
Most spontaneous speech contains disfluencies
such as partial words, filled pauses (e.g., ?uh?,
?um?, ?huh?), explicit editing terms (e.g., ?I
mean?), parenthetical asides and repairs. Of
these repairs pose particularly difficult problems
for parsing and related NLP tasks. This paper
presents an explicit generative model of speech
repairs and shows how it can eliminate this kind
of disfluency.
While speech repairs have been studied by
psycholinguists for some time, as far as we know
this is the first time a probabilistic model of
speech repairs based on a model of syntactic
structure has been described in the literature.
Probabilistic models have the advantage over
other kinds of models that they can in principle
be integrated with other probabilistic models to
produce a combined model that uses all avail-
able evidence to select the globally optimal anal-
ysis. Shriberg and Stolcke (1998) studied the lo-
cation and distribution of repairs in the Switch-
board corpus, but did not propose an actual
model of repairs. Heeman and Allen (1999) de-
scribe a noisy channel model of speech repairs,
but leave ?extending the model to incorporate
higher level syntactic . . . processing? to future
work. The previous work most closely related
to the current work is Charniak and Johnson
(2001), who used a boosted decision stub classi-
fier to classify words as edited or not on a word
by word basis, but do not identify or assign a
probability to a repair as a whole.
There are two innovations in this paper.
First, we demonstrate that using a syntactic
parser-based language model Charniak (2001)
instead of bi/trigram language models signifi-
cantly improves the accuracy of repair detection
and correction. Second, we show how Tree Ad-
joining Grammars (TAGs) can be used to pro-
vide a precise formal description and probabilis-
tic model of the crossed dependencies occurring
in speech repairs.
The rest of this paper is structured as fol-
lows. The next section describes the noisy chan-
nel model of speech repairs and the section af-
ter that explains how it can be applied to de-
tect and repair speech repairs. Section 4 evalu-
ates this model on the Penn 3 disfluency-tagged
Switchboard corpus, and section 5 concludes
and discusses future work.
2 A noisy channel model of repairs
We follow Shriberg (1994) and most other work
on speech repairs by dividing a repair into three
parts: the reparandum (the material repaired),
the interregnum that is typically either empty
or consists of a filler, and the repair. Figure 1
shows these three parts for a typical repair.
Most current probabilistic language models
are based on HMMs or PCFGs, which induce
linear or tree-structured dependencies between
words. The relationship between reparandum
and repair seems to be quite different: the
repair is a ?rough copy? of the reparandum,
often incorporating the same or very similar
words in roughly the same word order. That
is, they seem to involve ?crossed? dependen-
cies between the reparandum and the repair,
shown in Figure 1. Languages with an un-
bounded number of crossed dependencies can-
not be described by a context-free or finite-
state grammar, and crossed dependencies like
these have been used to argue natural languages
. . . a flight to Boston,
? ?? ?
Reparandum
uh, I mean,
? ?? ?
Interregnum
to Denver
? ?? ?
Repair
on Friday . . .
Figure 1: The structure of a typical repair, with crossing dependencies between reparandum and
repair.
Imean
uh
a flight to Boston
to Denver on Friday
Figure 2: The ?helical? dependency structure induced by the generative model of speech repairs
for the repair depicted in Figure 1.
are not context-free Shieber (1985). Mildly
context-sensitive grammars, such as Tree Ad-
joining Grammars (TAGs) and Combinatory
Categorial Grammars, can describe such cross-
ing dependencies, and that is why TAGs are
used here.
Figure 2 shows the combined model?s de-
pendency structure for the repair of Figure 1.
Interestingly, if we trace the temporal word
string through this dependency structure, align-
ing words next to the words they are dependent
on, we obtain a ?helical? type of structure famil-
iar from genome models, and in fact TAGs are
being used to model genomes for very similar
reasons.
The noisy channel model described here in-
volves two components. A language model de-
fines a probability distribution P(X) over the
source sentences X, which do not contain re-
pairs. The channel model defines a conditional
probability distribution P(Y |X) of surface sen-
tences Y , which may contain repairs, given
source sentences. In the work reported here,
X is a word string and Y is a speech tran-
scription not containing punctuation or partial
words. We use two language models here: a
bigram language model, which is used in the
search process, and a syntactic parser-based lan-
guage model Charniak (2001), which is used
to rescore a set of the most likely analysis ob-
tained using the bigram model. Because the
language model is responsible for generating the
well-formed sentence X, it is reasonable to ex-
pect that a language model that can model more
global properties of sentences will lead to bet-
ter performance, and the results presented here
show that this is the case. The channel model is
a stochastic TAG-based transducer; it is respon-
sible for generating the repairs in the transcript
Y , and it uses the ability of TAGs to straight-
forwardly model crossed dependencies.
2.1 Informal description
Given an observed sentence Y we wish to find
the most likely source sentence X? , where:
X? = argmax
X
P(X|Y ) = argmax
X
P(Y |X)P(Y ).
This is the same general setup that is used
in statistical speech recognition and machine
translation, and in these applications syntax-
based language models P(Y ) yield state-of-the-
art performance, so we use one such model here.
The channel model P(Y |X) generates sen-
tences Y given a source X. A repair can po-
tentially begin before any word of X. When a
repair has begun, the channel model incremen-
tally processes the succeeding words from the
start of the repair. Before each succeeding word
either the repair can end or else a sequence of
words can be inserted in the reparandum. At
the end of each repair, a (possibly null) inter-
regnum is appended to the reparandum.
The intuition motivating the channel model
design is that the words inserted into the
reparandum are very closely related those in the
repair. Indeed, in our training data over 60% of
the words in the reparandum are exact copies of
words in the repair; this similarity is strong evi-
dence of a repair. The channel model is designed
so that exact copy reparandum words will have
high probability.
We assume that X is a substring of Y , i.e.,
that the source sentence can be obtained by
deleting words from Y , so for a fixed observed
sentence there are only a finite number of pos-
sible source sentences. However, the number of
source sentences grows exponentially with the
length of Y , so exhaustive search is probably
infeasible.
TAGs provide a systematic way of formaliz-
ing the channel model, and their polynomial-
time dynamic programming parsing algorithms
can be used to search for likely repairs, at least
when used with simple language models like a
bigram language model. In this paper we first
identify the 20 most likely analysis of each sen-
tence using the TAG channel model together
with a bigram language model. Then each of
these analysis is rescored using the TAG chan-
nel model and a syntactic parser based language
model.
The TAG channel model?s analysis do not re-
flect the syntactic structure of the sentence be-
ing analyzed; instead they encode the crossed
dependencies of the speech repairs. If we want
to use TAG dynamic programming algorithms
to efficiently search for repairs, it is necessary
that the intersection (in language terms) of the
TAG channel model and the language model it-
self be describable by a TAG. One way to guar-
antee this is to use a finite state language model;
this motivates our use of a bigram language
model.
On the other hand, it seems desirable to use a
language model that is sensitive to more global
properties of the sentence, and we do this by
reranking the initial analysis, replacing the bi-
gram language model with a syntactic parser
based model. We do not need to intersect this
parser based language model with our TAG
channel model since we evaluate each analysis
separately.
2.2 The TAG channel model
The TAG channel model defines a stochastic
mapping of source sentences X into observed
sentences Y . There are several ways to de-
fine transducers using TAGs such as Shieber
and Schabes (1990), but the following simple
method, inspired by finite-state transducers,
suffices for the application here. The TAG de-
fines a language whose vocabulary is the set of
pairs (??{?})?(??{?}), where ? is the vocab-
ulary of the observed sentences Y . A string Z
in this language can be interpreted as a pair
of strings (Y,X), where Y is the concatena-
tion of the projection of the first components
of Z and X is the concatenation of the projec-
tion of the second components. For example,
the string Z = a:a flight:flight to:? Boston:?
uh:? I:? mean:? to:to Denver:Denver on:on Fri-
day:Friday corresponds to the observed string
Y = a flight to Boston uh I mean to Denver
on Friday and the source string X = a flight to
Denver on Friday.
Figure 3 shows the TAG rules used to gen-
erate this example. The nonterminals in this
grammar are of the form Nwx, Rwy:wx and I,
where wx is a word appearing in the source
string and wy is a word appearing in the ob-
served string. Informally, the Nwx nonterminals
indicate that the preceding word wx was an-
alyzed as not being part of a repair, while the
Rwy:wx that the preceding words wy and wx were
part of a repair. The nonterminal I generates
words in the interregnum of a repair. Encoding
the preceding words in the TAGs nonterminals
permits the channel model to be sensitive to
lexical properties of the preceding words. The
start symbol is N$, where ?$? is a distinguished
symbol used to indicate the beginning and end
of sentences.
2.3 Estimating the repair channel
model from data
The model is trained from the disfluency and
POS tagged Switchboard corpus on the LDC
Penn tree bank III CD-ROM (specifically, the
files under dysfl/dps/swbd). This version of the
corpus annotates the beginning and ending po-
sitions of repairs as well as fillers, editing terms,
asides, etc., which might serve as the interreg-
num in a repair. The corpus also includes punc-
tuation and partial words, which are ignored in
both training and evaluation here since we felt
that in realistic applications these would not be
available in speech recognizer output. The tran-
script of the example of Figure 1 would look
something like the following:
a/DT flight/NN [to/IN Boston/NNP +
{F uh/UH} {E I/PRP mean/VBP} to/IN
Denver/NNP] on/IN Friday/NNP
In this transcription the repair is the string
from the opening bracket ?[? to the interrup-
tion point ?+?; the interregnum is the sequence
of braced strings following the interregnum, and
the repair is the string that begins at the end of
the interregnum and ends at the closing bracket
?]?. The interregnum consists of the braced
(?1) Nwant
a:a Na ?
1 ? Pn(repair|a)
(?2) Na
flight:flight Rflight:flight
I?
Pn(repair|flight)
(?3) NDenver
on:on Non ?
1 ? Pn(repair|on)
(?5) I
uh I
I mean
Pi(uh Imean)
(?1) Rflight:flight
to:? Rto:to
R?flight:flight to:to
Pr(copy|flight,flight)
(?2) Rto:to
Boston:? RBoston:Denver
R?to:to Denver:Denver
Pr(subst|to, to)Pr(Boston|subst, to,Denver)
(?3) RBoston:Denver
R?Boston:Denver NDenver ?
Pr(nonrep|Boston,Denver)
(?4) RBoston,Denver
RBoston,tomorrow
R?Boston,Denver tomorrow:tomorrow
Pr(del|Boston,Denver)
(?5) RBoston,Denver
tomorrow:? Rtomorrow,Denver
R?Boston,Denver
Pr(ins|Boston,Denver)
Pr(tomorrow|ins,Boston,Denver)
. . .
?1
?2
?5 ?1
?2
?3
?3
?4
. . .
Nwant
a:a Na
flight:flight Rflight:flight
to:? Rto:to
Boston:? RBoston:Denver
RBoston:Denver
Rto:to
Rflight:flight
I
uh:? I
I:? mean:?
to:to
Denver:Denver
NDenver
on:on Non
Friday:Friday NFriday
. . .
Figure 3: The TAG rules used to generate the example shown in Figure 1 and their respective
weights, and the corresponding derivation and derived trees.
expressions immediately following the interrup-
tion point. We used the disfluency tagged
version of the corpus for training rather than
the parsed version because the parsed version
does not mark the interregnum, but we need
this information for training our repair channel
model. Testing was performed using data from
the parsed version since this data is cleaner, and
it enables a direct comparison with earlier work.
We followed Charniak and Johnson (2001) and
split the corpus into main training data, held-
out training data and test data as follows: main
training consisted of all sw[23]*.dps files, held-
out training consisted of all sw4[5-9]*.dps files
and test consisted of all sw4[0-1]*.mrg files.
We now describe how the weights on the TAG
productions described in subsection 2.2 are es-
timated from this training data. In order to es-
timate these weights we need to know the TAG
derivation of each sentence in the training data.
In order to uniquely determine this we need the
not just the locations of each reparandum, in-
terregnum and repair (which are annotated in
the corpus) but also the crossing dependencies
between the reparandum and repair words, as
indicated in Figure 1.
We obtain these by aligning the reparan-
dum and repair strings of each repair using
a minimum-edit distance string aligner with
the following alignment costs: aligning identi-
cal words costs 0, aligning words with the same
POS tag costs 2, an insertion or a deletion costs
4, aligning words with POS tags that begin with
the same letter costs 5, and an arbitrary sub-
stitution costs 7. These costs were chosen so
that a substitution will be selected over an in-
sertion followed by a deletion, and the lower
cost for substitutions involving POS tags be-
ginning with the same letter is a rough and
easy way of establishing a preference for align-
ing words whose POS tags come from the same
broad class, e.g., it results in aligning singular
and plural nouns, present and past participles,
etc. While we did not evaluate the quality of
the alignments since they are not in themselves
the object of this exercise, they seem to be fairly
good.
From our training data we estimate a number
of conditional probability distributions. These
estimated probability distributions are the lin-
ear interpolation of the corresponding empirical
distributions from the main sub-corpus using
various subsets of conditioning variables (e.g.,
bigram models are mixed with unigram models,
etc.) using Chen?s bucketing scheme Chen and
Goodman (1998). As is commonly done in lan-
guage modelling, the interpolation coefficients
are determined by maximizing the likelihood of
the held out data counts using EM. Special care
was taken to ensure that all distributions over
words ranged over (and assigned non-zero prob-
ability to) every word that occurred in the train-
ing corpora; this turns out to be important as
the size of the training data for the different
distributions varies greatly.
The first distribution is defined over the
words in source sentences (i.e., that do
not contain reparandums or interregnums).
Pn(repair|W ) is the probability of a repair be-
ginning after a word W in the source sentence
X; it is estimated from the training sentences
with reparandums and interregnums removed.
Here and in what follows, W ranges over ? ?
{$}, where ?$? is a distinguished beginning-of-
sentence marker. For example, Pn(repair|flight)
is the probability of a repair beginning after the
word flight. Note that repairs are relatively rare;
in our training data Pn(repair) ? 0.02, which is
a fairly strong bias against repairs.
The other distributions are defined over
aligned reparandum/repair strings, and are es-
timated from the aligned repairs extracted from
the training data. In training we ignored
all overlapping repairs (i.e., cases where the
reparandum of one repair is the repair of an-
other). (Naturally, in testing we have no such
freedom.) We analyze each repair as consisting
of n aligned word pairs (we describe the inter-
regnum model later). Mi is the ith reparan-
dum word and Ri is the corresponding repair
word, so both of these range over ? ? {?}.
We define M0 and R0 to be source sentence
word that preceded the repair (which is ?$? if
the repair begins at the beginning of a sen-
tence). We define M ?i and R?i to be the last non-?
reparandum and repair words respectively, i.e.,
M ?i = Mi if Mi 6= ? and M ?i = M ?i?1 oth-
erwise. Finally, Ti, i = 1 . . . n + 1, which in-
dicates the type of repair that occurs at posi-
tion i, ranges over {copy, subst, ins, del, nonrep},
where Tn+1 = nonrep (indicating that the re-
pair has ended), and for i = 1 . . . n, Ti = copy if
Mi = Ri, Ti = ins if Ri = ?, Ti = del if Mi = ?
and Ti = subst otherwise.
The distributions we estimate from the
aligned repair data are the following.
Pr(Ti|M ?i?1, R?i?1) is the probability of see-
ing repair type Ti following the reparan-
dum word M ?i?1 and repair word R?i?1; e.g.,
Pr(nonrep|Boston,Denver) is the probability of
the repair ending when Boston is the last
reparandum word and Denver is the last repair
word.
Pr(Mi|Ti = ins,M ?i?1, R?i) is the probability
that Mi is the word that is inserted into the
reparandum (i.e., Ri = ?) given that some word
is substituted, and that the preceding reparan-
dum and repair words are M ?i?1 and R?i. For ex-
ample Pr(tomorrow|ins,Boston,Denver) is the
probability that the word tomorrow is inserted
into the reparandum after the words Boston and
Denver, given that some word is inserted.
Pr(Mi|Ti = subst,M ?i?1, R?i) is the prob-
ability that Mi is the word that is substi-
tuted in the reparandum for R?i, given that
some word is substituted. For example,
Pr(Boston|subst, to,Denver) is the probability
that Boston is substituted for Denver, given
that some word is substituted.
Finally, we also estimated a probability dis-
tribution Pi(W ) over interregnum strings as fol-
lows. Our training corpus annotates what we
call interregnum expressions, such as uh and
I mean. We estimated a simple unigram distri-
bution over all of the interregnum expressions
observed in our training corpus, and also ex-
tracted the empirical distribution of the num-
ber of interregnum expressions in each repair.
Interregnums are generated as follows. First,
the number k of interregnum expressions is cho-
sen using the empirical distribution. Then k
interregnum expressions are independently gen-
erated from the unigram distribution of inter-
regnum expressions, and appended to yield the
interregnum string W .
The weighted TAG that constitutes the chan-
nel model is straight forward to define us-
ing these conditional probability distributions.
Note that the language model generates the
source string X. Thus the weights of the TAG
rules condition on the words in X, but do not
generate them.
There are three different schema defining the
initial trees of the TAG. These correspond to
analyzing a source word as not beginning a re-
pair (e.g., ?1 and ?3 in Figure 3), analyzing a
source word as beginning a repair (e.g., ?2), and
generating an interregnum (e.g., ?5).
Auxiliary trees generate the paired reparan-
dum/repair words of a repair. There are five dif-
ferent schema defining the auxiliary trees corre-
sponding to the five different values that Ti can
take. Note that the nonterminal Rm,r expanded
by the auxiliary trees is annotated with the last
reparandum and repair words M ?i?1 and R?i?1
respectively, which makes it possible to condi-
tion the rule?s weight on these words.
Auxiliary trees of the form (?1) gener-
ate reparandum words that are copies of
the corresponding repair words; the weight
on such trees is Pr(copy|M ?i?1, R?i?1). Trees
of the form (?2) substitute a reparan-
dum word for a repair word; their weight
is Pr(subst|M ?i?1, R?i?1)Pr(Mi|subst,M ?i?1, R?i).
Trees of the form (?3) end a repair; their weight
is Pr(nonrep|,M ?i?1, R?i?1). Auxiliary trees of
the form (?3) end a repair; they are weighted
Pr(nonrep|M ?i?1, R?i?1). Auxiliary trees of the
form (?4) permit the repair word R?i?1 to be
deleted in the reparandum; the weight of such
a tree is Pr(del|M ?i?1, R?i?1). Finally, auxiliary
trees of the form (?5) generate a reparandum
word Mi is inserted; the weight of such a tree is
Pr(ins|M ?i?1, R?i?1)Pr(Mi|ins,M ?i?1, R?i?1).
3 Detecting and repairing speech
repairs
The TAG just described is not probabilistic;
informally, it does not include the probability
costs for generating the source words. How-
ever, it is easy to modify the TAG so it does
include a bigram model that does generate the
source words, since each nonterminal encodes
the preceding source word. That is, we multi-
ply the weights of each TAG production given
earlier that introduces a source word Ri by
Pn(Ri|Ri?1). The resulting stochastic TAG is
in fact exactly the intersection of the channel
model TAG with a bigram language model.
The standard n5 bottom-up dynamic pro-
gramming parsing algorithm can be used with
this stochastic TAG. Each different parse of
the observed string Y with this grammar corre-
sponds to a way of analyzing Y in terms of a hy-
pothetical underlying sentence X and a number
of different repairs. In our experiments below
we extract the 20 most likely parses for each sen-
tence. Since the weighted grammar just given
does not generate the source string X, the score
of the parse using the weighted TAG is P(Y |X).
This score multiplied by the probability P(X)
of the source string using the syntactic parser
based language model, is our best estimate of
the probability of an analysis.
However, there is one additional complica-
tion that makes a marked improvement to
the model?s performance. Recall that we use
the standard bottom-up dynamic programming
TAG parsing algorithm to search for candidate
parses. This algorithm has n5 running time,
where n is the length of the string. Even though
our sentences are often long, it is extremely un-
likely that any repair will be longer than, say,
12 words. So to increase processing speed we
only compute analyses for strings of length 12
or less. For every such substring that can be an-
alyzed as a repair we calculate the repair odds,
i.e., the probability of generating this substring
as a repair divided by the probability of gener-
ating this substring via the non-repair rules, or
equivalently, the odds that this substring consti-
tutes a repair. The substrings with high repair
odds are likely to be repairs.
This more local approach has a number of
advantages over computing a global analysis.
First, as just noted it is much more efficient
to compute these partial analyses rather than
to compute global analyses of the entire sen-
tence. Second, there are rare cases in which
the same substring functions as both repair and
reparandum (i.e., the repair string is itself re-
paired again). A single global analysis would
not be able to capture this (since the TAG chan-
nel model does not permit the same substring
to be both a reparandum and a repair), but
we combine these overlapping repair substring
analyses in a post-processing operation to yield
an analysis of the whole sentence. (We do in-
sist that the reparandum and interregnum of a
repair do not overlap with those of any other
repairs in the same analysis).
4 Evaluation
This section describes how we evaluate our noisy
model. As mentioned earlier, following Char-
niak and Johnson (2001) our test data consisted
of all Penn III Switchboard tree-bank sw4[0-
1]*.mrg files. However, our test data differs
from theirs in that in this test we deleted all
partial words and punctuation from the data,
as this results in a more realistic test situation.
Since the immediate goal of this work is to
produce a program that identifies the words of a
sentence that belong to the reparandum of a re-
pair construction (to a first approximation these
words can be ignored in later processing), our
evaluation focuses on the model?s performance
in recovering the words in a reparandum. That
is, the model is used to classify each word in the
sentence as belonging to a reparandum or not,
and all other additional structure produced by
the model is ignored.
We measure model performance using stan-
dard precision p, recall r and f-score f , mea-
sures. If nc is the number of reparandum words
the model correctly classified, nt is the number
of true reparandum words given by the manual
annotations and nm is the number of words the
model predicts to be reparandum words, then
the precision is nc/nm, recall is nc/nt, and f is
2pr/(p + r).
For comparison we include the results of run-
ning the word-by-word classifier described in
Charniak and Johnson (2001), but where par-
tial words and punctuation have been removed
from the training and test data. We also pro-
vide results for our noisy channel model using
a bigram language model and a second trigram
model where the twenty most likely analyses are
rescored. Finally we show the results using the
parser language model.
CJ01? Bigram Trigram Parser
Precision 0.951 0.776 0.774 0.820
Recall 0.631 0.736 0.763 0.778
F-score 0.759 0.756 0.768 0.797
The noisy channel model using a bigram lan-
guage model does a slightly worse job at identi-
fying reparandum and interregnum words than
the classifier proposed in Charniak and Johnson
(2001). Replacing the bigram language model
with a trigram model helps slightly, and parser-
based language model results in a significant
performance improvement over all of the oth-
ers.
5 Conclusion and further work
This paper has proposed a novel noisy chan-
nel model of speech repairs and has used it to
identify reparandum words. One of the advan-
tages of probabilistic models is that they can be
integrated with other probabilistic models in a
principled way, and it would be interesting to
investigate how to integrate this kind of model
of speech repairs with probabilistic speech rec-
ognizers.
There are other kinds of joint models of
reparandum and repair that may produce a bet-
ter reparandum detection system. We have
experimented with versions of the models de-
scribed above based on POS bi-tag dependen-
cies rather than word bigram dependencies, but
with results very close to those presented here.
Still, more sophisticated models may yield bet-
ter performance.
It would also be interesting to combine this
probabilistic model of speech repairs with the
word classifier approach of Charniak and John-
son (2001). That approach may do so well be-
cause many speech repairs are very short, in-
volving only one or two words Shriberg and
Stolcke (1998), so the reparandum, interregnum
and repair are all contained in the surround-
ing word window used as features by the classi-
fier. On the other hand, the probabilistic model
of repairs explored here seems to be most suc-
cessful in identifying long repairs in which the
reparandum and repair are similar enough to be
unlikely to have been generated independently.
Since the two approaches seem to have different
strengths, a combined model may outperform
both of them.
References
Eugene Charniak and Mark Johnson. 2001.
Edit detection and parsing for transcribed
speech. In Proceedings of the 2nd Meeting
of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages
118?126. The Association for Computational
Linguistics.
Eugene Charniak. 2001. Immediate-head pars-
ing for language models. In Proceedings of the
39th Annual Meeting of the Association for
Computational Linguistics. The Association
for Computational Linguistics.
Stanley F. Chen and Joshua Goodman. 1998.
An empirical study of smoothing techniques
for language modeling. Technical Report TR-
10-98, Center for Research in Computing
Technology, Harvard University.
Peter A. Heeman and James F. Allen. 1999.
Speech repairs, intonational phrases, and dis-
course markers: Modeling speaker?s utter-
ances in spoken dialogue. Computational
Linguistics, 25(4):527?571.
Stuart M. Shieber and Yves Schabes. 1990.
Synchronous tree-adjoining grammars. In
Proceedings of the 13th International Confer-
ence on Computational Linguistics (COLING
1990), pages 253?258.
Stuart M. Shieber. 1985. Evidence against the
Context-Freeness of natural language. Lin-
guistics and Philosophy, 8(3):333?344.
Elizabeth Shriberg and Andreas Stolcke. 1998.
How far do speakers back up in repairs? a
quantitative model. In Proceedings of the In-
ternational Conference on Spoken Language
Processing, volume 5, pages 2183?2186, Syd-
ney, Australia.
Elizabeth Shriberg. 1994. Preliminaries to a
Theory of Speech Disfluencies. Ph.D. thesis,
University of California, Berkeley.
Proceedings of the 43rd Annual Meeting of the ACL, pages 173?180,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Coarse-to-fine n-best parsing and MaxEnt discriminative reranking
Eugene Charniak and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{mj|ec}@cs.brown.edu
Abstract
Discriminative reranking is one method
for constructing high-performance statis-
tical parsers (Collins, 2000). A discrim-
inative reranker requires a source of can-
didate parses for each sentence. This pa-
per describes a simple yet novel method
for constructing sets of 50-best parses
based on a coarse-to-fine generative parser
(Charniak, 2000). This method gener-
ates 50-best lists that are of substantially
higher quality than previously obtainable.
We used these parses as the input to a
MaxEnt reranker (Johnson et al, 1999;
Riezler et al, 2002) that selects the best
parse from the set of parses for each sen-
tence, obtaining an f-score of 91.0% on
sentences of length 100 or less.
1 Introduction
We describe a reranking parser which uses a reg-
ularized MaxEnt reranker to select the best parse
from the 50-best parses returned by a generative
parsing model. The 50-best parser is a probabilistic
parser that on its own produces high quality parses;
the maximum probability parse trees (according to
the parser?s model) have an f -score of 0.897 on
section 23 of the Penn Treebank (Charniak, 2000),
which is still state-of-the-art. However, the 50 best
(i.e., the 50 highest probability) parses of a sentence
often contain considerably better parses (in terms of
f -score); this paper describes a 50-best parsing al-
gorithm with an oracle f -score of 96.8 on the same
data.
The reranker attempts to select the best parse for
a sentence from the 50-best list of possible parses
for the sentence. Because the reranker only has
to consider a relatively small number of parses per
sentences, it is not necessary to use dynamic pro-
gramming, which permits the features to be essen-
tially arbitrary functions of the parse trees. While
our reranker does not achieve anything like the ora-
cle f -score, the parses it selects do have an f -score
of 91.0, which is considerably better than the maxi-
mum probability parses of the n-best parser.
In more detail, for each string s the n-best parsing
algorithm described in section 2 returns the n high-
est probability parses Y(s) = {y1(s), . . . , yn(s)}
together with the probability p(y) of each parse y ac-
cording to the parser?s probability model. The num-
ber n of parses was set to 50 for the experiments
described here, but some simple sentences actually
received fewer than 50 parses (so n is actually a
function of s). Each yield or terminal string in the
training, development and test data sets is mapped
to such an n-best list of parse/probability pairs; the
cross-validation scheme described in Collins (2000)
was used to avoid training the n-best parser on the
sentence it was being used to parse.
A feature extractor, described in section 3, is a
vector of m functions f = (f1, . . . , fm), where each
fj maps a parse y to a real number fj(y), which
is the value of the jth feature on y. So a feature
extractor maps each y to a vector of feature values
f(y) = (f1(y), . . . , fm(y)).
Our reranking parser associates a parse with a
173
score v?(y), which is a linear function of the feature
values f(y). That is, each feature fj is associated
with a weight ?j , and the feature values and weights
define the score v?(y) of each parse y as follows:
v?(y) = ? ? f(y) =
m
?
j=1
?jfj(y).
Given a string s, the reranking parser?s output y?(s)
on string s is the highest scoring parse in the n-best
parses Y(s) for s, i.e.,
y?(s) = arg max
y?Y(s)
v?(y).
The feature weight vector ? is estimated from the
labelled training corpus as described in section 4.
Because we use labelled training data we know the
correct parse y?(s) for each sentence s in the training
data. The correct parse y?(s) is not always a mem-
ber of the n-best parser?s output Y(s), but we can
identify the parses Y+(s) in Y(s) with the highest
f -scores. Informally, the estimation procedure finds
a weight vector ? that maximizes the score v?(y) of
the parses y ? Y+(s) relative to the scores of the
other parses in Y(s), for each s in the training data.
2 Recovering the n-best parses using
coarse-to-fine parsing
The major difficulty in n-best parsing, compared to
1-best parsing, is dynamic programming. For exam-
ple, n-best parsing is straight-forward in best-first
search or beam search approaches that do not use
dynamic programming: to generate more than one
parse, one simply allows the search mechanism to
create successive versions to one?s heart?s content.
A good example of this is the Roark parser
(Roark, 2001) which works left-to right through the
sentence, and abjures dynamic programming in fa-
vor of a beam search, keeping some large number of
possibilities to extend by adding the next word, and
then re-pruning. At the end one has a beam-width?s
number of best parses (Roark, 2001).
The Collins parser (Collins, 1997) does use dy-
namic programming in its search. That is, whenever
a constituent with the same history is generated a
second time, it is discarded if its probability is lower
than the original version. If the opposite is true, then
the original is discarded. This is fine if one only
wants the first-best, but obviously it does not directly
enumerate the n-best parses.
However, Collins (Collins, 2000; Collins
and Koo, in submission) has created an n-
best version of his parser by turning off dy-
namic programming (see the user?s guide to
Bikel?s re-implementation of Collins? parser,
http://www.cis.upenn.edu/ dbikel/software.html#stat-
parser). As with Roark?s parser, it is necessary to
add a beam-width constraint to make the search
tractable. With a beam width of 1000 the parser
returns something like a 50-best list (Collins,
personal communication), but the actual number of
parses returned for each sentences varies. However,
turning off dynamic programming results in a loss in
efficiency. Indeed, Collins?s n-best list of parses for
section 24 of the Penn tree-bank has some sentences
with only a single parse, because the n-best parser
could not find any parses.
Now there are two known ways to produce n-best
parses while retaining the use of dynamic program-
ming: the obvious way and the clever way.
The clever way is based upon an algorithm devel-
oped by Schwartz and Chow (1990). Recall the key
insight in the Viterbi algorithm: in the optimal parse
the parsing decisions at each of the choice points that
determine a parse must be optimal, since otherwise
one could find a better parse. This insight extends
to n-best parsing as follows. Consider the second-
best parse: if it is to differ from the best parse, then
at least one of its parsing decisions must be subop-
timal. In fact, all but one of the parsing decisions
in second-best parse must be optimal, and the one
suboptimal decision must be the second-best choice
at that choice point. Further, the nth-best parse can
only involve at most n suboptimal parsing decisions,
and all but one of these must be involved in one of
the second through the n?1th-best parses. Thus the
basic idea behind this approach to n-best parsing is
to first find the best parse, then find the second-best
parse, then the third-best, and so on. The algorithm
was originally described for hidden Markov models.
Since this first draft of this paper we have be-
come aware of two PCFG implementations of this
algorithm (Jimenez and Marzal, 2000; Huang and
Chang, 2005). The first was tried on relatively small
grammars, while the second was implemented on
top of the Bikel re-implementation of the Collins
174
parser (Bikel, 2004) and achieved oracle results for
50-best parses similar to those we report below.
Here, however, we describe how to find n-best
parses in a more straight-forward fashion. Rather
than storing a single best parse of each edge, one
stores n of them. That is, when using dynamic pro-
gramming, rather than throwing away a candidate if
it scores less than the best, one keeps it if it is one
of the top n analyses for this edge discovered so far.
This is really very straight-forward. The problem
is space. Dynamic programming parsing algorithms
for PCFGs require O(m2) dynamic programming
states, where m is the length of the sentence, so an
n-best parsing algorithm requires O(nm2). How-
ever things get much worse when the grammar is bi-
lexicalized. As shown by Eisner (Eisner and Satta,
1999) the dynamic programming algorithms for bi-
lexicalized PCFGs require O(m3) states, so a n-best
parser would require O(nm3) states. Things be-
come worse still in a parser like the one described in
Charniak (2000) because it conditions on (and hence
splits the dynamic programming states according to)
features of the grandparent node in addition to the
parent, thus multiplying the number of possible dy-
namic programming states even more. Thus nobody
has implemented this version.
There is, however, one particular feature of the
Charniak parser that mitigates the space problem: it
is a ?coarse-to-fine? parser. By ?coarse-to-fine? we
mean that it first produces a crude version of the
parse using coarse-grained dynamic programming
states, and then builds fine-grained analyses by split-
ting the most promising of coarse-grained states.
A prime example of this idea is from Goodman
(1997), who describes a method for producing a sim-
ple but crude approximate grammar of a standard
context-free grammar. He parses a sentence using
the approximate grammar, and the results are used
to constrain the search for a parse with the full CFG.
He finds that total parsing time is greatly reduced.
A somewhat different take on this paradigm is
seen in the parser we use in this paper. Here the
parser first creates a parse forest based upon a much
less complex version of the complete grammar. In
particular, it only looks at standard CFG features,
the parent and neighbor labels. Because this gram-
mar encodes relatively little state information, its dy-
namic programming states are relatively coarse and
hence there are comparatively few of them, so it can
be efficiently parsed using a standard dynamic pro-
gramming bottom-up CFG parser. However, pre-
cisely because this first stage uses a grammar that
ignores many important contextual features, the best
parse it finds will not, in general, be the best parse
according to the finer-grained second-stage gram-
mar, so clearly we do not want to perform best-first
parsing with this grammar. Instead, the output of
the first stage is a polynomial-sized packed parse
forest which records the left and right string posi-
tions for each local tree in the parses generated by
this grammar. The edges in the packed parse for-
est are then pruned, to focus attention on the coarse-
grained states that are likely to correspond to high-
probability fine-grained states. The edges are then
pruned according to their marginal probability con-
ditioned on the string s being parsed as follows:
p(nij,k | s) =
?(nij,k)?(nij,k)
p(s) (1)
Here nij,k is a constituent of type i spanning the
words from j to k, ?(nij,k) is the outside probability
of this constituent, and ?(nij,k) is its inside proba-
bility. From parse forest both ? and ? can be com-
puted in time proportional to the size of the compact
forest. The parser then removes all constituents nij,k
whose probability falls below some preset threshold.
In the version of this parser available on the web, this
threshold is on the order of 10?4.
The unpruned edges are then exhaustively eval-
uated according to the fine-grained probabilistic
model; in effect, each coarse-grained dynamic pro-
gramming state is split into one or more fine-grained
dynamic programming states. As noted above, the
fine-grained model conditions on information that is
not available in the coarse-grained model. This in-
cludes the lexical head of one?s parents, the part of
speech of this head, the parent?s and grandparent?s
category labels, etc. The fine-grained states inves-
tigated by the parser are constrained to be refine-
ments of the coarse-grained states, which drastically
reduces the number of fine-grained states that need
to be investigated.
It is certainly possible to do dynamic program-
ming parsing directly with the fine-grained gram-
mar, but precisely because the fine-grained grammar
175
conditions on a wide variety of non-local contex-
tual information there would be a very large number
of different dynamic programming states, so direct
dynamic programming parsing with the fine-grained
grammar would be very expensive in terms of time
and memory.
As the second stage parse evaluates all the re-
maining constituents in all of the contexts in which
they appear (e.g., what are the possible grand-parent
labels) it keeps track of the most probable expansion
of the constituent in that context, and at the end is
able to start at the root and piece together the overall
best parse.
Now comes the easy part. To create a 50-best
parser we simply change the fine-grained version of
1-best algorithm in accordance with the ?obvious?
scheme outlined earlier in this section. The first,
coarse-grained, pass is not changed, but the second,
fine-grained, pass keeps the n-best possibilities at
each dynamic programming state, rather than keep-
ing just first best. When combining two constituents
to form a larger constituent, we keep the best 50 of
the 2500 possibilities they offer. Naturally, if we
keep each 50-best list sorted, we do nothing like
2500 operations.
The experimental question is whether, in practice,
the coarse-to-fine architecture keeps the number of
dynamic programming states sufficiently low that
space considerations do not defeat us.
The answer seems to be yes. We ran the algorithm
on section 24 of the Penn WSJ tree-bank using the
default pruning settings mentioned above. Table 1
shows how the number of fine-grained dynamic pro-
gramming states increases as a function of sentence
length for the sentences in section 24 of the Tree-
bank. There are no sentences of length greater than
69 in this section. Columns two to four show the
number of sentences in each bucket, their average
length, and the average number of fine-grained dy-
namic programming structures per sentence. The fi-
nal column gives the value of the function 100?L1.5
where L is the average length of sentences in the
bucket. Except for bucket 6, which is abnormally
low, it seems that this add-hoc function tracks the
number of structures quite well. Thus the number of
dynamic programming states does not grow as L2,
much less as L3.
To put the number of these structures per sen-
Len Num Av sen Av strs 100 ? L1.5
sents length per sent
0?9 225 6.04 1167 1484
10?19 725 15.0 4246 5808
20?29 795 24.2 9357 11974
30?39 465 33.8 15893 19654
40?49 162 43.2 21015 28440
50?59 35 52.8 30670 38366
60?69 9 62.8 23405 49740
Table 1: Number of structures created as a function
of sentence length
n 1 2 10 25 50
f -score 0.897 0.914 0.948 0.960 0.968
Table 2: Oracle f -score as a function of number n
of n-best parses
tence in perspective, consider the size of such struc-
tures. Each one must contain a probability, the non-
terminal label of the structure, and a vector of point-
ers to it?s children (an average parent has slightly
more than two children). If one were concerned
about every byte this could be made quite small. In
our implementation probably the biggest factor is
the STL overhead on vectors. If we figure we are
using, say, 25 bytes per structure, the total space re-
quired is only 1.25Mb even for 50,000 dynamic pro-
gramming states, so it is clearly not worth worrying
about the memory required.
The resulting n-bests are quite good, as shown in
Table 2. (The results are for all sentences of sec-
tion 23 of the WSJ tree-bank of length ? 100.) From
the 1-best result we see that the base accuracy of the
parser is 89.7%.1 2-best and 10-best show dramatic
oracle-rate improvements. After that things start to
slow down, and we achieve an oracle rate of 0.968
at 50-best. To put this in perspective, Roark (Roark,
2001) reports oracle results of 0.941 (with the same
experimental setup) using his parser to return a vari-
able number of parses. For the case cited his parser
returns, on average, 70 parses per sentence.
Finally, we note that 50-best parsing is only a fac-
1Charniak in (Charniak, 2000) cites an accuracy of 89.5%.
Fixing a few very small bugs discovered by users of the parser
accounts for the difference.
176
tor of two or three slower than 1-best.
3 Features for reranking parses
This section describes how each parse y is mapped
to a feature vector f(y) = (f1(y), . . . , fm(y)). Each
feature fj is a function that maps a parse to a real
number. The first feature f1(y) = log p(y) is the
logarithm of the parse probability p according to
the n-best parser model. The other features are
integer valued; informally, each feature is associ-
ated with a particular configuration, and the feature?s
value fj(y) is the number of times that the config-
uration that fj indicates. For example, the feature
feat pizza(y) counts the number of times that a phrase
in y headed by eat has a complement phrase headed
by pizza.
Features belong to feature schema, which are ab-
stract schema from which specific features are in-
stantiated. For example, the feature feat pizza is an
instance of the ?Heads? schema. Feature schema are
often parameterized in various ways. For example,
the ?Heads? schema is parameterized by the type of
heads that the feature schema identifies. Following
Grimshaw (1997), we associate each phrase with a
lexical head and a function head. For example, the
lexical head of an NP is a noun while the functional
head of an NP is a determiner, and the lexical head
of a VP is a main verb while the functional head of
VP is an auxiliary verb.
We experimented with various kinds of feature
selection, and found that a simple count threshold
performs as well as any of the methods we tried.
Specifically, we ignored all features that did not vary
on the parses of at least t sentences, where t is the
count threshold. In the experiments described below
t = 5, though we also experimented with t = 2.
The rest of this section outlines the feature
schemata used in the experiments below. These fea-
ture schemata used here were developed using the
n-best parses provided to us by Michael Collins
approximately a year before the n-best parser de-
scribed here was developed. We used the division
into preliminary training and preliminary develop-
ment data sets described in Collins (2000) while
experimenting with feature schemata; i.e., the first
36,000 sentences of sections 2?20 were used as pre-
liminary training data, and the remaining sentences
of sections 20 and 21 were used as preliminary de-
velopment data. It is worth noting that develop-
ing feature schemata is much more of an art than
a science, as adding or deleting a single schema
usually does not have a significant effect on perfor-
mance, yet the overall impact of many well-chosen
schemata can be dramatic.
Using the 50-best parser output described here,
there are 1,148,697 features that meet the count
threshold of at least 5 on the main training data
(i.e., Penn treebank sections 2?21). We list each
feature schema?s name, followed by the number of
features in that schema with a count of at least 5, to-
gether with a brief description of the instances of the
schema and the schema?s parameters.
CoPar (10) The instances of this schema indicate
conjunct parallelism at various different depths.
For example, conjuncts which have the same
label are parallel at depth 0, conjuncts with the
same label and whose children have the same
label are parallel at depth 1, etc.
CoLenPar (22) The instances of this schema indi-
cate the binned difference in length (in terms
of number of preterminals dominated) in adja-
cent conjuncts in the same coordinated struc-
tures, conjoined with a boolean flag that indi-
cates whether the pair is final in the coordinated
phrase.
RightBranch (2) This schema enables the reranker
to prefer right-branching trees. One instance of
this schema returns the number of nonterminal
nodes that lie on the path from the root node
to the right-most non-punctuation preterminal
node, and the other instance of this schema
counts the number of the other nonterminal
nodes in the parse tree.
Heavy (1049) This schema classifies nodes by their
category, their binned length (i.e., the number
of preterminals they dominate), whether they
are at the end of the sentence and whether they
are followed by punctuation.
Neighbours (38,245) This schema classifies nodes
by their category, their binned length, and the
part of speech categories of the `1 preterminals
to the node?s left and the `2 preterminals to the
177
node?s right. `1 and `2 are parameters of this
schema; here `1 = 1 or `1 = 2 and `2 = 1.
Rule (271,655) The instances of this schema are
local trees, annotated with varying amounts
of contextual information controlled by the
schema?s parameters. This schema was in-
spired by a similar schema in Collins and Koo
(in submission). The parameters to this schema
control whether nodes are annotated with their
preterminal heads, their terminal heads and
their ancestors? categories. An additional pa-
rameter controls whether the feature is special-
ized to embedded or non-embedded clauses,
which roughly corresponds to Emonds? ?non-
root? and ?root? contexts (Emonds, 1976).
NGram (54,567) The instances of this schema are
`-tuples of adjacent children nodes of the same
parent. This schema was inspired by a simi-
lar schema in Collins and Koo (in submission).
This schema has the same parameters as the
Rule schema, plus the length ` of the tuples of
children (` = 2 here).
Heads (208,599) The instances of this schema are
tuples of head-to-head dependencies, as men-
tioned above. The category of the node that
is the least common ancestor of the head and
the dependent is included in the instance (this
provides a crude distinction between different
classes of arguments). The parameters of this
schema are whether the heads involved are lex-
ical or functional heads, the number of heads
in an instance, and whether the lexical item or
just the head?s part of speech are included in the
instance.
LexFunHeads (2,299) The instances of this feature
are the pairs of parts of speech of the lexical
head and the functional head of nodes in parse
trees.
WProj (158,771) The instances of this schema are
preterminals together with the categories of ` of
their closest maximal projection ancestors. The
parameters of this schema control the number `
of maximal projections, and whether the preter-
minals and the ancestors are lexicalized.
Word (49,097) The instances of this schema are
lexical items together with the categories of `
of their immediate ancestor nodes, where ` is
a schema parameter (` = 2 or ` = 3 here).
This feature was inspired by a similar feature
in Klein and Manning (2003).
HeadTree (72,171) The instances of this schema
are tree fragments consisting of the local trees
consisting of the projections of a preterminal
node and the siblings of such projections. This
schema is parameterized by the head type (lex-
ical or functional) used to determine the pro-
jections of a preterminal, and whether the head
preterminal is lexicalized.
NGramTree (291,909) The instances of this
schema are subtrees rooted in the least com-
mon ancestor of ` contiguous preterminal
nodes. This schema is parameterized by the
number ` of contiguous preterminals (` = 2 or
` = 3 here) and whether these preterminals are
lexicalized.
4 Estimating feature weights
This section explains how we estimate the feature
weights ? = (?1, . . . , ?m) for the feature functions
f = (f1, . . . , fm). We use a MaxEnt estimator to
find the feature weights ??, where L is the loss func-
tion and R is a regularization penalty term:
?? = argmin
?
LD(?) + R(?).
The training data D = (s1, . . . , sn?) is a se-
quence of sentences and their correct parses
y?(s1), . . . , y?(sn). We used the 20-fold cross-
validation technique described in Collins (2000)
to compute the n-best parses Y(s) for each sen-
tence s in D. In general the correct parse y?(s)
is not a member of Y(s), so instead we train the
reranker to identify one of the best parses Y+(s) =
argmaxy?Y(s) Fy?(s)(y) in the n-best parser?s out-
put, where Fy?(y) is the Parseval f -score of y eval-
uated with respect to y?.
Because there may not be a unique best parse for
each sentence (i.e., |Y+(s)| > 1 for some sentences
s) we used the variant of MaxEnt described in Rie-
zler et al (2002) for partially labelled training data.
178
Recall the standard MaxEnt conditional probability
model for a parse y ? Y:
P?(y|Y) =
exp v?(y)
?
y??Y exp v?(y?)
,where
v?(y) = ? ? f(y) =
m
?
j=1
?jfj(y).
The loss function LD proposed in Riezler et al
(2002) is just the negative log conditional likelihood
of the best parses Y+(s) relative to the n-best parser
output Y(s):
LD(?) = ?
n?
?
i=1
log P?(Y+(si)|Y(si)),where
P?(Y+|Y) =
?
y?Y+
P?(y|Y)
The partial derivatives of this loss function, which
are required by the numerical estimation procedure,
are:
?LD
?j
=
n?
?
i=1
E?[fj |Y(si)] ? E?[fj |Y+(si)]
E?[f |Y] =
?
y?Y
f(y)P?(y|Y)
In the experiments reported here, we used a Gaus-
sian or quadratic regularizer R(w) = c?mj=1 w2j ,
where c is an adjustable parameter that controls
the amount of regularization, chosen to optimize
the reranker?s f -score on the development set (sec-
tion 24 of the treebank).
We used the Limited Memory Variable Metric op-
timization algorithm from the PETSc/TAO optimiza-
tion toolkit (Benson et al, 2004) to find the optimal
feature weights ?? because this method seems sub-
stantially faster than comparable methods (Malouf,
2002). The PETSc/TAO toolkit provides a variety of
other optimization algorithms and flags for control-
ling convergence, but preliminary experiments on
the Collins? trees with different algorithms and early
stopping did not show any performance improve-
ments, so we used the default PETSc/TAO setting for
our experiments here.
5 Experimental results
We evaluated the performance of our reranking
parser using the standard PARSEVAL metrics. We
n-best trees f -score
New 0.9102
Collins 0.9037
Table 3: Results on new n-best trees and Collins n-
best trees, with weights estimated from sections 2?
21 and the regularizer constant c adjusted for op-
timal f -score on section 24 and evaluated on sen-
tences of length less than 100 in section 23.
trained the n-best parser on sections 2?21 of the
Penn Treebank, and used section 24 as development
data to tune the mixing parameters of the smooth-
ing model. Similarly, we trained the feature weights
? with the MaxEnt reranker on sections 2?21, and
adjusted the regularizer constant c to maximize the
f -score on section 24 of the treebank. We did this
both on the trees supplied to us by Michael Collins,
and on the output of the n-best parser described in
this paper. The results are presented in Table 3. The
n-best parser?s most probable parses are already of
state-of-the-art quality, but the reranker further im-
proves the f -score.
6 Conclusion
This paper has described a dynamic programming
n-best parsing algorithm that utilizes a heuristic
coarse-to-fine refinement of parses. Because the
coarse-to-fine approach prunes the set of possible
parse edges beforehand, a simple approach which
enumerates the n-best analyses of each parse edge
is not only practical but quite efficient.
We use the 50-best parses produced by this algo-
rithm as input to a MaxEnt discriminative reranker.
The reranker selects the best parse from this set of
parses using a wide variety of features. The sys-
tem we described here has an f -score of 0.91 when
trained and tested using the standard PARSEVAL
framework.
This result is only slightly higher than the highest
reported result for this test-set, Bod?s (.907) (Bod,
2003). More to the point, however, is that the sys-
tem we describe is reasonably efficient so it can
be used for the kind of routine parsing currently
being handled by the Charniak or Collins parsers.
A 91.0 f-score represents a 13% reduction in f-
179
measure error over the best of these parsers.2 Both
the 50-best parser, and the reranking parser can be
found at ftp://ftp.cs.brown.edu/pub/nlparser/, named
parser and reranker respectively.
Acknowledgements We would like to thanks
Michael Collins for the use of his data and many
helpful comments, and Liang Huang for providing
an early draft of his paper and very useful comments
on our paper. Finally thanks to the National Science
Foundation for its support (NSF IIS-0112432, NSF
9721276, and NSF DMS-0074276).
References
Steve Benson, Lois Curfman McInnes, Jorge J. Mor, and
Jason Sarich. 2004. Tao users manual. Technical Re-
port ANL/MCS-TM-242-Revision 1.6, Argonne Na-
tional Laboratory.
Daniel M. Bikel. 2004. Intricacies of collins parsing
model. Computational Linguistics, 30(4).
Rens Bod. 2003. An efficient implementation of an new
dop model. In Proceedings of the European Chapter
of the Association for Computational Linguists.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In The Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics, pages 132?139.
Michael Collins and Terry Koo. in submission. Discrim-
inative reranking for natural language parsing. Tech-
nical report, Computer Science and Artificial Intelli-
gence Laboratory, Massachusetts Institute of Technol-
ogy.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In The Proceedings of
the 35th Annual Meeting of the Association for Com-
putational Linguistics, San Francisco. Morgan Kauf-
mann.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Machine Learning: Pro-
ceedings of the Seventeenth International Conference
(ICML 2000), pages 175?182, Stanford, California.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In Proceedings of the 37th Annual
2This probably underestimates the actual improvement.
There are no currently accepted figures for inter-annotater
agreement on Penn WSJ, but it is no doubt well short of 100%.
If we take 97% as a reasonable estimate of the the upper bound
on tree-bank accuracy, we are instead talking about an 18% er-
ror reduction.
Meeting of the Association for Computational Linguis-
tics, pages 457?464.
Joseph Emonds. 1976. A Transformational Approach to
English Syntax: Root, Structure-Preserving and Local
Transformations. Academic Press, New York, NY.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 1997).
Jane Grimshaw. 1997. Projection, heads, and optimality.
Linguistic Inquiry, 28(3):373?422.
Liang Huang and David Chang. 2005. Better k-best pars-
ing. Technical Report MS-CIS-05-08, Department of
Computer Science, University of Pennsylvania.
Victor M. Jimenez and Andres Marzal. 2000. Computa-
tion of the n best parse trees for weighted and stochas-
tic context-free grammars. In Proceedings of the Joint
IAPR International Workshops on Advances in Pattern
Recognition. Springer LNCS 1876.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochas-
tic ?unification-based? grammars. In The Proceedings
of the 37th Annual Conference of the Association for
Computational Linguistics, pages 535?541, San Fran-
cisco. Morgan Kaufmann.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proceed-
ings of the Sixth Conference on Natural Language
Learning (CoNLL-2002), pages 49?55.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. III Maxwell, and Mark John-
son. 2002. Parsing the wall street journal using a
lexical-functional grammar and discriminative estima-
tion techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, pages 271?278. Morgan Kaufmann.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
R. Schwartz and Y.L. Chow. 1990. The n-best algo-
rithm: An efficient and exact procedure for finding
the n most likely sentence hypotheses. In Proceed-
ings of the IEEE International Conference on Acous-
tic, Speech, and Signal, Processing, pages 81?84.
180
Proceedings of the 43rd Annual Meeting of the ACL, pages 290?297,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Supervised and Unsupervised Learning for Sentence Compression
Jenine Turner and Eugene Charniak
Department of Computer Science
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{jenine|ec}@cs.brown.edu
Abstract
In Statistics-Based Summarization - Step
One: Sentence Compression, Knight and
Marcu (Knight and Marcu, 2000) (K&M)
present a noisy-channel model for sen-
tence compression. The main difficulty
in using this method is the lack of data;
Knight and Marcu use a corpus of 1035
training sentences. More data is not easily
available, so in addition to improving the
original K&M noisy-channel model, we
create unsupervised and semi-supervised
models of the task. Finally, we point out
problems with modeling the task in this
way. They suggest areas for future re-
search.
1 Introduction
Summarization in general, and sentence compres-
sion in particular, are popular topics. Knight and
Marcu (henceforth K&M) introduce the task of
statistical sentence compression in Statistics-Based
Summarization - Step One: Sentence Compression
(Knight and Marcu, 2000). The appeal of this prob-
lem is that it produces summarizations on a small
scale. It simplifies general compression problems,
such as text-to-abstract conversion, by eliminating
the need for coherency between sentences. The
model is further simplified by being constrained
to word deletion: no rearranging of words takes
place. Others have performed the sentence compres-
sion task using syntactic approaches to this problem
(Mani et al, 1999) (Zajic et al, 2004), but we fo-
cus exclusively on the K&M formulation. Though
the problem is simpler, it is still pertinent to cur-
rent needs; generation of captions for television and
audio scanning services for the blind (Grefenstette,
1998), as well as compressing chosen sentences for
headline generation (Angheluta et al, 2004) are ex-
amples of uses for sentence compression. In addi-
tion to simplifying the task, K&M?s noisy-channel
formulation is also appealing.
In the following sections, we discuss the K&M
noisy-channel model. We then present our cleaned
up, and slightly improved noisy-channel model. We
also develop unsupervised and semi-supervised (our
term for a combination of supervised and unsuper-
vised) methods of sentence compression with inspi-
ration from the K&M model, and create additional
constraints to improve the compressions. We con-
clude with the problems inherent in both models.
2 The Noisy-Channel Model
2.1 The K&M Model
The K&M probabilistic model, adapted from ma-
chine translation to this task, is the noisy-channel
model. In machine translation, one imagines that a
string was originally in English, but that someone
adds some noise to make it a foreign string. Analo-
gously, in the sentence compression model, the short
string is the original sentence and someone adds
noise, resulting in the longer sentence. Using this
framework, the end goal is, given a long sentence
l, to determine the short sentence s that maximizes
290
P (s | l). By Bayes Rule,
P (s | l) = P (l | s)P (s)P (l) (1)
The probability of the long sentence, P (l) can be ig-
nored when finding the maximum, because the long
sentence is the same in every case.
P (s) is the source model: the probability that s
is the original sentence. P (l | s) is the channel
model: the probability the long sentence is the ex-
panded version of the short. This framework in-
dependently models the grammaticality of s (with
P (s)) and whether s is a good compression of l
(P (l | s)).
The K&M model uses parse trees for the sen-
tences. These allow it to better determine the proba-
bility of the short sentence and to obtain alignments
from the training data. In the K&M model, the
sentence probability is determined by combining a
probabilistic context free grammar (PCFG) with a
word-bigram score. The joint rules used to create the
compressions are generated by aligning the nodes of
the short and long trees in the training data to deter-
mine expansion probabilities (P (l | s)).
Recall that the channel model tries to find the
probability of the long string with respect to the
short string. It obtains these probabilities by align-
ing nodes in the parsed parallel training corpus, and
counting the nodes that align as ?joint events.? For
example, there might be S ? NP VP PP in the long
sentence and S ? NP VP in the short sentence; we
count this as one joint event. Non-compressions,
where the long version is the same as the short, are
also counted. The expansion probability, as used in
the channel model, is given by
Pexpand(l | s) =
count(joint(l, s))
count(s) (2)
where count(joint(l, s)) is the count of alignments
of the long rule and the short. Many compressions
do not align exactly. Sometimes the parses do not
match, and sometimes there are deletions that are too
complex to be modeled in this way. In these cases
sentence pairs, or sections of them, are ignored.
The K&M model creates a packed parse forest of
all possible compressions that are grammatical with
respect to the Penn Treebank (Marcus et al, 1993).
Any compression given a zero expansion probability
according to the training data is instead assigned a
very small probability. A tree extractor (Langkilde,
2000) collects the short sentences with the highest
score for P (s | l).
2.2 Our Noisy-Channel Model
Our starting implementation is intended to follow
the K&M model fairly closely. We use the same
1067 pairs of sentences from the Ziff-Davis cor-
pus, with 32 used as testing and the rest as train-
ing. The main difference between their model and
ours is that instead of using the rather ad-hoc K&M
language model, we substitute the syntax-based lan-
guage model described in (Charniak, 2001).
We slightly modify the channel model equation to
be P (l | s) = Pexpand(l | s)Pdeleted, where Pdeleted
is the probability of adding the deleted subtrees back
into s to get l. We determine this probability also
using the Charniak language model.
We require an extra parameter to encourage com-
pression. We create a development corpus of 25 sen-
tences from the training data in order to adjust this
parameter. That we require a parameter to encourage
compression is odd as K&M required a parameter to
discourage compression, but we address this point in
the penultimate section.
Another difference is that we only generate short
versions for which we have rules. If we have never
before seen the long version, we leave it alone, and
in the rare case when we never see the long version
as an expansion of itself, we allow only the short
version. We do not use a packed tree structure, be-
cause we make far fewer sentences. Additionally,
as we are traversing the list of rules to compress the
sentences, we keep the list capped at the 100 com-
pressions with the highest Pexpand(l | s). We even-
tually truncate the list to the best 25, still based upon
Pexpand(l | s).
2.3 Special Rules
One difficulty in the use of training data is that so
many compressions cannot be modeled by our sim-
ple method. The rules it does model, immediate
constituent deletion, as in taking out the ADVP , of
S ? ADVP , NP VP ., are certainly common, but
many good deletions are more structurally compli-
cated. One particular type of rule, such as NP(1) ?
291
NP(2) CC NP(3), where the parent has at least one
child with the same label as itself, and the resulting
compression is one of the matching children, such
as, here, NP(2). There are several hundred rules of
this type, and it is very simple to incorporate into our
model.
There are other structures that may be common
enough to merit adding, but we limit this experiment
to the original rules and our new ?special rules.?
3 Unsupervised Compression
One of the biggest problems with this model of sen-
tence compression is the lack of appropriate train-
ing data. Typically, abstracts do not seem to con-
tain short sentences matching long ones elsewhere
in a paper, and we would prefer a much larger cor-
pus. Despite this lack of training data, very good
results were obtained both by the K&M model and
by our variant. We create a way to compress sen-
tences without parallel training data, while sticking
as closely to the K&M model as possible.
The source model stays the same, and we still
pay a probability cost in the channel model for ev-
ery subtree deleted. However, the way we determine
Pexpand(l | s) changes because we no longer have a
parallel text. We create joint rules using only the first
section (0.mrg) of the Penn Treebank. We count all
probabilistic context free grammar (PCFG) expan-
sions, and then match up similar rules as unsuper-
vised joint events.
We change Equation 2 to calculate Pexpand(s | l)
without parallel data. First, let us define svo (shorter
version of) to be: r1 svo r2 iff the righthand side of
r1 is a subsequence of the righthand side of r2. Then
define
Pexpand(l | s) =
count(l)
?
l?s.t. s svo l? count(l?)
(3)
This is best illustrated by a toy example. Consider
a corpus with just 7 rules: 3 instances of NP ? DT
JJ NN and 4 instances of NP ? DT NN.
P(NP ? DT JJ NN | NP ? DT JJ NN) = 1. To
determine this, you divide the count of NP ? DT JJ
NN = 3 by all the possible long versions of NP ?
DT JJ NN = 3.
P(NP ? DT JJ NN | NP ? DT NN) = 3/7. The
count of NP ? DT JJ NN = 3, and the possible long
versions of NP ? DT NN are itself (with count of 3)
and NP ? DT JJ NN (with count of 4), yielding a
sum of 7.
Finally, P(NP ? DT NN | NP ? DT NN) = 4/7.
The count of NP ? DT NN = 4, and since the short
(NP ? DT NN) is the same as above, the count of
the possible long versions is again 7.
In this way, we approximate Pexpand(l | s) with-
out parallel data.
Since some of these ?training? pairs are likely
to be fairly poor compressions, due to the artifi-
ciality of the construction, we restrict generation of
short sentences to not allow deletion of the head
of any subtree. None of the special rules are ap-
plied. Other than the above changes, the unsuper-
vised model matches our supervised version. As will
be shown, this rule is not constraining enough and
allows some poor compressions, but it is remarkable
that any sort of compression can be achieved with-
out training data. Later, we will describe additional
constraints that help even more.
4 Semi-Supervised Compression
Because the supervised version tends to do quite
well, and its main problem is that the model tends
to pick longer compressions than a human would,
it seems reasonable to incorporate the unsupervised
version into our supervised model, in the hope of
getting more rules to use. In generating new short
sentences, if we have compression probabilities in
the supervised version, we use those, including the
special rules. The only time we use an unsupervised
compression probability is when there is no super-
vised version of the unsupervised rule.
5 Additional Constraints
Even with the unsupervised constraint from section
3, the fact that we have artificially created our joint
rules gives us some fairly ungrammatical compres-
sions. Adding extra constraints improves our unsu-
pervised compressions, and gives us better perfor-
mance on the supervised version as well. We use a
program to label syntactic arguments with the roles
they are playing (Blaheta and Charniak, 2000), and
the rules for complement/adjunct distinction given
by (Collins, 1997) to never allow deletion of the
complement. Since many nodes that should not
292
be deleted are not labeled with their syntactic role,
we add another constraint that disallows deletion of
NPs.
6 Evaluation
As with Knight and Marcu?s (2000) original work,
we use the same 32 sentence pairs as our Test Cor-
pus, leaving us with 1035 training pairs. After ad-
justing the supervised weighting parameter, we fold
the development set back into the training data.
We presented four judges with nine compressed
versions of each of the 32 long sentences: A human-
generated short version, the K&M version, our first
supervised version, our supervised version with our
special rules, our supervised version with special
rules and additional constraints, our unsupervised
version, our supervised version with additional con-
straints, our semi-supervised version, and our semi-
supervised version with additional constraints. The
judges were asked to rate the sentences in two ways:
the grammaticality of the short sentences on a scale
from 1 to 5, and the importance of the short sen-
tence, or how well the compressed version retained
the important words from the original, also on a
scale from 1 to 5. The short sentences were ran-
domly shuffled across test cases.
The results in Table 1 show compression rates,
as well as average grammar and importance scores
across judges.
There are two main ideas to take away from these
results. First, we can get good compressions without
paired training data. Second, we achieved a good
boost by adding our additional constraints in two of
the three versions.
Note that importance is a somewhat arbitrary dis-
tinction, since according to our judges, all of the
computer-generated versions do as well in impor-
tance as the human-generated versions.
6.1 Examples of Results
In Figure 1, we give four examples of most compres-
sion techniques in order to show the range of perfor-
mance that each technique spans. In the first two ex-
amples, we give only the versions with constraints,
because there is little or no difference between the
versions with and without constraints.
Example 1 shows the additional compression ob-
tained by using our special rules. Figure 2 shows
the parse trees of the original pair of short and long
versions. The relevant expansion is NP ? NP1 ,
PP in the long version and simply NP1 in the short
version. The supervised version that includes the
special rules learned this particular common special
joint rule from the training data and could apply it
to the example case. This supervised version com-
presses better than either version of the supervised
noisy-channel model that lacks these rules. The un-
supervised version does not compress at all, whereas
the semi-supervised version is identical with the bet-
ter supervised version.
Example 2 shows how unsupervised and semi-
supervised techniques can be used to improve com-
pression. Although the final length of the sentences
is roughly the same, the unsupervised and semi-
supervised versions are able to take the action of
deleting the parenthetical. Deleting parentheses was
never seen in the training data, so it would be ex-
tremely unlikely to occur in this case. The unsuper-
vised version, on the other hand, sees both PRN ?
lrb NP rrb and PRN ? NP in its training data, and
the semi-supervised version capitalizes on this par-
ticular unsupervised rule.
Example 3 shows an instance of our initial super-
vised versions performing far worse than the K&M
model. The reason is that currently our supervised
model only generates compressions that it has seen
before, unlike the K&M model, which generates all
possible compressions. S ? S , NP VP . never occurs
in the training data, and so a good compression does
not exist. The unsupervised and semi-supervised
versions do better in this case, and the supervised
version with the added constraints does even better.
Example 4 gives an example of the K&M model
being outperformed by all of our other models.
7 Problems with Noisy Channel Models of
Sentence Compression
To this point our presentation has been rather nor-
mal; we draw inspiration from a previous paper, and
work at improving on it in various ways. We now
deviate from the usual by claiming that while the
K&M model works very well, there is a technical
problem with formulating the task in this way.
We start by making our noisy channel notation a
293
original: Many debugging features, including user-defined break points and
variable-watching and message-watching windows, have been added.
human: Many debugging features have been added.
K&M: Many debugging features, including user-defined points and
variable-watching and message-watching windows, have been added.
supervised: Many features, including user-defined break points and variable-watching
and windows, have been added.
super (+ extra rules, constraints): Many debugging features have been added.
unsuper (+ constraints): Many debugging features, including user-defined break
points and variable-watching and message-watching windows, have been added.
semi-supervised (+ constraints): Many debugging features have been added.
original: Also, Trackstar supports only the critical path method (CPM) of project
scheduling.
human: Trackstar supports the critical path method of project scheduling.
K&M: Trackstar supports only the critical path method (CPM) of scheduling.
supervised: Trackstar supports only the critical path method (CPM) of scheduling.
super (+ extra rules, constraints): Trackstar supports only the critical path method (CPM) of scheduling.
unsuper (+ constraints): Trackstar supports only the critical path method of project scheduling.
semi-supervised (+ constraints): Trackstar supports only the critical path method of project scheduling.
original: The faster transfer rate is made possible by an MTI-proprietary data
buffering algorithm that off-loads lock-manager functions from the Q-bus
host, Raimondi said.
human: The algorithm off-loads lock-manager functions from the Q-bus host.
K&M: The faster rate is made possible by a MTI-proprietary data buffering algorithm
that off-loads lock-manager functions from the Q-bus host, Raimondi said.
supervised: Raimondi said.
super (+ extra rules): Raimondi said.
super (+ extra rules, constraints): The faster transfer rate is made possible by an MTI-proprietary data buffering
algorithm, Raimondi said.
unsuper (+ constraints): The faster transfer rate is made possible, Raimondi said.
semi-supervised (+ constraints): The faster transfer rate is made possible, Raimondi said.
original: The SAS screen is divided into three sections: one for writing programs, one for
the system?s response as it executes the program, and a third for output tables
and charts.
human: The SAS screen is divided into three sections.
K&M: The screen is divided into one
super (+ extra rules): SAS screen is divided into three sections: one for writing programs, and a third
for output tables and charts.
super (+ extra rules, constraints): The SAS screen is divided into three sections.
unsupervised: The screen is divided into sections: one for writing programs, one for the system?s
response as it executes program, and third for output tables and charts.
unsupervised (+ constraints): Screen is divided into three sections: one for writing programs, one for the
system?s response as it executes program, and a third for output tables and charts.
semi-supervised: The SAS screen is divided into three sections: one for writing programs, one for
the system?s response as it executes the program, and a third for output tables
and charts.
semi-super (+ constraints): The screen is divided into three sections: one for writing programs, one for the
system?s response as it executes the program, and a third for output tables
and charts.
Figure 1: Compression Examples294
compression rate grammar importance
humans 53.33% 4.96 3.73
K&M 70.37% 4.57 3.85
supervised 79.85% 4.64 3.97
supervised with extra rules 67.41% 4.57 3.66
supervised with extra rules and constraints 68.44% 4.77 3.76
unsupervised 79.11% 4.38 3.93
unsupervised with constraints 77.93% 4.51 3.88
semi-supervised 81.19% 4.79 4.18
semi-supervised with constraints 79.56% 4.75 4.16
Table 1: Experimental Results
short: (S (NP (JJ Many) (JJ debugging) (NNS features))
(VP (VBP have) (VP (VBN been) (VP (VBN added))))(. .))
long: (S (NP (NP (JJ Many) (JJ debugging) (NNS features))(, ,)
(PP (VBG including) (NP (NP (JJ user-defined)(NN break)(NNS points)
(CC and)(NN variable-watching))
(CC and)(NP (JJ message-watching) (NNS windows))))(, ,))
(VP (VBP have) (VP (VBN been) (VP (VBN added))))(. .))
Figure 2: Joint Trees for special rules
bit more explicit:
argmaxsp(s, L = s | l, L = l) = (4)
argmaxsp(s, L = s)p(l, L = l | s, L = s)
Here we have introduced explicit conditioning
events L = l and L = s to state that that the sen-
tence in question is either the long version or the
short version. We do this because in order to get the
equation that K&M (and ourselves) start with, it is
necessary to assume the following
p(s, L = s) = p(s) (5)
p(l, L = l | s, L = s) = p(l | s) (6)
This means we assume that the probability of, say, s
as a short (compressed) sentence is simply its prob-
ability as a sentence. This will be, in general, false.
One would hope that real compressed sentences are
more probable as a member of the set of compressed
sentences than they are as simply a member of all
English sentences. However, neither K&M, nor we,
have a large enough body of compressed and origi-
nal sentences from which to create useful language
models, so we both make this simplifying assump-
tion. At this point it seems like a reasonable choice
root
vp
vb
buy
np
nns
toys
root
vp
vb
buy
np
jj
large
nns
toys
Figure 3: A compression example ? trees A and B
respectively
to make. In fact, it compromises the entire enter-
prise. To see this, however, we must descend into
more details.
Let us consider a simplified version of a K&M
example, but as reinterpreted for our model: how
the noisy channel model assigns a probability of the
compressed tree (A) in Figure 3 given the original
tree B.
We compute the probabilities p(A) and p(B | A)
as follows (Figure 4): We have divided the probabil-
ities up according to whether they are contributed by
the source or channel models. Those from the source
295
p(A) p(B | A)
p(s ? vp | H(s)) p(s ? vp | s ? vp)
p(vp ? vb np | H(vp)) p(vp ? vb np | vp ? vb np)
p(np ? nns | H(np)) p(np ? jj nns | np ? nns)
p(vb ? buy | H(vb)) p(vb ? buy | vb ? buy)
p(nns ? toys | H(nns)) p(nns ? toys | nns ? toys)
p(jj ? large | H(jj))
Figure 4: Source and channel probabilities for com-
pressing B into A
p(B) p(B | B)
p(s ? vp | H(s)) p(s ? vp | s ? vp)
p(vp ? vb np | H(vp)) p(vp ? vb np | vp ? vb np)
p(np ? jj nns | H(np)) p(np ? jj nns | np ? jj nns)
p(vb ? buy | H(vb)) p(vb ? buy | vb ? buy)
p(nns ? toys | H(nns)) p(nns ? toys | nns ? toys)
p(jj ? large | H(jj)) p(jj ? large | jj ? large)
Figure 5: Source and channel probabilities for leav-
ing B as B
model are conditioned on, e.g. H(np) the history in
terms of the tree structure around the noun-phrase.
In a pure PCFG this would only include the label of
the node. In our language model it includes much
more, such as parent and grandparent heads.
Again, following K&M, contrast this with the
probabilities assigned when the compressed tree is
identical to the original (Figure 5).
Expressed like this it is somewhat daunting, but
notice that if all we want is to see which probability
is higher (the compressed being the same as the orig-
inal or truly compressed) then most of these terms
cancel, and we get the rule, prefer the truly com-
pressed if and only if the following ratio is greater
than one.
p(np ? nns | H(np))
p(np ? jj nns | H(np))
p(np ? jj nns | np ? nns)
p(np ? jj nns | np ? jj nns) (7)
1
p(jj ? large | jj ? large)
In the numerator are the unmatched probabilities
that go into the compressed sentence noisy chan-
nel probability, and in the denominator are those for
when the sentence does not undergo any change. We
can make this even simpler by noting that because
tree-bank pre-terminals can only expand into words
p(jj ? large | jj ? large) = 1. Thus the last fraction
in Equation 7 is equal to one and can be ignored.
For a compression to occur, it needs to be less de-
sirable to add an adjective in the channel model than
in the source model. In fact, the opposite occurs.
The likelihood of almost any constituent deletion is
far lower than the probability of the constituents all
being left in. This seems surprising, considering that
the model we are using has had some success, but
it makes intuitive sense. There are far fewer com-
pression alignments than total alignments: identical
parts of sentences are almost sure to align. So the
most probable short sentence should be very barely
compressed. Thus we add a weighting factor to
compress our supervised version further.
K&M also, in effect, weight shorter sentences
more strongly than longer ones based upon their lan-
guage model. In their papers on sentence compres-
sion, they give an example similar to our ?buy large
toys? example. The equation they get for the channel
probabilities in their example is similar to the chan-
nel probabilities we give in Figures 3 and 4. How-
ever their source probabilities are different. K&M
did not have a true syntax-based language model
to use as we have. Thus they divided the language
model into two parts. Part one assigns probabilities
to the grammar rules using a probabilistic context-
free grammar, while part two assigns probabilities
to the words using a bi-gram model. As they ac-
knowledge in (Knight and Marcu, 2002), the word
bigram probabilities are also included in the PCFG
probabilities. So in their versions of Figures 3 and
4 they have both p(toys | nns) (from the PCFG)
and p(toys | buy) for the bigram probability. In
this model, the probabilities do not sum to one, be-
cause they pay the probabilistic price for guessing
the word ?toys? twice, based upon two different con-
ditioning events. Based upon this language model,
they prefer shorter sentences.
To reiterate this section?s argument: A noisy
channel model is not by itself an appropriate model
for sentence compression. In fact, the most likely
short sentence will, in general, be the same length
as the long sentence. We achieve compression by
weighting to give shorter sentences more likelihood.
In fact, what is really required is some model that
takes ?utility? into account, using a utility model
296
in which shorter sentences are more useful. Our
term giving preference to shorter sentences can be
thought of as a crude approximation to such a utility.
However, this is clearly an area for future research.
8 Conclusion
We have created a supervised version of the noisy-
channel model with some improvements over the
K&M model. In particular, we learned that adding
an additional rule type improved compression, and
that enforcing some deletion constraints improves
grammaticality. We also show that it is possible to
perform an unsupervised version of the compression
task, which performs remarkably well. Our semi-
supervised version, which we hoped would have
good compression rates and grammaticality, had
good grammaticality but lower compression than de-
sired.
We would like to come up with a better utility
function than a simple weighting parameter for our
supervised version. The unsupervised version prob-
ably can also be further improved. We achieved
much success using syntactic labels to constrain
compressions, and there are surely other constraints
that can be added.
However, more training data is always the easi-
est cure to statistical problems. If we can find much
larger quantities of training data we could allow for
much richer rule paradigms that relate compressed
to original sentences. One example of a rule we
would like to automatically discover would allow us
to compress all of our design goals or
(NP (NP (DT all))
(PP (IN of)
(NP (PRP$ our) (NN design) (NNS goals))))}
to all design goals or
(NP (DT all) (NN design) (NNS goals))
In the limit such rules blur the distinction between
compression and paraphrase.
9 Acknowledgements
This work was supported by NSF grant IIS-
0112435. We would like to thank Kevin Knight
and Daniel Marcu for their clarification and test sen-
tences, and Mark Johnson for his comments.
References
Roxana Angheluta, Rudradeb Mitra, Xiuli Jing, and
Francine-Marie Moens. 2004. K.U.Leuven summa-
rization system at DUC 2004. In Document Under-
standing Conference.
Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In The Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics, pages 234?240.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguis-
tics. The Association for Computational Linguistics.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In The Proceedings of
the 35th Annual Meeting of the Association for Com-
putational Linguistics, San Francisco. Morgan Kauf-
mann.
Gregory Grefenstette. 1998. Producing intelligent tele-
graphic text reduction to provide an audio scanning
service for the blind. In Working Notes of the AAAI
Spring Symposium on Intelligent Text Summarization,
pages 111?118.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization - step one: sentence compression. In
Proceedings of the 17th National Conference on Arti-
ficial Intelligence, pages 703?71.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. In Artificial Intelli-
gence, 139(1): 91-107.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proceedings of the 1st Annual Meeting
of the North American Chapter of the Association for
Computationl Linguistics.
Inderjeet Mani, Barbara Gates, and Eric Bloedorn. 1999.
Improving summaries by revising them. In The Pro-
ceedings of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics. The Association
for Computational Linguistics.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
David Zajic, Bonnie Dorr, and Richard Schwartz. 2004.
BBN/UMD at DUC 2004: Topiary. In Document Un-
derstanding Conference.
297
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 337?344,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Reranking and Self-Training for Parser Adaptation
David McClosky, Eugene Charniak, and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{dmcc|ec|mj}@cs.brown.edu
Abstract
Statistical parsers trained and tested on the
Penn Wall Street Journal (WSJ) treebank
have shown vast improvements over the
last 10 years. Much of this improvement,
however, is based upon an ever-increasing
number of features to be trained on (typi-
cally) the WSJ treebank data. This has led
to concern that such parsers may be too
finely tuned to this corpus at the expense
of portability to other genres. Such wor-
ries have merit. The standard ?Charniak
parser? checks in at a labeled precision-
recall f -measure of 89.7% on the Penn
WSJ test set, but only 82.9% on the test set
from the Brown treebank corpus.
This paper should allay these fears. In par-
ticular, we show that the reranking parser
described in Charniak and Johnson (2005)
improves performance of the parser on
Brown to 85.2%. Furthermore, use of the
self-training techniques described in (Mc-
Closky et al, 2006) raise this to 87.8%
(an error reduction of 28%) again with-
out any use of labeled Brown data. This
is remarkable since training the parser and
reranker on labeled Brown data achieves
only 88.4%.
1 Introduction
Modern statistical parsers require treebanks to
train their parameters, but their performance de-
clines when one parses genres more distant from
the training data?s domain. Furthermore, the tree-
banks required to train said parsers are expensive
and difficult to produce.
Naturally, one of the goals of statistical parsing
is to produce a broad-coverage parser which is rel-
atively insensitive to textual domain. But the lack
of corpora has led to a situation where much of
the current work on parsing is performed on a sin-
gle domain using training data from that domain
? the Wall Street Journal (WSJ) section of the
Penn Treebank (Marcus et al, 1993). Given the
aforementioned costs, it is unlikely that many sig-
nificant treebanks will be created for new genres.
Thus, parser adaptation attempts to leverage ex-
isting labeled data from one domain and create a
parser capable of parsing a different domain.
Unfortunately, the state of the art in parser
portability (i.e. using a parser trained on one do-
main to parse a different domain) is not good. The
?Charniak parser? has a labeled precision-recall
f -measure of 89.7% on WSJ but a lowly 82.9%
on the test set from the Brown corpus treebank.
Furthermore, the treebanked Brown data is mostly
general non-fiction and much closer to WSJ than,
e.g., medical corpora would be. Thus, most work
on parser adaptation resorts to using some labeled
in-domain data to fortify the larger quantity of out-
of-domain data.
In this paper, we present some encouraging re-
sults on parser adaptation without any in-domain
data. (Though we also present results with in-
domain data as a reference point.) In particular we
note the effects of two comparatively recent tech-
niques for parser improvement.
The first of these, parse-reranking (Collins,
2000; Charniak and Johnson, 2005) starts with a
?standard? generative parser, but uses it to gener-
ate the n-best parses rather than a single parse.
Then a reranking phase uses more detailed fea-
tures, features which would (mostly) be impossi-
ble to incorporate in the initial phase, to reorder
337
the list and pick a possibly different best parse.
At first blush one might think that gathering even
more fine-grained features from a WSJ treebank
would not help adaptation. However, we find that
reranking improves the parsers performance from
82.9% to 85.2%.
The second technique is self-training ? pars-
ing unlabeled data and adding it to the training
corpus. Recent work, (McClosky et al, 2006),
has shown that adding many millions of words
of machine parsed and reranked LA Times arti-
cles does, in fact, improve performance of the
parser on the closely related WSJ data. Here we
show that it also helps the father-afield Brown
data. Adding it improves performance yet-again,
this time from 85.2% to 87.8%, for a net error re-
duction of 28%. It is interesting to compare this to
our results for a completely Brown trained system
(i.e. one in which the first-phase parser is trained
on just Brown training data, and the second-phase
reranker is trained on Brown 50-best lists). This
system performs at a 88.4% level ? only slightly
higher than that achieved by our system with only
WSJ data.
2 Related Work
Work in parser adaptation is premised on the as-
sumption that one wants a single parser that can
handle a wide variety of domains. While this is the
goal of the majority of parsing researchers, it is not
quite universal. Sekine (1997) observes that for
parsing a specific domain, data from that domain
is most beneficial, followed by data from the same
class, data from a different class, and data from
a different domain. He also notes that different
domains have very different structures by looking
at frequent grammar productions. For these rea-
sons he takes the position that we should, instead,
simply create treebanks for a large number of do-
mains. While this is a coherent position, it is far
from the majority view.
There are many different approaches to parser
adaptation. Steedman et al (2003) apply co-
training to parser adaptation and find that co-
training can work across domains. The need to
parse biomedical literature inspires (Clegg and
Shepherd, 2005; Lease and Charniak, 2005).
Clegg and Shepherd (2005) provide an extensive
side-by-side performance analysis of several mod-
ern statistical parsers when faced with such data.
They find that techniques which combine differ-
Training Testing f -measureGildea Bacchiani
WSJ WSJ 86.4 87.0
WSJ Brown 80.6 81.1
Brown Brown 84.0 84.7
WSJ+Brown Brown 84.3 85.6
Table 1: Gildea and Bacchiani results on WSJ and
Brown test corpora using different WSJ and Brown
training sets. Gildea evaluates on sentences of
length ? 40, Bacchiani on all sentences.
ent parsers such as voting schemes and parse se-
lection can improve performance on biomedical
data. Lease and Charniak (2005) use the Charniak
parser for biomedical data and find that the use of
out-of-domain trees and in-domain vocabulary in-
formation can considerably improve performance.
However, the work which is most directly com-
parable to ours is that of (Ratnaparkhi, 1999; Hwa,
1999; Gildea, 2001; Bacchiani et al, 2006). All
of these papers look at what happens to mod-
ern WSJ-trained statistical parsers (Ratnaparkhi?s,
Collins?, Gildea?s and Roark?s, respectively) as
training data varies in size or usefulness (because
we are testing on something other than WSJ). We
concentrate particularly on the work of (Gildea,
2001; Bacchiani et al, 2006) as they provide re-
sults which are directly comparable to those pre-
sented in this paper.
Looking at Table 1, the first line shows us
the standard training and testing on WSJ ? both
parsers perform in the 86-87% range. The next
line shows what happens when parsing Brown us-
ing a WSJ-trained parser. As with the Charniak
parser, both parsers take an approximately 6% hit.
It is at this point that our work deviates from
these two papers. Lacking alternatives, both
(Gildea, 2001) and (Bacchiani et al, 2006) give
up on adapting a pure WSJ trained system, instead
looking at the issue of how much of an improve-
ment one gets over a pure Brown system by adding
WSJ data (as seen in the last two lines of Table 1).
Both systems use a ?model-merging? (Bacchiani
et al, 2006) approach. The different corpora are,
in effect, concatenated together. However, (Bac-
chiani et al, 2006) achieve a larger gain by weight-
ing the in-domain (Brown) data more heavily than
the out-of-domain WSJ data. One can imagine, for
instance, five copies of the Brown data concate-
nated with just one copy of WSJ data.
338
3 Corpora
We primarily use three corpora in this paper. Self-
training requires labeled and unlabeled data. We
assume that these sets of data must be in similar
domains (e.g. news articles) though the effective-
ness of self-training across domains is currently an
open question. Thus, we have labeled (WSJ) and
unlabeled (NANC) out-of-domain data and labeled
in-domain data (BROWN). Unfortunately, lacking
a corresponding corpus to NANC for BROWN, we
cannot perform the opposite scenario and adapt
BROWN to WSJ.
3.1 Brown
The BROWN corpus (Francis and Kuc?era, 1979)
consists of many different genres of text, intended
to approximate a ?balanced? corpus. While the
full corpus consists of fiction and nonfiction do-
mains, the sections that have been annotated in
Treebank II bracketing are primarily those con-
taining fiction. Examples of the sections annotated
include science fiction, humor, romance, mystery,
adventure, and ?popular lore.? We use the same
divisions as Bacchiani et al (2006), who base
their divisions on Gildea (2001). Each division of
the corpus consists of sentences from all available
genres. The training division consists of approx-
imately 80% of the data, while held-out develop-
ment and testing divisions each make up 10% of
the data. The treebanked sections contain approx-
imately 25,000 sentences (458,000 words).
3.2 Wall Street Journal
Our out-of-domain data is the Wall Street Journal
(WSJ) portion of the Penn Treebank (Marcus et al,
1993) which consists of about 40,000 sentences
(one million words) annotated with syntactic in-
formation. We use the standard divisions: Sec-
tions 2 through 21 are used for training, section 24
for held-out development, and section 23 for final
testing.
3.3 North American News Corpus
In addition to labeled news data, we make use
of a large quantity of unlabeled news data. The
unlabeled data is the North American News Cor-
pus, NANC (Graff, 1995), which is approximately
24 million unlabeled sentences from various news
sources. NANC contains no syntactic information
and sentence boundaries are induced by a simple
discriminative model. We also perform some basic
cleanups on NANC to ease parsing. NANC contains
news articles from various news sources including
the Wall Street Journal, though for this paper, we
only use articles from the LA Times portion.
To use the data from NANC, we use self-training
(McClosky et al, 2006). First, we take a WSJ
trained reranking parser (i.e. both the parser and
reranker are built from WSJ training data) and
parse the sentences from NANC with the 50-best
(Charniak and Johnson, 2005) parser. Next, the
50-best parses are reordered by the reranker. Fi-
nally, the 1-best parses after reranking are com-
bined with the WSJ training set to retrain the first-
stage parser.1 McClosky et al (2006) find that the
self-trained models help considerably when pars-
ing WSJ.
4 Experiments
We use the Charniak and Johnson (2005) rerank-
ing parser in our experiments. Unless mentioned
otherwise, we use the WSJ-trained reranker (as op-
posed to a BROWN-trained reranker). To evaluate,
we report bracketing f -scores.2 Parser f -scores
reported are for sentences up to 100 words long,
while reranking parser f -scores are over all sen-
tences. For simplicity and ease of comparison,
most of our evaluations are performed on the de-
velopment section of BROWN.
4.1 Adapting self-training
Our first experiment examines the performance
of the self-trained parsers. While the parsers are
created entirely from labeled WSJ data and unla-
beled NANC data, they perform extremely well on
BROWN development (Table 2). The trends are the
same as in (McClosky et al, 2006): Adding NANC
data improves parsing performance on BROWN
development considerably, improving the f -score
from 83.9% to 86.4%. As more NANC data is
added, the f -score appears to approach an asymp-
tote. The NANC data appears to help reduce data
sparsity and fill in some of the gaps in the WSJ
model. Additionally, the reranker provides fur-
ther benefit and adds an absolute 1-2% to the f -
score. The improvements appear to be orthogonal,
as our best performance is reached when we use
the reranker and add 2,500k self-trained sentences
from NANC.
1We trained a new reranker from this data as well, but it
does not seem to get significantly different performance.
2The harmonic mean of labeled precision (P) and labeled
recall (R), i.e. f = 2?P?RP+R
339
Sentences added Parser Reranking Parser
Baseline BROWN 86.4 87.4
Baseline WSJ 83.9 85.8
WSJ+50k 84.8 86.6
WSJ+250k 85.7 87.2
WSJ+500k 86.0 87.3
WSJ+750k 86.1 87.5
WSJ+1,000k 86.2 87.3
WSJ+1,500k 86.2 87.6
WSJ+2,000k 86.1 87.7
WSJ+2,500k 86.4 87.7
Table 2: Effects of adding NANC sentences to WSJ
training data on parsing performance. f -scores
for the parser with and without the WSJ reranker
are shown when evaluating on BROWN develop-
ment. For this experiment, we use the WSJ-trained
reranker.
The results are even more surprising when we
compare against a parser3 trained on the labeled
training section of the BROWN corpus, with pa-
rameters tuned against its held-out section. De-
spite seeing no in-domain data, the WSJ based
parser is able to match the BROWN based parser.
For the remainder of this paper, we will refer
to the model trained on WSJ+2,500k sentences of
NANC as our ?best WSJ+NANC? model. We also
note that this ?best? parser is different from the
?best? parser for parsing WSJ, which was trained
on WSJ with a relative weight4 of 5 and 1,750k
sentences from NANC. For parsing BROWN, the
difference between these two parsers is not large,
though.
Increasing the relative weight of WSJ sentences
versus NANC sentences when testing on BROWN
development does not appear to have a significant
effect. While (McClosky et al, 2006) showed that
this technique was effective when testing on WSJ,
the true distribution was closer to WSJ so it made
sense to emphasize it.
4.2 Incorporating In-Domain Data
Up to this point, we have only considered the sit-
uation where we have no in-domain data. We now
3In this case, only the parser is trained on BROWN. In sec-
tion 4.3, we compare against a fully BROWN-trained rerank-
ing parser as well.
4A relative weight of n is equivalent to using n copies of
the corpus, i.e. an event that occurred x times in the corpus
would occur x?n times in the weighted corpus. Thus, larger
corpora will tend to dominate smaller corpora of the same
relative weight in terms of event counts.
explore different ways of making use of labeled
and unlabeled in-domain data.
Bacchiani et al (2006) applies self-training to
parser adaptation to utilize unlabeled in-domain
data. The authors find that it helps quite a bit when
adapting from BROWN to WSJ. They use a parser
trained from the BROWN train set to parse WSJ and
add the parsed WSJ sentences to their training set.
We perform a similar experiment, using our WSJ-
trained reranking parser to parse BROWN train and
testing on BROWN development. We achieved a
boost from 84.8% to 85.6% when we added the
parsed BROWN sentences to our training. Adding
in 1,000k sentences from NANC as well, we saw a
further increase to 86.3%. However, the technique
does not seem as effective in our case. While the
self-trained BROWN data helps the parser, it ad-
versely affects the performance of the reranking
parser. When self-trained BROWN data is added to
WSJ training, the reranking parser?s performance
drops from 86.6% to 86.1%. We see a similar
degradation as NANC data is added to the train-
ing set as well. We are not yet able to explain this
unusual behavior.
We now turn to the scenario where we have
some labeled in-domain data. The most obvious
way to incorporate labeled in-domain data is to
combine it with the labeled out-of-domain data.
We have already seen the results (Gildea, 2001)
and (Bacchiani et al, 2006) achieve in Table 1.
We explore various combinations of BROWN,
WSJ, and NANC corpora. Because we are
mainly interested in exploring techniques with
self-trained models rather than optimizing perfor-
mance, we only consider weighting each corpus
with a relative weight of one for this paper. The
models generated are tuned on section 24 from
WSJ. The results are summarized in Table 3.
While both WSJ and BROWN models bene-
fit from a small amount of NANC data, adding
more than 250k NANC sentences to the BROWN
or combined models causes their performance to
drop. This is not surprising, though, since adding
?too much? NANC overwhelms the more accurate
BROWN or WSJ counts. By weighting the counts
from each corpus appropriately, this problem can
be avoided.
Another way to incorporate labeled data is to
tune the parser back-off parameters on it. Bac-
chiani et al (2006) report that tuning on held-out
BROWN data gives a large improvement over tun-
340
ing on WSJ data. The improvement is mostly (but
not entirely) in precision. We do not see the same
improvement (Figure 1) but this is likely due to
differences in the parsers. However, we do see
a similar improvement for parsing accuracy once
NANC data has been added. The reranking parser
generally sees an improvement, but it does not ap-
pear to be significant.
4.3 Reranker Portability
We have shown that the WSJ-trained reranker is
actually quite portable to the BROWN fiction do-
main. This is surprising given the large number
of features (over a million in the case of the WSJ
reranker) tuned to adjust for errors made in the 50-
best lists by the first-stage parser. It would seem
the corrections memorized by the reranker are not
as domain-specific as we might expect.
As further evidence, we present the results of
applying the WSJ model to the Switchboard cor-
pus ? a domain much less similar to WSJ than
BROWN. In Table 4, we see that while the parser?s
performance is low, self-training and reranking
provide orthogonal benefits. The improvements
represent a 12% error reduction with no additional
in-domain data. Naturally, in-domain data and
speech-specific handling (e.g. disfluency model-
ing) would probably help dramatically as well.
Finally, to compare against a model fully
trained on BROWN data, we created a BROWN
reranker. We parsed the BROWN training set with
20-fold cross-validation, selected features that oc-
curred 5 times or more in the training set, and
fed the 50-best lists from the parser to a numeri-
cal optimizer to estimate feature weights. The re-
sulting reranker model had approximately 700,000
features, which is about half as many as the WSJ
trained reranker. This may be due to the smaller
size of the BROWN training set or because the
feature schemas for the reranker were developed
on WSJ data. As seen in Table 5, the BROWN
reranker is not a significant improvement over the
WSJ reranker for parsing BROWN data.
5 Analysis
We perform several types of analysis to measure
some of the differences and similarities between
the BROWN-trained and WSJ-trained reranking
parsers. While the two parsers agree on a large
number of parse brackets (Section 5.2), there are
categorical differences between them (as seen in
Parser model Parser f -score Reranker f -score
WSJ 74.0 75.9
WSJ+NANC 75.6 77.0
Table 4: Parser and reranking parser performance
on the SWITCHBOARD development corpus. In
this case, WSJ+NANC is a model created from WSJ
and 1,750k sentences from NANC.
Model 1-best 10-best 25-best 50-best
WSJ 82.6 88.9 90.7 91.9
WSJ+NANC 86.4 92.1 93.5 94.3
BROWN 86.3 92.0 93.3 94.2
Table 6: Oracle f -scores of top n parses pro-
duced by baseline WSJ parser, a combined WSJ and
NANC parser, and a baseline BROWN parser.
Section 5.3).
5.1 Oracle Scores
Table 6 shows the f -scores of an ?oracle reranker?
? i.e. one which would always choose the parse
with the highest f -score in the n-best list. While
the WSJ parser has relatively low f -scores, adding
NANC data results in a parser with comparable ora-
cle scores as the parser trained from BROWN train-
ing. Thus, the WSJ+NANC model has better oracle
rates than the WSJ model (McClosky et al, 2006)
for both the WSJ and BROWN domains.
5.2 Parser Agreement
In this section, we compare the output of the
WSJ+NANC-trained and BROWN-trained rerank-
ing parsers. We use evalb to calculate how sim-
ilar the two sets of output are on a bracket level.
Table 7 shows various statistics. The two parsers
achieved an 88.0% f -score between them. Ad-
ditionally, the two parsers agreed on all brackets
almost half the time. The part of speech tagging
agreement is fairly high as well. Considering they
were created from different corpora, this seems
like a high level of agreement.
5.3 Statistical Analysis
We conducted randomization tests for the signifi-
cance of the difference in corpus f -score, based on
the randomization version of the paired sample t-
test described by Cohen (1995). The null hypoth-
esis is that the two parsers being compared are in
fact behaving identically, so permuting or swap-
ping the parse trees produced by the parsers for
341
WSJ tuned parser
BROWN tuned parser
WSJ tuned reranking parser
BROWN tuned reranking parser
NANC sentences added
f-
sc
o
re
2000k1750k1500k1250k1000k750k500k250k0k
87.8
87.0
86.0
85.0
83.8
Figure 1: Precision and recall f -scores when testing on BROWN development as a function of the number
of NANC sentences added under four test conditions. ?BROWN tuned? indicates that BROWN training data
was used to tune the parameters (since the normal held-out section was being used for testing). For ?WSJ
tuned,? we tuned the parameters from section 24 of WSJ. Tuning on BROWN helps the parser, but not for
the reranking parser.
Parser model Parser alone Reranking parser
WSJ alone 83.9 85.8
WSJ+2,500k NANC 86.4 87.7
BROWN alone 86.3 87.4
BROWN+50k NANC 86.8 88.0
BROWN+250k NANC 86.8 88.1
BROWN+500k NANC 86.7 87.8
WSJ+BROWN 86.5 88.1
WSJ+BROWN+50k NANC 86.8 88.1
WSJ+BROWN+250k NANC 86.8 88.1
WSJ+BROWN+500k NANC 86.6 87.7
Table 3: f -scores from various combinations of WSJ, NANC, and BROWN corpora on BROWN develop-
ment. The reranking parser used the WSJ-trained reranker model. The BROWN parsing model is naturally
better than the WSJ model for this task, but combining the two training corpora results in a better model
(as in Gildea (2001)). Adding small amounts of NANC further improves the models.
Parser model Parser alone WSJ-reranker BROWN-reranker
WSJ 82.9 85.2 85.2
WSJ+NANC 87.1 87.8 87.9
BROWN 86.7 88.2 88.4
Table 5: Performance of various combinations of parser and reranker models when evaluated on BROWN
test. The WSJ+NANC parser with the WSJ reranker comes close to the BROWN-trained reranking parser.
The BROWN reranker provides only a small improvement over its WSJ counterpart, which is not statisti-
cally significant.
342
Bracketing agreement f -score 88.03%
Complete match 44.92%
Average crossing brackets 0.94
POS Tagging agreement 94.85%
Table 7: Agreement between the WSJ+NANC
parser with the WSJ reranker and the BROWN
parser with the BROWN reranker. Complete match
is how often the two reranking parsers returned the
exact same parse.
the same test sentence should not affect the cor-
pus f -scores. By estimating the proportion of per-
mutations that result in an absolute difference in
corpus f -scores at least as great as that observed
in the actual output, we obtain a distribution-
free estimate of significance that is robust against
parser and evaluator failures. The results of this
test are shown in Table 8. The table shows that
the BROWN reranker is not significantly different
from the WSJ reranker.
In order to better understand the difference be-
tween the reranking parser trained on Brown and
the WSJ+NANC/WSJ reranking parser (a reranking
parser with the first-stage trained on WSJ+NANC
and the second-stage trained on WSJ) on Brown
data, we constructed a logistic regression model
of the difference between the two parsers? f -
scores on the development data using the R sta-
tistical package5. Of the 2,078 sentences in the
development data, 29 sentences were discarded
because evalb failed to evaluate at least one of
the parses.6 A Wilcoxon signed rank test on the
remaining 2,049 paired sentence level f -scores
was significant at p = 0.0003. Of these 2,049
sentences, there were 983 parse pairs with the
same sentence-level f -score. Of the 1,066 sen-
tences for which the parsers produced parses with
different f -scores, there were 580 sentences for
which the BROWN/BROWN parser produced a
parse with a higher sentence-level f -score and 486
sentences for which the WSJ+NANC/WSJ parser
produce a parse with a higher f -score. We
constructed a generalized linear model with a
binomial link with BROWN/BROWN f -score >
WSJ+NANC/WSJ f -score as the predicted variable,
and sentence length, the number of prepositions
(IN), the number of conjunctions (CC) and Brown
5http://www.r-project.org
6This occurs when an apostrophe is analyzed as a posses-
sive marker in the gold tree and a punctuation symbol in the
parse tree, or vice versa.
Feature Estimate z-value Pr(> |z|)
(Intercept) 0.054 0.3 0.77
IN -0.134 -4.4 8.4e-06 ***
ID=G 0.584 2.5 0.011 *
ID=K 0.697 2.9 0.003 **
ID=L 0.552 2.3 0.021 *
ID=M 0.376 0.9 0.33
ID=N 0.642 2.7 0.0055 **
ID=P 0.624 2.7 0.0069 **
ID=R 0.040 0.1 0.90
Table 9: The logistic model of BROWN/BROWN
f -score > WSJ+NANC/WSJ f -score identified by
model selection. The feature IN is the num-
ber prepositions in the sentence, while ID identi-
fies the Brown subcorpus that the sentence comes
from. Stars indicate significance level.
subcorpus ID as explanatory variables. Model
selection (using the ?step? procedure) discarded
all but the IN and Brown ID explanatory vari-
ables. The final estimated model is shown in Ta-
ble 9. It shows that the WSJ+NANC/WSJ parser
becomes more likely to have a higher f -score
than the BROWN/BROWN parser as the number
of prepositions in the sentence increases, and that
the BROWN/BROWN parser is more likely to have
a higher f -score on Brown sections K, N, P, G
and L (these are the general fiction, adventure and
western fiction, romance and love story, letters and
memories, and mystery sections of the Brown cor-
pus, respectively). The three sections of BROWN
not in this list are F, M, and R (popular lore, sci-
ence fiction, and humor).
6 Conclusions and Future Work
We have demonstrated that rerankers and self-
trained models can work well across domains.
Models self-trained on WSJ appear to be better
parsing models in general, the benefits of which
are not limited to the WSJ domain. The WSJ-
trained reranker using out-of-domain LA Times
parses (produced by the WSJ-trained reranker)
achieves a labeled precision-recall f -measure of
87.8% on Brown data, nearly equal to the per-
formance one achieves by using a purely Brown
trained parser-reranker. The 87.8% f -score on
Brown represents a 24% error reduction on the
corpus.
Of course, as corpora differences go, Brown is
relatively close to WSJ. While we also find that our
343
WSJ+NANC/WSJ BROWN/WSJ BROWN/BROWN
WSJ/WSJ 0.025 (0) 0.030 (0) 0.031 (0)
WSJ+NANC/WSJ 0.004 (0.1) 0.006 (0.025)
BROWN/WSJ 0.002 (0.27)
Table 8: The difference in corpus f -score between the various reranking parsers, and the significance of
the difference in parentheses as estimated by a randomization test with 106 samples. ?x/y? indicates that
the first-stage parser was trained on data set x and the second-stage reranker was trained on data set y.
?best? WSJ-parser-reranker improves performance
on the Switchboard corpus, it starts from a much
lower base (74.0%), and achieves a much less sig-
nificant improvement (3% absolute, 11% error re-
duction). Bridging these larger gaps is still for the
future.
One intriguing idea is what we call ?self-trained
bridging-corpora.? We have not yet experimented
with medical text but we expect that the ?best?
WSJ+NANC parser will not perform very well.
However, suppose one does self-training on a bi-
ology textbook instead of the LA Times. One
might hope that such a text will split the differ-
ence between more ?normal? newspaper articles
and the specialized medical text. Thus, a self-
trained parser based upon such text might do much
better than our standard ?best.? This is, of course,
highly speculative.
Acknowledgments
This work was supported by NSF grants LIS9720368, and
IIS0095940, and DARPA GALE contract HR0011-06-2-
0001. We would like to thank the BLLIP team for their com-
ments.
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. of the 2005 Meeting of the
Assoc. for Computational Linguistics (ACL), pages
173?180.
Andrew B. Clegg and Adrian Shepherd. 2005. Evalu-
ating and integrating treebank parsers on a biomedi-
cal corpus. In Proceedings of the ACL Workshop on
Software.
Paul R. Cohen. 1995. Empirical Methods for Artifi-
cial Intelligence. The MIT Press, Cambridge, Mas-
sachusetts.
Michael Collins. 2000. Discriminative reranking
for natural language parsing. In Machine Learn-
ing: Proceedings of the Seventeenth International
Conference (ICML 2000), pages 175?182, Stanford,
California.
W. Nelson Francis and Henry Kuc?era. 1979. Manual
of Information to accompany a Standard Corpus of
Present-day Edited American English, for use with
Digital Computers. Brown University, Providence,
Rhode Island.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 167?202.
David Graff. 1995. North American News Text Cor-
pus. Linguistic Data Consortium. LDC95T21.
Rebecca Hwa. 1999. Supervised grammar induction
using training data with limited constituent infor-
mation. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 72?80, University of Maryland.
Matthew Lease and Eugene Charniak. 2005. Parsing
biomedical literature. In Second International Joint
Conference on Natural Language Processing (IJC-
NLP?05).
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Comp. Lin-
guistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of HLT-NAACL 2006.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1-3):151?175.
Satoshi Sekine. 1997. The domain dependence of
parsing. In Proc. Applied Natural Language Pro-
cessing (ANLP), pages 96?102.
Mark Steedman, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proc. of European ACL (EACL), pages
331?338.
344
Proceedings of ACL-08: HLT, pages 834?842,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
You talking to me? A Corpus and Algorithm for Conversation
Disentanglement
Micha Elsner and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{melsner,ec}@@cs.brown.edu
Abstract
When multiple conversations occur simultane-
ously, a listener must decide which conversa-
tion each utterance is part of in order to inter-
pret and respond to it appropriately. We refer
to this task as disentanglement. We present a
corpus of Internet Relay Chat (IRC) dialogue
in which the various conversations have been
manually disentangled, and evaluate annota-
tor reliability. This is, to our knowledge, the
first such corpus for internet chat. We pro-
pose a graph-theoretic model for disentangle-
ment, using discourse-based features which
have not been previously applied to this task.
The model?s predicted disentanglements are
highly correlated with manual annotations.
1 Motivation
Simultaneous conversations seem to arise naturally
in both informal social interactions and multi-party
typed chat. Aoki et al (2006)?s study of voice con-
versations among 8-10 people found an average of
1.76 conversations (floors) active at a time, and a
maximum of four. In our chat corpus, the average is
even higher, at 2.75. The typical conversation, there-
fore, is one which is interrupted? frequently.
Disentanglement is the clustering task of dividing
a transcript into a set of distinct conversations. It is
an essential prerequisite for any kind of higher-level
dialogue analysis: for instance, consider the multi-
party exchange in figure 1.
Contextually, it is clear that this corresponds to
two conversations, and Felicia?s1 response ?excel-
1Real user nicknames are replaced with randomly selected
(Chanel) Felicia: google works :)
(Gale) Arlie: you guys have never worked
in a factory before have you
(Gale) Arlie: there?s some real unethical
stuff that goes on
(Regine) hands Chanel a trophy
(Arlie) Gale, of course ... thats how they
make money
(Gale) and people lose limbs or get killed
(Felicia) excellent
Figure 1: Some (abridged) conversation from our corpus.
lent? is intended for Chanel and Regine. A straight-
forward reading of the transcript, however, might in-
terpret it as a response to Gale?s statement immedi-
ately preceding.
Humans are adept at disentanglement, even in
complicated environments like crowded cocktail
parties or chat rooms; in order to perform this task,
they must maintain a complex mental representation
of the ongoing discourse. Moreover, they adapt their
utterances to some degree to make the task easier
(O?Neill and Martin, 2003), which suggests that dis-
entanglement is in some sense a ?difficult? discourse
task.
Disentanglement has two practical applications.
One is the analysis of pre-recorded transcripts in
order to extract some kind of information, such as
question-answer pairs or summaries. These tasks
should probably take as as input each separate con-
versation, rather than the entire transcript. Another
identifiers for ethical reasons.
834
application is as part of a user-interface system for
active participants in the chat, in which users target a
conversation of interest which is then highlighted for
them. Aoki et al (2003) created such a system for
speech, which users generally preferred to a conven-
tional system? when the disentanglement worked!
Previous attempts to solve the problem (Aoki et
al., 2006; Aoki et al, 2003; Camtepe et al, 2005;
Acar et al, 2005) have several flaws. They clus-
ter speakers, not utterances, and so fail when speak-
ers move from one conversation to another. Their
features are mostly time gaps between one utterance
and another, without effective use of utterance con-
tent. Moreover, there is no framework for a prin-
cipled comparison of results: there are no reliable
annotation schemes, no standard corpora, and no
agreed-upon metrics.
We attempt to remedy these problems. We present
a new corpus of manually annotated chat room data
and evaluate annotator reliability. We give a set of
metrics describing structural similarity both locally
and globally. We propose a model which uses dis-
course structure and utterance contents in addition
to time gaps. It partitions a chat transcript into dis-
tinct conversations, and its output is highly corre-
lated with human annotations.
2 Related Work
Two threads of research are direct attempts to solve
the disentanglement problem: Aoki et al (2006),
Aoki et al (2003) for speech and Camtepe et al
(2005), Acar et al (2005) for chat. We discuss
their approaches below. However, we should em-
phasize that we cannot compare our results directly
with theirs, because none of these studies publish re-
sults on human-annotated data. Although Aoki et al
(2006) construct an annotated speech corpus, they
give no results for model performance, only user sat-
isfaction with their conversational system. Camtepe
et al (2005) and Acar et al (2005) do give perfor-
mance results, but only on synthetic data.
All of the previous approaches treat the problem
as one of clustering speakers, rather than utterances.
That is, they assume that during the window over
which the system operates, a particular speaker is
engaging in only one conversation. Camtepe et al
(2005) assume this is true throughout the entire tran-
script; real speakers, by contrast, often participate
in many conversations, sequentially or sometimes
even simultaneously. Aoki et al (2003) analyze each
thirty-second segment of the transcript separately.
This makes the single-conversation restriction some-
what less severe, but has the disadvantage of ignor-
ing all events which occur outside the segment.
Acar et al (2005) attempt to deal with this prob-
lem by using a fuzzy algorithm to cluster speakers;
this assigns each speaker a distribution over conver-
sations rather than a hard assignment. However, the
algorithm still deals with speakers rather than utter-
ances, and cannot determine which conversation any
particular utterance is part of.
Another problem with these approaches is the in-
formation used for clustering. Aoki et al (2003) and
Camtepe et al (2005) detect the arrival times of mes-
sages, and use them to construct an affinity graph be-
tween participants by detecting turn-taking behavior
among pairs of speakers. (Turn-taking is typified by
short pauses between utterances; speakers aim nei-
ther to interrupt nor leave long gaps.) Aoki et al
(2006) find that turn-taking on its own is inadequate.
They motivate a richer feature set, which, however,
does not yet appear to be implemented. Acar et
al. (2005) adds word repetition to their feature set.
However, their approach deals with all word repe-
titions on an equal basis, and so degrades quickly
in the presence of noise words (their term for words
which shared across conversations) to almost com-
plete failure when only 1/2 of the words are shared.
To motivate our own approach, we examine some
linguistic studies of discourse, especially analysis of
multi-party conversation. O?Neill and Martin (2003)
point out several ways in which multi-party text chat
differs from typical two-party conversation. One key
difference is the frequency with which participants
mention each others? names. They hypothesize that
mentioning is a strategy which participants use to
make disentanglement easier, compensating for the
lack of cues normally present in face-to-face dia-
logue. Mentions (such as Gale?s comments to Ar-
lie in figure 1) are very common in our corpus, oc-
curring in 36% of comments, and provide a useful
feature.
Another key difference is that participants may
create a new conversation (floor) at any time, a pro-
cess which Sacks et al (1974) calls schisming. Dur-
835
ing a schism, a new conversation is formed, not
necessarily because of a shift in the topic, but be-
cause certain participants have refocused their atten-
tion onto each other, and away from whoever held
the floor in the parent conversation.
Despite these differences, there are still strong
similarities between chat and other conversations
such as meetings. Our feature set incorporates infor-
mation which has proven useful in meeting segmen-
tation (Galley et al, 2003) and the task of detect-
ing addressees of a specific utterance in a meeting
(Jovanovic et al, 2006). These include word rep-
etitions, utterance topic, and cue words which can
indicate the bounds of a segment.
3 Dataset
Our dataset is recorded from the IRC (Internet Re-
lay Chat) channel ##LINUX at freenode.net, using
the freely-available gaim client. ##LINUX is an un-
official tech support line for the Linux operating sys-
tem, selected because it is one of the most active chat
rooms on freenode, leading to many simultaneous
conversations, and because its content is typically
inoffensive. Although it is notionally intended only
for tech support, it includes large amounts of social
chat as well, such as the conversation about factory
work in the example above (figure 1).
The entire dataset contains 52:18 hours of chat,
but we devote most of our attention to three anno-
tated sections: development (706 utterances; 2:06
hr) and test (800 utts.; 1:39 hr) plus a short pilot sec-
tion on which we tested our annotation system (359
utts.; 0:58 hr).
3.1 Annotation
Our annotators were seven university students with
at least some familiarity with the Linux OS, al-
though in some cases very slight. Annotation of the
test dataset typically took them about two hours. In
all, we produced six annotations of the test set2.
Our annotation scheme marks each utterance as
part of a single conversation. Annotators are in-
structed to create as many, or as few conversations as
they need to describe the data. Our instructions state
that a conversation can be between any number of
2One additional annotation was discarded because the anno-
tator misunderstood the task.
people, and that, ?We mean conversation in the typ-
ical sense: a discussion in which the participants are
all reacting and paying attention to one another. . . it
should be clear that the comments inside a conver-
sation fit together.? The annotation system itself is a
simple Java program with a graphical interface, in-
tended to appear somewhat similar to a typical chat
client. Each speaker?s name is displayed in a differ-
ent color, and the system displays the elapsed time
between comments, marking especially long pauses
in red. Annotators group sentences into conversa-
tions by clicking and dragging them onto each other.
3.2 Metrics
Before discussing the annotations themselves, we
will describe the metrics we use to compare differ-
ent annotations; these measure both how much our
annotators agree with each other, and how well our
model and various baselines perform. Comparing
clusterings with different numbers of clusters is a
non-trivial task, and metrics for agreement on su-
pervised classification, such as the ? statistic, are not
applicable.
To measure global similarity between annota-
tions, we use one-to-one accuracy. This measure de-
scribes how well we can extract whole conversations
intact, as required for summarization or information
extraction. To compute it, we pair up conversations
from the two annotations to maximize the total over-
lap3, then report the percentage of overlap found.
If we intend to monitor or participate in the con-
versation as it occurs, we will care more about lo-
cal judgements. The local agreement metric counts
agreements and disagreements within a context k.
We consider a particular utterance: the previous
k utterances are each in either the same or a dif-
ferent conversation. The lock score between two
annotators is their average agreement on these k
same/different judgements, averaged over all utter-
ances. For example, loc1 counts pairs of adjacent
utterances for which two annotations agree.
836
Mean Max Min
Conversations 81.33 128 50
Avg. Conv. Length 10.6 16.0 6.2
Avg. Conv. Density 2.75 2.92 2.53
Entropy 4.83 6.18 3.00
1-to-1 52.98 63.50 35.63
loc 3 81.09 86.53 74.75
M-to-1 (by entropy) 86.70 94.13 75.50
Table 1: Statistics on 6 annotations of 800 lines of chat
transcript. Inter-annotator agreement metrics (below the
line) are calculated between distinct pairs of annotations.
3.3 Discussion
A statistical examination of our data (table 1) shows
that that it is eminently suitable for disentanglement:
the average number of conversations active at a time
is 2.75. Our annotators have high agreement on
the local metric (average of 81.1%). On the 1-to-
1 metric, they disagree more, with a mean overlap
of 53.0% and a maximum of only 63.5%. This level
of overlap does indicate a useful degree of reliabil-
ity, which cannot be achieved with naive heuristics
(see section 5). Thus measuring 1-to-1 overlap with
our annotations is a reasonable evaluation for com-
putational models. However, we feel that the major
source of disagreement is one that can be remedied
in future annotation schemes: the specificity of the
individual annotations.
To measure the level of detail in an annotation, we
use the information-theoretic entropy of the random
variable which indicates which conversation an ut-
terance is in. This quantity is non-negative, increas-
ing as the number of conversations grow and their
size becomes more balanced. It reaches its maxi-
mum, 9.64 bits for this dataset, when each utterance
is placed in a separate conversation. In our anno-
tations, it ranges from 3.0 to 6.2. This large vari-
ation shows that some annotators are more specific
than others, but does not indicate how much they
agree on the general structure. To measure this, we
introduce the many-to-one accuracy. This measure-
ment is asymmetrical, and maps each of the conver-
sations of the source annotation to the single con-
3This is an example of max-weight bipartite matching, and
can be computed optimally using, eg, max-flow. The widely
used greedy algorithm is a two-approximation, although we
have not found large differences in practice.
(Lai) need money
(Astrid) suggest a paypal fund or similar
(Lai) Azzie [sic; typo for Astrid?]: my
shack guy here said paypal too but i have
no local bank acct
(Felicia) second?s Azzie?s suggestion
(Gale) we should charge the noobs $1 per
question to [Lai?s] paypal
(Felicia) bingo!
(Gale) we?d have the money in 2 days max
(Azzie) Lai: hrm, Have you tried to set
one up?
(Arlie) the federal reserve system conspir-
acy is keeping you down man
(Felicia) Gale: all ubuntu users .. pay up!
(Gale) and susers pay double
(Azzie) I certainly would make suse users
pay.
(Hildegard) triple.
(Lai) Azzie: not since being offline
(Felicia) it doesn?t need to be ?in state?
either
Figure 2: A schism occurring in our corpus (abridged):
not all annotators agree on where the thread about charg-
ing for answers to techical questions diverges from the
one about setting up Paypal accounts. Either Gale?s or
Azzie?s first comment seems to be the schism-inducing
utterance.
versation in the target with which it has the great-
est overlap, then counts the total percentage of over-
lap. This is not a statistic to be optimized (indeed,
optimization is trivial: simply make each utterance
in the source into its own conversation), but it can
give us some intuition about specificity. In partic-
ular, if one subdivides a coarse-grained annotation
to make a more specific variant, the many-to-one
accuracy from fine to coarse remains 1. When we
map high-entropy annotations (fine) to lower ones
(coarse), we find high many-to-one accuracy, with a
mean of 86%, which implies that the more specific
annotations have mostly the same large-scale bound-
aries as the coarser ones.
By examining the local metric, we can see even
more: local correlations are good, at an average of
81.1%. This means that, in the three-sentence win-
dow preceding each sentence, the annotators are of-
837
ten in agreement. If they recognize subdivisions of
a large conversation, these subdivisions tend to be
contiguous, not mingled together, which is why they
have little impact on the local measure.
We find reasons for the annotators? disagreement
about appropriate levels of detail in the linguistic
literature. As mentioned, new conversations of-
ten break off from old ones in schisms. Aoki et
al. (2006) discuss conversational features associated
with schisming and the related process of affiliation,
by which speakers attach themselves to a conversa-
tion. Schisms often branch off from asides or even
normal comments (toss-outs) within an existing con-
versation. This means that there is no clear begin-
ning to the new conversation? at the time when it
begins, it is not clear that there are two separate
floors, and this will not become clear until distinct
sets of speakers and patterns of turn-taking are es-
tablished. Speakers, meanwhile, take time to ori-
ent themselves to the new conversation. An example
schism is shown in Figure 2.
Our annotation scheme requires annotators to
mark each utterance as part of a single conversation,
and distinct conversations are not related in any way.
If a schism occurs, the annotator is faced with two
options: if it seems short, they may view it as a mere
digression and label it as part of the parent conver-
sation. If it seems to deserve a place of its own, they
will have to separate it from the parent, but this sev-
ers the initial comment (an otherwise unremarkable
aside) from its context. One or two of the annota-
tors actually remarked that this made the task con-
fusing. Our annotators seem to be either ?splitters?
or ?lumpers?? in other words, each annotator seems
to aim for a consistent level of detail, but each one
has their own idea of what this level should be.
As a final observation about the dataset, we test
the appropriateness of the assumption (used in pre-
vious work) that each speaker takes part in only one
conversation. In our data, the average speaker takes
part in about 3.3 conversations (the actual number
varies for each annotator). The more talkative a
speaker is, the more conversations they participate
in, as shown by a plot of conversations versus utter-
ances (Figure 3). The assumption is not very accu-
rate, especially for speakers with more than 10 utter-
ances.
0 10 20 30 40 50 60
Utterances
0
1
2
3
4
5
6
7
8
9
10
T
h
r
e
a
d
s
Figure 3: Utterances versus conversations participated in
per speaker on development data.
4 Model
Our model for disentanglement fits into the general
class of graph partitioning algorithms (Roth and Yih,
2004) which have been used for a variety of tasks in
NLP, including the related task of meeting segmen-
tation (Malioutov and Barzilay, 2006). These algo-
rithms operate in two stages: first, a binary classifier
marks each pair of items as alike or different, and
second, a consistent partition is extracted.4
4.1 Classification
We use a maximum-entropy classifier (Daume? III,
2004) to decide whether a pair of utterances x and
y are in same or different conversations. The most
likely class is different, which occurs 57% of the
time in development data. We describe the classi-
fier?s performance in terms of raw accuracy (cor-
rect decisions / total), precision and recall of the
same class, and F-score, the harmonic mean of pre-
cision and recall. Our classifier uses several types
of features (table 2). The chat-specific features yield
the highest accuracy and precision. Discourse and
content-based features have poor accuracy on their
own (worse than the baseline), since they work best
on nearby pairs of utterances, and tend to fail on
more distant pairs. Paired with the time gap fea-
ture, however, they boost accuracy somewhat and
produce substantial gains in recall, encouraging the
model to group related utterances together.
The time gap, as discussed above, is the most
widely used feature in previous work. We exam-
4Our first attempt at this task used a Bayesian generative
model. However, we could not define a sharp enough posterior
over new sentences, which made the model unstable and overly
sensitive to its prior.
838
Chat-specific (Acc 73: Prec: 73 Rec: 61 F: 66)
Time The time between x and y in sec-
onds, bucketed logarithmically.
Speaker x and y have the same speaker.
Mention x mentions y (or vice versa),
both mention the same name, ei-
ther mentions any name.
Discourse (Acc 52: Prec: 47 Rec: 77 F: 58)
Cue words Either x or y uses a greeting
(?hello? &c), an answer (?yes?,
?no? &c), or thanks.
Question Either asks a question (explicitly
marked with ???).
Long Either is long (> 10 words).
Content (Acc 50: Prec: 45 Rec: 74 F: 56)
Repeat(i) The number of words shared
between x and y which have
unigram probability i, bucketed
logarithmically.
Tech Whether both x and y use tech-
nical jargon, neither do, or only
one does.
Combined (Acc 75: Prec: 73 Rec: 68 F: 71)
Table 2: Feature functions with performance on develop-
ment data.
ine the distribution of pauses between utterances in
the same conversation. Our choice of a logarithmic
bucketing scheme is intended to capture two char-
acteristics of the distribution (figure 4). The curve
has its maximum at 1-3 seconds, and pauses shorter
than a second are less common. This reflects turn-
taking behavior among participants; participants in
the same conversation prefer to wait for each others?
responses before speaking again. On the other hand,
the curve is quite heavy-tailed to the right, leading
us to bucket long pauses fairly coarsely.
Our discourse-based features model some pair-
0 10 100 1000
0
20
40
seconds
Fr
eq
ue
nc
y
Figure 4: Distribution of pause length (log-scaled) be-
tween utterances in the same conversation.
wise relationships: questions followed by answers,
short comments reacting to longer ones, greetings at
the beginning and thanks at the end.
Word repetition is a key feature in nearly every
model for segmentation or coherence, so it is no sur-
prise that it is useful here. We bucket repeated words
by their unigram probability5 (measured over the en-
tire 52 hours of transcript). The bucketing scheme
allows us to deal with ?noise words? which are re-
peated coincidentally.
The point of the repetition feature is of course to
detect sentences with similar topics. We also find
that sentences with technical content are more likely
to be related than non-technical sentences. We label
an utterance as technical if it contains a web address,
a long string of digits, or a term present in a guide
for novice Linux users 6 but not in a large news cor-
pus (Graff, 1995)7. This is a light-weight way to
capture one ?semantic dimension? or cluster of re-
lated words, in a corpus which is not amenable to
full LSA or similar techniques. LSA in text corpora
yields a better relatedness measure than simple rep-
etition (Foltz et al, 1998), but is ineffective in our
corpus because of its wide variety of topics and lack
of distinct document boundaries.
Pairs of utterances which are widely separated
in the discourse are unlikely to be directly related?
even if they are part of the same conversation, the
link between them is probably a long chain of in-
tervening utterances. Thus, if we run our classifier
on a pair of very distant utterances, we expect it to
default to the majority class, which in this case will
be different, and this will damage our performance
in case the two are really part of the same conver-
sation. To deal with this, we run our classifier only
on utterances separated by 129 seconds or less. This
is the last of our logarithmic buckets in which the
classifier has a significant advantage over the major-
ity baseline. For 99.9% of utterances in an ongoing
conversation, the previous utterance in that conver-
sation is within this gap, and so the system has a
5We discard the 50 most frequent words entirely.
6
?Introduction to Linux: A Hands-on Guide?. Machtelt
Garrels. Edition 1.25 from http://tldp.org/LDP/intro-
linux/html/intro-linux.html .
7Our data came from the LA times, 94-97? helpfully, it pre-
dates the current wide coverage of Linux in the mainstream
press.
839
chance of correctly linking the two.
On test data, the classifier has a mean accuracy of
68.2 (averaged over annotations). The mean preci-
sion of same conversation is 53.3 and the recall is
71.3, with mean F-score of 60. This error rate is
high, but the partitioning procedure allows us to re-
cover from some of the errors, since if nearby utter-
ances are grouped correctly, the bad decisions will
be outvoted by good ones.
4.2 Partitioning
The next step in the process is to cluster the utter-
ances. We wish to find a set of clusters for which the
weighted accuracy of the classifier would be max-
imal; this is an example of correlation clustering
(Bansal et al, 2004), which is NP-complete8. Find-
ing an exact solution proves to be difficult; the prob-
lem has a quadratic number of variables (one for
each pair of utterances) and a cubic number of tri-
angle inequality constraints (three for each triplet).
With 800 utterances in our test set, even solving the
linear program with CPLEX (Ilog, Inc., 2003) is too
expensive to be practical.
Although there are a variety of approximations
and local searches, we do not wish to investigate
partitioning methods in this paper, so we simply
use a greedy search. In this algorithm, we as-
sign utterance j by examining all previous utter-
ances i within the classifier?s window, and treat-
ing the classifier?s judgement pi,j ? .5 as a vote for
cluster(i). If the maximum vote is greater than 0,
we set cluster(j) = argmaxc votec. Otherwise j
is put in a new cluster. Greedy clustering makes at
least a reasonable starting point for further efforts,
since it is a natural online algorithm? it assigns each
utterance as it arrives, without reference to the fu-
ture.
At any rate, we should not take our objective func-
tion too seriously. Although it is roughly correlated
with performance, the high error rate of the classifier
makes it unlikely that small changes in objective will
mean much. In fact, the objective value of our output
solutions are generally higher than those for true so-
8We set up the problem by taking the weight of edge i, j as
the classifier?s decision pi,j ? .5. Roth and Yih (2004) use log
probabilities as weights. Bansal et al (2004) propose the log
odds ratio log(p/(1 ? p)). We are unsure of the relative merit
of these approaches.
lutions, which implies we have already reached the
limits of what our classifier can tell us.
5 Experiments
We annotate the 800 line test transcript using our
system. The annotation obtained has 63 conversa-
tions, with mean length 12.70. The average density
of conversations is 2.9, and the entropy is 3.79. This
places it within the bounds of our human annota-
tions (see table 1), toward the more general end of
the spectrum.
As a standard of comparison for our system, we
provide results for several baselines? trivial systems
which any useful annotation should outperform.
All different Each utterance is a separate conversa-
tion.
All same The whole transcript is a single conversa-
tion.
Blocks of k Each consecutive group of k utterances
is a conversation.
Pause of k Each pause of k seconds or more sepa-
rates two conversations.
Speaker Each speaker?s utterances are treated as a
monologue.
For each particular metric, we calculate the best
baseline result among all of these. To find the best
block size or pause length, we search over multiples
of 5 between 5 and 300. This makes these baselines
appear better than they really are, since their perfor-
mance is optimized with respect to the test data.
Our results, in table 3, are encouraging. On aver-
age, annotators agree more with each other than with
any artificial annotation, and more with our model
than with the baselines. For the 1-to-1 accuracy met-
ric, we cannot claim much beyond these general re-
sults. The range of human variation is quite wide,
and there are annotators who are closer to baselines
than to any other human annotator. As explained
earlier, this is because some human annotations are
much more specific than others. For very specific
annotations, the best baselines are short blocks or
pauses. For the most general, marking all utterances
the same does very well (although for all other an-
notations, it is extremely poor).
840
Other Annotators Model Best Baseline All Diff All Same
Mean 1-to-1 52.98 40.62 34.73 (Blocks of 40) 10.16 20.93
Max 1-to-1 63.50 51.12 56.00 (Pause of 65) 16.00 53.50
Min 1-to-1 35.63 33.63 28.62 (Pause of 25) 6.25 7.13
Mean loc 3 81.09 72.75 62.16 (Speaker) 52.93 47.07
Max loc 3 86.53 75.16 69.05 (Speaker) 62.15 57.47
Min loc 3 74.75 70.47 54.37 (Speaker) 42.53 37.85
Table 3: Metric values between proposed annotations and human annotations. Model scores typically fall between
inter-annotator agreement and baseline performance.
For the local metric, the results are much clearer.
There is no overlap in the ranges; for every test an-
notation, agreement is highest with other annota-
tor, then our model and finally the baselines. The
most competitive baseline is one conversation per
speaker, which makes sense, since if a speaker
makes two comments in a four-utterance window,
they are very likely to be related.
The name mention features are critical for our
model?s performance. Without this feature, the clas-
sifier?s development F-score drops from 71 to 56.
The disentanglement system?s test performance de-
creases proportionally; mean 1-to-1 falls to 36.08,
and mean loc 3 to 63.00, essentially baseline per-
formance. On the other hand, mentions are not
sufficient; with only name mention and time gap
features, mean 1-to-1 is 38.54 and loc 3 is 67.14.
For some utterances, of course, name mentions pro-
vide the only reasonable clue to the correct deci-
sion, which is why humans mention names in the
first place. But our system is probably overly depen-
dent on them, since they are very reliable compared
to our other features.
6 Future Work
Although our annotators are reasonably reliable, it
seems clear that they think of conversations as a hi-
erarchy, with digressions and schisms. We are in-
terested to see an annotation protocol which more
closely follows human intuition and explicitly in-
cludes these kinds of relationships.
We are also interested to see how well this feature
set performs on speech data, as in (Aoki et al, 2003).
Spoken conversation is more natural than text chat,
but when participants are not face-to-face, disentan-
glement remains a problem. On the other hand, spo-
ken dialogue contains new sources of information,
such as prosody. Turn-taking behavior is also more
distinct, which makes the task easier, but according
to (Aoki et al, 2006), it is certainly not sufficient.
Improving the current model will definitely re-
quire better features for the classifier. However, we
also left the issue of partitioning nearly completely
unexplored. If the classifier can indeed be improved,
we expect the impact of search errors to increase.
Another issue is that human users may prefer more
or less specific annotations than our model provides.
We have observed that we can produce lower or
higher-entropy annotations by changing the classi-
fier?s bias to label more edges same or different. But
we do not yet know whether this corresponds with
human judgements, or merely introduces errors.
7 Conclusion
This work provides a corpus of annotated data for
chat disentanglement, which, along with our pro-
posed metrics, should allow future researchers to
evaluate and compare their results quantitatively9.
Our annotations are consistent with one another, es-
pecially with respect to local agreement. We show
that features based on discourse patterns and the
content of utterances are helpful in disentanglement.
The model we present can outperform a variety of
baselines.
Acknowledgements
Our thanks to Suman Karumuri, Steve Sloman, Matt
Lease, David McClosky, 7 test annotators, 3 pilot
annotators, 3 anonymous reviewers and the NSF
PIRE grant.
9Code and data for this project will be available at
http://cs.brown.edu/people/melsner.
841
References
Evrim Acar, Seyit Ahmet Camtepe, Mukkai S. Kr-
ishnamoorthy, and Blent Yener. 2005. Model-
ing and multiway analysis of chatroom tensors. In
Paul B. Kantor, Gheorghe Muresan, Fred Roberts,
Daniel Dajun Zeng, Fei-Yue Wang, Hsinchun Chen,
and Ralph C. Merkle, editors, ISI, volume 3495 of
Lecture Notes in Computer Science, pages 256?268.
Springer.
Paul M. Aoki, Matthew Romaine, Margaret H. Szyman-
ski, James D. Thornton, Daniel Wilson, and Allison
Woodruff. 2003. The mad hatter?s cocktail party: a
social mobile audio space supporting multiple simul-
taneous conversations. In CHI ?03: Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 425?432, New York, NY, USA. ACM
Press.
Paul M. Aoki, Margaret H. Szymanski, Luke D.
Plurkowski, James D. Thornton, Allison Woodruff,
and Weilie Yi. 2006. Where?s the ?party? in ?multi-
party??: analyzing the structure of small-group socia-
ble talk. In CSCW ?06: Proceedings of the 2006 20th
anniversary conference on Computer supported coop-
erative work, pages 393?402, New York, NY, USA.
ACM Press.
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004.
Correlation clustering. Machine Learning, 56(1-
3):89?113.
Seyit Ahmet Camtepe, Mark K. Goldberg, Malik
Magdon-Ismail, and Mukkai Krishnamoorty. 2005.
Detecting conversing groups of chatters: a model, al-
gorithms, and tests. In IADIS AC, pages 89?96.
Hal Daume? III. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. Paper available
at http://pub.hal3.name#daume04cg-bfgs, implemen-
tation available at http://hal3.name/megam/, August.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
latent semantic analysis. Discourse Processes,
25(2&3):285?307.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation of
multi-party conversation. In ACL ?03: Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 562?569, Morristown, NJ,
USA. Association for Computational Linguistics.
David Graff. 1995. North American News Text Corpus.
Linguistic Data Consortium. LDC95T21.
Ilog, Inc. 2003. Cplex solver.
Natasa Jovanovic, Rieks op den Akker, and Anton Ni-
jholt. 2006. Addressee identification in face-to-face
meetings. In EACL. The Association for Computer
Linguistics.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In ACL.
The Association for Computer Linguistics.
Jacki O?Neill and David Martin. 2003. Text chat in ac-
tion. In GROUP ?03: Proceedings of the 2003 inter-
national ACM SIGGROUP conference on Supporting
group work, pages 40?49, New York, NY, USA. ACM
Press.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of CoNLL-2004, pages
1?8. Boston, MA, USA.
Harvey Sacks, Emanuel A. Schegloff, and Gail Jefferson.
1974. A simplest systematics for the organization of
turn-taking for conversation. Language, 50(4):696?
735.
842
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 41?44,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Coreference-inspired Coherence Modeling
Micha Elsner and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{melsner,ec}@cs.brown.edu
Abstract
Research on coreference resolution and sum-
marization has modeled the way entities are
realized as concrete phrases in discourse. In
particular there exist models of the noun
phrase syntax used for discourse-new versus
discourse-old referents, and models describ-
ing the likely distance between a pronoun and
its antecedent. However, models of discourse
coherence, as applied to information ordering
tasks, have ignored these kinds of information.
We apply a discourse-new classifier and pro-
noun coreference algorithm to the information
ordering task, and show significant improve-
ments in performance over the entity grid, a
popular model of local coherence.
1 Introduction
Models of discourse coherence describe the relation-
ships between nearby sentences, in which previous
sentences help make their successors easier to un-
derstand. Models of coherence have been used to
impose an order on sentences for multidocument
summarization (Barzilay et al, 2002), to evaluate
the quality of human-authored essays (Miltsakaki
and Kukich, 2004), and to insert new information
into existing documents (Chen et al, 2007).
These models typically view a sentence either as
a bag of words (Foltz et al, 1998) or as a bag of en-
tities associated with various syntactic roles (Lapata
and Barzilay, 2005). However, a mention of an en-
tity contains more information than just its head and
syntactic role. The referring expression itself con-
tains discourse-motivated information distinguish-
ing familiar entities from unfamiliar and salient from
non-salient. These patterns have been studied ex-
tensively, by linguists (Prince, 1981; Fraurud, 1990)
and in the field of coreference resolution. We draw
on the coreference work, taking two standard models
from the literature and applying them to coherence
modeling.
Our first model distinguishes discourse-new from
discourse-old noun phrases, using features based
on Uryupina (2003). Discourse-new NPs are those
whose referents have not been previously mentioned
in the discourse. As noted by studies since Hawkins
(1978), there are marked syntactic differences be-
tween the two classes.
Our second model describes pronoun coreference.
To be intelligible, pronouns must be placed close to
appropriate referents with the correct number and
gender. Centering theory (Grosz et al, 1995) de-
scribes additional constraints about which entities in
a discourse can be pronominalized: if there are pro-
nouns in a segment, they must include the backward-
looking center. We use a model which probabilisti-
cally attempts to describe these preferences (Ge et
al., 1998).
These two models can be combined with the en-
tity grid described by Lapata and Barzilay (2005)
for significant improvement. The magnitude of the
improvement is particularly interesting given that
Barzilay and Lapata (2005) do use a coreference sys-
tem but are unable to derive much advantage from it.
2 Discourse-new Model
In the task of discourse-new classification, the model
is given a referring expression (as in previous work,
we consider only NPs) from a document and must
41
determine whether it is a first mention (discourse-
new) or a subsequent mention (discourse-old). Fea-
tures such as full names, appositives, and restrictive
relative clauses are associated with the introduction
of unfamiliar entities into discourse (Hawkins, 1978;
Fraurud, 1990; Vieira and Poesio, 2000). Classi-
fiers in the literature include (Poesio et al, 2005;
Uryupina, 2003; Ng and Cardie, 2002). The sys-
tem of Nenkova and McKeown (2003) works in the
opposite direction. It is designed to rewrite the ref-
erences in multi-document summaries, so that they
conform to the common discourse patterns.
We construct a maximum-entropy classifier us-
ing syntactic and lexical features derived from
Uryupina (2003), and a publicly available learning
tool (Daume? III, 2004). Our system scores 87.4%
(F-score of the disc-new class on the MUC-7 for-
mal test set); this is comparable to the state-of-the-
art system of Uryupina (2003), which scores 86.91.
To model coreference with this system, we assign
each NP in a document a label Lnp ? {new, old}.
Since the correct labeling depends on the coref-
erence relationships between the NPs, we need
some way to guess at this; we take all NPs with
the same head to be coreferent, as in the non-
coreference version of (Barzilay and Lapata, 2005)2.
We then take the probability of a document as
?
np:NPs P (Lnp|np).
We must make several small changes to the model
to adapt it to this setting. For the discourse-new clas-
sification task, the model?s most important feature
is whether the head word of the NP to be classified
has occurred previously (as in Ng and Cardie (2002)
and Vieira and Poesio (2000)). For coherence mod-
eling, we must remove this feature, since it depends
on document order, which is precisely what we are
trying to predict. The coreference heuristic will also
fail to resolve any pronouns, so we discard them.
Another issue is that NPs whose referents are
familiar tend to resemble discourse-old NPs, even
though they have not been previously mentioned
(Fraurud, 1990). These include unique objects like
the FBI or generic ones like danger or percent. To
1Poesio et al (2005) score 90.2%, but on a different corpus.
2Unfortunately, this represents a substantial sacrifice; as
Poesio and Vieira (1998) show, only about 2/3 of definite de-
scriptions which are anaphoric have the same head as their an-
tecedent.
avoid using these deceptive phrases as examples of
discourse-newness, we attempt to heuristically re-
move them from the training set by discarding any
NP whose head occurs only once in the document3.
The labels we apply to NPs in our test data are
systematically biased by the ?same head? heuristic
we use for coreference. This is a disadvantage for
our system, but it has a corresponding advantage?
we can use training data labeled using the same
heuristic, without any loss in performance on the
coherence task. NPs we fail to learn about during
training are likely to be mislabeled at test time any-
way, so performance does not degrade by much. To
counter this slight degradation, we can use a much
larger training corpus, since we no longer require
gold-standard coreference annotations.
3 Pronoun Coreference Model
Pronoun coreference is another important aspect of
coherence? if a pronoun is used too far away from
any natural referent, it becomes hard to interpret,
creating confusion. Too many referents, however,
create ambiguity. To describe this type of restriction,
we must model the probability of the text containing
pronouns (denoted ri), jointly with their referents
ai. (This takes more work than simply resolving the
pronouns conditioned on the text.) The model of Ge
et al (1998) provides the requisite probabilities:
P (ai, ri|ai?1i ) =P (ai|h(ai), m(ai))
Pgen(ai, ri)Pnum(ai, ri)
Here h(a) is the Hobbs distance (Hobbs, 1976),
which measures distance between a pronoun and
prospective antecedent, taking into account various
factors, such as syntactic constraints on pronouns.
m(a) is the number of times the antecedent has
been mentioned previously in the document (again
using ?same head? coreference for full NPs, but
also counting the previous antecedents ai?1i ). Pgen
and Pnum are distributions over gender and num-
ber given words. The model is trained using a small
hand-annotated corpus first used in Ge et al (1998).
3Bean and Riloff (1999) and Uryupina (2003) construct
quite accurate classifiers to detect unique NPs. However, some
preliminary experiments convinced us that our heuristic method
worked well enough for the purpose.
42
Disc. Acc Disc. F Ins.
Random 50.00 50.00 12.58
Entity Grid 76.17 77.55 19.57
Disc-New 70.35 73.47 16.27
Pronoun 55.77 62.27 13.95
EGrid+Disc-New 78.88 80.31 21.93
Combined 79.60 81.02 22.98
Table 1: Results on 1004 WSJ documents.
Finding the probability of a document using this
model requires us to sum out the antecedents a. Un-
fortunately, because each ai is conditioned on the
previous ones, this cannot be done efficiently. In-
stead, we use a greedy search, assigning each pro-
noun left to right. Finally we report the probability
of the resulting sequence of pronoun assignments.
4 Baseline Model
As a baseline, we adopt the entity grid (Lapata and
Barzilay, 2005). This model outperforms a variety
of word overlap and semantic similarity models, and
is used as a component in the state-of-the-art system
of Soricut and Marcu (2006). The entity grid rep-
resents each entity by tracking the syntactic roles in
which it appears throughout the document. The in-
ternal syntax of the various referring expressions is
ignored. Since it also uses the ?same head? corefer-
ence heuristic, it also disregards pronouns.
Since the three models use very different feature
sets, we combine them by assuming independence
and multiplying the probabilities.
5 Experiments
We evaluate our models using two tasks, both based
on the assumption that a human-authored document
is coherent, and uses the best possible ordering of
its sentences (see Lapata (2006)). In the discrimina-
tion task (Barzilay and Lapata, 2005), a document
is compared with a random permutation of its sen-
tences, and we score the system correct if it indicates
the original as more coherent4.
4Since the model might refuse to make a decision by scor-
ing a permutation the same as the original, we also report
F-score, where precision is correct/decisions and recall is
correct/total.
Discrimination becomes easier for longer docu-
ments, since a random permutation is likely to be
much less similar to the original. Therefore we also
test our systems on the task of insertion (Chen et al,
2007), in which we remove a sentence from a doc-
ument, then find the point of insertion which yields
the highest coherence score. The reported score is
the average fraction of sentences per document rein-
serted in their original position (averaged over doc-
uments, not sentences, so that longer documents do
not disproportionally influence the results)5.
We test on sections 14-24 of the Penn Treebank
(1004 documents total). Previous work has fo-
cused on the AIRPLANE corpus (Barzilay and Lee,
2004), which contains short announcements of air-
plane crashes written by and for domain experts.
These texts use a very constrained style, with few
discourse-new markers or pronouns, and so our sys-
tem is ineffective; the WSJ corpus is much more
typical of normal informative writing. Also unlike
previous work, we do not test the task of completely
reconstructing a document?s order, since this is com-
putationally intractable and results on WSJ docu-
ments6 would likely be dominated by search errors.
Our results are shown in table 5. When run alone,
the entity grid outperforms either of our models.
However, all three models are significantly better
than random. Combining all three models raises dis-
crimination performance by 3.5% over the baseline
and insertion by 3.4%. Even the weakest compo-
nent, pronouns, contributes to the joint model; when
it is left out, the resulting EGrid + Disc-New model
is significantly worse than the full combination. We
test significance using Wilcoxon?s signed-rank test;
all results are significant with p < .001.
6 Conclusions
The use of these coreference-inspired models leads
to significant improvements in the baseline. Of the
two, the discourse-new detector is by far more ef-
fective. The pronoun model?s main problem is that,
although a pronoun may have been displaced from
its original position, it can often find another seem-
ingly acceptable referent nearby. Despite this issue
5Although we designed a metric that distinguishes near
misses from random performance, it is very well correlated with
exact precision, so, for simplicity?s sake, we omit it.
6Average 22 sentences, as opposed to 11.5 for AIRPLANE.
43
it performs significantly better than chance and is
capable of slightly improving the combined model.
Both of these models are very different from the lex-
ical and entity-based models currently used for this
task (Soricut and Marcu, 2006), and are probably
capable of improving the state of the art.
As mentioned, Barzilay and Lapata (2005) uses a
coreference system to attempt to improve the entity
grid, but with mixed results. Their method of com-
bination is quite different from ours; they use the
system?s judgements to define the ?entities? whose
repetitions the system measures7. In contrast, we do
not attempt to use any proposed coreference links;
as Barzilay and Lapata (2005) point out, these links
are often erroneous because the disorded input text
is so dissimilar to the training data. Instead we ex-
ploit our models? ability to measure the probability
of various aspects of the text.
Acknowledgements
Chen and Barzilay, reviewers, DARPA, et al
References
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: an entity-based approach. In ACL
2005.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004, pages 113?120.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2002. Inferring strategies for sentence ordering
in multidocument news summarization. Journal of Ar-
tificial Intelligence Results (JAIR), 17:35?55.
David L. Bean and Ellen Riloff. 1999. Corpus-based
identification of non-anaphoric noun phrases. In
ACL?99, pages 373?380.
Erdong Chen, Benjamin Snyder, and Regina Barzilay.
2007. Incremental text structuring with online hier-
archical ranking. In Proceedings of EMNLP.
Hal Daume? III. 2004. Notes on CG and LM-BFGS
optimization of logistic regressio n. Paper available
at http://pub.hal3.name#daume04cg-bfgs, implemen-
tation available at http://hal3.name/megam/, August.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
7We attempted this method for pronouns using our model,
but found it ineffective.
latent semantic analysis. Discourse Processes,
25(2&3):285?307.
Kari Fraurud. 1990. Definiteness and the processing of
noun phrases in natural discourse. Journal of Seman-
tics, 7(4):395?433.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A sta-
tistical approach to anaphora resolution. In Proceed-
ings of the Sixth Workshop on Very Large Corpora,
pages 161?171.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
John A. Hawkins. 1978. Definiteness and indefinite-
ness: a study in reference and grammaticality predic-
tion. Croom Helm Ltd.
Jerry R. Hobbs. 1976. Pronoun resolution. Technical
Report 76-1, City College New York.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In IJCAI, pages 1085?1090.
Mirella Lapata. 2006. Automatic evaluation of informa-
tion ordering: Kendall?s tau. Computational Linguis-
tics, 32(4):1?14.
E. Miltsakaki and K. Kukich. 2004. Evaluation of text
coherence for electronic essay scoring systems. Nat.
Lang. Eng., 10(1):25?55.
Ani Nenkova and Kathleen McKeown. 2003. Refer-
ences to named entities: a corpus study. In NAACL
?03, pages 70?72.
Vincent Ng and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. In COLING.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183?216.
Massimo Poesio, Mijail Alexandrov-Kabadjov, Renata
Vieira, Rodrigo Goulart, and Olga Uryupina. 2005.
Does discourse-new detection help definite description
resolution? In Proceedings of the Sixth International
Workshop on Computational Semantics, Tillburg.
Ellen Prince. 1981. Toward a taxonomy of given-new in-
formation. In Peter Cole, editor, Radical Pragmatics,
pages 223?255. Academic Press, New York.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In ACL-
2006.
Olga Uryupina. 2003. High-precision identification of
discourse new and unique noun phrases. In Proceed-
ings of the ACL Student Workshop, Sapporo.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
44
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 101?104,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Self-Training for Biomedical Parsing
David McClosky and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{dmcc|ec}@cs.brown.edu
Abstract
Parser self-training is the technique of
taking an existing parser, parsing extra
data and then creating a second parser
by treating the extra data as further
training data. Here we apply this tech-
nique to parser adaptation. In partic-
ular, we self-train the standard Char-
niak/Johnson Penn-Treebank parser us-
ing unlabeled biomedical abstracts. This
achieves an f -score of 84.3% on a stan-
dard test set of biomedical abstracts from
the Genia corpus. This is a 20% error re-
duction over the best previous result on
biomedical data (80.2% on the same test
set).
1 Introduction
Parser self-training is the technique of taking an
existing parser, parsing extra data and then cre-
ating a second parser by treating the extra data
as further training data. While for many years it
was thought not to help state-of-the art parsers,
more recent work has shown otherwise. In this
paper we apply this technique to parser adap-
tation. In particular we self-train the standard
Charniak/Johnson Penn-Treebank (C/J) parser
using unannotated biomedical data. As is well
known, biomedical data is hard on parsers be-
cause it is so far from more ?standard? English.
To our knowledge this is the first application of
self-training where the gap between the training
and self-training data is so large.
In section two, we look at previous work. In
particular we note that there is, in fact, very
little data on self-training when the corpora for
self-training is so different from the original la-
beled data. Section three describes our main
experiment on standard test data (Clegg and
Shepherd, 2005). Section four looks at some
preliminary results we obtained on development
data that show in slightly more detail how self-
training improved the parser. We conclude in
section five.
2 Previous Work
While self-training has worked in several do-
mains, the early results on self-training for pars-
ing were negative (Steedman et al, 2003; Char-
niak, 1997). However more recent results have
shown that it can indeed improve parser perfor-
mance (Bacchiani et al, 2006; McClosky et al,
2006a; McClosky et al, 2006b).
One possible use for this technique is for
parser adaptation ? initially training the parser
on one type of data for which hand-labeled trees
are available (e.g., Wall Street Journal (M. Mar-
cus et al, 1993)) and then self-training on a sec-
ond type of data in order to adapt the parser
to the second domain. Interestingly, there is lit-
tle to no data showing that this actually works.
Two previous papers would seem to address this
issue: the work by Bacchiani et al (2006) and
McClosky et al (2006b). However, in both cases
the evidence is equivocal.
Bacchiani and Roark train the Roark parser
(Roark, 2001) on trees from the Brown treebank
and then self-train and test on data from Wall
Street Journal. While they show some improve-
ment (from 75.7% to 80.5% f -score) there are
several aspects of this work which leave its re-
101
sults less than convincing as to the utility of self-
training for adaptation. The first is the pars-
ing results are quite poor by modern standards.1
Steedman et al (2003) generally found that self-
training does not work, but found that it does
help if the baseline results were sufficiently bad.
Secondly, the difference between the Brown
corpus treebank and the Wall Street Journal
corpus is not that great. One way to see this
is to look at out-of-vocabulary statistics. The
Brown corpus has an out-of-vocabulary rate of
approximately 6% when given WSJ training as
the lexicon. In contrast, the out-of-vocabulary
rate of biomedical abstracts given the same lex-
icon is significantly higher at about 25% (Lease
and Charniak, 2005). Thus the bridge the self-
trained parser is asked to build is quite short.
This second point is emphasized by the sec-
ond paper on self-training for adaptation (Mc-
Closky et al, 2006b). This paper is based on the
C/J parser and thus its results are much more
in line with modern expectations. In particu-
lar, it was able to achieve an f -score of 87% on
Brown treebank test data when trained and self-
trained on WSJ-like data. Note this last point.
It was not the case that it used the self-training
to bridge the corpora difference. It self-trained
on NANC, not Brown. NANC is a news corpus,
quite like WSJ data. Thus the point of that
paper was that self-training a WSJ parser on
similar data makes the parser more flexible, not
better adapted to the target domain in particu-
lar. It said nothing about the task we address
here. Thus our claim is that previous results are
quite ambiguous on the issue of bridging corpora
for parser adaptation.
Turning briefly to previous results on Medline
data, the best comparative study of parsers is
that of Clegg and Shepherd (2005), which eval-
uates several statistical parsers. Their best re-
sult was an f -score of 80.2%. This was on the
Lease/Charniak (L/C) parser (Lease and Char-
niak, 2005).2 A close second (1% behind) was
1This is not a criticism of the work. The results are
completely in line with what one would expect given the
base parser and the relatively small size of the Brown
treebank.
2This is the standard Charniak parser (without
the parser of Bikel (2004). The other parsers
were not close. However, several very good cur-
rent parsers were not available when this paper
was written (e.g., the Berkeley Parser (Petrov
et al, 2006)). However, since the newer parsers
do not perform quite as well as the C/J parser
on WSJ data, it is probably the case that they
would not significantly alter the landscape.
3 Central Experimental Result
We used as the base parser the standardly avail-
able C/J parser. We then self-trained the parser
on approximately 270,000 sentences ? a ran-
dom selection of abstracts from Medline.3 Med-
line is a large database of abstracts and citations
from a wide variety of biomedical literature. As
we note in the next section, the number 270,000
was selected by observing performance on a de-
velopment set.
We weighted the original WSJ hand anno-
tated sentences equally with self-trained Med-
line data. So, for example, McClosky et al
(2006a) found that the data from the hand-
annotated WSJ data should be considered at
least five times more important than NANC
data on an event by event level. We did no tun-
ing to find out if there is some better weighting
for our domain than one-to-one.
The resulting parser was tested on a test cor-
pus of hand-parsed sentences from the Genia
Treebank (Tateisi et al, 2005). These are ex-
actly the same sentences as used in the com-
parisons of the last section. Genia is a corpus
of abstracts from the Medline database selected
from a search with the keywords Human, Blood
Cells, and Transcription Factors. Thus the Ge-
nia treebank data are all from a small domain
within Biology. As already noted, the Medline
abstracts used for self-training were chosen ran-
domly and thus span a large number of biomed-
ical sub-domains.
The results, the central results of this paper,
are shown in Figure 1. Clegg and Shepherd
(2005) do not provide separate precision and
recall numbers. However we can see that the
reranker) modified to use an in-domain tagger.
3http://www.ncbi.nlm.nih.gov/PubMed/
102
System Precision Recall f -score
L/C ? ? 80.2%
Self-trained 86.3% 82.4% 84.3%
Figure 1: Comparison of the Medline self-trained
parser against the previous best
Medline self-trained parser achieves an f -score
of 84.3%, which is an absolute reduction in er-
ror of 4.1%. This corresponds to an error rate
reduction of 20% over the L/C baseline.
4 Discussion
Prior to the above experiment on the test data,
we did several preliminary experiments on devel-
opment data from the Genia Treebank. These
results are summarized in Figure 2. Here we
show the f -score for four versions of the parser
as a function of number of self-training sen-
tences. The dashed line on the bottom is the
raw C/J parser with no self-training. At 80.4, it
is clearly the worst of the lot. On the other hand,
it is already better than the 80.2% best previous
result for biomedical data. This is solely due to
the introduction of the 50-best reranker which
distinguishes the C/J parser from the preceding
Charniak parser.
The almost flat line above it is the C/J parser
with NANC self-training data. As mentioned
previously, NANC is a news corpus, quite like
the original WSJ data. At 81.4% it gives us a
one percent improvement over the original WSJ
parser.
The topmost line, is the C/J parser trained
on Medline data. As can be seen, even just a
thousand lines of Medline is already enough to
drive our results to a new level and it contin-
ues to improve until about 150,000 sentences at
which point performance is nearly flat. How-
ever, as 270,000 sentences is fractionally better
than 150,000 sentences that is the number of
self-training sentences we used for our results
on the test set.
Lastly, the middle jagged line is for an inter-
esting idea that failed to work. We mention it
in the hope that others might be able to succeed
where we have failed.
We reasoned that textbooks would be a par-
ticularly good bridging corpus. After all, they
are written to introduce someone ignorant of
a field to the ideas and terminology within it.
Thus one might expect that the English of a Bi-
ology textbook would be intermediate between
the more typical English of a news article and
the specialized English native to the domain.
To test this we created a corpus of seven texts
(?BioBooks?) on various areas of biology that
were available on the web. We observe in Fig-
ure 2 that for all quantities of self-training data
one does better with Medline than BioBooks.
For example, at 37,000 sentences the BioBook
corpus is only able to achieve and an f-measure
of 82.8% while the Medline corpus is at 83.4%.
Furthermore, BioBooks levels off in performance
while Medline has significant improvement left
in it. Thus, while the hypothesis seems reason-
able, we were unable to make it work.
5 Conclusion
We self-trained the standard C/J parser on
270,000 sentences of Medline abstracts. By do-
ing so we achieved a 20% error reduction over
the best previous result for biomedical parsing.
In terms of the gap between the supervised data
and the self-trained data, this is the largest that
has been attempted.
Furthermore, the resulting parser is of interest
in its own right, being as it is the most accurate
biomedical parser yet developed. This parser is
available on the web.4
Finally, there is no reason to believe that
84.3% is an upper bound on what can be
achieved with current techniques. Lease and
Charniak (2005) achieve their results using small
amounts of hand-annotated biomedical part-of-
speech-tagged data and also explore other pos-
sible sources or information. It is reasonable to
assume that its use would result in further im-
provement.
Acknowledgments
This work was supported by DARPA GALE con-
tract HR0011-06-2-0001. We would like to thank the
BLLIP team for their comments.
4http://bllip.cs.brown.edu/biomedical/
103
0 25000 50000 75000 100000 125000 150000 175000 200000 225000 250000 275000
Number of sentences added
80.0
80.2
80.4
80.6
80.8
81.0
81.2
81.4
81.6
81.8
82.0
82.2
82.4
82.6
82.8
83.0
83.2
83.4
83.6
83.8
84.0
84.2
84.4
R
e
r
a
n
k
i
n
g
 
p
a
r
s
e
r
 
f
-
s
c
o
r
e
WSJ+Medline
WSJ+BioBooks
WSJ+NANC
WSJ (baseline)
Figure 2: Labeled Precision-Recall results on development data for four versions of the parser as a function
of number of self-training sentences
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of
stochastic grammars. Computer Speech and Lan-
guage, 20(1):41?68.
Daniel M. Bikel. 2004. Intricacies of collins parsing
model. Computational Linguistics, 30(4).
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
Proc. AAAI, pages 598?603.
Andrew B. Clegg and Adrian Shepherd. 2005.
Evaluating and integrating treebank parsers on
a biomedical corpus. In Proceedings of the ACL
Workshop on Software.
Matthew Lease and Eugene Charniak. 2005. Pars-
ing biomedical literature. In Second International
Joint Conference on Natural Language Processing
(IJCNLP?05).
M. Marcus et al 1993. Building a large annotated
corpus of English: The Penn Treebank. Comp.
Linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006a. Effective self-training for parsing.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Main Conference,
pages 152?159.
David McClosky, Eugene Charniak, and Mark John-
son. 2006b. Reranking and self-training for
parser adaptation. In Proceedings of COLING-
ACL 2006, pages 337?344, Sydney, Australia,
July. Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, compact,
and interpretable tree annotation. In Proceed-
ings of COLING-ACL 2006, pages 433?440, Syd-
ney, Australia, July. Association for Computa-
tional Linguistics.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguis-
tics, 27(2):249?276.
Mark Steedman, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proc. of European ACL (EACL),
pages 331?338.
Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii.
2005. Syntax Annotation for the GENIA corpus.
Proc. IJCNLP 2005, Companion volume, pages
222?227.
104
Reading Comprehension Programs in a 
Statistical-Language-Processing Class* 
Eugene Charniak, Yasemin Altun, Rodrigo de Salvo Braz, 
Benjamin Garrett, Margaret Kosmala, Tomer Moscovich, Lixin Pang, 
Changhee Pyo, Ye Sun, Wei Wy, Zhongfa Yang, Shawn Zeller, and Lisa Zorn 
Brown University 
Abstract 
We present-some n w results for the reading 
comprehension task described in \[3\] that im- 
prove on the best published results - from 
36% in \[3\] to 41% (the best of the systems 
described herein). We discuss a variety of 
techniques that tend to give small improve- 
ments, ranging from the fairly simple (give 
verbs more weight in answer selection) to 
the fairly complex (use specific techniques 
for answering specific kinds of questions). 
1 Introduction 
CS241, the graduate course in statistical lan- 
guage processing at Brown University, had 
as its class project the creation of programs 
to answer reading-comprehension tests. In 
particular, we used the Remedia TM reading 
comprehension test data as annotated by a 
group at MITRE Corporation, henceforth 
called the Deep Read group \[3\]. The class di- 
vided itself into four groups with sizes rang- 
ing from two to four students. In the first 
half of the semester the goal was to repro- 
duce the results of Deep Read and of one 
aother. After this learning and debugging 
period the groups were encouraged to think 
of and implement new ideas. 
The Deep Read group provided us with 
an on-line version of the Remedia material 
along with several marked up versions of 
* This research was supported inpart by NSF grant 
LIS SBR 9720368. Thanks to Marc Light and the 
group at MITRE Corporation for providing the on- 
line versions of the reading comprehension material 
and the Brown Laboratory for Linguistic Informa- 
tion Processing (BLLIP) for providing the parsed 
and pronoun referenced versions. 
same. The material encompasses four grade 
levels - -  third through sixth. Each grade 
levels consists of thirty stories plus five ques- 
tions for each story. Each story has the form 
of a newspaper article, including a title and 
dateline. Following \[3\], we used grades three 
and six as our development corpus and four 
and five for testing. 
The questions on each story are typically 
one each of the "who, what, where, why, and 
when" varieties. The Deep Read group an- 
swered these questions by finding the sen- 
tence in the story that best answers the 
question. One of the marked up versions 
they provide indicates those sentences Titles 
and datelines are also considered possible an- 
swers to the questions. In about 10% of the 
cases Deep Read judged no sentence stand- 
ing on its own to be a good answer. In these 
cases no answer to the question is considered 
correct. In a few cases more than one answer 
is acceptable and all of them are so marked. 
Deep Read also provided a version with 
person/place/time arkings inserted auto- 
matically by the Alembic named-entity s s- 
tem \[4\]. Henceforth we refer to this as NE 
(named entity) material. As discussed be- 
low, these markings are quite useful. In addi- 
tion to the mark-ups provided by Deep Read, 
the groups were also g~ven a machine anno- 
tated version with full parse trees and pro- 
noun coreference. 
The Deep Read group suggests everal 
different metrics for judging the perfor- 
mance of reading-comprehension-question- 
answering programs. However, their data 
show that the performance of theii: programs 
goes up and down on all of the metrics in 
Methods  Resu l ts  
1 Best of Deep Read 36 
2 BOW Stem Coref Class 37 
3 BOV Stem NE Coref Tfidf Subj Why MainV 38 
4 BOV Stem NE Defaults Coref 38 
5 BOV Stem NE Defaults Qspecific 41 
BOW 
BOV 
Coref 
Class 
Defaults 
MainV 
NE 
Qspecific 
Subj 
Tfidf 
Why 
bag-of-words 
bag-of-verbs 
pronoun coreference 
Word-Net class membership 
Defaults from Figure 3 
Extra credit for main verb match 
named entity 
Specific techniques for all question types 
Prefer sentences with same subject 
term frequency times inverse document frequency 
Specific good words for "why" questions 
Figure 1: Some notable results 
tandem. We implemented several of those 
metrics ourselves, but to keep things sim- 
ple we only report results on one of them - 
how often (in percent) the program answers 
a question by choosing a correct sentence (as 
judged in the answer mark-ups). Following 
\[3\] we refer to this as the "humsent" (hu- 
man annotated sentence) metric. Note that 
if more than one sentence is marked as ac- 
ceptable, a program response of any of those 
sentences i considered correct. If no sen- 
tence is marked, the program cannot get the 
answer correct, so there is an upper bound of 
approximately 90% accuracy for this metric. 
The results were both en- and dis- 
couraging. On the encouraging side, three 
of the four groups were able to improve, at 
least somewhat, on the previous best results. 
On the other hand, the extra annotation 
we provided (machine-generated parses of all 
the sentences \[1\] and machine-generated pro- 
noun coreference information \[2\]) proved of 
limited utility. 
2 Resu l ts  
Figure 1 shows four of the results that bet- 
tered those of Deep Read. In the next sec- 
tion we discuss the techniques used in these 
programs. 
The performance of all the programs var- 
ied widely depending on the type of ques- 
tion to be answered. In particular, "why" 
questions proved the most difficult. (Deep 
Read observed the same phenomenon.) In 
Figure 2 we break down the results for sys- 
tem 3 in Figure 1 according to question type. 
This system was able to answer only 22% 
of the "why" questions correctly. Program 
5, which had the most complicated scheme 
for handling "why" questions, answered 26% 
correctly. 
3 D iscuss ion  
As noted above, the early phase of the 
project was concerned with replicating the 
Deep Read results. This we were able to 
do, although generally only to about 1.5 sig- 
nificant digits. It seems that one can get 
swings of several percentage points in per- 
formance just depending on, say, how one 
2 
Question Type Percent Correct 
When 32 
Where 50 
Who 57 
What 32 
Why 22 
Figure 2: Results by question type 
resolves ties in the bag-of-words cores, or 
whether one. considers capitalized and un- 
capitalized words the same. However, the 
numbers our groups got were in the same 
ballpark and, more importantly, the trends 
we found in the numbers were the same. For 
example, stemming helped a little, stop-lists 
actually hurt a very small amount, and the 
use of named-entity data gave the biggest 
single improvement of the various Deep Read 
techniques. 
We found two variations on bag-of-words 
that improved results both individually and 
when combined. The first of these is the 
"bag of verbs" (BOV) technique. In this 
scheme one first measures imilarity by do- 
ing bag-of-words, but looking only at verbs 
(obtained from the machine-generated parse 
trees we provided). If two sentences tied 
on BOV, then bag-of-words is used as a tie- 
breaker. As the usefulness of this technique 
was shown early in the project, all of the 
groups tried it. It seems to provide two or 
three percentage-point mprovement in a va- 
riety of circumstances. Most of our best re- 
sults were obtained when using this tech- 
nique. A further refinement of this tech- 
nique is to weight matching main verbs more 
highly. This is used in system 3. 
One group explored the idea of replacing 
bag-of-words with a scheme based upon the 
standard ocument-retrieval "tfidf" method. 
Document retrieval has long used a bag- 
of-words technique, in which the words are 
given different weights. So if our query has 
words wl...wn, the frequency of the word i in 
document in question is fi, and the number 
of documents that have word i is n, then the 
score for this document is 
L ~i (1) 
i=l  n i  
That is, we take the term frequency (tf = fi) 
times the inverse document frequency (idf = 
1/ni) and sum over the words in the query. 
Of course, our application is sentence re- 
trieval, not document retrieval, so we define 
term frequency as the number of times the 
word appears in the candidate sentence, and 
document frequency as the number of sen- 
tences in which this word appears. (If we 
use stemming, then this applies to stemmed 
words.) Replacing BOW (OR BOV) by 
tfidf gives a three to six percentage-point 
improvement, depending on the other tech- 
niques with which it is combined. This is 
somewhat surprising because, as stated ear- 
lier, stop-lists were observed both by Deep 
Read and ourselves to have a slight negative 
impact on performance. One might think 
that the tfidf scheme should have something 
like the same impact, as the words on the 
stop-list are exactly those that occur in many 
sentences on average, and thus ones whoes 
impact will be attenuated in tfidL That tfidf 
is nevertheless uccessful suggests (perhaps) 
that the words on the stop-lists are useful 
for settling ties, a situation where even the 
attenuated value provided in tfidf will work 
just fine. It may also be the case that it 
is useful to distinguish between those words 
that are more common and those that are 
less common, even though neither appear on 
the stop-list. 
The best results, however, were obtained 
by creating question-answering strategies for 
specific question types (who, what, where, 
why, when). For example, one simple strat- 
egy assigns a default answer to each ques- 
tion type (in case all of the other strategies 
produce a tie) and zero or more sentence lo- 
cations that should be eliminated from con- 
sideration (before any of the other strategies 
are used). The particulars of this "Defaults" 
strategy are shown in Figure 3. 
There were more complicated question- 
type strategies as well. As already noted, 
3 
Question Type Default Eliminate 
Who title dateline 
What 1st story line (none) 
When dateline (none) 
Where dateline title 
Why 1st story line title, 
dateline 
Figure 3: Default and eliminable sentences 
in the "Default" strategy 
"why" questions are the most difficult for 
bag-of-words. The reason is fairly intuitive. 
"Why" questions are of the form "Why did 
such-and-such happen?" Bag-of-words typ- 
ically finds a sentence of the form "Such 
and such happened." The following strategy 
makes use of the fact that the answer to the 
"why" question is often either the sentence 
preceding or following the sentence that de- 
scribes the event. 
If the first NP (noun-phrase) in the sen- 
tence following the match is a pronoun, 
choose that sentence: 
Q: Why did Chris write two books 
of his own? 
match: He has written two books 
of his own. 
A: They tell what it is like to be 
famous. 
If that rule does not apply, then if the first 
word of the matching sentence is "this", 
"that," "these" or "those", select the pre- 
vious sentence: 
Q: Why did Wang once get upset? 
A: When she was a little girl, her 
art teacher didn't like her paint- 
ings. 
match: This upset Wang. 
Finally, if neither of the above two rules ap- 
plies, look for sentences that have the follow- 
ing words and phrases (and morphological 
variants) which tend to answer why ques- 
tions: "show", "explain", "because", "no 
one knows", and "if so". If there is more 
than one such sentence, use bag-of-~words to 
decide between them: 
4 
Q: Why does Greenland have 
strange seasons? 
A: Because it is far north, it has 
four months of sunlight each year. 
A lot of the question-type-specific rules 
use the parse of the sentence to select key 
words that are more important matches than 
other words of the sentence. For example, 
"where" questions tended to come in two va- 
rieties: "Where AUX NP VP" (e.g., "Where 
did Fred find the dog?") and "Where AUX 
NP." (e.g:, "Where is the dog?"). In both 
cases the words of the NP are important o 
match, and in the first case the (stemmed) 
main verb of the VP is important. Also, sen- 
tences that have PPs (prepositional phrases) 
with a preposition that often indicates loca- 
tion (e.g., "in," "near," etc.) are given a 
boost by the weighting scheme. 
4 Conclusion 
We have briefly discussed several reading 
comprehension systems that are able to im- 
prove on the results of \[3\]. While these are 
positive results, many of the lessons learned 
in this exercise are more negative. In par- 
ticular, while the NE data clearly helped 
a few percent, most of the extra syntactic 
and semantic annotations (i.e., parsing and 
coreference) were either of very small utility, 
or their utility came about in idiosyncratic 
ways. For example, probably the biggest im- 
pact of the parsing data was that it allowed 
people to experiment with the bag-of-verbs 
technique. Also, the parse trees served as the 
language for describing very question spe- 
cific techniques, uch as the ones for "where" 
questions presented in the previous section. 
Thus our tentative conclusion is that we 
are still not at a point that a task like chil- 
dren's reading comprehension tests is a good 
testing ground for NLP techniques. To the 
extent that these standard techniques are 
useful, it seems to be only in conjunction 
with other methods that are more directly 
aimed at the task. 
Of course, this is not to say that some- 
one else will not come up with better syntac- 
tic/semantic annotations that more directly 
lead to improvements on such tests. We can 
only say that so far we have not been able 
to do so. 
References 
1. CHARNIAK, E. A maximum-entropy- 
inspired parser. In Proceedings of the 
2000 Conference of the North American 
Chapter of the Assocation for Computa- 
tional Linguistics. ACL, New Brunswick 
N J, 2000. 
2. GE, N.,-HALE, J. AND CHARNIAK, E. 
A statistical approach to anaphora reso- 
lution. In Proceedings of the Sixth Work- 
shop on Very Large Corpora. 1998, 161- 
171. 
3. HIRSCHMAN, L., LIGHT, M., BRECK, E. 
AND BURGER, J. D. Deep read: a read- 
ing comprehension system. In Proceedings 
of the ACL 1999. ACL, New Brunswick, 
N J, 1999, 325-332. 
4. VILAIN, M. AND DAY, D. Finite-state 
parsing by rule sequences. In Interna- 
tional Conferences on ComputationM Lin- 
guistics (COLING-96). The International 
Conmmittee on Computational Linguis- 
tics, 1996. 
5 
Parsing and Disuency Placement
Donald Engel
y
and Eugene Charniak
z
and Mark Johnson
z
Department of Physics, University of Pennsylvania
y
Brown Laboratory for Linguistic Information Processing
z
Brown University
Abstract
It has been suggested that some forms of speech
disuencies, most notable interjections and par-
entheticals, tend to occur disproportionally at
major clause boundaries [6] and thus might
serve to aid parsers in establishing these bound-
aries. We have tested a current statistical parser
[1] on Switchboard text with and without inter-
jections and parentheticals and found that the
parser performed better when not faced with
these extra phenomena. This suggest that for
current parsers, at least, interjection and paren-
thetical placement does not help in the parsing
process.
1 Introduction
It is generally recognized that punctuation
helps in parsing text. For example, Roark [5]
nds that removing punctuation decreases his
parser's accuracy from 86.6% to 83.8%. Our
experiments with the parser described in [1]
show a similar fallo. Unfortunately spoken
English does not come with punctuation, and
even when transcriptions add punctuation, as in
the Switchboard [4] corpus of transcribed (and
parsed) telephone calls, it's utility is small [5]
For this and other reasons there is considerable
interest in nding other aspects of speech that
might serve as a replacement.
One suggestion in this vein is that the place-
ment of some forms of speech errors might
encode useful linguistic information. Speech,
of course, contains many kinds of errors that
can make it more di?cult to parse than text.
Roughly speaking the previously mentioned
Switchboard corpus distinguishes three kinds of
errors:
 interjections (lled pauses) | \I, um, want
to leave"
 parentheticals | \I, you know, want to
leave"
 speech repairs | \I can, I want to leave"
Of these, speech repairs are the most injurious
to parsing. Furthermore, even if one's parser
can parse the sentence as it stands, that is not
su?cient. For example, in \I can, I want to
leave", it is not necessarily the case that the
speaker believes that he or she can, in fact,
leave, only that he or she wants to leave. Thus
in [2] speech repairs were rst detected in a sep-
arate module, and deleted before handing the
remaining text to the parser. The parser then
produced a parse of the text without the re-
paired section.
The other two kinds of errors, interjec-
tions, and parentheticals, (henceforth INTJs
and PRNs) are less problematic. In particular,
if they are left in the text either their seman-
tic content is compatible with the rest of the
utterance or there is no semantic content at all.
For example, Table 1 gives the 40 most common
INTJs, which comprise 97% of the total. (Un-
listed INTJs comprise the remaining 3%.) They
are easily recognized as not carrying much, if
any, content.
PRNs are more diverse. Table 2 lists the 40
most common PRNs. They only comprise 65%
of all cases, and many do contain semantics
content. In such cases, however, the semantic
content is compatible with the rest of the sen-
tence, so leaving them in is perfectly acceptable.
Thus [2], while endeavoring to detect and re-
move speech repairs, left interjections and par-
entheticals in the text for the parser to cope
with.
Indeed [6] nds that both interjections and
parentheticals tend to occur at major sentence
boundaries. Also [7] suggest that this prop-
                                            Association for Computational Linguistics.
                      Language Processing (EMNLP), Philadelphia, July 2002, pp. 49-54.
                         Proceedings of the Conference on Empirical Methods in Natural
Phrase Num. of Percent
INTJs
uh 17609 27.44
yeah 11310 17.62
uh-huh 7687 11.97
well 5287 8.238
um 3563 5.552
oh 2935 4.573
right 2873 4.477
like 1772 2.761
no 1246 1.941
okay 1237 1.927
yes 982 1.530
so 651 1.014
oh yeah 638 0.994
huh 558 0.869
now 410 0.638
really 279 0.434
sure 276 0.430
oh okay 269 0.419
see 261 0.406
oh really 260 0.405
huh-uh 185 0.288
wow 174 0.271
bye-bye 174 0.271
exactly 156 0.243
all right 146 0.227
yep 115 0.179
boy 111 0.172
oh no 102 0.158
bye 98 0.152
well yeah 91 0.141
gosh 91 0.141
oh gosh 88 0.137
oh yes 84 0.130
hey 75 0.116
uh yeah 71 0.110
anyway 71 0.110
oh uh-huh 70 0.109
say 63 0.098
oh goodness 61 0.095
uh no 56 0.087
Table 1: The 40 Most Common Interjections
Phrase Num. of Percent
PRNs
you know 431 37.02
I mean 105 9.020
I think 86 7.388
I guess 67 5.756
You know 44 3.780
I don't know 38 3.264
let's see 11 0.945
I I mean 10 0.859
I 'd say 9 0.773
I 'm sure 7 0.601
excuse me 6 0.515
what is it 6 0.515
I would say 5 0.429
you you know 5 0.429
let 's say 5 0.429
I think it 's 4 0.343
I 'm sorry 4 0.343
so to speak 3 0.257
I guess it 's 3 0.257
I don't think 3 0.257
I think it was 3 0.257
I would think 3 0.257
it seems 3 0.257
I guess it was 2 0.171
I know 2 0.171
I I I mean 2 0.171
seems like 2 0.171
Shall we say 2 0.171
I guess you could say 2 0.171
You're right 2 0.171
I believe 2 0.171
I think it was uh 2 0.171
I say 2 0.171
What I call 2 0.171
I don't know what part of
New Jersey you're in but 2 0.171
I should say 2 0.171
I guess not a sore thumb 1 0.085
I 'm trying to think 1 0.085
And it's hard to drag
her away 1 0.085
I don't know what you
call that 1 0.085
Table 2: The 40 Most Common Parentheticals
erty accounts for their observation that remov-
ing these disuencies does not help in language
modeling perplexity results. This strongly sug-
gests that INTJ/PRN location information in
speech text might in fact, improve parsing per-
formance by helping the parser locate con-
stituent boundaries with high accuracy. That is,
a statistic parser such as [1] or [3] when trained
on parsed Switchboard text with these phenom-
ena left in, might learn the statistical correla-
tions between them and phrase boundaries just
as they are obviously learning the correlations
between punctuation and phrase boundaries in
written text.
In this paper then we wish to determine if the
presence of INTJs and PRNs do help parsing, at
least for one state-of-the-art statistical parser
[1].
2 Experimental Design
The experimental design used was more com-
plicated than we initially expected. We had an-
ticipated that the experiments would be con-
ducted analogously to the \no punctuation" ex-
periments previously mentioned. In those ex-
periments one removes punctuation from all of
the corpus sentences, both for testing and train-
ing, and then one reports the results before and
after this removal. (Note that one must remove
punctuation from the training data as well so
that it looks like the non-punctuated testing
data it receives.) Parsing accuracy was mea-
sured in the usual way, using labeled precision
recall. Note, however, and this is a critical
point, that precision and recall are only mea-
sured on non-preterminal constituents. That is,
if we have a constituent
(PP (IN of)
(NP (DT the) (NN book)))
our measurements would note if we correctly
found the PP and the NP, but not the preter-
minals IN, DT, and NN. The logic of this is
to avoid confusing parsing results with part-of-
speech tagging, a much simpler problem.
Initially we conducted similarly designed ex-
periments, except rather than removing punc-
tuation, we removed INTJs and PRNs and com-
pared before and after precision/recall numbers.
These numbers seemed to conrm the antici-
pated results: the \after" numbers, the numbers
without INTJ/PRNs were signicantly worse,
suggesting that the presence of INTJ/PRNs
helped the parser.
Unfortunately, although ne for punctuation,
this experimental design is not su?cient for
measuring the eects of INTJ/PRNs on parsing.
The dierence is that punctuation itself is not
measured in the precision-recall numbers. That
is, if we had a phrase like
(NP (NP (DT a) (NN sentence))
(, ,)
(ADJP (JJ like)
(NP (DT this) (DT one))))
we would measure our accuracy on the three
NP's and the ADJP, but not on the pretermi-
nals, and it is only at the preterminal level that
punctuation appears.
The same cannot be said for INTJ/PRNs.
Consider the (slightly simplied) Switchboard
parse for a sentence like \I, you know, want to
leave":
(S (NP I)
(PRN , you know ,)
(VP want (S to leave)))
The parenthetical PRN is a full non-terminal
and thus is counted in precision/recall measure-
ments. Thus removing preterminals is chang-
ing what we wish to measure. In particu-
lar, when our initial results showed that re-
moval of INTJ/PRNs lowered precision/recall we
worried that it might be that INTJ/PRNs are
particularly easy to parse, and thus removing
them made things worse, not because of col-
lateral damage on our ability to parse other
constituents, but simply because we removed
a body of easily parseable constituents, leaving
the more di?cult constituents to be measured.
The above tables of INTJs and PRNs lends cre-
dence to this concern.
Thus in the experiments below all measure-
ments are obtained in the following fashion:
1. The parser is trained on switchboard data
with/without INTJ/PRNs or punctuation,
creating eight congurations: 4 for neither,
both, just INTJs, and just PRNs, times
two for with and without punctuation. We
tested with and without punctuation to
conrm Roark's earlier results showing that
they have little inuence in Switchboard
text.
2. The parser reads the gold standard testing
examples and depending on the congura-
tion INTJs and/or PRNS are removed from
the gold standard parse.
3. Finally the resulting parse is compared
with the gold standard. However, any re-
maining PRNs or INTJs are ignored when
computing the precision and recall statis-
tics for the parse.
To expand a bit on point (3) above, for an
experiment where we are parsing with INTJs,
but not PRNs, the resulting parse will, of course,
contain INTJs, but (a) they are not counted as
present in the gold standard (so we do not aect
recall statistics), and (b) they are not evaluated
in the guessed parse (so if one were labeled, say,
an S, it would not be counted against the parse).
The intent, again, is to not allow the results to
be inuenced by the fact that interjections and
parentheticals are much easier to nd than most
(if not) all other kinds of constituents.
3 Experimental Results
As in [2] the Switchboard parsed/merged cor-
pus directories two and three were used for
training. In directory four, les sw4004.mrg
to sw4153.mrg were used for testing, and
sw4519.mrg to sw4936 for development. To
avoid confounding the results with problems of
edit detection, all edited nodes were deleted
from the gold standard parses.
The results of the experiment are given in
table 3. We have shown results separately
with and without punctuation. A quick look
at the data indicates that both sets show the
same trends but with punctuation helping per-
formance by about 1.0% absolute in both pre-
cision and recall. Within both groups, as is al-
ways the case, we see that the parser does better
when restricted to shorter sentences (40 words
and punctuation or less). We see that removing
PRNs or INTJs separately both improve parsing
accuracy (e.g., from 87.201% to 87.845|that
the eect of removing both is approximately
additive (e.g., from 87201% to 88.863%, again
on the with-punctuation data). Both with and
without punctuation results hint that removing
Punc. PRN INTJ Sentences Sentences
 40  100
+ + + 88.93 87.20
+ + - 89.44 87.85
+ - + 89.13 87.99
+ - - 90.00 88.86
- + + 87.40 86.23
- + - 88.0 86.8
- - + 88.41 87.45
- - - 89.13 88.30
Table 3: Average of labeled precision/recall
data for parsing with/without parentheti-
cals/interjections
parentheticals was usually more helpful than re-
moving interjections. However in one case the
reverse was true (with-punctuation, sentences
 40) and in all cases the dierences are at or
under the edge of statistical reliability. In con-
trast, the dierences between removing neither,
removing one, or removing both INJs and PRNs
are quite comfortably statistically reliable.
4 Discussion
Based upon Tabel 3 our tentative conclusion is
that the information present in parentheticals
and interjections does not help parsing. There
are, however, reasons that this is a tentative con-
clusion.
First, in our eort to prevent the ease of
recognizing these constructions from giving an
unfair advantage to the parser when they are
present, it could be argued that we have given
the parser an unfair advantage when they are
absent. After all, even if these constructions are
easily recognized, the parser is not perfect on
them. While our labeled precision/recall mea-
surements are made in such a way that a mis-
take in the label of, say, an interjection, would
not eect the results, a mistake on it's position
typically would have an eect because the po-
sitions of constituents either before or after it
would be made incorrect. Thus the parser has
a harder task set for it when these constituents
are left in.
It would be preferable to have an experimen-
tal design that would somehow equalize things,
but we have been unable to nd one. Fur-
thermore it is instructive to contrast this situ-
ation with that of punctuation in Wall Street
Journal text. If we had found that parsing
without punctuation made things easier a sim-
ilar argument could be made that the without-
punctuation case was given an unfair advantage
since it had a lot fewer things to worry about.
But punctuation in well-edited text contains
more than enough information to overcome the
disadvantage. This does not seem to be the case
with INTJs and PRNs. Here the net information
content here seems to be negative.
A second, and in our estimation more serious,
objection to our conclusion is that we have only
done the experiment with one parser. Perhaps
there is something specic to this parser that
systematically underestimates the usefulness of
INTJ/PRN information. While we feel reason-
ably condent that any other current parser
would nd similar eects, it is at least possi-
ble to imagine that quite dierent parsers might
not. Statistical parsers condition the probabil-
ity of a constituent on the types of neighbor-
ing constituents. Interjections and parenthet-
icals have the eect of increasing the kinds of
neighbors one might have, thus splitting the
data and making it less reliable. The same is
true for punctuation, of course, but it seems
plausible that well edited punctuation is su?-
ciently regular that this problem is not too bad,
while spontaneous interjections and parentheti-
cals would not be so regular. Of course, nding
a parser design that might overcome this prob-
lem (assuming that this is the problem) is far
from obvious.
5 Conclusion
We have tested a current statistical parser [1] on
Switchboard text with and without interjections
and parentheticals and found that the parser
performs better when not faced with these ex-
tra phenomena. This suggest that for current
parsers, at least, interjection and parenthetical
placement does not help in the parsing process.
This is, of course, a disappointing result. The
phenomena are not going to go away, and what
this means is that there is probably no silver
lining.
We should also note that the idea that they
might help parsing grew from the observation
that interjections and parentheticals typically
occur at major clause boundaries. One might
then ask if our results cast any doubt on this
claim as well. We do not think so. Interjections
and parentheticals do tend to identify clause
boundaries. The problem is that many other
things do so as well, most notably normal gram-
matical word ordering. The question is whether
the information content of disuency placement
is su?cient to overcome the disruption of word
ordering that it entails. The answer, for current
parsers at least, seems to be "no".
6 Acknowledgements
We would like to acknowledge the members of
the Brown Laboratory for Linguistic Informa-
tion Processing, This research has been sup-
ported in part by NSF grants IIS 0085940, IIS
0112435, and DGE 9870676.
References
1. Charniak, E. A maximum-entropy-
inspired parser. In Proceedings of the 2000
Conference of the North American Chapter
of the Association for Computational Lin-
guistics. ACL, New Brunswick NJ, 2000,
132{139.
2. Charniak, E. and Johnson, M. Edit De-
tection and Parsing for Transcribed Speech.
In Proceedings of the North American As-
socation for Computational Linguistics 2001.
2001, 118{126.
3. Collins, M. J. Three generative lexical-
ized models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the
ACL. 1997, 16{23.
4. Godfrey, J. J., Holliman, E. C. and
McDaniel, J. SWITCHBOARD: Tele-
phone speech corpus for research and
development. . In Proceedings IEEE Con-
ference on Acoustics, Speech and Signal
Processing . San Francisco, 1992, 517{520 .
5. Roark, B. Robust Probabilistic Predictive
Syntactic Processing: Motivations, Models,
and Applications. In Ph.D. thesis. Depart-
ment of Cognitive Science, Brown University,
Providence, RI, 2001.
6. Shriberg, E. E. Preliminaries to a The-
ory of Speech Disuencies. In Ph.D. Disser-
tation. Department of Psychology, University
of California-Berkeley, 1994.
7. Stolcke, A. and Shriberg, E. Auto-
matic linguistic segmantation of conversa-
tional speech. In Proceedings of the 4th In-
ternational Conference on Spoken Language
Processing (ICSLP-96). 1996.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 301?307,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning Phrasal Categories
William P. Headden III, Eugene Charniak and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{headdenw|ec|mj}@cs.brown.edu
Abstract
In this work we learn clusters of contex-
tual annotations for non-terminals in the
Penn Treebank. Perhaps the best way
to think about this problem is to contrast
our work with that of Klein and Man-
ning (2003). That research used tree-
transformations to create various gram-
mars with different contextual annotations
on the non-terminals. These grammars
were then used in conjunction with a CKY
parser. The authors explored the space
of different annotation combinations by
hand. Here we try to automate the process
? to learn the ?right? combination auto-
matically. Our results are not quite as good
as those carefully created by hand, but they
are close (84.8 vs 85.7).
1 Introduction and Previous Research
It is by now commonplace knowledge that accu-
rate syntactic parsing is not possible given only
a context-free grammar with standard Penn Tree-
bank (Marcus et al, 1993) labels (e.g., S, NP ,
etc.) (Charniak, 1996). Instead researchers
condition parsing decisions on many other fea-
tures, such as parent phrase-marker, and, fa-
mously, the lexical-head of the phrase (Mager-
man, 1995; Collins, 1996; Collins, 1997; Johnson,
1998; Charniak, 2000; Henderson, 2003; Klein
and Manning, 2003; Matsuzaki et al, 2005) (and
others).
One particularly perspicuous way to view the
use of extra conditioning information is that of
tree-transformation (Johnson, 1998; Klein and
Manning, 2003). Rather than imagining the parser
roaming around the tree for picking up the infor-
mation it needs, we rather relabel the nodes to di-
rectly encode this information. Thus rather than
have the parser ?look? to find out that, say, the
parent of some NP is an S, we simply relabel the
NP as an NP [S].
This viewpoint is even more compelling if one
does not intend to smooth the probabilities. For
example, consider p(NP ? PRN | NP [S]) If
we have no intention of backing off this probabil-
ity to p(NP ? PRN | NP ) we can treat NP [S]
as an uninterpreted phrasal category and run all
of the standard PCFG algorithms without change.
The result is a vastly simplified parser. This is ex-
actly what is done by Klein and Manning (2003).
Thus the ?phrasal categories? of our title refer
to these new, hybrid categories, such as NP [S].
We hope to learn which of these categories work
best given that they cannot be made too specific
because that would create sparse data problems.
The Klein and Manning (2003) parser is an un-
lexicalized PCFG with various carefully selected
context annotations. Their model uses some par-
ent annotations, and marks nodes which initiate or
in certain cases conclude unary productions. They
also propose linguistically motivated annotations
for several tags, including V P , IN , CC ,NP and
S. This results in a reasonably accurate unlexical-
ized PCFG parser.
The downside of this approach is that their fea-
tures are very specific, applying different annota-
tions to different treebank nonterminals. For in-
stance, they mark right-recursive NP s and not
V P s (i.e., an NP which is the right-most child
of another NP ). This is because data sparsity is-
sues preclude annotating the nodes in the treebank
too liberally. The goal of our work is to automate
the process a bit, by annotating with more general
features that apply broadly, and by learning clus-
301
ters of these annotations.
Mohri and Roark (2006) tackle this problem by
searching for what they call ?structural zeros?or
sets of events which are individually very likely,
but are unlikely to coincide. This is to be con-
trasted with sets of events that do not appear to-
gether simply because of sparse data. They con-
sider a variety of statistical tests to decide whether
a joint event is a structural zero. They mark the
highest scoring nonterminals that are part of these
joint events in the treebank, and use the resulting
PCFG.
Coming to this problem from the standpoint of
tree transformation, we naturally view our work
as a descendent of Johnson (1998) and Klein and
Manning (2003). In retrospect, however, there are
perhaps even greater similarities to that of (Mager-
man, 1995; Henderson, 2003; Matsuzaki et al,
2005). Consider the approach of Matsuzaki et al
(2005). They posit a series of latent annotations
for each nonterminal, and learn a grammar using
an EM algorithm similar to the inside-outside al-
gorithm. Their approach, however, requires the
number of annotations to be specified ahead of
time, and assigns the same number of annotations
to each treebank nonterminal. We would like to
infer the number of annotations for each nonter-
minal automatically.
However, again in retrospect, it is in the work of
Magerman (1995) that we see the greatest similar-
ity. Rather than talking about clustering nodes, as
we do, Magerman creates a decision tree, but the
differences between clustering and decision trees
are small. Perhaps a more substantial difference
is that by not casting his problem as one of learn-
ing phrasal categories Magerman loses all of the
free PCFG technology that we can leverage. For
instance, Magerman must use heuristic search to
find his parses and incurs search errors because of
it. We use an efficient CKY algorithm to do ex-
haustive search in reasonable time.
Belz (2002) considers the problem in a man-
ner more similar to our approach. Beginning with
both a non-annotated grammar and a parent anno-
tated grammar, using a beam search they search
the space of grammars which can be attained via
merging nonterminals. They guide the search us-
ing the performance on parsing (and several other
tasks) of the grammar at each stage in the search.
In contrast, our approach explores the space of
grammars by starting with few nonterminals and
splitting them. We also consider a much wider
range of contextual information than just parent
phrase-markers.
2 Background
A PCFG is a tuple (V,M,?0, R, q : R ? [0, 1]),
where V is a set of terminal symbols; M = {?i}
is a set of nonterminal symbols; ?0 is a start or
root symbol; R is a set of productions of the form
?i ? ?, where ? is a sequence of terminals and
nonterminals; and q is a family of probability dis-
tributions over rules conditioned on each rule?s
left-hand side.
As in (Johnson, 1998) and (Klein and Man-
ning, 2003), we annotate the Penn treebank non-
terminals with various context information. Sup-
pose ? is a Treebank non-terminal. Let ? = ?[?]
denote the non-terminal category annotated with a
vector of context features ?. A PCFG is derived
from the trees in the usual manner, with produc-
tion rules taken directly from the annotated trees,
and the probability of an annotated rule q(? ?
?) = C(???)C(?) where C(? ? ?) and C(?) are the
number of observations of the production and its
left hand side, respectively.
We refer to the grammar resulting from extract-
ing annotated productions directly out of the tree-
bank as the base grammar.
Our goal is to partition the set of annotated non-
terminals into clusters ? = {?i}. Each possible
clustering corresponds to a PCFG, with the set of
non-terminals corresponding to the set of clusters.
The probability of a production under this PCFG
is
p(?i ? ?j?k) =
C(?i ? ?j?k)
C(?i)
where ?s ? ? are clusters of annotated non-
terminals and where:
C(?i ? ?j?k . . .) =
?
(?i,?j ,?k...)??i??j??k...C(?i ? ?j?k . . .)
We refer to the PCFG of some clustering as the
clustered grammar.
2.1 Features
Most of the features we use are fairly standard.
These include the label of the parent and grand-
parent of a node, its lexical head, and the part of
speech of the head.
Klein and Manning (2003) find marking non-
terminals which have unary rewrites to be helpful.
302
They also find useful annotating two preterminals
(DT ,RB) if they are the product of a unary pro-
duction. We generalize this via two width features:
the first marking a node with the number of non-
terminals to which it rewrites; the second marking
each preterminal with the width of its parent.
Another feature is the span of a nonterminal, or
the number of terminals it dominates, which we
normalize by dividing by the length of the sen-
tence. Hence preterminals have normalized spans
of 1/(length of the sentence), while the root has a
normalized span of 1.
Extending on the notion of a Base NP, intro-
duced by Collins (1996), we mark any nonter-
minal that dominates only preterminals as Base.
Collins inserts a unary NP over any base NPs with-
out NP parents. However, Klein and Manning
(2003) find that this hurts performance relative to
just marking the NPs, and so our Base feature does
not insert.
We have two features describing a node?s posi-
tion in the expansion of its parent. The first, which
we call the inside position, specifies the nonter-
minal?s position relative to the heir of its parent?s
head, (to the left or right) or whether the nontermi-
nal is the heir. (By ?heir? we mean the constituent
donates its head, e.g. the heir of an S is typically
the V P under the S.) The second feature, outside
position, specifies the nonterminal?s position rel-
ative to the boundary of the constituent: it is the
leftmost child, the rightmost child, or neither.
Related to this, we further noticed that several
of Klein & Manning?s (2003) features, such as
marking NP s as right recursive or possessive have
the property of annotating with the label of the
rightmost child (when they are NP and POS re-
spectively). We generalize this by marking all
nodes both with their rightmost child and (an anal-
ogous feature) leftmost child.
We also mark whether or not a node borders
the end of a sentence, save for ending punctuation.
(For instance, in this sentence, all the constituents
with the second ?marked? rightmost in their span
would be marked).
Another Klein and Manning (2003) feature we
try includes the temporal NP feature, where TMP
markings in the treebank are retained, and propa-
gated down the head inheritance path of the tree.
It is worth mentioning that all the features here
come directly from the treebank. For instance, the
part of speech of the head feature has values only
from the raw treebank tag set. When a preterminal
cluster is split, this assignment does not change the
value of this feature.
3 Clustering
The input to the clusterer is a set of annotated
grammar productions and counts. Our clustering
algorithm is a divisive one reminiscent of (Martin
et al, 1995). We start with a single cluster for each
Treebank nonterminal and one additional cluster
for intermediate nodes, which are described in sec-
tion 3.2.
The clustering method has two interleaved
parts: one in which candidate splits are generated,
and one in which we choose a candidate split to
enact.
For each of the initial clusters, we generate a
candidate split, and place that split in a prior-
ity queue. The priority queue is ordered by the
Bayesian Information Criterion (BIC), e.g.(Hastie
et al, 2003).
The BIC of a model M is defined as -2*(log
likelihood of the data according to M ) +dM*(log
number of observations). dM is the number of de-
grees of freedom in the model, which for a PCFG
is the number of productions minus the number
of nonterminals. Thus in this context BIC can be
thought of as optimizing the likelihood, but with a
penalty against grammars with many rules.
While the queue is nonempty, we remove a can-
didate split to reevaluate. Reevaluation is neces-
sary because, if there is a delay between when a
split is proposed and when a split is enacted, the
grammar used to score the split will have changed.
However, we suppose that the old score is close
enough to be a reasonable ordering measure for
the priority queue. If the reevaluated candidate is
no longer better than the second candidate on the
queue, we reinsert it and continue. However, if it
is still the best on the queue, and it improves the
model, we enact the split; otherwise it is discarded.
When a split is enacted, the old cluster is re-
moved from the set of nonterminals, and is re-
placed with the two new nonterminals of the split.
A candidate split for each of the two new clusters
is generated, and placed on the priority queue.
This process of reevaluation, enacting splits,
and generating new candidates continues until the
priority queue is empty of potential splits.
We select a candidate split of a particular cluster
as follows. For each context feature we generate
303
S^ROOT
NP^S
NNP^NP
Rex
CC^NP
and
NNP^NP
Ginger
VP^S
VBD^VP
ran
NP^VP
NN
home
Figure 1: A Parent annotated tree.
a potential nominee split. To do this we first par-
tition randomly the values for the feature into two
buckets. We then repeatedly try to move values
from one bucket to the other. If doing so results
in an improvement to the likelihood of the training
data, we keep the change, otherwise we reject it.
The swapping continues until moving no individ-
ual value results in an improvement in likelihood.
Suppose we have a grammar derived from a cor-
pus of a single tree, whose nodes have been anno-
tated with their parent as in Figure 1. The base
productions for this corpus are:
S[ROOT ] ? NP [S] V P [S] 1/1
V P [S] ? V BD[V P ] NP [V P ] 1/1
NP [S] ? NP [NP ] CC[NP ] NP [NP ] 1/1
NP [V P ] ? NN [NP ] 1/1
NP [NP ] ? NNP [NP ] 2/2
Suppose we are in the initial state, with a single
cluster for each treebank nonterminal. Consider
a potential split of the NP cluster on the par-
ent feature, which in this example has three val-
ues: S, V P , and NP . If the S and V P val-
ues are grouped together in the left bucket, and
the NP value is alone in the right bucket, we get
cluster nonterminals NPL = {NP [S], NP [V P ]}
and NPR = {NP [NP ]}. The resulting grammar
rules and their probabilities are:
S ? NPL V P 1/1
V P ? V BD NPL 1/1
NPL ? NPR CC NPR 1/2
NPL ? NN 1/2
NPR ? NNP 2/2
If however, V P is swapped to the right bucket
with NP , the rules become:
S ? NPL V P 1/1
V P ? V BD NPR 1/1
NPL ? NPR CC NPR 1/1
NPR ? NN 1/3
NPR ? NNP 2/3
The likelihood of the tree in Figure 1 is 1/4 under
the first grammar, but only 4/27 under the second.
Hence in this case we would reject the swap of V P
from the right to the left buckets.
The process of swapping continues until no im-
provement can be made by swapping a single
value.
The likelihood of the training data according to
the clustered grammar is
?
r?R
p(r)C(r)
for R the set of observed productions r = ?i ?
?j . . . in the clustered grammar. Notice that when
we are looking to split a cluster ?, only produc-
tions that contain the nonterminal ? will have
probabilities that change. To evaluate whether a
change increases the likelihood, we consider the
ratio between the likelihood of the new model, and
the likelihood of the old model.
Furthermore, when we move a value from one
bucket to another, only a fraction of the rules will
have their counts change. Suppose we are mov-
ing value x from the left bucket to the right when
splitting ?i. Let ?x ? ?i be the set of base nonter-
minals in ?i that have value x for the feature being
split upon. Only clustered rules that contain base
grammar rules which use nonterminals in ?x will
have their probability change. These observations
allow us to process only a relatively small number
of base grammar rules.
Once we have generated a potential nominee
split for each feature, we select the partitioning
which leads to the greatest improvement in the
BIC as the candidate split of this cluster. This can-
didate is placed on the priority queue.
One odd thing about the above is that in the lo-
cal search phase of the clustering we use likeli-
hood, while in the candidate selection phase we
use BIC. We tried both measures in each phase,
but found that this hybrid measure outperformed
using only one or the other.
3.1 Model Selection
Unfortunately, the grammar that results at the end
of the clustering process seems to overfit the train-
ing data. We resolve this by simply noting period-
ically the intermediate state of the grammar, and
using this grammar to parse a small tuning set (we
use the first 400 sentences of WSJ section 24, and
parse this every 50 times we enact a split). At the
conclusion of clustering, we select the grammar
304
AB C <D> E F
(a)
A
B [C,<D>,E,F]
C [<D>,E,F]
[<D>,E]
D E
F
(b)
Figure 2: (a) A production. (b) The production,
binarized.
with the highest f-score on this tuning set as the
final model.
3.2 Binarization
Since our experiments make use of a CKY
(Kasami, 1965) parser 1 we must modify the tree-
bank derived rules so that each expands to at most
two labels. We perform this in a manner simi-
lar to Klein and Manning (2003) and Matsuzaki
et al (2005) through the creation of intermediate
nodes, as in Figure 2. In this example, the nonter-
minal heir of A?s head is D, indicated in the figure
by marking D with angled brackets. The square
brackets indicate an intermediate node, and the la-
bels inside the brackets indicate that the node will
eventually be expanded into those labels.
Klein and Manning (2003) employ Collins?
(1999) horizontal markovization to desparsify
their intermediate nodes. This means that given
an intermediate node such as [C ?D?EF ] in Fig-
ure 2, we forget those labels which will not be ex-
panded past a certain horizon. Klein and Manning
(2003) use a horizon of two (or less, in some cases)
which means only the next two labels to be ex-
panded are retained. For instance in in this exam-
ple [C ?D?EF ] is markovized to [C ?D? . . . F ],
since C and F are the next two non-intermediate
labels.
Our mechanism lays out the unmarkovized in-
termediate rules in the same way, but we mostly
use our clustering scheme to reduce sparsity. We
do so by aligning the labels contained in the in-
termediate nodes in the order in which they would
be added when increasing the markovization hori-
1The implementation we use was created by Mark John-
son and used for the research in (Johnson, 1998). It is avail-
able at his homepage.
zon from zero to three. We also always keep
the heir label as a feature, following Klein and
Manning (2003). So for instance, [C ?D?EF ]
is represented as having Treebank label ?IN-
TERMEDIATE?, and would have feature vector
(D,C,F,E,D),while [?D?EF ] would have fea-
ture vector (D,F,E,D,?), where the first item
is the heir of the parent?s head. The ?-? in-
dicates that the fourth item to be expanded is
here non-existent. The clusterer would consider
each of these five features as for a single pos-
sible split. We also incorporate our other fea-
tures into the intermediate nodes in two ways.
Some features, such as the parent or grandpar-
ent, will be the same for all the labels in the in-
termediate node, and hence only need to be in-
cluded once. Others, such as the part of speech
of the head, may be different for each label. These
features we align with those of corresponding la-
bel in the Markov ordering. In our running ex-
ample, suppose each child node N has part of
speech of its head PN , and we have a parent fea-
ture. Our aligned intermediate feature vectors then
become (A,D,C, PC , F, PF , E, PE ,D, PD) and
(A,D,F, PF , E, PE ,D, PD,?,?). As these are
somewhat complicated, let us explain them by un-
packing the first, the vector for [C ?D?EF ]. Con-
sulting Figure 2 we see that its parent is A. We
have chosen to put parents first in the vector, thus
explaining (A, ...). Next comes the heir of the
constituent, D. This is followed by the first con-
stituent that is to be unpacked from the binarized
version, C , which in turn is followed by its head
part-of-speech PC , giving us (A,D,C, PC , ...).
We follow with the next non-terminal to be un-
packed from the binarized node and its head part-
of-speech, etc.
It might be fairly objected that this formulation
of binarization loses the information of whether a
label is to the left, right, or is the heir of the par-
ent?s head. This is solved by the inside position
feature, described in Section 2.1 which contains
exactly this information.
3.3 Smoothing
In order to ease comparison between our work
and that of Klein and Manning (2003), we follow
their lead in smoothing no production probabilities
save those going from preterminal to nonterminal.
Our smoothing mechanism runs roughly along the
lines of theirs.
305
LP LR F1 CB 0CB
Klein & Manning 86.3 85.1 85.7 1.31 57.2
Matsuzaki et al 86.1 86.0 86.1 1.39 58.3
This paper 84.8 84.8 84.8 1.47 57.1
Table 1: Parsing results on final test set (Section
23).
Run LP LR F1 CB 0CB
1 85.3 85.6 85.5 1.29 59.5
2 85.8 85.9 85.9 1.29 59.4
3 85.1 85.5 85.3 1.36 58.0
4 85.3 85.7 85.5 1.30 59.9
Table 2: Parsing results for grammars generated
using clusterer with different random seeds. All
numbers here are on the development test set (Sec-
tion 22).
Preterminal rules are smoothed as follows. We
consider several classes of unknown words, based
on capitalization, the presence of digits or hy-
phens, and the suffix. We estimate the probabil-
ity of a tag T given a word (or unknown class)
W , as p(T | W ) = C(T,W )+hp(T |unk)C(W )+h , where
p(T | unk) = C(T, unk)/C(unk) is the prob-
ability of the tag given any unknown word class.
In order to estimate counts of unknown classes,we
let the clusterer see every tree twice: once un-
modified, and once with the unknown class re-
placing each word seen less than five times. The
production probability p(W | T ) is then p(T |
W )p(W )/p(T ) where p(W ) and p(T ) are the re-
spective empirical distributions.
The clusterer does not use smoothed probabil-
ities in allocating annotated preterminals to clus-
ters, but simply the maximum likelihood estimates
as it does elsewhere. Smoothing is only used in the
parser.
4 Experiments
We trained our model on sections 2-21 of the Penn
Wall Street Journal Treebank. We used the first
400 sentences of section 24 for model selection.
Section 22 was used for testing during develop-
ment, while section 23 was used for the final eval-
uation.
5 Discussion
Our results are shown in Table 1. The first three
columns show the labeled precision, recall and f-
measure, respectively. The remaining two show
the number of crossing brackets per sentence,
and the percentage of sentences with no crossing
brackets.
Unfortunately, our model does not perform
quite as well as those of Klein and Manning (2003)
or Matsuzaki et al (2005). It is worth noting that
Matsuzaki?s grammar uses a different parse evalu-
ation scheme than Klein & Manning or we do.
We select the parse with the highest probability
according to the annotated grammar. Matsuzaki,
on the other hand, argues that the proper thing to
do is to find the most likely unannotated parse.
The probability of this parse is the sum over the
probabilities of all annotated parses that reduce
to that unannotated parse. Since calculating the
parse that maximizes this quantity is NP hard, they
try several approximations. One is what Klein &
Manning and we do. However, they have a better
performing approximation which is used in their
reported score. They do not report their score
on section 23 using the most-probable-annotated-
parse method. They do however compare the per-
formance of different methods using development
data, and find that their better approximation gives
an absolute improvement in f-measure in the .5-1
percent range. Hence it is probable that even with
their better method our grammar would not out-
perform theirs.
Table 2 shows the results on the development
test set (Section 22) for four different initial ran-
dom seeds. Recall that when splitting a cluster, the
initial partition of the base grammar nonterminals
is made randomly. The model from the second run
was used for parsing the final test set (Section 23)
in Table 1.
One interesting thing our method allows is for
us to examine which features turn out to be useful
in which contexts. We noted for each trereebank
nonterminal, and for each feature, how many times
that nonterminal was split on that feature, for the
grammar selected in the model selection stage. We
ran the clustering with these four different random
seeds.
We find that in particular, the clusterer only
found the head feature to be useful in very spe-
cific circumstances. It was used quite a bit to
split preterminals; but for phrasals it was only
used to split ADJP ,ADV P ,NP ,PP ,V P ,QP ,
and SBAR. The part of speech of the head was
only used to split NP and V P .
Furthermore, the grandparent tag appears to be
of importance primarily for V P and PP nonter-
306
minals, though it is used once out of the four runs
for NP s.
This indicates that perhaps lexical parsers might
be able to make do by only using lexical head and
grandparent information in very specific instances,
thereby shrinking the sizes of their models, and
speeding parsing. This warrants further investiga-
tion.
6 Conclusion
We have presented a scheme for automatically
discovering phrasal categories for parsing with a
standard CKY parser. The parser achieves 84.8%
precision-recall f-measure on the standard test-
section of the Penn WSJ-Treebank (section 23).
While this is not as accurate as the hand-tailored
grammar of Klein and Manning (2003), it is close,
and we believe there is room for improvement.
For starters, the particular clustering scheme is
only one of many. Our algorithm splits clus-
ters along particular features (e.g., parent, head-
part-of-speech, etc.). One alternative would be to
cluster simultaneously on all the features. It is
not obvious which scheme should be better, and
they could be quite different. Decisions like this
abound, and are worth exploring.
More radically, it is also possible to grow many
decision trees, and thus many alternative gram-
mars. We have been impressed by the success of
random-forest methods in language modeling (Xu
and Jelinek, 2004). In these methods many trees
(the forest) are grown, each trying to predict the
next word. The multiple trees together are much
more powerful than any one individually. The
same might be true for grammars.
Acknowledgement
The research presented here was funded in part by
DARPA GALE contract HR 0011-06-20001.
References
Anja Belz. 2002. Learning grammars for different
parsing tasks by partition search. In Proceedings of
the 19th international conference on Computational
Linguistics, pages 1?7.
Eugene Charniak. 1996. Tree-bank grammars. In
Proceedings of the Thirteenth National Conference
on Artificial Intelligence, pages 1031?1036. AAAI
Press/MIT Press.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL, pages
132?139.
Michael J. Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In The Pro-
ceedings of the 34th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 184?191.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In The Proceedings
of the 35th Annual Meeting of the Association for
Computational Linguistics.
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, The
University of Pennsylvania.
Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2003. The Elements of Statistical Learning.
Springer, New York.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Pro-
ceedings of HLT-NAACL 2003, pages 25?31.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Tadao Kasami. 1965. An efficient recognition and syn-
tax algorithm for context-free languages. Technical
Report AF-CRL-65-758, Air Force Cambridge Re-
search Laboratory.
Dan Klein and Christopher Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In The Proceedings of the 33rd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 276?283.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Sven Martin, Jo?rg Liermann, and Hermann Ney. 1995.
Algorithms for bigram and trigram word cluster-
ing. In Proceedings of the European Conference
on Speech, Communication and Technology, pages
1253?1256.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 2005 Meeting of the Association
for Computational Linguistics.
Mehryar Mohri and Brian Roark. 2006. Effective self-
training for parsing. In Proceedings of HLT-NAACL
2006.
Peng Xu and Fred Jelinek. 2004. Random forests
in language modeling. In Proceedings of EMNLP
2004.
307
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 674?683,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Top-Down Nearly-Context-Sensitive Parsing
Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University, Providence, RI 02912
ec@cs.brown.edu
Abstract
We present a new syntactic parser that
works left-to-right and top down, thus
maintaining a fully-connected parse tree
for a few alternative parse hypotheses. All
of the commonly used statistical parsers
use context-free dynamic programming al-
gorithms and as such work bottom up on
the entire sentence. Thus they only find
a complete fully connected parse at the
very end. In contrast, both subjective
and experimental evidence show that peo-
ple understand a sentence word-to-word as
they go along, or close to it. The con-
straint that the parser keeps one or more
fully connected syntactic trees is intended
to operationalize this cognitive fact. Our
parser achieves a new best result for top-
down parsers of 89.4%,a 20% error reduc-
tion over the previous single-parser best
result for parsers of this type of 86.8%
(Roark, 2001). The improved performance
is due to embracing the very large feature
set available in exchange for giving up dy-
namic programming.
1 Introduction
We present a new syntactic parser that works
top-down and left-to-right, maintaining a fully-
connected parse tree for a few alternative parse
hypotheses. It is a Penn treebank (Marcus et al,
1993) parser in that it is capable of parsing the
Penn treebank test sets, and is trained on the
now standard training set. It achieves a new
best result for this parser type.
All of the commonly used statistical parsers
available on the web such as the Collins(/Bikel)
(Collins, 2003) Charniak-Johnson(Charniak and
Johnson, 2005), and Petrov-Klein (Petrov et
al., 2006), parsers use context-free dynamic pro-
gramming algorithms so they work bottom up
on the entire sentence. Thus they only find a
complete fully-connected parse at the very end.
In contrast human syntactic parsing must be
fully connected (or close to it) as people are
able to apply vast amounts of real-world knowl-
edge to the process as it proceeds from word-to-
word(van Gompel and Pickering, 2007). Thus
any parser claiming cognitive plausibility must,
to a first approximation, work in this left-to-
right top-down fashion.
Our parser obtains a new best result for top-
down parsers of 89.4% (on section 23 of the Penn
Treebank). This is a 20% error reduction over
the previous best single-parser result of 86.8%,
achieved by Rork(Roark, 2001).
Our model is in the tradition of this lat-
ter parser. The current work?s superior per-
formance is not due to any innovation in ar-
chitecture but in how probability distributions
are computed. It differs from Roark in its ex-
plicit recognition that by giving up context-free
dynamic programming we may embrace near
context sensitivity and condition on many di-
verse pieces of information. (It is only ?near?
because we still only condition on a finite
amount of information.) This is made possi-
ble by use of random-forests (Amit and Geman,
1997; Breiman, 2004; Xu and Jelinek, 2004) to
choose features, provide smoothing, and finally
do the probability computation. To the best
of our knowledge ours is the first application of
random-forests to parsing.
674
Section two describes previous work on this
type of parser, and in particular gives details on
the Roark architecture we use. Section three de-
scribes how random forests allow us to integrate
the diverse information sources that context-
sensitive parsing allows. Section four gives im-
plementation details. Section five is devoted to
the main experimental finding of the paper along
with subsidiary results showing the effects of the
large feature set we now may use. Finally, sec-
tion six suggests that because this parser type is
comparatively little explored one may hope for
further substantial improvements, and proposes
avenues to be explored.
2 Previous Work on Top-Down
Parsing and the Roark Model
We care about top-down incremental parsing be-
cause it automatically satisfies the criteria we
have established for cognitive plausibility. Be-
fore looking at previous work on this type model
we briefly discuss work that does not meet the
criteria we have set out, but which people often
assume does so.
We are using the terms ?top-down? and ?left-
to-right? following e.g., (Abney and Johnson,
1991; Roark, 2001). In particular
In top-down strategies a node is enu-
merated before any of its descen-
dents.(Abney and Johnson, 1991)
In this era of statistical parsers it is useful to
think in terms of possible conditioning informa-
tion. In typical bottom up CKY parsing when
creating, say, a constituent X from positions i
to j we may not condition on its parent. That
the grammar is ?context-free? means that this
constituent may be used anywhere.
Using our definition, the Earley parsing al-
gorithm(Earley, 1970), which is often cited as
?top-down,? is no such thing. In fact, it is long
been noted that the Earley algorithm is ?almost
identical?(Graham et al, 1980) to CKY. Again,
when Earley posits an X it may not condition
on the parent.
Similarly, consider the more recent work of
Nivre(Nivre, 2003) and Henderson(Henderson,
parse (w0,n?1)
1 C[0](= h =< q, r, t >)?< 1, 1, ROOT >
2 for i = 0, n
3 do while ABOVE-THRESHOLD (h,C,N)
4 remove h from C
5 for all x such that p(x | t) > 0
6 let h? =< q?, r?, t? >
7 where q? = q ? p(x | t),
r? = LAP(t?, w?),
and t? = t ? x
8 if(x = w) then w? = wi+1
insert h? in N
9 else w? = w
10 insert h? in C
11 empty C
12 exchange C and N
13 output t(C[0]).
Figure 1: Roark?s Fully-Connected Parsing Algo-
rithm
2003). The reason these are not fully-connected
is the same. While they are incremental parsers,
they are not top down ? both are shift-reduce
parsers. Consider a constituency shift-reduce
mechanism. Suppose we have a context-free rule
S? NP VP. As we go left-to-right various termi-
nal and non-terminals are added to and removed
from the stack until at some point the top two
items are NP and VP. Then a reduce operation
replaces them with S. Note that this means that
none of the words or sub-constituents of either
the NP or VP are integrated into a single overall
tree until the very end. This is clearly not fully
connected. Since Nivre?s parser is a dependency
parser this exact case does not apply (as it does
not use CFG rules), but similar situations arise.
In particular, whenever a word is dependent on
a word that appears later in the string, it re-
mains unconnected on the stack until the sec-
ond word appears. Naturally this is transitive
so that the parser can, and presumably does,
process an unbounded number of words before
connecting them all together.
Here we follow the work of Roark (Roark,
2001) which is fully-connected. The basic al-
675
Current hypotheses Prob of next tree element
1 < 1.4 ? 10?3, 5 ? 10?2,(S (NP (NNS Terms)> p(x=?)? | t)=.64
2 < 7 ? 10?5, 5 ? 10?2,(S (S (NP (NNS Terms)>
3 < 9 ? 10?4, 8 ? 10?2,(S (NP (NNS Terms))> p(x=VP | t) =.88
4 p(x=S | t)= 2 ? 10?4
5 < 7 ? 10?5, 5 ? 10?2,(S (S (NP (NNS Terms)>
6 < 9 ? 10?4, 9 ? 10?2,(S (NP (NNS Terms)) (VP> p(x=AUX | t)= .38
7 < 7 ? 10?5, 5 ? 10?2,(S (S (NP (NNS Terms)>
8 < 2 ? 10?8, 9 ? 10?2,(S (NP (NNS Terms)) (S>
9 < 3 ? 10?4, 2 ? 10?1,(S (NP (NNS Terms)) (VP (AUX > p(x=?were? | t)= .21
10 < 7 ? 10?5, 5 ? 10?2,(S (S (NP (NNS Terms)>
11 < 2 ? 10?8, 9 ? 10?2,(S (NP (NNS Terms)) (S>
12 < 7 ? 10?5, 3 ? 10?1,(S (NP (NNS Terms)) (VP (AUX were)>
Figure 2: Parsing the second word of ?Terms were not disclosed.?
gorithm is given in Figure 1. (Note we have
simplified the algorithm in several ways.) The
input to the parser is a string of n words w0,n?1.
We pad the end of the string with an end-of-
sentence marker wn = ?. This has the special
property that p(? | t) = 1 for a complete tree t
of w0,n?1, zero otherwise.
There are two priority queues of hypotheses,
C (current), and N (next). A hypothesis h is a
three-tuple < q, r, t > where q is the probabil-
ity assigned to the current tree t. In Figure 1 h
always denotes C[0] the top-most element of C.
While we call t the ?tree?, it is a vector represen-
tation of a tree. For example, the tree (ROOT)
would be a vector of two elements, ROOT and
?)?, the latter indicating that the constituent la-
beled root is completed. Thus elements of the
vector are the terminals, non-terminals, and ?)?
? the close parenthesis element. Lastly r is the
?look-ahead probability? or LAP. LAP(w,h) is
(a crude estimate of) the probability that the
next word is w given h. We explain its purpose
below.
We go through the words one at a time. At
the start of our processing of wi we have hy-
potheses on C ordered by p ?q ? the probability
of the hypothesis so far times an estimate q of
the probability cost we encounter when trying
to now integrate wi. We remove each h from C
and integrate a new tree symbol x. If x = wi
it means that we have successfully added the
new word to the tree and this hypothesis goes
into the queue for the next word N . Other-
wise h does not yet represent an extension to
wi and we put it back on C to compete with the
other hypotheses waiting to be extended. The
look-ahead probability LAP(h) = q is intended
to keep a level playing field. If we put h back
onto C it?s probability p is lowered by the factor
p(x | h). On the other hand, if x is the correct
symbol, q should go up, so the two should offset
and h is still competitive.
We stop processing a word and move onto
the next when ABOVE-THRESHOLD returns
false. Without going into details, we have
adopted exactly the decision criteria and asso-
ciated parameters used by Roark so that the
accuracy numbers presumably reflect the same
amount of search. (The more liberal ABOVE-
THRESHOLD, the more search, and presum-
ably the more accurate results, everything else
being equal.)
Figure 2 shows a few points in the process-
ing of the second word of ?Terms were not dis-
closed.? Lines one and two show the current
queue at the start of processing. Line one has
676
the ultimately correct partial tree (S (NP (NNS
Terms). Note that the NP is not closed off as
the parser defers closing constituents until nec-
essary. On the right of line 1 we show the pos-
sible next tree pieces that could be added. Here
we simply have one, namely a right parenthesis
to close off the NP. (In reality there would be
many such x?s.) The result is that the hypoth-
esis of line 1 is removed from the queue, and
a new hypothesis is added back on C as this
new hypothesis does not incorporate the second
word.
Lines 3 and 5 now show the new state of C.
Again we remove the top candidate from C. The
right-hand side of lines 3 and 4 show two pos-
sible continuations for the h of line 3, start a
new VP or a new S. With line 3 removed from
the queue, and its two extensions added, we get
the new queue state shown in lines 6,7 and 8.
Line 6 shows the top-most hypothesis extended
by an AUX. This still has not yet incorporated
the next word into the parse, so this extension
is inserted in the current queue giving us the
queue state shown in 9,10,11. Finally line 9 is
extended with the word ?were.? This addition
incorporates the current word, and the resulting
extension, shown in line 12 is inserted in N , not
C, ending this example.
3 Random Forests
The Roark model we emulate requires the esti-
mation of two probability distributions: one for
the next tree element (non-terminals,terminals,
and ?)?) in the grammar, and one for the look-
ahead probability of the yet-to-be-incorporated
next word. In this section we use the first of
these for illustration.
We first consider how to construct a single
(non-random) decision tree for estimating this
distribution. A tree is a fully-connected directed
acyclic graph where each node has one input arc
(except for the root) and, for reasons we go into
later, either zero or two output arcs ? the tree
is binary. A node is a four-tuple < d, s, p, q >,
where d is a set of training instances, p, a prob-
ability distribution of the correct decisions for
all of the examples in d, and q a binary ques-
tion about the conditioning information for the
examples in d. The 0/1 answer to this ques-
tion causes the decision-tree program to follow
the left/right arc out of the node to the children
nodes. If q is null, the node is a leaf node. s
is a strict subset of the domain of the q for the
parent of h.
Decision tree construction starts with the root
node n where d consists of the several million
situations in the training data where the next
tree element needs to be guessed (according to
our probability distribution) based upon previ-
ous words and the analysis so far. At each itera-
tion one node is selected from a queue of unpro-
cessed nodes. A question q is selected, and based
upon its answers two descendents n1 and n2 are
created with d1 and d2 respectively, d1 ? d2 = d.
These are inserted in the queue of unprocessed
nodes and the process repeats. Termination can
be handled in multiple ways. We have chosen
to simply pick the number of nodes we create.
Nodes left on the queue are the leaf nodes of
the decision tree. We pick nodes from the heap
based upon how much they increased the prob-
ability of the data.
Still open is the selection of q at each iter-
ation. First pick a query type qt from a user
supplied set. In our case there are 27 types. Ex-
amples include the parent of the non-terminal
to be created, its predecessor, 2 previous, etc.
A complete list is given in Figure 4. Note that
the answers to these queries are not binary.
Secondly we turn our qt into a binary question
by creating two disjoint sets s1 and s2 s1?s2 = s
where s is the domain of qt. If a particular his-
tory h ? d is such that qt(h) = x and x ? s1 then
h is put in d1. Similarly for s2. For example, if
qt is the parent relation, and the parent in h is
NP, then h goes in d1 iff NP ? s1. We create the
sets si by initializing them randomly, and then
for each x ? s try moving x to the opposite set
si. If this results in a higher data probability we
keep it in its new si, otherwise it reverts to it?s
original si. This is repeated until no switch low-
ers probability. (Or were the a?s are individual
words, until no more than two words switch.)
We illustrate with a concrete example. One
important fact quickly impressed on the cre-
677
No. Q S p(?of?) p(?in?)
0 1
1 1 NN,IN 0.05 0.03
4 1 NNS,IN 0.09 0.06
12 2 RB,IN 0.17 0.11
16 3 PP,WHPP 0.27 0.18
39 20 NP,NX 0.51 0.16
40 20 S,VP 0.0004 0.19
Figure 3: Some nodes in a decision tree for p(wi | t)
ator of parsers is the propensity of preposi-
tional phrases (PP) headed by ?of? to attach
to noun phrases (NP) rather than verb phrases
(VP). Here we illustrate how a decision tree for
p(wi | t) captures this. Some of the top nodes in
this decision tree are shown in Figure 3. Each
line gives a node number, Q ? the question
asked at that node, examples of answers, and
probabilities for ?of? and ?in?. Questions are
specified by the question type numbers given in
Figure 4 in the next section. Looking at node 0
we see that the first question type is 1 ? par-
ent of the proposed word. The children trees
are 1 and 2. We see that prepositions (IN) have
been put in node 1. Since this is a binary choice,
about half the preterminals are covered in this
node. To get a feel for who is sharing this node
with prepositions each line gives two examples.
For node 1 this includes a lot of very different
types, including NN (common singular noun).
Node 1 again asks about the preterminal,
leading to node 4. At this point NN has split
off, but NNS (common plural noun) is still there.
Node 4 again asks about the preterminal, lead-
ing to node 12. By this point IN is only grouped
with things that are much closer, e.g. RB (ad-
verb).
Also note that at each node we give the prob-
ability of both ?of? and ?to? given the questions
and answers leading to that node. We can see
that the probability of ?of? goes up from 0.05
at node 1 to 0.27 at node 16. The probabili-
ties for ?to? go in lockstep. By node 16 we are
concentrating on prepositions heading preposi-
tional phrases, but nothing has been asked that
would distinguish between these two preposi-
tions. However, at node 16 we ask the ques-
tion ?who is the grandparent? leading to nodes
39 and 40. Node 39 is restricted to the answer
?noun phrase? and things that look very much
like noun phrases ? e.g., NX, a catchall for ab-
normal noun phrases, while 40 is restricted to
PP?s attaching to VP?s and S?s. At this point
note how the probability of ?of? dramatically
increases for node 39, and decreases for 40.
That the tree is binary forces the decision
tree to use information about words and non-
terminals one bit at a time. In particular, we
can now ask for a little information about many
different previous words in the sentence.
We go from a single decision tree to a random
forest by creating many trees, randomly chang-
ing the questions used at every node. First note
that in our greedy selection of si?s the outcome
depends on the initial random assignment of a?s.
Secondly, each qt produces its own binary ver-
sion q. Rather than picking the one that raises
the data probability the most, we choose it with
probability m. With probability 1 ? m we re-
peat this procedure on the list of q?s minus the
best. Given a forest of f trees we compute the
final probability by taking the average over all
the trees:
p(x | t) = 1f
?
j=1,f
pj(x | t)
where pj denotes the probability computed by
tree j.
4 Implementation Details
We have twenty seven basic query types as
shown in Figure 4. Each entry first gives identi-
fication numbers for the query types followed by
a description of types. The description is from
the point of view of the tree entry x to be added
to the tree. So the first line of Figure 4 speci-
fies six query types, the most local of which is
the label of the parent of x. For example, if we
have the local context ?(S (NP (DT the)? and
we are assigning a probability to the pretermi-
nal after DT, (e.g., NN) then the parent of x is
NP. Similarly one of the query types from line
two is one-previous, which is DT. Two previous
is ?, signifying nothing in this position.
678
1-6 The non-terminal of the parent, grandpar-
ent, parent3, up to parent6
7-10 The previous non-terminal, 2-previous, up
to 4-previous
11-14 The non-terminal just prior to the par-
ent, 2-prior, up to 4 prior
15-16 The non-terminal and terminals of the
head of the previous constituent
17-18 Same, but 2 previous
19-20 Same but previous to the parent
21-22 Same but 2 previous to the parent
23-24 The non-terminal and terminal symbols
just prior to the start of the current parent
constituent
25 The non-terminal prior to the grandparent
26 Depth in tree, binned logarithmically
27 Is a conjoined phrase prior to parent.
Figure 4: Question types
Random forests, at least in obvious imple-
mentations, are somewhat time intensive. Thus
we have restricted their use to the distribution
p(x | t). The forest size we picked is 4400 nodes.
For the look-ahead probability, LAP, we use a
single decision tree with greedy optimal ques-
tions and 1600 nodes.
We smooth our random forest probabilities
by successively backing off to distributions three
earlier in the decision tree. We use linear inter-
polation so
pl(x | t) = ?(cl)?p?l(x | t)+(1??(cl))?pl?3(x | t)
Here pl is the smoothed distribution for level l
of the tree and p?l is the maximum likelihood (un-
smoothed) distribution. We use Chen smooth-
ing so the linear interpolation parameters ? are
functions of the Chen number of the level l node.
See (Chen and Goodman, 1996). We could back
off to l ? 1, but this would slow the algorithm,
and seemed unnecessary.
Following (Klein and Manning, 2003) we han-
dle unknown and rare words by replacing them
with one of about twenty unknown word types.
For example, ?barricading? would be replaced
by UNK-ING, denoting an unknown word end-
ing in ?ing.? Any word that occurs less than
twenty times in the training corpus is consid-
ered rare. The only information that is retained
about it is the parts of speech with which it has
appeared. Future uses are restricted to these
pre-terminals.
Because random forests have so much latitude
in picking combinations of words for specific sit-
uations we have the impression that it can over-
fit the training data, although we have not done
an explicit study to confirm this. As a mild cor-
rective we only allow verbs appearing 75 times or
more, and all other words appearing 250 times or
more, to be conditioned upon in question types
16, 18, 20, 22, and 27. Because the inner loop of
random-forest training involves moving a condi-
tioning event to the other decedent node to see
if this raises training data probability, this also
substantially speeds up training time.
Lastly Roark obtained the results we quote
here with selective use of left-corner transforms
(Demers, 1977; Johnson and Roark, 2000). We
also use this technique but the details differ.
Roark uses left-corner transforms only for im-
mediately recursive NP?s, the most common sit-
uation by far. As it was less trouble to do
so, we use them for any immediately recursive
constituent. However, we are also aware that
in some respects left-corner transforms work
against the fully-connected tree rule as opera-
tionalizing the ?understand as you go along?
cognitive constraint. For example, the normal
sentence initial NP serves as the subject of the
sentence. However in Penn-treebank grammar
style an initial NP could also be a possessive
NP as in (S (NP (NP (DT The) (NN dog)
(POS ?s)))) Clearly this NP is not the subject.
Thus using left corner transforms on all NP?s
allows the parser to conflate differing semantic
situations into a single tree. To avoid this we
have added the additional restriction that we
only allow left-corner treatment when the head
words (and thus presumably the meaning) are
679
Precision Recall F
Collins 2003 88.3 88.1 88.2
Charniak 2000 89.6 89.5 89.6
C/J 2005 91.2 90.9 91.1
Petrov et.al. 2006 90.3 90.0 90.2
Roark 2001 87.1 86.6 86.8
C/R Perceptron 87.0 86.3 86.6
C/R Combined 89.1 88.4 88.8
This paper 89.8 89.0 89.4
Figure 5: Precision/Recall measurements, Penn
Treebank Section 23, Sentence length ? 100
the same. (Generally head-word rules dictate
that the POS is the head of the possessive NP.)
5 Results and Analysis
We trained the parser on the standard sections
2-21 of the Penn Tree-bank, and tested on all
sentences of length ? 100 of section 23. We
used section 24 for development.
Figure 5 shows the performance of our model
(last line, in bold) along with the performance
of other parsers. The first group of results show
the performance of standard parsers now in use.
While our performance of 89.4% f-measure needs
improvement before it would be worth-while us-
ing this parser for routine work, it has moved
past the accuracy of the Collins-Bikel (Collins,
2003; Bikel, 2004) parser and is not statistically
distinguishable from (Charniak, 2000).
The middle group of results in Figure 5 show
a very significant improvement over the original
Roark parser, (89.4% vs.86.8%). Although we
have not discussed it to this point, (Collins and
Roark, 2004) present a perceptron algorithm for
use with the Roark architecture. As seen above
(C/R Perceptron), this does not give any im-
provement over the original Roark model. As
is invariably the case, when combined the two
models perform much better than either by it-
self (C/R Combined ? 88.8%). However we still
achieve a 0.6% improvement over that result.
Naturally, a new combination using our parser
would almost surely register another significant
gain.
Conditioning Conditioning F-measure
Non-terminals Terminals
8 1 86.6
10 2 88.0
13 3 88.3
17 4 88.8
21 6 89.0
Figure 6: Labeled precision-recall results on section
24 of the Penn Tree-bank. All but one sentence of
length ? 100. (Last one not parsed).
In Figure 6 we show results illustrating how
parser performance improves as the probability
distributions are conditioned on more diverse in-
formation from the partial trees. The first line
has results when we condition on only the ?clos-
est? eight non-terminal and the previous word.
We successively add more distant conditioning
events. The last line (89.0% F-measure) corre-
sponds to our complete model but since we are
experimenting here on the development set the
result is not the same as in Figure 5. (The result
is consistent with the parsing community?s ob-
servation that the test set is slightly easier than
the development set ? e.g., average sentence
length is less.)
One other illustrative result: if we keep all
system settings constant and replace the random
forest mechanism by a single greedy optimal de-
cision tree for probability computation, perfor-
mance is reduced to 86.3% f-measure. While this
almost certainly overstates the performance im-
provement due to many random trees (the sys-
tem parameters could be better adjusted to the
one-tree case), it strongly suggests that nothing
like our performance could have been obtained
without the forests in random forests.
6 Conclusions and Future Work
We have presented a new top-down left-to-right
parsing model. Its performance of 89.4% is a
20% error reduction over the previous single-
parser performance, and indeed is a small im-
provement (0.6%) over the best combination-
parser result. The code is publically available.1
1http://bllip.cs.brown.edu/resources.shtml#software
680
PP
IN
than
NP
NNP NNP
General Motors
Figure 7: The start of an incorrect analysis for ?than
General Motors is worth?
IN
than NP VP
S
General Motors is NP
worth
SBAR
Figure 8: The correct analysis for ?than General Mo-
tors is worth?
Furthermore, as models of this sort have re-
ceived comparatively little attention, it seems
reasonable to think that significant further im-
provements may be found.
One particular topic in need of more study is
search errors. Consider the following example:
The government had to spend more
than General Motors is worth.
which is difficult for our parser. The problem is
integrating the words starting with ?than Gen-
eral Motors.? Initially the parser believes that
this is a prepositional phrase as shown in Fig-
ure 7. However, the correct tree-bank parse in-
corporates a subordinate sentential clause?than
General Motors is worth?, as in Figure 8. Un-
fortunately, before it gets to ?is? which disam-
biguates the two alternatives, the subordinate
clause version has fallen out of the parser?s beam
(unless, of course, one sets the beam-width to an
unacceptably high level). Furthermore, it does
not seem that there is any information available
IN
than NP VP
S
General Motors
SBAR
-NONE-
Figure 9: Alternative analysis for ?than General Mo-
tors?
when one starts working on ?than? to allow a
person to immediately pick the correct continu-
ation. It is also the case that the parsing model
gives the correct parse a higher probability if it
is available, showing that this is a search error,
not a model error.
If there is no information that would allow
a person to make the correct decision in time,
perhaps people do not need to make this deci-
sion. Rather the problem could be in the tree-
bank representation itself. Suppose we reana-
lyzed ?than General Motors? in this context as
in Figure 9. Here we would not need to guess
anything in advance of the (missing) VP. Fur-
thermore, we can make this change without loos-
ing the great benefit of the treebank for training
and testing. The change is local and determin-
istic. We can tree-transform the training data
and then untransform before scoring. It is our
impression that a few examples like this would
remove a large set of current search errors.
Three other kinds of information are often
added as additional annotations to syntactic
trees: Penn-Treebank form-function tags, trace
elements, and semantic roles. Most research
on such annotation takes the parsing process as
fixed and is solely concerned with improving the
retrieval of the annotation in question. When
they have been integrated with parsing, finding
the parse and the further annotation jointly has
not improved the parse. While it is certainly
possible that this would prove to be the same for
this new model, the use of random forests to in-
tegrate more diverse information sources might
help us to reverse this state of affairs.
Finally there is no reason why we even need
681
to stop our collection of features at sentence
boundaries ? information from previous sen-
tences is there for our perusal. There are
many known intra-sentence correlations, for ex-
ample ?sentences? that are actually fragments
are much more common if the previous sentence
is a question. The tense of sequential sentences
main verbs are correlated. Main clause sub-
jects are more likely to be co-referent. Certainly
the ?understanding? humans pick up helps them
assign structure to subsequent phrases. How
much, if any, of this meaning we can glean given
our current (lack-of) understanding of semantics
and pragmatics is an interesting question.
7 Acknowledgements
Thanks to members of BLLIP and Mark John-
son who read earlier drafts of this paper. Also
thanks to Brian Roark and Mark Johnson for
pointers to previous work.
References
Stephen Abney and Mark Johnson. 1991. Mem-
ory requirements and local ambiguities of parsing
strategies. Journal of Psycholinguistic Research,
20(3):233?250.
Y. Amit and D. Geman. 1997. Shape quantization
and recognition with randomized trees. Neural
Computation, 9:1545?1588.
Dan Bikel. 2004. On the Parameter Space of Lex-
icalized Statistical Parsing Models. Ph.D. thesis,
University of Pennsylvania.
Leo Breiman. 2004. Random forests. Machine
Learning, 45(1):5?32.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discrimina-
tive reranking. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics, pages 173?180, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In The Proceedings of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 132?139.
Stanley F. Chen and Joshua Goodman. 1996. An
empirical study of smoothing techniques for lan-
guage modeling. In Arivind Joshi and Martha
Palmer, editors, Proceedings of the Thirty-Fourth
Annual Meeting of the Association for Computa-
tional Linguistics, pages 310?318, San Francisco.
Morgan Kaufmann Publishers.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Vol-
ume, pages 111?118, Barcelona, Spain, July.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, 29(4):589?638.
A. Demers. 1977. Generalized left-corner parsing.
In Conference Record of the Fourth ACM Sym-
posium on Principles of Programming Languages,
1977 ACM SIGACT/SIGPLAN, pages 170?182.
Jay Earley. 1970. An efficient contex-free parsing al-
gorithm. Communications of the ACM, 6(8):451?
445.
Suzan L. Graham, Michael Harrison, and Walter L.
Ruzzo. 1980. An improved context-free recog-
nizer. ACM Transations on Programming Lan-
guages and Systems, 2(3):415?462.
James Henderson. 2003. Inducing history represen-
tations for broad coverage statistical parsing. In
Proceedings of HLT-NAACL 2003.
Mark Johnson and Brian Roark. 2000. Compact
non-left-recursive grammars using the selective
left-corner transform and factoring. In Proceed-
ings of COLING-2000.
Dan Klein and Christopher Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics.
Michell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?
330.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, compact,
and interpretable tree annotation. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
433?440, Sydney, Australia, July. Association for
Computational Linguistics.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguis-
tics, 27(2):249?276.
682
Roger van Gompel and Martin J. Pickering. 2007.
Syntactic parsing. In G. Gatskil, editor, The Ox-
ford handbook of psycholinguistics. Oxford Univer-
sity Press.
Peng Xu and Frederick Jelinek. 2004. Random
forests in language modeling. In Proceedings of
the 2004 Empirical Methods in Natural Language
Processing Conference. The Association for Com-
putational Linguistics.
683
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1433?1437,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Naive Bayes Word Sense Induction
Do Kook Choe
Brown University
Providence, RI
dc65@cs.brown.edu
Eugene Charniak
Brown University
Providence, RI
ec@cs.brown.edu
Abstract
We introduce an extended naive Bayes model
for word sense induction (WSI) and apply it to
a WSI task. The extended model incorporates
the idea the words closer to the target word are
more relevant in predicting its sense. The pro-
posed model is very simple yet effective when
evaluated on SemEval-2010 WSI data.
1 Introduction
The task of word sense induction (WSI) is to find
clusters of tokens of an ambiguous word in an un-
labeled corpus that have the same sense. For in-
stance, given a target word ?crane,? a good WSI sys-
tem should find a cluster of tokens referring to avian
cranes and another referring to mechanical cranes.
We believe that neighboring words contain enough
information that these clusters can be found from
plain texts.
WSI is related to word sense disambiguation
(WSD). In a WSD task, a system learns a sense clas-
sifier in a supervised manner from a sense-labeled
corpus. The performance of the learned classifier
is measured on some unseen data. WSD systems
perform better than WSI systems, but building la-
beled data can be prohibitively expensive. In addi-
tion, WSD systems are not suitable for newly cre-
ated words, new senses of existing words, or domain-
specific words. On the other hand, WSI systems can
learn new senses of words directly from texts because
these programs do not rely on a predefined set of
senses.
In Section 2 we describe relevant previous work. In
Section 3 and 4 we introduce the naive Bayes model
for WSI and inference schemes for the model. In Sec-
tion 5 we evaluate the model on SemEval-2010 data.
In Section 6 we conclude.
2 Related Work
Yarowsky (1995) introduces a semi-supervised
bootstrapping algorithm with two assumptions
that rivals supervised algorithms: one-sense-per-
collocation and one-sense-per-discourse. But this
algorithm cannot easily be scaled up because for
any new ambiguous word humans need to pick
a few seed words, which initialize the algorithm.
In order to automate the semi-supervised system,
Eisner and Karakos (2005) propose an unsupervised
bootstrapping algorithm. Their system tries many
different seeds for bootstrapping and chooses the
?best? classifier at the end. Eisner and Karakos?s
algorithm is limited in that their system is designed
for disambiguating words that have only 2 senses.
Bayesian WSI systems have been developed by
several authors. Brody and Lapata (2009) apply
Latent Dirichlet Allocation (LDA) (Blei et al,
2003) to WSI. They run a topic modeling algorithm
on texts with some fixed number of topics that
correspond to senses and induce a cluster by finding
target words assigned to the same topic. Their
system is evaluated on SemEval-2007 noun data
(Agirre and Soroa, 2007). Lau et al (2012) apply
a nonparametric model, Hierarchical Dirichlet Pro-
cesses (HDP), to SemEval-2010 data (Manandhar et
al., 2010).
3 Model
Following Yarowsky (1995), we assume that a word
in a document has one sense. Multiple occurrences
of a word in a document refer to the same object
or concept. The naive Bayes model is well suited
for this one-sense-per-document assumption. Each
document has one topic corresponding to the sense of
the target word that needs disambiguation. Context
words in a document are drawn from the conditional
distribution of words given the sense. Context words
are assumed to be independent from each other given
1433
the sense, which is far from being true yet effective.
3.1 Naive Bayes
The naive Bayes model assumes that every word in a
document is generated independently from the con-
ditional distribution of words given a sense, p(w|s).
The mathematical definition of the naive Bayes
model is as follows:
p(w) =
?
s
p(s,w) =
?
s
p(s)p(w |s)
=
?
s
p(s)
?
w
p(w|s), (1)
where w is a vector of words in the document. With
the model, a new document can be easily labeled
using the following classifier:
s? = argmax
s
p(s)
?
w
p(w|s), (2)
where s? is the label of the new document. In con-
trast to LDA-like models, it is easy to construct
the closed form classifier from the model. The pa-
rameters of the model, p(s) and p(w|s), can be
learned by maximizing the probability of the corpus,
p(d) =
?
d p(d) =
?
w p(w) where d is a vector of
documents and d = w .
3.2 Distance Incorporated Naive Bayes
Intuitively, context words near a target word are
more indicative of its sense than ones that are far-
ther away. To account for this intuition, we propose
a more sophisticated model that uses the distance
between a context word and a target word. Before
introducing the new model, we define a probability
distribution, f(w|s), that incorporates distances as
follows:
f(w|s) =
p(w|s)l(w)
?
w??W p(w
?|s)l(w)
, (3)
where l(w) = 1dist(w)x . W is a set of types in the cor-
pus. x is a tunable parameter that takes nonnegative
real values. With the new probability distribution,
the model and the classifier become:
p(w) =
?
s
p(s)
?
w
f(w|s) (4)
s? = argmax
s
p(s)
?
w
f(w|s), (5)
where f(w|s) replaces p(w|s). The naive Bayes
model is a special case; set x = 0. The new model
puts more weight on context words that are close
to the target word. The distribution of words that
are farther away approaches the uniform distribu-
tion. l(w) smoothes the distribution more as x be-
comes larger.
4 Inference
Given the generative model, we employ two inference
algorithms to learn the sense distribution and word
distributions given a sense. Expectation Maximiza-
tion (EM) is a natural choice for the naive Bayes
(Dempster et al, 1977). When initialized with ran-
dom parameters, EM gets stuck at local maxima. To
avoid local maxima, we use a Gibbs sampler for the
plain naive Bayes to learn parameters that initialize
EM.
5 Experiments
5.1 Data
We evaluate the model on SemEval-2010 WSI task
data (Manandhar et al, 2010). The task has 100
target words, 50 nouns and 50 verbs. For each target
word, there are training and test documents. Table
1 have details. The training and test data are plain
texts without sense tags. For evaluation, the inferred
sense labels are compared with human annotations.
To tune some parameters we use the trial data of
Training Testing Senses (#)
All 879807 8915 3.79
Nouns 716945 5285 4.46
Verbs 162862 3630 3.12
Table 1: Details of SemEval-2010 data
SemEval-2010. The trial data consists of training
and test portions of 4 verbs. On average there are
137 documents for each target word in the training
part of the trial data.
5.2 Task
Participants induce clusters from the training data
and use them to label the test data. Resources other
than NLP tools for morphology and syntax such as
lemmatizer, POS-tagger, and parser are not allowed.
Tuning parameters and inducing clusters are only
allowed during the training phase. After training,
participants submit their sense-labeled test data to
organizers.
LDA models are not compatible with the scoring
rules for the SemEval-2010 competition, and that is
the work against which we most want to compare.
These rules require that training be done strictly be-
fore the testing is done. Note however that LDA re-
quires learning the mixture weights of topics for each
1434
individual document p(topic | document). These are,
of course, learned during training. But the docu-
ments in the testing corpus have never been seen
before, so clearly their topic mixture weights are not
learned during training, and thus not learned at all.
The way to overcome this is by training on both
train and test documents, but this is exactly what
SemEval-2010 forbids.
5.3 Implementation Details
The documents are tokenized and stemmed by
Stanford tokenizer and stemmer. Stop words and
punctuation in the training and test data are
discarded. Words that occur at most 10 times are
discarded from the training data. Context words
within a window of 50 about a target word are used
to construct a bag-of-words.
When a target word appears more than once
in a document, the distance between that target
word and a context word is ambiguous. We define
this distance to be minimum distance between a
context word and an instance of the target word.
For example, the word ?chip? appears 3 times. For
? ? ? of memory chips . Currently , chips are pro-
duced by shining light through a mask to produce
an image on the chip , much as ? ? ?
Example 1: an excerpt from ?chip? test data
a context word, e.g., ?shining? there are three pos-
sible distances: 8 away from the first ?chip,? 4 away
from the second ?chip? and 11 away from the last
?chip.? We set the distance of ?shining? from the
target to 4.
We model each target word individually. We set ?,
a Dirichlet prior for senses, to 0.02 and ?, a Dirichlet
prior for contextual words, to 0.1 for the Gibbs sam-
pler as in Brody and Lapata (2009). We initialize
EM with parameters learned from the sampler. We
run EM until the likehood changes less than 1%. We
run the sampler 2000 iterations including 1000 itera-
tions of burn-in: 10 samples at an interval of 100 are
averaged. For comparison, we also evaluate EM with
random initialization. All reported scores (described
in Section 5.4) are averaged over ten different runs
of the program.1
5.3.1 Tuning Parameters
Two parameters, the number of senses and x of
the function l(w), need to be determined before run-
ning the program. To find a good setting we do grid
search on the trial data with the number of senses
1Code used for experiments is available for download at
http://cs.brown.edu/~dc65/.
ranging from 2 to 5 and x ranging from 0 to 1.1 with
an interval 0.1. Due to the small size of the training
portion of the trial data, words that occur once are
thrown out in the training portion. All the other pa-
rameters are as described in Section 5.3. We choose
(4, 0.4), which achieves the highest supervised recall.
See Table 2 for the performance of the model with
various parameter settings. With a fixed value of x,
a column is nearly unimodal in the number of senses
and vice versa. x = 0 is not optimal and there is
some noticeable difference between scores with opti-
mal x and scores with x = 0.
5.4 Evaluation
We compare our system to other WSI systems and
discuss two metrics for unsupervised evaluation (V-
Measure, paired F-Score) and one metric for super-
vised evaluation (supervised recall). We refer to the
true group of tokens as a gold class and to an induced
group of tokens as a cluster. We refer to the model
learned with the sampler and EM as NB, and to the
model learned with EM only as NB0.
5.4.1 Short Descriptions of Other WSI
Systems Evaluated on SemEval-2010
The baseline assigns every instance of a target
word with the most frequent sense (MFS). UoY runs
a clustering algorithm on a graph with words as
nodes and co-occurrences between words as edges
(Korkontzelos and Manandhar, 2010). Hermit ap-
proximates co-occurrence space with Random Index-
ing and applies a hybrid of k-means and Hierarchical
Agglomerate Clustering to co-occurrence space (Ju-
rgens and Stevens, 2010). NMFlib factors a matrix
using nonnegative matrix factorization and runs a
clustering algorithm on test instances represented by
factors (Van de Cruys et al, 2011).
5.4.2 V-Measure
V-Measure computes the quality of induced clus-
ters as the harmonic mean of two values, homo-
geneity and completeness. Homogeneity measures
whether instances of a cluster belong to a single gold
class. Completeness measures whether instances of a
gold class belong to a cluster. V-Measure is between
0 and 1; higher is better. See Table 3 for details of
V-Measure evaluation (#cl is the number of induced
clusters).
With respect to V-Measure, NB performs much
better than NB0. This holds for paired F-Score and
supervised recall evaluations. The sampler improves
the log-likelihood of NB by 3.8% on average (4.8%
on nouns and 2.9% on verbs).
Pedersen (2010) points out that it is possible to
increase the V-Measure of bad models by increasing
1435
#s \ x 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1
2 74.73 74.76 74.41 74.57 74.06 74.07 74.18 74.33 74.14 74.22 74.15 74.52
3 74.60 74.71 75.21 75.46 75.21 75.57 75.61 75.32 75.53 75.56 74.98 74.79
4 74.52 75.06 74.97 75.14 76.02 75.51 75.74 75.51 75.59 75.51 75.37 75.35
5 73.40 73.88 74.93 75.13 74.79 74.68 74.71 74.49 75.11 74.94 74.86 75.25
Table 2: Performance of the model with various parameters: supervised recall on the trial data. The best value from
each row is bold-faced. The scores are averaged over 100 runs.
VM(%) all nouns verbs #cl
NB 18.0 23.7 9.9 3.42
NB0 14.9 19.0 9.0 3.77
Hermit 16.2 16.7 15.6 10.78
UoY 15.7 20.6 8.5 11.54
NMFlib 11.8 13.5 9.4 4.80
MFS 0.0 0.0 0.0 1.00
Table 3: Unsupervised evaluation: V-Measure
the number of clusters. But increasing the number
of clusters harms paired F-Score, which results in
bad supervised recalls. NB attains a very high V-
Measure with few induced clusters, which indicates
that those clusters are high quality. Other systems
use more induced clusters but fail to attain the V-
Measure of NB.
5.4.3 Paired F-Score
Paired F-Score is the harmonic mean of paired re-
call and paired precision. Paired recall is fraction of
pairs belonging to the same gold class that belong
to the same cluster. Paired precision is fraction of
pairs belonging to the same cluster that belong to
the same class. See Table 4 for details of paired F-
Score evaluation.
As with V-Measure, it is possible to attain a high
paired F-Score by producing only one cluster. The
baseline, MFS, attains 100% paired recall, which to-
gether with the poor performance of WSI systems
makes its paired F-Score difficult to beat. V-Measure
and paired F-Score are meaningful when systems
produce about the same numbers of clusters as the
numbers of classes and attain high scores on these
metrics.
FS(%) all nouns verbs #cl
MFS 63.5 57.0 72.7 1.00
NB 52.9 52.5 53.5 3.42
NB0 46.8 47.4 46.0 3.77
UoY 49.8 38.2 66.6 11.54
NMFlib 45.3 42.2 49.8 4.80
Hermit 26.7 24.4 30.1 10.78
Table 4: Unsupervised evaluation: paired F-Score
5.4.4 Supervised Recall
For the supervised task, the test data is split into
two groups: one for mapping clusters to classes and
the other for standard WSD evaluation. 2 differ-
ent split schemes (80% mapping, 20% evaluation and
60% mapping, 40% evaluation) are evaluated. 5 ran-
dom splits are averaged for each split scheme. Map-
ping is induced automatically by the program pro-
vided by organizers. See Table 5 for details of super-
vised recall evaluation (#s is the average number of
classes mapped from clusters).2
SR(%) all nouns verbs #s
NB 65.4 62.6 69.5 1.72
NB0 63.5 59.8 69.0 1.76
NMFlib 62.6 57.3 70.2 1.82
UoY 62.4 59.4 66.8 1.51
MFS 58.7 53.2 66.6 1.00
Hermit 58.3 53.6 65.3 2.06
Table 5: Supervised evaluation: supervised recall, 80%
mapping and 20% evaluation
Overall our system performs better than other sys-
tems with respect to supervised recall. When a sys-
tem has higher V-Measure and paired F-Score on
nouns than another system, it achieves a higher su-
pervised recall on nouns too. However, this behav-
ior is not observed on verbs. For example, NB has
higher V-Measure and paired F-Score on verbs than
NMFlib but NB attains a lower supervised recall on
verbs than NMFlib. It is difficult to see which verbs
clusters are better than some other clusters.
6 Conclusion
Of the four SemEval-2010 evaluation metrics, and
restricting ourselves to systems obeying the evalua-
tion conditions for that competition, our new model
achieves new best results on three. The exception is
paired F-Score. As we note earlier, this metric tends
to assign very high scores when every word receives
only one sense, and our model is bested by the base-
line system that does exactly that.
260-40 split is omitted here due to almost identical result.
1436
If we loosen possible comparison systems, the
LDA/HDP model of Lau et al (2012) achieves supe-
rior numbers to ours for the two supervised metrics,
but at the expense of requiring LDA type processing
on the test data, something that the SemEval or-
ganizers ruled out, presumably with the reasonable
idea that such processing would not be feasible in
the real world. More generally, their system assigns
many senses (about 10) to each word, and thus no-
doubt does poorly on the paired F-Score (they do not
report results on V-Measure and paired F-Score).
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 7?12. Asso-
ciation for Computational Linguistics.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003.
Latent dirichlet alocation. the Journal of machine
Learning research, 3:993?1022.
Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, EACL ?09, pages 103?111,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Arthur P Dempster, Nan M Laird, and Donald B Ru-
bin. 1977. Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), pages 1?38.
Jason Eisner and Damianos Karakos. 2005. Bootstrap-
ping without the boot. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, HLT ?05, pages
395?402, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
David Jurgens and Keith Stevens. 2010. Hermit: Flex-
ible clustering for the semeval-2 wsi task. In Pro-
ceedings of the 5th international workshop on semantic
evaluation, pages 359?362. Association for Computa-
tional Linguistics.
Ioannis Korkontzelos and Suresh Manandhar. 2010. Uoy:
Graphs of unambiguous vertices for word sense induc-
tion and disambiguation. In Proceedings of the 5th
international workshop on semantic evaluation, pages
355?358. Association for Computational Linguistics.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense induc-
tion for novel sense detection. In Proceedings of the
13th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 591?601.
Association for Computational Linguistics.
Suresh Manandhar, Ioannis P Klapaftis, Dmitriy Dli-
gach, and Sameer S Pradhan. 2010. Semeval-2010
task 14: Word sense induction & disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 63?68. Association for Com-
putational Linguistics.
Ted Pedersen. 2010. Duluth-wsi: Senseclusters applied
to the sense induction task of semeval-2. In Proceed-
ings of the 5th international workshop on semantic
evaluation, pages 363?366. Association for Computa-
tional Linguistics.
Tim Van de Cruys, Marianna Apidianaki, et al 2011.
Latent semantic word sense induction and disambigua-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies (ACL/HLT), pages 1476?
1485.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceedings
of the 33rd annual meeting on Association for Compu-
tational Linguistics, ACL ?95, pages 189?196, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
1437
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1765?1775,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Generative Joint, Additive, Sequential Model
of Topics and Speech Acts in Patient-Doctor Communication
Byron C. Wallace?, Thomas A. Trikalinos?, M. Barton Laws?,
Ira B. Wilson? and Eugene Charniak?
?Dept. of Health Services, Policy & Practice, Brown University, Providence, RI
?Dept. of Computer Science, Brown University, Providence, RI
{byron wallace, thomas trikalinos, michael barton laws
ira wilson, eugene charniak}@brown.edu
Abstract
We develop a novel generative model of con-
versation that jointly captures both the top-
ical content and the speech act type asso-
ciated with each utterance. Our model ex-
presses both token emission and state tran-
sition probabilities as log-linear functions of
separate components corresponding to topics
and speech acts (and their interactions). We
apply this model to a dataset comprising anno-
tated patient-physician visits and show that the
proposed joint approach outperforms a base-
line univariate model.
1 Introduction
Communication involves at least two aspects: the
words one says and the acts one performs in saying
them. Examples of the latter include asking ques-
tions, issuing commands, and so on. These are re-
ferred to as speech acts under the sociolinguistic the-
ory of Austin (1955), which was further developed
by Searle (1969; 1985). Recognizing speech acts is
crucial to understanding communication because a
speaker?s meaning is only partially captured by the
words they use; much of their intent is expressed im-
plicitly via speech acts (Searle, 1969).
On this view, conversational utterances can be as-
signed both a topic and a speech act. The former
describes the subject matter of what was said and
the latter captures the ?social act? (e.g., promising)
performed by saying it. For example, the utterance
?Obama won the election? is topically political and
is an example of an information giving speech act.
?Did Obama win the election??, meanwhile, belongs
Role Utterance Topic Speech act
D Let me just write down some
of these issues here so I get
them straight in my mind.
Logistics Commissive
P Doctor you ain?t got to tell me
nuttin?.
Socializing Directive
P I?m in very good hands when
I?m around you.
Socializing Give Info.
P If push comes to a shove, you
open the window and throw
me out.
Socializing Humor/Levity
D I wanted to ask you, too - Biomedical Conv. Mgmt.
D you know you had that
colonic polyp -
Biomedical Ask Q.
D - is it two years from now that
they?re going to be doing the
repeat?
Biomedical Ask Q.
P Yeah. Biomedical Conv. Mgmt.
D We?ll do the repeat coloscopy
in about two years.
Biomedical Give Info.
Table 1: An excerpt from a patient-doctor interaction,
annotated with topic and speech act codes. The D and
P roles denote doctor and patient, respectively. Conv.
Mgmt. abbreviates conversation management; Ask Q. ab-
breviates ask question.
to the same topic but is a question. Both aspects are
necessary to understand conversation.
Previous computational work on speech acts ?
which we review in Section 6 ? has modeled them
in isolation (Perrault and Allen, 1980; Stolcke et al,
1998; Stolcke et al, 2000; Kim et al, 2010), i.e.,
independent of topical content. But a richer model
would account for both speech acts and the contex-
tualizing topic of each utterance. To this end, we de-
velop a novel joint, generative model of topics and
speech acts.
We focus on physician-patient communication as
a motivating domain. This is of interest because
1765
it is widely appreciated that effective communica-
tion is an integral part of clinical practice (Irwin and
Richardson, 2006; Makoul, 2001; Teutsch, 2003).
We provide an excerpt of a conversation between a
patient and their doctor annotated with topics and
speech acts in Table 1. Such annotations can provide
substantive insights into how doctors communicate
with patients (Ong et al, 1995).
A concrete example of this is the use of topic
and speech act codes to assess the efficacy of an
intervention meant to influence physician-patient
communication regarding adherence to antiretrovi-
ral (ARV) medication (Wilson et al, 2010). To
measure the effect of the intervention, investigators
performed a randomized control trial in which they
quantified change in communication patterns by tal-
lying the number of information giving speech acts
that fell under the ARV adherence topic. Without
assigning both topics and speech acts to utterances,
this analysis would not have been possible.
In this work, we develop a novel component-
based generative model for bivariate, sequentially
structured problems. Our approach extends the re-
cently proposed Sparse Additive Generative (SAGE)
model (Eisenstein et al, 2011) and similar recently
developed additive models (Paul and Dredze, 2012;
Paul et al, 2013) to the case of supervised sequen-
tial tasks to capture the joint conditional influence
of topics and speech acts, both with respect to token
generation and state transitions. For brevity, we refer
to this generative Joint, Additive, Sequential model
as JAS. In contrast to previous work on speech acts,
JAS provides a single, coherent generative model of
conversations. And because it is component-based,
this model provides a flexible framework for analyz-
ing communication patterns. We demonstrate that
JAS outperforms a generative univariate baseline in
topic/speech act prediction. Further, we automati-
cally reproduce an analysis of the aforementioned
randomized control trial, and in doing so show that
JAS reproduces the results more faithfully than a
univariate approach.
2 The Markov-Multinomial Model
We begin by considering a baseline generative ap-
proach to modeling topics and speech acts indepen-
dently. This simple approach was used by Stolcke et
al. (2000) to model speech acts. It accounts for only
a single output at each time point yt ? Y , and hence
here we model topics and speech acts independently.
A straight-forward (albeit na??ve) alternative
would be to treat the Cartesian product of topics
and speech acts as a single output space on which
emissions and transitions are conditioned, but this
space is too large and sparse for this approach to be
practicable. We note that the fully coupled HMM
(Brand et al, 1997) suffers from a similar exponen-
tial output state problem. The related factorial HMM
(FHMM) (Ghahramani and Jordan, 1997; Van Gael
et al, 2008), meanwhile, imposes unwarranted (in
our case) independence assumptions with respect to
state transitions along parallel chains, does not obvi-
ously lend itself to discrete observations (typically
Gaussians are assumed), and does not scale well
enough (in terms of training time) to be feasible for
our application.
The Markov-Multinomial (MM) comprises two
components; transitions and emissions. The former
is modeled by making a first-order Markov assump-
tion, specifically:
P (yt|y0, ..., yt?1) = P (yt|yt?1) = ?yt?1,yt (1)
Emissions can be modeled via a multinomial
that captures the conditional probabilities of to-
kens given labels. Denoting an utterance (an utter-
ance comprises the words corresponding to a single
speech act; see Section 4) at time t by ut and its la-
bel by yt, and making the standard na??ve assumption
that words are generated independently conditioned
on a label, we have:
P (ut|yt) =
?
w?ut
P (w|yt) =
?
w?ut
?yt,w (2)
Both sets of parameters (the ??s and the ? ?s) can be
estimated straight-forwardly using maximum like-
lihood (i.e., using observed counts). We can use
Viterbi decoding (Rabiner and Juang, 1986) to make
predictions for new sequences, as usual. To make
both topic and speech act predictions, we simply in-
duce models for each and make predictions indepen-
dently.
3 JAS: A Joint, Additive, Sequential Model
An obvious shortcoming of the simple MM model
outlined above is that it treats topics and speech acts
1766
as statistically independent. They are not (as con-
firmed at statistical significance p < .001 using a
?2 test). One would prefer a more expressive model
that conditions topic and speech act transitions as
well as the production of utterances jointly on both
the current topic and the current speech act.
More specifically, we would like a model that re-
flects the assumption that some latent intent gives
rise to both the topic and the speech act associated
with an utterance. This is consistent with Searle?s
(1969) notion of perlocutionary effects; one per-
forms speech acts with the aim of getting someone
to do something. Intent gives rise to the current
topic and speech act, and the current intent affects
the next; this induces a correlation between adjacent
topics and speech acts. This conceptual model is de-
picted graphically in the left-half of Figure 1.
The latent intent may be, e.g., to encourage a pa-
tient to take their medication more regularly. In our
application the topical content may be ARV adher-
ence and the type of speech act would be selected
by the provider (presumably to maximize the likeli-
hood of patient adherence). For example, she may
opt to urge imperatively (?You really need to take
your medicine?) or to implore with a question (?Will
you please remember to take your medicine??). Be-
cause we have no way of explicitly modeling intent
(it is never observed), we instead rely on variables
for which we have annotations (i.e., the topics and
speech acts; see Figure 1). We next describe the
model in more detail.
We refer to the topic set by Y , the speech act
set by S and the vocabulary as W . We denote the
(log of the) background probability of word w by
?w, and we will denote components corresponding
to deviations from ?w due to a specific topic (speech
act) by ?yw (?sw). Further, we include the component
?y,sw to capture interaction effects between topics and
speech acts. We assume that the conditional proba-
bility of word w belonging to an utterance ut with
corresponding topic yt and speech act st is log-linear
with respect to these components, i.e.:
P (w|yt, st) =
1
Zw
exp{?w+?
yt
w +?
st
w +?
st,yt
w } (3)
Where Zw is a normalizing term (implicitly condi-
tioned on yt and st) defined as:
Zw =
?
w??W
exp{?w? + ?
yt
w? + ?
st
w? + ?
st,yt
w? } (4)
We make the standard na??ve assumption that words
are generated independently, given the topic and
speech act of the utterance to which they belong:
P (ut|yt, st) =
?
w?ut
P (w|yt, st) (5)
The per-token emission probability just described
falls under the additive generative family of models
recently proposed by Eisenstein et al (2011). How-
ever, in addition to conditional token emission prob-
abilities, here we need also to model the transition
probabilities such that the likelihood of transition-
ing to topic yt (and to speech act st) reflects both the
previous topic and the previous speech act, captur-
ing the dependencies illustrated in Figure 1. To this
end, we model topic and speech act transition proba-
bilities as log-linear functions of the preceding topic
and speech act.
We denote log of the background topic frequen-
cies by piY , and components capturing the influence
of transitioning to topic yt due to the preceding topic
and speech act by ?yt?1,yt and ?st?1,yt respectively.
We also include a component ?(yt?1,st?1),yt that cor-
responds to the interaction effect on topic transi-
tion probability due to the preceding topic/speech
act pair. We then model the topic transition prob-
ability (given the preceding states) as:
P (yt|yt?1, st?1) =
1
Zy
exp{piYyt +?yt?1,yt +?st?1,yt +?(yt?1,st?1),yt}
(6)
Where Zy is a normalizing term for the topic transi-
tions (implicitly conditioned on st?1, yt?1):
Zy =
?
y??Y
exp{piYy?+?yt?1,y?+?st?1,y?+?(yt?1,st?1),y?}
(7)
Similarly, denoting by piS log-transformed speech
act background frequencies, and including analo-
gous components as above that correspond to the in-
fluence of the preceding topic, speech act and their
interaction on transitioning into speech act st, we
1767
Topic
t-1
Speech Act
t-1
Utterance
t-1
Topic
t
Speech Act
t
Utterance
t
Intent
t-1
Intent
t
Topic
t-1
Speech Act
t-1
Utterance
t-1
Topic
t
Speech Act
t
Utterance
t
Figure 1: The generative story of utterances, depicted graphically. On the left we show our motivating conceptualiza-
tion: a latent intent gives rise to both the topic and speech acts; these, in turn, jointly induce a distribution over words
and transitions. On the right we show our operationalization of this concept. For clarity, we have denoted arrows
capturing influence due to topics with dotted lines.
have:
P (st|st?1, yt?1) =
1
Zs
exp{piSst +?st?1,st +?yt?1,st +?(yt?1,st?1),st}
(8)
Where Zs is a normalizing constant for speech acts
analogous to Equation 7. Putting things together:
P (yt, st|st?1, yt?1, ut) =
P (ut|yt, st) ? P (yt|yt?1, st?1) ? P (st|st?1, yt?1)
(9)
As implied by Figure 1, this model assumes that the
topic and speech act at time t are conditionally in-
dependent given the preceding topic and speech act
(yt?1 and st?1). This is intuitively agreeable be-
cause time intervenes as a blocking factor; condi-
tioning the current topic on the current speech act
(or vice versa) would contradict the fact that these
occur simultaneously. Instead, the correlation is in-
duced by the preceding topic/speech act pair. (That
said, this is still a simplifying assumption, as one
may instead choose to model speech act selection as
conditional on topic (Traum and Larsson, 2003).)
Predictions can again be made via Viterbi de-
coding (Rabiner and Juang, 1986) over a matrix of
pairs of joint topic/speech act states. The strategy of
modeling (additive) components allows JAS to avoid
problems due to sparsity in this large output space.
Model parameters can be estimated using stan-
dard optimization techniques. We fix the ?back-
ground? frequencies ?, piY , piS to the log of the
corresponding observed proportions of words, top-
ics and speech acts, respectively. For the remaining
parameters, one can use descent-based optimization
methods. The partial derivative for the topic-to-topic
transition component ?y,y? with respect to the likeli-
hood, for example, is:
?
??y,y?
=
?
s?S
C(y,s),y? ? P (y
?|y, s)C(y,s),? (10)
Where C(y,s),y? denotes the observed count of tran-
sitions from topic/speech act pair (y, s) to y?, and
C(y,s),? denotes the total number of observed transi-
tions out of this pair. The term P (y?|y, s) is with
respect to the current parameter estimates and is
defined in Equation 6. The partial derivatives for
the other component parameters (both transition and
emission) are analogous. We use a Newton opti-
mization method similar to the approach outlined by
Eisenstein et al (2011).1 We assess convergence
by calculating predictive performance on a held-out
portion (5%) of the training dataset at each step,
halting the descent when this declines.
4 Dataset
We use a corpus of patient-provider visits annotated
with Generalized Medical Interaction Anaylsis Sys-
tem (GMIAS) codes. The GMIAS has been used
to: characterize interaction processes in physician-
patient communication about ARV adherence in the
1With the exception that we do not explicitly model the dis-
tribution over component variances.
1768
Topic; Speech act Count (prevalence)
ARV Adherence; Ask Q 2939 (0.013)
ARV Adherence; Commissive 245 (0.001)
ARV Adherence; Continuation 328 (0.001)
ARV Adherence; Conv. Management 4298 (0.018)
ARV Adherence; Directive 1650 (0.007)
ARV Adherence; Empathy 111 (0.000)
ARV Adherence; Give Information 12796 (0.055)
ARV Adherence; Humor/Levity 46 (0.000)
ARV Adherence; Missing/other 977 (0.004)
ARV Adherence; Social-Ritual 15 (0.000)
Biomedical; Ask Q 13753 (0.059)
Biomedical; Commissive 1049 (0.005)
Biomedical; Continuation 1005 (0.004)
Biomedical; Conv. Management 17611 (0.076)
Biomedical; Directive 4617 (0.020)
Biomedical; Empathy 423 (0.002)
Biomedical; Give Information 54231 (0.233)
Biomedical; Humor/Levity 255 (0.001)
Biomedical; Missing/other 4426 (0.019)
Biomedical; Social-Ritual 119 (0.001)
Logistics; Ask Q 5517 (0.024)
Logistics; Commissive 2308 (0.010)
Logistics; Continuation 435 (0.002)
Logistics; Conv. Management 9672 (0.042)
Logistics; Directive 5148 (0.022)
Logistics; Empathy 100 (0.000)
Logistics; Give Information 23351 (0.101)
Logistics; Humor/Levity 135 (0.001)
Logistics; Missing/other 2732 (0.012)
Logistics; Social-Ritual 285 (0.001)
Missing/other; Ask Q 820 (0.004)
Missing/other; Commissive 70 (0.000)
Missing/other; Continuation 1173 (0.005)
Missing/other; Conv. Management 1605 (0.007)
Missing/other; Directive 523 (0.002)
Missing/other; Empathy 48 (0.000)
Missing/other; Give Information 3994 (0.017)
Missing/other; Humor/Levity 27 (0.000)
Missing/other; Missing/other 12103 (0.052)
Missing/other; Social-Ritual 69 (0.000)
Psycho-Social; Ask Q 2933 (0.013)
Psycho-Social; Commissive 164 (0.001)
Psycho-Social; Continuation 208 (0.001)
Psycho-Social; Conv. Management 4433 (0.019)
Psycho-Social; Directive 787 (0.003)
Psycho-Social; Empathy 262 (0.001)
Psycho-Social; Give Information 15521 (0.067)
Psycho-Social; Humor/Levity 63 (0.000)
Psycho-Social; Missing/other 1199 (0.005)
Psycho-Social; Social-Ritual 36 (0.000)
Socializing; Ask Q 1283 (0.006)
Socializing; Commissive 79 (0.000)
Socializing; Continuation 85 (0.000)
Socializing; Conv. Management 2166 (0.009)
Socializing; Directive 222 (0.001)
Socializing; Empathy 73 (0.000)
Socializing; Give Information 8981 (0.039)
Socializing; Humor/Levity 306 (0.001)
Socializing; Missing/other 849 (0.004)
Socializing; Social-Ritual 1685 (0.007)
Table 2: Topic/speech act pairs and their counts.
context of an intervention trial (Wilson et al, 2010);
analyze communication about sexual risk behavior
(Laws et al, 2011a); elucidate the association of
visit length with constructs of patient-centeredness
(Laws et al, 2011b); and to describe provider-
patient communication regarding ARV adherence
compared with communication about other issues
(Laws et al, 2012). GMIAS annotation is described
at length elsewhere,2 but we summarize it here for
completeness.
GMIAS segments conversation into utterances.
An utterance is here defined as a single completed
speech act. Previous coding systems have simply
defined an utterance as conveying a single thought
(Roter and Larson, 2002) or any independent or un-
restrictive dependent clause of a sentence (Ford and
Ford, 1995). Stolcke et al (2000) followed Meteer
et al (1995) in using ?sentence-level units?. These
definitions provide helpful guidance to coders, but
many speech acts are poorly formed grammatically,
and cannot be described as a ?clause?. Further, some
speech acts cannot be said to convey a ?thought? (or
sentence) at all, but rather are pre-syntactical (e.g.,
interjections and non-lexical utterances like laugh-
ter). In any case, most natural segmentations of con-
versations probably largely agree with intuition, and
are not likely to differ substantially.
The model we develop in this work assumes that
transcripts have been manually segmented. While
this comes at some cost, segmenting is still much
cheaper than annotating transcripts. Manually an-
notating a single visit with GMIAS codes takes 2-
4 hours and must be performed by someone with
substantive domain expertise. By contrast, segment-
ing transcripts into utterances takes at most 1/4th of
the time as annotation and can be done by a less
highly-skilled individual. That said, in future work
we hope to explore incorporating automatic segmen-
tation methods (Galley et al, 2003; Eisenstein and
Barzilay, 2008) into our approach.
Each utterance is assigned a single topic code and
a single speech act code. Inter-rater agreement has
been observed to be relatively high for this task:
Kappa between three trained annotators and a ref-
erence annotation ranged from 0.89 to 1.0 for top-
ics and 0.81 to 0.95 for speech acts. We next de-
2
https://sites.google.com/a/brown.edu/m-barton-laws/home/gmias
1769
scribe the topics and speech acts we consider in
more detail; Table 2 enumerates all pairs of these
and their respective counts in the corpus. We note
that GMIAS defines a hierarchy of both topic and
speech act codes, but here we only attempt to cap-
ture the highest level codes in these hierarchies.
Topics comprise six major categories: ARV
adherence, biomedical, logistics, missing/other,
psycho-social and socializing. Antiretroviral (ARV)
adherence applies to utterances that address ARV
medication usage. Biomedical utterances subsume
clinical observations and diagnostic conclusions.
Utterances that concern the business of conducting a
physical examination fall under logistics. The miss-
ing/other topic covers a few cases, including utter-
ances that are effectively outside of the GMIAS uni-
verse and inaudible utterances; however we note that
missing/other is a topic explicitly assigned by hu-
man annotators. The psycho-social topic includes
such issues as substance abuse, recovery, employ-
ment and relationships. Finally, socializing refers to
casual conversation unrelated to the business of the
medical visit, and to social rituals such as greetings.
There are 10 speech acts:3 ask question, commis-
sive, continuation, conversation management, direc-
tive, empathy, give information, humor/levity, miss-
ing/other, and social-ritual. Ask question is self-
explanatory. Utterances in which the speaker makes
a promise or resolves to take action are commissives.
A continuation refers to the completion of a previ-
ously interrupted speech act (these are rare). Con-
versation management describes utterances that fa-
cilitate turn-taking or guide discussion (?talk about
talk?). Directives refer to statements that look to
control or influence the behavior of the interlocutor.
Utterances that express responses to emotions, con-
cerns or feelings are coded under empathy. Com-
munication of (purported) facts falls under give in-
formation. Humor/levity captures jokes and jovial
conversation. Missing/other is the same as for top-
ics. Finally, social-ritual utterances represent for-
malities (e.g., ?thank you?).
The corpus we use includes 360 GMIAS anno-
tated patient-provider interactions (median length:
605 utterances). This data originated as part of
3These are high-level speech acts; technically each consti-
tutes a category of speech act types.
a study designed to assess the role of the patient-
provider relationship in explaining racial/ethnic dis-
parities in HIV care. Study subjects were HIV care
providers and their patients at four US care sites.
The group responsible for the data are awaiting a
decision from the institutional review board (IRB)
regarding whether we can make this data publicly
available in some form.
5 Experimental Results
Markov-Multinomial Joint Additive Sequential0.19
0.20
0.21
0.22
0.23
0.24
0.25
0.26
av
er
ag
e F
-sc
or
e
Figure 2: Mean F-scores across all topic/speech act pairs
for the Markov-Multinomial (MM; left) and the proposed
Joint Additive Sequential (JAS; right) models. The thick
black line shows the mean difference over ten different
folds; the thin grey lines describe per-fold differences.
The proposed JAS model outperforms the baseline MM
model for all folds
Our evaluation includes two parts.4 First, we
perform standard cross-validation over the afore-
mentioned 360 annotated interactions, evaluating F-
measure for each topic/speech act pair. Second,
we look to automatically reproduce an analysis of
4Source code at: https://github.com/bwallace/JAS; unfortu-
nately we do not yet have permission to post the data.
1770
a randomized control trial that assessed the efficacy
of an intervention meant to alter physician-patient
communication. We show that JAS outperforms the
baseline approach with respect to both tasks.
We emphasize that while we are here compar-
ing predictive performance, we are specifically in-
terested in fully generative models of conversations
due to the longer-term applications we have in mind.
We would like, e.g., to use this model to assess the
variation in communicative approaches across dif-
ferent doctors, and generative models are more nat-
urally amenable to answering such exploratory ques-
tions. Indeed, perhaps the main strength of the ad-
ditive component based sequential model we have
proposed here is that it will allow us to easily in-
corporate physician-specific parameters that capture
deviations in provider speech act and/or topic tran-
sition patterns. Further, we may soon have access
to many unannotated transcripts, and we would like
to learn from these; generative approaches allow
straight-forward exploitation of unlabeled data. For
these reasons, we did not experiment with discrim-
inative models, e.g., Dynamic Conditional Random
Fields (DCRFs) (Sutton et al, 2007) for this work.
5.1 Cross-fold Validation
Our aim is to measure model performance in terms
of correctly identifying both the topic and speech act
corresponding to each utterance. We quantify this
via the F-score calculated for each topic/speech act
pair that is observed at least once. One can see in
Table 2 that many such pairs have low prevalence;
this can result in undefined F-scores (e.g., when no
utterances are assigned to a given pair). In this case,
it is reasonable to treat these as zero values, as is
commonly done (Forman and Scholz, 2010). This
penalizes models when they completely fail to iden-
tify an entire class of utterances.
We first report macro-averages, that is, averages
of the individual topic/speech act pair F-scores.
Figure 2 displays the macro-averaged F-score for
each of the 10 folds (grey lines connect folds)
and the average of these (thick black line). The
JAS model achieves an average macro-averaged F-
score of .234 versus the .207 achieved by base-
line Markov-Multinomial (MM) model; JAS outper-
forms MM on every fold.
For a more granular picture, Figure 3 displays av-
erage F-score differences with respect to every indi-
vidual topic and speech act pair for which this differ-
ence was non-zero. This is the (signed) difference of
the F-score achieved using JAS minus that achieved
using the MM model; black lines thus correspond
to pairs for which JAS outperformed MM, and red
lines to pairs for which MM outperformed JAS. The
latter achieves an improvement of >= .05 for 10
pairs, and results in an F-score of > .02 below that
attained by MM only once.
The relatively low F-scores for the metrics quanti-
fying performance with respect to the cross of topic
and speech act codes belie relatively good over-
all (marginal) predictive performance. That is, we
achieve much better performance with respect to
metrics that measure topic and speech act predic-
tions independently of one another. This is due to the
very large output space under consideration (see Ta-
ble 2). Specifically, averaged over ten runs, the MM
model achieves a marginal mean topic F-score of
.667 and marginal mean speech act F-score of .516.
JAS begets a marginal mean topic F-score of .661
and a marginal mean speech act F-score of .544;
hence the JAS model incurs an F-score loss of .006
(a 0.9% decrease) with respect to marginal topic
code prediction, but improves the marginal speech
act F-score by .028 (a 5.4% increase).
5.2 (Re-)Analysis of Randomized Control Trial
We also evaluated performance by tallying model
predictions over 116 held-out cases collected from
a randomized, cross-over study of an intervention
aimed at improving physicians knowledge of pa-
tients anti-retroviral (ARV) adherence (Wilson et al,
2010). The intervention was a report given to the
physician before a routine office visit that contained
information regarding the patients ARV usage and
their beliefs about ARV therapy. To explore the ef-
ficacy of this intervention, 58 paired (116 total) au-
dio recorded visits were annotated with GMIAS; 58
correspond to visits before which the provider was
not provided with the report (control cases), while
the other 58 correspond to visits before which they
were (intervention cases).
Wilson et al (2010) demonstrated that the in-
tervention indeed increased adherence-related dia-
logue, and specifically the number of information
giving speech acts performed by the physician un-
1771
Figure 3: Average difference in F-scores corresponding to specific topic/speech act pairs, sorted by magnitude. Black
lines (extending rightward) represent pairs for which JAS outperforms the baseline model; red lines (leftward) are
pairs for which baseline performs better.
True MM JAS
control intervention control intervention control intervention
10 (4, 28) 23 (11, 39) 13 (5, 33) 27 (16, 44) 12 (5, 28) 23 (14, 40)
Table 3: Utterance counts {Median (25th, 75th per-
centile)} for the ARV/information giving topic and speech
act pair. We show the ?gold standard? (True) tallies,
which were assigned by humans, and the counts taken
using the two models, MM and JAS. The JAS model pre-
dictions are closer to the true numbers.
derneath this topic. We attempted to reproduce this
finding using automated rather manual annotations.
To this end, we trained MM and JAS models over
the aforementioned 360 annotated visits and then
used this model to generate topic and speech act
code predictions for the utterances comprising the
116 held-out visits used for the analysis (these were
not part of the training set). We then assessed the
direction and magnitude of the change in the num-
ber of ARV adherence/information giving utterances
in the paired control versus intervention cases. We
compared the results for this analysis calculated us-
ing the true (manually assigned) codes to the results
calculated using the predicted codes.
Following the original analysis (Wilson et
al., 2010), we report the median number of
ARV/information giving utterances and correspond-
ing 25th and 75th percentiles over the 58 control and
intervention visits, as counted using the true (hu-
man) annotations and using the codes predicted by
the MM and JAS models. These are reported in Ta-
ble 3. The JAS model predictions better match the
true labels in all except one case (the lower 25th for
the controls, for which it predicts the same number
as the MM model).
6 Related work
There is a relatively long history of research into
modeling conversational speech acts in computa-
tional linguistics. Perrault and Allen (1980) con-
ducted pioneering work on computationally formal-
izing speech acts, though their work pre-dates statis-
tical NLP and is therefore not directly relevant to the
present work.
Stolcke et al (2000; 1998) proposed a probabilis-
tic approach to modeling conversational speech acts
based on the Hidden Markov Model (HMM) (Ra-
biner and Juang, 1986). They were interested in
modeling an unrestricted set of conversations, and
did not impose a hierarchy on the speech acts; they
1772
therefore enumerated many more speech acts (42)
than we do in the present work (recall that we use 10
?high-level? speech acts).5 Their model has served
as the baseline approach in the present work. Stol-
cke et al also considered jointly performing speech
recognition and speech act classification.
Others have investigated visual structures of
patient-provider interactions to qualitatively assess
communication in care. Specifically, (Cretchley et
al., 2010) leveraged concept maps to explore conver-
sations between people with schizophrenia and their
carers. Briefly, this approach allowed them to (qual-
itatively) identify two distinct conversational strate-
gies used by care-takers and their patients. Angus et
al. (Angus et al, 2012) presented a similar approach
in which they used text visualization software to ex-
plore patterns of (inferred) topics in consultations.
Another thread of research has investigated classi-
fying speech acts in emails into one of a small set of
?email speech acts?, e.g., request, propose, commit
(Cohen et al, 2004; Goldstein et al, 2006). Cohen et
al. (2004) demonstrated that good performance can
be achieved for this task via existing text classifica-
tion technologies. Elsewhere, researchers have ex-
plored automatically inferring ?speech acts? in vari-
ous other online social mediums, including message
board posts (Qadir and Riloff, 2011), Wikipedia talk
pages (Ferschke et al, 2012) and Twitter (Zhang et
al., 2012).
A separate line of inquiry concerns classifying di-
alogue acts in chat. Researchers have attempted di-
alogue act classification both for 1-on-1 (Kim et al,
2010) and multi-party (Kim et al, 2012; Clark and
Popescu-Belis, 2004) online chats. Ang et al (2005)
considered the task of jointly segmenting and clas-
sifying utterances comprising multiparty meetings,
while Hsueh and Moore (2006) proposed analogous
methods for topic segmentation and labeling (other
works on topic segmentation include (Galley et al,
2003) and (Eisenstein and Barzilay, 2008)). Incor-
porating such segmentation methods into the pro-
posed model (rather than relying on inputs to be
manually segmented beforehand) would be a natu-
ral extension of this work.
Additive component models of text have recently
5We note that only 8 of the 42 speech acts appeared with
greater than 1% frequency in Stolcke et al?s corpus.
gained traction (Eisenstein et al, 2011; Paul, 2012;
Paul and Dredze, 2012; Paul et al, 2013). To our
knowledge, this is the first extension of supervised
additive component models to a sequential task.6
7 Conclusions and Future Directions
We have proposed a novel Joint, Additive, Sequen-
tial (JAS) model of conversational topics and speech
acts. In contrast to previous approaches to mod-
eling conversational exchanges, this model factors
both the current topic and the current speech act into
token emission and state transition probabilities. We
demonstrated that this model consistently outper-
forms a univariate generative baseline that treats
speech acts and topics independently. Furthermore,
we showed JAS can automatically re-produce the
analysis of a randomized control trial designed to as-
sess the efficacy of an intervention to alter physician
communication habits with high-fidelity.
The generative component-based framework we
have introduced in this work provides a means of
exploring factors in patient-physician communica-
tion. One limitation of the model we have pre-
sented is that it makes several simplifying assump-
tions around dialogue. For example, we have ig-
nored non-linearities and ?back-channels? in con-
versation, and we have ignored differences across
physicians with respect to communication styles.
Going forward, we hope to address these limita-
tions. We also plan on extending this model to in-
vestigate qualitative questions surrounding patient-
physician communication quantitatively. For exam-
ple, we are interested in investigating how communi-
cation varies across hospitals and physicians. To ex-
plore this, we can add additional components to the
transition probability terms corresponding to differ-
ent hospitals and doctors. Ultimately, we would like
to correlate patterns in physician communication (as
gleaned from the model) with objective, measured
health outcomes (e.g., patient satisfaction and adher-
ence to ARVs).
6Though Paul (2012) recently proposed ?mixed-
membership? Markov models for unsupervised conversation
modeling.
1773
8 Acknowledgements
The authors thank members of the Brown Labora-
tory for Linguistic Information Processing (BLLIP)
and Kevin Small for providing helpful feedback on
earlier versions of this work. We also thank the three
anonymous EMNLP reviewers for insightful com-
ments. This work was partially supported by the Na-
tional Institute of Mental Health (2 K24MH092242,
R34MH089279 and R01MH083595) and by NIDA
(R01DA015679).
References
Jeremy Ang, Yang Liu, and Elizabeth Shriberg. 2005.
Automatic dialog act segmentation and classification
in multiparty meetings. In Proc. ICASSP, volume 1,
pages 1061?1064.
Daniel Angus, Bernadette Watson, Andrew Smith, Cindy
Gallois, and Janet Wiles. 2012. Visualising con-
versation structure across time: Insights into effective
doctor-patient consultations. PloS one, 7(6).
John Langshaw Austin. 1955. How to do things with
words, volume 88. Harvard University Press.
Matthew Brand, Nuria Oliver, and Alex Pentland. 1997.
Coupled hidden Markov models for complex action
recognition. In Computer Vision and Pattern Recog-
nition, 1997. Proceedings., 1997 IEEE Computer So-
ciety Conference on, pages 994?999. IEEE.
Alexander Clark and Andrei Popescu-Belis. 2004.
Multi-level dialogue act tags. In Proc. SIGdial, pages
163?170.
William W Cohen, Vitor R Carvalho, and Tom M
Mitchell. 2004. Learning to classify email into speech
acts. In Proceedings of EMNLP, volume 4. sn.
Julia Cretchley, Cindy Gallois, Helen Chenery, and An-
drew Smith. 2010. Conversations between carers
and people with schizophrenia: a qualitative analy-
sis using leximancer. Qualitative Health Research,
20(12):1611?1628.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 334?343. Association for
Computational Linguistics.
J. Eisenstein, A. Ahmed, and E.P. Xing. 2011. Sparse
additive generative models of text. In Proceedings of
ICML, pages 1041?1048.
Oliver Ferschke, Iryna Gurevych, and Yevgen Chebotar.
2012. Behind the article: Recognizing dialog acts in
wikipedia talk pages. In Proceedings of the 13th Con-
ference of the European Chapter of the Association for
Computational Linguistics (EACL 2012), pages 777?
786. Citeseer.
Jeffrey D Ford and Laurie W Ford. 1995. The role of
conversations in producing intentional change in or-
ganizations. Academy of Management Review, pages
541?570.
George Forman and Martin Scholz. 2010. Apples-to-
apples in cross-validation studies: pitfalls in classifier
performance measurement. volume 12, pages 49?57.
ACM.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation of
multi-party conversation. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics, pages 562?569. Association for Compu-
tational Linguistics.
Zoubin Ghahramani and Michael I Jordan. 1997. Facto-
rial hidden Markov models. Machine learning, 29(2-
3):245?273.
Jade Goldstein, Andrew Kwasinski, Paul Kingsbury,
R Sabin, and Albert McDowell. 2006. Annotating
subsets of the enron email corpus. In Proceedings of
the Third Conference on Email and Anti-Spam. Cite-
seer.
P-Y Hsueh and Johanna D Moore. 2006. Automatic
topic segmentation and labeling in multiparty dia-
logue. In Spoken Language Technology Workshop,
2006. IEEE, pages 98?101. IEEE.
Richard S Irwin and Naomi D Richardson. 2006.
Patient-focused careusing the right tools. CHEST
Journal, 130(1 suppl):73S?82S.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2010. Classifying dialogue acts in one-on-one
live chats. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 862?871. Association for Computational Lin-
guistics.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2012. Classifying dialogue acts in multi-party
live chats.
Michael Barton Laws, Ylisabyth S Bradshaw, Steven A
Safren, Mary Catherine Beach, Yoojin Lee, William
Rogers, and Ira B Wilson. 2011a. Discussion of sex-
ual risk behavior in HIV care is infrequent and appears
ineffectual: a mixed methods study. AIDS and Behav-
ior, 15(4):812?822.
Michael Barton Laws, Lauren Epstein, Yoojin Lee,
William Rogers, Mary Catherine Beach, and Ira B
Wilson. 2011b. The association of visit length and
measures of patient-centered communication in HIV
care: A mixed methods study. Patient Education and
Counseling, 85(3):e183?e188.
1774
Michael Barton Laws, Mary Catherine Beach, Yoojin
Lee, William H Rogers, Somnath Saha, P Todd Ko-
rthuis, Victoria Sharp, and Ira B Wilson. 2012.
Provider-patient adherence dialogue in HIV care: re-
sults of a multisite study. AIDS and Behavior, pages
1?12.
Gregory Makoul. 2001. Essential elements of communi-
cation in medical encounters: the kalamazoo consen-
sus statement. Academic Medicine, 76(4):390?393.
Marie W Meteer, Ann A Taylor, Robert MacIntyre, and
Rukmini Iyer. 1995. Dysfluency annotation stylebook
for the switchboard corpus. University of Pennsylva-
nia.
Lucille ML Ong, Johanna CJM De Haes, Alaysia M
Hoos, and Frits B Lammes. 1995. Doctor-patient
communication: a review of the literature. Social sci-
ence & medicine, 40(7):903?918.
Michael Paul and Mark Dredze. 2012. Factorial lda:
Sparse multi-dimensional text models. In Advances
in Neural Information Processing Systems 25, pages
2591?2599.
Michael J. Paul, Byron C. Wallace, and Mark Dredze.
2013. What affects patient (dis)satisfaction? analyz-
ing online doctor ratings with a joint topic-sentiment
model. In AAAI Workshop on Expanding the Bound-
aries of Health Informatics Using AI (HIAI).
Michael J Paul. 2012. Mixed membership Markov mod-
els for unsupervised conversation modeling. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 94?104.
Association for Computational Linguistics.
C Raymond Perrault and James F Allen. 1980. A plan-
based analysis of indirect speech acts. Computational
Linguistics, 6(3-4):167?182.
Ashequl Qadir and Ellen Riloff. 2011. Classifying sen-
tences as speech acts in message board posts. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 748?758. Asso-
ciation for Computational Linguistics.
Lawrence Rabiner and B Juang. 1986. An introduction
to hidden Markov models. ASSP Magazine, IEEE,
3(1):4?16.
Debra Roter and Susan Larson. 2002. The roter in-
teraction analysis system (rias): utility and flexibility
for analysis of medical interactions. Patient education
and counseling, 46(4):243?251.
John R Searle. 1969. Speech acts: An essay in the phi-
losophy of language. Cambridge university press.
John R Searle. 1985. Expression and meaning: Studies
in the theory of speech acts. Cambridge University
Press.
Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates,
Noah Coccaro, Daniel Jurafsky, Rachel Martin, Marie
Meteer, Klaus Ries, Paul Taylor, and Carol Van Ess-
Dykema. 1998. Dialog act modeling for conversa-
tional speech. In AAAI Spring Symposium on Apply-
ing Machine Learning to Discourse Processing, pages
98?105.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational linguistics, 26(3):339?373.
Charles Sutton, Andrew McCallum, and Khashayar Ro-
hanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling and
segmenting sequence data. The Journal of Machine
Learning Research, 8:693?723.
Carol Teutsch. 2003. Patient-doctor communication.
The medical clinics of North America, 87(5):1115.
David R Traum and Staffan Larsson. 2003. The infor-
mation state approach to dialogue management. In
Current and new directions in discourse and dialogue,
pages 325?353. Springer.
Jurgen Van Gael, Yee Whye Teh, and Zoubin Ghahra-
mani. 2008. The infinite factorial hidden Markov
model. In Neural Information Processing Systems,
volume 21.
Ira B Wilson, M Barton Laws, Steven A Safren, Yoo-
jin Lee, Minyi Lu, William Coady, Paul R Skolnik,
and William H Rogers. 2010. Provider-focused inter-
vention increases adherence-related dialogue, but does
not improve antiretroviral therapy adherence in per-
sons with HIV. Journal of acquired immune deficiency
syndromes, 53(3):338.
Renxian Zhang, Dehong Gao, and Wenjie Li. 2012. To-
wards scalable speech act recognition in twitter: Tack-
ling insufficient training data. EACL 2012, page 18.
1775
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 169?173,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Data Driven Language Transfer Hypotheses
Ben Swanson
Brown University
Providence, RI
chonger@cs.brown.edu
Eugene Charniak
Brown University
Providence, RI
ec@cs.brown.edu
Abstract
Language transfer, the preferential second
language behavior caused by similarities
to the speaker?s native language, requires
considerable expertise to be detected by
humans alone. Our goal in this work is to
replace expert intervention by data-driven
methods wherever possible. We define a
computational methodology that produces
a concise list of lexicalized syntactic pat-
terns that are controlled for redundancy
and ranked by relevancy to language trans-
fer. We demonstrate the ability of our
methodology to detect hundreds of such
candidate patterns from currently available
data sources, and validate the quality of
the proposed patterns through classifica-
tion experiments.
1 Introduction
The fact that students with different native lan-
guage backgrounds express themselves differ-
ently in second language writing samples has
been established experimentally many times over
(Tetreault et al., 2013), and is intuitive to most
people with experience learning a new language.
The exposure and understanding of this process
could potentially enable the creation of second
language (L2) instruction that is tailored to the na-
tive language (L1) of students.
The detectable connection between L1 and L2
text comes from a range of sources. On one end of
the spectrum are factors such as geographic or cul-
tural preference in word choice, which are a pow-
erful L1 indicator. On the other end lie linguistic
phenomena such as language transfer, in which the
preferential over-use or under-use of structures in
the L1 is reflected in the use of corresponding pat-
terns in the L2. We focus on language transfer in
this work, based on our opinion that such effects
are more deeply connected to and effectively uti-
lized in language education.
The inherent challenge is that viable language
transfer hypotheses are naturally difficult to con-
struct. By the requirement of contrasting different
L1 groups, hypothesis formulation requires deep
knowledge of multiple languages, an ability re-
served primarily for highly trained academic lin-
guists. Furthermore, the sparsity of any particular
language pattern in a large corpus makes it diffi-
cult even for a capable multilingual scholar to de-
tect the few patterns that evidence language trans-
fer. This motivates data driven methods for hy-
pothesis formulation.
We approach this as a representational problem,
requiring the careful definition of a class of lin-
guistic features whose usage frequency can be de-
termined for each L1 background in both L1 and
L2 text (e.g. both German and English written
by Germans). We claim that a feature exhibiting
a sufficiently non-uniform usage histogram in L1
that is mirrored in L2 data is a strong language
transfer candidate, and provide a quantified mea-
sure of this property.
We represent both L1 and L2 sentences in a
universal constituent-style syntactic format and
model language transfer hypotheses with con-
tiguous syntax sub-structures commonly known
as Tree Substitution Grammar (TSG) fragments
(Post and Gildea, 2009)(Cohn and Blunsom,
2010). With these features we produce a concise
ranked list of candidate language transfer hypothe-
ses and their usage statistics that can be automati-
cally augmented as increasing amounts of data be-
come available.
169
2 Related Work
This work leverages several recently released data
sets and analysis techniques, with the primary
contribution being the transformations necessary
to combine these disparate efforts. Our analy-
sis methods are closely tied to those described
in Swanson and Charniak (2013), which con-
trasts techniques for the discovery of discrimina-
tive TSG fragments in L2 text. We modify and
extend these methods to apply to the universal de-
pendency treebanks of McDonald et al. (2013),
which we will refer to below to as the UTB. Bilin-
gual lexicon construction (Haghighi et al., 2008)
is also a key component, although previous work
has focused primarily on nouns while we focus on
stopwords. We also transform the UTB into con-
stituent format, in a manner inspired by Carroll
and Charniak (1992).
There is a large amount of related research in
Native Language Identification (NLI), the task of
predicting L1 given L2 text. This work has culmi-
nated in a well attended shared task (Tetreault et
al., 2013), whose cited report contains an excellent
survey of the history of this task. In NLI, however,
L1 data is not traditionally used, and patterns are
learned directly from L2 text that has been anno-
tated with L1 labels. One notable outlier is Brooke
and Hirst (2012), which attempts NLI using only
L1 data for training using large online dictionar-
ies to tie L2 English bigrams and collocations to
possible direct translations from native languages.
Jarvis and Crossley (2012) presents another set of
studies that use NLI as a method to form language
transfer hypotheses.
3 Methodology
The first of the four basic requirements of our pro-
posed method is the definition of a class of features
F such that a single feature F ? F is capable
of capturing language transfer phenomenon. The
second is a universal representation of both L1 and
L2 data that allows us to count the occurrences of
any F in an arbitrary sentence. Third, as any suf-
ficiently expressive F is likely to be very large, a
method is required to propose an initial candidate
list C ? F . Finally, we refine C into a ranked list
H of language transfer hypotheses, where H has
also been filtered to remove redundancy.
In this work we define F to be the set of Tree
Substitution Grammar (TSG) fragments in our
data, which allows any connected syntactic struc-
ture to be used as a feature. As such, our universal
representation of L1/L2 data must be a constituent
tree structure of the general form used in syntactic
parsing experiments on the Penn Treebank. The
UTB gets us most of the way to our goal, defining
a dependency grammar with a universal set of part
of speech (POS) tags and dependency arc labels.
Two barriers remain to the use of standard TSG
induction algorithms. The first is to define a map-
ping from the dependency tree format to con-
stituency format. We use the following depen-
dency tree to illustrate our transformation.
ROOT DT NN VBZ PRP
The poodle chews it
root
det
nsubj
dobj
Under our transformation, the above dependency
parse becomes
ROOT
root
VBZ-L
nsubj
NN-L
det
DT
the
NN
poodle
VBZ
chews
VBZ-R
dobj
PRP
it
We also require a multilingual lexicon in the form
of a function M
L
(w) for each language L that
maps words to clusters representing their meaning.
In order to avoid cultural cues and reduce noise
in our mapping, we restrict ourselves to clusters
that correspond to a list of L2 stopwords. Any L2
words that do not appear on this list are mapped
to the unknown ?UNK? symbol, as are all for-
eign words that are not good translations of any
L2 stopword. Multiple words from a single lan-
guage can map to the same cluster, and it is worth
noting that this is true for L2 stopwords as well.
To determine the mapping functions M
L
we
train IBM translation models in both directions be-
tween the L2 and each L1. We create a graph in
which nodes are words, either the L2 stopwords or
any L1 word with some translation probability to
170
or from one of the L2 stopwords. The edges in this
graph exist only between L2 and L1 words, and
are directed with weight equal to the IBM model?s
translation probability of the edge?s target given
its source. We construct M
L
by removing edges
with weight below some threshold and calculating
the connected components of the resulting graph.
We then discard any cluster that does not contain
at least one word from each L1 and at least one L2
stopword.
To propose a candidate list C, we use the TSG
induction technique described in Swanson and
Charniak (2013), which simultaneously induces
multiple TSGs from data that has been partitioned
into labeled types. This method permits linguisti-
cally motivated constraints as to which grammars
produce each type of data. For an experimental
setup that considers n different L1s, we use 2n+1
data types; Figure 1 shows the exact layout used
in our experiments. Besides the necessary n data
types for each L1 in its actual native language form
and n in L2 form, we also include L2 data from
L2 native speakers. We also define 2n + 1 gram-
mars. We begin with n grammars that can each
be used exclusively by one native language data
type, representing behavior that is unique to each
native language (grammars A-C in Figure 1) . This
is done for the L2 as well (grammar G). Finally,
we create an interlanguage grammar for each of
our L1 types that can be used in derivation of both
L1 and L2 data produced by speakers of that L1
(grammars D-F).
The final step is to filter and rank the TSG frag-
ments produced in C, where filtering removes re-
dundant features and ranking provides some quan-
tification of our confidence in a feature as a lan-
guage transfer hypothesis. Swanson and Char-
niak (2013) provides a similar method for pure L2
data, which we modify for our purposes. For re-
dundancy filtering no change is necessary, and we
use their recommended Symmetric Uncertainty
method. For a ranking metric of how well a frag-
ment fits the profile of language transfer we adopt
the expected per feature loss (or risk) also de-
scribed in their work. For an arbitrary feature F ,
this is defined as
R(F ) =
1
|T
F
|
?
t?T
F
P
F
(L 6= L
?
t
)
where T
F
is the subset of the test data that contains
the feature F , and L
?
t
is the gold label of test da-
L2
Data
L1
Data
DE DE
FR FR
ES ES
EN
A
B
C
D
E
F
G
Figure 1: The multi-grammar induction setup used
in our experiments. Squares indicate data types,
and circles indicate grammars. Data type labels
indicate the native language of the speaker, and all
L2 data is in English.
tum t. While in their work the predictive distribu-
tion P
F
(L) is determined by the observed counts
of F in L2 training data, we take our estimates
directly from the L1 data of the languages under
study. This metric captures the extent to which the
knowledge of a feature F ?s L1 usage can be used
to predict its usage in L2.
The final result is a ranked and filtered list of hy-
potheses H . The elements of H can be subjected
to further investigation by experts and the accom-
panying histogram of counts contains the relevant
empirical evidence. As more data is added, the
uncertainty in the relative proportions of these his-
tograms and their corresponding R is decreased.
One additional benefit of our method is that TSG
induction is a random process, and repeated runs
of the sampling algorithm can produce different
features. Since redundancy is filtered automati-
cally, these different feature lists can be combined
and processed to potentially find additional fea-
tures given more computing time.
4 Results
Limited by the intersection of languages across
data sets, we take French, Spanish, and German
as our set of L1s with English as the L2. We use
the UTB for our native language data, which pro-
vides around 4000 sentences of human annotated
text for each L1. For our L2 data we use the ETS
Corpus of Non-Native English (Blanchard et al.,
2013), which consists of over 10K sentences per
L1 label drawn from TOEFL
r
exam essays. Fi-
171
nally, we use the Penn Treebank as our source of
native English data, for a total of seven data types;
four in English, and one in each L1.
When calculating metrics such as redundancy
and R(F ) we use all available data. For TSG
sampling, we balance our data sets to 4000 sen-
tences from each data type and sample using the
Enbuske sampler that was released with Swanson
and Charniak (2013). To construct word clusters,
we use Giza++ (Och and Ney, 2003) and train on
the Europarl data set (Koehn, 2005), using .25 as
a threshold for construction on connected compo-
nents.
We encourage the reader to peruse the full list
of results
1
, in which each item contains the infor-
mation in the following example.
advcl
VERB-L
mark
VERB
110
VERB-R
ES DE FR
L1 4.2 0.0 0.0
L2 2.3 0.3 0.3
This fragment corresponds to an adverbial
clause whose head is a verb in the cluster 110,
which contains the English word ?is? and its vari-
ous translations. This verb has a single left depen-
dent, a clause marker such as ?because?, and at
least one right dependent. Its prevalence in Span-
ish can explained by examining the translations of
the English sentence ?I like it because it is red?.
ES Me gusta porque es rojo.
DE Ich mag es, weil es rot ist.
FR Je l?aime parce qu?il est rouge.
Only in the Spanish sentence is the last pronoun
dropped, as in ?I like it because is red?. This
observation, along with the L1/L2 profile which
shows the count per thousand sentences in each
language provides a strong argument that this pat-
tern is indeed a form of language transfer.
Given our setup of three native languages, a fea-
ture with R(F ) < .66 is a candidate for language
transfer. However, several members of our filtered
list have R(F ) > .66, which is to say that their
1
bllip.cs.brown.edu/download/interlanguage corpus.pdf
 0.34
 0.36
 0.38
 0.4
 0.42
 0.44
 0  10  20  30  40  50  60  70  80  90C
las
sif
ica
tio
n A
cc
ura
cy
 (%
)
Sentences Per Test Case
Figure 2: Creating test cases that consist of sev-
eral sentences mediates feature sparsity, providing
clear evidence for the discriminative power of the
chosen feature set.
L2 usage does not mirror L1 usage. This is to be
expected in some cases due to noise, but it raises
the concern that our features withR(F ) < .66 are
also the result of noise in the data. To address this,
we apply our features to the task of cross language
NLI using only L1 data for training. If the varia-
tion ofR(F ) around chance is simply due to noise
then we would expect near chance (33%) classifi-
cation accuracy. The leftmost point in Figure 2
shows the initial result, using boolean features in
a log-linear classification model, where a test case
involves guessing an L1 label for each individual
sentence in the L2 corpus. While the accuracy
does exceed chance, the margin is not very large.
One possible explanation for this small margin
is that the language transfer signal is sparse, as it
is likely that language transfer can only be used to
correctly label a subset of L2 data. We test this by
combining randomly sampled L2 sentences with
the same L1 label, as shown along the horizontal
axis of Figure 2. As the number of sentences used
to create each test case is increased, we see an in-
crease in accuracy that supports the argument for
sparsity; if the features were simply weak predic-
tors, this curve would be flat. The resulting margin
is much larger, providing evidence that a signifi-
cant portion of our features with R(F ) < .66 are
not selected due to random noise in R and are in-
deed connected to language transfer.
The number and strength of these hypotheses is
easily augmented with more data, as is the number
of languages under consideration. Our results also
motivate future work towards automatic genera-
tion of L1 targeted language education exercises,
and the fact that TSG fragments are a component
of a well studied generative language model makes
them well suited to such generation tasks.
172
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins,
Aoife Cahill, and Martin Chodorow. 2013. Toefl11:
A corpus of non-native english. Technical report,
Educational Testing Service.
Julian Brooke and Graeme Hirst. 2012. Measur-
ing Interlanguage: Native Language Identification
with L1-influence Metrics. In Proceedings of the
Eighth International Conference on Language Re-
sources and Evaluation (LREC-2012), pages 779?
784, Istanbul, Turkey, May. European Language Re-
sources Association (ELRA). ACL Anthology Iden-
tifier: L12-1016.
Glenn Carroll and Eugene Charniak. 1992. Two exper-
iments on learning probabilistic dependency gram-
mars from corpora. Technical Report CS-92-16,
Brown University, Providence, RI, USA.
Trevor Cohn and Phil Blunsom. 2010. Blocked infer-
ence in bayesian tree substitution grammars. pages
225?230. Association for Computational Linguis-
tics.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In ACL, pages 771?779.
Scott Jarvis and Scott Crossley, editors. 2012. Ap-
proaching Language Transfer Through Text Classi-
fication: Explorations in the Detection-based Ap-
proach, volume 64. Multilingual Matters Limited,
Bristol, UK.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. MT Summit.
Ryan T. McDonald, Joakim Nivre, Yvonne
Quirmbach-Brundage, Yoav Goldberg, Dipan-
jan Das, Kuzman Ganchev, Keith Hall, Slav Petrov,
Hao Zhang, Oscar T?ackstr?om, Claudia Bedini,
N?uria Bertomeu Castell?o, and Jungmee Lee. 2013.
Universal dependency annotation for multilingual
parsing. In ACL (2), pages 92?97.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
45?48. Association for Computational Linguistics.
Ben Swanson and Eugene Charniak. 2013. Extracting
the native language signal for second language ac-
quisition. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 85?94, Atlanta, Georgia, June. As-
sociation for Computational Linguistics.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building
Educational Applications, Atlanta, GA, USA, June.
Association for Computational Linguistics.
173
Disentangling Chat
Micha Elsner?
Brown Laboratory for Linguistic
Information Processing (BLLIP)
Eugene Charniak??
Brown Laboratory for Linguistic
Information Processing (BLLIP)
When multiple conversations occur simultaneously, a listener must decide which conversation
each utterance is part of in order to interpret and respond to it appropriately. We refer to this task
as disentanglement. We present a corpus of Internet Relay Chat dialogue in which the various
conversations have been manually disentangled, and evaluate annotator reliability. We propose
a graph-based clustering model for disentanglement, using lexical, timing, and discourse-based
features. The model?s predicted disentanglements are highly correlated with manual annotations.
We conclude by discussing two extensions to the model, specificity tuning and conversation start
detection, both of which are promising but do not currently yield practical improvements.
1. Motivation
Simultaneous conversations seem to arise naturally in both informal social interactions
and multi-party typed chat. Aoki et al?s (2006) study of voice conversations among
8?10 people found an average of 1.76 conversations (floors) active at a time, and a
maximum of four. In our chat corpus, the average is even higher, at 2.75. The typical
conversation, therefore, does not form a contiguous segment of the chatroom transcript,
but is frequently broken up by interposed utterances from other conversations.
Disentanglement (also called thread detection [Shen et al 2006], thread extraction
[Adams and Martell 2008], and thread/conversation management [Traum 2004]) is
the clustering task of dividing a transcript into a set of distinct conversations. It is
an essential prerequisite for any kind of higher-level dialogue analysis. For instance,
consider the multi-party exchange in Figure 1.
Contextually, it is clear that this corresponds to two conversations, and Felicia?s1
response excellent is intended for Chanel and Regine. A straightforward reading of the
transcript, however, might interpret it as a response to Gale?s statement immediately
preceding.
? Brown Laboratory for Linguistic Information Processing, Brown University, Providence, RI 02912.
E-mail: melsner@cs.brown.edu.
?? Brown Laboratory for Linguistic Information Processing, Brown University, Providence, RI 02912.
E-mail: ec@cs.brown.edu.
1 Real user nicknames are replaced with randomly selected identifiers for ethical reasons.
Submission received: 27 January 2009; revised submission received: 1 November 2009; accepted for
publication: 3 March 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
Figure 1
Some (abridged) conversation from our corpus.
Humans are adept at disentanglement, even in complicated environments like
crowded cocktail parties or chat rooms; in order to perform this task, they must maintain
a complex mental representation of the ongoing discourse. Moreover, they adjust their
conversational behavior to make the task easier, mentioning names more frequently
than in spoken or two-party typed dialogues (O?Neill and Martin 2003). This need for
adaptation suggests that disentanglement can be challenging even for humans, and
therefore can serve as a useful stress test for computational models of discourse.
Disentanglement has two practical applications. One is the analysis of pre-recorded
transcripts in order to extract some kind of information, such as question?answer pairs
or summaries. These tasks should probably take as input each separate conversation,
rather than the entire transcript. Another application is as part of a user-interface system
for active participants in the chat, in which users target a conversation of interest
which is then highlighted for them. Aoki et al (2003) created such a system for speech,
which users generally preferred to a conventional system?when the disentanglement
worked!
We begin in Section 2 with an overview of related work. In Section 3, we present a
new corpus of manually annotated chat room data and evaluate annotator reliability. We
give a set of metrics (Section 3.2) describing structural similarity both locally and glob-
ally. In Section 4, we propose a model which uses supervised pairwise classification to
link utterances from the same conversation, followed by a greedy inference stage which
clusters the utterances into conversations. Our system uses time gap and utterance con-
tent features. Experimental results (Section 5) show that its output is highly correlated
with human annotations. Finally, in Sections 6 and 7, we investigate two extensions to
the basic model, specificity tuning and automatic detection of conversation starts.
To our knowledge, this is the first work to evaluate interannotator agreement for
the disentanglement task. It is also the first to use a supervised method to learn weights
for different feature types, rather than relying on cosine distance with uniform or hand-
tuned feature weights. It supplements standard word repetition and time gap features
with other feature types, including very powerful features based on name mentioning,
which is common in Internet Relay Chat.
2. Related Work
Several threads of research are direct attempts to solve the disentanglement problem.
The closest to our own work is that of Shen et al (2006), which performs conversation
disentanglement on an online chat corpus. Aoki et al (2003, 2006) disentangle speech,
rather than chat. Other work has slightly different goals than ours: Adams and Martell
(2008) attempt to find all utterances of a specific single conversation in Internet and
Navy tactical chat. Camtepe et al (2005) and Acar et al (2005) perform social network
390
Elsner and Charniak Disentangling Chat
analysis, extracting groups of speakers who talk to one another. This can be considered
a disentanglement task, although, as we will see (Section 5), the assumption that each
speaker participates in only one conversation is flawed.
Adams and Martell (2008) and Shen et al (2006) publish results on human-
annotated data; although we do not have their corpora, we discuss their evaluation
metrics (Section 3.2) and give a comparison to our own results herein (Section 5). Aoki
et al (2006) construct an annotated speech corpus, but they give no results for model
performance, only user satisfaction with their conversational system. Camtepe et al
(2005) and Acar et al (2005) do give performance results, but only on synthetic data.
Adams and Martell (2008) and Shen et al (2006) treat disentanglement in the same
way we do, as a clustering task where the objects to be clustered are the individual
utterances. Both their algorithms define a notion of distance (based on the cosine) and
a threshold parameter determining how close to the cluster center an utterance must be
before the clustering algorithm adds it. This stands in contrast to our own supervised
approach, where the distance metric is explicitly tuned on training data. The super-
vised method has the advantage that it can weigh individual feature types based on
their predictivity?unsupervised methods combine features either uniformly or using
heuristic methods. There is also no need for a separate tuning phase to determine the
threshold. On the other hand, supervised methods require labeled training data, and
may be more difficult to adapt to novel domains or corpora.
The remaining papers treat the problem as one of clustering speakers, rather than
utterances. That is, they assume that during the window over which the system oper-
ates, a particular speaker is engaging in only one conversation. Camtepe et al (2005)
state an explicit assumption that this is true throughout the entire transcript; real speak-
ers, by contrast, often participate in many conversations, sequentially or sometimes
even simultaneously. Aoki et al (2003) analyze each 30-second segment of the transcript
separately. This makes the single-conversation restriction somewhat less severe, but has
the disadvantage of ignoring all events which occur outside the segment.
Acar et al (2005) attempt to deal with this problem by using a fuzzy algorithm to
cluster speakers; this assigns each speaker a distribution over conversations rather than
making a hard assignment. However, the algorithm still deals with speakers rather than
utterances, and cannot determine which conversation any particular utterance is part of.
Another problem with these two approaches is the information used for clustering.
Aoki et al (2003) and Camtepe et al (2005) detect the arrival times of messages, and
use them to construct an affinity graph between participants by detecting turn-taking
behavior among pairs of speakers. (Turn-taking is typified by short pauses between
utterances; speakers aim neither to interrupt nor leave long gaps.) Aoki et al (2006) find
that turn-taking on its own is inadequate. They motivate a richer feature set, which,
however, does not yet appear to be implemented. Acar et al (2005) add word repetition
to their feature set. However, their approach deals with all word repetitions on an equal
basis, and so degrades quickly in the presence of ?noise words? (their term for words
which are shared across conversations) to almost complete failure when only half of the
words are shared.
Adams and Martell (2008) and Shen et al (2006) use a more robust representation
for lexical features: Term frequency?inverse document frequency (TF?IDF) weighted
unigrams, as often used in information extraction. This feature set works fairly well
alone, although time is also a key feature as in the other studies. Adams and Martell
(2008) also investigate WordNet hypernyms (Miller et al 1990) as a measure of semantic
relatedness, and use the identity of the speaker of a particular utterance as a feature. It
is unclear from their results whether these latter two features are effective or not.
391
Computational Linguistics Volume 36, Number 3
To motivate our own approach, we examine some linguistic studies of discourse,
especially analysis of multi-party conversation. O?Neill and Martin (2003) point out
several ways in which multi-party text chat differs from typical two-party conversation.
One key difference is the frequency with which participants mention each others?
names. They hypothesize that name mentioning is a strategy which participants use
to make disentanglement easier, compensating for the lack of cues normally present in
face-to-face dialogue. Mentions (such as Gale?s comments to Arlie in Figure 1) are very
common in our corpus, occurring in 36% of comments, and provide a useful feature.
Another key difference is that participants may create a new conversation (floor) at
any time, a process which Sacks, Schegloff, and Jefferson (1974) calls schisming. During
a schism, a new conversation is formed, not necessarily because of a shift in the topic,
but because certain participants have refocused their attention onto each other, and
away from whoever held the floor in the parent conversation.
Despite these differences, there are still strong similarities between chat and other
conversations such as meetings. Meetings do not typically allow multiple simultaneous
conversations, but analogues to schisms do exist, in the form of digressions or ?sub-
ordinate conversations,? in which the speaker addresses someone specific, who is then
expected to answer. Some meeting analysis systems attempt to discover where these
digressions begin, who is involved in them, and who has the floor when they end;
features used in this work are relevant to disentanglement.
The task of automatically determining the intended recipient of an utterance in a
meeting is called addressee identification. It requires detecting digressions and identi-
fying their participants. Several studies attempt this task. Jovanovic and op den Akker
(2004) and Jovanovic, op den Akker, and Nijholt (2006) perform addressee identification
using a complex feature set including linguistic cues like pronouns and discourse mark-
ers, temporal information, and gaze direction. They also find that addressee identity
can be annotated with high reliability (? = .7 for one set and .8 for another). Traum
(2004) discusses the necessity for addressee identification and disentanglement in the
design of a system for military dialogues involving virtual agents. Subsequent work
(Traum, Robinson, and Stephan 2004) develops a rule-based system with high accuracy
on addressee identification.
Another meeting-related task is floor tracking, which attempts to determine which
speaker has the floor after each utterance. This task involves modeling the coordination
strategies which speakers use to acquire or give up the floor, and so provides a good
model of an ongoing conversation. A detailed analysis is given in Chen et al (2006);
Chen (2008) also gives a model for detecting floor shifts. Hawes, Lin, and Resnik (2008)
use a conditional random fields (CRF) model to predict the next speaker in Supreme
Court oral argument transcripts.
A somewhat related area of research involves environments with higher latency
than real-time chat: message boards and e-mail. Content matching approaches tend
to work better in these settings, because while many chat messages are backchannel
responses or discourse requests (Yes, Why? and so forth), longer posts tend to be
contentful. Of the two tasks, e-mail is easier; Yeh and Harnly (2006) find that heuristic
information from message headers can be useful, as can content-based matching such
as detecting quotes from earlier messages. Wang et al (2008), an analysis of a student
discussion group, is the work on which Adams and Martell (2008) is based, and uses
very similar methodology based on TF-IDF.
Our two-stage classification and partitioning algorithm draws on work on corefer-
ence resolution. Many approaches to coreference (starting with Soon, Ng, and Lim 2001)
use such an approach, building global clusters based on pairwise decisions made by a
392
Elsner and Charniak Disentangling Chat
classifier. The global partitioning problem was identified as correlation clustering, an
NP-hard problem, by McCallum and Wellner (2004).
Finally, we briefly mention some work which appeared after we developed the
system described here. Wang and Oard (2009) is another system that uses TF?IDF
unigrams, but augments these feature vectors using the information retrieval technique
of message expansion. They report results on our corpus which improve on our own.
Adams (2008) attempts to use not only WordNet but Latent Dirichlet Allocation to
describe semantic relatedness. He finds both techniques ineffective. In addition, he
annotates a large corpus of Internet Relay Chat and similarly finds that annotators have
trouble agreeing. Elsner and Schudy (2009) explore different partitioning strategies,
improving on the greedy algorithm that we present.
3. Data Set
Our data set is recorded from IRC channel ##LINUX at free?node.net, using the freely
available gaim client. ##LINUX is an unofficial tech support line for the Linux operating
system, selected because it is one of the most active chat rooms on freenode, leading
to many simultaneous conversations, and because its content is typically inoffensive.
Although it is notionally intended only for tech support, it includes large amounts of
social chat as well, such as the conversation about factory work in Figure 1.
The entire data set contains over 52 hours of chat, but we devote most of our
attention to three annotated sections: development (706 utterances; 2:06 hr) and test
(800 utterances; 1:39 hr), plus a short pilot section on which we tested our annotation
system (359 utterances; 0:58 hr).
3.1 Annotation
We recruited and paid seven university students to annotate the test section. All had
at least some familiarity with the Linux OS, although in some cases very slight. Anno-
tation of the test data set typically took them about two hours. In all, we produced six
annotations of the test set.2
We have four annotations of the pilot set, by three volunteers and the experimenters.
The pilot set was used to prototype our annotation software, and also as a validation cor-
pus for our system. The development set was annotated only once, by the experimenter.
This data set is used for training.
Our annotation scheme marks each utterance as part of a single conversation.
Annotators are instructed to create as many or as few conversations as they need to
describe the data. Our instructions state that a conversation can be between any number
of people, and that, ?We mean conversation in the typical sense: a discussion in which
the participants are all reacting and paying attention to one another . . . it should be clear
that the comments inside a conversation fit together.? The annotation system itself is a
simple Java program with a graphical interface, intended to appear somewhat similar
to a typical chat client. Each speaker?s name is displayed in a different color, and the
system displays the elapsed time between comments, marking especially long pauses
2 One additional annotation was discarded because the annotator misunderstood the task.
393
Computational Linguistics Volume 36, Number 3
in red. Annotators group utterances into conversations by clicking and dragging them
onto each other.
3.2 Metrics
Before discussing the annotations themselves, we will describe the metrics we use to
compare different annotations; these measure both how much our annotators agree
with each other, and how well our model and various baselines perform. Comparing
clusterings with different numbers of clusters is a non-trivial task, and metrics for
agreement on supervised classification, such as the ? statistic, are not applicable.
To measure global similarity between annotations, we use one-to-one accuracy.
This measure describes how well we can extract whole conversations intact, as required
for summarization or information extraction. To compute it, we pair up conversations
from the two annotations to maximize the total overlap by computing an optimal max-
weight bipartite matching, then report the percentage of overlap found.3 One-to-one
accuracy is a standard metric in unsupervised part-of-speech tagging (e.g., Haghighi
and Klein 2006), and is equivalent to mention-based CEAF (Luo 2005) for coreference
resolution.
If we intend to monitor or participate in the conversation as it occurs, we will care
more about local judgments. The local agreement metric is a constrained form of the
Rand index for clusterings (Rand 1971) which counts agreements and disagreements for
pairs within a context k. We consider a particular utterance: The previous k utterances
are each in either the same or a different conversation. The lock score between two
annotators is their average agreement on these k same/different judgments, averaged
over all utterances. For example, loc1 counts pairs of adjacent utterances for which two
annotations agree.
Several related papers use some variant of the F-score metric to measure accuracy.
The most complete treatment is given in Shen et al (2006). They use a micro-averaged
F-score, which is defined by constructing a multiway matching between conversations
in the two annotations. For a gold conversation i with size ni, and a proposed conver-
sation j with size nj, with overlap of size nij, they define precision and recall (plus the
standard balanced F-score). The F-score of an entire annotation is a weighted sum over
the matching:
P =
nij
nj
R =
nij
ni
F(i, j) = 2PR
P+ R
F =
?
i
ni
n maxjF(i, j) (1)
This is the F-score we report for comparative purposes. Because the match is multiway,
the score is not symmetric; when measuring agreement between pairs of human anno-
tators (where there is no reason for one to be considered gold), we map the high-entropy
transcript to the lower one (the entropy of a transcript is defined subsequently, in
Equation 2). Micro-averaged F-scores are also popular in work on document clustering.
In general, scores using this metric are correlated with our other measurement of global
consistency, the one-to-one accuracy.
3 The matching can be computed efficiently with the so-called Hungarian algorithm or by reduction to max
flow. The widely used greedy algorithm is a two-approximation, although we have not found large
differences in practice.
394
Elsner and Charniak Disentangling Chat
Adams and Martell (2008) also report F-score, but using a somewhat different
definition. They define F-score only between a particular pair of conversations, and
report the score for a single selected conversation. They do not describe how this
reference conversation is chosen. It is also unclear how they determine which proposed
conversation to match to it?the one with the best F-score, or the one which contains
the first (or ?root?) utterance of the reference conversation. (The latter, although it may
be more useful for some applications, has an obvious problem?if the conversation is
retrieved perfectly except for the root utterance, the score will be zero.) For these reasons
we do not evaluate their metric.
3.3 Discussion
A statistical examination of our data (Table 1) shows that there is a substantial amount
of disentanglement to do: the average number of conversations active at a time (the
density) is 2.75. Our annotators have high agreement on the local metric (average of
81.1%). On the one-to-one metric, they disagree more, with a mean overlap of 53.0% and
a maximum of only 63.5%. Though this level of agreement is low, naive baselines score
even lower (see Section 5). Therefore the metric does indeed distinguish human-like
from baseline performance. Thus measuring one-to-one overlap with our annotations
is a reasonable evaluation for computational models. However, we feel that the major
source of disagreement is one that can be remedied in future annotation schemes: the
specificity of the individual annotations.
To measure the level of detail in an annotation, we use the information-theoretic
entropy of the random variable, which indicates which conversation an utterance is
in. This variable has as many potential values as the number of conversations in the
transcript, each value having probability proportional to its size. Thus, for a transcript
of length n, with conversations i each having size ni, the entropy is:
H(c) =
?
i
ni
n log2
ni
n (2)
This quantity is non-negative, increasing as the number of conversations grow and their
size becomes more balanced. It reaches its maximum, 9.64 bits for this data set, when
Table 1
Statistics on 6 annotations of 800 utterances of chat transcript. Inter-annotator agreement metrics
(below the line) are calculated between distinct pairs of annotations.
Mean Max Min
Conversations 81.33 128 50
Average conversation length 10.6 16.0 6.2
Average conversation density 2.75 2.92 2.53
Entropy 4.83 6.18 3.00
one-to-one 52.98 63.50 35.63
loc3 81.09 86.53 74.75
Many-to-1 (by entropy) 86.70 94.13 75.50
Shen F (by entropy) 53.87 66.08 35.43
395
Computational Linguistics Volume 36, Number 3
each utterance is placed in a separate conversation. In our annotations, it ranges from
3.0 to 6.2. This large variation shows that some annotators are more specific than others,
but does not indicate how much they agree on the general structure. To measure this,
we introduce the many-to-one accuracy. This measurement is asymmetrical, and maps
each of the conversations of the source annotation to the single conversation in the
targetwith which it has the greatest overlap, then counts the total percentage of overlap.
This is not a statistic to be optimized (indeed, optimization is trivial: Simply make each
utterance in the source into its own conversation), but it can give us some intuition
about specificity. In particular, if one subdivides a coarse-grained annotation to make a
more specific variant, the many-to-one accuracy from fine to coarse remains 1. When we
map high-entropy annotations (fine) to lower ones (coarse), we find high many-to-one
accuracy, with a mean of 86%, which implies that the more specific annotations have
mostly the same large-scale boundaries as the coarser ones.
By examining the local metric, we can see even more: Local correlations are good,
at an average of 81.1%. This means that, in the three-sentence window preceding each
sentence, the annotators are often in agreement. If they recognize subdivisions of a large
conversation, these subdivisions tend to be contiguous, not mingled together, which is
why they have little impact on the local measure.
We find reasons for the annotators? disagreement about appropriate levels of detail
in the linguistic literature. As mentioned, new conversations often break off from old
ones in schisms. Aoki et al (2006) discuss conversational features associated with
schisming and the related process of affiliation, by which speakers attach themselves to
a conversation. Schisms often branch off from asides or even normal comments (toss-
outs) within an existing conversation. This means that there is no clear beginning to the
new conversation?at the time when it begins, it is not clear that there are two separate
floors, and this will not become clear until distinct sets of speakers and patterns of turn-
taking are established. Speakers, meanwhile, take time to orient themselves to the new
conversation. Example schisms are shown in Figures 2 and 3.
Our annotation scheme requires annotators to mark each utterance as part of a
single conversation, and distinct conversations are not related in any way. If a schism
occurs, the annotator is faced with two options: If it seems short, they may view it as
a mere digression and label it as part of the parent conversation. If it seems to deserve
a place of its own, they will have to separate it from the parent, but this severs the
initial comment (an otherwise unremarkable aside) from its context. One or two of the
annotators actually remarked that this made the task confusing. Our annotators seem
Figure 2
A schism occurring in our corpus (abridged). The schism-inducing turn is Kandra?s comment,
marked by arrows. Annotators 0 and 2 begin a new conversation with this turn; 1, 4, and 5 group
it with the other utterances shown; 3 creates new conversations for both this turn and Madison?s
question immediately following.
396
Elsner and Charniak Disentangling Chat
Figure 3
A schism occurring in our corpus (abridged): not all annotators agree on where the thread about
charging for answers to technical questions diverges from the one about setting up Paypal
accounts. The schism begins just after Lai?s second comment (marked with arrows), to which
Gale and Azzie both respond (marked with double arrows). Annotators 1, 2, 4, and 5 begin a new
conversation with Gale?s response. Annotator 0 starts a new conversation with Azzie?s response.
Annotator 3 makes an error, linking the two responses to each other, but not to the parent.
Figure 4
Utterances versus conversations participated in per speaker on development data.
to be either ?splitters? or ?lumpers??in other words, each annotator seems to aim for
a consistent level of detail, but each one has their own idea of what this level should be.
As a final observation about the data set, we test the appropriateness of the assump-
tion (used in previous work) that each speaker takes part in only one conversation. In
our data, the average speaker takes part in about 3.3 conversations (the actual number
varies for each annotator). The more talkative a speaker is, the more conversations they
participate in, as shown by a plot of conversations versus utterances (Figure 4). The
assumption is not very accurate, especially for speakers with more than 10 utterances.
4. Model
Our model for disentanglement fits into the general class of graph partitioning algo-
rithms (Roth and Yih 2004) that have been used for a variety of tasks in NLP, including
coreference resolution (Soon, Ng, and Lim 2001) and the related task of meeting seg-
mentation (Malioutov and Barzilay 2006). These algorithms operate in two stages: First,
397
Computational Linguistics Volume 36, Number 3
Table 2
Feature functions with performance on development data.
Chat-specific (Acc: 73 Prec: 73 Rec: 61 F: 66)
Time The time between x and y in seconds, discretized into logarithmically sized bins.
Speaker x and y have the same speaker.
Mention x-y x mentions the speaker of y (or vice versa). For example, this feature is true for
a pair such as: Felicia ?Gale: ... and any utterance spoken by Gale.
Mention same Both x and y mention the same name.
Mention other either x or y mentions a third person?s name.
Discourse (Acc: 52 Prec: 47 Rec: 77 F: 58)
Cue words Either x or y uses a greeting (hello etc.), an answer (yes, no etc.), or thanks.
Question Either asks a question (explicitly marked with ?).
Long Either is long (> 10 words).
Content (Acc: 50 Prec: 45 Rec: 74 F: 56)
Repeat(i) The number of words shared between x and y which have unigram
probability i, binned logarithmically.
Tech Whether both x and y use technical jargon, neither do, or only one does.
Combined (Acc: 75 Prec: 73 Rec: 68 F: 71)
a binary classifier marks each pair of items as alike or different, and second, a consistent
partition is extracted.4
4.1 Classification
We use a maximum-entropy classifier (Daume? 2004) to decide whether a pair of utter-
ances x and y are in same or different conversations. The most likely class is different,
which occurs 57% of the time in the development data. We describe the classifier?s per-
formance in terms of raw accuracy (correct decisions/total), precision and recall of the
same class, and F-score, the harmonic mean of precision and recall. Our classifier uses
several types of features (Table 2). The chat-specific features yield the highest accuracy
and precision. Discourse and content-based features have poor accuracy on their own
(worse than the baseline), because they work best on nearby pairs of utterances, and
tend to fail on more distant pairs. Paired with the time gap feature, however, they boost
accuracy somewhat and produce substantial gains in recall, encouraging the model to
group related utterances together.
The classifier is trained on our single annotation of the 706-utterance development
section and validated against the 359-utterance pilot section.
4 Our first attempt at this task used a Bayesian generative model. However, we could not define a sharp
enough posterior over new sentences, which made the model unstable and overly sensitive to its prior.
398
Elsner and Charniak Disentangling Chat
Figure 5
Distribution of pause length (log-scaled) between utterances in the same conversation.
The time gap, as discussed earlier, is the most widely used feature in previous work.
Our choice of a logarithmic binning scheme is intended to capture two characteristics of
the distribution of pause lengths (shown in Figure 5). The curve has its maximum at 1?
3 seconds, and pauses shorter than a second are less common. This reflects turn-taking
behavior among participants; participants in the same conversation prefer to wait for
each others? responses before speaking again. On the other hand, the curve is quite
heavy-tailed to the right, leading us to bucket long pauses fairly coarsely. The specific
discretization we adopt for a time gap ? is bin(?) = floor(log1.5(?+ 1)). The particular
choice of 1.5 was chosen by hand to fit the observed scale of the curve.
Our discourse-based features model some pairwise relationships: questions fol-
lowed by answers, short comments reacting to longer ones, greetings at the beginning,
and thanks at the end.
Word repetition is a key feature in nearly every model for segmentation or coher-
ence, so it is no surprise that it is useful here. We discard the 50 most frequent words.
Then we bin all words by their unigram probability (bin(w) = floor(log10(p(w))) and
create an integer-valued feature for each bin, equal to the number of repeated words in
that bin. Unigram probabilities are calculated over the entire 52 hours of transcript. The
binning scheme allows us to deal with ?noise words? which are repeated coincidentally,
because these occur in high-probability bins where repetitions are given less weight.
The point of the repetition feature is of course to detect sentences with similar
topics. We also find that sentences with technical content are more likely to be related
than non-technical sentences. We label an utterance as technical if it contains a Web
address, a long string of digits, or a term present in a guide for novice Linux users5
but not in a large news corpus (Graff 1995).6 This is a lightweight way to capture
one ?semantic dimension? or cluster of related words. The technical word feature was
included because it improves our development classification score slightly, but it does
not have a significant effect on overall performance. Adams (2008) attempts to add
more semantic dimensions learned via Latent Dirichlet Allocation, and similarly finds
no improvement.
Pairs of utterances which are widely separated in the discourse are unlikely to be
directly related?even if they are part of the same conversation, the link between them
is probably a long chain of intervening utterances. Thus, if we run our classifier on a
pair of very distant utterances, we expect it to default to the majority class, which in this
case will be different, and this will damage our performance in case the two are really
part of the same conversation. To deal with this, we run our classifier only on utterances
5 Introduction to Linux: A Hands-on Guide. Machtelt Garrels. Edition 1.25 from
http://tldp.org/LDP/intro-linux/html/intro-linux.html.
6 Our data came from the LA Times, 1994?1997 ? helpfully, this corpus predates the current wide coverage
of Linux in the mainstream press.
399
Computational Linguistics Volume 36, Number 3
Figure 6
VOTE algorithm.
separated by 129 seconds or less. The cutoff of 129 seconds was chosen because, for
utterances further apart than this, the classifier has no significant advantage over the
majority baseline. For 99.9% of utterances in an ongoing conversation, the previous
utterance in that conversation is within this gap, and so the system has a chance of
correctly linking the two.
On test data, the classifier has a mean accuracy of 68.2 (averaged over annotations).
The mean precision of same conversation is 53.3 and the recall is 71.3, with a mean
F-score of 60. This error rate is high, but the partitioning procedure allows us to recover
from some of the errors, because if nearby utterances are grouped correctly, the bad
decisions will be outvoted by good ones.
4.2 Partitioning
The next step in the process is to cluster the utterances. We wish to find a set of clusters
for which the weighted accuracy of the classifier would be maximal; this is an example
of correlation clustering (Bansal, Blum, and Chawla 2004), which is NP-complete. The
input to our partitioning procedure is a graph with a node for each utterance; if the
classifier connects utterances i and j with probability p, we take the weight wij of edge ij
to be the log odds log(pij/(1 ? pij)).7 We create a variable xij for each pair of utterances,
which is 1 if the utterances are placed in the same conversation, and 0 if they are
separated. The log probability of the clustering, treating the edges as independent, is
?
ij:i<j wijxij. We attempt to maximize this quantity, subject to the constraint that the xij
must form a legitimate clustering such that xij = xjk = 1 implies xij = xik.
Finding an exact solution proves to be difficult; the problem has a quadratic number
of variables (one for each pair of utterances) and a cubic number of triangle inequality
constraints (three for each triplet).8 With 800 utterances in our test set, even solving
the linear relaxation of the problem with CPLEX (Ilog, Inc. 2003) is too expensive to be
practical.
Experiments on a variety of heuristic algorithms (Elsner and Schudy 2009) show
that a relatively good solution can be obtained using a greedy voting algorithm (Fig-
ure 6). In this algorithm, we assign utterance j by examining all previously assigned
7 The original version of our system used a different weighting scheme, wij = pij ? .5. The log-odds ratio
behaves similarly for our basic algorithm, but appears to be more robust to other partitioning algorithms
or tuning (see Section 6), so, for simplicity, we present it here as well.
8 There is a triangle inequality constraint for each triplet i, j, k: (1 ? xik ) ? (1 ? xij )+ (1 ? xjk ).
400
Elsner and Charniak Disentangling Chat
utterances i, and treating the classifier?s judgment wij as a vote for cluster(i). If the
maximum vote is greater than 0, we set cluster( j) = argmaxc votec. Otherwise j is put
in a new cluster.
If the utterances are considered in order, this is a natural on-line algorithm?it
assigns each utterance as it arrives, without reference to the future. Elsner and Schudy
(2009) show that performance can be improved by approximately 6% on the one-to-one
and F-score metrics using offline randomized and local search methods. The loc3 metric
is insensitive to these more complex search procedures.
5. Experiments
We annotate the 800-line test transcript using our system. The annotation obtained has
62 conversations, with mean length 12.90. The average density of conversations is 2.86,
and the entropy is 3.72. This places it within the bounds of our human annotations (see
Table 1), toward the more general end of the spectrum.
As a standard of comparison for our system, we provide results for several
baselines?trivial systems which any useful annotation should outperform.
All different Each utterance is a separate conversation.
All same The whole transcript is a single conversation.
Blocks of k Each consecutive group of k utterances is a conversation.
Pause of k Each pause of k seconds or more separates two conversations.
Speaker Each speaker?s utterances are treated as a monologue.
For each particular metric, we calculate the best baseline result among all of these.
To find the best block size or pause length, we search over multiples of five between
5 and 300. This makes these baselines appear better than they really are, because their
performance is optimized with respect to the test data. (A complete table of baseline
results is shown in Figure 7.)
We also calculate results for two more systems. One is a non-trivial baseline:
Time/mention Our system, using only time gap and mention-based features.
The other is an oracle, designed to test how well a segmentation system designed for
meeting or lecture data might possibly do on this task. If no conversation were ever
interrupted, such a system would be perfect (up to the limit of annotator agreement).
Figure 7
Metric values for all baselines.
401
Computational Linguistics Volume 36, Number 3
Table 3
Metric values between proposed annotations and human annotations. Model scores typically fall
between inter-annotator agreement and baseline performance.
Annotators Model Time/ment. Perf. Seg. Best Baseline
Mean one-to-one 52.98 41.23 38.62 26.20 35.08 (Pause 35)
Max one-to-one 63.50 52.12 44.12 36.50 56.00 (Pause 65)
Min one-to-one 35.63 31.62 30.62 15.38 27.50 (Blocks 80)
Mean loc3 81.09 72.94 68.69 75.98 62.16 (Speaker)
Max loc3 86.53 74.70 70.93 85.40 69.05 (Speaker)
Min loc3 74.75 70.77 66.37 69.05 54.37 (Speaker)
Mean Shen F 53.87 43.47 41.31 35.50 36.58 (Speaker)
Max Shen F 66.08 57.57 48.85 46.70 46.79 (Speaker)
Min Shen F 35.43 32.97 32.07 21.83 29.09 (Blocks 65)
Perfect segments The transcript is divided into contiguous segments, where all utter-
ances in a segment belong to the same conversation. The conversation assign-
ments are determined by the human annotation whose agreement with the others
is highest.
Our results, in Table 3, are encouraging. On average, annotators agree more with
each other than with any artificial annotation, and more with our model than with
the baselines. For the one-to-one accuracy metric, we cannot claim much beyond these
general results. The range of human variation is quite wide, and there are annotators
who are closer to baselines than to any other human annotator. As explained earlier,
this is because some human annotations are much more specific than others. For very
specific annotations, the best baselines are short blocks or pauses. For the most general,
marking all utterances the same does very well (although for all other annotations, it is
extremely poor).
For the local metric, the results are much clearer. There is no overlap in the ranges;
for every test annotation, agreement is highest with other annotators, then our model,
and finally the baselines. The most competitive baseline is one conversation per speaker,
which makes sense, since if a speaker makes two comments in a four-utterance win-
dow, they are very likely to be related.
The Shen F-score metric seems to perform similarly to the one-to-one accuracy,
which is unsurprising because they are both measures of global consistency. The largest
difference between them is that the speaker baseline outperforms blocks and pauses in
F-score (although not by very much), perhaps because it is more precise.
Shen et al (2006) report higher F-scores for their own best model: It obtains an
F-score of 61.2, whereas our model?s mean score is only 43.4. Because of the different
corpora, we are unable to explain this difference. Better results are also reported in Wang
and Oard (2009) and Elsner and Schudy (2009) (see Table 4).
Mention information alone is not sufficient for disentanglement; with only name
mention and time gap features, mean one-to-one is 38 and loc3 is 69. However, name
mention features are critical for our model. Without them, the classifier?s development
F-score drops from 71 to 56. The disentanglement system?s test performance decreases
proportionally; mean one-to-one falls to 36, and mean loc3 to 63, essentially baseline per-
formance. For some utterances, of course, name mentions provide the only reasonable
402
Elsner and Charniak Disentangling Chat
Table 4
Results reported by others on the same task.
Result F-score Notes
this model 43.4
Elsner and Schudy (2009) 50 improved partitioning inference
Wang and Oard (2009) 54 message expansion features
Shen et al (2006) 61.2 different corpus
clue to the correct decision, which is why humans mention names in the first place.
But our system is probably overly dependent on them, because they are very reliable
compared to our other features.
Because of the frequency with which conversations interleave, perfect segmenta-
tion alone is not sufficient to optimize either global metric, and generally does not
outperform the baselines. For the local metric, however, it generally does better than the
model. Here, performance depends mainly on whether the system can find the bound-
aries between one conversation and another, and it is less important to link the segments
of a particular conversation to one another, since these different segments often lie
outside the three-utterance horizon. Systems designed to detect segment boundaries,
like those for meetings, might contribute to improvement of this metric.
6. Specificity Tuning
Although our analysis shows that individual annotators can produce more or less
specific annotations of the same conversation, the system described here can produce
only a single annotation (for any given set of training data) with a fixed specificity. Now
we attempt to control the specificity parametrically, producing more and less specific
annotations on demand, without retraining the classifier.
The parameter we choose to alter is the bias of our pairwise classifier. A maximum-
entropy classifier has the form:
y(x) = 1
1+ exp(?(w ? x+ b))
(3)
Here w represents the vector of feature weights and b is the bias term; a positive b shifts
all judgments toward high-confidence same conversation decisions and a negative b
shifts them away. To alter the classifier, we add a constant ? to b. In general, increasing
the number and confidence of same decisions leads to larger, coarser partitionings,
and decreasing it creates smaller, finer ones. We measure specificity by examining the
entropy of the output annotation. Although entropy is generally an increasing function
of ?, the relationship is not always smooth, nor is it completely monotonic. Figure 8
plots entropy as a function of ?.
In Figure 9, we plot the one-to-one match between each test annotation and the
altered annotations produced by this method, as a function of the entropy. The unbiased
system creates an annotation with entropy 3.7. Although this yields reasonable results
for all human annotations, each of the annotations has a point of higher performance
at a different bias level. For instance, the line uppermost on the left side of the plot
403
Computational Linguistics Volume 36, Number 3
Figure 8
Entropy of the output annotation produced with bias factor ? on test data. ? = 0 corresponds to
the unbiased system.
shows overlap with a human transcript whose entropy is 3.0 bits; lower-entropy system
annotations correspond better with this annotator?s judgments.
For each human annotation, we evaluate the tuned system?s performance at the
entropy level of the original annotation. (This point is marked by the large dot on
Figure 9
One-to-one accuracy between biased system annotations and each test annotation, as a function
of entropy. The vertical line (at 3.72 bits) marks the scores obtained by the unbiased system with
? = 0. The large dot on each line is the score obtained at the entropy of the human annotation.
404
Elsner and Charniak Disentangling Chat
Table 5
Metric values between proposed annotations and human annotations on test data. The tuned
model (evaluated at the entropy of the human annotations) improves on one-to-one accuracy but
not on loc3.
Unbiased Model Tuned Model
Mean one-to-one 41.23 48.52
Max one-to-one 52.13 58.75
Min one-to-one 31.66 40.88
Mean loc3 72.94 73.64
Max loc3 74.70 75.87
Min loc3 70.77 69.95
each line in the figure.) To do this, we perform a line search over ? until we produce
a clustering whose entropy is within .25 bits of the original?s, then evaluate. In other
words, we measure performance given an additional piece of supervision?the annota-
tor?s preferred specificity level.
Results on the one-to-one metric are fairly good: Extreme and average scores are
listed in Table 5. The effects of this technique on the local metric are small (and in
many cases negative). This is not entirely surprising, as the local metric is less sensitive
to specificity of annotations. Slight positive effects occur only for the most and least
specific annotation, which are presumably so extreme that specificity begins to have a
slight effect even on local decisions.
Despite fairly large performance increases on the test set, we do not consider this
technique really reliable, because the relationship between the bias parameter and final
score is not smooth. Small changes in the bias can cause large shifts in entropy, and small
changes in entropy can have large effects on quality. (For instance, two annotations have
a sharp decline in score at about entropy 5.7, losing about 5% of performance with a
change of just over .1 bit.) Therefore it is not clear exactly how to choose a bias parameter
which will yield good performance. Matching the entropy of a human annotation seems
to work on the test data, but fails to improve scores on our development data. Moreover,
although for methodological simplicity we assume access to the exact target entropy for
each annotation, it is unlikely that a real user could express their desired specificity
so precisely. Figuring out a way to let the user select the desired entropy remains a
challenge.
7. Detecting Conversation Starts
In this section, we investigate better ways to find the beginnings of new conversations.
In the pairwise-linkage representation presented earlier, a new conversation is begun
when none of the previous utterances is strongly linked to the current utterance. This
representation spreads out the responsibility for detecting a new conversation over
many pairwise decisions. We are inspired by the use of discourse-new classifiers (also
called anaphoricity detectors) in coreference classification (Ng and Cardie 2002) to find
NPs which begin coreferential chains. Oracle experiments show that a similar detector
for utterances which begin conversations could improve disentanglement scores if it
were available. We attempt to develop such a detector, but without much success.
405
Computational Linguistics Volume 36, Number 3
Table 6
Metric values using an oracle new-conversation detector on test data.
Original Model +Oracle New Conversations
Mean one-to-one 41.23 46.75
Max one-to-one 52.13 53.50
Min one-to-one 31.66 42.13
Mean loc3 72.94 73.90
Max loc3 74.70 76.49
Min loc3 70.77 70.72
As a demonstration of the gains possible if a good classifier could be developed,
we show the oracle improvements possible on the test data, using an optimal new-
conversation detector as a hard constraint on inference (Table 6). The oracle detector
detects a conversation start if it occurs in the majority of human annotations, and the
inference algorithm is forced to start a new conversation if and only if the oracle has
detected one. Good conversation detection is capable of improving not only one-to-one
accuracy but local accuracy as well.
We can track the performance of realistic, non-oracle new-conversation detection
via the precision, recall, and F-score of the new conversation class (Table 7). As a
starting point, we report the accuracy obtained by the pairwise-linkage model and
greedy inference already presented. At 49% F-score, it is clearly not doing a good job.
It is possible to do better than this using information already represented in the
pairwise classifier: The time since the speaker of the utterance last spoke (logarithmi-
cally bucketed), and whether the utterance mentions a name. A better representation for
the problem allows the classifier to make somewhat more effective use of these features.
For reasons we cannot explain, adding discourse features like the presence of a question
or greeting does not improve performance. The simple classifier does improve slightly
on the baseline, up to 51%. These test results, however, are somewhat surprising to us.
On our development corpus, the corresponding scores are 69% and 75%. Because that
corpus contains an average (over three annotations) of 34 conversations, it is likely that
we were misled by coincidentally good results.
On the development set, where the classifier works well, its decisions can be inte-
grated with inference to yield substantial improvements in actual system performance.
Mean loc3 increases from 72% to about 78% and mean one-to-one accuracy from 41%
to about 66%. However, we find no improvement at all on the test data, because
the classifier has very low recall, and the resulting test annotations have far too few
conversations.
Table 7
Precision, recall, and F-score of the new conversation class on test data (average 81
conversations).
Precision Recall F-score
Pairwise system 56.08 43.44 48.96
Time/Mention Features 68.06 40.16 50.52
Human Annotators 64.30 61.70 61.14
406
Elsner and Charniak Disentangling Chat
8. Future Work
Although our annotators are reasonably reliable, it seems clear that they think of con-
versations as a hierarchy, with digressions and schisms. We are interested to see an an-
notation protocol which more closely follows human intuition. One suggestion (David
Traum, personal communication) is to drop the idea of partitioning entirely and have
annotators mark the data as a graph, linking each utterance to its parents and children
with links of various strengths. Such a scheme might yield more reliable annotations
than our current one, although testing this hypothesis would require new annotation
software and a different set of metrics. Any new annotation project should also investi-
gate whether annotators can define their desired specificity, and with what precision.
Our results on new conversation detection suggest that a high-performance clas-
sifier for this task could improve results substantially. It is also interesting to consider,
given the weakness of our technical words feature and the disappointing results using
Latent Dirichlet Allocation from Adams (2008), how semantic similarity might be use-
fully modeled.
Finally, we are interested to see how well this feature set performs on speech data,
as in Aoki et al (2003). Spoken conversation is more natural than text chat, but even
when participants are face-to-face, disentanglement remains a problem. On the other
hand, spoken dialogue contains new sources of information, such as prosody and gaze
direction. Turn-taking behavior is also more distinct, which makes the task easier, but
according to Aoki et al (2006), it is certainly not sufficient.
9. Conclusion
This work provides a corpus of annotated data for chat disentanglement, which, along
with our proposed metrics, should allow future researchers to evaluate and compare
their results quantitatively.9 Our annotations are consistent with one another, especially
with respect to local agreement. We show that features based on discourse patterns and
the content of utterances are helpful in disentanglement. The model we present can
outperform a variety of baselines.
Acknowledgments
Our thanks to Suman Karumuri, Steve
Sloman, Matt Lease, David McClosky, seven
test annotators, three pilot annotators, three
anonymous conference reviewers, three
anonymous journal reviewers, and the NSF
PIRE grant. We would also like to thank
Craig Martell and David Traum for their
very useful comments at the conference
presentation.
References
Acar, Evrim, Seyit Ahmet Camtepe,
Mukkai S. Krishnamoorthy, and Bu?lent
Yener. 2005. Modeling and multiway
analysis of chatroom tensors. In Paul B.
Kantor, Gheorghe Muresan, Fred Roberts,
Daniel Dajun Zeng, Fei-Yue Wang,
Hsinchun Chen, and Ralph C. Merkle,
editors, ISI, volume 3495 of Lecture Notes
in Computer Science. Springer, Berlin,
pages 256?268.
Adams, Paige H. 2008. Conversation Thread
Extraction and Topic Detection in Text-based
Chat. Ph.D. thesis, Naval Postgraduate
School.
Adams, Paige H. and Craig H. Martell. 2008.
Topic detection and extraction in chat.
International Conference on Semantic
Computing, 2:581?588.
Aoki, Paul M., Matthew Romaine,
Margaret H. Szymanski, James D.
Thornton, Daniel Wilson, and Allison
Woodruff. 2003. The mad hatter?s
cocktail party: A social mobile audio
9 Our software and data set are publicly available from cs.brown.edu/?melsner.
407
Computational Linguistics Volume 36, Number 3
space supporting multiple simultaneous
conversations. In CHI ?03: Proceedings of
the SIGCHI Conference on Human Factors
in Computing Systems, pages 425?432,
New York, NY.
Aoki, Paul M., Margaret H. Szymanski,
Luke D. Plurkowski, James D. Thornton,
Allison Woodruff, and Weilie Yi. 2006.
Where?s the ?party? in ?multi-party??:
Analyzing the structure of small-group
sociable talk. In CSCW ?06: Proceedings of
the 2006 20th Anniversary Conference on
Computer Supported Cooperative Work,
pages 393?402, New York, NY.
Bansal, Nikhil, Avrim Blum, and Shuchi
Chawla. 2004. Correlation clustering.
Machine Learning, 56(1-3):89?113.
Camtepe, Seyit Ahmet, Mark K. Goldberg,
Malik Magdon-Ismail, and Mukkai
Krishnamoorty. 2005. Detecting
conversing groups of chatters: A model,
algorithms, and tests. In IADIS AC,
pages 89?96, Algarve.
Chen, Lei. 2008. Incorporating Nonverbal
Features into Multimodal Models of
Human-to-Human Communication.
Ph.D. thesis, Purdue University.
Chen, Lei, Mary Harper, Amy Franklin,
Travis R. Rose, Irene Kimbara,
Zhongqiang Huang, and Francis
Quek. 2006. A multimodal analysis
of floor control in meetings. In
Proceedings of MLMI 06, pages 36?49,
Bethesda, MD.
Daume?, III, Hal. 2004. Notes on CG and
LM-BFGS optimization of logistic
regression. Paper available at http://
pub.hal3.name#daume04cg-bfgs.
Implementation available at http://
hal3.name/megam/.
Elsner, Micha and Warren Schudy. 2009.
Bounding and comparing methods for
correlation clustering beyond ILP. In
Proceedings of ILP-NLP, pages 19?27,
Boulder, CO.
Graff, David. 1995. North American News
Text Corpus. Linguistic Data Consortium.
LDC95T21.
Haghighi, Aria and Dan Klein. 2006.
Prototype-driven learning for sequence
models. In Proceedings of HLT-NAACL,
pages 320?327, New York, NY.
Hawes, Timothy, Jimmy Lin, and
Philip Resnik. 2008. Elements of a
computational model for multi-party
discourse: The turn-taking behavior
of supreme court justices. Technical
Report LAMP-TR-147/HCIL-2008-02,
University of Maryland, College Park.
Ilog, Inc. 2003. CPLEX solver. Available at
www-01.ibm.com/software/websphere/
ilog migration.html.
Jovanovic, Natasa and Rieks op den
Akker. 2004. Towards automatic
addressee identification in multi-party
dialogues. In Proceedings of the
5th SIGdial Workshop, pages 89?92,
Cambridge, MA.
Jovanovic, Natasa, Rieks op den Akker,
and Anton Nijholt. 2006. Addressee
identification in face-to-face meetings. In
Proceedings of EACL, Trento.
Luo, Xiaoqiang. 2005. On coreference
resolution performance metrics. In
Proceedings of HLT-EMNLP, pages 25?32,
Morristown, NJ.
Malioutov, Igor and Regina Barzilay. 2006.
Minimum cut model for spoken lecture
segmentation. In Proceedings of ACL,
pages 25?32, Sydney.
McCallum, Andrew and Ben Wellner.
2004. Conditional models of identity
uncertainty with application to noun
coreference. In Proceedings of the 18th
Annual Conference on Neural Information
Processing Systems (NIPS), pages 905?912,
Vancouver.
Miller, G., A. R. Beckwith, C. Fellbaum,
D. Gross, and K. Miller. 1990. Introduction
to Wordnet: An on-line lexical database.
International Journal of Lexicography,
3(4):235?244.
Ng, Vincent and Claire Cardie. 2002.
Identifying anaphoric and non-anaphoric
noun phrases to improve coreference
resolution. In COLING, Taipei.
O?Neill, Jacki and David Martin. 2003. Text
chat in action. In GROUP ?03: Proceedings
of the 2003 International ACM SIGGROUP
Conference on Supporting Group Work,
pages 40?49, New York, NY.
Rand, William M. 1971. Objective criteria for
the evaluation of clustering methods.
Journal of the American Statistical
Association, 66(336):846?850.
Roth, Dan and Wen-tau Yih. 2004. A linear
programming formulation for global
inference in natural language tasks. In
Proceedings of CoNLL-2004, pages 1?8,
Boston, MA.
Sacks, Harvey, Emanuel A. Schegloff,
and Gail Jefferson. 1974. A simplest
systematics for the organization of
turn-taking for conversation. Language,
50(4):696?735.
Shen, Dou, Qiang Yang, Jian-Tao Sun,
and Zheng Chen. 2006. Thread detection
in dynamic text message streams. In
408
Elsner and Charniak Disentangling Chat
SIGIR ?06: Proceedings of the 29th Annual
International ACM SIGIR Conference,
pages 35?42, New York, NY.
Soon, Wee Meng, Hwee Tou Ng, and
Daniel Chung Yong Lim. 2001. A
machine learning approach to
coreference resolution of noun
phrases. Computational Linguistics,
27(4):521?544.
Traum, D. 2004. Issues in multi-party
dialogues. In F. Dignum, editor, Advances
in Agent Communication. Springer Verlag,
Berlin, pages 201?211.
Traum, David R., Susan Robinson, and
Jens Stephan. 2004. Evaluation of
multi-party virtual reality dialogue
interaction. In Proceedings of the Fourth
International Conference on Language
Resources and Evaluation (LREC),
pages 1699?1702, Lisbon.
Wang, Lidan and Douglas W. Oard. 2009.
Context-based message expansion for
disentanglement of interleaved text
conversations. In Proceedings of
HLT-NAACL, pages 200?208, Boulder, CO.
Wang, Yi-Chia, Mahesh Joshi, William
Cohen, and Carolyn Rose?. 2008.
Recovering implicit thread structure in
newsgroup style conversations. In
Proceedings of the 2nd International
Conference on Weblogs and Social Media
(ICWSM II), Seattle, WA.
Yeh, Jen-Yuan and Aaron Harnly. 2006.
Email thread reassembly using similarity
matching. In Conference on Email and
Anti-Spam, Mountain View, CA.
409

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 28?36,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automatic Domain Adaptation for Parsing
David McCloskya,b
aStanford University
Stanford, CA, USA
mcclosky@stanford.edu
Eugene Charniakb
bBrown University
Providence, RI, USA
ec@cs.brown.edu
Mark Johnsonc,b
cMacquarie University
Sydney, NSW, Australia
mjohnson@science.mq.edu.au
Abstract
Current statistical parsers tend to perform well
only on their training domain and nearby gen-
res. While strong performance on a few re-
lated domains is sufficient for many situations,
it is advantageous for parsers to be able to gen-
eralize to a wide variety of domains. When
parsing document collections involving het-
erogeneous domains (e.g. the web), the op-
timal parsing model for each document is typ-
ically not obvious. We study this problem as
a new task ? multiple source parser adapta-
tion. Our system trains on corpora from many
different domains. It learns not only statistics
of those domains but quantitative measures of
domain differences and how those differences
affect parsing accuracy. Given a specific tar-
get text, the resulting system proposes linear
combinations of parsing models trained on the
source corpora. Tested across six domains,
our system outperforms all non-oracle base-
lines including the best domain-independent
parsing model. Thus, we are able to demon-
strate the value of customizing parsing models
to specific domains.
1 Introduction
In statistical parsing literature, it is common to see
parsers trained and tested on the same textual do-
main (Charniak and Johnson, 2005; McClosky et
al., 2006a; Petrov and Klein, 2007; Carreras et al,
2008; Suzuki et al, 2009, among others). Unfor-
tunately, the performance of these systems degrades
on sentences drawn from a different domain. This
issue can be seen across different parsing models
(Sekine, 1997; Gildea, 2001; Bacchiani et al, 2006;
McClosky et al, 2006b). Given that some aspects of
syntax are domain dependent (typically at the lexi-
cal level), single parsing models tend to not perform
well across all domains (see Table 1). Thus, statis-
tical parsers inevitably learn some domain-specific
properties in addition to the more general properties
of a language?s syntax. Recently, Daume? III (2007)
and Finkel and Manning (2009) showed techniques
for training models that attempt to separate domain-
specific and general properties. However, even when
given models for multiple training domains, it is not
straightforward to determine which model performs
best on an arbitrary piece of novel text.
This problem comes to the fore when one wants
to parse document collections where each document
is potentially its own domain. This shows up par-
ticularly when parsing the web. Recently, there
has been much interest in applying parsers to the
web for the purposes of information extraction and
other forms of analysis (c.f. the CLSP 2009 summer
workshop ?Parsing the Web: Large-Scale Syntactic
Processing?). The scale of the web demands an au-
tomatic solution to the domain detection and adap-
tation problems. Furthermore, it is not obvious that
human annotators can determine the optimal parsing
models for each web page.
Our goal is to study this exact problem. We create
a new parsing task, multiple source parser adapta-
tion, designed to capture cross-domain performance
along with evaluation metrics and baselines. Our
new task involves training parsing models on labeled
and unlabeled corpora from a variety of domains
(source domains). This is in contrast to standard do-
main adaptation tasks where there is a single source
domain. For evaluation, one is given a text (target
text) but not the identity of its domain. The chal-
lenge is determining how to best use the available
28
Test
Train BNC GENIA BROWN SWBD ETT WSJ Average
GENIA 66.3 83.6 64.6 51.6 69.0 66.6 67.0
BROWN 81.0 71.5 86.3 79.0 80.9 80.6 79.9
SWBD 70.8 62.9 75.5 89.0 75.9 69.1 73.9
ETT 72.7 65.3 75.4 75.2 81.9 73.2 73.9
WSJ 82.5 74.9 83.8 78.5 83.4 89.0 82.0
Table 1: Cross-domain f-score performance of the Charniak (2000) parser. Averages are macro-averages.
Performance drops as training and test domains diverge. On average, the WSJ model is the most accurate.
resources from training to maximize accuracy across
multiple target texts.
Broadly put, we model how domain differences
influence parsing accuracy. This is done by taking
several computational measures of domain differ-
ences between the target text and each source do-
main. We use these features in a simple linear re-
gression model which is trained to predict the accu-
racy of a parsing model (or, more generally, a mix-
ture of parsing models) on a target text. To parse
the target text, one simply uses the mixture of pars-
ing models with the highest predicted accuracy. We
show that our method is able to predict these accu-
racies quite well and thus effectively rank parsing
models formed from mixtures of labeled and auto-
matically labeled corpora.
In Section 2, we detail recent work on similar
tasks. Our regression-based approach is covered in
Section 3. We describe an evaluation strategy in Sec-
tion 4. Section 5 presents new baselines which are
intended to give a sense of current approaches and
their limitations. The results of our experiments are
detailed in Section 6 where we show that our system
outperforms all non-oracle baselines. We conclude
with a discussion and future work (Section 7).
2 Related work
The closest work to ours is Plank and Sima?an
(2008), where unlabeled text is used to group sen-
tences from WSJ into subdomains. The authors cre-
ate a model for each subdomain which weights trees
from its subdomain more highly than others. Given
the domain specific models, they consider different
parse combination strategies. Unfortunately, these
methods do not yield a statistically significant im-
provement.
Multiple source domain adaptation has been done
for other tasks (e.g. classification in (Blitzer et
al., 2007; Daume? III, 2007; Dredze and Cram-
mer, 2008)) and is related to multitask learning.
Daume? III (2007) shows that an extremely sim-
ple method delivers solid performance on a num-
ber of domain adaptation classification tasks. This is
achieved by making a copy of each feature for each
source domain plus the ?general? pseudodomain
(for capturing domain independent features). This
allows the classifier to directly model which features
are domain-specific. Finkel and Manning (2009)
demonstrate the hierarchical Bayesian extension of
this where domain-specific models draw from a gen-
eral base distribution. This is applied to classifica-
tion (named entity recognition) as well as depen-
dency parsing. These works describe how to train
models in many different domains but sidestep the
problem of domain detection. Thus, our work is or-
thogonal to theirs.
Our domain detection strategy draws on work in
parser accuracy prediction (Ravi et al, 2008; Kawa-
hara and Uchimoto, 2008). These works aim to pre-
dict the parser performance on a given target sen-
tence. Ravi et al (2008) frame this as a regression
problem. Kawahara and Uchimoto (2008) treat it
as a binary classification task and predict whether
a specific parse is at a certain level of accuracy or
higher. Ravi et al (2008) show that their system
can be used to return a ranking over different parsing
models which we extend to the multiple domain set-
ting. They also demonstrate that training their model
on WSJ allows them to accurately predict parsing
accuracy on the BROWN corpus. In contrast, our
models are trained over multiple domains to model
which factors influence cross-domain performance.
29
3 Approach
We start with the assumption that all target domains
are mixtures of our source domains.1 Intuitively,
these mixtures should give higher probability mass
to more similar source domains. This raises the
question of how to measure the similarity between
domains. Our method uses multiple complemen-
tary similarity measures between the target and each
source. We feed these similarity measures into a re-
gression model which learns how domain dissimi-
larities hurt parse accuracy. Thus, to parse a target
domain, we need only find the input that maximizes
the regression function ? that is, the highest scoring
mixture of source domains. Our system is similar to
Ravi et al (2008) in that both use regression to pre-
dict f-scores and some of the features are related.
3.1 Features
Our features are designed to help the regression
model determine if a particular source domain mix-
ture is well suited for a target domain as well as the
quality of a source domain mixture. While we ex-
plored a large number of features, we present here
only the three that were chosen by our feature selec-
tion method (Section 6.2).
Two of our features, COSINETOP50 and UN-
KWORDS, are designed to approximate how simi-
lar the target domain is to a specific source domain.
Only the surface form of the target text and auto-
matic analyses are available (e.g. we can tag or parse
the target text, but cannot use gold tags or trees).
Relative word frequencies are an important in-
dicator of domain. Cosine similarity uses a spa-
tial representation to summarize the word frequen-
cies in a corpus as a single vector. A common
method is to represent each corpus as a vector of
frequencies of the k most frequent words (Schu?tze,
1995). This method assigns high similarity to do-
mains with a large amount of overlap in the high-
frequency vocabulary items. We experimented with
several orders of magnitude for k (our feature selec-
tion method later chose k = 50 ? see Section 6.2).
Our second feature for comparing domains, UN-
1This may seem like a major limitation, but as we will show
later, our method works quite well at incorporating self-trained
(automatically parsed) corpora which can typically be obtained
for any domain.
KWORDS, returns the percentage of words in one
domain which never appear in the other domain.
This can be done on the word type or token level.
We opt for tokens since unknown words pose prob-
lems for parsing each time they occur. UNKWORDS
provides the percentage of words in the source
domain that are never seen in the target domain.
Whereas COSINETOP50 examines how similar the
high frequency words are from one domain, UN-
KWORDS tends to focus on the overlap of low fre-
quency words.
As described, COSINETOP50 and UNKWORDS
are functions only of two source domains and do not
take the mixing weights of source domains into ac-
count. We experimented with several methods of in-
corporating mixing weights into the feature value.
In practice, the one which worked best for us is to
divide the mixture weight of the source domain by
the raw feature value. This has the nice property that
when a source is not used, the adjusted feature value
is zero regardless of the raw feature value.
From pilot studies, we learned that a uniform mix-
ture of available source domains gave strong results
(further details on this in Section 5). Our last feature,
ENTROPY, is intended to let the regression system
leverage this and measures the entropy of the distri-
bution over source domains. This provides a sense
of uniformity.
3.2 Predicting cross-domain accuracy
For a given source domain mixture, we can create
a parsing model by linearly interpolating the pars-
ing model statistics from each source domain. The
key component of our approach is a domain-aware
linear regression model which predicts how well a
specific parsing model will do on a given target text.
The linear regressor is given values from the three
features from the previous section (COSINETOP50,
UNKWORDS, and ENTROPY) and returns an esti-
mate of the f-score the parsing model would achieve
the target text.
Training data for the regressor consists of ex-
amples of source domain mixtures and their ac-
tual f-scores on target texts. To produce this, we
randomly sampled source domain mixtures, created
parsing models for those mixtures, and then evalu-
ated the parsing models on all of our target texts.
We used a simple technique for randomly sam-
30
0 200 400 600 800 1000
Number of mixed parsing model samples
84.0
84.5
85.0
85.5
86.0
86.5
87.0
87.5
o
r
a
c
l
e
 
f
-
s
c
o
r
e
Figure 1: Cumulative oracle f-score (averaged over
all target domains) as more models are randomly
sampled. Most of the improvement comes the first
200 samples indicating that our samples seem to be
sufficient to cover the space of good source domain
mixtures.
pling source domain mixtures. First, we sample the
number of source domains to use. We draw values
from an exponential distribution and take their inte-
ger value until we obtain a number between two and
the number of source domains. This is parametrized
so that we typically only use a few corpora but still
have some chance of using all of them. Once we
know the number of source domains, we sample
their identities uniformly at random without replace-
ment from the list of all source domains. Finally,
we sample the weights for the source domains uni-
formly from a simplex. The dimension of the sim-
plex is the same as the number of source domains
so we end up with a probability distribution over the
sampled source domains.
In total, we sampled 1,040 source domain mix-
tures. We evaluated each of these source domain
mixtures on the six target domains giving us 6,240
data points in total. One may be concerned that
this is insufficient to cover the large space of source
domain mixtures. However, we show in Figure 1
that only about 200 samples are sufficient to achieve
good oracle performance2 in practice.
2We calculate this by picking the best available model for
each target domain and taking the average of their f-scores.
Train Test
Source Target Source Target
C \ {t} C \ {t} C \ {t} {t}
(a) Out-of-domain evaluation
Train Test
Source Target Source Target
C C \ {t} C {t}
(b) In-domain evaluation
Table 2: List of domains allowed in single round of
evaluation. In each round, the evaluation corpus is t.
C is the set of all target domains.
4 Evaluation
Multiple-source domain adaptation is a new task for
parsing and thus some thought must be given to eval-
uation methodology. We describe two evaluation
scenarios which differ in how foreign the target text
is from our source domains. Schemas for these eval-
uation scenarios are shown in Table 2. Note that
training and testing here refer to training and testing
of our regression model, not the parsing models.
In the first scenario, out-of-domain evaluation,
one target domain is completely removed from con-
sideration and only used to evaluate proposed mod-
els at test time. The regressor is trained on training
points that use any of the remaining corpora, C\{t},
as sources or targets. For example, if t = WSJ, we
can train the regressor on all data points which don?t
use WSJ (or any self-trained corpora derived from
WSJ) as a source or target domain. At test time, we
are given the text of WSJ?s test set. From this, our
system creates a parsing model using the remaining
available corpora for parsing the raw WSJ text.
This evaluation scenario is intended to evaluate
how well our system can adapt to an entirely new
domain with only raw text from the new domain
(for example, parsing biomedical text when none
is available in our list of source domains). Ide-
ally, we would have a large number of web pages
or other documents from other domains which we
could use solely for evaluation. Unfortunately, at
this time, only a handful of domains have been an-
notated with constituency structures under the same
This can pick different models for each target domain.
31
annotation guidelines. Instead, we hold out each
hand-annotated domain, t, (including any automat-
ically parsed corpora derived from that source do-
main) as a test set in a round-robin fashion.3 For
each round of the round robin we obtain an f-score
and we report the mean and variance of the f-scores
for each model.
The second scenario, in-domain evaluation, al-
lows the target domain, t, to be used as a source
domain in training but not as a target domain. This
is intended to evaluate the situation where the target
domain is not actually that different from our source
domains. The in-domain evaluation can approxi-
mate how our system would perform when, for ex-
ample, we have WSJ as a source domain and the tar-
get text is news from a source other than WSJ. Thus,
our model still has to learn that WSJ and the North
American News Text corpus (NANC) are good for
parsing news text like WSJ without seeing any direct
evaluations of the sort (WSJ and NANC can be used
in models which are evaluated on all other corpora,
though).
5 Baselines
Given that this is a new task for parsing, we needed
to create baselines which demonstrate the current
approaches to multiple-source domain adaptation.
One approach is to take all available corpora and
mix them together uniformly.4 The UNIFORM base-
line does exactly this using the available hand-built
training corpora. SELF-TRAINED UNIFORM uses
self-trained corpora as well. In the out-of-domain
scenario, these exclude the held out domain, but in
the in-domain setting, the held out domain is in-
cluded. These baselines are similar to the ALL and
WEIGHTED baselines in Daume? III (2007).
Another simple baseline is to use the same pars-
ing model regardless of target domain. This is how
large heterogeneous document collections are typi-
cally parsed currently. We use the WSJ corpus since
it is the best single corpus for parsing all six target
domains (see Table 1). We refer to this baseline as
FIXED SET: WSJ. In the out-of-domain scenario,
we fall back to SELF-TRAINED UNIFORM when the
3Thus, the schemas in Table 2 are schemas for each round.
4Accounting for size so that the larger corpora don?t over-
whelm the smaller ones.
target domain is WSJ while the in-domain scenario
uses the WSJ model throughout.
There are several interesting oracle baselines as
well which serve to measure the limits of our ap-
proach. These baselines examine the resulting
f-scores of models and pick the best model accord-
ing to some criteria. The first oracle baseline is
BEST SINGLE CORPUS which parses each corpus
with the source domain that maximizes performance
on the target domain. In almost all cases, this base-
line selects each corpus to parse itself.
Our second oracle baseline, BEST SEEN, chooses
the best parsing model from all those explored for
each test set. Recall that while training the regres-
sion model in Section 3.2, we needed to explore
many possible source domain mixtures to approxi-
mate the complete space of mixed parsing models.
To the extent that we can fully explore the space of
mixed parsing models, this baseline represents an
upper bound for model mixing approaches. Since
fully exploring the space of possible weightings is
intractable, it is not a true upper bound. While it
is theoretically possible to beat this pseudo-upper
bound, (indeed, this is the mark of a good domain
detection system) it is far from easy. We provide
BEST SINGLE CORPUS and BEST SEEN for both
in-domain and out-of-domain scenarios. The out-of-
domain scenario restricts the set of possible models
to those not including the target domain.
Finally, we searched for the BEST OVERALL
MODEL. This is the model with the highest aver-
age f-score across all six target domains. This base-
line can be thought of as an oracle version of FIXED
SET: WSJ and demonstrates the limit of using a sin-
gle parsing model regardless of target domain. Natu-
rally, the very nature of this baseline places it only in
the in-domain evaluation scenario. Since it was able
to select the model according to f-scores on our six
target domains, its performance on domains outside
that set is not guaranteed.
To provide a better sense of the space of mixed
parsing models, we also provide the WORST SEEN
baseline which picks the worst model available for a
specific target corpus.5
5This turns out to be GENIA for all corpora other than GENIA
and SWBD when the target domain is GENIA.
32
6 Experiments
Our experiments use the Charniak (2000) generative
parser. We describe the corpora used in our nine
source and six target domains in Section 6.1. In Sec-
tion 6.2, we provide a greedy strategy for picking
features to include in our regression model. The re-
sults of our experiments are in Section 6.3.
6.1 Corpora
We aimed to include as many different domains as
possible annotated under compatible schemes. We
also tried to include human-annotated corpora and
automatically labeled corpora (self-trained corpora
as in McClosky et al (2006a) which have been
shown to work well across domains). Our final
set includes text from news (WSJ, NANC), broad-
cast news (ETT), literature (BROWN, GUTENBERG),
biomedical (GENIA, MEDLINE), spontaneous speech
(SWBD), and the British National Corpus (BNC). In
our experiments, self-trained corpora cannot be used
as target domains since we lack gold annotations and
BNC is not used as a source domain due to its size.
An overview of our corpora is shown in Table 3.
We use news articles portion of the Wall Street
Journal corpus (WSJ) from the Penn Treebank (Mar-
cus et al, 1993) in conjunction with the self-trained
North American News Text Corpus (NANC, Graff
(1995)). The English Translation Treebank, ETT
(Bies, 2007), is the translation6 of broadcast news
in Arabic. For literature, we use the BROWN cor-
pus (Francis and Kuc?era, 1979) and the same di-
vision as (Gildea, 2001; Bacchiani et al, 2006;
McClosky et al, 2006b). We also use raw sen-
tences which we downloaded from Project Guten-
berg7 as a self-trained corpus. The Switchboard cor-
pus (SWBD) consists of transcribed telephone con-
versations. While the original trees include disflu-
ency information, we assume our speech corpora
have had speech repairs excised (e.g. using a sys-
tem such as Johnson et al (2004)). Our biomedi-
cal data comes from the GENIA treebank8 (Tateisi
et al, 2005), a corpus of abstracts from the Med-
line database.9 We downloaded additional sentences
6The transcription and translation were done by humans.
7http://gutenberg.org/
8http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/
9http://www.ncbi.nlm.nih.gov/PubMed/
from Medline for our self-trained MEDLINE corpus.
Unlike the other two self-trained corpora, we include
two versions of MEDLINE. These differ on whether
they were parsed using GENIA or WSJ as a base
model to study the effect on cross-domain perfor-
mance. Finally, we use a small number of sentences
from the British National Corpus (BNC) (Foster and
van Genabith, 2008).10 The sentences were chosen
randomly, so each one is potentially from a different
domain. On the other hand, BNC can be thought of
as its own domain in that it contains significant lex-
ical differences from the American English used in
our other corpora.
We preprocessed the corpora to standardize many
of the annotation differences. Thus, our results on
them may be slightly different than other works on
these corpora. Nevertheless, these changes should
not significantly impact overall the performance.
6.2 Feature selection
While our final model uses only three features, we
considered many other possible features (not de-
scribed due to space constraints). In order to explore
these without hill climbing on our test data, we cre-
ated a round-robin tuning scenario. Since the out-
of-domain evaluation scenario holds out one target
domain, this gives us six test evaluation rounds. For
each of these six rounds, we hold out one of the re-
maining five target domains for tuning. This gives
us 30 tuning evaluation rounds and we pick our fea-
tures to optimize our aggregate performance over all
of them. A model that performs well in this situation
has proven that it has useful features which transfer
to unknown target domains.
The next step is to determine the loss function
to minimize. Our primary guide is oracle f-score
loss which we determine as follows. We take all
test data points (i.e. mixed parsing models evalu-
ated on the target domain) and predict their f-scores
with our model. In particular for this measure, we
are interested in the point with the highest predicted
f-score. We take its actual f-score which we call
the candidate f-score. When tuning, we know the
true f-scores of all test points. The difference be-
tween the highest f-score (the oracle f-score for
10http://nclt.computing.dcu.ie/
?
jfoster/
resources/, downloaded January 8th, 2009.
33
Corpus Source? Target? Average length Train Tune Test
BNC ? 28.3 ? ? 1,000
BROWN ? ? 20.0 19,786 2,082 2,439
ETT ? ? 25.6 2,639 1,029 1,166
GENIA ? ? 27.5 14,326 1,361 1,360
MEDLINE ? 27.2 278,192 ? ?
SWBD ? ? 9.2 92,536 5,895 6,051
WSJ ? ? 25.5 39,832 1,346 2,416
NANC ? 23.2 915,794 ? ?
GUTENBERG ? 26.2 689,782 ? ?
MEDLINE ? 27.2 278,192 ? ?
Table 3: List of source and target domains, sizes of each division in trees, and average sentence length.
Indented rows indicate self-trained corpora parsed using the non-indented row as a base parser.
this dataset) and the candidate f-score is the oracle
f-score loss. Ties need to be handled correctly to
avoid degenerate models.11 If there is a tie for high-
est predicted f-score, the candidate f-score is the
one with the lowest actual f-score. This approach
is conservative but ensures that regression models
which give everything the same predicted f-score do
not receive zero oracle f-score loss.
Armed with a tuning regime and a loss function,
we created a procedure to pick the combination of
features to use. We used a parallelized best-first
search procedure. At each round, it expanded the
current best set of features by adding or removing
each feature where ?best? was determined by the loss
function. We explored over 6,000 settings, though
the best setting of (UNKWORDS, COSINETOP50,
ENTROPY) was found within the first 200 settings
explored. The best setting obtains an oracle f-score
loss of 0.37 and a root mean squared error of 0.48
? these numbers are quite low and show the high
accuracy of our regression model (similar to those
in Ravi et al (2008)). Additionally, the features are
complementary in that UNKWORDS focuses on low
frequency words whereas COSINETOP50 looks only
at high frequency words and ENTROPY functions as
a regularizer.
6.3 Results
We present an overview of our final results for out-
of-domain and in-domain evaluation in Table 4. The
11For example, regression models which assign every parsing
model the same f-score.
results include the f-score macro-averaged over the
six target domains and their standard deviation.
In both situations, the FIXED SET: WSJ baseline
performs fairly poorly. Not surprisingly, assuming
all of our target domains are close enough to WSJ
works badly for our set of target domains and it
does particularly poorly on SWBD and GENIA. On
average, the UNIFORM baseline does slightly bet-
ter for out-of-domain and over 3% better for in-
domain. UNIFORM actually does fairly well on out-
of-domain except on GENIA. In general, using more
source domains is better which partially explains the
success of UNIFORM. This seems to be the case
since even if a source domain is terribly mismatched
with the target domain, it may still be able to fill
in some holes left by the other source domains. Of
course, if it overpowers more relevant domains, per-
formance may suffer. The SELF-TRAINED UNI-
FORM baseline uses even more source domains as
well as the largest ones. In both scenarios, this dra-
matically improves performance and is the second
best non-oracle system. This baseline provides more
evidence as to the power of self-training for improv-
ing parser adaptation. If we excluded all self-trained
corpora, our performance on this task would be sub-
stantially worse. We believe the self-trained cor-
pora are beneficial in this task since they help reduce
data sparsity of smaller corpora. The BEST SINGLE
CORPUS baseline is poor in the out-of-domain sce-
nario primarily because the actual best single corpus
is excluded by the task specification in most cases.
When we move to in-domain, this baseline improves
34
Oracle Baseline or model Average f-score
? Worst seen 62.0 ? 6.1
? Best single corpus 81.0 ? 2.9
Fixed set: WSJ 81.0 ? 3.5
Uniform 81.4 ? 3.6
Self-trained uniform 83.4 ? 2.5
Our model 84.0 ? 2.5
? Best seen 84.3 ? 2.6
(a) Out-of-domain evaluation
Oracle Baseline or model Average f-score
Fixed set: WSJ 82.0 ? 4.8
Uniform 85.4 ? 2.4
? Best single corpus 85.6 ? 2.9
Self-trained uniform 86.1 ? 2.0
? Best overall model 86.2 ? 1.9
Our model 86.9 ? 2.4
? Best seen 87.5 ? 2.1
(b) In-domain evaluation
Table 4: Baselines and final results for the two multiple-source domain adaptation evaluation scenarios.
Results include f-scores, macro-averaged over all six target domains and their standard deviations.
but is still worse than SELF-TRAINED UNIFORM on
average. It beats SELF-TRAINED UNIFORM primar-
ily on WSJ, SWBD, and GENIA indicating that these
three domains are best when not diluted by others.
By definition, the WORST SEEN baseline does terri-
bly, almost 20% worse then BEST SINGLE CORPUS.
Our model is the best non-oracle system for both
evaluation scenarios. For out-of-domain evaluation,
our system is only 0.3% worse than the BEST SEEN
models for each target domain. For the in-domain
scenario, we are within 0.6% of the BEST SEEN
models. For a sense of scale, our out-of-domain and
in-domain f-scores on WSJ are 83.1% and 89.8%
respectively. Both numbers are quite close to the
BEST SEEN baseline. Additionally, our model is
0.7% better than the BEST OVERALL MODEL. Re-
call that the BEST OVERALL MODEL is the single
model with the best performance across all six tar-
get domains.12 By beating this baseline, we show
that there is value in customizing parsing models
to the target domain. It is also interesting that the
BEST OVERALL MODEL is only marginally better
than SELF-TRAINED UNIFORM. Without any fur-
ther information about the target corpus, an unin-
formed prior appears best.
7 Discussion
We have shown that for both out-of-domain and in-
domain evaluations, our system is well adapted to
predicting the effects of domain divergence on pars-
12Somewhat surprisingly, the best overall model uses almost
entirely self-trained corpora consisting of 9.5% GUTENBERG,
60.3% NANC, 26.0% MEDLINE (by GENIA), and 4.2% SWBD.
ing accuracy. Using the parsing model with the
highest predicted f-score leads to great performance
in practice. There is a substantial benefit to doing
this over existing approaches (using the same model
for all domains or mixing all training data together
uniformly). Creating a number of domain-specific
models and mixing them together as needed is a vi-
able approach.
One can think of our system as trying to esti-
mate document-level context. Our representation of
this context is simply a distribution over our source
domains, but one can imagine more complex op-
tions such as a high-dimensional vector space. Ad-
ditionally, our model separates domain and syntax
estimation, but a future direction is to learn these
jointly. This would combine our work with (Daume?
III, 2007; Finkel and Manning, 2009).
We have focused on the Charniak (2000) parser,
the first stage in the two stage Charniak and John-
son (2005) reranking parser. Applying our methods
to other generative parsers (such as (Collins, 1999;
Petrov and Klein, 2007)) is trivial, but it is less clear
how our methods can be applied to the discrimina-
tive reranker component of the two stage parser. One
avenue of approach is to incorporate the domain rep-
resentation into the feature space, as in Daume? III
(2007) but with more complex domain information.
Acknowledgments
This work was performed while the first author was
at Brown and supported by DARPA GALE contract
HR0011-06-2-0001. We would like to thank the BLLIP
team and our anonymous reviewers for their comments.
35
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Ann Bies. 2007. GALE Phase 3 Release 1 - English
Translation Treebank. Linguistic Data Consortium.
LDC2007E105.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Association for Computational Linguistics, Prague,
Czech Republic.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of
CoNLL 2008, pages 9?16, Manchester, England, Au-
gust.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the ACL 2005, pages 173?180.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the North American Chapter
of the ACL (NAACL), pages 132?139.
Michael Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, The Uni-
versity of Pennsylvania.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL 2007, Prague, Czech
Republic.
Mark Dredze and Koby Crammer. 2008. Online methods
for multi-domain learning and adaptation. In Proceed-
ings of the EMNLP 2008, pages 689?697, Honolulu,
Hawaii, October.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Proceed-
ings of HLT-NAACL 2009, pages 602?610, Boulder,
Colorado, June.
Jennifer Foster and Josef van Genabith. 2008. Parser
evaluation and the bnc: Evaluating 4 constituency
parsers with 3 metrics. In Proceedings LREC 2008,
Marrakech, Morocco, May.
W. Nelson Francis and Henry Kuc?era. 1979. Manual
of Information to accompany a Standard Corpus of
Present-day Edited American English, for use with
Digital Computers. Brown University, Providence,
Rhode Island.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Empirical Methods in Natural Language
Processing (EMNLP), pages 167?202.
David Graff. 1995. North American News Text Corpus.
Linguistic Data Consortium. LDC95T21.
Mark Johnson, Eugene Charniak, and Matthew Lease.
2004. An improved model for recognizing disfluen-
cies in conversational speech. In Proc. of the Rich Text
2004 Fall Workshop (RT-04F).
Daisuke Kawahara and Kiyotaka Uchimoto. 2008.
Learning reliability of parses for domain adaptation
of dependency parsing. In Third International Joint
Conference on Natural Language Processing (IJCNLP
?08).
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Comp. Linguis-
tics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark Johnson.
2006a. Effective self-training for parsing. In Proceed-
ings of HLT-NAACL 2006, pages 152?159.
David McClosky, Eugene Charniak, and Mark John-
son. 2006b. Reranking and self-training for parser
adaptation. In Proceedings of COLING-ACL 2006,
pages 337?344, Sydney, Australia, July. Association
for Computational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404?411, Rochester, New York, April. Association for
Computational Linguistics.
Barbara Plank and Khalil Sima?an. 2008. Subdomain
sensitive statistical parsing using raw corpora. In
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08), Marrakech, Mo-
rocco, May.
Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Au-
tomatic prediction of parser accuracy. In Proceedings
of the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 887?896, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proceedings of the 7th conference of the
EACL, pages 141?148.
Satoshi Sekine. 1997. The domain dependence of pars-
ing. In Proc. Applied Natural Language Processing
(ANLP), pages 96?102.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael
Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing.
In Proceedings EMNLP 2009, pages 551?560, Singa-
pore, August.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the GE-
NIA corpus. Proceedings of IJCNLP 2005, Compan-
ion volume, pages 222?227.
36
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 172?181,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Apples to Oranges: Evaluating Image Annotations from Natural Language
Processing Systems
Rebecca Mason and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University, Providence, RI 02912
{rebecca,ec}@cs.brown.edu
Abstract
We examine evaluation methods for systems
that automatically annotate images using co-
occurring text. We compare previous datasets
for this task using a series of baseline mea-
sures inspired by those used in information re-
trieval, computer vision, and extractive sum-
marization. Some of our baselines match or
exceed the best published scores for those
datasets. These results illuminate incorrect as-
sumptions and improper practices regarding
preprocessing, evaluation metrics, and the col-
lection of gold image annotations. We con-
clude with a list of recommended practices for
future research combining language and vi-
sion processing techniques.
1 Introduction
Automatic image annotation is an important area
with many applications such as tagging, generat-
ing captions, and indexing and retrieval on the web.
Given an input image, the goal is to generate rel-
evant descriptive keywords that describe the visual
content of the image. The Computer Vision (CV)
literature contains countless approaches to this task,
using a wide range of learning techniques and visual
features to identify aspects such as objects, people,
scenes, and events.
Text processing is computationally less expensive
than image processing and easily provides informa-
tion that is difficult to learn visually. For this reason,
most commerical image search websites identify the
semantic content of images using co-occurring text
exclusively. But co-occurring text is also a noisy
source for candidate annotations, since not all of the
text is visually relevant. Techniques from Natural
Language Processing help align descriptive words
and images. Some examples of previous research
use named-entity recognition to identify people in
images (Deschacht and Moens, 2007); term associa-
tion to estimate the ?visualness? of candidate anno-
tations (Boiy et al, 2008; Leong et al, 2010); and
topic models to annotate images given both visual
and textual features (Feng and Lapata, 2010b).
Image annotation using NLP is still an emerging
area with many different tasks, datasets, and eval-
uation methods, making it impossible to compare
many recent systems to each other. Although there is
some effort being made towards establishing shared
tasks1, it is not yet clear which kinds of tasks and
datasets will provide interesting research questions
and practical applications in the long term. Until
then, establishing general ?best practices? for NLP
image annotation will help advance and legitimitize
this work. In this paper, we propose some good prac-
tices and demonstrate why they are important.
2 Image Annotation Evaluation in CV and
NLP
In this section, we first review related work in im-
age annotation evaluation in computer vision, spe-
cific challenges, and proposed solutions. We then
relate these challenges to the NLP image annotation
task and some of the specific problems we propose
to address.
1http://imageclef.org/
172
2.1 Related Work in Computer Vision
The work of Mu?ller et al (2002) is one of the first
to address the issue of evaluation for image annota-
tion systems. While using the exact same annotation
system, dataset, and evaluation metric, they dramati-
cally improve the apparent performance of their sys-
tem by using dataset pruning heuristics.
Others have criticized commonly-used CV
datasets for being too ?easy? ? images with the
same keywords are extremely similar in low-level
features such as orientation, lighting, and color;
while differences between images with different
keywords are very clear (Westerveld and de Vries,
2003; Ponce et al, 2006; Herve? and Boujemaa,
2007; Tang and Lewis, 2007). These features are
unwittingly exploited by certain algorithms and
obscure the benefits of using more complex tech-
niques (Ponce et al, 2006). The problem is further
exacerbated by evaluation metrics which essentially
prefer precision over recall and are biased towards
certain keywords. Annotations in test data might
not include all of the ?correct? keywords, and
evaluation metrics need to account for the fact that
frequent keywords in the corpus are safer guesses
than keywords that appear less frequently (Monay
and Gatica-Perez, 2003).
New baseline techniques, evaluation metrics, and
datasets for image annotation have been developed
in response to these problems. Makadia et al (2008;
2010) define a basic set of low-level features, and
propose new baselines for more complex systems to
evaluate against. Barnard et al (2003) present a nor-
malized loss function to address the preference to-
wards precision in evaluation metrics. New datasets
are larger and provide more diverse images, and it is
now easy to obtain multiple human-annotations per
image thanks to distributed services such as Ama-
zon?s Mechanical Turk, and the ESP game (von
Ahn and Dabbish, 2004). Hanbury (2008) provides
an overview of popular CV annotation datasets and
methods used for building them.
2.2 Image Annotation using Natural Language
Processing
Many of the problems from CV image annotation
are also applicable to NLP image annotation, and
bringing NLP to the task brings new challenges as
well. One of these challenges is whether to allow
infrequent words to be pruned. In CV annotation it
is typical to remove infrequent terms from both the
keyword vocabulary and the evaluation data because
CV algorithms typically need a large number of ex-
amples to train on. However, using NLP systems
and baselines one can correctly annotate using key-
words that did not appear in the training set. Remov-
ing ?unlearnable? keywords from evaluation data, as
done in (Boiy et al, 2008; Feng and Lapata, 2010b),
artificially inflates performance against simple base-
lines such as term frequency.
Nearly all NLP annotation datasets use naturally-
occurring sources of images and text. A particu-
larly popular source is news images alongside cap-
tions or articles, which are collected online from
sources such as Yahoo! News (Berg et al, 2004; De-
schacht and Moens, 2007). There are also domain-
specific databases with images and descriptions such
as the art, antiques, and flowers corpora used in Boiy
et al (2008). Wikipedia has also been used as a
source of images and associated text (Tsikrika et al,
2011). These sources typically offer well-written
and cleanly formatted text but introduce the problem
of converting text into annotations, and the annota-
tions may not meet the requirements of the new task
(as shown in Section 3.1). Obtaining data via image
search engines is a common practice in CV (Fei-Fei
et al, 2004; Berg and Forsyth, 2006) and can also
be used to provide more challenging and diverse in-
stances of images and co-occurring text. The addi-
tional challenge for NLP is that text content on many
websites is written to improve their rank in search
engines, using techniques such as listing dozens of
popular keywords. Co-occurring text for retrieved
images on popular queries may not be representative
of the task to be performed.
3 Datasets
In this paper, we examine two established image an-
notation datasets: the BBC News Dataset of Feng
and Lapata (2008) (henceforth referred to as BBC),
and the general web dataset of Leong et al (2010)
(henceforth referred to as UNT). These datasets
were both built to evaluate image annotation systems
that use longer co-occurring text such as a news ar-
ticle or a webpage, but they use data from differ-
173
Dataset BBC UNT
data instances article, image, and caption from a
news story
image and text from a webpage
source of data scraped from BBC News website Google Image Search results
candidate keywords or
collocations for anno-
tation
descriptive unigram words from
training data
n ? 7-grams extracted from co-
occurring text; collocations must ap-
pear as article names on Wikipedia
gold annotations descriptive words from held-out im-
age captions
multiple human-authored annota-
tions for each image
evaluation metric precision and recall against gold an-
notations
metrics adapted from evaluation of
lexical substitutions (SemEval)
number of train in-
stances
3121 instances of related news arti-
cle, image, and caption
none (train using cross-validation)
number of test in-
stances
240 instances of news article and re-
lated image
300 instances of webpage with text
and image
preprocessing proce-
dure
lemmatize tokens, remove from
dataset al words that are not descrip-
tive or that appear fewer than five
times in training articles
stem all tokens
average number of
text tokens after
preprocessing
169 word tokens per article, 4.5 per
caption
278 word tokens per webpage
average document title
length
4 word tokens 6 word tokens
total vocabulary after
preprocessing
10479 word types 8409 word types
Table 1: Comparison of the BBC and UNT image annotation datasets.
ent domains, different sources of gold image anno-
tations, different preprocessing procedures, and dif-
ferent evaluation measures.
Table 1 provides an overview of the datasets;
while this section covers the source of the datsets
and their gold annotations in more detail.
3.1 BBC
The BBC Dataset (Feng and Lapata, 2008)2 contains
news articles, image captions, and images taken
from the BBC News website. Training instances
consist of a news article, image, and image caption
from the same news story. Test instances are just the
image and the article, and hold-out the caption as a
source of gold image annotations.
Using news image captions as annotations has
2Downloaded from http://homepages.inuf.ed.
ac.uk/s0677528/data.html
the disadvantage that captions often describe back-
ground information or relate the photo to the story,
rather than listing important entities in the image.
It also fails to capture variation in how humans de-
scribe images, since it is limited to one caption per
image.3 However, captions are a cheap source of
data; BBC has ten times as many images as UNT.
To address the problem of converting natural lan-
guage into annotations, a large amount of prepro-
cessing is performed. The established preprocessing
procedure for this dataset is to lemmatize and POS-
tag using TreeTagger (Schmid, 1994) then remove
all but the ?descriptive? words (defined as nouns, ad-
jectives, and certain classes of verbs). This leaves
a total text vocabulary of about 32K words, which
3The Pascal Sentences dataset (vision.cs.uiuc.edu/
pascal-sentences) provides multiple captions per image,
but they are not naturally-occurring.
174
is further reduced by removing words that appear
fewer than five times in the training set articles. Ta-
ble 1 shows the number of word tokens and types
after performing these steps.4
3.2 UNT
The UNT Dataset (Leong et al, 2010)5 consists
of images and co-occurring text from webpages.
The webpages are found by querying Google Image
Search with frequent English words, and randomly
selecting from the results.
Each image in UNT is annotated by five people
via Mechanical Turk. In order to make human and
system results comparable, human annotators are re-
quired to only select words and collocations that are
directly extracted from the text, and the gold anno-
tations are the count of how many times each key-
word or collocation is selected. The human annota-
tors write keywords into a text box; while the col-
locations are presented as a list of candidates and
annotators mark which ones are relevant. Human
annotators tend to select subsets of collocations in
addition to the entire collocation. For example, the
gold annotation for one image has ?university of
texas?, ?university of texas at dallas?, ?the univer-
sity of texas?, and ?the university of texas at dal-
las?, each selected by at least four of the five an-
notators. Additionally, annotators can select mul-
tiple forms of the same word (such as ?tank? and
?tanks?). Gold annotations are stemmed after they
are collected, and keywords with the same stem have
their counts merged. For this reason, many key-
words have a higher count than the number of an-
notators.
4 We are unable to reproduce work from Feng & Lapata
(2008; 2010a; 2010b) and Feng (2011). Specifically, our vocab-
ulary counts after preprocessing (as in Table 1) are much higher
than reported counts, although the number of tokens per arti-
cle/caption they report is higher than ours. We have contacted
the authors, who confirmed that they took additional steps to re-
duce the size of the vocabulary, but were unable to tell us exactly
what those steps are. Therefore, all system and baseline scores
presented on their dataset are of our own implementation, and
do not match those reported in previous publications.
5Downloaded from http://lit.csci.unt.edu/
index.php?P=research/downloads
4 Baselines
We run several baselines on the datasets. Term fre-
quency, tf*idf, and corpus frequency are features
that are often used in annotation systems, so it is im-
portant to test them on their own. Document Title
and tf*idf are both baselines that were used in the
original papers where these datasets came from.
Sentence extraction is a new baseline that we pro-
pose specifically for the BBC dataset, in order see if
we can exploit certain properties of the gold annota-
tions, which are also derived from sentences.
4.1 Term Frequency
Term frequency has been shown to be a power-
ful feature in summarization (Nenkova and Vander-
wende, 2005). Words that appear frequently are
considered more meaningful than infrequent words.
Term frequency is the number of times a term (ex-
cluding function words) appears in a document, di-
vided by the total number of terms in that document.
On the UNT dataset we use the stopword list in-
cluded with the MALLET6 toolkit, while the BBC
dataset doesn?t matter because the function words
have already been removed.
4.2 tf*idf
While term frequency baseline requires the use of an
ad hoc function word list, tf*idf adjusts the weights
of different words depending on how important they
are in the corpus. It is a standard baseline used for
information retrieval tasks, based on the intuition
that a word that appears in a smaller number of doc-
uments is more likely to be meaningful than a word
that appears in many documents.
tf*idf is the product of term frequency and inverse
document frequency ? idf(ti) = log Nni where N is
the number of documents in the corpus, and ni is
the number of documents that contain the term ti.
For the BBC Dataset, we base the idf weights on
the document frequency of the training articles. For
UNT, we use the reported tf*idf score which uses the
British National Corpus to calculate the idf scores.7
6mallet.cs.umass.edu
7We also ran tf*idf where for each document we recalcu-
late idf using the other 299, but it didn?t make any meaningful
difference.
175
4.3 Corpus Frequency
Image annotations in both NLP and CV tend to be
distributed with a relatively small number of fre-
quently occuring keywords, and a long tail of key-
words that only appear a few times. For UNT, we
use the total keyword frequency of all the gold an-
notations, except for the one document that we are
currently scoring. For BBC, we only measure the
frequency of keywords in the training set captions,
since we are specifically interested in the frequency
of terms in captions.
4.4 Document Title
For BBC, the news article headline, and for UNT,
the title of the webpage.
4.5 Sentence Extraction
Our baseline extracts the most central sentence from
the co-occurring text and uses descriptive words
from that sentence as the image annotation. Un-
like sentence extraction techniques from Feng and
Lapata (2010a), we determine which sentence to ex-
tract using the term frequency distribution directly.
We extract the sentence with the minimum KL-
divergence to the entire document.8
5 BBC Dataset Experiments
5.1 System Comparison
In addition to the baselines, we compare against the
Mix LDA system from Feng and Lapata (2010b). In
Mix LDA, each instance is represented as a bag of
textual features (unigrams) and visual features (SIFT
features quantized to discrete ?image words? using
k-means). A Latent Dirichlet Allocation topic model
is trained on articles, images, and captions from the
training set. Keywords are generated for an unseen
image and article pair by estimating the distribution
of topics that generates the test instance, then multi-
plying them with the word distributions in each topic
to find the probability of textual keywords for the
image. Text LDA is is the same model but only us-
ing words and not image features.
8One could also think of this as a version of the KLSum
summarization system (Haghighi and Vanderwende, 2009) that
stops after one sentence.
5.2 Evaluation
The evaluation metric and the source of gold anno-
tations is described in Table 1. For the baselines 4.1,
4.2, 4.3 and the Mix LDA system, the generated an-
notation for each test image is its ten most likely
keywords. We also run all baselines and the Mix
LDA system on an unpruned version of the dataset,
where infrequent terms are not removed from train-
ing data, test data, or the gold annotations. The pur-
pose of this evaluation is to see if candidate key-
words deemd ?unlearnable? by the Mix LDA system
can be learned by the baselines.
5.3 Results
The evaluation results for the BBC Dataset are
shown in Table 2. Clearly, term frequency is a
stronger baseline than tf*idf by a large margin. The
reason for this is simple: since nearly all of BBC?s
function words are removed during preprocessing,
the only words downweighted by the idf score are
common ? but meaningful ? words such as police or
government. This is worth pointing out because, in
many cases, the choice of using a term frequency or
tf*idf baseline is made based on what was used in
previous work. As we show here and in Section 6.3,
the choice of frequency baseline should be based on
the data and processing techniques being used.
We use the corpus frequency baseline to illus-
trate the difference between standard and include-
infrequent evaluations. Since including infrequent
words doesn?t change which are most frequent in
the dataset, precision for corpus frequency doesn?t
change. But since infrequent words are now in-
cluded in the evaluation data, we see a 0.5% drop in
recall (since corpus frequency won?t capture infre-
quent words). Compared to the other baselines, this
is not a large difference. Other baselines see a larger
drop in recall because they have both more gold key-
words to estimate and more candidate keywords to
consider. tf*idf is the most affected by this, because
idf overly favors very infrequent keywords, despite
their low term frequency. In comparison, the term
frequency baseline is not as negatively affected and
even improves in precision because there are some
cases where a word is very important to an article
in the test set but just didn?t appear very often in the
training set (see Table 3 for examples). But the base-
176
Standard Include-infrequent
Precision Recall F1 Precision Recall F1
Term Frequency 13.13 27.84 17.84 13.62 25.71 17.81
tf * idf 9.21 19.97 12.61 7.25 13.52 9.44
Doc Title 17.23 13.70 15.26 15.91 11.86 13.59
Corpus Frequency 3.17 6.52 4.26 3.17 6.02 4.15
Sentence Extraction 16.67 15.61 16.13 18.62 16.83 17.68
Mix LDA 7.30 16.16 10.06 7.50 13.98 9.76
Text LDA 8.38 17.46 11.32 7.79 14.52 10.14
Table 2: Image annotation results for previous systems and our proposed baselines on the BBC Dataset.
Cadbury increase
contamination
testing level
malaria parasite
spread mosquito
Table 3: Examples of gold annotations from the test sec-
tion of the BBC Dataset. The bolded words are the ones
that appear five or more times in the training set; the un-
bolded words appear fewer than five times and would be
removed from both the candidate and gold keywords in
the standard BBC evaluation.
lines with the best precision are the Doc Title and
Sentence Extraction baselines, which do not need to
generate ten keywords for every image.
While sentence extraction has a lower recall than
term frequency, it is the only baseline or system
that has improved recall when including infrequent
words. This is unexpected because our baseline se-
lects a sentence based on the term frequency of the
document, and the recall for term frequency fell.
One possible explanation is that extraction implic-
itly uses correlations between keywords. Probabili-
ties of objects appearing together in an image are not
independent; and the accuracy of annotations can be
improved by generating annotation keywords as a
set (Moran and Lavrenko, 2011). Recent works in
image captioning also use these correlations: explic-
itly, using graphical models (Kulkarni et al, 2011;
Yang et al, 2011); and implicitly, using language
models (Feng and Lapata, 2010a). In comparison,
sentence extraction is very implicit.
Unsurprisingly, the Text LDA and Mix LDA sys-
tems do worse on the include-infrequent evaluation
than they do on the standard, because words that
do not appear in the training set will not have high
probability in the trained topic models. We were un-
able to reproduce the reported scores for Mix LDA
from Feng and Lapata (2010b) where Mix LDA?s
scores were double the scores of Text LDA (see
Footnote 4). We were also unable to reproduce re-
ported scores for tf*idf and Doc Title (Feng and Lap-
ata, 2008). However, we have three reasons why we
believe our results are correct. First, BBC has more
keywords, and fewer images, than typically seen in
CV datasets. The BBC dataset is simply not suited
for learning from visual data. Second, a single SIFT
descriptor describes which way edges are oriented
at a certain point in an image (Lowe, 1999). While
certain types of edges may correlate to visual objects
also described in the text, we do not expect SIFT fea-
tures to be as informative as textual features for this
task. Third, we refer to the best system scores re-
ported by Leong et al (2010), who evaluate their text
mining system (see section 6.1) on the standard BBC
dataset.9 While their f1 score is slightly worse than
our term frequency baseline, they do 4.86% better
than tf*idf. But, using the baselines reported in Feng
and Lapata (2008), their improvement over tf*idf is
12.06%. Next, we compare their system against fre-
quency baselines using the 10 keyword generation
task on the UNT dataset (the oot normal scores in
table 5). Their best system performs 4.45% better
9Combined model; precision: 13.38, recall: 25.17, f1:
17.47. Crucially, they do not reimplement previous systems or
baselines, but use scores reported from Feng and Lapata (2008).
177
than term frequency, and 0.55% worse than tf*idf.10
Although it is difficult to compare different datasets
and evaluation metrics, our baselines for BBC seem
more reasonable than the reported baselines, given
their relative performance to Leong et als system.
6 UNT Dataset Experiments
6.1 System Comparison
We evaluate against the text mining system from
(Leong et al, 2010). Their system generates image
keywords by extracting text from the co-occurring
text of an image. It uses three features for select-
ing keywords. Flickr Picturability queries the Flickr
API with words from the text in order to find re-
lated image tags. Retrieved tags that appear as sur-
face forms in the text are rewarded proportional to
their frequency in the text. Wikipedia Salience as-
signs scores to words based on a graph-based mea-
sure of importance that considers each term?s docu-
ment frequency in Wikipedia. Pachinko Allocation
Model is a topic model that captures correlations be-
tween topics (Li and McCallum, 2006). PAM infers
subtopics and supertopics for the text, then retrieves
top words from the top topics as annotations. There
is also a combined model of these features using an
SVM with 10-fold cross-validation.
6.2 Evaluation
Evaluation on UNT uses a framework originally de-
veloped for the SemEval lexical substitution task
(McCarthy and Navigli, 2007). This framework
accounts for disagreement between annotators by
weighting each generated keyword by the number of
human annotators who also selected that keyword.
The scoring framework consists of four evaluation
measures: best normal, best mode, oot (out-of-ten)
normal, and oot mode.11
The two best evaluations find the accuracy of a
single ?best? keyword generated by the system12.
10And as we stated earlier, the relative performance of term
frequency vs tf*idf is different from dataset to dataset.
11Both the original framework and its adaptation by Leong
et al (2010) give precision and recall for each of the evaluation
measures. However, precision and recall are identical for all
baselines and systems, and only slightly different on the upper
bound (human) scores. To preserve space, we only present the
metric and scores for precision.
12In contrast to the original SemEval task, where systems can
Best normal measures the accuracy for each system
annotation aj as the number of times aj appears in
the Rj , the multi-set union of human tags, and aver-
ages over all the test images.
Bestnormal =
?
ij?I
|aj?Rj |
|Rj |
|I|
In oot normal, up to ten unordered guesses can be
made without penalty.
ootnormal =
?
ij?I
?
aj?Aj
|aj?Rj |
|Rj |
|I|
where Aj is the set of ten system annotations for
image ij .
The best mode and oot mode metrics are the same
as the normal metrics except they only evaluate sys-
tem annotations for images where Rj contains a sin-
gle most frequent tag. We use the scoring software
provided by SemEval13 with the gold annotation file
provided in the UNT Dataset.
6.3 Results
The results of the lexical substitution evaluation on
the UNT Dataset are shown in Table 5. The results
from the normal show support for our earlier idea
that the relative performance of term frequency vs
tf*idf depends on the dataset. Although the term fre-
quency baseline uses a stopword list, there are other
words that appear frequently enough to suggest they
are not meaningful to the document ? such as copy-
right disclaimers.
Recall that the mode evaluation is only measured
on data instances where the gold annotations have
a single most frequent keyword. While running
the evaluation script on the gold annotation file that
came with the UNT dataset, we discover that Se-
mEval only identifies 28 of the 300 instances as hav-
ing a single mode annotation, and that for 21 of
those 28 instances, the mode keyword is ?cartoon?.
Those 21/28 images correspond to the 75% best
mode score obtained by Corpus Frequency baseline.
Given the small number of instances that actually
make from zero to many ?best? guesses, penalized by the total
number of guesses made.
13http://nlp.cs.swarthmore.edu/semeval/
tasks/task10/data.shtml
178
cartoon(6), market(5), market share(5),
declin(3), imag(3), share(3), pictur(1),
illustr(1), cartoonstock(1), origin(1),
artist(1), meet(1), jfa0417(1), meeting-
copyright(1)
cartoon(6), bill gate(5), gate(4), monop-
oli(4), pearli gate(4), bill(3), imag(3),
caricatur(2), pictur(2), illustr(1), copy-
right(1), artist(1), own(1), pearli(1)
lift index(5), gener(3), index(3), con-
dit(2), comput(2), comput gener(2),
unstabl(2), zone(2), area(1), field(1),
between(1), stabl(1), encyclopedia(1),
thunderstorm(1), lift(1), free encyclope-
dia(1), wikipedia(1)
Table 4: Examples of gold annotations from the UNT Dataset.
Best Out-of-ten (oot)
Normal Mode Normal Mode
Term Frequency 5.67 14.29 33.40 89.29
tf * idf 5.94 14.29 38.40 78.57
Doc Title 6.40 7.14 35.19 92.86
Corpus Frequency 2.54 75.00 8.22 82.14
Flickr Picturability 6.32 78.57 35.61 92.86
Wikipedia Salience 6.40 7.14 35.19 92.86
Topic Model (PAM) 5.99 42.86 37.13 85.71
Combined (SVM) 6.87 67.49 37.85 100.00
Table 5: Image annotation results for our proposed baselines, the text mining systems from (Leong et al, 2010)
count towards these metrics, we conclude that mode
evaluation is not a meaningful way to compare im-
age annotation systems on the UNT dataset.
That said, the number of cartoons in the dataset
does seem to be strikingly high. Looking at the
source of the images, we find that 45 of the 300
images were collected from a single online cartoon
library. Predictably, we find that the co-occurring
text to these images contains a long list of keywords,
and little other text that is relevant to the image. We
looked at a small sample of the rest of the dataset
and found that many of the other text documents in
UNT also have keyword lists.
Including this types of text in a general web cor-
pus is not necessarily a problem, but it?s difficult to
measure the benefits of using complex techniques
like topic modeling and graph similarily to find and
extract annotations when in so many cases the anno-
tations have already been found and extracted. This
is shown in the normal evaluation results, where the
combined system is only slightly better at selecting
the single best keyword, and no better than tf*idf for
the out-of-ten measure.
7 Conclusion
The intent of this paper is not to teach researchers
how to inflate their own results, but to encourage bet-
ter practices. With that purpose in mind, we make
the following suggestions regarding future work in
this area:
179
Get to know your data. The ability to quickly
and cheaply collect very large ? but very noisy ? col-
lections of data from the internet is a great advance
for both NLP and CV research. However, there still
needs to be an appopriate match betwen the task be-
ing performed, the system being proposed, and the
dataset being used; and large noisy datasets can hide
unintended features or incorrect assumptions about
the data.
Use relevant gold annotations. Do not convert
other sources of data into annotations. When collect-
ing human annotations, avoid postprocessing steps
such as merging or deleting keywords that change
the annotators? original intent. Keep an open di-
alogue with annotators about issues that they find
confusing, since that is a sign of an ill-formed task.
Preprocessing should be simple and reprodu-
cable. The use of different preprocessing proce-
dures affects the apparent performance of systems
and sometimes has unintended consequences.
Use strong baselines and compare to other work
only when appropriate. Systems developed for dif-
ferent tasks or datasets can make for misleading
comparisons if they don?t use all features available.
Strong baselines explicitly exploit low-level features
that are implicitly exploited by proposed systems, as
well as low-level features of the dataset.
Don?t remove keywords from gold annotations.
Just because keywords are impossible for one sys-
tem to learn, does not mean they are impossible for
all systems to learn. Removing evaluation data arti-
ficially inflates system scores and limits comparison
to related work.
If a proposed system is to learn associations be-
tween visual and textual features, then it is neces-
sary to use larger datasets. In general, global an-
notations, such as scenes, is easiest; identifying spe-
cific objects is more difficult; and identification of
events, activities, and other abstract qualities has a
very low success rate (Fluhr et al, 2006). Alter-
nately, use simpler image features that are known
to have a high sucess rate. For example, Deschacht
and Moens (2007) used a face detector to determine
the number of faces in an image, and then used NLP
to determine the names of those people from associ-
ated text.
References
K. Barnard, P. Duygulu, D. Forsyth, N. De Freitas, D.M.
Blei, and M.I. Jordan. 2003. Matching words and
pictures. The Journal of Machine Learning Research,
3:1107?1135.
Tamara L. Berg and David A. Forsyth. 2006. Animals on
the web. In Proceedings of the 2006 IEEE Computer
Society Conference on Computer Vision and Pattern
Recognition - Volume 2, CVPR ?06, pages 1463?1470,
Washington, DC, USA. IEEE Computer Society.
T.L. Berg, A.C. Berg, J. Edwards, M. Maire, R. White,
Yee-Whye Teh, E. Learned-Miller, and D.A. Forsyth.
2004. Names and faces in the news. In Computer Vi-
sion and Pattern Recognition, 2004. CVPR 2004. Pro-
ceedings of the 2004 IEEE Computer Society Confer-
ence on, volume 2, pages II?848 ? II?854 Vol.2, june-2
july.
E. Boiy, K. Deschacht, and M.-F. Moens. 2008. Learn-
ing visual entities and their visual attributes from text
corpora. In Database and Expert Systems Applica-
tion, 2008. DEXA ?08. 19th International Workshop
on, pages 48 ?53, sept.
Koen Deschacht and Marie-Francine Moens. 2007. Text
analysis for automatic image annotation. In ACL, vol-
ume 45, page 1000.
Li Fei-Fei, Rob Fergus, and Pietro Perona. 2004. Learn-
ing generative visual models from few training ex-
amples: An incremental bayesian approach tested on
101 object categories. In Proceedings of the 2004
Conference on Computer Vision and Pattern Recog-
nition Workshop (CVPRW?04) Volume 12 - Volume
12, CVPRW ?04, pages 178?, Washington, DC, USA.
IEEE Computer Society.
Yansong Feng and Mirella Lapata. 2008. Automatic im-
age annotation using auxiliary text information. Pro-
ceedings of ACL-08: HLT, pages 272?280.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? automatic caption gener-
ation for news images. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1239?1249, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Yansong Feng and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In HLT-
NAACL, pages 831?839.
Yansong Feng. 2011. Automatic caption generation for
news images. Ph.D. thesis, University of Edinburgh.
Christian Fluhr, Pierre-Alain Mollic, and Patrick Hde.
2006. Usage-oriented multimedia information re-
trieval technological evaluation. In James Ze Wang,
Nozha Boujemaa, and Yixin Chen, editors, Multime-
dia Information Retrieval, pages 301?306. ACM.
180
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362?370, Boulder, Colorado, June. Association
for Computational Linguistics.
Allan Hanbury. 2008. A survey of methods for image
annotation. J. Vis. Lang. Comput., 19:617?627, Octo-
ber.
Nicolas Herve? and Nozha Boujemaa. 2007. Image an-
notation: which approach for realistic databases? In
Proceedings of the 6th ACM international conference
on Image and video retrieval, CIVR ?07, pages 170?
177, New York, NY, USA. ACM.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and generat-
ing simple image descriptions. In CVPR, pages 1601?
1608.
Chee Wee Leong, Rada Mihalcea, and Samer Hassan.
2010. Text mining for automatic image tagging. In
COLING, pages 647?655.
Wei Li and Andrew McCallum. 2006. Pachinko alloca-
tion: Dag-structured mixture models of topic correla-
tions. In Proceedings of the 23rd international confer-
ence on Machine learning, ICML ?06, pages 577?584,
New York, NY, USA. ACM.
D.G. Lowe. 1999. Object recognition from local scale-
invariant features. In Computer Vision, 1999. The Pro-
ceedings of the Seventh IEEE International Confer-
ence on, volume 2, pages 1150 ?1157 vol.2.
Ameesh Makadia, Vladimir Pavlovic, and Sanjiv Kumar.
2008. A new baseline for image annotation. In Pro-
ceedings of the 10th European Conference on Com-
puter Vision: Part III, ECCV ?08, pages 316?329,
Berlin, Heidelberg. Springer-Verlag.
A. Makadia, V. Pavlovic, and S. Kumar. 2010. Baselines
for image annotation. International Journal of Com-
puter Vision, 90(1):88?105.
D. McCarthy and R. Navigli. 2007. Semeval-2007 task
10: English lexical substitution task. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations (SemEval-2007), pages 48?53.
Florent Monay and Daniel Gatica-Perez. 2003. On im-
age auto-annotation with latent space models. In Pro-
ceedings of the eleventh ACM international conference
on Multimedia, Multimedia ?03, pages 275?278, New
York, NY, USA. ACM.
S. Moran and V. Lavrenko. 2011. Optimal tag sets for
automatic image.
Henning Mu?ller, Ste?phane Marchand-Maillet, and
Thierry Pun. 2002. The truth about corel - evaluation
in image retrieval. In Proceedings of the International
Conference on Image and Video Retrieval, CIVR ?02,
pages 38?49, London, UK, UK. Springer-Verlag.
Ani Nenkova and Lucy Vanderwende. 2005. The im-
pact of frequency on summarization. Technical report,
Microsoft Research.
J. Ponce, T. Berg, M. Everingham, D. Forsyth, M. Hebert,
S. Lazebnik, M. Marszalek, C. Schmid, B. Russell,
A. Torralba, C. Williams, J. Zhang, and A. Zisser-
man. 2006. Dataset issues in object recognition.
In Jean Ponce, Martial Hebert, Cordelia Schmid, and
Andrew Zisserman, editors, Toward Category-Level
Object Recognition, volume 4170 of Lecture Notes
in Computer Science, pages 29?48. Springer Berlin /
Heidelberg. 10.1007/11957959 2.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of international
conference on new methods in language processing,
volume 12, pages 44?49. Manchester, UK.
Jiayu Tang and Paul Lewis. 2007. A study of quality
issues for image auto-annotation with the corel data-
set. IEEE Transactions on Circuits and Systems for
Video Technology, Vol. 1(NO. 3):384?389, March.
T. Tsikrika, A. Popescu, and J. Kludas. 2011. Overview
of the wikipedia image retrieval task at imageclef
2011. In CLEF (Notebook Papers/LABs/Workshops):
CLEF.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, CHI ?04, pages 319?326, New York, NY,
USA. ACM.
Thijs Westerveld and Arjen P. de Vries. 2003. Ex-
perimental evaluation of a generative probabilistic im-
age retrieval model on ?easy? data. In In Proceedings
of the SIGIR Multimedia Information Retrieval Work-
shop 2003, Aug.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods in
Natural Language Processing (EMNLP), Edinburgh,
Scotland.
181
Proceedings of NAACL-HLT 2013, pages 85?94,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Extracting the Native Language Signal
for Second Language Acquisition
Ben Swanson
Brown University
Providence, RI
chonger@cs.brown.edu
Eugene Charniak
Brown University
Providence, RI
ec@cs.brown.edu
Abstract
We develop a method for effective extraction
of linguistic patterns that are differentially ex-
pressed based on the native language of the
author. This method uses multiple corpora
to allow for the removal of data set specific
patterns, and addresses both feature relevancy
and redundancy. We evaluate different rel-
evancy ranking metrics and show that com-
mon measures of relevancy can be inappro-
priate for data with many rare features. Our
feature set is a broad class of syntactic pat-
terns, and to better capture the signal we ex-
tend the Bayesian Tree Substitution Grammar
induction algorithm to a supervised mixture of
latent grammars. We show that this extension
can be used to extract a larger set of relevant
features.
1 Introduction
Native Language Identification (NLI) is a classifi-
cation task in which a statistical signal is exploited
to determine an author?s native language (L1) from
their writing in a second language (L2). This aca-
demic exercise is often motivated not only by fraud
detection or authorship attribution for which L1 can
be an informative feature, but also by its potential to
assist in Second Language Acquisition (SLA).
Our work focuses on the latter application and on
the observation that the actual ability to automati-
cally determine L1 from text is of limited utility in
the SLA domain, where the native language of a stu-
dent is either known or easily solicited. Instead, the
likely role of NLP in the context of SLA is to pro-
vide a set of linguistic patterns that students with
certain L1 backgrounds use with a markedly unusual
frequency. Experiments have shown that such L1
specific information can be incorporated into lesson
plans that improve student performance (Laufer and
Girsai, 2008; Horst et al 2008).
This is essentially a feature selection task with the
additional caveat that features should be individually
discriminative between native languages in order to
facilitate the construction of focused educational ex-
cersizes. With this goal, we consider metrics for
data set dependence, relevancy, and redundancy. We
show that measures of relevancy based on mutual in-
formation can be inappropriate in problems such as
ours where rare features are important.
While the majority of the methods that we con-
sider generalize to any of the various feature sets
employed in NLI, we focus on the use of Tree Sub-
stitution Grammar rules as features. Obtaining a
compact feature set is possible with the well known
Bayesian grammar induction algorithm (Cohn and
Blunsom, 2010), but its rich get richer dynamics can
make it difficult to find rare features. We extend the
induction model to a supervised mixture of latent
grammars and show how it can be used to incorpo-
rate linguistic knowledge and extract discriminative
features more effectively.
The end result of this technique is a filtered list of
patterns along with their usage statistics. This pro-
vides an enhanced resource for SLA research such
as Jarvis and Crossley (2012) which tackles the man-
ual connection of highly discriminative features with
plausible linguistic transfer explanations. We output
a compact list of language patterns that are empiri-
cally associated with native language labels, avoid-
85
ing redundancy and artifacts from the corpus cre-
ation process. We release this list for use by the
linguistics and SLA research communities, and plan
to expand it with upcoming releases of L1 labeled
corpora1.
2 Related Work
Our work is closely related to the recent surge of re-
search in NLI. Beginning with Koppel et al(2005),
several papers have proposed different feature sets
to be used as predictors of L1 (Tsur and Rappa-
port, 2007; Wong and Dras, 2011a; Swanson and
Charniak, 2012). However, due to the ubiquitous
use of random subsamples, different data prepara-
tion methods, and severe topic and annotation biases
of the data set employed, there is little consensus on
which feature sets are ideal or sufficient, or if any
reported accuracies reflect some generalizable truth
of the problem?s difficulty. To combat the bias of
a single data set, a new strain of work has emerged
in which train and test documents come from dif-
ferent corpora (Brooke and Hirst, 2012; Tetreault et
al, 2012; Bykh and Meurers, 2012). We follow this
cross corpus approach, as it is crucial to any claims
of feature relevance.
Feature selection itself is a well studied problem,
and the most thorough systems address both rele-
vancy and redundancy. While some work tackles
these problems by optimizing a metric over both si-
multaneously (Peng et al 2005), we decouple the
notions of relevancy and redundancy to allow ad-hoc
metrics for either, similar to the method of Yu and
Liu (2004). The measurement of feature relevancy
in NLI has to this point been handled primarily with
Information Gain, and elimination of feature redun-
dancy has not been considered.
Tree Substitution Grammars have recently been
successfully applied in several domains using the
induction algorithm presented by Cohn and Blun-
som (2010). Our hierarchical treatment builds on
this work by incorporating supervised mixtures over
latent grammars into this induction process. Latent
mixture techniques for NLI have been explored with
other feature types (Wong and Dras, 2011b; Wong
and Dras, 2012), but have not previously led to mea-
surable empirical gains.
1bllip.cs.brown.edu/download/nli corpus.pdf
3 Corpus Description
We first make explicit our experimental setup in or-
der to provide context for the discussion to follow.
We perform analysis of English text from Chinese,
German, Spanish, and Japanese L1 backgrounds
drawn from four corpora. The first three consist of
responses to essay prompts in educational settings,
while the fourth is submitted by users in an internet
forum.
The first corpus is the International Corpus of
Learner English (ICLE) (Granger et al 2002), a
mainstay in NLI that has been shown to exhibit a
large topic bias due to correlations between L1 and
the essay prompts used (Brooke and Hirst, 2011).
The second is the International Corpus of Crosslin-
guistic Interlanguage (ICCI) (Tono et al 2012),
which is annotated with sentence boundaries and has
yet to be used in NLI. The third is the public sample
of the Cambridge International Corpus (FCE), and
consists of short prompted responses. One quirk of
the FCE data is that several responses are written in
the form of letters, leading to skewed distributions
of the specialized syntax involved with use of the
second person. The fourth is the Lang8 data set in-
troduced by Brooke and Hirst (2011). This data set
is free of format, with no prompts or constraints on
writing aids. The samples are often very short and
are qualitatively the most noisy of the four data sets.
One distinctive experimental decision is to treat
each sentence as an individual datum. As document
length can vary dramatically, especially across cor-
pora, this gives increased regularity to the number
of features per data item. More importantly, this
creates a rough correspondence between feature co-
occurrence and the expression of the same under-
lying linguistic phenomenon, which is desirable for
automatic redundancy metrics.
We automatically detect sentence boundaries
when they are not provided, and parse all corpora
with the 6-split Berkeley Parser. As in previous NLI
work, we then replace all word tokens that do not oc-
cur in a list of 614 common words with an unknown
word symbol, UNK.
While these are standard data preprocessing steps,
from our experience with this problem we propose
additional practical considerations. First, we filter
the parsed corpora, retaining only sentences that are
86
parsed to a Clause Level2 tag. This is primarily due
to the fact that automatic sentence boundary detec-
tors must be used on the ICLE, Lang8, and FCE data
sets, and false positives lead to sentence fragments
that are parsed as NP, VP, FRAG, etc. The wild inter-
net text found in the Lang8 data set alo yields many
non-Clause Level parses from non-English text or
emotive punctuation. Sentence detection false neg-
atives, on the other hand, lead to run-on sentences,
and so we additionally remove sentences with more
than 40 words.
We also impose a simple preprocessing step for
better treatment of proper nouns. Due to the geo-
graphic distribution of languages, the proper nouns
used in a writer?s text naturally present a strong L1
signal. The obvious remedy is to replace all proper
nouns with UNK, but this is unfortunately insuffi-
cient as the structure of the proper noun itself can
be a covert signal of these geographical trends. To
fix this, we also remove all proper noun left sisters
of proper nouns. We choose to retain the rightmost
sister node in order to preserve the plurality of the
noun phrase, as the rightmost noun is most likely
the lexical head.
From these parsed, UNKed, and filtered corpora
we draw 2500 sentences from each L1 background
at random, for a total of 10000 sentences per corpus.
The exception is the FCE corpus, from which we
draw 1500 sentences per L1 due to its small size.
4 Tree Substitution Grammars
A Tree Substitution Grammar (TSG) is a model
of parse tree derivations that begins with a sin-
gle ROOT nonterminal node and iteratively rewrites
nonterminal leaves until none remain. A TSG
rewrite rule is a tree of any depth, as illustrated in
Figure 1, and can be used as a binary feature of a
parsed sentence that is triggered if the rule appears
in any derivation of that sentence.
Related NLI work compares a plethora of sug-
gested feature sets, ranging from character n-grams
to latent topic activations to labeled dependency
arcs, but TSG rules are best able to represent com-
plex lexical and syntactic behavior in a homoge-
neous feature type. This property is summed up
nicely by the desire for features that capture rather
2S, SINV, SQ, SBAR, or SBARQ
ROOT
S
NP VP
VBZ
loves
NP
NP
DT
the
NN
NN
man
NN
woman
Figure 1: A Tree Substitution Grammar capable of de-
scribing the feelings of people of all sexual orientations.
than cover linguistic phenomena (Johnson, 2012);
while features such as character n-grams, POS tag
sequences, and CFG rules may provide a usable L1
signal, each feature is likely covering some compo-
nent of a pattern instead of capturing it in full. TSG
rules, on the other hand, offer remarkable flexibil-
ity in the patterns that they can represent, potentially
capturing any contiguous parse tree structure.
As it is intractable to rank and filter the entire set
of possible TSG rules given a corpus, we start with
the large subset produced by Bayesian grammar in-
duction. The most widely used algorithm for TSG
induction uses a Dirichlet Process to choose a subset
of frequently reoccurring rules by repeatedly sam-
pling derivations for a corpus of parse trees (Cohn
and Blunsom, 2010). The rich get richer dynamic of
the DP leads to the use of a compact set of rules
that is an effective feature set for NLI (Swanson
and Charniak, 2012). However, this same property
makes rare rules harder to find.
To address this weakness, we define a general
model for TSG induction in labeled documents that
combines a Hierarchical Dirichlet Process (Teh et al
2005), with supervised labels in a manner similar to
upstream supervised LDA (Mimno and McCallum,
2008). In the context of our work the document label
? indicates both its authors native language L and
data set D. Each ? is associated with an observed
Dirichlet prior ??, and a hidden multinomial ?? over
grammars is drawn from this prior. The traditional
grammatical model of nonterminal expansion is aug-
mented such that to rewrite a symbol we first choose
a grammar from the document?s ?? and then choose
a rule from that grammar.
For those unfamiliar with these models, the basic
idea is to jointly estimate a mixture distribution over
grammars for each ?, as well as the parameters of
these grammars. The HDP is necessary as the size
87
of each of these grammars is essentially infinite. We
can express the generative model formally by defin-
ing the probability of a rule r expanding a symbol s
in a sentence labeled ? as
?? ? Dir(??)
zi? ?Mult(??)
Hs ? DP (?, P0(?|s))
Gks ? DP (?s, Hs)
ri?s ? Gzi?s
This is closely related to the application of the
Hierarchical Pitman Yor Process used in (Blunsom
and Cohn, 2010) and (Shindo et al 2012), which
interpolates between multiple coarse and fine map-
pings of the data items being clustered to deal with
sparse data. While the underlying Chinese Restau-
rant Process sampling algorithm is quite similar, our
approach differs in that it models several different
distributions with the same support that share a com-
mon prior.
By careful choice of the number of grammars K,
the Dirichlet priors ?, and the backoff concentration
parameter ?, a variety of interesting models can eas-
ily be defined, as demonstrated in our experiments.
5 Feature Selection
5.1 Dataset Independence
The first step in our L1 signal extraction pipeline
controls for patterns that occur too frequently in cer-
tain combinations of native language and data set.
Such patterns arise primarily from the reuse of es-
say prompts in the creation of certain corpora, and
we construct a hard filter to exclude features of this
type.
A simple first choice would be to rank the rules
in order of dependence on the corpus, as we expect
an irregularly represented topic to be confined to a
single data set. However, this misses the subtle but
important point that corpora have different qualities
such as register and author proficiency. Instead we
treat the set of sentences containing an arbitrary fea-
ture X as a set of observations of a pair of categor-
ical random variables L and D, representing native
language and data set respectively.
To see why this treatment is superior, consider the
outcomes for the two hypothetical features shown
L1 L2
D1 1000 500
D2 100 50
L1 L2
D1 1000 500
D2 750 750
Figure 2: Two hypothetical feature profiles that illustrate
the problems with filtering only on data set independence,
which prefers the right profile over the left. Our method
has the opposite preference.
in Figure 2. The left table has a high data set de-
pendence but exhibits a clean twofold preference for
L1 in both data sets, making it a desirable feature to
retain. Conversely, the right table shows a feature
where the distribution is uniform over data sets, but
has language preference in only one. This is a sign
of either a large variance in usage or some data set
specific tendency, and in either case we can not make
confident claims as to this feature?s association with
any native language.
The L-D dependence can be measured with Pear-
son?s ?2 test, although the specifics of its use as
a filter deserve some discussion. As we eliminate
the features for which the null hypothesis of inde-
pendence is rejected, our noisy data will cause us
to overzealously reject. In order to prevent the un-
neccesary removal of interesting patterns, we use a
very small p value as a cutoff point for rejection. In
all of our experiments the ?2 value corresponding to
p < .001 is in the twenties; we use ?2 > 100 as our
criteria for rejection.
Another possible source of error is the sparsity of
some features in our data. To avoid making pre-
dictions of rules for which we have not observed
a sufficient number of examples, we automatically
exclude any rule with a count less than five for any
L-D combination ?. This also satisfies the common
requirements for validity of the ?2 test that require
a minimum number of 5 expected counts for every
outcome.
5.2 Relevancy
We next rank the features in terms of their ability to
discriminate between L1 labels. We consider three
relevancy ranking metrics: Information Gain (IG),
Symmetric Uncertainty (SU), and ?2 statistic.
88
IG SU ?2
r .84 .72 .15
Figure 3: Sample Pearson correlation coefficients be-
tween different ranking functions and feature frequency
over a large set of TSG features.
IG(L,Xi) = H(L)?H(L|Xi)
SU(L,Xi) = 2
IG(L,Xi)
H(L) +H(Xi)
?2(Xi) =
?
m
(nim ?
Ni
M )
2
Ni
M
We define L as the Multinomial distributed L1 la-
bel taking values in {1, ...,M} andXi as a Bernoulli
distributed indicator of the presence or absence of
the ith feature, which we represent with the events
X+i and X
?
i respectively. We use the Maximum
Likelihood estimates of these distributions from the
training data to compute the necessary entropies for
IG and SU. For the ?2 metric we use nim, the count
of sentences with L1 labelm that contain featureXi,
and their sum over classes Ni.
While SU is often preferred over IG in feature se-
lection for several reasons, their main difference in
the context of selection of binary features is the addi-
tion of H(Xi) in the denominator, leading to higher
values for rare features under SU. This helps to
counteract a subtle preference for common features
that these metrics can exhibit in data such as ours, as
shown in Figure 3. The source of this preference is
the overwhelming contribution of p(X?i )H(L|X
?
i )
in IG(L,Xi) for rare features, which will be essen-
tially the maximum value of log(M). In most clas-
sification problems a frequent feature bias is a desir-
able trait, as a rare feature is naturally less likely to
appear and contribute to decision making.
We note that binary features in sentences are
sparsely observed, as the opportunity for use of the
majority of patterns will not exist in any given sen-
tence. This leads to a large number of rare features
that are nevertheless indicative of their author?s L1.
The ?2 statistic we employ is better suited to retain
such features as it only deals with counts of sen-
tences containing Xi.
The ranking behavior of these metrics is high-
lighted in Figure 4. We expect that features with
profiles like Xa and Xb will be more useful than
those like Xd, and only ?2 ranks these features ac-
cordingly. Another view of the difference between
the metrics is taken in Figure 5. As shown in the
left plot, IG and SU are nearly identical for the
most highly ranked features and significantly differ-
ent from ?2.
L1 L2 L3 L4 IG SU ?2
Xa 20 5 5 5 .0008 .0012 19.29
Xb 40 20 20 20 .0005 .0008 12.0
Xc 2000 500 500 500 .0178 .0217 385.7
Xd 1700 1800 1700 1800 .0010 .0010 5.71
Figure 4: Four hypothetical features in a 4 label clas-
sification problem, with the number of training items
from each class using the feature listed in the first four
columns. The top three features under each ranking are
shown in bold.
 0
 10
 20
 30
 40
 50
 0  10  20  30  40  50
# 
of
 sh
ar
ed
 fe
at
ur
es
Top n features
X-IG
X-SU
SU-IG
 0
 50
 100
 150
 200
 250
 300
 0  50  100  150  200  250  300
Top n features
X-IG
X-SU
SU-IG
Figure 5: For all pairs of relevancy metrics, we show the
number of features that appear in the top n of both. The
result for low n is highlighted in the left plot, showing a
high similarity between SU and IG.
5.3 Redundancy
The second component of thorough feature selection
is the removal of redundant features. From an ex-
perimental point of view, it is inaccurate to compare
feature selection systems under evaluation of the top
n features or the number of features with ranking
statistic at or beyond some threshold if redundancy
has not been taken into account. Furthermore, as
our stated goal is a list of discriminative patterns,
multiple representations of the same pattern clearly
89
degrade the quality of our output. This is especially
necessary when using TSG rules as features, as it is
possible to define many slightly different rules that
essentially represent the same linguistic act.
Redundancy detection must be able to both deter-
mine that a set of features are redundant and also
select the feature to retain from such a set. We use
a greedy method that allows us to investigate differ-
ent relevancy metrics for selection of the representa-
tive feature for a redundant set (Yu and Liu, 2004).
The algorithm begins with a list S containing the
full list of features, sorted by an arbitrary metric of
relevancy. While S is not empty, the most relevant
feature X? in S is selected for retention, and all fea-
tures Xi are removed from S if R(X?, Xi) > ? for
some redundancy metric R and some threshold ?.
We consider two probabilistic metrics for redun-
dancy detection, the first being SU, as defined in
the previous section. We contrast this metric with
Normalized Pointwise Mutual Information (NPMI)
which uses only the events A = X+a and B = X
+
b
and has a range of [-1,1].
NPMI(Xa, Xb) =
log(P (A|B))? log(P (A))
? log(P (A,B))
Another option that we explore is the structural
redundancy between TSG rules themselves. We de-
fine a 0-1 redundancy metric such that R(Xa, Xb) is
one if there exists a fragment that contains both Xa
and Xb with a total number of CFG rules less than
the sum of the number of CFG rules in Xa and Xb.
The latter constraint ensures that Xa and Xb overlap
in the containing fragment. Note that this is not the
same as a nonempty set intersection of CFG rules,
as can be seen in Figure 6.
S
NP
NN
VP
S
NP
PRP
VP
S
NP VP
VBZ
Figure 6: Three similar fragments that highlight the be-
havior of the structural redundancy metric; the first two
fragments are not considered redundant, while the third
is made redundant by either of the others.
6 Experiments
6.1 Relevancy Metrics
The traditional evaluation criterion for a feature se-
lection system such as ours is classification accuracy
or expected risk. However, as our desired output is
not a set of features that capture a decision bound-
ary as an ensemble, a per feature risk evaluation bet-
ter quantifies the performance of a system for our
purposes. We plot average risk against number of
predicted features to view the rate of quality degra-
dation under a relevancy measure to give a picture
of a each metric?s utility.
The per feature risk for a feature X is an eval-
uation of the ML estimate of PX(L) = P (L|X+)
from the training data on TX , the test sentences that
contain the feature X . The decision to evaluate only
sentences in which the feature occurs removes an
implicit bias towards more common features.
We calculate the expected risk R(X) using a 0-1
loss function, averaging over TX .
R(X) =
1
|TX |
?
t?TX
PX(L 6= L
?
t )
where L?t is the gold standard L1 label of test item
t. This metric has two important properties. First,
given any true distribution over class labels in TX ,
the best possible PX(L) is the one that matches
these proportions exactly, ensuring that preferred
features make generalizable predictions. Second, it
assigns less risk to rules with lower entropy, as long
as their predictions remain generalizable. This cor-
responds to features that find larger differences in
usage frequency across L1 labels.
The alternative metric of per feature classifica-
tion accuracy creates a one to one mapping between
features and native languages. This unnecessarily
penalizes features that are associated with multiple
native languages, as well as features that are selec-
tively dispreferred by certain L1 speakers. Also, we
wish to correctly quantify the distribution of a fea-
ture over all native languages, which goes beyond
correct prediction of the most probable.
Using cross validation with each corpus as a fold,
we plot the average R(X) for the best n features
against n for each relevancy metric in Figure 7. This
clearly shows that for highly ranked features ?2 is
90
 0.69
 0.7
 0.71
 0.72
 0.73
 0.74
 0.75
 0  20  40  60  80  100  120  140  160  180  200
A
ve
ra
ge
 E
xp
ec
te
d 
Lo
ss
Top n features
X2
IG
SU
Figure 7: Per-feature Average Expected Loss plotted
against top N features using ?2, IG, and SU as a rele-
vancy metric
able to best single out the type of features we de-
sire. Another point to be taken from the plot is
that it is that the top ten features under SU are
remarkably inferior. Inspection of these rules re-
veals that they are precisely the type of overly fre-
quent but only slightly discriminative features that
we predicted would corrupt feature selection using
IG based measures.
6.2 Redundancy Metrics
We evaluate the redundancy metrics by using the top
n features retained by redundancy filtering for en-
semble classification. Under this evaluation, if re-
dundancy is not being effectively eliminated perfor-
mance should increase more slowly with n as the
set of test items that can be correctly classified re-
mains relatively constant. Additionally, if the metric
is overzealous in its elimination of redundancy, use-
ful patterns will be eliminated leading to diminished
increase in performance. Figure 8 shows the tradeoff
between Expected Loss on the test set and the num-
ber of features used with SU, NPMI, and the overlap
based structural redundancy metric described above.
We performed a coarse grid search to find the opti-
mal values of ? for SU and NPMI.
Both the structural overlap hueristic and SU per-
form similarly, and outperform NPMI. Analysis re-
veals that NPMI seems to overstate the similarity of
large fragments with their small subcomponents. We
choose to proceed with SU, as it is not only faster in
our implementation but also can generalize to fea-
ture types beyond TSG rules.
 0.66
 0.67
 0.68
 0.69
 0.7
 0.71
 0.72
 0.73
 0  50  100  150  200  250  300
Ex
pe
ct
ed
 L
os
s
Top n features
overlap
SU
NPMI
Figure 8: The effects of redundancy filtering on classi-
fication performance using different redundancy metrics.
The cutoff values (?) used for SU and NPMI are .2 and .7
respectively.
6.3 TSG Induction
We demonstrate the flexibility and effectiveness of
our general model of mixtures of TSGs for labeled
data by example. The tunable parameters are the
number of grammars K, the Dirichlet priors ?? over
grammar distributions for each label ?, and the con-
centration parameter ? of the smoothing DP.
For a first baseline we set the number of grammars
K = 1, making the Dirichlet priors ? irrelevant.
With a large ? = 1020, we essentially recover the
basic block sampling algorithm of Cohn and Blun-
som (2010). We refer to this model as M1. Our
second baseline model, M2, sets K to the number of
native language labels, and sets the ? variables such
that each ? is mapped to a single grammar by its L1
label, creating a naive Bayes model. For M2 and
the subsequent models we use ? = 1000 to allow
moderate smoothing.
We also construct a model (M3) in which we set
K = 9 and ?? is such that three grammars are likely
for any single ?; one shared by all ? with the same
L1 label, one shared by all ? with the same corpus
label, and one shared by all ?. We compare this with
another K = 9 model (M4) where the ? are set to
be uniform across all 9 grammars.
We evaluate these systems on the percent of their
resulting grammar that rejects the hypothesis of lan-
guage independence using a ?2 test. Slight adjust-
ments were made to ? for these models to bring
their output grammar size into the range of approxi-
mately 12000 rules. We average our results for each
model over single states drawn from five indepen-
91
p < .1 p < .05 p < .01 p < .001
M1 56.5(3.1) 54.5(3.0) 49.8(2.7) 45.1(2.5)
M2 55.3(3.7) 53.7(3.6) 49.1(3.3) 44.7(3.0)
M3 59.0(4.1) 57.2(4.1) 52.4(3.6) 48.4(3.3)
M4 58.9(3.8) 57.0(3.7) 51.9(3.4) 47.2(3.1)
Figure 9: The percentage of rules from each model that
reject L1 independence at varying levels of statistical sig-
nificance. The first number is with respect to the number
rules that pass the L1/corpus independence and redun-
dancy tests, and the second is in proportion to the full list
returned by grammar induction.
dent Markov chains.
Our results in Figure 9 show that using a mixture
of grammars allows the induction algorithm to find
more patterns that fit arbitrary criteria for language
dependence. The intuition supporting this is that in
simpler models a given grammar must represent a
larger amount of data that is better represented with
more vague, general purpose rules. Dividing the re-
sponsibility among several grammars lets rare pat-
terns form clusters more easily. The incorporation of
informed structure in M3 further improves the per-
formance of this latent mixture technique.
7 Discussion
Using these methods, we produce a list of L1 as-
sociated TSG rules that we release for public use.
We perform grammar induction using model M3,
apply our data dependence and redundancy filters,
rank for relevancy using ?2 and filter at the level of
p < .1 statistical significance for relevancy. Each
entry consists of a TSG rule and its matrix of counts
with each ?. We provide the total for each L1 la-
bel, which shows the overall prediction of the pro-
portional use of that item. We also provide the ?2
statistics for L1 dependence and the dependence of
L1 and corpus.
It is speculative to assign causes to the discrimi-
native rules we report, and we leave quantification
of such statements to future work. However, the
strength of the signal, as evidenced by actual counts
in data, and the high level interpretation that can be
easily assigned to the TSG rules is promising. As
understanding the features requires basic knowledge
of Treebank symbols, we provide our interpretations
for some of the more interesting rules and summa-
rize their L1 distributions. Note that by describing a
rule as being preferred by a certain set of L1 labels,
our claim is relative to the other labels only; the true
cause could also be a dispreference in the comple-
ment of this set.
One interesting comparison made easy by our
method is the identification of similar structures that
have complementary L1 usage. An example is the
use of a prepositional phrase just before the first
noun phrase in a sentence, which is preferred in Ger-
man and Spanish, especially in the former. However,
German speakers disprefer a prepositional phrase
followed by a comma at the beginning of the sen-
tence, and Chinese speakers use this pattern more
frequently than the other L1s. Another contrastable
pair is the use of the word ?because? with upper or
lower case, signifying sentence initial or medial use.
The former is preferred in Chinese and Japanese
text, while the latter is preferred in German and even
more so in Spanish L1 data.
As these examples suggest, the data shows a
strong division of preference between European
and Asian languages, but many patterns exist that
are uniquely preferred in single languages as well.
Japanese speakers are seen to frequently use a per-
sonal pronoun as the subject of the sentence, while
Spanish speakers use the phrase ?the X of Y?, the
verb ?go?, and the determiner ?this? with markedly
higher frequency. Germans tend to begin sentences
with adverbs, and various modal verb constructions
are popular with Chinese speakers. We suspect these
patterns to be evidence of preference in the speci-
fied language, rather than dispreference in the other
three.
Our strategy in regard to the hard filters for L1-
corpus dependence and redundancy has been to pre-
fer recall to precision, as false positives can be easily
ignored through subsequent inspection of the data
we supply. This makes the list suitable for human
qualitative analysis, but further work is required for
its use in downstream automatic systems.
8 Conclusion
This work contributes to the goal of leveraging NLI
data in SLA applications. We provide evidence for
92
our hypothesis that relevancy metrics based on mu-
tual information are ill-suited for this task, and rec-
ommend the use of the ?2 statistic for rejecting the
hypothesis of language independence. Explicit con-
trols for dependence between L1 and corpus are
proposed, and redundancy between features are ad-
dressed as well. We argue for the use of TSG rules as
features, and develop an induction algorithm that is
a supervised mixture of hierarchical grammars. This
generalizable formalism is used to capture linguistic
assumptions about the data and increase the amount
of relevant features extracted at several thresholds.
This project motivates continued incorporation of
more data and induction of TSGs over these larger
data sets. This will improve the quality and scope of
the resulting list of discriminative syntax, allowing
broader use in linguistics and SLA research. The
prospect of high precision and recall in the extrac-
tion of such patterns suggests several interesting av-
enues for future work, such as determination of the
actual language transfer phenomena evidenced by an
arbitrary count profile. To achieve the goal of auto-
matic detection of plausible transfer the native lan-
guages themselves must be considered, as well as a
way to distinguish between preference and dispref-
erence based on usage statistics. Another exciting
application of such a refined list of patterns is the
automatic integration of its features in L1 targeted
SLA software.
References
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
Induction of Tree Substitution Grammars for Depen-
dency Parsing. Empirical Methods in Natural Lan-
guage Processing.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ?cheap? learner corpora. Conference of
Learner Corpus Research.
Julian Brooke and Graeme Hirst. 2012. Measuring In-
terlanguage: Native Language Identification with L1-
influence Metrics. LREC
Julian Brooke and Graeme Hirst. 2012. Robust, Lexical-
ized Native Language Identification. COLING.
Serhiy Bykh and Detmar Meurers. 2012. Native Lan-
guage Identification Using Recurring N-grams - Inves-
tigating Abstraction and Domain Dependence. COL-
ING.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing Compact but Accurate Tree-
Substitution Grammars. In Proceedings NAACL.
Trevor Cohn, and Phil Blunsom. 2010. Blocked infer-
ence in Bayesian tree substitution grammars. Associa-
tion for Computational Linguistics.
Gilquin, Gae?tanelle and Granger, Sylviane. 2011. From
EFL to ESL: Evidence from the International Corpus
of Learner English. Exploring Second-Language Va-
rieties of English and Learner Englishes: Bridging a
Paradigm Gap (Book Chapter).
Joshua Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. In Bod et al chapter 8..
S. Granger, E. Dagneaux and F. Meunier. 2002. Interna-
tional Corpus of Learner English, (ICLE).
Horst M., White J., Bell P. 2010. First and second lan-
guage knowledge in the language classroom. Interna-
tional Journal of Bilingualism.
Scott Jarvis and Scott Crossley 2012. Approaching Lan-
guage Transfer through Text Classification.
Mark Johnson 2011. How relevant is linguistics to com-
putational linguistics?. Linguistic Issues in Language
Technology.
Ekaterina Kochmar. 2011. Identification of a writer?s
native language by error analysis. Master?s Thesis.
Koppel, Moshe and Schler, Jonathan and Zigdon, Kfir.
2005. Determining an author?s native language by
mining a text for errors. Proceedings of the eleventh
ACM SIGKDD international conference on Knowl-
edge discovery in data mining.
Laufer, B and Girsai, N. 2008. Form-focused Instruction
in Second Language Vocabulary Learning: A Case for
Contrastive Analysis and Translation. Applied Lin-
guistics.
David Mimno and Andrew McCallum. 2008. Topic
Models Conditioned on Arbitrary Features with
Dirichlet-multinomial Regression. UAI.
Hanchuan Peng and Fuhui Long and Chris Ding. 2005.
Feature selection based on mutual information cri-
teria of max-dependency, max-relevance, and min-
redundancy. IEEE Transactions on Pattern Analysis
and Machine Intelligence.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. Association for Compu-
tational Linguistics.
Matt Post and Daniel Gildea. 2009. Bayesian Learning
of a Tree Substitution Grammar. Association for Com-
putational Linguistics.
Tono, Y., Kawaguchi, Y. & Minegishi, M. (eds.) . 2012.
Developmental and Cross-linguistic Perspectives in
Learner Corpus Research..
Oren Tsur and Ari Rappoport. 2007. Using classifier
features for studying the effect of native language on
the choice of written second language words. CACLA.
93
Shindo, Hiroyuki and Miyao, Yusuke and Fujino, Aki-
nori and Nagata, Masaaki 2012. Bayesian Symbol-
Refined Tree Substitution Grammars for Syntactic
Parsing. Association for Computational Linguistics.
Ben Swanson and Eugene Charniak. 2012. Native
Language Detection with Tree Substitution Grammars.
Association for Computational Linguistics.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2005. Hierarchical Dirichlet Pro-
cesses. Journal of the American Statistical Associa-
tion.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, Beata
Beigman-Klebanov and Martin Chodorow. 2012. Na-
tive Tongues, Lost and Found: Resources and Em-
pirical Evaluations in Native Language Identification.
COLING.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
analysis and native language identification. Proceed-
ings of the Australasian Language Technology Associ-
ation Workshop.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing Parse Structures for Native Language Identifica-
tion. Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing.
Sze-Meng Jojo Wong and Mark Dras. 2011. Topic Mod-
eling for Native Language Identification. Proceedings
of the Australasian Language Technology Association
Workshop.
Sze-Meng Jojo Wong, Mark Dras, Mark Johnson. 2012.
Exploring Adaptor Grammars for Native Language
Identification. EMNLP-CoNLL.
Lei Yu and Huan Liu. 2004. Efficient Feature Selection
via Analysis of Relevance and Redundancy. Journal
of Machine Learning Research.
94
Proceedings of the ACL 2010 Conference Short Papers, pages 33?37,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
The Same-head Heuristic for Coreference
Micha Elsner and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{melsner,ec}@cs.brown.edu
Abstract
We investigate coreference relationships
between NPs with the same head noun.
It is relatively common in unsupervised
work to assume that such pairs are
coreferent? but this is not always true, es-
pecially if realistic mention detection is
used. We describe the distribution of non-
coreferent same-head pairs in news text,
and present an unsupervised generative
model which learns not to link some same-
head NPs using syntactic features, improv-
ing precision.
1 Introduction
Full NP coreference, the task of discovering which
non-pronominal NPs in a discourse refer to the
same entity, is widely known to be challenging.
In practice, however, most work focuses on the
subtask of linking NPs with different head words.
Decisions involving NPs with the same head word
have not attracted nearly as much attention, and
many systems, especially unsupervised ones, op-
erate under the assumption that all same-head
pairs corefer. This is by no means always the case?
there are several systematic exceptions to the rule.
In this paper, we show that these exceptions are
fairly common, and describe an unsupervised sys-
tem which learns to distinguish them from coref-
erent same-head pairs.
There are several reasons why relatively little
attention has been paid to same-head pairs. Pri-
marily, this is because they are a comparatively
easy subtask in a notoriously difficult area; Stoy-
anov et al (2009) shows that, among NPs headed
by common nouns, those which have an exact
match earlier in the document are the easiest to
resolve (variant MUC score .82 on MUC-6) and
while those with partial matches are quite a bit
harder (.53), by far the worst performance is on
those without any match at all (.27). This effect
is magnified by most popular metrics for coref-
erence, which reward finding links within large
clusters more than they punish proposing spu-
rious links, making it hard to improve perfor-
mance by linking conservatively. Systems that
use gold mention boundaries (the locations of NPs
marked by annotators)1 have even less need to
worry about same-head relationships, since most
NPs which disobey the conventional assumption
are not marked as mentions.
In this paper, we count how often same-head
pairs fail to corefer in the MUC-6 corpus, show-
ing that gold mention detection hides most such
pairs, but more realistic detection finds large num-
bers. We also present an unsupervised genera-
tive model which learns to make certain same-
head pairs non-coreferent. The model is based
on the idea that pronoun referents are likely to
be salient noun phrases in the discourse, so we
can learn about NP antecedents using pronom-
inal antecedents as a starting point. Pronoun
anaphora, in turn, is learnable from raw data
(Cherry and Bergsma, 2005; Charniak and Elsner,
2009). Since our model links fewer NPs than the
baseline, it improves precision but decreases re-
call. This tradeoff is favorable for CEAF, but not
for b3.
2 Related work
Unsupervised systems specify the assumption of
same-head coreference in several ways: by as-
1Gold mention detection means something slightly differ-
ent in the ACE corpus, where the system input contains every
NP annotated with an entity type.
33
sumption (Haghighi and Klein, 2009), using
a head-prediction clause (Poon and Domingos,
2008), and using a sparse Dirichlet prior on word
emissions (Haghighi and Klein, 2007). (These
three systems, perhaps not coincidentally, use gold
mentions.) An exception is Ng (2008), who points
out that head identity is not an entirely reliable cue
and instead uses exact string match (minus deter-
miners) for common NPs and an alias detection
system for proper NPs. This work uses mentions
extracted with an NP chunker. No specific results
are reported for same-head NPs. However, while
using exact string match raises precision, many
non-matching phrases are still coreferent, so this
approach cannot be considered a full solution to
the problem.
Supervised systems do better on the task, but
not perfectly. Recent work (Stoyanov et al, 2009)
attempts to determine the contributions of various
categories of NP to coreference scores, and shows
(as stated above) that common NPs which partially
match an earlier mention are not well resolved by
the state-of-the-art RECONCILE system, which
uses pairwise classification. They also show that
using gold mention boundaries makes the corefer-
ence task substantially easier, and argue that this
experimental setting is ?rather unrealistic?.
3 Descriptive study: MUC-6
We begin by examining how often non-same-head
pairs appear in the MUC-6 coreference dataset.
To do so, we compare two artificial coreference
systems: the link-all strategy links all, and only,
full (non-pronominal) NP pairs with the same head
which occur within 10 sentences of one another.
The oracle strategy links NP pairs with the same
head which occur within 10 sentences, but only if
they are actually coreferent (according to the gold
annotation)2 The link-all system, in other words,
does what most existing unsupervised systems do
on the same-head subset of NPs, while the oracle
system performs perfectly.
We compare our results to the gold standard us-
ing two metrics. b3(Bagga and Baldwin, 1998)
is a standard metric which calculates a precision
and recall for each mention. The mention CEAF
(Luo, 2005) constructs a maximum-weight bipar-
2The choice of 10 sentences as the window size captures
most, but not all, of the available recall. Using nouns mention
detection, it misses 117 possible same-head links, or about
10%. However, precision drops further as the window size
increases.
tite matching between gold and proposed clusters,
then gives the percentage of entities whose gold
label and proposed label match. b3 gives more
weight to errors involving larger clusters (since
these lower scores for several mentions at once);
for mention CEAF, all mentions are weighted
equally.
We annotate the data with the self-trained Char-
niak parser (McClosky et al, 2006), then extract
mentions using three different methods. The gold
mentions method takes only mentions marked by
annotators. The nps method takes all base noun
phrases detected by the parser. Finally, the nouns
method takes all nouns, even those that do not
head NPs; this method maximizes recall, since it
does not exclude prenominals in phrases like ?a
Bush spokesman?. (High-precision models of the
internal structure of flat Penn Treebank-style NPs
were investigated by Vadas and Curran (2007).)
For each experimental setting, we show the num-
ber of mentions detected, and how many of them
are linked to some antecedent by the system.
The data is shown in Table 1. b3 shows a large
drop in precision when all same-head pairs are
linked; in fact, in the nps and nouns settings, only
about half the same-headed NPs are actually coref-
erent (864 real links, 1592 pairs for nps). This
demonstrates that non-coreferent same-head pairs
not only occur, but are actually rather common in
the dataset. The drop in precision is much less
obvious in the gold mentions setting, however;
most unlinked same-head pairs are not annotated
as mentions in the gold data, which is one reason
why systems run in this experimental setting can
afford to ignore them.
Improperly linking same-head pairs causes a
loss in precision, but scores are dominated by re-
call3. Thus, reporting b3 helps to mask the impact
of these pairs when examining the final f-score.
We roughly characterize what sort of same-
headed NPs are non-coreferent by hand-
examining 100 randomly selected pairs. 39
pairs denoted different entities (?recent employ-
ees? vs ?employees who have worked for longer?)
disambiguated by modifiers or sometimes by
discourse position. The next largest group (24)
consists of time and measure phrases like ?ten
miles?. 12 pairs refer to parts or quantities
3This bias is exaggerated for systems which only link
same-head pairs, but continues to apply to real systems; for
instance (Haghighi and Klein, 2009) has a b3 precision of 84
and recall of 67.
34
Mentions Linked b3 pr rec F mention CEAF
Gold mentions
Oracle 1929 1164 100 32.3 48.8 54.4
Link all 1929 1182 80.6 31.7 45.5 53.8
Alignment 1929 495 93.7 22.1 35.8 40.5
NPs
Oracle 3993 864 100 30.6 46.9 73.4
Link all 3993 1592 67.2 29.5 41.0 62.2
Alignment 3993 518 87.2 24.7 38.5 67.0
Nouns
Oracle 5435 1127 100 41.5 58.6 83.5
Link all 5435 2541 56.6 40.9 45.7 67.0
Alignment 5435 935 83.0 32.8 47.1 74.4
Table 1: Oracle, system and baseline scores on MUC-6 test data. Gold mentions leave little room
for improvement between baseline and oracle; detecting more mentions widens the gap between
them. With realistic mention detection, precision and CEAF scores improve over baselines, while recall
and f-scores drop.
(?members of...?), and 12 contained a generic
(?In a corporate campaign, a union tries...?). 9
contained an annotator error. The remaining 4
were mistakes involving proper noun phrases
headed by Inc. and other abbreviations; this case
is easy to handle, but apparently not the primary
cause of errors.
4 System
Our system is a version of the popular IBM model
2 for machine translation. To define our generative
model, we assume that the parse trees for the en-
tire document D are given, except for the subtrees
with root nonterminal NP, denoted ni, which our
system will generate. These subtrees are related
by a hidden set of alignments, ai, which link each
NP to another NP (which we call a generator) ap-
pearing somewhere before it in the document, or
to a null antecedent. The set of potential genera-
tors G (which plays the same role as the source-
language text in MT) is taken to be all the NPs
occurring within 10 sentences of the target, plus a
special null antecedent which plays the same role
as the null word in machine translation? it serves
as a dummy generator for NPs which are unrelated
to any real NP in G.
The generative process fills in all the NP nodes
in order, from left to right. This process ensures
that, when generating node ni, we have already
filled in all the NPs in the set G (since these all
precede ni). When deciding on a generator for
NP ni, we can extract features characterizing its
relationship to a potential generator gj . These fea-
tures, which we denote f(ni, gj , D), may depend
on their relative position in the document D, and
on any features of gj , since we have already gener-
ated its tree. However, we cannot extract features
from the subtree under ni, since we have yet to
generate it!
As usual for IBM models, we learn using EM,
and we need to start our alignment function off
with a good initial set of parameters. Since an-
tecedents of NPs and pronouns (both salient NPs)
often occur in similar syntactic environments, we
use an alignment function for pronoun corefer-
ence as a starting point. This alignment can be
learned from raw data, making our approach un-
supervised.
We take the pronoun model of Charniak and El-
sner (2009)4 as our starting point. We re-express
it in the IBM framework, using a log-linear model
for our alignment. Then our alignment (parame-
terized by feature weights w) is:
p(ai = j|G,D) ? exp(f(ni, gj , D) ? w)
The weights w are learned by gradient descent
on the log-likelihood. To use this model within
EM, we alternate an E-step where we calculate
the expected alignments E[ai = j], then an M-
step where we run gradient descent. (We have also
had some success with stepwise EM as in (Liang
and Klein, 2009), but this requires some tuning to
work properly.)
4Downloaded from http://bllip.cs.brown.edu.
35
As features, we take the same features as Char-
niak and Elsner (2009): sentence and word-count
distance between ni and gj , sentence position of
each, syntactic role of each, and head type of gj
(proper, common or pronoun). We add binary fea-
tures for the nonterminal directly over gj (NP, VP,
PP, any S type, or other), the type of phrases mod-
ifying gj (proper nouns, phrasals (except QP and
PP), QP, PP-of, PP-other, other modifiers, or noth-
ing), and the type of determiner of gj (possessive,
definite, indefinite, deictic, other, or nothing). We
designed this feature set to distinguish prominent
NPs in the discourse, and also to be able to detect
abstract or partitive phrases by examining modi-
fiers and determiners.
To produce full NPs and learn same-head coref-
erence, we focus on learning a good alignment
using the pronoun model as a starting point. For
translation, we use a trivial model, p(ni|gai) = 1
if the two have the same head, and 0 otherwise,
except for the null antecedent, which draws heads
from a multinomial distribution over words.
While we could learn an alignment and then
treat all generators as antecedents, so that only
NPs aligned to the null antecedent were not la-
beled coreferent, in practice this model would
align nearly all the same-head pairs. This is
true because many words are ?bursty?; the prob-
ability of a second occurrence given the first is
higher than the a priori probability of occurrence
(Church, 2000). Therefore, our model is actually a
mixture of two IBM models, pC and pN , where pC
produces NPs with antecedents and pN produces
pairs that share a head, but are not coreferent. To
break the symmetry, we allow pC to use any pa-
rameters w, while pN uses a uniform alignment,
w ? ~0. We interpolate between these two models
with a constant ?, the single manually set parame-
ter of our system, which we fixed at .9.
The full model, therefore, is:
p(ni|G,D) =?pT (ni|G,D)
+ (1? ?)pN (ni|G,D)
pT (ni|G,D) =
1
Z
?
j?G
exp(f(ni, gj , D) ? w)
? I{head(ni) = head(j)}
pT (ni|G,D) =
?
j?G
1
|G|
I{head(ni) = head(gj)}
NPs for which the maximum-likelihood gener-
ator (the largest term in either of the sums) is from
pT and is not the null antecedent are marked as
coreferent to the generator. Other NPs are marked
not coreferent.
5 Results
Our results on the MUC-6 formal test set are
shown in Table 1. In all experimental settings,
the model improves precision over the baseline
while decreasing recall? that is, it misses some le-
gitimate coreferent pairs while correctly exclud-
ing many of the spurious ones. Because of the
precision-recall tradeoff at which the systems op-
erate, this results in reduced b3 and link F. How-
ever, for the nps and nouns settings, where the
parser is responsible for finding mentions, the
tradeoff is positive for the CEAF metrics. For in-
stance, in the nps setting, it improves over baseline
by 57%.
As expected, the model does poorly in the gold
mentions setting, doing worse than baseline on
both metrics. Although it is possible to get very
high precision in this setting, the model is far too
conservative, linking less than half of the available
mentions to anything, when in fact about 60% of
them are coreferent. As we explain above, this ex-
perimental setting makes it mostly unnecessary to
worry about non-coreferent same-head pairs be-
cause the MUC-6 annotators don?t often mark
them.
6 Conclusions
While same-head pairs are easier to resolve than
same-other pairs, they are still non-trivial and de-
serve further attention in coreference research. To
effectively measure their effect on performance,
researchers should report multiple metrics, since
under b3 the link-all heuristic is extremely diffi-
cult to beat. It is also important to report results
using a realistic mention detector as well as gold
mentions.
Acknowledgements
We thank Jean Carletta for the SWITCHBOARD
annotations, and Dan Jurafsky and eight anony-
mous reviewers for their comments and sugges-
tions. This work was funded by a Google graduate
fellowship.
36
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In LREC Workshop on
Linguistics Coreference, pages 563?566.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
EACL, Athens, Greece.
Colin Cherry and Shane Bergsma. 2005. An Expecta-
tion Maximization approach to pronoun resolution.
In Proceedings of CoNLL, pages 88?95, Ann Arbor,
Michigan.
Kenneth W. Church. 2000. Empirical estimates of
adaptation: the chance of two Noriegas is closer to
p/2 than p2. In Proceedings of ACL, pages 180?186.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In Proceedings of ACL, pages 848?855.
Aria Haghighi and Dan Klein. 2009. Simple corefer-
ence resolution with rich syntactic and semantic fea-
tures. In Proceedings of EMNLP, pages 1152?1161.
Percy Liang and Dan Klein. 2009. Online EM for un-
supervised models. In HLT-NAACL.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of HLT-EMNLP,
pages 25?32, Morristown, NJ, USA. Association for
Computational Linguistics.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of HLT-NAACL, pages 152?159.
Vincent Ng. 2008. Unsupervised models for corefer-
ence resolution. In Proceedings of EMNLP, pages
640?649, Honolulu, Hawaii. Association for Com-
putational Linguistics.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov Logic.
In Proceedings of EMNLP, pages 650?659, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of ACL-IJCNLP, pages
656?664, Suntec, Singapore, August. Association
for Computational Linguistics.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the penn treebank. In Proceed-
ings of ACL, pages 240?247, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
37
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1179?1189,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Disentangling Chat with Local Coherence Models
Micha Elsner
School of Informatics
University of Edinburgh
melsner0@gmail.com
Eugene Charniak
Department of Computer Science
Brown University, Providence, RI 02912
ec@cs.brown.edu
Abstract
We evaluate several popular models of local
discourse coherence for domain and task gen-
erality by applying them to chat disentangle-
ment. Using experiments on synthetic multi-
party conversations, we show that most mod-
els transfer well from text to dialogue. Co-
herence models improve results overall when
good parses and topic models are available,
and on a constrained task for real chat data.
1 Introduction
One property of a well-written document is coher-
ence, the way each sentence ts into its context? sen-
tences should be interpretable in light of what has
come before, and in turn make it possible to inter-
pret what comes after. Models of coherence have
primarily been used for text-based generation tasks:
ordering units of text for multidocument summariza-
tion or inserting new text into an existing article.
In general, the corpora used consist of informative
writing, and the tasks used for evaluation consider
different ways of reordering the same set of textual
units. But the theoretical concept of coherence goes
beyond both this domain and this task setting? and
so should coherence models.
This paper evaluates a variety of local coher-
ence models on the task of chat disentanglement or
?threading?: separating a transcript of a multiparty
interaction into independent conversations
1
. Such
simultaneous conversations occur in internet chat
1
A public implementation is available via https://
bitbucket.org/melsner/browncoherence.
rooms, and on shared voice channels such as push-
to-talk radio. In these situations, a single, correctly
disentangled, conversational thread will be coherent,
since the speakers involved understand the normal
rules of discourse, but the transcript as a whole will
not be. Thus, a good model of coherence should be
able to disentangle sentences as well as order them.
There are several differences between disentan-
glement and the newswire sentence-ordering tasks
typically used to evaluate coherence models. Inter-
net chat comes from a different domain, one where
topics vary widely and no reliable syntactic annota-
tions are available. The disentanglement task mea-
sures different capabilities of a model, since it com-
pares documents that are not permuted versions of
one another. Finally, full disentanglement requires
a large-scale search, which is computationally dif-
cult. We move toward disentanglement in stages,
carrying out a series of experiments to measure the
contribution of each of these factors.
As an intermediary between newswire and inter-
net chat, we adopt the SWITCHBOARD (SWBD) cor-
pus. SWBD contains recorded telephone conversa-
tions with known topics and hand-annotated parse
trees; this allows us to control for the performance
of our parser and other informational resources. To
compare the two algorithmic settings, we use SWBD
for ordering experiments, and also articially entan-
gle pairs of telephone dialogues to create synthetic
transcripts which we can disentangle. Finally, we
present results on actual internet chat corpora.
On synthetic SWBD transcripts, local coherence
models improve performance considerably over our
baseline model, Elsner and Charniak (2008b). On
1179
internet chat, we continue to do better on a con-
strained disentanglement task, though so far, we are
unable to apply these improvements to the full task.
We suspect that, with better low-level annotation
tools for the chat domain and a good way of integrat-
ing prior information, our improvements on SWBD
could transfer fully to IRC chat.
2 Related work
There is extensive previous work on coherence mod-
els for text ordering; we describe several specic
models below, in section 2. This study focuses on
models of local coherence, which relate text to its
immediate context. There has also been work on
global coherence, the structure of a document as a
whole (Chen et al, 2009; Eisenstein and Barzilay,
2008; Barzilay and Lee, 2004), typically modeled
in terms of sequential topics. We avoid using them
here, because we do not believe topic sequences are
predictable in conversation and because such models
tend to be algorithmically cumbersome.
In addition to text ordering, local coherence mod-
els have also been used to score the uency of texts
written by humans or produced by machine (Pitler
and Nenkova, 2008; Lapata, 2006; Miltsakaki and
Kukich, 2004). Like disentanglement, these tasks
provide an algorithmic setting that differs from or-
dering, and so can demonstrate previously unknown
weaknesses in models. However, the target genre is
still informative writing, so they reveal little about
cross-domain exibility.
The task of disentanglement or ?threading? for
internet chat was introduced by Shen et al (2006).
Elsner and Charniak (2008b) created the publicly
available #LINUX corpus; the best published re-
sults on this corpus are those of Wang and Oard
(2009). These two studies use overlapping unigrams
to measure similarity between two sentences; Wang
and Oard (2009) use a message expansion tech-
nique to incorporate context beyond a single sen-
tence. Unigram overlaps are used to model coher-
ence, but more sophisticated methods using syntax
(Lapata and Barzilay, 2005) or lexical features (La-
pata, 2003) often outperform them on ordering tasks.
This study compares several of these methods with
Elsner and Charniak (2008b), which we use as a
baseline because there is a publicly available imple-
mentation
2
.
Adams (2008) also created and released a disen-
tanglement corpus. They use LDA (Blei et al, 2001)
to discover latent topics in their corpus, then measur-
ing similarity by looking for shared topics. These
features fail to improve their performance, which is
puzzling in light of the success of topic modeling for
other coherence and segmentation problems (Eisen-
stein and Barzilay, 2008; Foltz et al, 1998). The
results of this study suggest that topic models can
help with disentanglement, but that it is difcult to
nd useful topics for IRC chat.
A few studies have attempted to disentangle con-
versational speech (Aoki et al, 2003; Aoki et al,
2006), mostly using temporal features. For the most
part, however, this research has focused on auditory
processing in the context of the cocktail party prob-
lem, the task of attending to a specic speaker in
a noisy room (Haykin and Chen, 2005). Utterance
content has some inuence on what the listener per-
ceives, but only for extremely salient cues such as
the listener's name (Moray, 1959), so cocktail party
research does not typically use lexical models.
3 Models
In this section, we briey describe the models we in-
tend to evaluate. Most of them are drawn from pre-
vious work; one, the topical entity grid, is a novel
extension of the entity grid. For the experiments be-
low, we train the models on SWBD, sometimes aug-
mented with a larger set of automatically parsed con-
versations from the FISHER corpus. Since the two
corpora are quite similar, FISHER is a useful source
for extra data; McClosky et al (2010) uses it for
this purpose in parsing experiments. (We continue
to use SWBD/FISHER even for experiments on IRC,
because we do not have enough disentangled train-
ing data to learn lexical relationships.)
3.1 Entity grid
The entity grid (Lapata and Barzilay, 2005; Barzilay
and Lapata, 2005) is an attempt to model some prin-
ciples of Centering Theory (Grosz et al, 1995) in a
statistical manner. It represents a document in terms
of entities and their syntactic roles: subject (S), ob-
ject (O), other (X) and not present (-). In each new
2
cs.brown.edu/
?
melsner
1180
utterance, the grid predicts the role in which each
entity will appear, given its history of roles in the
previous sentences, plus a salience feature counting
the total number of times the entity occurs. For in-
stance, for an entity which is the subject of sentence
1, the object of sentence 2, and occurs four times in
total, the grid predicts its role in sentence 3 accord-
ing to the conditional P (jS;O; sal = 4).
As in previous work, we treat each noun in a doc-
ument as denoting a single entity, rather than using
a coreference technique to attempt to resolve them.
In our development experiments, we noticed that
coreferent nouns often occur farther apart in conver-
sation than in newswire, since they are frequently
referred to by pronouns and deictics in the interim.
Therefore, we extend the history to six previous ut-
terances. For robustness with this long history, we
model the conditional probabilities using multilabel
logistic regression rather than maximum likelihood.
This requires the assumption of a linear model, but
makes the estimator less vulnerable to overtting
due to sparsity, increasing performance by about 2%
in development experiments.
3.2 Topical entity grid
This model is a variant of the generative entity
grid, intended to take into account topical informa-
tion. To create the topical entity grid, we learn a
set of topic-to-word distributions for our corpus us-
ing LDA (Blei et al, 2001)
3
with 200 latent top-
ics. This model embeds our vocabulary in a low-
dimensional space: we represent each word w as
the vector of topic probabilities p(t
i
jw). We ex-
perimented with several ways to measure relation-
ships between words in this space, starting with the
standard cosine. However, the cosine can depend on
small variations in probability (for instance, if w has
most of its mass in dimension 1, then it is sensitive
to the exact weight of v for topic 1, even if this es-
sentially never happens).
To control for this tendency, we instead use the
magnitude of the dimension of greatest similarity:
sim(w; v) = max
i
min(w
i
; v
i
)
Tomodel coherence, we generalize the binary his-
3
www.cs.princeton.edu/
?
blei/
topicmodeling.html
tory features of the standard entity grid, which de-
tect, for example, whether entity e is the subject of
the previous sentence. In the topical entity grid, we
instead compute a real-valued feature which sums
up the similarity between entity e and the subject(s)
of the previous sentence.
These features can detect a transition like: ?The
House voted yesterday. The Senate will consider the
bill today.?. If ?House? and ?Senate? have a high
similarity, then the feature will have a high value,
predicting that ?Senate? is a good subject for the cur-
rent sentence. As in the previous section, we learn
the conditional probabilities with logistic regression;
we train in parallel by splitting the data and averag-
ing (Mann et al, 2009). The topics are trained on
FISHER, and on NANC for news.
3.3 IBM-1
The IBM translation model was rst considered for
coherence by Soricut and Marcu (2006), although a
less probabilistically elegant version was proposed
earlier (Lapata, 2003). This model attempts to gen-
erate the content words of the next sentence by trans-
lating them from the words of the previous sentence,
plus a null word; thus, it will learn alignments be-
tween pairs of words that tend to occur in adjacent
sentences. We learn parameters on the FISHER cor-
pus, and on NANC for news.
3.4 Pronouns
The use of a generative pronoun resolver for co-
herence modeling originates in Elsner and Char-
niak (2008a). That paper used a supervised model
(Ge et al, 1998), but we adapt a newer, unsuper-
vised model which they also make publicly available
(Charniak and Elsner, 2009)
4
. They model each pro-
noun as generated by an antecedent somewhere in
the previous two sentences. If a good antecedent is
found, the probability of the pronoun's occurrence
will be high; otherwise, the probability is low, sig-
naling that the text is less coherent because the pro-
noun is hard to interpret correctly.
We use the model as distributed for news text. For
conversation, we adapt it by running a few iterations
of their EM training algorithm on the FISHER data.
4
bllip.cs.brown.edu/resources.shtml\
#software
1181
3.5 Discourse-newness
Building on work from summarization (Nenkova
and McKeown, 2003) and coreference resolution
(Poesio et al, 2005), Elsner and Charniak (2008a)
use a model which recognizes discourse-new versus
old NPs as a coherence model. For instance, the
model can learn that ?President Barack Obama? is
a more likely rst reference than ?Obama?. Follow-
ing their work, we score discourse-newness with a
maximum-entropy classier using syntactic features
counting different types of NP modiers, and we use
NP head identity as a proxy for coreference.
3.6 Chat-specic features
Most disentanglement models use non-linguistic in-
formation alongside lexical features; in fact, times-
tamps and speaker identities are usually better cues
than words are. We capture three essential non-
linguistic features using simple generative models.
The rst feature is the time gap between one utter-
ance and the next within the same thread. Consistent
short gaps are a sign of normal turn-taking behavior;
long pauses do occur, but much more rarely (Aoki et
al., 2003). We round all time gaps to the nearest sec-
ond and model the distribution of time gaps using a
histogram, choosing bucket sizes adaptively so that
each bucket contains at least four datapoints.
The second feature is speaker identity; conver-
sations usually involve a small subset of the to-
tal number of speakers, and a few core speakers
make most of the utterances. We model the distri-
bution of speakers in each conversation using a Chi-
nese Restaurant Process (CRP) (Aldous, 1985) (tun-
ing the dispersion  to maximize development pe-
formance). The CRP's ?rich-get-richer? dynamics
capture our intuitions, favoring conversations domi-
nated by a few vociferous speakers.
Finally, we model name mentioning. Speakers
in IRC chat often use their addressee's names to co-
ordinate the chat (O'Neill and Martin, 2003), and
this is a powerful source of information (Elsner and
Charniak, 2008b). Our model classies each utter-
ance into either the start or continuation of a conver-
sational turn, by checking if the previous utterance
had the same speaker. Given this status, it computes
probabilities for three outcomes: no name mention,
a mention of someone who has previously spoken
in the conversation, or a mention of someone else.
(The third option is extremely rare; this accounts
for most of the model's predictive power). We learn
these probabilities from IRC training data.
3.7 Model combination
To combine these different models, we adopt the
log-linear framework of Soricut and Marcu (2006).
Here, each model P
i
is assigned a weight 
i
, and the
combined score P (d) is proportional to:
X
i

i
log(P
i
(d))
The weights  can be learned discriminatively,
maximizing the probability of d relative to a task-
specic contrast set. For ordering experiments, the
contrast set is a single random permutation of d; we
explain the training regime for disentanglement be-
low, in subsection 4.1.
4 Comparing orderings of SWBD
To measure the differences in performance caused
by moving from news to a conversational domain,
we rst compare our models on an ordering task,
discrimination (Barzilay and Lapata, 2005; Karama-
nis et al, 2004). In this task, we take an original
document and randomly permute its sentences, cre-
ating an articial incoherent document. We then test
to see if our model prefers the coherent original.
For SWBD, rather than compare permutations
of the individual utterances, we permute conversa-
tional turns (sets of consecutive utterances by each
speaker), since turns are natural discourse units in
conversation. We take documents numbered 2000?
3999 as training/development and the remainder as
test, yielding 505 training and 153 test documents;
we evaluate 20 permutations per document. As a
comparison, we also show results for the same mod-
els on WSJ, using the train-test split from Elsner and
Charniak (2008a); the test set is sections 14-24, to-
talling 1004 documents.
Purandare and Litman (2008) carry out similar ex-
periments on distinguishing permuted SWBD doc-
uments, using lexical and WordNet features in a
model similar to Lapata (2003). Their accuracy for
this task (which they call ?switch-hard?) is roughly
68%.
1182
WSJ SWBD
EGrid 76.4z 86.0
Topical EGrid 71.8z 70.9z
IBM-1 77.2z 84.9y
Pronouns 69.6z 71.7z
Disc-new 72.3z 55.0z
Combined 81.9 88.4
-EGrid 81.0 87.5
-Topical EGrid 82.2 90.5
-IBM-1 79.0z 88.9
-Pronouns 81.3 88.5
-Disc-new 82.2 88.4
Table 1: Discrimination F scores on news and dialogue.
z indicates a signicant difference from the combined
model at p=.01 and y at p=.05.
In Table 1, we show the results for individual
models, for the combined model, and ablation re-
sults for mixtures without each component. WSJ is
more difcult than SWBD overall because, on av-
erage, news articles are shorter than SWBD con-
versations. Short documents are harder, because
permuting disrupts them less. The best SWBD re-
sult is 91%; the best WSJ result is 82% (both for
mixtures without the topical entity grid). The WSJ
result is state-of-the-art for the dataset, improving
slightly on Elsner and Charniak (2008a) at 81%. We
test results for signicance using the non-parametric
Mann-Whitney U test.
Controlling for the fact that discrimination is eas-
ier on SWBD, most of the individual models perform
similarly in both corpora. The strongest models in
both cases are the entity grid and IBM-1 (at about
77% for news, 85% for dialogue). Pronouns and the
topical entity grid are weaker. The major outlier is
the discourse-new model, whose performance drops
from 72% for news to only 55%, just above chance,
for conversation.
The model combination results show that all the
models are quite closely correlated, since leaving
out any single model does not degrade the combi-
nation very much (only one of the ablations is sig-
nicantly worse than the combination). The most
critical in news is IBM-1 (decreasing performance
by 3% when removed); in conversation, it is the
entity grid (decreasing by about 1%). The topical
entity grid actually has a (nonsignicant) negative
impact on combined performance, implying that its
predictive power in this setting comes mainly from
information that other models also capture, but that
it is noisier and less reliable. In each domain, the
combined models outperform the best single model,
showing the information provided by the weaker
models is not completely redundant.
Overall, these results suggest that most previ-
ously proposed local coherence models are domain-
general; they work on conversation as well as
news. The exception is the discourse-newness
model, which benets most from the specic con-
ventions of a written style. Full names with titles
(like ?President Barack Obama?) are more common
in news, while conversation tends to involve fewer
completely unfamiliar entities and more cases of
bridging reference, in which grounding information
is given implicitly (Nissim, 2006). Due to its poor
performance, we omit the discourse-newness model
in our remaining experiments.
5 Disentangling SWBD
We now turn to the task of disentanglement, test-
ing whether models that are good at ordering also
do well in this new setting. We would like to hold
the domain constant, but we do not have any disen-
tanglement data recorded from naturally occurring
speech, so we create synthetic instances by merging
pairs of SWBD dialogues. Doing so creates an arti-
cial transcript in which two pairs of people appear
to be talking simultaneously over a shared channel.
The situation is somewhat contrived in that each
pair of speakers converses only with each other,
never breaking into the other pair's dialogue and
rarely using devices like name mentioning to make
it clear who they are addressing. Since this makes
speaker identity a perfect cue for disentanglement,
we do not use it in this section. The only chat-
specic model we use is time.
Because we are not using speaker information, we
remove all utterances which do not contain a noun
before constructing synthetic transcripts? these are
mostly backchannels like ?Yeah?. Such utterances
cannot be correctly assigned by our coherence mod-
els, which deal with content; we suspect most of
them could be dealt with by associating them with
the nearest utterance from the same speaker.
1183
Once the backchannels are stripped, we can cre-
ate a synthetic transcript. For each dialogue, we rst
simulate timestamps by sampling the number of sec-
onds between each utterance and the next from a dis-
cretized Gaussian: bN(0; 2:5)c. The interleaving of
the conversations is dictated by the timestamps. We
truncate the longer conversation at the length of the
shorter; this ensures a baseline score of 50% for the
degenerate model that assigns all utterances to the
same conversation.
We create synthetic instances of two types? those
where the two entangled conversations had differ-
ent topical prompts and those where they were the
same. (Each dialogue in SWBD focuses on a prese-
lected topic, such as shing or movies.) We entangle
dialogues from our ordering development set to use
for mixture training and validation; for testing, we
use 100 instances of each type, constructed from di-
alogues in our test set.
When disentangling, we treat each thread as inde-
pendent of the others. In other words, the probability
of the entire transcript is the product of the probabil-
ities of the component threads. Our objective is to
nd the set of threads maximizing this. As a com-
parison, we use the model of Elsner and Charniak
(2008b) as a baseline. To make their implementa-
tion comparable to ours, in this section we constrain
it to nd only two threads.
5.1 Disentangling a single utterance
Our rst disentanglement task is to correctly assign
a single utterance, given the true structure of the rest
of the transcript. For each utterance, we compare
two versions of the transcript, the original, and a
version where it is swapped into the other thread.
Our accuracy measures how often our models prefer
the original. Unlike full-scale disentanglement, this
task does not require a computationally demanding
search, so it is possible to run experiments quickly.
We also use it to train our mixture models for disen-
tanglement, by construct a training example for each
utterance i in our training transcripts. Since the El-
sner and Charniak (2008b) model maximizes a cor-
relation clustering objective which sums up indepen-
dent edge weights, we can also use it to disentangle
a single sentence efciently.
Our results are shown in Table 2. Again, re-
sults for individual models are above the line, then
Different Same Avg.
EGrid 80.2 72.9 76.6
Topical EGrid 81.7 73.3 77.5
IBM-1 70.4 66.7 68.5
Pronouns 53.1 50.1 51.6
Time 58.5 57.4 57.9
Combined 86.8 79.6 83.2
-EGrid 86.0 79.1 82.6
-Topical EGrid 85.2 78.7 81.9
-IBM-1 86.2 78.7 82.4
-Pronouns 86.8 79.4 83.1
-Time 84.5 76.7 80.6
E+C `08 78.2 73.5 75.8
Table 2: Average accuracy for disentanglement of a sin-
gle utterance on 200 synthetic multiparty conversations
from SWBD test.
our combined model, and nally ablation results for
mixtures omitting a single model. The results show
that, for a pair of dialogues that differ in topic, our
best model can assign a single sentence with 87%
accuracy. For the same topic, the accuracy is 80%.
In each case, these results improve on (Elsner and
Charniak, 2008b), which scores 78% and 74%.
Changing to this new task has a substantial im-
pact on performance. The topical model, which per-
formed poorly for ordering, is actually stronger than
the entity grid in this setting. IBM-1 underperforms
either grid model (69% to 77%); on ordering, it was
nearly as good (85% to 86%).
Despite their ordering performance of 72%, pro-
nouns are essentially useless for this task, at 52%.
This decline is due partly to domain, and partly
to task setting. Although SWBD contains more
pronominals than WSJ, many of them are rst
and second-person pronouns or deictics, which our
model does not attempt to resolve. Since the disen-
tanglement task involves moving only a single sen-
tence, if moving this sentence does not sever a re-
solvable pronoun from its antecedent, the model will
be unable to make a good decision.
As before, the ablation results show that all the
models are quite correlated, since removing any sin-
gle model from the mixture causes only a small de-
crease in performance. The largest drop (83% to
81%) is caused by removing time; though time is
a weak model on its own, it is completely orthogo-
1184
nal to the other models, since unlike them, it does
not depend on the words in the sentences.
Comparing results between ?different topic? and
?same topic? instances shows that ?same topic? is
harder? by about 7% for the combined model. The
IBM model has a relatively small gap of 3.7%, and
in the ablation results, removing it causes a larger
drop in performance for ?same? than ?different?;
this suggests it is somewhat more robust to similar-
ity in topic than entity grids.
Disentanglement accuracy is hard to predict given
ordering performance; the two tasks plainly make
different demands on models. One difference is that
the models which use longer histories (the two entity
grids) remain strong, while the models considering
only one or two previous sentences (IBM and pro-
nouns) do not do as well. Since the changes being
considered here affect only a single sentence, while
permutation affects the entire transcript, more his-
tory may help by making the model more sensitive
to small changes.
5.2 Disentangling an entire transcript
We now turn to the task of disentangling an entire
transcript at once. This is a practical task, motivated
by applications such as search and information re-
trieval. However, it is more difcult than assign-
ing only a single utterance, because decisions are
interrelated? an error on one utterance may cause
a cascade of poor decisions further down. It is also
computationally harder.
We use tabu search (Glover and Laguna, 1997) to
nd a good solution. The search repeatedly nds and
moves the utterance which would most improve the
model score if swapped from one thread to the other.
Unlike greedy search, tabu search is constrained not
to repeat a solution that it has recently visited; this
forces it to keep exploring when it reaches a local
maximum. We run 500 iterations of tabu search
(usually nding the rst local maximum after about
100) and return the best solution found.
We measure performance with one-to-one over-
lap, which maps the two clusters to the two gold
dialogues, then measures percent correct
5
. Our re-
sults (Table 3) show that, for transcripts with dif-
ferent topics, our disentanglement has 68% over-
5
The other popular metrics, F and loc
3
, are correlated.
Different Same Avg.
EGrid 60.3 57.1 58.7
Topical EGrid 62.3 56.8 59.6
IBM-1 56.5 55.2 55.9
Pronouns 54.5 54.4 54.4
Time 55.4 53.8 54.6
Combined 67.9 59.8 63.9
E+C `08 59.1 57.4 58.3
Table 3: One-to-one overlap between disentanglement re-
sults and truth on 200 synthetic multiparty conversations
from SWBD test.
lap with truth, extracting about two thirds of the
structure correctly; this is substantially better than
Elsner and Charniak (2008b), which scores 59%.
Where the entangled conversations have the same
topic, performance is lower, about 60%, but still bet-
ter than the comparison model with 57%. Since cor-
relations with the previous section are fairly reliable,
and the disentanglement procedure is computation-
ally intensive, we omit ablation experiments.
As we expect, full disentanglement is more dif-
cult than single-sentence disentanglement (com-
bined scores drop by about 20%), but the single-
sentence task is a good predictor of relative perfor-
mance. Entity grid models do best, the IBM model
remains useful, but less so than for discrimination,
and pronouns are very weak. The IBM model per-
forms similarly under both metrics (56% and 57%),
while other models perform worse on loc
3
. This
supports our suggestion that IBM's decline in per-
formance from ordering is indeed due to its using a
single sentence history; it is still capable of getting
local structures right, but misses global ones.
6 IRC data
In this section, we move from synthetic data to
real multiparty discourse recorded from internet chat
rooms. We use two datasets: the #LINUX corpus
(Elsner and Charniak, 2008b), and three larger cor-
pora, #IPHONE, #PHYSICS and #PYTHON (Adams,
2008). We use the 1000-line ?development? sec-
tion of #LINUX for tuning our mixture models and
the 800-line ?test? section for development experi-
ments. We reserve the Adams (2008) corpora for
testing; together, they consist of 19581 lines of chat,
with each section containing 500 to 1000 lines.
1185
Chat-specic 74.0
+EGrid 79.3
+Topical EGrid 76.8
+IBM-1 76.3
+Pronouns 73.9
+EGrid/Topic/IBM-1 78.3
E+C `08b 76.4
Table 4: Accuracy for single utterance disentanglement,
averaged over annotations of 800 lines of #LINUX data.
In order to use syntactic models like the entity
grid, we parse the transcripts using (McClosky et
al., 2006). Performance is bad, although the parser
does identify most of the NPs; poor results are typi-
cal for a standard parser on chat (Foster, 2010). We
postprocess the parse trees to retag ?lol?, ?haha? and
?yes? as UH (rather than NN, NNP and JJ).
In this section, we use all three of our chat-
specic models (sec. 2.0.6; time, speaker andmen-
tion) as a baseline. This baseline is relatively strong,
so we evaluate our other models in combination with
it.
6.1 Disentangling a single sentence
As before, we show results on correctly disentan-
gling a single sentence, given the correct structure
of the rest of the transcript. We average perfor-
mance on each transcript over the different annota-
tions, then average the transcripts, weighing them by
length to give each utterance equal weight.
Table 4 gives results on our development corpus,
#LINUX. Our best result, for the chat-specic fea-
tures plus entity grid, is 79%, improving on the com-
parison model, Elsner and Charniak (2008b), which
gets 76%. (Although the table only presents an av-
erage over all annotations of the dataset, this model
is also more accurate for each individual annota-
tor than the comparison model.) We then ran the
same model, chat-specic features plus entity grid,
on the test corpora from Adams (2008). These re-
sults (Table 5) are also better than Elsner and Char-
niak (2008b), at an average of 93% over 89%.
As pointed out in Elsner and Charniak (2008b),
the chat-specic features are quite powerful in this
domain, and it is hard to improve over them. Elsner
and Charniak (2008b), which has simple lexical fea-
tures, mostly based on unigram overlap, increases
#IPHONE #PHYSICS #PYTHON
+EGrid 92.3 96.6 91.1
E+C `08b 89.0 90.2 88.4
Table 5: Average accuracy for disentanglement of a sin-
gle utterance for 19581 total lines from Adams (2008).
performance over baseline by 2%. Both IBM and
the topical entity grid achieve similar gains. The en-
tity grid does better, increasing performance to 79%.
Pronouns, as before for SWBD, are useless.
We believe that the entity grid's good perfor-
mance here is due mostly to two factors: its use of
a long history, and its lack of lexicalization. The
grid looks at the previous six sentences, which dif-
ferentiates it from the IBM model and from Elsner
and Charniak (2008b), which treats each pair of sen-
tences independently. Using this long history helps
to distinguish important nouns from unimportant
ones better than frequency alone. We suspect that
our lexicalized models, IBM and the topical entity
grid, are hampered by poor parameter settings, since
their parameters were learned on FISHER rather than
IRC chat. In particular, we believe this explains why
the topical entity grid, which slightly outperformed
the entity grid on SWBD, is much worse here.
6.2 Full disentanglement
Running our tabu search algorithm on the full disen-
tanglement task yields disappointing results. Accu-
racies on the #LINUX dataset are not only worse than
previous work, but also worse than simple baselines
like creating one thread for each speaker. The model
nds far too many threads? it detects over 300, when
the true number is about 81 (averaging over annota-
tions). This appears to be related to biases in our
chat-specic models as well as in the entity grid;
the time model (which generates gaps between adja-
cent sentences) and the speaker model (which uses
a CRP) both assign probability 1 to single-utterance
conversations. The entity grid also has a bias toward
short conversations, because unseen entities are em-
pirically more likely to occur toward the beginning
of a conversation than in the middle.
A major weakness in our model is that we aim
only to maximize coherence of the individual con-
versations, with no prior on the likely length or num-
ber of conversations that will appear in the tran-
1186
script. This allows the model to create far too many
conversations. Integrating a prior into our frame-
work is not straightforward because we currently
train our mixture to maximize single-utterance dis-
entanglement performance, and the prior is not use-
ful for this task.
We experimented with xing parts of the tran-
script to the solution obtained by Elsner and Char-
niak (2008b), then using tabu search to ll in the
gaps. This constrains the number of conversations
and their approximate positions. With this structure
in place, we were able to obtain scores comparable
to Elsner and Charniak (2008b), but not improve-
ments. It appears that our performance increase on
single-sentence disentanglement does not transfer to
this task because of cascading errors and the neces-
sity of using external constraints.
7 Conclusions
We demonstrate that several popular models of lo-
cal coherence transfer well to the conversational do-
main, suggesting that they do indeed capture coher-
ence in general rather than specic conventions of
newswire text. However, their performance across
tasks is not as stable; in particular, models which
use less history information are worse for disentan-
glement.
Our results study suggest that while sophisticated
coherence models can potentially contribute to dis-
entanglement, they would benet greatly from im-
proved low-level resources for internet chat. Bet-
ter parsing, or at least NP chunking, would help for
models like the entity grid which rely on syntactic
role information. Larger training sets, or some kind
of transfer learning, could improve the learning of
topics and other lexical parameters. In particular,
our results on SWBD data conrm the conjecture of
(Adams, 2008) that LDA topic modeling is in prin-
ciple a useful tool for disentanglement? we believe a
topic-based model could also work on IRC chat, but
would require a better set of extracted topics. With
better parameters for these models and the integra-
tion of a prior, we believe that our good performance
on SWBD and single-utterance disentanglement for
IRC can be extended to full-scale disentanglement
of IRC.
Acknowledgements
We are extremely grateful to Regina Barzilay, Mark
Johnson, Rebecca Mason, Ben Swanson and Neal
Fox for their comments, to Craig Martell for the
NPS chat datasets and to three anonymous review-
ers. This work was funded by a Google Fellowship
for Natural Language Processing.
References
Paige H. Adams. 2008. Conversation Thread Extraction
and Topic Detection in Text-based Chat. Ph.D. thesis,
Naval Postgraduate School.
David Aldous. 1985. Exchangeability and related top-
ics. In Ecole d'Ete de Probabilities de Saint-Flour
XIII 1983, pages 1?198. Springer.
Paul M. Aoki, Matthew Romaine, Margaret H. Szyman-
ski, James D. Thornton, Daniel Wilson, and Allison
Woodruff. 2003. The mad hatter's cocktail party: a
social mobile audio space supporting multiple simul-
taneous conversations. In CHI '03: Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 425?432, New York, NY, USA. ACM
Press.
Paul M. Aoki, Margaret H. Szymanski, Luke D.
Plurkowski, James D. Thornton, Allison Woodruff,
and Weilie Yi. 2006. Where's the ?party? in ?multi-
party??: analyzing the structure of small-group socia-
ble talk. In CSCW '06: Proceedings of the 2006 20th
anniversary conference on Computer supported coop-
erative work, pages 393?402, New York, NY, USA.
ACM Press.
Regina Barzilay and Mirella Lapata. 2005. Modeling lo-
cal coherence: an entity-based approach. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL'05).
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113?120.
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2001. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:2003.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
EACL, Athens, Greece.
Harr Chen, S.R.K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Global models of document
structure using latent permutations. In Proceedings
of Human Language Technologies: The 2009 Annual
1187
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 371?
379, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In EMNLP, pages
334?343.
Micha Elsner and Eugene Charniak. 2008a.
Coreference-inspired coherence modeling. In
Proceedings of ACL-08: HLT, Short Papers, pages
41?44, Columbus, Ohio, June. Association for
Computational Linguistics.
Micha Elsner and Eugene Charniak. 2008b. You talk-
ing to me? a corpus and algorithm for conversation
disentanglement. In Proceedings of ACL-08: HLT,
pages 834?842, Columbus, Ohio, June. Association
for Computational Linguistics.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
latent semantic analysis. Discourse Processes,
25(2&3):285?307.
Jennifer Foster. 2010. ?cba to check the spelling?: In-
vestigating parser performance on discussion forum
posts. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
381?384, Los Angeles, California, June. Association
for Computational Linguistics.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A sta-
tistical approach to anaphora resolution. In Proceed-
ings of the Sixth Workshop on Very Large Corpora,
pages 161?171, Orlando, Florida. Harcourt Brace.
Fred Glover and Manuel Laguna. 1997. Tabu Search.
University of Colorado at Boulder.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Simon Haykin and Zhe Chen. 2005. The Cocktail Party
Problem. Neural Computation, 17(9):1875?1902.
Nikiforos Karamanis, Massimo Poesio, Chris Mellish,
and Jon Oberlander. 2004. Evaluating centering-
based metrics of coherence. In ACL, pages 391?398.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In IJCAI, pages 1085?1090.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the annual meeting of ACL, 2003.
Mirella Lapata. 2006. Automatic evaluation of informa-
tion ordering: Kendall's tau. Computational Linguis-
tics, 32(4):1?14.
Gideon Mann, Ryan McDonald, Mehryar Mohri, Nathan
Silberman, and Dan Walker. 2009. Efcient large-
scale distributed training of conditional maximum en-
tropy models. In Y. Bengio, D. Schuurmans, J. Laf-
ferty, C. K. I. Williams, and A. Culotta, editors, Ad-
vances in Neural Information Processing Systems 22,
pages 1231?1239.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 152?159.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic domain adaptation for parsing. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 28?36,
Los Angeles, California, June. Association for Com-
putational Linguistics.
Eleni Miltsakaki and K. Kukich. 2004. Evaluation of text
coherence for electronic essay scoring systems. Nat.
Lang. Eng., 10(1):25?55.
Neville Moray. 1959. Attention in dichotic listening: Af-
fective cues and the inuence of instructions. Quar-
terly Journal of Experimental Psychology, 11(1):56?
60.
Ani Nenkova and Kathleen McKeown. 2003. Refer-
ences to named entities: a corpus study. In NAACL
'03, pages 70?72.
Malvina Nissim. 2006. Learning information status of
discourse entities. In Proceedings of EMNLP, pages
94?102, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Jacki O'Neill and David Martin. 2003. Text chat in ac-
tion. In GROUP '03: Proceedings of the 2003 inter-
national ACM SIGGROUP conference on Supporting
group work, pages 40?49, New York, NY, USA. ACM
Press.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unied framework for predicting text qual-
ity. In Proceedings of the 2008 Conference on Empir-
ical Methods in Natural Language Processing, pages
186?195, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Massimo Poesio, Mijail Alexandrov-Kabadjov, Renata
Vieira, Rodrigo Goulart, and Olga Uryupina. 2005.
Does discourse-new detection help denite description
resolution? In Proceedings of the Sixth International
Workshop on Computational Semantics, Tillburg.
Amruta Purandare and Diane J. Litman. 2008. Analyz-
ing dialog coherence using transition patterns in lexi-
cal and semantic features. In FLAIRS Conference'08,
pages 195?200.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
1188
streams. In SIGIR '06: Proceedings of the 29th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 35?42,
New York, NY, USA. ACM.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the Association for Computational Lin-
guistics Conference (ACL-2006).
Lidan Wang and Douglas W. Oard. 2009. Context-based
message expansion for disentanglement of interleaved
text conversations. In Proceedings of NAACL-09.
1189
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125?129,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Extending the Entity Grid with Entity-Specic Features
Micha Elsner
School of Informatics
University of Edinburgh
melsner0@gmail.com
Eugene Charniak
Department of Computer Science
Brown University, Providence, RI 02912
ec@cs.brown.edu
Abstract
We extend the popular entity grid representa-
tion for local coherence modeling. The grid
abstracts away information about the entities it
models; we add discourse prominence, named
entity type and coreference features to distin-
guish between important and unimportant en-
tities. We improve the best result for WSJ doc-
ument discrimination by 6%.
1 Introduction
A well-written document is coherent (Halliday and
Hasan, 1976)? it structures information so that each
new piece of information is interpretable given the
preceding context. Models that distinguish coherent
from incoherent documents are widely used in gen-
eration, summarization and text evaluation.
Among the most popular models of coherence is
the entity grid (Barzilay and Lapata, 2008), a sta-
tistical model based on Centering Theory (Grosz et
al., 1995). The grid models the way texts focus
on important entities, assigning them repeatedly to
prominent syntactic roles. While the grid has been
successful in a variety of applications, it is still a
surprisingly unsophisticated model, and there have
been few direct improvements to its simple feature
set. We present an extension to the entity grid which
distinguishes between different types of entity, re-
sulting in signicant gains in performance
1
.
At its core, the grid model works by predicting
whether an entity will appear in the next sentence
1
A public implementation is available via https://
bitbucket.org/melsner/browncoherence.
(and what syntactic role it will have) given its his-
tory of occurrences in the previous sentences. For
instance, it estimates the probability that ?Clinton?
will be the subject of sentence 2, given that it was
the subject of sentence 1. The standard grid model
uses no information about the entity itself? the prob-
ability is the same whether the entity under discus-
sion is ?Hillary Clinton? or ?wheat?. Plainly, this
assumption is too strong. Distinguishing important
from unimportant entity types is important in coref-
erence (Haghighi and Klein, 2010) and summariza-
tion (Nenkova et al, 2005); our model applies the
same insight to the entity grid, by adding informa-
tion from syntax, a named-entity tagger and statis-
tics from an external coreference corpus.
2 Related work
Since its initial appearance (Lapata and Barzilay,
2005; Barzilay and Lapata, 2005), the entity grid
has been used to perform wide variety of tasks. In
addition to its rst proposed application, sentence
ordering for multidocument summarization, it has
proven useful for story generation (McIntyre and
Lapata, 2010), readability prediction (Pitler et al,
2010; Barzilay and Lapata, 2008) and essay scor-
ing (Burstein et al, 2010). It also remains a criti-
cal component in state-of-the-art sentence ordering
models (Soricut and Marcu, 2006; Elsner and Char-
niak, 2008), which typically combine it with other
independently-trained models.
There have been few attempts to improve the en-
tity grid directly by altering its feature representa-
tion. Filippova and Strube (2007) incorporate se-
mantic relatedness, but nd no signicant improve-
125
1 [Visual meteorological conditions]
S
prevailed for [the
personal cross country ight for which [a VFR ight
plan]
O
was led]
X
.
2 [The ight]
S
originated at [Nuevo Laredo , Mexico]
X
,
at [approximately 1300]
X
.
s conditions plan flight laredo
1 S O X -
2 - - S X
Figure 1: A short text (using NP-only mention detection),
and its corresponding entity grid. The numeric token
?1300? is removed in preprocessing.
ment over the original model. Cheung and Penn
(2010) adapt the grid to German, where focused con-
stituents are indicated by sentence position rather
than syntactic role. The best entity grid for English
text, however, is still the original.
3 Entity grids
The entity grid represents a document as a matrix
(Figure 1) with a row for each sentence and a column
for each entity. The entry for (sentence i, entity j),
which we write r
i;j
, represents the syntactic role that
entity takes on in that sentence: subject (S), object
(O), or some other role (X)
2
. In addition, there is a
special marker (-) for entities which do not appear at
all in a given sentence.
To construct a grid, we must rst decide which
textual units are to be considered ?entities?, and how
the different mentions of an entity are to be linked.
We follow the -COREFERENCE setting from Barzi-
lay and Lapata (2005) and perform heuristic coref-
erence resolution by linking mentions which share a
head noun. Although some versions of the grid use
an automatic coreference resolver, this often fails
to improve results; in Barzilay and Lapata (2005),
coreference improves results in only one of their tar-
get domains, and actually hurts for readability pre-
diction. Their results, moreover, rely on running
coreference on the document in its original order; in
a summarization task, the correct order is not known,
which will cause even more resolver errors.
To build a model based on the grid, we treat the
columns (entities) as independent, and look at lo-
cal transitions between sentences. We model the
2
Roles are determined heuristically using trees produced by
the parser of (Charniak and Johnson, 2005).
transitions using the generative approach given in
Lapata and Barzilay (2005)
3
, in which the model
estimates the probability of an entity's role in the
next sentence, r
i;j
, given its history in the previ-
ous two sentences, r
i 1;j
; r
i 2;j
. It also uses a sin-
gle entity-specic feature, salience, determined by
counting the total number of times the entity is men-
tioned in the document. We denote this feature vec-
tor F
i;j
. For example, the vector for ?ight? after the
last sentence of the example would be F
3;f light
=
hX;S; sal = 2i. Using two sentences of context
and capping salience at 4, there are only 64 possi-
ble vectors, so we can learn an independent multino-
mial distribution for each F . However, the number
of vectors grows exponentially as we add features.
4 Experimental design
We test our model on two experimental tasks, both
testing its ability to distinguish between correct
and incorrect orderings for WSJ articles. In doc-
ument discrimination (Barzilay and Lapata, 2005),
we compare a document to a random permutation of
its sentences, scoring the system correct if it prefers
the original ordering
4
.
We also evaluate on the more difcult task of sen-
tence insertion (Chen et al, 2007; Elsner and Char-
niak, 2008). In this task, we remove each sentence
from the article and test whether the model prefers to
re-insert it at its original location. We report the av-
erage proportion of correct insertions per document.
As in Elsner and Charniak (2008), we test on sec-
tions 14-24 of the Penn Treebank, for 1004 test doc-
uments. We test signicance using the Wilcoxon
Sign-rank test, which detects signicant differences
in the medians of two distributions
5
.
5 Mention detection
Our main contribution is to extend the entity grid
by adding a large number of entity-specic features.
Before doing so, however, we add non-head nouns
to the grid. Doing so gives our feature-based model
3
Barzilay and Lapata (2005) give a discriminative model,
which relies on the same feature set as discussed here.
4
As in previous work, we use 20 random permutations of
each document. Since the original and permutation might tie,
we report both accuracy and balanced F-score.
5
Our reported scores are means, but to test signicance of
differences in means, we would need to use a parametric test.
126
Disc. Acc Disc. F Ins.
Random 50.0 50.0 12.6
Grid: NPs 74.4 76.2 21.3
Grid: all nouns
y
77.8 79.7 23.5
Table 1: Discrimination scores for entity grids with dif-
ferent mention detectors onWSJ development documents.
y
indicates performance on both tasks is signicantly dif-
ferent from the previous row of the table with p=.05.
more information to work with, but is benecial
even to the standard entity grid.
We alter our mention detector to add all nouns
in the document to the grid
6
, even those which do
not head NPs. This enables the model to pick up
premodiers in phrases like ?a Bush spokesman?,
which do not head NPs in the Penn Treebank. Find-
ing these is also necessary to maximize coreference
recall (Elsner and Charniak, 2010). We give non-
head mentions the role X. The results of this change
are shown in Table 1; discrimination performance
increases about 4%, from 76% to 80%.
6 Entity-specic features
As we mentioned earlier, the standard grid model
does not distinguish between different types of en-
tity. Given the same history and salience, the same
probabilities are assigned to occurrences of ?Hillary
Clinton?, ?the airlines?, or ?May 25th?, even though
we know a priori that a document is more likely to
be about Hillary Clinton than it is to be about May
25th. This problem is exacerbated by our same-head
coreference heuristic, which sometimes creates spu-
rious entities by lumping together mentions headed
by nouns like ?miles? or ?dollars?. In this section,
we add features that separate important entities from
less important or spurious ones.
Proper Does the entity have a proper mention?
Named entity The majority OPENNLP Morton et
al. (2005) named entity label for the coreferen-
tial chain.
Modiers The total number of modiers in all men-
tions in the chain, bucketed by 5s.
Singular Does the entity have a singular mention?
6
Barzilay and Lapata (2008) uses NPs as mentions; we are
unsure whether all other implementations do the same, but we
believe we are the rst to make the distinction explicit.
News articles are likely to be about people and
organizations, so we expect these named entity tags,
and proper NPs in general, to be more important to
the discourse. Entities with many modiers through-
out the document are also likely to be important,
since this implies that the writer wishes to point
out more information about them. Finally, singular
nouns are less likely to be generic.
We also add some features to pick out entities
that are likely to be spurious or unimportant. These
features depend on in-domain coreference data, but
they do not require us to run a coreference resolver
on the target document itself. This avoids the prob-
lem that coreference resolvers do not work well for
disordered or automatically produced text such as
multidocument summary sentences, and also avoids
the computational cost associated with coreference
resolution.
Linkable Was the head word of the entity ever
marked as coreferring in MUC6?
Unlinkable Did the head word of the entity occur 5
times in MUC6 and never corefer?
Has pronouns Were there 5 or more pronouns
coreferent with the head word of the entity in
the NANC corpus? (Pronouns in NANC are
automatically resolved using an unsupervised
model (Charniak and Elsner, 2009).)
No pronouns Did the head word of the entity occur
over 50 times in NANC, and have fewer than 5
coreferent pronouns?
To learn probabilities based on these features,
we model the conditional probability p(r
i;j
jF ) us-
ing multilabel logistic regression. Our model has
a parameter for each combination of syntactic role
r, entity-specic feature h and feature vector F :
rhF . This allows the old and new features to in-
teract while keeping the parameter space tractable
7
.
In Table 2, we examine the changes in our esti-
mated probability in one particular context: an entity
with salience 3 which appeared in a non-emphatic
role in the previous sentence. The standard entity
grid estimates that such an entity will be the sub-
ject of the next sentence with a probability of about
7
We train the regressor using OWLQN (Andrew and Gao,
2007), modied and distributed by Mark Johnson as part of
the Charniak-Johnson parse reranker (Charniak and Johnson,
2005).
127
Context P(next role is subj)
Standard egrid .045
Head coref in MUC6 .013
...and proper noun .025
...and NE type person .037
...and 5 modiers overall .133
Never coref in MUC6 .006
...and NE type date .001
Table 2: Probability of an entity appearing as subject of
the next sentence, given the history - X, salience 3, and
various entity-specic features.
.04. For most classes of entity, we can see that this
is an overestimate; for an entity described by a com-
mon noun (such as ?the airline?), the probability as-
signed by the extended grid model is .01. If we
suspect (based on MUC6 evidence) that the noun
is not coreferent, the probability drops to .006 (?an
increase?)? if it is a date, it falls even further, to .001.
However, given that the entity refers to a person, and
some of its mentions are modied, suggesting the ar-
ticle gives a title or description (?Obama's Secretary
of State, Hillary Clinton?), the chance that it will be
the subject of the next sentence more than triples.
7 Experiments
Table 3 gives results for the extended grid model
on the test set. This model is signicantly better
than the standard grid on discrimination (84% ver-
sus 80%) and has a higher mean score on insertion
(24% versus 21%)
8
.
The best WSJ results in previous work are those of
Elsner and Charniak (2008), who combine the entity
grid with models based on pronoun coreference and
discourse-new NP detection. We report their scores
in the table. This comparison is unfair, however,
because the improvements from adding non-head
nouns improve our baseline grid sufciently to equal
their discrimination result. State-of-the-art results
on a different corpus and task were achieved by Sori-
cut and Marcu (2006) using a log-linear mixture of
an entity grid, IBM translation models, and a word-
correspondence model based on Lapata (2003).
8
For insertion using the model on its own, the median
changes less than the mean, and the change in median score is
not signicant. However, using the combined model, the change
is signicant.
Disc. Acc Disc. F Ins.
Random 50.00 50.00 12.6
Elsner+Charniak 79.6 81.0 23.0
Grid 79.5 80.9 21.4
Extended Grid 84.0
y
84.5 24.2
Grid+combo 82.6 84.0 24.3
ExtEGrid+combo 86.0
y
86.5 26.7
y
Table 3: Extended entity grid and combination model
performance on 1004 WSJ test documents. Combination
models incorporate pronoun coreference, discourse-new
NP detection, and IBM model 1.
y
indicates an extended
model score better than its baseline counterpart at p=.05.
To perform a fair comparison of our extended
grid with these model-combining approaches, we
train our own combined model incorporating an en-
tity grid, pronouns, discourse-newness and the IBM
model. We combine models using a log-linear mix-
ture as in Soricut and Marcu (2006), training the
weights to maximize discrimination accuracy.
The second section of Table 3 shows these model
combination results. Notably, our extended entity
grid on its own is essentially just as good as the com-
bined model, which represents our implementation
of the previous state of the art. When we incorpo-
rate it into a combination, the performance increase
remains, and is signicant for both tasks (disc. 86%
versus 83%, ins. 27% versus 24%). Though the im-
provement is not perfectly additive, a good deal of
it is retained, demonstrating that our additions to the
entity grid are mostly orthogonal to previously de-
scribed models. These results are the best reported
for sentence ordering of English news articles.
8 Conclusion
We improve a widely used model of local discourse
coherence. Our extensions to the feature set involve
distinguishing simple properties of entities, such as
their named entity type, which are also useful in
coreference and summarization tasks. Although our
method uses coreference information, it does not re-
quire coreference resolution to be run on the target
documents. Given the popularity of entity grid mod-
els for practical applications, we hope our model's
improvements will transfer to summarization, gen-
eration and readability prediction.
128
Acknowledgements
We are most grateful to Regina Barzilay, Mark John-
son and three anonymous reviewers. This work was
funded by a Google Fellowship for Natural Lan-
guage Processing.
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In ICML '07.
Regina Barzilay and Mirella Lapata. 2005. Modeling lo-
cal coherence: an entity-based approach. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL'05).
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: an entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Jill Burstein, Joel Tetreault, and Slava Andreyev. 2010.
Using entity-based features to model coherence in stu-
dent essays. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 681?684, Los Angeles, California, June. Asso-
ciation for Computational Linguistics.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
EACL, Athens, Greece.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
ne n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. of the 2005 Meeting of the Assoc. for
Computational Linguistics (ACL), pages 173?180.
Erdong Chen, Benjamin Snyder, and Regina Barzilay.
2007. Incremental text structuring with online hier-
archical ranking. In Proceedings of EMNLP.
Jackie Chi Kit Cheung and Gerald Penn. 2010. Entity-
based local coherence modelling using topological
elds. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 186?195, Uppsala, Sweden, July. Association
for Computational Linguistics.
Micha Elsner and Eugene Charniak. 2008. Coreference-
inspired coherence modeling. In Proceedings of ACL-
08: HLT, Short Papers, pages 41?44, Columbus, Ohio,
June. Association for Computational Linguistics.
Micha Elsner and Eugene Charniak. 2010. The same-
head heuristic for coreference. In Proceedings of ACL
10, Uppsala, Sweden, July. Association for Computa-
tional Linguistics.
Katja Filippova and Michael Strube. 2007. Extend-
ing the entity-grid coherence model to semantically
related entities. In Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation,
pages 139?142, Saarbr?ucken, Germany, June. DFKI
GmbH. Document D-07-01.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393, Los An-
geles, California, June. Association for Computational
Linguistics.
Michael Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman, London.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In IJCAI, pages 1085?1090.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the annual meeting of ACL, 2003.
Neil McIntyre and Mirella Lapata. 2010. Plot induction
and evolutionary search for story generation. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1562?1572,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Thomas Morton, Joern Kottmann, Jason Baldridge, and
Gann Bierner. 2005. Opennlp: A java-based nlp
toolkit. http://opennlp.sourceforge.net.
Ani Nenkova, Advaith Siddharthan, and Kathleen McK-
eown. 2005. Automatically learning cognitive status
for multi-document summarization of newswire. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing, pages 241?248, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 544?554, Uppsala, Sweden, July.
Association for Computational Linguistics.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the Association for Computational Lin-
guistics Conference (ACL-2006).
129
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 193?197,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Native Language Detection with Tree Substitution Grammars
Ben Swanson
Brown University
chonger@cs.brown.edu
Eugene Charniak
Brown University
ec@cs.brown.edu
Abstract
We investigate the potential of Tree Substitu-
tion Grammars as a source of features for na-
tive language detection, the task of inferring
an author?s native language from text in a dif-
ferent language. We compare two state of the
art methods for Tree Substitution Grammar
induction and show that features from both
methods outperform previous state of the art
results at native language detection. Further-
more, we contrast these two induction algo-
rithms and show that the Bayesian approach
produces superior classification results with a
smaller feature set.
1 Introduction
The correlation between a person?s native language
(L1) and aspects of their writing in a second lan-
guage (L2) can be exploited to predict L1 label given
L2 text. The International Corpus of Learner En-
glish (Granger et al 2002), or ICLE, is a large set
of English student essays annotated with L1 labels
that allows us to bring the power of supervised ma-
chine learning techniques to bear on this task. In
this work we explore the possibility of automatically
induced Tree Substitution Grammar (TSG) rules as
features for a logistic regression model1 trained to
predict these L1 labels.
Automatic TSG induction is made difficult by the
exponential number of possible TSG rules given a
corpus. This is an active area of research with two
distinct effective solutions. The first uses a nonpara-
metric Bayesian model to handle the large number
1a.k.a. Maximum Entropy Model
of rules (Cohn and Blunsom, 2010), while the sec-
ond is inspired by tree kernel methods and extracts
common subtrees from pairs of parse trees (Sangati
and Zuidema, 2011). While both are effective, we
show that the Bayesian method of TSG induction
produces superior features and achieves a new best
result at the task of native language detection.
2 Related Work
2.1 Native Language Detection
Work in automatic native language detection has
been mainly associated with the ICLE, published in
2002. Koppel et al(2005) first constructed such
a system with a feature set consisting of function
words, POS bi-grams, and character n-grams. These
features provide a strong baseline but cannot capture
many linguistic phenomena.
More recently, Wong and Dras (2011a) consid-
ered syntactic features for this task, using logis-
tic regression with features extracted from parse
trees produced by a state of the art statistical parser.
They investigated two classes of features: rerank-
ing features from the Charniak parser and CFG fea-
tures. They showed that while reranking features
capture long range dependencies in parse trees that
CFG rules cannot, they do not produce classification
performance superior to simple CFG rules. Their
CFG feature approach represents the best perform-
ing model to date for the task of native language de-
tection. Wong and Dras (2011b) also investigated
the use of LDA topic modeling to produce a latent
feature set of reduced dimensionality, but failed to
outperform baseline systems with this approach.
193
2.2 TSG induction
One inherent difficulty in the use of TSGs is con-
trolling the size of grammars automatically in-
duced from data, which with any reasonable corpus
quickly becomes too large for modern workstations
to handle. When automatically induced TSGs were
first proposed by Bod (1991), the problem of gram-
mar induction was tackled with random selection of
fragments or weak constraints that led to massive
grammars.
A more principled technique is to use a sparse
nonparametric prior, as was recently presented by
Cohn et al(2009) and Post and Gildea (2009). They
provide a local Gibbs sampling algorithm, and Cohn
and Blunsom (2010) later developed a block sam-
pling algorithm with better convergence behavior.
While this Bayesian method has yet to produce
state of the art parsing results, it has achieved state
of the art results for unsupervised grammar induc-
tion (Blunsom and Cohn, 2010) and has been ex-
tended to synchronous grammars for use in sentence
compression (Yamangil and Shieber, 2010).
More recently, (Sangati and Zuidema, 2011) pre-
sented an elegantly simple heuristic inspired by tree
kernels that they call DoubleDOP. They showed that
manageable grammar sizes can be obtained from a
corpus the size of the Penn Treebank by recording
all fragments that occur at least twice, subject to a
pairwise constraint of maximality. Using an addi-
tional heuristic to provide a distribution over frag-
ments, DoubleDOP achieved the current state of the
art for TSG parsing, competing closely with the ab-
solute best results set by refinement based parsers.
2.3 Fragment Based Classification
The use of parse tree fragments for classification
began with Collins and Duffy (2001). They used
the number of common subtrees between two parse
trees as a convolution kernel in a voted perceptron
and applied it as a parse reranker. Since then, such
tree kernels have been used to perform a variety of
text classification tasks, such as semantic role la-
beling (Moschitti et al 2008), authorship attribu-
tion (Kim et al 2010), or the work of Suzuki and
Isozaki (2006) that performs question classification,
subjectivity detection, and polarity identification.
Syntactic features have also been used in non-
kernelized classifiers, such as in the work of Wong
and Dras (2011a) mentioned in Section 2.1. Ad-
ditional examples include Raghavan et al(2010),
which uses a CFG language model to perform au-
thorship attribution, and Post (2011), which uses
TSG features in a logistic regression model to per-
form grammaticality detection.
3 Tree Substitution Grammars
Tree Substitution Grammars are similar to Context
Free Grammars, differing in that they allow rewrite
rules of arbitrary parse tree structure with any num-
ber of nonterminal or terminal leaves. We adopt the
common term fragment2 to refer to these rules, as
they are easily visualised as fragments of a complete
parse tree.
S
NP VP
VBZ
hates
NP
NP
NNP
George
NP
NN
broccoli
NP
NNS
shoes
Figure 1: Fragments from a Tree Substitution Grammar
capable of deriving the sentences ?George hates broccoli?
and ?George hates shoes?.
3.1 Bayesian Induction
Nonparametric Bayesian models can represent dis-
tributions of unbounded size with a dynamic param-
eter set that grows with the size of the training data.
One method of TSG induction is to represent a prob-
abilistic TSG with Dirichlet Process priors and sam-
ple derivations of a corpus using MCMC.
Under this model the posterior probability of a
fragment e is given as
P (e|e?, ?, P0) =
#e + ?P0
#? + ?
(1)
where e? is the multiset of fragments in the current
derivations excluding e, #e is the count of the frag-
ment e in e?, and #? is the total number of frag-
ments in e? with the same root node as e. P0 is
2As opposed to elementary tree, often used in related work
194
a PCFG distribution over fragments with a bias to-
wards small fragments. ? is the concentration pa-
rameter of the DP, and can be used to roughly tune
the number of fragments that appear in the sampled
derivations.
With this posterior distribution the derivations of
a corpus can be sampled tree by tree using the block
sampling algorithm of Cohn and Blunsom (2010),
converging eventually on a sample from the true
posterior of all derivations.
3.2 DoubleDOP Induction
DoubleDOP uses a heuristic inspired by tree kernels,
which are commonly used to measure similarity be-
tween two parse trees by counting the number of
fragments they share. DoubleDOP uses the same un-
derlying technique, but caches the shared fragments
instead of simply counting them. This yields a set
of fragments where each member is guaranteed to
appear at least twice in the training set.
In order to avoid unmanageably large grammars
only maximal fragments are retained in each pair-
wise extraction, which is to say that any shared frag-
ment that occurs inside another shared fragment is
discarded. The main disadvantage of this method
is that the complexity scales quadratically with the
training set size, as all pairs of sentences must be
considered. It is fully parallelizable, however, which
mediates this disadvantage to some extent.
4 Experiments
4.1 Methodology
Our data is drawn from the International Corpus
of Learner English (Version 2), which consists of
raw unsegmented English text tagged with L1 la-
bels. Our experimental setup follows Wong and
Dras (2011a) in analyzing Chinese, Russian, Bul-
garian, Japanese, French, Czech, and Spanish L1 es-
says. As in their work we randomly sample 70 train-
ing and 25 test documents for each language. All re-
ported results are averaged over 5 subsamplings of
the full data set.
Our data preproccesing pipeline is as fol-
lows: First we perform sentence segmentation with
OpenNLP and then parse each sentence with a 6
split grammar for the Berkeley Parser (Petrov et al
2006). We then replace all terminal symbols which
do not occur in a list of 598 function words3 with
a single UNK terminal. This aggressive removal of
lexical items is standard in this task and mitigates the
effect of other unwanted information sources such
as topic and geographic location that are correlated
with native language in the data.
We contrast three different TSG feature sets in our
experiments. First, to provide a baseline, we sim-
ply read off the CFG rules from the data set (note
that a CFG can be taken as a TSG with all frag-
ments having depth one). Second, in the method
we call BTSG, we use the Bayesian induction model
with the Dirichlet process? concentration parameters
tuned to 100 and run for 1000 iterations of sampling.
We take as our resulting finite grammar the frag-
ments that appear in the sampled derivations. Third,
we run the parameterless DoubleDOP (2DOP) in-
duction method.
Using the full 2DOP feature set produces over
400k features, which heavily taxes the resources of
a single modern workstation. To balance the feature
set sizes between 2DOP and BTSG we pass back
over the training data and count the actual number
of times each fragment recovered by 2DOP appears.
We then limit the list to the n most common frag-
ments, where n is the average number of fragments
recovered by the BTSG method (around 7k). We re-
fer to results using this trimmed feature set with the
label 2DOP, using 2DOP(F) to refer to DoubleDOP
with the full set of features.
Given each TSG, we create a binary feature func-
tion for each fragment e in the grammar such that the
feature fe(d) is active for a document d if there ex-
ists a derivation of some tree t ? d that uses e. Clas-
sification is performed with the Mallet package for
logistic regression using the default initialized Max-
EntTrainer.
5 Results
5.1 Predictive Power
The resulting classification accuracies are shown in
Table 1. The BTSG feature set gives the highest per-
formance, and both true TSG induction techniques
outperform the CFG baseline.
3We use the stop word list distributed with the ROUGE sum-
marization evaluation package.
195
Model Accuracy (%)
CFG 72.6
2DOP 73.5
2DOP(F) 76.8
BTSG 78.4
Table 1: Classification accuracy
The CFG result represents the work of Wong and
Dras (2011a), the previous best result for this task.
While in their work they report 80% accuracy with
the CFG model, this is for a single sampling of the
full data set. We observed a large variance in clas-
sification accuracy over such samplings, which in-
cludes some values in their reported range but with
a much lower mean. The numbers we report are
from our own implementation of their CFG tech-
nique, and all results are averaged over 5 random
samplings from the full corpus.
For 2DOP we limit the 2DOP(F) fragments by
choosing the 7k with maximum frequency, but there
may exist superior methods. Indeed, Wong and
Dras (2011a) claims that Information Gain is a better
criteria. However, this metric requires a probabilis-
tic formulation of the grammar, which 2DOP does
not supply. Instead of experimenting with different
limiting metrics, we note that when all 400k rules
are used, the averaged accuracy is only 76.8 percent,
which still lags behind BTSG.
5.2 Robustness
We also investigated different classification strate-
gies, as binary indicators of fragment occurrence
over an entire document may lead to noisy results.
Consider a single outlier sentence in a document
with a single fragment that is indicative of the in-
correct L1 label. Note that it is just as important in
the eyes of the classifier as a fragment indicative of
the correct label that appears many times. To inves-
tigate this phenomena we classified individual sen-
tences, and used these results to vote for each docu-
ment level label in the test set.
We employed two voting schemes. In the first,
VoteOne, each sentence contributes one vote to its
maximum probability label. In the second, VoteAll,
the probability of each L1 label is contributed as a
partial vote. Neither method increases performance
Model VoteOne (%) VoteAll (%)
CFG 69.6 74.7
2DOP 69.1 73.5
BTSG 72.5 76.5
Table 2: Sentence based classification accuracy
for BTSG or 2DOP, but what is more interesting
is that in both cases the CFG model outperforms
2DOP (with less than half of the features). The
robust behavior of the BTSG method shows that it
finds correctly discriminative features across several
sentences in each document to a greater extent than
other methods.
5.3 Concision
One possible explanation for the superior perfor-
mance of BTSG is that DDOP is prone to yielding
multiple fragments that represent the same linguistic
phenomena, leading to sets of highly correlated fea-
tures. While correlated features are not crippling to
a logistic regression model, they add computational
complexity without contributing to higher classifica-
tion accuracy.
To address this hypothesis empirically, we con-
sidered pairs of fragments eA and eB and calcu-
lated the pointwise mutual information (PMI) be-
tween events signifying their occurrence in a sen-
tence. For BTSG, the average pointwise mutual in-
formation over all pairs (eA, eB) is ?.14, while for
2DOP it is ?.01. As increasingly negative values
of PMI indicate exclusivity, this supports the claim
that DDOP?s comparative weakness is to some ex-
tent due to feature redundancy.
6 Conclusion
In this work we investigate automatically induced
TSG fragments as classification features for native
language detection. We compare Bayesian and Dou-
bleDOP induced features and find that the former
represents the data with less redundancy, is more ro-
bust to classification strategy, and gives higher clas-
sification accuracy. Additionally, the Bayesian TSG
features give a new best result for the task of native
language detection.
196
References
Mohit Bansal and Dan Klein 2010. Simple, accurate
parsing with an all-fragments grammar. Association
for Computational Linguistics.
Phil Blunsom and Trevor Cohn 2010. Unsupervised
Induction of Tree Substitution Grammars for Depen-
dency Parsing. Empirical Methods in Natural Lan-
guage Processing.
Rens Bod 1991. A Computational Model of Language
Performance: Data Oriented Parsing. Computational
Linguistics in the Netherlands.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing Compact but Accurate Tree-
Substitution Grammars. In Proceedings NAACL.
Trevor Cohn, and Phil Blunsom 2010. Blocked inference
in Bayesian tree substitution grammars. Association
for Computational Linguistics.
Michael Collins, Nigel Duffy 2001. Convolution Ker-
nels for Natural Language. Advances in Neural Infor-
mation Processing Systems.
Joshua Goodman 2003. Efficient parsing of DOP with
PCFG-reductions. In Bod et al chapter 8..
S. Granger, E. Dagneaux and F. Meunier. 2002. Interna-
tional Corpus of Learner English, (ICLE).
Sangkyum Kim, Hyungsul Kim, Tim Weninger, and Ji-
awei Han 2010. Authorship classification: a syn-
tactic tree mining approach. Proceedings of the ACM
SIGKDD Workshop on Useful Patterns.
Koppel, Moshe and Schler, Jonathan and Zigdon, Kfir.
2005. Determining an author?s native language by
mining a text for errors. Proceedings of the eleventh
ACM SIGKDD international conference on Knowl-
edge discovery in data mining.
Alessandro Moschitti, Daniele Pighin and Roberto Basili
2008. Tree Kernels for Semantic Role Labeling. Com-
putational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. Association for Compu-
tational Linguistics.
Matt Post and Daniel Gildea. 2009. Bayesian Learning
of a Tree Substitution Grammar. Association for Com-
putational Linguistics.
Matt Post. 2011. Judging Grammaticality with Tree Sub-
stitution Grammar Derivations. Association for Com-
putational Linguistics.
Sindhu Raghavan, Adriana Kovashka and Raymond
Mooney 2010. Authorship attribution using proba-
bilistic context-free grammars. Association for Com-
putational Linguistics.
Sangati, Federico and Zuidema, Willem 2011. Accurate
Parsing with Compact Tree-Substitution Grammars:
Double-DOP. Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing.
Jun Suzuki and Hideki Isozaki 2006. Sequence and tree
kernels with statistical feature mining. Advances in
Neural Information Processing Systems.
Sze-Meng Jojo Wong and Mark Dras 2011. Exploit-
ing Parse Structures for Native Language Identifica-
tion. Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing.
Sze-Meng Jojo Wong and Mark Dras 2011. Topic Mod-
eling for Native Language Identification. Proceedings
of the Australasian Language Technology Association
Workshop.
Elif Yamangil, Stuart M. Shieber 2010. Bayesian Syn-
chronous Tree-Substitution Grammar Induction and
Its Application to Sentence Compression.. Associa-
tion for Computational Linguistics.
197
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 302?310,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Context Free TAG Variant
Ben Swanson
Brown University
Providence, RI
chonger@cs.brown.edu
Eugene Charniak
Brown University
Providence, RI
ec@cs.brown.edu
Elif Yamangil
Harvard University
Cambridge, MA
elif@eecs.harvard.edu
Stuart Shieber
Harvard University
Cambridge, MA
shieber@eecs.harvard.edu
Abstract
We propose a new variant of Tree-
Adjoining Grammar that allows adjunc-
tion of full wrapping trees but still bears
only context-free expressivity. We provide
a transformation to context-free form, and
a further reduction in probabilistic model
size through factorization and pooling of
parameters. This collapsed context-free
form is used to implement efficient gram-
mar estimation and parsing algorithms.
We perform parsing experiments the Penn
Treebank and draw comparisons to Tree-
Substitution Grammars and between dif-
ferent variations in probabilistic model de-
sign. Examination of the most probable
derivations reveals examples of the lin-
guistically relevant structure that our vari-
ant makes possible.
1 Introduction
While it is widely accepted that natural language
is not context-free, practical limitations of ex-
isting algorithms motivate Context-Free Gram-
mars (CFGs) as a good balance between model-
ing power and asymptotic performance (Charniak,
1996). In constituent-based parsing work, the pre-
vailing technique to combat this divide between
efficient models and real world data has been to
selectively strengthen the dependencies in a CFG
by increasing the grammar size through methods
such as symbol refinement (Petrov et al, 2006).
Another approach is to employ a more power-
ful grammatical formalism and devise constraints
and transformations that allow use of essential ef-
ficient algorithms such as the Inside-Outside al-
gorithm (Lari and Young, 1990) and CYK pars-
ing. Tree-Adjoining Grammar (TAG) is a natural
starting point for such methods as it is the canoni-
cal member of the mildly context-sensitive family,
falling just above CFGs in the hierarchy of for-
mal grammars. TAG has a crucial advantage over
CFGs in its ability to represent long distance in-
teractions in the face of the interposing variations
that commonly manifest in natural language (Joshi
and Schabes, 1997). Consider, for example, the
sentences
These pretzels are making me thirsty.
These pretzels are not making me thirsty.
These pretzels that I ate are making me thirsty.
Using a context-free language model with
proper phrase bracketing, the connection between
the words pretzels and thirsty must be recorded
with three separate patterns, which can lead to
poor generalizability and unreliable sparse fre-
quency estimates in probabilistic models. While
these problems can be overcome to some extent
with large amounts of data, redundant representa-
tion of patterns is particularly undesirable for sys-
tems that seek to extract coherent and concise in-
formation from text.
TAG allows a linguistically motivated treatment
of the example sentences above by generating the
last two sentences through modification of the
first, applying operations corresponding to nega-
tion and the use of a subordinate clause. Un-
fortunately, the added expressive power of TAG
comes with O(n6) time complexity for essential
algorithms on sentences of length n, as opposed to
O(n3) for the CFG (Schabes, 1990). This makes
TAG infeasible to analyze real world data in a rea-
sonable time frame.
In this paper, we define OSTAG, a new way to
constrain TAG in a conceptually simple way so
302
SNP VP NP
NP
DT
the
NN
lack
NP
NNS
computers
VP
VBP
do
RB
not
VP
NP
NP PP
NP
PRP
I
PP
IN
of
PRP
them
VP
VB
fear
Figure 1: A simple Tree-Substitution Grammar using S as its start symbol. This grammar derives the
sentences from a quote of Isaac Asimov?s - ?I do not fear computers. I fear the lack of them.?
that it can be reduced to a CFG, allowing the use of
traditional cubic-time algorithms. The reduction is
reversible, so that the original TAG derivation can
be recovered exactly from the CFG parse. We pro-
vide this reduction in detail below and highlight
the compression afforded by this TAG variant on
synthetic formal languages.
We evaluate OSTAG on the familiar task of
parsing the Penn Treebank. Using an automati-
cally induced Tree-Substitution Grammar (TSG),
we heuristically extract an OSTAG and estimate
its parameters from data using models with var-
ious reduced probabilistic models of adjunction.
We contrast these models and investigate the use
of adjunction in the most probable derivations of
the test corpus, demonstating the superior model-
ing performance of OSTAG over TSG.
2 TAG and Variants
Here we provide a short history of the relevant
work in related grammar formalisms, leading up
to a definition of OSTAG. We start with context-
free grammars, the components of which are
?N,T,R, S?, where N and T are the sets of non-
terminal and terminal symbols respectively, and S
is a distinguished nonterminal, the start symbol.
The rules R can be thought of as elementary trees
of depth 1, which are combined by substituting a
derived tree rooted at a nonterminalX at some leaf
node in an elementary tree with a frontier node
labeled with that same nonterminal. The derived
trees rooted at the start symbol S are taken to be
the trees generated by the grammar.
2.1 Tree-Substitution Grammar
By generalizing CFG to allow elementary trees in
R to be of depth greater than or equal to 1, we
get the Tree-Substitution Grammar. TSG remains
in the family of context-free grammars, as can be
easily seen by the removal of the internal nodes
in all elementary trees; what is left is a CFG that
generates the same language. As a reversible al-
ternative that preserves the internal structure, an-
notation of each internal node with a unique index
creates a large number of deterministic CFG rules
that record the structure of the original elementary
trees. A more compact CFG representation can be
obtained by marking each node in each elemen-
tary tree with a signature of its subtree. This trans-
form, presented by Goodman (2003), can rein in
the grammar constant G, as the crucial CFG algo-
rithms for a sentence of length n have complexity
O(Gn3).
A simple probabilistic model for a TSG is a set
of multinomials, one for each nonterminal in N
corresponding to its possible substitutions in R. A
more flexible model allows a potentially infinite
number of substitution rules using a Dirichlet Pro-
cess (Cohn et al, 2009; Cohn and Blunsom, 2010).
This model has proven effective for grammar in-
duction via Markov Chain Monte Carlo (MCMC),
in which TSG derivations of the training set are re-
peatedly sampled to find frequently occurring el-
ementary trees. A straightforward technique for
induction of a finite TSG is to perform this non-
parametric induction and select the set of rules that
appear in at least one sampled derivation at one or
several of the final iterations.
2.2 Tree-Adjoining Grammar
Tree-adjoining grammar (TAG) (Joshi, 1985;
Joshi, 1987; Joshi and Schabes, 1997) is an exten-
sion of TSG defined by a tuple ?N,T,R,A, S?,
and differs from TSG only in the addition of a
303
VP
always VP
VP* quickly
+ S
NP VP
runs
? S
NP VP
always VP
VP
runs
quickly
Figure 2: The adjunction operation combines the auxiliary tree (left) with the elementary tree (middle)
to form a new derivation (right). The adjunction site is circled, and the foot node of the auxiliary tree is
denoted with an asterisk. The OSTAG constraint would disallow further adjunction at the bold VP node
only, as it is along the spine of the auxiliary tree.
set of auxiliary trees A and the adjunction oper-
ation that governs their use. An auxiliary tree ?
is an elementary tree containing a single distin-
guished nonterminal leaf, the foot node, with the
same symbol as the root of ?. An auxiliary tree
with root and foot node X can be adjoined into an
internal node of an elementary tree labeled with
X by splicing the auxiliary tree in at that internal
node, as pictured in Figure 2. We refer to the path
between the root and foot nodes in an auxiliary
tree as the spine of the tree.
As mentioned above, the added power afforded
by adjunction comes at a serious price in time
complexity. As such, probabilistic modeling for
TAG in its original form is uncommon. However,
a large effort in non-probabilistic grammar induc-
tion has been performed through manual annota-
tion with the XTAG project(Doran et al, 1994).
2.3 Tree Insertion Grammar
Tree Insertion Grammars (TIGs) are a longstand-
ing compromise between the intuitive expressivity
of TAG and the algorithmic simplicity of CFGs.
Schabes and Waters (1995) showed that by re-
stricting the form of the auxiliary trees in A and
the set of auxiliary trees that may adjoin at par-
ticular nodes, a TAG generates only context-free
languages. The TIG restriction on auxiliary trees
states that the foot node must occur as either the
leftmost or rightmost leaf node. This introduces
an important distinction between left, right, and
wrapping auxiliary trees, of which only the first
two are allowed in TIG. Furthermore, TIG disal-
lows adjunction of left auxiliary trees on the spines
of right auxiliary trees, and vice versa. This is
to prevent the construction of wrapping auxiliary
trees, whose removal is essential for the simplified
complexity of TIG.
Several probabilistic models have been pro-
posed for TIG. While earlier approaches such as
Hwa (1998) and Chiang (2000) relied on hueristic
induction methods, they were nevertheless sucess-
ful at parsing. Later approaches (Shindo et al,
2011; Yamangil and Shieber, 2012) were able to
extend the non-parametric modeling of TSGs to
TIG, providing methods for both modeling and
grammar induction.
2.4 OSTAG
Our new TAG variant is extremely simple. We al-
low arbitrary initial and auxiliary trees, and place
only one restriction on adjunction: we disallow
adjunction at any node on the spine of an aux-
iliary tree below the root (though we discuss re-
laxing that constraint in Section 4.2). We refer to
this variant as Off Spine TAG (OSTAG) and note
that it allows the use of full wrapping rules, which
are forbidden in TIG. This targeted blocking of
recursion has similar motivations and benefits to
the approximation of CFGs with regular languages
(Mohri and jan Nederhof, 2000).
The following sections discuss in detail the
context-free nature of OSTAG and alternative
probabilistic models for its equivalent CFG form.
We propose a simple but empirically effective
heuristic for grammar induction for our experi-
ments on Penn Treebank data.
3 Transformation to CFG
To demonstrate that OSTAG has only context-
free power, we provide a reduction to context-free
grammar. Given an OSTAG ?N,T,R,A, S?, we
define the set N of nodes of the corresponding
CFG to be pairs of a tree inR orA together with an
304
?: S
T
x
T
y
?: T
a T* a
?: T
b T* b
S ? X Y S ? X Y
X ? x X ? x
Y ? y Y ? y
X ? A
X ? B
Y ? A?
Y ? B?
A ? a X ? a X ? a X a
A? ? a Y ? a Y ? a Y a
X ? ? X
Y ? ? Y
B ? b X ?? b X ? b X b
B? ? b Y ?? b Y ? b Y b
X ?? ? X
Y ?? ? Y
(a) (b) (c)
Figure 3: (a) OSTAG for the language wxwRvyvR where w, v ? {a|b}+ and R reverses a string. (b) A
CFG for the same language, which of necessity must distinguish between nonterminalsX and Y playing
the role of T in the OSTAG. (c) Simplified CFG, conflating nonterminals, but which must still distinguish
between X and Y .
address (Gorn number (Gorn, 1965)) in that tree.
We take the nonterminals of the target CFG gram-
mar to be nodes or pairs of nodes, elements of the
setN +N ?N . We notate the pairs of nodes with
a kind of ?applicative? notation. Given two nodes
? and ??, we notate a target nonterminal as ?(??).
Now for each tree ? and each interior node ?
in ? that is not on the spine of ? , with children
?1, . . . , ?k, we add a context-free rule to the gram-
mar
? ? ?1 ? ? ? ?k (1)
and if interior node ? is on the spine of ? with
?s the child node also on the spine of ? (that is,
dominating the foot node of ? ) and ?? is a node (in
any tree) where ? is adjoinable, we add a rule
?(??)? ?1 ? ? ? ?s(??) ? ? ? ?k . (2)
Rules of type (1) handle the expansion of a node
not on the spine of an auxiliary tree and rules of
type (2) a spinal node.
In addition, to initiate adjunction at any node ??
where a tree ? with root ? is adjoinable, we use a
rule
?? ? ?(??) (3)
and for the foot node ?f of ? , we use a rule
?f (?)? ? (4)
The OSTAG constraint follows immediately
from the structure of the rules of type (2). Any
child spine node ?s manifests as a CFG nonter-
minal ?s(??). If child spine nodes themselves al-
lowed adjunction, we would need a type (3) rule
of the form ?s(??) ? ?s(??)(???). This rule itself
would feed adjunction, requiring further stacking
of nodes, and an infinite set of CFG nonterminals
and rules. This echoes exactly the stacking found
in the LIG reduction of TAG .
To handle substitution, any frontier node ? that
allows substitution of a tree rooted with node ??
engenders a rule
? ? ?? (5)
This transformation is reversible, which is to
say that each parse tree derived with this CFG im-
plies exactly one OSTAG derivation, with substi-
tutions and adjunctions coded by rules of type (5)
and (3) respectively. Depending on the definition
of a TAG derivation, however, the converse is not
necessarily true. This arises from the spurious am-
biguity between adjunction at a substitution site
(before applying a type (5) rule) versus the same
adjunction at the root of the substituted initial tree
(after applying a type (5) rule). These choices
lead to different derivations in CFG form, but their
TAG derivations can be considered conceptually
305
identical. To avoid double-counting derivations,
which can adversely effect probabilistic modeling,
type (3) and type (4) rules in which the side with
the unapplied symbol is a nonterminal leaf can be
omitted.
3.1 Example
The grammar of Figure 3(a) can be converted to
a CFG by this method. We indicate for each CFG
rule its type as defined above the production arrow.
All types are used save type (5), as substitution
is not employed in this example. For the initial
tree ?, we have the following generated rules (with
nodes notated by the tree name and a Gorn number
subscript):
? 1?? ?1 ?2 ?1 3?? ?(?1)
?1 1?? x ?1 3?? ?(?1)
?2 1?? y ?2 3?? ?(?2)
?2 3?? ?(?2)
For the auxiliary trees ? and ? we have:
?(?1) 2?? a ?1(?1) a
?(?2) 2?? a ?1(?2) a
?1(?1) 4?? ?1
?1(?2) 4?? ?2
?(?1) 2?? b ?1(?1) b
?(?2) 2?? b ?1(?2) b
?1(?1) 4?? ?1
?1(?2) 4?? ?2
The grammar of Figure 3(b) is simply a renaming
of this grammar.
4 Applications
4.1 Compact grammars
The OSTAG framework provides some leverage in
expressing particular context-free languages more
compactly than a CFG or even a TSG can. As
an example, consider the language of bracketed
palindromes
Pal = aiw aiwR ai
1 ? i ? k
w ? {bj | 1 ? j ? m}?
containing strings like a2 b1b3 a2 b3b1 a2. Any
TSG for this language must include as substrings
some subpalindrome constituents for long enough
strings. Whatever nonterminal covers such a
string, it must be specific to the a index within
it, and must introduce at least one pair of bs as
well. Thus, there are at least m such nontermi-
nals, each introducing at least k rules, requiring at
least km rules overall. The simplest such gram-
mar, expressed as a CFG, is in Figure 4(a). The
ability to use adjunction allows expression of the
same language as an OSTAG with k +m elemen-
tary trees (Figure 4(b)). This example shows that
an OSTAG can be quadratically smaller than the
corresponding TSG or CFG.
4.2 Extensions
The technique in OSTAG can be extended to ex-
pand its expressiveness without increasing gener-
ative capacity.
First, OSTAG allows zero adjunctions on each
node on the spine below the root of an auxiliary
tree, but any non-zero finite bound on the num-
ber of adjunctions allowed on-spine would simi-
larly limit generative capacity. The tradeoff is in
the grammar constant of the effective probabilis-
tic CFG; an extension that allows k levels of on
spine adjunction has a grammar constant that is
O(|N |(k+2)).
Second, the OSTAG form of adjunction is con-
sistent with the TIG form. That is, we can extend
OSTAG by allowing on-spine adjunction of left- or
right-auxiliary trees in keeping with the TIG con-
straints without increasing generative capacity.
4.3 Probabilistic OSTAG
One major motivation for adherence to a context-
free grammar formalism is the ability to employ
algorithms designed for probabilistic CFGs such
as the CYK algorithm for parsing or the Inside-
Outside algorithm for grammar estimation. In this
section we present a probabilistic model for an OS-
TAG grammar in PCFG form that can be used in
such algorithms, and show that many parameters
of this PCFG can be pooled or set equal to one and
ignored. References to rules of types (1-5) below
refer to the CFG transformation rules defined in
Section 3. While in the preceeding discussion we
used Gorn numbers for clarity, our discussion ap-
plies equally well for the Goodman transform dis-
cussed above, in which each node is labeled with a
signature of its subtree. This simply redefines ? in
the CFG reduction described in Section 3 to be a
subtree indicator, and dramatically reduces redun-
dancy in the generated grammar.
306
S ? ai Ti ai
Ti ? bj Ti bj
Ti ? ai
?i | 1 ? i ? k: S
ai T
ai
ai
?j | 1 ? j ? m: T
bj T* bj
(a) (b)
Figure 4: A CFG (a) and more compact OSTAG (b) for the language Pal
The first practical consideration is that CFG
rules of type (2) are deterministic, and as such
we need only record the rule itself and no asso-
ciated parameter. Furthermore, these rules employ
a template in which the stored symbol appears in
the left-hand side and in exactly one symbol on
the right-hand side where the spine of the auxil-
iary tree proceeds. One deterministic rule exists
for this template applied to each ?, and so we may
record only the template. In order to perform CYK
or IO, it is not even necessary to record the index
in the right-hand side where the spine continues;
these algorithms fill a chart bottom up and we can
simply propagate the stored nonterminal up in the
chart.
CFG rules of type (4) are also deterministic and
do not require parameters. In these cases it is not
necessary to record the rules, as they all have ex-
actly the same form. All that is required is a check
that a given symbol is adjoinable, which is true for
all symbols except nonterminal leaves and applied
symbols. Rules of type (5) are necessary to cap-
ture the probability of substitution and so we will
require a parameter for each.
At first glance, it would seem that due to the
identical domain of the left-hand sides of rules of
types (1) and (3) a parameter is required for each
such rule. To avoid this we propose the follow-
ing factorization for the probabilistic expansion of
an off spine node. First, a decision is made as to
whether a type (1) or (3) rule will be used; this cor-
responds to deciding if adjunction will or will not
take place at the node. If adjunction is rejected,
then there is only one type (1) rule available, and
so parameterization of type (1) rules is unneces-
sary. If we decide on adjunction, one of the avail-
able type (3) rules is chosen from a multinomial.
By conditioning the probability of adjunction on
varying amounts of information about the node,
alternative models can easily be defined.
5 Experiments
As a proof of concept, we investigate OSTAG in
the context of the classic Penn Treebank statistical
parsing setup; training on section 2-21 and testing
on section 23. For preprocessing, words that oc-
cur only once in the training data are mapped to
the unknown categories employed in the parser of
Petrov et al (2006). We also applied the annota-
tion from Klein and Manning (2003) that appends
?-U? to each nonterminal node with a single child,
drastically reducing the presence of looping unary
chains. This allows the use of a coarse to fine
parsing strategy (Charniak et al, 2006) in which
a sentence is first parsed with the Maximum Like-
lihood PCFG and only constituents whose prob-
ability exceeds a cutoff of 10?4 are allowed in
the OSTAG chart. Designed to facilitate sister ad-
junction, we define our binarization scheme by ex-
ample in which the added nodes, indicated by @,
record both the parent and head child of the rule.
NP
@NN-NP
@NN-NP
DT @NN-NP
JJ NN
SBAR
A compact TSG can be obtained automatically
using the MCMC grammar induction technique of
Cohn and Blunsom (2010), retaining all TSG rules
that appear in at least one derivation in after 1000
iterations of sampling. We use EM to estimate the
parameters of this grammar on sections 2-21, and
use this as our baseline.
To generate a set of TAG rules, we consider
each rule in our baseline TSG and find all possi-
307
All 40 #Adj #Wrap
TSG 85.00 86.08 ? ?
TSG? 85.12 86.21 ? ?
OSTAG1 85.42 86.43 1336 52
OSTAG2 85.54 86.56 1952 44
OSTAG3 85.86 86.84 3585 41
Figure 5: Parsing F-Score for the models under
comparison for both the full test set and sentences
of length 40 or less. For the OSTAG models, we
list the number of adjunctions in the MPD of the
full test set, as well as the number of wrapping
adjunctions.
ble auxiliary root and foot node pairs it contains.
For each such root/foot pair, we include the TAG
rule implied by removal of the structure above the
root and below the foot. We also include the TSG
rule left behind when the adjunction of this auxil-
iary tree is removed. To be sure that experimental
gains are not due to this increased number of TSG
initial trees, we calculate parameters using EM for
this expanded TSG and use it as a second base-
line (TSG?). With our full set of initial and aux-
iliary trees, we use EM and the PCFG reduction
described above to estimate the parameters of an
OSTAG.
We investigate three models for the probabil-
ity of adjunction at a node. The first uses a con-
servative number of parameters, with a Bernoulli
variable for each symbol (OSTAG1). The second
employs more parameters, conditioning on both
the node?s symbol and the symbol of its leftmost
child (OSTAG2).The third is highly parameterized
but most prone to data sparsity, with a separate
Bernoulli distribution for each Goodman index ?
(OSTAG3). We report results for Most Probable
Derivation (MPD) parses of section 23 in Figure
5.
Our results show that OSTAG outperforms both
baselines. Furthermore, the various parameteri-
zations of adjunction with OSTAG indicate that,
at least in the case of the Penn Treebank, the
finer grained modeling of a full table of adjunction
probabilities for each Goodman index OSTAG3
overcomes the danger of sparse data estimates.
Not only does such a model lead to better parsing
performance, but it uses adjunction more exten-
sively than its more lightly parameterized alterna-
tives. While different representations make direct
comparison inappropriate, the OSTAG results lie
in the same range as previous work with statistical
TIG on this task, such as Chiang (2000) (86.00)
and Shindo et al (2011) (85.03).
The OSTAG constraint can be relaxed as de-
scribed in Section 4.2 to allow any finite number of
on-spine adjunctions without sacrificing context-
free form. However, the increase to the grammar
constant quickly makes parsing with such models
an arduous task. To determine the effect of such a
relaxation, we allow a single level of on-spine ad-
junction using the adjunction model of OSTAG1,
and estimate this model with EM on the training
data. We parse sentences of length 40 or less in
section 23 and observe that on-spine adjunction is
never used in the MPD parses. This suggests that
the OSTAG constraint is reasonable, at least for
the domain of English news text.
We performed further examination of the MPD
using OSTAG for each of the sentences in the test
corpus. As an artifact of the English language, the
majority have their foot node on the left spine and
would also be usable by TIG, and so we discuss
the instances of wrapping auxiliary trees in these
derivations that are uniquely available to OSTAG.
We remove binarization for clarity and denote the
foot node with an asterisk.
A frequent use of wrapping adjunction is to co-
ordinate symbols such as quotes, parentheses, and
dashes on both sides of a noun phrase. One com-
mon wrapping auxiliary tree in our experiments is
NP
? NP* ? PP
This is used frequently in the news text of
the Wall Street Journal for reported speech when
avoiding a full quotation. This sentence is an ex-
ample of the way the rule is employed, using what
Joshi and Schabes (1997) referred to as ?factoring
recursion from linguistic constraints? with TAG.
Note that replacing the quoted noun phrase and
its following prepositional phrase with the noun
phrase itself yields a valid sentence, in line with
the linguistic theory underlying TAG.
Another frequent wrapping rule, shown below,
allows direct coordination between the contents of
an appositive with the rest of the sentence.
308
NP
NP , CC
or
NP* ,
This is a valuable ability, as it is common to
use an appositive to provide context or explanation
for a proper noun. As our information on proper
nouns will most likely be very sparse, the apposi-
tive may be more reliably connected to the rest of
the sentence. An example of this from one of the
sentences in which this rule appears in the MPD is
the phrase ?since the market fell 156.83, or 8 %,
a week after Black Monday?. The wrapping rule
allows us to coordinate the verb ?fell? with the pat-
tern ?X %? instead of 156.83, which is mapped to
an unknown word category.
These rules highlight the linguistic intuitions
that back TAG; if their adjunction were undone,
the remaining derivation would be a valid sen-
tence that simply lacks the modifying structure of
the auxiliary tree. However, the MPD parses re-
veal that not all useful adjunctions conform to this
paradigm, and that left-auxiliary trees that are not
used for sister adjunction are susceptible to this
behavior. The most common such tree is used to
create noun phrases such as
P&G?s share of [the Japanese market]
the House?s repeal of [a law]
Apple?s family of [Macintosh Computers]
Canada?s output of [crude oil]
by adjoining the shared unbracketed syntax onto
the NP dominating the bracketed text. If adjunc-
tion is taken to model modification, this rule dras-
tically changes the semantics of the unmodified
sentence. Furthermore, in some cases removing
the adjunction can leave a grammatically incorrect
sentence, as in the third example where the noun
phrase changes plurality.
While our grammar induction method is a crude
(but effective) heuristic, we can still highlight the
qualities of the more important auxiliary trees
by examining aggregate statistics over the MPD
parses, shown in Figure 6. The use of left-
auxiliary trees for sister adjunction is a clear trend,
as is the predominant use of right-auxiliary trees
for the complementary set of ?regular? adjunc-
tions, which is to be expected in a right branch-
ing language such as English. The statistics also
All Wrap Right Left
Total 3585 (1374) 41 (26) 1698 (518) 1846 (830)
Sister 2851 (1180) 17 (11) 1109 (400) 1725 (769)
Lex 2244 (990) 28 (19) 894 (299) 1322 (672)
FLex 1028 (558) 7 (2) 835 (472) 186 (84)
Figure 6: Statistics for MPD auxiliary trees us-
ing OSTAG3. The columns indicate type of aux-
iliary tree and the rows correspond respectively to
the full set found in the MPD, those that perform
sister adjunction, those that are lexicalized, and
those that are fully lexicalized. Each cell shows
the number of tokens followed by the number of
types of auxiliary tree that fit its conditions.
reflect the importance of substitution in right-
auxiliary trees, as they must capture the wide va-
riety of right branching modifiers of the English
language.
6 Conclusion
The OSTAG variant of Tree-Adjoining Grammar
is a simple weakly context-free formalism that
still provides for all types of adjunction and is
a bit more concise than TSG (quadratically so).
OSTAG can be reversibly transformed into CFG
form, allowing the use of a wide range of well
studied techniques in statistical parsing.
OSTAG provides an alternative to TIG as a
context-free TAG variant that offers wrapping ad-
junction in exchange for recursive left/right spine
adjunction. It would be interesting to apply both
OSTAG and TIG to different languages to deter-
mine where the constraints of one or the other are
more or less appropriate. Another possibility is the
combination of OSTAG with TIG, which would
strictly expand the abilities of both approaches.
The most important direction of future work for
OSTAG is the development of a principled gram-
mar induction model, perhaps using the same tech-
niques that have been successfully applied to TSG
and TIG. In order to motivate this and other re-
lated research, we release our implementation of
EM and CYK parsing for OSTAG1. Our system
performs the CFG transform described above and
optionally employs coarse to fine pruning and re-
laxed (finite) limits on the number of spine adjunc-
tions. As a TSG is simply a TAG without adjunc-
tion rules, our parser can easily be used as a TSG
estimator and parser as well.
1bllip.cs.brown.edu/download/bucketparser.tar
309
References
Eugene Charniak, Mark Johnson, Micha Elsner,
Joseph L. Austerweil, David Ellis, Isaac Hax-
ton, Catherine Hill, R. Shrivaths, Jeremy Moore,
Michael Pozar, and Theresa Vu. 2006. Multilevel
coarse-to-fine PCFG parsing. In North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.
Eugene Charniak. 1996. Tree-bank grammars. In As-
sociation for the Advancement of Artificial Intelli-
gence, pages 1031?1036.
David Chiang. 2000. Statistical parsing with
an automatically-extracted tree adjoining grammar.
Association for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2010. Blocked infer-
ence in bayesian tree substitution grammars. pages
225?230. Association for Computational Linguis-
tics.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 548?556.
Association for Computational Linguistics.
Christy Doran, Dania Egedi, Beth Ann Hockey, Banga-
lore Srinivas, and Martin Zaidel. 1994. XTAG sys-
tem: a wide coverage grammar for English. pages
922?928. Association for Computational Linguis-
tics.
J. Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. Bod et al 2003.
Saul Gorn. 1965. Explicit definitions and linguistic
dominoes. In Systems and Computer Science, pages
77?115.
Rebecca Hwa. 1998. An empirical evaluation of prob-
abilistic lexicalized tree insertion grammars. In Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics,
pages 557?563. Association for Computational Lin-
guistics.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages, vol-
ume 3, pages 69?124. Springer.
Aravind K Joshi. 1985. Tree adjoining grammars:
How much context-sensitivity is required to provide
reasonable structural descriptions? University of
Pennsylvania.
Aravind K Joshi. 1987. An introduction to tree ad-
joining grammars. Mathematics of Language, pages
87?115.
Dan Klein and Christopher D Manning. 2003. Accu-
rate unlexicalized parsing. pages 423?430. Associ-
ation for Computational Linguistics.
K. Lari and S. J. Young. 1990. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
pages 35?56.
Mehryar Mohri and Mark jan Nederhof. 2000. Regu-
lar approximation of context-free grammars through
transformation. In Robustness in language and
speech technology.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440. Association for Computational Linguistics.
Yves Schabes and Richard C. Waters. 1995. Tree
insertion grammar: a cubic-time, parsable formal-
ism that lexicalizes context-free grammar without
changing the trees produced. Computational Lin-
guistics, (4):479?513.
Yves Schabes. 1990. Mathematical and computa-
tional aspects of lexicalized grammars. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA, USA.
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.
2011. Insertion operator for bayesian tree substi-
tution grammars. pages 206?211. Association for
Computational Linguistics.
Elif Yamangil and Stuart M. Shieber. 2012. Estimat-
ing compact yet rich tree insertion grammars. pages
110?114. Association for Computational Linguis-
tics.
310
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 512?516,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Humans Require Context to Infer Ironic Intent
(so Computers Probably do, too)
Byron C. Wallace, Do Kook Choe, Laura Kertz and Eugene Charniak
Brown University
{byron wallace, do kook choe, laura kertz, eugene charniak}@brown.edu
Abstract
Automatically detecting verbal irony
(roughly, sarcasm) is a challenging task
because ironists say something other
than ? and often opposite to ? what they
actually mean. Discerning ironic intent
exclusively from the words and syntax
comprising texts (e.g., tweets, forum
posts) is therefore not always possible:
additional contextual information about
the speaker and/or the topic at hand is
often necessary. We introduce a new
corpus that provides empirical evidence
for this claim. We show that annota-
tors frequently require context to make
judgements concerning ironic intent, and
that machine learning approaches tend
to misclassify those same comments for
which annotators required additional
context.
1 Introduction & Motivation
This work concerns the task of detecting verbal
irony online. Our principal argument is that sim-
ple bag-of-words based text classification models
? which, when coupled with sufficient data, have
proven to be extremely successful for many natu-
ral language processing tasks (Halevy et al, 2009)
? are inadequate for irony detection. In this paper
we provide empirical evidence that context is often
necessary to recognize ironic intent.
This is consistent with the large body of prag-
matics/linguistics literature on irony and its us-
age, which has emphasized the role that context
plays in recognizing and decoding ironic utter-
ances (Grice, 1975; Clark and Gerrig, 1984; Sper-
ber and Wilson, 1981). But existing work on au-
tomatic irony detection ? reviewed in Section 2
? has not explicitly attempted to operationalize
such theories, and has instead relied on features
(mostly word counts) intrinsic to the texts that are
to be classified as ironic. These approaches have
achieved some success, but necessarily face an
upper-bound: the exact same sentence can be both
intended ironically and unironically, depending on
the context (including the speaker and the topic at
hand). Only obvious verbal ironies will be recog-
nizable from intrinsic features alone.
Here we provide empirical evidence for the
above claims. We also introduce a new annotated
corpus that will allow researchers to build models
that augment existing approaches to irony detec-
tion with contextual information regarding the text
(utterance) to be classified and its author. Briefly,
our contributions are summarized as follows.
? We introduce the first version of the reddit
irony corpus, composed of annotated com-
ments from the social news website reddit.
Each sentence in every comment in this cor-
pus has been labeled by three independent an-
notators as having been intended by the au-
thor ironically or not. This dataset is publicly
available.
1
? We provide empirical evidence that human
annotators consistently rely on contextual in-
formation to make ironic/unironic sentence
judgements.
? We show that the standard ?bag-of-words? ap-
proach to text classification fails to accurately
judge ironic intent on those cases for which
humans required additional context. This
suggests that, as humans require context to
make their judgements for this task, so too do
computers.
Our hope is that these observations and this
dataset will spur innovative new research on meth-
ods for verbal irony detection.
1
https://github.com/bwallace/
ACL-2014-irony
512
2 Previous Work
There has recently been a flurry of interesting
work on automatic irony detection (Tepperman
et al, 2006; Davidov et al, 2010; Carvalho et
al., 2009; Burfoot and Baldwin, 2009; Tsur et
al., 2010; Gonz?alez-Ib?a?nez et al, 2011; Filatova,
2012; Reyes et al, 2012; Lukin and Walker, 2013;
Riloff et al, 2013). In these works, verbal irony
detection has mostly been treated as a standard
text classification task, though with some innova-
tive approaches specific to detecting irony.
The most common data source used to experi-
ment with irony detection systems has been Twit-
ter (Reyes et al, 2012; Gonz?alez-Ib?a?nez et al,
2011; Davidov et al, 2010), though Amazon prod-
uct reviews have been used experimentally as well
(Tsur et al, 2010; Davidov et al, 2010; Reyes et
al., 2012; Filatova, 2012). Walker et al (2012)
also recently introduced the Internet Argument
Corpus (IAC), which includes a sarcasm label
(among others).
Some of the findings from these previous ef-
forts have squared with intuition: e.g., overzealous
punctuation (as in ?great idea!!!!?) is indicative of
ironic intent (Carvalho et al, 2009). Other works
have proposed novel approaches specifically for
irony detection: Davidov et al (2010), for ex-
ample, proposed a semi-supervised approach in
which they look for sentence templates indicative
of irony. Elsewhere, Riloff et al (2013) proposed
a method that exploits contrasting sentiment in the
same utterance to detect irony.
To our knowledge, however, no previous work
on irony detection has attempted to leverage
contextual information regarding the author or
speaker (external to the utterance). But this is nec-
essary in some cases, however. For example, in
the case of Amazon product reviews, knowing the
kinds of books that an individual typically likes
might inform our judgement: someone who tends
to read and review Dostoevsky is probably be-
ing ironic if she writes a glowing review of Twi-
light. Of course, many people genuinely do enjoy
Twilight and so if the review is written subtly it
will likely be difficult to discern the author?s in-
tent without this background. In the case of Twit-
ter, it is likely to be difficult to classify utterances
without considering the contextualizing exchange
of tweets (i.e., the conversation) to which they be-
long.
1
2
3
4
Figure 1: The web-based tool used by our annotators to la-
bel reddit comments. Enumerated interface elements are de-
scribed as follows: 1 the text of the comment to be anno-
tated ? sentences marked as ironic are highlighted; 2 buttons
to label sentences as ironic or unironic; 3 buttons to request
additional context (the embedding discussion thread or asso-
ciated webpage ? see Section 3.2); 4 radio button to provide
confidence in comment labels (low, medium or high).
3 Introducing the reddit Irony Dataset
Here we introduce the first version (? 1.0) of
our irony corpus. Reddit (http://reddit.
com) is a social-news website to which news
stories (and other links) are posted, voted on
and commented upon. The forum compo-
nent of reddit is extremely active: popular
posts often have well into 1000?s of user com-
ments. Reddit comprises ?sub-reddits?, which fo-
cus on specific topics. For example, http://
reddit.com/r/politics features articles
(and hence comments) centered around political
news. The current version of the corpus is avail-
able at: https://github.com/bwallace/
ACL-2014-irony. Data collection and annota-
tion is ongoing, so we will continue to release new
(larger) versions of the corpus in the future. The
present version comprises 3,020 annotated com-
ments scraped from the six subreddits enumerated
in Table 1. These comments in turn comprise a
total of 10,401 labeled sentences.
2
3.1 Annotation Process
Three university undergraduates independently
annotated each sentence in the corpus. More
specifically, annotators have provided binary ?la-
bels? for each sentence indicating whether or not
they (the annotator) believe it was intended by the
author ironically (or not). This annotation was
provided via a custom-built browser-based anno-
tation tool, shown in Figure 1.
We intentionally did not provide much guid-
ance to annotators regarding the criteria for what
2
We performed na??ve ?segmentation? of comments based
on punctuation.
513
sub-reddit (URL) description number of labeled comments
politics (r/politics) Political news and editorials; focus on the US. 873
conservative (r/conservative) A community for political conservatives. 573
progressive (r/progressive) A community for political progressives (liberals). 543
atheism (r/atheism) A community for non-believers. 442
Christianity (r/Christianity) News and viewpoints on the Christian faith. 312
technology (r/technology) Technology news and commentary. 277
Table 1: The six sub-reddits that we have downloaded comments from and the corresponding number of comments for which
we have acquired annotations in this ? version of the corpus. Note that we acquired labels at the sentence level, whereas the
counts above reflect comments, all of which contain at least one sentence.
constitutes an ?ironic? statement, for two reasons.
First, verbal irony is a notoriously slippery concept
(Gibbs and Colston, 2007) and coming up with an
operational definition to be consistently applied is
non-trivial. Second, we were interested in assess-
ing the extent of natural agreement between an-
notators for this task. The raw average agreement
between all annotators on all sentences is 0.844.
Average pairwise Cohen?s Kappa (Cohen, 1960)
is 0.341, suggesting fair to moderate agreement
(Viera and Garrett, 2005), as we might expect for
a subjective task like this one.
3.2 Context
Reddit is a good corpus for the irony detection
task in part because it provides a natural prac-
tical realization of the otherwise ill-defined con-
text for comments. In particular, each comment is
associated with a specific user (the author), and
we can view their previous comments. More-
over, comments are embedded within discussion
threads that pertain to the (usually external) con-
tent linked to in the corresponding submission (see
Figure 2). These pieces of information (previous
comments by the same user, the external link of
the embedding reddit thread, and the other com-
ments in this thread) constitute our context. All
of this is readily accessible. Labelers can opt to
request these pieces of context via the annotation
tool, and we record when they do so.
Consider the following example comment taken
from our dataset: ?Great idea on the talkathon
Cruz. Really made the republicans look like the
sane ones.? Did the author intend this statement
ironically, or was this a subtle dig on Senator
Ted Cruz? Without additional context it is diffi-
cult to know. And indeed, all three annotators re-
quested additional context for this comment. This
context at first suggests that the comment may
have been intended literally: it was posted in the
r/conservative subreddit (Ted Cruz is a conserva-
tive senator). But if we peruse the author?s com-
Figure 2: An illustrative reddit comment (highlighted). The
title (?Virginia Republican ...?) links to an article, providing
one example of contextualizing content. The conversational
thread in which this comment is embedded provides addi-
tional context. The comment in question was presumably in-
tended ironically, though without the aforementioned context
this would be difficult to conclude with any certainty.
ment history, we see that he or she repeatedly de-
rides Senator Cruz (e.g., writing ?Ted Cruz is no
Ronald Reagan. They aren?t even close.?). From
this contextual information, then, we can reason-
ably assume that the comment was intended iron-
ically (and all three annotators did so after assess-
ing the available contextual information).
4 Humans Need Context to Infer Irony
We explore the extent to which human annotators
rely on contextual information to decide whether
or not sentences were intended ironically. Recall
that our annotation tool allows labelers to request
additional context if they cannot make a decision
based on the comment text alone (Figure 1). On
average, annotators requested additional context
for 30% of comments (range across annotators of
12% to 56%). As shown in Figure 3, annotators
are consistently more confident once they have
consulted this information.
We tested for a correlation between these re-
quests for context and the final decisions regard-
ing whether comments contain at least one ironic
sentence. We denote the probability of at least one
annotator requesting additional context for com-
ment i by P (C
i
). We then model the probability
of this event as a linear function of whether or not
514
64
86
174
forced decision30 final decision
152
90
529
forced decision51 final decision
176
207
364
forced decision25 final decision
ironic ?ironic
ironic ?unironic
unironic ?unironic
unironic ?ironic
annotator 1
annotator 2 annotator 3
Figure 3: This plot illustrates the effect of viewing contextual information for three annotators (one table for each annotator).
For all comments for which these annotators requested context, we show forced (before viewing the requested contextual
content) and final (after) decisions regarding perceived ironic intent on behalf of the author. Each row shows one of four
possible decision sequences (e.g., a judgement of ironic prior to seeing context and unironic after). Numbers correspond to
counts of these sequences for each annotator (e.g., the first annotator changed their mind from ironic to unironic 86 times).
Cases that involve the annotator changing his or her mind are shown in red; those in which the annotator stuck with their initial
judgement are shown in blue. Color intensity is proportional to the average confidence judgements the annotator provided:
these are uniformly stronger after they have consulted contextualizing information. Note also that the context frequently results
in annotators changing their judgement.
any annotator labeled any sentence in comment i
as ironic. We code this via the indicator variable
I
i
which is 1 when comment i has been deemed
to contain an ironic sentence (by any of the three
annotators) and 0 otherwise.
logit{P (C
i
)} = ?
0
+ ?
1
I
i
(1)
We used the regression model shown in Equa-
tion 1, where ?
0
is an intercept and ?
1
captures
the correlation between requests for context for a
given comment and its ultimately being deemed
to contain at least one ironic sentence. We fit this
model to the annotated corpus, and found a signif-
icant correlation:
?
?
1
= 1.508 with a 95% confi-
dence interval of (1.326, 1.690); p < 0.001.
In other words, annotators request context sig-
nificantly more frequently for those comments
that (are ultimately deemed to) contain an ironic
sentence. This would suggest that the words
and punctuation comprising online comments
alone are not sufficient to distinguish ironic from
unironic comments. Despite this, most machine
learning based approaches to irony detection have
relied nearly exclusively on such intrinsic features.
5 Machines Probably do, too
We show that the misclassifications (with respect
to whether comments contain irony or not) made
by a standard text classification model signifi-
cantly correlate with those comments for which
human annotators requested additional context.
This provides evidence that bag-of-words ap-
proaches are insufficient for the general task of
irony detection: more context is necessary.
We implemented a baseline classification ap-
proach using vanilla token count features (binary
bag-of-words). We removed stop-words and lim-
ited the vocabulary to the 50,000 most frequently
occurring unigrams and bigrams. We added ad-
ditional binary features coding for the presence
of punctuational features, such as exclamation
points, emoticons (for example, ?;)?) and question
marks: previous work (Davidov et al, 2010; Car-
valho et al, 2009) has found that these are good
indicators of ironic intent.
For our predictive model, we used a linear-
kernel SVM (tuning the C parameter via grid-
search over the training dataset to maximize F1
score). We performed five-fold cross-validation,
recording the predictions y?
i
for each (held-out)
comment i. Average F1 score over the five-folds
was 0.383 with range (0.330, 0.412); mean recall
was 0.496 (0.446, 0.548) and average precision
was 0.315 (0.261, 0.380). The five most predictive
tokens were: !, yeah, guys, oh and shocked. This
represents reasonable performance (with intuitive
predictive tokens); but obviously there is quite a
bit of room for improvement.
3
We now explore empirically whether these mis-
classifications are made on the same comments for
which annotators requested context. To this end,
we introduce a variable M
i
for each comment i
such that M
i
= 1 if y?
i
6= y
i
, i.e., M
i
is an in-
3
Some of the recently proposed strategies mentioned in
Section 2 may improve performance here, but none of these
address the fundamental issue of context.
515
dicator variable that encodes whether or not the
classifier misclassified comment i. We then ran
a second regression in which the output variable
was the logit-transformed probability of the model
misclassifying comment i, i.e., P (M
i
). Here we
are interested in the correlation of the event that
one or more annotators requested additional con-
text for comment i (denoted by C
i
) and model mis-
classifications (adjusting for the comment?s true
label). Formally:
logit{P (M
i
)} = ?
0
+ ?
1
I
i
+ ?
2
C
i
(2)
Fitting this to the data, we estimated
?
?
2
= 0.971
with a 95% CI of (0.810, 1.133); p < 0.001. Put
another way, the model makes mistakes on those
comments for which annotators requested addi-
tional context (even after accounting for the an-
notator designation of comments).
6 Conclusions and Future Directions
We have described a new (publicly available) cor-
pus for the task of verbal irony detection. The
data comprises comments scraped from the so-
cial news website reddit. We recorded confidence
judgements and requests for contextualizing infor-
mation for each comment during annotation. We
analyzed this corpus to provide empirical evidence
that annotators quite often require context beyond
the comment under consideration to discern irony;
especially for those comments ultimately deemed
as being intended ironically. We demonstrated
that a standard token-based machine learning ap-
proach misclassified many of the same comments
for which annotators tend to request context.
We have shown that annotators rely on contex-
tual cues (in addition to word and grammatical fea-
tures) to discern irony and argued that this implies
computers should, too. The obvious next step is to
develop new machine learning models that exploit
the contextual information available in the corpus
we have curated (e.g., previous comments by the
same user, the thread topic).
7 Acknowledgement
This work was made possible by the Army Re-
search Office (ARO), grant #64481-MA.
References
C Burfoot and T Baldwin. 2009. Automatic satire de-
tection: are you having a laugh? In ACL-IJCNLP,
pages 161?164. ACL.
P Carvalho, L Sarmento, MJ Silva, and E de Oliveira.
2009. Clues for detecting irony in user-generated
contents: oh...!! it?s so easy;-). In CIKM workshop
on Topic-sentiment analysis for mass opinion, pages
53?56. ACM.
HH Clark and RJ Gerrig. 1984. On the pretense the-
ory of irony. Journal of Experimental Psychology,
113:121?126.
J Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement, 20:37?46.
D Davidov, O Tsur, and A Rappoport. 2010. Semi-
supervised recognition of sarcastic sentences in twit-
ter and amazon. pages 107?116.
E Filatova. 2012. Irony and sarcasm: Corpus gener-
ation and analysis using crowdsourcing. In LREC,
volume 12, pages 392?398.
RW Gibbs and HL Colston. 2007. Irony in language
and thought: a cognitive science reader. Lawrence
Erlbaum.
R Gonz?alez-Ib?a?nez, S Muresan, and N Wacholder.
2011. Identifying sarcasm in twitter: a closer look.
In ACL, volume 2, pages 581?586. Citeseer.
HP Grice. 1975. Logic and conversation. 1975, pages
41?58.
A Halevy, P Norvig, and F Pereira. 2009. The unrea-
sonable effectiveness of data. Intelligent Systems,
IEEE, 24(2):8?12.
S Lukin and M Walker. 2013. Really? well. ap-
parently bootstrapping improves the performance of
sarcasm and nastiness classifiers for online dialogue.
NAACL, pages 30?40.
A Reyes, P Rosso, and T Veale. 2012. A multidimen-
sional approach for detecting irony in twitter. LREC,
pages 1?30.
E Riloff, A Qadir, P Surve, LD Silva, N Gilbert, and
R Huang. 2013. Sarcasm as contrast between a pos-
itive sentiment and negative situation. In EMNLP,
pages 704?714.
D Sperber and D Wilson. 1981. Irony and the use-
mention distinction. 1981.
J Tepperman, D Traum, and S Narayanan. 2006.
?Yeah Right?: Sarcasm Recognition for Spoken Di-
alogue Systems.
O Tsur, D Davidov, and A Rappoport. 2010. ICWSM-
a great catchy name: Semi-supervised recognition
of sarcastic sentences in online product reviews. In
AAAI Conference on Weblogs and Social Media.
AJ Viera and JM Garrett. 2005. Understanding in-
terobserver agreement: the kappa statistic. Family
Medicine, 37(5):360?363.
MA Walker, JEF Tree, P Anand, R Abbott, and J King.
2012. A corpus for research on deliberation and de-
bate. In LREC, pages 812?817.
516
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 592?598,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Nonparametric Method for Data-driven Image Captioning
Rebecca Mason and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University, Providence, RI 02912
{rebecca,ec}@cs.brown.edu
Abstract
We present a nonparametric density esti-
mation technique for image caption gener-
ation. Data-driven matching methods have
shown to be effective for a variety of com-
plex problems in Computer Vision. These
methods reduce an inference problem for
an unknown image to finding an exist-
ing labeled image which is semantically
similar. However, related approaches for
image caption generation (Ordonez et al,
2011; Kuznetsova et al, 2012) are ham-
pered by noisy estimations of visual con-
tent and poor alignment between images
and human-written captions. Our work
addresses this challenge by estimating a
word frequency representation of the vi-
sual content of a query image. This al-
lows us to cast caption generation as an
extractive summarization problem. Our
model strongly outperforms two state-of-
the-art caption extraction systems accord-
ing to human judgments of caption rele-
vance.
1 Introduction
Automatic image captioning is a much studied
topic in both the Natural Language Processing
(NLP) and Computer Vision (CV) areas of re-
search. The task is to identify the visual content
of the input image, and to output a relevant natural
language caption.
Much prior work treats image captioning as
a retrieval problem (see Section 2). These ap-
proaches use CV algorithms to retrieve similar im-
ages from a large database of captioned images,
and then transfer text from the captions of those
images to the query image. This is a challenging
problem for two main reasons. First, visual simi-
larity measures do not perform reliably and do not
Query Image: Captioned Images:
1. 2. 3.
4. 5. 6.
1.) 3 month old baby girl with blue eyes in her crib
2.) A photo from the Ismail?s portrait shoot
3.) A portrait of a man, in black and white
4.) Portrait in black and white with the red rose
5.) I apparently had this saved in black and white as well
6.) Portrait in black and white
Table 1: Example of a query image from the SBU-
Flickr dataset (Ordonez et al, 2011), along with
scene-based estimates of visually similar images.
Our system models visual content using words that
are frequent in these captions (highlighted) and ex-
tracts a single output caption.
capture all of the relevant details which humans
might describe. Second, image captions collected
from the web often contain contextual or back-
ground information which is not visually relevant
to the image being described.
In this paper, we propose a system for transfer-
based image captioning which is designed to ad-
dress these challenges. Instead of selecting an out-
put caption according to a single noisy estimate
of visual similarity, our system uses a word fre-
quency model to find a smoothed estimate of vi-
sual content across multiple captions, as Table 1
illustrates. It then generates a description of the
query image by extracting the caption which best
represents the mutually shared content.
The contributions of this paper are as follows:
592
1. Our caption generation system effectively lever-
ages information from the massive amounts of
human-written image captions on the internet. In
particular, it exhibits strong performance on the
SBU-Flickr dataset (Ordonez et al, 2011), a noisy
corpus of one million captioned images collected
from the web. We achieve a remarkable 34%
improvement in human relevance scores over a
recent state-of-the-art image captioning system
(Kuznetsova et al, 2012), and 48% improvement
over a scene-based retrieval system (Patterson et
al., 2014) using the same computed image fea-
tures.
2. Our approach uses simple models which can
be easily reproduced by both CV and NLP re-
searchers. We provide resources to enable com-
parison against future systems.
1
2 Image Captioning by Transfer
The IM2TEXT model by Ordonez et al (2011)
presents the first web-scale approach to image cap-
tion generation. IM2TEXT retrieves the image
which is the closest visual match to the query im-
age, and transfers its description to the query im-
age. The COLLECTIVE model by Kuznetsova et
al. (2012) is a related approach which uses trained
CV recognition systems to detect a variety of vi-
sual entities in the query image. A separate de-
scription is retrieved for each visual entity, which
are then fused into a single output caption. Like
IM2TEXT, their approach uses visual similarity as
a proxy for textual relevance.
Other related work models the text more di-
rectly, but is more restrictive about the source
and quality of the human-written training data.
Farhadi et al (2010) and Hodosh et al (2013)
learn joint representations for images and cap-
tions, but can only be trained on data with very
strong alignment between images and descriptions
(i.e. captions written by Mechanical Turkers). An-
other line of related work (Fan et al, 2010; Aker
and Gaizauskas, 2010; Feng and Lapata, 2010)
generates captions by extracting sentences from
documents which are related to the query image.
These approaches are tailored toward specific do-
mains, such as travel and news, where images tend
to appear with corresponding text.
1
See http://bllip.cs.brown.edu/
download/captioning_resources.zip or ACL
Anthology.
3 Dataset
In this paper, we use the SBU-Flickr dataset
2
. Or-
donez et al (2011) query Flickr.com using a
huge number of words which describe visual en-
tities, in order to build a corpus of one million
images with captions which refer to image con-
tent. However, further analysis by Hodosh et al
(2013) shows that many captions in SBU-Flickr
(?67%) describe information that cannot be ob-
tained from the image itself, while a substantial
fraction (?23%) contain almost no visually rel-
evant information. Nevertheless, this dataset is
the only web-scale collection of captioned images,
and has enabled notable research in both CV and
NLP.
3
4 Our Approach
4.1 Overview
For a query image I
q
, our task is to generate a rele-
vant description by selecting a single caption from
C, a large dataset of images with human-written
captions. In this section, we first define the feature
space for visual similarity, then formulate a den-
sity estimation problem with the aim of modeling
the words which are used to describe visually sim-
ilar images to I
q
. We also explore methods for
extractive caption generation.
4.2 Measuring Visual Similarity
Data-driven matching methods have shown to be
very effective for a variety of challenging prob-
lems (Hays and Efros, 2008; Makadia et al,
2008; Tighe and Lazebnik, 2010). Typically these
methods compute global (scene-based) descriptors
rather than object and entity detections. Scene-
based techniques in CV are generally more robust,
and can be computed more efficiently on large
datasets.
The basic IM2TEXT model uses an equally
weighted average of GIST (Oliva and Torralba,
2001) and TinyImage (Torralba et al, 2008) fea-
tures, which coarsely localize low-level features
in scenes. The output is a multi-dimensional
image space where semantically similar scenes
(e.g. streets, beaches, highways) are projected
near each other.
2
http://tamaraberg.com/CLSP11/
3
In particular, papers stemming from the 2011 JHU-CLSP
Summer Workshop (Berg et al, 2012; Dodge et al, 2012;
Mitchell et al, 2012) and more recently, the best paper award
winner at ICCV (Ordonez et al, 2013).
593
Patterson and Hays (2012) present ?scene at-
tribute? representations which are characterized
using low-level perceptual attributes as used by
GIST (e.g. openness, ruggedness, naturalness),
as well as high-level attributes informed by open-
ended crowd-sourced image descriptions (e.g., in-
door lighting, running water, places for learning).
Follow-up work (Patterson et al, 2014) shows
that their attributes provide improved matching for
image captioning over IM2TEXT baseline. We
use their publicly available
4
scene attributes for
our experiments. Training set and query images
are represented using 102-dimensional real-valued
vectors, and similarity between images is mea-
sured using the Euclidean distance.
4.3 Density Estimation
As shown in Bishop (2006), probability density
estimates at a particular point can be obtained by
considering points in the training data within some
local neighborhood. In our case, we define some
region R in the image space which contains I
q
.
The probability mass of that space is
P =
?
R
p(I
q
)dI
q
(1)
and if we assume thatR is small enough such that
p(I
q
) is roughly constant in R, we can approxi-
mate
p(I
q
) ?
k
img
n
img
V
img
(2)
where k
img
is the number of images within R in
the training data, n
img
is the total number of im-
ages in the training data, and V
img
is the volume
ofR. In this paper, we fix k
img
to a constant value,
so that V
img
is determined by the training data
around the query image.
5
At this point, we extend the density estima-
tion technique in order to estimate a smoothed
model of descriptive text. Let us begin by consid-
ering p(w|I
q
), the conditional probability of the
word
6
w given I
q
. This can be described using a
4
https://github.com/genp/sun_
attributes
5
As an alternate approach, one could fix the value of
V
img
and determine k
img
from the number of points in R,
giving rise to the kernel density approach (a.k.a. Parzen
windows). However we believe the KNN approach is more
appropriate here, because the number of samples is nearly
10000 times greater than the number of dimensions in the
image representation.
6
Here, we use word to refer to non-function words, and
assume all function words have been removed from the cap-
tions.
Bayesian model:
p(w|I
q
) =
p(I
q
|w)p(w)
p(I
q
)
(3)
The prior for w is simply its unigram frequency in
C, where n
txt
w
and n
txt
are word token counts:
p(w) =
n
txt
w
n
txt
(4)
Note that n
txt
is not the same as n
img
because a
single captioned image can have multiple words
in its caption. Likewise, the conditional density
p(I
q
|w) ?
k
txt
w
n
txt
w
V
img
(5)
considers instances of observed words within R,
although the volume of R is still defined by the
image space. k
txt
w
is the number of times w is used
withinR while n
txt
w
is the total number of times w
is observed in C.
Combining Equations 2, 4, and 5 and canceling
out terms gives us the posterior probability:
p(w|I
q
) =
k
txt
w
k
img
?
n
img
n
txt
(6)
If the number of words in each caption is inde-
pendent of its image?s location in the image space,
then p(w|I
q
) is approximately the observed uni-
gram frequency for the captions insideR.
4.4 Extractive Caption Generation
We compare two selection methods for extractive
caption generation:
1. SumBasic SumBasic (Nenkova and Vander-
wende, 2005) is a sentence selection algorithm for
extractive multi-document summarization which
exclusively maximizes the appearance of words
which have high frequency in the original docu-
ments. Here, we adapt SumBasic to maximize the
average value of p(w|I
q
) in a single extracted cap-
tion:
output = argmax
c
txt
?R
?
w?c
txt
1
|c
txt
|
p(w|I
q
) (7)
The candidate captions c
txt
do not necessarily
have to be observed in R, but in practice we did
not find increasing the number of candidate cap-
tions to be more effective than increasing the size
ofR directly.
594
Figure 1: BLEU scores vs k for SumBasic extrac-
tion.
2. KL Divergence We also consider a KL
Divergence selection method. This method out-
performs the SumBasic selection method for ex-
tractive multi-document summarization (Haghighi
and Vanderwende, 2009). It also generates the best
extractive captions for Feng and Lapata (2010),
who caption images by extracting text from a re-
lated news article. The KL Divergence method is
output = argmin
c
txt
?R
?
w
p(w|I
q
) log
p(w|I
q
)
p(w|c
txt
)
(8)
5 Evaluation
5.1 Automatic Evaluation
Although BLEU (Papineni et al, 2002) scores
are widely used for image caption evaluation, we
find them to be poor indicators of the quality of
our model. As shown in Figure 1, our system?s
BLEU scores increase rapidly until about k = 25.
Past this point we observe the density estimation
seems to get washed out by oversmoothing, but the
BLEU scores continue to improve until k = 500
but only because the generated captions become
increasingly shorter. Furthermore, although we
observe that our SumBasic extracted captions ob-
tain consistently higher BLEU scores, our per-
sonal observations find KL Divergence captions to
be better at balancing recall and precision. Never-
theless, BLEU scores are the accepted metric for
recent work, and our KL Divergence captions with
k = 25 still outperform all other previously pub-
lished systems and baselines. We omit full results
here due to space, but make our BLEU setup with
captions for all systems and baselines available for
documentary purposes.
System Relevance
COLLECTIVE 2.38 (? = 1.45)
SCENE ATTRIBUTES 2.15 (? = 1.45)
SYSTEM 3.19 (? = 1.50)
HUMAN 4.09 (? = 1.14)
Table 2: Human evaluations of relevance: mean
ratings and standard deviations. See Section 5.2.
5.2 Human Evaluation
We perform our human evaluation of caption rele-
vance using a similar setup to that of Kuznetsova
et al (2012), who have humans rate the image cap-
tions on a 1-5 scale (5: perfect, 4: almost per-
fect, 3: 70-80% good, 2: 50-70% good, 1: to-
tally bad). Evaluation is performed using Amazon
Mechanical Turk. Evaluators are shown both the
caption and the query image, and are specifically
instructed to ignore errors in grammaticality and
coherence.
We generate captions using our system with KL
Divergence sentence selection and k = 25. We
also evaluate the original HUMAN captions for
the query image, as well as generated captions
from two recently published caption transfer sys-
tems. First, we consider the SCENE ATTRIBUTES
system (Patterson et al, 2014), which represents
both the best scene-based transfer model and a
k = 1 nearest-neighbor baseline for our system.
We also compare against the COLLECTIVE system
(Kuznetsova et al, 2012), which is the best object-
based transfer model.
In order to facilitate comparison, we use the
same test/train split that is used in the publicly
available system output for the COLLECTIVE sys-
tem
7
. However, we remove some query images
which have contamination between the train and
test set (this occurs when a photographer takes
multiple shots of the same scene and gives all the
images the exact same caption). We also note that
their test set is selected based on images where
their object detection systems had good perfor-
mance, and may not be indicative of their perfor-
mance on other query images.
Table 2 shows the results of our human study.
Captions generated by our system have 48%
improvement in relevance over the SCENE AT-
TRIBUTES system captions, and 34% improve-
7
http://www.cs.sunysb.edu/
?
pkuznetsova/generation/cogn/captions.
html
595
COLLECTIVE: One of the birds seen in
company of female and
juvenile.
View of this woman sit-
ting on the sidewalk in
Mumbai by the stained
glass. The boy walk-
ing by next to match-
ing color walls in gov t
building.
Found this mother bird
feeding her babies in
our maple tree on the
phone.
Found in floating grass
spotted alongside the
scenic North Cascades
Hwy near Ruby arm a
black bear.
SCENE
ATTRIBUTES:
This small bird is pretty
much only found in the
ancient Caledonian pine
forests of the Scottish
Highlands.
me and allison in front
of the white house
The sand in this beach
was black...I repeat
BLACK SAND
Not the green one, but
the almost ghost-like
white one in front of it.
SYSTEM: White bird found in
park standing on brick
wall
by the white house pine tree covered in ice
:)
Pink flower in garden w/
moth
HUMAN: Some black head bird
taken in bray head.
Us girls in front of the
white house
Male cardinal in snowy
tree knots
Black bear by the road
between Ucluelet and
Port Alberni, B.C.,
Canada
Table 3: Example query images and generated captions.
ment over the COLLECTIVE system captions. Al-
though our system captions score lower than the
human captions on average, there are some in-
stances of our system captions being judged as
more relevant than the human-written captions.
6 Discussion and Examples
Example captions are shown in Table 3. In many
instances, scene-based image descriptors provide
enough information to generate a complete de-
scription of the image, or at least a sufficiently
good one. However, there are some kinds of
images for which scene-based features alone are
insufficient. For example, the last example de-
scribes the small pink flowers in the background,
but misses the bear.
Image captioning is a relatively novel task for
which the most compelling applications are prob-
ably not yet known. Much previous work in im-
age captioning focuses on generating captions that
concretely describe detected objects and entities
(Kulkarni et al, 2011; Yang et al, 2011; Mitchell
et al, 2012; Yu and Siskind, 2013). However,
human-generated captions and annotations also
describe perceptual features, contextual informa-
tion, and other types of content. Additionally, our
system is robust to instances where entity detec-
tion systems fail to perform. However, one could
consider combined approaches which incorporate
more regional content structures. For example,
previous work in nonparametric hierarchical topic
modeling (Blei et al, 2010) and scene labeling
(Liu et al, 2011) may provide avenues for further
improvement of this model. Compression meth-
ods for removing visually irrelevant information
(Kuznetsova et al, 2013) may also help increase
the relevance of extracted captions. We leave these
ideas for future work.
References
Ahmet Aker and Robert Gaizauskas. 2010. Generating
image descriptions using dependency relational pat-
terns. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, ACL
?10, pages 1250?1258, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Alexander C Berg, Tamara L Berg, Hal Daume, Jesse
Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch,
Margaret Mitchell, Aneesh Sood, Karl Stratos, et al
2012. Understanding and predicting importance in
images. In Computer Vision and Pattern Recog-
nition (CVPR), 2012 IEEE Conference on, pages
3562?3569. IEEE.
Christopher M Bishop. 2006. Pattern recognition and
machine learning, volume 1. Springer New York.
David M. Blei, Thomas L. Griffiths, and Michael I. Jor-
dan. 2010. The nested chinese restaurant process
596
and bayesian nonparametric inference of topic hier-
archies. J. ACM, 57(2):7:1?7:30, February.
Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-
sch, Margaret Mitchell, Karl Stratos, Kota Yam-
aguchi, Yejin Choi, Hal Daum?e III, Alexander C.
Berg, and Tamara L. Berg. 2012. Detecting visual
text. In North American Chapter of the Association
for Computational Linguistics (NAACL).
Xin Fan, Ahmet Aker, Martin Tomko, Philip Smart,
Mark Sanderson, and Robert Gaizauskas. 2010.
Automatic image captioning from the web for gps
photographs. In Proceedings of the international
conference on Multimedia information retrieval,
MIR ?10, pages 445?448, New York, NY, USA.
ACM.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences from images.
In Proceedings of the 11th European conference on
Computer vision: Part IV, ECCV?10, pages 15?29,
Berlin, Heidelberg. Springer-Verlag.
Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? automatic caption genera-
tion for news images. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1239?1249, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Aria Haghighi and Lucy Vanderwende. 2009. Ex-
ploring content models for multi-document summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 362?370. Association for
Computational Linguistics.
James Hays and Alexei A Efros. 2008. Im2gps: esti-
mating geographic information from a single image.
In Computer Vision and Pattern Recognition, 2008.
CVPR 2008. IEEE Conference on, pages 1?8. IEEE.
Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
Data, models and evaluation metrics. Journal of Ar-
tificial Intelligence Research, 47:853?899.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and gener-
ating simple image descriptions. In CVPR, pages
1601?1608.
Polina Kuznetsova, Vicente Ordonez, Alexander C.
Berg, Tamara L. Berg, and Yejin Choi. 2012. Col-
lective generation of natural image descriptions. In
ACL.
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg, and Yejin Choi. 2013. Generalizing
image captions for image-text parallel corpus. In
ACL.
Ce Liu, Jenny Yuen, and Antonio Torralba. 2011.
Nonparametric scene parsing via label transfer.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 33(12):2368?2382.
Ameesh Makadia, Vladimir Pavlovic, and Sanjiv Ku-
mar. 2008. A new baseline for image annotation.
In Computer Vision?ECCV 2008, pages 316?329.
Springer.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Men-
sch, Alexander C. Berg, Tamara L. Berg, and Hal
Daum?e III. 2012. Midge: Generating image de-
scriptions from computer vision detections. In Euro-
pean Chapter of the Association for Computational
Linguistics (EACL).
Ani Nenkova and Lucy Vanderwende. 2005. The im-
pact of frequency on summarization.
Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: A holistic representation of the
spatial envelope. International Journal of Computer
Vision, 42:145?175.
V. Ordonez, G. Kulkarni, and T.L. Berg. 2011.
Im2text: Describing images using 1 million cap-
tioned photographs. In NIPS.
Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C
Berg, and Tamara L Berg. 2013. From large scale
image categorization to entry-level categories. In In-
ternational Conference on Computer Vision.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Genevieve Patterson and James Hays. 2012. Sun at-
tribute database: Discovering, annotating, and rec-
ognizing scene attributes. In Computer Vision and
Pattern Recognition (CVPR), 2012 IEEE Confer-
ence on, pages 2751?2758. IEEE.
Genevieve Patterson, Chen Xu, Hang Su, and James
Hays. 2014. The sun attribute database: Beyond
categories for deeper scene understanding. Interna-
tional Journal of Computer Vision.
Joseph Tighe and Svetlana Lazebnik. 2010. Su-
perparsing: scalable nonparametric image parsing
with superpixels. In Computer Vision?ECCV 2010,
pages 352?365. Springer.
Antonio Torralba, Robert Fergus, and William T Free-
man. 2008. 80 million tiny images: A large data
set for nonparametric object and scene recognition.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 30(11):1958?1970.
597
Yezhou Yang, Ching Lik Teo, Hal Daum?e III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods
in Natural Language Processing (EMNLP), Edin-
burgh, Scotland.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from video described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 53?63,
Sofia, Bulgaria. Association for Computational Lin-
guistics.
598
Variation of Entropy and Parse Trees of Sentences as a Function of the
Sentence Number
Dmitriy Genzel and Eugene Charniak
Brown Laboratory for Linguistic Information Processing
Department of Computer Science
Brown University
Providence, RI, USA, 02912
{dg,ec}@cs.brown.edu
Abstract
In this paper we explore the variation of
sentences as a function of the sentence
number. We demonstrate that while the
entropy of the sentence increases with the
sentence number, it decreases at the para-
graph boundaries in accordance with the
Entropy Rate Constancy principle (intro-
duced in related work). We also demon-
strate that the principle holds for differ-
ent genres and languages and explore the
role of genre informativeness. We investi-
gate potential causes of entropy variation
by looking at the tree depth, the branch-
ing factor, the size of constituents, and the
occurrence of gapping.
1 Introduction and Related Work
In many natural language processing applications,
such as parsing or language modeling, sentences are
treated as natural self-contained units. Yet it is well-
known that for interpreting the sentences the dis-
course context is often very important. The later
sentences in the discourse contain references to the
entities in the preceding sentences, and this fact is
often useful, e.g., in caching for language model-
ing (Goodman, 2001). The indirect influence of the
context, however, can be observed even when a sen-
tence is taken as a stand-alone unit, i.e., without its
context. It is possible to distinguish between a set
of earlier sentences and a set of later sentences with-
out any direct comparison by computing certain lo-
cal statistics of individual sentences, such as their
entropy (Genzel and Charniak, 2002). In this work
we provide additional evidence for this hypothesis
and investigate other sentence statistics.
1.1 Entropy Rate Constancy
Entropy, as a measure of information, is often used
in the communication theory. If humans have
evolved to communicate in the most efficient way
(some evidence for that is provided by Plotkin and
Nowak (2000)), then they would communicate in
such a way that the entropy rate would be constant,
namely, equal to the channel capacity (Shannon,
1948).
In our previous work (Genzel and Charniak,
2002) we propose that entropy rate is indeed con-
stant in human communications. When read in con-
text, each sentence would appear to contain roughly
the same amount of information, per word, whether
it is the first sentence or the tenth one. Thus the
tenth sentence, when taken out of context, must ap-
pear significantly more informative (and therefore
harder to process), since it implicitly assumes that
the reader already knows all the information in the
preceding nine sentences. Indeed, the greater the
sentence number, the harder to process the sentence
must appear, though for large sentence numbers this
would be very difficult to detect. This makes intu-
itive sense: out-of-context sentences are harder to
understand than in-context ones, and first sentences
can never be out of context. It is also demonstrated
empirically through estimating entropy rate of vari-
ous sentences.
In the first part of the present paper (Sections 2
and 3) we extend and further verify these results. In
the second part (Section 4), we investigate the poten-
tial reasons underlying this variation in complexity
by looking at the parse trees of the sentences. We
also discuss how genre and style affect the strength
of this effect.
1.2 Limitations of Preceding Work
In our previous work we demonstrate that the word
entropy rate increases with the sentence number; we
do it by estimating entropy of Wall Street Journal
articles in Penn Treebank in three different ways. It
may be the case, however, that this effect is corpus-
and language-specific. To show that the Entropy
Rate Constancy Principle is universal, we need to
confirm it for different genres and different lan-
guages. We will address this issue in Section 3.
Furthermore, if the principle is correct, it should
also apply to the sentences numbered from the be-
ginning of a paragraph, rather than from the begin-
ning of the article, since in either case there is a shift
of topic. We will discuss this in Section 2.
2 Within-Paragraph Effects
2.1 Implications of Entropy Rate Constancy
Principle
We have previously demonstrated (see Genzel and
Charniak (2002) for detailed derivation) that the
conditional entropy of the ith word in the sentence
(Xi), given its local context Li (the preceding words
in the same sentence) and global context Ci (the
words in all preceding sentences) can be represented
as
H(Xi|Ci, Li) = H(Xi|Li)? I(Xi, Ci|Li)
where H(Xi|Li) is the conditional entropy of the
ith word given local context, and I(Xi, Ci|Li) is
the conditional mutual information between the ith
word and out-of-sentence context, given the local
context. Since Ci increases with the sentence num-
ber, we will assume that, normally, it will provide
more and more information with each sentence. This
would cause the second term on the right to increase
with the sentence number, and since H(Xi|Ci, Li)
must remain constant (by our assumption), the first
term should increase with sentence number, and it
had been shown to do so (Genzel and Charniak,
2002).
Our assumption about the increase of the mutual
information term is, however, likely to break at the
paragraph boundary. If there is a topic shift at the
boundary, the context probably provides more infor-
mation to the preceding sentence, than it does to the
new one. Hence, the second term will decrease, and
so must the first one.
In the next section we will verify this experimen-
tally.
2.2 Experimental Setup
We use the Wall Street Journal text (years 1987-
1989) as our corpus. We take all articles that con-
tain ten or more sentences, and extract the first ten
sentences. Then we:
1. Group extracted sentences according to their
sentence number into ten sets of 49559 sen-
tences each.
2. Separate each set into two subsets, paragraph-
starting and non-paragraph-starting sentences1.
3. Combine first 45000 sentences from each set
into the training set and keep all remaining data
as 10 testing sets (19 testing subsets).
We use a simple smoothed trigram language model:
P (xi|x1 . . . xi?1) ? P (xi|xi?2xi?1)
= ?1P? (xi|xi?2xi?1)
+ ?2P? (xi|xi?1)
+ (1? ?1 ? ?2)P? (xi)
where ?1 and ?2 are the smoothing coefficients2,
and P? is a maximum likelihood estimate of the cor-
responding probability, e.g.,
P? (xi|xi?2xi?1) =
C(xi?2xi?1xi)
C(xi?2xi?1)
where C(xi . . . xj) is the number of times this se-
quence appears in the training data.
We then evaluate the resulting model on each of
the testing sets, computing per-word entropy of the
set:
H?(X) =
1
|X|
?
xi?X
logP (xi|xi?2xi?1)
1First sentences are, of course, all paragraph-starting.
2We have arbitrarily chosen the smoothing coefficients to be
0.5 and 0.3, correspondingly.
1 2 3 4 5 6 7 8 9 10
5.8
5.9
6.0
6.1
6.2
6.3
6.4
6.5
6.6
Sentence number
En
tro
py
 (b
its
)
all sentences
paragraph?starting
non?paragraph?starting
Figure 1: Entropy vs. Sentence number
2.3 Results and Discussion
As outlined above, we have ten testing sets, one for
each sentence number; each set (except for the first)
is split into two subsets: sentences that start a para-
graph, and sentences that do not. The results for full
sets, paragraph-starting subsets, and non-paragraph-
starting subsets are presented in Figure 1.
First, we can see that the the entropy for full
sets (solid line) is generally increasing. This re-
sult corresponds to the previously discussed effect
of entropy increasing with the sentence number. We
also see that for all sentence numbers the paragraph-
starting sentences have lower entropy than the non-
paragraph-starting ones, which is what we intended
to demonstrate. In such a way, the paragraph-
starting sentences are similar to the first sentences,
which makes intuitive sense.
All the lines roughly show that entropy increases
with the sentence number, but the behavior at the
second and the third sentences is somewhat strange.
We do not yet have a good explanation of this phe-
nomenon, except to point out that paragraphs that
start at the second or third sentences are probably
not ?normal? because they most likely do not indi-
cate a topic shift. Another possible explanation is
that this effect is an artifact of the corpus used.
We have also tried to group sentences based on
their sentence number within paragraph, but were
unable to observe a significant effect. This may be
due to the decrease of this effect in the later sen-
tences of large articles, or perhaps due to the relative
weakness of the effect3.
3 Different Genres and Languages
3.1 Experiments on Fiction
3.1.1 Introduction
All the work on this problem so far has focused
on the Wall Street Journal articles. The results are
thus naturally suspect; perhaps the observed effect
is simply an artifact of the journalistic writing style.
To address this criticism, we need to perform com-
parable experiments on another genre.
Wall Street Journal is a fairly prototypical exam-
ple of a news article, or, more generally, a writing
with a primarily informative purpose. One obvious
counterpart of such a genre is fiction4. Another al-
ternative might be to use transcripts of spoken dia-
logue.
Unfortunately, works of fiction, are either non-
homogeneous (collections of works) or relatively
short with relatively long subdivisions (chapters).
This is crucial, since in the sentence number experi-
ments we obtain one data point per article, therefore
it is impossible to use book chapters in place of arti-
cles.
3.1.2 Experimental Setup and Results
For our experiments we use War and Peace (Tol-
stoy, 1869), since it is rather large and publicly avail-
able. It contains only about 365 rather long chap-
ters5. Unlike WSJ articles, each chapter is not writ-
ten on a single topic, but usually has multiple topic
shifts. These shifts, however, are marked only as
paragraph breaks. We, therefore, have to assume
that each paragraph break represents a topic shift,
3We combine into one set very heterogeneous data: both 1st
and 51st sentence might be in the same set, if they both start
a paragraph. The experiment in Section 2.2 groups only the
paragraph-starting sentences with the same sentence number.
4We use prose rather than poetry, which presumably is
even less informative, because poetry often has superficial con-
straints (meter); also, it is hard to find a large homogeneous
poetry collection.
5For comparison, Penn Treebank contains over 2400 (much
shorter) WSJ articles.
1 2 3 4 58.05
8.1
8.15
8.2
8.25
8.3
Sentence number since beginning of paragraph
En
tro
py 
in b
its
Real run    
Control runs
Figure 2: War and Peace: English
and treat each paragraph as being an equivalent of a
WSJ article, even though this is obviously subopti-
mal.
The experimental setup is very similar to the one
used in Section 2.2. We use roughly half of the data
for training purposes and split the rest into testing
sets, one per each sentence number, counted from
the beginning of a paragraph.
We then evaluate the results using the same
method as in Section 2.2. We expect that the en-
tropy would increase with the sentence number, just
as in the case of the sentences numbered from the
article boundary. This effect is present, but is not
very pronounced. To make sure that it is statistically
significant, we also do 1000 control runs for com-
parison, with paragraph breaks inserted randomly at
the appropriate rate. The results (including 3 ran-
dom runs) can be seen in Figure 2. To make sure
our results are significant we compare the correla-
tion coefficient between entropy and sentence num-
ber to ones from simulated runs, and find them to be
significant (P=0.016).
It is fairly clear that the variation, especially be-
tween the first and the later sentences, is greater
than it would be expected for a purely random oc-
currence. We will see further evidence for this in the
next section.
3.2 Experiments on Other Languages
To further verify that this effect is significant and
universal, it is necessary to do similar experiments
in other languages. Luckily, War and Peace is also
digitally available in other languages, of which we
pick Russian and Spanish for our experiments.
We follow the same experimental procedure as in
Section 3.1.2 and obtain the results for Russian (Fig-
ure 3(a)) and Spanish (Figure 3(b)). We see that re-
sults are very similar to the ones we obtained for
English. The results are again significant for both
Russian (P=0.004) and Spanish (P=0.028).
3.3 Influence of Genre on the Strength of the
Effect
We have established that entropy increases with the
sentence number in the works of fiction. We ob-
serve, however, that the effect is smaller than re-
ported in our previous work (Genzel and Charniak,
2002) for Wall Street Journal articles. This is to be
expected, since business and news writing tends to
be more structured and informative in nature, grad-
ually introducing the reader to the topic. Context,
therefore, plays greater role in this style of writing.
To further investigate the influence of genre and
style on the strength of the effect we perform exper-
iments on data from British National Corpus (Leech,
1992) which is marked by genre.
For each genre, we extract first ten sentences of
each genre subdivision of ten or more sentences.
90% of this data is used as training data and 10%
as testing data. Testing data is separated into ten
sets: all the first sentences, all the second sentences,
and so on. We then use a trigram model trained on
the training data set to find the average per-word en-
tropy for each set. We obtain ten numbers, which
in general tend to increase with the sentence num-
ber. To find the degree to which they increase, we
compute the correlation coefficient between the en-
tropy estimates and the sentence numbers. We report
these coefficients for some genres in Table 1. To en-
sure reliability of results we performed the described
process 400 times for each genre, sampling different
testing sets.
The results are very interesting and strongly sup-
port our assumption that informative and struc-
tured (and perhaps better-written) genres will have
1 2 3 4 59.2
9.3
9.4
9.5
9.6
9.7
9.8
Sentence number since beginning of paragraph
En
tro
py 
in b
its
Real run    
Control runs
(a) Russian
1 2 3 4 58.2
8.25
8.3
8.35
8.4
8.45
8.5
8.55
8.6
8.65
En
tro
py 
in b
its
Sentence number since beginning of paragraph
Real run    
Control runs
(b) Spanish
Figure 3: War and Peace
stronger correlations between entropy and sentence
number. There is only one genre, tabloid newspa-
pers6, that has negative correlation. The four gen-
res with the smallest correlation are all quite non-
informative: tabloids, popular magazines, advertise-
ments7 and poetry. Academic writing has higher
correlation coefficients than non-academic. Also,
humanities and social sciences writing is probably
more structured and better stylistically than science
and engineering writing. At the bottom of the table
we have genres which tend to be produced by pro-
fessional writers (biography), are very informative
(TV news feed) or persuasive and rhetorical (parlia-
mentary proceedings).
3.4 Conclusions
We have demonstrated that paragraph boundaries of-
ten cause the entropy to decrease, which seems to
support the Entropy Rate Constancy principle. The
effects are not very large, perhaps due to the fact
6Perhaps, in this case the readers are only expected to look
at the headlines.
7Advertisements could be called informative, but they tend
to be sets of loosely related sentences describing various fea-
tures, often in no particular order.
that each new paragraph does not necessarily rep-
resent a shift of topic. This is especially true in a
medium like the Wall Street Journal, where articles
are very focused and tend to stay on one topic. In
fiction, paragraphs are often used to mark a topic
shift, but probably only a small proportion of para-
graph breaks in fact represents topic shifts. We also
observed that more informative and structured writ-
ing is subject to stronger effect than speculative and
imaginative one, but the effect is present in almost
all writing.
In the next section we will discuss the potential
causes of the entropy results presented both in the
preceding and this work.
4 Investigating Non-Lexical Causes
In our previous work we discuss potential causes
of the entropy increase. We find that both lexical
(which words are used) and non-lexical (how the
words are used) causes are present. In this section
we will discuss possible non-lexical causes.
We know that some non-lexical causes are
present. The most natural way to find these causes is
to examine the parse trees of the sentences. There-
fore, we collect a number of statistics on the parse
BNC genre Corr. coef.
Tabloid newspapers ?0.342? 0.014
Popular magazines 0.073? 0.016
Print advertisements 0.175? 0.015
Fiction: poetry 0.261? 0.013
Religious texts 0.328? 0.012
Newspapers: commerce/finance 0.365? 0.013
Non-acad: natural sciences 0.371? 0.012
Official documents 0.391? 0.012
Fiction: prose 0.409? 0.011
Non-acad: medicine 0.411? 0.013
Newspapers: sports 0.433? 0.047
Acad: natural sciences 0.445? 0.010
Non-acad: tech, engineering 0.478? 0.011
Non-acad: politics, law, educ. 0.512? 0.004
Acad: medicine 0.517? 0.007
Acad: tech, engineering 0.521? 0.010
Newspapers: news reportage 0.541? 0.009
Non-acad: social sciences 0.541? 0.008
Non-acad: humanities 0.598? 0.007
Acad: politics, laws, educ. 0.619? 0.006
Newspapers: miscellaneous 0.622? 0.009
Acad: humanities 0.676? 0.007
Commerce/finance, economics 0.678? 0.007
Acad: social sciences 0.688? 0.004
Parliamentary proceedings 0.774? 0.002
TV news script 0.850? 0.002
Biographies 0.894? 0.001
Table 1: Correlation coefficient for different genres
trees and investigate if any statistics show a signifi-
cant change with the sentence number.
4.1 Experimental Setup
We use the whole Penn Treebank corpus (Marcus et
al., 1993) as our data set. This corpus contains about
50000 parsed sentences.
Many of the statistics we wish to compute are very
sensitive to the length of the sentence. For example,
the depth of the tree is almost linearly related to the
sentence length. This is important because the aver-
age length of the sentence varies with the sentence
number. To make sure we exclude the effect of the
sentence length, we need to normalize for it.
We proceed in the following way. Let T be the set
of trees, and f : T ? R be some statistic of a tree.
Let l(t) be the length of the underlying sentence for
0 2 4 6 8 100.985
0.99
0.995
1
1.005
1.01
1.015
Bucket number (for sentence number)
Ad
just
ed 
tree
 de
pth
Figure 4: Tree Depth
tree t. Let L(n) = {t|l(t) = n} be the set of trees of
size n. Let Lf (n) be defined as 1|L(n)|
?
t?L(n) f(t),
the average value of the statistic f on all sentences
of length n. We then define the sentence-length-
adjusted statistic, for all t, as
f ?(t) =
f(t)
Lf (l(t))
The average value of the adjusted statistic is now
equal to 1, and it is independent of the sentence
length.
We can now report the average value of each
statistic for each sentence number, as we have done
before, but instead we will group the sentence num-
bers into a small number of ?buckets? of exponen-
tially increasing length8. We do so to capture the
behavior for all the sentence numbers, and not just
for the first ten (as before), as well as to lump to-
gether sentences with similar sentence numbers, for
which we do not expect much variation.
4.2 Tree Depth
The first statistic we consider is also the most nat-
ural: tree depth. The results can be seen in Figure
4.
In the first part of the graph we observe an in-
crease in tree depth, which is consistent with the in-
creasing complexity of the sentences. In the later
8For sentence number n we compute the bucket number as
blog1.5 nc
0 2 4 6 8 100.96
0.98
1
1.02
1.04
1.06
1.08
1.1
1.12
1.14
Bucket number (for sentence number)
Ad
jus
ted
 bra
nch
ing
 fac
tor
Branching factor
NPs only        
Base NPs only   
Figure 5: Branching factor
sentences, the depth decreases slightly, but still stays
above the depth of the first few sentences.
4.3 Branching Factor and NP Size
Another statistic we investigate is the average
branching factor, defined as the average number of
children of all non-leaf nodes in the tree. It does
not appear to be directly correlated with the sentence
length, but we normalize it to make sure it is on the
same scale, so we can compare the strength of re-
sulting effect.
Again, we expect lower entropy to correspond to
flatter trees, which corresponds to large branching
factor. Therefore we expect the branching factor to
decrease with the sentence number, which is indeed
what we observe (Figure 5, solid line).
Each non-leaf node contributes to the average
branching factor. It is likely, however, that the
branching factor changes with the sentence num-
ber for certain types of nodes only. The most obvi-
ous contributors for this effect seem to be NP (noun
phrase) nodes. Indeed, one is likely to use several
words to refer to an object for the first time, but only
a few words (even one, e.g., a pronoun) when refer-
ring to it later. We verify this intuitive suggestion,
by computing the branching factor for NP, VP (verb
phrase) and PP (prepositional phrase) nodes. Only
NP nodes show the effect, and it is much stronger
(Figure 5, dashed line) than the effect for the branch-
0 2 4 6 8 100.98
0.99
1
1.01
1.02
1.03
1.04
1.05
Bucket number (for sentence number)
Ad
jus
ted
 bra
nch
ing
 fac
tor
Branching factor             
Branching factor w/o base NPs
Figure 6: Branching Factor without Base NPs
ing factor.
Furthermore, it is natural to expect that most of
this effect arises from base NPs, which are defined
as the NP nodes whose children are all leaf nodes.
Indeed, base NPs show a slightly more pronounced
effect, at least with regard to the first sentence (Fig-
ure 5, dotted line).
4.4 Further Investigations
We need to determine whether we have accounted
for all of the branching factor effect, by proposing
that it is simply due to decrease in the size of the base
NPs. To check, we compute the average branching
factor, excluding base NP nodes.
By comparing the solid line in Figure 6 (the origi-
nal average branching factor result) with the dashed
line (base NPs excluded), you can see that base NPs
account for most, though not all of the effect. It
seems, then, that this problem requires further in-
vestigation.
4.5 Gapping
Another potential reason for the increase in the sen-
tence complexity might be the increase in the use of
gapping. We investigate whether the number of the
ellipsis constructions varies with the sentence num-
ber. We again use Penn Treebank for this experi-
0 2 4 6 8 100.4
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
Bucket number (for sentence number)
Ad
just
ed 
num
ber
 of 
gap
s
Figure 7: Number of ellipsis nodes
ment9.
As we can see from Figure 7, there is indeed a sig-
nificant increase in the use of ellipsis as the sentence
number increases, which presumably makes the sen-
tences more complex. Only about 1.5% of all the
sentences, however, have gaps.
5 Future Work and Conclusions
We have discovered a number of interesting facts
about the variation of sentences with the sentence
number. It has been previously known that the com-
plexity of the sentences increases with the sentence
number. We have shown here that the complexity
tends to decrease at the paragraph breaks in accor-
dance with the Entropy Rate Constancy principle.
We have verified that entropy also increases with the
sentence number outside of Wall Street Journal do-
main by testing it on a work of fiction. We have also
verified that it holds for languages other than En-
glish. We have found that the strength of the effect
depends on the informativeness of a genre.
We also looked at the various statistics that show
a significant change with the sentence number, such
as the tree depth, the branching factor, the size of
noun phrases, and the occurrence of gapping.
Unfortunately, we have been unable to apply these
results successfully to any practical problem so far,
9Ellipsis nodes in Penn Treebank are marked with *?* .
See Bies et al (1995) for details.
primarily because the effects are significant on av-
erage and not in any individual instances. Finding
applications of these results is the most important
direction for future research.
Also, since this paper essentially makes state-
ments about human processing, it would be very ap-
propriate to to verify the Entropy Rate Constancy
principle by doing reading time experiments on hu-
man subjects.
6 Acknowledgments
We would like to acknowledge the members of the
Brown Laboratory for Linguistic Information Pro-
cessing and particularly Mark Johnson for many
useful discussions. This research has been supported
in part by NSF grants IIS 0085940, IIS 0112435, and
DGE 9870676.
References
A. Bies, M. Ferguson, K. Katz, and R. MacIntyre, 1995.
Bracketing Guidelines for Treebank II Style Penn Tree-
bank Project. Penn Treebank Project, University of
Pennsylvania.
D. Genzel and E. Charniak. 2002. Entropy rate con-
stancy in text. In Proceedings of ACL?2002, Philadel-
phia.
J. T. Goodman. 2001. A bit of progress in language mod-
eling. Computer Speech and Language, 15:403?434.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn treebank. Computational Linguistics,
19:313?330.
J. B. Plotkin and M. A. Nowak. 2000. Language evo-
lution and information theory. Journal of Theoretical
Biology, pages 147?159.
C. E. Shannon. 1948. A mathematical theory of commu-
nication. The Bell System Technical Journal, 27:379?
423, 623?656, July, October.
L. Tolstoy. 1869. War and Peace. Available online,
in 4 languages (Russian, English, Spanish, Italian):
http://www.magister.msk.ru/library/tolstoy/wp/wp00.htm.
Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 49?54,
Portland, Oregon, June 23, 2011. c?2011 Association for Computational Linguistics
Extractive Multi-Document Summaries Should Explicitly Not Contain
Document-Specific Content
Rebecca Mason and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{rebecca,ec}@cs.brown.edu
Abstract
Unsupervised approaches to multi-document
summarization consist of two steps: find-
ing a content model of the documents to be
summarized, and then generating a summary
that best represents the most salient informa-
tion of the documents. In this paper, we
present a sentence selection objective for ex-
tractive summarization in which sentences are
penalized for containing content that is spe-
cific to the documents they were extracted
from. We modify an existing system, HIER-
SUM (Haghighi & Vanderwende, 2009), to use
our objective, which significantly outperforms
the original HIERSUM in pairwise user eval-
uation. Additionally, our ROUGE scores ad-
vance the current state-of-the-art for both su-
pervised and unsupervised systems with sta-
tistical significance.
1 Introduction
Multi-document summarization is the task of gener-
ating a single summary from a set of documents that
are related to a single topic. Summaries should con-
tain information that is relevant to the main ideas of
the entire document set, and should not contain in-
formation that is too specific to any one document.
For example, a summary of multiple news articles
about the Star Wars movies could contain the words
?Lucas ?and ?Jedi?, but should not contain the name
of a fan who was interviewed in one article. Most
approaches to this problem generate summaries ex-
tractively, selecting whole or partial sentences from
the original text, then attempting to piece them to-
gether in a coherent manner. Extracted text is se-
lected based on its relevance to the main ideas of the
document set. Summaries can be evaluated manu-
ally, or with automatic metrics such as ROUGE (Lin,
2004).
The use of structured probabilistic topic models
has made it possible to represent document set con-
tent with increasing complexity (Daume? & Marcu,
2006; Tang et al, 2009; Celikyilmaz & Hakkani-
Tur, 2010). Haghighi and Vanderwende (2009)
demonstrated that these models can improve the
quality of generic multi-document summaries over
simpler surface models. Their most complex hier-
archial model improves summary content by teasing
out the words that are not general enough to repre-
sent the document set as a whole. Once those words
are no longer included in the content word distri-
bution, they are implicitly less likely to appear in
the extracted summary as well. But this objective
does not sufficiently keep document-specific content
from appearing in multi-document summaries.
In this paper, we present a selection objective that
explicitly excludes document-specific content. We
re-implement the HIERSUM system from Haghighi
and Vanderwende (2009), and show that using our
objective dramatically improves the content of ex-
tracted summaries.
2 Modeling Content
The easiest way to model document content is to find
a probability distribution of all unigrams that appear
in the original documents. The highest frequency
words (after removing stop words) have a high like-
lihood of appearing in human-authored summaries
(Nenkova & Vanderwende, 2005). However, the raw
49
Figure 1: The graphical model for HIERSUM (Haghighi
& Vanderwende, 2009).
unigram distribution may contain words that appear
frequently in one document, but do not reflect the
content of the document set as a whole.
Probabilistic topic models provide a more prin-
cipled approach to finding a distribution of content
words. This idea was first presented by Daume?
and Marcu (2006) for their BAYESUM system for
query-focused summarization, and later adapted for
non-query summarization in the TOPICSUM system
by Haghighi and Vanderwende (2009). 1 In these
systems, each word from the original documents is
drawn from one of three vocabulary distributions.
The first, ?b, is the background distribution of gen-
eral English words. The second, ?d, contains vo-
cabulary that is specific to that one document. And
the third, ?c, is the distribution of content words for
that document set, and contains relevant words that
should appear in the generated summary.
HIERSUM (Haghighi & Vanderwende, 2009)
adds more structure to TOPICSUM by further split-
ting the content distribution into multiple sub-topics.
The content words in each sentence can be gener-
ated by either the general content topic or the con-
tent sub-topic for that sentence, and the words from
the general content distribution are considered when
building the summary.
1The original BAYESUM can also be used without a query,
in which case, BAYESUM and TOPICSUM are the exact same
model.
3 KL Selection
The KL-divergence between two unigram word dis-
tributions P and Q is given by KL(P ||Q) =
?
w P (w) log
P (w)
Q(w) . This quantity is used for sum-
mary sentence selection in several systems includ-
ing Lerman and McDonald (2009) and Haghighi
and Vanderwende (2009), and was used as a feature
in the discrimitive sentence ranking of Daume? and
Marcu (2006).
TOPICSUM and HIERSUM use the following KL
objective, which finds S?, the summary that min-
imizes the KL-divergence between the estimated
content distribution ?c and the summary word dis-
tribution PS:
S? = min
S:|S|?L
KL(?c||PS)
A greedy approximation is used to find S?. Start-
ing with an empty summary, sentences are greedily
added to the summary one at a time until the sum-
mary has reached the maximum word limit, L. The
values of PS are smoothed uniformly in order to en-
sure finite values of KL(?c||PS).
4 Why Document-Specific Words are a
Problem
The KL selection objective effectively ensures the
presence of highly weighted content words in the
generated summary. But it is asymmetric in that it
allows a high proportion of words in the summary
to be words that appear infrequently, or not at all,
in the content word distribution. This asymmetry
is the reason why the KL selection metric does not
sufficiently keep document-specific words out of the
generated summary.
Consider what happens when a document-specific
word is included in summary S. Assume that the
word wi does not appear (has zero probability) in
the content word distribution ?c, but does appear in
the document-specific distribution ?d for document
d. Then wi appearing in S has very little impact
on KL(?c||PS) =
?
j ?c(wj) log
?c(wj)
PS(wj)
because
?c(wi) = 0. There will be a slight impact because
the presence of the wordwi in S will cause the prob-
ability of other words in the summary to be sligntly
smaller. But in a summary of length 250 words (the
50
length used for the DUC summarization task) the
difference is negligible.
The reason why we do not simply substitute
a symmetrical metric for comparing distributions
(e.g., Information Radius) is because we want the se-
lection objective to disprefer only document-specific
words. Specifically, the selection objective should
not disprefer background English vocabulary.
5 KL(c)-KL(d) Selection
In contrast to the KL selection objective, our ob-
jective measures the similarity of both content and
document-specific word distributions to the ex-
tracted summary sentences. We combine these mea-
sures linearly:
S? = min
S:|S|?L
KL(?c||PS)?KL(?d||PS)
Our objective can be understood in comparison
to the MMR criterion by (Carbonell & Goldstein,
1998), which also utilizes a linear metric in order to
maximize informativeness of summaries while min-
imizing some unwanted quality of the extracted sen-
tences (in their case, redundancy). In contrast, our
criterion utilizes information about what kind of in-
formation should not be included in the summary,
which to our knowledge has not been done in previ-
ous summarization systems.2
For comparison to the previous KL objective, we
also use a greedy approximation for S?. However,
because we are extracting sentences from many doc-
uments, the distribution ?d is actually several distri-
butions, a separate distribution for each document
in the document set. The implementation we used
in our experiments is that, as we consider a sen-
tence s to be added to the previously selected sen-
tences S, we set ?d to be the document-specific
distribution of the document that s has been ex-
tracted from. So each time we add a sentence to
the summary, we find the sentence that minimizes
KL(?c||PS?s)?KL(?d(s)||PS?s). Another imple-
mentation we tried was combining all of the ?d dis-
tributions into one distribution, but we did not notice
any difference in the extracted summaries.
2A few anonymous reviewers asked if we tried to optimize
the value of ? for KL(?c||PS) ? ?KL(?d||PS). The answer
is yes, but optimizing ? to maximize ROUGE results in sum-
maries that are perceptibly worse, and manually tuning ? did
not seem to produce any benefit.
6 Evaluation
6.1 Data
We developed our sentence selection objective us-
ing data from the Document Understanding Con-
ference3 (DUC) 2006 summarization task, and used
data from DUC 2007 task for evaluations. In these
tasks, the system is given a set of 25 news arti-
cles related to an event or topic, and needs to gen-
erate a summary of under 250 words from those
documents.4 For each document set, four human-
authored summaries are provided for use with eval-
uations. The DUC 2006 data has 50 document sets,
and the DUC 2007 data has 45 document sets.
6.2 Automatic Evaluation
Systems are automatically evalatued using ROUGE
(Lin, 2004), which has good correlation with hu-
man judgments of summary content. ROUGE com-
pares n-gram recall between system-generated sum-
maries, and human-authored reference summaries.
The first two metrics we compare are unigram and
bigram recall, R-1 and R-2, respectively. The last
metric, R-SU4, measures recall of skip-4 bigrams,
which may skip one or two words in between the
two words to be measured. We set ROUGE to stem
both the system and reference summaries, scale our
results by 102 and present scores with and without
stopwords removed.
The ROUGE scores of the original HIERSUM sys-
tem are given in the first row of table 1, followed
by the scores of HIERSUM using our KL(c-d) se-
lection. The KL(c-d) selection outperforms the KL
selection in each of the ROUGE metrics shown. In
fact, these results are statistically significant over
the baseline KL selection for all but the unigram
metrics (R-1 with and without stopwords). These
results show that our KL(c-d) selection yields sig-
nificant improvements in terms of ROUGE perfor-
mance, since having fewer irrelevant words in the
summaries leaves room for words that are more rel-
evant to the content topic, and therefore more likely
to appear in the reference summaries.
The last two rows of table 1 show the scores
of two recent state-of-the-art multi-document sum-
3http://duc.nist.gov/
4Some DUC summarization tasks also provide a query or
focus for the summary, but we ignore these in this work.
51
System ROUGE w/o stopwords ROUGE w/ stopwords
R-1 R-2 R-SU4 R-1 R-2 R-SU4
HIERSUM w/ KL 34.6 7.3 10.4 43.1 9.7 15.3
HIERSUM w/ KL(c)-KL(d) 35.6 9.9 12.8 43.2 11.6 16.6
PYTHY 35.7 8.9 12.1 42.6 11.9 16.8
HYBHSUM 35.1 8.3 11.8 45.6 11.4 17.2
Table 1: ROUGE scores on the DUC 2007 document sets. The first two rows compare the results of the unigram
HIERSUM system with its original and our improved selection metrics. Bolded scores represent where our system has
a significant improvement over the orignal HIERSUM. For further comparison, the last two rows show the ROUGE
scores of two other state-of-the-art multi-document summarization systems (Toutanova et al, 2007; Celikyilmaz &
Hakkani-Tur, 2010). See section 6.2 for more details.
marization systems. Both of these systems se-
lect sentences discriminatively on many features
in order to maximize ROUGE scores. The first,
PYTHY (Toutanova et al, 2007), trains on dozens
of sentence-level features, such as n-gram and skip-
gram frequency, named entities, sentence length and
position, and also utilizes sentence compression.
The second, HYBHSUM (Celikyilmaz & Hakkani-
Tur, 2010), uses a nested Chinese restaurant process
(Blei et al, 2004) to model a hierarchical content
distribution with more complexity than HIERSUM,
and uses a regression model to predict scores for new
sentences.
For both of these systems, our summaries are sig-
nificantly better for R-2 and R-SU4 without stop-
words, and comparable in all other metrics.5 These
results show that our selection objective can make
a simple unsupervised model competitive with more
complicated supervised models.
6.3 Manual Evaluation
For manual evaluation, we performed a pairwise
comparison of summaries generated by HIERSUM
with both the original and our modified sentence se-
lection objective. Users were given the two sum-
maries to compare, plus a human-generated refer-
ence summary. The order that the summaries ap-
peared in was random. We asked users to select
which summary was better for the following ques-
5Haghighi and Vanderwende (2009) presented a version of
HIERSUM that models documents as a bag of bigrams, and pro-
vides results comparable to PYTHY. However, the bigram HI-
ERSUM model does not find consistent bags of bigrams.
System Q1 Q2 Q3 Q4
HIERSUM w/ KL 29 36 31 36
. . . w/ KL(c)-KL(d) 58 51 56 51
Table 2: Results of manual evaluation. Our criterion out-
performs the original HIERSUM for all attributes, and is
significantly better for Q1 and Q3. See section 6.3 for
details.
tions:6
Q1 Which was better in terms of overall content?
Q2 Which summary had less repetition?
Q3 Which summary was more coherent?
Q4 Which summary had better focus?
We took 87 pairwise preferences from participants
over Mechanical Turk.7 The results of our evalu-
ation are shown in table 2. For all attributes, our
criterion performs better than the original HIERSUM
selection criterion, and our results for Q1 and Q3 are
significantly better as determined by Fisher sign test
(two-tailed P value < 0.01).
These results confirm that our objective noticably
improves the content of extractive summaries by se-
lecting sentences that contain less document-specific
6These are based on the manual evaluation questions from
DUC 2007, and are the same questions asked in Haghighi and
Vanderwende (2009).
7In order to ensure quality results, we asked participants to
write a sentence on why they selected their preference for each
question. We also monitored the time taken to complete each
comparison. Overall, we rejected about 25% of responses we
received, which is similar to the percentage of responses re-
jected by Gillick and Liu (2010).
52
information. This leaves more room in the summary
for content that is relevant to the main idea of the
document set (Q1) and keeps out content that is not
relevant (Q4). Additionally, although neither crite-
rion explicitly addresses coherence, we found that a
significant proportion of users found our summaries
to be more coherent (Q3). We believe this may be
the case because the presence of document-specific
information can distract from the main ideas of the
summary, and make it less likely that the extracted
sentences will flow together.
There is no immediate explanation for why users
found our our summaries less repetitive (Q2), since
if anything the narrowing of topics due to the neg-
ative KL(?d||PS) term should make for more rep-
etition. We currently hypothesize that the improved
score is simply a spillover from the general improve-
ment in document quality.
7 Conclusion
We have described a new objective for sentence se-
lection in extractive multi-document summarization,
which is different in that it explicitly gives negative
weight to sentences that contain document-specific
words. Our objective significantly improves the per-
formance of an existing summarization system, and
improves on current best ROUGE scores with sig-
nificance.
We have observed that while the content in our
extracted summaries is often comparable to the con-
tent in human-written summaries, the extracted sum-
maries are still far weaker in terms of coherence and
repetition. Even though our objective significantly
improves coherence, more sophisticated methods of
decoding are still needed to produce readable sum-
maries. These problems could be addressed through
further refinement of the selection objective, through
simplification or compression of selected sentences,
and through improving the coherence of generated
summaries.
References
Blei, D. M., Griffiths, T. L., Jordan, M. I., & Tenen-
baum, J. B. (2004). Hierarchical topic models and
the nested chinese restaurant process. Advances
in Neural Information Processing Systems.
Carbonell, J., & Goldstein, J. (1998). The use
of mmr, diversity-based reranking for reordering
documents and producing summaries. Proceed-
ings of the 21st annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval (pp. 335?336). New York, NY,
USA: ACM.
Celikyilmaz, A., & Hakkani-Tur, D. (2010). A hy-
brid hierarchical model for multi-document sum-
marization. Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics (pp. 815?824). Stroudsburg, PA, USA: Asso-
ciation for Computational Linguistics.
Daume?, III, H., & Marcu, D. (2006). Bayesian
query-focused summarization. ACL-44: Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguis-
tics (pp. 305?312). Morristown, NJ, USA: Asso-
ciation for Computational Linguistics.
Gillick, D., & Liu, Y. (2010). Non-expert evaluation
of summarization systems is risky. Proceedings
of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Me-
chanical Turk (pp. 148?151). Stroudsburg, PA,
USA: Association for Computational Linguistics.
Haghighi, A., & Vanderwende, L. (2009). Exploring
content models for multi-document summariza-
tion. Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (pp. 362?370). Boulder, Col-
orado: Association for Computational Linguis-
tics.
Lerman, K., & McDonald, R. (2009). Contrastive
summarization: an experiment with consumer re-
views. Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, Companion Volume:
Short Papers (pp. 113?116). Stroudsburg, PA,
USA: Association for Computational Linguistics.
Lin, C.-Y. (2004). Rouge: a package for auto-
matic evaluation of summaries. Proceedings of
53
the Workshop on Text Summarization Branches
Out (WAS 2004). Barcelona, Spain.
Nenkova, A., & Vanderwende, L. (2005). The im-
pact of frequency on summarization (Technical
Report). Microsoft Research.
Tang, J., Yao, L., & Chen, D. (2009). Multi-topic
based query-oriented summarization. SDM?09
(pp. 1147?1158).
Toutanova, K., Brockett, C., Gamon, M., Jagarla-
mudi, J., Suzuki, H., & Vanderwende, L. (2007).
The PYTHY Summarization System: Microsoft
Research at DUC 2007. Proc. of DUC.
54
Proceedings of the NAACL HLT Workshop on Vision and Language (WVL ?13), pages 1?9,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Annotation of Online Shopping Images without Labeled Training Examples
Rebecca Mason and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University, Providence, RI 02912
{rebecca,ec}@cs.brown.edu
Abstract
We are interested in the task of image an-
notation using noisy natural text as training
data. An image and its caption convey dif-
ferent information, but are generated by the
same underlying concepts. In this paper, we
learn latent mixtures of topics that generate
image and product descriptions on shopping
websites by adapting a topic model for multi-
lingual data (Mimno et al, 2009). We use the
trained model to annotate test images without
corresponding text. We capture visual prop-
erties such as color, texture, shape, and ori-
entation by computing low-level image fea-
tures, and measure the contribution of each
type of visual feature towards the accuracy of
the model. Our model significantly outper-
forms both a competitive baseline and a pre-
vious topic model-based system.
1 Introduction
Image annotation is a classic problem in Computer
Vision. Given a query image, the task is to gen-
erate a set of textual labels that describe the visual
content. The typical approach to these problems is
to use supervised models, which require large num-
bers of hand-annotated examples for each of the la-
bels. However, the amount of information available
on the web continues to grow, the task of organiz-
ing and describing visual data becomes increasingly
complex. For example, a shopping website might ar-
range products into broad categories such as ?shoes?
and ?handbags? with each category containing tens
of thousands of products that are difficult for users
to search and navigate. It is often infeasible to dis-
cover all of the attributes within those categories that
are relevant to users and create labeled training ex-
amples for each of them.
Instead, we approach this problem by discovering
visual attributes from noisy natural language cap-
tions. That is, given a collection of images and cap-
tions found on the web, we learn a model of visual
and textual features. Then given a query image with
no text, we can generate likely descriptive words.
This is a difficult task because image captions on the
web are often noisy and incomplete: some captions
might not describe a particular visual feature, might
use a synonym for that feature, or might describe in-
formation that is not visual in the image at all.
A secondary motivation for this work is to use the
image annotations as a component in language gen-
eration systems such as for automatic image caption-
ing. We point to examples of previous work such
as Feng and Lapata (2010a) where image annota-
tions generated from a topic model are used to help
generate full sentences to describe images. Much of
the current research in image captioning is limited
by the current technology for object recognition in
Computer Vision. For example, SBU-Flickr dataset
(Ordonez et al, 2011) with 1 million images and
captions, is considered to be general-domain but is
actually built by querying Flickr using a pre-defined
term list related to visual attributes that there are
trained recognition systems for. While these sys-
tems can accurately generate descriptions for com-
mon visual objects and attributes, they are not as
well-suited for describing the ?long-tail? of visual
attributes which appear in many domain-specific
1
Two adjustable buckle straps top a
classic rubber rain boot grounded by
a thick lug sole for excellent wet-
weather traction.
Size(s) Available: 6, 11.5. Brand &
Style - VANS Kvd Width - Medium
(B, M) Heel Height - Shoe Size is
Womens Size 11.5 = Mens Size 10
1 Inch Heel Material - Canvas Upper
and Man Made Sole
Carlo Fellini - Evening clutch
beaded on a wave pattern
Table 1: Examples of data from the Attribute Discovery Dataset (Berg et al, 2010). The images are fairly clean and
uniform, while captions have more noise and variation.
datasets.
In this paper, we model image and text features
from the training data using a generative model. We
adapt the Polylingual topic model from Mimno et
al. (2009) to train on multi-modal data, and then use
the trained model to generate annotations for test im-
ages. We evaluate our model on two categories of
shopping images using a variety of types of com-
puted image features. For image annotation we out-
perform both a difficult baseline and previous work.
2 Related Work
We use the polylingual topic model from Mimno et
al. (2009), which was developed to model multi-
lingual corpora that are topically comparable be-
tween languages ? the documents are not direct
translations, but they cover the same ideas. For ex-
ample, English and Finnish Wikipedia pages about
skiing are roughly similar, but the subject is covered
more thoroughly in Finnish. Therefore, the number
of tokens assigned to the Finnish topic for skiing is
much higher than it is in the English. While Mimno
et al (2009) show that the model is effective in tasks
such as modeling topically comparable documents
across languages, our work is the first to show that
this model can be used to model data of different
modalities. Another quality of the polylingual topic
model is that words in different languages do not di-
rectly correspond with each other. This is a feature
of other multi-lingual topic models but would not
work for multi-modal data because a textual word
can carry more meaning by itself than an image fea-
ture can.
Countless approaches have been proposed for the
use of topic models in image annotation, but the
vast majority of these approaches consider the text
modality merely as labels for the image modality.
The most highly cited of these is the Correspon-
dence LDA (corr-LDA) model of Blei et al (2003),
where topics are learned using the image modality
alone, and each textual word must be generated by a
specific region in the image. However, more recent
work has started to recognize the textual modality
as a source of information in its own right. Jia et
al. (2011) present a model that allows different in-
formation to be emphasized in each modality, but it
requires very clean text; they do not use documents
with captions that cannot be easily parsed or pro-
cessed. Then, they stem all words, and disregard
sparse word tokens. This works when working with
sources such as Wikipedia, where text captions are
highly edited and consistently formatted. In compar-
ision, our work can be trained on corpra where the
text has poor or inconsistant quality. Additionally,
their work was for the task of image retrieval from a
text query, while we are generating text annotations
for a query image.
Our work is most similar to the MixLDA model
of Feng and Lapata (2010b), except MixLDA mod-
2
els images and their related text as a single bag-
of-features, with visual and textual features coming
from the same vocabulary. This means that some
topics should have a greater proportion of features
from one of modalities, if there is an idea that is bet-
ter expressed in one over the other. Their model was
developed for finding descriptive words given both
an image and a news article, and can also be used
on large and noisy amounts of data, so we compare
MixLDA against our model in the experiments.
Although we use the Attribute Discovery Dataset
of Berg et al (2010), their work is different from
ours in both problem formulation and the types of
attributes discovered. Their primary interest is to
characterize attributes according to how they are vi-
sually represented: global or local; color, texture, or
shape. Their work does not address the task of pre-
dicting attributes for unseen images. Additionally,
they do not work with individual descriptive words,
but cluster them using mutual information of vi-
sual attributes, creating a smaller number of ?visual
synsets?. For example, one of their visual synsets
for images and descriptions of womens handbags is
{mesh, interior, metal} and another is {silver, metal-
lic}. In comparison, in the topic model the same
word can be generated by more than one topic.
Liu et al (2010) examine the use of a variety of
image features in a Bayesian model in order to mea-
sure which are the best for classifying diverse ma-
terials such as stone, glass, and plastic. They found
that the image features they used for shape and color
were better indicators of the material of an object
than texture features, and their best combined model
did not include texture as a feature at all. We are also
interested in finding out whether our performance on
generating descriptive words is affected by different
types of image features.
3 Dataset
We use the Attribute Discovery Dataset from Berg
et al (2010).1 The dataset consists of pairs of im-
ages and captions taken from the shopping website
like.com. The data has four categories: women?s
shoes, handbags, earrings, and neckties. We run our
model on two categories, shoes and handbags, due
1http://tamaraberg.com/
attributesDataset/index.html
to their larger sizes ? 14764 and 9145 image-caption
pairs respectively ? and diversity of features. This is
a reasonable amount of data in the shopping images
domain; more than half of the number of compa-
rable products sold on large retail websites such as
Zappos.com or Amazon.com.
Compared to general datasets such as Pascal
Sentences, the images in the Attribute Discovery
Dataset are more uniform. All image files are
280x280 pixel JPEGs, and images of products are
typically taken from similar angles against a white
or a light-colored solid background. Only rarely do
the images have noisy backgrounds, such as a per-
son wearing the item, or the same item displayed in
multiple colors in one image. However, this does not
necessarily make our task much easier, since the vi-
sual attributes we wish to learn are not pre-defined as
they are in a general-domain dataset. And the lack of
hand-annotated data means no negative examples of
when an attribute is not present, which are typically
used to train visual classifiers.
Furthermore, the captions are extremely noisy in
this dataset. Compared to the 20 object types in the
Pascal Sentences dataset, or about one hundred in
COREL, here there are thousands of words that can
be used to describe features in the images, includ-
ing synonyms, multiple stems of words, and mis-
spellings. In addition to explicit visual descriptions
of the products, the captions describe ?less visual?
features such as details about the construction of the
item, during which season or activity it would be ap-
propriate to wear, or feelings that could be evoked
by looking at the item. These features are difficult
to represent as specific visual attributes, but can be
identified visually by domain-experts. Captions can
also include information that is non-visual such as
sizing and shipping information, or whether the item
is on sale.
The captions can be either full English sentences,
a list of features, or sometimes just a few words.
Longer captions in the dataset are truncated to 250
characters in length.
From our own obervations, we estimate about
10% of the captions in the shoes dataset contain
few or no descriptive words. At least 3.7% of the
shoes captions are entirely Javascript code, have sig-
nificant portions of code, or very long URLs. An-
other 5-6% either contain no information besides
3
sizing or shipping information, only the brand name
or model number of the shoe, or the caption is so
short that there are only one or two descriptive words
that could be used in our model. In the womens?
shoes category, we take some simple steps to remove
URLs and code to avoid learning accidental correla-
tion with legitimate features.2 However, we still use
all image and caption pairs in the training set, in-
cluding those which end up having empty captions,
since they are still useful for learning topics for vi-
sual features. For the handbags captions, we did not
try to remove code or long URLs since it seemed to
be less of a problem in that category.
4 Feature Representation
4.1 Text Features
The bag-of-words model is used for text. We use
Mxterminator (Reynar and Ratnaparkhi, 1997) to
split sentences in the captions (in many instances,
nothing is done in this step becuase there are no
full sentences in the caption), Stanford POS Tag-
ger(Toutanova et al, 2003) to tag words, then in-
clude adjectives, adverbs, verbs, and nouns in the
topic model (except for proper nouns and common
background English words from a stoplist). How-
ever, these tags are really more a rough estimate of
parts of speech due to the number of incomplete sen-
tences and phrases, and the fact that many of the
words used to describe styles or attributes of cloth-
ing have different meanings in colloqueal English.3
All tokens are converted to lower case, but there is
no stemming or lemmatization. After preprocessing,
the size of the shoes text vocabulary is 9578 words,
with an average of 16.33 descriptive words per im-
age, while the bags have a text vocabulary of 6309
word types with 15.41 descriptive words per image
on average.
4.2 Visual Features
The bag-of-features model is used for visual features
as well. Most of these features are standard in com-
puter vision research, and are also used in work we
cited in Section 2.
2Tokens removed: URLs, all tokens that end in ?.sh?, and a
few tokens obviously related to Javascript eg script, src, typeof,
var.
3Some examples of domain-specific words used in shopping
image descriptions: www.zappos.com/glossary
Shape: A SIFT descriptor describes which way
edges are oriented at a certain point in an image
(Lowe, 1999). It was develped to recognize the same
object under different scales and rotations. How-
ever, it is also commonly used for recognizing more
generalized types or features of objects. We use the
VLFeat open source library (Vedaldi and Fulkerson,
2008) to compute SIFT features at points of interest
and to cluster the SIFT features into discrete ?visual
terms? using the k-means algorithm. There are 750
visual terms for SIFT features.
Color: We use two representations for color,
RGB (red, green, blue) and HSV (hue, satura-
tion, value). 25 pixels are sampled from the cen-
ter 100x100 pixels of the image (to avoid sampling
from the background of the image). Those pixel val-
ues are also clustered to visual terms using k-means,
with 100 visual terms each.
Texture: Images are convolved with Gabor filters
at multiple orientations and scales, sampled at ran-
dom locations, then clustered to form texton features
for texture (Leung and Malik, 2001). We convert all
images to grayscale, then sample 25 locations from
the center of the image, and cluster to 100 visual
terms. We also have a color texton feature, where
we sample and cluster textons separately for the red,
green, and blue color channels.
Reflectance/Curvature:4 We use three types of
related features for gradients and curvature. The
first is a bag-of-HOG (histogram of gradients) fea-
ture set (Dalal and Triggs, 2005) computed over a
regular grid on the image to measure changes in in-
tensity.5 The most significant of those features (as
determined by L2 norm) are selected for each im-
age, and like previous features are clustered into vi-
sual terms using k-means. The second two types are
derivatives of HOG which include information about
the amount of curvature at each orientation of the
HOG descriptor.6
4
Figure 1: Polylingual topic model (Mimno et al, 2009)
5 Model
We model textual and visual features using the
polylingual topic model by Mimno et al (2009). In
this section, we describe how the generative process
and inference of this model is adapted to topically
comparable multi-modal data.
Figure 1 shows the original polylingual topic
model. We model multi-modal data using two ?lan-
guages?: txt for the bag-of-words captions, and img
for the combined visual terms. The generative pro-
cess is defined for an image and caption pair, w =<
wimg, wtxt >:
? ? Dir(?, ?m)
zimg ? P (zimg|?) =
?
n
?zimgn
ztxt ? P (ztxt|?) =
?
n
?ztxtn
wimg ? P (wimg|zimg,?img) = ?img
wimgn |z
img
n
wtxt ? P (wtxt|ztxt,?txt) = ?txtwtxtn |ztxtn
First, a topic distribution for w is drawn from an
asymmetric Dirichlet prior with concentration pa-
rameter ? and base measure m. Then a latent topic
assignment is drawn for each word token in wtxt,
and each discrete image feature in wimg. Once the
topic assignments are sampled, the observed tokens
are sampled according to their probability in the
modality-specific topics ?img = {?img1 , ..., ?
img
T }
and ?txt = {?txt1 , ..., ?
txt
T }.
4Note: These features are implemented using code from
(Felzenszwalb et al, ).
5There is significant overlap between these features, al-
though the benefits of overlap are lost due to the bag-of-features
model.
6Personal correspondance, work in progress.
To find the most probable descriptive words for an
unseen image, the first step is to estimate the topic
distribution that generated the image. Gibbs sam-
pling is used to sample topic assignments for visual
terms in the test image dimg:
P (zn = t|d
img, z\n,?
img, ?m)
? ?img
dimgn |t
(Nt)\n + ?mt
?
tNt ? 1 + ?
Assuming that the descriptive words are indepen-
dent, the probability of text word wi given dimg is:
P (wi|d
img) =
?
t
P (wi|z
txt
t )P (zt|d
img)
summing over all topics t ? T .
For training the model, we used the Polylingual
topic model implementation from the Mallet toolkit
(McCallum, 2002) (with some small modifcations
to use it for generation). We use 1000 iterations for
inference, with hyperparameter optimization every
10 iterations. In both shoes and bags categories, the
number of topics is 200, which was minimally tuned
by hand on the shoes data.
6 Experimental Setup and Evaluation
We first run our model on the larger category, shoes.
For both systems and baselines, we find the 10, 15,
and 20 most likely words for the test images. We
evaluate by computing precision and recall against
descriptive words from the held-out captions for
those images.7 We compute macro-averages of these
scores because there is a lot of variation between the
sizes of the captions in the dataset. The split between
training and test instances is 80/20%.
We also evaluate the contributions of different
types of image features. We evaluate the model for
each image feature individually (along with the text
features), as well as combinations of image features.
We compare against the MixLDA system and a
strong baseline. We choose MixLDA because it
is relatively easy to re-implement and because it
7We find descriptive words for test instances in the exact
same way we did for training instances in Section 4.1. Instances
where we did not find any useable descriptive words did not
count towards the evaluation.
5
10 words 15 words 20 words
P R F1 P R F1 P R F1
Baselines
MixLDA 21.02 13.80 16.66 17.41 17.15 17.13 14.88 19.53 16.89
Corpus frequency 21.03 13.73 16.61 17.51 17.14 17.32 15.41 20.12 17.45
Single Attribute
SIFT 27.00 16.30 20.34 22.84 20.65 21.69 20.09 24.22 21.96
Grayscale Texture 21.26 13.88 16.80 18.25 17.87 18.06 15.71 20.52 17.80
RGB Texture 24.77 14.93 18.63 21.01 18.99 19.95 18.49 22.29 20.21
HSV Color 22.17 13.35 16.67 18.59 16.79 17.65 16.48 19.85 18.01
RGB Color 23.21 13.98 17.45 19.78 17.88 18.78 17.53 21.12 19.15
HOG 26.33 15.87 19.80 22.36 20.21 21.23 19.60 23.62 21.42
TriHOG 24.60 14.82 18.50 20.64 18.66 19.60 18.14 21.87 19.83
TriHOG-Polar 26.03 15.69 19.58 22.06 19.94 20.95 19.32 23.29 21.12
Combined Models
All-Color 24.22 14.60 18.22 20.62 18.65 19.59 18.11 21.83 19.80
All-Texture 25.50 15.41 19.24 21.63 19.55 20.53 18.88 22.75 20.64
All-HOG 27.36 16.50 20.58 23.31 21.07 22.14 20.40 24.58 22.30
Combine All 29.31 17.70 22.04 24.88 22.49 23.63 21.71 26.16 23.73
SIFT+RGB Texture+HOG 28.62 17.25 21.52 24.35 22.01 23.12 21.20 25.55 23.17
Table 2: Results of evaluation in the women?s shoes cateogory (top 10-20 words).
has previously outperformed other image annota-
tion systems when trained on natural language cap-
tions. Because the MixLDA model originally only
used SIFT features, we compare it against the SIFT-
only version of our model, with each system using
the same computed image and text features. We
re-implement the MixLDA system mostly as it is
described in Feng and Lapata (2010b), with a few
changes to make it more comparable to our model:
Obviously in our version of MixLDA the test in-
stances are only the unseen image as there is no other
surrounding text. The number of topics is 200 (the
original MixLDA had more but that did not seem to
help here), and the ? and ? hyperparameters are op-
timized every 10 iterations.8
We also compare our model against corpus fre-
quency of words in the training set. Although this
may seem like a trivial baseline, previous work
8We used the Mallet toolkit?s Parallel LDA sampler for in-
ference, while a variational approach is used in the original.
However, we do not believe this would change the outcome of
this experiment. We also tried MixLDA without hyperparam-
eter optimization but we do not show those results as they are
significantly worse.
on image annotation from both computer vision
(Mu?ller et al, 2002; Monay and Gatica-Perez, 2003;
Barnard et al, 2003) and natural language process-
ing (Mason and Charniak, 2012) has shown that a
large portion of the keyword probability mass can
often be accounted for by a very small number of
words, allowing systems to game better-looking re-
sults by simply guessing the frequency distribution
of the text vocabulary. We find this to be espe-
cially true in the domain-specific case, where com-
mon terms (eg shoe, sole, heel, upper) are used in
almost every caption, and in some captions account
for most words used (such as the second example
in Table 1). While domain-frequent words are also
needed for generating new captions, we don?t want
them to account for all of the words our system gen-
erates. Of course, a human evaluation would be
another possible way of addressing this issue, but
it would be difficult and expensive to find enough
people who have sufficient knowledge of womens?
clothing and would be able to accurately say whether
the generated words are appropriate or not (words
such as hobo, PU, stacked, upper, and vamp). Also,
although the gold image captions are noisy, the num-
6
sole upper detail heel print fun fab-
ric patent uppers soles high shoe
rounded leather rubber lining elastic
animal toe feet
style upper heel leather strap sandal
lining toe dress satin shoe comfort
ankle sole adjustable outsole plat-
form stiletto rhinestone sandals
bag, leather, zip, pocket, hardware,
features, shoulder, flap, main, cell,
perfect, length, drop, zipper, clo-
sure, bold, phone, evening, holds,
hobo
This high heel platform shoe has a
patent leather upper with an orna-
menting bow at the toe, a leather lin-
ing, a rounded toe, and a rubber bot-
tom.Available Colors: Black Patent,
Cheetah Print PU.
Create a timeless look with these
Andie dress sandals from Col-
oriffics. Dyeable white satin matte
satin or metallic satin upper in a
two-piece dress-sandal style with
an open round toe crossing pleated
vamp straps with a dazzling rhine-
stone clasp and a wraparound heel
strap with an adjustable buckle clo-
sure.
Treesje Dakota Shoulder Bag Black
Shine - Designer Handbags
Table 3: Example results for unseen images. Both the top words generated by our model and the original held-out
captions for the images are shown. (Note: In the third example, ?hobo? is actually the term that is used to describe
that shape of handbag.)
ber of test documents is very large so we can find
significance on precision and recall using bootstrap
resampling.
We also ran the baseline system and our system
on the handbags category of the dataset. We did not
modify the system in any way when using the bags
dataset, just gave it different file for input.
7 Results and Discussion
The results of our evaluations are in Table 2. As
we expected, the corpus frequency baseline does
very well. It is comparable to MixLDA for 10
and 15 words, and significantly better than MixLDA
for over 20 words. However, the Polylingual topic
model using only SIFT features and text is much bet-
ter than both. The trained MixLDA model has topics
with both image and text features, so when estimat-
ing topics given only an image, it estimates that it
was generated by topics that have a high proportion
of image features. Though it also estimates some
topics that have a mix of visual and text features,
being able to generate good text descriptions from
those topics, the topics that have less text features
will be mainly determined by the smoothing param-
eter ? the uniform distribution, worse than guessing
the corpus distribution.
Out of the single attribute models, all except three
of the single feaure models were significantly higher
than corpus frequency on both precision and recall
at 10, 15, and 20 words. The exceptions are the two
color features and grayscale texture. For grayscale
texture, we had expected it would correlate well with
the material of the shoe; but either the low resolu-
tion of the images makes it difficult to distinguish
materials by their texture, or materials don?t corre-
late with the ?less visual? features as much as we
expected. Interestingly, since the material of an item
tends to correlate strongly with other attributes such
as shape and color, so our model still generates cor-
rect descriptive words for material in many cases.
While neither color nor texture were useful fea-
7
10 words 15 words 20 words
P R F1 P R F1 P R F1
Corpus frequency 17.58 13.19 11.76 13.19 12.70 12.94 11.76 15.10 13.22
Combined Model 24.41 15.67 19.09 21.01 20.22 20.61 18.76 24.09 21.10
Table 4: Results of evaluation in the handbags cateogory (top 10-20 words).
tures on their own, RGB Texture did very well as a
single attribute, and was within signficance of both
the combined color and combined texture models.
This may be related to the fact that RGB Textons
have a larger number of visual terms than those other
features, 100 for each of the three color channels.
Unlike material, we observed that the color of an
object is often not mentioned in the human-written
caption (as seen in the examples in Table 1), or sev-
eral colors are described in the caption where only
one is seen in the image (seen in some of the exam-
ples in Table 3). We also observed that our system
generates very few color words.
The gradient and shape-based features have the
best single-attribute performance by far. Both SIFT
and HOG capture shape at local points, but while
SIFT features are invariant to differences in position
or scale, HOG features are more sensitive to the way
the item is oriented in the image. Although the cur-
vature features TriHOG and TriHog-Polar are nearly
as good as HOG on their own, combining the three
HOG features does not significantly improve perfor-
mance of the model over HOG alone.
Not all of the single-attribute models performed
as well as others, but there was no case where re-
moving one of the features improved the perfor-
mance of the combined model. The fewest num-
ber of image attributes that our model could use
and still get within significance to the full combined
model is three ? SIFT, RGB Texture, and HOG.
However, we found that each image attribute does
slightly improve, the model, even if not by a signifi-
cant amount.
Our results on the handbags category of the
dataset are shown in Table 4. Although our scores
are not as high as they were in the shoes category,
the scores of the corpus frequency baseline are not
as high either, and our model does about as well over
the baseline in each category. But is worth reiterat-
ing that we were able to run our system on both the
bags and shoes shopping categories with absolutely
no modifications or tuning of parameters.
8 Conclusion and Future Work
In conclusion, we have shown that the polylingual
topic model works well for modeling topically com-
parable images and related text, and obtain competi-
tive results for the image annotation task. Our model
is trained on noisy image captions from the web,
rather than hand-labeled data.
For future work, we would like to further adapt
the polylingual topic model for multi-modal data by
allowing some topics to be generated only by one
modality or the other. We are also interested in char-
acterizing the image annotations in order to generate
a single most likely annotation for different types of
features such as texture or color. Finally, we are in-
terested in extending this model to use with other do-
mains of data. For natural images, we could use im-
age segmentation algorithms to separate the object
of interest from the background of the image, or we
could use scene classifcation to cluster the training
images by their background scene and train seprate
models for each.
References
K. Barnard, P. Duygulu, D. Forsyth, N. De Freitas, D.M.
Blei, and M.I. Jordan. 2003. Matching words and
pictures. The Journal of Machine Learning Research,
3:1107?1135.
Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.
2010. Automatic attribute discovery and character-
ization from noisy web data. In Proceedings of
the 11th European conference on Computer vision:
Part I, ECCV?10, pages 663?676, Berlin, Heidelberg.
Springer-Verlag.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
N. Dalal and B. Triggs. 2005. Histograms of oriented
gradients for human detection. In Computer Vision
8
and Pattern Recognition, 2005. CVPR 2005. IEEE
Computer Society Conference on, volume 1, pages 886
?893 vol. 1, june.
P. F. Felzenszwalb, R. B. Girshick, and D. McAllester.
Discriminatively trained deformable part models, re-
lease 4. http://people.cs.uchicago.edu/ pff/latent-
release4/.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? automatic caption gener-
ation for news images. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1239?1249, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Yansong Feng and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In HLT-
NAACL, pages 831?839.
Yangqing Jia, M. Salzmann, and T. Darrell. 2011. Learn-
ing cross-modality similarity for multinomial data. In
Computer Vision (ICCV), 2011 IEEE International
Conference on, pages 2407 ?2414, nov.
T. Leung and J. Malik. 2001. Representing and recog-
nizing the visual appearance of materials using three-
dimensional textons. International Journal of Com-
puter Vision, 43(1):29?44.
C. Liu, L. Sharan, E.H. Adelson, and R. Rosenholtz.
2010. Exploring features in a bayesian framework for
material recognition. In Computer Vision and Pattern
Recognition (CVPR), 2010 IEEE Conference on, pages
239?246. IEEE.
D.G. Lowe. 1999. Object recognition from local scale-
invariant features. In Computer Vision, 1999. The Pro-
ceedings of the Seventh IEEE International Confer-
ence on, volume 2, pages 1150 ?1157 vol.2.
R. Mason and E. Charniak. 2012. Apples to oranges:
Evaluating image annotations from natural language
processing systems. NAACL.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 2 - Volume 2, EMNLP ?09,
pages 880?889, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Florent Monay and Daniel Gatica-Perez. 2003. On im-
age auto-annotation with latent space models. In Pro-
ceedings of the eleventh ACM international conference
on Multimedia, Multimedia ?03, pages 275?278, New
York, NY, USA. ACM.
Henning Mu?ller, Ste?phane Marchand-Maillet, and
Thierry Pun. 2002. The truth about corel - evaluation
in image retrieval. In Proceedings of the International
Conference on Image and Video Retrieval, CIVR ?02,
pages 38?49, London, UK, UK. Springer-Verlag.
V. Ordonez, G. Kulkarni, and T.L. Berg. 2011. Im2text:
Describing images using 1 million captioned pho-
tographs. NIPS.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In In Proceedings of the Fifth Conference
on Applied Natural Language Processing, pages 16?
19.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In In Pro-
ceedings of HLT-NAACL 2003, pages 252?259.
A. Vedaldi and B. Fulkerson. 2008. VLFeat: An open
and portable library of computer vision algorithms.
http://www.vlfeat.org/.
9
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 11?20,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Domain-Specific Image Captioning
Rebecca Mason and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University, Providence, RI 02912
{rebecca,ec}@cs.brown.edu
Abstract
We present a data-driven framework for
image caption generation which incorpo-
rates visual and textual features with vary-
ing degrees of spatial structure. We pro-
pose the task of domain-specific image
captioning, where many relevant visual
details cannot be captured by off-the-shelf
general-domain entity detectors. We ex-
tract previously-written descriptions from
a database and adapt them to new query
images, using a joint visual and textual
bag-of-words model to determine the cor-
rectness of individual words. We imple-
ment our model using a large, unlabeled
dataset of women?s shoes images and nat-
ural language descriptions (Berg et al.,
2010). Using both automatic and human
evaluations, we show that our caption-
ing method effectively deletes inaccurate
words from extracted captions while main-
taining a high level of detail in the gener-
ated output.
1 Introduction
Broadly, the task of image captioning is: given a
query image, generate a natural language descrip-
tion of the image?s visual content. Both the im-
age understanding and language generation com-
ponents of this task are challenging open problems
in their respective fields. A wide variety of ap-
proaches have been proposed in the literature, for
both the specific task of caption generation as well
as related problems in understanding images and
text.
Typically, image understanding systems use su-
pervised algorithms to detect visual entities and
concepts in images. However, these typically re-
quire accurate hand-labeled training data, which
is not available in most specific domains. Ideally,
1. Extract existing human-authored caption according to
similarity of coarse visual features.
Query Image Nearest-Neighbor
Nearest-neighbor caption: This sporty sneaker clog keeps
foot cool and comfortable and fully supported.
2. Estimate correctness of extracted words using domain-
specific joint model of text and visual bag-of-word features.
This sporty sneaker clog keeps foot cool and comfortable and
fully supported.
3. Compress extracted caption to adapt its content while
maintaining grammatical correctness.
Output: This clog keeps foot comfortable and supported.
a domain-specific image captioning system would
learn in a less supervised fashion, using captioned
images found on the web.
This paper focuses on image caption genera-
tion for a specific domain ? images of women?s
shoes, collected from online shopping websites.
Our framework has three main components. We
extract an existing description from a database
of human-captions, by projecting query images
into a multi-dimensional space where structurally
similar images are near each other. We also
train a joint topic model to discover the latent
topics which generate both captions and images.
We combine these two approaches using sentence
compression to delete modifying details in the ex-
tracted caption which are not relevant to the query
image.
Our captioning framework is inspired by sev-
eral recent approaches at the intersection of Nat-
ural Language Processing and Computer Vision.
Previous work such as Farhadi et al. (2010) and
Ordonez et al. (2011) explore extractive methods
for image captioning, but these rely on general-
domain visual detection systems, and only gener-
11
ate extractive captions. Other models learn corre-
spondences between domain-specific images and
natural language captions (Berg et al., 2010; Feng
and Lapata, 2010b) but cannot generate descrip-
tions for new images without the use of auxil-
iary text. Kuznetsova et al. (2013) propose a
sentence compression model for editing image
captions, but their compression objective is not
conditioned on a query image, and their system
also requires general-domain visual detections.
This paper proposes an image captioning frame-
work which extends these ideas and culminates in
the first domain-specific image caption generation
system.
More broadly, our goal for image caption gener-
ation is to work toward less supervised captioning
methods which could be used to generate detailed
and accurate descriptions for a variety of long-tail
domains of captioned image data, such as in nature
and medicine.
2 Related Work
Our framework for domain-specific image cap-
tioning consists of three main components: ex-
tractive caption generation, image understanding
through topic modeling, and sentence compres-
sion.
1
These methods have previously been ap-
plied individually to related tasks such as gen-
eral domain image captioning and annotation. We
briefly describe some of the related work:
2.1 Extractive Caption Generation
In previous work on image caption extraction, cap-
tions are generated by retrieving human-authored
descriptions from visually similar images. Farhadi
et al. (2010) and Ordonez et al. (2011) retrieve
whole captions to apply to a query image, while
Kuznetsova et al. (2012) generate captions using
text retrieved from multiple sources. The descrip-
tions are related to visual concepts in the query
image, but these models use visual similarity to
approximate textual relevance; they do not model
image and textual features jointly.
2.2 Image Understanding
Recent improvements in state-of-the-art visual ob-
ject class detections (Felzenszwalb et al., 2010)
1
A research proposal for this framework and other image
captioning ideas was previously presented at NAACL Stu-
dent Research Workshop in 2013 (Mason, 2013). This paper
presents a completed project including implementation de-
tails and experimental results.
have enabled much recent work in image caption
generation (Farhadi et al., 2010; Ordonez et al.,
2011; Kulkarni et al., 2011; Yang et al., 2011;
Mitchell et al., 2012; Yu and Siskind, 2013). How-
ever, these systems typically rely on a small num-
ber of detection types, e.g. the twenty object cate-
gories from the PASCAL VOC challenge.
2
These
object categories include entities which are com-
monly described in general domain images (peo-
ple, cars, cats, etc) but these require labeled train-
ing data which is not typically available for the vi-
sually relevant entities in specific domains.
Our caption generation system employs a multi-
modal topic model from our previous work (Ma-
son and Charniak, 2013) which generates descrip-
tive words, but lacks the spatial structure needed
to generate a full sentence caption. Other previ-
ous work uses topic models to learn the semantic
correspondence between images and labels (e.g.
Blei and Jordan (2003)), but learning from natural
language descriptions is considerably more diffi-
cult because of polysemy, hypernymy, and mis-
alginment between the visual content of an im-
age and the content humans choose to describe.
The MixLDA model (Feng and Lapata, 2010b;
Feng and Lapata, 2010a) learns from news images
and natural language descriptions, but to generate
words for a new image it requires both a query
image and query text in the form of a news arti-
cle. Berg et al. (2010) use discriminative models
to discover visual attributes from online shopping
images and captions, but their models do not gen-
erate descriptive words for unseen images.
2.3 Sentence Compression
Typical models for sentence compression (Knight
and Marcu, 2002; Furui et al., 2004; Turner and
Charniak, 2005; Clarke and Lapata, 2008) have a
summarization objective: reduce the length of a
source sentence without changing its meaning. In
contrast, our objective is to change the meaning of
the source sentence, letting its overall correctness
relative to the query image determine the length
of the output. Our objective differs from that of
Kuznetsova et al. (2013), who compress image
caption sentences with the objective of creating a
corpus of generally transferrable image captions.
Their compression objective is to maximize the
probability of a caption conditioned on the source
2
http://pascallin.ecs.soton.ac.uk/
challenges/VOC/
12
Two adjustable buckle
straps top a classic rubber
rain boot grounded by a
thick lug sole for excellent
wet-weather traction.
Available in Plus Size. Faux
snake skin flats with a large
crossover buckle at the toe.
Padded insole for a comfort-
able all day fit.
Glitter-covered elastic up-
per in a two-piece dress san-
dal style with round open
toe. Single vamp strap with
contrasting trim matching
elasticized heel strap criss-
crosses at instep.
Explosive! These white
leather joggers are sure to
make a big impression. De-
tails count, including a toe
overlay, millennium trim
and lightweight raised sole.
Table 1: Example data from the Attribute Discovery Dataset (Berg et al., 2010). See Section 3.
image, while our objective is conditioned on the
query image that we are generating a caption for.
Additionally, their model also relies on general-
domain trained visual detections.
3 Dataset and Preprocessing
The dataset we use is the women?s shoes sec-
tion of the publicly available Attribute Discov-
ery Dataset
3
from Berg et al. (2010), which con-
sists of product images and captions scraped from
the shopping website Like.com. We use the
women?s shoes section of the dataset which has
14764 captioned images. Product descriptions de-
scribe many different attributes such as styles, col-
ors, fabrics, patterns, decorations, and affordances
(activities that can be performed while wearing the
shoe). Some examples are shown in Table 1.
For preprocessing in our framework, we first de-
termine an 80/20% train test split. We define a tex-
tual vocabulary of ?descriptive words?, which are
non-function words ? adjectives, adverbs, nouns
(except proper nouns), and verbs. This gives us
a total of 9578 descriptive words in the training
set, with an average of 16.33 descriptive words per
caption.
4 Image Captioning Framework
4.1 Extraction
To repeat, our overall process is to first find a cap-
tion sentence from our database to use as a tem-
plate, and then correct the template sentences us-
ing sentence compresion. We compress by remov-
3
http://tamaraberg.com/
attributesDataset/index.html
ing details that are probably not correct for the test
image. For example, if the sentence describes ?a
red slipper? but the shoe in the query image is yel-
low, we want to remove ?red? and keep the rest.
As in this simple example, the basic paradigm
for compression is to keep the head words of
phrases (?slipper?) and remove modifiers. Thus
we want to extraction stage of our scheme to be
more likely to find a candidate sentence with cor-
rect head words, figuring that the compression
stage can edit the mistakes. Our hypothesis is that
headwords tend to describe more spatially struc-
tured visual concepts, while modifier words de-
scribe those that are more easily represented using
local or unstructured features.
4
Table 2 contains
additional example captions with parses.
GIST (Oliva and Torralba, 2001) is a com-
monly used feature in Computer Vision which
coarsely localizes perceptual attributes (e.g. rough
vs smooth, natural vs manmade). By computing
the GIST of the images, we project them into a
multi-dimensional Euclidean space where images
with semantically similar structures are located
near each other. Thus the extraction stage of our
caption generation process selects a sentence from
the GIST nearest-neighbor to the query image.
5
4.2 Joint Topic Model
The second component of our framework incorpo-
rates visual and textual features using a less struc-
tured model. We use a multi-modal topic model
4
For example, the color ?red? can be described using a
bag of random pixels, while a ?slipper? is a spatial configura-
tion of parts in relationship to each other.
5
See Section 5.1 for additional implementation details.
13
Table 2: Example parses of women?s shoes descriptions. Our hypothesis is that the headwords in phrases
are more likely to describe visual concepts which rely on spatial locations or relationships, while modi-
fiers words can be represented using less-structured visual bag-of-words features.
to learn the latent topics which generate bag-of-
words features for an image and its caption.
The bag-of-words model for Computer Vision
represents images as a mixture of topics. Mea-
sures of shape, color, texture, and intensity are
computed at various points on the image and clus-
tered into discrete ?codewords? using the k-means
algorithm.
6
Unlike text words, an individual code-
word has little meaning on its own, but distri-
butions of codewords can provide a meaningful,
though unstructured, representation of an image.
An image and its caption do not express exactly
the same information, but they are topically re-
lated. We employ the Polylingual Topic Model
(Mimno et al., 2009), which is originally used to
model corresponding documents in different lan-
guages that are topically comparable, but not par-
allel translations. In particular, we employ our
previous work (Mason and Charniak, 2013) which
extends this model to topically similar images and
natural language captions. The generative process
for a captioned image starts with a single topic
distribution drawn from concentration parameter
? and base measure m:
? ? Dir(?, ?m) (1)
Modality-specific latent topic assignments z
img
and z
txt
are drawn for each of the text words and
codewords:
z
img
? P (z
img
|?) =
?
n
?
z
img
n
(2)
6
While space limits a more detailed explanation of visual
bag-of-word features, Section 5.2 provides a brief overview
of the specific visual attributes used in this model.
z
txt
? P (z
txt
|?) =
?
n
?
z
txt
n
(3)
Observed words are generated according to their
probabilities in the modality-specific topics:
w
img
? P (w
img
|z
img
,?
img
) = ?
img
w
img
n
|z
img
n
(4)
w
txt
? P (w
txt
|z
txt
,?
txt
) = ?
txt
w
txt
n
|z
txt
n
(5)
Given the uncaptioned query image q
img
and
the trained multi-modal topic model, it is now pos-
sible to infer the shared topic proportion for q
img
using Gibbs sampling:
P (z
n
= t|q
img
, z
\n
,?
img
, ?m)
? ?
img
q
img
n
|t
(N
t
)
\n
+ ?m
t
?
t
N
t
? 1 + ?
(6)
4.3 Sentence Compression
Let w = w
1
, w
2
, ..., w
n
be the words in the ex-
tracted caption for q
img
. For each word, we de-
fine a binary decision variable ?, such that ?
i
= 1
if w
i
is included in the output compression, and
?
i
= 0 otherwise. Our objective is to find values
of ? which generate a caption for q
img
which is
both semantically and grammatically correct.
We cast this problem as an Integer Linear Pro-
gram (ILP), which has previously been used for
the standard sentence compression task (Clarke
and Lapata, 2008; Martins and Smith, 2009). ILP
is a mathematical optimization method for deter-
mining the optimal values of integer variables in
order to maximize an objective given a set of con-
straints.
14
4.3.1 Objective
The ILP objective is a weighted linear combina-
tion of two measures which represent the correct-
ness and fluency of the output compression:
Correctness: Recall in Section 3 we defined
words as either descriptive words or function
words. For each descriptive word, we estimate
P (w
i
|q
img
), using topic proportions estimated us-
ing Equation 6:
P (w
i
|q
img
) =
?
t
P (w
i
|z
txt
t
)P (z
t
|q
img
) (7)
This is used to find I(w
i
), a function of the likeli-
hood of each word in the extracted caption:
I(w
i
) =
{
P (w
i
|q
img
)? P (w
i
), if descriptive
0, function word
(8)
This function considers the prior probability of w
i
because frequent words often have a high posterior
probability even when they are inaccurate. Thus
the sum
?
n
i=1
?
i
? I(w
i
) is the overall measure of
the correctness of a proposed caption conditioned
on q
img
.
Fluency: We formulate a trigram language
model as an ILP, which requires additional binary
decision variables: ?
i
= 1 if w
i
begins the out-
put compression, ?
ij
= 1 if the bigram sequence
w
i
, w
j
ends the compression, ?
ijk
= 1 if the tri-
gram sequence w
i
, w
j
, w
k
is in the compression,
and a special ?start token? ?
0
= 1. This language
model favors shorter sentences, which is not nec-
essarily the objective for image captioning, so we
introduce a weighting factor, ?, to lessen the ef-
fect.
Here is the combined objective, using P to rep-
resent logP :
max z =
(
n
?
i=1
?
i
? P (w
i
|start)
+
n?2
?
i=1
n?1
?
j=i+1
n
?
k=j+1
?
ijk
? P (w
k
|w
i
, w
j
)
+
n?1
?
i=0
n
?
j=i+1
?
ij
? P (end|w
i
, w
j
)
)
? ?
+
n
?
i=1
?
i
? I(w
i
) (9)
Sequential
1.)
?
i
?
i
= 1
2.) ?
k
? ?
k
?
?
k?2
i=0
?
k?1
j=1
?
ijk
= 0
?k : k ? 1...n
3.) ?
j
?
?
j?1
i=0
?
n
k=j+1
?
ijk
?
?
j?1
i=0
?
ij
= 0
?j : j ? 1...n
4.)
?
n?1
j=i+1
?
n
k=j+1
?
ijk
?
?
n
j=i+1
?
ij
?
?
i?1
h=0
?
hi
? ?
i
= 0
?i : i ? 1...n
5.)
?
n?1
i=0
?
n
j=i+1
?
ij
= 1
Modifier
1. If head of the extracted sentence= w
i
, then
?
i
= 1
2. If w
i
is head of a noun phrase, then ?
i
= 1
3. Punctuation and coordinating conjunctions
follow special rules (below). Otherwise, if
headof(w
i
) = w
j
, then ?
i
? ?
j
Other
1.
?
i
?
i
? 3
2. Define valid use of puncutation and coordi-
nating conjunctions.
Table 3: Summary of ILP constraints.
4.3.2 ILP Constraints
The ILP constraints ensure both the mathematical
validity of the model, and the grammatical correct-
ness of its output. Table 3 summarizes the list of
constraints. Sequential constraints are defined as
in Clarke (2008) ensure that the ordering of the tri-
grams is valid, and that the mathematical validity
of the model holds.
5 Implementation Details
5.1 Extraction
GIST features are computed using code by Oliva
and Torralba (2001)
7
. GIST is computed with im-
ages converted to grayscale; since color features
tend to act as modifiers in this domain. Nearest-
neighbors are selected according to minimum dis-
tance from q
img
to both a regularly-oriented and a
horizontally-flipped training image.
Only one sentence from the first nearest-
neighbor caption is extracted. In the case of multi-
sentence captions, we select the first suitable sen-
tence according to the following criteria 1.) has
at least five tokens, 2.) does not contain NNP or
NNPS (brand names), 3.) does not fail to parse
using Stanford Parser (Klein and Manning, 2003).
If the nearest-neighbor caption does not have any
sentences meeting these criteria, caption sentences
from the next nearest-neighbor(s) are considered.
7
http://people.csail.mit.edu/torralba/
code/spatialenvelope/
15
5.2 Joint Topic Model
We use the Joint Topic Model that we imple-
mented in our previous work; please see Mason
and Charniak (2013) for the full model and imple-
mentation details. The topic model is trained with
200 topics using the polylingual topic model im-
plementation from MALLET
8
. Briefly, the code-
words represent the following attributes:
SHAPE: SIFT (Lowe, 1999) describes the
shapes of detected edges in the image, using de-
scriptors which are invariant to changes in rotation
and scale.
COLOR: RGB (red, green, blue) and HSV (hue,
saturation, value) pixel values are sampled from a
central area of the image to represent colors.
TEXTURE: Textons (Leung and Malik, 2001)
are computed by convolving images with Gabor
filters at multiple orientations and scales, then
sampling the outputs at random locations.
INTENSITY: HOG (histogram of gradients)
(Dalal and Triggs, 2005) describes the direction
and intensity of changes in light. These features
are computed on the image over a densely sam-
pled grid.
5.3 Compression
The sentence compression ILP is implemented us-
ing the CPLEX optimization toolkit
9
. The lan-
guage model weighting factor in the objective is
? = 10
?3
, which was hand-tuned according to
observed output. The trigram language model
is trained on training set captions using Berke-
leyLM (Pauls and Klein, 2011) with Kneser-Ney
smoothing. For the constraints, we use parses
from Stanford Parser (Klein and Manning, 2003)
and the ?semantic head? variation of the Collins
headfinder Collins (1999).
6 Evaluation
6.1 Setup
We compare the following systems and baselines:
KL (EXTRACTION): The top performing ex-
tractive model from Feng and Lapata (2010a), and
the second-best captioning model overall. Using
estimated topic distributions from our joint model,
we extract the source with minimum KL Diver-
gence from q
img
.
8
http://mallet.cs.umass.edu/
9
http://www-01.ibm.com/
software/integration/optimization/
cplex-optimization-studio/
ROUGE-2 Average 95% Confidence int.
KL (EXTRACTION)
P .06114 ( .05690 - .06554 )
R .02499 ( .02325 - .02686)
F .03360 ( .03133 - .03600 )
GIST (EXTRACTION)
P .10894 ( .09934 - .11921 )
R .05474 ( .04926 - .06045)
F .06863 ( .06207 - .07534)
LM-ONLY (COMPRESSION)
P .13782 ( .12602 - .14864 )
R .02437 ( .02193 - .02700 )
F .03864 ( .03512 - .04229)
SYSTEM (COMPRESSION)
P .16752 (.15679 -.17882 )
R .05060 ( .04675 - .05524 )
F .07204 ( .06685 - .07802 )
Table 4: ROUGE-2 (bigram) scores. The pre-
cision of our system compression (bolded) sig-
nificantly improves over the caption that it com-
presses (GIST), without a significant decrease in
recall.
GIST (EXTRACTION): The sentence extracted
using GIST nearest-neighbors, and the uncom-
pressed source for the compression systems.
LM-ONLY (COMPRESSION): We include this
baseline to demonstrate that our model is effec-
tively conditioning output compressions on q
img
,
as opposed to simply generalizing captions as in
Kuznetsova et al. (2013)
10
. We modify the com-
pression ILP to ignore the content objective and
only maximize the trigram language model (still
subject to the constraints).
SYSTEM (COMPRESSION): Our full system.
Unfortunately, we cannot compare our system
against prior work in general-domain image cap-
tioning, because those models use visual detec-
tion systems which train on labeled data that is not
available in our domain.
6.2 Automatic Evaluation
We perform automatic evaluation using similar-
ity measures between automatically generated and
human-authored captions. Note that currently
our system and baselines only generate single-
sentence captions, but we compare against entire
10
Technically their model is conditioned on the source im-
age, in order to address alignment issues which are not appli-
cable in our setup.
16
BLEU@1
KL (EXTRACTION) .2098
GIST (EXTRACTION) .4259
LM-ONLY (COMPRESSION) .4780
SYSTEM (COMPRESSION) .4841
Table 5: BLEU@1 scores of generated captions
against human authored captions. Our model
(bolded) has the highest BLEU@1 score with sig-
nificance.
held-out captions in order to increase the amount
of text we have to compare against.
ROUGE (Lin, 2004) is a summarization eval-
uation metric which has also been used to eval-
uate image captions (Yang et al., 2011). It is
usually a recall-oriented measure, but we also re-
port precision and f-measure because our sen-
tence compressions do not improve recall. Table 4
shows ROUGE-2 (bigram) scores computed with-
out stopwords.
We observe that our system very significantly
improves ROUGE-2 precision of the GIST ex-
tracted caption, without significantly reducing re-
call. While LM-Only also improves precision
against GIST extraction, it indiscriminately re-
moves some words which are relevant to the
query image. We also observe that GIST extrac-
tion strongly outperforms the KL model, which
demonstrates the importance of visual structure.
We also report BLEU (Papineni et al., 2002)
scores, which are the most popularly accepted au-
tomatic metric for captioning evaluation (Farhadi
et al., 2010; Kulkarni et al., 2011; Ordonez et
al., 2011; Kuznetsova et al., 2012; Kuznetsova
et al., 2013). Results are very similar to the
ROUGE-2 precision scores, except the difference
between our system and LM-Only is less pro-
nounced because BLEU counts function words,
while ROUGE does not.
6.3 Human Evaluation
We perform human evaluation of compressions
generated by our system and LM-Only. Users are
shown the query image, the original uncompressed
caption, and a compressed caption, and are asked
two questions: does the compression improve the
accuracy of the caption, and is the compression
grammatical.
We collect 553 judgments from six women who
are native English-speakers and knowledgeable
Query Image GIST Nearest-Neighbor
Extraction: Shimmering snake-embossed leather upper in a
slingback evening dress sandal style with a round open toe.
Compression: Shimmering upper in a slingback evening
dress sandal style with a round open toe.
Query Image GIST Nearest-Neighbor
Extraction: This sporty sneaker clog keeps foot cool and
comfortable and fully supported.
Compression: This clog keeps foot comfortable and sup-
ported.
Query Image GIST Nearest-Neighbor
Extraction: Italian patent leather peep-toe ballet flat with a
signature tailored grosgrain bow.
Compression: leather ballet flat with a signature tailored
grosgrain bow.
Query Image GIST Nearest-Neighbor
Extraction: Platform high heel open toe pump with horsebit
available in silver guccissima leather with nickel hardware
with leather sole.
Compression: Platform high heel open toe pump with
horsebit available in leather with nickel hardware with
leather sole.
Table 6: Example output from our full system.
Red underlined words indicate the words which
are deleted by our compression model.
17
SYSTEM LM-ONLY
Yes No Yes No
Compression
improves
accuracy
63.2% 36.8% 42.6% 57.4%
Compression is
grammatical
73.1% 26.9% 82.2% 17.8%
Table 7: Human evaluation results.
about fashion.
11
Users were recruited via email
and did the study over the internet.
Table 7 reports the results of the human evalu-
ation. Users report 63.2% of SYSTEM compres-
sions improve accuracy over the original, while
the other 36.8% did not improve accuracy. (Keep
in mind that a bad compression does not make the
caption less accurate, just less descriptive.) LM-
ONLY improves accuracy for less than half of the
captions, which is significantly worse than SYS-
TEM captions (Fisher exact test, two-tailed p less
than 0.01).
Users find LM-Only compressions to be slightly
more grammatical than System compressions, but
the difference is not significant. (p > 0.05)
7 Conclusion
We introduce the task of domain-specific image
captioning and propose a captioning system which
is trained on online shopping images and natu-
ral language descriptions. We learn a joint topic
model of vision and text to estimate the correct-
ness of extracted captions, and use a sentence
compression model to propose a more accurate
output caption. Our model exploits the connection
between image and sentence structure, and can be
used to improve the accuracy of extracted image
captions.
The task of domain-specific image caption
generation has been overlooked in favor of the
general-domain case, but we believe the domain-
specific case deserves more attention. While
image captioning can be viewed as a complex
grounding problem, a good image caption should
do more than label the objects in the image. When
an expert looks at images in a specific domain, he
or she makes inferences that would not be made by
a non-expert. Providing this information to non-
11
About 15% of output compressions are the same for both
systems, and about 10% have no deleted words in the output
compression. We include the former in the human evaluation,
but not the latter.
Query Image GIST Nearest-Neighbor
Extraction: Classic ballet flats with decorative canvas
strap and patent leather covered buckle.
Compression: Classic ballet flats covered.
Query Image GIST Nearest-Neighbor
Extraction: This shoe is the perfect shoe for you , fea-
turing an open toe and a lace up upper with a high heel
, and a two tone color .
Compression: This shoe is the shoe , featuring an open toe
and upper with a high heel .
Table 8: Examples of bad performance. The top
example is a parse error, while the bottom exam-
ple deletes modifiers that are not part of the image
description.
expert users in the form of an image caption will
greatly expand the utility for automatic image cap-
tioning.
References
Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.
2010. Automatic attribute discovery and charac-
terization from noisy web data. In Proceedings of
the 11th European conference on Computer vision:
Part I, ECCV?10, pages 663?676, Berlin, Heidel-
berg. Springer-Verlag.
David M. Blei and Michael I. Jordan. 2003. Modeling
annotated data. In Proceedings of the 26th annual
international ACM SIGIR conference on Research
and development in informaion retrieval, SIGIR ?03,
pages 127?134, New York, NY, USA. ACM.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression an integer linear pro-
gramming approach. J. Artif. Int. Res., 31(1):399?
429, March.
James Clarke. 2008. Global Inference for Sentence
Compression: An Integer Linear Programming Ap-
proach. Dissertation, University of Edinburgh.
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
Philadelphia, PA, USA. AAI9926110.
18
N. Dalal and B. Triggs. 2005. Histograms of oriented
gradients for human detection. In Computer Vision
and Pattern Recognition, 2005. CVPR 2005. IEEE
Computer Society Conference on, volume 1, pages
886 ?893 vol. 1, june.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences from images.
In Proceedings of the 11th European conference on
Computer vision: Part IV, ECCV?10, pages 15?29,
Berlin, Heidelberg. Springer-Verlag.
P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and
D. Ramanan. 2010. Object detection with discrim-
inatively trained part based models. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
32(9):1627?1645.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? automatic caption genera-
tion for news images. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1239?1249, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Yansong Feng and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In
HLT-NAACL, pages 831?839.
Sadaoki Furui, Tomonori Kikuchi, Yousuke Shinnaka,
and Chiori Hori. 2004. Speech-to-text and speech-
to-speech summarization of spontaneous speech.
IEEE TRANS. ON SPEECH AND AUDIO PRO-
CESSING, 12(4):401?408.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kevin Knight and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artif. Intell.,
139(1):91?107, July.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and gener-
ating simple image descriptions. In CVPR, pages
1601?1608.
Polina Kuznetsova, Vicente Ordonez, Alexander C.
Berg, Tamara L. Berg, and Yejin Choi. 2012. Col-
lective generation of natural image descriptions. In
ACL.
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg, and Yejin Choi. 2013. Generalizing
image captions for image-text parallel corpus. In
ACL.
T. Leung and J. Malik. 2001. Representing and rec-
ognizing the visual appearance of materials using
three-dimensional textons. International Journal of
Computer Vision, 43(1):29?44.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
D.G. Lowe. 1999. Object recognition from local scale-
invariant features. In Computer Vision, 1999. The
Proceedings of the Seventh IEEE International Con-
ference on, volume 2, pages 1150 ?1157 vol.2.
Andr?e F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
gauge Processing, ILP ?09, pages 1?9, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
R. Mason and E. Charniak. 2013. Annotation of online
shopping images without labeled training examples.
Workshop on Vision and Language (WVL).
Rebecca Mason. 2013. Domain-independent caption-
ing of domain-specific images. NAACL Student Re-
search Workshop.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the
2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 2 - Volume
2, EMNLP ?09, pages 880?889, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Men-
sch, Alexander C. Berg, Tamara L. Berg, and Hal
Daum?e III. 2012. Midge: Generating image de-
scriptions from computer vision detections. In Euro-
pean Chapter of the Association for Computational
Linguistics (EACL).
Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: A holistic representation of the
spatial envelope. International Journal of Computer
Vision, 42:145?175.
V. Ordonez, G. Kulkarni, and T.L. Berg. 2011.
Im2text: Describing images using 1 million cap-
tioned photographs. In NIPS.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
19
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of ACL,
Portland, Oregon, June. Association for Computa-
tional Linguistics.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 290?297, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Yezhou Yang, Ching Lik Teo, Hal Daum?e III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods
in Natural Language Processing (EMNLP), Edin-
burgh, Scotland.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from video described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 53?63,
Sofia, Bulgaria. Association for Computational Lin-
guistics.
20
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 124?133,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Natural Language Generation with Vocabulary Constraints
Ben Swanson
Brown University
Providence, RI
chonger@cs.brown.edu
Elif Yamangil
Google Inc.
Mountain View, CA
leafer@google.com
Eugene Charniak
Brown University
Providence, RI
ec@cs.brown.edu
Abstract
We investigate data driven natural lan-
guage generation under the constraints
that all words must come from a fixed vo-
cabulary and a specified word must ap-
pear in the generated sentence, motivated
by the possibility for automatic genera-
tion of language education exercises. We
present fast and accurate approximations
to the ideal rejection samplers for these
constraints and compare various sentence
level generative language models. Our
best systems produce output that is with
high frequency both novel and error free,
which we validate with human and auto-
matic evaluations.
1 Introduction
Freeform data driven Natural Language Genera-
tion (NLG) is a topic explored by academics and
artists alike, but motivating its empirical study is a
difficult task. While many language models used
in statistical NLP are generative and can easily
produce sample sentences by running their ?gen-
erative mode?, if all that is required is a plausible
sentence one might as well pick a sentence at ran-
dom from any existing corpus.
NLG becomes useful when constraints exist
such that only certain sentences are valid. The
majority of NLG applies a semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their specific
meaning.
We study two constraints concerning the words
that are allowed in a sentence. The first sets a
fixed vocabulary such that only sentences where
all words are in-vocab are allowed. The second
demands not only that all words are in-vocab,
but also requires the inclusion of a specific word
somewhere in the sentence.
These constraints are natural in the construction
of language education exercises, where students
have small known vocabularies and exercises that
reinforce the knowledge of arbitrary words are re-
quired. To provide an example, consider a Chi-
nese teacher composing a quiz that asks students
to translate sentences from English to Chinese.
The teacher cannot ask students to translate words
that have not been taught in class, and would like
ensure that each vocabulary word from the current
book chapter is included in at least one sentence.
Using a system such as ours, she could easily gen-
erate a number of usable sentences that contain a
given vocab word and select her favorite, repeat-
ing this process for each vocab word until the quiz
is complete.
The construction of such a system presents two
primary technical challenges. First, while highly
parameterized models trained on large corpora are
a good fit for data driven NLG, sparsity is still
an issue when constraints are introduced. Tradi-
tional smoothing techniques used for prediction
based tasks are inappropriate, however, as they lib-
erally assign probability to implausible text. We
investigate smoothing techniques better suited for
NLG that smooth more precisely, sharing proba-
bility only between words that have strong seman-
tic connections.
The second challenge arises from the fact that
both vocabulary and word inclusion constraints
are easily handled with a rejection sampler that re-
peatedly generates sentences until one that obeys
the constraints is produced. Unfortunately, for
124
models with a sufficiently wide range of outputs
the computation wasted by rejection quickly be-
comes prohibitive, especially when the word in-
clusion constraint is applied. We define models
that sample directly from the possible outputs for
each constraint without rejection or backtracking,
and closely approximate the distribution of the
true rejection samplers.
We contrast several generative systems through
both human and automatic evaluation. Our best
system effectively captures the compositional na-
ture of our training data, producing error-free text
with nearly 80 percent accuracy without wasting
computation on backtracking or rejection. When
the word inclusion constraint is introduced, we
show clear empirical advantages over the simple
solution of searching a large corpus for an appro-
priate sentence.
2 Related Work
The majority of NLG focuses on the satisfaction
of a communicative goal, with examples such as
Belz (2008) which produces weather reports from
structured data or Mitchell et al. (2013) which gen-
erates descriptions of objects from images. Our
work is more similar to NLG work that concen-
trates on structural constraints such as generative
poetry (Greene et al., 2010) (Colton et al., 2012)
(Jiang and Zhou, 2008) or song lyrics (Wu et al.,
2013) (Ramakrishnan A et al., 2009), where spec-
ified meter or rhyme schemes are enforced. In
these papers soft semantic goals are sometimes
also introduced that seek responses to previous
lines of poetry or lyric.
Computational creativity is another subfield of
NLG that often does not fix an a priori meaning in
its output. Examples such as
?
Ozbal et al. (2013)
and Valitutti et al. (2013) use template filling tech-
niques guided by quantified notions of humor or
how catchy a phrase is.
Our motivation for generation of material for
language education exists in work such as Sumita
et al. (2005) and Mostow and Jang (2012), which
deal with automatic generation of classic fill in the
blank questions. Our work is naturally comple-
mentary to these efforts, as their methods require a
corpus of in-vocab text to serve as seed sentences.
3 Freeform Generation
For clarity in our discussion, we phrase the sen-
tence generation process in the following general
terms based around two classes of atomic units :
contexts and outcomes. In order to specify a gen-
eration system, we must define
1. the set C of contexts c
2. the set O of outcomes o
3. the ?Imply? function I(c, o)? List[c ? C]
4. M : derivation tree sentence
where I(c, o) defines the further contexts implied
by the choice of outcome o for the context c. Be-
ginning with a unique root context, a derivation
tree is created by repeatedly choosing an outcome
o for a leaf context c and expanding c to the new
leaf contexts specified by I(c, o). M converts be-
tween derivation tree and sentence text form.
This is simply a convenient rephrasing of the
Context Free Grammar formalism, and as such
the systems we describe all have some equivalent
CFG interpretation. Indeed, to describe a tradi-
tional CFG, let C be the set of symbols, O be the
rules of the CFG, and I(c, o) return a list of the
symbols on the right hand side of the rule o. To de-
fine an n-gram model, a context is a list of words,
an outcome a single word, and I(c, o) can be pro-
cedurally defined to drop the first element of c and
append o.
To perform the sampling required for derivation
tree construction we must define P (o|c). Using
M, we begin by converting a large corpus of sen-
tence segmented text into a training set of deriva-
tion trees. Maximum likelihood estimation of
P (o|c) is then as simple as normalizing the counts
of the observed outcomes for each observed con-
text. However, in order to obtain contexts for
which the conditional independence assumption
of P (o|c) is appropriate, it is necessary to con-
dition on a large amount of information. This
leads to sparse estimates even on large amounts of
training data, a problem that can be addressed by
smoothing. We identify two complementary types
of smoothing, and illustrate them with the follow-
ing sentences.
The furry dog bit me.
The cute cat licked me.
An unsmoothed bigram model trained on this
data can only generate the two sentences verba-
tim. If, however, we know that the tokens ?dog?
and ?cat? are semantically similar, we can smooth
by assuming the words that follow ?cat? are also
likely to follow ?dog?. This is easily handled with
125
traditional smoothing techniques that interpolate
between distributions estimated for both coarse,
P (w|w
?1
=[animal]), and fine, P (w|w
?1
=?dog?),
contexts. We refer to this as context smoothing.
However, we would also like to capture the in-
tuition that words which can be followed by ?dog?
can also be followed by ?cat?, which we will call
outcome smoothing. We extend our terminology
to describe a system that performs both types of
smoothing with the following
? the set
?
C of smooth contexts c?
? the set
?
O of smooth outcomes o?
? a smoothing function S
C
: C ?
?
C
? a smoothing function S
O
: O ?
?
O
dog [animal]
bit
[action]
4
2
1
3
5
7
6
2 Related Work
The application of structural constraints appears
in previous work in the form of generative po-
etry (Greene et al., 2010) or lyrics (Wu et al.,
2013), where specified meter or rhyme schemes
are enforced.
?
Ozbal et al. (2013) produces
freeform text by filling templates with respect to
abstract notions such as humor.
3 Freeform Generation
We first address the problem of freeform data
driven language generation directly. We do not
set a semantic goal but instead ask only that the
output be considered a valid sentence, seeking a
model that captures the variability of language.
For clarity in our discussion, we phrase the
generation process in the following general terms
based around two classes of atomic units : Con-
texts and Outcomes. In order to specify a genera-
tion system, we must define
1. the set C of contexts c
2. the set O of outcomes o
3. the ?Imply? function I(c, o) ? List[c ? C]
where I(c, o) defines the further contexts implied
by the choice of outcome o for the context c. This
model can be made probabilistic by the definition
of P (o|c), where each outcome is sampled inde-
pendently given its context. We also require the
existence of a single unique root context, and refer
to the result of repeated sampling of outcomes for
contexts as a derivation tree. Finally, a mapping
from derivation tree to surface form is required to
produce actual text.
This is simply a convenient rephrasing of the
Context Free Grammar formalism, and as such
the systems we describe all have some equivalent
CFG interpretation. Indeed, to describe a tradi-
tional CFG, let C be the set of nonterminals, O be
the rules of the CFG, and I(c, o) returns a list of
the nonterminals on the right hand side of the rule
o and does not depend on c. P (o|c) would enforce
the choice of rules with appropriate lefthand sides.
The Context-Outcome terms can be more natu-
ral when describing other models where we do not
want to explicitly define the space of nonterminals.
A simple example is an n-gram model, for which
a context is an ordered list of words, an outcome
a single word, and I(c, o) can be procedurally de-
fined to produce a list containg a single context
made by dropping the first word of the previous
context and appending the outcome to the end.
This formulation is well suited to data driven
estimation from a corpus of derivation trees.
While our methods are easily extended to mul-
tiple derivations for each single sentence, in this
work we assume access to a single derivation for
each sentence in our data set. Maximum likeli-
hood estimation of P (o|c) is then as simple as nor-
malizing the counts of the observed outcomes for
each observed context. However, in order to ob-
tain contexts for which the conditional indepen-
dence assumption of P (o|c) is appropriate, it is
necessary to condition on a large amount of in-
formation. This leads to sparse estimates even on
large amounts of training data, a problem that can
be addressed by smoothing.
We identify two complementary types of
smoothing, and illustrate them with the following
sentences.
The furry dog bit me.
The cute cat licked me.
Assuming a simple bigram model where con-
text is the previous word and the outcome a sin-
gle word, an unsmoothed model tr ined on this
data can only generate the two sentences verba-
tim. Imagine we have some way of knowing that
the tokens ?dog? and ?cat? are similar and would
like to leverage this fact . In our bigram model,
this amounts to the claim that the words that follow
?cat? are perhaps also likely to follow ?dog?. This
is easily handled with traditional smoothing tech-
niques, which interpolate between distributions
estimated for both coarse, P (w|w
?1
=[is-animal]),
and fine, P (w|w
?1
=?dog?), contexts. We refer to
this as context smoothing.
However, we would also like to capture the in-
tuition that words which can be followed by ?dog?
can also be followed by ?cat?, which we will call
outcome smoothing. We extend our terminology
to describe a system that performs both types of
smoothing with the following
? the set
?
C of smooth contexts c?
? the set
?
O of smooth outcomes o?
? a smoothing function S
C
: C ?
?
C
? a smoothing function S
O
: O ?
?
O
We describe the generative process with the fol-
lowing flowchart
2 Related Work
The application of structural constraints appears
in previous work in the form of generative po-
etry (Greene et al., 2010) or lyrics (Wu et al.,
2013), wher specified meter or rhyme schemes
are enforce .
?
Ozbal et al. (2013) produces
freeform text by filling templates with respect to
abstract notions such as humor.
3 Freeform Generation
We first address the problem of freeform data
driven language generation directly. We do not
set a semantic goal but in tead ask only that the
output b co sidered a valid sentence, seeking
m del that captures the variability of language.
For clarity in our discussion, we phrase the
generation proce s in the following general ter s
based around two classes of atomic unit : Con-
texts and Outcomes. In order to specify a genera-
tion system, we must define
1. the set C of contexts c
2. the set O of outcomes o
3. the ?Imply? function I(c, o) ? List[c ? C]
wher I(c, o) d fines the further contexts implied
by the choice of outcome o for the context c. This
model can be made probabilistic by the definition
of P (o|c), where each outcome is sampled inde-
pendently given its context. W lso require the
exist nce of a single un que r ot context, and refer
to the result of repeated sampling of outcomes f r
contexts as a derivation tree. Fi a ly, a mapping
f om derivation tree to s rface form is r quired to
produce actual text.
Thi is simply a co veni nt rephrasing of the
Context Free Grammar formalism, and as such
the systems w describe all have some equiv lent
CFG nterpretat on. Indeed, to describe a tradi-
tional CFG let C be the set of nonterminals, O be
the r le of the CFG, and I(c, o) returns a list of
the nonterminals on the right hand side of the rul
o and does ot d pend on c. P (o|c) would enforce
the choice of rules with appropriate lefthand sides.
The C ntext-Outcome term can be re natu-
ral wh n describing other models where we do no
want to expl cit y define the space of onterm nals.
A simple example is an n-gram model, for which
a context is an ordered list of words, an outcome
a single word, and I(c, o) can be procedurally d -
fined to pr duce a list containg a single context
made by dropping the first word of the previous
con ext and appending the outcome to the end.
This formulation is well suited to data driven
estim f om a corpus of derivation trees.
While our methods are easily extended to mul-
tiple derivations for each single sentence, in this
work we assume access to a single derivation for
each sentence in our data set. Maximum likeli-
hood estim tion of P (o|c) is then as simple as nor-
malizing th counts of the observed outcomes for
each bs rved context. However, in order to ob-
tain contexts for which the conditional indepen-
dence assump ion of P (o|c) is appropriate, it is
n cessary t condition on a large amount of in-
for ion. This leads to sparse estimates even on
la ge a ounts of training data, a problem that can
be addressed by smoothing.
We identify two complementary types of
smoothing, and illustrate them with the following
sentences.
The furry dog bit me.
The cute cat licked me.
Assuming a simple bigram model where con-
text is the previous word and the outcome a sin-
gle wor , an unsmooth d model trained n this
data can only generate the two sentences verba-
tim. Imagine we have some way of knowing that
the toke s ?d g? and ?cat? are similar and would
like to leverag this fact . In our bigram model,
this amounts to the claim that the words that follow
?cat? are perhaps also likely t follow ?dog?. This
is easily handled with traditional smoothing tech-
niques, which interpolate between distributions
estimated for both coarse, P (w|w
?1
=[is-animal]),
and fine, P (w|w
?1
=?dog?), contexts. We refer to
this as context smoothing.
However, we would also like to capture the in-
tuition that words which can be followed by ?dog?
can also be followed by ?cat?, which we will call
outcome sm thing. We extend our terminology
to describe a system that performs both types of
smoothing with the following
? the set
?
C of smooth contexts c?
? the set
?
O of smooth outcomes o?
? a smoothing function S
C
: C ?
?
C
? a smoothing function S
O
: O ?
?
O
We describe the generative process with the fol-
lowing flowchart
2 Related Work
The application of structural constraints appears
in previous work in the form of generative po-
etry (Greene et al., 2010) or lyrics (Wu et al.,
2013), where specified meter or rhyme schemes
are enforced.
?
Ozbal et al. (2013) produces
freeform text by filling templates with respect to
abstract notions such as humor.
3 Freeform Generation
We first address the problem of freeform data
driven language generation directly. We do not
set a s mantic goal but instead ask only that the
output be onsidered a valid sentence, seeking a
model that captures the variability of language.
For clarity in our discussion, we phrase the
generation process in the following general terms
based around two classes of atomic units : Con-
texts and Outcomes. In order to specify a genera-
tion system, we must define
1. the set C of cont xts c
2. the set O of outco es o
3. the ?Imply? function I(c, o) ? List[c ? C]
where I(c, o) define the fur r contexts impli d
by the choice of outcome o for the context . This
model can be made probabi istic by the definition
of P (o|c), wher each outcome is sampled inde-
pend ntly give its context. We also r quire the
existe ce of a single unique ro t context, and refer
to esult of r peated samp ing of utcomes for
contexts as a derivati n tree. Finally, a mapping
from derivation tree to sur ac form is required to
produce actual text.
T is is simply a convenient rephrasing of the
Co t xt Free Grammar formalism, and as suc
the systems we describe all hav some equivalent
CFG interpret tion. Indeed, to describe a tradi-
tional CFG, let C be the set of onterminals, O be
the rules of the CFG, and I( , o) returns a list of
the nonterminals on the right hand side of the rule
o and does not depend on c. P (o|c) would enforce
the choice of rules with appropriate lefthand sides.
The Context-Outcome terms can be more natu-
ral when describing other models where we do not
want to explicitly define the space of nonterminals.
A simple example is an n-gram model, for which
a context is an ordered list of words, an outcome
a single word, and I(c, o) can be procedurally de-
fined to produce a list containg a single context
made by dropping the first word of the pr ious
c ntext nd appe ding the outcome to the end.
This f rmul tion is w ll suited to data driven
estimation from a corpus f derivation trees.
While our methods are eas ly extended to mul-
tiple derivations for eac single se tence, in this
w rk we assum cces to a single derivation for
each sentence i our data set. Maximum lik li-
hood s imation of P (o|c) is then as simple as nor-
malizing the cou ts of the observed outcomes for
each observed context. However, in order to ob-
tain contexts for which the conditional indepen-
dence assumption of P (o|c) is appropriate, it is
necessary to condition on a large amount of in-
formation. This leads to sparse estimates even on
large amounts of training data, a problem that can
be addressed by smoothing.
We identify two complementary types of
smoothing, and illustrate them with the following
sentences.
The furry d g bit me.
The cut cat licked me.
Assuming a simple bigram model wher con-
text is the previous word and the outcome a sin-
gle word, an u smoothed model trained on this
at can only generate the two sentences verba-
tim. Imagine we have som way of knowing that
the tokens ?dog? and ?cat? a e sim lar and wou d
like to leverage this fact . In our bigram model,
this amounts to the clai that e words that follow
?cat? e perhap al o likely to ollow ?dog?. Thi
is easily handled wit traditional smo thi tech-
niques, which interpolate between distributions
estimated for both coar , P (w|w
?1
=[is-a imal]),
and fine, P (w|w
?1
=?dog?), contexts. We refer to
this as context smoothing.
However, we would also like to apture the in-
tuition that words which can be followed by ?dog?
can also be f llowed by ?cat?, which we will all
out m smoothing. We extend our terminology
to describe a system that performs both types of
smoothing with the following
? the set
?
C of smooth contexts c?
? the set
?
O of smooth outcomes o?
? a smoothing function S
C
: C ?
?
C
? a smoothing function S
O
: O ?
?
O
We describe the generative process with the fol-
lowing flowchart
2 Related Work
The application of structural constraints appears
in previous work in the form of generative po-
etry (Greene et al., 2010) or lyrics (Wu et al.,
2013), where specified meter or rhyme schemes
are enforced.
?
Ozbal et al. (2013) produces
freeform text by filling templates with respect to
abstract notions such as humor.
3 Freeform Gene ation
We first addr ss th problem of freeform data
driven language g neration directly. We do not
set a semantic goal but instead ask only that the
output b considered a valid s ntenc , seeking a
model that captures the vari bility f language.
For clarity in our discussion, we phrase the
generation process in the following general terms
based around two classes of atomic units : Con-
texts and Outcomes. In order to specify a genera-
tion system, we must define
1. the set C of contexts c
2. the set O of outcomes o
3. the ?Imply? function I(c, o) ? List[c ? C]
where I(c, o) defines the further contexts implied
by the choice of outcome o for the context c. This
model can be made probabilistic by the definition
of P (o|c), where each outcome is sampled inde-
pendently given its context. We also require the
existence of a single unique root context, and refer
to the result of repeated sampling of outcomes for
contexts as a derivation tree. Finally, a mapping
from derivation tree to surface form is required to
produce actual text.
This is simply a convenient rephrasing of the
Context Free Grammar formalism, and as such
the systems we describe all have some equivalent
CFG interpretation. Indeed, to describe a tradi-
tional CFG, let C be the set of nonterminals, O be
the rules of the CFG, and I(c, o) returns a list of
the nonterminals on the right hand side of the rule
o and does not depend on c. P (o|c) would enforce
the choice of rules with appropriate lefthand sides.
The Context-Outcome terms can be more natu-
ral when describing other models where we do not
want to explicitly define the space of nonterminals.
A simple example is an n-gram model, for which
a context is an ordered list of words, an outcome
a single word, and I(c, o) can be procedurally de-
fined to produce a list containg a single context
made by dro ping the first word of the previous
context and appending the outcome to the end.
This formulation is well suited to data driven
estimation from a corpus of derivation trees.
While our me hods are easily extended to mul-
tiple derivations for each single sentence, in this
work we assume access to a single derivation for
each sentence in our data set. Maximum likeli-
hood estimation of P (o|c) is then as simple as nor-
malizing the counts of the observed outcomes for
each observed context. However, in order to ob-
tain contexts for which the conditional indepen-
dence assumption of P (o|c) is appropriate, it is
necessary to condition on a large amount of in-
f rmation. This leads to sparse estimates even on
large amounts of training data, a problem that can
be addressed by smoothing.
We identify two complementary types of
smoothing, and illustrate them with the following
sentences.
The furry d g bit me.
The cute cat licked me.
Assuming a simple bigram model where con-
text is the previous word and the outcome a sin-
gle word, an unsmoothed model trained on this
data can only gener e the two senten es verba-
tim. Imagine we have some way of knowing that
the tokens ?dog? and ?cat? are similar and would
like to leverage this fact . In our bigram model,
this amounts to the claim that the words that follow
?cat? are perhaps also likely to follow ?dog?. This
is easily handled with traditional smoothing tech-
niques, which interpolate between distributions
estimated for bot coarse, P (w|w
?1
=[is-animal]),
and fine, P (w|w
?1
=?dog?), contexts. We refer to
this as context smoothing.
However, we would also like to capture the in-
tuition that words which can be followed by ?dog?
can also be followed by ?cat?, which we will call
outcome smoothing. We extend our terminology
to describe a system that performs both types of
smoothing with the following
? the set
?
C of smooth contexts c?
? the set
?
O of smooth outcomes o?
? a smoothing function S
C
: C ?
?
C
? a smoothing function S
O
: O ?
?
O
We describe the generative process with the fol-
lowing flowchart
Figure 1: A flow chart depicting the decisions
made when choosing an outcome for a context.
The large circles show the set of items associated
with each decision, and contain examples items
for a bigram model where S
C
and S
O
map words
(e.g. dog) to semantic classes (e.g. [animal]).
We describe the smoothed generative process
with the flowchart shown in Figure 1. In order to
choose an outcome for a given context, two deci-
sions must be made. First, we must decide which
context we will employ, the true context or the
smooth context, marked by edges 1 or 2 respec-
tively. Next, we choose to generate a true outcome
or a smooth outcome, and if we select the latter
we use edge 6 to choose a true outcome given the
smooth outcome. The decision between edges 1
and 2 can be sampled from a Bernoulli random
variable with parameter ?
c
, with one variable es-
timated for each context c. The decision between
edges 5 and 3 and the one between 4 and 7 can also
be made with Bernoulli random variables, with pa-
rameter sets ?
c
and ?
c?
respectively.
This yields the full form of the unconstrained
probabilistic generative model as follows
P (o|c) = ?
c
P
1
(o|c) + (1? ?
c
)P
2
(o|S
C
(c))
P
1
(o|c) = ?
c
P
5
(o|c)+
(1? ?
c
)P
7
(o|o?)P
3
(o?|c) (1)
P
2
(o|c?) = ?
c?
P
6
(o|c)+
(1? ?
c?
)P
7
(o|o?)P
4
(o?|c?)
requiring estimation of the ? and ? variables as
well as the five multinomial distributions P
3?7
.
This can be done with a straightforward applica-
tion of EM.
4 Limiting Vocabulary
A primary concern in the generation of language
education exercises is the working vocabulary of
the students. If efficiency were not a concern, the
natural solution to the vocabulary constraint would
be rejection sampling: simply generate sentences
until one happens to obey the constraint. In this
section we show how to generate a sentence di-
rectly from this constrained set with a distribution
closely approximating that of the rejection sam-
pler.
4.1 Pruning
The first step is to prune the space of possible sen-
tences to those that obey the vocabulary constraint.
For the models we investigate there is a natural
predicate V (o) that is true if and only if an out-
come introduces a word that is out of vocab, and
so the vocabulary constraint is equivalent to the
requirement that V (o) is false for all possible out-
comes o. Considering transitions along edges in
Figure 1, the removal of all transitions along edges
5,6, and 7 that lead to outcomes where V (o) is true
satisfies this property.
Our remaining concern is that the generation
process does not reach a failure case. Again
considering transitions in Figure 1, failure occurs
when we require P (o|c) for some c and there is no
transition to c on edge 1 or S
C
(c) along edge 2.
We refer to such a context as invalid. Our goal,
which we refer to as consistency, is that for all
126
valid contexts c, all outcomes o that can be reached
in Figure 1 satisfy the property that all members of
I(c, o) are valid contexts.
To see how we might end up in failure, consider
a trigram model on POS/word pairs for which S
C
is the identity function and S
O
backs off to the
POS tag. Given a context c = (
(
t
?2
w
?2
)
,
(
t
?1
w
?1
)
) if
we generate along a path using edge 6 we will
choose a smooth outcome t
0
that we have seen
following c in the data and then independenently
choose a w
0
that has been observed with tag t
0
.
This implies a following context (
(
t
?1
w
?1
)
,
(
t
0
w
0
)
). If
we have estimated our model with observations
from data, there is no guarantee that this context
ever appeared, and if so there will be no available
transition along edges 1 or 2.
Let the list
?
I(c, o) be the result of the mapped
application of S
C
to each element of I(c, o). In
order to define an efficient algorithm, we require
the following property D referring to the amount
of information needed to determine
?
I(c, o). Sim-
ply put, D states if the smoothed context and out-
come are fixed, then the implied smooth contexts
are determined.
D {S
C
(c), S
O
(o)} ?
?
I(c, o)
To highlight the statement D makes, consider the
trigram POS/word model described above, but let
S
C
also map the POS/word pairs in the context
to their POS tags alone. D holds here because
given S
C
(c) = (t
?2
, t
?1
) and S
O
(o) = t
0
from
the outcome, we are able to determine the implied
smooth context (t
?1
, t
0
). If context smoothing in-
stead produced S
C
(c) = (t
?2
),D would not hold.
If D holds then we can show consistency based
on the transitions in Figure 1 alone as any com-
plete path through Figure 1 defines both c? and o?.
By D we can determine
?
I(c, o) for any path and
verify that all its members have possible transi-
tions along edge 2. If the verification passes for
all paths then the model is consistent.
Algorithm 1 produces a consistent model by
verifying each complete path in the manner just
described. One important feature is that it pre-
serves the invariant that if a context c can be
reached on edge 1, then S
C
(c) can be reached on
edge 2. This means that if the verification fails
then the complete path produces an invalid con-
text, even though we have only checked the mem-
bers of
?
I(c, o) against path 2.
If a complete path produces an invalid con-
text, some transition along that path must be re-
Algorithm 1 Pruning Algorithm
Initialize with all observed transitions
for all out of vocab o do
remove ?? o from edges 5,6, and 7
end for
repeat
for all paths in flow chart do
if ?c? ?
?
I(c, o) s.t. c? is invalid then
remove transition from edge 5,7,3 or 4
end if
end for
Run FIXUP
until edge 2 transitions did not change
moved. It is never optimal to remove transitions
from edges 1 or 2 as this unnecessarily removes
all downstream complete paths as well, and so for
invalid complete paths along 1-5 and 2-7 Algo-
rithm 1 removes the transitions along edges 5 and
7. The choice is not so simple for the complete
paths 1-3-6 and 2-4-6, as there are two remaining
choices. Fortunately, D implies that breaking the
connection on edge 3 or 4 is optimal as regardless
of which outcome is chosen on edge 6,
?
I(c, o) will
still produce the same invalid c?.
After removing transitions in this manner, some
transitions on edges 1-4 may no longer have any
outgoing transitions. The subroutine FIXUP re-
moves such transitions, checking edges 3 and 4
before 1 and 2. If FIXUP does not modify edge 2
then the model is consistent and Algorithm 1 ter-
minates.
4.2 Estimation
In order to replicate the behavior of the rejection
sampler, which uses the original probability model
P (o|c) from Equation 1, we must set the probabil-
ities P
V
(o|c) of the pruned model appropriately.
We note that for moderately sized vocabularies it
is feasible to recursively enumerate C
V
, the set of
all reachable contexts in the pruned model. In
further discussion we simplify the representation
of the model to a standard PCFG with C
V
as its
symbol set and its PCFG rules indexed by out-
comes. This also allows us to construct the reach-
ability graph for C
V
, with an edge from c
i
to c
j
for each c
j
? I(c
i
, o). Such an edge is given
weight P (o|c), the probability under the uncon-
strained model, and zero weight edges are not in-
cluded.
Our goal is to retain the form of the stan-
127
dard incremental recursive sampling algorithm for
PCFGs. The correctness of this algorithm comes
from the fact that the probability of a rule R ex-
panding a symbolX is precisely the probability of
all trees rooted atX whose first rule isR. This im-
plies that the correct sampling distribution is sim-
ply the distribution over rules itself. When con-
straints that disallow certain trees are introduced,
the probability of all trees whose first rule is R
only includes the mass from valid trees, and the
correct sampling distribution is the renormaliza-
tion of these values.
Let the goodness of a contextG(c) be the proba-
bility that a full subtree generated from c using the
unconstrained model obeys the vocabulary con-
straint. Knowledge of G(c) for all c ? C
V
al-
lows the calculation of probabilities for the pruned
model with
P
V
(o|c) ? P (o|c)
?
c
?
?I(c,o)
G(c
?
) (2)
While G(c) can be defined recursively as
G(c) =
?
o?O
P (o|c)
?
c
?
?I(c,o)
G(c
?
) (3)
its calculation requires that the reachability graph
be acyclic. We approximate an acyclic graph by
listing all edges in order of decreasing weight and
introducing edges as long as they do not create cy-
cles. This can be done efficiently with a binary
search over the edges by weight. Note that this ap-
proximate graph is used only in recursive estima-
tion of G(c), and the true graph can still be used
in Equation 2.
5 Generating Up
In this section we show how to efficiently gener-
ate sentences that contain an arbitrary word w
?
in
addition to the vocabulary constraint. We assume
the ability to easily find C
w
?
, a subset of C
V
whose
use guarantees that the resulting sentence contains
w
?
. Our goal is once again to efficiently emulate
the rejection sampler, which generates a derivation
tree T and accepts if and only if it contains at least
one member of C
w
?
.
Let T
w
?
be the set of derivation trees that would
be accepted by the rejection sampler. We present
a three stage generative model and its associated
probability distribution P
w
?
(?) over items ? for
which there is a functional mapping into T
w
?
.
In addition to the probabilities P
V
(o|c) from the
previous section, we require an estimate of E(c),
the expected number of times each context c ap-
pears in a single tree. This can be computed effi-
ciently using the mean matrix, described in Miller
and Osullivan (1992). This |C
V
| ? |C
V
| matri x M
has its entries defined as
M(i, j) =
?
o?O
P (o|c
i
)#(c
j
, c
i
, o) (4)
where the operator # returns the number of times
context c
j
appears I(c
i
, o). Defining a 1 ? |C
V
|
start state vector z
0
that is zero everywhere and 1
in the entry corresponding to the root context gives
E(z) =
?
?
i=0
z
0
M
i
which can be iteratively computed with sparse ma-
trix multiplication. Note that the ith term in the
sum corresponds to expected counts at depth i in
the derivation tree. With definitions of context and
outcome for which very deep derivations are im-
probable, it is reasonable to approximate this sum
by truncation.
Our generation model operates in three phases.
1. Chose a start context c
0
? C
w
?
2. Generate a spine S of contexts and outcomes
connecting c
0
to the root context
3. Fill in the full derivation tree T below all re-
maining unexpanded contexts
In the first phase, c
0
is sampled from the multi-
nomial
P
1
(c
0
) =
E(c
0
)
?
c?C
w
?
E(c)
(5)
The second step produces a spine S, which is
formally an ordered list of triples. Each element
of S records a context c
i
, an outcome o
i
, and the
index k in I(c
i
, o
i
) of the child along which the
spine progresses. The members of S are sampled
independantly given the previously sampled con-
text, starting from c
0
and terminating when the
root context is reached. Intuitively this is equiv-
alent to generating the path from the root to c
0
in
a bottom up fashion.
We define the probability P
?
of a triple
(c
i
, o
i
, k) given a previously sampled context c
j
128
as
P
?
({c
i
, o
i
, k}|c
j
) ?
{
E(c
i
)P
V
(o
i
|c
i
) I(c
i
, o
i
)[k] = c
j
0 otherwise
(6)
Let S = (c
1
, o
1
, k
1
) . . . (c
n
, o
n
, k
n
) be the re-
sults of this recursive sampling algorithm, where
c
n
is the root context, and c
1
is the parent context
of c
0
. The total probability of a spine S is then
P
2
(S|c
0
) =
|S|
?
i=1
E(c
i
)P
V
(o
i
|c
i
)
Z
i?1
(7)
Z
i?1
=
?
(c,o)?I
c
i?1
E(c)P
V
(o|c)#(c
i?1
, c, o)
(8)
where I
c?1
is the set of all (c, o) for which
P
?
(c, o, k|c
i?1
) is non-zero for some k. A key
observation is that Z
i?1
= E(c
i?1
), which can-
cels nearly all of the expected counts from the full
product. Along with the fact that the expected
count of the root context is one, the formula sim-
plifies to
P
2
(S|c
0
) =
|S|
?
i=1
P
V
(o
i
|c
i
)
E(c
0
)
(9)
The third step generates a final tree T by fill-
ing in subtrees below unexpanded contexts on the
spine S using the original generation algorithm,
yielding results with probability
P
3
(T |S) =
?
(c,o)?T/S
P
V
(o|c) (10)
where the set T/S includes all contexts that are
not ancestors of c
0
, as their outcomes are already
specified in S.
We validate this algorithm by considering its
distrubution over complete derivation trees T ?
T
w
?
. The algorithm generates ? = (T, S, c
0
) and
has a simple functional mapping into T
w
?
by ex-
tracting the first member of ? .
Combining the probabilities of our three steps
gives
P
w
?
(?) =
E(c
0
)
?
c?C
w
?
E(c)
|S|
?
i=1
P
V
(o
i
|c
i
)
E(c
0
)
?
(c,o)?T/S
P
V
(o|c)
P
w
?
(?) =
P
V
(T )
?
c?C
w
?
E(c)
=
1
?
P
V
(T ) (11)
where ? is a constant and
P
V
(T ) =
?
(c,o)?T
P
V
(o|c)
is the probability of T under the original model.
Note that several ? may map to the same T by
using different spines, and so
P
w
?
(T ) =
?(T )
?
P
V
(T ) (12)
where ?(T ) is the number of possible spines, or
equivalently the number of contexts c ? C
w
?
in T .
Recall that our goal is to efficiently emulate the
output of a rejection sampler. An ideal system P
w
?
would produce the complete set of derivation trees
accepted by the rejection sampler using P
V
, with
probabilities of each derivation tree T satisfying
P
w
?
(T ) ? P
V
(T ) (13)
Consider the implications of the following as-
sumption
A each T ? T
w
?
contains exactly one c ? C
w
?
A ensures that ?(T ) = 1 for all T , unifying Equa-
tions 12 and 13. A does not generally hold in prac-
tice, but its clear exposition allows us to design
models for which it holds most of the time, lead-
ing to a tight approximation.
The most important consideration of this type is
to limit redundancy in C
w
?
. For illustration con-
sider a dependency grammar model with parent
annotation where a context is the current word and
its parent word. When specifying C
w
?
for a partic-
ular w
?
, we might choose all contexts in which w
?
appears as either the current or parent word, but
a better choice that more closely satisfies A is to
choose contexts where w
?
appears as the current
word only.
129
END
END
Freeform Generation from a Fixed Vocabulary
Abstract
We investigate data driven natural lan-
guage generation under the constraint that
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximations to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
can easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at random from any existing corpus.
NLG is useful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first sets a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The second demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Freeform Generation from a Fixed Vocabulary
Abstract
We investigate data driven natural lan-
guage generation under the constraint that
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximations to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivat ng its empiri-
cal study is a difficult task. While many language
models used in statistical NLP ar generative and
can easily produce sample sentences from distri-
bu ions estim ed rom d ta, if all that is required
is a plausible sentence one might as well pick one
at random from any exi ting corpus.
NLG is useful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first sets a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The second demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Freefor Generation fro a Fixed Vocabulary
bstract
e investigate data driven natural lan-
guage generation under the constraint that
all ords ust co e fro a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified ord
ust also appear in the sentence. e
present fast approxi ations to the ideal re-
jection sa plers and increase variability in
generated text through controlled s ooth-
ing.
I tr cti
JJ
s li s i s
li
l
JJ
big
JJ S
big dogs
ata driven atural anguage eneration
( ) is a fascinating topic explored by aca-
de ics and artists alike, but otivating its e piri-
cal st is a iffic lt tas . ile a la a e
els se i statistical are e erati e a
ca easil r ce sa le se te ces fr istri-
ti s sti t r t , if ll t t is r ir
is l si l s t i t s ll i
t r fr i ti r .
i f l tr i t r li
t l rt i l i l t r li .
j it li t ti t i t
t t , i t it
i ti l . t i ti
Freeform Generation from a Fixed Vocabulary
Abstract
We investigate data driven natural lan-
guage generation under the constraint that
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximations t the ideal re-
jection samplers and increase variability in
generated text thro gh controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she d gs
ROOT VBZ NNS
lik s dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
can easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at random from any existing corpus.
NLG is useful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first sets a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The second demands not only that all words are
in-v cab, but specifies the inclusion of a single ar-
bitrary wo d somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Freeform Generation from a Fixed Vocabulary
Abstract
We investigate data driven natural lan-
guage generation under the constraint that
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximations to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
he dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
c n easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at random from any existing corpus.
NLG is useful when co straints are applied such
that only certain plausible sentences are valid. The
ajority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first sets a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The second demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Freeform Generation from a Fixed Vocabulary
Abstract
We investigate data driven natural lan-
guage generation under the constraint that
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximatio s to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
can easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at r ndom from any existing corpus.
NLG is useful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first s ts a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The second demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, w re students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Freeform Generation from a Fixed Vocabulary
Abstract
We investigate data driven natural lan-
guage generation under the constraint that
all words must come from a fixed arbi-
trary vocabul ry. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximations to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
can easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at random from any existing corpus.
NLG is useful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific c nstraints concern-
i g the word that are all wed in a s ntence. The
first sets a fixed vocabulary such that only sen-
tences where ll words are in-vocab are allowed.
The second demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge f arbitrary words are required. This use
Freeform Generatio fro a Fixed Vocabulary
Abstract
We inves igate data driven natural lan-
guage g ne ation under the constraint that
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specifi d word
must also appear in the sentence. We
present fast approximations to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 In roduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
can easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at random from any existing corpus.
NLG is useful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first sets a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The second demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Freeform Generation from a Fixed Vocabulary
Abstract
We inves gate data driven natural l -
guage generation u der the constraint tha
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximations to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natur l Language Generation
(NLG) is a fascinati g topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
can easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at random from any existing corpus.
NLG is us ful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first sets a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The seco d demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Figure 2: The generation system SPINEDEP draws on ep n ency tree sy t x where we use the term
node to refer to a POS/word pair. Contexts consist of a nod , its parent ode, and grandparent POS tag,
as shown in squares. Outcomes, shown in squares with rounded right s des, are full lists of dependents
or the END symbol. The shaded rectangles contain the results o I(c, o) from the indicated (c, o) pair.
6 Experiments
We train our models on sentences drawn from the
Simple English Wikipedia
1
. We obtained these
sentences from a data dump which we liberally fil-
tered to remove items such as lists and sentences
longer than 15 words or shorter then 3 words. We
parsed this data with the recently updated Stanford
Parser (Socher et al., 2013) to Penn Treebank con-
stituent form, and removed any sentence that did
not parse to a top level S containing at least one
NP and one VP child. Even with such strong fil-
ters, we retained over 140K sentences for use as
training data, and provide this exact set of parse
trees for use in future work.
2
Inspired by the application in language educa-
tion, for our vocabulary list we use the English Vo-
cabulary Profile (Capel, 2012), which predicts stu-
dent vocabulary at different stages of learning En-
glish as a second language. We take the most ba-
sic American English vocabulary (the A1 list), and
retrieve all inflections for each word using Sim-
pleNLG (Gatt and Reiter, 2009), yielding a vocab-
ulary of 1226 simple words and punctuation.
To mitigate noise in the data, we discard any
pair of context and outcome that appears only once
in the training data, and estimate the parameters of
the unconstrained model using EM.
6.1 Model Comparison
We experimented with many generation models
before converging on SPINEDEP, described in
Figure 2, which we use in these experiments.
1
http://simple.wikipedia.org
2
data url anon for review
Corr(%) % uniq
SPINEDEP unsmoothed 87.6 5.0
SPINEDEP WordNe 78.3 32.5
SPINEDEP word2vec 5000 72.6 52.9
SPINEDEP word2vec 500 65.3 60.2
KneserNey-5 64.0 25.8
DMV 33.7 71.2
Figure 3: System comparison based on human
judged correctness and the percentage of unique
sentences in a sample of 100K.
SPINEDEP uses dependency grammar elements,
with parent and grandparent information in the
contexts to capture such distinctions as that be-
tween main and clausal verbs. Its outcomes are
full configurations of dependents, capturing co-
ordinations such as subject-object pairings. This
specificity greatly increases the size of the model
and in turn reduces the speed of the true rejection
sampler, which fails over 90% of the time to pro-
duce an in-vocab sentence.
We found that large amounts of smoothing
quickly diminishes the amount of error free out-
put, and so we smooth very cautiously, map-
ping words in the contexts and outcomes to
fine semantic classes. We compare the use
of human annotated hypernyms from Word-
net (Miller, 1995) with automatic word clusters
from word2vec (Mikolov et al., 2013), based on
vector space word embeddings, evaluating both
500 and 5000 clusters for the latter.
We compare these models against several base-
line alternatives, shown in Figure 3. To determine
130
correctness, used Amazon Mechanical Turk, ask-
ing the question: ?Is this sentence plausible??. We
further clarified this question in the instructions
with alternative definitions of plausibility as well
as both positive and negative examples. Every sen-
tence was rated by five reviewers and its correct-
ness was determined by majority vote, with a .496
Fleiss kappa agreement. To avoid spammers, we
limited our hits to Turkers with an over 95% ap-
proval rating.
Traditional language modeling techniques such
as such as the Dependency Model with Va-
lence (Klein and Manning, 2004) and 5-gram
Kneser Ney (Chen and Goodman, 1996) perform
poorly, which is unsurprising as they are designed
for tasks in recognition rather than generation. For
n-gram models, accuracy can be greatly increased
by decreasing the amount of smoothing, but it be-
comes difficult to find long n-grams that are com-
pletely in-vocab and results become redundant,
parroting the few completely in-vocab sentences
from the training data. The DMV is more flex-
ible, but makes assumptions of conditional inde-
pendence that are far too strong. As a result it
is unable to avoid red flags such as sentences not
ending in punctuation or strange subject-object co-
ordinations. Without smoothing, SPINEDEP suf-
fers from a similar problem as unsmoothed n-gram
models; high accuracy but quickly vanishing pro-
ductivity.
All of the smoothed SPINEDEP systems show
clear advantages over their competitors. The
tradeoff between correctness and generative ca-
pacity is also clear, and our results suggest that the
number of clusters created from the word2vec em-
beddings can be used to trace this curve. As for the
ideal position in this tradeoff, we leave such deci-
sions which are particular to specific application to
future work, arbitrarily using SPINEDEP WordNet
for our following experiments.
6.2 Fixed Vocabulary
To show the tightness of the approximation pre-
sented in Section 4.2, we evaluate three settings
for the probabilities of the pruned model. The first
is a weak baseline that sets all distributions to uni-
form. For the second, we simply renormalize the
true model?s probabilities, which is equivalent to
setting G(c) = 1 for all c in Equation 2. Finally,
we use our proposed method to estimate G(c).
We show in Figure 4 that our estimation method
Corr(%) -LLR
True RS 79.3 ?
Uniform 47.3 96.2
G(c) = 1 77.0 25.0
G(c) estimated 78.3 1.0
Figure 4: A comparison of our system against both
a weak and a strong baseline based on correctness
and the negative log of the likelihood ratio mea-
suring closeness to the true rejection sampler.
more closely approximates the distribution of the
rejection sampler by drawing 500K samples from
each model and comparing them with 500K sam-
ples from the rejection sampler itself. We quantify
this comparison with the likelihood ratio statistic,
evaluating the null hypothesis that the two sam-
ples were drawn from the same distribution. Not
only does our method more closely emulate that of
the rejection sampler, be we see welcome evidence
that closeness to the true distribution is correlated
with correctness.
6.3 Word Inclusion
To explore the word inclusion constraint, for each
word in our vocabulary list we sample 1000 sen-
tences that are constrained to include that word
using both unsmoothed and WordNet smoothed
SPINEDEP. We compare these results to the ?Cor-
pus? model that simply searches the training data
and uniformly samples from the existing sentences
that satisfy the constraints. This corpus search ap-
proach is quite a strong baseline, as it is trivial to
implement and we assume perfect correctness for
its results.
This experiment is especially relevant to our
motivation of language education. The natural
question when proposing any NLG approach is
whether or not the ability to automatically produce
sentences outweighs the requirement of a post-
process to ensure goal-appropriate output. This
is a challenging task in the context of language
education, as most applications such as exam or
homework creation require only a handful of sen-
tences. In order for an NLG solution to be appro-
priate, the constraints must be so strong that a cor-
pus search based method will frequently produce
too few options to be useful. The word inclusion
constraint highlights the strengths of our method
as it is not only highly plausible in a language ed-
131
# < 10 # > 100 Corr(%)
Corpus 987 26 100
Unsmooth 957 56 89.0
Smooth 544 586 79.0
Figure 5: Using systems that implement the word
inclusion constraint, this table shows the number
of words for which the amount of unique sentences
out of 1000 samples was less than 10 or greater
than 100, along with the correctness of each sys-
tem.
ucation setting but difficult to satisfy by chance in
large corpora.
Figure 5 shows that the corpus search approach
fails to find more than ten sentences that obey the
word inclusion constraints for most target words.
Moreover, it is arguably the case that unsmoothed
SPINEDEP is even worse due to its inferior cor-
rectness. With the addition of smoothing, how-
ever, we see a drastic shift in the number of words
for which a large number of sentences can be pro-
duced. For the majority of the vocabulary words
this model generates over 100 sentences that obey
both constraints, of which approximately 80% are
valid English sentences.
7 Conclusion
In this work we address two novel NLG con-
straints, fixed vocabulary and fixed vocabulary
with word inclusion, that are motivated by lan-
guage education scenarios. We showed that un-
der these constraints a highly parameterized model
based on dependency tree syntax can produce a
wide range of accurate sentences, outperforming
the strong baselines of popular generative lan-
guage models. We developed a pruning and es-
timation algorithm for the fixed vocabulary con-
straint and showed that it not only closely approx-
imates the true rejection sampler but also that the
tightness of approximation is correlated with hu-
man judgments of correctness. We showed that
under the word inclusion constraint, precise se-
mantic smoothing produces a system whose abili-
ties exceed the simple but powerful alternative of
looking up sentences in large corpora.
SPINEDEP works surprisingly well given the
widely held stigma that freeform NLG produces
either memorized sentences or gibberish. Still, we
expect that better models exist, especially in terms
of definition of smoothing operators. We have pre-
sented our algorithms in the flexible terms of con-
text and outcome, and clearly stated the properties
that are required for the full use of our methodol-
ogy. We have also implemented our code in these
general terms
3
, which performs EM based param-
eter estimation as well as efficient generation un-
der the constraints discussed above. All systems
used in this work with the exception of 5-gram in-
terpolated Kneser-Ney were implemented in this
way, are included with the code, and can be used
as templates.
We recognize several avenues for continued
work on this topic. The use of form-based con-
straints such as word inclusion has clear applica-
tion in language education, but many other con-
straints are also desirable. The clearest is perhaps
the ability to constrain results based on a ?vocab-
ulary? of syntactic patterns such as ?Not only ...
but also ...?. Another extension would be to incor-
porate the rough communicative goal of response
to a previous sentence as in Wu et al. (2013) and
attempt to produce in-vocab dialogs such as are
ubiquitous in language education textbooks.
Another possible direction is in the improve-
ment of the context-outcome framework itself.
While we have assumed a data set of one deriva-
tion tree per sentence, our current methods eas-
ily extend to sets of weighted derivations for each
sentence. This suggests the use of techinques that
have proved effective in grammar estimation that
reason over large numbers of possible derivations
such as Bayesian tree substitution grammars or un-
supervised symbol refinement.
References
Anja Belz. 2008. Automatic generation of weather
forecast texts using comprehensive probabilistic
generation-space models. Natural Language Engi-
neering, 14(4):431?455.
A. Capel. 2012. The english vocabulary profile.
http://vocabulary.englishprofile.org/.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Simon Colton, Jacob Goodwin, and Tony Veale. 2012.
Full face poetry generation. In Proceedings of the
3
url anon for review
132
Third International Conference on Computational
Creativity, pages 95?102.
Albert Gatt and Ehud Reiter. 2009. Simplenlg: A re-
alisation engine for practical applications. In Pro-
ceedings of the 12th European Workshop on Natu-
ral Language Generation, ENLG ?09, pages 90?93,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Erica Greene, Tugba Bodrumlu, and Kevin Knight.
2010. Automatic analysis of rhythmic poetry with
applications to generation and translation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 524?533, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Long Jiang and Ming Zhou. 2008. Generating chi-
nese couplets using a statistical mt approach. In
Proceedings of the 22nd International Conference
on Computational Linguistics-Volume 1, pages 377?
384. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure: Mod-
els of dependency and constituency. In Proceed-
ings of the 42Nd Annual Meeting on Association for
Computational Linguistics, ACL ?04, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Michael I. Miller and Joseph A. Osullivan. 1992. En-
tropies and combinatorics of random branching pro-
cesses and context-free languages. IEEE Transac-
tions on Information Theory, 38.
George A. Miller. 1995. Wordnet: A lexical database
for english. COMMUNICATIONS OF THE ACM,
38:39?41.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2013. Generating expressions that refer to visible
objects. In HLT-NAACL, pages 1174?1184.
Jack Mostow and Hyeju Jang. 2012. Generating di-
agnostic multiple choice comprehension cloze ques-
tions. In Proceedings of the Seventh Workshop
on Building Educational Applications Using NLP,
pages 136?146. Association for Computational Lin-
guistics.
G?ozde
?
Ozbal, Daniele Pighin, and Carlo Strapparava.
2013. Brainsup: Brainstorming support for creative
sentence generation. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1446?
1455, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Ananth Ramakrishnan A, Sankar Kuppan, and
Sobha Lalitha Devi. 2009. Automatic generation
of tamil lyrics for melodies. In Proceedings of the
Workshop on Computational Approaches to Linguis-
tic Creativity, pages 40?46. Association for Compu-
tational Linguistics.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing With Composi-
tional Vector Grammars. In ACL.
Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Ya-
mamoto. 2005. Measuring non-native speak-
ers? proficiency of english by using a test with
automatically-generated fill-in-the-blank questions.
In Proceedings of the second workshop on Building
Educational Applications Using NLP, pages 61?68.
Association for Computational Linguistics.
Alessandro Valitutti, Hannu Toivonen, Antoine
Doucet, and Jukka M. Toivanen. 2013. ?let every-
thing turn well in your wife?: Generation of adult
humor using lexical constraints. In ACL (2), pages
243?248.
Dekai Wu, Karteek Addanki, Markus Saers, and
Meriem Beloucif. 2013. Learning to freestyle: Hip
hop challenge-response induction via transduction
rule segmentation. In EMNLP, pages 102?112.
133
