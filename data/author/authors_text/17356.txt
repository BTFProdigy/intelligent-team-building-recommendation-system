Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 478?482, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
Columbia NLP: Sentiment Detection of Subjective Phrases in Social Media
Sara Rosenthal
Department of Computer Science
Columbia University
New York, NY 10027, USA
sara@cs.columbia.edu
Kathleen McKeown
Department of Computer Science
Columbia University
New York, NY 10027, USA
kathy@cs.columbia.edu
Abstract
We present a supervised sentiment detection
system that classifies the polarity of subjec-
tive phrases as positive, negative, or neutral. It
is tailored towards online genres, specifically
Twitter, through the inclusion of dictionaries
developed to capture vocabulary used in on-
line conversations (e.g., slang and emoticons)
as well as stylistic features common to social
media. We show how to incorporate these
new features within a state of the art system
and evaluate it on subtask A in SemEval-2013
Task 2: Sentiment Analysis in Twitter.
1 Introduction
People use social media to write openly about their
personal experiences, likes and dislikes. The follow-
ing sentence from Twitter is a typical example: ?To-
morrow I?m coming back from Barcelona...I don?t
want! :(((?. The ability to detect the sentiment ex-
pressed in social media can be useful for understand-
ing what people think about the restaurants they
visit, the political viewpoints of the day, and the
products they buy. These sentiments can be used
to provided targeted advertising, automatically gen-
erate reviews, and make various predictions, such as
political outcomes.
In this paper we develop a sentiment detection al-
gorithm for social media that classifies the polarity
of sentence phrases as positive, negative, or neutral
and test its performance in Twitter through the par-
ticipation in the expression level task (subtask A)
of the SemEval-2013 Task 2: Sentiment Analysis
in Twitter (Wilson et al, 2013) which the authors
helped organize. To do so, we build on previous
work on sentiment detection algorithms for the more
formal news genre, notably the work of Agarwal et
al (2009), but adapt it for the language of social me-
dia, in particular Twitter. We show that exploiting
lexical-stylistic features and dictionaries geared to-
ward social media are useful in detecting sentiment.
In this rest of this paper, we discuss related work,
including the state of the art sentiment system (Agar-
wal et al, 2009) our method is based on, the lexicons
we used, our method, and experiments and results.
2 Related Work
Several recent papers have explored sentiment anal-
ysis in Twitter. Go et al(2009) and Pak and
Paroubek (2010) classify the sentiment of tweets
containing emoticons using n-grams and POS. Bar-
bosa and Feng (2010) detect sentiment using a po-
larity dictionary that includes web vocabulary and
tweet-specific social media features. Bermingham
and Smeaton (2010) compare polarity detection in
twitter to blogs and movie reviews using lexical fea-
tures. Agarwal et al(2011) perform polarity senti-
ment detection on the entire tweet using features that
are somewhat similar to ours: the DAL, lexical fea-
tures (e.g. POS and n-grams), social media features
(e.g. slang and hashtags) and tree kernel features. In
contrast to this related work, our approach is geared
towards predicting sentiment is at the phrase level as
opposed to the tweet level.
3 Lexicons
Several lexicons are used in our system. We use the
DAL and expand it with WordNet, as it was used in
478
Corpus DAL
NNP
(Post
DAL)
Word
Length-
ening
WordNet Wiktionary Emoticons
Punctuation
& Numbers
Not
Covered
Twitter - Train 42.9% 19.2% 1.4% 10.2% 12.7% 0.3% 1.5% 11.7%
Twitter - Dev 57.3% 13.8% 1.1% 7.1% 12.2% 0.4% 2.7% 5.4%
Twitter - Test 49.9% 15.6% 1.4% 9.6% 12.1% 0.5% 1.6% 9.3%
SMS - Test 60.1% 3.6% 0.6% 7.9% 14.7% 0.6% 1.9% 10.3%
Table 1: Coverage for each of the lexicons in the training and test corpora?s.
the original work (Agarwal et al, 2009), and expand
it further to use Wiktionary and an emoticon lexicon.
We consider proper nouns that are not in the DAL to
be objective. We also shorten words that are length-
ened to see if we can find the shortened version in
the lexicons (e.g. sweeeet? sweet). The coverage
of the lexicons for each corpus is shown in Table 1.
3.1 DAL
The Dictionary of Affect and Language (DAL)
(Whissel, 1989) is an English language dictionary
of 8742 words built to measure the emotional mean-
ing of texts. In addition to using newswire, it was
also built from individual sources such as interviews
on abuse, students? retelling of a story, and adoles-
cent?s descriptions of emotions. It therefore covers a
broad set of words. Each word is given three scores
(pleasantness - also called evaluation (ee), active-
ness (aa), and imagery (ii)) on a scale of 1 (low)
to 3 (high). We compute the polarity of a chunk in
the same manner as the original work (Agarwal et
al., 2009), using the sum of the AE Space Score?s
(|
?
ee2 + aa2|) of each word within the chunk.
3.2 WordNet
The DAL does cover a broad set of words, but we
will still often encounter words that are not included
in the dictionary. Any word that is not in the DAL
and is not a proper noun is accessed in WordNet
(Fellbaum, 1998) 1 and, if it exists, the DAL scores
of the synonyms of its first sense are used in its
place. In addition to the original approach, if there
are no synonyms we look at the hypernym. We then
compute the average scores (ee, aa, and ii) of all the
words and use that as the score for the word.
1We cannot use SentiWordNet because we are interested in
the DAL scores
3.3 Wiktionary
We use Wiktionary, an online dictionary, to supple-
ment the common words that are not found in Word-
Net and the DAL. We first examine all ?form of? re-
lationships for the word such as ?doesnt? is a ?mis-
spelling of? ?doesn?t?, and ?tonite? is an ?alternate
form of? ?tonight?. If no ?form of? relationships ex-
ist, we take all the words in the definitions that have
their own Wiktionary page and look up the scores
for each word in the DAL. (e.g., the verb definition
for LOL (laugh out loud) in Wiktionary is ?To laugh
out loud? with ?laugh? having its own Wiktionary
definition; it is therefore looked up in the DAL and
the score for ?laugh? is used for ?LOL?.) We then
compute the average scores (ee, aa, and ii) of all the
words and use that as the score for the word.
3.4 Emoticon Dictionary
emoticon :) :D <3 :( ;)
definition happy laughter love sad wink
Table 2: Popular emoticons and their definitions
We created a simple lexicon to map common
emoticons to a definition in the DAL. We looked at
over 1000 emoticons gathered from several lists on
the internet2 and computed their frequencies within
a LiveJournal blog corpus. (In the future we would
like to use an external Twitter corpus). We kept
the 192 emoticons that appeared at least once and
mapped each emoticon to a single word definition.
The top 5 emoticons and their definitions are shown
in Table 2. When an emoticon is found in a tweet we
look up its definition in the DAL.
4 Methods
We run our data through several pre-processing steps
to preserve emoticons and expand contractions. We
2www.chatropolis.com, www.piology.org, en.wikipedia.org
479
General Social Media
Feature Example Feature Example
Capital Words Hello Emoticons :)
Out of Vocabulary duh Acronyms LOL
Punctuation . Repeated Questions ???
Repeated Punctuation #@. Exclamation Points !
Punctuation Count 5 Repeated Exclamations !!!!
Question Marks ? Word Lengthening sweeeet
Ellipses ... All Caps HAHA
Avg Word Length 5 Links/Images www.url.com
Table 3: List of lexical-stylistic features and examples.
then pre-process the sentences to add Part-of-Speech
tags (POS) and chunk the sentences using the CRF
tagger and chunker (Phan, 2006a; Phan, 2006b).
The chunker uses three labels, ?B? (beginning), ?I?
(in), and ?O? (out). The ?O? label tends to be ap-
plied to punctuation which one typically wants to
ignore. However, in this context, punctation can be
very important (e.g. exclamation points, and emoti-
cons). Therefore, we append words/phrases tagged
as O to the prior B-I chunk.
We apply the dictionaries to the preprocessed sen-
tences to generate lexical, syntactic, and stylistic
features. All sets of features were reduced using chi-
square in Weka (Hall et al, 2009).
4.1 Lexical and Syntactic Features
We include POS tags and the top 500 n-gram fea-
tures(Agarwal et al, 2009). We experimented with
different amounts of n-grams and found that more
than 500 n-grams reduced performance.
The DAL and other dictionaries are used along
with a negation state machine(Agarwal et al, 2009)
to determine the polarity for each word in the sen-
tence. We include all the features described in the
original system (Agarwal et al, 2009).
4.2 Lexical-Stylistic Features
We include several lexical-stylistic features (see Ta-
ble 3) that can occur in all datasets. We divide these
features into two groups, general: ones that are
common across online and traditional genres, and
social media: one that are far more common in on-
line genres. Examples of general style features are
exclamation points and ellipses. Examples of social
media style features are emoticons and word length-
ening. Word lengthening is a common phenomenon
Figure 1: Percentage of lexical-stylistic features that are
negative (top), neutral (middle), and positive (bottom) in
the Twitter training corpus.
in social media where letters are repeated to indi-
cate emphasis (e.g. sweeeet). It is particularly com-
mon in opinionated words (Brody and Diakopoulos,
2011). The count values of each feature was normal-
ized by the number of words in the phrase.
The percentage of lexical-stylistic features that
are positive/negative/neutral is shown in Figure 1.
For example, emoticons tend to indicate a positive
phrase in Twitter. Each stylistic feature accounts for
less than 2% of the sentence but at least one of the
stylistic features exists in 61% of the Tweets.
We also computed the most frequent emoticons
(<3, :D), acronyms (lol), and punctuation symbols
(#) within a subset of the Twitter training set and
included those as additional features.
5 Experiments and Results
This task was evaluated on the Twitter dataset pro-
vided by Semeval-2013 Task 2, subtask A, which the
authors helped organize. Therefore, a large portion
of time was spent on creating the dataset.
480
Experiment Twitter SMS
Dev Test
Majority 36.3 38.1 31.5
Just DAL 70.1 72.3 67.1
WordNet 72.2 73.6 67.7
Wiktionary 72.8 73.7 68.7
Style 71.5 73.7 69.7
n-grams 75.2 75.7 72.5
WordNet+Style 73.2 74.6 70.1
Dictionaries+Style 74.0 75.0 70.2
Dictionaries+Style+n-grams 75.8 77.6 73.3
Table 4: Experiments using the Twitter corpus. Results
are shown using average F-measure of the positive and
negative class. All experiments include the DAL. The
dictionaries refer to WordNet, Wiktionary, and Emoticon.
Style refers to Lexical-Stylistic features. All results ex-
ceed the majority baseline significantly.
We ran all of our experiments in Weka (Hall et
al., 2009) using Logistic Regression. We also exper-
imented with other learning methods but found that
this worked best. All results are shown using the av-
erage F-measure of the positive and negative class.
We tuned our system for Semeval-2013 Task 2,
subtask A, using the provided development set and
ran it on the provided Twitter and SMS test data.
Our results are shown in Table 4 with all results
being statistically significant over a majority base-
line. We also use the DAL as a baseline to in-
dicate how useful lexical-stylistic features (specifi-
cally those geared towards social media) and the dic-
tionaries are in improving the performance of sen-
timent detection of phrases in online genres in con-
trast to using just the DAL. The results that are statis-
tically significant (computed using the Wilcoxon?s
test, p ? .02) shown in bold. Our best results for
each dataset include all features with an average F-
measure of 77.6% and 73.3% for the Twitter and
SMS test sets respectively resulting in a significant
improvement of more than 5% for each test set over
the DAL baseline.
At the time of submission, we had not experi-
mented with n-grams, and therefore chose the Dic-
tionaries+Style system as our final version for the
official run resulting in a rank of 12/22 (75% F-
measure) for Twitter and 13/19 (70.2% F-measure)
for SMS. Our rank with the best system, which in-
cludes n-grams, would remain the same for Twitter,
but bring our rank up to 10/19 for SMS.
We looked more closely at the impact of our new
features and as one would expect, feature selection
found the general and social media style features
(e.g. emoticons, :(, lol, word lengthening) to be use-
ful in Twitter and SMS data. Using additional online
dictionaries is useful in Twitter and SMS, which is
understandable because they both have poor cover-
age in the DAL and WordNet. In all cases using
n-grams was the most useful which indicates that
context is most important. Using Dictionaries and
Style in addition to n-grams did provide a signifi-
cant improvement in the Twitter test set, but not in
the Twitter Dev and SMS test set.
6 Conclusion and Future Work
We have explored whether social media features,
Wiktionary, and emoticon dictionaries positively im-
pact the accuracy of polarity detection in Twitter and
other online genres. We found that social media re-
lated features can be used to predict sentiment in
Twitter and SMS. In addition, Wiktionary helps im-
prove the word coverage and though it does not pro-
vide a significant improvement over WordNet, it can
be used in place of WordNet. On the other hand, we
found that using the DAL and n-grams alone does al-
most as well as the best system. This is encouraging
as it indicates that content is important and domain
independent sentiment systems can do a good job of
predicting sentiment in social media.
The results of the SMS messages dataset indicate
that even though the online genres are different, the
training data in one online genre can indeed be used
to predict results with reasonable accuracy in the
other online genre. These results show promise for
further work on domain adaptation across different
kinds of social media.
7 Acknowledgements
This research was partially funded by (a) the ODNI,
IARPA, through the U.S. Army Research Lab and
(b) the DARPA DEFT Program. All statements of
fact, opinion or conclusions contained herein are
those of the authors and should not be construed as
representing the official views, policies, or positions
of IARPA, the ODNI, the Department of Defense, or
the U.S. Government.
481
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen R. Mcke-
own. 2009. Contextual phrase-level polarity analysis
using lexical affect scoring and syntactic n-grams. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, EACL ?09, pages 24?32, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analy-
sis of twitter data. In Proceedings of the Workshop
on Language in Social Media (LSM 2011), pages 30?
38, Portland, Oregon, June. Association for Computa-
tional Linguistics.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In COLING (Posters), pages 36?44.
Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: is brevity an advan-
tage? In Jimmy Huang, Nick Koudas, Gareth J. F.
Jones, Xindong Wu, Kevyn Collins-Thompson, and
Aijun An, editors, CIKM, pages 1833?1836. ACM.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! using word
lengthening to detect sentiment in microblogs. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562?570,
Edinburgh, Scotland, UK., July. Association for Com-
putational Linguistics.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, Cambridge,
MA ; London, May.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing, pages 1?6.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
Xuan-Hieu Phan. 2006a. Crfchunker: Crf english phrase
chunker.
Xuan-Hieu Phan. 2006b. Crftagger: Crf english phrase
tagger.
C. M. Whissel. 1989. The dictionary of affect in lan-
guage. In R. Plutchik and H. Kellerman, editors, Emo-
tion: theory research and experience, volume 4, Lon-
don. Acad. Press.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013.
Semeval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the 7th International Workshop on
Semantic Evaluation. Association for Computational
Linguistics.
482
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 198?202,
Dublin, Ireland, August 23-24, 2014.
Columbia NLP: Sentiment Detection of Sentences and Subjective Phrases
in Social Media
Sara Rosenthal
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
sara@cs.columbia.edu
Apoorv Agarwal
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
apoorv@cs.columbia.edu
Kathleen McKeown
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
kathy@cs.columbia.edu
Abstract
We present two supervised sentiment de-
tection systems which were used to com-
pete in SemEval-2014 Task 9: Senti-
ment Analysis in Twitter. The first sys-
tem (Rosenthal and McKeown, 2013) clas-
sifies the polarity of subjective phrases as
positive, negative, or neutral. It is tai-
lored towards online genres, specifically
Twitter, through the inclusion of dictionar-
ies developed to capture vocabulary used
in online conversations (e.g., slang and
emoticons) as well as stylistic features
common to social media. The second sys-
tem (Agarwal et al., 2011) classifies entire
tweets as positive, negative, or neutral. It
too includes dictionaries and stylistic fea-
tures developed for social media, several
of which are distinctive from those in the
first system. We use both systems to par-
ticipate in Subtasks A and B of SemEval-
2014 Task 9: Sentiment Analysis in Twit-
ter. We participated for the first time in
Subtask B: Message-Level Sentiment De-
tection by combining the two systems to
achieve improved results compared to ei-
ther system alone.
1 Introduction
In this paper we describe two prior sentiment de-
tection algorithms for social media. Both systems
(Rosenthal and McKeown, 2013; Agarwal et al.,
2011) classify the polarity of sentence phrases and
This work is licensed under a Creative Commons At-
tribution 4.0 International License. Page numbers and pro-
ceedings footer are added by the organizers. License details:
http://creativecommons.org/licenses/by/4.0/
tweets as positive, negative, or neutral. These al-
gorithms were used to participate in the the expres-
sion level task (Subtask A) and message level task
(Subtask B) of the SemEval-2014 Task 9: Senti-
ment Analysis in Twitter (Rosenthal et al., 2014)
which one of the authors helped organize.
We first show improved results compared to our
participation in the prior year in the expression-
level task (Subtask A) by incorporating a new dic-
tionary and new features into the system. Our fo-
cus this year was on Subtask B which we partici-
pated in for the first time. We integrated two sys-
tems to achieve improved results compared to ei-
ther system alone. Our analysis shows that the first
system performs better on recall while the second
system performs better on precision. We used con-
fidence metrics outputted by the systems to deter-
mine which answer should be used. This resulted
in a slight improvement in the Twitter dataset com-
pared to either system alone. In this rest of this
paper, we discuss related work, the methods for
each system, and experiments and results for each
subtask using the data provided by Semeval-2014
Task 9: Sentiment Analysis in Twitter (Rosenthal
et al., 2014).
2 Related Work
Several recent papers have explored sentiment
analysis in Twitter. Go et al (2009) and Pak
and Paroubek (2010) classify the sentiment of
tweets containing emoticons using n-grams and
POS. Barbosa and Feng (2010) detect sentiment
using a polarity dictionary that includes web vo-
cabulary and tweet-specific social media features.
Bermingham and Smeaton (2010) compare polar-
ity detection in twitter to blogs and movie reviews
using lexical features.
Finally, there is a large amount of related work
198
through the participants of Semeval 2013 Task
2, and Semeval 2014 Task9: Sentiment Analysis
in Twitter (Nakov et al., 2013; Rosenthal et al.,
2014). A full list of teams and results can be found
in the task description papers.
3 Phrased-Based Sentiment Detection
We developed a phrase based sentiment detection
system geared towards Social Media by augment-
ing the state of the art system developed by Agar-
wal et al. (2009) to include additional dictionar-
ies such as Wiktionary and new features such as
word lengthening (e.g. helllllloooo) and emoti-
cons (e.g. :)) (Rosenthal and McKeown, 2013).
We initially evaluated our system through our par-
ticipation in the first Sentiment Analysis in Twitter
task (Nakov et al., 2013). We have improved our
system this year by adding a new dictionary and
additional features.
3.1 Lexicons
We assign a prior polarity score to each word by
using the scores provided by the Dictionary of
Affect in Language (DAL) (Whissel, 1989) aug-
mented with WordNet (Fellbaum, 1998) to im-
prove coverage. We additionally augment it with
Wiktionary, emoticon, and acronym dictionaries
to improve coverage in social media (Rosenthal
and McKeown, 2013). The DAL covers 50.1% of
the vocabulary, 16.5% are proper nouns which we
exclude due to their lack of polarity. WordNet cov-
ers 8.7% of the vocabulary and Wiktionary covers
12.5% of the vocabulary. Finally, 3.6% of the vo-
cabulary are emoticons, acronyms, word length-
ening, and forms of punctuation. 8.6% of the vo-
cabulary is not covered which means we find a
prior polarity for 96.4% of the vocabulary. In ad-
dition to these dictionaries we also use SentiWord-
Net (Baccianella et al., 2010) as a new distinct fea-
ture that is used in addition to the prior polarity
computed from the DAL scores.
3.2 Method
We include POS tags and the top n-gram fea-
tures as described in prior work (Agarwal et al.,
2009; Rosenthal and McKeown, 2013). The DAL
and other dictionaries are used along with a nega-
tion state machine (Agarwal et al., 2009) to deter-
mine the polarity for each word in the sentence.
We include all the features described in the orig-
inal system (Agarwal et al., 2009) such as the
Data Set Majority 2013 2014
Twitter Dev 38.14 77.6 81.5
Twitter Test 42.22 N/A 76.54
Twitter Sarcasm 39.81 N/A 61.76
SMS 31.45 73.3 74.55
LiveJournal 33.42 N/A 78.19
Table 1: A comparison between the 2013 and 2014
results for Subtask A using the SemEval Twitter
training corpus. All results exceed the majority
baseline of the positive class significantly.
DAL scores, polar chunk n-grams, and count of
syntactic chunks with their prior polarity based
on the chunks position. Finally, we include sev-
eral lexical-stylistic features that can occur in all
datasets. We divide these features into two groups,
general: ones that are common across online and
traditional genres (e.g. exclamation points), and
social media: one that are far more common in
online genres (e.g. emoticons). The features are
described in further detail in the precursor to this
work (Rosenthal and McKeown, 2013). Feature
selection was performed using chi-square in Weka
(Hall et al., 2009).
In addition we introduce some new features
that were not used in the prior year. SentiWord-
Net (Baccianella et al., 2010) is a sentiment dic-
tionary built upon WordNet that contains scores
for each word where scores > 0 indicate the word
is positive and scores < 0 indicate the word is neg-
ative. We sum the scores for each word in the
phrase and use this as a single polarity feature.
We found that this feature alone gave us a 2% im-
provement over our best results from last year. We
also include some other minor features: tweet and
phrase length and the position of the phrase within
the tweet.
3.3 Experiments and Results
We ran all of our experiments in Weka (Hall et al.,
2009) using Logistic Regression. We also exper-
imented with other learning methods (e.g. SVM
and Naive Bayes) but found that Logistic Regres-
sion worked the same or better than other methods.
All results are shown using the average F-measure
of the positive and negative class. The results are
compared against the majority baseline of the pos-
itive class. We do not use neutral/objective as the
majority class because it is not included in the av-
erage F-score in the Semeval task.
The full results in the participation of SemEval
2014: Sentiment Analysis in Twitter, Subtask A,
199
are shown in Table 1. Our system outperforms the
majority baseline significantly in all classes. Our
submitted system was trained using 3-way clas-
sification (positive/negative/polarity). It included
all the dictionaries from prior years and the top
100 n-grams with feature selection. In addition,
it included SentiWordNet and the other new fea-
tures added in 2014 which provided a 4% increase
compared to our best results during the prior year
(77.6% to 81.5%) and a rank of 10/20 amongst the
constrained systems which used no external data.
Our results on the new test set is 76.54% for a rank
of 14/20. We do not do well in detecting the po-
larity of phrases in sarcastic tweets. This is consis-
tent with the other teams as sarcastic tweets tend to
have their polarity flipped. The improvements to
our system provided a 1% boost in the SMS data
with a rank of 15/20. Finally, in the LiveJournal
dataset we had an F-Score of 78.19% for a rank of
12/20.
4 Message-Level Sentiment Detection
Our message-level system combines two prior sys-
tems to achieve improved results. The first system
inputs an entire tweet as a ?phrase? to the phrase-
level sentiment detection system described in Sec-
tion 3. The second system is described below.
4.1 Lexicons
The second system (Agarwal et al., 2011) makes
use of two dictionaries distinctive from the other
system: 1) an emoticon dictionary and 2) an
acronym dictionary. The emoticon dictionary was
prepared by hand-labeling 170 emoticons listed on
Wikipedia.
1
For example, :) is labeled as positive
whereas :=( is labeled as negative. Each emoticon
is assigned a label from the following set of labels:
Extremely-positive, Extremely-negative, Positive,
Negative, and Neutral. We compile an acronym
dictionary from an on-line resource.
2
The dictio-
nary has translations for 5,184 acronyms. For ex-
ample, lol is translated to laughing out loud.
4.2 Prior Polarity Scoring
A number of our features are based on prior po-
larity of words. As in the phrase-based system we
too build off of prior work (Agarwal et al., 2009)
by using the DAL and augmenting it with Word-
net. However, we do not follow the earlier method
1
http://en.wikipedia.org/wiki/List of emoticons
2
http://www.noslang.com/
but use it as motivation. We consider words with
with a polarity score (using the pleasantness met-
ric from the DAL) of less than 0.5 as negative,
higher than 0.8 as positive and the rest as neutral.
If a word is not directly found in the dictionary, we
retrieve all synonyms from Wordnet. We then look
for each of the synonyms in the DAL. If any syn-
onym is found in the DAL, we assign the original
word the same pleasantness score as its synonym.
If none of the synonyms is present in the DAL, the
word is not associated with any prior polarity. For
the given data we directly found the prior polar-
ity of 50.1% of the words. We find the polarity of
another 8.7% of the words by using WordNet. So
we find prior polarity of about 58.7% of English
language words.
4.3 Features
We propose a set of 50 features. We calculate these
features for the whole tweet and for the last one-
third of the tweet. In total, we get 100 additional
features. Our features may be divided into three
broad categories: ones that are primarily counts
of various features and therefore the value of the
feature is a natural number ? N. Second, we in-
clude features whose value is a real number ? R.
These are primarily features that capture the score
retrieved from DAL. The third category is features
whose values are boolean ? B. These are bag of
words, presence of exclamation marks and capital-
ized text. Each of these broad categories is divided
into two subcategories: Polar features and Non-
polar features. We refer to a feature as polar if we
calculate its prior polarity either by looking it up in
DAL (extended through WordNet) or in the emoti-
con dictionary. All other features which are not
associated with any prior polarity fall in the Non-
polar category. Each of the Polar and Non-polar
features is further subdivided into two categories:
POS and Other. POS refers to features that cap-
ture statistics about parts-of-speech of words and
Other refers to all other types of features.
A more detailed explanation of the system can
be found in Agarwal et al (2011).
4.4 Combined System
Our analysis showed that the first system performs
better on recall while the second system performs
better on precision. We also found that there were
785 tweets in the development set where one sys-
tem got it correct and the other one got it incorrect.
This leaves room for a significant improvement
200
Experiment Twitter SMS LiveJournal
Dev Test Sarcasm
Majority 29.19 34.64 27.73 19.03 27.21
Phrase-Based System 62.09 64.74 40.75 56.86 62.22
Tweet-Level System 62.4 63.73 42.41 60.54 69.44
Combined System 64.6 65.42 40.02 59.84 68.79
Table 2: A comparison between the different systems using the Twitter training corpus provided by the
SemEval task for Subtask B. All results exceed the majority baseline of the positive class significantly.
compared to using each system independently. We
combined the two systems for the evaluation by
using the confidence provided by the phrase-based
system. If the phrase-based system was < 70%
confident we use the message-level system.
4.5 Experiments and Results
This task was evaluated on the Twitter dataset pro-
vided by Semeval-2013 Task 2, Subtask B. All re-
sults are shown using the average F-measure of the
positive and negative class. The full results in the
participation of SemEval 2014: Sentiment Anal-
ysis in Twitter, Subtask B, are shown in Table 2.
All the results outperform the majority baseline of
the more prominent positive polarity class signifi-
cantly. The combined system outperforms the in-
dividual systems for the Twitter development and
test set. It does not outperform the sarcasm test set,
but this may be due to the small size; it contains
only 100 tweets. The Tweet-Level system outper-
forms the phrase-based and combined system for
the LiveJournal and SMS test sets. A closer look at
the results indicated that the phrase-based system
has particular difficulty with the short sentences
which are more common in SMS and LiveJour-
nal. For example, the average number of charac-
ters in a tweet is 120 whereas it is 95.6 in SMS
messages (Nakov et al., 2013). Short sentences
are harder because there are fewer polarity words
which causes the phrase-based system to incor-
rectly pick neutral. In addition, short sentences are
harder because the BOW feature space, which is
huge and already sparse, becomes sparser and in-
dividual features start to over-fit. Part of this prob-
lem is handled by using Senti-features so the space
will be less sparse.
Our ranking in the Twitter 2013 and SMS 2013
development data is 18/50 and 20/50 respectively.
Our rank in the Twitter 2014 test set is 15/50 and
our rank in the LiveJournal test set is 19/50. Based
on our rankings it is clear that our systems are
geared more towards Twitter than other social me-
dia. Finally our ranking in the Sarcasm test set is
41/50. Although this ranking is quite low, it is in
fact encouraging. It indicates that the sarcasm has
switched the polarity of the tweet. In the future we
would like to include a system (e.g. (Gonz?alez-
Ib?a?nez et al., 2011)) that can detect whether the
tweet is sarcastic.
5 Discussion and Future Work
We participated in Semeval-2014 Task 9: Senti-
ment Analysis in Twitter Subtasks A and B. In
Subtask A, we show that adding additional fea-
tures related to location and using SentiWord-
Net gives us improvement compared to our prior
system. In Subtask B, we show that combining
two systems achieves slight improvements over
using either system alone. Combining the two
system achieves greater coverage as the systems
use different emoticon and acronym dictionar-
ies and the phrase-based system uses Wiktionary.
The message-level system is geared toward entire
tweets whereas the phrase-based is geared toward
phrases (even though, in this case we consider the
entire tweet to be a ?phrase?). This is reflective in
several features, such as the position of the target
phrase and the syntactic chunk scores in the phrase
based system and the features related to the last
third of the tweet in the message-level system. In
the future, we?d like to perform an error analysis to
determine the source of our errors and specific ex-
amples of the kind of differences found in the two
systems. Finally, we have found that at times the
scores of the DAL do not line up with polarity in
social media. Therefore, we would like to explore
including more sentiment dictionaries instead of,
or in addition to, the DAL.
6 Acknowledgements
This research was funded by the DARPA DEFT
Program. All statements of fact, opinion or con-
clusions contained herein are those of the authors
and should not be construed as representing the
official views, policies, or positions of the Depart-
ment of Defense, or the U.S. Government.
201
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen R. Mcke-
own. 2009. Contextual phrase-level polarity anal-
ysis using lexical affect scoring and syntactic n-
grams. In Proceedings of the 12th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, EACL ?09, pages 24?32, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
analysis of twitter data. In Proceedings of the Work-
shop on Language in Social Media (LSM 2011),
pages 30?38, Portland, Oregon, June. Association
for Computational Linguistics.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan
Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC?10), Valletta, Malta, may. European
Language Resources Association (ELRA).
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on twitter from biased and noisy
data. In COLING (Posters), pages 36?44.
Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: is brevity an advan-
tage? In Jimmy Huang, Nick Koudas, Gareth J. F.
Jones, Xindong Wu, Kevyn Collins-Thompson, and
Aijun An, editors, CIKM, pages 1833?1836. ACM.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, Cam-
bridge, MA ; London, May.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing, pages 1?6.
Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in
twitter: A closer look. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies:
Short Papers - Volume 2, HLT ?11, pages 581?586,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan
Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC?10), Valletta, Malta, may. European
Language Resources Association (ELRA).
Sara Rosenthal and Kathleen McKeown. 2013.
Columbia nlp: Sentiment detection of subjective
phrases in social media. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 478?482, Atlanta, Georgia, USA, June. Asso-
ciation for Computational Linguistics.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. Semeval-2014 task 9: Sen-
timent analysis in twitter. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval 2014), Dublin, Ireland, August. The COL-
ING 2014 Organizing Committee.
C. M. Whissel. 1989. The dictionary of affect in lan-
guage. In R. Plutchik and H. Kellerman, editors,
Emotion: theory research and experience, volume 4,
London. Acad. Press.
202
