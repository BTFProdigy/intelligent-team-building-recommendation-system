Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 1?8,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Statistically-Driven Alignment-Based Multiword Expression
Identification for Technical Domains
Helena de Medeiros Caseli?, Aline Villavicencio??, Andre? Machado?, Maria Jose? Finatto?
?Department of Computer Science, Federal University of Sa?o Carlos (Brazil)
?Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
?Department of Computer Sciences, Bath University (UK)
?Institute of Language and Linguistics, Federal University of Rio Grande do Sul (Brazil)
helenacaseli@dc.ufscar.br, avillavicencio@inf.ufrgs.br,
ammachado@inf.ufrgs.br, mfinatto@terra.com.br
Abstract
Multiword Expressions (MWEs) are one
of the stumbling blocks for more precise
Natural Language Processing (NLP) sys-
tems. Particularly, the lack of coverage
of MWEs in resources can impact nega-
tively on the performance of tasks and ap-
plications, and can lead to loss of informa-
tion or communication errors. This is es-
pecially problematic in technical domains,
where a significant portion of the vocab-
ulary is composed of MWEs. This pa-
per investigates the use of a statistically-
driven alignment-based approach to the
identification of MWEs in technical cor-
pora. We look at the use of several sources
of data, including parallel corpora, using
English and Portuguese data from a corpus
of Pediatrics, and examining how a sec-
ond language can provide relevant cues for
this tasks. We report results obtained by
a combination of statistical measures and
linguistic information, and compare these
to the reported in the literature. Such an
approach to the (semi-)automatic identifi-
cation of MWEs can considerably speed
up lexicographic work, providing a more
targeted list of MWE candidates.
1 Introduction
A multiword expression (MWE) can be defined
as any word combination for which the syntac-
tic or semantic properties of the whole expres-
sion cannot be obtained from its parts (Sag et
al., 2002). Examples of MWEs are phrasal verbs
(break down, rely on), compounds (police car, cof-
fee machine), idioms (rock the boat, let the cat
out of the bag). They are very numerous in lan-
guages, as Biber et al (1999) note, accouting for
between 30% and 45% of spoken English and 21%
of academic prose, and for Jackendoff (1997) the
number of MWEs in a speaker?s lexicon is of the
same order of magnitude as the number of single
words. However, these estimates are likely to be
underestimates if we consider that for language
from a specific domain the specialized vocabulary
is going to consist largely of MWEs (global warm-
ing, protein sequencing) and new MWEs are con-
stantly appearing (weapons of mass destruction,
axis of evil).
Multiword expressions play an important role
in Natural Language Processing (NLP) applica-
tions, which should not only identify the MWEs
but also be able to deal with them when they are
found (Fazly and Stevenson, 2007). Failing to
identify MWEs may cause serious problems for
many NLP tasks, especially those envolving some
kind of semantic processing. For parsing, for in-
stance, Baldwin et al (2004), found that for a ran-
dom sample of 20,000 strings from the British Na-
tional Corpus (BNC) even with a broad-coverage
grammar for English (Flickinger, 2000) missing
MWEs accounted for 8% of total parsing errors.
Therefore, there is an enormous need for robust
(semi-)automated ways of acquiring lexical infor-
mation for MWEs (Villavicencio et al, 2007) that
can significantly extend the coverage of resources.
For example, one can more than double the num-
ber of verb-particle constructions (VPCs) entries
in a dictionary, such as the Alvey Natural Lan-
guage Tools (Carroll and Grover, 1989), just ex-
tracting VPCs from a corpus like the BNC (Bald-
win, 2005). Furthermore, as MWEs are language
dependent and culturally motivated, identifying
the adequate translation of MWE occurrences is an
important challenge for machine translation meth-
ods.
In this paper, we investigate experimentally the
use of an alignment-based approach for the iden-
tification of MWEs in technical corpora. We look
at the use of several sources of data, including par-
1
allel corpora, using English and Portuguese data
from a corpus of Pediatrics, and examining how
a second language can provide relevant cues for
this tasks. In this way, cost-effective tools for the
automatic alignment of texts can generate a list
of MWE candidates with their appropriate trans-
lations. Such an approach to the (semi-)automatic
identification of MWEs can considerably speed up
lexicographic work, providing a more targeted list
of MWE candidates and their translations, for the
construction of bilingual resources, and/or with
some semantic information for monolingual re-
sources.
The remainder of this paper is structured as fol-
lows. Section 2 briefly discusses MWEs and some
previous works on methods for automatically ex-
tracting them. Section 3 presents the resources
used while section 4 describes the methods pro-
posed to extract MWEs as a statistically-driven by-
product of an automatic word alignment process.
Section 5 presents the evaluation methodology and
analyses the results and section 6 finishes this pa-
per with some conclusions and proposals for fu-
ture work.
2 Related Work
The term Multiword Expression has been used to
describe a large number of distinct but related phe-
nomena, such as phrasal verbs (e.g. come along),
nominal compounds (e.g. frying pan), institution-
alised phrases (e.g. bread and butter), and many
others (Sag et al, 2002). They are very frequent in
everyday language and this is reflected in several
existing grammars and lexical resources, where
almost half of the entries are Multiword Expres-
sions.
However, due to their heterogeneous charac-
teristics, MWEs present a tough challenge for
both linguistic and computational work (Sag et
al., 2002). Some MWEs are fixed, and do not
present internal variation, such as ad hoc, while
others allow different degrees of internal vari-
ability and modification, such as touch a nerve
(touch/find a nerve) and spill beans (spill sev-
eral/musical/mountains of beans). In terms of se-
mantics, some MWEs are more opaque in their
meaning (e.g. to kick the bucket as to die), while
others have more transparent meanings that can be
inferred from the words in the MWE (e.g. eat up,
where the particle up adds a completive sense to
eat). Therefore, providing appropriate methods
for the automatic identification and treatment of
these phenomena is a real challenge for NLP sys-
tems.
A variety of approaches has been proposed for
automatically identifying MWEs, differing basi-
cally in terms of the type of MWE and lan-
guage to which they apply, and the sources of
information they use. Although some work on
MWEs is type independent (e.g. (Zhang et al,
2006; Villavicencio et al, 2007)), given the het-
erogeneity of MWEs much of the work looks in-
stead at specific types of MWE like collocations
(Pearce, 2002), compounds (Keller and Lapata,
2003) and VPCs (Baldwin, 2005; Villavicencio,
2005; Carlos Ramisch and Aline Villavicencio and
Leonardo Moura and Marco Idiart, 2008). Some
of these works concentrate on particular languages
(e.g. (Pearce, 2002; Baldwin, 2005) for English
and (Piao et al, 2006) for Chinese), but some
work has also benefitted from asymmetries in lan-
guages, using information from one language to
help deal with MWEs in the other (e.g. (na Vil-
lada Moiro?n and Tiedemann, 2006; Caseli et al,
2009)).
As basis for helping to determine whether a
given sequence of words is in fact an MWE (e.g.
ad hoc vs the small boy) some of these works em-
ploy linguistic knowledge for the task (Villavicen-
cio, 2005), while others employ statistical meth-
ods (Pearce, 2002; Evert and Krenn, 2005; Zhang
et al, 2006; Villavicencio et al, 2007) or combine
them with some kinds of linguistic information
such as syntactic and semantic properties (Bald-
win and Villavicencio, 2002; Van de Cruys and na
Villada Moiro?n, 2007) or automatic word align-
ment (na Villada Moiro?n and Tiedemann, 2006).
Statistical measures of association have been
commonly used for this task, as they can be demo-
cratically applied to any language and MWE type.
However, there is no consensus about which mea-
sure is best suited for identifying MWEs in gen-
eral. Villavicencio et al (2007) compared some of
these measures (mutual information, permutation
entropy and ?2) for the type-independent detec-
tion of MWEs and found that Mutual Information
seemed to differentiate MWEs from non-MWEs,
but the same was not true of ?2. In addition, Ev-
ert and Krenn (2005) found that for MWE iden-
tification the efficacy of a given measure depends
on factors like the type of MWEs being targeted
for identification, the domain and size of the cor-
2
pora used, and the amount of low-frequency data
excluded by adopting a threshold. Nonetheless,
Villavicencio et al (2007), discussing the influ-
ence of the corpus size and nature over the meth-
ods, found that these different measures have a
high level of agreement about MWEs, whether
in carefully constructed corpora or in more het-
erogeneous web-based ones. They also discuss
the results obtained from adopting approaches like
these for extending the coverage of resources, ar-
guing that grammar coverage can be significantly
increased if MWEs are properly identified and
treated (Villavicencio et al, 2007).
Among the methods that use additional infor-
mation along with statistics to extract MWE, the
one proposed by na Villada Moiro?n and Tiede-
mann (2006) seems to be the most similar to our
approach. The main difference between them is
the way in which word alignment is used in the
MWE extraction process. In this paper, the word
alignment is the basis for the MWE extraction
process while Villada Moiro?n and Tiedemann?s
method uses the alignment just for ranking the
MWE candidates which were extracted on the ba-
sis of association measures (log-likelihood and
salience) and head dependence heuristic (in parsed
data).
Our approach, as described in details by Caseli
et al (2009), also follows to some extent that
of Zhang et al (2006), as missing lexical en-
tries for MWEs and related constructions are de-
tected via error mining methods, and this paper fo-
cuses on the extraction of generic MWEs as a by-
product of an automatic word alignment. Another
related work is the automatic detection of non-
compositional compounds (NCC) by Melamed
(1997) in which NCCs are identified by analyz-
ing statistical translation models trained in a huge
corpus by a time-demanding process.
Given this context, our approach proposes
the use of alignment techniques for identifying
MWEs, looking at sequences detected by the
aligner as containing more than one word, which
form the MWE candidates. As a result, sequences
of two or more consecutive source words are
treated as MWE candidates regardless of whether
they are translated as one or more target words.
3 The Corpus and Reference Lists
The Corpus of Pediatrics used in these experi-
ments contains 283 texts in Portuguese with a total
of 785,448 words, extracted from the Jornal de Pe-
diatria. From this corpus, the Pediatrics Glossary,
a reference list containing multiword terms and re-
curring expressions, was semi-automatically con-
structed, and manually checked.1 The primary aim
of the Pediatrics Glossary, as an online resource
for long-distance education, was to train, qualify
and support translation students on the domain of
pediatrics texts.
The Pediatrics Glossary was built from the
36,741 ngrams that occurred at least 5 times in the
corpus. These were automatically cleaned or re-
moved using some POS tag patterns (e.g. remov-
ing prepositions from terms that began or ended
with them). In addition, if an ngram was part of a
larger ngram, only the latter appeared in the Glos-
sary, as is the case of aleitamento materno (mater-
nal breastfeeding) which is excluded as it is con-
tained in aleitamento materno exclusivo (exclusive
maternal breastfeeding). This post-processing re-
sulted in 3,645 ngrams, which were manually
checked by translation students, and resulted in
2,407 terms, with 1,421 bigrams, 730 trigrams and
339 ngrams with n larger than 3 (not considered in
the experiments presented in this paper).
4 Statistically-Driven and
Alignment-Based methods
4.1 Statistically-Driven method
Statistical measures of association have been
widely employed in the identification of MWEs.
The idea behind their use is that they are an in-
expensive language and type independent means
of detecting recurrent patterns. As Firth famously
said a word is characterized by the company it
keeps and since we expect the component words
of an MWE to occur frequently together, then
these measures can give an indication of MWE-
ness. In this way, if a group of words co-occurs
with significantly high frequency when compared
to the frequencies of the individual words, then
they may form an MWE. Indeed, measures such
as Pointwise Mutual Information (PMI), Mutual
Information (MI), ?2, log-likelihood (Press et al,
1992) and others have been employed for this
task, and some of them seem to provide more ac-
curate predictions of MWEness than others. In
fact, in a comparison of some measures for the
type-independent detection of MWEs, MI seemed
1Available in the TEXTQUIM/UFRGS website: http:
//www.ufrgs.br/textquim
3
to differentiate MWEs from non-MWEs, but the
same was not true of ?2 (Villavicencio et al,
2007). In this work we use two commonly em-
ployed measures for this task: PMI and MI,
as implemented in the Ngram Statistics Package
(Banerjee and Pedersen, 2003).
From the Portuguese portion of the Corpus of
Pediatrics, 196,105 bigram and 362,663 trigram
MWE candidates were generated, after filtering
ngrams containing punctuation and numbers. In
order to evaluate how these methods perform with-
out any linguistic filtering, the only threshold em-
ployed was a frequency cut-off of 2 occurrences,
resulting in 64,839 bigrams and 54,548 trigrams.
Each of the four measures were then calculated for
these ngrams, and we ranked each n-gram accord-
ing to each of these measures. The average of all
the rankings is used as the combined measure of
the MWE candidates.
4.2 Alignment-Based method
The second of the MWE extraction approaches
to be investigated in this paper is the alignment-
based method. The automatic word alignment of
two parallel texts ? a text written in one (source)
language and its translation to another (target) lan-
guage ? is the process of searching for correspon-
dences between source and target words and se-
quences of words. For each word in a source sen-
tence equivalences in the parallel target sentence
are looked for. Therefore, taking into account a
word alignment between a source word sequence
S (S = s1 . . . sn with n ? 2) and a target word
sequence T (T = t1 . . . tm with m ? 1), that
is S ? T , the alignmet-based MWE extracion
method assumes that: (a) S and T share some se-
mantic features, and (b) S may be a MWE.
In other words, the alignment-based MWE ex-
traction method states that the sequence S will be
a MWE candidate if it is aligned with a sequence
T composed of one or more words (a n : m align-
ment with n ? 2 and m ? 1). For example,
the sequence of two Portuguese words aleitamento
materno ? which occurs 202 times in the cor-
pus used in our experiments ? is a MWE can-
didate because these two words were joined to be
aligned 184 times with the word breastfeeding (a
2 : 1 alignment), 8 times with the word breast-
fed (a 2 : 1 alignment), 2 times with breastfeeding
practice (a 2 : 2 alignment) and so on.
Thus, notice that the alignment-based MWE ex-
traction method does not rely on the conceptual
asymmetries between languages since it does not
expect that a source sequence of words be aligned
with a single target word. The method looks
for the sequences of source words that are fre-
quently joined together during the alignment de-
spite the number of target words involved. These
features indicate that the method priorizes preci-
sion in spite of recall.
It is also important to say that although the se-
quences of source and target words resemble the
phrases used in the phrase-based statistical ma-
chine translation (SMT), they are indeed a re-
finement of them. More specifically, although
both approaches rely on word alignments per-
formed by GIZA++2 (Och and Ney, 2000), in
the alignment-based approach not all sequences of
words are considered as phrases (and MWE can-
didates) but just those with an alignment n : m
(n >= 2) with a target sequence. To confirm
this assumption a phrase-based SMT system was
trained with the same corpus used in our exper-
iments and the number of phrases extracted fol-
lowing both approaches were compared. While
the SMT extracted 819,208 source phrases, our
alignment-based approach (without applying any
part-of-speech or frequency filter) extracted only
34,277. These results show that the alignment-
based approach refines in some way the phrases
of SMT systems.
In this paper, we investigate experimentally
whether MWEs can be identified as a by-product
of the automatic word alignment of parallel texts.
We focus on Portuguese MWEs from the Corpus
of Pediatrics and the evaluation is performed us-
ing the bigrams and trigrams from the Pediatrics
Glossary as gold standard.
To perform the extraction of MWE candi-
dates following the alignment-based approach,
first, the original corpus had to be sentence and
word aligned and Part-of-Speech (POS) tagged.
For these preprocessing steps were used, re-
spectively: a version of the Translation Cor-
pus Aligner (TCA) (Hofland, 1996), the statisti-
cal word aligner GIZA++ (Och and Ney, 2000)
and the morphological analysers and POS taggers
from Apertium3 (Armentano-Oller et al, 2006).
2GIZA++ is a well-known statistical word aligner that can
be found at: http://www.fjoch.com/GIZA++.html
3Apertium is an open-source machine translation en-
gine and toolbox available at: http://www.apertium.
org.
4
From the preprocessed corpus, the MWE candi-
dates are extracted as those in which two or more
words have the same alignment, that is, they are
linked to the same target unit. This initial list of
MWE candidates is, then, filtered to remove those
candidates that: (a) match some sequences of POS
tags or words (patterns) defined in previous exper-
iments (Caseli et al, 2009) or (b) whose frequency
is below a certain threshold. The remaining units
in the candidate list are considered to be MWEs.
Several filtering patterns and minimum fre-
quency thresholds were tested and three of them
are presented in details here. The first one (F1)
is the same used during the manual building of
the reference lists of MWEs: (a) patterns begin-
ning with Article + Noun and beginning or finish-
ing with verbs and (b) with a minimum frequency
threshold of 5.
The second one (F2) is the same used in the
(Caseli et al, 2009), mainly: (a) patterns begin-
ning with determiner, auxiliary verb, pronoun, ad-
verb, conjunction and surface forms such as those
of the verb to be (are, is, was, were), relatives
(that, what, when, which, who, why) and prepo-
sitions (from, to, of ) and (b) with a minimum fre-
quency threshold of 2.
And the third one (F3) is the same as (Caseli et
al., 2009) plus: (a) patterns beginning or finishing
with determiner, adverb, conjunction, preposition,
verb, pronoun and numeral and (b) with a mini-
mum frequency threshold of 2.
5 Experiments and Results
Table 1 shows the top 5 and the bottom 5 ranked
candidates returned by PMI and the alignment-
based approach. Although some of the results are
good, especially the top candidates, there is still
considerable noise among the candidates, as for
instance jogar video game (lit. play video game).
From table 1 it is also possible to notice that the
alignment-based approach indeed extracts Pedi-
atrics terms such as aleitamento materno (breast-
feeding) and also other possible MWE that are not
Pediatrics terms such as estados unidos (United
States).
In table 2 we show the precision (number of
correct candidates among the proposed ones), re-
call (number of correct candidates among those in
reference lists) and F-measure ((2 ? precision ?
recall)/(precision + recall)) figures for the as-
sociation measures using all the candidates (on the
PMI alignment-based
Online Mendelian Inheritance faixa eta?ria
Beta Technology Incorporated aleitamento materno
Lange Beta Technology estados unidos
Oxido Nitrico Inalatorio hipertensa?o arterial
jogar video game leite materno
... ...
e um de couro cabeludo
e a do bloqueio lact??feros
se que de emocional anatomia
e a da neonato a termo
e de nao duplas ma?es bebe?s
Table 1: Top 5 and Bottom 5 MWE candidates
ranked by PMI and alignment-based approach
pt MWE candidates PMI MI
# proposed bigrams 64,839 64,839
# correct MWEs 1403 1403
precision 2.16% 2.16%
recall 98.73% 98.73%
F 4.23% 4.23%
# proposed trigrams 54,548 54,548
# correct MWEs 701 701
precision 1.29% 1.29%
recall 96.03% 96.03%
F 2.55% 2.55%
# proposed bigrams 1,421 1,421
# correct MWEs 155 261
precision 10.91% 18.37%
recall 10.91% 18.37%
F 10.91% 18.37%
# proposed trigrams 730 730
# correct MWEs 44 20
precision 6.03% 2.74%
recall 6.03% 2.74%
F 6.03% 2.74%
Table 2: Evaluation of MWE candidates - PMI and
MI
first half of the table) and using the top 1,421 bi-
gram and 730 trigram candidates (on the second
half). From these latter results, we can see that the
top candidates produced by these measures do not
agree with the Pediatrics Glossary, since there are
only at most 18.37% bigram and 6.03% trigram
MWEs among the top candidates, as ranked by
MI and PMI respectively. Interestingly, MI had a
better performance for bigrams while for trigrams
PMI performed better.
On the other hand, looking at the alignment-
based method, 34,277 pt MWE candidates were
extracted and Table 3 sumarizes the number of
candidates filtered following the three filters de-
scribed in 4.2: F1, F2 and F3.
To evaluate the efficacy of the alignment-based
method in identifying multiword terms of Pedi-
atrics, an automatic comparison was performed
using the Pediatrics Glossary. In this auto-
5
pt MWE candidates F1 F2 F3
# filtered by POS patterns 24,996 21,544 32,644
# filtered by frequency 9,012 11,855 1,442
# final Set 269 878 191
Table 3: Number of pt MWE candidates filtered
in the alignment-based approach
pt MWE candidates F1 F2 F3
# proposed bigrams 250 754 169
# correct MWEs 48 95 65
precision 19.20% 12.60% 38.46%
recall 3.38% 6.69% 4.57%
F 5.75% 8.74% 8.18%
# proposed trigrams 19 110 20
# correct MWEs 1 9 4
precision 5.26% 8.18% 20.00%
recall 0.14% 1.23% 0.55%
F 0.27% 2.14% 1.07%
# proposed bi/trigrams 269 864 189
# correct MWEs 49 104 69
precision 18.22% 12.04% 36,51%
recall 2.28% 4.83% 3.21%
F 4.05% 6.90% 5.90%
Table 4: Evaluation of MWE candidates
matic comparision we considered the final lists of
MWEs candidates generated by each filter in table
3. The number of matching entries and the values
for precision, recall and F-measure are showed in
table 4.
The different values of extracted MWEs (in ta-
ble 3) and evaluated ones (in table 4) are due to
the restriction of considering only bigrams and tri-
grams in the Pediatrics Glossary. Then, longer
MWEs ? such as doenc?a arterial coronariana
prematura (premature coronary artery disease)
and pequenos para idade gestacional (small for
gestational age) ? extracted by the alignment-
based method are not being considered at the mo-
ment.
After the automatic comparison using the Pedi-
atrics Glossary, an analysis by human experts was
performed on one of the derived lists ? that with
the best precision values so far (from filter F3).
The human analysis was necessary since, as stated
in (Caseli et al, 2009), the coverage of reference
lists may be low, and it is likely that a lot of MWE
candidates that were not found in the Pediatrics
Glossary are nonetheless true MWEs. In this pa-
per only the pt MWE candidates extracted using
filter F3 (as described in section 4.2) were manu-
ally evaluated.
From the 191 pt MWE candidates extracted af-
ter F3, 69 candidates (36.1% of the total amount)
were found in the bigrams or trigrams in the
Glossary (see table 4). Then, the remaining 122
candidates (63.9%) were analysed by two native-
speakers human judges, who classified each of the
122 candidates as true, if it is a multiword expres-
sion, or false, otherwise independently of being
a Pediatrics term. For the judges, a sequence of
words was considered a MWE mainly if it was:
(1) a proper name or (2) a sequence of words for
which the meaning cannot be obtained by com-
pounding the meanings of its component words.
The judgments of both judges were compared
and a disagreement of approximately 12% on mul-
tiwords was verified. This disagreement was also
measured by the kappa (K) measure (Carletta,
1996), with k = 0.73, which does not prevent
conclusions to be drawn. According to Carletta
(1996), among other authors, a value of k between
0.67 and 0.8 indicates a good agreement.
In order to calculate the percentage of true can-
didates among the 122, two approaches can be
followed, depending on what criteria one wants
to emphasize: precision or coverage (not recall
because we are not calculating regarding a refer-
ence list). To emphasize the precision, one should
consider as genuine MWEs only those candidates
classified as true by both judges, on the other hand,
to emphasize the coverage, one should consider
also those candidates classified as true by just one
of them. So, from 191 MWE candidates, 126
(65.97%) were classified as true by both judges
and 145 (75.92%) by at least one of them.
6 Conclusions and Future Work
MWEs are a complex and heterogeneous set of
phenomena that defy attempts to capture them
fully, but due to their role in communication they
need to be properly accounted for in NLP applica-
tions and tasks.
In this paper we investigated the identifica-
tion of MWEs from technical domain, test-
ing statistically-driven and alignment-based ap-
proaches for identifying MWEs from a Pediatrics
parallel corpus. The alignment-based method gen-
erates a targeted precision-oriented list of MWE
candidates, while the statistical methods produce
recall-oriented results at the expense of precision.
Therefore, the combination of these methods can
produce a set of MWE candidates that is both more
precise than the latter and has more coverage than
the former. This can significantly speed up lex-
icographic work. Moreover, the results obtained
6
show that in comparison with the manual extrac-
tion of MWEs, this approach can provide also a
general set of MWE candidates in addition to the
manually selected technical terms.
Using the alignment-based extraction method
we notice that it is possible to extract MWEs that
are Pediatrics terms with a precision of 38% for
bigrams and 20% for trigrams, but with very low
recall since only the MWEs in the Pediatrics Glos-
sary were considered correct. However, after a
manual analysis carried out by two native speakers
of Portuguese we found that the percentage of true
MWEs considered by both or at least one of them
were, respectively, 65.97% and 75.92%. This was
a significative improvement but it is important to
say that, in this manual analysis, the human ex-
perts classified the MWEs as true independently of
them being Pediatrics terms. So, as future work we
intend to carry out a more carefull analysis with
experts in Pediatrics to evaluate how many MWEs
candidates are also Pediatrics terms.
In addition, we plan to investigate a weighted
combination of these methods, favouring those
that have better precision. Finally, we also in-
tend to apply the results obtained in to the semi-
automatic construction of ontologies.
Acknowledgments
We would like to thank the TEXTQUIM/UFRGS
group for making the Corpus of Pediatrics and Pe-
diatrics Glossary available to us. We also thank the
financial support of FAPESP in bulding the paral-
lel corpus. This research has been partly funded
by the FINEP project COMUNICA.
References
Carme Armentano-Oller, Rafael C. Carrasco, Anto-
nio M. Corb??-Bellot, Mikel L. Forcada, Mireia
Ginest??-Rosell, Sergio Ortiz-Rojas, Juan Anto-
nio Pe?rez-Ortiz, Gema Ram??rez-Sa?nchez, Felipe
Sa?nchez-Mart??nez, and Miriam A. Scalco. 2006.
Open-source Portuguese-Spanish machine transla-
tion. In R. Vieira, P. Quaresma, M.G.V. Nunes, N.J.
Mamede, C. Oliveira, and M.C. Dias, editors, Pro-
ceedings of the 7th International Workshop on Com-
putational Processing of Written and Spoken Por-
tuguese, (PROPOR 2006), volume 3960 of Lecture
Notes in Computer Science, pages 50?59. Springer-
Verlag, May.
Timothy Baldwin and Aline Villavicencio. 2002. Ex-
tracting the Unextractable: A Case Study on Verb-
particles. In Proceedings of the 6th Conference on
Natural Language Learning (CoNLL-2002), pages
98?104, Taipei, Taiwan.
Timothy Baldwin, Emily M. Bender, Dan Flickinger,
Ara Kim, and Stephan Oepen. 2004. Road-testing
the English Resource Grammar over the British Na-
tional Corpus. In Proceedings of the Fourth In-
ternational Conference on Language Resources and
Evaluation (LREC 2004), pages 2047?2050, Lisbon,
Portugal.
Timothy Baldwin. 2005. The deep lexical acquisition
of English verb-particles. Computer Speech and
Language, Special Issue on Multiword Expressions,
19(4):398?414.
Satanjeev Banerjee and Ted Pedersen. 2003. The De-
sign, Implementation and Use of the Ngram Statis-
tics Package. In In Proceedings of the Fourth Inter-
national Conference on Intelligent Text Processing
and Computational Linguistics, pages 370?381.
Douglas Biber, Stig Johansson, Geoffrey Leech, Susan
Conrad, and Edward Finegan. 1999. Grammar of
Spoken and Written English. Longman, Harlow.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: the kappa statistics. Computational
Linguistics, 22(2):249?254.
Carlos Ramisch and Aline Villavicencio and Leonardo
Moura and Marco Idiart. 2008. Picking them
up and Figuring them out: Verb-Particle Construc-
tions, Noise and Idiomaticity. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL 2008), pages 49?56.
John Carroll and Claire Grover. 1989. The derivation
of a large computational lexicon of English from
LDOCE. In Bran Boguraev and Ted Briscoe, editors,
Computational Lexicography for Natural Language
Processing, pages 117?134. Longman, Harlow, UK.
Helena M. Caseli, Carlos Ramisch, Maria G. V. Nunes,
and Aline Villavicencio. 2009. Alignment-based
extraction of multiword expressions. Language Re-
sources and Evaluation, to appear.
Stefan Evert and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of statis-
tical association measures. Computer Speech and
Language, 19(4):450?466.
Afsaneh Fazly and Suzanne Stevenson. 2007. Distin-
guishing Subtypes of Multiword Expressions Using
Linguistically-Motivated Statistical Measures. In
Proceedings of the Workshop on A Broader Perspec-
tive on Multiword Expressions, pages 9?16, Prague,
June.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28.
7
Knut Hofland. 1996. A program for aligning English
and Norwegian sentences. In S. Hockey, N. Ide,
and G. Perissinotto, editors, Research in Humanities
Computing, pages 165?178, Oxford. Oxford Univer-
sity Press.
Ray Jackendoff. 1997. Twistin? the night away. Lan-
guage, 73:534?59.
Frank Keller and Mirella Lapata. 2003. Using the Web
to Obtain Frequencies for Unseen Bigrams. Compu-
tational Linguistics, 29(3):459?484.
I. Dan Melamed. 1997. Automatic Discovery of
Non-Compositional Compounds in Parallel Data. In
eprint arXiv:cmp-lg/9706027, pages 6027?+, June.
Bego na Villada Moiro?n and Jorg Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word-alignment. In Proceedings of the Workshop
on Multi-word-expressions in a Multilingual Context
(EACL-2006), pages 33?40, Trento, Italy.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the ACL, pages 440?447,
Hong Kong, China, October.
Darren Pearce. 2002. A Comparative Evaluation of
Collocation Extraction Techniques. In Proceedings
of the Third International Conference on Language
Resources and Evaluation, pages 1?7, Las Palmas,
Canary Islands, Spain.
Scott S. L. Piao, Guangfan Sun, Paul Rayson, and
Qi Yuan. 2006. Automatic Extraction of Chi-
nese Multiword Expressions with a Statistical Tool.
In Proceedings of the Workshop on Multi-word-
expressions in a Multilingual Context (EACL-2006),
pages 17?24, Trento, Italy, April.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 1992. Numerical
Recipes in C: The Art of Scientific Computing. Sec-
ond edition. Cambridge University Press.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
Expressions: A Pain in the Neck for NLP. In Pro-
ceedings of the Third International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing-2002), volume 2276 of (Lecture
Notes in Computer Science), pages 1?15, London,
UK. Springer-Verlag.
Tim Van de Cruys and Bego na Villada Moiro?n. 2007.
Semantics-based Multiword Expression Extraction.
In Proceedings of the Workshop on A Broader Pre-
spective on Multiword Expressions, pages 25?32,
Prague, June.
Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
Evaluation of Automatically Acquired Multiword
Expressions for Grammar Engineering. In Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1034?1043, Prague, June.
Aline Villavicencio. 2005. The Availability of Verb-
Particle Constructions in Lexical Resources: How
Much is Enough? Journal of Computer Speech and
Language Processing, 19(4):415?432.
Yi Zhang, Valia Kordoni, Aline Villavicencio, and
Marco Idiart. 2006. Automated Multiword Ex-
pression Prediction for Grammar Engineering. In
Proceedings of the Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties, pages 36?44, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
8
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 111?114,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
LIHLA: Shared task system description
Helena M. Caseli, Maria G. V. Nunes
NILC ? ICMC ? Univ. Sa?o Paulo
CP 668P, 13560-970 Sa?o Carlos?SP, Brazil
{helename,gracan}@icmc.usp.br
Mikel L. Forcada
Transducens ? DLSI ? Univ. d?Alacant
E-03071 Alacant, Spain
mlf@dlsi.ua.es
Abstract
In this paper we describe LIHLA, a lexical
aligner which uses bilingual probabilis-
tic lexicons generated by a freely availa-
ble set of tools (NATools) and language-
independent heuristics to find links be-
tween single words and multiword units
in sentence-aligned parallel texts. The
method has achieved an alignment error
rate of 22.72% and 44.49% on English?
Inuktitut and Romanian?English parallel
sentences, respectively.
1 Introduction
Alignment of words and multiword units plays an
important role in many natural language processing
(NLP) applications, such as example-based machine
translation (EBMT) (Somers, 1999) and statistical
machine translation (SMT) (Ayan et al, 2004; Och
and Ney, 2000), transfer rule learning (Carl, 2001;
Menezes and Richardson, 2001), bilingual lexi-
cography (Go?mez Guinovart and Sacau Fontenla,
2004), and word sense disambiguation (Gale et al,
1992), among others.
Aligning two (or more) texts means finding
correspondences (translation equivalences) between
segments (paragraphs, sentences, words, etc.) of the
source text and segments of its translation (the tar-
get text). Following the same idea of many recently
proposed approaches on lexical alignment (e.g., Wu
and Wang (2004) and Ayan et al (2004)), the
method described in this paper, LIHLA (Language-
Independent Heuristics Lexical Aligner) starts from
statistical alignments between single words (de-
fined in bilingual lexicons) and applies language-
independent heuristics to them, aiming at finding the
best alignments between words or multiword units.
Although the most frequent alignment category is
1 : 1 (in which one source word is translated exactly
as one target word), other categories such as omis-
sions (1 : 0 or 0 : 1) or those involving multiword
units (n : m, with n and/or m ? 1) are also possible.
This paper is organized as follows: section 2 ex-
plains how LIHLA works; section 3 describes some
experiments carried out with LIHLA together with
their results and, in section 4, some concluding re-
marks are presented.
2 How LIHLA works
As the first step, LIHLA uses alignments between
single words defined in two bilingual lexicons
(source?target and target?source) generated from
sentence-aligned parallel texts using NATools.1
Given two sentence-aligned corpus files, the NA-
Tools word aligner ?based on the Twenty-One sys-
tem (Hiemstra, 1998)? counts the co-occurrences
of words in all aligned sentence pairs and builds a
sparse matrix of word-to-word probabilities (Model
A) using an iterative expectation-maximization al-
gorithm (5 iterations by default). Finally, the ele-
ments with higher values in the matrix are cho-
sen to compose two probabilistic bilingual lexi-
cons (source?target and target?source) (Simo?es and
Almeida, 2003). For each word in the corpus, each
1NATools is a set of tools developed to work with parallel
corpora, which is freely available in http://natura.di.
uminho.pt/natura/natura/.
111
bilingual lexicon gives: the number of occurrences
of that word in the corpus (its absolute frequency)
and its most likely translations together with their
probabilities.
The construction of the bilingual lexicons is an
independent prior step for the alignment performed
by LIHLA and the same bilingual lexicons can be
used several times to align parallel sentences.
So, using the two bilingual lexicons generated
by NATools and some language-independent heuris-
tics, LIHLA tries to find the best alignment between
source and target tokens (words, numbers, special
characters, etc.) in a pair of parallel sentences. For
each source token sj in source sentence S, LIHLA
will look for the best token ti in the target parallel
sentence T applying these heuristics in sequence:
1. Exact match
LIHLA creates a 1 : 1 alignment between sj
and ti if they are identical. This heuristic stays
for exact matches, for instance, between proper
names and numbers.
2. Best candidate according to the bilingual
lexicon
LIHLA looks for possible translations of sj in
the source?target bilingual lexicon (BS) and
makes an intersection between them and the
words in T . In this intersection, if no candi-
date word identical to those in BS is found,
then LIHLA tries to look for cognates for
those words using the longest common subse-
quence ratio (LCSR).2 By doing this, LIHLA
can deal with small changes in possible trans-
lations such as different forms of the same verb,
changes in gender and/or number of nouns,
adjectives, and so on.
Then, LIHLA selects the best target candidate
word ti for sj ?the best candidate word accor-
ding to BS among those in a position which
is favorably situated in relation to sj? and
looks for multiword units involving sj and ti
?those words that occur immediately before
and/or after sj (for source multiword units) or
2The LCSR of two words is computed by dividing the length
of their longest common subsequence by the length of the
longer word. For example, the LCSR of Portuguese word alin-
hamento and Spanish word alineamiento is 1012 ' 0.83 as their
longest common subsequence is a-l-i-n-a-m-e-n-t-o.
ti (for target multiword units) and are not pos-
sible translations for other words in T and S,
respectively. According to the multiword units
that have (or not) been found, a 1 : 1, 1 : n,
m : 1 or m : n alignment is established. An
omission alignment for sj (1 : 0) can also be
established if no target candidate word ti that
satisfies this heuristic is available.
3. Cognates
If no possible translation for sj is found in the
bilingual lexicon and the target sentence (T ) at
the same time, LIHLA uses the LCSR to look
for cognates for sj in T and sets a 1 : 1 align-
ment between sj and its best cognate or a 1 : 0
alignment if there is no cognate available.
These heuristics are applied while alignments can
still be produced and a maximum number of itera-
tions is not reached (see section 3 for the number
of iterations performed in the experiments described
in this paper). Furthermore, at the first iteration,
all words with a frequency higher than a set thres-
hold are ignored to avoid erroneous alignments since
all subsequent alignments are based on the previous
ones.
In its last step (which is optional and has not
been performed in the experiments described in
this paper), LIHLA aligns the remaining unaligned
source and target tokens between two pairs of al-
ready aligned tokens establishing several 1 : 1 align-
ments when there are the same number of source
and target tokens, or just one alignment involving
all source and target tokens if they exist in different
quantities. The decision of creating n 1 : 1 align-
ments in spite of just one n : n alignment when there
is the same number of source and target tokens is due
to the fact that a 1 : 1 alignment is more likely to be
found than a n : n one.
3 Experiments
In this section we present the experiments carried
out with LIHLA for the ?Shared task on word align-
ment? in the Workshop on Building and Using Pa-
rallel Texts during ACL2005. Systems participa-
ting in this shared task were provided with training
data (consisting of sentence-aligned parallel texts)
for three pairs of languages: English?Inuktitut,
112
Romanian?English and English?Hindi. Further-
more, the systems would choose to participate in one
or both subtasks of ?limited resources? (where sys-
tems were allowed to use only the resources pro-
vided) and ?unlimited resources? (where systems
were allowed to use any resources in addition to
those provided). The system described in this pa-
per, LIHLA, participated in the subtask of limited re-
sources aligning English?Inuktitut and Romanian?
English test sets.
The training sets ?composed of 338,343
English?Inuktitut aligned sentences (omission cases
were excluded from the whole set of 340,526 pairs)
and 48,478 Romanian?English aligned ones? were
used to build the bilingual lexicons. Then,
without changing any default parameter (threshold
for LCSR, maximum number of iterations, etc.),
LIHLA aligned the 75 English?Inuktitut and the 203
Romanian?English parallel sentences on test sets.
The whole alignment process (bilingual lexicon ge-
neration and alignment itself) did not take more than
17 minutes for English?Inuktitut (3 iterations per
sentence, on average) and 7 minutes for Romanian?
English (4 iterations per sentence, on average).
The evaluation was run with respect to precision,
recall, F -measure, and alignment error rate (AER)
considering sure and probable alignments but not
NULL ones (Mihalcea and Pedersen, 2003). Tables
1 and 2 present metric values for English?Inuktitut
and Romanian?English alignments, respectively, as
provided by the organization of the shared task.
Metric Sure Probable
Precision 46.55% 79.53%
Recall 73.72% 18.71%
F -measure 57.07% 30.30%
AER 22.72%
Table 1: LIHLA results for English?Inuktitut
Metric Sure Probable
Precision 57.68% 57.68%
Recall 53.51% 53.51%
F -measure 55.51% 55.51%
AER 44.49%
Table 2: LIHLA results for Romanian?English
The results obtained in these experiments were
not so good as those achieved by LIHLA on the
language pairs for which it was developed, that
is, 92.48% of precision and 88.32% of recall on
Portuguese?Spanish parallel texts and 84.35% of
precision and 76.39% of recall on Portuguese?
English ones.3
The poor performance in the English?Inuktikut
task may be partly due to the fact that Inuktikut is
a polysynthetic language, that is, one in which, un-
like in English, words are formed by long strings of
concatenated morphemes. This makes it difficult for
NATools to build reasonable dictionaries and lead
to a predominance of n : 1 alignments, which are
harder to determine ?this fact can be confirmed by
the better precision of LIHLA when probable align-
ments were considered (see table 1). The perfor-
mance in the English?Romanian task, not very far
from the English?Portuguese task used to tune up
the parameters of the algorithm, is harder to explain
without further analysis.
The difference in precision and recall between
the two language pairs is due to the fact that on
the English?Inuktitut reference corpus in addition to
sure alignments the probable ones were also anno-
tated while in Romanian?English only sure align-
ments are found. This indicates that evaluating
alignment systems is not a simple task since their
performance depends not only on the language pairs
and the quality of parallel corpora (constant criteria
in this shared task) but also the way the reference
corpus is built.
So, at this moment, it would be unfair to blame
the worse performance of LIHLA on its alignment
methodology since it has been applied to the new
language pairs without changing any of its default
parameters. Maybe a simple optimization of para-
meters for each pair of languages could bring better
results and also the impact of size and quality of
training and reference corpora used in these experi-
ments should be investigated. Then, the only conclu-
sion that can be taken at this moment is that LIHLA,
with its heuristics and/or default parameters, can not
be indistinctly applied to any pair of languages.
Despite of its performance, LIHLA has some
3For more details of these experiments see (Caseli et al, ac-
cepted paper).
113
advantages when compared to other lexical align-
ment methods found in the literature, such as: it
does not need to be trained for a new pair of lan-
guages (as in Och and Ney (2000)) and neither does
it require pre-processing steps to handle texts (as
in Go?mez Guinovart and Sacau Fontenla (2004)).
Furthermore, the whole alignment process (bilingual
lexical generation and alignment itself) has proved
to be very fast as mentioned previously.
4 Concluding remarks
This paper has presented a lexical alignment
method, LIHLA, which aligns words and multi-
word units based on initial statistical word-to-word
correspondences and language-independent heuris-
tics.
In the experiments carried out at the ?Shared
task on word alignment? which took place at the
Workshop on Building and Using Parallel Texts
during ACL2005, LIHLA has been evaluated on
English?Inuktitut and Romanian?English parallel
texts achieving an AER of 22.72% and 44.49%,
respectively.
As future work, we aim at investigating the impact
of using additional linguistic information (such as
part-of-speech tags) on LIHLA?s performance. Also,
as a long-term goal, LIHLA will be part of a system
implemented to learn transfer rules from sequences
of aligned words.
Acknowledgments
We thank FAPESP, CAPES, CNPq and the
Spanish Ministry of Science & Technology (Project
TIC2003-08681-C02-01) for financial support.
References
Necip F. Ayan, Bonnie J. Dorr, and Nizar Habash. 2004.
Multi-Align: Combining linguistic and statistical tech-
niques to improve alignments for adaptable MT. In
R. E. Frederking and K. B. Taylor, editors, Proceed-
ings of the 6th Conference of the AMTA (AMTA-2004),
number 3265 in Lecture Notes in Artificial Inteligence
(LNAI), pages 17?26. Springer-Verlag Berlin Heidel-
berg.
Michael Carl. 2001. Inducing probabilistic invertible
translation grammars from aligned texts. In Pro-
ceedings of CoNLL-2001, pages 145?151, Toulouse,
France.
Helena M. Caseli, Maria das Grac?as V. Nunes, and
Mikel L. Forcada. (accepted paper). LIHLA: A
lexical aligner based on language-independent heuris-
tics. In Proceedings of the V Encontro Nacional de
Intelige?ncia Artificial (ENIA05), Sa?o Leopoldo, RS,
Brazil.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. Using bilingual materials to develop
word sense disambiguation methods. In Proceedings
of the 4th International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI
1992), pages 101?112, Montreal, Canada, June.
Xavier Go?mez Guinovart and Elena Sacau Fontenla.
2004. Me?todos de optimizacio?n de la extraccio?n de
le?xico bilingu?e a partir de corpus paralelos. Proce-
samiento del Lenguaje Natural, 33:133?140.
Djoerd Hiemstra. 1998. Multilingual domain modeling
in Twenty-One: automatic creation of a bi-directional
translation lexicon from a parallel corpus. In Pe-
ter Arno Coppen, Hans van Halteren, and Lisanne Te-
unissen, editors, Proceedings of the 8th CLIN meeting,
pages 41?58.
Arul Menezes and Stephen D. Richardson. 2001. A best-
first alignment algorithm for automatic extraction of
transfer mappings from bilingual corpora. In Proceed-
ings of the Workshop on Data-driven Machine Trans-
lation at 39th Annual Meeting of the ACL (ACL-2001),
pages 39?46, Toulouse, France.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation
exercise for word alignment. In HLT-NAACL 2003
Workshop: Building and Using Parallel Texts Data
Driven Machine Translation and Beyond, pages 1?10,
Edmonton, May?June.
Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the ACL (ACL-2000), pages 440?
447, Hong Kong, China, October.
Alberto M. Simo?es and Jose? J. Almeida. 2003. NA-
Tools ? A statistical word aligner workbench. Pro-
cessamiento del Lenguaje Natural, 31:217?224.
Harold Somers. 1999. Review article: Example-based
machine translation. Machine Translation, 14(2):113?
157.
Hua Wu and Haifeng Wang. 2004. Improving domain-
specific word alignment with a general bilingual cor-
pus. In R. E. Frederking and K. B. Taylor, editors, Pro-
ceedings of the 6th Conference of the AMTA (AMTA-
2004), number 3265 in Lecture Notes in Artificial
Inteligence (LNAI), pages 262?271. Springer-Verlag
Berlin Heidelberg.
114
Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,
pages 1?7, Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Computational Linguistics in Brazil: An Overview 
 
Thiago A. S. Pardo1, Caroline V. Gasperin1, Helena M. Caseli2,  
Maria das Gra?as V. Nunes1 
N?cleo Interinstitucional de Ling??stica Computacional (NILC) 
1 Instituto de Ci?ncias Matem?ticas e de Computa??o, Universidade de S?o Paulo 
Av. Trabalhador S?o-carlense, 400 - Centro 
P.O.Box 668. 13560-970 - S?o Carlos/SP, Brazil 
2
 Departamento de Computa??o, Universidade Federal de S?o Carlos 
Rod. Washington Lu?s, Km 235 
P.O.Box 676. 13565-905 - S?o Carlos/SP, Brazil 
{taspardo,cgasperin}@icmc.usp.br, helenacaseli@dc.ufscar.br, 
gracan@icmc.usp.br 
 
 
 
Abstract 
In this paper we give an overview of 
Computational Linguistics / Natural 
Language Processing in Brazil, describing the 
general research scenario, the main research 
groups, existing events and journals, and the 
perceived challenges, among other relevant 
information. We also identify opportunities 
for collaboration. 
1 Brazilian Research Scenario 
Computational Linguistics (CL) / Natural 
Language Processing (NLP) is an emerging and 
growing area in Brazil. Although there is no 
consensus, it is traditionally understood as a 
research field within Artificial Intelligence, 
gathering researchers mainly from Computer 
Science/Engineering and Linguistics. There is also 
modest interaction with Information Sciences area. 
In general the CL/NLP area in Brazil started 
with researchers that finished their PhD abroad 
and, after coming back, initiated the first CL/NLP 
projects. Since then, but mainly more recently, the 
area has experienced some internationalization due 
to the fact that the number of undergraduate and 
graduate students that undergo internships on 
renowned foreign NLP research centers has 
increased. In Brazil, PhD students have the 
possibility to take their complete PhD course 
abroad or, alternatively, only a part of it. In both 
cases, students may count on Brazilian funding 
agencies. 
The area is more strongly represented and 
promoted by Brazilian Computer Society (SBC)1, 
particularly by its Special Interest Group on NLP 
(CEPLN)2, created in 2007. It is interesting that 
most researchers in Brazil (independent from their 
background area) do not differentiate CL from 
NLP, using both terms interchangeably. 
Research in Brazil is carried out mainly at 
public universities and at a few private universities 
and business companies. Differently from most 
countries, in Brazil public universities are 
generally considered the top ones, although 
exceptions do exist. 
Currently, there are no undergraduate courses on 
CL/NLP in Brazil, therefore researchers in this 
field come mainly from Computer Science and 
Linguistics courses. However, there are a few 
graduate courses on CL/NLP, with both computing 
and language emphases, such as the MSc and PhD 
programs at USP/S?o Carlos3, UFSCar4, 
UNESP/Araraquara5, PUC-RS6, and UFRGS7, 
among others.  
                                                           
1
 http://www.sbc.org.br 
2
 http://www.sbc.org.br/cepln 
3
 http://www.icmc.usp.br/~posgrad/computacao.html 
4
 http://ppgcc.dc.ufscar.br and http://www.ppgl.ufscar.br 
5
 http://www.fclar.unesp.br/poslinpor 
6
 http://www.pucrs.br/inf/pos 
7
 http://www.ufrgs.br 
1
Funding for research comes mainly from 
governmental agencies. Nowadays Brazil has 4 
agencies that significantly support research in the 
country (in this order): CNPq8 (National Council 
for Scientific and Technological Development), 
FAPESP9 (S?o Paulo Research Foundation), 
CAPES10 (Coordena??o de Aperfei?oamento de 
Pessoal de N?vel Superior), and FINEP11 (Research 
and Projects Financing). Private funding is still 
modest, which reflects the limited interaction 
between universities and companies. Some of the 
above agencies have tried to change this scenario 
by providing special joint university-industry 
funding programs. For instance, FAPESP and 
Microsoft Research recently formed a partnership 
to fund socially relevant projects in the state of S?o 
Paulo, e.g., the PorSimples12 text simplification 
project. FAPESP also funds special university-
company programs, where the research to be 
developed must be of interest to a company, which, 
in turn, has to support the research and work 
together with the researchers. 
NLP research in Brazil is varied and deals not 
only with Portuguese processing, but also with 
English and Spanish mainly. Given that Portuguese 
is among the most spoken languages in the world 
(it is estimated that almost 250 million people 
speak some variant of Portuguese in the world13), 
research interests on Portuguese processing is 
shared with other countries, mainly Portugal. In 
this sense, Portugal has launched an initiative to 
create and maintain a unified information storage 
center that indexes resources and publications 
for/on Portuguese processing. The initiative is the 
Linguateca project14, which was officially created 
in 2002, but initial works date back to 1998. Santos 
(2009) presents and evaluates the work carried out 
by Linguateca. 
Brazil and Portugal have a history of partnership 
on Portuguese processing, which formally started 
in 1993 with the first PROPOR conference 
(PROPOR event series is introduced in Section 4). 
                                                           
8
 http://www.cnpq.br 
9
 http://www.fapesp.br 
10
 http://www.capes.gov.br 
11
 http://www.finep.gov.br 
12
 http://caravelas.icmc.usp.br 
13
 Besides Brazil and Portugal, Portuguese is an official 
language in Angola, Cape Verde, East Timor, Equatorial 
Guinea, Guinea-Bissau, Macau, Mozambique, and S?o Tom? 
and Pr?ncipe. 
14
 http://www.linguateca.pt 
We maintain this partnership active by having 
collaborative projects and promoting joint events.  
As far as we know, other Portuguese speaking 
countries do not have a tradition of CL/NLP 
research. However, curiously, there are researchers 
from other non-Portuguese speaking countries that 
develop relevant research on Portuguese language. 
For example, to the best of our knowledge, 
currently the best syntactical parsers for 
Portuguese were developed by researchers from 
Denmark and the USA. These researchers actively 
work with the Brazilian research community. 
In what follows, we briefly present the Brazilian 
research profile (Section 2), the main research 
groups (Section 3), and the Brazilian events and 
journals (Section 4). We also report the main 
challenges for research in Brazil (Section 5) and 
the collaboration opportunities with other 
American researchers that we envision (Section 6). 
2 Research Profile 
In 2009 CEPLN proposed a survey of the status of 
CL/NLP research in Brazil and published the 
results during the 7th Brazilian Symposium in 
Information and Human Language Technology 
(Pardo et al, 2009). The survey aimed at gathering 
information both about researchers (such as their 
location, education level, number of students, etc.) 
and their research (main research topics, number of 
funded projects, main challenges, etc.). 
The survey was carried out mainly on-line. A 
call for participation was sent to all known e-mail 
lists from scientific associations from varied areas. 
Data was also obtained from the Registry of Latin 
American Researchers in Natural Language 
Processing and Computational Linguistics15. 
148 researchers responded to the survey: 35% of 
these were academic staff with a PhD degree, 16% 
academic staff with a Master's degree, 1% 
academic staff with a Bachelors degree, 9% PhD 
students, 26% Master's students, 14% 
undergraduate students, and 5% others. Table 1 
summarizes the main results of the survey, 
showing the percentage of answers for each issue. 
One may see that CL/NLP research is mainly 
carried out in the south and southeast regions of 
Brazil. 
 
                                                           
15
 http://ww.d.umn.edu/~tpederse/registry/registry.cgi 
2
Table 1. CEPLN survey 
Issues Results 
Geographic distribution 48% S?o Paulo state 
18% Rio Grande do Sul state 
8% Paran? state 
7% Rio de Janeiro state 
19% Other states 
National collaboration 52% Yes, 48% No 
International Collaboration 25% Yes, 75% No 
Background area 62% Computer Science 
29% Linguistics 
9% Other 
Supervision of postgraduate students 28% Yes, 72% No 
Funded projects 28% Yes, 72% No 
Source of funding 43% Federal government agencies 
25% S?o Paulo state government agency 
31% Other state government agencies 
 
. 
  
 
Figure 1. Research topics 
 
The survey also inquired the participants about 
their research topics. Figure 1 shows the 
distribution of topics among researchers who 
responded to the survey. Researchers could mark 
as many research topics as they wanted. Some 
topics subsume others, so these were marked more 
often by respondents. 
Ontologies and semantics were the topics 
marked by most respondents. We believe that there 
is indeed a significant number of researchers 
working on them, but we also believe that they are 
not the main topic of research of most people who 
3
listed them. For example, the statistics for 
?Ontologies? probably also include researchers 
who simply make use of ontologies in their work 
and not necessarily develop ontologies or ontology 
generation methods. Other researchers believe that 
we are in a changing period, moving from syntax-
centered research to semantics-centered research, 
due to the fact that more recently the community 
has produced more robust semantic tools and 
resources, e.g., the first versions of Portuguese 
language wordnets, as TeP 2.016, Wordnet.PT17, 
and MWN.PT18, as well as named entities 
recognizers, e.g., REMBRANDT19. 
Interestingly, corpus linguistics is one of the 
hottest topics but, at the same time, it is not seen as 
a genuine CL/NLP topic: most researchers that 
indicated corpus linguistics as a research topic 
marked it as ?other area of interest?. Some 
researchers have advocated that CL/NLP area and 
corpus linguistics should be considered a unique 
area, while others argue that these areas have 
different purposes and, therefore, different 
scientific methods, what would avoid such 
unification. Text mining is another curious case: 
research on this theme is mostly carried out by 
non-CL/NLP researchers, but instead by 
researchers on general AI and database areas 
Based on the publications on the last Brazilian 
scientific events and on the fact that we personally 
know most of the CL/NLP researchers in Brazil, 
we dare to indicate the following topics as the most 
recurrent ones (in no particular order): text 
summarization, machine translation, text 
simplification, automatic discourse analysis, 
coreference and anaphora resolution, information 
retrieval, text mining, terminology/lexicon 
research, ontologies and semantic tagging, and 
corpus linguistics. 
Based on the survey, we estimate that Brazil has 
about 250 researchers (including students) with 
interest in CL/NLP area. Although only 148 
researchers attended the CEPLN survey, we 
computed other researchers in the Registry of Latin 
American Researchers in Natural Language 
Processing and Computational Linguistics and in 
the CEPLN e-mail list that did not attend the 
survey. In general, we estimate that about 35-40 of 
                                                           
16
 http://www.nilc.icmc.usp.br/tep2/index.htm 
17
 http://www.clul.ul.pt/clg/wordnetpt 
18
 http://mwnpt.di.fc.ul.pt 
19
 http://xldb.di.fc.ul.pt/Rembrandt 
these are active researchers, whose main topic of 
research is CL/NLP, and who supervise 
undergraduate and graduate students on the 
subject. We also estimate that there are 5-10 
researchers on speech processing that actively 
collaborate with the CL/NLP community. 
3 Main research groups 
The largest CL/NLP research group in Brazil is 
NILC (Interinstitutional Center for Research and 
Development in Computational Linguistics)20, 
which includes researchers mainly from University 
of S?o Paulo (USP; Computer Science and Physics 
departments), Federal University of S?o Carlos 
(UFSCar; Computer Science and Linguistics 
departments) and State University of S?o Paulo 
(UNESP; Linguistics department). The group was 
created at 1993. 
NILC has a long history of research in CL/NLP, 
which has thrived since the ReGra21 project, in 
which the grammar checker for Portuguese that is 
currently used within Microsoft Word since its 
2000 version was built. In fact, ReGra project was 
born from a university-industry collaboration, one 
of the few successful ones in CL/NLP area in 
Brazil. At the moment most of the research at 
NILC is concentrated on the following topics: 
automatic summarization, text simplification, 
coreference resolution, and terminology. NILC has 
hosted STIL 2009 (STIL event series is introduced 
in the next section). NILC also currently holds the 
presidency of CEPLN. 
The NLP group at the Computer Science 
department at the Catholic University of Rio 
Grande do Sul (PUC-RS)22 also has a tradition of 
research on CL/NLP. Their current projects focus 
on information retrieval, ontology engineering and 
anaphora resolution. The group also has research 
on multi-agent systems applied to NLP tasks and, 
more recently, on text categorization. The group 
hosts PROPOR 2010 (PROPOR event series is 
also introduced in the next section). The group has 
held the presidency of CEPLN from its creation 
(2007) until 2009. 
The above research group and NILC form the 
main CL/NLP research vein in Brazil. They have 
joint research projects and have strong 
                                                           
20
 http://www.nilc.icmc.usp.br 
21
 http://www.nilc.icmc.usp.br/nilc/projects/regra.htm 
22
 http://www.inf.pucrs.br/~linatural 
4
collaboration, constantly hosting graduate students 
from each other in internship research periods. 
There are also other very relevant NLP groups 
in Brazil that regularly carry out projects on the 
area. We may cite the Catholic University of Rio 
de Janeiro (PUC-Rio)23, Federal University of Rio 
Grande do Sul (UFRGS), State University of 
Campinas (UNICAMP), University of the Sinos 
River Valley (Unisinos), and State University of 
Maring? (UEM), among others. 
4 Events and Journals 
The Brazilian Symposium on Information and 
Human Language Technology (STIL) is the main 
event on CL/NLP in South America and is in its 
seventh edition. It is promoted by CEPLN and is 
carried out since 1993. It is intended to be a forum 
for gathering everyone with interest in CL/NLP. It 
happens regularly (every one or two years) and 
accepts contributions in Portuguese, Spanish and 
English. Details about the event are available at 
www.nilc.icmc.usp.br/til. 
The International Conference on Computational 
Processing of Portuguese Language (PROPOR) is 
an international conference jointly promoted by 
Brazil and Portugal and is in its ninth edition. It is 
the main conference with focus on Portuguese 
language, giving equal space to research on text 
and speech processing. It is carried out in Brazil 
and in Portugal interchangeably (every two or 
three years) and accepts submissions in English 
only. PROPOR?s proceedings are published as part 
of Springer Lecture Notes series. Details about the 
event are available at 
www.nilc.icmc.usp.br/cgpropor. 
STIL and PROPOR are the most relevant 
conferences for researchers in CL/NLP in Brazil. 
Their last editions received support from NAACL. 
AI events are also recurrent forums for CL/NLP 
researchers. The Brazilian AI events are the 
Brazilian Symposium on Artificial Intelligence 
(SBIA)24 and the National Meeting on Artificial 
Intelligence (ENIA)25, also promoted by SBC. 
They are already in their twentieth and seventh 
editions, respectively. 
Other related events in Brazil are the Corpus 
                                                           
23
 www.letras.puc-rio.br/Clic/ogrupo.htm 
24
 http://www.jointconference.fei.edu.br/ 
25
 http://csbc2009.inf.ufrgs.br/ 
Linguistics Meeting (ELC)26 and Brazilian School 
on Computational Linguistics (EBRALC)27, which 
are in their eighth and third editions, respectively. 
These events are mainly organized by the 
Linguistics research community. EBRALC is 
mainly intended for new students in the area and 
has been held together with ELC. 
Brazilian researchers count mainly on the 
following journals for national periodical 
publications: 
 JBCS28 (Journal of the Brazilian Computer 
Society), which is published by SBC and 
covers all Computer Science areas, including 
CL/NLP; 
 RITA29 (Journal of Theoretical and Applied 
Computing), also of general scope. 
 
It is important to cite Linguam?tica30, which is an 
European initiative to publish CL/NLP research on 
the Iberian languages. 
CEPLN is also organizing a joint journal with 
other SBC AI-related special interest groups. 
5 Challenges 
At STIL 2009, the research community discussed 
challenging issues (raised by respondents of the 
CEPLN survey) that hamper research on CL/PLN 
in Brazil. The main issues raised were: 
 Lack of large and robust language resources 
for Portuguese; 
 Lack of formal models for linguistic 
description and analysis of Portuguese; 
 Difficulty in attracting students and researchers 
to the area; 
 Lack of multidisciplinary collaboration; 
 CL/NLP marginalization in both Computer 
Science and Linguistics. 
 Poor interaction between universities and 
industry; 
 Insufficient funding. 
 
Here we discuss some of these points. Although 
Portuguese has got state of the art tools (as POS 
taggers and syntactic parsers) and comprehensive 
corpora of contemporary written language, there is 
                                                           
26
 http://www.corpuslg.org/elc/Inicial.html 
27
 http://www.corpuslg.org/ebralc/Inicial.html 
28
 http://www.springer.com/computer+science/journal/13173 
29
 http://www.seer.ufrgs.br/index.php/rita 
30
 http://linguamatica.pt 
5
still a need for resources for particular applications 
or domains. Many researchers feel that Portuguese 
syntactic parsers (which are considered basic NLP 
tools) and wordnet-like resources are still too 
limited, not attending their demands. Brazil also 
lacks representative spoken corpora, what may be 
explained by the fact that, in Brazil, written and 
spoken language processing communities have 
modest interaction. While written language 
processing research is reported at SBC events, 
spoken language processing is mainly conducted 
under SBrT (Brazilian Telecommunications 
Society)31. PROPOR series have tried to bring 
together these two communities, fostering joint 
research and mutual awareness of both research 
lines.  
The lack of formal models for Portuguese 
linguistic description and analysis was mainly 
perceived by linguists that work with CL/NLP. In 
fact, they acknowledge that Brazil has no tradition 
in carrying out events on these themes, what would 
eventually harm CL/NLP research. This goes along 
with Sp?rck Jones (2007) opinion paper. One first 
step towards overcoming this lack of formal 
models for Portuguese description was the 
Workshop on Portuguese Description32, carried out 
together with the last edition of STIL. 
Another point that deserves attention is the 
sentiment that CL/NLP research suffers from 
marginalization in both Computer Science and 
Linguistics areas, as it is usually the case for 
multidisciplinary subjects. We believe this might 
be fueled by the way research is assessed in Brazil. 
In Brazil, the quality of research is mainly assessed 
by the publications generated from it, and 
publication vehicles from Linguistics are usually 
rated worse in Computer Science, and vice versa. It 
is expected that different areas may have different 
scientific methods and perspectives, as well as it is 
natural that such differences are mirrored in any 
evaluation instrument. However, such factors lead 
some researchers to feel uncomfortable with the 
multidisciplinary nature of CL/NLP field and the 
way they are recognized in their own major areas. 
Many researchers (not only from Brazil, but also 
from Portugal) have supported that CL/NLP should 
become a new ?major? area, instead of being part 
of Computer Science or Linguistics. 
                                                           
31
 http://www.sbrt.org.br 
32
 http://www.ppgl.ufscar.br/jdp/index.html 
Concerning insufficient funding, we believe that 
the main complaints came from Brazilian regions 
other than south and southeast, which currently 
concentrate CL/NLP research. In fact, during a 
lengthy discussion at STIL 2009 about the raised 
challenges, this issue was dismissed by many 
participants as non-representative. We believe that 
the funding situation in each region of Brazil 
contributes to the status of research on all topics, 
not particularly CL/NLP, in these regions. While 
in most Brazilian states researchers have to 
compete for funding from national agencies, some 
states (mainly in the southeast region) can rely on 
strong state-based funding agencies, such as 
FAPESP, in the state of S?o Paulo. 
6 Opportunities for Collaboration 
We believe that there are many opportunities for 
collaboration on CL/NLP with other researchers in 
the Americas, mainly due to the fact that the 
research community in Brazil works not only with 
Portuguese, but also with English and Spanish. 
One first step towards collaboration in Latin 
America was given in the event CHARLA 2008 
(Grand Challenges in Computer Science Research 
in Latin America Workshop). Organized by several 
scientific societies (including SBC), the event 
aimed at contributing to the definition of a long-
term research agenda in Latin America with the 
potential to significantly advance science and 
motivate the networking of abilities and 
competencies in Latin America. One of the 
recognized challenges was ?multilinguism?, which 
involves several CL/NLP topics. CHARLA 
immediate impact in Brazil was the adaptation of 
Brazilian CL/NLP events to receive contributions 
in Spanish, which has a vast number of speakers in 
Latin America. Contributions in English were 
already traditionally considered in Brazilian 
events. 
We believe that another important source of 
collaboration comes from awareness of the 
ongoing research projects in the Americas. 
Workshops such as this seem to be a channel for 
the exchange of information. We envision that 
initial collaborations may arise within machine 
translation projects, which naturally already deal 
with the representative languages of the Americas. 
Letting aside technical collaboration, we believe 
there is room for higher-level concrete actions that 
6
could foster collaboration in the Americas. These 
are actions that may increase the visibility of the 
research done in Latin America, as well as 
motivate new research. One first action that we 
envisage is the opening of evaluation challenges 
and shared tasks to the languages of the Americas 
other than English. For instance, 
contests/conferences such as TAC33, 
Senseval/SemEval34, and TREC35, among others, 
might make Portuguese/Spanish datasets available, 
as CLEF36 has done in its last editions. This has 
certainly an organizational cost, but it may turn out 
to be a valuable investment.  
Another action that could stimulate the progress 
of CL/NLP research in Latin America consists of 
including the proceedings of other American 
CL/NLP conferences in the ACL Anthology37, for 
example, the proceedings of STIL and PROPOR, 
to mention the Brazilian examples. This could be 
restricted to conferences that received 
ACL/NAACL endorsement and/or sponsorship.  
While the first action we proposed would make 
it feasible for more countries to participate in the 
evaluation contests, the second action would allow 
the works carried out in these countries to be better 
known. 
In a different strategy, we imagine that it must 
be possible for regional scientific associations to 
establish formal partnerships, granting some 
advantages to associated researchers from the 
corresponding countries, such as: registration 
discounts in the CL/NLP conferences from the 
countries (for instance, ACL/NAACL members 
would have discounts for registering in Brazilian 
events, as well as SBC members for ACL/NAACL 
events); and distribution of relevant publications 
for members of the associations (for instance, SBC 
traditionally distributes to its members the JBCS 
journal, which is considered a prestigious 
international publication). 
Our last idea would be to create a fund (possibly 
through the associations? partnership) for funding 
visits for knowledge transfer (1-2 weeks) for 
researchers and mainly students. These could be an 
opportunity for studying/working with researchers 
from other countries that work on topics of 
                                                           
33
 http://www.nist.gov/tac 
34
 http://www.senseval.org 
35
 http://trec.nist.gov 
36
 http://www.clef-campaign.org 
37
 http://aclweb.org/anthology-new 
interest, as well as for renowned researchers to 
visit research groups in order to stimulate work on 
a particular topic. Such opportunities would be 
very positive for Brazilian students. 
We believe that the actions suggested above can 
lead to a more integrated research scenario in the 
Americas. 
Acknowledgments 
The authors are grateful to SBC, CEPLN, 
FAPESP, and CAPES for supporting this work and 
the realization of STIL 2009, where part of the data 
shown in this paper was presented. 
References  
Pardo, T.A.S.; Caseli, H.M.; Nunes, M.G.V. (2009). 
Mapeamento da Comunidade Brasileira de 
Processamento de L?nguas Naturais. In the 
Proceedings of the 7th Brazilian Symposium in 
Information and Human Language Technology - 
STIL, pp. 1-21. September 8-10, S?o Carlos/SP, 
Brazil. 
Santos, D. (2009). Caminhos percorridos no mapa da 
portuguesifica??o: A Linguateca em perspectiva. 
Linguam?tica, N. 1, pp. 25-58. 
Sp?rck Jones, K. (2007). Computational Linguistics: 
What About the Linguistics? Computational 
Linguistics, Last Words Section, Vol. 33, N. 3, pp. 
437-441. 
7
Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,
pages 24?31, Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using Common Sense to generate culturally contextualized
Machine Translation
Helena de Medeiros Caseli Bruno Akio Sugiyama
Department of Computer Science (DC)
Federal University of Sa?o Carlos (UFSCar)
Rod. Washington Lu??s, km 235 ? CP 676
CEP 13565-905, Sa?o Carlos, SP, Brazil
{helenacaseli,bruno sugiyama,junia}@dc.ufscar.br
Junia Coutinho Anacleto
Abstract
This paper reports an ongoing work in
applying Common Sense knowledge to
Machine Translation aiming at generating
more culturally contextualized translations.
Common Sense can be defined as the
knowledge shared by a group of people in
a given time, space and culture; and this
knowledge, here, is represented by a semantic
network called ConceptNet. Machine
Translation, in turn, is the automatic process
of generating an equivalent translated version
of a source sentence. In this work we intend
to use the knowledge represented in two
ConceptNets, one in Brazilian Portuguese and
another in English, to fix/filter translations
built automatically. So, this paper presents
the initial ideas of our work, the steps taken
so far as well as some opportunities for
collaboration.
1 Introduction
In this paper we describe an ongoing work con-
cerning the studies in gathering and using Common
Sense knowledge and building Machine Translation
applications. Common Sense (CS) can be defined
as the knowledge shared by a group of people in a
given time, space and culture.1 Machine Translation
(MT), in turn, is the application of computer
programs to generate a translated equivalent version
of a source text, in a target language.
1This definition of Common Sense is adopted by Open Mind
Common Sense (OMCS) and Brazilian Open Mind Common
Sense (OMCS-Br) projects and is only one of the several
possible definitions.
MT is one of the oldest and most important
areas of Natural Language Processing (NLP) /
Computational Linguistics (CL).2 From its begin-
nings we have witnessed some changes in the
proposed MT paradigms ranging from the basic
level ?in which MT is performed by just replacing
words in a source language by words in a target
language? to more sophisticated ones ?which rely
on manually created translation rules (Rule-based
Machine Translation) or automatically generated
statistical models (Statistical Machine Translation,
SMT). Nowadays, the majority of the researches has
being centered around the phrase-based statistical
MT (PB-SMT) approach ?such as (Koehn et
al., 2003) and (Och and Ney, 2004). PB-SMT
is considered the state-of-the-art according to the
automatic evaluation measures BLEU (Papineni et
al., 2002) and NIST (Doddington, 2002)3.
Although PB-SMT models have achieved the
state-of-the-art translation quality, there are strong
evidences that these models will not be able to
go further without more linguistically motivated
features, as stated by Tinsley and Way (2009). This
is already being illustrated by the recent shift of
researches towards linguistically enriched models as
(Koehn and Hoang, 2007) and (Tinsley and Way,
2009) among others.
Following the same idea of these most recent
researches, here we are also interested in seeing
2In this paper we will use the terms NLP and CL
interchangeably since this is the assumption adopted in Brazil.
3BLEU and NIST are two automatic measures widely
applied to evaluate the target MT output sentence regarding one
our more reference sentences.
24
how it is possible to improve MT performance
based on more linguistically motivated features.
In our case, we intend to investigate how to
apply Common Sense knowledge to generate more
culturally contextualized automatic translations.
For example, considering the translation of
slangs4 as in the English sentence ?Jump, you
chicken!?5. In this case, the word ?chicken? do
not mean ?a kind of bird? but ?a coward? or ?a
person who is not brave?. However, its translation
to Portuguese (?galinha?) can also be applied as a
slang with a completely different meaning. In the
Portuguese language, the slang ?galinha? means a
men with a lot of girlfriends. Although the problem
stated in the given example could also be fixed by
some dictionary entries, CS knowledge is the kind
of information that varies a lot and frequently can
not be found in traditional dictionaries. Thus, we
believe that the CS knowledge derived from the
OMCS projects is an alternative way to cope with
these translation problems.
Before presenting our ideas, section 2 describes
some related work on SMT and more recent
linguistically motivated empirical MT. Common
sense and the Open Mind Common Sense project
are the subjects of sections 3. Section 4 brings some
of our ideas on how to apply the common sense
knowledge in the automatic translation from/to
Brazilian Portuguese and to/from English. After
presenting the current scenario of our ongoing work,
we point out some opportunities for collaboration in
section 5. Finally, section 6 finishes this paper with
insights about the next steps of our research.
2 Machine Translation
Machine Translation (MT) has about 70 years of
history and lot of its recent achievements are directly
related to the advances in computer science, which
enable almost everyone to have access and use
MT tools. Some of these tools were traditionally
developed following the rule-based approach (e.g.,
4Slangs are typically cultural because they characterize the
mode of a group?s speech in a given space and time.
5Sentence extracted from Cambridge Advanced Learner?s
Dictionary: http://dictionary.cambridge.org/
define.asp?key=13018&dict=CALD.
Systran6 and Apertium7) but the statistical approach
is now being widely applied at least in part (e.g.,
Google8) (Cancedda et al, 2009).
The SMT was born in the late 1980s as an effort of
researchers from IBM (Brown et al, 1990). In those
days, SMT was performed based on two models:
a word-based translation model and a language
model. While the first model is concerned with
the production of target equivalent versions of the
source sentences, the second one guarantees that the
output sentence is a possible one (it is grammatical
and fluent) in the target language. In the current PB-
SMT systems, the word-based models were replaced
by the phrase-based ones built based on sequences of
words (the phrases).9
The translation and language models used in SMT
are built from a training parallel corpora (a set
of source sentences and their translations into the
target language) by means of IBM models (Brown
et al, 1993) which calculate the probability of
a given source word (or sequences of words) be
translated to a target word (or sequence of words).
The availability of some open-source toolkits (such
as Moses (Koehn et al, 2007)10) to train, test
and evaluate SMT models has helping the widely
employment of this MT approach to perhaps almost
any language pair and corpus type. In fact, SMT
is an inexpensive, easy and language independent
way for detecting recurrent phrases that form the
language and translation models.
However, while PB-SMT models have achieved
the state-of-the-art translation quality, its perfor-
mance seams to be stagnated. Consequently, there is
a recent common trend towards enriching the current
models with some extra knowledge as the new
approaches of factored translation models (Koehn
and Hoang, 2007) or syntax-based (or syntax-
augmented) MT systems (Tiedemann and Kotze?,
2009; Tinsley and Way, 2009; Zollmann et al,
2008).
More related to our work are the proposals of
Musa et al (2003) and Chung et al (2005). Both
6http://www.systransoft.com/
7http://www.apertium.org/
8http://www.google.com/language_tools
9In SMT, a phrase is a sequence of two or more words even
though they do not form a syntactic phrase.
10http://www.statmt.org/moses/
25
of them are CS-based translation tools which take
the topics of a bilingual conversation guessed by a
topic spotting mechanism, and use them to generate
phrases that can be chosen by the end-user to follow
the conversation. Since they are interactive tools,
the phrases are first displayed on the screen in the
end-user?s native language and, then, he/she selects
a phrase to be translated (by a text-to-speech engine)
in the language in which the conversation is taking
place.
In our work, the main goal is also investigating
new ways to improve MT performance, but instead
of greater BLEU or NIST values we are interested
in producing more culturally contextualized transla-
tions. Similarly to (Musa et al, 2003) and (Chung
et al, 2005), we intend to help two bilingual users
to develop a communication. However, in our
case we are not only concerned with the language
differences, but also the cultural divergences. To
achieve this ambitious goal we rely on common
sense knowledge collected from Brazilian and
North-American individuals as explained in the next
section.
3 Common Sense
Common sense (CS) plays an important role in
the communication between two people as the
interchanged messages carries their prior beliefs,
attitudes, and values (Anacleto et al, 2006b).
When this communication involves more than one
language, translation tools can help to deal with the
language barrier but they are not able to cope with
the cultural one. In this case, the CS knowledge is a
powerful mean to guarantee that the understanding
will overcomes the cultural differences.
The CS knowledge applied in our research was
collaboratively collected from volunteers through
web sites and reflects the culture of their com-
munities (Anacleto et al, 2006a; Anacleto et al,
2006b). More specifically, our research relies on CS
collected as an effort of the Open Mind Common
Sense projects in Brazil (OMCS-Br11) and in the
USA (OMCS12).
The OMCS started in 1999, at the MIT Media
Lab, to collect common sense from volunteers on
11http://www.sensocomum.ufscar.br
12http://commons.media.mit.edu/en/
the Internet. More than ten years later, this project
encompass many different areas, languages, and
problems. Nowadays, there are over a million
sentences in the English site collected from over
15,000 contributors.13
OMCS-Br is a younger project that has being
developed by LIA-DC/UFSCar (Advanced Interac-
tion Laboratory of the Federal University of Sa?o
Carlos) since August 2005. Figure 1 illustrates the
OMCS-Br architecture to collect and manipulate CS
knowledge in five work fronts: (1) common sense
knowledge collection, (2) knowledge representation,
(3) knowledge manipulation, (4) access and (5) use.
A detailed explanation of each work front can be
found in (Anacleto et al, 2008a).14
As can be seen in Figure 1, the CS knowledge
is collected in the OMCS-Br site (bottom-left) by
means of templates15. Then, the collected fact is
stored in a knowledge base (up-left) from which it is
converted into graphs that form a semantic network.
These semantic networks, called ConceptNets, are
composed of nodes and arcs (to connect nodes) as
shown in the bottom-right part of Figure 1. The
nodes represent the knowledge derived from the CS
base while the arcs represent relations between two
nodes based on studies on the theory of (Minsky,
1986). Examples of Minsky relations extracted
from the ConceptNet, in English, related to the term
?book? are: IsA (?book? IsA ?literary work?),
UsedFor (?book? UsedFor ?learn?), CapableOf
(?book? CapableOf ?store useful knowledge?),
PartOf (?book? PartOf ?library?) and DefinedAs
(?book? DefinedAs ?foundation knowledge?).
Figure 2 brings an extract of our Brazilian
ConceptNet (Anacleto et al, 2008b) and Figure 3,
a parallel extract obtained from the North-American
ConceptNet (Singh, 2002). As it is possible to notice
from these figures, there is a straight relationship
between these ConceptNets. It is possible to find
many cases in which relations in English have
13http://csc.media.mit.edu/
14Examples of successful applications using the CS knowl-
edge derived from OMCS-Br can be fount at http://lia.
dc.ufscar.br/
15The templates are semi-structured statements in natural
language with some gaps that should be filled out with the
contributors? knowledge so that the final statement corresponds
to a common sense fact (Anacleto et al, 2008a).
26
Figure 1: OMCS-Br Project architecture (Anacleto et al, 2008a)
their counterpart in Portuguese as in the example
given in which ?book? is connected with ?learn?
by the relation UsedFor and the book?s translation
to Portuguese, ?livro?, is also linked with the
translation of learn (?aprender?) by a relation of the
same type.
Different from other researches using semantic
networks, such as MindNet16 (Vanderwende et
al., 2005), WordNet17 (Fellbaum, 1998) and
FrameNet18 (Baker et al, 1998), here we propose
the application of source and target ConceptNets
together in the same application.
16http://research.microsoft.com/en-us/
projects/mindnet/
17http://wordnet.princeton.edu/
18http://framenet.icsi.berkeley.edu/
4 Culturally Contextualized Machine
Translation
As presented in the previous sections, the main
goal of our research is to investigate how CS
knowledge can help MT systems to generate more
culturally contextualized translations. To do so, we
are working with two ConceptNets derived from
OMCS and OMCS-Br projects, that represent the
CS knowledge in English and Brazilian Portuguese,
respectively, as presented in section 3.
In this context, we intend to investigate the
application of CS knowledge in the MT process in
three different moments:
1. Before the automatic translation ? In this case
the source sentence input is enriched with
some CS knowledge (for example, context
information) that can help the MT tool to
choose the best translation;
27
Figure 2: Graphical representation of the Brazilian ConceptNet (Meuchi et al, 2009)
Figure 3: Graphical representation of the North-American ConceptNet (Meuchi et al, 2009)
2. During the automatic translation ? In this case
the CS knowledge is used as a new feature in
the machine learning process of translation;
3. After the automatic translation ? In this case
some target words in the output sentence can be
enriched with CS knowledge (for example, the
knowledge derived from the ?DefinedAs? or
?IsA? Minsky relations) to better explain their
meanings.
Currently, we are dealing with the last moment
and planing some ways to fix/filter the target
sentences produced by a SMT system. This part
of the work is being carried out in the scope of a
master?s project which aims at building a bilingual
culturally contextualized chat. By using a SMT tool
(SMTT) and a CS knowledge tool (CSKT), this chat
will help the communication between two users with
different languages and cultures.
The SMTT is a phrase-based one trained using
Moses and a corpus of 17,397 pairs of Portuguese?
English parallel sentences with 1,026,512 tokens
(494,391 in Portuguese and 532,121 in English).
The training corpus contains articles from the online
version of the Brazilian scientific magazine Pesquisa
FAPESP19 written in Brazilian Portuguese (original)
and English (version) and, thus, a vocabulary that
do not fit exactly the one found in chats. The
SMTT trained based on this training corpus had
a performance of 0.39 BLEU and 8.30 NIST for
Portuguese?English translation and 0.36 BLEU and
7.83 NIST for English?Portuguese, in a test corpus
composed of 649 new parallel sentences from
the same domain of the training corpus (Caseli
and Nunes, 2009).20 For our experiments with
culturally-contextualized MT, the option of using
SMT models trained on general language in spite of
building specific ones for the chat domain was taken
aiming at measuring the impact that the CSKT has
on the final translation.
The CSKT, in turn, will help one user to
write his/her messages taking into account the
19http://revistapesquisa.fapesp.br
20In previous experiments carried out on the same corpora,
the best online MT system was Google with 0.33 BLEU and
7.61 NIST for Portuguse?English and 0.31 BLEU and 6.87
NIST for English?Portuguese translation (Caseli et al, 2006).
28
cultural differences between he/she and the other
user. A culturally contextualized translation will
be generated by applying the knowledge derived
from the two ConceptNets (see section 3) to fix/filter
the automatically generated translations in a semi-
automatic process assisted by both chat users.
To illustrate the use of both tools in the production
of a culturally contextualized translation, lets work
with slangs in the following example. Imagine a
Brazilian and an American communicating through
our bilingual chat supported by the SMTT and the
CSKT. The American user writes the sentence:
American says: ?Hey dude, I will borrow a C-
note from someone tomorrow!?.
Supposing that our SMTT is not able to provide
a translation for the words ?dude? and ?C-note?
?what is, indeed, a true possibility? outputting
an incomplete translation in which these words
remain untranslated. Consequently, the Brazilian
user would not understand the American?s sentence
incompletely translated to Portuguese. So, since
the SMTT do not know the translation of these
slangs, the CSKT will be started to look for
possible definitions in the CS knowledge bases.
At this moment, the CSKT could provide some
basic information about the untranslated words,
for example that ?dude is a slang? and ?dude (is)
defined as guy? or that ?C-note (is) defined as 100
dollars?, etc. Being aware of the untranslated words
and their cultural meanings displayed by the CSKT,
the American user could change or edit his/her
original message by writing:
American says: ?Hey guy, I will borrow 100
dollars from someone tomorrow!?.
The final edited sentence has a higher probability
to occur in the target language than the original one
and, so, to be corrected translated by the SMTT.
In addition to this master?s project, we are also
developing two undergraduate researches aiming at
discovering useful knowledge from the ?parallel?
ConceptNets. The first ongoing undergraduate
research (Barchi et al, 2009) aims at aligning the
parallel concepts found in Brazilian and English
ConceptNets. This alignment can be performed, for
example, based on lexical alignments automatically
generated by GIZA++21 (Och and Ney, 2000) or the
hierarchical structure of the nodes and arcs in the
ConceptNets. The second ongoing undergraduate
research (Meuchi et al, 2009), in turn, is involved
with the enrichment of one ConceptNet based on the
relations found in the other (parallel) ConceptNet
and also in lexically aligned parallel texts.
5 Opportunities for Collaboration
The work described in this paper presents the
first steps towards applying semantic knowledge to
generate more culturally contextualized translations
between Brazilian Portuguese and English texts.
In this sense, we see some opportunities for
collaboration regarding the roles that are played by:
(1) our research work, (2) the semantic resources
available to be used and (3) the resources and results
that will be produced by our work.
First of all, this work is a joint effort of two
research areas: NLP/CL (machine translation) and
human-computer interaction (HCI) (common sense
knowledge gathering and usage). From this fact, we
see a great opportunity to bring a new ?vision? to
the NLP/CL applications in which we are concerned
with not only to produce a correct answer to the
proposed problem, but also an answer that sounds
more natural and user-friendly. So, regarding our
work?s role, we see the opportunity to improve
the collaboration between researchers from NLP/CL
and HCI.
The second possibility of collaboration envi-
sioned by us is related to other sources of semantic
knowledge that could be applied to our work.
Although we are using common sense knowledge
to support the generation of more culturally con-
textualized translations, other semantic information
bases could also be applied. In this case, we
believe that this workshop is a great opportunity
to be aware of other research projects that apply
semantic knowledge to MT or are engaged with
the construction of semantic resources that could be
used in our work.
Finally, we also see a future source of col-
laboration regarding the use of the bilingual
resources obtained as the product of this research.
21http://code.google.com/p/giza-pp/
29
The parallel-aligned (in Brazilian Portuguese and
English) common sense base, the translation know-
ledge inferred from this aligned base or even
the bilingual culturally contextualized chat would
be useful in other research projects in MT or
other bilingual applications such as information
retrieval or summarization. We also believe that the
methodology applied to develop these resources and
the results obtained from this work could be applied
to other language pairs to derive new bilingual
similar resources.
6 Conclusions and Future Work
In this paper we have described the first ideas and
steps towards the culturally contextualized machine
translation, a new approach to generate automatic
translations using a phrase-based SMT tool and a
common sense knowledge tool.
It is important to say that this proposal involves
researchers from NLP/CL an HCI and it brings an
opportunity for collaboration between these related
areas. Furthermore, this work aims at stimulating
researchers from other countries to work with the
Brazilian Portuguese and presenting its ideas in this
workshop is a great opportunity to achieve this goal.
Future steps of this ongoing work are concerned
with the implementation of the proposed prototypes
designed for the bilingual culturally contextualized
chat, the alignment and the enrichment of the
ConceptNets. After the implementation of these
prototypes they will be tested and refined to
encompass the needed improvements.
Acknowledgments
We thank the support of Brazilian agencies CAPES,
CNPq and FAPESP and also the workshop organiz-
ers by making possible the presentation of this work.
References
Junia Coutinho Anacleto, Henry Lieberman,
Aparecido Augusto de Carvalho, Va?nia Paula
de Almeida Ne?ris, Muriel de Souza Godoi, Marie
Tsutsumi, Jose? H. Espinosa, Ame?rico Talarico
Neto, and Silvia Zem-Mascarenhas. 2006a. Using
common sense to recognize cultural differences. In
IBERAMIA-SBIA, pages 370?379.
Junia Coutinho Anacleto, Henry Lieberman, Marie
Tsutsumi, Vnia Neris, Aparecido Carvalho, Jose
Espinosa, and Silvia Zem-mascarenhas. 2006b.
Can common sense uncover cultural differences in
computer applications. In Proceedings of IFIP
WCC2006, Spring-Verlag, pages 1?10.
Junia Coutinho Anacleto, Aparecido Fabiano P. de
Carvalho, Alexandre M. Ferreira, Eliane N. Pereira,
and Alessandro J. F. Carlos. 2008a. Common sense-
based applications to advance personalized learning.
In Proceedings of the IEEE International Conference
on Systems, Man and Cybernetics (SMC 2008), pages
3244?3249, Singapore.
Junia Coutinho Anacleto, Aparecido Fabiano P. de
Carvalho, Eliane N. Pereira, Alexandre M. Ferreira,
and Alessandro J. F. Carlos. 2008b. Machines with
good sense: How can computers become capable of
sensible reasoning? In IFIP AI, pages 195?204.
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet Project. In
Proceedings of the COLING-ACL, Montreal, Canada.
Paulo Henrique Barchi, Helena de Medeiros Caseli, and
Junia Coutinho Anacleto. 2009. Alinhamento de
Grafos: Investigac?a?o do Alinhamento de ConceptNets
para a Traduc?a?o Automa?tica. In Anais do I Workshop
de Iniciac?a?o Cient??fica em Tecnologia da Informac?a?o
e da Linguagem Humana (TILic), pages 1?4.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
Computational Linguistics, 16(2):79?85.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19:263?311.
Nicola Cancedda, Marc Dymetman, George Foster, and
Cyril Goutte, 2009. A Statistical Machine Translation
Primer, chapter 1, pages 1?36. The MIT Press.
Helena de Medeiros Caseli and Israel Aono Nunes.
2009. Statistical machine translation: little changes
big impacts. In Proceedings of the 7th Brazilian
Symposium in Information and Human Language
Technology, pages 1?9.
Helena de Medeiros Caseli, Maria das Grac?as
Volpe Nunes, and Mikel L. Forcada. 2006. Automatic
induction of bilingual resources from aligned parallel
corpora: application to shallow-transfer machine
translation. Machine Translation, 20:227?245.
Jae-woo Chung, Rachel Kern, and Henry Lieberman.
2005. Topic Spotting Common Sense Translation
Assistant. In Gerrit C. van der Veer and Carolyn Gale,
editors, Extended Abstracts Proceedings of the 2005
30
Conference on Human Factors in Computing Systems
(CHI 2005), Portland, Oregon, USA, April 2-7. ACM.
G. Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram cooccurrence
statistics. In Proceedings of the Human Language
Technology Conference (HLT 2002), pages 128?132.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press, Cambridge,
MA.
Philipp Koehn and Hieu Hoang. 2007. Factored
Translation Models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 868?876, Prague, June.
Association for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the Human Language Technology
(HLT/NAACL 2003), pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL
2007), pages 177?180, Prague, Czech Republic, June.
Association for Computational Linguistics.
La??s Augusta Silva Meuchi, Helena de Medeiros Caseli,
and Junia Coutinho Anacleto. 2009. Infere?ncia de
relac?o?es em ConceptNets com base em corpus paralelo
alinhado. In Anais do VI WorkShop de Trabalhos
de Iniciac?a?o Cient??fica (WTIC) - evento integrante do
WebMedia 2009, pages 1?3.
M. Minsky. 1986. The Society of Mind. Simon and
Schuster, New York.
Rami Musa, Madleina Scheidegger, Andrea Kulas, and
Yoan Anguilet. 2003. Globuddy, a dynamic broad
context phrase book. In CONTEXT, pages 467?474.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the ACL (ACL 2000), pages
440?447, Hong Kong, China.
Franz Josef Och and Hermann Ney. 2004. The
Alignment Template Approach to Statistical Machine
Translation. Computational Linguistics, 30(4):417?
449.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual meeting
of the Association for Computational Linguistics (ACL
2002), pages 311?318.
P. Singh. 2002. The OpenMind Commonsense
project. KurzweilAI.net. Available at:
<http://web.media.mit.edu/?push/OMCSProject.pdf>.
Jo?rg Tiedemann and Gideon Kotze?. 2009. Building a
large machine?aligned parallel treebank. In Marco
Passarotti, Adam Przepirkowski, Savina Raynaud, and
Frank Van Eynde, editors, Proceedings of the 8th
International Workshop on Treebanks and Linguistic
Theories (TLT?08), pages 197?208. EDUCatt, Mi-
lano/Italy.
John Tinsley and Andy Way. 2009. Automatically
generated parallel treebanks and their exploitability in
machine translation. Machine Translation, 23:1?22.
Lucy Vanderwende, Gary Kacmarcik, Hisami Suzuki,
and Arul Menezes. 2005. Mindnet: an
automatically-created lexical resource. In Proceed-
ings of HLT/EMNLP on Interactive Demonstrations,
pages 8?9, Morristown, NJ, USA. Association for
Computational Linguistics.
Andreas Zollmann, Ashish Venugopal, Franz Och,
and Jay Ponte. 2008. A systematic comparison
of phrase?based, hierarchical and syntax?augmented
statistical mt. In COLING ?08: Proceedings of
the 22nd International Conference on Computational
Linguistics, pages 1145?1152, Morristown, NJ, USA.
Association for Computational Linguistics.
31
