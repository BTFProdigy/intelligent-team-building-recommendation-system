Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 707?714,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Translating HPSG-style Outputs of a Robust Parser
into Typed Dynamic Logic
Manabu Sato? Daisuke Bekki? Yusuke Miyao? Jun?ichi Tsujii??
? Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan
? Center for Evolutionary Cognitive Sciences, University of Tokyo
Komaba 3-8-1, Meguro-ku, Tokyo 153-8902, Japan
?School of Informatics, University of Manchester
PO Box 88, Sackville St, Manchester M60 1QD, UK
?SORST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012, Japan
? {sa-ma, yusuke, tsujii}@is.s.u-tokyo.ac.jp
? bekki@ecs.c.u-tokyo.ac.jp
Abstract
The present paper proposes a method
by which to translate outputs of a ro-
bust HPSG parser into semantic rep-
resentations of Typed Dynamic Logic
(TDL), a dynamic plural semantics de-
fined in typed lambda calculus. With
its higher-order representations of con-
texts, TDL analyzes and describes
the inherently inter-sentential nature of
quantification and anaphora in a strictly
lexicalized and compositional manner.
The present study shows that the pro-
posed translation method successfully
combines robustness and descriptive ad-
equacy of contemporary semantics. The
present implementation achieves high
coverage, approximately 90%, for the
real text of the Penn Treebank corpus.
1 Introduction
Robust parsing technology is one result of the
recent fusion between symbolic and statistical
approaches in natural language processing and
has been applied to tasks such as information
extraction, information retrieval and machine
translation (Hockenmaier and Steedman, 2002;
Miyao et al, 2005). However, reflecting the
field boundary and unestablished interfaces be-
tween syntax and semantics in formal theory
of grammar, this fusion has achieved less in
semantics than in syntax.
For example, a system that translates the
output of a robust CCG parser into seman-
tic representations has been developed (Bos et
al., 2004). While its corpus-oriented parser at-
tained high coverage with respect to real text,
the expressive power of the resulting semantic
representations is confined to first-order predi-
cate logic.
The more elaborate tasks tied to discourse
information and plurality, such as resolution
of anaphora antecedent, scope ambiguity, pre-
supposition, topic and focus, are required to
refer to ?deeper? semantic structures, such as
dynamic semantics (Groenendijk and Stokhof,
1991).
However, most dynamic semantic theories
are not equipped with large-scale syntax that
covers more than a small fragment of target
languages. One of a few exceptions is Min-
imal Recursion Semantics (MRS) (Copestake
et al, 1999), which is compatible with large-
scale HPSG syntax (Pollard and Sag, 1994)
and has affinities with UDRS (Reyle, 1993).
For real text, however, its implementation, as
in the case of the ERG parser (Copestake
and Flickinger, 2000), restricts its target to the
static fragment of MRS and yet has a lower
coverage than corpus-oriented parsers (Baldwin,
to appear).
The lack of transparency between syntax and
discourse semantics appears to have created a
tension between the robustness of syntax and
the descriptive adequacy of semantics.
In the present paper, we will introduce
a robust method to obtain dynamic seman-
tic representations based on Typed Dynamic
Logic (TDL) (Bekki, 2000) from real text
by translating the outputs of a robust HPSG
parser (Miyao et al, 2005). Typed Dy-
namic Logic is a dynamic plural seman-
tics that formalizes the structure underlying
the semantic interactions between quantifica-
tion, plurality, bound variable/E-type anaphora
707
re?????e7?t xi1 ? ? ?xin ? ?G(i7?e)7?t .? gi7?e.g ? G? r
?
gx1, . . . ,gxm
?
? ? prop ? ?G(i7?e)7?t .? gi7?e.g ? G???hi7?e.h ? ?G?
?
? prop
...? prop
?
? ? ?G(i7?e)7?t .(? ? ? ?(?G))
re f
?
xi
?
[? prop] [? prop] ? ?G(i7?e)7?t .
?
?
?
i f G
?
x = ?G
?
x
then ? gi 7?e.g ? ?G? G?x = ?G
?
x
otherwise unde f ined
?
?
?
?
??
where prop ? ((i 7? e) 7? t) 7? (i 7? e) 7? t
g? ? G? 7?t ? Gg
G(i7?e) 7?t
.
xi ? ? de.?gi7?e.g ? G?gx = d
?
??
Figure 1: Propositions of TDL (Bekki, 2005)
and presuppositions. All of this complex
discourse/plurality-related information is encap-
sulated within higher-order structures in TDL,
and the analysis remains strictly lexical and
compositional, which makes its interface with
syntax transparent and straightforward. This is
a significant advantage for achieving robustness
in natural language processing.
2 Background
2.1 Typed Dynamic Logic
Figure 1 shows a number of propositions de-
fined in (Bekki, 2005), including atomic pred-
icate, negation, conjunction, and anaphoric ex-
pression. Typed Dynamic Logic is described in
typed lambda calculus (G?del?s System T) with
four ground types: e(entity), i(index), n(natural
number), and t(truth). While assignment func-
tions in static logic are functions in meta-
language from type e variables (in the case of
first-order logic) to objects in the domain De,
assignment functions in TDL are functions in
object-language from indices to entities. Typed
Dynamic Logic defines the notion context as
a set of assignment functions (an object of
type (i 7? e) 7? t) and a proposition as a func-
tion from context to context (an object of type
((i 7? e) 7? t) 7? (i 7? e) 7? t). The conjunctions
of two propositions are then defined as com-
posite functions thereof. This setting conforms
to the view of ?propositions as information
flow?, which is widely accepted in dynamic
semantics.
Since all of these higher-order notions are
described in lambda terms, the path for compo-
sitional type-theoretic semantics based on func-
tional application, functional composition and
type raising is clarified. The derivations of
TDL semantic representations for the sentences
?A boy ran. He tumbled.? are exemplified in
Figure 2 and Figure 3. With some instantia-
tion of variables, the semantic representations
of these two sentences are simply conjoined
and yield a single representation, as shown in
(1).
?
????
boy0x1s1
run0e1s1
agent 0e1x1
re f (x2) [ ]
?
tumble0e2s2
agent 0e2x2
?
?
????(1)
The propositions boy0x1s1, run
0e1s1 and
agent 0e1x1 roughly mean ?the entity referred
to by x1 is a boy in the situation s1?, ?the
event referred to by e1 is a running event in
the situation s1?, and ?the agent of event e1
is x1?, respectively.
The former part of (1) that corresponds to
the first sentence, filtering and testing the input
context, returns the updated context schema-
tized in (2). The updated context is then
passed to the latter part, which corresponds to
the second sentence as its input.
? ? ? x1 s1 e1 ? ? ?
john situation1 running1
john situation2 running2
...
...
...
(2)
This mechanism makes anaphoric expressions,
such as ?He? in ?He tumbles?, accessible to its
preceding context; namely, the descriptions of
their presuppositions can refer to the preceding
context compositionally. Moreover, the refer-
ents of the anaphoric expressions are correctly
calculated as a result of previous filtering and
testing.
708
?a?
? ni7?i7?p7?p.?wi 7?i7?i7?p7?p.
? ei.? si.? ? p.nx1s
?
wx1es?
?
?boy?
? xi.? si.? ? p.
?
boy0xs?
?
?wi7?i7?i7?p7?p.? ei.? si.? ? p.
?
boy0x1s
wx1es?
?
?ran?
? sb j(i7?i 7?i7?p7?p)7?i 7?i7?p7?p.
sb j
?
? xi.? ei.? si.? ? p.
"
run0es
agent 0ex?
#!
? ei.? si.? ? p.
?
??
boy0x1s1
run0es
agent 0ex1?
?
??
Figure 2: Derivation of a TDL semantic representation of ?A boy ran?.
?he?
?wi7?i7?i7?p7?p.
? ei.? si.? ? p.re f ?x2
?
[ ]
?
wx2es?
?
?tumbled?
? sb j(i7?i7?i7?p7?p)7?i7?i7?p7?p.
sb j
?
? xi.? ei.? si.? ? p.
"
tumble0es
agent 0ex?
#!
? ei.? si.? ? p.re f ?x2
?
[ ]
?
tumble0e2s2
agent 0e2x2
?
Figure 3: Derivation of TDL semantic representation of ?He tumbled?.
Although the antecedent for x2 is not de-
termined in this structure, the possible candi-
dates can be enumerated: x1, s1 and e1, which
precede x2. Since TDL seamlessly represents
linguistic notions such as ?entity?, ?event? and
?situation?, by indices, the anaphoric expres-
sions, such as ?the event? and ?that case?, can
be treated in the same manner.
2.2 Head-driven Phrase Structure
Grammar
Head-driven Phrase Structure Grammar (Pollard
and Sag, 1994) is a kind of lexicalized gram-
mar that consists of lexical items and a small
number of composition rules called schema.
Schemata and lexical items are all described
in typed feature structures and the unification
operation defined thereon.
?
?????
PHON ?boy?
SYN
SEM
?
??????
HEAD
?
noun
MOD h i
?
VAL
"
SUBJ h i
COMPS h i
SPR hdeti
#
SLASH h i
?
??????
?
?????
(3)
Figure 4 is an example of a parse tree,
where the feature structures marked with the
same boxed numbers have a shared struc-
ture. In the first stage of the derivation of
this tree, lexical items are assigned to each
of the strings, ?John? and ?runs.? Next, the
mother node, which dominates the two items,
?
??
PHON ?John runs?
HEAD 1
SUBJ h i
COMPS h i
?
??
?
??
PHON ?John?
HEAD noun
SUBJ h i
COMPS h i
?
?? : 2
?
???
PHON ?runs?
HEAD verb : 1
SUBJ h 2 i
COMPS h i
?
???
John runs
Figure 4: An HPSG parse tree
is generated by the application of Subject-Head
Schema. The recursive application of these op-
erations derives the entire tree.
3 Method
In this section, we present a method to de-
rive TDL semantic representations from HPSG
parse trees, adopting, in part, a previous
method (Bos et al, 2004). Basically, we first
assign TDL representations to lexical items that
are terminal nodes of a parse tree, and then
compose the TDL representation for the en-
tire tree according to the tree structure (Figure
5). One problematic aspect of this approach is
that the composition process of TDL semantic
representations and that of HPSG parse trees
are not identical. For example, in the HPSG
709
?
?
PHON ?John runs?
HEAD 1
SUBJ h i
COMPS h i
?
?
Subject-Head Schema
* ? e.? s.? ? .
re f (x1) [John0x1s1]
"
run0es
agent 0ex1?
#
?run
_empty_
+
Composition Rules
normal composition
word formation
nonlocal application
unary derivation
?
?
PHON ?John?
HEAD noun
SUBJ h i
COMPS h i
?
? : 2
?
??
PHON ?runs?
HEAD verb : 1
SUBJ h 2 i
COMPS h i
?
??
Assignment Rules
? ?w.? e.? s.? ? .
re f (x1) [John0x1s1] [wx1es? ]
?John
_empty_
?* ? sb j.sb j?
? x.? e.? s.? ? .
"
run0es
agent 0ex?
#!
?run
_empty_
+
John runs John runs
Figure 5: Example of the application of the rules
parser, a compound noun is regarded as two
distinct words, whereas in TDL, a compound
noun is regarded as one word. Long-distance
dependency is also treated differently in the
two systems. Furthermore, TDL has an opera-
tion called unary derivation to deal with empty
categories, whereas the HPSG parser does not
have such an operation.
In order to overcome these differences and
realize a straightforward composition of TDL
representations according to the HPSG parse
tree, we defined two extended composition
rules, word formation rule and non-local
application rule, and redefined TDL unary
derivation rules for the use in the HPSG
parser. At each step of the composition, one
composition rule is chosen from the set of
rules, based on the information of the schemata
applied to the HPSG tree and TDL represen-
tations of the constituents. In addition, we de-
fined extended TDL semantic representations,
referred to as TDL Extended Structures (TD-
LESs), to be paired with the extended compo-
sition rules.
In summary, the proposed method is com-
prised of TDLESs, assignment rules, composi-
tion rules, and unary derivation rules, as will
be elucidated in subsequent sections.
3.1 Data Structure
A TDLES is a tuple hT, p,ni, where T is an
extended TDL term, which can be either a
TDL term or a special value ? . Here, ?
is a value used by the word formation rule,
which indicates that the word is a word modi-
fier (See Section 3.3). In addition, p and n are
the necessary information for extended compo-
sition rules, where p is a matrix predicate in T
and is used by the word formation rule, and
n is a nonlocal argument, which takes either
a variable occurring in T or an empty value.
This element corresponds to the SLASH fea-
ture in HPSG and is used by the nonlocal
application rule.
The TDLES of the common noun ?boy? is
given in (4). The contents of the structure
are T , p and n, beginning at the top. In
(4), T corresponds to the TDL term of ?boy?
in Figure 2, p is the predicate boy, which is
identical to a predicate in the TDL term (the
identity relation between the two is indicated
by ???). If either T or p is changed, the other
will be changed accordingly. This mechanism
is a part of the word formation rule, which
offers advantages in creating a new predicate
from multiple words. Finally, n is an empty
value.
* ? x.? s.? ? .
?
?boy0xs?
?
?boy
_empty_
+
(4)
3.2 Assignment Rules
We define assignment rules to associate HPSG
lexical items with corresponding TDLESs. For
closed class words, such as ?a?, ?the? or
?not?, assignment rules are given in the form
of a template for each word as exemplified
below.
"
PHON ?a?
HEAD det
SPEC hnouni
#
?
* ? x.? s.? ? .
? ? n.?w.? e.? s.? ? .
nx1s
?
wx1es?
?
?
_empty_
_empty_
+(5)
710
Shown in (5) is an assignment rule for the
indefinite determiner ?a?. The upper half of
(5) shows a template of an HPSG lexical item
that specifies its phonetic form as ?a?, where
POS is a determiner and specifies a noun. A
TDLES is shown in the lower half of the fig-
ure. The TDL term slot of this structure is
identical to that of ?a? in Figure 2, while slots
for the matrix predicate and nonlocal argument
are empty.
For open class words, such as nouns, verbs,
adjectives, adverbs and others, assignment rules
are defined for each syntactic category.
?
?????
PHON P
HEAD noun
MOD hi
SUBJ hi
COMPS hi
SPR hdeti
?
?????
?
* ? x.? s.? ? .
?
?P0xs?
?
?P
_empty_
+
(6)
The assignment rule (6) is for common nouns.
The HPSG lexical item in the upper half of (6)
specifies that the phonetic form of this item is
a variable, P, that takes no arguments, does
not modify other words and takes a specifier.
Here, POS is a noun. In the TDLES assigned
to this item, an actual input word will be sub-
stituted for the variable P, from which the ma-
trix predicate P0 is produced. Note that we can
obtain the TDLES (4) by applying the rule of
(6) to the HPSG lexical item of (3).
As for verbs, a base TDL semantic represen-
tation is first assigned to a verb root, and the
representation is then modified by lexical rules
to reflect an inflected form of the verb. This
process corresponds to HPSG lexical rules for
verbs. Details are not presented herein due to
space limitations.
3.3 Composition Rules
We define three composition rules: the func-
tion application rule, the word formation
rule, and the nonlocal application rule.
Hereinafter, let SL = hTL, pL,nLi and SR =
hTR, pR,nRi be TDLESs of the left and the
right daughter nodes, respectively. In addition,
let SM be TDLESs of the mother node.
Function application rule: The composition
of TDL terms in the TDLESs is performed by
function application, in the same manner as in
the original TDL, as explained in Section 2.1.
Definition 3.1 (function application rule). If
Type
?
TL
?
= ? and Type?TR
?
= ? 7? ? then
SM =
* TRTL
pR
union
?
nL,nR
?
+
Else if Type
?
TL
?
= ? 7? ? and Type?TR
?
= ? then
SM =
* TLTR
pL
union
?
nL,nR
?
+
In Definition 3.1, Type(T ) is a function
that returns the type of TDL term T , and
union(nL,nR) is defined as:
union
?
nL,nR
?
=?
??
??
empty i f nL = nR = _empty_
n i f nL = n, nR = _empty_
n i f nL = _empty_, nR = n
unde f ined i f nL 6= _empty_, nR 6= _empty_
This function corresponds to the behavior of
the union of SLASH in HPSG. The composi-
tion in the right-hand side of Figure 5 is an
example of the application of this rule.
Word formation rule: In natural language,
it is often the case that a new word is cre-
ated by combining multiple words, for exam-
ple, ?orange juice?. This phenomenon is called
word formation. Typed Dynamic Logic and
the HPSG parser handle this phenomenon in
different ways. Typed Dynamic Logic does
not have any rule for word formation and re-
gards ?orange juice? as a single word, whereas
most parsers treat ?orange juice? as the sepa-
rate words ?orange? and ?juice?. This requires
a special composition rule for word formation
to be defined. Among the constituent words of
a compound word, we consider those that are
not HPSG heads as word modifiers and define
their value for T as ? . In addition, we apply
the word formation rule defined below.
Definition 3.2 (word formation rule). If
Type
?
TL
?
= ? then
SM =
* TR
concat
?
pL, pR
?
nR
+
Else if Type
?
TR
?
= ? then
SM =
* TL
concat
?
pL, pR
?
nL
+
711
concat (pL, pR) in Definition 3.2 is a func-
tion that returns a concatenation of pL and pR.
For example, the composition of a word mod-
ifier ?orange? (7) and and a common noun
?juice? (8) will generate the TDLES (9).
? ?
orange
_empty_
?
(7)
* ? x.? s.? ? .
?
? juice0xs?
?
? juice
_empty_
+
(8)
* ? x.? s.? ? .
?
?orange_ juice0xs?
?
?orange_ juice
_empty_
+
(9)
Nonlocal application rule: Typed Dynamic
Logic and HPSG also handle the phenomenon
of wh-movement differently. In HPSG, a wh-
phrase is treated as a value of SLASH, and
the value is kept until the Filler-Head Schema
are applied. In TDL, however, wh-movement
is handled by the functional composition rule.
In order to resolve the difference between
these two approaches, we define the nonlocal
application rule, a special rule that introduces
a slot relating to HPSG SLASH to TDLESs.
This slot becomes the third element of TD-
LESs. This rule is applied when the Filler-
Head Schema are applied in HPSG parse trees.
Definition 3.3 (nonlocal application rule).
If Type
?
TL
?
= (? 7? ? ) 7? ? , Type?TR
?
= ? ,
Type
?
nR
?
= ? and the Filler-Head Schema are applied
in HPSG, then
SM =
*
TL
?? nR.TR
?
pL
_empty_
+
3.4 Unary Derivation Rules
In TDL, type-shifting of a word or a phrase is
performed by composition with an empty cat-
egory (a category that has no phonetic form,
but has syntactic/semantic functions). For ex-
ample, the phrase ?this year? is a noun phrase
at the first stage and can be changed into a
verb modifier when combined with an empty
category. Since many of the type-shifting rules
are not available in HPSG, we defined unary
derivation rules in order to provide an equiva-
lent function to the type-shifting rules of TDL.
These unary rules are applied independently
with HPSG parse trees. (10) and (11) illus-
trate the unary derivation of ?this year?. (11)
Table 1: Number of implemented rules
assignment rules
HPSG-TDL template 51
for closed words 16
for open words 35
verb lexical rules 27
composition rules
binary composition rules 3
function application rule
word formation rule
nonlocal application rule
unary derivation rules 12
is derived from (10) using a unary derivation
rule.
? ?w.? e.? s.? ? .re f ?x1
??
?year0x1s1
??
wx1es?
?
?year
_empty_
?(10)
* ? v.? e.? s.? ? .
re f
?
x1
??
?year0x1s1
??
ves
?
mod 0ex1?
??
?year
_empty_
+
(11)
4 Experiment
The number of rules we have implemented is
shown in Table 1. We used the Penn Treebank
(Marcus, 1994) Section 22 (1,527 sentences) to
develop and evaluate the proposed method and
Section 23 (2,144 sentences) as the final test
set.
We measured the coverage of the construc-
tion of TDL semantic representations, in the
manner described in a previous study (Bos
et al, 2004). Although the best method for
strictly evaluating the proposed method is to
measure the agreement between the obtained
semantic representations and the intuitions of
the speaker/writer of the texts, this type of
evaluation could not be performed because of
insufficient resources. Instead, we measured
the rate of successful derivations as an indica-
tor of the coverage of the proposed system.
The sentences in the test set were parsed by
a robust HPSG parser (Miyao et al, 2005),
and HPSG parse trees were successfully gen-
erated for 2,122 (98.9%) sentences. The pro-
posed method was then applied to these parse
trees. Table 2 shows that 88.3% of the un-
712
Table 2: Coverage with respect to the test set
covered sentences 88.3 %
uncovered sentences 11.7 %
assignment failures 6.2 %
composition failures 5.5 %
word coverage 99.6 %
Table 3: Error analysis: the development set
# assignment failures 103
# unimplemented words 61
# TDL unsupporting words 17
# nonlinguistic HPSG lexical items 25
# composition failures 72
# unsupported compositions 20
# invalid assignments 36
# nonlinguistic parse trees 16
seen sentences are assigned TDL semantic rep-
resentations. Although this number is slightly
less than 92.3%, as reported by Bos et al,
(2004), it seems reasonable to say that the pro-
posed method attained a relatively high cover-
age, given the expressive power of TDL.
The construction of TDL semantic represen-
tations failed for 11.7% of the sentences. We
classified the causes of the failure into two
types. One of which is application failure of
the assignment rules (assignment failure); that
is, no assignment rules are applied to a num-
ber of HPSG lexical items, and so no TD-
LESs are assigned to these items. The other
is application failure of the composition rules
(composition failure). In this case, a type mis-
match occurred in the composition, and so a
TDLES was not derived.
Table 3 shows further classification of the
causes categorized into the two classes. We
manually investigated all of the failures in the
development set.
Assignment failures are caused by three fac-
tors. Most assignment failures occurred due to
the limitation in the number of the assignment
rules (as indicated by ?unimplemented words?
in the table). In this experiment, we did not
implement rules for infrequent HPSG lexical
items. We believe that this type of failure
will be resolved by increasing the number of
ref($1)[]
[lecture($2,$3) &
past($3) &
agent($2,$1) &
content($2,$4) &
ref($5)[]
[every($6)[ball($6,$4)]
[see($7,$4) &
present($4) &
agent($7,$5) &
theme($7,$6) &
tremendously($7,$4) &
ref($8)[]
[ref($9)[groove($9,$10)]
[be($11,$4) &
present($4) &
agent($11,$8) &
in($11,$9) &
when($11,$7)]]]]]
Figure 6: Output for the sentence: ?When
you?re in the groove, you see every ball
tremendously,? he lectured.
assignment rules. The second factor in the
table, ?TDL unsupported words?, refers to ex-
pressions that are not covered by the current
theory of TDL. In order to resolve this type of
failure, the development of TDL is required.
The third factor, ?nonlinguistic HPSG lexical
items? includes a small number of cases in
which TDLESs are not assigned to the words
that are categorized as nonlinguistic syntactic
categories by the HPSG parser. This problem
is caused by ill-formed outputs of the parser.
The composition failures can be further clas-
sified into three classes according to their
causative factors. The first factor is the ex-
istence of HPSG schemata for which we have
not yet implemented composition rules. These
failures will be fixed by extending of the def-
inition of our composition rules. The sec-
ond factor is type mismatches due to the un-
intended assignments of TDLESs to lexical
items. We need to further elaborate the as-
signment rules in order to deal with this prob-
lem. The third factor is parse trees that are
linguistically invalid.
The error analysis given above indicates that
we can further increase the coverage through
the improvement of the assignment/composition
rules.
Figure 6 shows an example of the output
for a sentence in the development set. The
variables $1, . . . ,$11 are indices that
713
represent entities, events and situations. For
example, $3 represents a situation and $2
represents the lecturing event that exists
in $3. past($3) requires that the sit-
uation is past. agent($2,$1) requires
that the entity $1 is the agent of $2.
content($2,$4) requires that $4 (as a
set of possible worlds) is the content of
$2. be($11,$4) refers to $4. Finally,
every($6)[ball($6,$4)][see($7,$4)
...] represents a generalized quantifier
?every ball?. The index $6 serves as an
antecedent both for bound-variable anaphora
within its scope and for E-type anaphora out-
side its scope. The entities that correspond to
the two occurrences of ?you? are represented
by $8 and $5. Their unification is left as
an anaphora resolution task that can be easily
solved by existing statistical or rule-based
methods, given the structural information of
the TDL semantic representation.
5 Conclusion
The present paper proposed a method by which
to translate HPSG-style outputs of a robust
parser (Miyao et al, 2005) into dynamic se-
mantic representations of TDL (Bekki, 2000).
We showed that our implementation achieved
high coverage, approximately 90%, for real
text of the Penn Treebank corpus and that the
resulting representations have sufficient expres-
sive power of contemporary semantic theory
involving quantification, plurality, inter/intra-
sentential anaphora and presupposition.
In the present study, we investigated the
possibility of achieving robustness and descrip-
tive adequacy of semantics. Although previ-
ously thought to have a trade-off relationship,
the present study proved that robustness and
descriptive adequacy of semantics are not in-
trinsically incompatible, given the transparency
between syntax and discourse semantics.
If the notion of robustness serves as a cri-
terion not only for the practical usefulness of
natural language processing but also for the
validity of linguistic theories, then the compo-
sitional transparency that penetrates all levels
of syntax, sentential semantics, and discourse
semantics, beyond the superficial difference be-
tween the laws that govern each of the levels,
might be reconsidered as an essential principle
of linguistic theories.
References
Timothy Baldwin, John Beavers, Emily M. Bender,
Dan Flickinger, Ara Kim and Stephan Oepen (to
appear) Beauty and the Beast: What running a
broad-coverage precision grammar over the BNC
taught us about the grammar ? and the cor-
pus, In Linguistic Evidence: Empirical, Theoreti-
cal, and Computational Perspectives, Mouton de
Gruyter.
Daisuke Bekki. 2000. Typed Dynamic Logic for
Compositional Grammar, Doctoral Dissertation,
University of Tokyo.
Daisuke Bekki. 2005. Typed Dynamic Logic and
Grammar: the Introduction, manuscript, Univer-
sity of Tokyo,
Johan Bos, Stephen Clark, Mark Steedman, James
R. Curran and Julia Hockenmaier. 2004. Wide-
Coverage Semantic Representations from a CCG
Parser, In Proc. COLING ?04, Geneva.
Ann Copestake, Dan Flickinger, Ivan A. Sag and
Carl Pollard. 1999. Minimal Recursion Seman-
tics: An introduction, manuscript.
Ann Copestake and Dan Flickinger. 2000.
An open-source grammar development environ-
ment and broad-coverage English grammar using
HPSG In Proc. LREC-2000, Athens.
Jeroen Groenendijk and Martin Stokhof. 1991. Dy-
namic Predicate Logic, In Linguistics and Philos-
ophy 14, pp.39-100.
Julia Hockenmaier and Mark Steedman. 2002. Ac-
quiring Compact Lexicalized Grammars from a
Cleaner Treebank, In Proc. LREC-2002, Las Pal-
mas.
Mitch Marcus. 1994. The Penn Treebank: A
revised corpus design for extracting predicate-
argument structure. In Proceedings of the ARPA
Human Language Technolog Workshop, Prince-
ton, NJ.
Yusuke Miyao, Takashi Ninomiya and Jun?ichi Tsu-
jii. 2005. Corpus-oriented Grammar Develop-
ment for Acquiring a Head-driven Phrase Struc-
ture Grammar from the Penn Treebank, in IJC-
NLP 2004, LNAI3248, pp.684-693. Springer-
Verlag.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar, Studies in Contem-
porary Linguistics. University of Chicago Press,
Chicago, London.
Uwe Reyle. 1993. Dealing with Ambiguities by
Underspecification: Construction, Representation
and Deduction, In Journal of Semantics 10,
pp.123-179.
714
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 273?277,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Building Japanese Textual Entailment Specialized Data Sets
for Inference of Basic Sentence Relations
Kimi Kaneko ? Yusuke Miyao ? Daisuke Bekki ?
? Ochanomizu University, Tokyo, Japan
? National Institute of Informatics, Tokyo, Japan
? {kaneko.kimi | bekki}@is.ocha.ac.jp
? yusuke@nii.ac.jp
Abstract
This paper proposes a methodology for
generating specialized Japanese data sets
for textual entailment, which consists of
pairs decomposed into basic sentence rela-
tions. We experimented with our method-
ology over a number of pairs taken from
the RITE-2 data set. We compared
our methodology with existing studies
in terms of agreement, frequencies and
times, and we evaluated its validity by in-
vestigating recognition accuracy.
1 Introduction
In recognizing textual entailment (RTE), auto-
mated systems assess whether a human reader
would consider that, given a snippet of text t1 and
some unspecified (but restricted) world knowl-
edge, a second snippet of text t2 is true. An ex-
ample is given below.
Ex. 1) Example of a sentence pair for RTE
? Label: Y
? t1: Shakespeare wrote Hamlet and Macbeth.
? t2: Shakespeare is the author of Hamlet.
?Label? on line 1 shows whether textual entail-
ment (TE) holds between t1 and t2. The pair is
labeled ?Y? if the pair exhibits TE and ?N? other-
wise.
It is difficult for computers to make such as-
sessments because pairs have multiple interrelated
basic sentence relations (BSRs, for detailed in-
formation on BSRs, see section 3). Recognizing
each BSRs in pairs exactly is difficult for com-
puters. Therefore, we should generate special-
ized data sets consisting of t1-t2 pairs decomposed
into BSRs and a methodology for generating such
data sets since such data and methodologies for
Japanese are unavailable at present.
This paper proposes a methodology for gener-
ating specialized Japanese data sets for TE that
consist of monothematic t1-t2 pairs (i.e., pairs in
which only one BSR relevant to the entailment
relation is highlighted and isolated). In addition,
we compare our methodology with existing stud-
ies and analyze its validity.
2 Existing Studies
Sammons et al(2010) point out that it is necessary
to establish a methodology for decomposing pairs
into chains of BSRs, and that establishing such
methodology will enable understanding of how
other existing studies can be combined to solve
problems in natural language processing and iden-
tification of currently unsolvable problems. Sam-
mons et al experimented with their methodology
over the RTE-5 data set and showed that the recog-
nition accuracy of a system trained with their spe-
cialized data set was higher than that of the system
trained with the original data set. In addition, Ben-
tivogli et al(2010) proposed a methodology for
classifying more details than was possible in the
study by Sammons et al.
However, these studies were based on only En-
glish data sets. In this regard, the word-order
rules and the grammar of many languages (such
as Japanese) are different from those of English.
We thus cannot assess the validity of methodolo-
gies for any Japanese data set because each lan-
guage has different usages. Therefore, it is neces-
sary to assess the validity of such methodologies
with specialized Japanese data sets.
Kotani et al (2008) generated specialized
Japanese data sets for RTE that were designed
such that each pair included only one BSR. How-
ever, in that approach the data set is generated ar-
tificially, and BSRs between pairs of real world
texts cannot be analyzed.
We develop our methodology by generating
specialized data sets from a collection of pairs
from RITE-21 binary class (BC) subtask data sets
containing sentences from Wikipedia. RITE-2 is
273
an evaluation-based workshop focusing on RTE.
Four subtasks are available in RITE-2, one of
which is the BC subtask whereby systems assess
whether there is TE between t1 and t2. The rea-
son why we apply our methodology to part of the
RITE-2 BC subtask data set is that we can con-
sider the validity of the methodology in view of
the recognition accuracy by using the data sets
generated in RITE-2 tasks, and that we can an-
alyze BSRs in real texts by using sentence pairs
extracted from Wikipedia.
3 Methodology
In this study, we extended and refined the method-
ology defined in Bentivogli et al(2010) and devel-
oped a methodology for generating Japanese data
sets broken down into BSRs and non-BSRs as de-
fined below.
Basic sentence relations (BSRs):
? Lexical: Synonymy, Hypernymy, Entailment,
Meronymy;
? Phrasal: Synonymy, Hypernymy, Entailment,
Meronymy, Nominalization, Corference;
? Syntactic: Scrambling, Case alteration, Modi-
fier, Transparent head, Clause, List, Apposi-
tion, Relative clause;
? Reasoning: Temporal, Spatial, Quantity, Im-
plicit relation, Inference;
Non-basic sentence relations (non-BSRs)?
? Disagreement: Lexical, Phrasal, Modal, Mod-
ifier, Temporal, Spatial, Quantity;
Mainly, we used relations defined in Bentivogli
et al(2010) and divided Synonymy, Hypernymy,
Entailment and Meronymy into Lexical and
Phrasal. The differences between our study and
Bentivogli et al(2010) are as follows. Demonymy
and Statements in Bentivogli et al(2010) were
not considered in our study because they were
not necessary for Japanese data sets. In addi-
tion, Scrambling, Entailment, Disagreement:
temporal, Disagreement: spatial and Disagree-
ment: quantity were newly added in our study.
Scrambling is a rule for changing the order of
phrases and clauses. Entailment is a rule whereby
the latter sentence is true whenever the former is
true (e.g., ?divorce?? ?marry?). Entailment is a
rule different from Synonymy, Hypernymy and
Meronymy.
The rules for decomposition are schematized as
follows:
1http://www.cl.ecei.tohoku.ac.jp/rite2/doku.php
? Break down pairs into BSRs in order to bring
t1 close to t2 gradually, as the interpretation
of the converted sentence becomes wider
? Label each pair of BSRs or non-BSRs
such that each pair is decomposed to ensure
that there are not multiple BSRs
An example is shown below, where the underlined
parts represent the revised points.
t1? ???????? ????? ? ????? ????
Shakespearenom Hamlet com Macbethacc writepast?Shakespeare wrote Hamlet and Macbeth.?[List] ???????? ?????? ????
Shakespearenom Hamletacc writepast?Shakespeare wrote Hamlet.?
t2?[Synonymy] ???????? ?????? ?? ????
?phrasal Shakespearenom Hamletgen authorcomp becop?Shakespeare is the author of Hamlet.?
Table 1: Example of a pair with TE
An example of a pair without TE is shown below.
t1? ?????? ???????? ???
Bulgarianom Eurasia.continentdat becop?Bulgaria is on the Eurasian continent.?
[Entailment] ?????? ???? ????
? phrasal Bulgarianom continental.statecomp becop?Bulgaria is a continental state.?
t2?[Disagreement] ?????? ?? ????
?lexical Bulgarianom island.countrycomp becop?Bulgaria is an island country.?
Table 2: Example of a pair without TE (Part 1)
To facilitate TE assessments like Table 3, non-
BSR labels were used in decomposing pairs. In
addition, we allowed labels to be used several
times when some BSRs in a pair are related to ?N?
assessments.
t1? ?????? ???????? ???
Bulgarianom Eurasia.continentdat becop?Bulgaria is on the Eurasian continent.?
[Disagreement] ?????? ???????? ???
?modal Bulgarianom Eurasia.continentdat becop?neg?Bulgaria is not on the Eurasian continent.?
t2?[Synonymy] ?????? ?????? ?????
?lexical Bulgarianom Europedat belongcop?neg?Bulgaria does not belong to Europe.?
Table 3: Example of a pair without TE (Part 2)
As mentioned above, the idea here is to decom-
pose pairs in order to bring t1 closer to t2, the
latter of which in principle has a wider semantic
scope. We prohibited the conversion of t2 because
it was possible to decompose the pairs such that
they could be true even if there was no TE. Never-
theless, since it is sometimes easier to convert t2,
274
we allowed the conversion of t2 in only the case
that t1 contradicted t2 and the scope of t2 did not
overlap with that of t1 even if t2 was converted and
TE would be unchanged. An example in case that
we allowed to convert t2 is shown below. Bold-
faced types in Table 4 shows that it becomes easy
to compare t1 with t2 by converting to t2.
t1? ??? ?????? ???????
Tomnom today breakfastacc eatpast?neg?Tom didn?t eat breakfast today.?
[Scrambling] ??? ??? ??? ???????
today Tomnom breakfastacc eatpast?neg?Today, Tom didn?t eat breakfast.?
t2? ??? ??? ??? ????
this.morning Tomnom breadacc eatpast?This morning, Tom ate bread and salad.?
[Entailment] ??? ??? ??? ????
?phrasal today Tomnom breakfastacc eatpast?Today, Tom ate breakfast.?
[Disagreement] ?????????????
?modal ?Today, Tom ate breakfast.?
Table 4: Example of conversion of t2
4 Results
4.1 Comparison with Existing Studies
We applied our methodology to 173 pairs from the
RITE-2 BC subtask data set. The pairs were de-
composed by one annotator, and the decomposed
pairs were assigned labels by two annotators. Dur-
ing labeling, we used the labels presented in Sec-
tion 3 and ?unknown? in cases where pairs could
not be labeled. Our methodology was developed
based on 112 pairs, and by using the other 61 pairs,
we evaluated the inter-annotator agreement as well
as the frequencies and times of decomposition.
The agreement for 241 monothematic pairs gen-
erated from 61 pairs amounted to 0.83 and was
computed as follows. The kappa coefficient for
them amounted 0.81.
Agreement = ?Agreed?? labels/Total 2
Bentivogli et al (2010) reported an agreement
rate of 0.78, although they computed the agree-
ment by using the Dice coefficient (Dice, 1945),
and therefore the results are not directly compara-
ble to ours. Nevertheless, the close values suggest
2Because the ?Agreed? pairs were clear to be classi-
fied as ?Agreed?, where ?Total? is the number of pairs la-
beled ?Agreed? subtracted from the number of labeled pairs.
?Agreed? labels is the number of pairs labeled ?Agreed? sub-
tract from the number of pairs with the same label assigned
by the two annotators.
that our methodology is comparable to that in Ben-
tivogli?s study in terms of agreement.
Table 5 shows the distribution of monothematic
pairs with respect to original Y/N pairs.
Or
igin
alp
air
s Monothematic pairs
Y N Total
Y (32) 116 ? 116
N (29) 96 29 125
Total (61) 212 29 241
Table 5: Distribution of monothematic pairs with
respect to original Y/N pairs
When the methodology was applied to 61 pairs,
a total of 241 and an average of 3.95 monothe-
matic pairs were derived. The average was slightly
greater than the 2.98 reported in (Bentivogli et al,
2010). For pairs originally labeled ?Y? and ?N?, an
average of 3.62 and 3.31 monothematic pairs were
derived, respectively. Both average values were
slightly higher than the values of 3.03 and 2.80 re-
ported in (Bentivogli et al, 2010). On the basis of
the small differences between the average values
in our study and those in (Bentivogli et al, 2010),
we are justified in saying that our methodology is
valid.
Table 6 3 shows the distribution of BSRs in t1-
t2 pairs in an existing study and the present study.
We can see from Table 6 thatCorferencewas seen
more frequently in Bentivogli?s study than in our
study, while Entailment and Scrambling were
seen more frequently in our study. This demon-
strates that differences between languages are rele-
vant to the distribution and classification of BSRs.
An average of 5 and 4 original pairs were de-
composed per hour in our study and Bentivogli?s
study, respectively. This indicates that the com-
plexity of our methodology is not much different
from that in Bentivogli et al(2010).
4.2 Evaluation of Accuracy in BSR
In the RITE-2 formal run4, 15 teams used our spe-
cialized data set for the evaluation of their systems.
Table 7 shows the average of F1 scores5 for each
BSR.
Scrambling and Modifier yielded high scores
(close to 90%). The score of List was also
3Because ?lexical? and ?phrasal? are classified together
in Bentivogli et al(2010), they are not shown separately in
Table 6.
4In RITE-2, data generated by our methodology were re-
leased as ?unit test data?.
5The traditional F1 score is the harmonic mean of preci-
sion and recall.
275
BSR Monothematic pairsBentivogli et al Present studyTotal Y N Total Y NSynonymy 25 22 3 45 45 0Hypernymy 5 3 2 5 5 0Entailment - - - 44 44 0Meronymy 7 4 3 1 1 0Nominalization 9 9 0 1 1 0Corference 49 48 1 3 3 0Scrambling - - - 15 15 0Case alteration 7 5 2 7 7 0Modifier 25 15 10 42 42 0Transparent head 6 6 0 1 1 0Clause 5 4 1 14 14 0List 1 1 0 3 3 0Apposition 3 2 1 1 1 0Relative clause 1 1 0 8 8 0Temporal 2 1 1 1 1 0Spatial 1 1 0 1 1 0Quantity 6 0 6 0 0 0Implicit relation 7 7 0 18 18 0Inference 40 26 14 2 2 0Disagreement: lexical/phrasal 3 0 3 27 0 27Disagreement: modal 1 0 1 1 0 1Disagreement: temporal - - - 1 0 1Disagreement: spatial - - - 0 0 0Disagreement: quantity - - - 0 0 0Demonymy 1 1 0 - - -Statements 1 1 0 - - -total 205 157 48 241 212 29
Table 6: Distribution of BSRs in t1-t2 pairs in an
existing study and in the present study using our
methodology
BSR F1(%) Monothematic MissPairsScrambling 89.6 15 4Modifier 88.8 42 0List 88.6 3 0Temporal 85.7 1 1Relative clause 85.4 8 2Clause 85.0 14 2Hypernymy: lexical 85.0 5 1Disagreement: phrasal 80.1 25 0Case alteration 79.9 7 2Synonymy: lexical 79.7 9 6Transparent head 78.6 1 2Implicit relation 75.7 18 2Synonymy: phrasal 73.6 36 9Corference 70.9 3 1Entailment: phrasal 70.2 44 7Disagreement: lexical 69.0 2 0Meronymy: lexical 64.3 1 1Nominalization 64.3 1 0Apposition 50.0 1 1Spatial 50.0 1 1Inference 40.5 2 2Disagreement: modal 35.7 1 0Disagreement: temporal 28.6 1 1Total - 241 41
Table 7: Average F1 scores in BSR and frequen-
cies of misclassifications by annotators
nearly 90%, although the data sets included only
3 instances. These scores were high because
pairs with these BSRs are easily recognized in
terms of syntactic structure. By contrast, Dis-
agreement: temporal, Disagreement: modal,
Inference, Spatial and Apposition yielded low
scores (less than 50%). The scores of Disagree-
ment: lexical, Nominalization and Disagree-
ment: Meronymy were about 50-70%. BSRs
that yielded scores of less than 70% occurred less
than 3 times, and those that yielded scores of not
more than 70% occurred 3 times or more, except
for Temporal and Transparent head. Therefore,
the frequencies of BSRs are related to F1 scores,
and we should consider how to build systems that
recognize infrequent BSRs accurately. In addi-
tion, F1 scores in Synonymy: phrasal and En-
tailment: phrasal are low, although these are la-
beled frequently. This is one possible direction of
future work.
Table 7 also shows the number of pairs in BSR
to which the two annotators assigned different la-
bels. For example, one annotator labeled t2 [Ap-
position] while the other labeled t2 [Spatial] in
the following pair:
Ex. 2) Example of a pair for RTE
? t1: Tokyo, the capital of Japan, is in Asia.
? t2: The capital of Japan is in Asia.
We can see from Table 7 that the F1 scores for
BSRs, which are often assessed as different by dif-
ferent people, are generally low, except for several
labels, such as Synonymy: lexical and Scram-
bling. For this reason, we can conjecture that
cases in which computers experience difficulty de-
termining the correct labels are correlated with
cases in which humans also experience such dif-
ficulty.
5 Conclusions
This paper presented a methodology for generat-
ing Japanese data sets broken down into BSRs
and Non-BSRs, and we conducted experiments in
which we applied our methodology to 61 pairs
extracted from the RITE-2 BC subtask data set.
We compared our method with that of Bentivogli
et al(2010) in terms of agreement as well as
frequencies and times of decomposition, and we
obtained similar results. This demonstrated that
our methodology is as feasible as Bentivogli et
al.(2010) and that differences between languages
emerge only as the different sets of labels and the
different distributions of BSRs. In addition, 241
monothematic pairs were recognized by comput-
ers, and we showed that both the frequencies of
BSRs and the rate of misclassification by humans
are relevant to F1 scores.
Decomposition patterns were not empirically
compared in the present study and will be investi-
gated in future work. We will also develop an RTE
inference system by using our specialized data set.
276
References
Bentivogli, L., Cabrio, E., Dagan, I, Giampiccolo, D.,
Leggio, M. L., Magnini,B. 2010. Building Textual
Entailment Specialized Data Sets: a Methodology
for Isolating Linguistic Phenomena Relevant to In-
ference. In Proceedings of LREC 2010, Valletta,
Malta.
Dagan, I, Glickman, O., Magnini, B. 2005. Recog-
nizing Textual Entailment Challenge. In Proc. of
the First PASCAL Challenges Workshop on RTE.
Southampton, U.K.
Kotani, M., Shibata, T., Nakata, T, Kurohashi, S. 2008.
Building Textual Entailment Japanese Data Sets and
Recognizing Reasoning Relations Based on Syn-
onymy Acquired Automatically. In Proceedings of
the 14th Annual Meeting of the Association for Nat-
ural Language Processing, Tokyo, Japan.
Magnini, B., Cabrio, E. 2009. Combining Special-
izedd Entailment Engines. In Proceedings of LTC
?09. Poznan, Poland.
Dice, L. R. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297-
302.
Mark Sammons, V.G.Vinod Vydiswaran, Dan Roth.
2010. ?Ask not what textual entailment can do for
you...?. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, Uppsala, Sweden, pp. 1199-1208.
277
Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 33?39,
Gothenburg, Sweden, April 26, 2014.
c
?2014 Association for Computational Linguistics
Building a Japanese Corpus of Temporal-Causal-Discourse Structures
Based on SDRT for Extracting Causal Relations
Kimi Kaneko
1
Daisuke Bekki
1,2,3
1
Ochanomizu University, Tokyo, Japan
2
National Institute of Informatics, Tokyo, Japan
3
CREST, Japan Science and Technology Agency, Saitama, Japan
{kaneko.kimi | bekki}@is.ocha.ac.jp
Abstract
This paper proposes a methodology for
generating specialized Japanese data sets
for the extraction of causal relations, in
which temporal, causal and discourse re-
lations at both the fact level and the epis-
temic level, are annotated. We applied
our methodology to a number of text frag-
ments taken from the Balanced Corpus of
Contemporary Written Japanese. We eval-
uated the feasibility of our methodology in
terms of agreement and frequencies, and
discussed the results of the analysis.
1 Introduction
In recent years, considerable attention has been
paid to deep semantic processing. Many studies
(Betherd et al., 2008), (Inui et al., 2007), (Inui
et al., 2003), (Riaz and Girju, 2013) have been
recently conducted on deep semantic processing,
and causal relation extraction (CRE) is one of the
specific tasks in deep semantic processing. Re-
search on CRE is still developing and there are
many obstacles that must be overcome.
Inui et al. (2003) acquired cause and effect
pairs from text, where the antecedent events were
taken as causes and consequent events were taken
as effects based on Japanese keywords such as
kara and node. In (1), for example, the an-
tecedent ame-ga hutta (?it rained?) and the conse-
quent mizutamari-ga dekita (?puddles emerged?)
are acquired as a pair of cause and effect.
(1) Ame-ga
rain-NOM
hutta-node
fall-past-because
mizutamari-ga
puddles-NOM
dekita.
emerge-past
?Because it rained, puddles emerged.?
However, antecedents are not always causes or
reasons for consequents in Japanese, as illustrated
by the following example.
(2) Zinsinziko-ga
injury.accident-NOM
okita-kara
happen-past-because
densya-ga
trains-NOM
tiensita
delay-past
to-iu-wake-dewanai.
it.is.not.the.case.that
?It is not the case that the trains were
delayed because an injury accident hap-
pened.?
In example (2), the antecedent zinsinziko-ga okita
(?an injury accident happened?) is not the cause
of the consequent densya-ga tiensita (?the trains
were delayed?). Though in such sentences that
contain causal expressions there are no causal re-
lations between antecedents and consequents, in
existing studies each sentence containing a causal
expression was extracted as knowledge represent-
ing cause and effect, such as in (Inui et al., 2003).
It is difficult for computers to auto-recognize and
exclude such cases.
In this paper, we report on the analysis of nec-
essary information for acquiring more accurate
cause-effect knowledge and propose a methodol-
ogy for creating a Japanese corpus for CRE. First,
we introduce previous studies and describe infor-
mation that should be used to annotate data sets.
Next, we describe our methodology based on Seg-
mented Discourse Representation Theory (SDRT)
(Asher et al., 2003). Finally, we evaluate the va-
lidity of our methodology in terms of agreement
and frequency, and analyze the results.
2 Previous Studies
In this section, we introduce previous studies on
annotation of temporal, causal and other types of
relations and present a linguistic analysis of tem-
poral and causal relations.
Betherd et al. (2008) generated English data
sets annotated with temporal and causal relations
and analyzed interactions between the two types of
33
relations. In addition, these specialized data sets
were evaluated in terms of agreement and accu-
racy. Relations were classified into two causal cat-
egories (CAUSAL, NO-REL) and three temporal
categories (BEFORE, AFTER, NO-REL). In re-
gard to the evaluation, Betherd et al. pointed out
that the classification was coarse-grained, and that
reanalysis would have to be performed with more
fine-grained relations.
Inui et al. (2005) characterized causal expres-
sions in Japanese text and built Japanese corpus
with tagged causal relations. However, usages
such as that illustrated in (2) and interactions be-
tween temporal relations and causal relations were
not analyzed.
Tamura (2012) linguistically analyzed temporal
and causal relations and pointed out that in rea-
son/purpose constructions in Japanese, the event
time indicated by the tense sometimes contradicts
the actual event time, and that the information nec-
essary to recognize the order between events lies
in the choice of the fact and the epistemic levels
(we will come back to these notions in the sec-
tion 3.4), and the explicit or implicit meaning of
a sentence in the causal expressions in Japanese.
Furthermore, some causal expressions in Japanese
are free from the absolute and relative tense sys-
tems, and both the past and non-past forms can be
freely used in main and subordinate clauses (Chin,
1984) (an example is given in the next section). In
other words, temporal relations are not always re-
solved earlier than causal relations, and therefore
we should resolve temporal relations and causal
relations simultaneously.
Asher et al. (2003) proposed SDRT in order to
account for cases where discourse relations affect
the truth condition of sentences. Because tempo-
ral relations constrain causal relations, the explicit
or implicit meaning of a sentences and the epis-
temic level information affects preceding and fol-
lowing temporal relations in causal expressions in
Japanese, recognition also affects causal relations.
Therefore, the annotation of both causal relations
and discourse relations in corpora is expected to be
useful for CRE. Moreover, which characteristics
(such as tense, actual event time, time when the
event is recognized, meaning and structure of the
sentence and causal relations) will serve as input
and which of them will serve as output depends on
the time and place. Therefore, we should also take
into account discourse relations together with tem-
poral and causal relations. We can create special-
ized data sets for evaluating these types of infor-
mation together by annotating text with discourse,
temporal and causal relations.
However, discourse relations of SDRT are not
distributed into discourse relations and temporal
relations, and as a result the classification of labels
becomes unnecessarily complex. Therefore, it is
necessary to rearrange discourse relations as in the
following example.
(3) Inu-wa
dog-NOM
niwa-o
garden-ACC
kakemawatta.
run-past
Neko-wa
cat-NOM
kotatu-de
kotatsu.heater-LOC
marukunatte-ita.
be.curled.up-past
?The dog ran in the garden. The cat was
curled up in the kotatsu heater.?
This pair of sentences is an antithesis, so we an-
notate it with the ?Contrast? label in SDRT. On the
other hand, the situation described in the first sen-
tence overlaps with that of the second sentence, so
we annotate this pair of sentences with the ?Back-
ground? label as well. Though there are many
cases in which we can annotate a sentence with
discourse relations in this way, dividing temporal
relations from discourse relations as in this study
allows us to avoid overlapping discourse relations.
This study was performed with the aim to rear-
range SDRT according to discourse relations, tem-
poral relations and causal relations separately, and
we generated specialized data sets according to
our methodology. In addition, occasionally it is
necessary to handle the actual event time and the
time when the event was recognized individually.
An example is given below.
(4) Asu
tomorrow
tesuto-ga
exam-NOM
aru-node,
take.place-nonpast-because,
kyoo-wa
today-TOP
benkyoo-suru-koto-ni
to.study-DAT
sita.
decide-past
?Because there will be an exam tomorrow,
I decided to study today.
Before we evaluate the consequent kyoo-wa
benkyoo-suru-koto-ni sita (?I decided to study to-
day?), we should recognize the fact of the an-
tecedent Asu tesuto-ga aru (?there will be an exam
tomorrow?). Whether we deal with the actual
34
Label Description
Precedence(A,B) End time (A) < start time (B)
In other words, event A temporally precedes event B.
Overlap(A,B) Start time (A) < end time (B) ? end time (B) < end time (A)?
In other words, event A temporally overlaps with event B.
Subsumption(A,B) Start time (A) ? end time (B) & End time (A) ? end time (B)?
In other words, event A temporally subsumes event B.
Table 1: Temporal relations list
Level Description
Cause(A,B) The event in A and the event in B are in a causal relation.
Table 2: Causal relation
event time or the time when the event was recog-
nized depends on the circumstances. Therefore,
we decided to annotate text at the fact and epis-
temic levels in parallel to account for such a dis-
tinction.
3 Methodology
We extended and refined SDRT and developed our
own methodology for annotating main and subor-
dinate clauses, phrases located between main and
subordinate clauses (e.g., continuative conjuncts
in Japanese), two consecutive sentences and two
adjoining nodes with a discourse relation. We also
defined our own method for annotating proposi-
tions with causal and temporal relations. The re-
sult of tagging example (5a) is shown in (5b).
(5) a. Kaze-ga
wind-NOM
huita.
blow-past
Harigami-ga
poster-NOM
hagare,
come.off-past
tonda.
flow-past
?The wind blew. A poster came off and
flew away.?
b. [Precedence(pi1,pi3),Explanation(pi1,pi3),
Cause(pi1,pi3)],
[Precedence(pi2,pi4), Explanation(pi2,pi4),
Cause(pi2,pi4)]
pi2pi1
Kaze-ga huita.
pi4pi3
Harigami-ga hagare, tonda.
The remainder of this section is structured as fol-
lows. Sections 3.1 and 3.2 deal with temporal and
causal relations, respectively. Section 3.3 covers
discourse relations, and Section 3.4 describes the
fact level and the epistemic level.
3.1 Temporal Relations
We consider the following three temporal relations
(Table 1). We assume that they represent the rela-
tions between two events in propositions and indi-
cate a start time and an end time. In addition, we
also assume that (start time of e)? (end time of e)
for all events. Based on this, the temporal place-
ment of each two events is limited to the three re-
lations in Table 1.
In this regard, Japanese non-past predicates
occasionally express habitually repeating events,
which have to be distinguished from events occur-
ring later than the reference point. In this paper, in
annotating the scope of the repetition, habitually
repeating events are described as in the following
example.
(6) a. Taiin-go,
After.retirement
{kouen-o
park-ACC
hasiru}
repeat
to.run
yoo-ni-site-iru.
have.a.custom
?After retiring, I have a custom to {run
in the park}
repeat
.?
b. {supootu-inryo-o
Sports.drink-ACC
nonda-ato,
drink-past-after
kouen-o
park-ACC
hasiru}
repeat
run
yoo-ni-site-iru.
have.a.custom
?I have a custom that {I run in the park
after having a sports drink}
repeat
.?
3.2 Causal Relations
We tag pairs of clauses with the following relation
(Table 2) only if there is a causal relation between
events in the proposition. By annotating text with
discourse relations, a fact and epistemic level and
temporal relations, we can describe the presence
35
Label Description
Alternation(A,B) ?A or B?, where the pair of A and B corresponds to logical disjunction (?).
Consequence(A,B) ?If A then B?, where the pair of A and B corresponds to logical implication (?).
Elaboration(A,B) B explains A in detail in the discourse relation.
B of the event is part of A of the event.
Narration(A,B) A and B are in the same situation, and
the pair of A and B corresponds to logical conjunction (?).
Explanation(A,B) The discourse relation indicates A as a cause and B as an effect.
Contrast(A,B) ?A but B?, where A and B are paradoxical.
Commentary(A,B) The content of A is summarized or complemented in B.
Table 3: Discourse relations list
SDRT Our methodology Rules
Alternation(A,B) Alternation(A,B) NA
Consequence(A,B) Consequence(A,B) NA
Elaboration(A,B) Elaboration(A,B) ? A,B (Elaboration(A,B)? Subsumption (A,B))
Narration(A,B) Precedence(A,B) ? Narration(A,B) NA
Background(A,B) Subsumption(A,B) ? Narration(A,B) NA
Result(A,B) Explanation(A,B)
Explanation(A,B) Cause(A,B) ? A,B (Cause(A,B)? Temp rel(A,B)) 1
Contrast(A,B) Contrast(A,B) NA
Commentary(A,B) Commentary(A,B) NA
Table 4: Correspondence between SDRT and our methodology
of causation in finer detail than (Betherd et al.,
2008).
3.3 Discourse Relations
We consider the following discourse relations
based on SDRT (Table 3). There are also relations
that impose limitations on temporal and causal re-
lations (Table 4). The way temporal, causal and
discourse relations affect each other is described
below together with their correspondence to the
relations in SDRT. Bold-faced entries represent
relations integrated in SDRT in our study.
Such limitations on temporal relations provides in-
formation for making a decision in terms of tem-
poral order and cause/effect in the ?de-tensed?
sentence structure
2
(Chin, 1984) in Japanese. An
example is given below.
(7) Kinoo
yesterday
anna-ni
that.much
taberu-kara,
eat-past-because
kyoo
today
onaka-ga
stomach-NOM
itaku
ache-cont
natta-nda.
become-noda
2
Temp rel(A,B) ?
Precedence(A,B)? Overlap(A,B)? Subsumption(A,B)
3
According to (Chin, 1984), ?de-tensed? is a relation
whereby the phrase has lost the meaning contributed by tense,
namely, the logical aspect of the semantic relation between an
antecedent and a consequent has eliminated the aspect tem-
poral relation between them.
?Because you ate that much yesterday, you
have a stomachache today.?
(7) [Precedence(pi1,pi3),Explanation(pi1,pi3),
Cause(pi1,pi3)],
[Precedence(pi2,pi4),Explanation(pi2,pi4),
Cause(pi2,pi4)]
pi2pi1
Kinoo anna-ni taberu-kara,
pi4pi3
kyoo onaka-ga itaku natta-nda.
This is a sentence where the subordinate clause is
in non-past tense and the main clause is in past
tense. Then, we may mistakenly interpret the
event in the subordinate clause as occurring after
the event of the main clause. However, we can de-
termine that in fact it occurred before the event in
the main clause based on the rule imposed by the
?Cause? relation.
3.4 Fact Level and Epistemic Level
A fact level proposition refers to an event and
its states, while an epistemic level proposition
refers to speaker?s recognizing event of a described
event. In Japanese, the latter form is often marked
by the suffix noda that attaches to all kinds of
predicates (which may also be omitted). Both
overt and covert noda introduce embedded struc-
tures, and we annotate them in such a way that a
fact level proposition is embedded in an epistemic
level proposition.
Semantically, the most notable difference be-
tween the two levels is that the tense in the former
36
represents the time that an event takes place, while
the tense in the latter represents the time that the
speaker recognizes the event.
This distinction between the two types of propo-
sitions is carried over to the distinction between
the fact level and the epistemic level causal rela-
tions. We annotate the former by the tag ?Cause?
and the latter by the tag ?Explanation?.
In Japanese, a causal marker such as node (a
continuation form of noda) and kara are both used
in the fact level and the epistemic level. The fact
level causality is a causal relation between the
two events, while the epistemic level causality is a
causal relation between the two recognizing events
of the two events mentioned. Therefore, in the
causal construction, it happens that the precedence
relations between the subordinate and the matrix
clauses in the fact level and the epistemic level do
not coincide, as in the following example.
(8) Kesa
this.morning
nani-mo
nothing-NOM
hoodoo-sare-nakatta-node,
report-passive-NEG.past-because,
kinoo-wa
yesterday-TOP
mebosii
notable
ziken-wa
events-NOM
nakatta-noda.
be-NEG-noda
?Because nothing was reported this morn-
ing, there were no notable event yester-
day.?
[Precedence(pi3,pi1),Explanation(pi3,pi1),
Cause(pi3,pi1)],
[Precedence(pi2,pi4), Explanation(pi2,pi4),
Cause(pi2,pi4)]
pi2pi1
Kesa nani-mo hoodoo-sare-nakatta-
node,
pi4pi3
kinoo-wa mebosii
ziken-wa nakatta-noda.
The temporal relation at the fact level is that pi3
precedes pi1. By contrast, that at the epistemic
level is that pi2 precedes pi4. By describing the
relation between pi1 and pi3 and that between pi2
and pi4 separately, we can reproduce the relation-
ship at both levels.
3.5 Merits
We defined our methodology for annotating text
fragments at both the fact and epistemic levels in
parallel with temporal, causal and discourse re-
lations. Therefore, we can generate specialized
data sets that enable estimating the causality in the
fact and epistemic levels by various cues (such as
known causal relations, truth condition, conjunc-
tions and temporal relations between sentences or
clauses).
In addition, we can say that causal expressions
without causation are not in a causal relation (and
vice versa) by annotating text with both discourse
and causal relations.
4 Results
We applied our methodology to 66 sentences from
the Balanced Corpus of Contemporary Written
Japanese (BCCWJ) (Maekawa, 2008). The sen-
tences were decomposed by one annotator, and la-
bels were assigned to the decomposed segments
by two annotators. During labeling, we used the
labels presented in Section 3. Our methodology
was developed based on 96 segments (38 sen-
tences), and by using the other 100 segments (28
sentences), we evaluated the inter-annotator agree-
ment as well as the frequencies of decomposition
and times of annotation. The agreement for 196
segments generated from 28 sentences amounted
to 0.68 and was computed as follows (the kappa
coefficient for them amounted to 0.79).
Agreement = Agreed labels/Total labels
Analyzing more segments in actual text and im-
proving our methodology can lead to further im-
provement in terms of agreement.
Table 5 shows the distribution of labels into seg-
ments in our study.
label segments
Total fact epistemic
Precedence 25 14 11
Overlap 7 4 3
Subsumption 61 29 32
total 94 47 47
Cause 14 8 6
total 14 8 6
Alternation ? ? ?
Consequence 6 3 3
Elaboration 4 2 2
Narration 66 33 33
Explanation 14 7 7
Contrast 2 1 1
Commentary 94 47 47
Table 5: Distribution of labels in segments in our
study
37
We can see from Table 5 that ?Narration? was
the most frequent one, while ?Alternation? never
appeared. As s result, we can assume that frequent
relations will be separated from non-frequent rela-
tions. So far, all the relations are either frequent or
non-frequent. We should re-analyze the data with
more samples again.
When the methodology was applied to 28 sen-
tences, a total of 100 and an average of 3.57 seg-
ments were derived. This is the number of seg-
ments at both the fact and epistemic levels. With-
out dividing the fact and epistemic levels, an aver-
age of 1.79 segments were derived.
On average, 11 segments per hour were tagged
in our study. Although we should evaluate the va-
lidity after having computed the average decom-
position times, it is assumed that our methodology
is valid when focusing only on labeling.
5 Discussion
We analyzed errors in this annotation exercise.
The annotators often found difficulties in judging
temporal relations in the following two cases: (1)
the case where it was difficult to determine the
scope of the segments pairing and (2) the case
where formalization of lexical meaning is difficult.
In regard to the first case, how to divide seg-
ments sometimes affects temporal relations. In the
following example, consider the temporal relation
between the first and the second sentences.
(9) Marason-ni
marathon-DAT
syutuzyoo-sita.
participate-past.
sonohi-wa
that.day-TOP
6zi-ni
6:00-at
kisyoo-si,
get.up-past,
10zi-ni
10:00-at
totyoo-kara
Metropolitan.Government-from
syuppatu-site,
leave-past,
12zi-ni
12:00-at
kansoo-sita.
finish.running-past.
?I participated in marathon. I got up at
6:00 on that day and left the Metropolitan
Government at 10:00 and finished running
at 12:00.?
When we focus on the first segment of the sec-
ond sentnce (?I got up at 6:00?), its relation to the
first sentence appears to be ?Precedence?. How-
ever, if we consider the second and the third seg-
ments as the same segment, their relation to the
first sentence appears to be ?Subsumption?.
Therefore, we should establish clear criteria for
the segmentation. Although we currently adopts a
criterion that we chose smaller segment in unclear
cases, there still remain 9 unclear cases (tempo-
ral:5, discourse:4).
One of the reason why Kappa coefficient marks
relatively high score is that we only compare the
labels and ignore the difference in the segmenta-
tions. Criteria for deciding the segment scope in
paring segments will improve our methodology.
The second case is exemplified by the tempo-
ral relation between the subordinate clause and the
main clause in the following sentence.
(10) Migawari-no
scapegoat-GEN
tomo-o
friend-ACC
sukuu-tame-ni
to.save
hasiru-noda.
run-noda.
?I run to save my friend who is my scape-
goat.?
If we consider that the saving event only spans
over the very moment of saving, the relation be-
tween the clauses appears to be ?Precedence?.
However, if we consider that running event is a
part of the saving event, the relation between the
clauses is ?Subsumption?.
Thus, judging lexical meaning with respect to
when events start and end involves some difficul-
ties and they yield delicate cases in judging tem-
poral relations.
These problems are mutually related, and the
first problem arises when the components of a lex-
ical meaning are displayed explicitly in the sen-
tence, and the second problem arises when they
are implicit.
6 Conclusions
We analyzed and proposed our methodology based
on SDRT for building a more precise Japanese
corpus for CRE. In addition, we annotated 196
segments (66 sentences) in BCCWJ with tempo-
ral relations, discourse relations, causal relations
and fact level and epistemic level propositions and
evaluated the annotations of 100 segments (28 sen-
tences) in terms of agreement, frequencies and
times for decompositions. We reported and an-
alyzed the result and discussed problems of our
methodology.
The discrepancies of decomposition patterns
were not yet empirically compared in the present
study and will be investigated in future work.
38
References
Asher N. and Lascaridas A. 2003. Logics of Con-
versation: Studies in Natural Language Processing.
Cambridge University Press, Cambridge, UK.
Bethard S., Corvey W. and Kilingenstein S. 2008.
Building a Corpus of Temporal Causal Structure.
LREC 2008, Marrakech, Morocco.
Chin M. 1984. Tense of the predicates for clauses
of compound statement binded by conjunctive parti-
cle -?Suru-Ga? and ?Shita-Ga?, ?Suru-Node? and
?Shita-Node? etc.-. Language Teaching Research
Article.
Inui T., Inui K. and Matsumoto Y. 2005. Acquir-
ing Causal Knowledge from Text Using the Con-
nective Marker Tame. ACM Transactions on Asian
Language Information Processing (ACM-TALIP),
Vol.4, Issue 4, Special Issue on Recent Advances
in Information Processing and Access for Japanese,
435?474.
Inui T., Inui K. and Matsumoto Y. 2003. What Kinds
and Amounts of Causal Knowledge Can Be Aquired
from Text by Using Connective Markers as Clues.
The 6th International Conference on Discovery Sci-
ence (DS-2003), 180?193.
Inui T., Takamura H. and Okumura M. 2007. Latent
Variable Models for Causal Knowledge Acquisition.
Alexander Gelbukh(Ed.), Computational Linguistics
and Intelligent Text Processing, Lecture Notes in
Computer Science, 4393:85?96.
Maekawa K. 2008. Balanced Corpus of Contemporary
Written Japanese. In Proceedings of the 6th Work-
shop on Asian Language Resources (ALR), 101?
102.
Riaz M. and Girju R. 2013. Toward a Bet-
ter Understanding of Causality between Verbal
Events:Extraction and Analysis of the Causal Power
of Verb-Verb Associations. In Proceedings of the
SIGDIAL 2013 Conference, Metz, France 21?30.
Tamura S. 2012. Causal relations and epistemic per-
spectives: Studies on Japanese causal and purposive
constructions. Doctoral thesis, Kyoto University.
39
