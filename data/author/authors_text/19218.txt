Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 383?389,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Building Sentiment Lexicons for All Major Languages
Yanqing Chen
Computer Science Dept.
Stony Brook University
Stony Brook, NY 11794
cyanqing@cs.stonybrook.edu
Steven Skiena
Computer Science Dept.
Stony Brook University
Stony Brook, NY 11794
skiena@cs.stonybrook.edu
Abstract
Sentiment analysis in a multilingual
world remains a challenging problem, be-
cause developing language-specific senti-
ment lexicons is an extremely resource-
intensive process. Such lexicons remain a
scarce resource for most languages.
In this paper, we address this lexicon gap
by building high-quality sentiment lexi-
cons for 136 major languages. We in-
tegrate a variety of linguistic resources
to produce an immense knowledge graph.
By appropriately propagating from seed
words, we construct sentiment lexicons for
each component language of our graph.
Our lexicons have a polarity agreement
of 95.7% with published lexicons, while
achieving an overall coverage of 45.2%.
We demonstrate the performance of our
lexicons in an extrinsic analysis of 2,000
distinct historical figures? Wikipedia ar-
ticles on 30 languages. Despite cul-
tural difference and the intended neutrality
of Wikipedia articles, our lexicons show
an average sentiment correlation of 0.28
across all language pairs.
1 Introduction
Sentiment analysis of English texts has become a
large and active research area, with many commer-
cial applications, but the barrier of language limits
the ability to assess the sentiment of most of the
world?s population.
Although several well-regarded sentiment lexi-
cons are available in English (Esuli and Sebastiani,
2006; Liu, 2010), the same is not true for most
of the world?s languages. Indeed, our literature
search identified only 12 publicly available sen-
timent lexicons for only 5 non-English languages
(Chinese mandarin, German, Arabic, Japanese and
Italian). No doubt we missed some, but it is clear
that these resources are not widely available for
most important languages.
In this paper, we strive to produce a comprehen-
sive set of sentiment lexicons for the worlds? major
languages. We make the following contributions:
? New Sentiment Analysis Resources ? We have
generated sentiment lexicons for 136 major
languages via graph propagation which are
now publicly available
1
. We validate our own
work through other publicly available, human
annotated sentiment lexicons. Indeed, our
lexicons have polarity agreement of 95.7%
with these published lexicons, plus an over-
all coverage of 45.2%.
? Large-Scale Language Knowledge Graph
Analysis ? We have created a massive com-
prehensive knowledge graph of 7 million vo-
cabulary words from 136 languages with over
131 million semantic inter-language links,
which proves valuable when doing alignment
between definitions in different languages.
? Extrinsic Evaluation ? We elucidate the sen-
timent consistency of entities reported in dif-
ferent language editions of Wikipedia using
our propagated lexicons. In particular, we
pick 30 languages and compute sentiment
scores for 2,000 distinct historical figures.
Each language pair exhibits a Spearman sen-
timent correlation of at least 0.14, with an av-
erage correlation of 0.28 over all pairs.
The rest of this paper is organized as follows.
We review related work in Section 2. In Section
3, we describe our resource processing and de-
sign decisions. Section 4 discusses graph propaga-
tion methods to identify sentiment polarity across
languages. Section 5 evaluates our results against
1
https://sites.google.com/site/datascienceslab/projects/
383
each available human-annotated lexicon. Finally,
in Section 6 we present our extrinsic evaluation
of sentiment consistency in Wikipedia prior to our
conclusions.
2 Related Work
Sentiment analysis is an important area of NLP
with a large and growing literature. Excellent sur-
veys of the field include (Liu, 2013; Pang and Lee,
2008), establishing that rich online resources have
greatly expanded opportunities for opinion min-
ing and sentiment analysis. Godbole et al (2007)
build up an English lexicon-based sentiment anal-
ysis system to evaluate the general reputation of
entities. Taboada et al (2011) present a more so-
phisticated model by considering patterns, includ-
ing negation and repetition using adjusted weights.
Liu (2010) introduces an efficient method, at the
state of the art, for doing sentiment analysis and
subjectivity in English.
Researchers have investigated topic or domain
dependent approaches to identify opinions. Jijk-
oun et al (2010) focus on generating topic spe-
cific sentiment lexicons. Li et al (2010) extract
sentiment with global and local topic dependency.
Gindl et al (2010) perform sentiment analysis ac-
cording to cross-domain contextualization and Pak
and Paroubek (2010) focus on Twitter, doing re-
search on colloquial format of English.
Work has been done to generalize sentiment
analysis to other languages. Denecke (2008) per-
forms multilingual sentiment analysis using Sen-
tiWordNet. Mihalcea et al (2007) learn multi-
lingual subjectivity via cross-lingual projections.
Abbasi et al (2008) extract specific language fea-
tures of Arabic which requires language-specific
knowledge. G??nsc?a et al (2011) work on better
sentiment analysis system in Romanian.
The ready availability of machine translation to
and from English has prompted efforts to employ
translation for sentiment analysis (Bautin et al,
2008). Banea et al (2008) demonstrate that ma-
chine translation can perform quite well when ex-
tending the subjectivity analysis to multi-lingual
environment, which makes it inspiring to replicate
their work on lexicon-based sentiment analysis.
Machine learning approaches to sentiment anal-
ysis are attractive, because of the promise of re-
duced manual processing. Boiy and Moens (2009)
conduct machine learning sentiment analysis us-
ing multilingual web texts. Deep learning ap-
proaches draft off of distributed word embedding
which offer concise features reflecting the seman-
tics of the underlying vocabulary. Turian et al
(2010) create powerful word embedding by train-
ing on real and corrupted phrases, optimizing for
the replaceability of words. Zou et al (2013) com-
bine machine translation and word representation
to generate bilingual language resources. Socher
et al (2012) demonstrates a powerful approach to
English sentiment using word embedding, which
can easily be extended to other languages by train-
ing on appropriate text corpora.
3 Knowledge Graph Construction
In this section we will describe how we leverage
off a variety of NLP resources to construct the se-
mantic connection graph we will use to propagate
sentiment lexicons.
Figure 1: Illustration of our knowledge graph,
showing links between words and edge represen-
tation to preserve source identity. For each edge
between corresponding words, a 5-bit integer will
record the existence of 5 possible semantic links.
The Polyglot project (Al-Rfou et al, 2013)
identified the 100,000 most frequently used words
in each language?s Wikipedia. Drawing a can-
didate lexicon from Wikipedia has some down-
sides (e.g. limited use of informal words), but is
representative and convenient over a large num-
ber of languages. In particular, we collect total
of 7,741,544 high-frequency words from 136 lan-
guages to serve as vertices in our graph.
We seek to identify as many semantic links
across languages as possible to connect our net-
work, and so integrated several resources:
? Wiktionary ? This growing resource has en-
384
tries for 171 languages, edited by people
with sufficient background knowledge. Wik-
tionary provides about 19.7% of the total
links covering 382,754 vertices in our graph.
? Machine Translation - We script the Google
translation API to get even more semantic
links. In particular we ask for translations of
each word in our English vocabulary to 57
languages with available translators as well
as going from each known vocabulary word
in other languages to English. In total, ma-
chine translation provides 53.2% of the to-
tal links and establishes connections between
3.5 million vertices.
? Transliteration Links ? Natural flow brings
words across languages with little morpho-
logical change. Closely related language
pairs (i.e. Russian and Ukrainian) share many
characters/words in common. Though not al-
ways true, words with same spelling usually
have similar meanings so this can improve
the coverage of semantic links. Translitera-
tion provides 22.1% of the total links in our
experiment.
? WordNet ? Finally, we gather synonyms and
antonyms of English words from WordNet,
which prove particularly useful in propagat-
ing sentiment across languages. In total we
collect over 100,000 pairs of synonyms and
antonyms and created 5.0% of the total links.
Links do not always agree in a bidirectional
manner, particularly for multi-sense words, thus
all links in our network are unidirectional. Figure
1 illustrates how we encode links from different
resources in an integer edge value.
4 Graph Propagation
Sentiment propagation starts from English senti-
ment lexicons. Through semantic links in our
knowledge graph, words are able to extend their
sentiment polarities to adjacent neighbors. We
experimented with both graph propagation algo-
rithm (Velikovich et al, 2010) and label propaga-
tion algorithm (Zhu and Ghahramani, 2002; Rao
and Ravichandran, 2009). The primary differ-
ence between is that label propagation takes multi-
ple paths between two vertices into consideration,
while graph propagation utilizes only the best path
between word pairs.
We report results from using Liu?s lexicons
(Liu, 2010) as seed words. Liu?s lexicons con-
tain 2006 positive words and 4783 negative words.
Of these, 1422 positive words and 2956 negative
words (roughly 64.5%) appear among the 100,000
English vertices in our graph.
Dataset Propagation Acc Cov
Arabic
Label 0.93 0.45
Graph 0.94 0.46
German
Label 0.97 0.31
Graph 0.97 0.32
English
Label 0.92 0.55
Graph 0.90 0.69
Italian
Label 0.73 0.29
Graph 0.72 0.32
Japanese
Label 0.57 0.12
Graph 0.56 0.15
Chinese-1
Label 0.95 0.62
Graph 0.94 0.65
Chinese-2
Label 0.97 0.70
Graph 0.97 0.72
Table 1: Graph propagation vs label propagation.
Acc represents the ratio of identical polarity be-
tween our analysis and the published lexicons.
Cov reflects what faction of our lexicons overlap
with published lexicons.
Our knowledge network is comprised of links
from a heterogeneous collection of sources, of dif-
ferent coverage and reliability. For the task of de-
ciding sentiment polarity of words, only antonym
links are negative. An edge gains zero weight
if both negative and positive links exist. Edges
having multiple positive links will be credited the
highest weight among all these links. We con-
ducted a grid search on the weight of each type of
links to maximize the best overall accuracy on our
test data of published non-English sentiment lexi-
cons. To avoid potential overfitting problems, grid
search starts from SentiWordNet English lexicons
(Esuli and Sebastiani, 2006) instead of Liu?s.
5 Lexicon Evaluation
We collected all available published sentiment lex-
icons from non-English languages to serve as stan-
dard for our evaluation, including Arabic, Italian,
German and Chinese. Coupled with English senti-
ment lexicons provides in total seven different test
cases to experiment against, specifically:
385
Language ?lexicon? +/- Ratio Language ?lexicon? +/- Ratio Language ?lexicon? +/- Ratio
Afrikaans 2299 0.40 Albanian 2076 0.41 Amharic 46 0.63
Arabic 2794 0.41 Aragonese 97 0.47 Armenian 1657 0.43
Assamese 493 0.49 Azerbaijani 1979 0.41 Bashkir 19 0.63
Basque 1979 0.40 Belarusian 1526 0.43 Bengali 2393 0.42
Bosnian 2020 0.42 Breton 184 0.42 Bulgarian 2847 0.40
Burmese 461 0.48 Catalan 3204 0.37 Cebuano 56 0.54
Chechen 26 0.65 Chinese 3828 0.34 Chuvash 17 0.76
Croatian 2208 0.40 Czech 2599 0.41 Danish 3340 0.38
Divehi 67 0.67 Dutch 3976 0.38 English 4376 0.32
Esperanto 2604 0.40 Estonian 2105 0.41 Faroese 123 0.43
Finnish 3295 0.40 French 4653 0.35 Frisian 224 0.43
Gaelic 345 0.50 Galician 2714 0.37 German 3974 0.38
Georgian 2202 0.40 Greek 2703 0.39 Gujarati 2145 0.44
Haitian 472 0.44 Hebrew 2533 0.36 Hindi 3640 0.39
Hungarian 3522 0.38 Icelandic 1770 0.40 Ido 183 0.49
Interlingua 326 0.50 Indonesian 2900 0.37 Italian 4491 0.36
Irish 1073 0.45 Japanese 1017 0.39 Javanese 168 0.51
Kazakh 81 0.65 Kannada 2173 0.42 Kirghiz 246 0.49
Khmer 956 0.49 Korean 2118 0.42 Kurdish 145 0.48
Latin 2033 0.46 Latvian 1938 0.42 Limburgish 93 0.46
Lithuanian 2190 0.41 Luxembourg 224 0.52 Macedonian 2965 0.39
Malagasy 48 0.54 Malayalam 393 0.50 Malay 2934 0.39
Maltese 863 0.50 Marathi 1825 0.48 Manx 90 0.51
Mongolian 130 0.52 Nepali 504 0.49 Norwegian 3089 0.37
Nynorsk 1894 0.39 Occitan 429 0.40 Oriya 360 0.51
Ossetic 12 0.67 Panjabi 79 0.63 Pashto 198 0.50
Persian 2477 0.39 Polish 3533 0.39 Portuguese 3953 0.35
Quechua 47 0.55 Romansh 116 0.48 Romanian 3329 0.39
Russian 2914 0.43 Sanskrit 178 0.59 Sami 24 0.71
Serbian 2034 0.41 Sinhala 1122 0.43 Slovak 2428 0.43
Slovene 2244 0.42 Spanish 4275 0.36 Sundanese 476 0.50
Swahili 1314 0.42 Swedish 3722 0.39 Tamil 2057 0.40
Tagalog 1858 0.44 Tajik 97 0.62 Tatar 76 0.50
Telugu 2523 0.41 Thai 1279 0.51 Tibetan 24 0.63
Turkmen 78 0.56 Turkish 2500 0.39 Uighur 18 0.44
Ukrainian 2827 0.41 Urdu 1347 0.39 Uzbek 111 0.57
Vietnamese 1016 0.38 Volapuk 43 0.70 Walloon 193 0.32
Welsh 1647 0.42 Yiddish 395 0.43 Yoruba 276 0.50
Table 2: Sentiment lexicon statistics. We tag 10 languages having most/least sentiment words with
blue/green color and 10 languages having highest/lowest ratio of positive words with orange/purple color.
? Arabic: (Abdul-Mageed et al, 2011).
? German: (Remus et al, 2010).
? English: (Esuli and Sebastiani, 2006).
? Italian: (Basile and Nissim, 2013).
? Japanese: (Kaji and Kitsuregawa, 2007).
? Chinese-1, Chinese-2: (He et al, 2010).
We present the accuracy and coverage achieved
by two propagation model in Table 1. Both mod-
els achieve similar accuracy while slightly more
words in graph propagation can be verified via
published lexicons. Performance is not good on
Japanese because of mismatching between our
dictionary and the test data.
Table 2 reveals that very sparse sentiment lex-
icons resulted for a small but notable fraction of
the languages we analyzed. In particular, only 20
languages yielded lexicons of less than 100 words.
Without exception, they all have very small avail-
able definitions in Wikitionary. By contrast, 48
languages had lexicons with over 2,000 words, an-
other 16 with between 1,000 and 2,000: clearly
large enough to perform a meaningful analysis.
6 Extrinsic Evaluation: Consistency of
Wikipedia Sentiment
We consider evaluating our lexicons on the con-
sistency of Wikipedia pages about a particular in-
dividual person among various languages. As
our candidate entities for analysis, we use the
Wikipedia pages of 2,000 most significant peo-
ple as measured in the recent book Who?s Bigger?
(Skiena and Ward, 2013). The sentiment polar-
ity of a page is simply computed by subtracting
386
Type Person Z-score distribution
Good
Leonardo da Vinci
Steven Spielberg
Bad
Adolf Hitler
Osama bin Laden
Table 3: Z-score distribution examples. We label 10 languages with their language code and other using
tick marks on the x-axis.
the number of negative words from that of posi-
tive words, divided by the sum of both.
The differing ratio of positive and negative po-
larity terms in Table 2 means that sentiment cannot
be directly compared across languages. For more
consistent evaluation we compute the z-score of
each entity against the distribution of all its lan-
guage?s entities.
We use the Spearman correlation coefficient to
measure the consistence of sentiment distribution
across all entities with pages in a particular lan-
guage pair. Figure 2 shows the results for 30 lan-
guages with largest propagated sentiment lexicon
size. All pairs of language exhibit positive corre-
lation (and hence generally stable and consistent
sentiment), with an average correlation of 0.28.
Finally, Table 3 illustrates sentiment consis-
tency over all 136 languages (represented by blue
tick marks), with the first 10 languages in Figure 2
granted labels. Respected artists like Steven Spiel-
berg and Leonardo da Vinci show as consistently
positive sentiment as notorious figures like Osama
bin Laden and Adolf Hitler are negative.
7 Conclusions
Our knowledge graph propagation is generally ef-
fective at producing useful sentiment lexicons. In-
terestingly, the ratio of positive sentiment words
is strongly connected with number of sentiment
words ? it is noteworthy that English has the
smallest ratio of positive lexicon terms. The
Figure 2: Heatmap of sentiment correlation be-
tween 30 languages.
phenomenon possibly shows that many negative
words reflecting cultural nuances do not translate
wel. We believe that this ratio can be consid-
ered as quality measurement of the propagation.
Similar approaches can be extended to other NLP
tasks using different semantic links, specific dic-
tionary and special seed words. Future work will
revolve around learning modifiers, negation terms,
and various entity/sentiment attribution.
Acknowledgments
This research was partially supported by NSF
Grants DBI-1060572 and IIS-1017181, and a
Google Faculty Research Award.
387
References
Ahmed Abbasi, Hsinchun Chen, and Arab Salem.
2008. Sentiment analysis in multiple languages:
Feature selection for opinion classification in web
forums. ACM Transactions on Information Systems
(TOIS), 26(3):12.
Muhammad Abdul-Mageed, Mona T Diab, and Mo-
hammed Korayem. 2011. Subjectivity and senti-
ment analysis of modern standard arabic. In ACL
(Short Papers), pages 587?591.
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word represen-
tations for multilingual nlp. arXiv preprint
arXiv:1307.1662.
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 127?135. Association
for Computational Linguistics.
Valerio Basile and Malvina Nissim. 2013. Sentiment
analysis on italian tweets. WASSA 2013, page 100.
M. Bautin, L. Vijayarenu, and S. Skiena. 2008. In-
ternational sentiment analysis for news and blogs.
Second Int. Conf. on Weblogs and Social Media
(ICWSM 2008).
Erik Boiy and Marie-Francine Moens. 2009. A
machine learning approach to sentiment analysis
in multilingual web texts. Information retrieval,
12(5):526?558.
Kerstin Denecke. 2008. Using sentiwordnet for mul-
tilingual sentiment analysis. In Data Engineering
Workshop, 2008. ICDEW 2008. IEEE 24th Interna-
tional Conference on, pages 507?512. IEEE.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of LREC, volume 6,
pages 417?422.
Stefan Gindl, Albert Weichselbraun, and Arno Scharl.
2010. Cross-domain contextualisation of sentiment
lexicons. 19th European Conference on Artificial
Intelligence (ECAI).
Alexandru-Lucian G??nsc?a, Emanuela Boros?, Adrian
Iftene, Diana Trandab
?
At?, Mihai Toader, Marius
Cor??ci, Cenel-Augusto Perez, and Dan Cristea.
2011. Sentimatrix: multilingual sentiment analy-
sis service. In Proceedings of the 2nd Workshop
on Computational Approaches to Subjectivity and
Sentiment Analysis, pages 189?195. Association for
Computational Linguistics.
Namrata Godbole, Manja Srinivasaiah, and Steven
Skiena. 2007. Large-scale sentiment analysis for
news and blogs. ICWSM, 7.
Yulan He, Harith Alani, and Deyu Zhou. 2010. Ex-
ploring english lexicon knowledge for chinese senti-
ment analysis. CIPS-SIGHAN Joint Conference on
Chinese Language Processing.
Valentin Jijkoun, Maarten de Rijke, and Wouter
Weerkamp. 2010. Generating focused topic-
specific sentiment lexicons. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 585?594. Association for
Computational Linguistics.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing lexicon for sentiment analysis from massive col-
lection of html documents. In EMNLP-CoNLL,
pages 1075?1083.
Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010.
Sentiment analysis with global topics and local de-
pendency. In AAAI.
Bing Liu. 2010. Sentiment analysis and subjectivity.
Handbook of natural language processing, 2:568.
Bing Liu. 2013. Sentiment Analysis and Opinion Min-
ing. Morgan and Claypool.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective lan-
guage via cross-lingual projections. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 976.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In LREC.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 675?682. Association for Computational Lin-
guistics.
Robert Remus, Uwe Quasthoff, and Gerhard Heyer.
2010. Sentiws-a publicly available german-
language resource for sentiment analysis. In LREC.
Steven Skiena and Charles Ward. 2013. Who?s Big-
ger?: Where Historical Figures Really Rank. Cam-
bridge University Press.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic com-
positionality through recursive matrix-vector spaces.
In Proceedings of the 2012 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
388
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry
Hannan, and Ryan McDonald. 2010. The viability
of web-derived polarity lexicons. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 777?785. As-
sociation for Computational Linguistics.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from labeled and unlabeled data with label propa-
gation. Technical report, Technical Report CMU-
CALD-02-107, Carnegie Mellon University.
Will Y Zou, Richard Socher, Daniel Cer, and Christo-
pher D Manning. 2013. Bilingual word embeddings
for phrase-based machine translation. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1393?1398.
389
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 183?192,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Polyglot: Distributed Word Representations for Multilingual NLP
Rami Al-Rfou? Bryan Perozzi
Computer Science Dept. Stony Brook University Stony Brook, NY 11794
{ralrfou, bperozzi, skiena}@cs.stonybrook.edu
Steven Skiena
Abstract
Distributed word representations (word
embeddings) have recently contributed
to competitive performance in language
modeling and several NLP tasks. In
this work, we train word embeddings for
more than 100 languages using their cor-
responding Wikipedias. We quantitatively
demonstrate the utility of our word em-
beddings by using them as the sole fea-
tures for training a part of speech tagger
for a subset of these languages. We find
their performance to be competitive with
near state-of-art methods in English, Dan-
ish and Swedish. Moreover, we inves-
tigate the semantic features captured by
these embeddings through the proximity
of word groupings. We will release these
embeddings publicly to help researchers in
the development and enhancement of mul-
tilingual applications.
1 Introduction
Building multilingual processing systems is a
challenging task. Every NLP task involves dif-
ferent stages of preprocessing and calculating in-
termediate representations that will serve as fea-
tures for later stages. These stages vary in com-
plexity and requirements for each individual lan-
guage. Despite recent momentum towards devel-
oping multilingual tools (Nivre et al, 2007; Hajic?
et al, 2009; Pradhan et al, 2012), most of NLP
research still focuses on rich resource languages.
Common NLP systems and tools rely heavily on
English specific features and they are infrequently
tested on multiple datasets. This makes them hard
to port to new languages and tasks (Blitzer et al,
2006).
A serious bottleneck in the current approach
for developing multilingual systems is the require-
ment of familiarity with each language under con-
sideration. These systems are typically carefully
tuned with hand-manufactured features designed
by experts in a particular language. This approach
can yield good performance, but tends to create
complicated systems which have limited portabil-
ity to new languages, in addition to being hard to
enhance and maintain.
Recent advancements in unsupervised feature
learning present an intriguing alternative. In-
stead of relying on expert knowledge, these ap-
proaches employ automatically generated task-
independent features (or word embeddings) given
large amounts of plain text. Recent developments
have led to state-of-art performance in several
NLP tasks such as language modeling (Bengio
et al, 2006; Mikolov et al, 2010), and syntactic
tasks such as sequence tagging (Collobert et al,
2011). These embeddings are generated as a result
of training ?deep? architectures, and it has been
shown that such representations are well suited for
domain adaptation tasks (Glorot et al, 2011; Chen
et al, 2012).
We believe two problems have held back the
research community?s adoption of these methods.
The first is that learning representations of words
involves huge computational costs. The process
usually involves processing billions of words over
weeks. The second is that so far, these systems
have been built and tested mainly on English.
In this work we seek to remove these barriers
to entry by generating word embeddings for over
a hundred languages using state-of-the-art tech-
niques. Specifically, our contributions include:
? Word embeddings - We will release word
embeddings for the hundred and seventeen
languages that have more than 10,000 ar-
ticles on Wikipedia. Each language?s vo-
cabulary will contain up to 100,000 words.
The embeddings will be publicly available at
183
(www.cs.stonybrook.edu/?dsl), for
the research community to study their charac-
teristics and build systems for new languages.
We believe our embeddings represent a valu-
able resource because they contain a minimal
amount of normalization. For example, we
do not lower case words for European lan-
guages as other studies have done for En-
glish. This preserves features of the under-
lying language.
? Quantitative analysis - We investigate
the embedding?s performance on a part-of-
speech (PoS) tagging task, and conduct qual-
itative investigation of the syntactic and se-
mantic features they capture. Our experi-
ments represent a valuable chance to evalu-
ate distributed word representations for NLP
as the experiments are conducted in a consis-
tent manner and a large number of languages
are covered. As the embeddings capture in-
teresting linguistic features, we believe the
multilingual resource we are providing gives
researchers a chance to create multilingual
comparative experiments.
? Efficient implementation - Training these
models was made possible by our contri-
butions to Theano (machine learning library
(Bergstra et al, 2010)). These optimizations
empower researchers to produce word em-
beddings under different settings or for dif-
ferent corpora than Wikipedia.
The rest of this paper is as follows. In Section
2, we give an overview of semi-supervised learn-
ing and learning representations related work. We
then describe, in Section 3, the network used to
generate the word embeddings and its characteris-
tics. Section 4 discusses the details of the corpus
collection and preparation steps we performed.
Next, in Section 5, we discuss our experimental
setup and the training progress over time. In Sec-
tion 6 we discuss the semantic features captured
by the embeddings by showing examples of the
word groupings in multiple languages. Finally,
in Section 7 we demonstrate the quality of our
learned features by training a PoS tagger on sev-
eral languages and then conclude.
2 Related Work
There is a large body of work regarding semi-
supervised techniques which integrate unsuper-
vised feature learning with discriminative learning
methods to improve the performance of NLP ap-
plications. Word clustering has been used to learn
classes of words that have similar semantic fea-
tures to improve language modeling (Brown et al,
1992) and knowledge transfer across languages
(Ta?ckstro?m et al, 2012). Dependency parsing
and other NLP tasks have been shown to bene-
fit from such a large unannotated corpus (Koo et
al., 2008), and a variety of unsupervised feature
learning methods have been shown to unilaterally
improve the performance of supervised learning
tasks (Turian et al, 2010). (Klementiev et al,
2012) induce distributed representations for a pair
of languages jointly, where a learner can be trained
on annotations present in one language and ap-
plied to test data in another.
Learning distributed word representations is a
way to learn effective and meaningful information
about words and their usages. They are usually
generated as a side effect of training parametric
language models as probabilistic neural networks.
Training these models is slow and takes a signif-
icant amount of computational resources (Bengio
et al, 2006; Dean et al, 2012). Several sugges-
tions have been proposed to speed up the training
procedure, either by changing the model architec-
ture to exploit an algorithmic speedup (Mnih and
Hinton, 2009; Morin and Bengio, 2005) or by esti-
mating the error by sampling (Bengio and Senecal,
2008).
(Collobert and Weston, 2008) shows that word
embeddings can almost substitute NLP common
features on several tasks. The system they built,
SENNA, offers part of speech tagging, chunking,
named entity recognition, semantic role labeling
and dependency parsing (Collobert, 2011). The
system is built on top of word embeddings and per-
forms competitively compared to state of art sys-
tems. In addition to pure performance, the system
has a faster execution speed than comparable NLP
pipelines (Al-Rfou? and Skiena, 2012).
To speed up the embedding generation process,
SENNA embeddings are generated through a pro-
cedure that is different from language modeling.
The representations are acquired through a model
that distinguishes between phrases and corrupted
versions of them. In doing this, the model avoids
the need to normalize the scores across the vocab-
ulary to infer probabilities. (Chen et al, 2013)
shows that the embeddings generated by SENNA
184
Apple apple Bush bush corpora dangerous
Dell tomato Kennedy jungle notations costly
Paramount bean Roosevelt lobster digraphs chaotic
Mac onion Nixon sponge usages bizarre
Flex potato Fisher mud derivations destructive
Table 1: Words nearest neighbors as they appear in the English embeddings.
perform well in a variety of term-based evaluation
tasks. Given the training speed and prior perfor-
mance on NLP tasks in English, we generate our
multilingual embeddings using a similar network
architecture to the one SENNA used.
However, our work differs from SENNA in the
following ways. First, we do not limit our mod-
els to English, we train embeddings for a hundred
and seventeen languages. Next, we preserve lin-
guistic features by avoiding excessive normaliza-
tion to the text. For example, our English model
places ?Apple? closer to IT companies and ?ap-
ple? to fruits. More examples of linguistic fea-
tures preserved by our model are shown in Table
1. This gives us the chance to evaluate the embed-
dings performance over PoS tagging without the
need for manufactured features. Finally, we re-
lease the embeddings and the resources necessary
to generate them to the community to eliminate
any barriers.
Despite the progress made in creating dis-
tributed representations, combining them to pro-
duce meaning is still a challenging task. Sev-
eral approaches have been proposed to address
feature compositionality for semantic problems
such as paraphrase detection (Socher et al, 2011),
and sentiment analysis (Socher et al, 2012) using
word embeddings.
3 Distributed Word Representation
Distributed word representations (word embed-
dings) map the index of a word in a dictionary to a
feature vector in high-dimension space. Every di-
mension contributes to multiple concepts, and ev-
ery concept is expressed by a combination of sub-
set of dimensions. Such mapping is learned by
back-propagating the error of a task through the
model to update random initialized embeddings.
The task is usually chosen such that examples can
be automatically generated from unlabeled data
(i.e so it is unsupervised). In case of language
modeling, the task is to predict the last word of
a phrase that consists of n words.
In our work, we start from the example con-
struction method outlined in (Bengio et al, 2009).
They train a model by requiring it to distinguish
between the original phrase and a corrupted ver-
sion of the phrase. If it does not score the
original one higher than the corrupted one (by
a margin), the model will be penalized. More
precisely, for a given sequence of words S =
[wi?n . . . wi . . . wi+n] observed in the corpus T ,
we will construct another corrupted sequence S?
by replacing the word in the middle wi with a word
wj chosen randomly from the vocabulary. The
neural network represents a function score that
scores each phrase, the model is penalized through
the hinge loss function J(T ) as shown in 1.
J(T ) = 1|T |
?
i?T
|1?score(S?)+score(S)|+ (1)
Figure 1 shows a neural network that takes a se-
quence of words with size 2n + 1 to compute a
score. First, each word is mapped through a vo-
cabulary dictionary with the size |V | to an index
that is used to index a shared matrix C with the
size |V |?M where M is the size of the vector rep-
resenting the word. Once the vectors are retrieved,
they are concatenated into one vector called pro-
jection layer P with size (2n + 1) ?M . The pro-
jection layer plays the role of an input to a hidden
layer with size |H|, the activations A of which are
calculated according to equation 3, where W1, b1
are the weights and bias of the hidden layer.
A = tanh(W1P + b1) (2)
To calculate the phrase score, a linear combina-
tion of the hidden layer activations A is computed
using W2 and b2.
score(P ) = W2A+ b2 (3)
Therefore, the five parameters that have to be
learned are W1, W2, b1, b2, C with a total number
of parameters (2n+ 1) ?M ?H +H +H + 1+
|V | ?M ?M ? (nH + |V |) .
185
CImagination
C
is
C
greater
C
than
C
detail
Score
Hidden Layer
H
C
M
|
V
|
Projection 
Layer
Figure 1: Neural network architecture. Words are
retrieved from embeddings matrix C and concate-
nated at the projection layer as an input to com-
puter the hidden layer activation. The score is
the linear combination of the activation values of
the hidden layer. The scores of two phrases are
ranked according to hinge loss to distinguish the
corrupted phrase from the original one.
4 Corpus Preparation
We have chosen to generate our word embeddings
from Wikipedia. In addition to size, there are other
desirable properties that we wish for the source of
our language model to have:
? Size and variety of languages - As of this
writing (April, 2013), 42 languages had more
than 100,000 article pages, and 117 lan-
guages had more than 10,000 article pages.
? Well studied - Wikipedia is a prolific re-
source in the literature, and has been used
for a variety of problems. Particularly,
Wikipedia is well suited for multilingual ap-
plications (Navigli and Ponzetto, 2010).
? Quality - Wikipedians strive to write arti-
cles that are readable, accurate, and consist
of good grammar.
? Openly accessible - Wikipedia is a resource
available for free use by researchers
? Growing - As technology becomes more ac-
cessible, the size and scope of the multilin-
gual Wikipedia effort continues to expand.
To process Wikipedia markup, we first extract
the text using a modified version of the Bliki en-
gine1. Next we must tokenize the text. We rely
on an OpenNLP probabilistic tokenizer whenever
possible, and default to the Unicode text segmen-
tation2 algorithm offered by Lucene when we have
no such OpenNLP model. After tokenization, we
normalize the tokens to reduce their sparsity. We
have two main normalization rules. The first re-
places digits with the symbol #, so ?1999? be-
comes ####. In the second, we remove hyphens
and brackets that appear in the middle of a token.
As an additional rule for English, we map non-
Latin characters to their unicode block groups.
In order to capture the syntactic and semantic
features of words, we must observe each word sev-
eral times in each of its valid contexts. This re-
quirement, when combined with the Zipfian dis-
tribution of words in natural language, implies that
learning a meaningful representation of a language
requires a huge amount of unstructured text. In
practice we deal with this limitation by restricting
ourselves to considering the most frequently oc-
curring tokens in each language.
Table 2 shows the size of each language corpus
in terms of tokens, number of word types and cov-
erage of text achieved by building a vocabulary out
of the most frequent 100,000 tokens, |V |. Out of
vocabulary (OOV) words are replaced with a spe-
cial token ?UNK?.
While Wikipedia has 284 language specific en-
cyclopedias, only five of them have more than a
million articles. The size drops dramatically, such
that the 42nd largest Wikipedia, Hindi, has slightly
above 100,000 articles and the 100th, Tatar, has
slightly over 16,000 articles3.
Significant Wikipedias in size have a word cov-
erage over 92% except for German, Russian, Ara-
bic and Czech which shows the effect of heavy us-
age of morphological forms in these languages on
the word usage distribution.
The highest word coverage we achieve is unsur-
prisingly for Chinese. This is expected given the
limited size vocabulary of the language - the num-
ber of entries in the Contemporary Chinese Dictio-
nary are estimated to be 65 thousand words (Shux-
iang, 2004).
1Java Wikipedia API (Bliki engine) - http://code.
google.com/p/gwtwiki/
2http://www.unicode.org/reports/tr29/
3http://meta.wikimedia.org/w/index.
php?title=List_of_Wikipedias&oldid=
5248228
186
Language Tokens Words Coverage?106 ?103
English 1,888 12,125 96.30%
German 687 9,474 91.78%
French 473 4,675 95.78%
Spanish 399 3,978 96.07%
Russian 328 5,959 90.43%
Italian 322 3,642 95.52%
Portuguese 197 2,870 95.68%
Dutch 197 3,712 93.81%
Chinese 196 423 99.67%
Swedish 101 2,707 92.36%
Czech 80 2,081 91.84%
Arabic 52 1,834 91.78%
Danish 44 1,414 93.68%
Bulgarian 39 1,114 94.35%
Slovene 30 920 94.42%
Hindi 23 702 96.25%
Table 2: Statistics of a subset of the languages pro-
cessed. The second column reports the number of
tokens found in the corpus in millions while the
third column reports the word types found in thou-
sands. The coverage indicates the percentage of
the corpus that will be matching words in a vocab-
ulary consists of the most frequent 100 thousand
words.
5 Training
For our experiments, we build a model as the one
described in Section 3 using Theano (Bergstra et
al., 2010). We choose the following parameters,
context window size 2n + 1 = 5, vocabulary
|V | = 100, 000, word embedding size M = 64,
and hidden layer size H = 32. The intuition, here,
is to maximize the relative size of the embeddings
compared to the rest of the network. This might
force the model to store the necessary information
in the embeddings matrix instead of the hidden
layer. Another benefit is that we will avoid over-
fitting on the smaller Wikipedias. Increasing the
window size or the embedding size slows down
the training speed, making it harder to converge
within a reasonable time.
The examples are generated by sweeping a win-
dow over sentences. For each sentence in the cor-
pus, all unknown words are replaced with a special
token ?UNK? and sentences are padded with ?S?,
?/S? tokens. In case the window exceeds the edges
of a sentence, the missing slots are filled with our
padding token, ?PAD?.
Figure 2: Training and test errors of the French
model after 23 days of training. We did not notice
any overfitting while training the model. The error
curves are smoother the larger the language corpus
is.
To train the model, we consider the data in mini-
batches of size 16. Every 16 examples, we es-
timate the gradient using stochastic gradient de-
scent (Bottou, 1991), and update the parameters
which contributed to the error using backpropaga-
tion (Rumelhart et al, 2002). Calculating an exact
gradient is prohibitive given that the dataset size is
in millions of examples. We calculate the devel-
opment error by sampling randomly 10000 mini-
batches from the development dataset.
For each language, we set the batch size to 16
examples, and the learning rate to be 0.1. Follow-
ing, (Collobert et al, 2011)?s advice, we divide
each layer by the fan in of that layer, and we con-
sider the embeddings layer to have a fan in of 1.
We divide the corpus to three sets, training, devel-
opment and testing with the following percentages
90, 5, 5 respectively.
One disadvantage of the approach used by (Col-
lobert et al, 2011) is that there is no clear stop-
ping criteria for the model training process. We
have noticed that after a few weeks of training,
the model?s performance reaches the point where
there is no significant decrease in the average loss
over the development set, and when this occurs we
manually stop the training. An interesting prop-
erty of this model is that we did not notice any
sign of overfitting for large Wikipedias. This could
be explained by the infinite amount of examples
we can generate by randomly choosing the re-
187
Word Translation Word Translation Word Word
Fr
en
ch
rouge red
Sp
an
ish
dentista dentist
En
gli
sh
Mumbai Bombay
juane yellow peluquero barber Chennai Madras
rose pink gineco?log gynecologist Bangalore Shanghai
blanc white camionero truck driver Kolkata Calultta
orange orange oftalmo?logo ophthalmologist Cairo Bangkok
bleu blue telegrafista telegraphist Hyderabad Hyderabad
Ar
ab
ic
Ar
ab
ic
Ge
rm
an
Jkr  thanks ??d ? two boys Eisenbahnbetrieb rail operations
?Jkr  and thanks  nA? two sons Fahrbetrieb driving
yA? greetings ??d?? two boys Reisezugverkehr passenger trains
Jkr ? thanks + diacritic Vf?? two children Fa?hrverkehr ferries
?Jkr ? and thanks + diacritic  ny? two sons Handelsverkehr Trade
?rbA hello  ntA? two daughters Schu?lerverkehr students Transport
Ru
ssi
an
Ch
ine
se
Transliteration
Ita
lia
n
????? Putin dongzhi Winter Solstice papa Pope
???????? Yanukovych chunfen Vernal Equinox Papa Pope
??????? Trotsky xiazhi Summer solstice pontefice pontiff
?????? Hitler qiufen Autumnal Equinox basileus basileus
?????? Stalin ziye Midnight canridnale cardinal
???????? Medvedev chuxi New Year?s Eve frate friar
Table 3: Examples of the nearest five neighbors of every word in several languages. Translation is
retrieved from http://translate.google.com.
placement word in the corrupted phrase. Figure
2 shows a typical learning curve of the training.
As the number of examples have been seen so far
increased both the training error and the develop-
ment error go down.
6 Qualitative Analysis
In order to understand how the embeddings space
is organized, we examine the subtle information
captured by the embeddings through investigating
the proximity of word groups. This information
has the potential to help researchers develop ap-
plications that use such semantic and syntactic in-
formation. The embeddings not only capture syn-
tactic features, as we will demonstrate in Section
4, but also demonstrate the ability to capture in-
teresting semantic information. Table 3 shows dif-
ferent words in several languages. For each word
on top of each list, we rank the vocabulary accord-
ing to their Euclidean distance from that word and
show the closest five neighboring words.
? French & Spanish - Expected groupings of
colors and professions is clearly observed.
? English - The example shows how the em-
bedding space is aware of the name change
that happened to a group of Indian cities.
?Mumbai? used to be called ?Bombay?,
?Chennai? used to be called ?Madras and
?Kolkata? used to be called ?Calcutta?. On
the other hand, ?Hyderabad? stayed at a sim-
ilar distance from both names as they point to
the same conceptual meaning.
? Arabic - The first example shows the word
?Thanks?. Despite not removing the diacrit-
ics from the text, the model learned that the
two surface forms of the word mean similar
things and, therefore, grouped them together.
In Arabic, conjunction words do not get sepa-
rated from the following word. Usually, ?and
thanks? serves as a letter signature as ?sin-
cerely? is used in English. The model learned
that both words {?and thanks?, ?thanks? }
are similar, regardless their different forms.
The second example illustrates a specific syn-
tactic morphological feature of Arabic, where
enumeration of couples has its own form.
? German - The example demonstrates that the
compositional semantics of multi-unit words
are still preserved.
? Russian - The model learned to group Rus-
sian/Soviet leaders and other figures related
to the Soviet history together.
? Chinese - The list contains three solar terms
that are part of the traditional East Asian lu-
nisolar calendars. The remaining two terms
correspond to traditional holidays that occur
at the same dates of these solar terms.
? Italian - The model learned that the lower
and upper cases of the word has similar
meaning.
7 Sequence Tagging
Here we analyze the quality of the models we have
generated. To test the quantitative performance of
the embeddings, we use them as the sole features
for a well studied NLP task, part of speech tag-
ging.
To demonstrate the capability of the learned dis-
188
Language Source Test TnTUnknown Known All
German Tiger? (Brants et al, 2002) 89.17% 98.60% 97.85% 98.10%
Bulgarian BTB? (Simov et al, 2002) 75.74% 98.33% 96.33% 97.50%
Czech PDT 2.5 (Bejc?ek et al, 2012) 71.98% 99.15% 97.13% 99.10%
Danish DDT? (Kromann, 2003) 73.03% 98.07% 96.45% 96.40%
Dutch Alpino? (Van der Beek et al, 2002) 73.47% 95.85% 93.86% 95.00%
English PennTreebank (Marcus et al, 1993) 75.97% 97.74% 97.18% 96.80%
Portuguese Sint(c)tica? (Afonso et al, 2002) 75.36% 97.71% 95.95% 96.80%
Slovene SDT? (Dz?eroski et al, 2006) 68.82% 95.17% 93.46% 94.60%
Swedish Talbanken05? (Nivre et al, 2006) 83.54% 95.77% 94.68% 94.70%
Table 4: Results of our model against several PoS datasets. The performance is measured using accuracy
over the test datasets. Third column represents the total accuracy of the tagger the former two columns
reports the accuracy over known words and OOV words (unknown). The results are compared to the
TnT tagger results reported by (Petrov et al, 2012).
?CoNLL 2006 dataset
tributed representations in extracting useful word
features, we train a PoS tagger over the subset of
languages that we were able to acquire free anno-
tated resources for. We choose our tagger for this
task to be a neural network because it has a fast
convergence rate based on our initial experiments.
The part of speech tagger has similar architec-
ture to the one used for training the embeddings.
However we have changed some of the network
parameters, specifically, we use a hidden layer of
size 300 and learning rate of 0.3. The network is
trained by minimizing the negative of the log like-
lihood of the labeled data. To tag a specific word
wi we consider a window with size 2n where n
in our experiment is equal to 2. Equation 4 shows
how we construct a feature vector F by concate-
nating (?) the embeddings of the words occurred
in the window, where C is the matrix that contains
the embeddings of the language vocabulary.
F =
i+2?
j=i?2
C[wj ] (4)
The feature vector will be fed to the network and
the error will back propagated back to the embed-
dings.
The results of this experiment are presented in
Table 4. We train and test our models on the uni-
versal tagset proposed by (Petrov et al, 2012).
This universal tagset maps each original tag in a
treebank to one out of twelve general PoS tags.
This simplifies the comparison of classifiers per-
formance across languages. We compare our re-
sults to a similar experiment conducted in their
work, where they trained a TnT tagger (Brants,
2000) on several treebanks. The TnT tagger is
based on Markov models and depends on trigram
counts observed in the labeled data. It was cho-
sen for its fast speed and (near to) state-of-the-art
accuracy, without language specific tuning.
The performance of embeddings is competitive
in general. Surprisingly, it is doing better than the
TnT tagger in English and Danish. Moreover, our
performance is so close in the case of Swedish.
This task is hard for our tagger for two reasons.
The first is that we do not add OOV words seen
during training of the tagger to our vocabulary.
The second is that all OOV words are substituted
with one representation, ?UNK? and there is no
character level information used to inform the tag-
ger about the characteristic of the OOV words.
On the other hand, the performance on the
known words is strong and consistent showing the
value of the features learned about these words
from the unsupervised stage. Although the word
coverage of German and Czech are low in the orig-
inal Wikipedia corpora (See Table 2), the features
learned are achieving great accuracy on the known
words. They both achieve above 98.5% accuracy.
It is noticeable that the Slovene model performs
the worst, under both known and unknown words
categories. It achieves only 93.46% accuracy on
the test dataset. Given that the Slovene embed-
dings were trained on the least amount of data
among all other embeddings we test here, we ex-
pect the quality to go lower for the other smaller
Wikipedias not tested here.
189
In Table 5, we present how well the vocabulary
of each language?s embeddings covered the part of
speech datasets. The datasets come from a differ-
ent domain than Wikipedia, and this is reflected in
the results.
In Table 6, we present the results of training the
same neural network part of speech tagger with-
out using our embeddings as initializations. We
found that the embeddings benefited all the lan-
guages we considered, and observed the greatest
benefit in languages which had a small number of
training examples. We believe that these results
illustrate the performance
Language % Token % WordCoverage Coverage
Bulgarian 94.58 77.70
Czech 95.37 65.61
Danish 95.41 80.03
German 94.04 60.68
English 98.06 79.73
Dutch 96.25 77.76
Portuguese 94.09 72.66
Slovene 95.33 83.67
Swedish 95.87 73.92
Table 5: Coverage statistics of the embedding?s
vocabulary on the part of speech datasets after nor-
malization. Token coverage is the raw percentage
of words which were known, while the Word cov-
erage ignores repeated words.
8 Conclusion
Distributed word representations represent a valu-
able resource for any language, but particularly for
resource-scarce languages. We have demonstrated
how word embeddings can be used as off-the-shelf
solution to reach near to state-of-art performance
over a fundamental NLP task, and we believe that
our embeddings will help researchers to develop
tools in languages with which they have no exper-
tise.
Moreover, we showed several examples of in-
teresting semantic relations expressed in the em-
beddings space that we believe will lead to inter-
esting applications and improve tasks as semantic
compositionality.
While we have only considered the properties of
word embeddings as features in this work, it has
been shown that using word embeddings in con-
junction with traditional NLP features can signifi-
Language # Training AccuracyExamples Drop
Bulgarian 200,049 -2.01%
Czech 1,239,687 -0.86%
Danish 96,581 -1.77%
German 735,826 -0.89%
English 950,561 -0.25%
Dutch 208,418 -1.37%
Portuguese 212,749 -0.91%
Slovene 27,284 -2.68%
Swedish 199,509 -0.82%
Table 6: Accuracy of randomly initialized tag-
ger compared to our results. Using the embed-
dings was generally helpful, especially in lan-
guages where we did not have many training ex-
amples. The scores presented are the best we
found for each language (languages with more re-
sources could afford to train longer before overfit-
ting).
cantly improve results on NLP tasks (Turian et al,
2010; Collobert et al, 2011). With this in mind,
we believe that the entire research community can
benefit from our release of word embeddings for
over 100 languages.
We hope that these resources will advance the
study of possible pair-wise mappings between em-
beddings of several languages and their relations.
Our future work in this area includes improving
the models by increasing the size of the context
window and their domain adaptivity through in-
corporating other sources of data. We will be
investigating better strategies for modeling OOV
words. We see improvements to OOV word han-
dling as essential to ensure robust performance of
the embeddings on real-world tasks.
Acknowledgments
This research was partially supported by NSF
Grants DBI-1060572 and IIS-1017181, with ad-
ditional support from TexelTek.
References
Susana Afonso, Eckhard Bick, Renato Haber, and Di-
ana Santos. 2002. Floresta sinta? (c) tica?: a treebank
for portuguese. In Proc. of the Third Intern. Conf. on
Language Resources and Evaluation (LREC), pages
1698?1703.
Rami Al-Rfou? and Steven Skiena. 2012. Speedread:
A fast named entity recognition pipeline. In Pro-
190
ceedings of the 24th International Conference on
Computational Linguistics (Coling 2012), pages 53?
61, Mumbai, India, December. Coling 2012 Orga-
nizing Committee.
Eduard Bejc?ek, Jarmila Panevova?, Jan Popelka, Pavel
Stran?a?k, Magda S?evc???kova?, Jan S?te?pa?nek, and
Zdene?k Z?abokrtsky?. 2012. Prague Dependency
Treebank 2.5 ? a revisited version of PDT 2.0.
In Proceedings of COLING 2012, pages 231?246,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
Yoshua Bengio and J-S Senecal. 2008. Adaptive im-
portance sampling to accelerate training of a neu-
ral probabilistic language model. Neural Networks,
IEEE Transactions on, 19(4):713?722.
Y. Bengio, H. Schwenk, J.S. Sene?cal, F. Morin, and J.L.
Gauvain. 2006. Neural probabilistic language mod-
els. Innovations in Machine Learning, pages 137?
186.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston.
2009. Curriculum learning. In International Con-
ference on Machine Learning, ICML.
James Bergstra, Olivier Breuleux, Fre?de?ric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Conference on Empirical Meth-
ods in Natural Language Processing, Sydney, Aus-
tralia.
Le?on Bottou. 1991. Stochastic gradient learning in
neural networks. In Proceedings of Neuro-N??mes
91, Nimes, France. EC2.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The tiger
treebank. In IN PROCEEDINGS OF THE WORK-
SHOP ON TREEBANKS AND LINGUISTIC THEO-
RIES, pages 24?41.
Thorsten Brants. 2000. Tnt: a statistical part-of-
speech tagger. In Proceedings of the sixth confer-
ence on Applied natural language processing, pages
224?231. Association for Computational Linguis-
tics.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Minmin Chen, Zhixiang Xu, Kilian Weinberger, and
Fei Sha. 2012. Marginalized denoising autoen-
coders for domain adaptation. In John Langford and
Joelle Pineau, editors, Proceedings of the 29th Inter-
national Conference on Machine Learning (ICML-
12), ICML ?12, pages 767?774. ACM, New York,
NY, USA, July.
Yanqing Chen, Bryan Perozzi, Rami Al-Rfou?, and
Steven Skiena. 2013. The expressive power of word
embeddings. CoRR, abs/1301.3226.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In International
Conference on Machine Learning, ICML.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493?2537,
November.
Ronan Collobert. 2011. Deep learning for efficient
discriminative parsing. In AISTATS.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen,
Matthieu Devin, Quoc Le, Mark Mao, Marc?Aurelio
Ranzato, Andrew Senior, Paul Tucker, Ke Yang, and
Andrew Ng. 2012. Large scale distributed deep net-
works. In P. Bartlett, F.C.N. Pereira, C.J.C. Burges,
L. Bottou, and K.Q. Weinberger, editors, Advances
in Neural Information Processing Systems 25, pages
1232?1240.
Sas?o Dz?eroski, Tomaz? Erjavec, Nina Ledinek, Petr Pa-
jas, Zdenek Z?abokrtsky, and Andreja Z?ele. 2006.
Towards a slovene dependency treebank. In Proc. of
the Fifth Intern. Conf. on Language Resources and
Evaluation (LREC).
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of the Twenty-eight International Confer-
ence on Machine Learning (ICML?11), volume 27,
pages 97?110, June.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of COLING
2012, pages 1459?1474, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In In Proc. ACL/HLT.
191
Matthias Trautner Kromann. 2003. The danish depen-
dency treebank and the dtag treebank tool. In Pro-
ceedings of the Second Workshop on Treebanks and
Linguistic Theories (TLT), page 217.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313?330.
T. Mikolov, M. Karafia?t, L. Burget, J. Cernocky, and
S. Khudanpur. 2010. Recurrent neural network
based language model. Proceedings of Interspeech.
Andriy Mnih and Geoffrey E Hinton. 2009. A scalable
hierarchical distributed language model. Advances
in neural information processing systems, 21:1081?
1088.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on artifi-
cial intelligence and statistics, pages 246?252.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
Babelnet: Building a very large multilingual seman-
tic network. In Proceedings of the 48th annual meet-
ing of the association for computational linguistics,
pages 216?225. Association for Computational Lin-
guistics.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006.
Talbanken05: A swedish treebank with phrase struc-
ture and dependency annotation. In Proceedings of
the fifth International Conference on Language Re-
sources and Evaluation (LREC), pages 1392?1395.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915?932, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Nicoletta Cal-
zolari (Conference Chair), Khalid Choukri, Thierry
Declerck, Mehmet Ug?ur Dog?an, Bente Maegaard,
Joseph Mariani, Jan Odijk, and Stelios Piperidis, ed-
itors, Proceedings of the Eight International Con-
ference on Language Resources and Evaluation
(LREC?12), Istanbul, Turkey, may. European Lan-
guage Resources Association (ELRA).
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
of the Sixteenth Conference on Computational Natu-
ral Language Learning (CoNLL 2012), Jeju, Korea.
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 2002. Learning representations by back-
propagating errors. Cognitive modeling, 1:213.
Lu Shuxiang. 2004. The Contemporary Chinese Dic-
tionary (Xiandai Hanyu Cidian). Commercial Press.
Kiril Simov, Petya Osenova, Milena Slavcheva,
Sia Kolkovska, Elisaveta Balabanova, Dimitar
Doikoff, Krassimira Ivanova, Er Simov, and Milen
Kouylekov. 2002. Building a linguistically inter-
preted corpus of bulgarian: the bultreebank. In In:
Proceedings of LREC 2002, Canary Islands.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Advances in
Neural Information Processing Systems 24.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic com-
positionality through recursive matrix-vector spaces.
In Proceedings of the 2012 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 477?487. Asso-
ciation for Computational Linguistics.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 384?394. Association for Com-
putational Linguistics.
Leonoor Van der Beek, Gosse Bouma, Rob Malouf,
and Gertjan Van Noord. 2002. The alpino depen-
dency treebank. Language and Computers, 45(1):8?
22.
192
