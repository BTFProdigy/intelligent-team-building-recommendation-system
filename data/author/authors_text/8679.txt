Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 22?32, Prague, June 2007. c?2007 Association for Computational Linguistics
What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA
Mengqiu Wang and Noah A. Smith and Teruko Mitamura
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{mengqiu,nasmith,teruko}@cs.cmu.edu
Abstract
This paper presents a syntax-driven ap-
proach to question answering, specifically
the answer-sentence selection problem for
short-answer questions. Rather than us-
ing syntactic features to augment exist-
ing statistical classifiers (as in previous
work), we build on the idea that ques-
tions and their (correct) answers relate to
each other via loose but predictable syntac-
tic transformations. We propose a prob-
abilistic quasi-synchronous grammar, in-
spired by one proposed for machine trans-
lation (D. Smith and Eisner, 2006), and pa-
rameterized by mixtures of a robust non-
lexical syntax/alignment model with a(n
optional) lexical-semantics-driven log-linear
model. Our model learns soft alignments as
a hidden variable in discriminative training.
Experimental results using the TREC dataset
are shown to significantly outperform strong
state-of-the-art baselines.
1 Introduction and Motivation
Open-domain question answering (QA) is a widely-
studied and fast-growing research problem. State-
of-the-art QA systems are extremely complex. They
usually take the form of a pipeline architecture,
chaining together modules that perform tasks such
as answer type analysis (identifying whether the
correct answer will be a person, location, date,
etc.), document retrieval, answer candidate extrac-
tion, and answer reranking. This architecture is so
predominant that each task listed above has evolved
into its own sub-field and is often studied and evalu-
ated independently (Shima et al, 2006).
At a high level, the QA task boils down to only
two essential steps (Echihabi andMarcu, 2003). The
first step, retrieval, narrows down the search space
from a corpus of millions of documents to a fo-
cused set of maybe a few hundred using an IR en-
gine, where efficiency and recall are the main fo-
cus. The second step, selection, assesses each can-
didate answer string proposed by the first step, and
finds the one that is most likely to be an answer
to the given question. The granularity of the tar-
get answer string varies depending on the type of
the question. For example, answers to factoid ques-
tions (e.g., Who, When, Where) are usually single
words or short phrases, while definitional questions
and other more complex question types (e.g., How,
Why) look for sentences or short passages. In this
work, we fix the granularity of an answer to a single
sentence.
Earlier work on answer selection relies only on
the surface-level text information. Two approaches
are most common: surface pattern matching, and
similarity measures on the question and answer, rep-
resented as bags of words. In the former, pat-
terns for a certain answer type are either crafted
manually (Soubbotin and Soubbotin, 2001) or ac-
quired from training examples automatically (Itty-
cheriah et al, 2001; Ravichandran et al, 2003;
Licuanan and Weischedel, 2003). In the latter,
measures like cosine-similarity are applied to (usu-
ally) bag-of-words representations of the question
and answer. Although many of these systems have
achieved very good results in TREC-style evalua-
tions, shallow methods using the bag-of-word repre-
sentation clearly have their limitations. Examples of
22
cases where the bag-of-words approach fails abound
in QA literature; here we borrow an example used by
Echihabi and Marcu (2003). The question is ?Who
is the leader of France??, and the sentence ?Henri
Hadjenberg, who is the leader of France ?s Jewish
community, endorsed ...? (note tokenization), which
is not the correct answer, matches all keywords in
the question in exactly the same order. (The cor-
rect answer is found in ?Bush later met with French
President Jacques Chirac.?)
This example illustrates two types of variation
that need to be recognized in order to connect this
question-answer pair. The first variation is the
change of the word ?leader? to its semantically re-
lated term ?president?. The second variation is the
syntactic shift from ?leader of France? to ?French
president.? It is also important to recognize that
?France? in the first sentence is modifying ?com-
munity?, and therefore ?Henri Hadjenberg? is the
?leader of ... community? rather than the ?leader of
France.? These syntactic and semantic variations oc-
cur in almost every question-answer pair, and typi-
cally they cannot be easily captured using shallow
representations. It is also worth noting that such
syntactic and semantic variations are not unique to
QA; they can be found in many other closely related
NLP tasks, motivating extensive community efforts
in syntactic and semantic processing.
Indeed, in this work, we imagine a generative
story for QA in which the question is generated
from the answer sentence through a series of syn-
tactic and semantic transformations. The same story
has been told for machine translation (Yamada and
Knight, 2001, inter alia), in which a target language
sentence (the desired output) has undergone seman-
tic transformation (word to word translation) and
syntactic transformation (syntax divergence across
languages) to generate the source language sen-
tence (noisy-channel model). Similar stories can
also be found in paraphrasing (Quirk et al, 2004;
Wu, 2005) and textual entailment (Harabagiu and
Hickl, 2006; Wu, 2005).
Our story makes use of a weighted formalism
known as quasi-synchronous grammar (hereafter,
QG), originally developed by D. Smith and Eisner
(2006) for machine translation. Unlike most syn-
chronous formalisms, QG does not posit a strict iso-
morphism between the two trees, and it provides
an elegant description for the set of local configura-
tions. In Section 2 we situate our contribution in the
context of earlier work, and we give a brief discus-
sion of quasi-synchronous grammars in Section 3.
Our version of QG, called the Jeopardy model, and
our parameter estimation method are described in
Section 4. Experimental results comparing our ap-
proach to two state-of-the-art baselines are presented
in Section 5. We discuss portability to cross-lingual
QA and other applied semantic processing tasks in
Section 6.
2 Related Work
To model the syntactic transformation process, re-
searchers in these fields?especially in machine
translation?have developed powerful grammatical
formalisms and statistical models for representing
and learning these tree-to-tree relations (Wu and
Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed,
2004; Ding and Palmer, 2005; Quirk et al, 2005;
Galley et al, 2006; Smith and Eisner, 2006, in-
ter alia). We can also observe a trend in recent work
in textual entailment that more emphasis is put on
explicit learning of the syntactic graph mapping be-
tween the entailed and entailed-by sentences (Mac-
Cartney et al, 2006).
However, relatively fewer attempts have been
made in the QA community. As pointed out by
Katz and Lin (2003), most early experiments in
QA that tried to bring in syntactic or semantic
features showed little or no improvement, and it
was often the case that performance actually de-
graded (Litkowski, 1999; Attardi et al, 2001). More
recent attempts have tried to augment the bag-of-
words representation?which, after all, is simply a
real-valued feature vector?with syntactic features.
The usual similarity measures can then be used on
the new feature representation. For example, Pun-
yakanok et al (2004) used approximate tree match-
ing and tree-edit-distance to compute a similarity
score between the question and answer parse trees.
Similarly, Shen et al (2005) experimented with de-
pendency tree kernels to compute similarity between
parse trees. Cui et al (2005) measured sentence
similarity based on similarity measures between de-
pendency paths among aligned words. They used
heuristic functions similar to mutual information to
23
assign scores to matched pairs of dependency links.
Shen and Klakow (2006) extend the idea further
through the use of log-linear models to learn a scor-
ing function for relation pairs.
Echihabi and Marcu (2003) presented a noisy-
channel approach in which they adapted the IBM
model 4 from statistical machine translation (Brown
et al, 1990; Brown et al, 1993) and applied it to QA.
Similarly, Murdock and Croft (2005) adopted a sim-
ple translation model from IBM model 1 (Brown et
al., 1990; Brown et al, 1993) and applied it to QA.
Porting the translation model to QA is not straight-
forward; it involves parse-tree pruning heuristics
(the first two deterministic steps in Echihabi and
Marcu, 2003) and also replacing the lexical trans-
lation table with a monolingual ?dictionary? which
simply encodes the identity relation. This brings us
to the question that drives this work: is there a statis-
tical translation-like model that is natural and accu-
rate for question answering? We propose Smith and
Eisner?s (2006) quasi-synchronous grammar (Sec-
tion 3) as a general solution and the Jeopardy model
(Section 4) as a specific instance.
3 Quasi-Synchronous Grammar
For a formal description of QG, we recommend
Smith and Eisner (2006). We briefly review the cen-
tral idea here. QG arose out of the empirical obser-
vation that translated sentences often have some iso-
morphic syntactic structure, but not usually in en-
tirety, and the strictness of the isomorphism may
vary across words or syntactic rules. The idea is that,
rather than a synchronous structure over the source
and target sentences, a tree over the target sentence
is modeled by a source-sentence-specific grammar
that is inspired by the source sentence?s tree.1 This
is implemented by a ?sense??really just a subset
of nodes in the source tree?attached to each gram-
mar node in the target tree. The senses define an
alignment between the trees. Because it only loosely
links the two sentences? syntactic structure, QG is
particularly well-suited for QA insofar as QA is like
?free? translation.
A concrete example that is easy to understand
is a binary quasi-synchronous context-free grammar
1Smith and Eisner also show how QG formalisms generalize
synchronous grammar formalisms.
(denoted QCFG). Let VS be the set of constituent to-
kens in the source tree. QCFG rules would take the
augmented form
?X, S1? ? ?Y, S2??Z, S3?
?X, S1? ? w
where X,Y, and Z are ordinary CFG nonterminals,
each Si ? 2VS (subsets of nodes in the source tree
to which the nonterminals align), and w is a target-
language word. QG can be made more or less ?lib-
eral? by constraining the cardinality of the Si (we
force all |Si| = 1), and by constraining the relation-
ships among the Si mentioned in a single rule. These
are called permissible ?configurations.? An example
of a strict configuration is that a target parent-child
pair must align (respectively) to a source parent-
child pair. Configurations are shown in Table 1.
Here, following Smith and Eisner (2006), we use
a weighted, quasi-synchronous dependency gram-
mar. Apart from the obvious difference in appli-
cation task, there are a few important differences
with their model. First, we are not interested in the
alignments per se; we will sum them out as a hid-
den variable when scoring a question-answer pair.
Second, our probability model includes an optional
mixture component that permits arbitrary features?
we experiment with a small set of WordNet lexical-
semantics features (see Section 4.4). Third, we ap-
ply a more discriminative training method (condi-
tional maximum likelihood estimation, Section 4.5).
4 The Jeopardy Model
Our model, informally speaking, aims to follow the
process a player of the television game show Jeop-
ardy! might follow. The player knows the answer
(or at least thinks he knows the answer) and must
quickly turn it into a question.2 The question-answer
pairs used on Jeopardy! are not precisely what we
have in mind for the real task (the questions are not
specific enough), but the syntactic transformation in-
spires our model. In this section we formally define
2A round of Jeopardy! involves a somewhat involved and
specific ?answer? presented to the competitors, and the first
competitor to hit a buzzer proposes the ?question? that leads to
the answer. For example, an answer might be, This Eastern Eu-
ropean capital is famous for defenestrations. In Jeopardy! the
players must respond with a queston: What is Prague?
24
this probability model and present the necessary al-
gorithms for parameter estimation.
4.1 Probabilistic Model
The Jeopardy model is a QG designed for QA. Let
q = ?q1, ..., qn? be a question sentence (each qi is a
word), and let a = ?a1, ..., am? be a candidate an-
swer sentence. (We will use w to denote an abstract
sequence that could be a question or an answer.) In
practice, these sequences may include other infor-
mation, such as POS, but for clarity we assume just
words in the exposition. Let A be the set of can-
didate answers under consideration. Our aim is to
choose:
a? = argmax
a?A
p(a | q) (1)
At a high level, we make three adjustments. The
first is to apply Bayes? rule, p(a | q) ? p(q |
a) ? p(a). Because A is known and is assumed to
be generated by an external extraction system, we
could use that extraction system to assign scores
(and hence, probabilities p(a)) to the candidate an-
swers. Other scores could also be used, such as
reputability of the document the answer came from,
grammaticality, etc. Here, aiming for simplicity, we
do not aim to use such information. Hence we treat
p(a) as uniform over A.3
The second adjustment adds a labeled, directed
dependency tree to the question and the answer.
The tree is produced by a state-of-the-art depen-
dency parser (McDonald et al, 2005) trained on
the Wall Street Journal Penn Treebank (Marcus et
al., 1993). A dependency tree on a sequence w =
?w1, ..., wk? is a mapping of indices of words to in-
dices of their syntactic parents and a label for the
syntactic relation, ? : {1, ..., k} ? {0, ..., k} ? L.
Each word wi has a single parent, denoted w?(i).par .
Cycles are not permitted. w0 is taken to be the invis-
ible ?wall? symbol at the left edge of the sentence; it
has a single child (|{i : ?(i) = 0}| = 1). The label
for wi is denoted ?(i).lab.
The third adjustment involves a hidden variable
X , the alignment between question and answer
3The main motivation for modeling p(q | a) is that it is eas-
ier to model deletion of information (such as the part of the sen-
tence that answers the question) than insertion. Our QG does
not model the real-world knowledge required to fill in an an-
swer; its job is to know what answers are likely to look like,
syntactically.
words. In our model, each question-word maps to
exactly one answer-word. Let x : {1, ..., n} ?
{1, ...,m} be a mapping from indices of words in q
to indices of words in a. (It is for computational rea-
sons that we assume |x(i)| = 1; in general x could
range over subsets of {1, ...,m}.) Because we de-
fine the correspondence in this direction, note that it
is possible for multple question words to map to the
same answer word.
Why do we treat the alignmentX as a hidden vari-
able? In prior work, the alignment is assumed to be
known given the sentences, but we aim to discover
it from data. Our guide in this learning is the struc-
ture inherent in the QG: the configurations between
parent-child pairs in the question and their corre-
sponding, aligned words in the answer. The hidden
variable treatment lets us avoid commitment to any
one x mapping, making the method more robust to
noisy parses (after all, the parser is not 100% ac-
curate) and any wrong assumptions imposed by the
model (that |x(i)| = 1, for example, or that syntactic
transformations can explain the connection between
q and a at all).4
Our model, then, defines
p(q, ?q | a, ?a) =
?
x
p(q, ?q, x | a, ?a) (2)
where ?q and ?a are the question tree and answer
tree, respectively. The stochastic process defined by
our model factors cleanly into recursive steps that
derive the question from the top down. The QG de-
fines a grammar for this derivation; the grammar de-
pends on the specific answer.
Let ? iw refer to the subtree of ?w rooted at wi. The
model is defined by:
p(? iq | qi, ?q(i), x(i), ?a) = (3)
p#kids(|{j : ?q(j) = i, j < i}| | qi, left)
?p#kids(|{j : ?q(j) = i, j > i}| | qi, right)
?
?
j:?q(j)=i
m?
x(j)=0
pkid (qj , ?q(j).lab | qi, ?q(i), x(i), x(j), ?a)
?p(? jq | qj , ?q(j), x(j), ?a)
4If parsing performance is a concern, we might also treat the
question and/or answer parse trees as hidden variables, though
that makes training and testing more computationally expen-
sive.
25
Note the recursion in the last line. While the above
may be daunting, in practice it boils down only to
defining the conditional distribution pkid , since the
number of left and right children of each node need
not be modeled (the trees are assumed known)?
p#kids is included above for completeness, but in the
model applied here we do not condition it on qi and
therefore do not need to estimate it (since the trees
are fixed).
pkid defines a distribution over syntactic children
of qi and their labels, given (1) the word qi, (2) the
parent of qi, (3) the dependency relation between
qi and its parent, (4) the answer-word qi is aligned
to, (5) the answer-word the child being predicted is
aligned to, and (6) the remainder of the answer tree.
4.2 Dynamic Programming
Given q, the score for an answer is simply p(q, ?q |
a, ?a). Computing the score requires summing over
alignments and can be done efficiently by bottom-up
dynamic programming. Let S(j, `) refer to the score
of ? jq, assuming that the parent of qj , ?q(j).par , is
aligned to a`. The base case, for leaves of ?q, is:
S(j, `) = (4)
p#kids(0 | qj , left) ? p#kids(0 | qj , right)
?
m?
k=0
pkid (qj , ?q(j).lab | q?q(j) , `, k, ?a)
Note that k ranges over indices of answer-words to
be aligned to qj . The recursive case is
S(i, `) = (5)
p#kids(|{j : ?q(j) = i, j < i}| | qj , left)
?p#kids(|{j : ?q(j) = i, j > i}| | qj , right)
?
m?
k=0
pkid (qi, ?q(i).lab | q?q(i), `, k, ?a)
?
?
j:?q(j)=i
S(j, k)
Solving these equations bottom-up can be done
in O(nm2) time and O(nm) space; in practice this
is very efficient. In our experiments, computing the
value of a question-answer pair took two seconds on
average.5 We turn next to the details of pkid , the core
of the model.
4.3 Base Model
Our base model factors pkid into three conditional
multinomial distributions.
pbasekid (qi, ?q(i).lab | q?q(i), `, k, ?a) =
p(qi.pos | ak.pos) ? p(qi.ne | ak.ne)
?p(?q(i).lab | config(?q, ?a, i)) (6)
where qi.pos is question-word i?s POS label and
qi.ne is its named-entity label. config maps
question-word i, its parent, and their alignees to
a QG configuration as described in Table 1; note
that some configurations are extended with addi-
tional tree information. The base model does not
directly predict the specific words in the question?
only their parts-of-speech, named-entity labels, and
dependency relation labels. This model is very sim-
ilar to Smith and Eisner (2006).
Because we are interested in augmenting the QG
with additional lexical-semantic knowledge, we also
estimate pkid by mixing the base model with a
model that exploits WordNet (Miller et al, 1990)
lexical-semantic relations. The mixture is given by:
pkid (? | ?) = ?p
base
kid (? | ?)+(1??)p
ls
kid (? | ?) (7)
4.4 Lexical-Semantics Log-Linear Model
The lexical-semantics model plskid is defined by pre-
dicting a (nonempty) subset of the thirteen classes
for the question-side word given the identity of
its aligned answer-side word. These classes in-
clude WordNet relations: identical-word, synonym,
antonym (also extended and indirect antonym), hy-
pernym, hyponym, derived form, morphological
variation (e.g., plural form), verb group, entailment,
entailed-by, see-also, and causal relation. In ad-
dition, to capture the special importance of Wh-
words in questions, we add a special semantic re-
lation called ?q-word? between any word and any
Wh-word. This is done through a log-linear model
with one feature per relation. Multiple relations may
fire, motivating the log-linear model, which permits
?overlapping? features, and, therefore prediction of
5Experiments were run on a 64-bit machine with 2? 2.2GHz
dual-core CPUs and 4GB of memory.
26
any of the possible 213 ? 1 nonempty subsets. It
is important to note that this model assigns zero
probability to alignment of an answer-word with
any question-word that is not directly related to it
through any relation. Such words may be linked in
the mixture model, however, via pbasekid .
6
(It is worth pointing out that log-linear models
provide great flexibility in defining new features. It
is straightforward to extend the feature set to include
more domain-specific knowledge or other kinds of
morphological, syntactic, or semantic information.
Indeed, we explored some additional syntactic fea-
tures, fleshing out the configurations in Table 1 in
more detail, but did not see any interesting improve-
ments.)
parent-child Question parent-child pair align respec-
tively to answer parent-child pair. Aug-
mented with the q.-side dependency la-
bel.
child-parent Question parent-child pair align respec-
tively to answer child-parent pair. Aug-
mented with the q.-side dependency la-
bel.
grandparent-child Question parent-child pair align respec-
tively to answer grandparent-child pair.
Augmented with the q.-side dependency
label.
same node Question parent-child pair align to the
same answer-word.
siblings Question parent-child pair align to sib-
lings in the answer. Augmented with
the tree-distance between the a.-side sib-
lings.
c-command The parent of one answer-side word is
an ancestor of the other answer-side
word.
other A catch-all for all other types of config-
urations, which are permitted.
Table 1: Syntactic alignment configurations are par-
titioned into these sets for prediction under the Jeop-
ardy model.
4.5 Parameter Estimation
The parameters to be estimated for the Jeopardy
model boil down to the conditional multinomial
distributions in pbasekid , the log-linear weights in-
side of plskid , and the mixture coefficient ?.
7 Stan-
6It is to preserve that robustness property that the models are
mixed, and not combined some other way.
7In our experiments, all log-linear weights are initialized to
be 1; all multinomial distributions are initialized as uniform dis-
dard applications of log-linear models apply con-
ditional maximum likelihood estimation, which for
our case involves using an empirical distribution p?
over question-answer pairs (and their trees) to opti-
mize as follows:
max
?
?
q,?q,a,?a
p?(q, ?q,a, ?a) log p?(q, ?q | a, ?a)
? ?? ?
P
x p?(q,?q,x|a,?a)
(8)
Note the hidden variable x being summed out; that
makes the optimization problem non-convex. This
sort of problem can be solved in principle by condi-
tional variants of the Expectation-Maximization al-
gorithm (Baum et al, 1970; Dempster et al, 1977;
Meng and Rubin, 1993; Jebara and Pentland, 1999).
We use a quasi-Newton method known as L-BFGS
(Liu and Nocedal, 1989) that makes use of the gra-
dient of the above function (straightforward to com-
pute, but omitted for space).
5 Experiments
To evaluate our model, we conducted experiments
using Text REtrieval Conference (TREC) 8?13 QA
dataset.8
5.1 Experimental Setup
The TREC dataset contains questions and answer
patterns, as well as a pool of documents returned by
participating teams. Our task is the same as Pun-
yakanok et al (2004) and Cui et al (2005), where
we search for single-sentence answers to factoid
questions. We follow a similar setup to Shen and
Klakow (2006) by automatically selecting answer
candidate sentences and then comparing against a
human-judged gold standard.
We used the questions in TREC 8?12 for training
and set aside TREC 13 questions for development
(84 questions) and testing (100 questions). To gen-
erate the candidate answer set for development and
testing, we automatically selected sentences from
each question?s document pool that contains one or
more non-stopwords from the question. For gen-
erating the training candidate set, in addtion to the
sentences that contain non-stopwords from the ques-
tion, we also added sentences that contain correct
tributions; ? is initialized to be 0.1.
8We thank the organizers and NIST for making the dataset
publicly available.
27
answer pattern. Manual judgement was produced
for the entire TREC 13 set, and also for the first 100
questions from the training set TREC 8?12.9 On av-
erage, each question in the development set has 3.1
positive and 17.1 negative answers. There are 3.6
positive and 20.0 negative answers per question in
the test set.
We tokenized sentences using the standard tree-
bank tokenization script, and then we performed
part-of-speech tagging using MXPOST tagger (Rat-
naparkhi, 1996). The resulting POS-tagged sen-
tences were then parsed using MSTParser (McDon-
ald et al, 2005), trained on the entire Penn Treebank
to produce labeled dependency parse trees (we used
a coarse dependency label set that includes twelve
label types). We used BBN Identifinder (Bikel et al,
1999) for named-entity tagging.
As answers in our task are considered to be sin-
gle sentences, our evaluation differs slightly from
TREC, where an answer string (a word or phrase
like 1977 or George Bush) has to be accompanied
by a supporting document ID. As discussed by Pun-
yakanok et al (2004), the single-sentence assump-
tion does not simplify the task, since the hardest part
of answer finding is to locate the correct sentence.
From an end-user?s point of view, presenting the
sentence that contains the answer is often more in-
formative and evidential. Furthermore, although the
judgement data in our case are more labor-intensive
to obtain, we believe our evaluation method is a bet-
ter indicator than the TREC evaluation for the qual-
ity of an answer selection algorithm.
To illustrate the point, consider the example ques-
tion, ?When did James Dean die?? The correct an-
9More human-judged data are desirable, though we will ad-
dress training from noisy, automatically judged data in Sec-
tion 5.4. It is important to note that human judgement of an-
swer sentence correctness was carried out prior to any experi-
ments, and therefore is unbiased. The total number of questions
in TREC 13 is 230. We exclude from the TREC 13 set questions
that either have no correct answer candidates (27 questions), or
no incorrect answer candidates (19 questions). Any algorithm
will get the same performance on these questions, and therefore
obscures the evaluation results. 6 such questions were also ex-
cluded from the 100 manually-judged training questions, result-
ing in 94 questions for training. For computational reasons (the
cost of parsing), we also eliminated answer candidate sentences
that are longer than 40 words from the training and evaluation
set. After these data preparation steps, we have 348 positive
Q-A pairs for training, 1,415 Q-A pairs in the development set,
and 1,703 Q-A pairs in the test set.
swer as appeared in the sentence ?In 1955, actor
James Dean was killed in a two-car collision near
Cholame, Calif.? is 1955. But from the same docu-
ment, there is another sentence which also contains
1955: ?In 1955, the studio asked him to become a
technical adviser on Elia Kazan?s ?East of Eden,?
starring James Dean.? If a system missed the first
sentence but happened to have extracted 1955 from
the second one, the TREC evaluation grants it a ?cor-
rect and well-supported? point, since the document
ID matches the correct document ID?even though
the latter answer does not entail the true answer. Our
evaluation does not suffer from this problem.
We report two standard evaluation measures com-
monly used in IR and QA research: mean av-
erage precision (MAP) and mean reciprocal rank
(MRR). All results are produced using the standard
trec eval program.
5.2 Baseline Systems
We implemented two state-of-the-art answer-finding
algorithms (Cui et al, 2005; Punyakanok et al,
2004) as strong baselines for comparison. Cui et
al. (2005) is the answer-finding algorithm behind
one of the best performing systems in TREC eval-
uations. It uses a mutual information-inspired score
computed over dependency trees and a single align-
ment between them. We found the method to be brit-
tle, often not finding a score for a testing instance
because alignment was not possible. We extended
the original algorithm, allowing fuzzy word align-
ments through WordNet expansion; both results are
reported.
The second baseline is the approximate tree-
matching work by Punyakanok et al (2004). Their
algorithm measures the similarity between ?q and ?a
by computing tree edit distance. Our replication is
close to the algorithm they describe, with one subtle
difference. Punyakanok et al used answer-typing in
computing edit distance; this is not available in our
dataset (and our method does not explicitly carry out
answer-typing). Their heuristics for reformulating
questions into statements were not replicated. We
did, however, apply WordNet type-checking and ap-
proximate, penalized lexical matching. Both results
are reported.
28
development set test set
training dataset model MAP MRR MAP MRR
100 manually-judged TreeMatch 0.4074 0.4458 0.3814 0.4462
+WN 0.4328 0.4961 0.4189 0.4939
Cui et al 0.4715 0.6059 0.4350 0.5569
+WN 0.5311 0.6162 0.4271 0.5259
Jeopardy (base only) 0.5189 0.5788 0.4828 0.5571
Jeopardy 0.6812 0.7636 0.6029 0.6852
+2,293 noisy Cui et al 0.2165 0.3690 0.2833 0.4248
+WN 0.4333 0.5363 0.3811 0.4964
Jeopardy (base only) 0.5174 0.5570 0.4922 0.5732
Jeopardy 0.6683 0.7443 0.5655 0.6687
Table 2: Results on development and test sets. TreeMatch is our implementation of Punyakanok et al
(2004); +WN modifies their edit distance function using WordNet. We also report our implementation of
Cui et al (2005), along with our WordNet expansion (+WN). The Jeopardy base model and mixture with
the lexical-semantics log-linear model perform best; both are trained using conditional maximum likelihood
estimation. The top part of the table shows performance using 100 manually-annotated question examples
(questions 1?100 in TREC 8?12), and the bottom part adds noisily, automatically annotated questions 101?
2,393. Boldface marks the best score in a column and any scores in that column not significantly worse
under a a two-tailed paired t-test (p < 0.03).
5.3 Results
Evaluation results on the development and test sets
of our model in comparison with the baseline algo-
rithms are shown in Table 2. Both our model and
the model in Cui et al (2005) are trained on the
manually-judged training set (questions 1-100 from
TREC 8?12). The approximate tree matching algo-
rithm in Punyakanok et al (2004) uses fixed edit dis-
tance functions and therefore does not require train-
ing. From the table we can see that our model signif-
icantly outperforms the two baseline algorithms?
even when they are given the benefit of WordNet?
on both development and test set, and on both MRR
and MAP.
5.4 Experiments with Noisy Training Data
Although manual annotation of the remaining 2,293
training sentences? answers in TREC 8?12 was too
labor-intensive, we did experiment with a simple,
noisy automatic labeling technique. Any answer
that had at least three non-stop word types seen in
the question and contains the answer pattern defined
in the dataset was labeled as ?correct? and used in
training. The bottom part of Table 2 shows the re-
sults. Adding the noisy data hurts all methods, but
the Jeopardy model maintains its lead and consis-
tently suffers less damage than Cui et al (2005).
(The TreeMatch method of Punyakanok et al (2004)
does not use training examples.)
5.5 Summing vs. Maximizing
Unlike most previous work, our model does not try
to find a single correspondence between words in the
question and words in the answer, during training or
during testing. An alternative method might choose
the best (most probable) alignment, rather than the
sum of all alignment scores. This involves a slight
change to Equation 3, replacing the summation with
a maximization. The change could be made during
training, during testing, or both. Table 3 shows that
summing is preferable, especially during training.
6 Discussion
The key experimental result of this work is that
loose syntactic transformations are an effective way
to carry out statistical question answering.
One unique advantage of our model is the mix-
ture of a factored, multinomial-based base model
and a potentially very rich log-linear model. The
base model gives our model robustness, and the log-
29
test set
training decoding MAP MRR
? ? 0.6029 0.6852
? max 0.5822 0.6489
max ? 0.5559 0.6250
max max 0.5571 0.6365
Table 3: Experimental results on comparing sum-
ming over alignments (?) with maximizing (max)
over alignments on the test set. Boldface marks the
best score in a column and any scores in that column
not significantly worse under a a two-tailed paired t-
test (p < 0.03).
linear model allows us to throw in task- or domain-
specific features. Using a mixture gives the advan-
tage of smoothing (in the base model) without hav-
ing to normalize the log-linear model by summing
over large sets. This powerful combination leads
us to believe that our model can be easily ported
to other semantic processing tasks where modeling
syntactic and semantic transformations is the key,
such as textual entailment, paraphrasing, and cross-
lingual QA.
The traditional approach to cross-lingual QA is
that translation is either a pre-processing or post-
processing step done independently from the main
QA task. Notice that the QG formalism that we have
employed in this work was originally proposed for
machine translation. We might envision transfor-
mations that are performed together to form ques-
tions from answers (or vice versa) and to translate?
a Jeopardy! game in which bilingual players must
ask a question in a different language than that in
which the answer is posed.
7 Conclusion
We described a statistical syntax-based model that
softly aligns a question sentence with a candidate
answer sentence and returns a score. Discrimina-
tive training and a relatively straightforward, barely-
engineered feature set were used in the implementa-
tion. Our scoring model was found to greatly out-
perform two state-of-the-art baselines on an answer
selection task using the TREC dataset.
Acknowledgments
The authors acknowledge helpful input from three
anonymous reviewers, Kevin Gimpel, and David
Smith. This work is supported in part by
ARDA/DTO Advanced Question Answering for
Intelligence (AQUAINT) program award number
NBCHC040164.
References
Giuseppe Attardi, Antonio Cisternino, Francesco
Formica, Maria Simi, Alessandro Tommasi, Ellen M.
Voorhees, and D. K. Harman. 2001. Selectively using
relations to improve precision in question answering.
In Proceedings of the 10th Text REtrieval Conference
(TREC-10), Gaithersburg, MD, USA.
Leonard E. Baum, Ted Petrie, George Soules, and Nor-
man Weiss. 1970. A maximization technique occur-
ring in the statistical analysis of probabilistic functions
of Markov chains. The Annals of Mathematical Statis-
tics, 41(1):164?171.
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns whats? in
a name. Machine Learning, 34(1-3):211?231.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Frederick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16(2):79?85.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-
Seng Chua. 2005. Question answering passage re-
trieval using dependency relations. In Proceedings of
the 28th ACM-SIGIR International Conference on Re-
search and Development in Information Retrieval, Sal-
vador, Brazil.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):1?38.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43st Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Ann Arbor, MI, USA.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), Sapporo,
Japan.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Sapporo, Japan.
30
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and the 44st Annual Meeting of the
Association for Computational Linguistics (COLING-
ACL), Sydney, Australia.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics (ACL), Sapporo, Japan.
Sanda Harabagiu and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain question
answering. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
Annual Meeting of the Association for Computational
Linguistics (COLING-ACL), Sydney, Australia.
Abraham Ittycheriah, Martin Franz, and Salim Roukos.
2001. IBM?s statistical question answering system?
TREC-10. In Proceedings of the 10th Text REtrieval
Conference (TREC-10), Gaithersburg, MD, USA.
Tony Jebara and Alex Pentland. 1999. Maximum con-
ditional likelihood via bound maximization and the
CEM algorithm. In Proceedings of the 1998 Confer-
ence on Advances in Neural Information Processing
Systems II (NIPS), pages 494?500, Denver, CO, USA.
Boris Katz and Jimmy Lin. 2003. Selectively using
relations to improve precision in question answering.
In Proceedings of the EACL-2003 Workshop on Nat-
ural Language Processing for Question Answering,
Gaithersburg, MD, USA.
Jinxi Xu Ana Licuanan and Ralph Weischedel. 2003.
Trec2003 qa at bbn: Answering definitional questions.
In Proceedings of the 12th Text REtrieval Conference
(TREC-12), Gaithersburg, MD, USA.
Kenneth C. Litkowski. 1999. Question-answering us-
ing semantic relation triples. In Proceedings of the
8th Text REtrieval Conference (TREC-8), Gaithers-
burg, MD, USA.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Math. Programming, 45:503?528.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Manning.
2006. Learning to recognize features of valid textual
entailments. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL), New York, NY, USA.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the penn treebank. Computational Lin-
guistics, 19(2):313?330.
Ryan McDonald, Koby Crammer, and Fernado Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43st Annual Meeting of
the Association for Computational Linguistics (ACL),
Ann Arbor, MI, USA.
I. Dan Melamed. 2004. Algorithms for syntax-aware
statistical machine translation. In Proceedings of the
Conference on Theoretical and Methodological Issues
in Machine Translation (TMI), Baltimore, MD, USA.
Xiao-Li Meng and Donald B. Rubin. 1993. Maximum
likelihood estimation via the ECM algorithm: A gen-
eral framework. Biometrika, 80:267?278.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
WordNet: an on-line lexical database. International
Journal of Lexicography, 3(4).
Vanessa Murdock and W. Bruce Croft. 2005. A trans-
lation model for sentence retrieval. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing
(HLT-EMNLP), Vancouver, BC, USA.
Vasin Punyakanok, Dan Roth, and Wen-Tau Yih. 2004.
Mapping dependencies trees: An application to ques-
tion answering. In Proceedings of the 8th Interna-
tional Symposium on Artificial Intelligence and Math-
ematics, Fort Lauderdale, FL, USA.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Barcelona, Spain.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguistics
(ACL), Ann Arbor, MI, USA.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Philadelphia, PA, USA.
Deepak Ravichandran, Abharam Ittycheriah, and Salim
Roukos. 2003. Automatic derivation of surface text
patterns for a maximum entropy based question an-
swering system. In Proceedings of the Human Lan-
guage Technology Conference and North American
Chapter of the Association for Computational Linguis-
tics (HLT-NAACL), Edmonton, Canada.
Dan Shen and Dietrich Klakow. 2006. Exploring corre-
lation of dependency relation paths for answer extrac-
tion. In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics (COLING-ACL), Sydney, Australia.
Dan Shen, Geert-Jan M. Kruijff, and Dietrich Klakow.
2005. Exploring syntactic relation patterns for ques-
tion answering. In Proceedings of the Second Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP), Jeju Island, Republic of Korea.
Hideki Shima, Mengqiu Wang, Frank Lin, and Teruko
Mitamura. 2006. Modular approach to error analysis
and evaluation for multilingual question answering. In
Proceedings of the Fifth International Conference on
Language Resources and Evaluation (LREC), Genoa,
Italy.
31
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projection
of syntactic dependencies. In Proceedings of the HLT-
NAACL Workshop on Statistical Machine Translation,
New York, NY, USA.
Martin M. Soubbotin and Sergei M. Soubbotin. 2001.
Patterns for potential answer expressions as clues to
the right answers. In Proceedings of the 10th Text
REtrieval Conference (TREC-10), Gaithersburg, MD,
USA.
Dekai Wu and Hongsing Wong. 1998. Machine
translation with a stochastic grammatical channel.
In Proceedings of the 17th International Conference
on Computational Linguistics (COLING), Montreal,
Canada.
Dekai Wu. 2005. Recognizing paraphrases and textual
entailment using inversion transduction grammars. In
Proceedings of the ACL Workshop on Empirical Mod-
eling of Semantic Equivalence and Entailment, Ann
Arbor, MI, USA.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), Toulouse, France.
32
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 425?432,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Fast, Accurate Deterministic Parser for Chinese
Mengqiu Wang Kenji Sagae Teruko Mitamura
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
{mengqiu,sagae,teruko}@cs.cmu.edu
Abstract
We present a novel classifier-based deter-
ministic parser for Chinese constituency
parsing. Our parser computes parse trees
from bottom up in one pass, and uses
classifiers to make shift-reduce decisions.
Trained and evaluated on the standard
training and test sets, our best model (us-
ing stacked classifiers) runs in linear time
and has labeled precision and recall above
88% using gold-standard part-of-speech
tags, surpassing the best published re-
sults. Our SVM parser is 2-13 times faster
than state-of-the-art parsers, while produc-
ing more accurate results. Our Maxent
and DTree parsers run at speeds 40-270
times faster than state-of-the-art parsers,
but with 5-6% losses in accuracy.
1 Introduction and Background
Syntactic parsing is one of the most fundamental
tasks in Natural Language Processing (NLP). In
recent years, Chinese syntactic parsing has also
received a lot of attention in the NLP commu-
nity, especially since the release of large collec-
tions of annotated data such as the Penn Chi-
nese Treebank (Xue et al, 2005). Corpus-based
parsing techniques that are successful for English
have been applied extensively to Chinese. Tradi-
tional statistical approaches build models which
assign probabilities to every possible parse tree
for a sentence. Techniques such as dynamic pro-
gramming, beam-search, and best-first-search are
then employed to find the parse tree with the high-
est probability. The massively ambiguous nature
of wide-coverage statistical parsing,coupled with
cubic-time (or worse) algorithms makes this ap-
proach too slow for many practical applications.
Deterministic parsing has emerged as an attrac-
tive alternative to probabilistic parsing, offering
accuracy just below the state-of-the-art in syn-
tactic analysis of English, but running in linear
time (Sagae and Lavie, 2005; Yamada and Mat-
sumoto, 2003; Nivre and Scholz, 2004). Encour-
aging results have also been shown recently by
Cheng et al (2004; 2005) in applying determin-
istic models to Chinese dependency parsing.
We present a novel classifier-based determin-
istic parser for Chinese constituency parsing. In
our approach, which is based on the shift-reduce
parser for English reported in (Sagae and Lavie,
2005), the parsing task is transformed into a suc-
cession of classification tasks. The parser makes
one pass through the input sentence. At each parse
state, it consults a classifier to make shift/reduce
decisions. The parser then commits to a decision
and enters the next parse state. Shift/reduce deci-
sions are made deterministically based on the lo-
cal context of each parse state, and no backtrack-
ing is involved. This process can be viewed as a
greedy search where only one path in the whole
search space is considered. Our parser produces
both dependency and constituent structures, but in
this paper we will focus on constituent parsing.
By separating the classification task from the
parsing process, we can take advantage of many
machine learning techniques such as classifier en-
semble. We conducted experiments with four
different classifiers: support vector machines
(SVM), Maximum-Entropy (Maxent), Decision
Tree (DTree) and memory-based learning (MBL).
We also compared the performance of three differ-
ent classifier ensemble approaches (simple voting,
classifier stacking and meta-classifier).
Our best model (using stacked classifiers) runs
in linear time and has labeled precision and
recall above 88% using gold-standard part-of-
speech tags, surpassing the best published results
(see Section 5). Our SVM parser is 2-13 times
faster than state-of-the-art parsers, while produc-
425
ing more accurate results. Our Maxent and DTree
parsers are 40-270 times faster than state-of-the-
art parsers, but with 5-6% losses in accuracy.
2 Deterministic parsing model
Like other deterministic parsers, our parser as-
sumes input has already been segmented and
tagged with part-of-speech (POS) information
during a preprocessing step1. The main data struc-
tures used in the parsing algorithm are a queue and
a stack. The input word-POS pairs to be processed
are stored in the queue. The stack holds the partial
parse trees that are built during parsing. A parse
state is represented by the content of the stack and
queue.
The classifier makes shift/reduce decisions
based on contextual features that represent the
parse state. A shift action removes the first item
on the queue and puts it onto the stack. A reduce
action is in the form of Reduce-{Binary|Unary}-
X, where {Binary|Unary} denotes whether one or
two items are to be removed from the stack, and X
is the label of a new tree node that will be domi-
nating the removed items. Because a reduction is
either unary or binary, the resulting parse tree will
only have binary and/or unary branching nodes.
Parse trees are also lexicalized to produce de-
pendency structures. For lexicalization, we used
the same head-finding rules reported in (Bikel,
2004). With this additional information, reduce
actions are now in the form of Reduce-{Binary
|Unary}-X-Direction. The ?Direction? tag gives
information about whether to take the head-node
of the left subtree or the right subtree to be the
head of the new tree, in the case of binary reduc-
tion. A simple transformation process as described
in (Sagae and Lavie, 2005) is employed to con-
vert between arbitrary branching trees and binary
trees. This transformation breaks multi-branching
nodes down into binary-branching nodes by in-
serting temporary nodes; temporary nodes are col-
lapsed and removed when we transform a binary
tree back into a multi-branching tree.
The parsing process succeeds when all the items
in the queue have been processed and there is only
one item (the final parse tree) left on the stack.
If the classifier returns a shift action when there
are no items left on the queue, or a reduce ac-
tion when there are no items on the stack, the
1We constructed our own POS tagger based on SVM; see
Section 3.3.
parser fails. In this case, the parser simply com-
bines all the items on the stack into one IP node,
and outputs this as a partial parse. Sagae and
Lavie (2005) have shown that this algorithm has
linear time complexity, assuming that classifica-
tion takes constant time. The next example il-
lustrates the process for the input ?Y? (Brown)
6? (visits)?0 (Shanghai)? that is tagged with
the POS sequence ?NR (Proper Noun) VV (Verb)
NR (Proper Noun)?.
1. In the initial parsing state, the stack (S) is
empty, and the queue (Q) holds word and
POS tag pairs for the input sentence.
(S): Empty
(Q): NR
Y?
VV
6?
NR
?0
2. The first action item that the classifier gives
is a shift action.
(S): NR
Y?
(Q): VV
6?
NR
?0
3. The next action is a reduce-Unary-NP, which
means reducing the first item on the stack to a
NP node. Node (NRY?) becomes the head
of the new NP node and this information is
marked by brackets. The new parse state is:
(S): NP (NRY?)
NR
Y?
(Q): VV
6?
NR
?0
4. The next action is shift.
(S): NP (NRY?)
NR
Y?
VV
6?
(Q): NR
?0
5. The next action is again shift.
(S): NP (NRY?)
NR
Y?
VV
6?
NR
?0
(Q): Empty
6. The next action is reduce-Unary-NP.
(S): NP (NRY?)
NR
Y?
VV
6?
NP (NR?0)
NR
?0
(Q): Empty
7. The next action is reduce-Binary-VP-Left.
The node (VV6?) will be the head of the
426
new VP node.
(S): NP (NRY?)
NR
Y?
VP (VV6?)
VV
6?
NP (NR?0)
NR
?0
(Q): Empty
8. The next action is reduce-Binary-IP-Right.
Since after the action is performed, there will
be only one tree node(IP) left on the stack and
no items on the queue, this is the final action.
The final state is:
(S): IP (VV6?)
NP (NRY?)
NR
Y?
VP (VV6?)
VV
6?
NP (NR?0)
NR
?0
(Q): Empty
3 Classifiers and Feature Selection
Classification is the key component of our parsing
model. We conducted experiments with four dif-
ferent types of classifiers.
3.1 Classifiers
Support Vector Machine: Support Vector Ma-
chine is a discriminative classification technique
which solves the binary classification problem by
finding a hyperplane in a high dimensional space
that gives the maximum soft margin, based on
the Structural Risk Minimization Principle. We
used the TinySVM toolkit (Kudo and Matsumoto,
2000), with a degree 2 polynomial kernel. To train
a multi-class classifier, we used the one-against-all
scheme.
Maximum-Entropy Classifier: In a
Maximum-entropy model, the goal is to esti-
mate a set of parameters that would maximize
the entropy over distributions that satisfy certain
constraints. These constraints will force the model
to best account for the training data (Ratnaparkhi,
1999). Maximum-entropy models have been used
for Chinese character-based parsing (Fung et al,
2004; Luo, 2003) and POS tagging (Ng and Low,
2004). In our experiments, we used Le?s Maxent
toolkit (Zhang, 2004). This implementation uses
the Limited-Memory Variable Metric method for
parameter estimation. We trained all our models
using 300 iterations with no event cut-off, and
a Gaussian prior smoothing value of 2. Maxent
classifiers output not only a single class label, but
also a number of possible class labels and their
associated probability estimate.
Decision Tree Classifier: Statistical decision
tree is a classic machine learning technique that
has been extensively applied to NLP. For exam-
ple, decision trees were used in the SPATTER sys-
tem (Magerman, 1994) to assign probability dis-
tribution over the space of possible parse trees.
In our experiment, we used the C4.5 decision
tree classifier, and ignored lexical features whose
counts were less than 7.
Memory-Based Learning: Memory-Based
Learning approaches the classification problem
by storing training examples explicitly in mem-
ory, and classifying the current case by finding
the most similar stored cases (using k-nearest-
neighbors). We used the TiMBL toolkit (Daele-
mans et al, 2004) in our experiment, with k = 5.
3.2 Feature selection
For each parse state, a set of features are
extracted and fed to each classifier. Fea-
tures are distributionally-derived or linguistically-
based, and carry the context of a particular parse
state. When input to the classifier, each feature is
treated as a contextual predicate which maps an
outcome and a context to true, false value.
The specific features used with the classifiers
are listed in Table 1.
Sun and Jurafsky (2003) studied the distribu-
tional property of rhythm in Chinese, and used the
rhythmic feature to augment a PCFG model for
a practical shallow parsing task. This feature has
the value 1, 2 or 3 for monosyllabic, bi-syllabic or
multi-syllabic nouns or verbs. For noun and verb
phrases, the feature is defined as the number of
words in the phrase. Sun and Jurafsky found that
in NP and VP constructions there are strong con-
straints on the word length for verbs and nouns
(a kind of rhythm), and on the number of words
in a constituent. We employed these same rhyth-
mic features to see whether this property holds for
the Penn Chinese Treebank data, and if it helps in
the disambiguation of phrase types. Experiments
show that this feature does increase classification
accuracy of the SVM model by about 1%.
In both Chinese and English, there are punctu-
ation characters that come in pairs (e.g., parenthe-
ses). In Chinese, such pairs are more frequent
(quotes, single quotes, and book-name marks).
During parsing, we note how many opening punc-
427
1 A Boolean feature indicates if a closing punctuation is expected or not.
2 A Boolean value indicates if the queue is empty or not.
3 A Boolean feature indicates whether there is a comma separating S(1) and S(2) or not.
4 Last action given by the classifier, and number of words in S(1) and S(2).
5 Headword and its POS of S(1), S(2), S(3) and S(4), and word and POS of Q(1), Q(2), Q(3) and Q(4).
6 Nonterminal label of the root of S(1) and S(2), and number of punctuations in S(1) and S(2).
7 Rhythmic features and the linear distance between the head-words of the S(1) and S(2).
8 Number of words found so far to be dependents of the head-words of S(1) and S(2).
9 Nonterminal label, POS and headword of the immediate left and right child of the root of S(1) and S(2).
10 Most recently found word and POS pair that is to the left of the head-word of S(1) and S(2).
11 Most recently found word and POS pair that is to the right of the head-word of S(1) and S(2).
Table 1: Features for classification
tuations we have seen on the stack. If the number
is odd, then feature 2 will have value 1, otherwise
0. A boolean feature is used to indicate whether or
not an odd number of opening punctuations have
been seen and a closing punctuation is expected;
in this case the feature gives a strong hint to the
parser that all the items in the queue before the
closing punctuation, and the items on the stack
after the opening punctuation should be under a
common constituent node which begins and ends
with the two punctuations.
3.3 POS tagging
In our parsing model, POS tagging is treated as
a separate problem and it is assumed that the in-
put has already been tagged with POS. To com-
pare with previously published work, we evaluated
the parser performance on automatically tagged
data. We constructed a simple POS tagger using
an SVM classifier. The tagger makes two passes
over the input sentence. The first pass extracts fea-
tures from the two words and POS tags that came
before the current word, the two words follow-
ing the current word, and the current word itself
(the length of the word, whether the word con-
tains numbers, special symbols that separates for-
eign first and last names, common Chinese family
names, western alphabets or dates). Then the tag
is assigned to the word according to SVM classi-
fier?s output. In the second pass, additional fea-
tures such as the POS tags of the two words fol-
lowing the current word, and the POS tag of the
current word (assigned in the first pass) are used.
This tagger had a measured precision of 92.5% for
sentences ? 40 words.
4 Experiments
We performed experiments using the Penn Chi-
nese Treebank. Sections 001-270 (3484 sentences,
84,873 words) were used for training, 271-300
(348 sentences, 7980 words) for development, and
271-300 (348 sentences, 7980 words) for testing.
The whole dataset contains 99629 words, which is
about 1/10 of the size of the English Penn Tree-
bank. Standard corpus preparation steps were
done prior to parsing, so that empty nodes were
removed, and the resulting A over A unary rewrite
nodes are collapsed. Functional labels of the non-
terminal nodes are also removed, but we did not
relabel the punctuations, unlike in (Jiang, 2004).
Bracket scoring was done by the EVALB pro-
gram2, and preterminals were not counted as con-
stituents. In all our experiments, we used labeled
recall (LR), labeled precision (LP) and F1 score
(harmonic mean of LR and LP) as our evaluation
metrics.
4.1 Results of different classifiers
Table 2 shows the classification accuracy and pars-
ing accuracy of the four different classifiers on the
development set for sentences ? 40 words, with
gold-standard POS tagging. The runtime (Time)
of each model and number of failed parses (Fail)
are also shown.
Classification Parsing Accuracy
Model Accuracy LR LP F1 Fail Time
SVM 94.3% 86.9% 87.9% 87.4% 0 3m 19s
Maxent 92.6% 84.1% 85.2% 84.6% 5 0m 21s
DTree1 92.0% 78.8% 80.3% 79.5% 42 0m 12s
DTree2 N/A 81.6% 83.6% 82.6% 30 0m 18s
MBL 90.6% 74.3% 75.2% 74.7% 2 16m 11s
Table 2: Comparison of different classifier mod-
els? parsing accuracies on development set for sen-
tences ? 40 words, with gold-standard POS
For the DTree learner, we experimented with
two different classification strategies. In our first
approach, the classification is done in a single
stage (DTree1). The learner is trained for a multi-
2http://nlp.cs.nyu.edu/evalb/
428
class classification problem where the class labels
include shift and all possible reduce actions. But
this approach yielded a lot of parse failures (42 out
of 350 sentences failed during parsing, and par-
tial parse tree was returned). These failures were
mostly due to false shift actions in cases where
the queue is empty. To alleviate this problem, we
broke the classification process down to two stages
(DTree2). A first stage classifier makes a binary
decision on whether the action is shift or reduce.
If the output is reduce, a second-stage classifier de-
cides which reduce action to take. Results showed
that breaking down the classification task into two
stages increased overall accuracy, and the number
of failures was reduced to 30.
The SVM model achieved the highest classifi-
cation accuracy and the best parsing results. It
also successfully parsed all sentences. The Max-
ent model?s classification error rate (7.4%) was
30% higher than the error rate of the SVM model
(5.7%), and its F1 (84.6%) was 3.2% lower than
SVM model?s F1 (87.4%). But Maxent model was
about 9.5 times faster than the SVM model. The
DTree classifier achieved 81.6% LR and 83.6%
LP. The MBL model did not perform well; al-
though MBL and SVM differed in accuracy by
only about 3 percent, the parsing results showed
a difference of more than 10 percent. One pos-
sible explanation for the poor performance of
the MBL model is that all the features we used
were binary features, and memory-based learner
is known to work better with multivalue features
than binary features in natural language learning
tasks (van den Bosch and Zavrel, 2000).
In terms of speed and accuracy trade-off, there
is a 5.5% trade-off in F1 (relative to SVM?s F1)
for a roughly 14 times speed-up between SVM
and two-stage DTree. Maxent is more balanced
in the sense that its accuracy was slightly lower
(3.2%) than SVM, and was just about as fast as the
two-stage DTree on the development set. The high
speed of the DTree and Maxent models make them
very attractive in applications where speed is more
critical than accuracy. While the SVM model
takes more CPU time, we show in Section 5 that
when compared to existing parsers, SVM achieves
about the same or higher accuracy but is at least
twice as fast.
Using gold-standard POS tagging, the best clas-
sifier model (SVM) achieved LR of 87.2% and LP
of 88.3%, as shown in Table 4. Both measures sur-
pass the previously known best results on parsing
using gold-standard tagging. We also tested the
SVM model using data automatically tagged by
our POS tagger, and it achieved LR of 78.1% and
LP of 81.1% for sentences ? 40 words, as shown
in Table 3.
4.2 Classifier Ensemble Experiments
Classifier ensemble by itself has been a fruitful
research direction in machine learning in recent
years. The basic idea in classifier ensemble is
that combining multiple classifiers can often give
significantly better results than any single classi-
fier alone. We experimented with three different
classifier ensemble strategies: classifier stacking,
meta-classifier, and simple voting.
Using the SVM classifier?s results as a baseline,
we tested these approaches on the development
set. In classifier stacking, we collect the outputs
from Maxent, DTree and TiMBL, which are all
trained on a separate dataset from the training set
(section 400-650 of the Penn Chinese Treebank,
smaller than the original training set). We use their
classification output as features, in addition to the
original feature set, to train a new SVM model
on the original training set. We achieved LR of
90.3% and LP of 90.5% on the development set,
a 3.4% and 2.6% improvement in LR and LP, re-
spectively. When tested on the test set, we gained
1% improvement in F1 when gold-standard POS
tagging is used. When tested with automatic tag-
ging, we achieved a 0.5% improvement in F1. Us-
ing Bikel?s significant tester with 10000 times ran-
dom shuffle, the p-value for LR and LP are 0.008
and 0.457, respectively. The increase in recall
is statistically significant, and it shows classifier
stacking can improve performance.
On the other hand, we did not find meta-
classification and simple voting very effective. In
simple voting, we make the classifiers to vote in
each step for every parse action. The F1 of sim-
ple voting method is downgraded by 5.9% rela-
tive to SVM model?s F1. By analyzing the inter-
agreement among classifiers, we found that there
were no cases where Maxent?s top output and
DTree?s output were both correct and SVM?s out-
put was wrong. Using the top output from Maxent
and DTree directly does not seem to be comple-
mentary to SVM.
In the meta-classifier approach, we first col-
lect the output from each classifier trained on sec-
429
MODEL ? 40 words ? 100 words UnlimitedLR LP F1 POS LR LP F1 POS LR LP F1 POS
Bikel & Chiang 2000 76.8% 77.8% 77.3% - 73.3% 74.6% 74.0% - - - - -
Levy & Manning 2003 79.2% 78.4% 78.8% - - - - - - - - -
Xiong et al 2005 78.7% 80.1% 79.4% - - - - - - - - -
Bikel?s Thesis 2004 78.0% 81.2% 79.6% - 74.4% 78.5% 76.4% - - - - -
Chiang & Bikel 2002 78.8% 81.1% 79.9% - 75.2% 78.0% 76.6% - - - - -
Jiang?s Thesis 2004 80.1% 82.0% 81.1% 92.4% - - - - - - - -
Sun & Jurafsky 2004 85.5% 86.4% 85.9% - - - - - 83.3% 82.2% 82.7% -
DTree model 71.8% 76.9% 74.4% 92.5% 69.2% 74.5% 71.9% 92.2% 68.7% 74.2% 71.5% 92.1%
SVM model 78.1% 81.1% 79.6% 92.5% 75.5% 78.5% 77.0% 92.2% 75.0% 78.0% 76.5% 92.1%
Stacked classifier model 79.2% 81.1% 80.1% 92.5% 76.7% 78.4% 77.5% 92.2% 76.2% 78.0% 77.1% 92.1%
Table 3: Comparison with related work on the test set using automatically generated POS
tion 1-210 (roughly 3/4 of the entire training set).
Then specifically for Maxent, we collected the top
output as well as its associated probability esti-
mate. Then we used the outputs and probabil-
ity estimate as features to train an SVM classifier
that makes a decision on which classifier to pick.
Meta-classifier results did not change at all from
our baseline. In fact, the meta-classifier always
picked SVM as its output. This agrees with our
observation for the simple voting case.
5 Comparison with Related Work
Bikel and Chiang (2000) constructed two parsers
using a lexicalized PCFG model that is based on
Collins? model 2 (Collins, 1999), and a statisti-
cal Tree-adjoining Grammar(TAG) model. They
used the same train/development/test split, and
achieved LR/LP of 76.8%/77.8%. In Bikel?s the-
sis (2004), the same Collins emulation model
was used, but with tweaked head-finding rules.
Also a POS tagger was used for assigning tags
for unseen words. The refined model achieved
LR/LP of 78.0%/81.2%. Chiang and Bikel (2002)
used inside-outside unsupervised learning algo-
rithm to augment the rules for finding heads, and
achieved an improved LR/LP of 78.8%/81.1%.
Levy and Manning (2003) used a factored model
that combines an unlexicalized PCFG model with
a dependency model. They achieved LR/LP
of 79.2%/78.4% on a different test/development
split. Xiong et al (2005) used a similar model to
the BBN?s model in (Bikel and Chiang, 2000),
and augmented the model by semantic categori-
cal information and heuristic rules. They achieved
LR/LP of 78.7%/80.1%. Hearne and Way (2004)
used a Data-Oriented Parsing (DOP) approach
that was optimized for top-down computation.
They achieved F1 of 71.3 on a different test and
training set. Jiang (2004) reported LR/LP of
80.1%/82.0% on sentences ? 40 words (results
not available for sentences ? 100 words) by ap-
plying Collins? parser to Chinese. In Sun and
Jurafsky (2004)?s work on Chinese shallow se-
mantic parsing, they also applied Collin?s parser
to Chinese. They reported up-to-date the best
parsing performance on Chinese Treebank. They
achieved LR/LP of 85.5%/86.4% on sentences ?
40 words, and LR/LP of 83.3%/82.2% on sen-
tences ? 100 words, far surpassing all other pre-
viously reported results. Luo (2003) and Fung et
al. (2004) addressed the issue of Chinese text seg-
mentation in their work by constructing character-
based parsers. Luo integrated segmentation, POS
tagging and parsing into one maximum-entropy
framework. He achieved a F1 score of 81.4% in
parsing. But the score was achieved using 90% of
the 250K-CTB (roughly 2.5 times bigger than our
training set) for training and 10% for testing. Fung
et al(2004) also took the maximum-entropy mod-
eling approach, but augmented by transformation-
based learning. They used the standard training
and testing split. When tested with gold-standard
segmentation, they achieved a F1 score of 79.56%,
but POS-tagged words were treated as constituents
in their evaluation.
In comparison with previous work, our parser?s
accuracy is very competitive. Compared to Jiang?s
work and Sun and Jurafsky?s work, the classifier
ensemble model of our parser is lagging behind by
1% and 5.8% in F1, respectively. But compared
to all other works, our classifier stacking model
gave better or equal results for all three measures.
In particular, the classifier ensemble model and
SVM model of our parser achieved second and
third highest LP, LR and F1 for sentences ? 100
words as shown in Table 3. (Sun and Jurafsky did
not report results on sentences ? 100 words, but
it is worth noting that out of all the test sentences,
430
only 2 sentences have length > 100).
Jiang (2004) and Bikel (2004)3 also evaluated
their parsers on the test set for sentences ? 40
words, using gold-standard POS tagged input. Our
parser gives significantly better results as shown
in Table 4. The implication of this result is two-
fold. On one hand, it shows that if POS tagging
accuracy can be increased, our parser is likely to
benefit more than the other two models; on the
other hand, it also indicates that our deterministic
model is less resilient to POS errors. Further de-
tailed analysis is called for, to study the extent to
which POS tagging errors affects the deterministic
parsing model.
Model LR LP F1
Bikel?s Thesis 2004 80.9% 84.5% 82.7%
Jiang?s Thesis 2004 84.5% 88.0% 86.2%
DTree model 80.5% 83.9% 82.2%
Maxent model 81.4% 82.8% 82.1%
SVM model 87.2% 88.3% 87.8%
Stacked classifier model 88.3% 88.1% 88.2%
Table 4: Comparison with related work on the test
set for sentence ? 40 words, using gold-standard
POS
To measure efficiency, we ran two publicly
available parsers (Levy and Manning?s PCFG
parser (2003) and Bikel?s parser (2004)) on
the standard test set and compared the run-
time4. The runtime of these parsers are shown
in minute:second format in Table 5. Our SVM
model is more than 2 times faster than Levy and
Manning?s parser, and more than 13 times faster
than Bikel?s parser. Our DTree model is 40 times
faster than Levy and Manning?s parser, and 270
times faster than Bikel?s parser. Another advan-
tage of our parser is that it does not take as much
memory as these other parsers do. In fact, none
of the models except MBL takes more than 60
megabytes of memory at runtime. In compari-
son, Levy and Manning?s PCFG parser requires
more than 400 mega-bytes of memory when pars-
ing long sentences (70 words or longer).
6 Discussion and future work
One unique attraction of this deterministic pars-
ing framework is that advances in machine learn-
ing field can be directly applied to parsing, which
3Bikel?s parser used gold-standard POS tags for unseen
words only. Also, the results are obtained from a parser
trained on 250K-CTB, about 2.5 times bigger than CTB 1.0.
4All the experiments were conducted on a Pentium IV
2.4GHz machine with 2GB of RAM.
Model runtime
Bikel 54m 6s
Levy & Manning 8m 12s
Our DTree model 0m 14s
Our Maxent model 0m 24s
Our SVM model 3m 50s
Table 5: Comparison of parsing speed
opens up lots of possibilities for continuous im-
provements, both in terms of accuracy and effi-
ciency. For example, in this paper we experi-
mented with one method of simple voting. An al-
ternative way of doing simple voting is to let the
parsers vote on membership of constituents after
each parser has produced its own parse tree (Hen-
derson and Brill, 1999), instead of voting at each
step during parsing.
Our initial attempt to increase the accuracy of
the DTree model by applying boosting techniques
did not yield satisfactory results. In our exper-
iment, we implemented the AdaBoost.M1 (Fre-
und and Schapire, 1996) algorithm using re-
sampling to vary the training set distribution.
Results showed AdaBoost suffered severe over-
fitting problems and hurts accuracy greatly, even
with a small number of samples. One possible
reason for this is that our sample space is very
unbalanced across the different classes. A few
classes have lots of training examples while a large
number of classes are rare, which could raise the
chance of overfitting.
In our experiments, SVM model gave better re-
sults than the Maxent model. But it is important
to note that although the same set of features were
used in both models, a degree 2 polynomial ker-
nel was used in the SVM classifier while Maxent
only has degree 1 features. In our future work, we
will experiment with degree 2 features and L1 reg-
ularization in the Maxent model, which may give
us closer performance to the SVM model with a
much faster speed.
7 Conclusion
In this paper, we presented a novel determinis-
tic parser for Chinese constituent parsing. Us-
ing gold-standard POS tags, our best model (us-
ing stacked classifiers) runs in linear time and has
labeled recall and precision of 88.3% and 88.1%,
respectively, surpassing the best published results.
And with a trade-off of 5-6% in accuracy, our
DTree and Maxent parsers run at speeds 40-270
times faster than state-of-the-art parsers. Our re-
431
sults have shown that the deterministic parsing
framework is a viable and effective approach to
Chinese parsing. For future work, we will fur-
ther improve the speed and accuracy of our mod-
els, and apply them to more Chinese and multi-
lingual natural language applications that require
high speed and accurate parsing.
Acknowledgment
This work was supported in part by ARDA?s
AQUAINT Program. We thank Eric Nyberg for
his help during the final preparation of this paper.
References
Daniel M. Bikel and David Chiang. 2000. Two sta-
tistical parsing models applied to the Chinese Tree-
bank. In Proceedings of the Second Chinese Lan-
guage Processing Workshop, ACL ?00.
Daniel M. Bikel. 2004. On the Parameter Space of
Generative Lexicalized Statistical Parsing Models.
Ph.D. thesis, University of Pennsylvania.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2004. Deterministic dependency structure
analyzer for Chinese. In Proceedings of IJCNLP
?04.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2005. Machine learning-based dependency
analyzer for Chinese. In Proceedings of ICCC ?05.
David Chiang and Daniel M. Bikel. 2002. Recovering
latent information in treebanks. In Proceedings of
COLING ?02.
Michael John Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2004. Timbl version 5.1 ref-
erence guide. Technical report, Tilburg University.
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a new boosting algorithm. In Proceed-
ings of ICML ?96.
Pascale Fung, Grace Ngai, Yongsheng Yang, and Ben-
feng Chen. 2004. A maximum-entropy Chinese
parser augmented by transformation-based learning.
ACM Transactions on Asian Language Information
Processing, 3(2):159?168.
Mary Hearne and Andy Way. 2004. Data-oriented
parsing and the Penn Chinese Treebank. In Proceed-
ings of IJCNLP ?04.
John Henderson and Eric Brill. 1999. Exploiting di-
versity in natural language processing: Combining
parsers. In Proceedings of EMNLP ?99.
Zhengping Jiang. 2004. Statistical Chinese parsing.
Honours thesis, National University of Singapore.
Taku Kudo and Yuji Matsumoto. 2000. Use of support
vector learning for chunk identification. In Proceed-
ings of CoNLL and LLL ?00.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese Treebank?
In Proceedings of ACL ?03.
Xiaoqiang Luo. 2003. A maximum entropy Chinese
character-based parser. In Proceedings of EMNLP
?03.
David M. Magerman. 1994. Natural Language Pars-
ing as Statistical Pattern Recognition. Ph.D. thesis,
Stanford University.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
EMNLP ?04.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of COLING ?04.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1-3):151?175.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the IWPT ?05.
Honglin Sun and Daniel Jurafsky. 2003. The effect of
rhythm on structural disambiguation in Chinese. In
Proceedings of SIGHAN Workshop ?03.
Honglin Sun and Daniel Jurafsky. 2004. Shallow se-
mantic parsing of Chinese. In Proceedings of the
HLT/NAACL ?04.
Antal van den Bosch and Jakub Zavrel. 2000. Un-
packing multi-valued symbolic features and classes
in memory-based language learning. In Proceedings
of ICML ?00.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,
and Yueliang Qian. 2005. Parsing the Penn Chinese
Treebank with semantic knowledge. In Proceedings
of IJCNLP ?05.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of IWPT ?03.
Le Zhang, 2004. Maximum Entropy Modeling Toolkit
for Python and C++. Reference Manual.
432
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 205?208,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using Part-of-Speech Reranking to Improve Chinese Word Segmentation
Mengqiu Wang Yanxin Shi
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{mengqiu,yanxins}@cs.cmu.edu
Abstract
Chinese word segmentation and Part-of-
Speech (POS) tagging have been com-
monly considered as two separated tasks.
In this paper, we present a system that
performs Chinese word segmentation and
POS tagging simultaneously. We train a
segmenter and a tagger model separately
based on linear-chain Conditional Ran-
dom Fields (CRF), using lexical, morpho-
logical and semantic features. We propose
an approximated joint decoding method
by reranking the N-best segmenter out-
put, based POS tagging information. Ex-
perimental results on SIGHAN Bakeoff
dataset and Penn Chinese Treebank show
that our reranking method significantly
improve both segmentation and POS tag-
ging accuracies.
1 Introduction
Word segmentation and Part-of-speeching (POS)
tagging are the most fundamental tasks in Chinese
natural language processing (NLP). Traditionally,
these two tasks were treated as separate and in-
dependent processing steps chained together in a
pipeline. In such pipeline systems, errors intro-
duced at the early stage cannot be easily recov-
ered in later steps, causing a cascade of errors
and eventually harm overall performance. Intu-
itively, a correct segmentation of the input sen-
tence is more likely to give rise to a correct POS
tagging sequence than an incorrect segmentation.
Hinging on this idea, one way to avoid error prop-
agation in chaining subtasks such as segmentation
and POS tagging is to exploit the learning trans-
fer (Sutton and McCallum, 2005) among sub-
tasks, typically through joint inference. Sutton et
al. (2004) presented dynamic conditional random
fields (DCRF), a generalization of the traditional
linear-chain CRF that allow representation of in-
teraction among labels. They used loopy belief
propagation for inference approximation. Their
empirical results on the joint task of POS tagging
and NP-chunking suggested that DCRF gave supe-
rior performance over cascaded linear-chain CRF.
Ng and Low (2004) and Luo (2003) also trained
single joint models over the Chinese segmentation
and POS tagging subtasks. In their work, they
brought the two subtasks together by treating it as
a single tagging problem, for which they trained a
maximum entropy classifier to assign a combined
word boundary and POS tag to each character.
A major challenge, however, exists in doing
joint inference for complex and large-scale NLP
application. Sutton and McCallum (Sutton and
McCallum, 2005) suggested that in many cases ex-
act inference can be too expensive and thus formi-
dable. They presented an alternative approach in
which a linear-chain CRF is trained separately for
each subtask at training time, but at decoding time
they combined the learned weights from the CRF
cascade into a single grid-shaped factorial CRF
to perform joint decoding and make predictions
for all subtasks. Similar to (Sutton and McCal-
lum, 2005), in our system we also train a cas-
cade of linear-chain CRF for the subtasks. But
at decoding time, we experiment with an alterna-
tive approximation method to joint decoding, by
taking the n-best hypotheses from the segmenta-
tion model and use the POS tagging model for
reranking. We evaluated our system on the open
tracks of SIGHAN Bakeoff 2006 dataset. Fur-
thermore, to evaluate our reranking method?s im-
pact on the POS tagging task, we also performed
10-fold cross-validation tests on the 250k Penn
205
Chinese Treebank (CTB) (Xue et al, 2002). Re-
sults from both evaluations suggest that our simple
reranking method is very effective. We achieved
a consistent performance gain on both segmenta-
tion and POS tagging tasks over linearly-cascaded
CRF. Our official F-scores on the 2006 Bakeoff
open tracks are 0.935 (UPUC), 0.964 (CityU),
0.952 (MSRA) and 0.949 (CKIP).
2 Algorithm
Given an observed Chinese character sequence
X = {C1, C2, ..., Cn}, let S and T denote a seg-
mentation sequence and a POS tagging sequence
over X. Our goal is to find a segmentation se-
quence S? and a POS tagging sequence T? that max-
imize the posterior probability :
P (S,T|X = {C1, C2, ..., Cn}) (1)
Applying chain rule, we can further derive from
Equation 1 the following:
< S?, T? >
= arg max
S,T
P (T|S,X = {C1, C2, ..., Cn})
?P (S|X = {C1, C2, ..., Cn}) (2)
Since we have factorized the joint probability
in Equation 1 into two terms, we can now model
these two components using conditional random
fields (Lafferty et al, 2001). Linear-chain CRF
models define conditional probability, P (Z|X), by
linear-chain Markov random fields. In our case, X
is the sequence of characters or words, and Z is
the segmentation labels for characters (START or
NON-START, used to indicate word boundaries)
or the POS tagging for words (NN, VV, JJ, etc.).
The conditional probability is defined as:
P (Z|X) = 1N(X) exp (
T
?
t=1
K
?
k=1
?kfk(Z,X, t))
(3)
where N(X) is a normalization term to guaran-
tee that the summation of the probability of all
label sequences is one. fk(Z,X, t) is the kth
localfeaturefunction at sequence position t. It
maps a pair of X and Z and an index t to {0,1}.
(?1, ..., ?K) is a weight vector to be learned from
training set. A large positive value of ?i means
that the ith feature function?s value is frequent to
be 1, whereas a negative value of ?i means the ith
feature function?s value is unlikely to be 1.
At decoding time, we are interested in finding
the segmentation sequence S? and POS tagging se-
quence T? that maximizes the probability defined
in Equation 2. Instead of exhaustively searching
the whole space of all possible segmentations, we
restrict our searching to S = {S1,S2, ...,SN},
where S is the restricted search space consisting
of N-best decoded segmentation sequences. This
N-best list of segmentation sequences, S, can be
obtained using modified Viterbi algorithm and A*
search (Schwartz and Chow, 1990).
3 Features
3.1 Features for Segmentation
We adopted the basic segmentation features used
in (Ng and Low, 2004). These features are summa-
rized in Table 1 ((1.1)-(1.7)). In these templates,
C0 refers to the current character, and C?n, Cn re-
fer to the characters n positions to the left and right
of the current character, respectively. Pu(C0) in-
dicates whether C0 is a punctuation. T (Cn) clas-
sifies the character Cn into four classes: num-
bers, dates (year, month, date), English letters and
all other characters. LBegin(C0), LEnd(C0) and
LMid(C0) represent the maximum length of words
found in a lexicon1 that contain the current char-
acter as either the first, last or middle character, re-
spectively. Single(C0) indicates whether the cur-
rent character can be found as a single word in the
lexicon.
Besides the adopted basic features mentioned
above, we also experimented with additional se-
mantic features (Table 1 (1.8)). For (1.8), Sem0
refers to the semantic class of current character,
and Sem?1, Sem1 represent the semantic class
of characters one position to the left and right of
the current character, respectively. We obtained
a character?s semantic class from HowNet (Dong
and Dong, 2006). Since many characters have
multiple semantic classes defined by HowNet, it
is a non-trivial task to choose among the differ-
ent semantic classes. We performed contextual
disambiguation of characters? semantic classes by
calculating semantic class similarities. For ex-
ample, let us assume the current character is
(look,read) in a word context of ?(read
1We compiled our lexicon from three external re-
sources. HowNet: www.keenage.com; On-Line Chinese
Tools: www.mandarintools.com; Online Dictionary from
Peking University: http://ccl.pku.edu.cn/doubtfire/Course/
Chinese%20Information%20Processing/Source Code/
Chapter 8/Lexicon full 2000.zip
206
newspaper). The character (look) has two se-
mantic classes in HowNet, i.e. ?(read) and Keyword Translation Accuracy and Cross-Lingual Question  
Answering in Chinese and Japanese 
 
Teruko Mitamura 
Carnegie Mellon 
University 
Pittsburgh, PA USA 
teruko@cs.cmu.edu 
 Mengqiu Wang 
Carnegie Mellon 
University 
Pittsburgh, PA USA 
mengqiu@cs.cmu.edu
Hideki Shima 
Carnegie Mellon 
University 
Pittsburgh, PA USA 
hideki@cs.cmu.edu
Frank Lin 
Carnegie Mellon 
University 
Pittsburgh, PA USA 
frank+@cs.cmu.edu
 
 
  
 
Abstract 
In this paper, we describe the extension 
of an existing monolingual QA system 
for English-to-Chinese and English-to-
Japanese cross-lingual question answer-
ing (CLQA). We also attempt to charac-
terize the influence of translation on 
CLQA performance through experimen-
tal evaluation and analysis. The paper 
also describes some language-specific is-
sues for keyword translation in CLQA. 
1 Introduction 
The JAVELIN system is a modular, extensible 
architecture for building question-answering 
(QA) systems (Nyberg, et al, 2005).  Since the 
JAVELIN architecture is language-independent, 
we extended the original English version of 
JAVELIN for cross-language question answering 
(CLQA) in Chinese and Japanese.  The same 
overall architecture was used for both systems, 
allowing us to compare the performance of the 
two systems. In this paper, we describe how we 
extended the monolingual system for CLQA (see 
Section 3).  Keyword translation is a crucial ele-
ment of the system; we describe our translation 
module in Section 3.2.  In Section 4, we evaluate 
the end-to-end CLQA systems using three differ-
ent translation methods.  Language-specific 
translation issues are discussed in Section 5. 
2 Javelin Architecture 
The JAVELIN system is composed of four main 
modules: the Question Analyzer (QA), Retrieval 
Strategist (RS), Information eXtractor (IX) and 
Answer Generator (AG). Inputs to the system are 
processed by these modules in the order listed 
above. The QA module is responsible for parsing 
the input question, assigning the appropriate an-
swer type to the question, and producing a set of 
keywords. The RS module is responsible for 
finding documents containing answers to the 
question, using keywords produced by the QA 
module. The IX module finds and extracts an-
swers from the documents based on the answer 
type, and then produces a ranked list of answer 
candidates. The AG module normalizes and clus-
ters the answer candidates to rerank and generate 
a final ranked list. The overall monolingual ar-
chitecture is shown in Figure 1. 
3 Extension for Cross-Lingual QA 
Because of JAVELIN?s modular design, signifi-
cant changes to the monolingual architecture 
were not required. We customized the system in 
order to handle Unicode characters and ?plug in? 
cross-lingual components and resources. 
For the Question Analyzer, we created the 
Keyword Translator, a sub-module for translat-
ing keywords. The Retrieval Strategist was 
adapted to search in multilingual corpora. The 
Information Extractors use language-independent 
extraction algorithms. The Answer Generator 
uses language-specific sub-modules for normali-
zation, and a language-independent algorithm for 
answer ranking. The overall cross-lingual archi-
tecture is shown in Figure 2.  The rest of this sec-
tion explains the details of each module. 
3.1 Question Analyzer 
The Question Analyzer (QA) is responsible for 
extracting information from the input question in 
order to formulate a representation of the  
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
31
 Figure1: Javelin Monolingual Architecture Figure2: Javelin Architecture with Cross-Lingual 
Extension 
 
information required to answer the question.  
Input questions are processed using the RASP 
parser (Korhonen and Briscoe, 2004), and the 
module output contains three main components: 
a) selected keywords; b) the answer type (e.g. 
numeric-expression, person-name, location); and 
c) the answer subtype (e.g. author, river, city).  
The selected keywords are words or phrases 
which are expected to appear in documents with 
correct answers. In order to reduce noise in the 
document retrieval phase, we use stop-word lists 
to eliminate high-frequency terms; for example, 
the term ?old? is not included as a keyword for 
?how-old? questions. 
We extended the QA module with a keyword 
translation sub-module, so that translated key-
words can be used to retrieve documents from 
multilingual corpora. This straightforward ap-
proach has been used by many other CLQA sys-
tems. An alternative approach is to first translate 
the whole question sentence from English to the 
target language, and then analyze the translated 
question. Our reasons for favoring keyword 
translation are two-fold. First, to translate the 
question to the target language and analyze it, we 
would have to replace the English NLP compo-
nents in the Question Analyzer with their coun-
terparts for the target language. In contrast, key-
word translation decouples the question analysis 
from the translation, and requires no language 
specific resources during question analysis. The 
second reason is that machine translation is not 
perfect, and therefore the resulting translation(s) 
for the question may be incomplete or ungram-
matical, thus adding to the complexity of the 
analysis task. One could argue that when trans-
lating the full sentence instead of just the key-
words, we can better utilize state-of-art machine 
translation techniques because more context in-
formation is available. But for our application, an 
accurate translation of functional words (such as 
prepositions or conjunctions) is less important. 
We focus more on words that carry more content 
information, such as verbs and nouns. We will 
present more detail on the use of contextual in-
formation for disambiguation in the next section. 
In some recent work (Kwok, 2005, Mori and 
Kawagishi, 2005), researchers have combined 
these two approaches, but to date no studies have 
compared their effectiveness. 
3.2 Translation Module 
The Translation Module (TM) is used by the QA 
module to translate keywords into the language 
of the target corpus. Instead of combining multi-
ple translation candidates with a disjunctive 
query operator (Isozaki et al, 2005), the TM se-
lects the best combination of translated keywords 
from several sources: Machine Readable Dic-
tionaries (MRDs), Machine Translation systems 
(MTs) and Web-mining-Based Keyword Trans-
lators (WBMTs) (Nagata et al, 2001, Li et al, 
2003). For translation from English to Japanese, 
we used two MRDs, eight MTs and one WBMT. 
If none of them return a translation, the word is 
transliterated into kana for Japanese (for details 
on transliteration, see Section 5.2). For transla-
tion from English to Chinese, we used one MRD, 
three MTs and one WBMT. After gathering all 
possible translations for every keyword, the TM 
uses a noisy channel model to select the best 
combination of translated keywords. The TM 
estimates model statistics using the World Wide 
Web. Details of the translation selection method 
are described in the rest of this subsection. 
 
The Noisy Channel Model: In the noisy channel 
model, an undistorted signal passes through a 
noisy channel and becomes distorted.  Given the 
distorted signal, we are to find the original, un-
distorted signal. IBM applied the noisy channel 
model idea to translation of sentences from 
aligned parallel corpora, where the source lan-
guage sentence is the distorted signal, and the 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
32
target language sentence is the original signal 
(Brown et al, 1990). We adopt this model for 
disambiguating keyword translation, with the 
source language keyword terms as the distorted 
signal and the target language terms as the origi-
nal signal.  The TM's job is to find the target lan-
guage terms given the source language terms, by 
finding the probability of the target language 
terms given the source language terms  P(T|S).  
 
Using Bayes' Rule, we can break the equation 
down to several components: 
 
)(
)|()(
)|(
SP
TSPTP
STP
?=  
Because we are comparing probabilities of dif-
ferent translations of the same source keyword 
terms, we can simplify the problem to be: 
 
)|()()|( TSPTPSTP ?=  
 
We can now reduce the equation to two compo-
nents. P(T) is the language model and P(S|T) is 
the translation model. If we assume independ-
ence among the translations of individual terms, 
we can represent the translation probability of a 
keyword by the product of the probabilities of 
the individual term translations: 
 
?=
i
ii tsPTSP )|()|(  
 
Estimating Probabilities using the World 
Wide Web: For estimating the probabilities of 
the translation model and the language model, 
we chose to gather statistics from the World 
Wide Web. There are three advantages in utiliz-
ing the web for gathering translation statistics: 1) 
it contains documents written in many different 
languages, 2) it has high coverage of virtually all 
types of words and phrases, and 3) it is con-
stantly updated. However, we also note that the 
web contains a lot of noisy data, and building up 
web statistics is time-consuming unless one has 
direct access to a web search index. 
 
Estimating Translation Model Probabilities: 
We make an assumption that terms that are trans-
lations of each other co-occur more often in 
mixed-language web pages than terms that are 
not translations of each other. This assumption is 
analogous to Turney?s work on the co-
occurrence of synonyms (Turney, 2001). We 
then define the translation probability of each 
keyword translation as: 
?=
j
jii
jii
jii tsco
tsco
tsP
)),(log(
)),(log(
)|(
,
,
,
 
Where si is the i-th term in the source language 
and ti,j is the j-th translation candidate for si. Let 
hits be a number of web pages retrieved from a 
certain search engine. co(si, t i,j) is the hits given 
a query si and ti,j., where log is applied to adjust 
the count so that translation probabilities can still 
be comparable at higher counts. 
 
Estimating Language Model Probabilities: In 
estimating the language model, we simply obtain 
hits given a conjunction of all the candidate 
terms in the target language, and divide that 
count by the sum of the occurrences of the indi-
vidual terms: 
 
?=
i
i
n
to
tttco
TP
)(
),...,(
)( 21  
The final score of a translation candidate for a 
query is the product of the translation model 
score P(S|T) and the language model score P(T). 
 
Smoothing and Pruning: As with most statisti-
cal calculations in language technologies, there is 
a data sparseness problem when calculating the 
language model score. Also, because statistics 
are gathered real-time by accessing a remote 
search engine via internet, it can take a long time 
to process a single query when there is a large 
number of translation candidates. We describe 
methods for smoothing the language model and 
pruning the set of translation candidates below. 
The data sparseness problem occurs when 
there are many terms in the query, and the terms 
are relatively rare keywords. When calculating 
the language model score, it is possible that none 
of the translation candidates appear on any web 
page. To address this issue, we propose a "mov-
ing-window smoothing" algorithm: 
 
? When the target keyword co-occurrence 
count with n keywords is below a set 
threshold for all of the translation candi-
dates, we use a moving window of size 
n-1 that "moves" through the keywords 
in sequence, splitting the set of keywords 
into two sets, each with n-1 keywords. 
 
? If the co-occurrence count of all of these 
sets of keywords is above the threshold, 
return the product of the language model 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
33
score of these two sets as the language 
model score. 
 
? If not, decrease the window and repeat 
until either all of the split sets are above 
the threshold or n = 1. 
 
The moving window smoothing technique 
gradually relaxes the search constraint without 
losing the "connectivity" of keywords (there is 
always overlap in the split parts) before finally 
backing off to just the individual keywords. 
However, there are two issues worth noting with 
this approach: 
 
1. "Moving-window smoothing" assumes 
that keywords that are next to each other 
are also more semantically related, 
which may not always be the case. 
 
2. "Moving-window smoothing" tends to 
give the keywords near the middle of the 
question more weight, which may not be 
desirable. 
 
A better smoothing technique may be used 
with trying all possible "splits" at each stage, but 
this would greatly increase the time cost. There-
fore, we chose the moving-window smoothing as 
a trade-off between a more robust smoothing 
technique that tries all possible split combina-
tions and no smoothing at all. 
The set of possible translation candidates is 
produced by creating all possible combinations 
of the translations of individual keywords. For a 
question with n keywords and an average of m 
possible translations per keyword, the number of 
possible combinations is mn. This quickly be-
comes intractable as we have to access a search 
engine at least mn times just for the language 
model score. Therefore, pruning is needed to cut 
down the number of translation candidates. We 
prune possible translation candidates twice dur-
ing each run, using early and late pruning: 
 
1. Early Pruning: We prune possible trans-
lations of the individual keywords before 
combining them to make all possible 
translations of a query. We use a very 
simple pruning heuristic based on target 
word frequency using a word frequency 
list. Very rare translations produced by a 
resource are not considered. 
 
2. Late Pruning: We prune possible transla-
tion candidates of the entire set of key-
words after calculating translation prob-
abilities. Since the calculation of the 
translation probabilities requires little 
access to the web, we can calculate only 
the language model score for the top N 
candidates with the highest translation 
score and prune the rest. 
 
An Example of English to Chinese Keyword 
Translation Selection: Suppose we translate the 
following question from English to Chinese.    
 
"What if Bush leaves Iraq?" 
 
Three keywords are extracted: ?Bush?, 
?leaves?, and ?Iraq.? Using two MT systems and 
an MRD, we obtain the following translations: 
 
 i=1 i=2 i=3 
Source Bush leaves Iraq 
Target j=1 ?? ?? ??? 
Target j=2 ?? ??  
Table 1. E-C Keyword Translation 
 
"Bush" and "leaves" both have two transla-
tions because they are ambiguous keywords, 
while "Iraq" is unambiguous. Translation (1,1) 
means bush as in a shrub, and translation (1,2) 
refers to the person named Bush. Translation 
(2,1) is the verb "to go away", and translation 
(2,2) is the noun for leaf. Note that we would like 
translation (1,2) and translation (2,1) because 
they match the sense of the word intended by the 
user. Now we can create all possible combina-
tions of the keywords in the target language: 
 
"?? ?? ???" 
"?? ?? ???" 
"?? ?? ???" 
"?? ?? ???" 
 
Query "Bush" 
"??" 
"Bush" 
"??" 
"leaves" 
"??" 
"leaves" 
"??" 
"Iraq" 
"???" 
hits 3790 41100 5780 7240 24500 
Table 2. Translation Pair Page Counts 
 
 
Candidate Translation Score 
"?? ?? ???" 0.215615 
"?? ?? ???" 0.221219 
"?? ?? ???" 0.277970 
"?? ?? ???" 0.285195 
Table 3. Translation Scores 
 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
34
By calculating hits, we obtain the statistics and 
the translation scores shown in Table 2 and 3. 
Now we can proceed to use the search engine to 
obtain language model statistics, which we use to 
obtain the language model. Then, together with 
the translation model score, we calculate the 
overall score1. 
 
Query ?? ?? ?? ?? ??? 
hits 428K 459K 1490K 1100K 9590K 
Table 4. Individual Term Page Counts 
 
Query hits 
"?? ?? ???" 1200 
"?? ?? ???" 455 
"?? ?? ???" 17300 
"?? ?? ???" 2410 
Table 5. Target Language Query Page Counts 
 
Cand Translation Language Overall 
?? 
?? 
??? 
2.1562E-1 1.0428E-4 2.2483E-5 
?? 
?? 
??? 
2.2122E-1 4.0925E-5 9.0533E-6 
?? 
?? 
??? 
2.7797E-1 1.4993E-3 4.1675E-4 
?? 
?? 
??? 
2.8520E-1 2.1616E-4 6.1649E-5 
 
Table 6. Translation Score, Language Model 
Score, and Overall Score 
 
As shown in Table 6, we select the most prob-
able combination of translated keywords with the 
highest overall score (the third candidate), which 
is the correct translation of the English keywords. 
3.3 Retrieval Strategies 
The Retrieval Strategist (RS) module retrieves 
documents from a corpus in response to a query.  
For document retrieval, the RS uses the Lemur 
3.0 toolkit (Ogilvie and Callan, 2001). Lemur 
supports structured queries using operators such 
as Boolean AND, Synonym, Ordered/Un-
Ordered Window and NOT. An example of a 
structured query is shown below: 
 
                                                 
1  For simplicity, we don?t apply smoothing 
and pruning. 
 
#BAND( #OD4(???? ??) 
?? 
#SYN(*organization *person) ) 
 
In formulating a structured query, the RS uses an 
incremental relaxation technique, starting from 
an initial query that is highly constrained; the 
algorithm searches for all the keywords and data 
types in close proximity to each other. The prior-
ity is based on a function of the likely answer 
type, keyword type (word, proper name, or 
phrase) and the inverse document frequency of 
each keyword. The query is gradually relaxed 
until the desired number of relevant documents is 
retrieved.  
3.4 Information Extraction 
In the JAVELIN system, the Information Ex-
tractor (IX) is not a single module that uses one 
extraction algorithm; rather, it is an abstract in-
terface which allows different information ex-
tractor implementations to be plugged into 
JAVELIN. These different extractors can be used 
to produce different results for comparison, or 
the results of running them all in parallel can be 
merged. Here we will describe just one of the 
extractors, the one which is currently the best 
algorithm in our CLQA experiment: the Light IX. 
The Light IX module uses simple, distance-
based algorithms to find a named entity that 
matches the expected answer type and is ?clos-
est? to all the keywords according to some dis-
tance measure.  The algorithm considers as an-
swer candidates only those terms that are tagged 
as named entities which match the desired an-
swer type.  The score for an answer candidate a 
is calculated as follows: 
 
)()()( aDistScoreaOccScoreaScore ?+?= ??   
 
where ? + ? = 1, OccScore is the occurrence 
score and DistScore is the distance score. Both 
OccScore and DistScore return a number be-
tween zero and one, and likewise Score returns a 
number between zero and one. Usually, ? is 
much smaller than ?. The occurrence score for-
mula is: 
n
kExist
aOccScore
n
i i? == 1 )()(  
where a is the answer candidate and ki is the i-th 
keyword, and n is the number of keywords. Exist  
returns 1 if the i-th keyword exists in the docu-
ment, and 0 otherwise. The distance score for 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
35
each answer candidate is calculated according to 
the following formula: 
n
kaDist
aDistScore
n
i
i
? == 1 ),(
1
)(  
This formula produces a score between zero 
and one. If the i-th keyword does not exist in a 
document, the equation inside the summation 
will return zero. If the i-th keyword appears more 
than once in the document, the one closest to the 
answer candidate is considered. An additional 
restriction is that the answer candidate cannot be 
one of the keywords. The Dist function is the 
distance measure, which has two definitions: 
 
1. ),(),( batTokensAparbaDist =  
2. )),(log(),( batTokensAparbaDist =   
 
The first definition simply counts the number 
of tokens between two terms.  The second defini-
tion is a logarithmic measure. The function re-
turns the number of tokens from a to b; if a and b 
are adjacent, the count is 1; if a and b are sepa-
rated by one token, the count is 2, and so on. A 
token can either be a character or a word; for the 
E-C, we used character-based tokenization, 
whereas for the E-J, we use word-based tokeni-
zation. By heuristics obtained from training re-
sults, we used the linear Dist measure for E-C 
and logarithmic Dist measure for E-J in the 
evaluation. 
This algorithm is a simple statistical approach 
which requires no language-specific external 
tools beyond word segmentation and a named-
entity tagger. It is not as sophisticated as other 
approaches which perform deep linguistic analy-
sis, but one advantage is faster adaptation to mul-
tiple languages. In our experiments, this simple 
algorithm performs at the same level as a FST-
based approach (Nyberg, et al 2005). 
3.5 Answer Generator 
The task of the Answer Generator (AG) module 
is to produce a ranked list of answer candidates 
from the IX output. The AG is designed to nor-
malize answer candidates by resolving represen-
tational differences (e.g. in how numbers, dates, 
etc. are expressed in text). This canonicalization 
makes it possible to combine answer candidates 
that differ only in surface form. 
Even though the AG module plays an impor-
tant role in JAVELIN, we did not use its full po-
tential in our E-C and E-J systems, since we 
lacked some language-specific resources re-
quired for multilingual answer merging. 
4 Evaluation and Effect of Translation 
Accuracy 
To evaluate the effect of translation accuracy on 
the overall performance of the CLQA system, we 
conducted several experiments using different 
translation methods. Three different runs were 
carried out for both the E-C and E-J systems, 
using the same 200-question test set and the 
document corpora provided by the NTCIR 
CLQA task. The first run was a fully automatic 
run using the original translation module in the 
CLQA system; the result is exactly same as the 
one we submitted to NTCIR5 CLQA. For the 
second run, we manually translated the keywords 
that were selected by the Question Analyzer 
module. This translation was done by looking at 
only the selected keywords, but not the original 
question. For both E-C and E-J tasks, the NTCIR 
organizers provided the translations for the Eng-
lish questions, which we assume are the gold-
standard translations. Taking advantage of this 
resource, in the third run we simply looked up 
the corresponding term for each English keyword 
from the gold-standard translation of the ques-
tion. The results for these runs are shown in Ta-
ble 7 and 8 below. 
 
  
Translation 
Accuracy 
Top1 
  
Top1+U 
  
Run 1 69.3% 15 (7.5%) 23 (11.5%) 
Run 2 85.5% 16 (8.0%) 31 (15.5%) 
Run 3 100% 18 (9.0%) 38 (19.0%) 
Table 7.  Effect of Translation (E-C) 
 
 
  
Translation 
Accuracy 
Top1 
  
Top1+U 
  
Run 1 54.2% 20 (10.0%) 25 (12.5%) 
Run 2 81.2% 19 (9.5%) 30 (15.0%) 
Run 3 100% 18 (9.0%) 31 (15.5%) 
Table 8.  Effect of Translation (E-J) 
 
We found that in the NTCIR task, the sup-
ported/correct document set was not complete. 
Some answers judged as unsupported were in-
deed well supported, but the supporting docu-
ment did not appear in NTCIR's correct docu-
ment set. Therefore, we think the Top1+U col-
umn is more informative for this evaluation. 
From Table 7 and 8, it is obvious that the overall 
performance increases as translation accuracy 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
36
increases. From Run1 to Run2, we eliminated all 
the overt translation errors produced by the sys-
tem, and also corrected word-sense errors. Then 
from Run2 to Run3, we made different lexical 
choices among the seemingly all correct transla-
tions of a word. This type of inappropriateness 
cannot be classified as an error, but it makes a 
difference in QA systems, especially at the docu-
ment retrieval stage. For example, the phrase 
"Kyoto Protocol" can have two valid transla-
tions: ???? or ?????. Both translations 
would be understandable to a human, but the sec-
ond translation will appear much more frequently 
than the first one in the document set. This type 
of lexical choice is hard to make, because we 
would need either subtle domain-specific knowl-
edge, or knowledge about the target corpus; nei-
ther is easily obtainable.  
    Comparing Run 1 and 3 in Table 8, we see 
that improving keyword translation had less 
overall impact on the E-J system. Information 
extraction (including named entity identification) 
did not perform as well in E-J.  We also com-
pared the translation effect on cross-lingual 
document retrieval (Figure 3).  As we can see, 
Run 3 retrieved supporting documents more fre-
quently in rank 1 than in Run 1 or 2. From these 
preliminary investigations, it would seem that 
information extraction and/or answer generation 
must be improved for English-Japanese CLQA. 
 
Figure3: Comparison of three runs: Cross-lingual 
document retrieval performance in E-J 
5 Translation Issues 
In this section, we discuss language specific key-
word translation issues for Chinese and Japanese 
CLQA. 
5.1 Chinese 
One prominent problem in Chinese keyword 
translation is word sense disambiguation. In 
question answering systems, the translation re-
sults are used directly in information retrieval, 
which exhibits a high dependency on the lexical 
form of a word but not so much on the meaning. 
In other words, having a different lexical form 
from the corresponding term in corpora is the 
same as having a wrong translation. For exam-
ple, to translate the word ?bury? into Chinese, 
our system gives a translation of ? , which 
means ?bury? as the action of digging a hole, 
hiding some items in the hole and then covering 
it with earth. But the desired translation, as it 
appears in the document is ? , which means 
?bury? too, but specifically for burial in funerals. 
Even more challenging are regional language 
differences. In our system, for example, the cor-
pora are newswire articles written in Traditional 
Chinese from Taiwan, and if we use an MT sys-
tem that produces translations in Simplified Chi-
nese followed by conversion to Traditional Chi-
nese, we may run into problems. The MT system 
generates Simplified Chinese translations first, 
which may suggest that the translation resources 
it uses were written in Simplified Chinese and 
originate from mainland China. In mainland 
China and in Taiwan, people commonly use dif-
ferent words for describing the same thing, espe-
cially for proper nouns like foreign names. Table 
9 lists some examples. Therefore if the MT sys-
tem generates its output using text from 
mainland China, it may produce a different word 
than the one used in Taiwan, which may not ap-
pear in the corpora. This could lead to failure in 
document retrieval.  
 
English  Mainland China Taiwan 
Band ?? ?? 
Computer Game ???? ?? 
World Guinness 
Record 
??????? ?????? 
 
The Catcher in 
the Rye 
??????? ???? 
 
Nelson ??? ??? 
Salinger ??? ??? 
Creutzfeldt 
Jakob Disease 
????? ???? 
 
Luc Besson ?? ?? ??? 
Pavarotti ???? ???? 
Table 9. Different Translation in Chinese 
5.2 Japanese 
Representational Gaps: One of the advantages 
of using structured queries and automatic query 
formulation in the RS is that the system is able to 
handle slight representational gaps between a 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
37
translated query and corresponding target words 
in the corpus.  
For example, Werner Spies appears as ???
?? ? ????? in our Japanese preproc-
essed corpus and therefore ????? ????
?, which is missing a dot between last and first 
name, is a wrong translation if our retrieval 
module only allows exact match. Lemur supports 
an Ordered Distance Operator where the terms 
within a #ODN operator must be found within N 
words of each other in the text in order to con-
tribute to the document's belief value. This en-
ables us to bridge the representational gaps; such 
as when #OD1(????? ?????) does not 
match any words in the corpus, #OD2(????? 
?????) is formulated in the next step in or-
der to capture ????? ? ?????. 
 
Transliteration in WBMT: After detecting 
Japanese nouns written in romaji (e.g. Funaba-
shi), we transliterated them into hiragana for a 
better result in WBMT. This is because we are 
assuming higher positive co-occurrence between 
kana and kanji (i.e. ???? and ??) than be-
tween romaji and kanji (i.e. funabashi and??). 
When there are multiple transliteration candi-
dates, we iterate through each candidate. 
 
Document Retrieval in Kana: Suppose we are 
going to transliterate Yusuke. This romaji can be 
mapped to kana characters with relatively less 
ambiguity (i.e. ??? , ????), when com-
pared to their subsequent transliteration to kanji 
(i.e. ??, ??, ??, ??, ?? etc.).  Therefore, 
indexing kana readings in the corpus and query-
ing in kana is sometimes a useful technique for 
CLQA, given the difficulty in converting romaji 
to kana and romaji to kanji.  
To implement this approach, the Japanese cor-
pus was first preprocessed by annotating named 
entities and by chunking morphemes. Then, we 
annotated a kana reading for each named entity. 
At query time, if there is no translation found 
from other resources, the TM transliterates ro-
maji to kana as a back-off strategy. 
6 Conclusion 
We described how we extended an existing 
monolingual (English) system for CLQA (Eng-
lish to Chinese and English to Japanese), includ-
ing a translation disambiguation technique 
which uses a noisy channel model with probabil-
ity estimations using web as corpora. We dis-
cussed the influence of translation accuracy on 
CLQA by presenting experimental results and 
analysis. We concluded by introducing some 
language-specific issues for keyword translation 
from English to Chinese and Japanese which we 
hope to address in ongoing research. 
Acknowledgements 
This work is supported by the Advanced Re-
search and Development Activity (ARDA)?s 
Advanced Question Answering for Intelligent 
(AQUAINT) Program. 
References  
Brown, P., J. Cocke, S.D. Pietra, V.D. Pietra, F. 
Jelinek., J. Lafferty, R. Mercer, and P. Roossin. 
1990. A Statistical Approach to Machine Transla-
tion. Computational Linguistics, 16(2):38?45.  
Isozaki, H., K. Sudoh and H. Tsukada. 2005. NTT?s 
Japanese-English Cross-Language Question An-
swering System. In Proceedings of the NTCIR 
Workshop 5 Meeting, pages 186-193. 
Korhonen, A. and E. Briscoe. 2004. Extended Lexi-
cal-Semantic Classification of English Verbs. Pro-
ceedings of the HLT/NAACL '04 Workshop on 
Computational Lexical Semantics, pages 38-45. 
Kwok, K., P. Deng, N. Dinstl and S. Choi. 2005. 
NTCIR-5 English-Chinese Cross Language Ques-
tion-Answering Experiments using PIRCS. In Pro-
ceedings of the NTCIR Workshop 5 Meeting. 
Li, Hang, Yunbo Cao, and Cong Li. 2003. Using Bi-
lingual Web Data To Mine and Rank Translations, 
IEEE Intelligent Systems 18(4), pages 54-59. 
Mori, T. and M. Kawagishi. 2005. A Method of Cross 
Language Question-Answering Based on Machine 
Translation and Transliteration. In Proceedings of 
the NTCIR Workshop 5 Meeting. 
Nagata,  N., T. Saito, and K. Suzuki. 2001. Using the 
Web as a Bilingual Dictionary, In Proceedings of 
ACL 2001 Workshop Data-Driven Methods in Ma-
chine Translation, pages 95-102 
Nyberg, E., R. Frederking, T. Mitamura, J. M. Bilotti, 
K. Hannan, L. Hiyakumoto, J. Ko, F. Lin, L. Lita, 
V. Pedro, A. Schlaikjer. 2005. JAVELIN I and II in 
TREC2005. In Proceedings of TREC 2005. 
Ogilvie, P. and J. Callan. 2001. Experiments Using 
the Lemur Toolkit. In Proceedings of the 2001 Text 
REtrieval Conference (TREC 2001), pages 103-
108. 
Turney, P.D. 2001, Mining the Web for synonyms: 
PMI-IR versus LSA on  TOEFL, Proceedings of the 
Twelfth European Conference on Machine Learn-
ing, pages 491-502. 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
38
A Re-examination of Dependency Path Kernels for Relation Extraction
Mengqiu Wang
Computer Science Department
Stanford University
mengqiu@cs.stanford.edu
Abstract
Extracting semantic relations between enti-
ties from natural language text is an impor-
tant step towards automatic knowledge ex-
traction from large text collections and the
Web. The state-of-the-art approach to rela-
tion extraction employs Support Vector Ma-
chines (SVM) and kernel methods for classi-
fication. Despite the diversity of kernels and
the near exhaustive trial-and-error on ker-
nel combination, there lacks a clear under-
standing of how these kernels relate to each
other and why some are superior than oth-
ers. In this paper, we provide an analysis of
the relative strength and weakness of several
kernels through systematic experimentation.
We show that relation extraction can bene-
fit from increasing the feature space through
convolution kernel and introducing bias to-
wards more syntactically meaningful feature
space. Based on our analysis, we propose
a new convolution dependency path kernel
that combines the above two benefits. Our
experimental results on the standard ACE
2003 datasets demonstrate that our new ker-
nel gives consistent and significantly better
performance than baseline methods, obtain-
ing very competitive results to the state-of-
the-art performance.
1 Introduction
There exists a large body of knowledge embedded in
unstructured natural language text on the Web. The
sheer volume and heterogeneity of such knowledge
renders traditional rule-based and manually-crafted
knowledge extraction systems unsuitable. Thus it
calls for methods that automatically extract knowl-
edge from natural language text. An important step
towards automatic knowledge discovery is to extract
semantic relations between entities.
Two types of collections are commonly studied
for relation extraction. The first type is annotated
newswire text made available by programs such as
Message Understanding Conferences (MUC) and
Automatic Content Extraction (ACE). The types of
entities that are of interest to these programs include
person, organization, facilities, location and GPE
(Geo-political entities). Given entities in a docu-
ment, the relation extraction task is to identify ex-
plicit semantic relationship such as Located-In and
Citizen-Of between pairs of entities. For example, in
the sentence ?The funeral was scheduled for Thurs-
day in Paris at the Saint-Germain-des-Pres Church?,
the organization Saint-Germain-des-Pres Church is
?Located-In? GPE Paris. The second type of collec-
tion that has been widely studied is biomedical liter-
ature (Bunescu and Mooney, 2005b; Giuliano et al,
2006; McDonald et al, 2005b), promoted by evalu-
ation programs such as BioCreAtIvE and JNLPBA
2004. In this particular domain, studies often focus
on specific entities such as genes and proteins. And
the kinds of relations to extract are usually gene-to-
protein interactions.
The predominant approach to relation extraction
treats the task as a multi-class classification prob-
lem, in which different relation types form differ-
ent output classes. Early work employed a diverse
range of features in a linear classifier (commonly
referred to as ?feature-based? approaches), includ-
ing lexical features, syntactic parse features, de-
pendency features and semantic features (Jiang and
Zhai, 2007; Kambhatla, 2004; Zhou et al, 2005).
These approaches were hindered by drawbacks such
as limited feature space and excessive feature en-
gineering. Kernel methods (Cortes and Vapnik,
1995; Cristianini and Shawe-Taylor, 2000) on the
other hand can explore a much larger feature space
very efficiently. Recent studies on relation extrac-
tion have shown that by combining kernels with
Support-vector Machines (SVM), one can obtain re-
sults superior to feature-based methods (Bunescu
841
and Mooney, 2005b; Bunescu and Mooney, 2005a;
Culotta and Sorensen, 2004; Cumby and Roth,
2003; Zelenko et al, 2003; Zhang et al, 2006a;
Zhang et al, 2006b; Zhao and Grishman, 2005).
Despite the large number of recently proposed
kernels and their reported success, there lacks a clear
understanding of their relative strength and weak-
ness. In this study, we provide a systematic com-
parison and analysis of three such kernels ? sub-
sequence kernel (Bunescu and Mooney, 2005b), de-
pendency tree kernel (Culotta and Sorensen, 2004)
and dependency path kernel (Bunescu and Mooney,
2005a). We replicated these kernels and conducted
experiments on the standard ACE 2003 newswire
text evaluation set. We show that whereas some ker-
nels are less effective than others, they exhibit prop-
erties that are complementary to each other. In par-
ticular, We found that relation extraction can benefit
from increasing the feature space through convolu-
tion kernel and introducing bias towards more syn-
tactically meaningful feature space.
Drawn from our analysis, we further propose
a new convolution dependency path kernel which
combines the benefits of the subsequence kernel and
shortest path dependency kernel. Comparing to the
previous kernels, our new kernel gives consistent
and significantly better performance than all three
previous kernels that we look at.
2 Related Work
Statistical methods for relation extraction can be
roughly categorized into two categories: feature-
based and kernel-based.
Feature-based methods (Jiang and Zhai, 2007;
Kambhatla, 2004; Zhou et al, 2005) use pre-defined
feature sets to extract features to train classifica-
tion models. Zhou et al (2005) manually crafted
a wide range of features drawn from sources such
as lexical, syntactic and semantic analyses. Com-
bined with SVM, they reported the best results at
the time on ACE corpus. Kambhatla (2004) took
a similar approach but used multivariate logistic re-
gression (Kambhatla, 2004). Jiang & Zhai (2007)
gave a systematic examination of the efficacy of un-
igram, bigram and trigram features drawn from dif-
ferent representations ? surface text, constituency
parse tree and dependency parse tree.
One drawback of these feature-based methods is
that the feature space that can be explored is often
limited. On the other hand, kernel-based methods
offer efficient solutions that allow us to explore a
much larger (often exponential, or in some cases, in-
finite) feature space in polynomial time, without the
need to explicitly represent the features.
Lodhi et al (2002) described a convolution string
kernel, which measures the similarity between two
strings by recursively computing matching of all
possible subsequences of the two strings. Bunescu
& Mooney (2005b) generalized the string kernel to
work with vectors of objects occurred in relation ex-
traction. In a later work also done by Bunescu &
Mooney (2005a), they proposed a kernel that com-
putes similarities between nodes on the shortest de-
pendency paths that connect the entities. Their ker-
nel assigns no-match to paths that are of different
length. And for paths that are of the same length, it
simply computes the product of the similarity score
of node pairs at each index. The dependency tree
kernel proposed by Zelenko et al (2003) was also
inspired by the string kernel of Lodhi et al (2002).
Their kernel walks down the parse trees from the
root and computes a similarity score for children
nodes at each depth level using the same subse-
quence algorithm as the string kernel. Culotta &
Sorensen (2004) worked on the same idea but ap-
plied it to dependency parse trees. Prior to these two
tree kernels, Collins & Duffy (2001) proposed a con-
volution tree kernel for natural language tasks. Their
kernel has since been applied to relation extraction
by Zhang et al (2006a). The tree kernel consid-
ers matching of all subtrees that share the same
production rule at the root of the subtree. Zhang
et al (2006a) showed results that are significantly
better than the previous two dependency tree ker-
nels. They obtained further improvements in their
later paper (2006b) by composing the tree kernel
with a simple entity kernel and raising the compos-
ite kernel to polynomial degree 2. Another study on
kernel composition is the work by Zhao & Grish-
man (2005).
It is worth noting that although there exist stan-
dard evaluation datasets such as ACE 2003 and
2004, many of the aforementioned work report re-
sults on non-standard datasets or splits, making it
difficult to directly compare the performance. We
842
feel that there is a sense of increasing confusion
down this line of research. Although partly due to
the lack of compatibility in evaluation results, we
believe it is more due to the lack of understanding in
the relative strength and weakness of these kernels.
Therefore we focus on analyzing and understanding
the pros and cons of different kernels, through sys-
tematic comparison and experimentation.
3 Kernel Methods for Relation Extraction
In this Section we first give a very brief introduc-
tion to kernel methods. We then present the al-
gorithms behind three kernels that we are particu-
larly interested in: subsequence kernel (Bunescu and
Mooney, 2005b), dependency tree kernel (Culotta
and Sorensen, 2004) and shortest path dependency
kernel (Bunescu and Mooney, 2005a).
3.1 SVM and Kernels
Support-Vector Machines (Cortes and Vapnik, 1995;
Cristianini and Shawe-Taylor, 2000) learn to find
hyperplanes that separate the positive and negative
data points so that the margin between the support-
vector points and the hyperplane is maximized. The
dual formulation of the optimization problem in-
volves only computing the dot product of feature
vectors. This is equivalent to mapping the data
points into a high dimensional space. And the sepa-
rating plane learnt in the high dimensional space can
give non-linear decision boundaries. The dot prod-
uct of data points can be computed using a kernel
function K(X,Y ) = ??(X), ?(Y )? for any map-
ping function. A valid kernel function satisfies cer-
tain properties: it is symmetric and the Gram matrix
G formed by K(X,Y ) is positive semi-definite.
3.2 Subsequence Kernel
The subsequence kernel introduced in (Bunescu
and Mooney, 2005b) is a generalization of the
string kernel first introduced by Lodhi et al (2002).
The feature space of the original string kernel
?stringkernel is defined as ?stringkernel = ?char,
where ?char is simply a set of characters. Bunescu
& Mooney (2005a) re-defined the feature space to
be ?x = ?1??2?? ? ???k, where ?1,?2, ? ? ? ,?k
can be some arbitray disjoint feature spaces, such as
the set of words, part-of-speech (POS) tags, etc. We
can measure the number of common features shared
by two feature vectors x, y ? ?x using function
c(x, y). Let s, t be two sequences over the feature
set ?x, we use |s| to denote the length of s. Thus s
can be written out as s1 ? ? ? s|s|. We use s[i : j] to
denote a continuous subsequence si ? ? ? sj of s. Let
i = (i1, ? ? ? , i|i|) be a sequence of |i| indices in s,
we define the length of the index sequence i to be
l(i) = i|i| ? i1 + 1. Similarly we have index se-
quence j in t of length l(j).
Let ?? = ?1 ? ?2 ? ? ? ? ? ?k be the set of all
possible features. A sequence u ? ??? is a subse-
quence of feature vector sequence s if there exists a
sequence of |u| indices i, such that uk ? sik , ?k ?
{1, ? ? ? , |u|}. Follow the notions in (Bunescu and
Mooney, 2005b; Cumby and Roth, 2003), we use
u ? s[i] as a shorthand for the above component-
wise ??? relationship. Now we can define the kernel
function Kn(s, t) to be the total number of weighted
common subsequence of length n between the two
sequeneces s and t.
Kn(s, t) =
?
u??n?
?
i:u?s[i]
?
j:u?t[j]
?l(i)+l(j) (1)
where ? is a decaying factor ? 1, penalizing long,
sparse subsequence. We can re-write this kernel
function as
Kn(s, t) =
?
i:|i|=n
?
j:|j|=n
n
?
k=1
c(sik , tjk)?l(i)+l(j) (2)
(Bunescu and Mooney, 2005b) showed that us-
ing the recursive dynamic programming algorithm
from (Cumby and Roth, 2003), the kernel Kn(s, t)
can be computed in O(kn|s||t |) time.
3.3 From Subsequence to Tree Kernels
We will use an example to illustrate the relation be-
tween the dependency tree kernels proposed by (Cu-
lotta and Sorensen, 2004; Zelenko et al, 2003) and
the subsequence kernel we introduced above. Con-
sider two instances of the ?Located-In? relations
?his actions in Brcko? and ?his recent arrival in Bei-
jing?. The dependency parse trees of these two sen-
tences are shown below.
843
actions
NNS
NOUN
his
PRP
PERSON
in
IN
Brcko
NNP
NOUN
LOCATION
arrival
NN
NOUN
his
PRP
PERSON
recent
ADJ
in
IN
Beijing
NNP
NOUN
LOCATION
The entities in these two relations are the pro-
noun mentions of ?his?, and two locations ?Br-
cko? and ?Beijing?, all shown in italic. The de-
pendency tree kernel visits nodes in the two trees
starting from the root. And at each depth level, it
takes nodes that are at that level and form two se-
quences of nodes. For example, in the example in-
stances, nodes at one level below the root forms
vectors s=?{his, PRP, PERSON},{in, IN}? and
t=?{his,PRP,PERSON},{recent, ADJ},{in, IN}?. It
then makes use of the subsequence kernel in the
previous section to compute the total number of
weighted subsequences between these two vectors.
The kernel returns the sum of subsequence match-
ing scores at each depth level as the final score.
3.4 Shortest Path Dependency Kernel
The shortest path dependency kernel proposed by
Bunescu & Mooney (2005a) also works with depen-
dency parse trees. Reuse our example in the previ-
ous section, the shortest dependency path between
entity his and Brcko in the first sentence is s=?{his,
PRP, PERSON}, {actions, NNS, NOUN}, {in, IN},
{Brcko, NNP, NOUN, LOCATION}?; and the path
between his and Beijing in the second sentence is
t=?{his, PRP, PERSON}, {arrival, NN, NOUN},
{in, IN}, {Beijing, NNP, NOUN, LOCATION}?.
Since most dependency parser output connected
trees, finding the shortest path between two nodes
is trivial. Once the two paths are found, the kernel
simply computes the product of the number of com-
mon features between a pair of nodes at each index
along the path. If the two paths have different num-
ber of nodes, the kernel assigns 0 (no-match) to the
pair. Formally, the kernel is defined as:
K(s, t) =
{
0, if |s| 6= |t|
?n
i=1 c(si, ti), if |s| = |t|
(3)
5-fold CV on ACE 2003
kernel method Precision Recall F1
subsequence 0.703 0.389 0.546
dependency tree 0.681 0.290 0.485
shortest path 0.747 0.376 0.562
Table 1: Results of different kernels on ACE 2003
training set using 5-fold cross-validation.
4 Experiments and Analysis
We implemented the above three kernels and con-
ducted a set of experiments to compare these ker-
nels. By minimizing divergence in our experiment
setup and implementation for these kernels, we hope
to reveal intrinsic properties of different kernels.
4.1 Experiment setup
We conducted experiments using the ACE 2003
standard evaluation set. Training set of this collec-
tion contains 674 doc and 9683 relations. The test
set contains 97 doc and 1386 relations. 5 entity types
(Person, Organization, Location, Facilities and Geo-
political Entities) and 5 top-level relation types (At,
Near, Part-of, Role and Social) are manually anno-
tated in this collection. Since no development set is
given, we report results in this section only on the
training set, using 5-fold cross-validation, and de-
fer the comparison of results on the test set till Sec-
tion 6. Corpus preprocessing is done as the follow-
ing: sentence segmentation was performed using the
tool from CCG group at UIUC 1; words are then to-
kenized and tagged with part-of-speech using MX-
POST (Ratnaparkhi, 1996) and dependency parsing
is performed using MSTParser (McDonald et al,
2005a). We used the SVM-light (Joachims, 2002)
toolkit and augmented it with our custom kernels.
SVM parameters are chosen using cross-validation
(C=2.4), and the decaying factor in all kernels are
uniformally set to be 0.75. We report precision (P),
recall (R) and F-measure (F) on the training (5-fold
cross-validation) and test set.
4.2 Comparison of Kernels
In table 1 we listed results of the above three kernels
on the training set using 5-fold cross-validation. A
1http://l2r.cs.uiuc.edu/?cogcomp/atool.
php?tkey=SS
844
first glimpse of the results tells us that the shortest
path kernel performs the best in terms of F-measure,
while the dependency tree kernel did the worst. The
performance of subsequence kernel is not as good
as the dependency path kernel, but the difference is
small. In particular, the subsequence kernel gave the
best recall, whereas the dependency path kernel gave
the highest precision.
To understand why shortest path kernel performs
better than the subsequence kernel, let us review the
definition of these two kernels. The subsequence
kernel considers all subsequences of feature vector
sequences that are formed by all words occurred in-
between two entities in a sentence; while the shortest
path kernel only considers feature vector sequences
formed by words that are connected through a de-
pendency path. In general, the sequences consid-
ered in the dependency path kernel are more com-
pact than the sequences used in the subsequence ker-
nel. Actually, in most cases the dependency path se-
quence is indeed one particular subsequence of the
entire subsequence used in subsequence kernel. Ar-
guably, this particular subsequence is the one that
captures the most important syntactic information.
Although the feature spaces of the dependency path
kernels are not subsets of the subsequence kernel,
we can clearly see that we get higher precisions
by introducing bias towards the syntactically more
meaningful feature space.
However, the dependency path kernel is fairly
rigid and imposes many hard constraints such as re-
quiring the two paths to have exactly the same num-
ber of nodes. This restriction is counter-intuitive. To
illustrate this, let us reconsider the example given in
Section 3. In that example, it is obviously the case
that the two instances of relations have very similar
dependency path connecting the entities. However,
the second path is one node longer than the first path,
and therefore the dependency path kernel will de-
clare no match for them. The subsequence kernel, on
the other hand, considers subsequence matching and
therefore inherently incorporates a notion of fuzzy
matching. Furthermore, we have observed from the
training data that many short word sequences carry
strong relational information; hence only part of the
entire dependency path is truly meaningful in most
cases. It also helps to understand why subsequence
kernel has better recall than dependency path kernel.
ACE 2003 test set
kernel method Precision Recall F1
subsequence 0.673 0.499 0.586
dependency tree 0.621 0.362 0.492
shortest path 0.691 0.462 0.577
convolution dep. path 0.725 0.541 0.633
(Zhang et al, 2006b) 0.773 0.656 0.709
Table 2: Results on the ACE 2003 test set. We ref-
erence the best-reported score (in italic) on this test
set, given by (Zhang et al, 2006b)
The disappointing performance of the depen-
dency tree kernel can also be explained by our anal-
ysis. Although the dependency tree kernel performs
subsequence matching for nodes at each depth level,
it is unclear what the relative syntactic or semantic
relation is among sibling nodes in the dependency
tree. The sequence formed by sibling nodes is far
less intuitive from a linguistic point of view than the
sequence formed by nodes on a dependency path.
To summarize the above results, we found that de-
pendency path kernel benefits from a reduction in
feature space by using syntactic dependency infor-
mation. But the subsequence kernel has an edge in
recall by allowing fuzzy matching and expanding the
feature space into convolution space. We will show
in the following section that these two benefits are
complementary and can be combined to give better
performance.
5 Combining the Benefits ? A New Kernel
It is a natural extension to combine the two bene-
fits that we have identified in the previous section.
The idea is simple: we want to allow subsequence
matching in order to gain more flexibility and there-
fore higher recall, but constrain the sequence from
which to deduce subsequences to be the dependency
path sequence. We call the combined kernel a ?con-
volution dependency path kernel?.
6 Final Test Results
We obtained the final results on the test set of the
ACE 2003 collection, using the same experimental
setting as above. The results are listed in Table 2.
From the table we can see that the performances of
the previous three kernels hold up qualitatively on
845
the test set as cross-validation on training set. There
is one exception that the shortest path kernel?s F-
measure score is no longer better than the subse-
quence kernel on the test set, but the difference is
small. And our new convolution dependency path
kernel beats all above three kernels in precision, re-
call and F-measure, suggesting that our analysis is
accurate and the benefits we outlined are truly com-
plementary.
Comparing to the best reported results on the
same test set from (Zhang et al, 2006b), our scores
are not as high, but the results are quite competitive,
given our minimum efforts on tuning kernel param-
eters and trying out kernel combinations.
7 Conclusion
We re-examined three existing kernel methods for
relation extraction. We conducted experiments on
the standard ACE 2003 evaluation set and showed
that whereas some kernels are less effective than
others, they exhibit properties that are complemen-
tary to each other. In particular, we found that rela-
tion extraction can benefit from increasing the fea-
ture space through convolution kernel and introduc-
ing bias towards more syntactically meaningful fea-
ture space. Drawn from our analysis, we proposed
a new convolution dependency path kernel which
combines the benefits of the subsequence kernel and
shortest path dependency kernel. Comparing with
previous kernels, our new kernel consistently and
significantly outperforms all three previous kernels,
suggesting that our analyses of the previously pro-
posed kernels are correct.
References
R. C. Bunescu and R. J. Mooney. 2005a. A shortest path
dependency kernel for relation extraction. In Proceed-
ings of HLT/EMNLP.
R. C. Bunescu and R. J. Mooney. 2005b. Subsequence
kernels for relation extraction. In Proceedings of
NIPS.
M. Collins and N. Duffy. 2001. Convolution kernels for
natural language. In Proceedings of NIPS.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine Learning, 20(3):273?297.
N. Cristianini and J. Shawe-Taylor. 2000. An Introduc-
tion to Support-vector Machines. Cambridge Univer-
sity Press.
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proceedings of ACL.
C. M. Cumby and D. Roth. 2003. On kernel methods for
relation learning. In Proceedings of ICML.
C. Giuliano, A. Lavelli, and L. Romano. 2006. Ex-
ploiting shallow linguistic information for relation ex-
traction from biomedical literature. In Proceedings of
EACL.
J. Jiang and C. Zhai. 2007. A systematic exploration of
the feature space for relation extraction. In Proceed-
ings of NAACL-HLT.
T. Joachims. 2002. Learning to Classify Text Using Sup-
port Vector Machines. Ph.D. thesis, Universit??at Dort-
mund.
N. Kambhatla. 2004. Combining lexical, syntactic and
semantic features with maximum entropy models for
extracting relations. In Proceedings of ACL.
H. Lodhi, C Saunders, J. Shawe-Taylor, N. Cristianini,
and C Watkins. 2002. Text classification using string
kernels. JMLR, 2:419?444.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
R. McDonald, F. Pereira, S. Kulick, S. Winters, Y. Jin,
and P. White. 2005b. Simple algorithms for complex
relation extraction with applications to biomedical ie.
In Proceedings of ACL.
A. Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of EMNLP.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel
methods for relation extraction. JMLR, 3:1083?1106.
M. Zhang, J. Zhang, and J. Su. 2006a. Exploring syntac-
tic features for relation extraction using a convolution
tree kernel. In Proceedings of NAACL-HLT.
M. Zhang, J. Zhang, J. Su, and G. Zhou. 2006b. A com-
posite kernel to extract relations between entities with
both flat and structured features. In Proceedings of
ACL.
S. Zhao and R. Grishman. 2005. Extraction relations
with integrated information using kernel methods. In
Proceedings of ACL.
G. Zhou, S. Jian, J. Zhang, and M. Zhang. 2005. Ex-
ploring various knowledge in relation extraction. In
Proceedings of ACL.
846
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1164?1172,
Beijing, August 2010
Probabilistic Tree-Edit Models with Structured Latent Variables for
Textual Entailment and Question Answering
Mengqiu Wang
Computer Science Department
Stanford University
mengqiu@cs.stanford.edu
Christopher D. Manning
Computer Science Department
Stanford University
manning@cs.stanford.edu
Abstract
A range of Natural Language Process-
ing tasks involve making judgments about
the semantic relatedness of a pair of sen-
tences, such as Recognizing Textual En-
tailment (RTE) and answer selection for
Question Answering (QA). A key chal-
lenge that these tasks face in common
is the lack of explicit alignment annota-
tion between a sentence pair. We capture
the alignment by using a novel probabilis-
tic model that models tree-edit operations
on dependency parse trees. Unlike previ-
ous tree-edit models which require a sep-
arate alignment-finding phase and resort
to ad-hoc distance metrics, our method
treats alignments as structured latent vari-
ables, and offers a principled framework
for incorporating complex linguistic fea-
tures. We demonstrate the robustness of
our model by conducting experiments for
RTE and QA, and show that our model
performs competitively on both tasks with
the same set of general features.
1 Introduction
Many complex Natural Language Processing
(NLP) applications can be broken down to a sub-
task of evaluating the semantic relationship of
pairs of sentences (e.g., in Question Answering,
answer selection involve comparing each answer
candidate against the question). This means that
research aiming at analyzing pairs of semanti-
cally related natural language sentences is promis-
ing because of its reusability: it is not tied to
a particular internal representation of meanings,
but it nevertheless serves as a first step towards
full meaning understanding, which is applicable
to a number of applications. At the same time,
this paradigm clearly defines the input and output
space, facilitating system comparison and stan-
dard evaluation. Tasks of this paradigm have
drawn much of the focus in recent NLP research,
including Recognizing Textual Entailment (RTE),
answer selection for Question Answering (QA),
Paraphrase Identification (PI), Machine Transla-
tion Evaluation (MTE), and many more.
In each of these tasks, inputs to the systems are
pairs of sentences that may or may not convey the
desired semantic property (e.g., in RTE, whether
the hypothesis sentence can be entailed from the
premise sentence; in QA, whether the answer can-
didate sentence correctly answers the question),
and the output of the system is a binary classifi-
cation decision (or a regression score,as in MTE).
Earlier studies in these domains have concluded
that simple word overlap measures (e.g., bag of
words, n-grams) have a surprising degree of util-
ity (Papineni et al, 2002; Jijkoun and de Ri-
jke, 2005b), but are nevertheless not sufficient for
these tasks (Jijkoun and de Rijke, 2005a). A com-
mon problem identified in these earlier systems is
the lack of understanding of the semantic relation
between words and phrases. Later systems that
include more linguistic features extracted from re-
sources such as WordNet have enjoyed more suc-
cess (MacCartney et al, 2006). Studies have also
shown that certain prominent syntactic features
are often found beneficial (Snow et al, 2006).
More recent studies gained further leverage from
systematic exploration of the syntactic feature
space through analysis of parse trees (Wang et al,
1164
2007; Das and Smith, 2009).
There are two key challenges imposed by these
tasks. The first challenge has to do with the hidden
alignment structures embedded in the sentence
pairs. It is straightforward to see that in order
to extract word-matching and/or syntax-matching
features, inevitably one has to consider the align-
ment between words and/or syntactic parts. These
alignments are not given as inputs, and it is a
non-trivial task to decide what the correct align-
ment is. Alignment-based approach have been
proven effective by many RTE, QA and MTE sys-
tems (Haghighi et al, 2005; Wang et al, 2007;
MacCartney et al, 2008; Das and Smith, 2009,
inter alia). Although alignment is a commonly
used approach, it is not the only one. Other stud-
ies have successfully applied theorem proving and
logical induction techniques, translating both sen-
tences to knowledge representations and then do-
ing inference on these representations (Moldovan
et al, 2003; Raina et al, 2005; de Salvo Braz
et al, 2005; MacCartney and Manning, 2007, in-
ter alia).
A second challenge arises when a system needs
to combine various sources of evidence (i.e., sur-
face text features, semantic features, and syntactic
features) to make a global classification decision.
Quite often these features are heavily overlapping
and sometimes contradicting, and thus a robust
learning scheme that knows when to activate what
feature is desired. Traditional approaches employ
a two-stage or multi-stage model where tasks are
broken down into alignment finding, feature ex-
traction, and feature learning subtasks (Haghighi
et al, 2005; MacCartney et al, 2008). The align-
ment finding task is typically done by commit-
ting to a one best alignment, and subsequent fea-
tures are extracted only according to this align-
ment. A large body of literature in joint learning
has demonstrated that such an approach can suffer
from cascaded errors at testing, and does not ben-
efit from the potential for joint learning (Finkel et
al., 2006).
In this paper, we present a novel undirected
graphical model to address these challenges. A
promising approach to these challenges is model-
ing the alignment as an edit operation sequence
over parse tree representation, an approach pio-
neered by (Punyakanok et al, 2004; Kouylekov
and Magnini, 2006; Harmeling, 2007; Mehdad,
2009). We improve upon this earlier work by
showing how alignment structures can be inher-
ently learned as structured latent variables in our
model. Tree edits are represented internally as
state transitions in a Finite-State Machine (FSM),
and our model is parameterized as a Condi-
tional Random Field (CRF) (Lafferty et al, 2001),
which allows us to incorporate a diverse set of ar-
bitrarily overlapping features.
In comparison to previous work that exploits
various ad-hoc or heuristic ways of incorporating
tree-edit operations, our model provides an ele-
gant and much more principled way of describing
tree-edit operations in a probabilistic setting.
2 Tree-edit CRF for Classification
A training instance consists of a pair of sentences
and an associated binary judgment. In RTE, for
example, the input sentence pairs is made up of
a text sentence (e.g., Gabriel Garcia Marquez is
a novelist and winner of the Nobel prize for lit-
erature.) and a hypothesis sentence (e.g., Gabriel
Garcia Marquez won the Nobel for Literature.).
The pair is judged to be true if the hypothesis can
be entailed from the text (e.g., the answer is true
for the example sentence pair).
Formally, we denote the text sentence as txt and
the hypothesis sentence as hyp, and denote their
labeled dependency parse trees as ?t and ?h, re-
spectively. We use the binary variable z ? {0,1}
to denote the judgment.
The generative story behind our model is a
parse tree transformation process. ?t is trans-
formed into ?h through a sequence of tree ed-
its. Examples of tree edits are delete child, in-
sert parent, and substitute current. An edit se-
quence e = e1 . . .em is valid if ?t can be success-
fully turned into ?h according to e. An example of
a trivial valid edit sequence is one that first deletes
all nodes in ?t then inserts all nodes in ?h.
Delete, insert and substitute form the three ba-
sic edit operations. Each step in an edit sequence
is also linked with current edit positions in both
trees, denoted as e.p = e1.p . . .em.p. We index
the tree nodes using a level-order tree traversal
scheme (i.e., root is visited first and assigned in-
1165
dex 0, then each one of the first level children
of the root is visited in turn, and assigned an in-
dex number incremented by 1). It is worth noting
that every valid edit sequence has a correspond-
ing alignment mapping. Nodes that are inserted
or deleted are aligned to null, and nodes that are
substituted are aligned. One can find many edit
sequence for the same alignment, by altering the
order of edit operations.
We extend these basic edit operations into more
elaborate edit operations based on the linguistic
and syntactic properties of the current tree nodes
that they fire on. For example, the following are
all possible edit operations: delete a noun that is
SUB of the root, delete a named-entity of type
PERSON, substitute roots of the tree. In our
experiments, we designed a set of 45 edit op-
erations (12 delete, 12 insert and 21 substitute).
More details of the edit operations are described
in ?4. Depending on the specific application do-
main, more sophisticated and verbose tree edit op-
erations can be designed and easily incorporated
into our model. In particular, tree edit opera-
tions involving deleting, inserting or substituting
entire treelets seem interesting and promising, re-
quiring merely a simple extension to the forward-
backward dynamic programming.
Next, we design a Finite-State Machine (FSM)
in which each edit operation is mapped to a unique
state, and an edit sequence is mapped into a tran-
sition sequence among states (denoted as e.a =
e1.a . . .em.a). In brief, an edit sequence is as-
sociated with a sequence of edit positions in the
trees (e.p = e1.p . . .em.p), as well as a transition
sequence among states (e.a= e1.a . . .em.a).
The probability of an edit sequence e given the
parse trees is defined as:
P(e | ?t,?h) = 1Z
|e|?
i=1
exp ? ? f(ei?1,ei,?t,?h) (1)
where f are feature functions, ? are associated fea-
ture weights, and Z is the partition function to be
defined next.
Recall that our training data is composed of not
only positive examples but also negative exam-
ples. In order to take advantage of this label in-
formation, we adopt an interesting discriminative
learning framework first introduced by McCallum
et al (2005). We call the FSM state set described
above the positive state set (S1), and duplicate the
exact same set of states, and call the new set nega-
tive state set (S0). We then add a starting state(Ss),
and add non-deterministic transitions from Ss to
every state in S1. We then add the same transi-
tions for S0. We now arrive at a new FSM struc-
ture where upon arriving at the starting state, one
makes a non-deterministic decision to enter either
the positive set or the negative set and stay in that
set until reaching the end of the edit sequence,
since no transitions are allowed across the positive
and negative set. Each edit operation sequence
can now be associated with a sequence of posi-
tive states as well as a sequence of negative states.
The intuitive idea is that during training, we want
to maximize the weights of the positive examples
in the positive state set and minimize their weights
in the negative state set, and vice versa. In other
words, we want the positive state set to attract
positive examples but push away negative exam-
ples. Figure 1 illustrates two example valid edit
sequences in the FSM, one in the positive state set
and one in the negative state set.
Formally, the partition function Z in (1) is de-
fined as the sum of weights of all valid edit se-
quences in both the positive set and negative set.
Features extracted from positive states are disjoint
from features extracted from negative states.
Z = ?
e: e.a?Ss+{S0?S1}?
|e|?
i=1
exp ? ? f(ei?1,ei,?t,?h)
Recall z ? {0,1} is the binary judgment indi-
cator variable. The conditional probability of z is
obtained by marginalizing over all edit sequences
that have state transitions in the state set corre-
sponding to z:
P(z | ?t,?h) = ?
e: e.a?Ss+S?z
P(e | ?t,?h) (2)
The L2-norm penalized log-likelihood over n
training examples (L) is our training objective
function:
L=
n?
j=1
log(P(z( j) | ?( j)t ,?( j)h ))? ???
2
2?2 (3)
At test time, the z with higher probability is taken
as our prediction outcome.
1166
Figure 1: This diagram illustrates the FSM architecture. There is a single start state, and we can transit into either the positive
state set (nodes that are not shaded), or the negative state set (shaded nodes). Here we show two examples of valid edit
sequences. They result in the same alignment structure as show in the bottom half of the diagram (dotted lines across the two
sentences are alignment links). Numbers over the arcs in the state diagram denote the edit sequence index, and numbers under
each word in the parse tree diagram denote each node?s level-order index number.
3 Parameter Estimation
We used Expectation Maximization method since
the objective function given in (3) is non-convex.
In the M-step, finding the optimal parameters un-
der the current model expectation involves com-
puting forward-backward style dynamic program-
ming (DP) in a three-dimensional table (two for
inputs and one for states) and optimization using
L-BFGS method. In practice the resulting DP ta-
ble can be quite large (for a sentence pair of length
100, and 2 sets of 45 states, we obtain 900,000 en-
tries). We improved efficiency by pruning out par-
tial sequences that do not lead to a complete valid
sequence and pre-compute the state-transition ta-
ble and features.
4 Edit Operations
Table 1 lists the groups of edit operations we de-
signed and their descriptions. Not shown in the
table are three default edits ( insert, delete and
substitute), which fire when none of the more spe-
cific edit operations match. Edit operations listed
in the the top-left section capture basic match-
ing, deletion and insertion of surface text, part-of-
speech tags and named-entity tags. The top-right
section capture alignments of semantically related
words, based on relational information extracted
from various linguistic resources, such as Word-
Net and NomBank. And the bottom section cap-
ture syntactic edits. Note that multiple edit opera-
tions can fire at the same edit position if conditions
are matched (e.g., we can choose to delete if there
are more words to edit in txt, or to insert if there
are more words to edit in hyp).
5 Features
One of the most distinctive advantages of our
model compared to previous tree-edit based mod-
els is the ability to include a wide range of non-
independent, rich linguistic features. The features
we employed can be broken down into two cat-
egories. The first category is zero-order features
that model the current edit step. They consist of
a conditioning property of the current edit, and
the current state in the FSM. The second cate-
gory is first-order features that capture state tran-
sitions, by concatenating the current FSM state
with the previous FSM state. One simple form of
zero-order feature is the current FSM state itself.
The FSM states already carry a lot of information
about the current edits. Conditioning properties
are used to further describe the current edit. They
are often more fine-grained and complex (e.g.,
1167
Surface edits Semantic edits
{I,D,S}-{POS} insert/delete/substitute words of a POS type, S-SYNONYM substitute two words that are synonymswhere POS is noun, verb or proper noun S-HYPERNYM substitute two words that are hypernyms
{I,D,S}-NE insert/delete/substitute named-entity words S-ANTONYM substitute two words that are antonyms
{I,D,S}-LIKE insert/delete/substitute words that expresses likeli-hood, e.g., maybe, possibly S-ACRONYM
substitute two words in which one is an acronym of
the other
{I,D,S}-MODAL insert/delete/substitute modal verbs, e.g., can,could, may S-NOMBANK
substitute two words that are related according to
NomBank
S-{SAME/DIFF} the words being substituted are the same or differ-ent S-NUM-0,1
substitute two words that are both numerical val-
ues, and 1 if they match, 0 if they mismatch
Syntactic edits
{I,D,S}-ROOT insert/delete/substitute root of the trees
{I,D,S}-{REL} insert/delete/substitute a tree node of grammatical relation type, where REL is either SUB, OBJ, VC or PRD
Table 1: List of edit operations. I for INSERT, D for DELETE, and S for SUBSTITUTE.
syntactic-matching conditions listed below). To
give an example, in Figure 1, the second edit oper-
ation in the example sequence is S-NE. A match-
ing condition feature that fires with this state could
be substitute NE type PERSON, which tells us
exactly what type of named-entity is being sub-
stituted.
It is notable that in designing edit operations
and features, there is a continuum of choice in
terms of how much information to be encoded as
features versus edit operations. To better illustrate
the trade-off, consider the two extreme cases of
this continuum. At one extreme, we can design a
system where there are only three basic edit op-
erations, and all extra information in our current
set of edit operations can be encoded as features.
For example, in this case edit operation S-NE
would become S with feature substitute NE. The
other extreme is to encode every zero-order fea-
ture as a separate edit operation. The amount
of information encoded in the zero-order features
and edit operations is the same in both cases, but
the difference lies in first-order features and ef-
ficiency. When encoding more information as
edit operations (and thus more states in FSM),
first-order features become much more expres-
sive; whereas when encoding more information
as features, computation becomes cheaper as the
number of possible state transition sequences is
reduced. In our experiments, we aim to keep a
minimal set of edit operations that are meaning-
ful but not overly verbose, and encode additional
information as features. Each feature is a binary
feature initialized with weight 0.
Due to space limitation, we list the most im-
portant zero-order features. Many of these fea-
tures are inspired by MacCartney et al (2006)
and Snow et al (2006), but not as sophisticated.
Word matching features. These features detect
if a text word and a hypothesis word match the
following conditions:
1. have the same lemma
2. one is a phrase and contains the other word
3. are multi-word phrases and parts match
4. have the same/different named-entity type(s) + the
named-entity type(s)
Tree structure features. These features try to
capture syntactic matching/mismatching informa-
tion from the labeled dependency parse trees. 1.
whether the roots of the two trees are aligned
2. parent-child pair match
3. (2.) and labels also match
4. (2.) and labels mismatch
5. (4.) and detailing the mismatching labels
6. parent+label match, child mismatch
7. child and label match, parents are {hyper/syno/anto}nym
8. looking for specific SUB/OBJ/PRD construct as in Snow
et al (2006).
6 Preprocessing
In all of our experiments, each input pair of
text and hypothesis sentence is preprocessed as
following: Sentences were first tokenized by
the standard Penn TreeBank tokenization script,
and then we used MXPOST tagger (Ratnaparkhi,
1996) for part-of-speech (POS) tagging. POS
tagged sentences were then parsed by MST-
Parser (McDonald et al, 2005) to produce labeled
dependency parse trees. The parser was trained
1168
on the entire Penn TreeBank. The last step in the
pipeline is named-entity tagging using Stanford
NER Tagger (Finkel et al, 2005).
7 RTE Experiments
Given an input text sentence and a hypothesis
sentence, the task of RTE is to make predictions
about whether or not the hypothesis can be en-
tailed from the text sentence. We use standard
evaluation datasets RTE1-3 from the Pascal RTE
Challenges (Dagan et al, 2006). For each RTE
dataset, we train a tree-edit CRF model on the
training portion and evaluate on the testing por-
tion. We report accuracy of classification results,
and precision and recall for the true entailment
class. There is a balanced positive-negative sam-
ple distribution in each dataset, so a random base-
line gives 50% classification accuracy. We used
RTE1 for feature selection and tuning ? in the L2
regularizer (? = 5 was used). RTE2 and RTE3
were reserved for testing.
Our system is compared with four systems
on RTE2 and three other systems on the RTE3
dataset.1 We chose these systems for compari-
son because they make use of syntactic depen-
dencies and lexical semantic information. No-
tably other systems that give state-of-the-art per-
formance on RTE use non-comparable techniques
such as theorem-proving and logical induction,
and often involve significant manual engineering
specifically for RTE, thus do not make meaningful
comparison to our model.
For RTE2, Kouylekov and Magnini (2006) ex-
perimented with various TED cost functions and
found a combination scheme to work the best for
RTE. Vanderwende et al (2006) used syntactic
heuristic matching rules with a lexical-similarity
back-off model. Nielsen et al (2006) extracted
features from dependency path, and combined
themwith word-alignment features in a mixture of
experts classifier. Zanzotto et al (2006) proposed
a syntactic cross-pair similarity measure for RTE.
For RTE3, Harmeling (2007) took a similar
classification-based approach with transformation
sequence features. Marsi et al (2007) described
a system using dependency-based paraphrasing
1Different systems are used for comparison because none
of these systems reported performance on both datasets.
RTE2 Acc.% Prec.% Rec.%
Vanderwende et al, 2006 60.2 59.0 67.0
K&M, 2006 60.5 58.9 70.0
Nielsen et al, 2006 61.1 59.0 73.3
Zanzotto et al, 2006 63.9 60.8 78.0
Tree-edit CRF 63.0 61.7 68.5
RTE3 Acc.% Prec.% Rec.%
Marsi et al, 2007 59.1 - -
Harmeling, 2007 59.5 - -
de Marneffe et al, 2006 60.5 61.8 60.2
Tree-edit CRF 61.1 61.3 65.3
Table 2: Results on RTE2 and RTE3 dataset. Results for de
Marneffe et al (2006) were reported by MacCartney and
Manning (2008).
techniques for RTE. de Marneffe et al (2006) de-
scribed a system where best alignments between
the sentence pairs were first found, then classifi-
cation decisions were made based on these align-
ments.
Table 2 presents RTE results. Our model per-
forms competitively on both datasets. On RTE2,
our model gives second best performance among
the methods we compare against, and the differ-
ence in accuracy from the best system is quite
small (7 out of 800 examples). We observe a
larger gap in recall, suggesting our method tends
to give higher precision, which is also commonly
found in other syntax-based systems (Snow et al,
2006). It is worth noting that Zanzotto et al
(2006) achieved second place in the official RTE2
evaluation. On RTE3, our model outperforms the
other syntax-based systems compared. In partic-
ular, out system gives the same precision level as
the second best system (de Marneffe et al, 2006)
without sacrificing as much recall, which is the
most common drawback found in syntax-based
systems.
8 QA Experiments
A second Tree-edit CRF model was trained for
the task of answer selection for Question Answer-
ing. In this task, the input pair consists of a short
factoid question (e.g., Who beat Floyd Patterson
to take the title away?) and an answer candidate
sentence (e.g., He saw Ingemar Johansson knock
down Floyd Patterson seven times there in win-
ning the heavyweight title.). The pair is judged
positive if the answer candidate sentence correctly
answers the question and provides sufficient con-
1169
System MAP MRR
Punyakanok et al, 2004 0.4189 0.4939
Cui et al, 2005 0.4350 0.5569
Wang et al, 2007 0.6029 0.6852
H&S, 2010 0.6091 0.6917
Tree-edit CRF 0.5951 0.6951
Table 3: Results on QA task reported in Mean Average Pre-
cision (MAP) and Mean Reciprocal Rank (MRR).
textual support (i.e., does not merely contain the
answer key, for example, ?Ingemar Johansson
was a world heavyweight champion? would not
be a correct answer). We followed the same ex-
perimental setup as Wang et al (2007) and Heil-
man and Smith (2010). The training portion of
the dataset consists of 5919 manually judged Q/A
pairs from previous QA tracks at Text REtrieval
Conference (TREC 8?12). There are also 1374
Q/A pairs for development and 1866 Q/A pairs
for testing, both from the TREC 3 evaluation. The
task is framed as a sentence retrieval task, and thus
Mean Average Precision (MAP) and Mean Recip-
rocal Rank (MRR) are reported for the ranked list
of most probable answer candidates. We com-
pare out model with four other systems. Wang et
al. (2007) proposed a Quasi-synchronous Gram-
mar formulation of the problem which also mod-
els alignment as structured latent variables, but in
a generative probabilistic model. Their method
gives the current state-of-the-art performance on
this task. Heilman and Smith (2010) presented
a classification-based approach with tree-edit fea-
tures extracted from a tree kernel. Cui et al
(2005) developed a dependency-tree based in-
formation discrepancy measure. Punyakanok et
al. (2004) used a generalized Tree-edit Distance
method to score mappings between dependency
parse trees. All systems were evaluated against
the same dataset as the one we used. Results of
replicated systems for the last two were reported
by Wang et al (2007), with lexical-semantic aug-
mentation from WordNet.
Results in Table 3 show that our model gives the
same level of performance as Wang et al (2007),
with no statistically significant difference (p > 5
in sign test). Both systems out-perform the other
two earlier systems significantly.
9 Discussion
Our experiments on RTE and QA applications
demonstrated that Tree-edit CRF models provide
results competitive with previous syntax-based
methods. Even though the improvements were
quite moderate in some cases, the important point
is that our model provides a novel principled
framework. It works across different problem do-
mains with minimal domain knowledge and fea-
ture engineering, whereas previous methods are
only engineered for a particular task and are hard
to generalize to new problems.
While the current Tree-edit CRF model can
model a large set of linguistic phenomenon and
tree-transformations, it has some clear limitations.
One of the biggest drawbacks is the lack of sup-
port for modeling phrasal re-ordering, which is a
very common and important linguistic phenom-
ena. It is not straightforward to implement re-
ordering in the current model because it breaks
the word-order constraint which admits tractable
forward-backward style dynamic programming.
However, this shortcoming can be addressed par-
tially by extending the model to deal with con-
strained re-ordering per Zhang (1996).
10 Related Work
Tree Edit Distance (TED) have been studied
extensively in theoretical and algorithmic re-
search (Klein, 1989; Zhang and Shasha, 1989;
Bille, 2005). In recent years we have seen many
work on applying TED based methods for NLP-
related tasks (Punyakanok et al, 2004; Kouylekov
and Magnini, 2006; Harmeling, 2007; Mehdad,
2009). Mehdad (2009) proposed a method based
on particle swarm optimization technique to au-
tomatically learn the TED cost function. Another
work that also developed an interesting approach
to stochastic tree edit distance is Bernard et al
(2008), but unfortunately experiments in the pa-
per were limited to digit recognition and tasks on
small artificial datasets.
Many different approaches to modeling
sentence alignment have been proposed be-
fore (Haghighi et al, 2005; MacCartney et al,
2008). Haghighi et al (2005) treated alignment
finding in RTE as a graph matching problem
1170
between sentence parse trees. MacCartney et
al. (2008) described a phrase-based alignment
model for MT, trained by the Perceptron learning
algorithm. A line of work that offers similar
treatment of alignment to our model is the
Quasi-synchronous Grammar (QG) (Smith and
Eisner, 2006; Wang et al, 2007; Das and Smith,
2009). QG models alignments between two parse
trees as structured latent variables. The generative
story of QG describes one that builds the parse
tree of one sentence, loosely conditioned on the
parse tree of the other sentence. This formalism
prefers but is not confined to tree isomorphism,
therefore possesses more model flexibility than
synchronous grammars.
The work of McCallum et al (2005) inspired
the discriminative training framework that we
used in our experiments. They presented a String
Edit Distance model that also learns alignments as
hidden structures for simple tasks such as restau-
rant name matching.
Our work is also closely related to other re-
cent work on learning probabilistic models involv-
ing structural latent variables (Clark and Curran,
2004; Petrov et al, 2007; Blunsom et al, 2008;
Chang et al, 2010). The Tree-edit CRF model we
present here is a new addition to this family of in-
teresting models for discriminative learning with
structural latent variables.
11 Conclusion
We described a Tree-edit CRF model for predict-
ing semantic relatedness of pairs of sentences.
Our approach generalizes TED in a principled
probabilistic model that embeds alignments as
structured latent variables. We demonstrate a
wide-range of lexical-semantic and syntactic fea-
tures can be easily incorporated into the model.
Discriminatively trained, the Tree-edit CRF led to
competitive performance on the task of Recogniz-
ing Textual Entailment and answer selection for
Question Answering.
References
Bernard, M., L. Boyer, A. Habrard, and M. Sebban.
2008. Learning probabilistic models of tree edit dis-
tance. Pattern Recognition, 41(8):2611?2629.
Bille, P. 2005. A survey on tree edit distance and
related problems. Theoretical Computer Science,
337(1-3):217?239.
Blunsom, P., T. Cohn, and M. Osborne. 2008. A dis-
criminative latent variable model for statistical ma-
chine translation. In Proceedings of ACL-HLT.
Chang, Ming-Wei, Dan Goldwasser, Dan Roth, and
Vivek Srikumar. 2010. Discriminative learning
over constrained latent representations. In Proceed-
ings of NAACL-HLT.
Clark, S. and J. R. Curran. 2004. Parsing the wsj using
ccg and log-linear models. In Proceedings of ACL.
Cui, Hang, Renxu Sun, Keya Li, Min-Yen Kan, and
Tat-Seng Chua. 2005. Question answering passage
retrieval using dependency relations. In Proceed-
ings of SIGIR.
Dagan, I., O. Glickman, and B. Magnini. 2006. The
pascal recognising textual entailment challenge.
Machine Learning Challenges, LNCS, 3944:177?
190.
Das, Dipanjan and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of ACL-IJCNLP.
de Marneffe, M.-C., B. MacCartney, T. Grenager,
D. Cer, A. Rafferty, and C. D. Manning. 2006.
Learning to distinguish valid textual entailments.
In Proceedings of the second PASCAL Challenges
Workshop on RTE.
de Salvo Braz, R., R. Girju, V. Punyakanok, D. Roth,
and M. Sammons. 2005. An inference model for
semantic entailment and question-answering. In
Proceedings of AAAI.
Finkel, J. R., T. Grenager, and C. D. Manning. 2005.
Incorporating non-local information into informa-
tion extraction systems by gibbs sampling. In Pro-
ceedings of ACL.
Finkel, J. R., C. D. Manning, and A. Y. Ng. 2006.
Solving the problem of cascading errors: Approx-
imate bayesian inference for linguistic annotation
pipelines. In Proceedings of EMNLP.
Haghighi, A., A. Y. Ng, and C. D. Manning. 2005. Ro-
bust textual inference via graph matching. In Pro-
ceedings of EMNLP.
Harmeling, S. 2007. An extensible probabilistic
transformation-based approach to the third recog-
nizing textual entailment challenge. In Proceedings
of ACL PASCAL Workshop on Textual Entailment
and Paraphrasing.
1171
Heilman, M. and N. A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings
of NAACL-HLT.
Jijkoun, V. and M. de Rijke. 2005a. Recognizing tex-
tual entailment: Is word similarity enough?. In Ma-
chine Learning Challenge Workshop, volume 3944
of LNCS, pages 449?460. Springer.
Jijkoun, V. and M. de Rijke. 2005b. Recognizing tex-
tual entailment using lexical similarity. In Proceed-
ings of the PASCAL Challenges Workshop on RTE.
Klein, P. N. 1989. Computing the edit-distance be-
tween unrooted ordered trees. In Proceedings of
European Symposium on Algorithms.
Kouylekov, M. and B. Magnini. 2006. Tree edit dis-
tance for recognizing textual entailment: Estimating
the cost of insertion. In Proceedings of the second
PASCAL Challenges Workshop on RTE.
Lafferty, J., A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
MacCartney, Bill and Christopher D. Manning. 2007.
Natural logic for textual inference. In Proceedings
of Workshop on Textual Entailment and Paraphras-
ing at ACL 2007.
MacCartney, B. and C. D. Manning. 2008. Model-
ing semantic containment and exclusion in natural
language inference. In Proceedings of COLING.
MacCartney, B., T. Grenager, M.-C. de Marneffe,
D. Cer, and C. D. Manning. 2006. Learning to
recognize features of valid textual entailments. In
Proceedings of HLT-NAACL.
MacCartney, B., M. Galley, and C. D. Manning. 2008.
A phrase-based alignment model for natural lan-
guage inference. In Proceedings of EMNLP.
Marsi, E., E. Krahmer, and W. Bosma. 2007.
Dependency-based paraphrasing for recognizing
textual entailment. In Proceedings of ACL PASCAL
Workshop on Textual Entailment and Paraphrasing.
McCallum, A., K. Bellare, and F. Pereira. 2005.
A conditional random field for discriminatively-
trained finite-state string edit distance. In Proceed-
ings of UAI.
McDonald, R., K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
Mehdad, Yashar. 2009. Automatic cost estimation for
tree edit distance using particle swarm optimization.
In Proceedings of ACL.
Moldovan, D., C. Clark, S. Harabagiu, and S. Maio-
rano. 2003. Cogex: A logic prover for question
answering. In Proceedings of HLT-NAACL.
Nielsen, R. D., W. Ward, and J. H. Martin. 2006. To-
ward dependency path based entailment. In Pro-
ceedings of the second PASCAL Challenges Work-
shop on RTE.
Papineni, K., S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL.
Petrov, S., A. Pauls, and D. Klein. 2007. Discrimi-
native log-linear grammars with latent variables. In
Proceedings of NIPS.
Punyakanok, V., D. Roth, and W. Yih. 2004. Map-
ping dependencies trees: An application to question
answering. In Proceedings of AI-Math.
Raina, R., A. Y. Ng, , and C. Manning. 2005. Robust
textual inference via learning and abductive reason-
ing. In Proceedings of AAAI.
Ratnaparkhi, Adwait. 1996. A maximum entropy
part-of-speech tagger. In Proceedings of EMNLP.
Smith, D. A. and J. Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syn-
tactic dependencies. In Proceedings of the HLT-
NAACL Workshop on Statistical Machine Transla-
tion.
Snow, R., L. Vanderwende, and A. Menezes. 2006.
Effectively using syntax for recognizing false entail-
ment. In Proceedings of HLT-NAACL.
Vanderwende, L., A. Menezes, and R. Snow. 2006.
Microsoft research at rte-2: Syntactic contributions
in the entailment task: an implementation. In Pro-
ceedings of the second PASCAL Challenges Work-
shop on RTE.
Wang, M., N. A. Smith, and T. Mitamura. 2007.
What is the jeopardy model? a quasi-synchronous
grammar for question answering. In Proceedings of
EMNLP-CoNLL.
Zanzotto, F. M., A. Moschitti, M. Pennacchiotti, and
M.T. Pazienza. 2006. Learning textual entailment
from examples. In Proceedings of the second PAS-
CAL Challenges Workshop on RTE.
Zhang, K. and D. Shasha. 1989. Simple fast algo-
rithms for the editing distance between trees and re-
lated problems. SIAM Journal of Computing, 18.
Zhang, K. 1996. A constrained edit distance between
unordered labeled trees. Algorithmica, 15(3):205?
222.
1172
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 984?994, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Probabilistic Finite State Machines for Regression-based MT Evaluation
Mengqiu Wang and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305 USA
{mengqiu,manning}@cs.stanford.edu
Abstract
Accurate and robust metrics for automatic eval-
uation are key to the development of statistical
machine translation (MT) systems. We first
introduce a new regression model that uses a
probabilistic finite state machine (pFSM) to
compute weighted edit distance as predictions
of translation quality. We also propose a novel
pushdown automaton extension of the pFSM
model for modeling word swapping and cross
alignments that cannot be captured by stan-
dard edit distance models. Our models can eas-
ily incorporate a rich set of linguistic features,
and automatically learn their weights, elimi-
nating the need for ad-hoc parameter tuning.
Our methods achieve state-of-the-art correla-
tion with human judgments on two different
prediction tasks across a diverse set of standard
evaluations (NIST OpenMT06,08; WMT06-
08).
1 Introduction
Research in automatic machine translation (MT) eval-
uation metrics has been a key driving force behind
the recent advances of statistical machine transla-
tion (SMT) systems. The early seminal work on
automatic MT metrics (e.g., BLEU and NIST) is
largely based on n-gram matches (Papineni et al
2002; Doddington, 2002). Despite their simplicity,
these measures have shown good correlation with hu-
man judgments, and enabled large-scale evaluations
across many different MT systems, without incurring
the huge labor cost of human evaluation (Callison-
Burch et al(2009; 2010; 2011), inter alia). Recent
studies have also confirmed that tuning MT systems
against better MT metrics ? using algorithms like
MERT (Och, 2003) ? leads to better system perfor-
mance (He and Way, 2009; Liu et al 2011).
Later metrics that move beyond n-grams achieve
higher accuracy and improved robustness from re-
sources like WordNet synonyms (Miller et al 1990),
and paraphrasing (Snover et al 2009; Denkowski
and Lavie, 2010). But a common problem in these
metrics is they typically resort to ad-hoc tuning
methods instead of principled approaches to incor-
porate linguistic features. Recent models use linear
or SVM regression and train them against human
judgments to automatic learn feature weights, and
have shown state-of-the-art correlation with human
judgments (Kulesza and Shieber, 2004; Albrecht and
Hwa, 2007a; Albrecht and Hwa, 2007b; Sun et al
2008; Pado et al 2009). The drawback, however,
is they rely on time-consuming preprocessing mod-
ules to extract linguistic features (e.g., a full end-to-
end textual entailment system was needed in Pado et
al. (2009)), which severely limits their practical use.
Furthermore, these models employ a large number
of features (on the order of hundreds), and conse-
quently make the model predictions opaque and hard
to analyze.
In this paper, we propose a simple yet powerful
probabilistic Finite State Machine (pFSM) for the
task of MT evaluation. It is built on the backbone of
weighted edit distance models, but learns to weight
edit operations in a probabilistic regression frame-
work. One of the major contributions of this pa-
per is a novel extension of the pFSM model into a
probabilistic Pushdown Automaton (pPDA), which
enhances traditional edit-distance models with the
ability to model phrase shift and word swapping. Fur-
thermore, we give a new log-linear parameterization
to the pFSM model, which allows it to easily incor-
984
porate rich linguistic features. We experiment with a
set of simple features based on labeled head-modifier
dependency structure, in order to test the hypothesis
that modeling overall sentence structure can lead to
more accurate evaluation measures.
We conducted extensive experiments on a di-
verse set of standard evaluation data sets (NIST
OpenMT06, 08; WMT06, 07, 08). Our model
achieves or surpasses state-of-the-art results on all
test sets.
2 pFSMs for MT Regression
We start off by framing the problem of machine trans-
lation evaluation in terms of weighted edit distances
calculated using probabilistic finite state machines
(pFSMs). A FSM defines a language by accepting
a string of input tokens in the language, and reject-
ing those that are not. A probabilistic FSM defines
the probability that a string is in a language, extend-
ing on the concept of a FSM. Commonly used mod-
els such as HMMs, n-gram models, Markov Chains
and probabilistic finite state transducers all fall in
the broad family of pFSMs (Knight and Al-Onaizan,
1998; Eisner, 2002; Kumar and Byrne, 2003; Vidal
et al 2005). Unlike all the other applications of
FSMs where tokens in the language are words, in
our language tokens are edit operations. A string of
tokens that our pFSM accepts is an edit sequence that
transforms a reference translation (denoted as ref )
into a system translation (sys).
Our pFSM has a unique start and stop state, and
one state per edit operation (i.e., Insert, Delete, Sub-
stitution). The probability of an edit sequence e is
generated by the model is the product of the state tran-
sition probabilities in the pFSM, formally described
as:
w(e | s,r) =
?
|e|
k=1 exp ? ? f(ek?1,ek,s,r)
Z
(1)
We featurize each of the state changes with a log-
linear parameterization; f is a set of binary feature
functions defined over pairs of neighboring states
(by the Markov assumption) and the input sentences,
and ? are the associated feature weights; r and s are
shorthand for ref and sys; Z is a partition function.
In this basic pFSM model, the feature functions are
simply identity functions that emit the current state,
and the state transition sequence of the previous state
and the current state.
The feature weights are then automatically learned
by training a global regression model where some
translational equivalence judgment score (e.g., hu-
man assessment score, or HTER (Snover et al
2006)) for each sys and ref translation pair is the
regression target (y?). We introduce a new regression
variable y ? R which is the log-sum of the unnormal-
ized weights (Eqn. (1)) of all edit sequences, formally
expressed as:
y = log ?
e??e?
|e
?
|
?
k=1
exp ? ? f(ek?1,ek,s,r) (2)
e? denotes a valid edit sequence. Since the ?gold?
edit sequence are not given at training or prediction
time, we treat the edit sequences as hidden variables
and sum them out. The sum over an exponential
number of edit sequences in e? is solved efficiently
using a forward-backward style dynamic program.
Any edit sequence that does not lead to a complete
transformation of the translation pair has a probability
of zero in our model. Our regression target then seeks
to minimize the least squares error with respect to y?,
plus a L2-norm regularizer term parameterized by ? :
? ? = min
?
{?
si,ri
[y?i ? (
yi
|si|+ |ri|
+?)]2 +????2}
(3)
The |si|+ |ri| is a length normalization term for the
ith training instance, and ? is a scaling constant for
adjusting to different scoring standards (e.g., 7-point
scale vs. 5-point scale), whose value is automatically
learned. At test time, y/(|s|+ |r|)+? is computed
as the predicted score.
We replaced the standard substitution edit opera-
tion with three new operations: Sword for same word
substitution, Slemma for same lemma substitution, and
Spunc for same punctuation substitution. In other
words, all but the three matching-based substitutions
are disallowed. The start state can transition into any
of the edit states with a constant unit cost, and each
edit state can transition into any other edit state if
and only if the edit operation involved is valid at the
current edit position (e.g., the model cannot transi-
tion into Delete state if it is already at the end of ref ;
similarly it cannot transition into Slemma unless the
985
Figure 1: This diagram illustrates an example translation pair in the Chinese-English portion of OpenMT08 data set
(Doc:AFP CMN 20070703.0005, system09, sent 1). The three rows below are the best state transition sequences
according to the three proposed models. The corresponding alignments generated by the models (pFSM, pPDA,
pPDA+f ) are shown with different styled lines, with later models in the order generating strictly more alignments than
earlier ones. The gold human evaluation score is 6.5 (on a 7-point scale), and model predictions are: pPDA+f 5.5, pPDA
4.3, pFSM 3.1, METEORR 3.2, TERR 2.8.
lemma of the two words under edit in sys and ref
match). When the end of both sentences are reached,
the model transitions into the stop state and ends
the edit sequence. The first row in Figure 1 starting
with pFSM shows a state transition sequence for an
example sys/ref translation pair. 1 There exists a one-
to-one correspondence between substitution edits and
word alignments. Therefore this example state tran-
sition sequence correctly generates an alignment for
the word 43 and people.
It is helpful to compare with the TER met-
ric (Snover et al 2006), which is based on the idea
of word error rate measured in terms of edit distance,
to better understand the intuition behind our model.
There are two major improvements in our model: 1)
the edit operations in our model are weighted, as
defined by the feature functions and weights; 2) the
weights are automatically learned, instead of being
uniform or manually set; and 3) we model state transi-
tions, which can be understood as a bigram extension
of the unigram edit distance model used in TER. For
example, if in our learned model the feature for two
consecutive Sword states has a positive weight, then
our model would favor consecutive same word sub-
1It is safe to ignore the second and third row in Figure 1 for
now, their explanations are forthcoming in Section 2.2.
stitutions, whereas in the TER model the order of
the substitution does not matter. The extended TER-
plus (Snover et al 2009) metric addresses the first
problem but not the other two.
2.1 Soft-max Interpretation
There is also an alternative interpretation of the model
as a simple soft-max approximation that is very intu-
itive and easy to understand. For ease of illustration,
we introduce a quantity Q(e | s,r) to be the score of
an edit sequence, defined simply as the sum of the
dot product of feature values and feature weights:
Q(e | s,r) =
|e|
?
i=1
? ? f(ei?1,ei,s,r)
For the regression task, the intuition is that we want
y to take on the score (Q) of the best edit sequence:
y = max
e?e?
Q(e | s,r)
But since the max function is non-differentiable, we
replace it with a softmax:
y = log ?
e?e?
exp
? ?? ?
softmax
Q(e | s,r)
Substituting in Q, we arrive at the same objective
function as (2).
986
2.2 Restricted pPDA Extension
A shortcoming of edit distance models is that they
cannot handle long-distance word swapping ? a
pervasive phenomenon found in most natural lan-
guages. 2 Edit operations in standard edit distance
models need to obey strict incremental order in their
edit position, in order to admit efficient dynamic pro-
gramming solutions. The same limitation is shared
by our pFSM model, where the Markov assumption
is made based on the incremental order of edit po-
sitions. Although there is no known solution to the
general problem of computing edit distance where
long-distance swapping is permitted (Dombb et al
2010), approximate algorithms do exist. We present
a simple but novel extension of the pFSM model to a
restricted probabilistic pushdown automaton (pPDA),
to capture non-nested word swapping within limited
distance, which covers a majority of word swapping
in observed in real data (Wu, 2010).
A pPDA, in its simplest form, is a pFSM where
each control state is equipped with a stack (Esparza
and Kucera, 2005). The addition of stacks for each
transition state endows the machine with memory,
extending its expressiveness beyond that of context-
free formalisms. By construction, at any stage in a
normal edit sequence, the pPDA model can ?jump?
forward within a fixed distance (controlled by a max
distance parameter) to a new edit position on either
side of the sentence pair, and start a new edit subse-
quence from there. Assuming the jump was made on
the sys side, 3 the machine remembers its current edit
position in sys as Jstart , and the destination position
on sys after the jump as Jlanding.
We constrain our model so that the only edit op-
erations that are allowed immediately following a
?jump? are from the set of substitution operations
(e.g., Sword). And after at least one substitution
has been made, the device can now ?jump? back
to Jstart , remembering the current edit position as
Jend . Another constraint here is that after the back-
ward ?jump?, all edit operations are permitted except
for Insert, which cannot take place until at least one
2The edit distance algorithm described in Cormen et
al. (2001) can only handle adjacent word swapping (transpo-
sition), but not long-distance swapping.
3Recall that we transform ref into sys, and thus on the sys
side, we can only insert but not delete. The argument applies
equally to the case where the jump was made on the other side.
substitution has been made. When the edit sequence
advances to position Jlanding, the only operation al-
lowed at that point is another ?jump? forward opera-
tion to position Jend , at which point we also clear all
memory about jump positions and reset.
An intuitive explanation is that when pPDA makes
the first forward jump, a gap is left in sys that has
not been edited yet. It remembers where it left off,
and comes back to it after some substitutions have
been made to complete the edit sequence. The sec-
ond row in Figure 1 (starting with pPDA) illustrates
an edit sequence in a pPDA model that involves three
?jump? operations, which are annotated and indexed
by number 1-3 in the example. ?Jump 1? creates an
un-edited gap between word 43 and western, after
two substitutions, the model makes ?jump 2? to go
back and edit the gap. The only edit permitted im-
mediately after ?jump 2? is deleting the comma in
ref, since inserting the word 43 in sys before any sub-
stitution is disallowed. Once the gap is completed,
the model resumes at position Jend by making ?jump
3?, and completes the jump sequence. The ?jumps?
allowed the model to align words such as western In-
dia, in addition to the alignments of 43 people found
by the pFSM.
In a general pPDA model without the limited dis-
tance and non-nestedness jump constraints, there
could be recursive jump structures, which violates
the finite state property that we are looking for. The
constraints we introduced upper-bounds possible re-
ordering, and the resulting model is finite state. In
practice, we found that our extension gives a big
boost to model performance (cf. Section 4.1), with
only a modest increase in computation time. 4
2.3 Parameter Estimation
Since the least squares operator preserves convexity,
and the inner log-sum-exponential function is con-
vex, the resulting objective function is also convex.
For parameter learning, we used the limited memory
quasi-newton method (Liu and Nocedal, 1989) to
find the optimal feature weights and scaling constant
for the objective. We initialized ? =~0, ? = 0, and
? = 5. We also threw away features occurring fewer
than five times in training corpus. Two variants of the
4The length of the longest edit sequence with jumps only
increased by 0.5 ?max(|s|, |r|) in the worst case, and on the
whole swapping is rare in comparison to basic edits.
987
forward-backward style dynamic programming algo-
rithm were used for computing gradients in the pFSM
and pPDA models, similar to other sequence models
such as HMMs and CRFs. Details are omitted here
for brevity.
3 Rich Linguistic Features
In this section we will add new substitution opera-
tions beyond those introduced in Section 2, to capture
various linguistic phenomena. These new substitu-
tion operations correspond to new transition states in
the pPDA.
3.1 Synonyms
Our first set of features matches words that have
synonym relations according to WordNet (Miller et
al., 1990). Synonyms have been found to be very
useful in METEOR and TERplus, and can be easily
built into our model as a new substitution operation
Ssyn.
3.2 Paraphrasing
Newer versions of METEOR and TERplus both
found that inclusion of phrase-based matching greatly
improves model robustness and accuracy (Denkowski
and Lavie, 2010; Snover et al 2009). We add a sub-
stitution operator (Spara) that matches words that are
paraphrases. To better take advantage of paraphrase
information at the multi-word phrase level, we ex-
tended our substitution operations to match longer
phrases by adding one-to-many and many-to-many
n-gram block substitutions. In preliminary experi-
ments, we found that most of the gain came from
unigrams and bigrams, with little to no additional
gains from trigrams. Therefore, we limited our ex-
periments to bigram pFSM and pPDA models, and
pruned the paraphrase table adopted from TERplus 5
to unigrams and bigrams, resulting in 2.5 million
paraphrase pairs.
3.3 Sentence Structure
A problem that remains largely unaddressed by most
popular MT evaluation metrics is the overall good-
ness of the translated sentence?s structure (Liu et al
2005; Owczarzak et al 2008). Translations with
5Available from www.umiacs.umd.edu/~snover/terp.
good local n-gram coverage but horrible global syn-
tactic ordering are not unusual in SMT outputs. Such
translations usually score well with existing metrics
but poorly among human evaluators.
In our model, when we detect consecutive bigram
substitutions in the state transition, we examine the
head-modifier dependency between the two words on
each side of the sentence pair. A feature is triggered if
and only if there is a head-modifier relation between
the two words on each side, the labeled dependency
on the two sides match, and it is one of subject, ob-
ject or predicative relations. We deliberately left out
features that model mismatches of dependency labels,
because we found parsing output from translations
to be usually very poor. Since parsing results are
generally more reliable for more fluent translations,
our hope is that by only modeling parse matches, our
model will be able to pick them up as positive signals,
indicating good translation quality.
4 Experiments
The goal of our experiments is to test both the ac-
curacy and robustness of the proposed new models.
We then show that modeling word swapping and rich
linguistics features further improve our results.
To better situate our work among past research
and to draw meaningful comparison, we use exactly
the same standard evaluation data sets and metrics
as Pado et al(2009), which is currently the state-
of-the-art result for regression-based MT evaluation.
We consider four widely used MT metrics (BLEU,
NIST, METEOR (v0.7), and TER) as our baselines.
Since our models are trained to regress human eval-
uation scores, to make a direct comparison in the
same regression setting, we also train a small lin-
ear regression model for each baseline metric in the
same way as described in Pado et al(2009). These
regression models are strictly more powerful than
the baseline metrics and show higher robustness and
better correlation with human judgments. 6 We also
compare our models with the state-of-the-art linear
regression models reported in Pado et al(2009) that
6The baseline metric (e.g., BLEU) computes its raw score by
taking the geometric mean of n-gram precision scores (1? n? 4)
scaled by a brevity penalty. The regression model learns to com-
bine these fine-grained scores more intelligently, by optimizing
their weights to regress human judgments. See Pado et al(2009)
for more discussion.
988
combine features from multiple MT evaluation met-
rics (MT), as well as rich linguistic features from a
textual entailment system (RTE).
In all of our experiments, each reference and sys-
tem translation sentence pair is tokenized using the
Penn Treebank (Marcus et al 1993) tokenization
script, and lemmatized by the Porter Stemmer (Porter,
1980). For the overall sentence structure experi-
ment, translations are additionally part-of-speech
tagged with MXPOST tagger (Ratnaparkhi, 1996),
and parsed withMSTParser (McDonald et al 2005) 7
labeled dependency parser. Statistical significance
tests are performed using the paired bootstrap resam-
pling method (Koehn, 2004).
We divide our experiments into two sections, based
on two different prediction tasks ? predicting abso-
lute scores and predicting pairwise preference.
4.1 Exp. 1: Predicting Absolute Scores
The first task is to evaluate a system translation
on a seven point Likert scale against a single ref-
erence. Higher scores indicate translations that are
closer to the meaning intended by the reference. Hu-
man ratings in the form of absolute scores are avail-
able for standard evaluation data sets such as NIST
OpenMT06,08.8 Since our model makes predictions
at the granularity of a whole sentence, we focus on
sentence-level evaluation. A metric?s goodness is
judged by how well it correlates with human judg-
ments, and Spearman?s rank correlation (?) is re-
ported for all experiments in this section.
We used the NIST OpenMT06 corpus for develop-
ment purposes, and reserved the NIST OpenMT08
corpus for post-development evaluation. The
OpenMT06 data set contains 1,992 English trans-
lations of Arabic newswire text from 8 MT systems.
For development, we used a 2-fold cross-validation
scheme with splits at the first 1,000 and last 992 sen-
tences. The OpenMT08 data set contains English
translations of newswire text from three languages
(Arabic has 2,769 pairs from 13 MT systems; Chi-
nese has 1,815 pairs from 15; and Urdu has 1,519
pairs, from 7). We followed the same experimental
setup as Pado et al(2009), using a ?round robin?
training/testing scheme, i.e., we train a model on data
7Trained on the entire Penn Treebank.
8Available from http://www.nist.gov.
from two languages, making predictions for the third.
We also show results of models trained on the entire
OpenMT08 data set and tested on OpenMT06.
4.1.1 pFSM vs. pPDA
Data Set pFSM pPDA
tr te n1 n2 j1 j2 j5 j10
A+C U 54.6 54.8 55.6 55.0 55.3 55.3
A+U C 59.9 59.8 58.0 61.4 63.8 64.0
C+U A 61.2 61.2 60.2 59.9 60.4 60.2
Table 1: pFSM vs. pPDA results for the round-robin
approach on OpenMT08 data set over three languages
(A=Arabic, C=Chinese, U=Urdu). Numbers in this table
are Spearman?s ? for correlation between human assess-
ment scores and model predictions; tr stands for training
set, and te stands for test set. nx means the model has
x-gram block edits. jy means the model has jump distance
limit y. The Best result for each test set row is highlighted
in bold.
The second and third columns under the pFSM
label in Table 1 compares our bigram block edit ex-
tension for the pFSM model. Although we do not
yet see a significant performance gain (or loss) from
adding block edits, they will enable longer paraphrase
matches in later experiments.
Columns 5 through 8 in Table 1 show experimental
results validating the contribution of our pPDA ex-
tension to the pFSM model (cf. Section 2.2). We can
see that the pPDA extension gave modest improve-
ments on the Urdu test set, but at a small decrease
in performance on the Arabic data. However, for
Chinese, there is a substantial gain, particularly with
jump distances of five or longer. This trend is even
more pronounced at the long jump distance of 10,
consistent with the observation that Chinese-English
translations exhibit much more medium and long dis-
tance reordering than languages like Arabic (Birch et
al., 2009).
4.1.2 Evaluating Linguistic Features
Experimental results evaluating the benefits of
each linguistic feature set are presented in Table 3.
The first row is the pPDA model with jump distance
limit 5, without other additional features. The next
three rows are the results of adding each of the three
feature sets described in Section 3.
Overall, we observed that only paraphrase match-
ing features gave a significant boost to performance.
989
Data Set Our Metrics Baseline Metrics Combined Metrics
train test pFSM pPDA pPDA+f BLEUR NISTR TERR METR MTR RTER MT+RTER
A+C U 54.6 55.3 57.2 49.9 49.5 50.1 49.1 50.1 54.5 55.6
A+U C 59.9 63.8 65.8 53.9 53.1 50.3 61.1 57.3 58.0 62.7
C+U A 61.2 60.4 59.8 52.5 50.4 54.5 60.1 55.2 59.9 61.1
MT08 MT06 65.2 63.4 64.5 57.6 55.1 63.8 62.1 62.6 62.2 65.2
Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)
refers to the regression model trained for each baseline metric, same as Pado et al(2009). The first three rows are
round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are
results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman?s rank
correlation ? between human assessment scores and model predictions. The pPDA column describes our pPDA model
with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase
and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted
in bold in each row.
Urdu Chinese Arabic
pPDA 55.3 63.8 60.4
+Synonym 55.6 63.7 60.7
+Tree 55.3 63.8 60.3
+Paraphrase 57.1 65.4 60.0
+Syn+Tree+Para 57.2 65.8 59.8
Table 3: Results for OpenMT08 with linguistic features,
using the same round robin scheme as in Table 1. Numbers
in this table are Spearman?s rank correlation ? between
human assessment scores and model predictions. Best
results on each test set are highlighted in bold.
The row starting with pPDA+f in Figure 1 shows
an example where adding paraphrase features allow
pPDA+f to find more correct alignments and make
better predictions than pPDA.
No significant improvements from synonym and
dependency tree matching features are evident from
the results. An examination of the feature statistics in
training data showed that the parse tree features have
very low occurrence counts. On the Chinese+Urdu
training set, for example, the features for subject,
object and predicative labeled dependency matches
fired only 55, 784 and 13 times, respectively. As a
reference point for the scale of feature counts, the
?same word? match feature fired 875,375 times on
the same data set. And our qualitative assessment of
the labeled dependency parser outputs was that the
quality is very poor on system translations. For future
work, more elaborate parse feature engineering could
be a promising direction, but is outside the scope of
our study.
In combination, the joint feature set of synonym,
paraphrase and parse tree features gave modest im-
provements over the paraphrase feature alone on the
Chinese test set.
4.1.3 Overall Comparison
Results of our proposed models compared against
the baseline models described in Pado et al(2009)
are shown in Table 2. The pPDA+f model has access
to paraphrase information, which is not available
to the baselines, so it should not be directly com-
pared with. But the pFSM and pPDA models do
not use any additional information other than words
and lemmas, and thus make a fair comparison with
the baseline metrics. 9 We can see from the table
that pFSM significantly outperforms all baselines on
Urdu and Arabic, but trails behind METEORR on
Chinese by a small margin (1.2 point in Spearman?s
?). On Chinese data set, the pPDA extension gives
results significantly better than the best baseline met-
rics for Chinese (2.7 better than METEORR). Both
the pFSM and pPDA models also significantly outper-
form the MTR linear regression model that combines
the outputs of all four baselines, on all three source
languages. This demonstrates that our regression
model is more robust and accurate than a state-of-
the-art system combination linear-regression model.
Both pFSM and pPDA learned to assign a lower neg-
ative feature weight for deletion than insertion (i.e.,
it is bad to insert an unseen word into system trans-
9METEORR actually has an unfair advantage in this compari-
son, since it uses synonym information from WordNet; TERR
on the other hand has a disadvantage because it does not use
lemmas. Lemma is added later in the TERplus extension (Snover
et al 2009).
990
lation, but worse if words from reference translation
are deleted), which corresponds to the setting in ME-
TEOR where recall is given more importance than
precision (Banerjee and Lavie, 2005).
The RTER and MT+RTER linear regression mod-
els benefit from the rich linguistic features in the
textual entailment system?s output. It has access to
all the features in pPDA+f such as paraphrase and de-
pendency parse relations, and many more (e.g., Norm
Bank, part-of-speech, negation, antonyms). However,
our pPDA+f model rivals the performance of RTER
and MT+RTER on Arabic (with no statistically sig-
nificant difference from RTER), and greatly improve
over these two models on Urdu and Chinese. Most
noticeably, pPDA+f is 7.7 points better than RTER
on Chinese.
Consistent with our earlier observation on
OpenMT08 data set that the pPDA model performs
slightly worse than the pFSM model on Arabic, the
same performance decrease is seen in OpenMT06
data set, which is also Arabic-to-English.
As shown earlier in Table 3, the combined set of
paraphrase, parsing and synonym features in pPDA+f
helps for Urdu and Chinese, but not for Arabic. Here
we found that even though the pPDA+f model is still
worse than pFSM on OpenMT06 tests, it did give a
decent improvement to pPDA model, closing up the
gap with pFSM.
Other than robustness and accuracy, simplicity is
also an important trait we seek in good MT met-
rics. Our models only have a few tens of features
(instead of hundreds of features as found in RTER
and MT+RTER), which makes interpretation of the
model?s prediction relatively easy. On an important
practical note, our model is much more lightweight
than the RTER or MTR system. It runs at a much
faster speed with a smaller memory footprint, hence
potentially useable in MERT training.
4.2 Exp. 2: Predicting Pairwise Preferences
To further test our model?s robustness, we evaluate
it on WMT data sets with a different prediction task
in which metrics make pairwise preference judg-
ments between translation systems. The WMT06-
08 data sets are much larger in comparison to the
OpenMT06 and 08 data. They contain MT outputs of
over 40 systems from five different source languages
(French, German, Spanish, Czech, and Hungarian).
The WMT06, 07 and 08 sets contains 10,159, 5,472
and 6,856 sentence pairs, respectively. We used por-
tions of WMT 06 and 07 data sets 10 that are anno-
tated with absolute scores on a five point scale for
training, and the WMT08 data set annotated with
pairwise preference for testing.
To generate pairwise preference predictions, we
first predict an absolute score for each system trans-
lation, then compare the scores between each system
pair, and give preference to the higher score. We
adopt the sentence-level evaluation metric used in
Pado et al(2009), which measures the consistency
(accuracy) of metric predictions with human prefer-
ences. The random baseline for this task on WMT08
data set is 39.8%. 11
Models WMT06 WMT07 WMT06+07
pPDA+f 51.6 52.4 52.0
BLEUR 49.7 49.5 49.6
METEORR 51.4 51.4 51.5
NISTR 50.0 50.3 50.2
TERR 50.9 51.0 51.2
MTR 50.8 51.5 51.5
RTER 51.8 50.7 51.9
MT+RTER 52.3 51.8 52.5
Table 4: Pairwise preference prediction results on WMT08
test set. Each column shows a different training data set.
Numbers in this table are model?s consistency with human
pairwise preference judgments. Best result on each test
set is highlighted in bold.
Results are shown in Table 4. Similar to the results
on OpenMT experiments, our model consistently out-
performed BLEUR, METEORR, NISTR and TERR.
Our model also gives better performance than the
MTR ensemble model on all three tests; and ties with
RTER in two out of the three tests but performs sig-
nificantly better on the other test. The MT+RTER
ensemble model is better on two tests, but worse
on the other. But overall the two systems are quite
comparable, with less than 0.6% accuracy difference.
The results also show that our method is stable across
different training sets, with test accuracy differences
less than 0.4%.
10Available from http://www.statmt.org.
11The random baseline is not 50% for two reasons: (1) human
judgments include contradictory and tie annotations; (2) tran-
sitivity constraints need to be respected in total ordering. For
details, see Pado et al(2009).
991
4.3 Qualitative Analysis
Example (1) shows a system and reference translation
pair in the Chinese test portion of OpenMT08.
(1) REF: Two Jordanese sentenced1 for plotting2
an attack3 on Americans4
SYS: The name of Jordan plotting2 attacks3
Americans4 were sentenced1 to death
Human annotators give this example a score of 4.0,
but TERR and METEORR both assigned erroneously
low scores (1.0 and 2.2, respectively). Words with the
same subscript index were aligned by pPDA model.
This example exhibits a word swapping phenomenon,
and our model was able to capture it correctly. TERR
clearly suffered from not being able to model word
swapping in this case. It also missed out the word
pair attack and attacks due to the lack of lemma sup-
port. The reason why METEORR assigned such a low
score for this example is because none of the matched
words in the reference were adjacent to each other,
causing a high fragmentation penalty. The fragmenta-
tion penalty term has two parameters that need to be
manually tuned, and has a high variance across exam-
ples and data sets. This example illustrates models
that require ad-hoc tuning tend not to be robust. Our
pPDA model (without linguistic feature) was able
to make a prediction of 3.7, much closer to human
judgment.
4.4 MetricsMATR10 and WMT12 Results
An earlier version of the pFSM model that was
trained on the OpenMT08 data set was submitted
to the single reference sentence level track at Met-
ricsMATR10 (Peterson and Przybocki, 2010) NIST
evaluation. Even though our system was not in the
most ideal state at the time of the evaluation, 12 and
was trained on a small amount of data, the pFSM
model still performed competitively against other
metrics. Noticeably, we achieved second best results
for Human-targeted Translation Edit Rate (HTER)
assessment, trailing behind TERplus with no statisti-
cally significant difference. On average, our system
made 5th place among 15 different sites and 7th place
among 25 different metrics, averaged across 9 assess-
ment types.
12Unfortunately the version we submitted in 2010 was plagued
with a critical bug. More general enhancements have been made
to the model since.
We submitted the version of the pPDA+f model
trained on the WMT07 dataset to the ?into English?
segment-leval track of the WMT 2012 Shared Eval-
uation Metrics Task (Callison-Burch et al 2012).
Our model achieved the highest score (measured by
Kendall?s tau correlation) on all four language pairs
(Fr-En, De-En, Es-En and Cs-En), and tied for the
first place with METEOR v1.3 on average correla-
tion.
5 Related Work
Features and Representation
One of the findings in our experimentation is that
paraphrasing helps boosting model accuracy, and
the idea of using paraphrases in MT evaluation was
first proposed by Zhou et al(2006). Several re-
cent studies have introduced metrics over dependency
parses (Liu et al 2005; Owczarzak et al 2008; He et
al., 2010), but their improvements over n-gram mod-
els at the sentence level are not always consistent (Liu
et al 2005; Peterson and Przybocki, 2010). Other
than string-based methods, recent work has explored
more alternative representations for MT evaluation,
such as network properties (Amancio et al 2011),
semantic role structures (Lo and Wu, 2011), and the
quality of word order (Birch and Osborne, 2011).
Modeling
The idea of using extended edit distance models with
block movements was also explored in Leusch et
al. (2003). However, their model is largely empirical
and not in a probabilistic learning setting. The line
of work on probabilistic tree-edit distance models
bears a strong connection to this work (McCallum
et al 2005; Bernard et al 2008; Wang and Man-
ning, 2010; Emms, 2012). In particular, our pFSM
model and the log-linear parameterization were in-
spired by Wang and Manning (2010). Another body
of literature that is closely related to this work is
FSM models for word alignment (Vogel et al 1996;
Saers et al 2010; Berg-Kirkpatrick et al 2010). The
stochastic Inversion Transduction Grammar in Saers
et al(2010) for instance, is a pFSM with special
constraints. More recently, Saers and Wu (2011) fur-
ther explored the connection between Linear Trans-
duction Grammars and FSMs. There is a close tie
992
between our pFSM model and the HMM model in
Berg-Kirkpatrick et al(2010). Both models adopted
a log-linear parameterization for the state transition
distribution, 13 but in their case the HMM model and
the pFSM arc weights are normalized locally, and the
objective is non-convex.
6 Conclusion
We described a probabilistic finite state machine
based on string edits and a novel pushdown automa-
ton extension for the task of machine translation eval-
uation. The models admit a rich set of linguistic
features, and are trained to learn feature weights auto-
matically by optimizing a regression objective. The
proposed models achieve state-of-the-art results on
a wide range of standard evaluations, and are much
more lightweight than previous regression models,
making them suitable candidates to be used in MERT
training.
Acknowledgements
We gratefully acknowledge the support of Defense
Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-
09-C-0181 and the support of the DARPA Broad
Operational Language Translation (BOLT) program
through IBM. Any opinions, findings, and conclusion
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the view of the DARPA, AFRL, or the US govern-
ment.
References
J. Albrecht and R. Hwa. 2007a. A re-examination of
machine learning approaches for sentence-level MT
evaluation. In Proceedings of ACL.
J. Albrecht and R. Hwa. 2007b. Regression for sentence-
level MT evaluation with pseudo references. In Pro-
ceedings of ACL.
D.R. Amancio, M.G.V. Nunes, O.N. Oliveira Jr., T.A.S.
Pardo, L. Antiqueira, and L. da F. Costa. 2011. Using
metrics from complex networks to evaluate machine
translation. Physica A, 390(1):131?142.
S. Banerjee and A. Lavie. 2005. Meteor: An automatic
metric for MT evaluation with improved correlation
13Similar parameterization was also used in much previous
work, such as Riezler et al(2000).
with human judgments. In Proceedings of ACL Work-
shop on Intrinsic and Extrinsic Evaluation Measures.
T. Berg-Kirkpatrick, A. Bouchard-Cote, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In Proceedings of NAACL.
M. Bernard, L. Boyer, A. Habrard, and M. Sebban. 2008.
Learning probabilistic models of tree edit distance. Pat-
tern Recognition, 41(8):2611?2629.
A. Birch and M. Osborne. 2011. Reordering metrics for
MT. In Proceedings of ACL/HLT.
A. Birch, P. Blunsom, and M. Osborne. 2009. A quantita-
tive analysis of reordering phenomena. In Proceedings
of WMT 09.
C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder.
2009. Findings of the 2009 Workshop on Statistical
Machine Translation. In Proceedings of the Fourth
Workshop on Statistical Machine Translation.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
M. Przybocki, and O. Zaidan. 2010. Findings of the
2010 joint workshop on Statistical Machine Translation
and metrics for Machine Translation. In Proceedings
of Joint WMT 10 and MetricsMatr Workshop at ACL.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 workshop on statistical ma-
chine translation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation.
C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Soricut,
and L. Specia. 2012. Findings of the 2012 workshop
on Statistical Machine Translation. In Proceedings of
Seventh Workshop on Statistical Machine Translation
at NAACL.
T. Cormen, C. Leiserson, R. Rivest, and C. Stein. 2001.
Introduction to Algorithms, Second Edition. MIT Press.
M. Denkowski and A. Lavie. 2010. Extending the ME-
TEOR machine translation evaluation metric to the
phrase level. In Proceedings of HLT/NAACL.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram cooccurrence statistics.
In Proceedings of HLT.
Y. Dombb, O. Lipsky, B. Porat, E. Porat, and A. Tsur.
2010. The approximate swap and mismatch edit dis-
tance. Theoretical Computer Science, 411(43).
J. Eisner. 2002. Parameter estimation for probabilistic
finite-state transducers. In Proceedings of ACL.
M. Emms. 2012. On stochastic tree distances and their
training via expectation-maximisation. In Proceedings
of International Conference on Pattern Recognition
Application and Methods.
J. Esparza and A. Kucera. 2005. Quantitative analysis
of probabilistic pushdown automata: Expectations and
variances. In Proceedings of the 20th Annual IEEE
Symposium on Logic in Computer Science.
993
Y. He and A.Way. 2009. Improving the objective function
in minimum error rate training. In Proceedings of MT
Summit XII.
Y. He, J. Du, A. Way, and J. van Genabith. 2010. The
DCU dependency-based metric inWMT-MetricsMATR
2010. In Proceedings of Joint WMT 10 and Metrics-
Matr Workshop at ACL.
K. Knight and Y. Al-Onaizan. 1998. Translation with
finite-state devices. In Proceedings of AMTA.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proceedings of EMNLP.
A. Kulesza and S. Shieber. 2004. Robust machine trans-
lation evaluation with entailment features. In Proceed-
ings of TMI.
S. Kumar and W. Byrne. 2003. A weighted finite state
transducer implementation of the alignment template
model for statistical machine translation. In Proceed-
ings of HLT/NAACL.
G. Leusch, N. Ueffing, and H. Ney. 2003. A novel string-
to-string distance measure with applications to machine
translation evaluation. In Proceedings of MT Summit I.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory BFGS method for large scale optimization. Math.
Programming, 45:503?528.
D. Liu, , and D. Gildea. 2005. Syntactic features for eval-
uation of machine translation. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures.
C. Liu, D. Dahlmeier, and H. Ng. 2011. Better eval-
uation metrics lead to better machine translation. In
Proceedings of EMNLP.
C. Lo and D. Wu. 2011. MEANT: An inexpensive, high-
accuracy, semi-automatic metric for evaluating transla-
tion utility based on semantic roles. In Proceedings of
ACL/HLT.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of english: the
Penn Treebank. Computational Linguistics, 19(2):313?
330.
A. McCallum, K. Bellare, and F. Pereira. 2005. A condi-
tional random field for discriminatively-trained finite-
state string edit distance. In Proceedings of UAI.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
G. A.Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. J.
Miller. 1990. WordNet: an on-line lexical database.
International Journal of Lexicography, 3(4).
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proceedings of ACL.
K. Owczarzak, J. van Genabith, and A. Way. 2008. Evalu-
ating machine translation with LFG dependencies. Ma-
chine Translation, 21(2):95?119.
S. Pado, M. Galley, D. Jurafsky, and C. D. Manning. 2009.
A learning approach to improving sentence-level MT
evaluation. In Proceedings of ACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of ACL.
K. Peterson and M. Przybocki. 2010. NIST 2010 metrics
for machine translation evaluation (MetricsMaTr10)
official release of results.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
A. Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of EMNLP.
S. Riezler, D. Prescher, J. Kuhn, and M. Johnson. 2000.
Lexicalized stochastic modeling of constraint-based
grammars using log-linear measures and em training.
In Proceedings of ACL.
M. Saers and D. Wu. 2011. Linear transduction grammars
and zipper finite-state transducers. In Proceedings of
Recent Advances in Natural Language Processing.
M. Saers, J. Nivre, and D. Wu. 2010. Word alignment
with stochastic bracketing linear inversion transduction
grammar. In Proceedings of NAACL.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
AMTA.
M. Snover, , N. Madnani, B. Dorr, and R. Schwartz. 2009.
Fluency, adequacy, or HTER? exploring different hu-
man judgments with a tunable MT metric. In Proceed-
ings of WMT09 Workshop.
S. Sun, Y. Chen, and J. Li. 2008. A re-examination on
features in regression based approach to automatic MT
evaluation. In Proceedings of ACL.
E. Vidal, F. Thollard, C. de la Higuera, F. Casacuberta,
and R. C. Carrasco. 2005. Probabilistic finite-state ma-
chines part I. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 27(7):1013?1025.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proceed-
ings of COLING.
M. Wang and C.D. Manning. 2010. Probabilistic tree-
edit models with structured latent variables for textual
entailment and question answering. In Proceedings of
COLING.
D. Wu, 2010. CRC Handbook of Natural Language Pro-
cessing, chapter Alignment, pages 367?408. CRC
Press.
L. Zhou, C.Y. Lin, and E. Hovy. 2006. Re-evaluating
machine translation results with paraphrase support. In
Proceedings of EMNLP.
994
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1170?1179,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Feature Noising for Log-linear Structured Prediction
Sida I. Wang?, Mengqiu Wang?, Stefan Wager?,
Percy Liang, Christopher D. Manning
Department of Computer Science, ?Department of Statistics
Stanford University, Stanford, CA 94305, USA
{sidaw, mengqiu, pliang, manning}@cs.stanford.edu
swager@stanford.edu
Abstract
NLP models have many and sparse features,
and regularization is key for balancing model
overfitting versus underfitting. A recently re-
popularized form of regularization is to gen-
erate fake training data by repeatedly adding
noise to real data. We reinterpret this noising
as an explicit regularizer, and approximate it
with a second-order formula that can be used
during training without actually generating
fake data. We show how to apply this method
to structured prediction using multinomial lo-
gistic regression and linear-chain CRFs. We
tackle the key challenge of developing a dy-
namic program to compute the gradient of the
regularizer efficiently. The regularizer is a
sum over inputs, so we can estimate it more
accurately via a semi-supervised or transduc-
tive extension. Applied to text classification
and NER, our method provides a >1% abso-
lute performance gain over use of standard L2
regularization.
1 Introduction
NLP models often have millions of mainly sparsely
attested features. As a result, balancing overfitting
versus underfitting through good weight regulariza-
tion remains a key issue for achieving optimal per-
formance. Traditionally, L2 or L1 regularization is
employed, but these simple types of regularization
penalize all features in a uniform way without tak-
ing into account the properties of the actual model.
An alternative approach to regularization is to
generate fake training data by adding random noise
to the input features of the original training data. In-
tuitively, this can be thought of as simulating miss-
?Both authors contributed equally to the paper
ing features, whether due to typos or use of a pre-
viously unseen synonym. The effectiveness of this
technique is well-known in machine learning (Abu-
Mostafa, 1990; Burges and Scho?lkopf, 1997; Simard
et al, 2000; Rifai et al, 2011a; van der Maaten
et al, 2013), but working directly with many cor-
rupted copies of a dataset can be computationally
prohibitive. Fortunately, feature noising ideas often
lead to tractable deterministic objectives that can be
optimized directly. Sometimes, training with cor-
rupted features reduces to a special form of reg-
ularization (Matsuoka, 1992; Bishop, 1995; Rifai
et al, 2011b; Wager et al, 2013). For example,
Bishop (1995) showed that training with features
that have been corrupted with additive Gaussian
noise is equivalent to a form of L2 regularization in
the low noise limit. In other cases it is possible to
develop a new objective function by marginalizing
over the artificial noise (Wang and Manning, 2013;
van der Maaten et al, 2013).
The central contribution of this paper is to show
how to efficiently simulate training with artificially
noised features in the context of log-linear struc-
tured prediction, without actually having to gener-
ate noised data. We focus on dropout noise (Hinton
et al, 2012), a recently popularized form of artifi-
cial feature noise where a random subset of features
is omitted independently for each training example.
Dropout and its variants have been shown to out-
perform L2 regularization on various tasks (Hinton
et al, 2012; Wang and Manning, 2013; Wan et al,
2013). Dropout is is similar in spirit to feature bag-
ging in the deliberate removal of features, but per-
forms the removal in a preset way rather than ran-
domly (Bryll et al, 2003; Sutton et al, 2005; Smith
et al, 2005).
1170
Our approach is based on a second-order approx-
imation to feature noising developed among others
by Bishop (1995) and Wager et al (2013), which al-
lows us to convert dropout noise into a form of adap-
tive regularization. This method is suitable for struc-
tured prediction in log-linear models where second
derivatives are computable. In particular, it can be
used for multiclass classification with maximum en-
tropy models (a.k.a., softmax or multinomial logis-
tic regression) and for the sequence models that are
ubiquitous in NLP, via linear chain Conditional Ran-
dom Fields (CRFs).
For linear chain CRFs, we additionally show how
we can use a noising scheme that takes advantage
of the clique structure so that the resulting noising
regularizer can be computed in terms of the pair-
wise marginals. A simple forward-backward-type
dynamic program can then be used to compute the
gradient tractably. For ease of implementation and
scalability to semi-supervised learning, we also out-
line an even faster approximation to the regularizer.
The general approach also works in other clique
structures in addition to the linear chain when the
clique marginals can be computed efficiently.
Finally, we extend feature noising for structured
prediction to a transductive or semi-supervised set-
ting. The regularizer induced by feature noising
is label-independent for log-linear models, and so
we can use unlabeled data to learn a better regu-
larizer. NLP sequence labeling tasks are especially
well suited to a semi-supervised approach, as input
features are numerous but sparse, and labeled data
is expensive to obtain but unlabeled data is abundant
(Li and McCallum, 2005; Jiao et al, 2006).
Wager et al (2013) showed that semi-supervised
dropout training for logistic regression captures a
similar intuition to techniques such as entropy regu-
larization (Grandvalet and Bengio, 2005) and trans-
ductive SVMs (Joachims, 1999), which encourage
confident predictions on the unlabeled data. Semi-
supervised dropout has the advantage of only us-
ing the predicted label probabilities on the unlabeled
data to modulate an L2 regularizer, rather than re-
quiring more heavy-handed modeling of the unla-
beled data as in entropy regularization or expecta-
tion regularization (Mann and McCallum, 2007).
In experimental results, we show that simulated
feature noising gives more than a 1% absolute boost
yt yt+1yt?1f (yt, xt )f (yt?1, yt ) f (yt, yt+1)yt yt+1yt?1f (yt, xt )f (yt?1, yt ) f (yt, yt+1)
Figure 1: An illustration of dropout feature noising
in linear-chain CRFs with only transition features
and node features. The green squares are node fea-
tures f(yt, xt), and the orange squares are edge fea-
tures f(yt?1, yt). Conceptually, given a training ex-
ample, we sample some features to ignore (generate
fake data) and make a parameter update. Our goal is
to train with a roughly equivalent objective, without
actually sampling.
in performance over L2 regularization, on both text
classification and an NER sequence labeling task.
2 Feature Noising Log-linear Models
Consider the standard structured prediction problem
of mapping some input x ? X (e.g., a sentence)
to an output y ? Y (e.g., a tag sequence). Let
f(y, x) ? Rd be the feature vector, ? ? Rd be the
weight vector, and s = (s1, . . . , s|Y|) be a vector of
scores for each output, with sy = f(y, x) ? ?. Now
define a log-linear model:
p(y | x; ?) = exp{sy ?A(s)}, (1)
where A(s) = log
?
y exp{sy} is the log-partition
function. Given an example (x,y), parameter esti-
mation corresponds to choosing ? to maximize p(y |
x; ?).
The key idea behind feature noising is to artifi-
cially corrupt the feature vector f(y, x) randomly
1171
into some f?(y, x) and then maximize the average
log-likelihood of y given these corrupted features?
the motivation is to choose predictors ? that are ro-
bust to noise (missing words for example). Let s?,
p?(y | x; ?) be the randomly perturbed versions cor-
responding to f?(y, x). We will also assume the
feature noising preserves the mean: E[f?(y, x)] =
f(y, x), so that E[s?] = s. This can always be done
by scaling the noised features as described in the list
of noising schemes.
It is useful to view feature noising as a form of
regularization. Since feature noising preserves the
mean, the feature noising objective can be written as
the sum of the original log-likelihood plus the dif-
ference in log-normalization constants:
E[log p?(y | x; ?)] = E[s?y ?A(s?)] (2)
= log p(y | x; ?)?R(?, x), (3)
R(?, x)
def
= E[A(s?)]?A(s). (4)
Since A(?) is convex, R(?, x) is always positive by
Jensen?s inequality and can therefore be interpreted
as a regularizer. Note that R(?, x) is in general non-
convex.
Computing the regularizer (4) requires summing
over all possible noised feature vectors, which can
imply exponential effort in the number of features.
This is intractable even for flat classification. Fol-
lowing Bishop (1995) and Wager et al (2013), we
approximate R(?, x) using a second-order Taylor
expansion, which will allow us to work with only
means and covariances of the noised features. We
take a quadratic approximation of the log-partition
function A(?) of the noised score vector s? around
the the unnoised score vector s:
A(s?) u A(s) +?A(s)>(s?? s) (5)
+
1
2
(s?? s)>?2A(s)(s?? s).
Plugging (5) into (4), we obtain a new regularizer
Rq(?, x), which we will use as an approximation to
R(?, x):
Rq(?, x) =
1
2
E[(s?? s)>?2A(s)(s?? s)] (6)
=
1
2
tr(?2A(s) Cov(s?)). (7)
This expression still has two sources of potential in-
tractability, a sum over an exponential number of
noised score vectors s? and a sum over the |Y| com-
ponents of s?.
Multiclass classification If we assume that the
components of s? are independent, then Cov(s?) ?
R|Y|?|Y| is diagonal, and we have
Rq(?, x) =
1
2
?
y?Y
?y(1? ?y)Var[s?y], (8)
where the mean ?y
def
= p?(y | x) is the model prob-
ability, the variance ?y(1??y) measures model un-
certainty, and
Var[s?y] = ?
>Cov[f?(y, x)]? (9)
measures the uncertainty caused by feature noising.1
The regularizerRq(?, x) involves the product of two
variance terms, the first is non-convex in ? and the
second is quadratic in ?. Note that to reduce the reg-
ularization, we will favor models that (i) predict con-
fidently and (ii) have stable scores in the presence of
feature noise.
For multiclass classification, we can explicitly
sum over each y ? Y to compute the regularizer,
but this will be intractable for structured prediction.
To specialize to multiclass classification for the
moment, let us assume that we have a separate
weight vector for each output y applied to the same
feature vector g(x); that is, the score sy = ?y ? g(x).
Further, assume that the components of the noised
feature vector g?(x) are independent. Then we can
simplify (9) to the following:
Var[s?y] =
?
j
Var[gj(x)]?
2
yj . (10)
Noising schemes We now give some examples of
possible noise schemes for generating f?(y, x) given
the original features f(y, x). This distribution af-
fects the regularization through the variance term
Var[s?y].
? Additive Gaussian:
f?(y, x) = f(y, x) + ?, where ? ?
N (0, ?2Id?d).
1Here, we are using the fact that first and second derivatives
of the log-partition function are the mean and variance.
1172
In this case, the contribution to the regularizer
from noising is Var[s?y] =
?
j ?
2?2yj .
? Dropout:
f?(y, x) = f(y, x)  z, where  takes the el-
ementwise product of two vectors. Here, z is
a vector with independent components which
has zi = 0 with probability ?, zi = 11?? with
probability 1 ? ?. In this case, Var[s?y] =
?
j
gj(x)2?
1?? ?
2
yj .
? Multiplicative Gaussian:
f?(y, x) = f(y, x)  (1 + ?), where
? ? N (0, ?2Id?d). Here, Var[s?y] =?
j gj(x)
2?2?2yj . Note that under our second-
order approximation Rq(?, x), the multiplica-
tive Gaussian and dropout schemes are equiva-
lent, but they differ under the original regular-
izer R(?, x).
2.1 Semi-supervised learning
A key observation (Wager et al, 2013) is that
the noising regularizer R (8), while involving a
sum over examples, is independent of the output
y. This suggests estimating R using unlabeled
data. Specifically, if we have n labeled examples
D = {x1, x2, . . . , xn} and m unlabeled examples
Dunlabeled = {u1, u2, . . . , un}, then we can define a
regularizer that is a linear combination the regular-
izer estimated on both datasets, with ? tuning the
tradeoff between the two:
R?(?,D,Dunlabeled) (11)
def
=
n
n+ ?m
( n?
i=1
R(?, xi) + ?
m?
i=1
R(?, ui)
)
.
3 Feature Noising in Linear-Chain CRFs
So far, we have developed a regularizer that works
for all log-linear models, but?in its current form?
is only practical for multiclass classification. We
now exploit the decomposable structure in CRFs to
define a new noising scheme which does not require
us to explicitly sum over all possible outputs y ? Y .
The key idea will be to noise each local feature vec-
tor (which implicitly affects many y) rather than
noise each y independently.
Assume that the output y = (y1, . . . , yT ) is a se-
quence of T tags. In linear chain CRFs, the feature
vector f decomposes into a sum of local feature vec-
tors gt:
f(y, x) =
T?
t=1
gt(yt?1, yt, x), (12)
where gt(a, b, x) is defined on a pair of consecutive
tags a, b for positions t? 1 and t.
Rather than working with a score sy for each
y ? Y , we define a collection of local scores
s = {sa,b,t}, for each tag pair (a, b) and posi-
tion t = 1, . . . , T . We consider noising schemes
which independently set g?t(a, b, x) for each a, b, t.
Let s? = {s?a,b,t} be the corresponding collection of
noised scores.
We can write the log-partition function of these
local scores as follows:
A(s) = log
?
y?Y
exp
{
T?
t=1
syt?1,yt,t
}
. (13)
The first derivative yields the edge marginals under
the model, ?a,b,t = p?(yt?1 = a, yt = b | x), and
the diagonal elements of the Hessian ?2A(s) yield
the marginal variances.
Now, following (7) and (8), we obtain the follow-
ing regularizer:
Rq(?, x) =
1
2
?
a,b,t
?a,b,t(1? ?a,b,t)Var[s?a,b,t],
(14)
where ?a,b,t(1? ?a,b,t) measures model uncertainty
about edge marginals, and Var[s?a,b,t] is simply the
uncertainty due to noising. Again, minimizing the
regularizer means making confident predictions and
having stable scores under feature noise.
Computing partial derivatives So far, we have
defined the regularizer Rq(?, x) based on feature
noising. In order to minimize Rq(?, x), we need to
take its derivative.
First, note that log?a,b,t is the difference of a re-
stricted log-partition function and the log-partition
function. So again by properties of its first deriva-
tive, we have:
? log?a,b,t = Ep?(y|x,yt?1=a,yt=b)[f(y, x)] (15)
? Ep?(y|x)[f(y, x)].
1173
Using the fact that ??a,b,t = ?a,b,t? log?a,b,t and
the fact that Var[s?a,b,t] is a quadratic function in ?,
we can simply apply the product rule to derive the
final gradient?Rq(?, x).
3.1 A Dynamic Program for the Conditional
Expectation
A naive computation of the gradient ?Rq(?, x) re-
quires a full forward-backward pass to compute
Ep?(y|yt?1=a,yt=b,x)[f(y, x)] for each tag pair (a, b)
and position t, resulting in a O(K4T 2) time algo-
rithm.
In this section, we reduce the running time to
O(K2T ) using a more intricate dynamic program.
By the Markov property of the CRF, y1:t?2 only de-
pends on (yt?1, yt) through yt?1 and yt+1:T only
depends on (yt?1, yt) through yt.
First, it will be convenient to define the partial
sum of the local feature vector from positions i to
j as follows:
Gi:j =
j?
t=i
gt(yt?1, yt, x). (16)
Consider the task of computing the feature expecta-
tion Ep?(y|yt?1=a,yt=b)[f(y, x)] for a fixed (a, b, t).
We can expand this quantity into
?
y:yt?1=a,yt=b
p?(y?(t?1:t) | yt?1 = a, yt = b)G1:T .
Conditioning on yt?1, yt decomposes the sum into
three pieces:
?
y:yt?1=a,yt=b
[gt(yt?1 = a, yt = b, x) + F
a
t +B
b
t ],
where
F at =
?
y1:t?2
p?(y1:t?2 | yt?1 = a)G1:t?1, (17)
Bbt =
?
yt+1:T
p?(yt+1:T | yt = b)Gt+1:T , (18)
are the expected feature vectors summed over the
prefix and suffix of the tag sequence, respectively.
Note that F at and B
b
t are analogous to the forward
and backward messages of standard CRF inference,
with the exception that they are vectors rather than
scalars.
We can compute these messages recursively in the
standard way. The forward recurrence is
F at =
?
b
p?(yt?2 = b | yt?1 = a)
[
gt(yt?2 = b, yt?1 = a, x) + F
b
t?1
]
,
and a similar recurrence holds for the backward mes-
sages Bbt .
Running the resulting dynamic program takes
O(K2Tq) time and requires O(KTq) storage,
where K is the number of tags, T is the sequence
length and q is the number of active features. Note
that this is the same order of dependence as normal
CRF training, but there is an additional dependence
on the number of active features q, which makes
training slower.
4 Fast Gradient Computations
In this section, we provide two ways to further im-
prove the efficiency of the gradient calculation based
on ignoring long-range interactions and based on ex-
ploiting feature sparsity.
4.1 Exploiting Feature Sparsity and
Co-occurrence
In each forward-backward pass over a training ex-
ample, we need to compute the conditional ex-
pectations for all features active in that example.
Naively applying the dynamic program in Section 3
is O(K2T ) for each active feature. The total com-
plexity has to factor in the number of active fea-
tures, q. Although q only scales linearly with sen-
tence length, in practice this number could get large
pretty quickly. For example, in the NER tagging ex-
periments (cf. Section 5), the average number of
active features per token is about 20, which means
q ' 20T ; this term quickly dominates the compu-
tational costs. Fortunately, in sequence tagging and
other NLP tasks, the majority of features are sparse
and they often co-occur. That is, some of the ac-
tive features would fire and only fire at the same lo-
cations in a given sequence. This happens when a
particular token triggers multiple rare features.
We observe that all indicator features that only
fired once at position t have the same conditional ex-
pectations (and model expectations). As a result, we
can collapse such a group of features into a single
1174
feature as a preprocessing step to avoid computing
identical expectations for each of the features. Do-
ing so on the same NER tagging experiments cuts
down q/T from 20 to less than 5, and gives us a 4
times speed up at no loss of accuracy. The exact
same trick is applicable to the general CRF gradient
computation as well and gives similar speedup.
4.2 Short-range interactions
It is also possible to speed up the method by re-
sorting to approximate gradients. In our case, the
dynamic program from Section 3 together with the
trick described above ran in a manageable amount
of time. The techniques developed here, however,
could prove to be useful on larger tasks.
Let us rewrite the quantity we want to compute
slightly differently (again, for all a, b, t):
T?
i=1
Ep?(y|x,yt?1=a,yt=b)[gi(yi?1, yi, x)]. (19)
The intuition is that conditioned on yt?1, yt, the
terms gi(yi?1, yi, x) where i is far from t will be
close to Ep?(y|x)[gi(yi?1, yi, x)].
This motivates replacing the former with the latter
whenever |i? k| ? r where r is some window size.
This approximation results in an expression which
only has to consider the sum of the local feature vec-
tors from i?r to i+r, which is captured byGi?r:i+r:
Ep?(y|yt?1=a,yt=b,x)[f(y, x)]? Ep?(y|x)[f(y, x)]
? Ep?(y|yt?1=a,yt=b,x)[Gt?r:t+r] (20)
? Ep?(y|x)[Gt?r:t+r].
We can further approximate this last expression by
letting r = 0, obtaining:
gt(a, b, x)? Ep?(y|x)[gt(yt?1, yt, x)]. (21)
The second expectation can be computed from the
edge marginals.
The accuracy of this approximation hinges on the
lack of long range dependencies. Equation (21)
shows the case of r = 0; this takes almost no addi-
tional effort to compute. However, for some of our
experiments, we observed a 20% difference with the
real derivative. For r > 0, the computational savings
are more limited, but the bounded-window method
is easier to implement.
Dataset q d K Ntrain Ntest
CoNLL 20 437906 5 204567 46666
SANCL 5 679959 12 761738 82405
20news 81 62061 20 15935 3993
RCV14 76 29992 4 9625/2 9625/2
R21578 47 18933 65 5946 2347
TDT2 130 36771 30 9394/2 9394/2
Table 1: Description of datasets. q: average number
of non-zero features per example, d: total number
of features, K: number of classes to predict, Ntrain:
number of training examples, Ntest: number of test
examples.
5 Experiments
We show experimental results on the CoNLL-2003
Named Entity Recognition (NER) task, the SANCL
Part-of-speech (POS) tagging task, and several doc-
ument classification tasks.2 The datasets used are
described in Table 1. We used standard splits when-
ever available; otherwise we split the data at ran-
dom into a test set and a train set of equal sizes
(RCV14, TDT2). CoNLL has a development set
of size 51578, which we used to tune regulariza-
tion parameters. The SANCL test set is divided into
3 genres, namely answers, newsgroups, and
reviews, each of which has a corresponding de-
velopment set.3
5.1 Multiclass Classification
We begin by testing our regularizer in the simple
case of classification where Y = {1, 2, . . . ,K} for
K classes. We examine the performance of the nois-
ing regularizer in both the fully supervised setting as
well as the transductive learning setting.
In the transductive learning setting, the learner
is allowed to inspect the test features at train time
(without the labels). We used the method described
in Section 2.1 for transductive dropout.
2The document classification data are available
at http://www.csie.ntu.edu.tw/?cjlin/
libsvmtools/datasets and http://www.cad.
zju.edu.cn/home/dengcai/Data/TextData.html
3The SANCL dataset has two additional genres?emails and
weblogs?that we did not use, as we did not have access to
development sets for these genres.
1175
Dataset K None L2 Drop +Test
CoNLL 5 78.03 80.12 80.90 81.66
20news 20 81.44 82.19 83.37 84.71
RCV14 4 95.76 95.90 96.03 96.11
R21578 65 92.24 92.24 92.24 92.58
TDT2 30 97.74 97.91 98.00 98.12
Table 2: Classification performance and transduc-
tive learning results on some standard datasets.
None: use no regularization, Drop: quadratic ap-
proximation to the dropout noise (8), +Test: also use
the test set to estimate the noising regularizer (11).
5.1.1 Semi-supervised Learning with Feature
Noising
In the transductive setting, we used test data
(without labels) to learn a better regularizer. As an
alternative, we could also use unlabeled data in place
of the test data to accomplish a similar goal; this
leads to a semi-supervised setting.
To test the semi-supervised idea, we use the same
datasets as above. We split each dataset evenly into
3 thirds that we use as a training set, a test set and an
unlabeled dataset. Results are given in Table 3.
In most cases, our semi-supervised accuracies are
lower than the transductive accuracies given in Table
2; this is normal in our setup, because we used less
labeled data to train the semi-supervised classifier
than the transductive one.4
5.1.2 The Second-Order Approximation
The results reported above all rely on the ap-
proximate dropout regularizer (8) that is based on a
second-order Taylor expansion. To test the validity
of this approximation we compare it to the Gaussian
method developed by Wang and Manning (2013) on
a two-class classification task.
We use the 20-newsgroups alt.atheism vs
soc.religion.christian classification task;
results are shown in Figure 2. There are 1427 exam-
4The CoNNL results look somewhat surprising, as the semi-
supervised results are better than the transductive ones. The
reason for this is that the original CoNLL test set came from a
different distributions than the training set, and this made the
task more difficult. Meanwhile, in our semi-supervised experi-
ment, the test and train sets are drawn from the same distribu-
tion and so our semi-supervised task is actually easier than the
original one.
Dataset K L2 Drop +Unlabeled
CoNLL 5 91.46 91.81 92.02
20news 20 76.55 79.07 80.47
RCV14 4 94.76 94.79 95.16
R21578 65 90.67 91.24 90.30
TDT2 30 97.34 97.54 97.89
Table 3: Semisupervised learning results on some
standard datasets. A third (33%) of the full dataset
was used for training, a third for testing, and the rest
as unlabeled.
10?6 10?4 10?2 100 102
0.78
0.8
0.82
0.84
0.86
0.88
0.9
L2 regularization strength (?)
Ac
cu
rac
y
 
 
L2 only
L2+Gaussian dropout
L2+Quadratic dropout
Figure 2: Effect of ? in ????22 on the testset perfor-
mance. Plotted is the test set accuracy with logis-
tic regression as a function of ? for the L2 regular-
izer, Gaussian dropout (Wang and Manning, 2013)
+ additional L2, and quadratic dropout (8) + L2 de-
scribed in this paper. The default noising regularizer
is quite good, and additional L2 does not help. No-
tice that no choice of ? in L2 can help us combat
overfitting as effectively as (8) without underfitting.
ples with 22178 features, split evenly and randomly
into a training set and a test set.
Over a broad range of ? values, we find that
dropout plus L2 regularization performs far better
than using just L2 regularization for any value of
?. We see that Gaussian dropout appears to per-
form slightly better than the quadratic approxima-
tion discussed in this paper. However, our quadratic
approximation extends easily to the multiclass case
and to structured prediction in general, while Gaus-
sian dropout does not. Thus, it appears that our ap-
proximation presents a reasonable trade-off between
1176
computational efficiency and prediction accuracy.
5.2 CRF Experiments
We evaluate the quadratic dropout regularizer in
linear-chain CRFs on two sequence tagging tasks:
the CoNLL 2003 NER shared task (Tjong Kim Sang
and De Meulder, 2003) and the SANCL 2012 POS
tagging task (Petrov and McDonald, 2012) .
The standard CoNLL-2003 English shared task
benchmark dataset (Tjong Kim Sang and De Meul-
der, 2003) is a collection of documents from
Reuters newswire articles, annotated with four en-
tity types: Person, Location, Organization, and
Miscellaneous. We predicted the label sequence
Y = {LOC, MISC, ORG, PER, O}T without con-
sidering the BIO tags.
For training the CRF model, we used a compre-
hensive set of features from Finkel et al (2005) that
gives state-of-the-art results on this task. A total
number of 437906 features were generated on the
CoNLL-2003 training dataset. The most important
features are:
? The word, word shape, and letter n-grams (up to
6gram) at current position
? The prediction, word, and word shape of the pre-
vious and next position
? Previous word shape in conjunction with current
word shape
? Disjunctive word set of the previous and next 4
positions
? Capitalization pattern in a 3 word window
? Previous two words in conjunction with the word
shape of the previous word
? The current word matched against a list of name
titles (e.g., Mr., Mrs.)
The F?=1 results are summarized in Table 4. We
obtain a 1.6% and 1.1% absolute gain on the test
and dev set, respectively. Detailed results are bro-
ken down by precision and recall for each tag and are
shown in Table 6. These improvements are signifi-
cant at the 0.1% level according to the paired boot-
strap resampling method of 2000 iterations (Efron
and Tibshirani, 1993).
For the SANCL (Petrov and McDonald, 2012)
POS tagging task, we used the same CRF framework
with a much simpler set of features
? word unigrams: w?1, w0, w1
? word bigram: (w?1, w0) and (w0, w1)
F?=1 None L2 Drop
Dev 89.40 90.73 91.86
Test 84.67 85.82 87.42
Table 4: CoNLL summary of results. None: no reg-
ularization, Drop: quadratic dropout regularization
(14) described in this paper.
F?=1 None L2 Drop
newsgroups
Dev 91.34 91.34 91.47
Test 91.44 91.44 91.81
reviews
Dev 91.97 91.95 92.10
Test 90.70 90.67 91.07
answers
Dev 90.78 90.79 90.70
Test 91.00 90.99 91.09
Table 5: SANCL POS tagging F?=1 scores for the 3
official evaluation sets.
We obtained a small but consistent improvement
using the quadratic dropout regularizer in (14) over
the L2-regularized CRFs baseline.
Although the difference on SANCL is small,
the performance differences on the test sets of
reviews and newsgroups are statistically sig-
nificant at the 0.1% level. This is also interesting
because here is a situation where the features are ex-
tremely sparse, L2 regularization gave no improve-
ment, and where regularization overall matters less.
6 Conclusion
We have presented a new regularizer for learning
log-linear models such as multiclass logistic regres-
sion and conditional random fields. This regularizer
is based on a second-order approximation of fea-
ture noising schemes, and attempts to favor mod-
els that predict confidently and are robust to noise
in the data. In order to apply our method to CRFs,
we tackle the key challenge of dealing with feature
correlations that arise in the structured prediction
setting in several ways. In addition, we show that
the regularizer can be applied naturally in the semi-
supervised setting. Finally, we applied our method
to a range of different datasets and demonstrate con-
sistent gains over standard L2 regularization. Inves-
1177
Precision Recall F?=1
LOC 91.47% 91.12% 91.29
MISC 88.77% 81.07% 84.75
ORG 85.22% 84.08% 84.65
PER 92.12% 93.97% 93.04
Overall 89.84% 88.97% 89.40
(a) CoNLL dev. set with no regularization
Precision Recall F?=1
92.05% 92.84% 92.44
90.51% 83.52% 86.87
88.35% 85.23% 86.76
93.12% 94.19% 93.65
91.36% 90.11% 90.73
(b) CoNLL dev. set with L2 reg-
ularization
Precision Recall F?=1
93.59% 92.69% 93.14
93.99% 81.47% 87.28
92.48% 84.61% 88.37
94.81% 95.11% 94.96
93.85% 89.96% 91.86
(c) CoNLL dev. set with dropout
regularization
Tag Precision Recall F?=1
LOC 87.33% 84.47% 85.87
MISC 78.93% 77.12% 78.02
ORG 78.70% 79.49% 79.09
PER 88.82% 93.11% 90.92
Overall 84.28% 85.06% 84.67
(d) CoNLL test set with no regularization
Precision Recall F?=1
87.96% 86.13% 87.03
77.53% 79.30% 78.41
81.30% 80.49% 80.89
90.30% 93.33% 91.79
85.57% 86.08% 85.82
(e) CoNLL test set with L2 reg-
ularization
Precision Recall F?=1
86.26% 87.74% 86.99
81.52% 77.34% 79.37
88.29% 81.89% 84.97
92.15% 92.68% 92.41
88.40% 86.45% 87.42
(f) CoNLL test set with dropout
regularization
Table 6: CoNLL NER results broken down by tags and by precision, recall, and F?=1. Top: development
set, bottom: test set performance.
tigating how to better optimize this non-convex reg-
ularizer online and convincingly scale it to the semi-
supervised setting seem to be promising future di-
rections.
Acknowledgements
The authors would like to thank the anonymous re-
viewers for their comments. We gratefully acknowl-
edge the support of the Defense Advanced Research
Projects Agency (DARPA) Broad Operational Lan-
guage Translation (BOLT) program through IBM.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the author(s) and do not necessarily reflect the view
of the DARPA, or the US government. S. Wager is
supported by a BC and EJ Eaves SGF Fellowship.
References
Yaser S. Abu-Mostafa. 1990. Learning from hints in
neural networks. Journal of Complexity, 6(2):192?
198.
Chris M. Bishop. 1995. Training with noise is equiva-
lent to Tikhonov regularization. Neural computation,
7(1):108?116.
Robert Bryll, Ricardo Gutierrez-Osuna, and Francis
Quek. 2003. Attribute bagging: improving accuracy
of classifier ensembles by using random feature sub-
sets. Pattern recognition, 36(6):1291?1302.
Chris J.C. Burges and Bernhard Scho?lkopf. 1997. Im-
proving the accuracy and speed of support vector ma-
chines. In Advances in Neural Information Processing
Systems, pages 375?381.
Brad Efron and Robert Tibshirani. 1993. An Introduction
to the Bootstrap. Chapman & Hall, New York.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd annual meeting of
the Association for Computational Linguistics, pages
363?370.
Yves Grandvalet and Yoshua Bengio. 2005. Entropy
regularization. In Semi-Supervised Learning, United
Kingdom. Springer.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R. Salakhutdinov.
2012. Improving neural networks by preventing
co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved se-
quence segmentation and labeling. In Proceedings of
the 44th annual meeting of the Association for Com-
putational Linguistics, ACL-44, pages 209?216.
Thorsten Joachims. 1999. Transductive inference for
1178
text classification using support vector machines. In
Proceedings of the International Conference on Ma-
chine Learning, pages 200?209.
Wei Li and Andrew McCallum. 2005. Semi-supervised
sequence modeling with syntactic topic models. In
Proceedings of the 20th national conference on Arti-
ficial Intelligence - Volume 2, AAAI?05, pages 813?
818.
Gideon S. Mann and Andrew McCallum. 2007. Sim-
ple, robust, scalable semi-supervised learning via ex-
pectation regularization. In Proceedings of the Inter-
national Conference on Machine Learning.
Kiyotoshi Matsuoka. 1992. Noise injection into inputs
in back-propagation learning. Systems, Man and Cy-
bernetics, IEEE Transactions on, 22(3):436?440.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 shared task on parsing the web. Notes of the First
Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL).
Salah Rifai, Yann Dauphin, Pascal Vincent, Yoshua Ben-
gio, and Xavier Muller. 2011a. The manifold tangent
classifier. Advances in Neural Information Processing
Systems, 24:2294?2302.
Salah Rifai, Xavier Glorot, Yoshua Bengio, and Pascal
Vincent. 2011b. Adding noise to the input of a model
trained with a regularized objective. arXiv preprint
arXiv:1104.3250.
Patrice Y. Simard, Yann A. Le Cun, John S. Denker, and
Bernard Victorri. 2000. Transformation invariance in
pattern recognition: Tangent distance and propagation.
International Journal of Imaging Systems and Tech-
nology, 11(3):181?197.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005.
Logarithmic opinion pools for conditional random
fields. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, pages 18?
25. Association for Computational Linguistics.
Charles Sutton, Michael Sindelar, and Andrew McCal-
lum. 2005. Feature bagging: Preventing weight un-
dertraining in structured discriminative learning. Cen-
ter for Intelligent Information Retrieval, U. of Mas-
sachusetts.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: language-
independent named entity recognition. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL 2003 - Volume 4, CONLL ?03,
pages 142?147.
Laurens van der Maaten, Minmin Chen, Stephen Tyree,
and Kilian Q. Weinberger. 2013. Learning with
marginalized corrupted features. In Proceedings of the
International Conference on Machine Learning.
Stefan Wager, Sida Wang, and Percy Liang. 2013.
Dropout training as adaptive regularization. arXiv
preprint:1307.1493.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, and
Rob Fergus. 2013. Regularization of neural networks
using dropconnect. In Proceedings of the Interna-
tional Conference on Machine learning.
Sida Wang and Christopher D. Manning. 2013. Fast
dropout training. In Proceedings of the International
Conference on Machine Learning.
1179
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1710?1720,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Learning Biological Processes with Global Constraints
Aju Thalappillil Scaria?, Jonathan Berant?, Mengqiu Wang and Christopher D. Manning
Stanford University, Stanford
Justin Lewis and Brittany Harding
University of Washington, Seattle
Peter Clark
Allen Institute for Artificial Intelligence, Seattle
Abstract
Biological processes are complex phenom-
ena involving a series of events that are re-
lated to one another through various relation-
ships. Systems that can understand and rea-
son over biological processes would dramat-
ically improve the performance of semantic
applications involving inference such as ques-
tion answering (QA) ? specifically ?How??
and ?Why?? questions. In this paper, we
present the task of process extraction, in
which events within a process and the rela-
tions between the events are automatically ex-
tracted from text. We represent processes by
graphs whose edges describe a set of temporal,
causal and co-reference event-event relations,
and characterize the structural properties of
these graphs (e.g., the graphs are connected).
Then, we present a method for extracting rela-
tions between the events, which exploits these
structural properties by performing joint in-
ference over the set of extracted relations.
On a novel dataset containing 148 descrip-
tions of biological processes (released with
this paper), we show significant improvement
comparing to baselines that disregard process
structure.
1 Introduction
A process is defined as a series of inter-related
events that involve multiple entities and lead to an
end result. Product manufacturing, economical de-
velopments, and various phenomena in life and so-
cial sciences can all be viewed as types of processes.
Processes are complicated objects; consider for ex-
ample the biological process of ATP synthesis de-
scribed in Figure 1. This process involves 12 en-
tities and 8 events. Additionally, it describes rela-
tions between events and entities, and the relation-
ship between events (e.g., the second occurrence of
the event ?enter?, causes the event ?changing?).
?Both authors equally contributed to the paper
Automatically extracting the structure of pro-
cesses from text is crucial for applications that re-
quire reasoning, such as non-factoid QA. For in-
stance, answering a question on ATP synthesis, such
as ?How do H+ ions contribute to the production
of ATP?? requires a structure that links H+ ions
(Figure 1, sentence 1) to ATP (Figure 1, sentence
4) through a sequence of intermediate events. Such
?How?? questions are common on FAQ websites
(Surdeanu et al, 2011), which further supports the
importance of process extraction.
Process extraction is related to two recent lines
of work in Information Extraction ? event extrac-
tion and timeline construction. Traditional event ex-
traction focuses on identifying a closed set of events
within a single sentence. For example, the BioNLP
2009 and 2011 shared tasks (Kim et al, 2009; Kim
et al, 2011) consider nine events types related to
proteins. In practice, events are currently almost al-
ways extracted from a single sentence. Process ex-
traction, on the other hand, is centered around dis-
covering relations between events that span multiple
sentences. The set of possible event types in process
extraction is also much larger.
Timeline construction involves identifying tem-
poral relations between events (Do et al, 2012; Mc-
Closky and Manning, 2012; D?Souza and Ng, 2013),
and is thus related to process extraction as both fo-
cus on event-event relations spanning multiple sen-
tences. However, events in processes are tightly cou-
pled in ways that go beyond simple temporal order-
ing, and these dependencies are central for the pro-
cess extraction task. Hence, capturing process struc-
ture requires modeling a larger set of relations that
includes temporal, causal and co-reference relations.
In this paper, we formally define the task of
process extraction and present automatic extraction
methods. Our approach handles an open set of event
types and works over multiple sentences, extract-
ing a rich set of event-event relations. Furthermore,
1710
7/5/13 7:01 PMbrat
Page 1 of 1http://127.0.0.1:8001/index.xhtml#/examples/emnlp2013/p66
H+ ions flowing down their gradient enter a half channel in a stator, which is anchored in the membrane.
H+ ions enter binding sites within a rotor, changing the shape of each subunit so that the rotor spins within the membrane.
Spinning of the rotor causes an internal rod to spin as well.
Turning of the rod activates catalytic sites in the knob that can produce ATP from ADP and P_i.
Entity Event Entity Event Entity Entity
cotemp
prev
Entity Event Entity Entity Event Entity Entity Event Entity
same
causes causes
prev
Event Entity Entity Event
same
same causes
Event Entity Event Entity Event Entity Entity
resultsame
raw-materialcauses causes
1
2
3
4
Figure 1: Partial annotation of the ATP synthesis process. Most of the semantic roles have been removed for simplicity.
we characterize a set of global properties of process
structure that can be utilized during process extrac-
tion. For example, all events in a process are some-
how connected to one another. Also, processes usu-
ally exhibit a ?chain-like? structure reflecting pro-
cess progression over time. We show that incor-
porating such global properties into our model and
performing joint inference over the extracted rela-
tions significantly improves the quality of process
structures predicted. We conduct experiments on a
novel dataset of process descriptions from the text-
book ?Biology? (Campbell and Reece, 2005) that
were annotated by trained biologists. Our method
does not require any domain-specific knowledge and
can be easily adapted to non-biology domains.
The main contributions of this paper are:
1. We define process extraction and characterize
processes? structural properties.
2. We model global structural properties in pro-
cesses and demonstrate significant improve-
ment in extraction accuracy.
3. We publicly release a novel data set of 148
fully annotated biological process descrip-
tions along with the source code for our sys-
tem. The dataset and code can be down-
loaded from http://nlp.stanford.edu/
software/bioprocess/.
2 Process Definition and Dataset
We define a process description as a paragraph or
sequence of tokens x = {x1, . . . x|x|} that describes
a series of events related by temporal and/or causal
relations. For example, in ATP synthesis (Figure 1),
the event of rotor spinning causes the event where
an internal rod spins.
We model the events within a process and their
relations by a directed graph P = (V,E), where
the nodes V = {1, . . . , |V |} represent event men-
tions and labeled edges E correspond to event-event
relations. An event mention v ? V is defined by a
trigger tv, which is a span of words xi, xi+1, . . . , xj ;
and by a set of argument mentions Av, where each
argument mention av ? Av is also a span of words
labeled by a semantic role l taken from a set L. For
example, in the last event mention of ATP synthesis,
tv = produce, and one of the argument mentions is
av = (ATP, RESULT). A labeled edge (u, v, r) in the
graph describes a relation r ? R between the event
mentions u and v. The task of process extraction is
to extract the graph P from the text x.1
A natural way to break down process extraction
into sub-parts is to first perform semantic role label-
ing (SRL), that is, identify triggers and predict ar-
gument mentions with their semantic role, and then
extract event-event relations between pairs of event
mentions. In this paper, we focus on the second
step, where given a set of event triggers T , we find
all event-event relations, where a trigger represents
the entire event. For completeness, we now describe
the semantic roles L used in our dataset, and then
1Argument mentions are also related by coreference rela-
tions, but we neglect that since it is not central in this paper.
1711
present the set of event-event relationsR.
The setL contains standard semantic roles such as
AGENT, THEME, ORIGIN, DESTINATION and LO-
CATION. Two additional semantic roles were em-
ployed that are relevant for biological text: RESULT
corresponds to an entity that is the result of an event,
and RAW-MATERIAL describes an entity that is used
or consumed during an event. For example, the last
event ?produce? in Figure 1, has ?ATP? as the RE-
SULT, and ?ADP? as the RAW-MATERIAL.
The event-event relation set R contains the fol-
lowing (assuming a labeled edge (u, v, r)):
1. PREV denotes that u is an event immediately
before v. Thus, the edges (u, v, PREV) and
(v, w, PREV), preclude the edge (u,w, PREV).
For example, in ?When a photon strikes
. . . energy is passed . . . until it reaches . . . ?,
there is no edge (strikes, reaches, PREV) due
to the intervening event ?passed?.
2. COTEMP denotes that events u and v overlap in
time (e.g., the first two event mentions flowing
and enter in Figure 1).
3. SUPER denotes that event u includes event
v. For instance, in ?During DNA replica-
tion, DNA polymerases proofread each nu-
cleotide. . . ? there is an edge (DNA replication,
proofread, SUPER).
4. CAUSES denotes that event u causes event v
(e.g., the relation between changing and spins
in sentence 2 of Figure 1).
5. ENABLES denotes that event u creates precon-
ditions that allow event v to take place. For
example, the description ?. . . cause cancer cells
to lose attachments to neighboring cells. . . , al-
lowing them to spread into nearby tissues? has
the edge (lose, spread, ENABLES). An in-
tuitive way to think about the difference be-
tween Causes and Enables is the following: if
u causes v this means that if u happens, then
v happens. If u enables v, then if u does not
happen, then v does not happen.
6. SAME denotes that u and v both refer to the
same event (spins and Spinning in Figure 1).
Early work on temporal logic (Allen, 1983) con-
tained more temporal relations than are used in our
Avg Min Max
# of sentences 3.80 1 15
# of tokens 89.98 19 319
# of events 6.20 2 15
# of non-NONE relations 5.64 1 24
Table 1: Process statistics over 148 process descriptions.
NONE is used to indicate no relation.
relation set R. We chose a relation set R that cap-
tures the essential aspects of temporal relations be-
tween events in a process, while keeping the annota-
tion as simple as possible. For instance, we include
the SUPER relation that appears in temporal anno-
tations such as the Timebank corpus (Pustejovsky
et al, 2003) and Allen?s work, but in practice was
not considered by many temporal ordering systems
(Chambers and Jurafsky, 2008; Yoshikawa et al,
2009; Do et al, 2012). Importantly, our relation set
also includes the relations CAUSES and ENABLES,
which are fundamental to modeling processes and
go beyond simple temporal ordering.
We also added event coreference (SAME) to R.
Do et al (2012) used event coreference information
in a temporal ordering task to modify probabilities
provided by pairwise classifiers prior to joint infer-
ence. In this paper, we simply treat SAME as an-
other event-event relation, which allows us to easily
perform joint inference and employ structural con-
straints that combine both coreference and temporal
relations simultaneously. For example, if u and v are
the same event, then there can exist no w, such that
u is before w, but v is after w (see Section 3.3)
We annotated 148 process descriptions based on
the aforementioned definitions. Further details on
annotation and data set statistics are provided in Sec-
tion 4 and Table 1.
Structural properties of processes Coherent pro-
cesses exhibit many structural properties. For ex-
ample, two argument mentions related to the same
event cannot overlap ? a constraint that has been
used in the past in SRL (Toutanova et al, 2008). In
this paper we focus on three main structural prop-
erties of the graph P . First, in a coherent pro-
cess, all events mentioned are related to one another,
and hence the graph P must be connected. Sec-
ond, processes tend to have a ?chain-like? structure
where one event follows another, and thus we expect
1712
Deg. Gold Local Global
0 0 29 0
1 219 274 224
2 369 337 408
3 46 14 17
? 4 22 2 7
Table 2: Node degree distribution for event mentions on
the training set. Predictions for the Local and Global
models were obtained using 10-fold cross validation.
nodes? degree to generally be ? 2. Indeed, 90% of
event mentions have degree ? 2, as demonstrated
by the Gold column of Table 2. Last, if we consider
relations between all possible triples of events in a
process, clearly some configurations are impossible,
while others are common (illustrated in Figure 2).
In Section 3.3, we show that modeling these proper-
ties using a joint inference framework improves the
quality of process extraction significantly.
3 Joint Model for Process Extraction
Given a paragraph x and a trigger set T , we wish
to extract all event-event relations E. Similar to Do
et al (2012), our model consists of a local pairwise
classifier and global constraints. We first introduce
a classifier that is based on features from previous
work. Next, we describe novel features specific for
process extraction. Last, we incorporate global con-
straints into our model using an ILP formulation.
3.1 Local pairwise classifier
The local pairwise classifier predicts relations be-
tween all event mention pairs. In order to model
the direction of relations, we expand the set R to
include the reverse of four directed relations: PREV-
NEXT, SUPER- SUB, CAUSES-CAUSED, ENABLES-
ENABLED. After adding NONE to indicate no rela-
tion, and including the undirected relations COTEMP
and SAME,R contains 11 relations. The classifier is
hence a function f : T ? T ? R. As an example,
f(ti, tj) = PREV iff f(tj , ti) = NEXT. Let n be the
number of triggers in a process, and ti be the i-th
trigger in its description. Since f(ti, tj) completely
determines f(tj , ti), it suffices to consider only pairs
with i < j. Note that the process graph P is undi-
rected under the new definition ofR.
Table 3 describes features from previous
Feature Description
POS Pair of POS tags
Lemma Pair of lemmas
Prep? Preposition lexeme, if in a prepositional phrase
Sent. count Quantized number of sentences between triggers
Word count Quantized number of words between triggers
LCA Least common ancestor on constituency tree, if exists
Dominates? Whether one trigger dominates other
Share Whether triggers share a child on dependency tree
Adjacency Whether two triggers are adjacent
Words btw. For adjacent triggers, content words between triggers
Temp. btw. For adjacent triggers, temporal connectives (from a
small list) between triggers
Table 3: Features extracted for a trigger pair (ti, tj). As-
teriks (*) indicate features that are duplicated, once for
each trigger.
work (Chambers and Jurafsky, 2008; Do et al,
2012) extracted for a trigger pair (ti, tj). Some
features were omitted since they did not yield
improvement in performance on a development set
(e.g., lemmas and part-of-speech tags of context
words surrounding ti and tj), or they require gold
annotations provided in TimeBank, which we do
not have (e.g., tense and aspect of triggers). To
reduce sparseness, we convert nominalizations into
their verbal forms when computing word lemmas,
using WordNet?s (Fellbaum, 1998) derivation links.
3.2 Classifier extensions
A central source of information to extract event-
event relations from text are connectives such as af-
ter, during, etc. However, there is variability in the
occurrence of these connectives as demonstrated by
the following two sentences (connectives in bold-
face, triggers in italics):
1. Because alleles are exchanged during gene flow, ge-
netic differences are reduced.
2. During gene flow, alleles are exchanged, and genetic
differences are hence reduced.
Even though both sentences express the same re-
lation (exchanged, reduced,CAUSES), the connec-
tives used and their linear position with respect to the
triggers are different. Also, in sentence 1, gene flow
intervenes between exchanged and reduced. Since
our dataset is small, we wish to identify the trig-
gers related to each connective, and share features
between such sentences. We do this using the syn-
tactic structure and by clustering the connectives.
1713
tj
ti tk
(a) SAME transitivity
SAMESAME
SAME
tj
ti tk
(b) CAUSE-COTEMP
CAUSES
CAUSES COTEMP
tj
ti tk
(c) COTEMP transitivity
COTEMPCOTEMP
COTEMP / SAME
tj
ti tk
(d) SAME contradiction
PREVPREV
SAME
tj
ti tk
(e) PREV contradiction
PREVPREV
PREV
Figure 2: Relation triangles (a)-(c) are common in the gold standard while (d)-(e) are impossible.
Sentence 1 presents a typical case where by walk-
ing up the dependency tree from the marker because,
we can find the triggers related by this marker:
because
mark
???? exchanged
advcl
???? reduced. When-
ever a trigger is the head of an adverbial clause and
marked by a mark dependency label, we walk on the
dependency tree and look for a trigger in the main
clause that is closest to the root (or the root itself
in this example). By utilizing the syntactic struc-
ture, we can correctly spot that the trigger gene flow
is not related to the trigger exchanged through the
connective because, even though they are linearly
closer. In order to reduce sparseness of connectives,
we created a hand-made clustering of 30 connectives
that maps words into clusters2 (e.g., because, since
and hence to a ?causality? cluster). After locating
the relevant pair of triggers, we use these clusters
to fire the same feature for connectives belonging to
the same cluster. We perform a similar procedure
whenever a trigger is part of a prepositional phrase
(imagine sentence 1 starting with ?due to allele ex-
change during gene flow . . . ?) by walking up the
constituency tree, but details are omitted for brevity.
In sentence 2, the connective hence is an adverbial
modifier of the trigger reduced. We look up the clus-
ter for the connective hence and fire the same feature
for the adjacent triggers exchanged and reduced.
We further extend our features to handle the rich
relation set necessary for process extraction. The
first event of a process is often expressed as a nom-
inalization and includes subsequent events (SUPER
relation), e.g., ?The Calvin cycle begins by incor-
porating...?. To capture this, we add a feature that
fires when the first event of the process description
is a noun. We also add two features targeted at the
2The full set of connectives and their clustering are provided
as part of our publicly released package.
SAME relation: one indicating if the lemmas of ti
and tj are the same, and another specifying the de-
terminer of tj , if it exists. Certain determiners in-
dicate that an event trigger has already been men-
tioned, e.g., the determiner this hints a SAME rela-
tion in ?The next steps decompose citrate back to
oxaloacetate. This regeneration makes . . . ?. Last,
we add as a feature the dependency path between ti
and tj , if it exists, e.g., in ?meiosis produces cells
that divide . . . ?, the feature
dobj
???
rcmod
???? is fired for
the trigger pair produces and divide. In Section 4.1
we empirically show that our extensions to the local
classifier substantially improve performance.
For our pairwise classifier, we train a maximum
entropy classifier that computes a probability pijr
for every trigger pair (ti, tj) and relation r. Hence,
f(ti, tj) = arg maxr pijr.
3.3 Global Constraints
Naturally, pairwise classifiers are local models that
can violate global properties in the process structure.
Figure 3 (left) presents an example for predictions
made by the pairwise classifier, which result in two
triggers (deleted and dupcliated) that are isolated
from the rest of the triggers. In this section, we dis-
cuss how we incorporate constraints into our model
to generate coherent global process structures.
Let ?ijr be the score for a relation r between the
trigger pair (ti, tj) (e.g., ?ijr = log pijr), and yijr be
the corresponding indicator variable. Our goal is to
find an assignment for the indicators y = {yijr | 1 ?
i < j ? n, r ? R}. With no global constraints this
can be formulated as the following ILP:
1714
arg max
y
?
ijr
?ijryijr (1)
s.t.?i,j
?
r
yijr = 1
where the constraint ensures exactly one relation be-
tween each event pair. We now describe constraints
that result in a coherent global process structure.
Connectivity Our ILP formulation for enforcing
connectivity is a minor variation of the one sug-
gested by Martins et al (2009) for dependency pars-
ing. In our setup, we want P to be a connected undi-
rected graph, and not a directed tree. However, an
undirected graph P is connected iff there exists a
directed tree that is a subgraph of P when edge di-
rections are ignored. Thus the resulting formulation
is almost identical and is based on flow constraints
which ensure that there is a path from a designated
root in the graph to all other nodes.
Let R? be the set R \ NONE. An edge (ti, tj) is
in E iff there is some non-NONE relation between
ti and tj , i.e. iff yij :=
?
r?R? yijr is equal to 1.
For each variable yij we define two auxiliary binary
variables zij and zji that correspond to edges of the
directed tree that is a subgraph of P . We ensure that
the edges in the tree exist also in P by tying each
auxiliary variable to its corresponding ILP variable:
?i<j zij ? yij , zji ? yij (2)
Next, we add constraints that ensure that the graph
structure induced by the auxiliary variables is a tree
rooted in an arbitrary node 1 (The choice of root
does not affect connectivity). We add for every i 6= j
a flow variable ?ij which specifies the amount of
flow on the directed edge zij .
?
i
zi1 = 0, ?j 6=1
?
i
zij = 1 (3)
?
i
?1i = n? 1 (4)
?j 6=1
?
i
?ij ?
?
k
?jk = 1 (5)
?i 6=j ?ij ? n ? zij (6)
Equation 3 says that all nodes in the graph have
exactly one parent, except for the root that has no
parents. Equation 4 ensures that the outgoing flow
from the root is n?1, and Equation 5 states that each
of the other n ? 1 nodes consume exactly one unit
of flow. Last, Equation 6 ties the auxiliary variables
to the flow variables, making sure that flow occurs
only on edges. The combination of these constraints
guarantees that the graph induced by the variables
zij is a directed tree and consequently the graph in-
duced by the objective variables y is connected.
Chain structure A chain is a connected graph
where the degree of all nodes is ? 2. Table 2
presents nodes? degree and demonstrates that indeed
process graphs are close to being chains. The fol-
lowing constraint bounds nodes? degree by 2:
?j(
?
i<j
yij +
?
j<k
yjk ? 2) (7)
Since graph structures are not always chains, we
add this as a soft constraint, that is, we penalize the
objective for each node with degree > 2. The chain
structure is one of the several soft constraints we
enforce. Thus, our modified objective function is
?
ijr ?ijryijr +
?
k?K ?kCk, where K is the set of
soft constraints, ?k is the penalty (or reward for de-
sirable structures), and Ck indicates whether a con-
straint is violated (or satisfied). Note that under this
formulation our model is simply a constrained con-
ditional model (wei Chang et al, 2012). The param-
eters ?k are tuned on a development set (see Sec-
tion 4).
Relation triads A relation triad (or a re-
lation triangle) for any three triggers ti, tj
and tk in a process is a 3-tuple of relations
(f(ti, tj), f(tj , tk), f(ti, tk)). Clearly, some triads
are impossible while others are quite common. To
find triads that could improve process extraction, the
frequency of all possible triads in both the training
set and the output of the pairwise classifier were
found, and we focused on those for which the clas-
sifier and the gold standard disagree. We are inter-
ested in triads that never occur in training data but
are predicted by the classifier, and vice versa. Fig-
ure 2 illustrates some of the triads found and Equa-
1715
tions 8-12 provide the corresponding ILP formula-
tions. Equations 8-10 were formulated as soft con-
straints (expanding the setK) and were incorporated
by defining a reward ?k for each triad type.3 On
the other hand, Equations 11-12 were formulated as
hard constraints to prevent certain structures.
1. SAME transitivity (Figure 2a, Eqn. 8): Co-
reference transitivity has been used in past
work (Finkel and Manning, 2008) and we in-
corporate it by a constraint that encourages tri-
ads that respect transitivity.
2. CAUSE-COTEMP (Figure 2b, Eqn. 9): If ti
causes both tj and tk, then often tj and tk are
co-temporal. E.g, in ?genetic drift has led to
a loss of genetic variation and an increase in
the frequency of . . .?, a single event causes two
subsequent events that occur simultaneously.
3. COTEMP transitivity (Figure 2c, Eqn. 10): If
ti is co-temporal with tj and tj is co-temporal
with tk, then usually ti and tk are either co-
temporal or denote the same event.
4. SAME contradiction (Figure 2d, Eqn. 11): If
ti is the same event as tk, then their tempo-
ral ordering with respect to a third trigger tj
may result in a contradiction, e.g., if tj is af-
ter ti, but before tk. We define 5 temporal
categories that generate
(5
2
)
possible contradic-
tions, but for brevity present just one represen-
tative hard constraint. This constraint depends
on prediction of temporal and co-reference re-
lations jointly.
5. PREV contradiction (Figure 2e, Eqn. 12): As
mentioned (Section 3.3), if ti is immediately
before tj , and tj is immediately before tk, then
ti cannot be immediately before tk.
yijSAME + yjkSAME + yikSAME ? 3 (8)
yijCAUSES + yikCAUSES + yjkCOTEMP ? 3 (9)
yijCOTEMP + yjkCOTEMP + yikCOTEMP+
yikSAME ? 3 (10)
yijPREV + yjkPREV + yikSAME ? 2 (11)
yijPREV + yjkPREV ? yikNONE ? 1 (12)
3We experimented with a reward for certain triads or a
penalty for others and empirically found that using rewards re-
sults in better performance on the development set.
We used the Gurobi optimization package4 to
find an exact solution for our ILP, which contains
O(n2|R|) variables and O(n3) constraints. We also
developed an equivalent formulation amenable to
dual decomposition (Sontag et al, 2011), which is a
faster approximation method. But practically, solv-
ing the ILP exactly with Gurobi was quite fast (av-
erage/median time per process: 0.294 sec/0.152 sec
on a standard laptop).
4 Experimental Evaluation
We extracted 148 process descriptions by going
through chapters from the textbook ?Biology? and
marking any contiguous sequence of sentences that
describes a process, i.e., a series of events that lead
towards some objective. Then, each process descrip-
tion was annotated by a biologist. The annotator was
first presented with annotation guidelines and anno-
tated 20 descriptions. The annotations were then
discussed with the authors, after which all process
descriptions were annotated. After training a sec-
ond biologist, we measured inter-annotator agree-
ment ? = 0.69, on 30 random process descriptions.
Process descriptions were parsed with Stanford
constituency and dependency parsers (Klein and
Manning, 2003; de Marneffe et al, 2006), and 35
process descriptions were set aside as a test set
(number of training set trigger pairs: 1932, number
of test set trigger pairs: 906). We performed 10-
fold cross validation over the training set for feature
selection and tuning of constraint parameters. For
each constraint type (connectivity, chain-structure,
and five triad constraints) we introduced a param-
eter and tuned the seven parameters by coordinate-
wise ascent, where for hard constraints a binary pa-
rameter controls whether the constraint is used, and
for soft constraints we attempted 10 different re-
ward/penalty values. For our global model we de-
fined ?ijr = log pijr, where pijr is the probability at
edge (ti, tj) for label r, given by the pairwise clas-
sifier.
We test the following systems: (a) All-Prev: Since
the most common process structure was chain-like,
we simply predict PREV for every two adjacent trig-
gers in text. (b) Localbase: A pairwise classifier with
features from previous work (Section 3.1) (c) Local:
4www.gurobi.com
1716
Temporal Full
P R F1 P R F1
All-Prev 58.4 54.8 56.6 34.1 32.0 33.0
Localbase 61.5 51.8 56.2 52.1 43.9 47.6
Local 63.2 55.7? 59.2 54.7 48.3? 51.3
Chain 64.5 60.5?? 62.4? 56.1 52.6?? 54.3?
Global 63.9 61.4?? 62.6?? 56.2 54.0?? 55.0??
Table 4: Test set results on all experiments. Best number
in each column is bolded. ? and ? denote statistical signif-
icance (p < 0.01) against Localbase and Local baselines,
respectively.
A pairwise classifier with all features (Section 3.2)
(d) Chain: For every two adjacent triggers, choose
the non-NONE relation with highest probability ac-
cording to Local. This baseline heuristically com-
bines our structural assumptions with the pairwise
classifier. We deterministically choose a connected
chain structure, and then use the classifier to label
the edges. (e) Global: Our full model that uses ILP
inference.
To evaluate system performance we compare the
set of predictions on all trigger pairs to the gold stan-
dard annotations and compute micro-averaged pre-
cision, recall and F1. We perform two types of eval-
uations: (a) Full: evaluation on our full set of 11
relations (b) Temporal: Evaluation on temporal re-
lations only, by collapsing PREV, CAUSES, and EN-
ABLES to a single category and similarly for NEXT,
CAUSED, and ENABLED (inter-annotator agreement
? = 0.75). We computed statistical significance
of our results with the paired bootstrap resampling
method of 2000 iterations (Efron and Tibshirani,
1993), where the units resampled are trigger-trigger-
relation triples.
4.1 Results
Table 4 presents performance of all systems. We see
that using global constraints improves performance
almost invariably on all measures in both full and
temporal evaluations. Particularly, in the full eval-
uation Global improves recall by 12% and overall
F1 improves significantly by 3.7 points against Lo-
cal (p < 0.01). Recall improvement suggests that
modeling connectivity allowed Global to add cor-
rect relations in cases where some events were not
connected to one another.
The Local classifier substantially outperforms
Localbase. This indicates that our novel features
(Section 3.2) are important for discriminating be-
tween process relations. Specifically, in the full eval-
uation Local improves precision more than in the
temporal evaluation, suggesting that designing syn-
tactic and semantic features for connectives is useful
for distinguishing PREV, CAUSES, and ENABLES
when the amount of training data is small.
The Chain baseline performs only slightly worse
than our global model. This demonstrates the strong
tendency of processes to proceed linearly from one
event to the other, which is a known property of dis-
course structure (Schegloff and Sacks, 1973). How-
ever, since the structure is deterministically fixed,
Chain is highly inflexible and does not allow any
extensions or incorporation of other structural con-
straints or domain knowledge. Thus, it can be used
as a simple and efficient approximation but is not a
good candidate for a real system. Further support
for the linear nature of process structure is provided
by the All-Prev baseline, which performs poorly in
the full evaluation, but in temporal evaluation works
reasonably well.
Table 2 presents the degree distribution of Local
and Global on the development set comparing to the
gold standard. The degree distribution of Global is
more similar to the gold standard than Local. In par-
ticular, the connectivity constraint ensures that there
are no isolated nodes and shifts mass from nodes
with degree 0 and 1 to nodes with degree 2.
Table 5 presents the order in which constraints
were introduced into the global model using coor-
dinate ascent on the development set. Connectivity
is the first constraint to be introduced, and improves
performance considerably. The chain constraint, on
the other hand, is included third and the improve-
ment in F1 score is relatively smaller. This can be
explained by the distribution of degrees in Table 2
which shows that the predictions of Local does not
have many nodes with degree > 2. As for triad con-
straints, we see that four constraints are important
and are included in the model, but one is discarded.
Last, we examined the results of Global when
macro-averaging over processes, i.e., assigning each
process the same weight by computing recall, pre-
cision and F1 for each process and averaging those
scores. We found that results are quite similar
(with a slight improvement): in the full evalua-
1717
Order Parameter name Value (?) F1 score
? Local model ? 49.9
1 Connectivity constraint ? 51.2
2 SAME transitivity 0.5 52.9
3 Chain constraint -0.5 53.3
4 CAUSE-COTEMP 1.0 53.7
6 PREV contradiction ? 53.8
7 SAME contradiction ? 53.9
Table 5: Order by which constraint parameters were set
using coordinate ascent on the development set. For each
parameter, the value chosen and F1 score after including
the constraint are provided. Negative values correspond
to penalties, positive values to rewards, and a value of?
indicates a hard constraint.
tion Global obtains R/P/F1 of 56.4/55.0/55.7, and
in the temporal evaluation Global obtains R/P/F1 of
63.8/62.3/63.1.
4.2 Qualitative Analysis
Figure 3 shows two examples where global con-
straints corrected the predictions of Local. In Fig-
ure 3, left, Local failed to predict the causal rela-
tions skipped-deleted and used-duplicated, possibly
because they are not in the same sentence and are not
adjacent to one another. By enforcing the connectiv-
ity constraint, Global correctly adds the correct re-
lations and connects deleted and duplicated to the
other triggers in the process.
In Figure 3, right, Local predicts a structure that
results in a ?SAME contradiction? structure. The
triggers bind and binds cannot denote the same event
if a third trigger secrete is temporally between them.
However, Local predicts they are the same event, as
they share a lemma. Global prohibits this structure
and correctly predicts the relation as NONE.
To better understand the performance of Local,
we analyzed the confusion matrix generated based
on its predictions. Although this is a challenging
11-class classification task, most of the mass is con-
centrated on the matrix diagonal, as desired. Error
analysis reveals that 17.5% of all errors are con-
fusions between NONE and PREV, 11.1% between
PREV and CAUSES, and 8.6% between PREV and
COTEMP. This demonstrates that distinguishing the
classes PREV, CAUSES and COTEMP is challenging
for Local. Our current global constraints do not ad-
dress this type of error, and thus an important direc-
tion for future work is to improve the local model.
The global model depends on the predictions of
the local classifier, and so enforcing global con-
straints does not guarantee improvement in perfor-
mance. For instance, if Local produces a graph that
is disconnected (e.g., deleted in Figure 3, left), then
Global will add an edge. However, the label of the
edge is determined by scores computed based on
the local classifier, and if this prediction is wrong,
we will now be penalized for both the false nega-
tive of the correct class (just as before), and also for
the false positive of the predicted class. Despite that
we see that Global improves overall performance by
3.7 F1 points on the test set.
5 Related Work
A related line of work is biomedical event extrac-
tion in recent BioNLP shared tasks (Kim et al,
2009; Kim et al, 2011). Earlier work employed a
pipeline architecture where first events are found,
and then their arguments are identified (Miwa et al,
2010; Bjo?rne et al, 2011). Subsequent methods pre-
dicted events and arguments jointly using Markov
logic (Poon and Vanderwende, 2010) and depen-
dency parsing algorithms (McClosky et al, 2011).
Riedel and McCallum (2011) further improved per-
formance by capturing correlations between events
and enforcing consistency across arguments.
Temporal event-event relations have been ex-
tensively studied (Chambers and Jurafsky, 2008;
Yoshikawa et al, 2009; Denis and Muller, 2011;
Do et al, 2012; McClosky and Manning, 2012;
D?Souza and Ng, 2013), and we leverage such
techniques in our work (Section 3.1). However,
we extend beyond temporal relations alone, and
strongly rely on dependencies between process
events. Chambers and Jurafsky (2011) learned event
templates (or frames), where events that are related
to one another and their semantic roles are extracted.
Recently, Cheung et al (2013) proposed an unsuper-
vised generative model for inducing such templates.
A major difference in our work is that we do not
learn typical event relations from a large and redun-
dant corpus, but are given a paragraph and have a
?one-shot? chance to extract the process structure.
We showed in this paper that global structural
properties lead to significant improvements in ex-
traction accuracy, and ILP is an effective framework
1718
shifts
skippedCAUSESCAUSES
usedCAUSESCAUSES
deletedCAUSESCAUSES
duplicatedCAUSESCAUSES bind
secreteCOTEMPPREV bindsSAMENONE
PREVENABLES
Figure 3: Process graph fragments. Black edges (dotted) are predictions of Local, green (solid) are predictions of
Global, and gold (dashed) are gold standard edges. To reduce clutter, we present the predictions of Global only when
it disagrees with Local. In all other cases, the predictions of Global and Local are identical. Original text, Left: ?... the
template shifts . . . , and a part of the template strand is either skipped by the replication machinery or used twice as a
template. As a result, a segment of DNA is deleted or duplicated.? Right: ?Cells of mating type A secrete a signaling
molecule, which can bind to specific receptor proteins on nearby cells. At the same time, cells secrete factor, which
binds to receptors on A cells.?
for modeling global constraints. Similar observa-
tions and techniques have been proposed in other
information extraction tasks. Reichart and Barzi-
lay (2012) tied information from multiple sequence
models that describe the same event by using global
higher-order potentials. Berant et al (2011) pro-
posed a global inference algorithm to identify entail-
ment relations. There is an abundance of examples
of enforcing global constraints in other NLP tasks,
such as in coreference resolution (Finkel and Man-
ning, 2008), parsing (Rush et al, 2012) and named
entity recognition (Wang et al, 2013).
6 Conclusion
Developing systems that understand process de-
scriptions is an important step towards building ap-
plications that require deeper reasoning, such as bi-
ological process models from text, intelligent tutor-
ing systems, and non-factoid QA systems. In this
paper we have presented the task of process extrac-
tion, and developed methods for extracting relations
between process events. Processes contain events
that are tightly coupled through strong dependen-
cies. We have shown that exploiting these structural
dependencies and performing joint inference over all
event mentions can significantly improve accuracy
over several baselines. We have also released a new
dataset containing 148 fully annotated descriptions
of biological processes. Though the models we built
were trained on biological processes, they do not en-
code domain specific information, and hence should
be extensible to other domains.
In this paper we assumed that event triggers are
given as input. In future work, we want to perform
trigger identification jointly with extraction of event-
event relations. As explained in Section 4.2, the
performance of our system is confined by the per-
formance of the local classifier, which is trained on
relatively small amounts of data. Since data annota-
tion is expensive, it is important to improve the lo-
cal classifier without increasing the annotation bur-
den. For example, one can use unsupervised meth-
ods that learn narrative chains (Chambers and Ju-
rafsky, 2011) to provide some prior on the typical
order of events. Alternatively, we can search on the
web for redundant descriptions of the same process
and use this redundancy to improve classification.
Last, we would like to integrate our method into QA
systems and allow non-factoid questions that require
deeper reasoning to be answered by matching the
questions against the learned process structures.
Acknowledgments
The authors would like to thank Roi Reichart for
fruitful discussion and the anonymous reviewers for
their constructive feedback. This work was partially
funded by Vulcan Inc. The second author was spon-
sored by a Rothschild fellowship.
References
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Commun. ACM, 26(11):832?843.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Learning entailment relations by global graph
structure optimization. Journal of Computational Lin-
guistics, 38(1).
1719
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2011. Extract-
ing contextualized complex biological events with rich
graph-based feature sets. Computational Intelligence,
27(4):541?557.
Neil Campbell and Jane Reece. 2005. Biology. Ben-
jamin Cummings.
Nathanael Chambers and Daniel Jurafsky. 2008. Jointly
combining implicit constraints improves temporal or-
dering. In Proceedings of EMNLP.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
ACL, pages 976?986.
Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
Proceedings of NAACL-HLT.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC.
Pascal Denis and Philippe Muller. 2011. Predicting
globally-coherent temporal structures from texts via
endpoint inference and graph decomposition. In Pro-
ceedings of IJCAI.
Quang Do, Wei Lu, and Dan Roth. 2012. Joint infer-
ence for event timeline construction. In Proceedings
of EMNLP-CoNLL.
Jennifer D?Souza and Vincent Ng. 2013. Classifying
temporal relations with rich linguistic knowledge. In
Proceedings of NAACL-HLT.
Bradley Efron and Robert Tibshirani. 1993. An introduc-
tion to the bootstrap, volume 57. CRC press.
Christiane Fellbaum, editor. 1998. WordNet: An elec-
tronic lexical database. MIT Press.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of ACL.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Junichi Tsujii. 2009. Overview of
BioNLP 09 shared task on event extraction. In Pro-
ceedings of BioNLP.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Junichi Tsujii. 2011. Overview of BioNLP
shared task 2011. In Proceedings of BioNLP.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL.
Andre? L. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
ACL/IJCNLP.
David McClosky and Christopher D. Manning. 2012.
Learning constraints for consistent timeline extraction.
In Proceedings of EMNLP-CoNLL, pages 873?882.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011. Event extraction as dependency pars-
ing. In Proceedings of ACL, pages 1626?1635.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010. Event extraction with complex event
classification using rich features. J. Bioinformatics
and Computational Biology, 8(1).
Hoifung Poon and Lucy Vanderwende. 2010. Joint in-
ference for knowledge extraction from biomedical lit-
erature. In Proceedings of HLT-NAACL.
James Pustejovsky, Jose? M. Castan?o, Robert Ingria,
Roser Sauri, Robert J. Gaizauskas, Andrea Setzer,
Graham Katz, and Dragomir R. Radev. 2003.
TimeML: Robust specification of event and temporal
expressions in text. In New Directions in Question An-
swering.
Roi Reichart and Regina Barzilay. 2012. Multi-event ex-
traction guided by global constraints. In Proceedings
of HLT-NAACL.
Sebastian Riedel and Andrew McCallum. 2011. Fast and
robust joint models for biomedical event extraction. In
Proceedings of EMNLP.
Alexander M. Rush, Roi Reichert, Michael Collins, and
Amir Globerson. 2012. Improved parsing and POS
tagging using inter-sentence consistency constraints.
In Proceedings of EMNLP.
Emanuel A Schegloff and Harvey Sacks. 1973. Opening
up closings. Semiotica, 8(4):289?327.
David Sontag, Amir Globerson, and Tommi Jaakkola.
2011. Introduction to dual decomposition for in-
ference. In Suvrit Sra, Sebastian Nowozin, and
Stephen J. Wright, editors, Optimization for Machine
Learning. MIT Press.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics, 37(2).
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2):161?
191.
Mengqiu Wang, Wanxiang Che, and Christopher D. Man-
ning. 2013. Effective bilingual constraints for semi-
supervised learning of named entity recognizers. In
Proceedings of AAAI.
Ming wei Chang, Lev Ratinov, and Dan Roth. 2012.
Structured learning with constrained conditional mod-
els. Machine Learning, 88(3):399?431, 6.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with Markov logic. In Proceedings
of ACL/IJCNLP.
1720
Proceedings of NAACL-HLT 2013, pages 52?62,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Named Entity Recognition with Bilingual Constraints
Wanxiang Che? Mengqiu Wang? Christopher D. Manning? Ting Liu?
?{car, tliu}@ir.hit.edu.cn
School of Computer Science and Technology
Harbin Institute of Technology
Harbin, China, 150001
?{mengqiu, manning}@stanford.edu
Computer Science Department
Stanford University
Stanford, CA, 94305
Abstract
Different languages contain complementary
cues about entities, which can be used to im-
prove Named Entity Recognition (NER) sys-
tems. We propose a method that formu-
lates the problem of exploring such signals on
unannotated bilingual text as a simple Inte-
ger Linear Program, which encourages entity
tags to agree via bilingual constraints. Bilin-
gual NER experiments on the large OntoNotes
4.0 Chinese-English corpus show that the pro-
posed method can improve strong baselines
for both Chinese and English. In particular,
Chinese performance improves by over 5%
absolute F1 score. We can then annotate a
large amount of bilingual text (80k sentence
pairs) using our method, and add it as up-
training data to the original monolingual NER
training corpus. The Chinese model retrained
on this new combined dataset outperforms the
strong baseline by over 3% F1 score.
1 Introduction
Named Entity Recognition (NER) is an important
task for many applications, such as information ex-
traction and machine translation. State-of-the-art su-
pervised NER methods require large amounts of an-
notated data, which are difficult and expensive to
produce manually, especially for resource-poor lan-
guages.
A promising approach for improving NER per-
formance without annotating more data is to exploit
unannotated bilingual text (bitext), which are rela-
tively easy to obtain for many language pairs, bor-
rowing from the resources made available by statis-
tical machine translation research.1 Different lan-
guages contain complementary cues about entities.
For example, in Figure 1, the word ?? (Ben)? is
common in Chinese but rarely appears as a trans-
lated foreign name. However, its aligned word on
the English side (?Ben?) provides a strong clue that
this is a person name. Judicious use of this type of
bilingual cues can help to recognize errors a mono-
lingual tagger would make, allowing us to produce
more accurately tagged bitext. Each side of the
tagged bitext can then be used to expand the orig-
inal monolingual training dataset, which may lead
to higher accuracy in the monolingual taggers.
Previous work such as Li et al (2012) and Kim
et al (2012) demonstrated that bilingual corpus an-
notated with NER labels can be used to improve
monolingual tagger performance. But a major draw-
back of their approaches are the need for manual
annotation efforts to create such corpora. To avoid
this requirement, Burkett et al (2010) suggested a
?multi-view? learning scheme based on re-ranking.
Noisy output of a ?strong? tagger is used as training
data to learn parameters of a log-linear re-ranking
model with additional bilingual features, simulated
by a ?weak? tagger. The learned parameters are then
reused with the ?strong? tagger to re-rank its own
outputs for unseen inputs. Designing good ?weak?
taggers so that they complement the ?view? of bilin-
gual features in the log-linear re-ranker is crucial to
the success of this algorithm. Unfortunately there is
no principled way of designing such ?weak? taggers.
In this paper, we would like to explore a conceptu-
ally much simpler idea that can also take advantage
1opus.lingfil.uu.se
52
TheO chairmanO ofO theB?ORG FederalI?ORG ReserveI?ORG isO BenB?PER BernankeI?PER
???B?ORG ??O ?O ?B?PER ???I?PER
Figure 1: Example of NER labels between two word-aligned bilingual parallel sentences.
of the large amount of unannotated bitext, without
complicated machinery. More specifically, we in-
troduce a joint inference method that formulates the
bilingual NER tagging problem as an Integer Linear
Program (ILP) and solves it during decoding. We
propose a set of intuitive and effective bilingual con-
straints that encourage NER results to agree across
the two languages.
Experimental results on the OntoNotes 4.0 named
entity annotated Chinese-English parallel corpus
show that the proposed method can improve the
strong Chinese NER baseline by over 5% F1 score
and also give small improvements over the English
baseline. Moreover, by adding the automatically
tagged data to the original NER training corpus
and retraining the monolingual model using an up-
training regimen (Petrov et al, 2010), we can im-
prove the monolingual Chinese NER performance
by over 3% F1 score.
2 Constraint-based Monolingual NER
NER is a sequence labeling task where we assign
a named entity tag to each word in an input sen-
tence. One commonly used tagging scheme is the
BIO scheme. The tag B-X (Begin) represents the
first word of a named entity of type X, for example,
PER (Person) or LOC (Location). The tag I-X (In-
side) indicates that a word is part of an entity but not
first word. The tag O (Outside) is used for all non-
entity words.2 See Figure 1 for an example tagged
sentence.
Conditional Random Fields (CRF) (Lafferty et al,
2001) is a state-of-the-art sequence labeling model
widely used in NER. A first-order linear-chain CRF
2While the performance of NER is measured at the entity
level (not the tag level).
defines the following conditional probability:
PCRF (y|x) =
1
Z(x)
?
i
Mi(yi, yi?1|x) (1)
where x and y are the input and output sequences,
respectively, Z(x) is the partition function, and Mi
is the clique potential for edge clique i. Decoding
in CRF involves finding the most likely output se-
quence that maximizes this objective, and is com-
monly done by the Viterbi algorithm.
Roth and Yih (2005) proposed an ILP inference
algorithm, which can capture more task-specific and
global constraints than the vanilla Viterbi algorithm.
Our work is inspired by Roth and Yih (2005). But
instead of directly solving the shortest-path problem
in the ILP formulation, we re-define the conditional
probability as:
PMAR(y|x) =
?
i
P (yi|x) (2)
where P (yi|x) is the marginal probability given by
an underlying CRF model computed using forward-
backward inference. Since the early HMM litera-
ture, it has been well known that using the marginal
distributions at each position works well, as opposed
to Viterbi MAP sequence labeling (Me?rialdo, 1994).
Our experimental results also supports this claim, as
we will show in Section 6. Our objective is to find
an optimal NER tag sequence:
y? = argmax
y
PMAR(y|x)
= argmax
y
?
i
logP (yi|x) (3)
Then an ILP can be used to solve the inference
problem as classification problem with constraints.
53
The objective function is:
max
|x|?
i=1
?
y?Y
zyi logP
y
i (4)
where Y is the set of all possible named entity tags.
P yi = P (yi = y|x) is the CRF marginal probabil-
ity that the ith word is tagged with y, and zyi is an
indicator that equals 1 iff the ith word is tagged y;
otherwise, zyi is 0.
If no constraints are identified, then Eq. (4)
achieves maximum when all zyi are assigned to 1,
which violates the condition that each word should
only be assigned a single entity tag. We can express
this with constraints:
?i :
?
y?Y
zyi = 1 (5)
After adding the constraints, the probability of the
sequence is maximized when each word is assigned
the tag with highest probability. However, some in-
valid results may still exist. For example a tag O
may be wrongly followed by a tag I-X, although a
named entity cannot start with I-X. Therefore, we
can add the following constraints:
?i,?X : zB-Xi?1 + zI-Xi?1 ? zI-Xi ? 0 (6)
which specifies that when the ith word is tagged with
I-X (zI-Xi = 1), then the previous word can only be
tagged with B-X or I-X (zB-Xi?1 + zI-Xi?1 ? 1).
3 NER with Bilingual Constraints
This section demonstrates how to jointly perform
NER for two languages with bilingual constraints.
We assume sentences have been aligned into pairs,
and the word alignment between each pair of sen-
tences is also given.
3.1 Hard Bilingual Constraints
We first introduce the simplest hard constraints, i.e.,
each word alignment pair should have the same
named entity tag. For example, in Figure 1, the
Chinese word ????? was aligned with the En-
glish words ?the?, ?Federal? and ?Reserve?. There-
fore, they have the same named entity tags ORG.3
3The prefix B- and I- are ignored.
Similarly, ??? and ?Ben? as well as ????? and
?Bernanke? were all tagged with the tag PER.
The objective function for bilingual NER can be
expressed as follows:
max
|xc|?
i=1
?
y?Y
zyi logP
y
i +
|xe|?
j=1
?
y?Y
zyj logP
y
j (7)
where P yi and P
y
j are the probabilities of the i
th Chi-
nese word and jth English word to be tagged with y,
respectively. xc and xe are respectively the Chinese
and English sentences.
Similar to monolingual constrained NER (Sec-
tion 2), monolingual constraints are added for each
language as shown in Eqs. (8) and (9):
?i :
?
y?Y
zyi = 1;?j :
?
y?Y
zyj = 1 (8)
?i,?X : zB-Xi + zI-Xi ? zB-Xi+1 ? 0 (9)
?j,?X : zB-Xj + zI-Xj ? zB-Xj+1 ? 0
Bilingual constraints are added in Eq. (10):
?(i, j) ? A,?X : zB-Xi + zI-Xi = zB-Xj + zI-Xj (10)
where A = {(i, j)} is the word alignment pair set,
i.e., the ith Chinese word and the jth English word
were aligned together. Chinese word i is tagged with
a named entity type X (zB-Xi + zI-Xi = 1), iff English
word j is tagged with X (zB-Xj +zI-Xj = 1). Therefore,
these hard bilingual constraints guarantee that when
two words are aligned, they are tagged with the same
named entity tag.
However, in practice, aligned word pairs do not
always have the same tag because of the difference
in annotation standards across different languages.
For example, in Figure 2(a), the Chinese word ???
?? is a location. However, it is aligned to the words,
?development? and ?zone?, which are not named en-
tities in English. Word alignment error is another se-
rious problem that can cause violation of hard con-
straints. In Figure 2(b), the English word ?Agency?
is wrongly aligned with the Chinese word ?? (re-
port)?. Thus, these two words cannot be assigned
with the same tag.
To address these two problems, we present a prob-
abilistic model for bilingual NER which can lead to
54
ThisO developmentO zoneO isO locatedO inO . . .
??O ???B?LOC ?O ?O . . .
(a) Inconsistent named entity standards
XinhuaB?ORG NewsI?ORG AgencyI?ORG FebruaryO 16thO
???B?ORG ??B?LOC ???O ?O
(b) Word alignment error
Figure 2: Errors of hard bilingual constraints method.
an optimization problem with two soft bilingual con-
straints:
1) allow word-aligned pairs to have different
named entity tags; 2) consider word alignment prob-
abilities to reduce the influence of wrong word align-
ments.
3.2 Soft Constraints with Tag Uncertainty
The new probabilistic model for bilingual NER is:
P (yc,ye|xc,xe, A) =
P (yc,ye,xc,xe, A)
P (xc,xe, A)
= P (yc,xc,xe, A)
P (xc,xe, A)
? P (ye,xc,xe, A)
P (xc,xe, A)
? P (yc,ye,xc,xe, A)P (xc,xe, A)
P (yc,xc,xe, A)P (ye,xc,xe, A)
(11)
? P (yc|xc)P (ye|xe)
P (yc,ye|A)
P (yc|A)P (ye|A)
(12)
where yc and ye respectively denotes Chinese and
English named entity output sequences. A is the set
of word alignment pairs.
If we assume that named entity tag assignments in
Chinese is only dependent on the observed Chinese
sentence, then we can drop the A and xe term in the
first factor of Eq. (11), and arrive at the first factor of
Eq. (12); similarly we can use the same assumption
to derive the second factor in Eq. (12) for English;
alternatively, if we assume the named entity tag as-
signments are only dependent on the cross-lingual
word associations via word alignment, then we can
drop xc and xe terms in the third factor of Eq. (11)
and arrive at the third factor of Eq. (12). These fac-
tors represent the two major sources of information
in the model: monolingual surface observation, and
cross-lingual word associations.
The first two factors of Eq. (12) can be further
decomposed into the product of probabilities of all
words in each language sentence like Eq. (2).
Assuming that the tags are independent between
different word alignment pairs, then the last factor
of Eq. (12) can be decomposed into:
P (yc,ye|A)
P (yc|A)P (ye|A)
=
?
a?A
P (ycayea)
P (yca)P (yea)
=
?
a?A
?ycyea (13)
where yca and yea respectively denotes Chinese and
English named entity tags in a word alignment pair
a. ?ycye = P (ycye)P (yc)P (ye) is the pointwise mutual infor-
mation (PMI) score between a Chinese named en-
tity tag yc and an English named entity tag ye. If
yc = ye, then the score will be high; otherwise the
score will be low. A number of methods for calculat-
ing the scores are provided at the end of this section.
We use ILP to maximize Eq. (12). The new ob-
jective function is expressed as follow:
max
|xc|?
i=1
?
y?Y
zyi logP
y
i +
|xe|?
j=1
?
y?Y
zyj logP
y
j
+
?
a?A
?
yc?Y
?
ye?Y
zycyea log ?ycyea (14)
where zycyea is an indicator that equals 1 iff the Chi-
nese and English named entity tags are yc and ye
respectively, given a word alignment pair a; other-
wise, zycyea is 0.
Monolingual constraints such as Eqs. (8) and (9)
need to be added. In addition, one and only one pos-
sible named entity tag pair exists for a word align-
ment pair. This condition can be expressed as the
following constraints:
?a ? A :
?
yc?Y
?
ye?Y
zycyea = 1 (15)
When the tag pair of a word alignment pair is de-
termined, the corresponding monolingual named en-
55
tity tags can also be identified. This rule can be ex-
pressed by the following constraints:
?a = (i, j) ? A : zycyea ? zyci , zycyea ? z
ye
j (16)
Thus, if zycyea = 1, then zyci and z
ye
j must be both
equal to 1. Here, the ith Chinese word and the jth
English word are aligned together.
In contrast to hard bilingual constraints, inconsis-
tent named entity tags for an aligned word pair are
allowed in soft bilingual constraints, but are given
lower ?ycye scores.
To calculate the ?ycye score, an annotated bilin-
gual NER corpus is consulted. We count from all
word alignment pairs the number of times yc and ye
occur together (C(ycye)) and separately (C(yc) and
C(ye)). Afterwards, ?ycye is calculated with maxi-
mum likelihood estimation as follows:
?ycye = P (ycye)
P (yc)P (ye)
= N ? C(ycye)
C(yc)C(ye)
(17)
where N is the total number of word alignment
pairs.
However, in this paper, we assume that no named
entity annotated bilingual corpus is available. Thus,
the above method is only used as Oracle. A real-
istic method for calculating the ?ycye score requires
the use of two initial monolingual NER models, such
as baseline CRF, to predict named entity tags for
each language on an unannotated bitext. We count
from this automatically tagged corpus the statistics
mentioned above. This method is henceforth re-
ferred to as Auto.
A simpler approach is to manually set the value
of ?ycye : if yc = ye then we assign a larger value
to ?ycye ; else we assign an ad-hoc smaller value. In
fact, if we set ?ycye = 1 iff yc = ye; otherwise,
?ycye = 0, then the soft constraints backs off to hard
constraints. We refer to this set of soft constraints as
Soft-tag.
3.3 Constraints with Alignment Uncertainty
So far, we assumed that a word alignment set A is
known. In practice, only the word alignment proba-
bility Pa for each word pair a is provided. We can
set a threshold ? for Pa to tune the set A: a ? A
iff Pa ? ?. This condition can be regarded as a
kind of hard word alignment. However, the follow-
ing problem exists: the smaller the ?, the noisier the
word alignments are; the larger the ?, the more pos-
sible word alignments are lost. To ameliorate this
problem, we introduce another set of soft bilingual
constraints.
We can re-express Eq. (13) as follows:
?
a?A
?ycyea =
?
a?A
(?ycyea )Ia (18)
where A is the set of all word pairs between two
languages. Ia = 1 iff Pa ? ?; otherwise, Ia = 0.
We can then replace the hard indicator Ia with
the word alignment probability Pa, Eq. (14) is then
transformed into the following equation:
max
|Wc|?
i
?
y?Y
zyi logP
y
i +
|We|?
j
?
y?Y
zyj logP
y
j
+
?
a?A
?
yc?Y
?
ye?Y
zycyea Pa log ?ycyea (19)
We name the set of constraints above
Soft-align, which has the same constraints
as Soft-tag, i.e., Eqs. (8), (9), (15) and (16).
4 Experimental Setup
We conduct experiments on the latest OntoNotes
4.0 corpus (LDC2011T03). OntoNotes is a large,
manually annotated corpus that contains various text
genres and annotations, such as part-of-speech tags,
named entity labels, syntactic parse trees, predicate-
argument structures and co-references (Hovy et al,
2006). Aside from English, this corpus also con-
tains several Chinese and Arabic corpora. Some of
these corpora contain bilingual parallel documents.
We used the Chinese-English parallel corpus with
named entity labels as our development and test
data. This corpus includes about 400 document pairs
(chtb 0001-0325, ectb 1001-1078). We used odd-
numbered documents as development data and even-
numbered documents as test data. We used all other
portions of the named entity annotated corpus as
training data for the monolingual systems. There
were a total of?660 Chinese documents (?16k sen-
tences) and ?1,400 English documents (?39k sen-
tences). OntoNotes annotates 18 named entity types,
such as person, location, date and money. In this
paper, we selected the four most common named
entity types, i.e., PER (Person), LOC (Location),
56
Chinese NER Templates
00: 1 (class bias param)
01: wi+k,?1 ? k ? 1
02: wi+k?1 ? wi+k, 0 ? k ? 1
03: shape(wi+k),?4 ? k ? 4
04: prefix(wi, k), 1 ? k ? 4
05: prefix(wi?1, k), 1 ? k ? 4
06: suffix(wi, k), 1 ? k ? 4
07: suffix(wi?1, k), 1 ? k ? 4
08: radical(wi, k), 1 ? k ? len(wi)
Unigram Features
yi? 00 ? 08
Bigram Features
yi?1 ? yi? 00 ? 08
Table 1: Basic features of Chinese NER.
ORG (Organization) and GPE (Geo-Political Enti-
ties), and discarded the others.
Since the bilingual corpus is only aligned at the
document level, we performed sentence alignment
using the Champollion Tool Kit (CTK).4 After re-
moving sentences with no aligned sentence, a total
of 8,249 sentence pairs were retained.
We used the BerkeleyAligner,5 to produce
word alignments over the sentence-aligned datasets.
BerkeleyAligner also gives posterior probabilities
Pa for each aligned word pair.
We used the CRF-based Stanford NER tagger (us-
ing Viterbi decoding) as our baseline monolingual
NER tool.6 English features were taken from Finkel
et al (2005). Table 1 lists the basic features of
Chinese NER, where ? means string concatenation
and yi is the named entity tag of the ith word wi.
Moreover, shape(wi) is the shape of wi, such as
date and number. prefix/suffix(wi, k) denotes the
k-characters prefix/suffix of wi. radical(wi, k) de-
notes the radical of the kth Chinese character of wi.7
len(wi) is the number of Chinese characters in wi.
To make the baseline CRF taggers stronger, we
added word clustering features to improve gener-
alization over unseen data for both Chinese and
English. Word clustering features have been suc-
cessfully used in several English tasks, including
4champollion.sourceforge.net
5code.google.com/p/berkeleyaligner
6nlp.stanford.edu/software/CRF-NER.shtml,
which has included our English and Chinese NER implementations.
7The radical of a Chinese character can be found at: www.
unicode.org/charts/unihan.html
NER (Miller et al, 2004) and dependency pars-
ing (Koo et al, 2008). To our knowledge, this work
is the first use of word clustering features for Chi-
nese NER. A C++ implementation of the Brown
word clustering algorithms (Brown et al, 1992) was
used to obtain the word clusters (Liang, 2005).8
Raw text was obtained from the fifth edition of Chi-
nese Gigaword (LDC2011T13). One million para-
graphs from Xinhua news section were randomly
selected, and the Stanford Word Segmenter with
LDC standard was applied to segment Chinese text
into words.9 About 46 million words were obtained
which were clustered into 1,000 word classes.
5 Threshold Tuning
During development, we tuned the word alignment
probability thresholds to find the best value. Figure 3
shows the performance curves.
When the word alignment probability threshold ?
is set to 0.9, the hard bilingual constraints perform
well for both Chinese and English. But as the thresh-
olds value gets smaller, and more noisy word align-
ments are introduced, we see the hard bilingual con-
straints method starts to perform badly.
In Soft-tag setting, where inconsistent tag as-
signments within aligned word pairs are allowed but
penalized, different languages have different optimal
threshold values. For example, Chinese has an opti-
mal threshold of 0.7, whereas English has 0.2. Thus,
the optimal thresholds for different languages need
to be selected with care when Soft-tag is applied
in practice.
Soft-align eliminates the need for careful
tuning of word alignment thresholds, and therefore
can be more easily used in practice. Experimen-
tal results of Soft-align confirms our hypothe-
sis ? the performance of both Chinese and English
NER systems improves with decreasing threshold.
However, we can still improve efficiency by set-
ting a low threshold to prune away very unlikely
word alignments. We set the threshold to 0.1 for
Soft-align to increase speed, and we observed
very minimal performance lost when doing so.
We also found that automatically estimated bilin-
gual tag PMI scores (Auto) gave comparable results
8github.com/percyliang/brown-cluster
9nlp.stanford.edu/software/segmenter.shtml
57
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9threshold of word alignment probability
55
60
65
70
75
per
form
anc
e (F
1)
HardSoft-label (Oracle)Soft-label (Auto)Soft-align (Oracle)Soft-align (Auto)
(a) Chinese
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9threshold of word alignment probability
60
65
70
75
80
per
form
anc
e (F
1)
HardSoft-label (Oracle)Soft-label (Auto)Soft-align (Oracle)Soft-align (Auto)
(b) English
Figure 3: Performance curves of different bilingual constraints methods on development set.
to Oracle. Therefore this technique is effective
for computing the PMI scores, avoiding the need of
manually annotating named entity bilingual corpus.
6 Bilingual NER Results
The main results on Chinese and English test sets
with the optimal word alignment threshold for each
method are shown in Table 2.
The CRF-based Chinese NER with and without
word clustering features are compared here. The
word clustering features significantly (p < 0.01) im-
proved the performance of Chinese NER, 10 giving
us a strong Chinese NER baseline.11 The effective-
ness of word clustering for English NER has been
proved in previous work.
The performance of ILP with only monolingual
constraints is quite comparable with the CRF re-
sults, especially on English. The greater ILP perfor-
mance on English is probably due to more accurate
marginal probabilities estimated by the English CRF
model.
The ILP model with hard bilingual constraints
gives a slight performance improvement on Chi-
nese, but affects performance negatively on English.
Once we introduced tagging uncertainties into the
Soft-tag bilingual constraints, we see a very sig-
10We use paired bootstrap resampling significance test (Efron
and Tibshirani, 1993).
11To the best of our knowledge, there was no performance
report of state-of-the-art NER results on the latest OntoNotes
dataset.
nificant (p < 0.01) performance boost on Chinese.
This method also improves the recall on English,
with a smaller decrease in precision. Overall, it im-
proves English F1 score by about 0.4%, which is un-
fortunately not statistically significant.
Compared with Soft-tag, the final
Soft-align method can further improve
performance on both Chinese and English. This is
likely to be because: 1) Soft-align includes
more word alignment pairs, thereby improving
recall; and 2) uses probabilities to cut wrong
word alignments, thereby improving precision. In
particular, compared with the strong CRF baseline,
the gain on Chinese side is almost 5.5% in absolute
F1 score.
Decoding/inferenc efficiency of different methods
are shown in the last column of Table 2.12 Com-
pared with Viterbi decoding in CRF, monolingual
ILP decoding is about 2.3 times slower. Bilingual
ILP decoding, with either hard or soft constraints, is
significantly slower than the monolingual methods.
The reason is that the number of monolingual ILP
constraints doubles, and there are additionally many
more bilingual constraints. The difference in speed
between the Soft-tag and Soft-align meth-
ods is attributed to the difference in number of word
alignment pairs.
Since each sentence pair can be decoded indepen-
12CPU: Intel Xeon E5-2660 2.20GHz. And the speed cal-
culation of ILP inference methods exclude the time needed to
obtain marginal probabilities from the CRF models.
58
Chinese English Speed
P R F1 P R F1 #sent/s
CRF (No Cluster) 74.74 56.17 64.13 ? ? ? ?
CRF (Word Cluster) 76.90 63.32 69.45 82.95 76.67 79.68 317.3
Monolingual ILP 76.20 63.06 69.01 82.88 76.68 79.66 138.0
Hard 74.38 65.78 69.82 82.66 75.36 78.84 21.1
Soft-tag (Auto) 77.37 71.14 74.13 81.36 78.74 80.03 5.9
Soft-align (Auto) 77.71 72.51 75.02 81.94 78.35 80.10 1.5
Table 2: Results on bilingual parallel test set.
dently, parallelization the decoding process can re-
sult in significant speedup.
7 Semi-supervised NER Results
The above results show the usefulness of our method
in a bilingual setting, where we are presented with
sentence aligned data, and are tagging both lan-
guages at the same time. To have a greater impact
on general monolingual NER systems, we employ
a semi-supervised learning setting. First, we tag a
large amount of unannotated bitext with our bilin-
gual constraint-based NER tagger. Then we mix the
automatically tagged results with the original mono-
lingual Chinese training data to train a new model.
Our bitext is derived from the Chinese-English
part of the Foreign Broadcast Information Service
corpus (FBIS, LDC2003E14). The best perform-
ing bilingual model Soft-align with threshold
? = 0.1 was used under the same experimental set-
ting as described in Section 4
Method #sent P R F1
CRF ?16k 76.90 63.32 69.45
Semi
10k 77.60 66.51 71.62
20k 77.28 67.26 71.92
40k 77.40 67.81 72.29
80k 77.44 68.64 72.77
Table 3: Semi-supervised results on Chinese test set.
Table 3 shows that the performance of the semi-
supervised method improves with more additional
data. We simply appended these data to the orig-
inal training data. We also have done the experi-
ments to down weight the additional training data
by duplicating the original training data. There
was some slight improvements, but not very signif-
icant. Finally, when we add 80k sentences, the F1
score is improved by 3.32%, which is significantly
(p < 0.01) better than the baseline, and most of the
contribution comes from recall improvement.
Before the end of experimental section, let us
summarize the usage of different kinds of data re-
sources used in our experiments, as shown in Ta-
ble 4, where  and ? denote whether the corre-
sponding resources are required. In the bilingual
case, during training, only the monolingual named
entity annotated data (NE-mono) is necessary to
train a monolingual NER tagger. During the test,
unannotated bitext (Bitext) is required by the word
aligner and our bilingual NER tagger. Named entity
annotated bitext (NE-bitext) is used to evaluate our
bilingual model. In the semi-supervised case, be-
sides the original NE-mono data, the Bitext is used
as input to our bilingual NER tagger to product ad-
ditional training data. To evaluate the final NER
model, only NE-mono is needed.
NE-mono Bitext NE-bitext
Bilingual
train  ? ?
test ?  
Semi
train   ?
test  ? ?
Table 4: Summarization of the data resource usage
8 Related Work
Previous work explored the use of bilingual corpora
to improve existing monolingual analyzers. Huang
et al (2009) proposed methods to improve parsing
performance using bilingual parallel corpus. Li et
al. (2012) jointly labeled bilingual named entities
with a cyclic CRF model, where approximate in-
ference was done using loopy belief propagation.
These methods require manually annotated bilingual
59
corpora, which are expensive to construct, and hard
to obtain. Kim et al (2012) proposed a method of
labeling bilingual corpora with named entity labels
automatically based on Wikipedia. However, this
method is restricted to topics covered by Wikipedia.
Similar to our work, Burkett et al (2010) also as-
sumed that annotated bilingual corpora are scarce.
Beyond the difference discussed in Section 1, their
re-ranking strategy may lose the correct named en-
tity results if they are not included in the top-N out-
puts. Furthermore, we consider the word alignment
probabilities in our method which can reduce the in-
fluence of word alignment errors. Finally, we test
our method on a large standard publicly available
corpus (8,249 sentences), while they used a much
smaller (200 sentences) manually annotated bilin-
gual NER corpus for results validation.
In addition to bilingual corpora, bilingual dictio-
naries are also useful resources. Huang and Vo-
gel (2002) and Chen et al (2010) proposed ap-
proaches for extracting bilingual named entity pairs
from unannotated bitext, in which verification is
based on bilingual named entity dictionaries. How-
ever, large-scale bilingual named entity dictionaries
are difficult to obtain for most language pairs.
Yarowsky and Ngai (2001) proposed a projection
method that transforms high-quality analysis results
of one language, such as English, into other lan-
guages on the basis of word alignment. Das and
Petrov (2011) applied the above idea to part-of-
speech tagging with a more complex model. Fu et al
(2011) projected English named entities onto Chi-
nese by carefully designed heuristic rules. Although
this type of method does not require manually an-
notated bilingual corpora or dictionaries, errors in
source language results, wrong word alignments and
inconsistencies between the languages limit applica-
tion of this method.
Constraint-based monolingual methods by using
ILP have been successfully applied to many natural
language processing tasks, such as Semantic Role
Labeling (Punyakanok et al, 2004), Dependency
Parsing (Martins et al, 2009) and Textual Entail-
ment (Berant et al, 2011). Zhuang and Zong (2010)
proposed a joint inference method for bilingual se-
mantic role labeling with ILP. However, their ap-
proach requires training an alignment model with a
manually annotated corpus.
9 Conclusions
We proposed a novel ILP based inference algorithm
with bilingual constraints for NER. This method
can jointly infer bilingual named entities without
using any annotated bilingual corpus. We in-
vestigate various bilingual constraints: hard and
soft constraints. Out empirical study on large-
scale OntoNotes Chinese-English parallel NER data
showed that Soft-align method, which allows
inconsistent named entity tags between two aligned
words and considers word alignment probabilities,
can significantly improve over the performance of
a strong Chinese NER baseline. Our work is the
first to evaluate performance on a large-scale stan-
dard dataset. Finally, we can also improve mono-
lingual Chinese NER performance significantly, by
combining the original monolingual training data
with new data obtained from bitext tagged by our
method. The final ILP-based bilingual NER tag-
ger with soft constraints is publicly available at:
github.com/carfly/bi_ilp
Future work could apply the bilingual constraint-
based method to other tasks, such as part-of-speech
tagging and relation extraction.
Acknowledgments
The authors would like to thank Rob Voigt and the
three anonymous reviewers for their valuable com-
ments and suggestions. We gratefully acknowledge
the support of the National Natural Science Foun-
dation of China (NSFC) via grant 61133012, the
National ?863? Project via grant 2011AA01A207
and 2012AA011102, the Ministry of Education Re-
search of Social Sciences Youth funded projects
via grant 12YJCZH304, the Defense Advanced Re-
search Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181 and
the support of the DARPA Broad Operational Lan-
guage Translation (BOLT) program through IBM.
Any opinions, findings, and conclusion or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the view of
the DARPA, AFRL, or the US government.
60
References
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 610?619, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18(4):467?479, December.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010. Learning better monolingual models with unan-
notated bilingual text. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 46?54, Uppsala, Sweden, July.
Association for Computational Linguistics.
Yufeng Chen, Chengqing Zong, and Keh-Yih Su. 2010.
On jointly recognizing and aligning bilingual named
entities. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 631?639, Uppsala, Sweden, July. Association
for Computational Linguistics.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based pro-
jections. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 600?609, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
B. Efron and R. J. Tibshirani. 1993. An Introduction to
the Bootstrap. Chapman & Hall, New York.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 363?370, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Ruiji Fu, Bing Qin, and Ting Liu. 2011. Generating
chinese named entity data from a parallel corpus. In
Proceedings of 5th International Joint Conference on
Natural Language Processing, pages 264?272, Chiang
Mai, Thailand, November. Asian Federation of Natural
Language Processing.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, NAACL-Short ?06,
pages 57?60, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Fei Huang and Stephan Vogel. 2002. Improved named
entity translation and bilingual named entity extrac-
tion. In Proceedings of the 4th IEEE International
Conference on Multimodal Interfaces, ICMI 2002,
Washington, DC, USA. IEEE Computer Society.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1222?1231, Singapore, August. Association for
Computational Linguistics.
Sungchul Kim, Kristina Toutanova, and Hwanjo Yu.
2012. Multilingual named entity recognition using
parallel data and metadata from wikipedia. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 694?702, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08: HLT, pages 595?603, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Shankar Kumar. 2005. Minimum bayes-risk techniques
in automatic speech recognition and statistical ma-
chine translation. Ph.D. thesis, Baltimore, MD, USA.
AAI3155633.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, and Fei
Huang. 2012. Joint bilingual name tagging for paral-
lel corpora. In Proceedings of the 21st ACM Inter-
national Conference on Information and Knowledge
Management (CIKM 2012), Honolulu, Hawaii, Octo-
ber.
Percy Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, MIT.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 342?350, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
Bernard Me?rialdo. 1994. Tagging english text with a
probabilistic model. Comput. Linguist., 20(2):155?
171.
61
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Daniel Marcu Susan Dumais
and Salim Roukos, editors, HLT-NAACL 2004: Main
Proceedings, pages 337?342, Boston, Massachusetts,
USA, May 2 - May 7. Association for Computational
Linguistics.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate deter-
ministic question parsing. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 705?713, Cambridge, MA,
October. Association for Computational Linguistics.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zi-
mak. 2004. Semantic role labeling via integer lin-
ear programming inference. In Proceedings of Coling
2004, pages 1346?1352, Geneva, Switzerland, Aug
23?Aug 27. COLING.
Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields. In
Proceedings of the 22nd international conference on
Machine learning, ICML ?05, pages 736?743, New
York, NY, USA. ACM.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
the second meeting of the North American Chapter of
the Association for Computational Linguistics on Lan-
guage technologies, NAACL ?01, pages 1?8, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Tao Zhuang and Chengqing Zong. 2010. Joint inference
for bilingual semantic role labeling. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 304?314, Cambridge,
MA, October. Association for Computational Linguis-
tics.
62
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1073?1082,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Joint Word Alignment and Bilingual Named Entity Recognition
Using Dual Decomposition
Mengqiu Wang
Stanford University
Stanford, CA 94305
mengqiu@cs.stanford.edu
Wanxiang Che
Harbin Institute of Technology
Harbin, China, 150001
car@ir.hit.edu.cn
Christopher D. Manning
Stanford University
Stanford, CA 94305
manning@cs.stanford.edu
Abstract
Translated bi-texts contain complemen-
tary language cues, and previous work
on Named Entity Recognition (NER)
has demonstrated improvements in perfor-
mance over monolingual taggers by pro-
moting agreement of tagging decisions be-
tween the two languages. However, most
previous approaches to bilingual tagging
assume word alignments are given as fixed
input, which can cause cascading errors.
We observe that NER label information
can be used to correct alignment mis-
takes, and present a graphical model that
performs bilingual NER tagging jointly
with word alignment, by combining two
monolingual tagging models with two uni-
directional alignment models. We intro-
duce additional cross-lingual edge factors
that encourage agreements between tag-
ging and alignment decisions. We design
a dual decomposition inference algorithm
to perform joint decoding over the com-
bined alignment and NER output space.
Experiments on the OntoNotes dataset
demonstrate that our method yields signif-
icant improvements in both NER and word
alignment over state-of-the-art monolin-
gual baselines.
1 Introduction
We study the problem of Named Entity Recogni-
tion (NER) in a bilingual context, where the goal
is to annotate parallel bi-texts with named entity
tags. This is a particularly important problem for
machine translation (MT) since entities such as
person names, locations, organizations, etc. carry
much of the information expressed in the source
sentence. Recognizing them provides useful in-
formation for phrase detection and word sense dis-
ambiguation (e.g., ?melody? as in a female name
has a different translation from the word ?melody?
in a musical sense), and can be directly leveraged
to improve translation quality (Babych and Hart-
ley, 2003). We can also automatically construct a
named entity translation lexicon by annotating and
extracting entities from bi-texts, and use it to im-
prove MT performance (Huang and Vogel, 2002;
Al-Onaizan and Knight, 2002). Previous work
such as Burkett et al (2010b), Li et al (2012) and
Kim et al (2012) have also demonstrated that bi-
texts annotated with NER tags can provide useful
additional training sources for improving the per-
formance of standalone monolingual taggers.
Because human translation in general preserves
semantic equivalence, bi-texts represent two per-
spectives on the same semantic content (Burkett et
al., 2010b). As a result, we can find complemen-
tary cues in the two languages that help to dis-
ambiguate named entity mentions (Brown et al,
1991). For example, the English word ?Jordan?
can be either a last name or a country. Without
sufficient context it can be difficult to distinguish
the two; however, in Chinese, these two senses are
disambiguated: ???? as a last name, and ????
as a country name.
In this work, we first develop a bilingual NER
model (denoted as BI-NER) by embedding two
monolingual CRF-based NER models into a larger
undirected graphical model, and introduce addi-
tional edge factors based on word alignment (WA).
Because the new bilingual model contains many
cyclic cliques, exact inference is intractable. We
employ a dual decomposition (DD) inference al-
gorithm (Bertsekas, 1999; Rush et al, 2010) for
performing approximate inference. Unlike most
1073
f1 f2 f3 f4 f5 f6
e1 e2 e3 e4 e5 e6
Xinhua News Agency Beijing Feb 16
B-ORG I-ORG I-ORG [O] B-LOC O O
??? ? ?? ? ?? ??
B-ORG O B-GPE O O O
Figure 1: Example of NER labels between two word-aligned bilingual parallel sentences. The [O] tag is
an example of a wrong tag assignment. The dashed alignment link between e3 and f2 is an example of
alignment error.
previous applications of the DD method in NLP,
where the model typically factors over two com-
ponents and agreement is to be sought between the
two (Rush et al, 2010; Koo et al, 2010; DeNero
and Macherey, 2011; Chieu and Teow, 2012), our
method decomposes the larger graphical model
into many overlapping components where each
alignment edge forms a separate factor. We design
clique potentials over the alignment-based edges
to encourage entity tag agreements. Our method
does not require any manual annotation of word
alignments or named entities over the bilingual
training data.
The aforementioned BI-NER model assumes
fixed alignment input given by an underlying word
aligner. But the entity span and type predictions
given by the NER models contain complementary
information for correcting alignment errors. To
capture this source of information, we present a
novel extension that combines the BI-NER model
with two uni-directional HMM-based alignment
models, and perform joint decoding of NER and
word alignments. The new model (denoted as
BI-NER-WA) factors over five components: one
NER model and one word alignment model for
each language, plus a joint NER-alignment model
which not only enforces NER label agreements but
also facilitates message passing among the other
four components. An extended DD decoding algo-
rithm is again employed to perform approximate
inference.
We give a formal definition of the Bi-NER
model in Section 2, and then move to present the
Bi-NER-WA model in Section 3.
2 Bilingual NER by Agreement
The inputs to our models are parallel sentence
pairs (see Figure 1 for an example in English and
Chinese). We denote the sentences as e (for En-
glish) and f (for Chinese). We assume access
to two monolingual linear-chain CRF-based NER
models that are already trained. The English-side
CRF model assigns the following probability for a
tag sequence ye:
PCRFe (ye|e) =
?
vi?Ve
?(vi)
?
(vi,vj)?De
?(vi, vj)
Ze(e)
where Ve is the set of vertices in the CRF and
De is the set of edges. ?(vi) and ?(vi, vj) are
the node and edge clique potentials, and Ze(e)
is the partition function for input sequence e un-
der the English CRF model. We let k(ye) be the
un-normalized log-probability of tag sequence ye,
defined as:
k(ye) = log
?
? ?
vi?Ve
?(vi)
?
(vi,vj)?De
?(vi, vj)
?
?
Similarly, we define model PCRFf and un-
normalized log-probability l(yf) for Chinese.
We also assume that a set of word alignments
(A = {(i, j) : ei ? fj}) is given by a word
aligner and remain fixed in our model.
For clarity, we assume ye and yf are binary vari-
ables in the description of our algorithms. The ex-
tension to the multi-class case is straight-forward
and does not affect the core algorithms.
2.1 Hard Agreement
We define a BI-NER model which imposes hard
agreement of entity labels over aligned word pairs.
At inference time, we solve the following opti-
1074
mization problem:
max
ye,yf
log (PCRFe (ye)) + log
(
PCRFf
(
yf
))
=max
ye,yf
k(ye) + l(yf)? logZe(e)? logZf (f)
'max
ye,yf
k(ye) + l(yf)
3 yei = yfj ?(i, j) ? A
We dropped the Ze(e) and Zf(f) terms because
they remain constant at inference time.
The Lagrangian relaxation of this term is:
L
(
ye,yf,U
)
=
k (ye) + l
(
yf
)
+
?
(i,j)?A
u(i, j)
(
yei ? yfj
)
where u(i, j) are the Lagrangian multipliers.
Instead of solving the Lagrangian directly, we
can form the dual of this problem and solve it us-
ing dual decomposition (Rush et al, 2010):
min
U
(
max
ye
?
?k (ye) +
?
(i,j)?A
u(i, j)yei
?
?
+max
yf
?
?l
(
yf
)
?
?
(i,j)?A
u(i, j)yfj
?
?
)
Similar to previous work, we solve this DD
problem by iteratively updating the sub-gradient
as depicted in Algorithm 1. T is the maximum
number of iterations before early stopping, and ?t
is the learning rate at time t. We adopt a learning
rate update rule from Koo et al (2010) where ?t is
defined as 1N , where N is the number of times weobserved a consecutive dual value increase from
iteration 1 to t.
A thorough introduction to the theoretical foun-
dations of dual decomposition algorithms is be-
yond the scope of this paper; we encourage un-
familiar readers to read Rush and Collins (2012)
for a full tutorial.
2.2 Soft Agreement
The previously discussed hard agreement model
rests on the core assumption that aligned words
must have identical entity tags. In reality, however,
this assumption does not always hold. Firstly, as-
suming words are correctly aligned, their entity
tags may not agree due to inconsistency in anno-
tation standards. In Figure 1, for example, the
Algorithm 1 DD inference algorithm for hard
agreement model.
?(i, j) ? A : u(i, j) = 0
for t? 1 to T do
ye? ? argmax k (ye) + ?
(i,j)?A
u(i, j)yei
yf? ? argmax l
(
yf
)
? ?
(i,j)?A
u(i, j)yfj
if ?(i, j) ? A : ye?i = yf?j then
return (ye?,yf?)
end if
for all (i, j) ? A do
u(i, j)? u(i, j) + ?t
(
yf?j ? ye?i
)
end for
end for
return (ye?(T),yf?(T)
)
word ?Beijing? can be either a Geo-Political En-
tity (GPE) or a location. The Chinese annotation
standard may enforce that ?Beijing? should always
be tagged as GPE when it is mentioned in isola-
tion, while the English standard may require the
annotator to judge based on word usage context.
The assumption in the hard agreement model can
also be violated if there are word alignment errors.
In order to model this uncertainty, we extend
the two previously independent CRF models into a
larger undirected graphical model, by introducing
a cross-lingual edge factor ?(i, j) for every pair of
word positions (i, j) ? A. We associate a clique
potential function h(i,j)(yei , yfj) for ?(i, j):
h(i,j)
(
yei , yfj
)
= pmi
(
yei , yfj
)P? (ei,fj)
where pmi(yei , yfj) is the point-wise mutual in-
formation (PMI) of the tag pair, and we raise it
to the power of a posterior alignment probability
P? (ei, fj). For a pair of NEs that are aligned with
low probability, we cannot be too sure about the
association of the two NEs, therefore the model
should not impose too much influence from the
bilingual agreement model; instead, we will let the
monolingual NE models make their decisions, and
trust that those are the best estimates we can come
up with when we do not have much confidence in
their bilingual association. The use of the poste-
rior alignment probability facilitates this purpose.
Initially, each of the cross-lingual edge factors
will attempt to assign a pair of tags that has the
highest PMI score, but if the monolingual taggers
do not agree, a penalty will start accumulating
over this pair, until some other pair that agrees bet-
ter with the monolingual models takes the top spot.
1075
Simultaneously, the monolingual models will also
be encouraged to agree with the cross-lingual edge
factors. This way, the various components effec-
tively trade penalties indirectly through the cross-
lingual edges, until a tag sequence that maximizes
the joint probability is achieved.
Since we assume no bilingually annotated NER
corpus is available, in order to get an estimate of
the PMI scores, we first tag a collection of unan-
notated bilingual sentence pairs using the mono-
lingual CRF taggers, and collect counts of aligned
entity pairs from this auto-generated tagged data.
Each of the ?(i, j) edge factors (e.g., the edge
between node f3 and e4 in Figure 1) overlaps with
each of the two CRF models over one vertex (e.g.,
f3 on Chinese side and e4 on English side), and
we seek agreement with the Chinese CRF model
over tag assignment of fj , and similarly for ei on
English side. In other words, no direct agreement
between the two CRF models is enforced, but they
both need to agree with the bilingual edge factors.
The updated optimization problem becomes:
max
ye(k)yf(l)ye(h)yf(h)
k
(
ye(k)
)
+ l
(
yf (l)
)
+
?
(i,j)?A
h(i,j)
(
ye(h)i , yf
(h)
j
)
3 ?(i, j) ? A :
(
ye(k)i = ye
(h)
i
)
?
(
yf (l)j = y
f (h)
j
)
where the notation ye(k)i denotes tag assignment to
word ei by the English CRF and ye(h)i denotes as-
signment to word ei by the bilingual factor; yf (l)j
denotes the tag assignment to word fj by the Chi-
nese CRF and yf (h)j denotes assignment to word
fj by the bilingual factor.
The updated DD algorithm is illustrated in Al-
gorithm 2 (case 2). We introduce two separate
sets of dual constraints we and wf, which range
over the set of vertices on their respective half
of the graph. Decoding the edge factor model
h(i,j)(yei , y
f
j) simply involves finding the pair of
tag assignments that gives the highest PMI score,
subject to the dual constraints.
The way DD algorithms work in decomposing
undirected graphical models is analogous to other
message passing algorithms such as loopy belief
propagation, but DD gives a stronger optimality
guarantee upon convergence (Rush et al, 2010).
3 Joint Alignment and NER Decoding
In this section we develop an extended model in
which NER information can in turn be used to
improve alignment accuracy. Although we have
seen more than a handful of recent papers that ap-
ply the dual decomposition method for joint in-
ference problems, all of the past work deals with
cases where the various model components have
the same inference output space (e.g., dependency
parsing (Koo et al, 2010), POS tagging (Rush et
al., 2012), etc.). In our case the output space is
the much more complex joint alignment and NER
tagging space. We propose a novel dual decom-
position variant for performing inference over this
joint space.
Most commonly used alignment models, such
as the IBM models and HMM-based aligner are
unsupervised learners, and can only capture sim-
ple distortion features and lexical translational fea-
tures due to the high complexity of the structure
prediction space. On the other hand, the CRF-
based NER models are trained on manually anno-
tated data, and admit richer sequence and lexical
features. The entity label predictions made by the
NER model can potentially be leveraged to correct
alignment mistakes. For example, in Figure 1, if
the tagger knows that the word ?Agency? is tagged
I-ORG, and if it also knows that the first comma
in the Chinese sentence is not part of any entity,
then we can infer it is very unlikely that there ex-
ists an alignment link between ?Agency? and the
comma.
To capture this intuition, we extend the BI-NER
model to jointly perform word alignment and NER
decoding, and call the resulting model BI-NER-
WA. As a first step, instead of taking the output
from an aligner as fixed input, we incorporate two
uni-directional aligners into our model. We name
the Chinese-to-English aligner model as m(Be)
and the reverse directional model n(Bf ). Be is
a matrix that holds the output of the Chinese-to-
English aligner. Each be(i, j) binary variable in
Be indicates whether fj is aligned to ei; similarly
we define output matrix Bf and bf (i, j) for Chi-
nese. In our experiments, we used two HMM-
based alignment models. But in principle we can
adopt any alignment model as long as we can per-
form efficient inference over it.
We introduce a cross-lingual edge factor ?(i, j)
in the undirected graphical model for every pair of
word indices (i, j), which predicts a binary vari-
1076
Algorithm 2 DD inference algorithm for joint
alignment and NER model. A line marked with (2)
means it applies to the BI-NER model; a line marked with
(3) means it applies to the BI-NER-WA model.
S ? A (2)
S ? {(i, j) : ?i ? |e|, ?j ? |f |} (3)
?i ? |e| : wei = 0; ?j ? |f | : wfj = 0 (2,3)
?(i, j) ? S : de(i, j) = 0, df (i, j) = 0 (3)
for t? 1 to T do
ye(k)? ? argmax k
(
ye(k)
)
+
?
i?|e|
wei ye
(k)
i (2,3)
yf(l)? ? argmax l
(
yf(l)
)
+
?
i?|f |
wfj yf
(l)
j (2,3)
Be??argmax m (Be) + ?
(i,j)
de(i, j)be(i, j) (3)
Bf??argmax n
(
Bf
)
+
?
(i,j)
df(i, j)bf(i, j) (3)
for all (i, j) ? S do
(ye(h)?i yf
(h)?
j )? ?wei ye
(h)
i ? wfj yf
(h)
j
+ argmax h(i,j)(ye
(q)
i yf
(q)
j ) (2)
(ye(q)?i yf
(q)?
j a(i, j)?)? ?wei ye
(q)
i ? wfj yf
(q)
j
+ argmax q(i,j)(ye
(q)
i yf
(q)
j a(i, j))
? de(i, j)a(i, j)? df(i, j)a(i, j) (3)
end for
Conv = (ye(k)=ye(q) ? yf(l)=yf(q)) (2)
Conv = (Be=A=Bf ? ye(k)=ye(q)? yf(l)=yf(q)) (3)
if Conv = true , then
return
(
ye(k)? ,yf(l)?
)
(2)
return
(
ye(k)? ,yf(l)? ,A
)
(3)
else
for all i ? |e| do
wei ? wei + ?t
(
ye(q|h)?i ? ye
(k)?
i
)
(2,3)
end for
for all j ? |f | do
wfj ? wfj + ?t
(
yf
(q|h)?
j ? yf
(l)?
j
)
(2,3)
end for
for all (i, j) ? S do
de(i, j)? de(i, j) + ?t (ae?(i, j)? be?(i, j)) (3)
df(i, j)? df(i, j) + ?t
(
af?(i, j)? bf?(i, j)
) (3)
end for
end if
end for
return
(
ye(k)?(T) ,yf
(l)?
(T)
)
(2)
return
(
ye(k)?(T) ,yf
(l)?
(T) ,A(T )
)
(3)
able a(i, j) for an alignment link between ei and
fj . The edge factor also predicts the entity tags for
ei and fj .
The new edge potential q is defined as:
q(i,j)
(
yei , yfj , a(i, j)
)
=
log(P (a(i, j) = 1)) + S(yei , yfj |a(i, j))P (a(i,j)=1)
S(yei , yfj |a(i, j))=
{
pmi(yei , y
f
j), if a(i, j) = 1
0, else
P (a(i, j) = 1) is the alignment probability as-
signed by the bilingual edge factor between node
ei and fj . We initialize this value to P? (ei, fj) =
1
2(Pm(ei, fj) + Pn(ei, fj)), where Pm(ei, fj) and
Pn(ei, fj) are the posterior probabilities assigned
by the HMM-aligners.
The joint optimization problem is defined as:
max
ye(k)yf(l)ye(h)yf(h)BeBfA
k(ye(k)) + l(yf (l))+
m(Be) + n(Bf) +
?
(i?|e|,j?|f |)
q(i,j)(ye
h
i , yf
(h)
j , a(i, j))
3 ?(i, j) :
(
be(i, j)=a(i, j)
)
?
(
bf (i, j)=a(i, j)
)
? if a(i, j) = 1 then
(
ye(k)i =ye
(h)
i
)
?
(
yf (l)j =y
f (h)
j
)
We include two dual constraints de(i, j) and
df (i, j) over alignments for every bilingual edge
factor ?(i, j), which are applied to the English and
Chinese sides of the alignment space, respectively.
The DD algorithm used for this model is given
in Algorithm 2 (case 3). One special note is that
after each iteration when we consider updates to
the dual constraint for entity tags, we only check
tag agreements for cross-lingual edge factors that
have an alignment assignment value of 1. In other
words, cross-lingual edges that are not aligned do
not affect bilingual NER tagging.
Similar to ?(i, j), ?(i, j) factors do not provide
that much additional information other than some
selectional preferences via PMI score. But the
real power of these cross-language edge cliques
is that they act as a liaison between the NER
and alignment models on each language side, and
encourage these models to indirectly agree with
each other by having them all agree with the edge
cliques.
It is also worth noting that since we decode
the alignment models with Viterbi inference, ad-
ditional constraints such as the neighborhood con-
straint proposed by DeNero and Macherey (2011)
can be easily integrated into our model. The
neighborhood constraint enforces that if fj is
aligned to ei, then fj can only be aligned to ei+1
or ei?1 (with a small penalty), but not any other
word position. We report results of adding neigh-
borhood constraints to our model in Section 6.
4 Experimental Setup
We evaluate on the large OntoNotes (v4.0) cor-
pus (Hovy et al, 2006) which contains manually
1077
annotated NER tags for both Chinese and En-
glish. Document pairs are sentence aligned us-
ing the Champollion Tool Kit (Ma, 2006). Af-
ter discarding sentences with no aligned counter-
part, a total of 402 documents and 8,249 paral-
lel sentence pairs were used for evaluation. We
will refer to this evaluation set as full-set. We use
odd-numbered documents as the dev set and even-
numbered documents as the blind test set. We
did not perform parameter tuning on the dev set
to optimize performance, instead we fix the ini-
tial learning rate to 0.5 and maximum iterations to
1,000 in all DD experiments. We only use the dev
set for model development.
The Stanford CRF-based NER tagger was used
as the monolingual component in our models
(Finkel et al, 2005). It also serves as a state-
of-the-art monolingual baseline for both English
and Chinese. For English, we use the default tag-
ger setting from Finkel et al (2005). For Chi-
nese, we use an improved set of features over the
default tagger, which includes distributional sim-
ilarity features trained on large amounts of non-
overlapping data.1
We train the two CRF models on all portions
of the OntoNotes corpus that are annotated with
named entity tags, except the parallel-aligned por-
tion which we reserve for development and test
purposes. In total, there are about 660 train-
ing documents (?16k sentences) for Chinese and
1,400 documents (?39k sentences) for English.
Out of the 18 named entity types that are an-
notated in OntoNotes, which include person, lo-
cation, date, money, and so on, we select the four
most commonly seen named entity types for evalu-
ation. They are person, location, organization and
GPE. All entities of these four types are converted
to the standard BIO format, and background to-
kens and all other entity types are marked with
tag O. When we consider label agreements over
aligned word pairs in all bilingual agreement mod-
els, we ignore the distinction between B- and I-
tags.
We report standard NER measures (entity pre-
cision (P), recall (R) and F1 score) on the test
set. Statistical significance tests are done using the
paired bootstrap resampling method (Efron and
Tibshirani, 1993).
For alignment experiments, we train two uni-
1The exact feature set and the CRF implementation
can be found here: http://nlp.stanford.edu/
software/CRF-NER.shtml
directional HMM models as our baseline and
monolingual alignment models. The parameters
of the HMM were initialized by IBM Model 1 us-
ing the agreement-based EM training algorithms
from Liang et al (2006). Each model is trained
for 2 iterations over a parallel corpus of 12 mil-
lion English words and Chinese words, almost
twice as much data as used in previous work that
yields state-of-the-art unsupervised alignment re-
sults (DeNero and Klein, 2008; Haghighi et al,
2009; DeNero and Macherey, 2011).
Word alignment evaluation is done over the
sections of OntoNotes that have matching gold-
standard word alignment annotations from GALE
Y1Q4 dataset.2 This subset contains 288 docu-
ments and 3,391 sentence pairs. We will refer
to this subset as wa-subset. This evaluation set
is over 20 times larger than the 150 sentences
set used in most past evaluations (DeNero and
Klein, 2008; Haghighi et al, 2009; DeNero and
Macherey, 2011).
Alignments input to the BI-NER model are
produced by thresholding the averaged posterior
probability at 0.5. In joint NER and alignment ex-
periments, instead of posterior thresholding, we
take the direct intersection of the Viterbi-best
alignment of the two directional models. We re-
port the standard P, R, F1 and Alignment Error
Rate (AER) measures for alignment experiments.
An important past work to make comparisons
with is Burkett et al (2010b). Their method
is similar to ours in that they also model bilin-
gual agreement in conjunction with two CRF-
based monolingual models. But instead of using
just the PMI scores of bilingual NE pairs, as in
our work, they employed a feature-rich log-linear
model to capture bilingual correlations. Parame-
ters in their log-linear model require training with
bilingually annotated data, which is not readily
available. To counter this problem, they proposed
an ?up-training? method which simulates a super-
vised learning environment by pairing a weak clas-
sifier with strong classifiers, and train the bilin-
gual model to rank the output of the strong classi-
fier highly among the N-best outputs of the weak
classifier. In order to compare directly with their
method, we obtained the code behind Burkett et
al. (2010b) and reproduced their experimental set-
ting for the OntoNotes data. An extra set of 5,000
unannotated parallel sentence pairs are used for
2LDC Catalog No. LDC2006E86.
1078
Chinese English
P R F1 P R F1
Mono 76.89 61.64 68.42 81.98 74.59 78.11
Burkett 77.52 65.84 71.20 82.28 76.64 79.36
Bi-soft 79.14 71.55 75.15 82.58 77.96 80.20
Table 1: NER results on bilingual parallel test set.
Best numbers on each measure that are statistically
significantly better than the monolingual baseline
and Burkett et al (2010b) are highlighted in bold.
training the reranker, and the reranker model se-
lection was performed on the development dataset.
5 Bilingual NER Results
The main results on bilingual NER over the test
portion of full-set are shown in Table 1. We
initially experimented with the hard agreement
model, but it performs quite poorly for reasons we
discussed in Section 2.2. The BI-NER model with
soft agreement constraints, however, significantly
outperforms all baselines. In particular, it achieves
an absolute F1 improvement of 6.7% in Chinese
and 2.1% in English over the CRF monolingual
baselines.
A well-known issue with the DD method is
that when the model does not necessarily con-
verge, then the procedure could be very sensi-
tive to hyper-parameters such as initial step size
and early termination criteria. If a model only
gives good performance with well-tuned hyper-
parameters, then we must have manually anno-
tated data for tuning, which would significantly
reduce the applicability and portability of this
method to other language pairs and tasks. To eval-
uate the parameter sensitivity of our model, we
run the model from 50 to 3000 iterations before
early stopping, and with 6 different initial step
sizes from 0.01 to 1. The results are shown in Fig-
ure 2. The soft agreement model does not seem to
be sensitive to initial step size and almost always
converges to a superior solution than the baseline.
6 Joint NER and Alignment Results
We present results for the BI-NER-WA model
in Table 2. By jointly decoding NER with word
alignment, our model not only maintains signifi-
cant improvements in NER performance, but also
yields significant improvements to alignment per-
formance. Overall, joint decoding with NER alone
yields a 10.8% error reduction in AER over the
baseline HMM-aligners, and also gives improve-
0 0.01 0.05
0.1 0.2 0.5
1 2
30001000
800500
300100
5073
74
75
76
77
78
79
80
initial step sizemax no. of iterations
F1 sc
ore
Figure 2: Performance variance of the soft agree-
ment models on the Chinese dev dataset, as a func-
tion of step size (x-axis) and maximum number of
iterations before early stopping (y-axis).
ment over BI-NER in NER. Adding additional
neighborhood constraints gives a further 6% er-
ror reduction in AER, at the cost of a small loss
in Chinese NER. In terms of word alignment re-
sults, we see great increases in F1 and recall, but
precision goes down significantly. This is be-
cause the joint decoding algorithm promotes an ef-
fect of ?soft-union?, by encouraging the two uni-
directional aligners to agree more often. Adding
the neighborhood constraints further enhances this
union effect.
7 Error Analysis and Discussion
We can examine the example in Figure 3 to gain
an understanding of the model?s performance. In
this example, a snippet of a longer sentence pair is
shown with NER and word alignment results. The
monolingual Chinese tagger provides a strong cue
that word f6 is a person name because the unique
4-character word pattern is commonly associated
with foreign names in Chinese, and also the word
is immediately preceded by the word ?president?.
The English monolingual tagger, however, con-
fuses the aligned word e0 with a GPE.
Our bilingual NER model is able to correct this
error as expected. Similarly, the bilingual model
corrects the error over e11. However, the model
also propagates labeling errors from the English
side over the entity ?Tibet Autonomous Region? to
the Chinese side. Nevertheless, the resulting Chi-
nese tags are arguably more useful than the origi-
nal tags assigned by the baseline model.
In terms of word alignment, the HMM models
failed badly on this example because of the long
1079
NER-Chinese NER-English word alignment
P R F1 P R F1 P R F1 AER
HMM-WA - - - - - - 90.43 40.95 56.38 43.62
Mono-CRF 82.50 66.58 73.69 84.24 78.70 81.38 - - - -
Bi-NER 84.87 75.30 79.80 84.47 81.45 82.93 - - - -
Bi-NER-WA 84.42 76.34 80.18 84.25 82.20 83.21 77.45 50.43 61.09 38.91
Bi-NER-WA+NC 84.25 75.09 79.41 84.28 82.17 83.21 76.67 54.44 63.67 36.33
Table 2: Joint alignment and NER test results. +NC means incorporating additional neighbor constraints
from DeNero and Macherey (2011) to the model. Best number in each column is highlighted in bold.
f0 f1 f2 f3 f4 f5 f6
e0 e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11
Suolangdaji , president of Tibet Auto. Region branch of Bank of China
B-PER O O O B-GPE I-GPE I-GPE O O B-ORG I-ORG I-ORG
B-PER O O O [B-LOC] [I-LOC] [I-LOC] O O B-ORG I-ORG I-ORG
[B-GPE] O O O [B-LOC] [I-LOC] [I-LOC] O O [O] [O] [B-GPE]
?? ?? ?? ??? ?? ?? ????
B-ORG I-ORG B-GPE O O O B-PER
B-ORG I-ORG [B-LOC] [I-LOC] O O B-PER
B-ORG I-ORG [O] O O O B-PER
Figure 3: An example output of our BI-NER-WA model. Dotted alignment links are the oracle, dashed
links are alignments from HMM baseline, and solid links are outputs of our model. Entity tags in the
gold line (closest to nodes ei and fj) are the gold-standard tags; in the green line (second closest to
nodes) are output from our model; and in the crimson line (furthest from nodes) are baseline output.
distance swapping phenomena. The two unidirec-
tional HMMs also have strong disagreements over
the alignments, and the resulting baseline aligner
output only recovers two links. If we were to take
this alignment as fixed input, most likely we would
not be able to recover the error over e11, but the
joint decoding method successfully recovered 4
more links, and indirectly resulted in the NER tag-
ging improvement discussed above.
8 Related Work
The idea of employing bilingual resources to im-
prove over monolingual systems has been ex-
plored by much previous work. For example,
Huang et al (2009) improved parsing performance
using a bilingual parallel corpus. In the NER
domain, Li et al (2012) presented a cyclic CRF
model very similar to our BI-NER model, and
performed approximate inference using loopy be-
lief propagation. The feature-rich CRF formula-
tion of bilingual edge potentials in their model is
much more powerful than our simple PMI-based
bilingual edge model. Adding a richer bilingual
edge model might well further improve our results,
and this is a possible direction for further experi-
mentation. However, a big drawback of this ap-
proach is that training such a feature-rich model
requires manually annotated bilingual NER data,
which can be prohibitively expensive to generate.
How and where to obtain training signals with-
out manual supervision is an interesting and open
question. One of the most interesting papers in this
regard is Burkett et al (2010b), which explored
an ?up-training? mechanism by using the outputs
from a strong monolingual model as ground-truth,
and simulated a learning environment where a
bilingual model is trained to help a ?weakened?
monolingual model to recover the results of the
strong model. It is worth mentioning that since
our method does not require additional training
and can take pretty much any existing model as
?black-box? during decoding, the richer and more
accurate bilingual model learned from Burkett et
al. (2010b) can be directly plugged into our model.
A similar dual decomposition algorithm to ours
was proposed by Riedel and McCallum (2011)
for biomedical event detection. In their Model
3, the trigger and argument extraction models
are reminiscent of the two monolingual CRFs in
our model; additional binding agreements are en-
forced over every protein pair, similar to how we
enforce agreement between every aligned word
1080
pair. Martins et al (2011b) presented a new DD
method that combines the power of DD with the
augmented Lagrangian method. They showed
that their method can achieve faster convergence
than traditional sub-gradient methods in models
with many overlapping components (Martins et
al., 2011a). This method is directly applicable to
our work.
Another promising direction for improving
NER performance is in enforcing global label
consistency across documents, which is an idea
that has been greatly explored in the past (Sut-
ton and McCallum, 2004; Bunescu and Mooney,
2004; Finkel et al, 2005). More recently, Rush
et al (2012) and Chieu and Teow (2012) have
shown that combining local prediction models
with global consistency models, and enforcing
agreement via DD is very effective. It is straight-
forward to incorporate an additional global consis-
tency model into our model for further improve-
ments.
Our joint alignment and NER decoding ap-
proach is inspired by prior work on improving
alignment quality through encouraging agreement
between bi-directional models (Liang et al, 2006;
DeNero and Macherey, 2011). Instead of enforc-
ing agreement in the alignment space based on
best sequences found by Viterbi, we could opt
to encourage agreement between posterior prob-
ability distributions, which is related to the pos-
terior regularization work by Grac?a et al (2008).
Cromie`res and Kurohashi (2009) proposed an ap-
proach that takes phrasal bracketing constraints
from parsing outputs, and uses them to enforce
phrasal alignments. This idea is similar to our joint
alignment and NER approach, but in our case the
phrasal constraints are indirectly imposed by en-
tity spans. We also differ in the implementation
details, where in their case belief propagation is
used in both training and Viterbi inference.
Burkett et al (2010a) presented a supervised
learning method for performing joint parsing and
word alignment using log-linear models over parse
trees and an ITG model over alignment. The
model demonstrates performance improvements
in both parsing and alignment, but shares the com-
mon limitations of other supervised work in that it
requires manually annotated bilingual joint pars-
ing and word alignment data.
Chen et al (2010) also tackled the problem of
joint alignment and NER. Their method employs a
set of heuristic rules to expand a candidate named
entity set generated by monolingual taggers, and
then rank those candidates using a bilingual named
entity dictionary. Our approach differs in that we
provide a probabilistic formulation of the problem
and do not require pre-existing NE dictionaries.
9 Conclusion
We introduced a graphical model that combines
two HMM word aligners and two CRF NER tag-
gers into a joint model, and presented a dual de-
composition inference method for performing ef-
ficient decoding over this model. Results from
NER and word alignment experiments suggest that
our method gives significant improvements in both
NER and word alignment. Our techniques make
minimal assumptions about the underlying mono-
lingual components, and can be adapted for many
other tasks such as parsing.
Acknowledgments
The authors would like to thank Rob Voigt and
the three anonymous reviewers for their valuable
comments and suggestions. We gratefully ac-
knowledge the support of the National Natural
Science Foundation of China (NSFC) via grant
61133012, the National ?863? Project via grant
2011AA01A207 and 2012AA011102, the Min-
istry of Education Research of Social Sciences
Youth funded projects via grant 12YJCZH304,
and the support of the U.S. Defense Advanced
Research Projects Agency (DARPA) Broad Op-
erational Language Translation (BOLT) program
through IBM.
Any opinions, findings, and conclusion or rec-
ommendations expressed in this material are those
of the authors and do not necessarily reflect the
view of DARPA, or the US government.
References
Yaser Al-Onaizan and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual
resources. In Proceedings of ACL.
Bogdan Babych and Anthony Hartley. 2003. Im-
proving machine translation quality with automatic
named entity recognition. In Proceedings of the
7th International EAMT workshop on MT and other
Language Technology Tools, Improving MT through
other Language Technology Tools: Resources and
Tools for Building MT.
1081
Dimitri P. Bertsekas. 1999. Nonlinear Programming.
Athena Scientific, New York.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1991. Word-
sense disambiguation using statistical methods. In
Proceedings of ACL.
Razvan Bunescu and Raymond J. Mooney. 2004.
Collective information extraction with relational
Markov networks. In Proceedings of ACL.
David Burkett, John Blitzer, and Dan Klein. 2010a.
Joint parsing and alignment with weakly synchro-
nized grammars. In Proceedings of NAACL-HLT.
David Burkett, Slav Petrov, John Blitzer, and Dan
Klein. 2010b. Learning better monolingual mod-
els with unannotated bilingual text. In Proceedings
of CoNLL.
Yufeng Chen, Chengqing Zong, and Keh-Yih Su.
2010. On jointly recognizing and aligning bilingual
named entities. In Proceedings of ACL.
Hai Leong Chieu and Loo-Nin Teow. 2012. Com-
bining local and non-local information with dual de-
composition for named entity recognition from text.
In Proceedings of 15th International Conference on
Information Fusion (FUSION).
Fabien Cromie`res and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In Proceedings of
EACL/ IJCNLP.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL.
John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In Proceedings of ACL.
Brad Efron and Robert Tibshirani. 1993. An Introduc-
tion to the Bootstrap. Chapman & Hall, New York.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of ACL.
Joao Grac?a, Kuzman Ganchev, and Ben Taskar. 2008.
Expectation maximization and posterior constraints.
In Proceedings of NIPS.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised ITG models. In Proceedings of ACL.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: the 90% solution. In Proceedings of
NAACL-HLT.
Fei Huang and Stephan Vogel. 2002. Improved named
entity translation and bilingual named entity extrac-
tion. In Proceedings of the 2002 International Con-
ference on Multimodal Interfaces (ICMI).
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP.
Sungchul Kim, Kristina Toutanova, and Hwanjo Yu.
2012. Multilingual named entity recognition using
parallel data and metadata from Wikipedia. In Pro-
ceedings of ACL.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of EMNLP.
Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, and
Fei Huang. 2012. Joint bilingual name tagging for
parallel corpora. In Proceedings of CIKM.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL.
Xiaoyi Ma. 2006. Champollion: A robust parallel text
sentence aligner. In Proceedings of LREC.
Andre? F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and Ma?rio A. T. Figueiredo. 2011a. Dual
decomposition with many overlapping components.
In Proceedings of EMNLP.
Andre F. T. Martins, Noah A. Smith, Eric P. Xing,
Pedro M. Q. Aguiar, and Ma?rio A. T. Figueiredo.
2011b. Augmenting dual decomposition for map in-
ference. In Proceedings of the International Work-
shop on Optimization for Machine Learning (OPT
2010).
Sebastian Riedel and Andrew McCallum. 2011. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of EMNLP.
Alexander M. Rush and Michael Collins. 2012. A tu-
torial on dual decomposition and Lagrangian relax-
ation for inference in natural language processing.
JAIR, 45:305?362.
Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natural
language processing. In Proceedings of EMNLP.
Alexander M. Rush, Roi Reichert, Michael Collins, and
Amir Globerson. 2012. Improved parsing and POS
tagging using inter-sentence consistency constraints.
In Proceedings of EMNLP.
Charles Sutton and Andrew McCallum. 2004. Col-
lective segmentation and labeling of distant entities
in information extraction. In Proceedings of ICML
Workshop on Statistical Relational Learning and Its
connections to Other Fields.
1082
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 193?198,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Two Knives Cut Better Than One:
Chinese Word Segmentation with Dual Decomposition
Mengqiu Wang
Computer Science Department
Stanford University
Stanford, CA 94305
Rob Voigt
Linguistics Department
Stanford University
Stanford, CA 94305
{mengqiu,manning}@cs.stanford.edu robvoigt@stanford.edu
Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
Abstract
There are two dominant approaches to
Chinese word segmentation: word-based
and character-based models, each with re-
spective strengths. Prior work has shown
that gains in segmentation performance
can be achieved from combining these
two types of models; however, past efforts
have not provided a practical technique
to allow mainstream adoption. We pro-
pose a method that effectively combines
the strength of both segmentation schemes
using an efficient dual-decomposition al-
gorithm for joint inference. Our method
is simple and easy to implement. Ex-
periments on SIGHAN 2003 and 2005
evaluation datasets show that our method
achieves the best reported results to date
on 6 out of 7 datasets.
1 Introduction
Chinese text is written without delimiters between
words; as a result, Chinese word segmentation
(CWS) is an essential foundational step for many
tasks in Chinese natural language processing. As
demonstrated by (Shi and Wang, 2007; Bai et
al., 2008; Chang et al, 2008; Kummerfeld et al,
2013), the quality and consistency of segmentation
has important downstream impacts on system per-
formance in machine translation, POS tagging and
parsing.
State-of-the-art performance in CWS is high,
with F-scores in the upper 90s. Still, challenges
remain. Unknown words, also known as out-of-
vocabulary (OOV) words, lead to difficulties for
word- or dictionary-based approaches. Ambiguity
can cause errors when the appropriate segmenta-
tion is determined contextually, such as?? (?tal-
ent?) and? /? (?just able?) (Gao et al, 2003).
There are two primary classes of models:
character-based, where the foundational units for
processing are individual Chinese characters (Xue,
2003; Tseng et al, 2005; Zhang et al, 2006;
Wang et al, 2010), and word-based, where the
units are full words based on some dictionary or
training lexicon (Andrew, 2006; Zhang and Clark,
2007). Sun (2010) details their respective theo-
retical strengths: character-based approaches bet-
ter model the internal compositional structure of
words and are therefore more effective at inducing
new OOV words; word-based approaches are bet-
ter at reproducing the words of the training lexi-
con and can capture information from significantly
larger contextual spans. Prior work has shown per-
formance gains from combining these two types
of models to exploit their respective strengths, but
such approaches are often complex to implement
and computationally expensive.
In this work, we propose a simple and prin-
cipled joint decoding method for combining
character-based and word-based segmenters based
on dual decomposition. This method has strong
optimality guarantees and works very well empir-
ically. It is easy to implement and does not re-
quire retraining of existing character- and word-
based segmenters. Perhaps most importantly, this
work presents a much more practical and usable
form of classifier combination in the CWS context
than existing methods offer.
Experimental results on standard SIGHAN
2003 and 2005 bake-off evaluations show that our
model outperforms the character and word base-
lines by a significant margin. In particular, out
approach improves OOV recall rates and segmen-
tation consistency, and gives the best reported re-
sults to date on 6 out of 7 datasets.
2 Models for CWS
Here we describe the character-based and word-
based models we use as baselines, review existing
approaches to combination, and describe our algo-
rithm for joint decoding with dual decomposition.
193
2.1 Character-based Models
In the most commonly used contemporary ap-
proach to character-based segmentation, first pro-
posed by (Xue, 2003), CWS is seen as a charac-
ter sequence tagging task, where each character
is tagged on whether it is at the beginning, mid-
dle, or end of a word. Conditional random fields
(CRF) (Lafferty et al, 2001) have been widely
adopted for this task, and give state-of-the-art re-
sults (Tseng et al, 2005). In a first-order linear-
chain CRF model, the conditional probability of a
label sequence y given a word sequence x is de-
fined as:
P (y|x) =
1
Z
|y|
?
t=1
exp (? ? f(x, y
t
, y
t+1
))
f(x, y
t
, y
t?1
) are feature functions that typically
include surrounding character n-gram and mor-
phological suffix/prefix features. These types of
features capture the compositional properties of
characters and are likely to generalize well to un-
known words. However, the Markov assumption
in CRF limits the context of such features; it is
difficult to capture long-range word features in this
model.
2.2 Word-based Models
Word-based models search through lists of word
candidates using scoring functions that directly
assign scores to each. Early word-based seg-
mentation work employed simple heuristics like
dictionary-lookup maximum matching (Chen and
Liu, 1992). More recently, Zhang and Clark
(2007) reported success using a linear model
trained with the average perceptron algorithm
(Collins, 2002). Formally, given input x, their
model seeks a segmentation y such that:
F (y|x) = max
y?GEN(x)
(? ? ?(y))
F (y|x) is the score of segmentation result y.
Searching through the entire GEN(x) space is
intractable even with a local model, so a beam-
search algorithm is used. The search algorithm
consumes one character input token at a time, and
iterates through the existing beams to score two
new alternative hypotheses by either appending
the new character to the last word in the beam, or
starting a new word at the current position.
Algorithm 1 Dual decomposition inference algo-
rithm, and modified Viterbi and beam-search algo-
rithms.
?i ? {1 to |x|} : ?k ? {0, 1} : u
i
(k) = 0
for t? 1 to T do
y
c?
= argmax
y
P (y
c
|x) +
?
i?|x|
u
i
(y
c
i
)
y
w?
= argmax
y?GEN(x)
F (y
w
|x)?
?
j?|x|
u
j
(y
w
j
)
if y
c?
= y
w?
then
return (y
c?
,y
w?
)
end if
for all i ? {1 to |x|} do
?k ? {0, 1} : u
i
(k) = u
i
(k) + ?
t
(2k ? 1)(y
w?
i
?
y
c?
i
)
end for
end for
return (y
c?
,y
w?
)
Viterbi:
V
1
(1) = 1, V
1
(0) = 0
for i = 2 to |x| do
?k ? {0, 1} : V
i
(k) = argmax
k
?
P
i
(k|k
?
)V
i?1
k
?
+
u
i
(k)
end for
Beam-Search:
for i = 1 to |x| do
for item v = {w
0
, ? ? ? , w
j
} in beam(i) do
append x
i
to w
j
, score(v)
+
= u
i
(0)
v = {w
0
, ? ? ? , w
j
, x
i
}, score(v)
+
= u
i
(1)
end for
end for
2.3 Combining Models with Dual
Decomposition
Various mixing approaches have been proposed to
combine the above two approaches (Wang et al,
2006; Lin, 2009; Sun et al, 2009; Sun, 2010;
Wang et al, 2010). These mixing models perform
well on standard datasets, but are not in wide use
because of their high computational costs and dif-
ficulty of implementation.
Dual decomposition (DD) (Rush et al, 2010)
offers an attractive framework for combining these
two types of models without incurring high costs
in model complexity (in contrast to (Sun et al,
2009)) or decoding efficiency (in contrast to bag-
ging in (Wang et al, 2006; Sun, 2010)). DD has
been successfully applied to similar situations for
combining local with global models; for example,
in dependency parsing (Koo et al, 2010), bilingual
sequence tagging (Wang et al, 2013) and word
alignment (DeNero and Macherey, 2011).
The idea is that jointly modelling both
character-sequence and word information can be
computationally challenging, so instead we can try
to find outputs that the two models are most likely
194
Academia Sinica Peking Univ.
R P F
1
R
oov
C R P F
1
R
oov
C
Char-based CRF 95.2 93.6 94.4 58.9 0.064 94.6 95.3 94.9 77.8 0.089
Word-based Perceptron 95.8 95.0 95.4 69.5 0.060 94.1 95.5 94.8 76.7 0.099
Dual-decomp 95.9 94.9 95.4 67.7 0.055 94.8 95.7 95.3 78.7 0.086
City Univ. of Hong Kong Microsoft Research
R P F
1
R
oov
C R P F
1
R
oov
C
Char-based CRF 94.7 94.0 94.3 76.1 0.065 96.4 96.6 96.5 71.3 0.074
Word-based Perceptron 94.3 94.0 94.2 71.7 0.073 97.0 97.2 97.1 74.6 0.063
Dual-decomp 95.0 94.4 94.7 75.3 0.062 97.3 97.4 97.4 76.0 0.055
Table 1: Results on SIGHAN 2005 datasets. R
oov
denotes OOV recall, and C denotes segmentation
consistency. Best number in each column is highlighted in bold.
to agree on. Formally, the objective of DD is:
max
y
c
,y
w
P (y
c
|x) + F (y
w
|x) s.t. y
c
= y
w
(1)
where y
c
is the output of character-based CRF, y
w
is the output of word-based perceptron, and the
agreements are expressed as constraints. s.t. is
a shorthand for ?such that?.
Solving this constrained optimization problem
directly is difficult. Instead, we take the La-
grangian relaxation of this term as:
L (y
c
,y
w
,U) = (2)
P (y
c
|x) + F (y
w
|x) +
?
i?|x|
u
i
(y
c
i
? y
w
i
)
where U is the set of Lagrangian multipliers that
consists of a multiplier u
i
at each word position i.
We can rewrite the original objective with the
Lagrangian relaxation as:
max
y
c
,y
w
min
U
L (y
c
,y
w
,U) (3)
We can then form the dual of this problem by
taking the min outside of the max, which is an up-
per bound on the original problem. The dual form
can then be decomposed into two sub-components
(the two max problems in Eq. 4), each of which is
local with respect to the set of Lagrangian multi-
pliers:
min
U
(
max
y
c
?
?
P (y
c
|x) +
?
i?|x|
u
i
(y
c
i
)
?
?
(4)
+max
y
w
?
?
F (y
w
|x)?
?
j?|x|
u
j
(y
w
j
)
?
?
)
This method is called dual decomposition (DD)
(Rush et al, 2010). Similar to previous work
(Rush and Collins, 2012), we solve this DD prob-
lem by iteratively updating the sub-gradient as de-
picted in Algorithm 1.
1
In each iteration, if the
best segmentations provided by the two models do
not agree, then the two models will receive penal-
ties for the decisions they made that differ from the
other. This penalty exchange is similar to message
passing, and as the penalty accumulates over itera-
tions, the two models are pushed towards agreeing
with each other. We also give an updated Viterbi
decoding algorithm for CRF and a modified beam-
search algorithm for perceptron in Algorithm 1. T
is the maximum number of iterations before early
stopping, and ?
t
is the learning rate at time t. We
adopt a learning rate update rule from Koo et al
(2010) where ?
t
is defined as
1
N
, where N is the
number of times we observed a consecutive dual
value increase from iteration 1 to t.
3 Experiments
We conduct experiments on the SIGHAN 2003
(Sproat and Emerson, 2003) and 2005 (Emer-
son, 2005) bake-off datasets to evaluate the ef-
fectiveness of the proposed dual decomposition
algorithm. We use the publicly available Stan-
ford CRF segmenter (Tseng et al, 2005)
2
as our
character-based baseline model, and reproduce
the perceptron-based segmenter from Zhang and
Clark (2007) as our word-based baseline model.
We adopted the development setting from
(Zhang and Clark, 2007), and used CTB sections
1-270 for training and sections 400-931 for devel-
opment in hyper-parameter setting; for all results
given in tables, the models are trained and eval-
uated on the standard train/test split for the given
dataset. The optimized hyper-parameters used are:
1
See Rush and Collins (2012) for a full introduction to
DD.
2
http://nlp.stanford.edu/software/segmenter.shtml
195
`2
regularization parameter ? in CRF is set to
3; the perceptron is trained for 10 iterations with
beam size 200; dual decomposition is run to max
iteration of 100 (T in Algo. 1) with step size 0.1
(?
t
in Algo. 1).
Beyond standard precision (P), recall (R) and
F
1
scores, we also evaluate segmentation consis-
tency as proposed by (Chang et al, 2008), who
have shown that increased segmentation consis-
tency is correlated with better machine transla-
tion performance. The consistency measure cal-
culates the entropy of segmentation variations ?
the lower the score the better. We also report
out-of-vocabulary recall (R
oov
) as an estimation of
the model?s generalizability to previously unseen
words.
4 Results
Table 1 shows our empirical results on SIGHAN
2005 dataset. Our dual decomposition method
outperforms both the word-based and character-
based baselines consistently across all four sub-
sets in both F
1
and OOV recall (R
oov
). Our
method demonstrates a robustness across domains
and segmentation standards regardless of which
baseline model was stronger. Of particular note
is DD?s is much more robust in R
oov
, where the
two baselines swing a lot. This is an important
property for downstream applications such as en-
tity recognition. The DD algorithm is also more
consistent, which would likely lead to improve-
ments in applications such as machine translation
(Chang et al, 2008).
The improvement over our word- and character-
based baselines is also seen in our results on the
earlier SIGHAN 2003 dataset. Table 2 puts our
method in the context of earlier systems for CWS.
Our method achieves the best reported score on 6
out of 7 datasets.
5 Discussion and Error Analysis
On the whole, dual decomposition produces state-
of-the-art segmentations that are more accurate,
more consistent, and more successful at induc-
ing OOV words than the baseline systems that it
combines. On the SIGHAN 2005 test set, in
over 99.1% of cases the DD algorithm converged
within 100 iterations, which gives an optimality
guarantee. In 77.4% of the cases, DD converged
in the first iteration. The number of iterations to
convergence histogram is plotted in Figure 1.
SIGHAN 2005
AS PU CU MSR
Best 05 95.2 95.0 94.3 96.4
Zhang et al 06 94.7 94.5 94.6 96.4
Z&C 07 94.6 94.5 95.1 97.2
Sun et al 09 - 95.2 94.6 97.3
Sun 10 95.2 95.2 95.6 96.9
Dual-decomp 95.4 95.3 94.7 97.4
SIGHAN 2003
Best 03 96.1 95.1 94.0
Peng et al 04 95.6 94.1 92.8
Z&C 07 96.5 94.0 94.6
Dual-decomp 97.1 95.4 94.9
Table 2: Performance of dual decomposition in
comparison to past published results on SIGHAN
2003 and 2005 datasets. Best reported F
1
score
for each dataset is highlighted in bold. Z&C 07
refers to Zhang and Clark (2007). Best 03, 05 are
results of the winning systems for each dataset in
the respective shared tasks.
Error analysis In many cases the relative con-
fidence of each model means that dual decom-
position is capable of using information from
both sources to generate a series of correct
segmentations better than either baseline model
alone. The example below shows a difficult-to-
segment proper name comprised of common char-
acters, which results in undersegmentation by the
character-based CRF and oversegmentation by the
word-based perceptron, but our method achieves
the correct middle ground.
Gloss Tian Yage / ?s / creations
Gold ??? /? /??
CRF ???? /??
PCPT ?? /? /? /??
DD ??? /? /??
A powerful feature of the dual decomposition
approach is that it can generate correct segmenta-
tion decisions in cases where a voting or product-
of-experts model could not, since joint decod-
ing allows the sharing of information at decod-
ing time. In the following example, both baseline
models miss the contextually clear use of the word
?? (?sweets / snack food?) and instead attach?
to the prior word to produce the otherwise com-
mon compound ??? (?a little bit?); dual de-
composition allows the model to generate the cor-
rect segmentation.
Gloss Enjoy / a bit of / snack food / , ...
Gold ?? /?? /?? /?
CRF ?? /??? /? /?
PCPT ?? /??? /? /?
DD ?? /?? /?? /?
196
Figure 1: No. of iterations till DD convergence.
We found more than 400 such surprisingly ac-
curate instances in our dual decomposition output.
Finally, since dual decomposition is a method of
joint decoding, it is still liable to reproduce errors
made by the constituent systems.
6 Conclusion
In this paper we presented an approach to Chinese
word segmentation using dual decomposition for
system combination. We demonstrated that this
method allows for joint decoding of existing CWS
systems that is more accurate and consistent than
either system alone, and further achieves the best
performance reported to date on standard datasets
for the task. Perhaps most importantly, our ap-
proach is straightforward to implement and does
not require retraining of the underlying segmenta-
tion models used. This suggests its potential for
broader applicability in real-world settings than
existing approaches to combining character-based
and word-based models for Chinese word segmen-
tation.
Acknowledgements
We gratefully acknowledge the support of the U.S.
Defense Advanced Research Projects Agency
(DARPA) Broad Operational Language Transla-
tion (BOLT) program through IBM. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the view of DARPA, or
the US government.
References
Galen Andrew. 2006. A hybrid Markov/semi-Markov
conditional random field for sequence segmentation.
In Proceedings of EMNLP.
Ming-Hong Bai, Keh-Jiann Chen, and Jason S. Chang.
2008. Improving word alignment by adjusting chi-
nese word segmentation. In Proceedings of the third
International Joint Conference on Natural Lan-
guage Processing (IJCNLP).
Pichuan Chang, Michel Galley, and Chris Manning.
2008. Optimizing chinese word segmentation for
machine translation performance. In Proceedings of
the ACL Workshop on Statistical Machine Transla-
tion.
Keh-Jiann Chen and Shing-Huan Liu. 1992. Word
identification for mandarin chinese sentences. In
Proceedings of COLING.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP.
John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In Proceedings of ACL.
Thomas Emerson. 2005. The second international
Chinese word segmentation bakeoff. In Proceed-
ings of the fourth SIGHAN workshop on Chinese
language Processing.
Jianfeng Gao, Mu Li, and Chang-Ning Huang. 2003.
Improved source-channel models for Chinese word
segmentation. In Proceedings of ACL.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of EMNLP.
Jonathan K. Kummerfeld, Daniel Tse, James R. Cur-
ran, and Dan Klein. 2013. An empirical examina-
tion of challenges in chinese parsing. In Proceed-
ings of ACL-Short.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of 18th International
Conference on Machine Learning (ICML).
Dekang Lin. 2009. Combining language modeling and
discriminative classification for word segmentation.
In Proceedings of the 10th International Conference
on Intelligent Text Processing and Computational
Linguistics (CICLing).
Alexander M. Rush and Michael Collins. 2012. A tu-
torial on dual decomposition and Lagrangian relax-
ation for inference in natural language processing.
JAIR, 45:305?362.
197
Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natural
language processing. In Proceedings of EMNLP.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
crfs based joint decoding method for cascaded seg-
mentation and labeling tasks. In Proceedings of
Joint Conferences on Artificial Intelligence (IJCAI).
Richard Sproat and Thomas Emerson. 2003. The
first international Chinese word segmentation bake-
off. In Proceedings of the second SIGHAN work-
shop on Chinese language Processing.
Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-
masa Tsuruoka, and Jun?ichi Tsujii. 2009. A dis-
criminative latent variable chinese segmenter with
hybrid word/character information. In Proceedings
of HLT-NAACL.
Weiwei Sun. 2010. Word-based and character-
basedword segmentation models: Comparison and
combination. In Proceedings of COLING.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurasfky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the fourth SIGHAN
workshop on Chinese language Processing.
Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and
Xihong Wu. 2006. Chinese word segmentation with
maximum entropy and n-gram language model. In
Proceedings of the fifth SIGHAN workshop on Chi-
nese language Processing.
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010.
A character-based joint model for chinese word seg-
mentation. In Proceedings of COLING.
Mengqiu Wang, Wanxiang Che, and Christopher D.
Manning. 2013. Joint word alignment and bilingual
named entity recognition using dual decomposition.
In Proceedings of ACL.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. International Journal of Compu-
tational Linguistics and Chinese Language Process-
ing, pages 29?48.
Yue Zhang and Stephen Clark. 2007. Chinese seg-
mentation with a word-based perceptron algorithm.
In Proceedings of ACL.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro
Sumita. 2006. Subword-based tagging by condi-
tional random fields for Chinese word segmentation.
In Proceedings of HLT-NAACL.
198
Cross-lingual Projected Expectation Regularization for
Weakly Supervised Learning
Mengqiu Wang and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305 USA
{mengqiu,manning}@cs.stanford.edu
Abstract
We consider a multilingual weakly supervised
learning scenario where knowledge from an-
notated corpora in a resource-rich language
is transferred via bitext to guide the learning
in other languages. Past approaches project
labels across bitext and use them as features
or gold labels for training. We propose a
new method that projects model expectations
rather than labels, which facilities transfer
of model uncertainty across language bound-
aries. We encode expectations as constraints
and train a discriminative CRF model using
Generalized Expectation Criteria (Mann and
McCallum, 2010). Evaluated on standard
Chinese-English and German-English NER
datasets, our method demonstrates F1 scores
of 64% and 60% when no labeled data is
used. Attaining the same accuracy with su-
pervised CRFs requires 12k and 1.5k labeled
sentences. Furthermore, when combined with
labeled examples, our method yields signifi-
cant improvements over state-of-the-art super-
vised methods, achieving best reported num-
bers to date on Chinese OntoNotes and Ger-
man CoNLL-03 datasets.
1 Introduction
Supervised statistical learning methods have en-
joyed great popularity in Natural Language Process-
ing (NLP) over the past decade. The success of su-
pervised methods depends heavily upon the avail-
ability of large amounts of annotated training data.
Manual curation of annotated corpora is a costly and
time consuming process. To date, most annotated re-
sources resides within the English language, which
hinders the adoption of supervised learning methods
in many multilingual environments.
To minimize the need for annotation, significant
progress has been made in developing unsupervised
and semi-supervised approaches to NLP (Collins
and Singer 1999; Klein 2005; Liang 2005; Smith
2006; Goldberg 2010; inter alia) . More recent
paradigms for semi-supervised learning allow mod-
elers to directly encode knowledge about the task
and the domain as constraints to guide learning
(Chang et al., 2007; Mann and McCallum, 2010;
Ganchev et al., 2010). However, in a multilingual
setting, coming up with effective constraints require
extensive knowledge of the foreign1 language.
Bilingual parallel text (bitext) lends itself as a
medium to transfer knowledge from a resource-rich
language to a foreign languages. Yarowsky and Ngai
(2001) project labels produced by an English tag-
ger to the foreign side of bitext, then use the pro-
jected labels to learn a HMM model. More recent
work applied the projection-based approach to more
language-pairs, and further improved performance
through the use of type-level constraints from tag
dictionary and feature-rich generative or discrimina-
tive models (Das and Petrov, 2011; Ta?ckstro?m et al.,
2013).
In our work, we propose a new projection-based
method that differs in two important ways. First,
we never explicitly project the labels. Instead, we
project expectations over the labels. This projection
1For experimental purposes, we designate English as the
resource-rich language, and other languages of interest as ?for-
eign?. In our experiments, we simulate the resource-poor sce-
nario using Chinese and German, even though in reality these
two languages are quite rich in resources.
55
Transactions of the Association for Computational Linguistics, 2 (2014) 55?66. Action Editor: Lillian Lee.
Submitted 9/2013; Revised 12/2013; Published 2/2014. c?2014 Association for Computational Linguistics.
acts as a soft constraint over the labels, which al-
lows us to transfer more information and uncertainty
across language boundaries. Secondly, we encode
the expectations as constraints and train a model by
minimizing divergence between model expectations
and projected expectations in a Generalized Expec-
tation (GE) Criteria (Mann and McCallum, 2010)
framework.
We evaluate our approach on Named Entity
Recognition (NER) tasks for English-Chinese and
English-German language pairs on standard public
datasets. We report results in two settings: a weakly
supervised setting where no labeled data or a small
amount of labeled data is available, and a semi-
supervised settings where labeled data is available,
but we can gain predictive power by learning from
unlabeled bitext.
2 Related Work
Most semi-supervised learning approaches embody
the principle of learning from constraints. There are
two broad categories of constraints: multi-view con-
straints, and external knowledge constraints.
Examples of methods that explore multi-view
constraints include self-training (Yarowsky, 1995;
McClosky et al., 2006),2 co-training (Blum and
Mitchell, 1998; Sindhwani et al., 2005), multi-
view learning (Ando and Zhang, 2005; Carlson et
al., 2010), and discriminative and generative model
combination (Suzuki and Isozaki, 2008; Druck and
McCallum, 2010).
An early example of using knowledge as con-
straints in weakly-supervised learning is the work
by Collins and Singer (1999). They showed that the
addition of a small set of ?seed? rules greatly im-
prove a co-training style unsupervised tagger. Chang
et al. (2007) proposed a constraint-driven learning
(CODL) framework where constraints are used to
guide the selection of best self-labeled examples to
be included as additional training data in an iterative
EM-style procedure. The kind of constraints used
in applications such as NER are the ones like ?the
words CA, Australia, NY are LOCATION? (Chang
et al., 2007). Notice the similarity of this partic-
2A multi-view interpretation of self-training is that the self-
tagged additional data offers new views to learners trained on
existing labeled data.
ular constraint to the kinds of features one would
expect to see in a discriminative MaxEnt model.
The difference is that instead of learning the valid-
ity (or weight) of this feature from labeled exam-
ples ? since we do not have them ? we can con-
strain the model using our knowledge of the domain.
Druck et al. (2009) also demonstrated that in an ac-
tive learning setting where annotation budget is lim-
ited, it is more efficient to label features than ex-
amples. Other sources of knowledge include lexi-
cons and gazetteers (Druck et al., 2007; Chang et
al., 2007).
While it is straight-forward to see how resources
such as a list of city names can give a lot of mileage
in recognizing locations, we are also exposed to the
danger of over-committing to hard constraints. For
example, it becomes problematic with city names
that are ambiguous, such as Augusta, Georgia.3
To soften these constraints, Mann and McCallum
(2010) proposed the Generalized Expectation (GE)
Criteria framework, which encodes constraints as a
regularization term over some score function that
measures the divergence between the model?s ex-
pectation and the target expectation. The connection
between GE and CODL is analogous to the relation-
ship between hard (Viterbi) EM and soft EM, as il-
lustrated by Samdani et al. (2012).
Another closely related work is the Posterior
Regularization (PR) framework by Ganchev et al.
(2010). In fact, as Bellare et al. (2009) have shown,
in a discriminative model these two methods opti-
mize exactly the same objective.4 The two differ
in optimization details: PR uses a EM algorithm
to approximate the gradients which avoids the ex-
pensive computation of a covariance matrix between
features and constraints, whereas GE directly cal-
culates the gradient. However, later results (Druck,
2011) have shown that using the Expectation Semir-
ing techniques of Li and Eisner (2009), one can
compute the exact gradients of GE in a Conditional
Random Fields (CRF) (Lafferty et al., 2001) at costs
3This is a city in the state of Georgia in USA, famous for its
golf courses. It is ambiguous since both Augusta and Georgia
can also be used as person names.
4The different terminology employed by GE and PR may
be confusing to discerning readers, but the ?expectation? in the
context of GE means the same thing as ?marginal posterior? as
in PR.
56
no greater than computing the gradients of ordinary
CRF. And empirically, GE tends to perform more ac-
curately than PR (Bellare et al., 2009; Druck, 2011).
Obtaining appropriate knowledge resources for
constructing constraints remain as a bottleneck in
applying GE and PR to new languages. However,
a number of past work recognizes parallel bitext as a
rich source of linguistic constraints, naturally cap-
tured in the translations. As a result, bitext has
been effectively utilized for unsupervised multilin-
gual grammar induction (Alshawi et al., 2000; Sny-
der et al., 2009), parsing (Burkett and Klein, 2008),
and sequence labeling (Naseem et al., 2009).
A number of recent work also explored bilin-
gual constraints in the context of simultaneous bilin-
gual tagging, and showed that enforcing agreements
between language pairs give superior results than
monolingual tagging (Burkett et al., 2010; Che et
al., 2013; Wang et al., 2013a). Burkett et al. (2010)
also demonstrated a uptraining (Petrov et al., 2010)
setting where tag-induced bitext can be used as ad-
ditional monolingual training data to improve mono-
lingual taggers. A major drawback of this approach
is that it requires a readily-trained tagging models in
each languages, which makes a weakly supervised
setting infeasible. Another intricacy of this approach
is that it only works when the two models have com-
parable strength, since mutual agreements are en-
forced between them.
Projection-based methods can be very effective
in weakly-supervised scenarios, as demonstrated by
Yarowsky and Ngai (2001), and Xi and Hwa (2005).
One problem with projected labels is that they are
often too noisy to be directly used as training sig-
nals. To mitigate this problem, Das and Petrov
(2011) designed a label propagation method to au-
tomatically induce a tag lexicon for the foreign lan-
guage to smooth the projected labels. Fossum and
Abney (2005) filter out projection noise by com-
bining projections from from multiple source lan-
guages. However, this approach is not always viable
since it relies on having parallel bitext from multi-
ple source languages. Li et al. (2012) proposed the
use of crowd-sourced Wiktionary as additional re-
sources for inducing tag lexicons. More recently,
Ta?ckstro?m et al. (2013) combined token-level and
type-level constraints to constrain legitimate label
sequences and and recalibrate the probability distri-
bution in a CRF. The tag dictionary used for POS
tagging are analogous to the gazetteers and name
lexicons used for NER by Chang et al. (2007).
Our work is also closely related to Ganchev et
al. (2009). They used a two-step projection method
similar to Das and Petrov (2011) for dependency
parsing. Instead of using the projected linguis-
tic structures as ground truth (Yarowsky and Ngai,
2001), or as features in a generative model (Das
and Petrov, 2011), they used them as constraints
in a PR framework. Our work differs by project-
ing expectations rather than Viterbi one-best labels.
We also choose the GE framework over PR. Experi-
ments in Bellare et al. (2009) and Druck (2011) sug-
gest that in a discriminative model (like ours), GE
is more accurate than PR. More recently, Ganchev
and Das (2013) further extended this line of work
to directly train discriminative sequence models us-
ing cross lingual projection with PR. The types of
constraints applied in this new work are similar to
the ones in the monolingual PR setting proposed by
Ganchev et al. (2010), where the total counts of la-
bels of a particular kind are expected to match some
fraction of the projected total counts. Our work dif-
fer in that we enforce expectation constraints at to-
ken level, which gives tighter guidance to learning
the model.
3 Approach
Given bitext between English and a foreign lan-
guage, our goal is to learn a CRF model in the
foreign language from little or no labeled data.
Our method performs Cross-Lingual Projected
Expectation Regularization (CLiPER).
For every aligned sentence pair in the bitext, we
first compute the posterior marginal at each word po-
sition on the English side using a pre-trained English
CRF tagger; then for each aligned English word, we
project its posterior marginal as expectations to the
aligned word position on the foreign side. Figure 1
shows a snippet of a sentence from real corpus. No-
tice that if we were to directly project the Viterbi
best assignment from English to Chinese, all three
Chinese words that are named entities would have
gotten the wrong tags. But projecting the English
CRF model expectations preserves some uncertain-
ties, informing the Chinese model that there is a 40%
57
a reception in Luobu Linka . . . . . . met with representatives of Zhongguo Ribao
O:0.0032 O:0.0037 GPE:0.0000 GPE:0.0000PER:0.0000 PER:0.0000 PER:0.0000
GPE:0.0042 GPE:0.0042 LOC:0.0003 LOC:0.0003GPE:0.0000 GPE:0.0000 GPE:0.0000
ORG:0.0308 ORG:0.0307 O:0.0012 O:0.0011ORG:0.0000 ORG:0.0000 ORG:0.0000
LOC:0.3250 LOC:0.3256 ORG:0.4060 ORG:0.4061LOC:0.0000 LOC:0.0000 LOC:0.0000
PER:0.6369 PER:0.6377 PER:0.5925 PER:0.5925O:1.0000 O:1.0000 O:1.0000
? ???? ?? ? ??? . . . . . . ?? ? ?? ?? ??
PER:0.6373 PER:0.5925 PER:0.5925O:1.0000 O:1.0000 O:1.0000
LOC:0.3253 ORG:0.4060 ORG:0.4061LOC:0.0000 LOC:0.0000 LOC:0.0000
ORG:0.0307 O:0.0012 O:0.0011ORG:0.0000 ORG:0.0000 ORG:0.0000
GPE:0.0042 LOC:0.0003 LOC:0.0003GPE:0.0000 GPE:0.0000 GPE:0.0000
O:0.0035 GPE:0.0000 GPE:0.0000PER:0.0000 PER:0.0000 PER:0.0000
Figure 1: Diagram illustrating the projection of model expectation from English to Chinese. The posterior
probabilities assigned by the English CRF model is shown above each English word; automatically induced
word alignments are shown in red; the correct projected labels for Chinese words are shown in green, and
incorrect labels are shown in red.
chance that ?????? (China Daily) is an organi-
zation in this context.
We would like to learn a CRF model in the for-
eign language that has similar expectations as the
projected expectations from English. To this end,
we adopt the Generalized Expectation (GE) Crite-
ria framework introduced by Mann and McCallum
(2010). In the remainder of this section, we follow
the notation used in (Druck, 2011) to explain our ap-
proach.
3.1 CLiPER
The general idea of GE is that we can express our
preferences over models through constraint func-
tions. A desired model should satisfy the imposed
constraints by matching the expectations on these
constraint functions with some target expectations
(attained by external knowledge like lexicons or in
our case transferred knowledge from English). We
define a constraint function ?i,lj for each word po-sition i and output label assignment lj . ?i,lj = 0 isa constraint in that position i cannot take label lj .
The set {l1, ? ? ? , lm} denotes all possible label as-
signment for each yi, and m is number of label val-
ues. Ai is the set of English words aligned to Chi-
nese word i. ?i,lj are defined for all position i suchthat Ai 6= ?. In other words, the constraint function
applies only to Chinese word positions that have at
least one aligned English word. Each ?i,lj (y) can
be treated as a Bernoulli random variable, and we
concatenate the set of all ?i,lj into a random vector
?(y), where ?k = ?i,lj if k = i ?m + j. We drop
the (y) in ? for simplicity.
The target expectation over ?i,lj , denoted as ??i,lj ,
is the expectation of assigning label lj to English
word Ai under the English conditional probability
model. When multiple English words are aligned to
the same foreign word, we average the expectations.
The expectation over ? under a conditional prob-
ability model P (y|x;?) is denoted as EP (y|x;?)[?],
and simplified as E?[?] whenever it is unambigu-
ous.
The conditional probability model P (y|x;?) in
our case is defined as a standard linear-chain CRF:5
P (y|x;?) = 1Z(x;?)exp
( n?
i
?f(x, yi, yi?1)
)
where f is a set of feature functions; ? are the match-
ing parameters to learn; n = |x|.
The objective function to maximize in a standard
CRF is the log probability over a collection of la-
beled documents:
LCRF (?) =
a??
a=1
logP (y?a|xa;?) (1)
a? is the number of labeled sentences. y? is an ob-
served label sequence.
The objective function to maximize in GE is de-
fined as the sum over all unlabeled examples on the
5We simplify notation by dropping the L2 regularizer in the
CRF definition, but apply it in our experiments.
58
foreign side of bitext, denoted as xb, over some cost
function S between the model expectation over ?
(E?[?]) and the target expectation (??).
We choose S to be the negative L22 squared error
sum6 defined as:
LGE(?) =
n??
b=1
S
(
EP (yb|xb;?)[?(yb)], ??b
)
=
n??
b=1
????b ? E?[?(yb)]?22 (2)
n? is the total number of unlabeled bitext sentence
pairs.
When both labeled and bitext training data are
available, the joint objective is the sum of Eqn. 1
and 2. Each is computed over the labeled training
data and foreign half in the bitext, respectively.
We can optimize this joint objective by comput-
ing the gradients and use a gradient-based optimiza-
tion method such as L-BFGS. Gradients of LCRF
decomposes down to the gradients over each la-
beled training example (x,y?). Computing the gra-
dient of LGE decomposes down to the gradients of
S(EP (y|xb;?[?]) for each unlabeled foreign sentence
x and the constraints over this example ? . The gra-
dients can be calculated as:
?
??S(E?[?]) = ?
?
??
(
??? E?[?]
)T (
??? E?[?]
)
= 2
(
??? E?[?]
)T ( ?
??E?[?]
)
We redefine the penalty vector u = 2
(
??? E?[?]
)
to be u. ???E?[?] is a matrix where each columncontains the gradients for a particular model feature
? with respect to all constraint functions ?. It can be
6In general, other loss functions such as KL-divergence can
also be used for S. We found L22 to work well in practice.
computed as:
?
??E?[?] =
?
y
?(y) ???P (y|x;?)
=
?
y
?(y) ???
( 1
Z(x;?)exp(?
T f(x,y))
)
=
?
y
?(y)
(
1
Z(x;?)
( ?
?? exp(?
T f(x,y))
)
+ exp(?T f(x,y))
( ?
??
1
Z(x;?)
))
=
?
y
?(y)
(
P (y|x;?)f(x,y)T
? P (y|x;?)
?
y?
P (y?|x;?)f(x,y?)T
)
=
?
y
P (y|x;?)
?
y
?(y)f(x,y)T
?
(?
y
P (y|x;?)?(y)
)(?
y
P (y|x;?)f(x,y)T
)
= COVP (y|x;?) (?(y), f(x,y)) (3)
= E?[?fT ]? E?[?]E?[fT ] (4)
Eqn. 3 gives the intuition of how optimization works
in GE. In each iteration of L-BFGS, the model pa-
rameters are updated according to their covariance
with the constraint features, scaled by the differ-
ence between current expectation and target expec-
tation. The term E?[?fT ] in Eqn. 4 can be com-
puted using a dynamic programming (DP) algo-
rithm, but solving it directly requires us to store a
matrix of the same dimension as fT in each step
of the DP. We can reduce the complexity by using
the same trick as in (Li and Eisner, 2009) for com-
puting Expectation Semiring. The resulting algo-
rithm has complexity O(nm2), which is the same as
the standard forward-backward inference algorithm
for CRF. (Druck, 2011, 93) gives full details of this
derivation.
3.2 Hard vs. soft Projection
Projecting expectations instead of one-best label as-
signments from English to foreign language can
be thought of as a soft version of the method de-
scribed in (Das and Petrov, 2011) and (Ganchev et
59
al., 2009). Soft projection has its advantage: when
the English model is not certain about its predic-
tions, we do not have to commit to the current best
prediction. The foreign model has more freedom
to form its own belief since any marginal distribu-
tion it produces would deviates from a flat distri-
bution by just about the same amount. In general,
preserving uncertainties till later is a strategy that
has benefited many NLP tasks (Finkel et al., 2006).
Hard projection can also be treated as a special case
in our framework. We can simply recalibrate pos-
terior marginal of English by assigning probability
mass 1 to the most likely outcome, and zero ev-
erything else out, effectively taking the argmax of
the marginal at each word position. We refer to
this version of expectation as the ?hard? expecta-
tion. In the hard projection setting, GE training re-
sembles a ?project-then-train? style semi-supervised
CRF training scheme (Yarowsky and Ngai, 2001;
Ta?ckstro?m et al., 2013). In such a training scheme,
we project the one-best predictions of English CRF
to the foreign side through word alignments, then in-
clude the newly ?tagged? foreign data as additional
training data to a standard CRF in the foreign lan-
guage. Rather than projecting labels on a per-word
basis, Yarowsky and Ngai (2001) also explored an
alternative method for noun-phrase (NP) bracketing
task that amounts to projecting the spans of NPs
based on the observation that individual NPs tend to
retain their sequential spans across translations. We
experimented with the same method for NER, but
found that this method of projecting the NE spans
does not help in reducing noise and actually lowers
model performance.
Besides the difference in projecting expecta-
tions rather than hard labels, our method and the
?project-then-train? scheme also differ by optimiz-
ing different objectives: CRF optimizes maximum
conditional likelihood of the observed label se-
quence, whereas GE minimizes squared error be-
tween model?s expectation and ?hard? expectation
based on the observed label sequence. In the case
where squared error loss is replaced with a KL-
divergence loss, GE has the same effect as marginal-
izing out all positions with unknown projected la-
bels, allowing more robust learning of uncertainties
in the model. As we will show in the experimen-
O PER LOC ORG GPE
O 291339 391 141 1281 221
PER 1263 6721 5 56 73
LOC 409 23 546 123 133
ORG 2423 143 52 8387 196
GPE 566 239 69 668 6604
O PER LOC ORG MISC
O 81209 24 38 155 103
PER 77 5725 41 69 10
LOC 49 40 3743 127 60
ORG 178 102 142 4075 91
MISC 175 41 30 114 1826
Table 1: Raw counts in the error confusion matrix of
English CRF models. Top table contains the counts
on OntoNotes test data, and bottom table contains
CoNLL-03 test data counts. Rows are the true la-
bels and columns are the observed labels. For exam-
ple, item at row 2, column 3 of the top table reads:
we observed 5 times where the true label should be
PERSON, but English CRF model output label LO-
CATION.
tal results in Section 4.2, soft projection in combi-
nation of the GE objective significantly outperforms
the project-then-train style CRF training scheme.
3.3 Source-side noise
An additional source of noise comes from errors
generated by the source-side English CRF mod-
els. We know that the English CRF models gives
F1 score of 81.68% on the OntoNotes dataset for
English-Chinese experiment, and 90.45% on the
CoNLL-03 dataset for English-German experiment.
We present a simple way of modeling English-side
noise by picturing the following process: the la-
bels assigned by the English CRF model (denoted
as y) are some noised version of the true labels (de-
noted as y?). We can recover the probability of the
true labels by marginalizing over the observed la-
bels: P (y?|x) =?y P (y?|y) ? P (y|x). P (y|x) is
the posterior probabilities given by the CRF model,
and we can approximate P (y?|y) by the column-
normalized error confusion matrix shown in Table 1.
This source-side noise model is likely to be overly
simplistic. Generally speaking, we could build much
more sophisticated noising model for the source-
side, possibly conditioning on context, or capturing
higher-order label sequences.
60
4 Experiments
We conduct experiments on Chinese and German
NER. We evaluate CLiPER in two learning set-
tings: weakly supervised and semi-supervised. In
the weakly supervised setting, we simulate the con-
dition of having no labeled training data, and evalu-
ate the model learned from bitext alone. We then
vary the amount of labeled data available to the
model, and examine the model?s learning curve. In
the semi-supervised setting, we assume our model
has access to the full labeled data; our goal is to
improve performance of the supervised method by
learning from additional bitext.
4.1 Dataset and setup
We used the latest version of Stanford NER Toolkit7
as our base CRF model in all experiments. Fea-
tures for English, Chinese and German CRFs are
documented extensively in (Che et al., 2013) and
(Faruqui and Pado?, 2010) and omitted here for
brevity. It it worth noting that the current Stan-
ford NER models include recent improvements from
semi-supervise learning approaches that induces dis-
tributional similarity features from large word clus-
ters. These models represent the current state-of-
the-art in supervised methods, and serve as a very
strong baseline.
For Chinese NER experiments, we follow the
same setup as Che et al. (2013) to evaluate on the
latest OntoNotes (v4.0) corpus (Hovy et al., 2006).8
A total of 8,249 sentences from the parallel Chinese
and English Penn Treebank portion 9 are reserved
for evaluation. Odd-numbered documents are used
as development set, and even-numbered documents
are held out as blind test set. The rest of OntoNotes
annotated with NER tags are used to train the En-
glish and Chinese CRF base taggers. There are
about 16k and 39k labeled sentences for Chinese and
English training, respectively. The English CRF tag-
ger trained on this training corpus gives F1 score
of 81.68% on the OntoNotes test set. Four enti-
ties types10 are used for both Chinese and English
with a IO tagging scheme.11 The English-Chinese
7http://www-nlp.stanford.edu/ner
8LDC catalogue No.: LDC2011T03
9File numbers: chtb 0001-0325, ectb 1001-1078
10PERSON, LOCATION, ORGANIZATION and GPE.
11We did not adopt the commonly seen BIO tagging scheme
bitext comes from the Foreign Broadcast Informa-
tion Service corpus (FBIS).12 We randomly sampled
80k parallel sentence pairs to use as bitext in our
experiments. It is first sentence aligned using the
Champollion Tool Kit,13 then word aligned with the
BerkeleyAligner.14
For German NER experiments, we evaluate us-
ing the standard CoNLL-03 NER corpus (Sang and
Meulder, 2003). The labeled training set has 12k and
15k sentences, containing four entity types.15 An
English CRF model is also trained on the CoNLL-
03 English data with the same entity types. For bi-
text, we used a randomly sampled set of 40k parallel
sentences from the de-en portion of the News Com-
mentary dataset.16 The English CRF tagger trained
on CoNLL-03 English training corpus gives F1 score
of 90.4% on the CoNLL-03 test set.
We report typed entity precision (P), recall (R)
and F1 score. Statistical significance tests are done
using a paired bootstrap resampling method with
1000 iterations, averaged over 5 runs. We com-
pare against three recently approaches that were in-
troduced in Section 2. They are: semi-supervised
learning method using factored bilingual models
with Gibbs sampling (Wang et al., 2013a); bilin-
gual NER using Integer Linear Programming (ILP)
with bilingual constraints, by (Che et al., 2013);
and constraint-driven bilingual-reranking approach
(Burkett et al., 2010). The code from (Che et al.,
2013) and (Wang et al., 2013a) are publicly avail-
able.17 Code from (Burkett et al., 2010) is obtained
through personal communications.
Since the objective function in Eqn. 2 is non-
convex, we adopted the early stopping training
scheme from (Turian et al., 2010) as the following:
after each iteration in L-BFGS training, the model
(Ramshaw and Marcus, 1999), because when projected across
swapping word alignments, the ?B-? and ?I-? tag distinction
may not be well-preserved and may introduce additional noise.
12The FBIS corpus is a collection of radio news casts and
contains translations of openly available news and information
from media sources outside the United States. The LDC cata-
logue No. is LDC2003E14.
13champollion.sourceforge.net
14code.google.com/p/berkeleyaligner
15PERSON, LOCATION, ORGANIZATION and MISCELLA-
NEOUS.
16http://www.statmt.org/wmt13/
training-parallel-nc-v8.tgz
17https://github.com/stanfordnlp/CoreNLP
61
is evaluated against the development set; the train-
ing procedure is terminated if no improvements have
been made in 20 iterations.
4.2 Weakly supervised results
Figure 2a and 2b show results of weakly supervised
learning experiments. Quite remarkably, on Chinese
test set, our proposed method (CLiPER) achieves a
F1 score of 64.4% with 80k bitext, when no labeled
training data is used. In contrast, the supervised
CRF baseline would require as much as 12k labeled
sentences to attain the same accuracy. Results on the
German test set is less striking. With no labeled data
and 40k of bitext, CLiPER performs at F1 of 60.0%,
the equivalent of using 1.5k labeled examples in the
supervised setting. When combined with 1k labeled
examples, performance of CLiPER reaches 69%, a
gain of over 5% absolute over supervised CRF. We
also notice that supervised CRF model learns much
faster in German than Chinese. This result is not too
surprising, since it is well recognized that Chinese
NER is more challenging than German or English.
The best supervised results for Chinese is 10-20%
(F1 score) behind best German and English super-
vised results. Chinese NER relies more on lexical-
ized features, and therefore needs more labeled data
to achieve good coverage. The results suggest that
CLiPER seems to be very effective at transferring
lexical knowledge from English to Chinese.
Figure 2c and 2d compares soft GE projection
with hard GE projection and the ?project-then-train?
style CRF training scheme (cf. Section 3.2). We
observe that both soft and hard GE projection sig-
nificantly outperform the ?project-then-train? style
training scheme. The difference is especially pro-
nounced on the Chinese results when fewer labeled
examples are available. Soft projection gives better
accuracy than hard projection when no labeled data
is available, and also has a faster learning rate.
Incorporating source-side noise using the method
described in Section 3.3 gives a small improvement
on Chinese with supervised data, increasing F1 score
from 64.40% to 65.50%. This improvement is statis-
tically significant at 92% confidence interval. How-
ever, on the German data, we observe a tiny de-
crease with no statistical significance in F1 score,
dropping from 59.88% to 59.66%. A likely ex-
planation of the difference is that the English CRF
model in the English-Chinese experiment, which is
trained on OntoNotes data, has a much higher error
rate (18.32%) than the English CRF model in the
English-German experiment trained on CoNLL-03
(9.55%). Therefore, modeling noise in the English-
Chinese case is likely to have a greater effect than
the English-German case.
4.3 Semi-supervised results
In the semi-supervised experiments, we let the CRF
model use the full set of labeled examples in addi-
tion to the unlabeled bitext. Results on the test set
are shown in Table 2. All semi-supervised baselines
are tested with the same number of unlabeled bitext
as CLiPER in each language. The ?project-then-
train? semi-supervised training scheme severely
hurts performance on Chinese, but gives a small im-
provement on German. Moreover, on Chinese it
learns to achieve high precision but at a significant
loss in recall. On German its behavior is the oppo-
site. Such drastic and erratic imbalance suggest that
this method is not robust or reliable. The other three
semi-supervised baselines (row 3-5) all show im-
provements over the CRF baseline, consistent with
their reported results. CLIPERs gives the best re-
sults on both Chinese and German, yielding statis-
tically significant improvements over all baselines
except for CWD13 on German. The hard projection
version of CLiPER also gives sizable gain over CRF.
However, in comparison, CLIPERs is superior.
The improvements of CLIPERs over CRF on
Chinese test set is over 2.8% in absolute F1. The
improvement over CRF on German is almost a per-
cent. To our knowledge, these are the best reported
numbers on the OntoNotes Chinese and CoNLL-03
German datasets.
4.4 Efficiency
Another advantage of our proposed approach is ef-
ficiency. Because we eliminated the previous multi-
stage ?uptraining? paradigm, but instead integrating
the semi-supervised and supervised objective into
one joint objective, we are able to attain signifi-
cant speed improvements over all methods except
CRFptt. Table 3 shows the required training time.
62
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
0
10
20
30
40
50
60
70
80
# of labeled training sentences [k]
F1sc
ore[%
]
supervised CRFCLiPPER soft
(a) Chinese Test
0 1 2 3 4 5 6 7 8 9 10 11 12
0
10
20
30
40
50
60
70
80
# of labeled training sentences [k]
F1sc
ore[%
]
supervised CRFCLiPPER soft
(b) German Test
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
4446
4850
5254
5658
6062
6466
6870
7274
76
# of labeled training sentences [k]
F1sc
ore[%
]
CRF projectionCLiPPER hard
CLiPPER soft
(c) Soft vs. Hard on Chinese Test
0 1 2 3 4 5 6 7 8 9 10 11 1254
56
58
60
62
64
66
68
70
72
74
76
78
80
# of labeled training sentences [k]
F1sc
ore[%
]
CRF projectionCLiPPER hard
CLiPPER soft
(d) Soft vs. Hard on German Test
[??] ??? ? [??] ??
A monument commemorating [Vice President Gao GangPER ] was completed in [HengshanLOC ]
(e) Word proceeding ?monument? is PERSON
[??] [???] ?? [??] ??? ??
Introduction of [QikouLOC ] [Chairman MaoPER ] [Yellow RiverLOC ] crossing monument
(f) Word proceeding ?monument? is LOCATION
Figure 2: Top four figures show performance curves of CLiPER with varying amounts of available labeled
training data in a weakly supervised setting. Vertical axes show the F1 score on the test set. Performance
curves of supervised CRF and ?project-then-train? CRF are plotted for comparison. Bottom two figures are
examples of aligned sentence pairs in Chinese and English.
63
Chinese German
P R F1 P R F1
CRF 79.09 63.59 70.50 86.69 71.30 78.25
CRFptt 84.01 45.29 58.85 81.50 75.56 78.41
BPBK10 79.25 65.67 71.83 84.00 72.17 77.64
CWD13 81.31 65.50 72.55 85.99 72.98 78.95
WCD13a 80.31 65.78 72.33 85.98 72.37 78.59
WCD13b 78.55 66.54 72.05 85.19 72.98 78.62
CLiPERh 83.67 64.80 73.04?? 86.52 72.02 78.61?
CLiPERs 82.57 65.99 73.35???? 87.11 72.56 79.17????
Table 2: Test set Chinese, German NER results.
Best number of each column is highlighted in
bold. CRF is the supervised baseline. CRFptt is
the ?project-then-train? semi-supervised scheme for
CRF. BPBK10 is (Burkett et al., 2010), WCD13 is
(Wang et al., 2013a), CWD13A is (Che et al., 2013),
and WCD13B is (Wang et al., 2013b) . CLIPERs
and CLIPERh are the soft and hard projections. ?
indicates F1 scores that are statistically significantly
better than CRF baseline at 99.5% confidence level;
? marks significance over CRFptt with 99.5% con-
fidence; ? and ? marks significance over WCD13
with 99.9% and 94% confidence; and  marks sig-
nificance over CWD13 with 99.7% confidence; ?
marks significance over BPBK10 with 99.9% con-
fidence.
5 Discussions
Figure 2e and 2f give two examples of cross-lingual
projection methods in action. Both examples have
a named entity that immediately proceeds the word
????? (monument) in the Chinese sentence. In
Figure 2e, the word ???? has literal meaning of a
hillock located at a high position, which also hap-
pens to be the name of a former vice president of
China. Without having previously observed this
word as a person name in the labeled training data,
the CRF model does not have enough evidence to
believe that this is a PERSON, instead of LOCATION.
But the aligned words in English (?Gao Gang?) are
clearly part of a person name as they were pre-
ceded by a title (?Vice President?). The English
model has high expectation that the aligned Chi-
nese word of ?Gao Gang? is also a PERSON. There-
fore, projecting the English expectations to Chinese
provides a strong clue to help disambiguating this
word. Figure 2f gives another example: the word
????(Huang He, the Yellow River of China) can
Chinese German
CRF 19m30s 7m15s
CRFptt 34m2s 12m45s
WCD13 3h17m 1h1m
CWD13a 16h42m 4h49m
CWD13b 16h42m 4h49m
BPBK10 6h16m 2h42m
CLiPERh 1h28m 16m30s
CLiPERs 1h40m 18m51s
Table 3: Timing stats during model training.
be confused with a person name since ???(Huang
or Hwang) is also a common Chinese last name.18.
Again, knowing the translation in English, which
has the indicative word ?River? in it, helps disam-
biguation.
The CRFptt and CLIPERh methods successfully
labeled these two examples correctly, but failed to
produce the correct label for the example in Fig-
ure 1. On the other hand, a model trained with the
CLIPERs method does correctly label both entities
in Figure 1, demonstrating the merits of the soft pro-
jection method.
6 Conclusion
We introduced a domain and language independent
semi-supervised method for training discriminative
models by projecting expectations across bitext. Ex-
periments on Chinese and German NER show that
our method, learned over bitext alone, can rival per-
formance of supervised models trained with thou-
sands of labeled examples. Furthermore, applying
our method in a setting where all labeled examples
are available also shows improvements over state-of-
the-art supervised methods. Our experiments also
showed that soft expectation projection is more fa-
vorable to hard projection. This technique can be
generalized to all sequence labeling tasks, and can
be extended to include more complex constraints.
For future work, we plan to apply this method to
more language pairs and also explore data selection
strategies and modeling alignment uncertainties.
18In fact, a people search of the name?? on the most pop-
ular Chinese social network (renren.com) returns over 13,000
matches.
64
Acknowledgments
The authors would like to thank Jennifer Gillenwa-
ter for a discussion that inspired this work, Behrang
Mohit and Nathan Schneider for their help with the
Arabic NER data, and David Burkett for providing
the source code of their work for comparison. We
would also like to thank editor Lillian Lee and the
three anonymous reviewers for their valuable com-
ments and suggestions. We gratefully acknowledge
the support of the U.S. Defense Advanced Research
Projects Agency (DARPA) Broad Operational Lan-
guage Translation (BOLT) program through IBM.
Any opinions, findings, and conclusion or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the view of
DARPA, or the US government.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Head-transducer models for speech translation
and their automatic acquisition from bilingual data.
Machine Translation, 15.
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for text
chunking. In Proceedings of ACL.
Kedar Bellare, Gregory Druck, and Andrew McCallum.
2009. Alternating projections for learning with expec-
tation constraints. In Proceedings of UAI.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of COLT.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of EMNLP.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010. Learning better monolingual models with unan-
notated bilingual text. In Proceedings of CoNLL.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of WSDM.
Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In Proceedings of ACL.
Wanxiang Che, Mengqiu Wang, and Christopher D. Man-
ning. 2013. Named entity recognition with bilingual
constraints. In Proceedings of NAACL.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL.
Gregory Druck and Andrew McCallum. 2010. High-
performance semi-supervised learning using discrim-
inatively constrained generative models. In Proceed-
ings of ICML.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2007. Leveraging existing resources using generalized
expectation criteria. In Proceedings of NIPSWorkshop
on Learning Problem Design.
Gregory Druck, Burr Settles, and Andrew McCallum.
2009. Active learning by labeling features. In Pro-
ceedings of EMNLP.
Gregory Druck. 2011. Generalized Expectation Criteria
for Lightly Supervised Learning. Ph.D. thesis, Univer-
sity of Massachusetts Amherst.
Manaal Faruqui and Sebastian Pado?. 2010. Training and
evaluating a German named entity recognizer with se-
mantic generalization. In Proceedings of KONVENS.
Jenny Rose Finkel, Christopher D. Manning, and An-
drew Y. Ng. 2006. Solving the problem of cascading
errors: Approximate bayesian inference for linguistic
annotation pipelines. In Proceedings of EMNLP.
Victoria Fossum and Steven Abney. 2005. Automatically
inducing a part-of-speech tagger by projecting from
multiple source languages across aligned corpora. In
Proceedings of IJCNLP.
Kuzman Ganchev and Dipanjan Das. 2013. Cross-
lingual discriminative learning of sequence models
with posterior regularization. In Proceedings of
EMNLP.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In Proceedings of ACL.
Kuzman Ganchev, Jo ao Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. JMLR, 10:2001?2049.
Andrew B. Goldberg. 2010. New Directions in Semi-
supervised Learning. Ph.D. thesis, University of
Wisconsin-Madison.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% solution. In Proceedings of NAACL-HLT.
Dan Klein. 2005. The Unsupervised Learning of Natural
Language Structure. Ph.D. thesis, Stanford Univer-
sity.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of ICML.
65
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
EMNLP.
Shen Li, Jo ao Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings of
EMNLP-CoNLL.
Percy Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
Gideon Mann and Andrew McCallum. 2010. General-
ized expectation criteria for semi-supervised learning
with weakly labeled data. JMLR, 11:955?984.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of NAACL-HLT.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein,
and Regina Barzilay. 2009. Multilingual part-of-
speech tagging: Two unsupervised approaches. JAIR,
36:1076?9757.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate deter-
ministic question parsing. In Proceedings of EMNLP.
Lance A. Ramshaw and Mitchell P. Marcus. 1999. Text
chunking using transformation-based learning. Natu-
ral Language Processing Using Very Large Corpora,
11:157?176.
Rajhans Samdani, Ming-Wei Chang, and Dan Roth.
2012. Unified expectation maximization. In Proceed-
ings of NAACL.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: language-
independent named entity recognition. In Proceedings
of CoNLL.
Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin.
2005. A co-regularization approach to semi-
supervised learning with multiple views. In Proceed-
ings of ICML Workshop on Learning with Multiple
Views, International Conference on Machine Learn-
ing.
Noah A. Smith. 2006. Novel Estimation Methods for
Unsupervised Discovery of Latent Structure in Natu-
ral Language Text. Ph.D. thesis, Johns Hopkins Uni-
versity.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In Proceedings of ACL.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In Proceedings of ACL.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan Mc-
Donald, and Joakim Nivre. 2013. Token and type
constraints for cross-lingual part-of-speech tagging. In
Proceedings of ACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of ACL.
Mengqiu Wang, Wanxiang Che, and Christopher D. Man-
ning. 2013a. Effective bilingual constraints for semi-
supervised learning of named entity recognizers. In
Proceedings of AAAI.
Mengqiu Wang, Wanxiang Che, and Christopher D. Man-
ning. 2013b. Joint word alignment and bilingual
named entity recognition using dual decomposition.
In Proceedings of ACL.
Chenhai Xi and Rebecca Hwa. 2005. A backoff model
for bootstrapping resources for non-english languages.
In Proceedings of HLT-EMNLP.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
NAACL.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of ACL.
66
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 648?654,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Stanford: Probabilistic Edit Distance Metrics for STS
Mengqiu Wang and Daniel Cer?
Computer Science Department
Stanford University
Stanford, CA 94305 USA
{mengqiu,danielcer}@cs.stanford.edu
Abstract
This paper describes Stanford University?s
submission to SemEval 2012 Semantic Tex-
tual Similarity (STS) shared evaluation task.
Our proposed metric computes probabilistic
edit distance as predictions of semantic sim-
ilarity. We learn weighted edit distance in
a probabilistic finite state machine (pFSM)
model, where state transitions correspond to
edit operations. While standard edit dis-
tance models cannot capture long-distance
word swapping or cross alignments, we rectify
these shortcomings using a novel pushdown
automaton extension of the pFSM model. Our
models are trained in a regression framework,
and can easily incorporate a rich set of lin-
guistic features. The performance of our edit
distance based models is contrasted with an
adaptation of the Stanford textual entailment
system to the STS task. Our results show that
the most advanced edit distance model, pPDA,
outperforms our entailment system on all but
one of the genres included in the STS task.
1 Introduction
We describe a probabilistic edit distance based met-
ric, which was originally designed for evaluating
machine translation quality, for computing seman-
tic textual similarity (STS). This metric models
weighted edit distance in a probabilistic finite state
machine (pFSM), where state transitions correspond
to edit operations. The weights of the edit op-
erations are automatically learned in a regression
framework. One of the major contributions of this
? Daniel Cer is one of the organizers for the STS task. The
STS test set data was not used in any way for the development
or training of the systems described in this paper.
paper is a novel extension of the pFSM model into a
probabilistic Pushdown Automaton (pPDA), which
enhances traditional edit-distance models with the
ability to model phrase shift and word swapping.
Furthermore, we give a new log-linear parameteri-
zation to the pFSM model, which allows it to easily
incorporate rich linguistic features. We contrast the
performance of our probabilistic edit distance metric
with an adaptation of the Stanford textual entailment
system to the STS task.
2 pFSMs for Semantic Textual Similarity
We start off by framing the problem of semantic tex-
tual similarity in terms of weighted edit distance cal-
culated using probabilistic finite state machines (pF-
SMs). A FSM defines a language by accepting a
string of input tokens in the language, and reject-
ing those that are not. A probabilistic FSM defines
the probability that a string is in a language, extend-
ing on the concept of a FSM. Commonly used mod-
els such as HMMs, n-gram models, Markov Chains
and probabilistic finite state transducers all fall in
the broad family of pFSMs (Knight and Al-Onaizan,
1998; Eisner, 2002; Kumar and Byrne, 2003; Vi-
dal et al, 2005). Unlike all the other applications
of FSMs where tokens in the language are words, in
our language tokens are edit operations. A string of
tokens that our FSM accepts is an edit sequence that
transforms one side of the sentence pair (denoted as
s1) into the other side (s2).
Our pFSM has a unique start and stop state, and
one state per edit operation (i.e., Insert, Delete, Sub-
stitution). The probability of an edit sequence e is
generated by the model is the product of the state
transition probabilities in the pFSM, formally de-
648
Figure 1: This diagram illustrates an example sentence pair from the statistical machine translation subtask of STS.
The three rows below are the best state transition (edit) sequences that transforms REF to SYS, according to the basic
pFSM model, the extended pPDA model, and pPDA model with synonym and paraphrase linguistic features. The
corresponding alignments generated by the models (pFSM, pPDA, pPDA+f ) are shown with different styled lines,
with later models in the order generating strictly more alignments than earlier ones. The gold human evaluation score
is 6.5, and model predictions are: pPDA+f 5.5, pPDA 4.3, pFSM 3.1.
scribed as:
w(e | s1,s2) = ?
|e|
i=1 exp ? ? f(ei?1,ei,s1,s2)
Z
(1)
We featurize each of the state changes with a log-
linear parameterization; f is a set of binary feature
functions defined over pairs of neighboring states
(by the Markov assumption) and the input sentences,
and ? are the associated feature weights; Z is a parti-
tion function. In this basic pFSM model, the feature
functions are simply identity functions that emit the
current state, and the state transition sequence of the
previous state and the current state.
The feature weights are then automatically
learned by training a global regression model where
the human judgment score for each sentence pair is
the regression target (y?). Since the ?gold? edit se-
quence are not given at training or prediction time,
we treat the edit sequences as hidden variables and
sum over them in our model. We introduce a new
regression variable y ? R which is the log-sum of
the unnormalized weights (Eqn. (1)) of all edit se-
quences, formally expressed as:
y = log
?
e??e?
|e? |
?
i=1
exp ? ? f(ei?1,ei,s1,s2) (2)
e? is the set of all possible alignments. The sum
over an exponential number of edit sequences in e?
is solved efficiently using a forward-backward style
dynamic program. Any edit sequence that does not
lead to a complete transformation of the sentence
pair has a probability of zero in our model. Our
regression target then seeks to minimize the least
squares error with respect to y?, plus a L2-norm regu-
larizer term parameterized by ? :
?
? = min
?
{
?
s1i ,s2i
[y?i ? (
y
|s1i |+ |s2i |
+?)]2 +????2}
(3)
The |s1i |+ |s2i | is a length normalization term for
the ith training instance, and ? is a scaling con-
stant whose value is to be learned. At test time,
y/(|s1|+ |s2|) + ? is computed as the predicted
score.
We replaced the standard substitution edit oper-
ation with three new operations: Sword for same
word substitution, Slemma for same lemma substitu-
tion, and Spunc for same punctuation substitution. In
other words, all but the three matching-based substi-
tutions are disallowed. The start state can transition
into any of the edit states with a constant unit cost,
and each edit state can transition into any other edit
state if and only if the edit operation involved is valid
at the current edit position (e.g., the model cannot
transition into Delete state if it is already at the end
649
of s1; similarly it cannot transition into Slemma unless
the lemma of the two words under edit in s1 and s2
match). When the end of both sentences are reached,
the model transitions into the stop state and ends
the edit sequence. The first row in Figure 1 start-
ing with pFSM shows a state transition sequence for
an example sentence pair. 1 There exists a one-to-
one correspondence between substitution edits and
word alignments. Therefore this example state tran-
sition sequence correctly generates an alignment for
the word 43 and people.
2.1 pPDA Extension
A shortcoming of edit distance models is that they
cannot handle long-distance word swapping ? a
pervasive phenomenon found in most natural lan-
guages. 2 Edit operations in standard edit distance
models need to obey strict incremental order in
their edit position, in order to admit efficient dy-
namic programming solutions. The same limitation
is shared by our pFSM model, where the Markov
assumption is made based on the incremental or-
der of edit positions. Although there is no known
solution to the general problem of computing edit
distance where long-distance swapping is permit-
ted (Dombb et al, 2010), approximate algorithms do
exist. We present a simple but novel extension of the
pFSM model to a probabilistic pushdown automa-
ton (pPDA), to capture non-nested word swapping
within limited distance, which covers a majority of
word swapping in observed in real data (Wu, 2010).
A pPDA, in its simplest form, is a pFSM where
each control state is equipped with a stack (Esparza
and Kucera, 2005). The addition of stacks for each
transition state endows the machine with memory,
extending its expressiveness beyond that of context-
free formalisms. By construction, at any stage in a
normal edit sequence, the pPDA model can ?jump?
forward within a fixed distance (controlled by a max
distance parameter) to a new edit position on either
side of the sentence pair, and start a new edit subse-
quence from there. Assuming the jump was made on
1It is safe to ignore the second and third row in Figure 1 for
now, their explanations are forthcoming in Section 2.1.
2The edit distance algorithm described in Cormen et
al. (2001) can only handle adjacent word swapping (transpo-
sition), but not long-distance swapping.
the s2 side, 3 the machine remembers its current edit
position in s2 as Jstart , and the destination position
on s2 after the jump as Jlanding.
We constrain our model so that the only edit op-
erations that are allowed immediately following a
?jump? are from the set of substitution operations
(e.g., Sword). And after at least one substitution
has been made, the device can now ?jump? back to
Jstart , remembering the current edit position as Jend .
Another constraint here is that after the backward
?jump?, all edit operations are permitted except for
Delete, which cannot take place until at least one
substitution has been made. When the edit sequence
advances to position Jlanding, the only operation al-
lowed at that point is another ?jump? forward opera-
tion to position Jend , at which point we also clear all
memory about jump positions and reset.
An intuitive explanation is that when pPDA
makes the first forward jump, a gap is left in s2 that
has not been edited yet. It remembers where it left
off, and comes back to it after some substitutions
have been made to complete the edit sequence. The
second row in Figure 1 (starting with pPDA) illus-
trates an edit sequence in a pPDA model that in-
volves three ?jump? operations, which are annotated
and indexed by number 1-3 in the example. ?Jump
1? creates an un-edited gap between word 43 and
western, after two substitutions, the model makes
?jump 2? to go back and edit the gap. The only edit
permitted immediately after ?jump 2? is deleting the
comma in s1, since inserting the word 43 in s2 before
any substitution is disallowed. Once the gap is com-
pleted, the model resumes at position Jend by making
?jump 3?, and completes the jump sequence.
The ?jumps? allowed the model to align words
such as western India, in addition to the alignments
of 43 people found by the pFSM. In practice, we
found that our extension gives a big boost to model
performance (cf. Section 4), with only a modest in-
crease in computation time. 4
3Recall that we transform s1 into s2, and thus on the s2 side,
we can only insert but not delete. The argument applies equally
to the case where the jump was made on the other side.
4The length of the longest edit sequence with jumps only
increased by 0.5 ?max(|s1|, |s2|) in the worst case, and by and
large swapping is rare in comparison to basic edits.
650
Figure 2: Stanford Entailment Recognizer: The pipelined approach used by the Stanford entailment recognizer to
analyze sentence pairs and determine whether or not an entailment relationship is present. The entailment recognizer
first obtains dependency parses for both the passage and the hypothesis. These parses are then aligned based upon
lexical and structural similarity between the two dependency graphs. From the aligned graphs, features are extracted
that suggest the presence or absence of an entailment relationship. Figure courtesy of (Pado et al, 2009).
2.2 Parameter Estimation
Since the least squares operator preserves convexity,
and the inner log-sum-exponential function is con-
vex, the resulting objective function is also convex.
For parameter learning, we used the limited mem-
ory quasi-newton method (Liu and Nocedal, 1989)
to find the optimal feature weights and scaling con-
stant for the objective. We initialized ? = 0?, ? = 0,
and ? = 5. We also threw away features occurring
fewer than five times in training corpus. Gradient
calculation was similar to other pFSM models, such
as HMMs, we omitted the details here, for brevity.
2.3 Rich Linguistic Features
We add new substitution operations beyond those
introduced in Section 2, to capture synonyms and
paraphrase in the sentence pair. Synonym rela-
tions are defined according to WordNet (Miller et
al., 1990), and paraphrase matches are given by a
lookup table. To better take advantage of paraphrase
information at the multi-word phrase level, we ex-
tended our substitution operations to match longer
phrases by adding one-to-many and many-to-many
bigram block substitutions. In our experiments on
machine translation evaluation task, which our met-
ric was originally developed for, we found that most
of the gain came from unigrams and bigrams, with
little to no additional gains from trigrams. There-
fore, we limited our experiments to bigram pFSM
and pPDA models, and pruned the paraphrase table
adopted from TERplus 5 to unigrams and bigrams,
resulting in 2.5 million paraphrase pairs. Trained on
all available training data, the resulting pPDA model
has a total of 218 features.
2.4 Model Configuration
We evaluate both the pFSM and pPDA models with
the addition of rich linguistic features, as described
in the previous section. For pPDA model, the jump
distance is set to five. For each model, we experi-
mented with two different training schemes. In the
5Available from www.umiacs.umd.edu/~snover/terp.
651
HYP: Virus was infected.
REF: No one was infected by the virus.
no entailment no entailment
HYP: The virus did not infect anybody.
REF: No one was infected by the virus.
entailment entailment
Figure 3: Semantic similarity as determined by mutual textual entailment. Figure courtesy of (Pado et al, 2009).
first scheme, we train a separate model for each sec-
tion of the training dataset (i.e., MSRpar, MSRvid,
and SMTeuroparl), and use that model to test on
their respective test set. For the two unseen test
sets (SMTnews and OnWN), we used a joint model
trained on all of the available training data. We re-
fer to this scheme as Indi henceforth. In the second
scheme, we used the joint model trained on all train-
ing data to make preditions for all test sets (we refer
to this scheme as All). Our official submission con-
tains two runs ? pFSM with scheme Indi, and pPDA
with scheme All.
3 Textual Entailment for STS
We contrast the performance of the probabilistic edit
distance metrics with an adaptation of the Stanford
Entailment Recognizer to the STS task. In this sec-
tion, we review the textual entailment task, the op-
eration of the Stanford Entailment Recognizer, and
describe how we adapted our entailment system to
the STS task.
3.1 Recognizing Textual Entailment
The Recognizing Textual Entailment (RTE) task
(Dagan et al, 2005) involves determining whether
the meaning of one text can be inferred from an-
other. The text providing the ground truth for the
evaluation is known as the passage while the text
being tested for entailment is known as the the hy-
pothesis. A passage entails a hypothesis if a casual
speaker would consider the inference to be correct.
This intentionally side-steps strict logical entailment
and implicitly brings in all of the world knowledge
speakers use to interpret language.
The STS task and RTE differ in two significant
ways. First, the RTE task is one directional. If a
hypothesis sentence is implied by a passage, the in-
verse does not necessarily hold (e.g., ?John is out-
side in the snow without a coat.? casually implies
?John is cold?, but not vice versa). Second, the RTE
task forces systems to make a boolean choice about
entailment, rather than the graded scale of semantic
relatedness implied by STS.
3.2 Textual Entailment System Description
Shown in Figure 2, the Stanford entailment sys-
tem uses a linguistically rich multi-stage annotation
pipeline. Incoming sentence pairs are first depen-
dency parsed. The dependency parse trees are then
transformed into semantic graphs containing addi-
tional annotations such as named entities and coref-
erence. The two semantic graphs are then aligned
based upon structural overlap and lexical semantic
similarity using a variety of word similarity metrics
based on WordNet, vector space distributional sim-
ilarity as calculated by InfoMap, and a specialized
module for matching ordinal values. The system
then supplies the aligned semantic graphs as input to
a number of feature producing modules. Some mod-
ules produce gross aggregate scores, such as return-
ing the alignment quality between the two sentences.
Others look for specific phenomena that suggest the
presence or absence of an entailment relationship,
such as a match or mismatch in polarity (e.g., ?died?
vs. ?didn?t die?), tense, quantification, and argument
structure. The resulting features are then passed on
to a down stream classifier to predict whether or not
an entailment relationship exists.
3.3 Adapting RTE to STS
In order to adapt our entailment recognition sys-
tem to STS, we follow the same approach Pado
et al (2009) used to successfully adapt the entail-
ment system to machine translation evaluation. As
shown in Figure 3, for each pair of sentences pre-
sented to the system, we run the entailment system
in both directions and extract features that describe
whether the first sentence entails the second and vice
versa for the opposite direction. This setup effec-
tively treats the STS task as a bidirectional variant
of the RTE task. The extracted bidirectional entail-
ment features are then passed on to a support vec-
652
Models All MSRpar MSRvid SMTeuro OnWn SMTnews
pFSMIndi 0.6354(38) 0.3795 0.5350 0.4377 - -
pFSMAll 0.3727 0.3769 0.4569 0.4256 0.6052 0.4164
pPDAIndi 0.6808 0.4244 0.5051 0.4554 - -
pPDAAll 0.4229(77) 0.4409 0.4698 0.4558 0.6468 0.4769
Entailment 0.5589(55) 0.4374 0.8037 0.3533 0.3077 0.3235
Table 1: Absolute score prediction results on STS12 test set. Numbers in this table are Pearson correlation scores. Best
result on each test set is highlighted in bold. Numbers in All column that has superscript are the official submissions.
Their relative rank among 89 systems in shown in parentheses.
tor machine regression (SVR) model, which predicts
the STS score for the sentence pair. As in Pado et
al. (2009), we augment the bidirectional entailment
features with sentence level BLEU scores, in order
to improve robustness over noisy non-grammatical
data. We trained the SVR model using libSVM over
all of the sentence pairs in the STS training set. The
model uses a Gaussian kernel with ? = 0.125, an
SVR ?-loss of 0.25, and margin violation cost, C, of
2.0. These hyperparameters were selected by cross
validation over the training set.
4 Results
From Table 1, we can see that the pPDA model
performed better than the pFSM model on all test
sets except the MSRvid section. This result clearly
demonstrates the power of the pPDA extension
in modeling long-distance word swapping. The
MSRvid test set has the shortest overall sentence
length (13, versus 35 forMSRpar), and therefore it is
not too surprising that long distance word swapping
did not help much here. Furthermore, the pPDA
model shows a much more pronounced performance
gain than pFSM when tested on unseen datasets
(OnWn and SMTnews), suggesting that the pPDA
model is more robust across domain. A second ob-
servation is that the Indi training scheme seems to
work better than the All approach, which shows hav-
ing more training data does not compensate the dif-
ferent characteristics of each training portion. Our
best metric on all test set is the pPDAIndi model,
with a Pearson?s correlation score of 0.6808. If
interpolated into the official submitted runs rank-
ing, it would be placed at the 22nd place among
89 runs. Among the three official runs submitted
to the shared task (pPDAAll, pFSMIndi and En-
tailment), pFSMIndi performs the best, placed at
38th place among 89 runs. Since our metrics were
originally designed for statistical machine transla-
tion (MT) evaluation, we found that on the unseen
SMTNews test set, which consists of news conversa-
tion sentence pairs from the MT domain, our pPDA
model placed at a much higher position (13 among
89 runs).
In comparison to results on MT evaluation
task (Wang and Manning, 2012), we found that the
pPDA and pFSM models work less well on STS.
Whereas in MT evaluation it is common to have
access to thousands of training examples, there is
an order of magnitude less available training data
in STS. Therefore, learning hundreds of feature pa-
rameters in our models from such few examples are
likely to be ill-posed.
Overall, the RTE system did not perform as well
as the regression based models except for MSRvid
domain , which has the shortest overall sentence
length. Our qualitative evaluation suggests that
MSRvid domain seems to exhibit the least degree of
lexical divergence between the sentence pairs, thus
making this task easier than other domains (the me-
dian score of all 89 official systems for MSRvid
is 0.7538, while the median for MSRpar and SM-
Teuroparl is 0.5128 and 0.4437, respectively). The
relative rank of RTE for MSRvid is 21 among 89,
whereas the pFSM and pPDA systems ranked 80 and
83, respectively. The low performance of pFSM and
pPDA on this task significantly affected the ranking
of these two systems on the ALL evaluation measure.
We do not have a clear explanation why RTE system
thrives on this easier task while pPDA and pFSM
suffers. In the future, we aim to gain a better under-
standing of the characteristics of the two different
systems, and explore combination techniques.
653
5 Conclusion
We describe a metric for computing sentence level
semantic textual similarity, which is based on a
probabilistic finite state machine model that com-
putes weighted edit distance. Our model admits a
rich set of linguistic features, and can be trained to
learn feature weights automatically by optimizing
a regression objective. A novel pushdown automa-
ton extension was also presented for capturing long-
distance word swapping. Our models outperformed
Stanford textual entailment system on all but one of
the genres on the STS task.
Acknowledgements
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181 and the support of the
DARPA Broad Operational Language Translation
(BOLT) program through IBM. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL,
or the US government.
References
T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein.
2001. Introduction to Algorithms, Second Edition.
MIT Press.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL recognising textual entailment challenge. In
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment.
Y. Dombb, O. Lipsky, B. Porat, E. Porat, and A. Tsur.
2010. The approximate swap and mismatch edit dis-
tance. Theoretical Computer Science, 411(43).
J. Eisner. 2002. Parameter estimation for probabilistic
finite-state transducers. In Proceedings of ACL.
J. Esparza and A. Kucera. 2005. Quantitative analysis
of probabilistic pushdown automata: Expectations and
variances. In Proceedings of the 20th Annual IEEE
Symposium on Logic in Computer Science.
K. Knight and Y. Al-Onaizan. 1998. Translation with
finite-state devices. In Proceedings of AMTA.
S. Kumar and W. Byrne. 2003. A weighted finite state
transducer implementation of the alignment template
model for statistical machine translation. In Proceed-
ings of HLT/NAACL.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory BFGS method for large scale optimization. Math.
Programming, 45:503?528.
G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. J. Miller. 1990. WordNet: an on-line lexical
database. International Journal of Lexicography, 3(4).
S. Pado, D. Cer, M. Galley, D. Jurafsky, and C. Man-
ning. 2009. Measuring machine translation quality as
semantic equivalence: A metric based on entailment
features. Machine Translation, 23:181?193.
E. Vidal, F. Thollard, C. de la Higuera, F. Casacuberta,
and R. C. Carrasco. 2005. Probabilistic finite-state
machines part I. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 27(7):1013?1025.
M. Wang and C. Manning. 2012. SPEDE: Probabilistic
edit distance metrics for sentence level MT evaluation.
In Proceedings of WMT.
D. Wu, 2010. CRC Handbook of Natural Language Pro-
cessing, chapter How to Select an Answer String?,
pages 367?408. CRC Press.
654
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 76?83,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SPEDE: Probabilistic Edit Distance Metrics for MT Evaluation
Mengqiu Wang and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305 USA
{mengqiu,manning}@cs.stanford.edu
Abstract
This paper describes Stanford University?s sub-
mission to the Shared Evaluation Task of WMT
2012. Our proposed metric (SPEDE) com-
putes probabilistic edit distance as predictions
of translation quality. We learn weighted edit
distance in a probabilistic finite state machine
(pFSM) model, where state transitions corre-
spond to edit operations. While standard edit
distance models cannot capture long-distance
word swapping or cross alignments, we rectify
these shortcomings using a novel pushdown
automaton extension of the pFSM model. Our
models are trained in a regression framework,
and can easily incorporate a rich set of linguis-
tic features. Evaluated on two different pre-
diction tasks across a diverse set of datasets,
our methods achieve state-of-the-art correla-
tion with human judgments.
1 Introduction
We describe the Stanford Probabilistic Edit Distance
Evaluation (SPEDE) metric, which makes predic-
tions of translation quality by computing weighted
edit distance. We model weighted edit distance in
a probabilistic finite state machine (pFSM), where
state transitions correspond to edit operations. The
weights of the edit operations are then automatically
learned in a regression framework. One of the ma-
jor contributions of this paper is a novel extension
of the pFSM model into a probabilistic Pushdown
Automaton (pPDA), which enhances traditional edit-
distance models with the ability to model phrase shift
and word swapping. Furthermore, we give a new log-
linear parameterization to the pFSM model, which
allows it to easily incorporate rich linguistic features.
We conducted extensive experiments on a di-
verse set of standard evaluation data sets (NIST
OpenMT06, 08; WMT06, 07, 08). Our models
achieve or surpass state-of-the-art results on all test
sets.
2 Related Work
Research in automatic machine translation (MT) eval-
uation metrics has been a key driving force behind
the recent advances of statistical machine transla-
tion (SMT) systems. The early seminal work on
automatic MT metrics (e.g., BLEU and NIST) is
largely based on n-gram matches (Papineni et al,
2002; Doddington, 2002). Despite their simplicity,
these measures have shown good correlation with hu-
man judgments, and enabled large-scale evaluations
across many different MT systems, without incurring
the huge labor cost of human evaluation (Callison-
Burch et al (2009; 2010; 2011), inter alia).
Later metrics that move beyond n-grams achieve
higher accuracy and improved robustness from re-
sources like WordNet synonyms (Miller et al, 1990),
paraphrasing (Zhou et al, 2006; Snover et al, 2009;
Denkowski and Lavie, 2010), and syntactic parse
structures (Liu et al, 2005; Owczarzak et al, 2008;
He et al, 2010). But a common problem in these
metrics is they typically resort to ad-hoc tuning meth-
ods instead of principled approaches to incorporate
linguistic features. Recent models use linear or
SVM regression and train them against human judg-
ments to automatic learn feature weights, and have
shown state-of-the-art correlation with human judg-
ments (Albrecht and Hwa, 2007a; Albrecht and Hwa,
2007b; Sun et al, 2008; Pado et al, 2009). The
drawback, however, is they rely on time-consuming
76
Figure 1: This diagram illustrates an example translation pair in the Chinese-English portion of OpenMT08 data set
(Doc:AFP CMN 20070703.0005, system09, sent 1). The three rows below are the best state transition (edit) sequences
that transforms REF to SYS, according to the three proposed models. The corresponding alignments generated by the
models (pFSM, pPDA, pPDA+f ) are shown with different styled lines, with later models in the order generating strictly
more alignments than earlier ones. The gold human evaluation score is 6.5, and model predictions are: pPDA+f 5.5,
pPDA 4.3, pFSM 3.1, METEORR 3.2, TERR 2.8.
preprocessing modules to extract linguistic features
(e.g., a full end-to-end textual entailment system was
needed in Pado et al (2009)), which severely lim-
its their practical use. Furthermore, these models
employ a large number of features (on the order of
hundreds), and consequently make the model predic-
tions opaque and hard to analyze.
3 pFSMs for MT Regression
We start off by framing the problem of machine trans-
lation evaluation in terms of weighted edit distance
calculated using probabilistic finite state machines
(pFSMs). A FSM defines a language by accepting a
string of input tokens in the language, and rejecting
those that are not. A probabilistic FSM defines the
probability that a string is in a language, extending on
the concept of a FSM. Commonly used models such
as HMMs, n-gram models, Markov Chains, proba-
bilistic finite state transducers and PCFGs all fall in
the broad family of pFSMs (Knight and Al-Onaizan,
1998; Eisner, 2002; Kumar and Byrne, 2003; Vidal
et al, 2005). Unlike all the other applications of
FSMs where tokens in the language are words, in
our language tokens are edit operations. A string of
tokens that our FSM accepts is an edit sequence that
transforms a reference translation (denoted as ref )
into a system translation (sys).
Our pFSM has a unique start and stop state, and
one state per edit operation (i.e., Insert, Delete, Sub-
stitution). The probability of an edit sequence e is
generated by the model is the product of the state tran-
sition probabilities in the pFSM, formally described
as:
w(e | s,r) =
1
Z
|e|
?
i=1
exp ? ? f(ei?1,ei,s,r) (1)
We featurize each of the state changes with a log-
linear parameterization; f is a set of binary feature
functions defined over pairs of neighboring states
(by the Markov assumption) and the input sentences,
and ? are the associated feature weights; r and s are
shorthand for ref and sys; Z is a partition function.
In this basic pFSM model, the feature functions are
simply identity functions that emit the current state,
and the state transition sequence of the previous state
and the current state.
The feature weights are then automatically learned
by training a global regression model where some
translational equivalence judgment score (e.g., hu-
man assessment score, or HTER (Snover et al,
2006)) for each sys and ref translation pair is the
regression target (y?). Since the ?gold? edit sequence
are not given at training or prediction time, we treat
the edit sequences as hidden variables and sum over
77
them in our model. We introduce a new regression
variable y ? R which is the log-sum of the unnormal-
ized weights (Eqn. (1)) of all edit sequences, formally
expressed as:
y = log ?
e??e?
|e
?
|
?
i=1
exp ? ? f(ei?1,ei,s,r) (2)
The sum over an exponential number of edit se-
quences in e? is solved efficiently using a forward-
backward style dynamic program. Any edit sequence
that does not lead to a complete transformation of
the translation pair has a probability of zero in our
model. Our regression target then seeks to minimize
the least squares error with respect to y?, plus a L2-
norm regularizer term parameterized by ? :
? ? = min
?
{?
si,ri
[y?i ? (
y
|si|+ |ri|
+?)]2 +????2}
(3)
The |si|+ |ri| is a length normalization term for the
ith training instance, and ? is a scaling constant for
adjusting to different scoring standards (e.g., 7-point
scale vs. 5-point scale), whose value is automatically
learned. At test time, y/(|s|+ |r|)+? is computed
as the predicted score.
We replaced the standard substitution edit opera-
tion with three new operations: Sword for same word
substitution, Slemma for same lemma substitution, and
Spunc for same punctuation substitution. In other
words, all but the three matching-based substitutions
are disallowed. The start state can transition into any
of the edit states with a constant unit cost, and each
edit state can transition into any other edit state if
and only if the edit operation involved is valid at the
current edit position (e.g., the model cannot transi-
tion into Delete state if it is already at the end of ref ;
similarly it cannot transition into Slemma unless the
lemma of the two words under edit in sys and ref
match). When the end of both sentences are reached,
the model transitions into the stop state and ends the
edit sequence. The first row in Figure 1 starting with
pFSM shows a state transition sequence for an exam-
ple sys/ref translation pair. There exists a one-to-one
correspondence between substitution edits and word
alignments. Therefore this example state transition
sequence correctly generates an alignment for the
word 43 and people.
It is helpful to compare with the TER met-
ric (Snover et al, 2006), which is based on the idea
of word error rate measured in edit distance, to better
understand the intuition behind our model. There
are two major improvements in our model: 1) the
edit operations in our model are weighted, as defined
by the feature functions and weights; 2) the weights
are automatically learned, instead of being uniform
or manually set; and 3) we model state transitions,
which can be understood as a bigram extension of
the unigram edit distance model used in TER. For
example, if in our learned model the feature for two
consecutive Sword states has a positive weight, then
our model would favor consecutive same word sub-
stitutions, whereas in the TER model the order of
the substitution does not matter. The extended TER-
plus (Snover et al, 2009) metric addresses the first
problem but not the other two.
3.1 pPDA Extension
A shortcoming of edit distance models is that they
cannot handle long-distance word swapping ? a
pervasive phenomenon found in most natural lan-
guages. 1 Edit operations in standard edit distance
models need to obey strict incremental order in their
edit position, in order to admit efficient dynamic pro-
gramming solutions. The same limitation is shared
by our pFSM model, where the Markov assumption
is made based on the incremental order of edit po-
sitions. Although there is no known solution to the
general problem of computing edit distance where
long-distance swapping is permitted (Dombb et al,
2010), approximate algorithms do exist. We present
a simple but novel extension of the pFSM model
to a probabilistic pushdown automaton (pPDA), to
capture non-nested word swapping within limited
distance, which covers a majority of word swapping
in observed in real data (Wu, 2010).
A pPDA, in its simplest form, is a pFSM where
each control state is equipped with a stack (Esparza
and Kucera, 2005). The addition of stacks for each
transition state endows the machine with memory,
extending its expressiveness beyond that of context-
free formalisms. By construction, at any stage in a
normal edit sequence, the pPDA model can ?jump?
1The edit distance algorithm described in Cormen et
al. (2001) can only handle adjacent word swapping (transpo-
sition), but not long-distance swapping.
78
forward within a fixed distance (controlled by a max
distance parameter) to a new edit position on either
side of the sentence pair, and start a new edit subse-
quence from there. Assuming the jump was made on
the sys side, 2 the machine remembers its current edit
position in sys as Jstart , and the destination position
on sys after the jump as Jlanding.
We constrain our model so that the only edit op-
erations that are allowed immediately following a
?jump? are from the set of substitution operations
(e.g., Sword). And after at least one substitution
has been made, the device can now ?jump? back
to Jstart , remembering the current edit position as
Jend . Another constraint here is that after the back-
ward ?jump?, all edit operations are permitted except
for Delete, which cannot take place until at least one
substitution has been made. When the edit sequence
advances to position Jlanding, the only operation al-
lowed at that point is another ?jump? forward opera-
tion to position Jend , at which point we also clear all
memory about jump positions and reset.
An intuitive explanation is that when pPDA makes
the first forward jump, a gap is left in sys that has
not been edited yet. It remembers where it left off,
and comes back to it after some substitutions have
been made to complete the edit sequence. The sec-
ond row in Figure 1 (starting with pPDA) illustrates
an edit sequence in a pPDA model that involves three
?jump? operations, which are annotated and indexed
by number 1-3 in the example. ?Jump 1? creates an
un-edited gap between word 43 and western, after
two substitutions, the model makes ?jump 2? to go
back and edit the gap. The only edit permitted imme-
diately after ?jump 2? is deleting the comma in ref,
since inserting the word 43 in sys before any substi-
tution is disallowed. Once the gap is completed, the
model resumes at position Jend by making ?jump 3?,
and completes the jump sequence.
The ?jumps? allowed the model to align words
such as western India, in addition to the alignments
of 43 people found by the pFSM. In practice, we
found that our extension gives a big boost to model
performance (cf. Section 5.1), with only a modest
increase in computation time. 3
2Recall that we transform ref into sys, and thus on the sys
side, we can only insert but not delete. The argument applies
equally to the case where the jump was made on the other side.
3The length of the longest edit sequence with jumps only
3.2 Parameter Estimation
Since the least squares operator preserves convexity,
and the inner log-sum-exponential function is con-
vex, the resulting objective function is also convex.
For parameter learning, we used the limited memory
quasi-newton method (Liu and Nocedal, 1989) to find
the optimal feature weights and scaling constant for
the objective. We initialized ? =~0, ? = 0, and ? = 5.
We also threw away features occurring fewer than
five times in training corpus. Gradient calculation
was similar to other pFSM models, such as HMMs,
we omitted the details here, for brevity.
4 Rich Linguistic Features
We add new substitution operations beyond those in-
troduced in Section 3, to capture synonyms and para-
phrase in the translations. Synonym relations are de-
fined according to WordNet (Miller et al, 1990), and
paraphrase matches are given by a lookup table used
in TERplus (Snover et al, 2009). To better take ad-
vantage of paraphrase information at the multi-word
phrase level, we extended our substitution operations
to match longer phrases by adding one-to-many and
many-to-many bigram block substitutions.
5 Experiments
The goal of our experiments is to test both the ac-
curacy and robustness of the proposed new models.
We then show that modeling word swapping and rich
linguistics features further improve our results.
To better situate our work among past research
and to draw meaningful comparison, we use exactly
the same standard evaluation data sets and metrics
as Pado et al (2009), which is currently the state-
of-the-art result for regression-based MT evaluation.
We consider four widely used MT metrics (BLEU,
NIST, METEOR (Banerjee and Lavie, 2005) (v0.7),
and TER) as our baselines. Since our models are
trained to regress human evaluation scores, to make
a direct comparison in the same regression setting,
we also train a small linear regression model for each
baseline metric in the same way as descried in Pado
et al (2009). These regression models are strictly
more powerful than the baseline metrics and show
higher robustness and better correlation with human
increased by 0.5 ?max(|s|, |r|) in the worst case, and by and
large swapping is rare in comparison to basic edits.
79
Data Set Our Metrics Baseline Metrics
train test pFSM pPDA pPDA+f BLEUR NISTR TERR METR MTR RTER MT+RTER
A+C U 54.6 55.3 57.2 49.9 49.5 50.1 49.1 50.1 54.5 55.6
A+U C 59.9 63.8 65.7 53.9 53.1 50.3 61.1 57.3 58.0 62.7
C+U A 61.2 60.4 59.8 52.5 50.4 54.5 60.1 55.2 59.9 61.1
MT08 MT06 65.2 63.4 64.5 57.6 55.1 63.8 62.1 62.6 62.2 65.2
Table 1: Overall results on OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR) refers to the
regression model trained for each baseline metric, same as Pado et al (2009). The first three rows are round-robin
train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are results trained
on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman?s rank correlation ?
between human assessment scores and model predictions. The pPDA column describes our pPDA model with jump
distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms and paraphrase features
(cf. Section 4). Best results and scores that are not statistically significantly worse are highlighted in bold in each row.
judgments. 4 We also compare our models with the
state-of-the-art linear regression models reported in
Pado et al (2009) that combine features from mul-
tiple MT evaluation metrics (MT), as well as rich
linguistic features from a textual entailment system
(RTE).
In all of our experiments, each reference and sys-
tem translation sentence pair is tokenized using the
PTB (Marcus et al, 1993) tokenization script, and
lemmatized by the Porter Stemmer (Porter, 1980).
Statistical significance tests are performed using the
paired bootstrap resampling method (Koehn, 2004).
We divide our experiments into two sections, based
on two different prediction tasks ? predicting abso-
lute scores and predicting pairwise preference.
5.1 Exp. 1: Predicting Absolute Scores
The first task is to evaluate a system translation
on a seven point Likert scale against a single ref-
erence. Higher scores indicate translations that are
closer to the meaning intended by the reference. Hu-
man ratings in the form of absolute scores are avail-
able for standard evaluation data sets such as NIST
OpenMT06,08.5 Since our model makes predictions
at the granularity of a whole sentence, we focus on
sentence-level evaluation. A metric?s goodness is
judged by how well it correlates with human judg-
ments, and Spearman?s rank correlation (?) is re-
ported for all experiments in this section.
We used the NIST OpenMT06 corpus for develop-
ment purposes, and reserved the NIST OpenMT08
corpus for post-development evaluation. The
4See Pado et al (2009) for more discussion.
5Available from http://www.nist.gov.
OpenMT06 data set contains 1,992 English trans-
lations of Arabic newswire text from 8 MT systems.
For development, we used a 2-fold cross-validation
scheme with splits at the first 1,000 and last 992 sen-
tences. The OpenMT08 data set contains English
translations of newswire text from three languages
(Arabic has 2,769 pairs from 13 MT systems; Chi-
nese has 1,815 pairs from 15; and Urdu has 1,519
pairs, from 7). We followed the same experimental
setup as Pado et al (2009), using a ?round robin?
training/testing scheme, i.e., we train a model on data
from two languages, making predictions for the third.
We also show results of models trained on the entire
OpenMT08 data set and tested on OpenMT06.
Overall Comparison
Results of our proposed models compared against
the baseline models described in Pado et al (2009)
are shown in Table 1. The pFSM and pPDA mod-
els do not use any additional information other than
words and lemmas, and thus make a fair comparison
with the baseline metrics. 6 We can see from the ta-
ble that pFSM significantly outperforms all baselines
on Urdu and Arabic, but trails behind METEORR
on Chinese by a small margin (1.2 point in Spear-
man?s ?). On Chinese data set, the pPDA exten-
sion gives results significantly better than the best
baseline metrics for Chinese (2.7 better than METE-
ORR). It is also significantly better than pFSM (by
6METEORR actually has an unfair advantage in this compari-
son, since it uses synonym information from WordNet; TERR
on the other hand has a disadvantage because it does not use
lemmas. Lemma is added later in the TERplus extension (Snover
et al, 2009).
80
3.9 points), suggesting that modeling word swapping
is particularly rewarding for Chinese language. On
the other hand, pPDA model does not perform bet-
ter than the pFSM model on Arabic in MT08 and
OpenMT06 (which is also Arabic-to-English). This
observation is consistent with findings in earlier work
that Chinese-English translations exhibit much more
medium and long distance reordering than languages
like Arabic (Birch et al, 2009).
Both the pFSM and pPDA models also signifi-
cantly outperform the MTR linear regression model
that combines the outputs of all four baselines, on all
three source languages. This demonstrates that our
regression model is more robust and accurate than a
state-of-the-art system combination linear-regression
model. The RTER and MT+RTER linear regression
models benefit from the rich linguistic features in the
textual entailment system?s output. It has access to
all the features in pPDA+f such as paraphrase and de-
pendency parse relations, and many more (e.g., Norm
Bank, part-of-speech, negation, antonyms). However,
our pPDA+f model rivals the performance of RTER
and MT+RTER on Arabic (with no statistically sig-
nificant difference from RTER), and greatly improve
over these two models on Urdu and Chinese. Most
noticeably, pPDA+f is 7.7 points better than RTER
on Chinese.
5.2 Exp. 2: Predicting Pairwise Preferences
To further test our model?s robustness, we evaluate
it on WMT data sets with a different prediction task
in which metrics make pairwise preference judg-
ments between translation systems. The WMT06-
08 data sets are much larger in comparison to the
OpenMT06 and 08 data. They contain MT outputs of
over 40 systems from five different source languages
(French, German, Spanish, Czech, and Hungarian).
The WMT06, 07 and 08 sets contains 10,159, 5,472
and 6,856 sentence pairs, respectively. We used por-
tions of WMT 06 and 07 data sets 7 that are annotated
with absolute scores on a five point scale for training,
and the WMT08 data set annotated with pairwise
preference for testing.
To generate pairwise preference predictions, we
first predict an absolute score for each system trans-
lation, then compare the scores between each system
7Available from http://www.statmt.org.
pair, and give preference to the higher score. We
adopt the sentence-level evaluation metric used in
Pado et al (2009), which measures the consistency
(accuracy) of metric predictions with human prefer-
ences. The random baseline for this task on WMT08
data set is 39.8%.
Models WMT06 WMT07 WMT06+07
pPDA+f 51.6 52.4 52.0
BLEUR 49.7 49.5 49.6
METEORR 51.4 51.4 51.5
NISTR 50.0 50.3 50.2
TERR 50.9 51.0 51.2
MTR 50.8 51.5 51.5
RTER 51.8 50.7 51.9
MT+RTER 52.3 51.8 52.5
Table 2: Pairwise preference prediction results on WMT08
test set. Each column shows a different training data set.
Numbers in this table are model?s consistency with human
pairwise preference judgments. Best result on each test
set is highlighted in bold.
Results are shown in Table 2. Similar to the results
on OpenMT experiments, our model consistently out-
performed BLEUR, METEORR, NISTR and TERR.
Our model also gives better performance than the
MTR ensemble model on all three tests; and ties with
RTER in two out of the three tests but performs sig-
nificantly better on the other test. The MT+RTER
ensemble model is better on two tests, but worse
on the other. But overall the two systems are quite
comparable, with less than 0.6% accuracy difference.
The results also show that our method is stable across
different training sets, with test accuracy differences
less than 0.4%.
6 Conclusion
We described the SPEDE metric for sentence level
MT evaluation. It is based on probabilistic finite state
machines to compute weighted edit distance. Our
model admits a rich set of linguistic features, and
can be trained to learn feature weights automatically
by optimizing a regression objective. A novel push-
down automaton extension was also presented for
capturing long-distance word swapping. Our metrics
achieve state-of-the-art results on a wide range of
standard evaluations, and are much more lightweight
than previous regression models.
81
Acknowledgements
We gratefully acknowledge the support of Defense
Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-
09-C-0181 and the support of the DARPA Broad
Operational Language Translation (BOLT) program
through IBM. Any opinions, findings, and conclusion
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the view of the DARPA, AFRL, or the US govern-
ment.
References
J. Albrecht and R. Hwa. 2007a. A re-examination of
machine learning approaches for sentence-level MT
evaluation. In Proceedings of ACL.
J. Albrecht and R. Hwa. 2007b. Regression for sentence-
level MT evaluation with pseudo references. In Pro-
ceedings of ACL.
S. Banerjee and A. Lavie. 2005. Meteor: An automatic
metric for MT evaluation with improved correlation
with human judgments. In Proceedings of ACL Work-
shop on Intrinsic and Extrinsic Evaluation Measures.
A. Birch, P. Blunsom, and M. Osborne. 2009. A quantita-
tive analysis of reordering phenomena. In Proceedings
of WMT 09.
C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder.
2009. Findings of the 2009 Workshop on Statistical
Machine Translation. In Proceedings of the Fourth
Workshop on Statistical Machine Translation.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
M. Przybocki, and O. Zaidan. 2010. Findings of the
2010 joint workshop on Statistical Machine Translation
and metrics for Machine Translation. In Proceedings
of Joint WMT 10 and MetricsMatr Workshop at ACL.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 workshop on statistical ma-
chine translation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation.
T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein.
2001. Introduction to Algorithms, Second Edition. MIT
Press.
M. Denkowski and A. Lavie. 2010. Extending the ME-
TEOR machine translation evaluation metric to the
phrase level. In Proceedings of HLT/NAACL.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram cooccurrence statistics.
In Proceedings of HLT.
Y. Dombb, O. Lipsky, B. Porat, E. Porat, and A. Tsur.
2010. The approximate swap and mismatch edit dis-
tance. Theoretical Computer Science, 411(43).
J. Eisner. 2002. Parameter estimation for probabilistic
finite-state transducers. In Proceedings of ACL.
J. Esparza and A. Kucera. 2005. Quantitative analysis
of probabilistic pushdown automata: Expectations and
variances. In Proceedings of the 20th Annual IEEE
Symposium on Logic in Computer Science.
Y. He, J. Du, A. Way, and J. van Genabith. 2010. The
DCU dependency-based metric inWMT-MetricsMATR
2010. In Proceedings of Joint WMT 10 and Metrics-
Matr Workshop at ACL.
K. Knight and Y. Al-Onaizan. 1998. Translation with
finite-state devices. In Proceedings of AMTA.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proceedings of EMNLP.
S. Kumar and W. Byrne. 2003. A weighted finite state
transducer implementation of the alignment template
model for statistical machine translation. In Proceed-
ings of HLT/NAACL.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory BFGS method for large scale optimization. Math.
Programming, 45:503?528.
D. Liu, , and D. Gildea. 2005. Syntactic features for eval-
uation of machine translation. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of english: the
Penn Treebank. Computational Linguistics, 19(2):313?
330.
G. A.Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. J.
Miller. 1990. WordNet: an on-line lexical database.
International Journal of Lexicography, 3(4).
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proceedings of ACL.
K. Owczarzak, J. van Genabith, and A. Way. 2008. Evalu-
ating machine translation with LFG dependencies. Ma-
chine Translation, 21(2):95?119.
S. Pado, M. Galley, D. Jurafsky, and C. D. Manning. 2009.
Robust machine translation evaluation with entailment
features. In Proceedings of ACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of ACL.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
AMTA.
M. Snover, , N. Madnani, B. Dorr, and R. Schwartz. 2009.
Fluency, adequacy, or HTER? exploring different hu-
man judgments with a tunable MT metric. In Proceed-
ings of WMT09 Workshop.
82
S. Sun, Y. Chen, and J. Li. 2008. A re-examination on
features in regression based approach to automatic MT
evaluation. In Proceedings of ACL.
E. Vidal, F. Thollard, C. de la Higuera, F. Casacuberta,
and R. C. Carrasco. 2005. Probabilistic finite-state ma-
chines part I. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 27(7):1013?1025.
D. Wu, 2010. CRC Handbook of Natural Language Pro-
cessing, chapter How to Select an Answer String?,
pages 367?408. CRC Press.
L. Zhou, C.Y. Lin, and E. Hovy. 2006. Re-evaluating
machine translation results with paraphrase support. In
Proceedings of EMNLP.
83
