Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 231?235,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Coreference Resolution Evaluation for Higher Level Applications
Don Tuggener
Unversity of Zurich
Institute of Computational Linguistics
tuggener@cl.uzh.ch
Abstract
This paper presents an evaluation frame-
work for coreference resolution geared to-
wards interpretability for higher-level ap-
plications. Three application scenarios
for coreference resolution are outlined and
metrics for them are devised. The metrics
provide detailed system analysis and aim
at measuring the potential benefit of using
coreference systems in preprocessing.
1 Introduction
Coreference Resolution is often described as an
important preprocessing step for higher-level ap-
plications. However, the commonly used coref-
erence evaluation metrics (MUC, BCUB, CEAF,
BLANC) treat coreference as a generic clustering
problem and perform cluster similarity measures
to evaluate coreference system outputs. Mentions
are seen as unsorted generic items rather than lin-
early ordered linguistic objects (Chen and Ng,
2013). This makes it arguably hard to interpret
the scores and assess the potential benefit of using
a coreference system as a preprocessing step.
Therefore, this paper proposes an evaluation
framework for coreference systems which aims at
bridging the gap between coreference system de-
velopment, evaluation, and higher level applica-
tions. For this purpose, we outline three types
of application scenarios which coreference resolu-
tion can benefit and devise metrics for them which
are easy to interpret and provide detailed system
output analysis based on any available mention
feature.
2 Basic Concepts
Like other coreference metrics, we adapt the con-
cepts of Recall and Precision from evaluation in
Information Retrieval (IR) to compare mentions
in a system output (the response) to the anno-
tated mentions in a gold standard (the key). To
stay close to the originally clear definitions of Re-
call and Precision in IR, Recall is aimed at iden-
tifying how many of the annotated key mentions
are correctly resolved by a system, and Precision
will measure the correctness of the returned sys-
tem mentions.
However, if we define Recall as
tp
tp+fn
, the de-
nominator will not include key mentions that have
been put in the wrong coreference chain, and will
not denote all mentions in the key. Therefore,
borrowing nomenclature from (Durrett and Klein,
2013), we introduce an additional error class,
wrong linkage (wl), which signifies key mentions
that have been linked to incorrect antecedents. Re-
call can then be defined as
tp
tp+wl+fn
and Precision
as
tp
tp+wl+fp
. Recall then extends over all key men-
tions, and Precision calculation includes all sys-
tem mentions.
Furthermore, including wrong linkage in the
Recall equation prevents it from inflating com-
pared to Precision when a large number of key
mentions are incorrectly resolved. Evaluation
is also sensitive to the anaphoricity detection
problem. For example, an incorrectly resolved
anaphoric ?it? pronoun is counted as wrong link-
age and thus also affects Recall, while a resolved
pleonastic ?it? pronoun is considered a false posi-
tive which is only penalized by Precision. Beside
the ?it? pronoun, this is of particular relevance for
noun markables, as determining their referential
status is a non-trivial subtask in coreference res-
olution.
As we evaluate each mention individually, we
are able to measure performance regarding any
feature type of a mention, e.g. PoS, number, gen-
der, semantic class etc. We will focus on men-
tion types based on PoS tags (i.e. pronouns, nouns
etc.), as they are often the building blocks of coref-
erence systems. Furthermore, mention type based
231
performance analysis is informative for higher-
level applications, as they might be specifically in-
terested in certain mention types.
3 Application Scenarios
Next, we will outline three higher-level applica-
tion types which consume coreference and devise
relevant metrics for them.
3.1 Models of entity distributions
The first application scenario subsumes models
that investigate distributions and patterns of en-
tity occurrences in discourse. For example, Cen-
tering theory (Grosz et al., 1995) and the thereof
derived entity grid model (Barzilay and Lapata,
2008; Elsner and Charniak, 2011) record transi-
tions of grammatical functions that entities occur
with in coherent discourse. These models can
benefit from coreference resolution if entities are
pronominalized or occur as a non-string matching
nominal mentions.
Another application which tracks sequences of
entity occurrences is event sequence modeling.
Such models investigate prototypical sequences of
events to derive event schemes or templates of suc-
cessive events (Lee et al., 2012; Irwin et al., 2011;
Kuo and Chen, 2007). Here, coreference res-
olution can help link pronominalized arguments
of events to their previous mention and, thereby,
maintain the event argument sequence.
The outlined applications in this scenario pri-
marily rely on the identification of correct and
gapless sequences of entity occurrences. We can
approximate this requirement in a metric by re-
quiring the immediate antecedent of a mention in
a response chain to be the immediate antecedent
of that mention in the key chain.
Note that this restriction deems mentions as in-
correct, if they skip an antecedent but are resolved
to another antecedent in the correct chain. For ex-
ample, given a key [A-B-C-D], mention D in a re-
sponse [A-B-D] would not be considered correct,
as the immediate antecedent is not the same as in
the key. The original sequence of the entity?s oc-
currence is broken between mention B and D in
the response, as mention C is missing.
We use the following algorithm (table 1) to cal-
culate Recall and Precision for evaluating imme-
diate antecedents. Let K be the key and S be the
system response. Let e be an entity denoted by m
n
mentions.
01 for e
k
? K:
02 for m
i
? e
k
? i > 0:
03 if ??e
s
,m
j
: (e
s
? S ?m
j
? e
s
?m
j
= m
i
?
?predecessor(m
j
, e
s
)) ? fn++
04 elif ?e
s
,m
j
: (e
s
? S ?m
j
? e
s
?m
j
= m
i
?
predecessor(m
i
, e
k
) = predecessor(m
j
, e
s
))
? tp++
05 else wl++
06 for e
s
? S:
07 for m
i
? e
s
? i > 0:
08 if ??e
k
,m
j
: (e
k
? K ?m
j
? e
k
?m
j
= m
i
?
?predecessor(m
j
, e
k
)) ? fp++
Table 1: Algorithm for calculating Recall and Pre-
cision.
We traverse the key K and each entity e
k
in
it
1
. We evaluate each mention m
i
in e
k
, except for
the first one (line 2), as we investigate coreference
links. If no response chain exists that contains
m
i
and its predecessor, we count m
i
as a false
negative (line 3). This condition subsumes the
case where m
i
is not in the response, and the case
where m
i
is the first mention of a response chain.
In the latter case, the system has deemed m
i
to be
non-anaphoric (i.e. the starter of a chain), while it
is anaphoric in the key
2
. We check whether the
immediate predecessor of m
i
in the key chain e
k
is also the immediate predecessor of m
j
in the re-
sponse chain e
s
(line 4). If true, we count m
i
as a
true positive, or as wrong linkage otherwise.
We traverse the response chains to detect spu-
rious system mentions, i.e. mentions not in the
key, and count them as false positives, i.e. non-
anaphoric markables that have been resolved by
the system (lines 6-8). Here, we also count men-
tions in the response, which have no predecessor
in a key chain, as false positives. If a mention
in the response chain is the chain starter in a key
chain, it means that the system has falsely deemed
it to be anaphoric and we regard it as a false posi-
tive
3
.
3.2 Inferred local entities
The second application scenario relies on corefer-
ence resolution to infer local nominal antecedents.
For example, in Summarization, a target sentence
may contain a pronoun which should be replaced
by a nominal antecedent to avoid ambiguities and
ensure coherence in the summary. Machine Trans-
1
We disregard singleton entities, as it is not clear what
benefit a higher level application could gain from them.
2
(Durrett and Klein, 2013) call this error false new (FN).
3
This error is called false anaphoric (FA) by (Durrett and
Klein, 2013).
232
lation can benefit from pronoun resolution in lan-
guage pairs where nouns have grammatical gen-
der. In such language pairs, the gender of a pro-
noun antecedent has to be retrieved in the source
language in order to insert the pronoun with the
correct gender in the target language.
In these applications, it is not sufficient to link
pronouns to other pronouns of the same corefer-
ence chain because they do not help infer the un-
derlying entity. Therefore, in our metric, we re-
quire the closest preceding nominal antecedent of
a mention in a response chain to be an antecedent
in the key chain.
The algorithm for calculation of Recall and Pre-
cision is similar to the one in table 1. We modify
lines 3 and 4 to require the closest nominal an-
tecedent of m
i
in the response chain e
s
to be an
antecedent of m
j
in the corresponding key chain
e
k
, where m
j
= m
i
, i.e.:
?m
h
? e
s
: is closest noun(m
h
,m
i
) ?
?e
k
,m
j
,m
l
: (e
k
? K ? m
j
? e
k
? m
j
=
m
i
?m
l
? e
k
? l < j ?m
l
= m
h
) ? tp++
Note that we cannot process chains without a
nominal mention in this scenario
4
. Therefore, we
skip evaluation for such e
k
? K. We still want
to find incorrectly inferred nominal antecedents of
anaphoric mentions, i.e. mentions in e
s
? S that
have been assigned a nominal antecedent in the re-
sponse but have none in the key and count them as
wrong linkage, as they infer an incorrect nominal
antecedent. Therefore, we traverse all e
s
? S and
add to the algorithm:
?m
i
? e
s
: ?is noun(m
i
) ? ?m
h
? e
s
:
is noun(m
h
) ? ?e
k
,m
j
: (e
k
? K ? m
j
?
e
k
?m
j
= m
i
? ??m
l
? e
k
: is noun(m
l
)) ?
wl++
3.3 Finding contexts for a specific entity
The last scenario we consider covers applications
that are primarily query driven. Such applications
search for references to a given entity and analyze
or extract its occurrence contexts. For example,
Sentiment Analysis searches large text collections
for occurrences of a target entity and then derives
polarity information from its contexts. Biomedical
relation mining looks for interaction contexts of
specific genes or proteins etc.
4
We found that 476 of 4532 key chains (10.05%) do not
contain a nominal mention. Furthermore, we do not treat
cataphora (i.e. pronouns at chain start) in this scenario. We
found that 241 (5.31%) of the key chains start with cataphoric
pronouns.
For these applications, references to relevant en-
tities have to be accessible by queries. For ex-
ample, if a sentiment system investigates polarity
contexts of the entity ?Barack Obama?, given a
key chain [Obama - the president - he], a response
chain [the president - he] is not sufficient, because
the higher level application is not looking for in-
stances of the generic ?president? entity.
Therefore, we determine an anchor mention for
each coreference chain which represents the most
likely unique surface form an entity occurs with.
As a simple approximation, we choose the first
nominal mention of a coreference chain to be the
anchor of the entity, because first mentions of enti-
ties introduce them to discourse and are, therefore,
generally informative, unambiguous, semantically
extensive and are likely to contain surface forms a
higher level application will query.
Entity Detection
01 for e
k
? K:
02 if ?m
n
? e
k
: is noun(m
n
)
? m
anchor
= determine anchor(e
k
)
03 if ?m
anchor
? ?e
s
? S : m
anchor
? e
s
? tp++
04 else ? fn++
05 for e
s
? S:
06 if ?m
n
? e
s
: is noun(m
n
)
? m
anchor
= determine anchor(e
s
)
07 if ??e
k
? K : m
anchor
? e
k
? fp++
Entity Mentions
01 for e
k
? K : ?m
anchor
? ?e
s
? S : m
anchor
? e
s
:
02 for m
i
? e
k
:
03 if m
i
? e
s
? tp++
04 else? fn++
05 for m
i
? e
s
:
06 if m
i
? ? e
k
? fp++
Table 2: Algorithm for calculating Recall and Pre-
cision using anchor mentions.
To calculate Recall and Precision, we align
coreference chains in the responses to those in the
key via their anchors and then measure how many
(in)correct references to that anchor the corefer-
ence systems find (table 2). We divide evaluation
into entity detection (ED), which measures how
many of the anchor mentions a system identifies.
We then measure the quality of the entity men-
tions (EM) for only those entities which have been
aligned through their anchors.
The quality of the references to the anchor men-
tions are not directly comparable between sys-
tems, as their basis is not the same if the num-
ber of aligned anchors differs. Therefore, we cal-
culate the harmonic mean of entity detection and
entity mentions to enable direct system compari-
233
son. Where applicable, we obtain the named en-
tity class of the entity and measure performance
for each such class.
4 Evaluation
We apply our metrics to three available corefer-
ence systems, namely the Berkley system (Dur-
rett and Klein, 2013), the IMS system (Bj?orkelund
and Farkas, 2012), and the Stanford system (Lee
et al., 2013) and their responses for the CoNLL
2012 shared task test set for English (Pradhan et
al., 2012). Tables 3 and 4 report the results.
Immediate antecedent Inferred antecedent
R P F R P F
BERK (Durrett and Klein, 2013)
NOUN 45.06 47.06 46.04 55.54 60.37 57.85
PRP 67.66 64.87 66.24 48.92 53.62 51.16
PRP$ 74.49 74.32 74.41 61.95 66.80 64.28
TOTAL 56.60 56.91 56.76 52.94 58.04 55.37
IMS (Bj?orkelund and Farkas, 2012)
NOUN 38.01 43.09 40.39 46.90 54.96 50.61
PRP 69.06 68.64 68.85 43.04 57.42 49.20
PRP$ 72.57 72.11 72.34 51.51 63.54 56.90
TOTAL 53.55 57.55 55.48 45.27 56.47 50.25
STAN (Lee et al., 2013)
NOUN 38.51 42.92 40.60 50.03 57.62 53.56
PRP 65.55 61.09 63.25 36.67 45.97 40.80
PRP$ 66.12 65.70 65.91 40.64 52.38 45.77
TOTAL 51.70 52.69 52.19 43.01 51.73 46.97
Table 3: Antecedent based evaluation
We note that the system ranking based on the
MELA score
5
is retained by our metrics. MELA
rates the Berkley system best (61.62), followed by
the IMS system (57.42), and then the Stanford sys-
tem (55.69).
Beside detailed analysis based on PoS tags, our
metrics reveal interesting nuances. Somewhat ex-
pectedly, noun resolution is worse when the imme-
diate antecedent is evaluated, than if the next nom-
inal antecedent is analyzed. Symmetrically in-
verse, pronouns achieve higher scores when their
direct antecedent is measured, as compared to
when the next nominal antecedent has to be cor-
rect.
Our evaluation shows that the IMS system
achieves a higher score for pronouns than the
Berkley system when immediate antecedents are
measured and has a higher Precision for pronouns
regarding the inferred antecedents. The Berkley
system performs best mainly due to Recall. For
e.g. personal pronouns (PRP), Berkley has the
5
MUC+BCUB+CEAFE
3
following counts for the inferred antecedents:
tp=2687, wl=1935, fn=871, fp=389, while IMS
shows tp=2243, wl=1376, fn=1592, fp=287. This
indicates that the IMS Recall is lower because of
the high false negative count, rather than being due
to too many wrong linkages.
Finally, table 4 suggests that the IMS systems
performs significantly worse in the PERSON class
than the other systems and is outperformed by the
Stanford system in the ORG class, but performs
best in the GPE class.
R P F F?
PERSON (18.69%)
BERK
ED 64.02 75.88 69.45
67.11
EM 63.60 66.29 64.92
IMS
ED 45.66 51.69 48.48
52.74
EM 47.67 73.45 57.82
STAN
ED 56.33 59.74 57.98
61.61
EM 53.84 84.37 65.73
GPE (13.28%)
BERK
ED 73.21 77.36 75.23
75.71
EM 69.89 83.73 76.19
IMS
ED 73.51 74.17 73.84
76.21
EM 69.94 90.04 78.73
STAN
ED 70.24 76.62 73.29
75.24
EM 68.44 88.81 77.30
ORG (9.63%)
BERK
ED 62.78 67.13 64.88
67.62
EM 66.87 74.78 70.60
IMS
ED 44.98 54.30 49.20
56.85
EM 57.26 81.66 67.32
STAN
ED 49.68 58.56 53.75
59.41
EM 57.25 79.05 66.41
TOTAL (100%)
BERK
ED 58.65 53.19 55.79
63.41
EM 72.65 74.28 73.45
IMS
ED 47.16 42.66 44.80
55.24
EM 65.88 79.40 72.01
STAN
ED 48.62 41.40 44.72
55.27
EM 65.66 80.48 72.32
Table 4: Anchor mention based evaluation
5 Conclusion
We have presented a simple evaluation framework
for coreference evaluation with higher level ap-
plications in mind. The metrics allow specific
performance measurement regarding different an-
tecedent requirements and any mention feature,
such as PoS type, lemma, or named entity class,
which can aid system development and compari-
son. Furthermore, the metrics do not alter system
rankings compared to the commonly used evalua-
tion approach
6
.
6
The scorers are freely available on our website:
http://www.cl.uzh.ch/research/coreferenceresolution.html
234
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Com-
put. Linguist., 34(1):1?34, March.
Anders Bj?orkelund and Rich?ard Farkas. 2012. Data-
driven multilingual coreference resolution using re-
solver stacking. In Joint Conference on EMNLP and
CoNLL - Shared Task, CoNLL ?12, pages 49?55,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Chen Chen and Vincent Ng. 2013. Linguistically
aware coreference evaluation metrics. In Proceed-
ings of the 6th International Joint Conference on
Natural Language Processing, pages 1366?1374.
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, Seattle, Washington, Oc-
tober. Association for Computational Linguistics.
Micha Elsner and Eugene Charniak. 2011. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
?11, pages 125?129, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Barbara J. Grosz, Scott Weinstein, and Aravind K.
Joshi. 1995. Centering: a framework for modeling
the local coherence of discourse. Comput. Linguist.,
21(2):203?225, June.
Joseph Irwin, Mamoru Komachi, and Yuji Matsumoto.
2011. Narrative schema as world knowledge for
coreference resolution. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, CONLL Shared Task
?11, pages 86?92, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
June-Jei Kuo and Hsin-Hsi Chen. 2007. Cross-
document event clustering using knowledge mining
from co-reference chains. Inf. Process. Manage.,
43(2):327?343, March.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity
and event coreference resolution across documents.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 489?500, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4).
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
of the Sixteenth Conference on Computational Natu-
ral Language Learning (CoNLL 2012), Jeju, Korea.
235
Proceedings of BioNLP Shared Task 2011 Workshop, pages 151?152,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
An Incremental Model for the Coreference Resolution Task of BioNLP 2011
Don Tuggener, Manfred Klenner, Gerold Schneider, Simon Clematide, Fabio Rinaldi
Institute of Computational Linguistics, University of Zurich, Switzerland
{tuggener,klenner,gschneid,siclemat,rinaldi}@cl.uzh.ch
Abstract
We introduce our incremental coreference res-
olution system for the BioNLP 2011 Shared
Task on Protein/Gene interaction. The benefits
of an incremental architecture over a mention-
pair model are: a reduction of the number
of candidate pairs, a means to overcome the
problem of underspecified items in pair-wise
classification and the natural integration of
global constraints such as transitivity. A fil-
tering system takes into account specific fea-
tures of different anaphora types. We do not
apply Machine Learning, instead the system
classifies with an empirically derived salience
measure based on the dependency labels of the
true mentions. The OntoGene pipeline is used
for preprocessing.
1 Introduction
The Coreference Resolution task of BioNLP fo-
cused on finding anaphoric references to proteins
and genes. Only antecedent-anaphora pairs are con-
sidered in evaluation and not full coreference sets.
Although it might not seem to be necessary to gen-
erate full coreference sets, anaphora resolution still
benefits from their establishment. Our incremental
approach (Klenner et al, 2010) naturally enforces
transitivity constraints and thereby reduces the num-
ber of potential antecedent candidates. The system
achieved good results in the BioNLP 2011 shared
task (Fig. 1)
Team R P F1
A 22.18 73.26 34.05
Our model 21.48 55.45 30.96
B 19.37 63.22 29.65
C 14.44 67.21 23.77
D 3.17 3.47 3.31
E 0.70 0.25 0.37
Figure 1: Protein/Gene Coreference Task
2 Preprocessing: The OntoGene Pipeline
OntoGene?s text mining system is based on an
internally-developed fast, broad-coverage, deep-
syntactic parsing system (Schneider, 2008). The
parser is wrapped into a pipeline which uses a num-
ber of other NLP tools. The parser is a key compo-
nent in a pipeline of NLP tools (Rinaldi et al, 2010),
used to process input documents. First, in a pre-
processing stage, the input text is transformed into
a custom XML format, and sentences and tokens
boundaries are identified. The OntoGene pipeline
also includes a step of term annotation and disam-
biguation, which are not used for the BioNLP shared
task, since relevant terms are already provided in
both the training and test corpora. The pipeline also
includes part-of-speech taggers, a lemmatizer and a
syntactic chunker.
When the pipeline finishes, each input sentence
has been annotated with additional information,
which can be briefly summarized as follows: sen-
tences are tokenized and their borders are detected;
each sentence and each token has been assigned an
ID; each token is lemmatized; tokens which be-
long to terms are grouped; each term is assigned a
normal-form and a semantic type; tokens and terms
are then grouped into chunks; each chunk has a
type (NP or VP) and a head token; each sentence
is described as a syntactic dependency structure. All
this information is represented as a set of predicates
and stored into the Knowledge Base of the system,
which can then be used by different applications,
such as the OntoGene Relation Miner (Rinaldi et al,
2006) and the OntoGene Protein-Protein Interaction
discovery tool (Rinaldi et al, 2008).
3 Our Incremental Model for Coreference
Resolution
1 for i=1 to length(I)
2 for j=1 to length(C)
3 rj := virtual prototype of coreference set Cj
4 Cand := Cand ? rj if compatible(rj ,mi)
5 for k= length(B) to 1
6 bk:= the k-th licensed buffer element
7 Cand := Cand ? bk if compatible(bk,mi)
8 if Cand = {} then B := B ?mi
9 if Cand 6= {} then
10 antei := most salient element of Cand
11 C := augment(C,antei,mi)
Figure 2: Incremental model: base algorithm
151
Fig. 2 shows the base algorithm. Let I be the
chronologically ordered list of NPs, C be the set
of coreference sets and B a buffer, where NPs are
stored, if they are not anaphoric (but might be valid
antecedents). Furthermore mi is the current NP and
? means concatenation of a list and a single item.
The algorithm proceeds as follows: a set of an-
tecedent candidates is determined for each NP mi
(steps 1 to 7) from the coreference sets (rj) and the
buffer (bk). A valid candidate rj or bk must be com-
patible with mi. The definition of compatibility de-
pends on the POS tags of the anaphor-antecedent
pair. The most salient available candidate is selected
as antecedent for mi.
3.1 Restricted Accessibility of Antecedent
Candidates
In order to reduce underspecification, mi is com-
pared to a virtual prototype of each coreference set
(similar to e.g. (Luo et al, 2004; Yang et al, 2004;
Rahman and Ng, 2009)). The virtual prototype bears
morphologic and semantic information accumulated
from all elements of the coreference set. Access to
coreference sets is restricted to the virtual prototype.
This reduces the number of considered pairs (from
the cardinality of a set to 1).
3.2 Filtering based on Anaphora Type
Potentionally co-refering NPs are extracted from the
OntoGene pipeline based on POS tags. We then ap-
ply filtering based on anaphora type: Reflexive pro-
nouns must be bound to a NP that is governed by the
same verb. Relative pronouns are bound to the clos-
est NP in the left context. Personal and possessive
pronouns are licensed to bind to morphologically
compatible antecedent candidates within a window
of two sentences. Demonstrative NPs containing the
lemmata ?protein? or ?gene? are licensed to bind to
name containing mentions. Demonstrative NPs not
containing the trigger lemmata can be resolved to
string matching NPs preceding them1.
3.3 Binding Theory as a Filter
We know through binding theory that ?modulator?
and ?it? cannot be coreferent in the sentence ?Over-
expression of protein inhibited stimulus-mediated
transcription, whereas modulator enhanced it?.
Thus, the pair ?modulator?-?it? need not be consid-
ered at all. We have not yet implemented a full-
1As we do not perform anaphoricity determination of nom-
inal NPs, we do not consider bridging anaphora (anaphoric
nouns that are connected to their antecedents through seman-
tic relations and cannot be identified by string matching).
blown binding theory. Instead, we check if the an-
tecedent and the anaphor are governed by the same
verb.
4 An Empirically-based Salience Measure
Our salience measure is a partial adaption of the
measure from (Lappin and Leass, 1994). The
salience of a NP is solely defined by the salience
of the dependency label it bears. The salience of a
dependency label, D, is estimated by the number of
true mentions (i.e. co-refering NPs) that bear D (i.e.
are connected to their heads with D), divided by the
total number of true mentions (bearing any D). The
salience of the label subject is thus calculated by:
Number of truementions bearing subject
Total number of truementions
We get a hierarchical ordering of the dependency la-
bels (subject > object > pobject > ...) according to
which antecedents are ranked and selected.
References
Manfred Klenner, Don Tuggener, and Angela Fahrni. 2010. Inkre-
mentelle koreferenzanalyse fu?r das deutsche. In Proceedings der
10. Konferenz zur Verarbeitung Natu?rlicher Sprache.
Shalom Lappin and Herbert J Leass. 1994. An algorithm for pronomi-
nal anaphora resolution. Computational Linguistics, 20:P. 535?561.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla, and
Salim Roukos. 2004. A mention-synchronous coreference resolu-
tion algorithm based on the bell tree. In Proceedings of the 42nd
Annual Meeting on Association for Computational Linguistics.
Altaf Rahman and Vincent Ng. 2009. Supervised models for corefer-
ence resolution. In Proceedings of the 2009 Conference on Empir-
ical Methods in Natural Language Processing: Volume 2 - Volume
2, EMNLP ?09, pages 968?977, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand, Michael Hess, and
Martin Romacker. 2006. An Environment for Relation Mining over
Richly Annotated Corpora: the case of GENIA. BMC Bioinformat-
ics, 7(Suppl 3):S3.
Fabio Rinaldi, Thomas Kappeler, Kaarel Kaljurand, Gerold Schnei-
der, Manfred Klenner, Simon Clematide, Michael Hess, Jean-Marc
von Allmen, Pierre Parisot, Martin Romacker, and Therese Vachon.
2008. OntoGene in BioCreative II. Genome Biology, 9(Suppl
2):S13.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand, Simon Clematide,
Therese Vachon, and Martin Romacker. 2010. OntoGene in
BioCreative II.5. IEEE/ACM Transactions on Computational Bi-
ology and Bioinformatics, 7(3):472?480.
Gerold Schneider. 2008. Hybrid Long-Distance Functional Depen-
dency Parsing. Doctoral Thesis, Institute of Computational Linguis-
tics, University of Zurich.
Xiaofeng Yang, Jian Su, Guodong Zhou, and Chew Lim Tan. 2004. An
np-cluster based approach to coreference resolution. In Proceedings
of the 20th international conference on Computational Linguistics.
152
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 81?85,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
An Incremental Model for Coreference Resolution with Restrictive
Antecedent Accessibility
Manfred Klenner
Institute of Computational Linguistics
University of Zurich
klenner@cl.uzh.ch
Don Tuggener
Institute of Computational Linguistics
University of Zurich
tuggener@cl.uzh.ch
Abstract
We introduce an incremental model for coref-
erence resolution that competed in the CoNLL
2011 shared task (open regular). We decided
to participate with our baseline model, since it
worked well with two other datasets. The ben-
efits of an incremental over a mention-pair ar-
chitecture are: a drastic reduction of the num-
ber of candidate pairs, a means to overcome
the problem of underspecified items in pair-
wise classification and the natural integration
of global constraints such as transitivity. We
do not apply machine learning, instead the
system uses an empirically derived salience
measure based on the dependency labels of the
true mentions. Our experiments seem to indi-
cate that such a system already is on par with
machine learning approaches.
1 Introduction
With notable exceptions (Luo et al, 2004; Yang et
al., 2004; Daume III and Marcu, 2005; Culotta et
al., 2007; Klenner, 2007; Rahman and Ng, 2009;
Klenner and Ailloud, 2009; Cai and Strube, 2010;
Raghunathan et al, 2010) supervised approaches to
coreference resolution are often realized by pairwise
classification of anaphor-antecedent candidates. A
popular and often reimplemented approach is pre-
sented in (Soon et al, 2001). As recently discussed
in (Ng, 2010), the so called mention-pair model suf-
fers from several design flaws which originate from
the locally confined perspective of the model:
? Generation of (transitively) redundant pairs, as
the formation of coreference sets (coreference
clustering) is done after pairwise classification
? Thereby generation of skewed training sets
which lead to classifiers biased towards nega-
tive classification
? No means to enforce global constraints such as
transitivity
? Underspecification of antecedent candidates
These problems can be remedied by an incremen-
tal entity-mention model, where candidate pairs are
evaluated on the basis of the emerging coreference
sets. A clustering phase on top of the pairwise clas-
sifier no longer is needed and the number of candi-
date pairs is reduced, since from each coreference
set (be it large or small) only one mention (the most
representative one) needs to be compared to a new
anaphor candidate. We form a ?virtual prototype?
that collects information from all the members of
each coreference set in order to maximize ?repre-
sentativeness?. Constraints such as transitivity and
morphological agreement can be assured by just a
single comparison. If an anaphor candidate is com-
patible with the virtual prototype, then it is by defini-
tion compatible with all members of the coreference
set.
We designed our system to work purely with a
simple, yet empirically derived salience measure. It
turned out that it outperformed (for German and En-
glish, using CEAF, B-cubed and Blanc) the systems
from the 2010?s SemEval shared task1 on ?corefer-
ence resolution in multiple languages?. Only with
the more and more questioned (Luo, 2005; Cai and
1We have carried out a post task evaluation with the data
provided on the SemEval web page.
81
Strube, 2010) MUC measure our system performed
worse (at least for English). Our system uses real
preprocessing (i.e. a dependency parser (Schneider,
2008)) and extracts markables (nouns, named enti-
ties and pronouns) from the chunks and based on
POS tags delivered by the preprocessing pipeline.
Since we are using a parser, we automatically take
part in the open regular session. Please note that the
dependency labels are the only additional informa-
tion being used by our system.
2 Our Incremental Model
Fig. 1 shows the basic algorithm. Let I be the
chronologically ordered list of markables, C be the
set of coreference sets (i.e. the coreference partition)
and B a buffer, where markables are stored, if they
are not found to be anaphoric (but might be valid
antecedents, still). Furthermore mi is the current
markable and ? means concatenation of a list and
a single item. The algorithm proceeds as follows: a
set of antecedent candidates is determined for each
markable mi (steps 1 to 7) from the coreference sets
and the buffer. A valid candidate rj or bk must be
compatible with mi. The definition of compatibility
depends on the POS tags of the anaphor-antecedent
pair (in order to be coreferent, e.g. two pronouns
must agree in person, number and gender etc.).
In order to reduce underspecification, mi is com-
pared to a virtual prototype of each coreference set.
The virtual prototype bears information accumu-
lated from all elements of the coreference set. For
instance, assume a candidate pair ?she .. Clinton?.
Since the gender of ?Clinton? is unspecified, the pair
might or might not be a good candidate. But if there
is a coreference set aleady including ?Clinton?, let?s
say: {?Hilary Clinton?, her, she} then we know the
gender from the other members and are more save
in our decision. The virtual prototype here would be
something like: singular, feminine, human.
From the set of candidates, Cand, the most salient
antei ? Cand is selected (step 10) and the coref-
erence partition is augmented (step 11). If antei
comes from a coreference set, mi is added to that
set. Otherwise (antei is from the buffer), a new set is
formed, {antei,mi}, and added to the set of coref-
erence sets.
2.1 Restricted Accessibility of Antecedent
Candidates
As already discussed, access to coreference sets
is restricted to the virtual prototype - the concrete
members are invisible. This reduces the number of
considered pairs (from the cardinality of a set to 1).
Moreover, we also restrict the access to buffer el-
ements: if an antecedent candidate, rj , from a coref-
erence set exists, then elements from the buffer, bk,
are only licensed if they are more recent than rj . If
both appear in the same sentence, the buffer element
must be more salient in order to get licensed.
2.2 Filtering based on Anaphora Type
There is a number of conditions not shown in the
basic algorithm from Fig. 1 that define compatibil-
ity of antecedent and anaphor candidates based on
POS tags. Reflexive pronouns must be bound in the
subclause they occur, more specifically to the sub-
ject governed by the same verb. Personal and pos-
sessive pronouns are licensed to bind to morphologi-
cally compatible antecedent candidates (named enti-
ties, nouns2 and pronouns) within a window of three
sentences.
We use the information given by CoNLL input
data to identify ?speaker? and the person adressed by
?you?. ?I? refers to one of the coreference sets whose
speaker is the person who, according to the CoNLL
data, is the producer of the sentence. ?You? refers
to the producer of the last sentence not being pro-
duced by the current ?speaker?. If one didn?t have
access to these data, it would be impossible to cor-
rectly identify the reference of ?I?, since turn taking
is not indicated in the pure textual data.
As we do not use machine learning, we only
apply string matching techniques to match nom-
inal NPs and leave out bridging anaphora (i.e.
anaphoric nouns that are connected to their an-
tecedents through a semantic relation such as hy-
ponymy and cannot be identified by string matching
therefore). Named entities must either match com-
pletely or the antecedent must be longer than one
token and all tokens of the anaphor must be con-
tained in the antecedent (to capture relations such
2To identify animacy and gender of NEs we use a list of
known first names annotated with gender information. To ob-
tain animacy information for common nouns we conduct a
WordNet lookup.
82
1 for i=1 to length(I)
2 for j=1 to length(C)
3 rj := virtual prototype of coreference set Cj
4 Cand := Cand ? rj if compatible(rj ,mi)
5 for k= length(B) to 1
6 bk:= the k-th licensed buffer element
7 Cand := Cand ? bk if compatible(bk,mi)
8 if Cand = {} then B := B ?mi
9 if Cand 6= {} then
10 antei := most salient element of Cand
11 C := augment(C,antei,mi)
Figure 1: Incremental Model: Base Algorithm
as ?Hillary Clinton ... Clinton?). Demonstrative NPs
are mapped to nominal NPs by matching their heads.
Definite NPs match with noun chunks that are longer
than one token3 and must be contained completely
without the determiner (e.g. ?Recent events ... the
events?). From the candidates that pass these filters
the most salient one is selected as antecedent. If two
or more candidates with equal salience are available,
the closest one is chosen.
2.3 Binding Theory as a Filter
There is another principle that help reduce the num-
ber of candidates even further: binding theory. We
know that ?He? and ?him? cannot be coreferent in
the sentence ?He gave him the book?. Thus, the pair
?He?-?him? need not be considered at all. Actually,
there are subtle restrictions to be captured here. We
have not implemented a full-blown binding theory
on top of our dependency parser, yet. Instead, we
approximated binding restrictions by subclause de-
tection. ?He? and ?him? in the example above are in
the same subclause (the main clause) and are, thus,
exclusive. This is true for nouns and personal pro-
nouns, only. Possesive and reflexive pronouns are
allowed to be bound in the same subclause.
2.4 An Empirically-based Salience Measure
Since we look for a simple and fast salience measure
and do not apply machine learning in our baseline
system, our measure is solely based on the gram-
matical functions (given by the dependency labels)
of the true mentions. Grammatical functions have
3If we do not apply this restriction too many false positives
are produced.
played a major role in calculating salience, espe-
cially in rule based system such as (Hobbs, 1976;
Lappin and Leass, 1994; Mitkov et al, 2002; Sid-
dharthan, 2003). Instead of manually specifying
the weights for the dependency labels like (Lappin
and Leass, 1994), we derived them empirically from
the coreference CoNLL 2011 gold standard (train-
ing data). The salience of a dependency label, D,
is estimated by the number of true mentions in the
gold standard that bear D (i.e. are connected to their
heads with D), divided by the total number of true
mentions. The salience of the label subject is thus
calculated by:
Number of truementions bearing subject
Total number of truementions
For a given dependency label, this fraction indicates
how strong is the label a clue for bearing an an-
tecedent. This way, we get a hierarchical order-
ing of the dependency labels (subject > object >
pobject > ...) according to which antecedents are
ranked. Clearly, future work will have to establish
a more elaborate calculation of salience. To our
surprise, however, this salience measure performed
quite well, at least together with our incremental ar-
chiteture.
3 Evaluation
The results of our evaluation over the CoNLL 2011
shared task development set are given in Fig. 2 (de-
velopment set) and 3 (official results on the test set).
The official overall score of our system in the
open regular setting is 51.77.
Our results are mediocre. There are several rea-
83
Metric R P F1
CEAFM 49.73 49.73 49.73
CEAFE 44.26 37.70 40.72
BCUB 59.17 71.66 66.06
BLANC 62.70 72.74 64.82
MUC 42.20 49.21 45.44
Figure 2: CoNLL 2011 Development Set Results
Metric R P F1
CEAFM 50.03 50.03 50.03
CEAFE 41.28 39.70 40.48
BCUB 61.70 68.61 64.97
BLANC 66.05 73.90 69.05
MUC 49.04 50.71 49.86
Figure 3: CoNLL 2011 Test Set Results
sons for that. First and foremost, the scorer requires
chunk extensions to match perfectly. That is, even
if the head of an antecedent is found, this does not
count if the chunk extension of that noun phrase was
not correctly identified. Since chunks do not play a
major role in depencendy parsing, our approxima-
tion might be faulty4. Another shortcomming are
nominal anaphora that can not be identified by string
matching (e.g. Obama ... The president). Our sim-
ple salience-based approach does not cope at all with
this type of anaphora.
4 Related Work
(Ng, 2010) discusses the entity-mention model
which operates on emerging coreference sets to cre-
ate features describing the relation of an anaphor
candidate and established coreference sets. (Luo
et al, 2004) implemented such a model but it per-
formed worse than the mention-pair model. (Yang
et al, 2004) presented an incremental model which
used some coreference set specific features, namely
introducing the number of mentions in a set as a
feature besides checking for morphological compat-
ibility with all mentions in a set. They also report
that the set size feature only marginally improves or
in some combinations even worsens system perfor-
mance. (Daume III and Marcu, 2005) introduced
a wide range of set specific features, capturing set
4Especially Asiatic names pose problems to our parser, quite
often the extensions could not get correctly fixed.
count, size and distribution amongst others, in a joint
model for the ACE data.
All the above mentioned systems use an incre-
mental model to generate features describing the
emerging coreference sets and the anaphor candi-
date. In contrast, we use an incremental architecture
to control pair generation in order to prevent gener-
ation of either redundant or irrelevant pairs.
5 Conclusions
We have introduced an incremental model for coref-
erence resolution based on an empirically derived
salience measure that is meant as a simple and
very fast baseline system. We do not use machine
learning, nor do we resolve more complex nominal
anaphora such as ?Obama ... The president? (but we
handle those that can be resolved by simple pattern
matching, e.g. Hilary Clinton .. Clinton). Given
these restrictions, our system performed well.
The central idea of our approach is that the evolv-
ing coreference sets should restrict the access to an-
tecedent candidates in a twofold way: by use of vir-
tual prototypes that accumulate the properties of all
members of a coreference set (e.g. wrt. animacy),
but also by restricting reachable buffer elements (i.e.
yet unattached markables).
The benefits of our incremental model are:
? due to the restricted access to antecedent candi-
dates, the number of generated candidate pairs
can be reduced drastically5
? no coreference clustering phase is needed
? the problem of underspecification that exists for
any pair-wise model can be compensated by a
virtual prototype that accumulates the proper-
ties of the elements of a coreference set
These benefits are independent of the underly-
ing classification scheme, be it a simple salience-
based one or a more advanced machine learning one.
The work presented here thus would like to opt for
further research based on incremental architectures.
Web demos for English and German are available6.
5We observed a reduction over 75% in some experiments
when moving from a mention-pair to an incremental entity-
mention model.
6http://kitt.cl.uzh.ch/kitt/coref/
84
References
Jie Cai and Michael Strube. 2010. Evaluation metrics
for end-to-end coreference resolution systems. In Pro-
ceedings of the SIGdial 2010 Conference: The 11th
Annual Meeting of the Special Interest Group on Dis-
course and Dialogue.
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for coreference
resolution. In Human Language Technologies 2007:
The Conference of the North American Chapter of the
Association for Computational Linguistics; Proceed-
ings of the Main Conference, pages 81?88, Rochester,
New York, April. Association for Computational Lin-
guistics.
Hal Daume III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In HLT ?05: Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 97?104, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Jerry R. Hobbs. 1976. Pronoun resolution. Technical
Report 76-1, Research Report, Department of Com-
puter Sciences, City College, City University of New
York.
Manfred Klenner and Etienne Ailloud. 2009. Opti-
mization in Coreference Resolution Is Not Needed: A
Nearly-Optimal Zero-One ILP Algorithm with Inten-
sional Constraints. In Proc. of the EACL.
Manfred Klenner. 2007. Enforcing consistency on coref-
erence sets. In In Recent Advances in Natural Lan-
guage Processing (RANLP), pages 323?328.
Shalom Lappin and Herbert J Leass. 1994. An algorithm
for pronominal anaphora resolution. Computational
Linguistics, 20:535?561.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In HLT ?05: Proceedings of the con-
ference on Human Language Technology and Empir-
ical Methods in Natural Language Processing, pages
25?32, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Ruslan Mitkov, Richard Evans, and Constantin Orasan.
2002. A new, fully automatic version of mitkov?s
knowledge-poor pronoun resolution method. In CI-
CLing ?02: Proceedings of the Third International
Conference on Computational Linguistics and Intel-
ligent Text Processing, pages 168?186, London, UK.
Springer-Verlag.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?10, pages 492?
501, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 2 - Volume 2, EMNLP
?09, pages 968?977, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Gerold Schneider. 2008. Hybrid Long-Distance Func-
tional Dependency Parsing. Doctoral Thesis, Institute
of Computational Linguistics, Univ. of Zurich.
Advaith Siddharthan. 2003. Resolving pronouns ro-
bustly: Plumbing the depths of shallowness. In Pro-
ceedings of the Workshop on Computational Treat-
ments of Anaphora, 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL 2003).
Wee M. Soon, Hwee T. Ng, and Daniel. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544, December.
Xiaofeng Yang, Jian Su, Guodong Zhou, and Chew Lim
Tan. 2004. An np-cluster based approach to corefer-
ence resolution. In Proceedings of the 20th interna-
tional conference on Computational Linguistics.
85
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 116?120,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
UZH in the BioNLP 2013 GENIA Shared Task
Gerold Schneider, Simon Clematide, Tilia Ellendorff, Don Tuggener, Fabio Rinaldi,
{rinaldi,gschneid,siclemat,ellendorff,tuggener}@cl.uzh.ch
Institute of Computational Linguistics, University of Zurich, Switzerland
Gintare? Grigonyte?
Stockholm University, Department of Linguistics, Section for Computational Linguistics
gintare@ling.su.se
Abstract
We describe a biological event detec-
tion method implemented for the Genia
Event Extraction task of BioNLP 2013.
The method relies on syntactic depen-
dency relations provided by a general NLP
pipeline, supported by statistics derived
from Maximum Entropy models for can-
didate trigger words, for potential argu-
ments, and for argument frames.
1 Introduction
The OntoGene team at the University of Zurich
has developed text mining applications based on
a combination of deep-linguistic analysis and ma-
chine learning techniques (Rinaldi et al, 2012b;
Clematide and Rinaldi, 2012; Rinaldi et al, 2010).
Our approaches have proven competitive in sev-
eral shared task evaluations (Rinaldi et al, 2013;
Clematide et al, 2011; Rinaldi et al, 2008). Addi-
tionally, we have developed advanced systems for
the curation of the biomedical literature (Rinaldi
et al, 2012a).
Our participation in the Genia Event Extraction
task of BioNLP 2013 (Kim et al, 2013) was moti-
vated by the desire of testing our technologies on
a more linguistically motivated task. In the course
of our participation we revised several modules of
our document processing pipeline, however we did
not have sufficient resources to completely revise
the final module which generates the event struc-
tures, and we still relied on a module which we
had developed for our previous participation to the
BioNLP shared task.
The final submission was composed by our
standard preprocessing module (described briefly
in section 2) and novel probability models (section
3), combined within the old event generator (sec-
tion 4).
2 Preprocessing
The OntoGene environment is based on a pipeline
of several NLP tools which all operate on a com-
mon XML representation of the original docu-
ment.
Briefly, the pipeline includes modules for
sentence-splitting, tokenization, part-of-speech
tagging, lemmatization, stemming, term-
recognition (not used for the BioNLP shared
task), chunking, dependency-parsing and event
generation. Different variants of those modules
have been used in different instantiations of the
pipeline. For the BioNLP 2013 participation,
lingpipe was used for sentence splitting, tok-
enization and PoS tagging, morpha (Minnen et
al., 2001) was used for lemmatization, a python
implementation of the Porter stemmer for stem-
ming, LTTT (Grover et al, 2000), was used for
chunking, and the Pro3Gres parser (Schneider,
2008) for dependency analysis.
As we have made good experiences with a
rule based system for anaphora resolution in the
BioNLP 2011 shared task (Tuggener et al, 2011),
we implemented a similar approach that resolves
anaphors to terms identified during preprocessing.
Rules contain patterns like ?X such as Y? or ?X
is a Y?, and pronouns are resolved to the nearest
grammatical subject or object. Anaphora resolu-
tion led to an improvement of 0.2% recall on the
development set, while precision was hardly af-
fected.
3 Probability models
Several probability models have been computed
from the training data in order to be used to score
and filter candidate events generated by the sys-
tem. The following models played a role in the
final submission:
P (eventType | trigger candidate) (1)
116
P (frame ? eventType | trigger candidate) (2)
P (role ? eventType | protein) (3)
P (role(t, d) | synpath(t, d)) (4)
For all of them we computed global Maximum
Likelihood Estimations (MLE), using the training
and development datasets from the 2013 and 2011
challenges. For all of the models above, except
for the last one, we also estimated the probabili-
ties by a Maximum Entropy (ME) approach. The
MegaM tool (Daume? III, 2004) allows for a super-
vised training of binary classifiers where the class
probability is optimized by adjusting the feature
weights and not just the binary classification deci-
sion itself. This helps to deal with the imbalanced
classes such as the distribution of true or false trig-
gerword candidates.
For the classification of trigger candidates
(Equation 1), a binary ME classifier for each event
type is separately trained, based on local and
global features as described below. The trigger-
word candidates are collected from the training
data using their stemmed representation as a selec-
tion criterion. We generally exclude triggerword
candidates that occur in less than 1% as true trig-
gers in the training set. Within the data, we found
that triggers that consist of more than one word are
rather rare (less than 5% of all triggers, most of
them occurring once). However, we transformed
these multiword triggers to singleword triggers,
replacing them by their first content word.
The choice of ME features, partly inspired by
(Ekbal et al, 2013), can be grouped into features
derived from the triggerword itself (word), fea-
tures from the sentence of the triggerword (con-
text), and features from article-wide information
(global).
Word features: (1) The text, lemma, part of
speech (PoS), stem and local syntactic dependency
of the triggerword candidate as computed by the
Pro3Gres parser. (2) Information whether a trig-
gerword candidate is head of a chunk as well as
whether the chunk is nominal or verbal
Context features: Unigrams and bigrams in a
window of variable size to left and right of the trig-
gerword candidate; three types of uni- and bigrams
are used: PoS, lemmas and stems; for unigrams we
also include the lower-cased words; for bigrams,
the triggerword candidate itself is included in the
first bigram to either side.
Global features: (1) Presence or absence of a
protein in a window of a given size around the
triggerword candidate (Boolean feature); only the
most frequent proteins of an article are considered.
(2) The zone in an article where the triggerword
candidate appears, e.g. Title/Abstract, Introduc-
tion, Background, Material and Methods, Results
and Discussion, Caption and Conclusion.
Feature engineering was done by testing differ-
ent combinations of settings (window size, thresh-
olds) with the aim of finding an optimal overall
ME model which reaches the lowest error rates for
all event types. The error rate of the candidate set
was measured as the cumulative error mass com-
puted from the assigned class probability as fol-
lows: if the trigger candidate is a true positive, the
error is 1 minus the probability assigned by the
classifier. If the candidate is a false positive, the
error is the probability assigned by the classifier.
Our approach does not allow us to compute an er-
ror rate for false negatives, because we simply rely
on the set of trigger words seen in the training data
as possible candidates.
In these experiments, we discovered that for
most event types an optimal setting for the context
features considers a wide span of about 20 tokens
to the left and right of the triggerword. Includ-
ing bigrams of lemmas, stems and PoS delivered
the best results compared to including only one or
two of these bigram types. Context features can be
parameterized according to how much positional
information they contain: the distance of a word
to the right and left of the trigger, only the direc-
tion (left or right) or no position information at all
(bag of unigrams/bigrams). We found that the ex-
act positional information is only important for the
first word to the left and right (adjacent to the trig-
gerword), whereas for all words that are further
away it is favorable to only use the direction in re-
lation to the trigger. A window size of 10 words
within which proteins are found in the context of
a triggerword gave the best results. The optimal
number of the most frequent proteins considered
within this window was found to be the 10 most
frequent proteins within an article.
The second type of ME classifier (Equation 2)
has the purpose of calculating the probabilities of
event frames for all event types given a trigger
word. We use the term frame for a combination of
arguments that an event is able to accept as theme
and cause and whether these arguments are real-
117
ized as proteins or subevents.
For the classification of proteins (Equation 3),
again separate binary ME classifiers were built in
order to estimate the probability that a protein has
a role (theme or cause) in an event of a given type.
4 Event Generation
We tested two independent event generation mod-
ules, one based on a revision of our previous 2009
submission (Kaljurand et al, 2009) and one which
is a totally new implementation. We could do only
preliminary tests with the second module, which
however showed promising results, in particular
with much better recall than the older module (up
to 65.23%), despite the very little time that we
could invest in its development. The best F-score
that we could reach was still slightly inferior to the
one of the old module at the deadline for submis-
sion of results. In the rest of this paper we will
describe only the module which was used in the
official submission.
The event extraction process consists of three
phases. First, event candidates are generated,
based on trigger words and their context, using the
ME and MLE probabilities pT (equation 1).
Second, individual arguments of an event are
generated. We calculate the MLE probability pR
of an argument role (e.g. Theme) to occur as part
of a given event type, as follows:
pR(Role |EventType) =
f(Role ? EventType)
f(EventType)
(5)
We obtained the best results on the development
corpus when combining the probabilities as:
pA =
pT ? pT ? pR
pT + pT + pR
(6)
We generate arguments, using an MLE syntac-
tic path and an ME argument model, as follows.
The syntactic path between the trigger word and
every term (protein or subordinate event) is con-
sidered. If they are syntactically connected, and
if the probability of a syntactic path to express an
event is above a threshold, it is selected. As this is
a filtering step, it negatively affects recall.
We calculate the MLE probability ppath that
a syntactic configuration fills an argument slot.
Syntactic configurations consist of the head word
(trigger) HWord, the head event type HType, the
dependent word DWord, the dependent event type
DType, and the syntactic path Path between them.
In order to deal with sparse data, we use a
smoothed model.
ppath(Arg |HWord, HType, DWord, DType, Path) =
1
w1+w2+w3
? (
w1 ?
f(HWord, HType, DWord, DType, Path?Arg)
f(HWord, HType, DWord, DType, Path) +
w2 ?
f(HType, DType, Path?Arg)
f(HType, DType, Path) +
w3 ?
f(HType, DType?Arg)
f(HType, DType) ) (7)
The weights were emprically set as w1 = 4,
w2 = 2 and w3 = 1.5. The fact that the weights
decrease approximates a back-off model. The final
probability had to be larger than 0.2.
We have also used an ME model which delivers
the probability parg that a term is the argument of
a specific event, see formula 3. If this ME model
predicts with a probability of above 80% that the
term is not an argument, the search fails. Other-
wise, the probabilities are combined. On the de-
velopment corpus, we achieved best results when
using the harmonic mean:
pargument = 2 ?
ppath ? parg
ppath + parg
(8)
As a last step, the several arguments of an event
are combined into a frame. We have tested mod-
els predicting an entire frame directly, and models
combining the individual arguments generated in
the previous step. The latter approach performed
better. Any permutation of the argument candi-
dates could constitute a frame. Only frames seen
in the training corpus for a given event type are
considered. We have again used an ME and an
MLE model for predicting frames.
The ME model predicts pframe.ME , see for-
mula 2. We have also used two MLE models:
the first one delivers the probability pframe.MLE
based on the event type only, the second one
pframeword.MLE also considers the trigger word
and is much sparser (a low default is thus used for
unseen words). The probability of the individual
arguments also needs to be taken into considera-
tion. We used the mean of the individual argu-
ments? probabilities (pargs?mean).
5 Evaluation
In our analysis of errors, we noticed that frames
with more than one argument are created ex-
tremely rarely. The problem is that frames with
several arguments are rarer because the context
often does not offer the possibility to attach sev-
eral arguments. Therefore, we consistently un-
dergenerated with pargs?mean as outlined above.
118
Event Class gold (match) answer (match) recall prec. fscore
SVT-TOTAL 1117 ( 619) 851 ( 619) 55.42 72.74 62.91
EVT-TOTAL 1490 ( 698) 1103 ( 698) 46.85 63.28 53.84
REG-TOTAL 1694 ( 168) 618 ( 168) 9.92 27.18 14.53
All events total 3184 ( 866) 1721 ( 866) 27.20 50.32 35.31
Table 1: Results on the development set, measured using ?strict equality?.
Event Class gold (match) answer (match) recall prec. fscore
Gene expression 619 (400) 497 (400) 64.62 80.48 71.68
Transcription 101 (26) 100 (26) 25.74 26.00 25.87
Protein catabolism 14 (10) 15 (10) 71.43 66.67 68.97
Localization 99 (34) 39 (34) 34.34 87.18 49.28
=[SIMPLE ALL]= 833 (470) 651 (470) 56.42 72.20 63.34
Binding 333 (74) 264 (74) 22.22 28.03 24.79
Protein modification 1 (0) 0 (0) 0.00 0.00 0.00
Phosphorylation 160 (119) 168 (119) 74.38 70.83 72.56
Ubiquitination 30 (0) 0 (0) 0.00 0.00 0.00
Acetylation 0 (0) 0 (0) 0.00 0.00 0.00
Deacetylation 0 (0) 0 (0) 0.00 0.00 0.00
=[PROT-MOD ALL]= 191 (119) 168 (119) 62.30 70.83 66.30
Regulation 288 (23) 84 (23) 7.99 27.38 12.37
Positive regulation 1130 (129) 444 (129) 11.42 29.05 16.39
Negative regulation 526 (54) 166 (54) 10.27 32.53 15.61
=[REGULATION ALL]= 1944 (206) 694 (206) 10.60 29.68 15.62
==[EVENT TOTAL]== 3301 (869) 1777 (869) 26.33 48.90 34.23
Table 2: Results on the test data, measured using ?strict equality?.
We have added a number of heuristics to boost
multi-argument frames. Multiplying the probabil-
ity of a frame by its cubed length (giving two-
argument slots 9 times higher probability), and
giving Cause-slots 50% higher scores globally led
to best results.
We mainly trained and evaluated using the
?strict equality? evaluation criteria as our refer-
ence. The results on the development data are
shown in table 1. With more relaxed equality def-
initions, the results were always a few percentage
points better. Our results in the official test run are
shown in table 2. In sum, our submitted system
has good performance for simple events, bad per-
formance for Binding events, and a bias towards
precision due to a syntactic-based filtering step.
6 Conclusions and Future work
Our participation in the 2013 BioNLP shared task
was a useful opportunity to revise components of
the OntoGene pipeline and begin the implemen-
tation of a novel event generator. Due to lack of
time, it was not completed in time for the official
submission. We will continue its development and
use the BioNLP datasets.
Acknowledgments
This research is partially funded by the
Swiss National Science Foundation (grant
105315 130558/1).
References
[Clematide and Rinaldi2012] Simon Clematide and
Fabio Rinaldi. 2012. Ranking relations between
diseases, drugs and genes for a curation task.
Journal of Biomedical Semantics, 3(Suppl 3):S5.
[Clematide et al2011] Simon Clematide, Fabio Ri-
naldi, and Gerold Schneider. 2011. Ontogene at
calbc ii and some thoughts on the need of document-
wide harmonization. In Proceedings of the CALBC
II workshop, EBI, Cambridge, UK, 16-18 March.
[Daume? III2004] Hal Daume? III. 2004. Notes on
CG and LM-BFGS optimization of logistic regres-
sion. Paper available at http://pub.hal3.
name#daume04cg-bfgs, implementation avail-
able at http://hal3.name/megam/, August.
[Ekbal et al2013] Asif Ekbal, Sriparna Saha, and
Sachin Girdhar. 2013. Evolutionary approach for
classifier ensemble: An application to bio-molecular
event extraction. In Ajith Abraham and Sabu M
Thampi, editors, Intelligent Informatics, volume 182
of Advances in Intelligent Systems and Computing,
pages 9?15. Springer Berlin Heidelberg.
119
[Grover et al2000] Claire Grover, Colin Matheson, An-
drei Mikheev, and Marc Moens. 2000. Lt ttt - a flex-
ible tokenisation tool. In Proceedings of Second In-
ternational Conference on Language Resources and
Evaluation (LREC 2000).
[Kaljurand et al2009] Kaarel Kaljurand, Gerold
Schneider, and Fabio Rinaldi. 2009. UZurich in the
BioNLP 2009 Shared Task. In Proceedings of the
BioNLP workshop, Boulder, Colorado.
[Kim et al2013] Jin-Dong Kim, Yue Wang, and Ya-
mamoto Yasunori. 2013. The genia event extraction
shared task, 2013 edition - overview. In Proceedings
of BioNLP Shared Task 2013 Workshop, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
[Minnen et al2001] Guido Minnen, John Carroll, and
Darren Pearce. 2001. Applied morphological pro-
cessing of English. Natural Language Engineering,
7(3):207?223.
[Rinaldi et al2008] Fabio Rinaldi, Thomas Kappeler,
Kaarel Kaljurand, Gerold Schneider, Manfred Klen-
ner, Simon Clematide, Michael Hess, Jean-Marc
von Allmen, Pierre Parisot, Martin Romacker, and
Therese Vachon. 2008. OntoGene in BioCreative
II. Genome Biology, 9(Suppl 2):S13.
[Rinaldi et al2010] Fabio Rinaldi, Gerold Schneider,
Kaarel Kaljurand, Simon Clematide, Therese Va-
chon, and Martin Romacker. 2010. OntoGene in
BioCreative II.5. IEEE/ACM Transactions on Com-
putational Biology and Bioinformatics, 7(3):472?
480.
[Rinaldi et al2012a] Fabio Rinaldi, Simon Clematide,
Yael Garten, Michelle Whirl-Carrillo, Li Gong,
Joan M. Hebert, Katrin Sangkuhl, Caroline F. Thorn,
Teri E. Klein, and Russ B. Altman. 2012a. Using
ODIN for a PharmGKB re-validation experiment.
Database: The Journal of Biological Databases and
Curation.
[Rinaldi et al2012b] Fabio Rinaldi, Gerold Schneider,
and Simon Clematide. 2012b. Relation mining ex-
periments in the pharmacogenomics domain. Jour-
nal of Biomedical Informatics, 45(5):851?861.
[Rinaldi et al2013] Fabio Rinaldi, Simon Clematide,
Simon Hafner, Gerold Schneider, Gintare
Grigonyte, Martin Romacker, and Therese Va-
chon. 2013. Using the ontogene pipeline for
the triage task of biocreative 2012. The Journal
of Biological Databases and Curation, Oxford
Journals.
[Schneider2008] Gerold Schneider. 2008. Hy-
brid Long-Distance Functional Dependency Pars-
ing. Doctoral Thesis, Institute of Computational
Linguistics, University of Zurich.
[Tuggener et al2011] D Tuggener, M Klenner,
G Schneider, S Clematide, and F Rinaldi. 2011. An
incremental model for the coreference resolution
task of bionlp 2011. In BioNLP 2011, pages 151?
152. Association for Computational Linguistics
(ACL), June.
120
