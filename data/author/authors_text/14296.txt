Coling 2010: Poster Volume, pages 1122?1130,
Beijing, August 2010
Informed ways of improving data-driven
dependency parsing for German
Wolfgang Seeker
University of Stuttgart
Inst. fu?r Maschinelle Sprachverarbeitung
seeker@ims.uni-stuttgart.de
Bernd Bohnet
University of Stuttgart
Inst. fu?r Maschinelle Sprachverarbeitung
Bernd.Bohnet@ims.uni-stuttgart.de
Lilja ?vrelid
University of Potsdam
Institut fu?r Linguistik
ovrelid@uni-potsdam.de
Jonas Kuhn
University of Stuttgart
Inst. fu?r Maschinelle Sprachverarbeitung
jonas@ims.uni-stuttgart.de
Abstract
We investigate a series of targeted modifi-
cations to a data-driven dependency parser
of German and show that these can be
highly effective even for a relatively well
studied language like German if they are
made on a (linguistically and methodolog-
ically) informed basis and with a parser
implementation that allows for fast and
robust training and application. Mak-
ing relatively small changes to a range
of very different system components, we
were able to increase labeled accuracy on
a standard test set (from the CoNLL 2009
shared task), ignoring gold standard part-
of-speech tags, from 87.64% to 89.40%.
The study was conducted in less than five
weeks and as a secondary project of all
four authors. Effective modifications in-
clude the quality and combination of auto-
assigned morphosyntactic features enter-
ing machine learning, the internal feature
handling as well as the inclusion of global
constraints and a combination of different
parsing strategies.
1 Introduction
The past years have seen an enormous surge of in-
terest in dependency parsing, mainly in the data-
driven paradigm, and with a particular emphasis
on covering a whole set of languages with a single
approach. The reasons for this interest are mani-
fold; the availability of shared task data from var-
ious CoNLL conferences (among others (Buch-
holz and Marsi, 2006; Hajic? et al, 2009)), com-
prising collections of languages based on a sin-
gle representation format, has certainly been in-
strumental. But likewise, the straightforward use-
fulness of dependency representations for a num-
ber of tasks plays an important role. The rela-
tive language independence of the representations
makes dependency parsing particularly attractive
for multilingually oriented work, including ma-
chine translation.
As data-driven approaches to dependency pars-
ing have reached a certain level of maturity, it may
appear as if further improvements of parsing per-
formance have to rely on relatively advanced tun-
ing procedures, such as sophisticated automatic
feature selection procedures or combinations of
different parsing approaches with complementary
strengths. It is indeed still hard to pinpoint the
structural properties of a language (or annotation
scheme) that make the parsing task easier for a
particular approach, so it may seem best to leave
the decision to a higher-level procedure.
This paper starts from the suspicion that
while sophisticated tuning procedures are cer-
tainly helpful, one should not underestimate the
potential of relatively simple modifications of the
experimental set-up, such as a restructuring of as-
pects of the dependency format, a targeted im-
provement of the quality of automatically as-
signed features, or a simplification of the feature
space for machine learning ? the modifications
just have to be made in an informed way. This
1122
presupposes two things: (i) a thorough linguistic
understanding of the issues at hand, and (ii) a rel-
atively powerful and robust experimental machin-
ery which allows for experimentation in various
directions and which should ideally support a fast
turn-around cycle.
We report on a small pilot study exploring the
potential of relatively small, informed modifica-
tions as a way of improving parsing accuracy
even for a language that has received considerable
attention in the parsing literature, including the
dependency parsing literature, namely German.
Within a timeframe of five weeks and spending
only a few hours a day on the project (between a
group of four people), we were able to reach some
surprising improvements in parsing accuracy.
By way of example, we experimented with
modifications in a number of rather different sys-
tem areas, which we will discuss in the course
of this paper after a brief discussion of related
work and the data basis in Section 2. Based on a
second-order maximum spanning tree algorithm,
we used a hash kernel to facilitate the mapping
of the features onto their weights for a very large
number of features (Section 3); we modified the
dependency tree representation for prepositional
phrases, adding hierarchical structure that facili-
tates the picking up of generalizations (Section 4).
We take advantage of a morphological analyzer
to train an improved part-of-speech tagger (Sec-
tion 5), and we use knowledge about the structure
of morphological paradigms and the morphology-
syntax interface in the feature design for machine
learning (Section 6). As is known from other stud-
ies, the combination of different parsing strategies
is advantageous; we include a relatively simple
parser stacking procedure in our pilot study (Sec-
tion 7), and finally, we apply Integer Linear Pro-
gramming in a targeted way to add some global
constraints on possible combinations of arc labels
with a single head (Section 8). Section 9 offers a
brief conclusion.
2 Related Work and Data Basis
We quickly review the situation in data-driven de-
pendency parsing in general and on applying it to
German specifically.
The two main approaches to data-driven de-
pendency parsing are transition based dependency
parsing (Nivre, 2003; Yamada and Matsumoto,
2003; Titov and Henderson, 2007) and maximum
spanning tree based dependency parsing (Eis-
ner, 1996; Eisner, 2000; McDonald and Pereira,
2006). Transition based parsers typically have
a linear or quadratic complexity (Attardi, 2006).
Nivre (2009) introduced a transition based non-
projective parsing algorithm that has a worst case
quadratic complexity and an expected linear pars-
ing time. Titov and Henderson (2007) combined
a transition based parsing algorithm, using beam
search, with a latent variable machine learning
technique.
Maximum spanning tree based dependency
parsers decompose a dependency structure into
factors. The factors of the first order maximum
spanning tree parsing algorithm are edges consist-
ing of the head, the dependent (child) and the edge
label. This algorithm has a quadratic complexity.
The second order parsing algorithm of McDonald
and Pereira (2006) uses a separate algorithm for
edge labeling. In addition to the first order fac-
tors, this algorithm uses the edges to those chil-
dren which are closest to the dependent and has a
complexity of O(n3). The second order algorithm
of Carreras (2007) uses in addition to McDonald
and Pereira (2006) the child of the dependent oc-
curring in the sentence between the head and the
dependent as well as the edge from the dependents
to a grandchild. The edge labeling is an integral
part of the algorithm which requires an additional
loop over the labels. This algorithm therefore has
a complexity of O(n4). Johansson and Nugues
(2008) reduced the required number of loops over
the edge labels by considering only the edges that
existed in the training corpus for a distinct head
and child part-of-speech tag combination.
Predating the surge of interest in data-based
dependency parsing, there is a relatively long
tradition of dependency parsing work on Ger-
man, including for instance Menzel and Schro?der
(1998) and Duchier and Debusmann (2001). Ger-
man was included in the CoNLL shared tasks in
2006 (Multilingual Dependency Parsing, (Buch-
holz and Marsi, 2006)) and in 2009 (Syntactic and
Semantic Dependencies in Multiple Languages,
(Hajic? et al, 2009)) with data based on the TIGER
1123
corpus (Brants et al, 2002) in both cases. Since
the original TIGER treebank is in a hybrid phrase-
structural/dependency format with a relatively flat
hierarchical structure, conversion to a pure depen-
dency format involves some non-trivial steps. The
2008 ACL Workshop on Parsing German included
a specific shared task on dependency parsing of
German (Ku?bler, 2008), based on two sets of data:
again the TIGER corpus ? however with a differ-
ent conversion routine than for the CoNLL tasks ?
and the Tu?Ba-D/Z corpus (Hinrichs et al, 2004).
In the 2006 CoNLL task and in the 2008 ACL
Workshop task, the task was dependency parsing
with given gold standard part-of-speech tags from
the corpus. This is a valid way of isolating the
specific subproblem of parsing, however it is clear
that the task does not reflect the application set-
ting which includes noise from automatic part-of-
speech tagging. In the 2009 CoNLL task, both
gold standard tags and automatically assigned tags
were provided. The auto-tagged version was cre-
ated with the standard model of the TreeTagger
(Schmid, 1995) (i.e., with no domain-specific tag-
ger training).
In our experiments, we used the data set from
the 2009 CoNLL task, for which the broadest
comparison of recent parsing approaches exists.
The highest-scoring system in the shared task was
Bohnet (2009) with a labeled accuracy (LAS) of
87.48%, on auto-tagged data. The highest-scoring
(in fact the only) system in the dependency pars-
ing track of the 2008 ACL Workshop on parsing
German was Hall and Nivre (2008) with an LAS
of 90.80% on gold-tagged data, and with a data
set that is not comparable to the CoNLL data.1
3 Hash Kernel
Our parser is based on a second order maximum
spanning tree algorithm and uses MIRA (Cram-
mer et al, 2006) as learning technique in combi-
nation with a hash kernel. The hash kernel has
a higher accuracy since it can use additional fea-
tures found during the creation of the dependency
1To get an idea of how the data sets compare, we trained
the version of our parser described in Section 3 (i.e., with-
out most of the linguistically informed improvements) on
this data, achieving labeled accuracy of 92.41%, compared
to 88.06% for the 2009 CoNLL task version.
tree in addition to the features extracted from the
training examples. The modification to MIRA is
simple: we replace the feature-index mapping that
maps the features to indices of the weight vector
by a random function. Usually, the feature-index
mapping in the support vector machine has two
tasks: The mapping maps the features to an index
and it filters out features that never occurred in a
dependency tree. In our approach, we do not filter
out these features, but use them as additional fea-
tures. It turns out that this choice improves pars-
ing quality. Instead of the feature-index mapping
we use the following hash function:2
h ? |(l xor(l ? 0xffffffff00000000 >> 32))% size|
The Hash Kernel for structured data uses the hash
function h : J ? {1...n} to index ? where ?
maps the observations X to a feature space. We
define ?(x, y) as the numeric feature representa-
tion indexed by J . The learning problem is to fit
the function F so that the errors of the predicted
parse tree y are as low as possible. The scoring
function of the Hash Kernel is defined as:3
F (x, y) = ??w ? ?(x, y)
For different j, the hash function h(j) might gen-
erate the same value k. This means that the hash
function maps more than one feature to the same
weight which causes weight collisions. This pro-
cedure is similar to randomization of weights (fea-
tures), which aims to save space by sharing val-
ues in the weight vector (Blum, 2006; Rahimi
and Recht, 2008). The Hash Kernel shares values
when collisions occur that can be considered as
an approximation of the kernel function, because
a weight might be adapted due to more than one
feature. The approximation works very well with
a weight vector size of 115 million values.
With the Hash Kernel, we were able to improve
on a baseline parser that already reaches a quite
high LAS of 87.64% which is higher than the top
score for German (87.48%) in the CoNLL Shared
task 2009. The Hash Kernel improved that value
by 0.42 percentage points to 88.06%. In addition
to that, we obtain a large speed up in terms of pars-
ing time. The baseline parser spends an average of
426 milliseconds to parse a sentence of the test
2>> n shifts n bits right, and % is the modulo operation.
3??w is the weight vector and the size of ??w is n.
1124
set and the parser with Hash Kernel only takes
126 milliseconds which is an increase in speed
of 3.4 times. We get the large speed up because
the memory access to a large array causes many
CPU cache misses which we avoid by replacing
the feature-index mapping with a hash function.
As mentioned above, the speedup influences the
experimenters? opportunities for explorative de-
velopment since it reduces the turnaround time for
experimental trials.
4 Restructuring of PPs
In a first step, we applied a treebank transforma-
tion to our data set in order to ease the learning
for the parser. We concentrated on prepositional
phrases (PP) to get an idea how much this kind
of transformation can actually help a parser. PPs
are notoriously flat in the TIGER Treebank anno-
tation (from which our data are derived) and they
do not embed a noun phrase (NP) but rather attach
all parts of the noun phrase directly at PP level.
This annotation was kept in the dependency ver-
sion and it can cause problems for the parser since
there are two different ways of annotating NPs: (i)
for normal NPs where all dependents of the noun
are attached as daughters of the head noun and (ii)
for NPs in PPs where all dependents of the noun
are attached as daughters to the preposition thus
being sisters to their head noun. We changed the
annotation of PPs by identifying the head noun in
the PP and attaching all of its siblings to it. To find
the correct head, we used a heuristic in the style of
Magerman (1995). The head is chosen by taking
the rightmost daughter of the preposition that has
a category label according to the heuristic and is
labeled with NK (noun kernel element).
Table 1 shows the parser performance on the
data after PP-restructuring.4 The explanation for
the benefit of the restructuring is of course that
4Note that we are evaluating against a gold standard here
(and in the rest of the paper) which has been restructured as
well. With a different gold standard one could argue that the
absolute figures we obtain are not fully comparable with the
original CoNLL shared task. However, since we are doing
dependency parsing, the transformation does neither add nor
remove any nodes from the structure nor do we change any
labels. The only thing that is done during the transforma-
tion is the reattachment of some daughters of a PP. This is
only a small modification, and it is certainly linguistically
warranted.
now there is only one type of NP in the whole cor-
pus which eases the parser?s task to correctly learn
and identify them.
dev. set test set
LAS UAS LAS UAS
hash kernel 87.40 89.79 88.06 90.24
+restructured 87.49 89.97 88.30 90.44
Table 1: Parser performance on restructured data
Since restructuring parts of the corpus seems
beneficial, there might be other structures where
more consistent annotation could help the parser,
e. g., coordination or punctuation (like in the 2008
ACL Workshop data set, cp. Footnote 1).
5 Part-of-Speech Tagging
High quality part-of-speech (PoS) tags can greatly
improve parsing quality. Having a verb wrongly
analyzed as a noun and similar mistakes are very
likely to mislead the parser in its decision process.
A lot of the parser?s features include PoS tags and
reducing the amount of errors during PoS tagging
will therefore reduce misleading feature values as
well. Since the quality of the automatically as-
signed PoS tags in the German CoNLL ?09 data
is not state-of-the-art (see Table 2 below), we de-
cided to retag the data with our own tagger which
uses additional information from a symbolic mor-
phological analyzer to direct a statistical classifier.
For the assignment of PoS tags, we apply
a standard maximum entropy classification ap-
proach (see Ratnaparkhi (1996)). The classes of
the classifier are the PoS categories defined in the
Stuttgart-Tu?bingen Tag Set (STTS) (Schiller et al,
1999). We use standard binarized features like
the word itself, its last three letters, whether the
word is capitalized, contains a hyphen, a digit or
whether it consists of digits only. As the only non-
binary feature, word length is recorded. These
standard features are augmented by a number of
binary features that support the classification pro-
cess by providing a preselection of possible PoS
tags. Every word is analyzed by DMOR, a finite
state morphological analyzer, from whose output
analyses all different PoS tags are collected and
added to the feature set. For example, DMOR
assigns the PoS tags NN (common noun) and
ADJD (predicative adjective) to the word gegan-
1125
gen (gone). From these analyses two features are
generated, namely possible-tag:NN and possible-
tag:ADJD, which are strong indicators for the
classifier that one of these classes is very likely
to be the correct one. The main idea here is to
use the morphological analyzer as a sort of lexicon
that preselects the set of possible tags beforehand
and then use the classifier to do the disambigua-
tion (see Jurish (2003) for a more sophisticated
system based on Hidden-Markov models that uses
roughly the same idea). Since the PoS tags are in-
cluded in the feature set, the classifier is still able
to assign every class defined in STTS even if it is
not in the preselection. Where the morphological
analyzer does not know the word in question we
add features for every PoS tag representing a pro-
ductive word class in German, making the reason-
able assumption that the morphology knows about
all closed-class words and word forms. Finally,
we add word form and possible tag features for
the previous and the following word to the feature
set thus simulating a trigram tagger. We used the
method of Kazama and Tsujii (2005) which uses
inequality constraints to do a very efficient feature
selection5 to train the maximum entropy model.
We annotated the entire corpus with versions
of our own tagger, i.e., the training, development
and test data. In order to achieve a realistic be-
havior (including remaining tagging errors, which
the parser may be able to react to if they are sys-
tematic), it was important that each section was
tagged without any knowledge of the gold stan-
dard tags. For the development and test portion,
this is straightforward: we trained a model on the
gold PoS of the training portion of the data and
applied it to retag these two portions. Retagging
the training portion was a bit trickier since we
could not use a model trained on the same data,
but at the same time, we wanted to use a tagger
of similarly high quality ? i.e. one that has seen a
similar amount of training data. The training set
was therefore split into 20 different parts and for
every split, a tagging model was trained on the
other 19 parts which then was used to retag the
remaining 20th part. Table 2 shows the quality
of our tagger evaluated on the German CoNLL
5We used a width factor of 1.0.
?09 data in terms of accuracy and compares it
to the originally annotated PoS tags which have
been assigned by using the TreeTagger (Schmid,
1995) together with the German tagging model
provided from the TreeTagger website. Tagging
accuracy improves consistently by about 2 per-
centage points which equates to an error reduction
of 44.55 % to 49.0 %.
training development test
original 95.69 95.51 95.46
retagged 97.61 97.71 97.52
error red. 44.55% 49.00% 45.37%
Table 2: Tagging accuracy
Table 3 shows the parser performance when
trained on the newly tagged data. The consider-
able improvements in tagging accuracy visibly af-
fect parsing accuracy, raising both the labeled and
the unlabeled attachment score by 0.66 percentage
points (LAS) and 0.51 points (UAS) for the de-
velopment set and by 0.45 points (LAS) and 0.64
points (UAS) for the test set.
dev. set test set
LAS UAS LAS UAS
restructured 87.49 89.97 88.30 90.44
+retagged 88.15 90.48 88.75 91.08
Table 3: Parser performance on retagged data
6 Morphological Information
German, as opposed to English, exhibits a rela-
tively rich morphology. Predicate arguments and
nominal adjuncts are marked with special case
morphology which allows for a less restricted
word order in German. The German case system
comprises four different case values, namely nom-
inative, accusative, dative and genitive case. Sub-
jects and nominal predicates are usually marked
with nominative case, objects receive accusative
or dative case and genitive case is usually used
to mark possessors in possessive constructions.
There are also some temporal and spatial nominal
adjuncts which require certain case values. Since
case is used to mark the function of a noun phrase
in a clause, providing case information to a parser
might improve its performance.
The morphological information in the German
CoNLL ?09 data contains much more information
than case alone and previous models (baseline,
1126
hash kernel, retagged) have used all of it. How-
ever, since we aim to improve a syntactic parser,
we would like to exclude all morphological infor-
mation from the parsing process that is not obvi-
ously relevant to syntax, e. g. mood or tense. By
reducing the morphological annotations to those
that are syntactically relevant, we hope to reduce
the noise that is introduced by irrelevant informa-
tion. (One might expect that machine learning and
feature selection should ?filter out? irrelevant fea-
tures, but given the relative sparsity of unambigu-
ous instances of the linguistically relevant effects,
drawing the line based on just a few thousand sen-
tences of positive evidence would be extremely
hard even for a linguist.)
We annotated every case-bearing word in the
corpus with its case information using DMOR.
With case-bearing words, we mean nouns, proper
nouns, attributive adjectives, determiners and all
kinds of pronouns. Other types of morphologi-
cal information was discarded. We did not use
the manually annotated and disambiguated mor-
phological information already present in the cor-
pus for two reasons: the first one is the same as
with the PoS tagging. Since it is unrealistic to
have gold-standard annotation in a real-world ap-
plication which deals with unseen data, we want
the parser to learn from and hopefully adapt to
imperfectly annotated data. The second reason
is the German-inherent form syncretism in nom-
inal paradigms. The German noun inflection sys-
tem is with over ten different (productive and
non-productive) inflectional patterns quite com-
plicated, and to make matters worse, there are
only five different morphological markers to dis-
tinguish 16 different positions in the pronoun, de-
terminer and adjective paradigms and eight differ-
ent positions in the noun paradigms. Some po-
sitions in the paradigm will therefore always be
marked in the same way and we would like the
parser to learn that some word forms will always
be ambiguous with respect to their case value.
We also conducted experiments where we an-
notated number and gender values in addition to
case. The idea behind this is that number and gen-
der might help to further disambiguate case val-
ues. The downside of this is the increase in fea-
ture values. Combining case and number features
means a multiplication of their values creating
eight new feature values instead of four. Adding
gender annotation raises this number to 24. Be-
side the disambiguation of case, there is also an-
other reason why we might want to add num-
ber and gender: Inside a German noun phrase,
all parts have to agree on their case and number
feature in order to produce a well-formed noun
phrase. Furthermore, the head noun governs the
gender feature of the other parts. Thus, all three
features can be relevant to the construction of a
syntactic structure.6 Table 4 shows the results of
our experiments with morphological features.
dev. set test set
LAS UAS LAS UAS
retagged 88.15 90.48 88.75 91.08
no morph. 87.78 90.18 88.60 90.92
+case 88.04 90.48 88.77 91.13
+c+n 88.21 90.62 88.88 91.13
+c+n+g 87.96 90.33 88.73 90.99
Table 4: Parser performance with morph. infor-
mation (c=case, n=number, g=gender)
The no morph row in Table 4 shows, that
using no morphological information at all de-
creases parser performance. When only case val-
ues are annotated, the parser performance does
not change much in comparison to the retagged
model, so there is no benefit here. Adding num-
ber features on the other hand improves parsing
results significantly. This seems to support our in-
tuition that number helps in disambiguating case
values. However, adding gender information does
not further increase this effect but hurts parser per-
formance even more than case annotation alone.
This leaves us with a puzzle here. Annotating case
and number helps the parser, but case alone or
having case, number and gender together affects
performance negatively. A possible explanation
might be that the effect of the gender information
is masked by the increased number of feature val-
ues (24) which confuses the parsing algorithm.
7 Parser Stacking
Nivre and McDonald (2008) show how two dif-
ferent approaches to data-driven dependency pars-
6Person would be another syntactically relevant informa-
tion. However, since we are dealing with a newspaper cor-
pus, first and second person features appear very rarely.
1127
ing, the graph-based and transition-based ap-
proaches, may be combined and subsequently
learn to complement each other to achieve im-
proved parsing results for different languages.
MaltParser (Nivre et al, 2006) is a language-
independent system for data-driven dependency
parsing which is freely available.7 It is based on a
deterministic parsing strategy in combination with
treebank-induced classifiers for predicting parsing
actions. MaltParser employs a rich feature repre-
sentation in order to guide parsing. For the train-
ing of the Malt parser model that we use in the
stacking experiments, we use learner and parser
settings identical to the ones optimized for Ger-
man in the CoNLL-X shared task (Nivre et al,
2006). Furthermore, we employ the technique
of pseudo-projective parsing described in Nilsson
and Nivre (2005) and a split prediction strategy for
predicting parse transitions and arc labels (Nivre
and Hall, 2008).8 In order to obtain automatic
parses for the whole data set, we perform a 10-
fold split. For the parser stacking, we follow the
approach of Nivre and McDonald (2008), using
MaltParser as a guide for the MST parser with the
hash kernel, i.e., providing the arcs and labels as-
signed by MaltParser as features. Table 5 shows
the scores we obtain by parser stacking. Although
our version of MaltParser does not quite have the
same performance as for instance the version of
Hall and Nivre (2008), its guidance leads to a
small improvement in the overall parsing results.
dev. set test set
LAS UAS LAS UAS
MaltParser 82.47 85.78 83.84 86.8
our parser 88.21 90.62 88.88 91.13
+stacking 88.42 90.77 89.28 91.40
Table 5: Stacked parser performance with guid-
ance by MaltParser
7http://maltparser.org
8The feature models make use of information about the
lexical form (FORM), the predicted PoS (PPOS) and the de-
pendency relation constructed thus far during parsing (DEP).
In addition, we make use of the predicted values for other
morphological features (PFEATS). We employ the arc-eager
algorithm (Nivre, 2003) in combination with SVM learners,
using LIBSVM with a polynomial kernel.
8 Relabeling
In the relabeling step, we pursue the idea that
some erroneous parser decisions concerning the
distribution of certain labels might be detected and
repaired in post-processing. In German and in
most other languages, there are syntactic restric-
tions on the number of subjects and objects that
a verb might select. The parser will learn this be-
havior during training. However, since it is using a
statistical model with a limited context, it can still
happen that two or more of the same grammati-
cal functions are annotated for the same verb. But
having two subjects annotated for a single verb
makes this particular clause uninterpretable for
subsequently applied tasks. Therefore, we would
like to detect those doubly annotated grammatical
functions and correct them in a controlled way.
The detection algorithm is simple: Running
over the words of the output parse, we check for
every word whether it has two or more daughters
annotated with the same grammatical function and
if we find one, we relabel all of its daughters.9 For
the relabeling, we applied a dependency-version
of the function labeler described in Seeker et al
(2010) which uses a maximum entropy classifier
that is restrained by a number of hard constraints
implemented as an Integer Linear Program. These
constraints model the aforementioned selectional
restrictions on the number of certain types of ver-
bal arguments. Since these are hard constraints,
the labeler is not able to annotate more than one
of those grammatical functions per verb. If we
count the number of sentences that contain doubly
annotated grammatical functions in the best pars-
ing results from the previous section, we get 189
for the development set and 153 for the test set.
About two thirds of the doubly annotated func-
tions are subjects and the biggest part of the re-
maining third are accusative objects which are the
most common arguments of German verbs.
Table 6 shows the final results after relabeling
the output of the best performing parser config-
uration from the previous section. The improve-
ments on the overall scores are quite small, which
9The grammatical functions we are looking for are SB
(subject), OA (accusative object), DA (dative), OG (genitive
object), OP (prepositional object), OC (clausal object), PD
(predicate) and OA2 (second accusative object).
1128
dev. set test set
LAS UAS LAS UAS
stacking 88.42 90.77 89.28 91.40
+relabeling 88.48 90.77 89.40 91.40
Table 6: Parse quality after relabeling
is partly due to the fact that the relabeling affects
only a small subset of all labels used in the data.
Furthermore, the relabeling only takes place if a
doubly annotated function is detected; and even
if the relabeling is applied we have no guarantee
that the labeler will assign the labels correctly (al-
though we are guaranteed to not get double func-
tions). Table 7 shows the differences in precision
and recall for the grammatical functions between
the original and the relabeled test set. As one can
see, scores stay mostly the same except for SB,
OA and DA. For OA, scores improve both in recall
and precision. For DA, we trade a small decrease
in precision for a huge improvement in recall and
vice versa for SB, but on a much smaller scale.
Generally spoken, relabeling is a local repair strat-
egy that does not have so much effect on the over-
all score but can help to get some important labels
correct even if the parser made the wrong deci-
sion. Note that the relabeler can only repair incor-
rect label decisions, it cannot help with wrongly
attached words.
original relabeled
rec prec rec prec
DA 64.2 83.2 74.7 79.6
OA 88.9 85.8 90.7 88.2
OA2 0.0 NaN 0.0 NaN
OC 95.2 93.5 95.1 93.7
OG 33.3 66.7 66.7 80.0
OP 54.2 80.8 54.2 79.9
PD 77.1 76.8 77.1 76.8
SB 91.0 90.6 90.7 93.7
Table 7: Improvements on grammatical functions
in the relabeled test set
9 Conclusion
We presented a sequence of modifications to a
data-driven dependency parser of German, depart-
ing from a state-of-the-art set-up in an imple-
mentation that allows for fast and robust train-
ing and application. Our pilot study tested what
can be achieved in a few weeks if the data-driven
technique is combined with a linguistically in-
formed approach, i.e., testing hypotheses of what
should be particularly effective in a very targeted
way. Most modifications were relatively small,
addressing very different dimensions in the sys-
tem, such as the handling of features in the Ma-
chine Learning, the quality and combination of
automatically assigned features and the ability to
take into account global constraints, as well as the
combination of different parsing strategies. Over-
all, labeled accuracy on a standard test set (from
the CoNLL 2009 shared task), ignoring gold stan-
dard part-of-speech tags, increased significantly
from 87.64% (baseline parser without hash ker-
nel) to 89.40%.10 We take this to indicate that a
targeted and informed approach like the one we
tested can have surprising effects even for a lan-
guage that has received relatively intense consid-
eration in the parsing literature.
Acknowledgements
We would like to thank Sandra Ku?bler, Yannick
Versley and Yi Zhang for their support. This work
was partially supported by research grants from
the Deutsche Forschungsgemeinschaft as part of
SFB 632 ?Information Structure? at the Univer-
sity of Potsdam and SFB 732 ?Incremental Speci-
fication in Context? at the University of Stuttgart.
References
Attardi, G. 2006. Experiments with a Multilanguage Non-
Projective Dependency Parser. In Proceedings of CoNLL,
pages 166?170.
Blum, A. 2006. Random Projection, Margins, Kernels, and
Feature-Selection. In LNCS, pages 52?68. Springer.
Bohnet, B. 2009. Efficient Parsing of Syntactic and Se-
mantic Dependency Structures. In Proceedings of CoNLL
2009).
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the Workshop on Treebanks and Lin-
guistic Theories, Sozopol.
Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In In Proc. of
CoNLL, pages 149?164.
Carreras, X. 2007. Experiments with a Higher-order Projec-
tive Dependency Parser. In EMNLP/CoNLL.
10?= 0.01, measured with a tool by Dan Bikel from
www.cis.upenn.edu/? dbikel/download/compare.pl
1129
Crammer, K., O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2006. Online Passive-Aggressive Algorithms. Journal of
Machine Learning Research, 7:551?585.
Duchier, Denys and Ralph Debusmann. 2001. Topologi-
cal dependency trees: a constraint-based account of linear
precedence. In Proceedings of ACL 2001, pages 180?
187, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Eisner, J. 1996. Three New Probabilistic Models for Depen-
dency Parsing: An Exploration. In Proceedings of Coling
1996, pages 340?345, Copenhaen.
Eisner, J., 2000. Bilexical Grammars and their Cubic-time
Parsing Algorithms, pages 29?62. Kluwer Academic
Publishers.
Hajic?, J., M. Ciaramita, R. Johansson, D. Kawahara,
M. Anto`nia Mart??, L. Ma`rquez, A. Meyers, J. Nivre,
S. Pado?, J. S?te?pa?nek, P. Stran?a?k, M. Surdeanu, N. Xue,
and Y. Zhang. 2009. The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple Lan-
guages. In Proceedings of the 13th CoNLL-2009, June
4-5, Boulder, Colorado, USA.
Hall, Johan and Joakim Nivre. 2008. A dependency-driven
parser for German dependency and constituency represen-
tations. In Proceedings of the Workshop on Parsing Ger-
man, pages 47?54, Columbus, Ohio, June. Association for
Computational Linguistics.
Hinrichs, Erhard, Sandra Ku?bler, Karin Naumann, Heike
Telljohann, and Julia Trushkina. 2004. Recent develop-
ments in linguistic annotations of the tu?ba-d/z treebank.
In Proceedings of the Third Workshop on Treebanks and
Linguistic Theories, pages 51?62, Tu?bingen, Germany.
Johansson, R. and P. Nugues. 2008. Dependency-based
Syntactic?Semantic Analysis with PropBank and Nom-
Bank. In Proceedings of the Shared Task Session of
CoNLL-2008, Manchester, UK.
Jurish, Bryan. 2003. A hybrid approach to part-of-speech
tagging. Technical report, Berlin-Brandenburgische
Akademie der Wissenschaften.
Kazama, Jun?Ichi and Jun?Ichi Tsujii. 2005. Maximum en-
tropy models with inequality constraints: A case study on
text categorization. Machine Learning, 60(1):159?194.
Ku?bler, Sandra. 2008. The PaGe 2008 shared task on pars-
ing german. In Proceedings of the Workshop on Parsing
German, pages 55?63, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Magerman, David M. 1995. Statistical decision-tree mod-
els for parsing. In Proceedings of ACL 1995, pages 276?
283, Morristown, NJ, USA. Association for Computa-
tional Linguistics Morristown, NJ, USA.
McDonald, R. and F. Pereira. 2006. Online Learning of Ap-
proximate Dependency Parsing Algorithms. In In Proc.
of EACL, pages 81?88.
Menzel, Wolfgang and Ingo Schro?der. 1998. Decision pro-
cedures for dependency parsing using graded constraints.
In Proceedings of the COLING-ACL ?98 Workshop on
Processing of Dependency-Based Grammars, pages 78?
87.
Nilsson, Jens and Joakim Nivre. 2005. Pseudo-projective
dependency parsing. In Proceedings of ACL 2005, pages
99?106.
Nivre, Joakim and Johan Hall. 2008. A dependency-driven
parser for German dependency and constituency represen-
tations. In Proceedings of the ACL Workshop on Parsing
German.
Nivre, J. and R. McDonald. 2008. Integrating Graph-Based
and Transition-Based Dependency Parsers. In ACL-08,
pages 950?958, Columbus, Ohio.
Nivre, Joakim, Jens Nilsson, Johan Hall, Gu?ls?en Eryig?it, and
Svetoslav Marinov. 2006. Labeled pseudo-projective de-
pendency parsing with Support Vector Machines. In Pro-
ceedings of CoNLL 2006.
Nivre, J. 2003. An Efficient Algorithm for Projective De-
pendency Parsing. In 8th International Workshop on
Parsing Technologies, pages 149?160, Nancy, France.
Nivre, J. 2009. Non-Projective Dependency Parsing in Ex-
pected Linear Time. In Proceedings of the 47th Annual
Meeting of the ACL and the 4th IJCNLP of the AFNLP,
pages 351?359, Suntec, Singapore.
Rahimi, A. and B. Recht. 2008. Random Features for
Large-Scale Kernel Machines. In Platt, J.C., D. Koller,
Y. Singer, and S. Roweis, editors, Advances in Neural
Information Processing Systems, volume 20. MIT Press,
Cambridge, MA.
Ratnaparkhi, Adwait. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP 1996,
volume 1, pages 133?142.
Schiller, Anne, Simone Teufel, and Christine Sto?ckert. 1999.
Guidelines fu?r das Tagging deutscher Textcorpora mit
STTS (Kleines und gro?es Tagset). Technical Report Au-
gust, Universita?t Stuttgart.
Schmid, Helmut. 1995. Improvements in part-of-speech tag-
ging with an application to German. In Proceedings of the
ACL SIGDAT-Workshop, volume 11.
Seeker, Wolfgang, Ines Rehbein, Jonas Kuhn, and Josef Van
Genabith. 2010. Hard Constraints for Grammatical Func-
tion Labelling. In Proceedings of ACL 2010, Uppsala.
Titov, I. and J. Henderson. 2007. A Latent Variable Model
for Generative Dependency Parsing. In Proceedings of
IWPT, pages 144?155.
Yamada, H. and Y. Matsumoto. 2003. Statistical Depen-
dency Analysis with Support Vector Machines. In Pro-
ceedings of IWPT, pages 195?206.
1130
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 928?939, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Generating Non-Projective Word Order in Statistical Linearization
Bernd Bohnet Anders Bjo?rkelund Jonas Kuhn Wolfgang Seeker Sina Zarrie?
Institut fu?r Maschinelle Sprachverarbeitung
University of Stuttgart
{bohnetbd,anders,jonas,seeker,zarriesa}@ims.uni-stuttgart.de
Abstract
We propose a technique to generate non-
projective word orders in an efficient statisti-
cal linearization system. Our approach pre-
dicts liftings of edges in an unordered syntac-
tic tree by means of a classifier, and uses a
projective algorithm for tree linearization. We
obtain statistically significant improvements
on six typologically different languages: En-
glish, German, Dutch, Danish, Hungarian, and
Czech.
1 Introduction
There is a growing interest in language-independent
data-driven approaches to natural language genera-
tion (NLG). An important subtask of NLG is sur-
face realization, which was recently addressed in the
2011 Shared Task on Surface Realisation (Belz et
al., 2011). Here, the input is a linguistic representa-
tion, such as a syntactic dependency tree lacking all
precedence information, and the task is to determine
a natural, coherent linearization of the words.
The standard data-driven approach is to traverse
the dependency tree deciding locally at each node on
the relative order of the head and its children. The
shared task results have proven this approach to be
both effective and efficient when applied to English.
It is what federal support should try to achieve
SBJ
ROOT OBJ
NMOD SBJ
PRD
VC OPRD IM
Figure 1: A non-projective example from the CoNLL
2009 Shared Task data set for parsing (Hajic? et al 2009).
However, the approach can only generate pro-
jective word orders (which can be drawn with-
out any crossing edges). Figure 1 shows a non-
projective word order: the edge connecting the ex-
tracted wh-pronoun with its head crosses another
edge. Once what has been ordered relative to
achieve, there are no ways of inserting intervening
material. In this case, only ungrammatical lineariza-
tions can be produced from the unordered input tree:
(1) a. *It is federal support should try to what achieve
b. *It is federal support should try to achieve what
c. *It is try to achieve what federal support should
Although rather infrequent in English, non-
projective word orders are quite common in lan-
guages with a less restrictive word order. In these
languages, it is often possible to find a grammati-
cally correct projective linearization for a given in-
put tree, but discourse coherence, information struc-
ture, and stylistic factors will often make speak-
ers prefer some non-projective word order.1 Figure
2 shows an object fronting example from German
where the edge between the subject and the finite
verb crosses the edge between the object and the full
verb. Various other constructions, such as extraposi-
tion of (relative) clauses or scrambling, can lead to
non-projectivity. In languages where word order is
driven to an even larger degree by information struc-
ture, such as Czech and Hungarian, non-projectivity
can likewise result from various ordering decisions.
These phenomena have been studied extensively in
1A categorization of non-projective edges in the Prague
Dependency Treebank (Bo?hmova? et al 2000) is presented in
Hajic?ova? et al(2004).
928
the linguistic literature, and for certain languages,
work on rule-based generation has addressed certain
aspects of the problem.
Das Mandat will er zuru?ckgeben .
the.ACC mandate.ACC want.3SG he.NOM return.INF .
NK
OA#?
SB OC
?
?He wants to return the mandate.?
Figure 2: German object fronting with complex verb in-
troducing a non-projective edge.
In this paper, we aim for a general data-driven ap-
proach that can deal with various causes for non-
projectivity and will work for typologically dif-
ferent languages. Our technique is inspired by
work in data-driven multilingual parsing, where
non-projectivity has received considerable attention.
In pseudo-projective parsing (Kahane et al 1998;
Nivre and Nilsson, 2005), the parsing algorithm is
restricted to projective structures, but the issue is
side-stepped by converting non-projective structures
to projective ones prior to training and application,
and then restoring the original structure afterwards.
Similarly, we split the linearization task in two
stages: initially, the input tree is modified by lifting
certain edges in such a way that new orderings be-
come possible even under a projectivity constraint;
the second stage is the original, projective lineariza-
tion step. In parsing, projectivization is a determin-
istic process that lifts edges based on the linear or-
der of a sentence. Since the linear order is exactly
what we aim to produce, this deterministic conver-
sion cannot be applied before linearization. There-
fore, we use a statistical classifier as our initial lift-
ing component. This classifier has to be trained on
suitable data, and it is an empirical question whether
the projective linearizer can take advantage of this
preceding lifting step.
We present experiments on six languages with
varying degrees of non-projective structures: En-
glish, German, Dutch, Danish, Czech and Hungar-
ian, which exhibit substantially different word order
properties. Our approach achieves significant im-
provements on all six languages. On German, we
also report results of a pilot human evaluation.
2 Related Work
An important concept for tree linearization are word
order domains (Reape, 1989). The domains are bags
of words (constituents) that are not allowed to be dis-
continuous. A straightforward method to obtain the
word order domains from dependency trees and to
order the words in the tree is to use each word and
its children as domain and then to order the domains
and contained words recursively. As outlined in the
introduction, the direct mapping of syntactic trees to
domains does not provide the possibility to obtain
all possible correct word orders.
Linearization systems can be roughly distin-
guished as either rule-based or statistical systems. In
the 2011 Shared Task on Surface Realisation (Belz
et al 2011), the top performing systems were all
statistical dependency realizers (Bohnet et al 2011;
Guo et al 2011; Stent, 2011).
Grammar-based approaches map dependency
structures or phrase structures to a tree that repre-
sents the linear precedence. These approaches are
mostly able to generate non-projective word orders.
Early work was nearly exclusively applied to phrase
structure grammars (e.g. (Kathol and Pollard, 1995;
Rambow and Joshi, 1994; Langkilde and Knight,
1998)). Concerning dependency-based frameworks,
Bro?ker (1998) used the concept of word order do-
mains to separate surface realization from linear
precedence trees. Similarly, Duchier and Debus-
mann (2001) differentiate Immediate Dominance
trees (ID-trees) from Linear Precedence trees (LP-
trees). Gerdes and Kahane (2001) apply a hierarchi-
cal topological model for generating German word
order. Bohnet (2004) employs graph grammars to
map between dependency trees and linear prece-
dence trees represented as hierarchical graphs. In the
frameworks of HPSG, LFG, and CCG, a grammar-
based generator produces word order candidates that
might be non-projective, and a ranker is used to se-
lect the best surface realization (Cahill et al 2007;
White and Rajkumar, 2009).
Statistical methods for linearization have recently
become more popular (Langkilde and Knight, 1998;
Ringger et al 2004; Filippova and Strube, 2009;
Wan et al 2009; He et al 2009; Bohnet et al 2010;
Guo et al 2011). They typically work by travers-
ing the syntactic structure either bottom-up (Filip-
929
pova and Strube, 2007; Bohnet et al 2010) or top-
down (Guo et al 2011; Bohnet et al 2011). These
linearizers are mostly applied to English and do not
deal with non-projective word orders. An excep-
tion is Filippova and Strube (2007), who contribute
a study on the treatment of preverbal and postver-
bal constituents for German focusing on constituent
order at the sentence level. The work most similar
to ours is that of Gamon et al(2002). They use
machine-learning techniques to lift edges in a pre-
processing step to a surface realizer. Their objec-
tive is the same as ours: by lifting, they avoid cross-
ing edges. However, contrary to our work, they use
phrase-structure syntax and focus on a limited num-
ber of cases of crossing branches in German only.
3 Lifting Dependency Edges
In this section, we describe the first of the two stages
in our approach, namely the classifier that lifts edges
in dependency trees. The classifier we aim to train
is meant to predict liftings on a given unordered de-
pendency tree, yielding a tree that, with a perfect lin-
earization, would not have any non-projective edges.
3.1 Preliminaries
The dependency trees we consider are of the form
displayed in Figure 1. More precisely, all words (or
nodes) form a rooted tree, where every node has ex-
actly one parent (or head). Edges point from head
to dependent, denoted in the text by h? d, where h
is the head and d the dependent. All nodes directly
or transitively depend on an artificial root node (de-
picted in Figure 1 as the incoming edge to is).
We say that a node a dominates a node d if a is
an ancestor of d. An edge h ? d is projective iff
h dominates all nodes in the linear span between h
and d. Otherwise it is non-projective. Moreover,
a dependency tree is projective iff all its edges are
projective. Otherwise it is non-projective.
A lifting of an edge h? d (or simply of the node
d) is an operation that replaces h ? d with g ? d,
given that there exists an edge g ? h in the tree, and
undefined otherwise (i.e. the dependent d is reat-
tached to the head of its head).2 When the lifting
2The undefined case occurs only when d depends on the
root, and hence cannot be lifted further; but these edges are by
definition projective, since the root dominates the entire tree.
operation is applied n successive times to the same
node, we say the node was lifted n steps.
3.2 Training
During training we make use of the projectivization
algorithm described by Nivre and Nilsson (2005).
It works by iteratively lifting the shortest non-
projective edges until the tree is projective. Here,
shortest edge refers to the edge spanning over the
fewest number of words. Since finding the shortest
edge relies on the linear order, instead of lifting the
shortest edge, we lift non-projective edges ordered
by depth in the tree, starting with the deepest nested
edge. A lifted version of the tree from Figure 1 is
shown in Figure 3. The edge of what has been lifted
three steps (the original edge is dotted), and the tree
is no longer non-projective.
It is what federal support should try to achieve
SBJ
ROOT OBJ
OBJNMOD SBJ
PRD
VC OPRD IM
Figure 3: The sentence from Figure 1, where what has
been assigned a new head (solid line). The original edge
is dotted.
We model the edge lifting problem as a multi-
class classification problem and consider nodes one
at a time and ask the question ?How far should this
edge be lifted??, where classes correspond to lifting
0, 1, 2, ..., n steps. To create training instances we
use the projectivization algorithm mentioned above.
We traverse the nodes of the tree sorted by depth.
For multiple nodes at the same depth, ties are broken
by linear order, i.e. for multiple nodes at the same
depth, the leftmost is visited first. When a node is
visited, we create a training instance out of it. Its
class is determined by the number of steps it would
be lifted by the projectivization algorithm given the
linear order (in most cases the class corresponds to
no lifting, since most edges are projective). As we
traverse the nodes, we also execute the liftings (if
any) and update the tree on the fly.
The training instances derived are used to train a
logistic regression classifier using the LIBLINEAR
package (Fan et al 2008). The features used for
the lifting classifier are described in Table 1. Since
we use linear classifiers, our feature set al con-
tains conjunctions of atomic features. The features
930
Atomic features
?x ? {w,wp, wgp, wch, ws, wun} morph(x), label(x), lemma(x), PoS(x)
?x ? {wgc, wne, wco} label(x), lemma(x), PoS(x)
Complex features
?x ? {w,wp, wgp} lemma(x)+PoS(x), label(x)+PoS(x), label(x)+lemma(x)
?x ? {wch, ws, wun}, y = w lemma(x)+lemma(y), PoS(y)+lemma(x), PoS(y)+lemma(x)
?x ? {w,wp, wgp}, y = HEAD(x) lemma(x)+lemma(y), lemma(x)+PoS(y), PoS(x)+lemma(y)
?x ? {w,wp, wgp}, y = HEAD(x), z = HEAD(y) PoS(x)+PoS(y)+PoS(z), label(x)+label(y)+label(z)
?x ? {wch, ws, wun}, y = HEAD(x), z = HEAD(y) PoS(x)+PoS(y)+PoS(z), label(x)+label(y)+label(z)
Non-binary features
?x ? {w,wp, wgp} SUBTREESIZE(x), RELSUBTREESIZE(x)
Table 1: Features used for lifting. w refers to the word (dependent) in question. And with respect to w, wp is the
parent; wgp is the grandparent; wch are children; ws are siblings; wun are uncles (i.e. children of the grandparent,
excluding the parent); wgc are grandchildren; wne are nephews (i.e. grandchildren of the parent that are not children
of w); wco are cousins (i.e. grandchildren of the grandparent that are not w or siblings of w). The non-binary feature
functions refer to: SUBTREESIZE ? the absolute number of nodes below x, RELSUBTREESIZE ? the relative size of
the subtree rooted at x with respect to the whole tree.
involve the lemma, dependency edge label, part-of-
speech tag, and morphological features of the node
in question, and of several neighboring nodes in the
dependency tree. We also have a few non-binary fea-
tures that encode the size of the subtree headed by
the node and its ancestors.
We ran preliminary experiments to determine the
optimal architecture. First, other ways of modeling
the liftings are conceivable. To find new reattach-
ment points, Gamon et al(2002) propose two other
ways, both using a binary classifier: applying the
classifier to each node x along the path to the root
asking ?Should d be reattached to x??; or lifting one
step at a time and applying the classifier iteratively
until it says stop. They found that the latter outper-
formed the former. We tried this method, but found
that it was inferior to the multi-class model and more
frequently over- or underlifted.
Second, to avoid data sparseness for infrequent
lifting distances, we introduce a maximum number
of liftings. We found that a maximum of 3 gave the
best performance. In the pseudocode below, we re-
fer to this number as maxsteps.3 This means that we
are able to predict the correct lifting for most (but
not all) of the non-projective edges in our data sets
(cf. Table 3).
Third, as Nivre and Nilsson (2005) do for pars-
3During training, nodes that are lifted further than maxsteps
are assigned to the class corresponding to maxsteps. This ap-
proach worked better than ignoring the training instance or
treating it as a non-lifting (i.e. a lifting of 0 steps).
ing, we experimented with marking edges that were
lifted by indicating this on the edge labels. In the
case of parsing, this step is necessary in order to re-
verse the liftings in the parser output. In our case,
it could potentially be beneficial for both the lifting
classifier, and for the linearizer. However, we found
that marking liftings at best gave similar results as
not marking, so we kept the original labels without
marking.
3.3 Decoding
In the decoding stage, an unordered tree is given and
the goal is to lift edges that would be non-projective
with respect to the gold linear order. Similarly to
how training instances are derived, the decoding al-
gorithm traverses the tree bottom-up and visits every
node once. Ties between nodes at the same depth are
broken in an arbitrary but deterministic way. When
a node is visited, the classifier is applied and the cor-
responding lifting is executed. Pseudocode is given
in Algorithm 1.4
Different orderings of nodes at the same depth
can lead to different lifts. The reason is that lift-
ings are applied immediately and this influences the
features when subsequent nodes are considered. For
instance, consider two sibling nodes ni and nj . If
ni is visited before nj , and ni is lifted, this means
4The MIN function is used to guarantee that the edge is not
lifted beyond the root node of the tree. This does not happen
in practice though, since the feature set of the classifier include
features that implicitly encode the proximity to the root node.
931
that at the time we visit nj , ni is no longer a sibling
of nj , but rather an uncle. An obvious extension of
the decoding algorithm presented above is to apply
beam search. This allows us to consider nj both in
the context where ni has been lifted and when it has
not been lifted.
1 N? NODES(T )
2 SORT-BY-DEPTH-BREAK-TIES-ARBITRARILY(N,T )
3 foreach node ? N do
4 feats? EXTRACT-FEATURES(node, T )
5 steps? CLASSIFY(feats)
6 steps? MIN(steps,ROOT-DIST(node))
7 LIFT(node, T, steps)
8 return T
Algorithm 1: Greedy decoding for lifting.
Pseudocode for the beam search decoder is given
in Algorithm 2. The algorithm keeps an agenda of
trees to explore as each node is visited. For every
node, it clones the current tree and applies every pos-
sible lifting. Every tree also has an associated score,
which is the sum of the scores of each lifting so far.
The score of a lifting is defined to be the log proba-
bility returned from the logistic classifier. After ex-
ploring all trees in the agenda, the k-best new trees
from the beam are extracted and put back into the
agenda. When all nodes have been visited, the best
tree in the agenda is returned. For the experiments
the beam size (k in Algorithm 2) was set to 20.
1 N? NODES(T )
2 SORT-BY-DEPTH-BREAK-TIES-ARBITRARILY(N,T )
3 Tscore ? 0
4 Agenda? {T}
5 foreach node ? N do
6 Beam? ?
7 foreach tree ? Agenda do
8 feats? EXTRACT-FEATURES(node, tree)
9 m? MIN(maxsteps,ROOT-DIST(node))
10 foreach s ? 0 .. maxsteps do
11 t? CLONE(tree)
12 score? GET-LIFT-SCORE(feats, s)
13 tscore = tscore + score
14 LIFT(node, t, s)
15 Beam? Beam ? {t}
16 Agenda? EXTRACTKBEST(Beam, k)
17 return EXTRACTKBEST(Agenda, 1)
Algorithm 2: Beam decoding for lifting.
While beam search allows us to explore the search
space somewhat more thoroughly, a large number of
possibilities remain unaccounted for. Again, con-
sider the sibling nodes ni and nj when ni is visited
before nj . The beam allows us to consider nj both
when ni is lifted and when it is not. However, the
situation where nj is visited before ni is still never
considered. Ideally, all permutations of nodes at the
same depth should be explored before moving on.
Unfortunately this leads to a combinatorial explo-
sion of permutations, and exhaustive search is not
tractable. As an approximation, we create two or-
derings and run the beam search twice. The dif-
ference between the orderings is that in the second
one all ties are reversed. As this bibeam consistently
improved over the beam in Algorithm 2, we only
present these results in Section 5 (there denoted sim-
ply Beam).
4 Linearization
A linearizer searches for the optimal word order
given an unordered dependency tree, where the op-
timal word order is defined as the single reference
order of the dependency tree in the gold standard.
We employ a statistical linearizer that is trained on a
corpus of pairs consisting of unordered dependency
trees and their corresponding sentences. The lin-
earization method consists of the following steps:
Creating word order domains. In the first step,
we build the word order domains dh for all nodes
h ? y of a dependency tree y. A domain is defined
as a node and all of its direct dependents. For ex-
ample, the tree shown in Figure 3 has the following
domains: {it, be, should}, {what, support, should, try},
{federal, support}, {try, to}, {to, achieve}
If an edge was lifted before the linearization, the
lifted node will end up in the word order domain of
its new head rather than in the domain of its original
head. This way, the linearizer can deduce word or-
ders that would result in non-projective structures in
the non-lifted tree.
Ordering the words of the domains. In the sec-
ond step, the linearizer orders the words of each do-
main. The position of a subtree is determined by the
position of the head of the subtree in the enclosing
domain. Algorithm 3 shows the tree linearization
algorithm. In our implementation, the linearizer tra-
verses the tree either top-down or bottom-up.
932
1 // T is the dependency tree with lifted nodes
2 beam-size? 1000
3 for h ? T do
4 domainh? GET-DOMAIN(T ,h)
5 // initialize the beam with a empty word list
6 Agendah? ()
7 foreach w ? domainh do
8 // beam for extending word order lists
9 Beam? ()
10 foreach l ? Agendah do
11 // clone list l and append the word w
12 if w 6? l then
13 l? ? APPEND(l,m)
14 Beam? Beam ? l?
15 score[l?]? COMPUTE-SCORE(l?)
16 if | Beam | > beam-size then
17 SORT-LISTS-DESCENDING-TO-
SCORE(Beam,score)
18 Agendah? SUBLIST(0,beam-size,Beam)
19 else
20 Agendah? Beam
21 foreach l ? Beam do
22 SCOREg[l]? SCORE[l] +
GLOBAL-SCORE(l)
23 Agendah? Beam
24 return Beam
Algorithm 3: Dependency Tree Linearization.
The linearization algorithm initializes the word
order beam (agendah) with an empty order () (line
6). It then iterates over the words of a domain (lines
7-20). In the first iteration, the algorithm clones and
extends the empty word order list () by each word
of the sentence (line 12-15). If the beam (beam)
exceeds a certain size (beam-size), it is sorted by
score and pruned to maximum beam size (beam-
size) (lines 16-20). The following example illus-
trates the extensions of the beam for the top domain
shown in Figure 3.
Iter. agendabe
0: ()
1: ((it) (be) (should))
2: ((it be) (it should) (be it) (be should) ...)
The beam enables us to apply features that encode
information about the first tokens and the last token,
which are important for generating, e.g. the word
order of questions, i. e. if the last token is a question
mark then the sentence should probably be a ques-
tion (cf. feature set shown in Table 2). Furthermore,
the beam enables us to generate alternative lineariza-
tions. For this, the algorithm iterates over the alter-
native word orders of the domains in order to as-
semble different word orders on the sentence level.5
Finally, when traversing the tree bottom-up, the al-
gorithm has to use the different orders of the already
ordered subtrees as context, which also requires a
search over alternative word orders of the domains.
Training of the Linearizer. We use MIRA
(Crammer et al 2006) for the training of the lin-
earizer. The classifier provides a score that we use to
rank the alternative word orders. Algorithm 3 calls
two functions to compute the score: compute-score
(line 15) for features based on pairs of words and tri-
grams and compute-global-score for features based
on word patterns of a domain. Table 2 shows the
feature set for the two functions. In the case that the
linearization of a word order domain is incorrect the
algorithm updates its weight vector w. The follow-
ing equation shows the update function of the weight
vector:
w = w + ?h(?(dh, T, xg)? ?(dh, T, xp))
We update the weight vector w by adding the dif-
ference of the feature vector representation of the
correct linearization xg and the wrongly predicted
linearization xp, multiplied by ? . ? is the passive-
aggressive update factor as defined below. The suf-
fered lossh is ?(dh, T, xp)? ?(dh, T, xg).
? = lossh||?(dh,T,xg)??(dh,T,xp)||2
Creating the word order of a sentence. The lin-
earizer traverses the tree either top-down or bottom-
up and assembles the results in the surface order.
The bottom-up linearization algorithm can take into
account features drawn from the already ordered
subtrees while the top-down algorithm can employ
as context only the unordered nodes. However, the
bottom-up algorithm additionally has to carry out a
search over the alternative linearization of the sub-
domains, as different orders of the subdomain pro-
vide different context features. This leads to a higher
linearization time. We implemented both, but could
only find a rather small accuracy difference. In the
following, we therefore present results only for the
top-down method.
5The beam also makes it possible to employ a generative
language model to rerank alternative linearizations.
933
Atomic features
For nodes w ?
domainh
lemma(w), label(w), PoS(w), num-children(w), num-grandchildren(w), label-children(w),
PoS-children(w)
For domain
domainh
head(w1,w2), head(w1,w2,w3), label(head), PoS(head), PoS(w1), label(wn), label(wn?1),
contains-?(domainh)
Complex features
For bigrams
(w1, w2) ?
domainh
feat2: label(w1)+label(w2), label(w1)+lemma(w2), lemma(w1)+lemma(w2), PoSw1+PoSw2
feat3: label(w1)+num-children(w2)+num-children(w1),PoS-child(w1)+label(w1)+label(w2)
feat4: label(w1)+label(w2)+lemma(w2)+PoS(w1), label(w1)+label(w2)+PoS(head)+head(w1,w2)
feat5: label(w1)+label(w2)+PoS(head)+label(head)+head(w1,w2)
For trigrams
(w1, w2, w3) ?
domainh
feat3: lemma(w1)+lemma(w2)+lemma(w3)
feat4: PoS(w1)+PoS(w2)+PoS(w3)+head(w1,w2,w3)
feat5: label(w1)+label(w2)+label(w3)+PoS(w1)+head(w1,w2,w3)
For sentence s feat6: label(w1)+label(wn?1)+lemma(head)+lemma(w1)+lemma(wn?1)
feat7: PoS(w1)+PoS(w2)+PoS(w3)+PoS(wn?1)+PoS(wn?2)+PoS(wn?3)+contains-?(s)
Table 2: Exemplified features used for scoring linearizations of a word order domain (see Algorithm 3). Atomic
features which represent properties of a node or a domain are conjoined into feature vectors of different lengths.
Linearizations are scored based on bigrams, trigrams, and global sentence-level features.
5 Experiments
We conduct experiments on six European languages
with varying degrees of word order restrictions:
While English word order is very restrictive, Czech
and Hungarian exhibit few word order constraints.
Danish, Dutch, and German (so-called V2, i. e.
verb-second, languages) show a relatively free word
order that is however more restrictive than in Hun-
garian or Czech. The English and the Czech data
are from the CoNLL 2009 Shared Task data sets
(Hajic? et al 2009). The Danish and the Dutch data
are from the CoNLL 2006 Shared Task data sets
(Buchholz and Marsi, 2006). For Hungarian, we use
the Hungarian Dependency Treebank (Vincze et al
2010), and for German, we use a dependency con-
version by Seeker and Kuhn (2012).
# sent?s np sent?s np edges np ? 3 lifts
English 39,279 7.63 % 0.39% 98.39%
German 36,000 28.71% 2.34% 94.98%
Dutch 13,349 36.44% 5.42% 99.80%
Danish 5,190 15.62 % 1.00% 96.72%
Hungarian 61,034 15.81% 1.45% 99.82%
Czech 38,727 22.42% 1.86% 99.84%
Table 3: Size of training sets, percentage of non-
projective (np) sentences and edges, percentage of np
edges covered by 3 lifting steps.
Table 3 shows the sizes of the training corpora
and the percentage of non-projective sentences and
edges in the data. Note that the data sets for Dan-
ish and Dutch are quite small. English has the least
percentage of non-projective edges. Czech, Ger-
man, and Dutch show the highest percentage of non-
projective edges. The last column shows the per-
centage of non-projective edges that can be made
projective by at most 3 lifting steps.
5.1 Setup
In our two-stage approach, we first train the lifting
classifier. The results for this classifier are reported
in Section 5.2.
Second, we train the linearizer on the output of
the lifting classifier. To assess the impact of the
lifting technique on linearization, we built four sys-
tems on each language: (a) a linearizer trained on
the original, non-lifted dependency structures (No-
lift), two trained on the automatically lifted edges
(comparing (b) the beam and (c) greedy decoding),
(d) one trained on the oracle, i. e. gold-lifted struc-
tures, which gives us an upper bound for the lifting
technique. The linearization results are reported in
Section 5.3.
In this two-stage setup, we have the problem that,
if we re-apply the lifting classifier on the data it was
trained on, the input for the linearizer will be better
during training than during testing. To provide real-
istic training data for the linearizer, we make a 10-
fold cross-validation of the lifting classifier on the
training set, and use this as training data for the lin-
earizer. The lifting classifier that is applied to the
test set is trained on the entire training set.
934
5.2 Lifting results
To evaluate the performance of the lifting classifier,
we present precision, recall, and F-measure results
for each language. We also compute the percentage
of sentences that were handled perfectly by the lift-
ing classifier. Precision and recall are defined the
usual way in terms of true positives, false positives,
and false negatives, where true positives are edges
that should be lifted and were lifted correctly; false
positives are edges that should not be lifted but were
and edges that should be lifted and were lifted, but
were reattached in the wrong place; false negatives
are edges that should be lifted but were not.
The performance of both the greedy decoder and
the bibeam decoder are shown in Table 4. The scores
are taken on the cross-validation on the training set,
as this provides more reliable figures. The scores
are micro-averaged, i.e. all folds are concatenated
and compared to the entire training set.
Although the major evaluation of the lifting is
given by the performance of the linearizer, Table 4
gives us some clues about the lifting. We see that
precision is generally much higher than recall. We
believe this is related to the fact that some phenom-
ena encoded by non-projective edges are more sys-
tematic and thus easier to learn than others (e. g. wh-
extraction vs. relative clause extraposition). We also
find that beam search consistently yields modest in-
creases in performance.
Greedy Beam
P R F1 Perfect P R F1 Perfect
Eng 77.31 50.45 61.05 95.76 78.85 50.63 61.66 95.83
Ger 72.33 63.59 67.68 81.91 72.05 64.41 68.02 81.97
Dut 76.66 74.89 75.77 79.28 78.07 76.49 77.27 80.34
Dan 85.90 58.55 69.64 92.76 85.90 58.55 69.64 92.74
Hun 72.60 61.61 66.66 88.46 73.06 64.77 68.67 88.73
Cze 77.79 55.00 64.44 86.28 77.31 55.68 64.74 86.33
Table 4: Precision, recall, F-measure and perfect projec-
tivization results for the lifting classifier.
5.3 Linearization Results and Discussion
We evaluate the linearizer with standard metrics: n-
gram overlap measures (BLEU, NIST), edit distance
(Edit), and the proportion of exactly linearized sen-
tences (Exact). As a means to assess the impact of
lifting more precisely, we propose the word-based
measure Exactlift which only looks at the words
with an incoming lifted edge. The Exactlift score
then corresponds to the percentage of these words
that has been realized in the exact same position as
in the original sentence.
LangLift BLEU NIST Edit Exact Exactlift Nlift
EngNolift 0.911 15.09 0.922 56.40 0.00 0
EngGreedy 0.914 15.10 0.923 57.27 59.87 152
EngBeam 0.916 15.11 0.925 58.48 62.82 156
EngOracle 0.923 15.14 0.928 60.73 70.42 240
GerNolift 0.792 13.76 0.844 40.4 0.00 0
GerGreedy 0.811 13.86 0.864 42.9 55.21 480
GerBeam 0.813 13.86 0.866 43.3 56.47 487
GerOracle 0.843 13.97 0.889 49.95 72.87 634
DutNolift 0.743 11.31 0.796 30.05 0.00 0
DutGreedy 0.784 11.47 0.797 37.56 41.02 256
DutBeam 0.778 11.46 0.8 37.05 47.45 255
DutOracle 0.825 11.63 0.848 44.82 70.55 292
DanNolift 0.836 11.80 0.886 44.41 0.00 0
DanGreedy 0.852 11.88 0.90 45.96 67.65 34
DanBeam 0.858 11.90 0.90 48.76 67.65 34
DanOracle 0.865 11.92 0.90 50.93 74.42 43
HunNolift 0.755 15.70 0.839 30.71 0.00 0
HunGreedy 0.764 15.71 0.844 31.98 41,81 1,538
HunBeam 0.764 15.71 0.844 31.98 41.37 1,581
HunOracle 0.777 15.79 0.849 34.30 57.53 1,933
CzeNolift 0.693 14.32 0.789 25.14 0.00 0
CzeGreedy 0.711 14.45 0.797 26.85 42.04 923
CzeBeam 0.712 14.45 0.795 26.37 41.34 941
CzeOracle 0.729 14.52 0.806 28.79 53.12 1,282
Table 5: Performance of linearizers using different lift-
ings, Exactlift is the exact match for words with an in-
coming lifted edge, Nlift is the total number of lifted
edges.
The results are presented in Table 5. On each
language, the predicted liftings significantly im-
prove on the non-lifted baseline (except the greedy
decoding in English).6 The differences between
the beam and the greedy decoding are not signif-
icant. The scores on the oracle liftings suggest
that the impact of lifting on linearization is heav-
ily language-dependent: It is highest on the V2-
languages, and somewhat smaller on English, Hun-
garian, and Czech. This is not surprising since the
V2-languages (especially German and Dutch) have
the highest proportion of non-projective edges and
sentences (see Table 3). On the other hand, En-
glish has a very small number of non-projective
edges, such that the BLEU score (which captures
the n-gram level) reflects the improvement by only
6We used a t-test, with ? = 0.01.
935
a small increase. However, note that, on the sen-
tence level, the percentage of exactly regenerated
sentences increases by 2 points which suggests that
a non-negligible amount of non-projective sentences
can now be generated more fluently.
50556065707580
Eng
Ger
Dut
Dan
Hun
Cze
langua
ge
accuracy
periph
ery left right
Figure 4: Accuracy for the linearization of the sentences?
left and right periphery, the bars are upper and lower
bounds of the non-lifted and the gold-lifted baseline.
The Exactlift measure refines this picture: The
linearization of the non-projective edges is relatively
exact in English, and much less precise in Hungarian
and Czech where Exactlift is even low on the gold-
lifted edges. The linearization quality is also quite
moderate on Dutch where the lifting leads to con-
siderable improvements. These tendencies point to
some important underlying distinctions in the non-
projective word order phenomena over which we
are generalizing: In certain cases, the linearization
seems to systematically follow from the fact that the
edge has to be lifted, such as wh-extraction in En-
glish (Figure 1). In other cases, the non-projective
linearization is just an alternative to other grammati-
cal, but maybe less appropriate, realizations, such as
the prefield-occupation in German (Figure 2).
Since a lot of non-projective word orders affect
the clause-initial or clause-final position, we evalu-
ate the exact match of the left periphery (first three
words) and the right periphery (last three words) of
the sentence. The accuracies obtained are plotted
in Figure 4, where the lower and upper bars corre-
spond to the lower and upper bound from the non-
lifted and the gold-lifted baseline. It clearly emerges
from this figure that the range of improvements ob-
tainable from lifting is closely tied to the general
linearization quality, and also to word order prop-
erties of the languages. Thus, the range of sentences
affected by the lifting is clearly largest for the V2-
languages. The accuracies are high, but the ranges
are small for English, whereas the accuracies are low
and the ranges quite small for Czech and Hungarian.
System BLEU NIST
(Bohnet et al 2011) (ranked 1st) 0.896 13.93
(Guo et al 2011) (ranked 2nd) 0.862 13.68
Baseline-Non-Lifted + LM 0.896 13.94
Beam-Lifted + LM 0.901 13.96
Table 6: Results on the development set of the 2011
Shared Task on Surface Realisation data, (the test set was
not officially released).
We also evaluated our linearizer on the data of
2011 Shared Task on Surface Realisation, which is
based on the English CoNLL 2009 data (like our
previous evaluations) but excludes information on
morphological realization. For training and evalu-
ation, we used the exact set up of the Shared Task.
For the morphological realization, we used the mor-
phological realizer of Bohnet et al(2010) that pre-
dicts the word form using shortest edit scripts. For
the language model (LM), we use a 5-gram model
with Kneser-Ney (Kneser and Ney, 1995) smoothing
derived from 11 million sentences of the Wikipedia.
In Table 6, we compare our two linearizers (with
and without lifting) to the two top systems of the
2011 Shared Task on Surface Realisation, (Bohnet et
al., 2011) and (Guo et al 2011). Without the lifting,
our system reaches a score comparable to the top-
ranked system in the Shared Task. With the lifting,
we get a small7 but statistically significant improve-
ment in BLEU such that our system reaches a higher
score than the top ranked systems. This shows that
the improvements we obtain from the lifting carry
over to more complex generation tasks which in-
clude morphological realization.
5.4 Human Evaluation
We have carried out a pilot human evaluation on the
German data in order to see whether human judges
prefer word orders obtained from the lifting-based
7Remember that English has the least percentage of non-
projective edges in our data sets, which are however important
to linearize correctly (see Figure 1).
936
linearizer. In particular, we wanted to check whether
the lifting-based linearizer produces more natural
word orders for sentences that had a non-projective
tree in the corpus, and maybe less natural word or-
ders on originally projective sentences. Therefore,
we divided the evaluated items into originally pro-
jective and non-projective sentences.
We asked four annotators to judge 60 sentence
pairs comparing the lifting-based against the non-
lifted linearizer using the toolkit by Kow and Belz
(2012). All annotators are students, two of them
have a background in linguistics. The items were
randomly sampled from the subset of the develop-
ment set containing those sentences where the lin-
earizers produced different surface realizations. The
items are subdivided into 30 originally projective
and 30 originally non-projective sentences.
For each item, we presented the original context
sentence from the corpus and the pair of automat-
ically produced linearizations for the current sen-
tence. The annotators had to decide on two crite-
ria: (i) which sentence do they prefer? (ii) how flu-
ent is that sentence? In both cases, we used con-
tinuous sliders as rating tools, since humans seem
to prefer them (Belz and Kow, 2011). For the first
criterion, the slider positions were mapped to values
from -50 (preference for left sentence) to 50 (pref-
erence for right sentence). If the slider position is
zero, both sentences are equally preferred. For the
second criterion, the slider positions were mapped
to values from 0 (absolutely broken sentence) to 100
(perfectly fluent sentence).
Sentences Scores Equal Lifted Non-lifted
All
% selected 44.58% 35.0% 20.42%
Fluency 56.14 75.77 72.78
Preference 0 34.75 31.06
Non-
Proj.
% selected 29.63% 58.33% 12.04%
Fluency 43.06 76.27 68.85
Preference 0 37.52 24.46
Proj.
% selected 56.82% 15.91% 27.27%
Fluency 61.72 74.29 74.19
Preference 0 26.43 33.44
Table 7: Results from human evaluation.
Table 7 presents the results averaged over all sen-
tences, as well as for the subsets of non-projective
and projective sentences. We report the percentage
of items where the judges selected both, the lifted, or
non-lifted sentence, alongside with the average flu-
ency score (0-100) and preference strength (0-50).
On the entire set of items, the judges selected both
sentences in almost half of the cases. However, on
the subset of non-projective sentences, the lifted ver-
sion is clearly preferred and has a higher average
fluency and preference strength. The percentage of
zero preference items is much higher on the sub-
set of projective sentences. Moreover, the average
fluency of the zero preference items is remarkably
higher on the projective sentences than on the non-
projective subset. We conclude that humans have
a strong preference for lifting-based linearizations
on non-projective sentences. We attribute the low
fluency score on the non-projective zero preference
items to cases where the linearizer did not get a cor-
rect lifting or could not linearize the lifting correctly
such that the lifted and the non-lifted version were
not appropriate. On the other hand, incorrect lift-
ings on projective sentences do not necessarily seem
to result in deprecated linearizations, which leads to
the high percentage of zero preferences with a good
average fluency on this subset.
6 Conclusion
We have presented a novel technique to linearize
sentences for a range of languages that exhibit non-
projective word order. Our approach deals with non-
projectivity by lifting edges in an unordered input
tree which can then be linearized by a standard pro-
jective linearization algorithm.
We obtain significant improvements for the
lifting-based linearization on English, German,
Dutch, Danish, Czech and Hungarian, and show that
lifting has the largest impact on the V2-languages.
In a human evaluation carried out on German we
also show that human judges clearly prefer lifting-
based linearizations on originally non-projective
sentences, and, on the other hand, that incorrect lift-
ings do not necessarily result in bad realizations of
the sentence.
Acknowledgments
This work was funded by the Deutsche Forschungs-
gemeinschaft (DFG) via the SFB 732 ?Incremental
Specification in Context?. We would also like to
thank Anna Ha?tty and our four annotators for their
contribution to the human evaluation.
937
References
A. Belz and E. Kow. 2011. Discrete vs. Continuous Rat-
ing Scales for Language Evaluation in NLP. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 230?235, Portland, Oregon, USA,
June. Association for Computational Linguistics.
A. Belz, M. White, D. Espinosa, D. Hogan, E. Kow, and
A. Stent. 2011. The First Surface Realisation Shared
Task: Overview and Evaluation Results. In ENLG?11.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2000.
The Prague Dependency Treebank: A Three-level an-
notation scenario. In A. Abeille?, editor, Treebanks:
Building and using syntactically annotated corpora.,
chapter 1, pages 103?127. Kluwer Academic Publish-
ers, Amsterdam.
B. Bohnet, L. Wanner, S. Mille, and A. Burga. 2010.
Broad coverage multilingual deep sentence generation
with a stochastic multi-level realizer. In Coling 2010,
pages 98?106.
B. Bohnet, S. Mille, B. Favre, and L. Wanner. 2011.
<stumaba>: From deep representation to surface. In
Proceedings of the Generation Challenges Session at
the 13th European Workshop on NLG, pages 232?235,
Nancy, France.
B. Bohnet. 2004. A Graph Grammar Approach to Map
Between Dependency Trees and Topological Models.
In IJCNLP, pages 636?645.
N. Bro?ker. 1998. Separating Surface Order and Syntactic
Relations in a Dependency Grammar. In COLING-
ACL 98.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Natu-
ral Language Learning, pages 149?164, Morristown,
NJ, USA. Association for Computational Linguistics.
A. Cahill, M. Forst, and C. Rohrer. 2007. Stochastic real-
isation ranking for a free word order language. ENLG
?07, pages 17?24.
K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2006. Online Passive-Aggressive Algorithms. Jour-
nal of Machine Learning Research, 7:551?585.
D. Duchier and R. Debusmann. 2001. Topological de-
pendency trees: A constraint-based account of linear
precedence. In Proceedings of the ACL.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008.
LIBLINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871?1874.
K. Filippova and M. Strube. 2007. Generating con-
stituent order in german clauses. In ACL, pages 320?
327.
K. Filippova and M. Strube. 2009. Tree linearization in
English: improving language model based approaches.
In NAACL, pages 225?228, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
M. Gamon, E. Ringger, R. Moore, S. Corston-Olivier,
and Z. Zhang. 2002. Extraposition: A case study in
German sentence realization. In Proceedings of Col-
ing 2002. Association for Computational Linguistics.
K. Gerdes and S. Kahane. 2001. Word order in german:
A formal dependency grammar using a topological hi-
erarchy. In Proceedings of the ACL.
Y. Guo, D. Hogan, and J. van Genabith. 2011. Dcu at
generation challenges 2011 surface realisation track.
In Proceedings of the Generation Challenges Session
at the 13th European Workshop on NLG, pages 227?
229.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara, M.-
A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,
J. Stepa?nek, P. Strana?k, M. Surdeanu, N. Xue, and
Y. Zhang. 2009. The CoNLL-2009 shared task:
Syntactic and Semantic dependencies in multiple lan-
guages. In Proceedings of the 13th CoNLL Shared
Task, pages 1?18, Boulder, Colorado.
E. Hajic?ova?, J. Havelka, P. Sgall, K. Vesela?, and D. Ze-
man. 2004. Issues of projectivity in the prague de-
pendency treebank. Prague Bulletin of Mathematical
Linguistics, 81.
W. He, H. Wang, Y. Guo, and T. Liu. 2009. Dependency
Based Chinese Sentence Realization. In Proceedings
of the ACL and of the IJCNLP, pages 809?816.
S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-
projectivity: A polynomially parsable non-projective
dependency grammar. In COLING-ACL, pages 646?
652.
A. Kathol and C. Pollard. 1995. Extraposition via com-
plex domain formation. In Meeting of the Association
for Computational Linguistics, pages 174?180.
R. Kneser and H. Ney. 1995. In In Proceedings of the
IEEE International Conference on Acoustics, Speech
and Signal Processing, pages 181?184.
E. Kow and A. Belz. 2012. LGRT-Eval: A Toolkit for
Creating Online Language Evaluation Experiments.
In Proceedings of the 8th International Conference on
Language Resources and Evaluation (LREC?12).
I. Langkilde and K. Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL, pages 704?710.
J. Nivre and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?05), pages 99?106, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
O. Rambow and A. K. Joshi. 1994. A formal look at
dependency grammars and phrase-structure grammars,
with special consideration of word-order phenomena.
938
In Leo Wanner, editor, Current Issues in Meaning-Text
Theory. Pinter, London, UK.
M. Reape. 1989. A logical treatment of semi-free word
order and bounded discontinuous constituency. In
Proceedings of the EACL, EACL ?89, pages 103?110.
E. Ringger, M. Gamon, R. C. Moore, D. Rojas, M. Smets,
and S. Corston-Oliver. 2004. Linguistically informed
statistical models of constituent structure for ordering
in sentence realization. In COLING ?04, pages 673?
679.
W. Seeker and J. Kuhn. 2012. Making Ellipses Explicit
in Dependency Conversion for a German Treebank. In
Proceedings of LREC 2012, Istanbul, Turkey. Euro-
pean Language Resources Association (ELRA).
A. Stent. 2011. Att-0: Submission to generation chal-
lenges 2011 surface realization shared task. In Pro-
ceedings of the Generation Challenges Session at the
13th European Workshop on Natural Language Gener-
ation, pages 230?231, Nancy, France, September. As-
sociation for Computational Linguistics.
V. Vincze, D. Szauter, A. Alma?si, G. Mo?ra, Z. Alexin,
and J. Csirik. 2010. Hungarian Dependency Tree-
bank. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC 2010), pages 1855?1862, Valletta, Malta.
S. Wan, M. Dras, R. Dale, and C. Paris. 2009. Improving
grammaticality in statistical sentence generation: In-
troducing a dependency spanning tree algorithm with
an argument satisfaction model. In EACL, pages 852?
860.
M. White and R. Rajkumar. 2009. Perceptron reranking
for CCG realization. In EMNLP?09, pages 410?419,
Singapore, August.
939
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 333?344,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
The Effects of Syntactic Features in Automatic Prediction of Morphology
Wolfgang Seeker and Jonas Kuhn
Institute for Natural Language Processing
University of Stuttgart
{seeker,jonas}@ims.uni-stuttgart.de
Abstract
Morphology and syntax interact considerably
in many languages and language processing
should pay attention to these interdependen-
cies. We analyze the effect of syntactic fea-
tures when used in automatic morphology pre-
diction on four typologically different lan-
guages. We show that predicting morphology
for languages with highly ambiguous word
forms profits from taking the syntactic context
of words into account and results in state-of-
the-art models.
1 Introduction
In this paper, we investigate the interplay between
syntax and morphology with respect to the task of
assigning morphological descriptions (or tags) to
each token of a sentence. Specifically, we examine
the effect of syntactic information when it is inte-
grated into the feature model of a morphological tag-
ger. We test the effect of syntactic features on four
languages ? Czech, German, Hungarian, and Span-
ish ? and find that syntactic features improve our tag-
ger considerably for Czech and German, but not for
Hungarian and Spanish. Our analysis of construc-
tions that show morpho-syntactic agreement sug-
gests that syntactic features are important if the lan-
guage shows frequent word form syncretisms1 that
can be disambiguated by the syntactic context.
The meaning of a sentence is structurally encoded
1Syncretism describes the situation where a word form is
ambiguous between several different morphological descrip-
tions within its inflection paradigm.
by morphological and syntactic means.2 Different
languages, however, use them to a different extent.
Languages like English encode grammatical infor-
mation (like the subject vs object status of an argu-
ment) via word order, whereas languages like Czech
or Hungarian use different word forms. Automatic
analysis of languages with rich morphology needs
to pay attention to the interaction between morphol-
ogy and syntax in order to arrive at suitable com-
putational models. Linguistic theory (e. g., Bresnan
(2001), Melc?uk (2009)) suggests many interactions
between morphology and syntax. For example, lan-
guages with a case system use different forms of the
same word to mark different syntactic (or seman-
tic) relations (Blake, 2001). In many languages, two
words that participate in a syntactic relation show
covariance in some or all of their morphological fea-
tures (so-called agreement, Corbett (2006)).3
Automatic annotation of morphology assigns
morphological descriptions (e. g., nominative-
singular-masculine) to word forms. It is usually
modeled as a sequence model, often in combination
with part-of-speech tagging and lemmatization
(Collins, 2002; Hajic?, 2004; Smith et al, 2005;
Chrupa?a et al, 2008, and others). Sequence models
achieve high accuracy and coverage but since they
only use linear context they only approximate some
of the underlying hierarchical relationships. As
an example for these hierarchical relationships,
2And also by prosodic means, which we will not discuss
since text-based tools rarely have access to this information.
3For example, in English, the subject of a sentence and the
finite verb agree with respect to their number and person fea-
ture.
333
die wirtschaftlich am weitesten entwickelten , modernen und zum Teil katholisch gepra?gten Regionen
nom/acc.pl.fem nom/acc.pl.fem
the economic - most developed , modern and to part catholic influenced regions
NK
MO
PM MO
NK
CJ
CD
MO
NK MO
CJ
?the regions that are economically most developed, modern, and partly catholic?
Figure 1: Example of a German noun phrase. First and last word agree in number, gender, and case value.
Figure 1 shows a German noun phrase taken from
the German TiGer corpus (Brants et al, 2002).
The two bold-faced words are the determiner and
the head noun of the phrase, and they agree in
their gender, number, and case values. The word
Regionen (regions) is four-way ambiguous for its
case value, which is reduced to a two-way ambi-
guity between nominative and accusative by the
determiner. Further disambiguation would require
information about the syntactic role of the noun
phrase in a sentence. There are 11 tokens between
these two words, which would require a context
window of at least 13 to capture the agreement
relation within a sequence model. Syntactically,
however, as indicated by the dependency tree,
the determiner and the head are linked directly.
The interdependency between morphology and
syntax in the example thus manifests itself in the
morphological disambiguation of a highly syncretic
word form because of its government or agreement
relation to its respective syntactic head/dependents.
Of course, the sequence model is most of the
time a reasonable approximation, because the ma-
jority of noun phrases in the TiGer corpus are not
as long as the example in Figure 1.4 Furthermore,
not all languages show this kind of relationship be-
tween morphological forms and syntactic relation as
demonstrated for German. But taking advantage of
the morphosyntactic dependencies in a language can
give us better models that may even be capable of
handling the more difficult or rare cases. We there-
fore advocate that models for predicting morphology
should be designed with the typological characteris-
tics of a language and its morphosyntactic properties
in mind, and should, where appropriate, integrate
4We find 57,551 noun phrases with less than three tokens
between determiner and noun and 4,670 with three or more.
syntactic information in order to better model the
morphosyntactic interdependencies of the language.
In the remainder of the paper, we show empiri-
cally that taking syntactic information into account
produces state-of-the-art models for languages with
a high interdependency between morphology and
syntax. We use a simple setup, where we combine
a morphological tagger and a dependency parser in
a bootstrapping architecture in order to analyze the
effect of syntactic information on the performance
of the morphological tagger (Section 2). Using syn-
tactic features in morphology prediction requires a
syntactically annotated corpus for training a statisti-
cal parser, which may not be available for languages
with few resources. We show in Section 3 that only
very little syntactically annotated data is required to
achieve the improvements. We furthermore expect
that the improved morphological information also
improves parsing performance and present a prelim-
inary experiment in Section 4.
2 Experiments
In this section, we present a series of experiments
that investigate the effect of syntactic information on
the prediction of morphological features. We start
by describing our data sets and the system that we
used for the experiments.
2.1 Languages and Data Sets
We test our hypotheses on four different languages:
Czech, German, Hungarian, and Spanish.
Spanish, a Romance language, and German, a
Germanic language, constitute inflecting languages
that show verbal and nominal morphology, but not
as sophisticated as Czech and Hungarian. As we
will see in the experiments, it is relatively easy to
334
predict the morphological information annotated in
the Spanish data set.
Czech and Hungarian represent languages with
very rich morphological systems both in verbal and
nominal morphological paradigms. They differ sig-
nificantly in the way in which morphological infor-
mation is encoded in word forms. Czech, a Slavic
language, is an inflecting language, where one suf-
fix may signal several different morphological cate-
gories simultaneously (e. g., number, gender, case).
In contrast, Hungarian, a Finno-Ugric language, is
of the agglutinating type, where each morphological
category is marked by its own morpheme.
Both German and Czech show various form syn-
cretisms in their inflection paradigms. Form syn-
cretisms emerge when the same word form is am-
biguous between several different morphological de-
scriptions, and they are a major challenge to auto-
matic morphological analysis. Spanish shows syn-
cretism in the verbal inflection paradigms. In Hun-
garian, form syncretisms are much less frequent.
The case paradigm of Hungarian only shows one
form syncretism between dative and genitive case
(out of about 18 case suffixes).
All languages show agreement between subject
and verb, and within the noun phrase. The word or-
der in Czech and Hungarian is very variable whereas
it is more restrictive in Spanish and German.
As our data, we use the CoNLL 2009 Shared
Task data sets (Hajic? et al, 2009) for Czech and
Spanish. For German, we use the dependency
conversion of the TiGer treebank by Seeker and
Kuhn (2012), splitting it into 40k/5k/5k sentences
for training/development/test. For Hungarian, we
use the Szeged Dependency Treebank (Vincze et al,
2010), with the split of Farkas et al (2012).
2.2 System Description
To test our hypotheses, we implemented a tagger
that assigns full morphological descriptions to each
token in a sentence. The system was inspired by the
morphological tagger included in mate-tools.5 Like
the tagger provided with mate-tools, it is a classifier
that tags each token using the surrounding tokens in
5A collection of language independent, data-driven analysis
tools for lemmatization, pos-tagging, morphological analysis,
and dependency parsing: http://code.google.com/p/mate-tools
its feature model. Models are trained using passive-
aggressive online training (Crammer et al, 2003).
The system makes two passes over each sentence:
The first pass provides predicted tags that are used
as features during the second pass. We also adopted
the idea of a tag filter, which deterministically as-
signs tags for words that always occur with the same
tag in the training data.
For all matters of syntactic annotation in this pa-
per, we use the graph-based dependency parser by
Bohnet (2010), also included in mate-tools. All data
sets are annotated with gold syntactic information,
which is used to train the parsing models.
For our experiments, we use a bootstrapping ap-
proach: the parser uses the output of the morphology
in its feature set, and the morphological tagger we
want to analyze uses the output of the parser as syn-
tactic features. Since it is best to keep the training
setting as similar as possible to the test setting, we
use 10-fold jackknifing to annotate our training data
with predicted morphology or syntax respectively.
Jackknifing differs from cross-validation only in
its purpose. Cross-validation is used for evaluating
data, jackknifing is used to annotate data. The data
set is split into n parts, and n-1 parts are used to train
a model for annotating the nth part. This is then
rotated n times such that each part is annotated by
the automatic tool without training it on its own test
data. Jackknifing is important for creating a realis-
tic training scenario that provides automatic prepro-
cessing. For annotating development and test sets,
models are trained on the jackknifed training set.
2.3 The Effects of Syntactic Features
In the first experiment, we use the system described
in Section 2.2 to predict morphological information
on all four languages. We start with describing the
general setup and the feature set, and continue with
a discussion of the results.
The experimental setup is as follows: the German
and Spanish data sets are annotated with lemma and
part-of-speech information using 10-fold jackknif-
ing. The annotation is done with mate-tools? lem-
matizer and pos-tagger. For Czech and Hungarian,
we keep the annotation provided with the data sets.
Note that our experimental setup does not include
lemmas or part-of-speech tags as part of the predic-
tion of the morphology but annotates them in a pre-
335
processing step. It is not necessary to separate part-
of-speech and lemma from the prediction of mor-
phology and, in fact, many systems perform these
steps simultaneously (e. g. Spoustova? et al (2009)).
Doing morphology prediction as a separate step al-
lows us to use lemma and part-of-speech informa-
tion in the feature set.6
static features
form form1b form2b
form3b form1a lemma2a
pos1b pos2b pos1a
form+pos pos+s1 pos+s2
pos+s3 pos+s4 lemma+p2
lemma+p3 pos+number form+form1b
pos+pos1a pos+pos1b+pos2b s1+s1 1b
s1+s1 1a s2+s2 1a last-verb-lemma
last-verb-pos next-verb-lemma next-verb-pos
dynamic features
tag1b+tag2b tag2b+tag3b tag1a
tag1a+tag1b tag1a+tag2a tag2a+tag3a
pos1b+case1b last-verb-tag next-verb-tag
pos1b+case1b+pos2b+case2b
Hungarian only features
pos+uppercase
Czech only features
pos+p2
Spanish only features
s5 p1 p4
p5 s2 1a s3 1a
s4 1a
Table 1: Baseline feature set. form means word form,
lemma is lemma, pos is part-of-speech, s1/p1 stand for
suffix and prefix of length 1 (characters), tag is the mor-
phological tag predicted by the system, 1b/1a means 1
token before/after the current token, and + marks feature
conjunctions. number marks if the form contains a digit.
After preprocessing the data, our baseline system
is trained using the feature set shown in Table 1. The
baseline system does not make use of any syntactic
information but predicts morphological information
based solely on tokens and their linear context. The
features are divided into static features, which can be
computed on the input, and dynamic features, which
are computed also on previous output of the system
(cf. two passes in Section 2.2).
6Lemma and part-of-speech prediction may also profit from
syntactic information, see e.g. Prins (2004) or Bohnet and Nivre
(2012).
The feature sets in Table 1 were developed specif-
ically for our experiments and are the result of an
automatic forward/backward feature selection pro-
cess. The purpose of the feature selection was to ar-
rive at a baseline system that performs well without
any syntactic information. With such an optimized
baseline system, we can measure the contribution of
syntactic features more reliably.
The last-verb/next-verb and pos+case features are
variants of the features proposed in Votrubec (2006).
They extract information about the first verb within
the last 10/the next 30 tokens in the sentence. The
case feature extracts the case value from previously
assigned morphological tags. Note that the verb
features are approximating syntactic information by
making the assumption that the closest verbs are
likely to be syntactic heads for many words.
static features
h lemma h s2 h s3 pos+h pos s1+h s1
h dir h dir+h pos
ld s1 ld s2 ld p1 ld p4
dynamic features
h tag ld tag
Table 2: Syntactic features. h and ld mark features from
the head and the left-most daughter, dir is a binary fea-
ture marking the direction of the head with respect to the
current token.
After training the baseline models, we use them to
annotate the whole data set with morphological in-
formation (using 10-fold jackknifing for the training
portions). We then use 10-fold jackknifing again to
annotate the data sets with the dependency parser.
At this point, all our data sets are annotated with
predicted morphology from our baseline system and
with syntactic information from the parser, which
uses the morphological information from our base-
line system in its feature set. We can now retrain our
morphological tagger using features that are derived
from the dependency trees provided by the parser.
Note that this is not a stacking architecture, since
the second system does not use the predicted mor-
phology output from the baseline system. The loop
simply ensures that we get the best possible syntac-
tic features.
We extract two kinds of syntactic features: fea-
tures of the syntactic head of the current token, and
336
dev set test set
all oov all oov
Czech
morfette 90.37 68.66 90.01 67.25
our baseline 92.51 73.12 92.29 72.58
pred syntax *93.18 74.04 *92.82 73.11
gold syntax *93.64 75.20 *93.30 74.96
German
morfette 86.78 66.37 84.58 61.05
our baseline 90.92 72.52 89.11 69.67
pred syntax *92.07 75.06 *90.10 71.18
gold syntax *92.70 *76.29 *90.87 *73.20
Hungarian
morfette *96.19 *85.82 95.99 *85.43
our baseline 96.08 84.49 95.94 83.76
pred syntax 96.18 84.70 96.11 83.85
gold syntax *96.46 85.30 *96.35 84.50
Spanish
morfette 97.83 89.67 97.76 91.00
our baseline 97.83 89.05 97.59 90.88
pred syntax 97.84 89.08 97.67 90.91
gold syntax 98.11 90.34 97.88 91.61
Table 3: The effect of syntactic features when predicting
morphological information. * mark statistically signifi-
cantly better models compared to our baseline (sentence-
based t-test with ? = 0.05).
features of the left-most daughter of the current to-
ken. We also experimented with other types, e. g.
the right-most daughter, but these features did not
improve the model. This is likely due to the way
these languages encode morphological information
and may be different for other languages. From the
head and the left-most daughter, we construct fea-
tures about form, lemma, affixes, and tags. Table 2
lists the syntactic features that we use in the model.
With the syntactic features available due to the
parsing step, we train new models with the full sys-
tem. For each language, we run four experiments.
The first two are baseline experiments, where we
use the off-the-shelf morphological tagger morfette
(Chrupa?a et al, 2008) and our own baseline sys-
tem, both of which do not use any syntactic features.
In the third experiment, we evaluate our full system
using the syntactic features provided by the depen-
dency parser. As an oracle experiment, we also re-
port results on the full system when using the gold
standard syntax from the treebank. Table 3 presents
all results in terms of accuracy on all tokens (all)
dev set test set
all oov all oov
Czech
featurama 94.75 84.12 94.78 84.23
our baseline 93.80 80.47 93.57 80.53
pred syntax *94.40 81.51 *94.24 81.61
gold syntax *94.80 82.45 *94.64 82.80
German
RFTagger 90.63 72.11 89.04 70.80
our baseline 92.59 80.73 91.48 78.83
pred syntax *93.70 82.71 *92.51 80.20
gold syntax *94.28 *84.12 *93.32 *82.35
Hungarian
our baseline 97.27 92.61 97.03 91.28
pred syntax 97.38 92.39 97.19 91.50
gold syntax *97.63 92.79 *97.45 91.92
Spanish
our baseline 98.23 92.46 98.02 93.15
pred syntax 98.24 92.30 98.07 93.03
gold syntax 98.40 92.82 *98.22 93.64
Table 4: The effect of syntactic features when predicting
morphology using lexicons. * mark statistically signifi-
cantly better models compared to our baseline (sentence-
based t-test with ? = 0.05).
and out-of-vocabulary tokens only (oov). Out-of-
vocabulary tokens do not occur in the training data.
We find trends along several axes: Generally, the
syntactic features work well for Czech and Ger-
man, whereas for Hungarian and Spanish, they do
not yield any significant improvement. The im-
provements for German and Czech are between 0.5
(Czech) and 1.0 (German) percentage points abso-
lute in token accuracy, and between 0.2 (Czech test
set) and 2.5 (German dev set) percentage points ab-
solute in accuracy of unknown words. There are no
obvious differences between the development and
the test set in any of the languages.
Compared to the morfette baseline, we find our
systems to be either superior or equal to morfette in
terms of token accuracy. Regarding accuracy on un-
known words, morfette outperforms our systems for
Hungarian, but is outperformed on Czech and Ger-
man. For Spanish, all systems yield similar results.
Looking at the oracle experiment, we see that for
all languages, the system can learn something from
syntax. For Czech and German, this is clearly the
337
case, for Hungarian and Spanish, the differences are
small but visible. There are pronounced differences
between the predicted and the gold syntax experi-
ments in Czech and German. Clearly, the parser
makes mistakes that propagate through to the pre-
diction of the morphology.
2.4 Syntax vs Lexicon
The current state-of-the-art in predicting morpho-
logical features makes use of morphological lexi-
cons (e.g. Hajic? (2000), Hakkani-Tu?r et al (2002),
Hajic? (2004)). Lexicons define the possible morpho-
logical descriptions of a word and a statistical model
selects the most probable one among them. In the
following experiment, we test whether the contribu-
tion of syntactic features is similar or different to the
contribution of morphological lexicons.
Lexicons encode important knowledge that is dif-
ficult to pick up in a purely statistical system, e. g.
the gender of nouns, which often cannot be deduced
from the word form (Corbett, 1991).7
We extend our system from the previous experi-
ment to include information from a morphological
dictionaries. For Czech, we use the morphologi-
cal analyzer distributed with the Prague Dependency
Treebank 2 (Hajic? et al, 2006). For German, we
use DMor (Schiller, 1994). For Hungarian, we use
(Tro?n et al, 2006), and for Spanish, we use the mor-
phological analyzer included in Freeling (Carreras et
al., 2004). The output of the analyzers is given to the
system as features that simply record the presence of
a particular morphological analysis for the current
word. The system can thus use the output of any
tool regardless of its annotation scheme, especially
if the annotation scheme of the treebank is different
from the one of the morphological analyzer.
Table 4 presents the results of experiments where
we add the output of the morphological analyzers
to our system. Again, we run experiments with and
without syntactic features. For Czech, we also show
results from featurama8 with the feature set devel-
oped by Votrubec (2006). For German, we show re-
sults for RFTagger (Schmid and Laws, 2008).
As expected, the information from the morpho-
logical lexicon improves the overall performance
7Lexicons are also often used to speed up processing con-
siderably by restricting the search space of the statistical model.
8http://sourceforge.net/projects/featurama/
considerably compared to the results in Table 3, es-
pecially on unknown tokens. This shows that even
with the considerable amounts of training data avail-
able nowadays, rule-based morphological analyzers
are important resources for morphological descrip-
tion (cf. Hajic? (2000)). The contribution of syn-
tactic features in German and Czech is almost the
same as in the previous experiment, indicating that
the syntactic features contribute information that is
orthogonal to that of the morphological lexicon. The
lexicon provides lexical knowledge about a word
form, while the syntactic features provide the syn-
tactic context that is needed in German and Czech
to decide on the right morphological tag.
2.5 Language Differences
From the previous experiments, we conclude that
syntactic features help in the prediction of morphol-
ogy for Czech and German, but not for Hungarian
and Spanish. To further investigate the difference
between Czech and German on the one hand, and
Hungarian and Spanish on the other, we take a closer
look at the output of the tagger.
We find an interesting difference between the
two pairs of languages, namely the performance
with respect to agreement. Agreement is a phe-
nomenon where morphology and syntax strongly in-
teract. Morphological features co-vary between two
items in the sentence, but the relation between these
items can occur at various linguistic levels (Corbett,
2006). If the syntactic information helps with pre-
dicting morphological information, we expect this
to be particularly helpful with getting agreement
right. All languages show agreement to some ex-
tent. Specifically, all languages show agreement in
number (and person) between the subject and the
verb of a clause. Czech, German, and Spanish show
agreement in number, gender, and case (not Span-
ish) within a noun phrase. Hungarian shows case
agreement within the noun phrase only rarely, e.g.
for attributively used demonstrative pronouns.
In order to test the effect on agreement, we mea-
sure the accuracy on tokens that are in an agreement
relation with their syntactic head. We counted sub-
ject verb agreement as well as agreement with re-
spect to number, gender, and case (where applicable)
between a noun and its dependent adjective and de-
terminer. Table 5 displays the counts from the devel-
338
opment sets of each language. We compare the base-
line system that does not use any syntactic informa-
tion with the output of the morphological tagger that
uses the gold syntax. We use the gold syntax rather
than the predicted one in order to eliminate any in-
fluence from parsing errors. As can be seen from the
results, the level of agreement relations in Czech and
German improves when using syntactic information,
whereas in Spanish and Hungarian, only very tiny
changes occur.
agreement baseline gold syntax
Czech
sbj-verb 3199/4044 = 79.10 3264/4044 = 80.71
NP case 8719/9132 = 95.48 8821/9132 = 96.59
NP num 8933/9132 = 97.82 9016/9132 = 98.73
NP gen 8493/9132 = 93.00 8768/9132 = 96.01
German
sbj-verb 4412/4696 = 93.95 4562/4696 = 97.15
NP case 13340/13951 = 95.62 13510/13951 = 96.84
NP num 13631/13951 = 97.71 13788/13951 = 98.83
NP gen 13253/13951 = 95.00 13528/13951 = 96.97
Hungarian
sbj-verb 8653/10219 = 84.68 8655/10219 = 84.70
NP case 402/891 = 45.12 412/891 = 46.24
Spanish
sbj-verb 1930/2004 = 96.31 1932/2004 = 96.41
NP num 8810/8849 = 99.56 8816/8849 = 99.63
NP gen 8810/8849 = 99.56 8821/8849 = 99.68
Table 5: Agreement counts in morphological annotation
compared between the baseline system and the oracle
system using gold syntax.
For Czech and German, these results sugguest
that syntactic information helps with agreement. We
believe that the reasons why it does not help for
Hungarian and Spanish are the following: for Span-
ish, we see that also the baseline model achieves
very high accuracies (cf. Table 3) and also high rates
of correct agreement. It seems that for Spanish, syn-
tactic context is simply not necessary to make the
correct prediction. For Hungarian, the reason lies
within the inflectional paradigms of the language,
which do not show any form syncretism, mean-
ing that word forms in Hungarian are usually not
ambiguous within one morphological category (e.g.
case). Making a morphological tag prediction, how-
ever, is difficult only if the word form itself is am-
biguous between several morphological tags. In this
case, using the agreement relation between the word
and its syntactic head can help the system making
the proper prediction. This is the situation that we
find in Czech and German, where form syncretism
is pervasive in the inflectional paradigms.
2.6 Syntactic Features in Czech
In Section 2.4 we compared the performance of our
system on Czech to another system, featurama (see
Table 4). Featurama outperforms our baseline sys-
tem by a percentage point in token accuracy (and
even more for unknown tokens). Syntactic informa-
tion closes that gap to a large extent but only using
gold syntax gets our system on a par with featurama.
The question then arises whether the syntactic
features actually contribute something new to the
task, or whether the same effect could also be
achieved with linear context features alone as in fea-
turama. In order to test this we run an additional
experiment, where we add some of the syntax fea-
tures to the feature set of featurama. Specifically,
we add the static features from Table 2 that do not
use lemma or part-of-speech information. Due to the
way featurama works, we cannot use features from
the morphological tags (the dynamic features).
The results in Table 6 show that also featurama
profits from syntactic features, which corroborates
the findings from the previous experiments. We also
note again that better syntax would improve results
even more.
dev set test set
all oov all oov
featurama 94.75 84.12 94.78 84.23
pred syntax 95.18 84.65 95.09 84.52
gold syntax *95.39 84.62 *95.34 85.03
Table 6: Syntactic features for featurama (Czech). * mark
statistically significantly better models compared to feat-
urama (sentence-based t-test with ? = 0.05).
3 How Much Syntax is Needed?
Syntactic features require syntactically annotated
corpora. Without a treebank to train the parser, the
morphology cannot profit from syntactic features.9
This may be problematic for languages for which
there is no treebank, because creating a treebank is
expensive. Fortunately, it turns out that very small
amounts of syntactically annotated data are enough
9Which is of course only a problem for statistical parsers.
339
German Czech
 88
 89
 90
 91
 92
 93
 94
 0  5000  10000  15000  20000  25000  30000  35000  40000
accu
racy
 of m
orph
olog
y
# of sentences in training data of syntactic parser
devtest
 88
 89
 90
 91
 92
 93
 94
 0  5000  10000  15000  20000  25000  30000  35000  40000
accu
racy
 of m
orph
olog
y
# of sentences in training data of syntactic parser
devtest
Figure 2: Dependency between amount of training data for syntactic parser and quality of morphological prediction.
to provide a parsing quality that is sufficient for the
morphological tagger.
In order to test what amount of training data is
needed, we train several parsing models on increas-
ing amounts of syntactically annotated data. For ex-
ample, the first experiment uses the first 1,000 sen-
tences of the treebank. We perform 5-fold jackknif-
ing with the parser on these sentences to annotate
them with syntax. Then we train one parsing model
on these 1,000 sentences and use it to annotate the
rest of the training data as well as the development
and the test set. This gives us the full data set an-
notated with syntax that was learned from the first
1,000 sentences of the treebank. The morphologi-
cal tagger is then trained on the full training set and
applied to development and test set.
Figure 2 shows the dependency between the
amount of training data given to the parser and the
quality of the morphological tagger using syntac-
tic features provided by this parser. The left-most
point corresponds to a model that does not use syn-
tactic information. For both languages, German
and Czech, we find that already 1,000 sentences are
enough training data for the parser to provide useful
syntactic information to the morphological tagger.
After 5,000 sentences, both curves flatten out and
stay on the same level. We conclude that using syn-
tactic features for morphological prediction is viable
even if there is only small amounts of syntactic data
available to train the parser.
As a related experiment, we also test if we can get
the same effect with a very simple and thus much
faster parser. We use the brute-force algorithm de-
scribed in Covington (2001), which selects for each
token in the sentence another token as the head. It
does not have any tree requirements, so it is not even
guaranteed to yield a cycle-free tree structure. In Ta-
ble 7, we compare the simple parser with the mate-
parser, both trained on the first 5,000 sentences of
the treebank. Evaluation is done in terms of labeled
(LAS) and unlabeled attachment score (UAS).10
dev set test set
LAS UAS LAS UAS
Czech
simple parser (5k) 71.57 78.96 69.09 77.23
full parser (5k) 76.77 84.38 74.70 83.00
German
simple parser (5k) 83.06 85.23 78.56 81.18
full parser (5k) 87.56 90.08 83.69 86.58
Table 7: Simple parser vs full parser ? syntactic quality.
Trained on first 5,000 sentences of the training set.
As expected, the simple parser performs much
worse in terms of syntactic quality. Table 8 shows
the performance of the morphological tagger when
using the output of both parsers as syntactic fea-
tures. For Czech, both parsers seem to supply sim-
ilar information to the morphological tagger, while
for German, using the full parser is clearly better.
In both cases, the morphological tagger outperforms
the models that do not use syntactic information (cf.
Table 3). The performance on unknown words is
however much worse for both languages. We con-
clude that even with a simple parser and little train-
ing data, the morphology can make use of syntactic
information to some extent.
10LAS: correct edges with correct labelsall edges , UAS:
correct edges
all edges
340
dev set test set
all oov all oov
Czech
no syntax 92.51 73.12 92.29 72.58
simple syntax 92.96 73.45 92.53 72.66
full syntax 93.08 73.64 92.69 73.39
German
no syntax 90.92 72.52 89.11 69.67
simple syntax 91.52 73.34 89.66 70.52
full syntax 91.92 83.46 89.91 80.50
Table 8: Simple parser vs full parser ? morphological
quality. The parsing models were trained on the first
5,000 sentences of the training data, the morphological
tagger was trained on the full training set.
4 Does Better Morphology lead to Better
Parses?
In the previous sections, we show that syntactic in-
formation improves a model for predicting morphol-
ogy for Czech and German, where syntax and mor-
phology interact considerably. A natural question
then is whether the improvement also occurs in the
other direction, namely whether the improved mor-
phology also leads to better parsing models.
In the previous experiments, we run a 10-fold
jackknifing process to annotate the training data with
morphological information using no syntactic fea-
tures and afterwards use jackknifing with the parser
to annotate syntax. The syntax is subsequently used
as features for our predicted-syntax experiments.
We can apply the same process once more with the
morphology prediction in order to annotate the train-
ing data with morphological information that is pre-
dicted using the syntactic features. A parser trained
on this data will then use the improved morphology
as features. If the improved morphology has an im-
pact on the parser, the quality of the second parsing
model should then be superior to the first parsing
model, which uses the morphology predicted with-
out syntactic information. Note that for the follow-
ing experiments, neither morphology model uses the
morphological lexicon.
Table 9 presents the evaluation of the two pars-
ing models (one using morphology without syntactic
features, the other one using the improved morphol-
ogy). The results show no improvement in parsing
performance when using the improved morphology.
Looking closer at the output, we find differences be-
dev set test set
LAS UAS LAS UAS
Czech
baseline morph 81.73 88.45 81.02 87.77
morph w/ syntax 81.63 88.37 80.83 87.61
German
baseline morph 91.16 92.97 88.06 90.24
morph w/ syntax 91.20 92.97 88.15 90.34
Table 9: Impact of the improved morphology on the qual-
ity of the dependency parser for Czech and German.
tween the two parsing models with respect to gram-
matical functions that are morphologically marked.
For example, in German, performance on subjects
and accusative objects improves while performance
for dative objects and genitives decreases. This sug-
gests different strengths in the two parsing models.
However, the question how to make use of the im-
proved morphology in parsing clearly needs more
research in the future. A promising avenue may be
the approach by Hohensee and Bender (2012).
5 Related Work
Morphological taggers have been developed for
many languages. The most common approach is the
combination of a morphological lexicon with a sta-
tistical disambiguation model (Hakkani-Tu?r et al,
2002; Hajic?, 2004; Smith et al, 2005; Spoustova?
et al, 2009; Zsibrita et al, 2013).
Our work has been inspired by Versley et al
(2010), who annotate a treebank with morphologi-
cal information after the syntax had been annotated
already. The system used a finite-state morphology
to propose a set of candidate tags for each word,
which is then further restricted using hand-crafted
rules over the already available syntax tree.
Lee et al (2011) pursue the idea of jointly predict-
ing syntax and morphology, out of the motivation
that joint models should model the problem more
faithfully. They demonstrate that both sides can use
information from each other. However, their model
is computationally quite demanding and its overall
performance falls far behind the standard pipeline
approach where both tasks are done in sequence.
The problem of modeling the interaction between
morphology and syntax has recently attracted some
attention in the SPMRL workshops (Tsarfaty et al,
341
2010). Modeling morphosyntactic relations explic-
itly has been shown to improve statistical parsing
models (Tsarfaty and Sima?an, 2010; Goldberg and
Elhadad, 2010; Seeker and Kuhn, 2013), but the co-
dependency between morphology and syntax makes
it a difficult problem, and linguistic intuition is often
contradicted by the empirical findings. For example,
Marton et al (2013) show that case information is
the most helpful morphological feature for parsing
Arabic, but only if it is given as gold information,
whereas using case information from an automatic
system may even harm the performance.
Morphologically rich languages pose different
challenges for automatic systems. In this paper, we
work with European languages, where the problem
of predicting morphology can be reduced to a tag-
ging problem. In languages like Arabic, Hebrew,
or Turkish, widespread ambiguity in segmentation
of single words into meaningful morphemes adds an
additional complexity. Given a good segmentation
tool that takes care of this, our approach is appli-
cable to these languages as well. For Hebrew, this
problem has also been addressed by jointly mod-
eling segmentation, morphological prediction, and
syntax (Cohen and Smith, 2007; Goldberg and Tsar-
faty, 2008; Goldberg and Elhadad, 2013).
6 Conclusion
In this paper, we have demonstrated that using syn-
tactic information for predicting morphological in-
formation is helpful if the language shows form syn-
cretism in combination with morphosyntactic phe-
nomena like agreement. A model that uses syntactic
information is superior to a sequence model because
it leverages the syntactic dependencies that may hold
between morphologically dependent words as sug-
gested by linguistic theory. We also showed that
only small amounts of training data for a statistical
parser would be needed to improve the morphologi-
cal tagger. Making use of the improved morphology
in the dependency parser is not straight-forward and
requires more investigation in the future.
Modeling the interaction between morphology
and syntax is important for building successful pars-
ing pipelines for languages with free word order and
rich morphology. Moreover, our experiments show
that paying attention to the individual properties of a
language can help us explain and predict the behav-
ior of automatic tools. Thus, the term ?morpholog-
ically rich language? should be viewed as a broad
term that covers many different languages, whose
differences among each other may be as important as
the difference with languages with a less rich mor-
phology.
Acknowledgments
We would like to thank Jan Hajic? and Jan S?te?pa?nek
for their kind help with the Czech morphology and
featurama. We would also like to thank Thomas
Mu?ller for sharing resources and thoughts with us,
and Anders Bjo?rkelund for commenting on earlier
versions of this paper. This work was funded by
the Deutsche Forschungsgemeinschaft (DFG) via
SFB 732 ?Incremental Specification in Context?,
project D8.
References
Barry J. Blake. 2001. Case. Cambridge University
Press, Cambridge, New York, 2nd edition.
Bernd Bohnet and Joakim Nivre. 2012. A Transition-
Based System for Joint Part-of-Speech Tagging and
Labeled Non-Projective Dependency Parsing. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1455?
1465, Jeju, South Korea. Association for Computa-
tional Linguistics.
Bernd Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings of
the 23rd International Conference on Computational
Linguistics, pages 89?97, Beijing, China. International
Committee on Computational Linguistics.
Sabine Brants, Stefanie Dipper, Silvia Hansen-Shirra,
Wolfgang Lezius, and George Smith. 2002. The
TIGER treebank. In Proceedings of the 1st Workshop
on Treebanks and Linguistic Theories, pages 24?41,
Sozopol, Bulgaria.
Joan Bresnan. 2001. Lexical-Functional Syntax. Black-
well Publishers.
Xavier Carreras, Isaac Chao, Llus Padr, and Muntsa Padr.
2004. Freeling: An open-source suite of language
analyzers. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC?04), pages 239?242. European Language Re-
sources Association (ELRA).
342
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van
Genabith. 2008. Learning morphology with mor-
fette. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC?08), pages 2362?2367, Marrakech, Morocco.
European Language Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 208?217, Prague,
Czech Republic. Association for Computational Lin-
guistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1?8. Association for
Computational Linguistics, July.
Greville G. Corbett. 1991. Gender. Cambridge Text-
books in Linguistics. Cambridge University Press.
Greville G. Corbett. 2006. Agreement. Cambridge Text-
books in Linguistics. Cambridge University Press.
Michael A. Covington. 2001. A fundamental algorithm
for dependency parsing (with corrections). In Pro-
ceedings of the 39th Annual ACM Southeast Confer-
ence, Athens, Gorgia. ACM.
Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, and
Yoram Singer. 2003. Online passive-aggressive algo-
rithms. In Proceedings of the 16th Annual Conference
on Neural Information Processing Systems, volume 7,
pages 1217?1224, Cambridge, Massachusetts, USA.
MIT Press.
Richa?rd Farkas, Veronika Vincze, and Helmut Schmid.
2012. Dependency parsing of hungarian: Baseline re-
sults and challenges. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 55?65, Avignon,
France. Association for Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. Easy first
dependency parsing of modern Hebrew. In Proceed-
ings of the NAACL HLT 2010 First Workshop on Sta-
tistical Parsing of Morphologically-Rich Languages,
pages 103?107, Los Angeles, California, USA. Asso-
ciation for Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2013. Word seg-
mentation, unknown-word resolution, and morpholog-
ical agreement in a hebrew parsing system. Computa-
tional Linguistics, 39(1):121?160.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics, pages 371?379, Columbus, Ohio. Association for
Computational Linguistics.
Jan Hajic?. 2000. Morphological Tagging: Data vs. Dic-
tionaries. In Proceedings of the 6th ANLP Conference
/ 1st NAACL Meeting, pages 94?101, Seattle, Wash-
ington. Association for Computational Linguistics.
Jan Hajic?. 2004. Disambiguation of Rich Inflection
(Computational Morphology of Czech). Nakladatel-
stv?? Karolinum, Prague, Czech Republic.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr Sgall,
Petr Pajas, Jan S?te?pa?nek, Ji?? Havelka, and Marie
Mikulova?. 2006. Prague Dependency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan Stepa?nek, Pavel Strana?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and Semantic dependen-
cies in multiple languages. In Proceedings of the
13th Conference on Computational Natural Language
Learning: Shared Task, pages 1?18, Boulder, Col-
orado, USA. Association for Computational Linguis-
tics.
Dilek Z. Hakkani-Tu?r, Kemal Oflazer, and Go?khan Tu?r.
2002. Statistical morphological disambiguation for
agglutinative languages. Computers and the Humani-
ties, 36(4):381?410.
Matt Hohensee and Emily M. Bender. 2012. Getting
more from morphology in multilingual dependency
parsing. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 315?326, Montre?al, Canada. Association
for Computational Linguistics.
John Lee, Jason Naradowsky, and David A. Smith. 2011.
A discriminative model for joint morphological disam-
biguation and dependency parsing. In Proceedings of
the 49th annual meeting of the Association for Compu-
tational Linguistics, pages 885?894, Portland, USA.
Association for Computational Linguistics.
Yuval Marton, Nizar Habash, and Owen Rambow. 2013.
Dependency parsing of modern standard arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Igor Melc?uk. 2009. Dependency in linguistic descrip-
tion.
Robbert Prins. 2004. Beyond N in N-gram tagging. In
Leonoor Van Der Beek, Dmitriy Genzel, and Daniel
Midgley, editors, Proceedings of the ACL 2004 Student
Research Workshop, pages 61?66, Barcelona, Spain.
Association for Computational Linguistics.
Anne Schiller. 1994. Dmor - user?s guide. Technical
report, University of Stuttgart.
343
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics, pages 777?784, Morristown, NJ,
USA. Association for Computational Linguistics.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation, pages 3132?3139, Istanbul, Turkey. European
Language Resources Association (ELRA).
Wolfgang Seeker and Jonas Kuhn. 2013. Morphologi-
cal and syntactic case in statistical dependency pars-
ing. Computational Linguistics, 39(1):23?55.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 475?482, Vancouver, British Columbia, Canada,
October. Association for Computational Linguistics.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 763?771, Athens, Greece. Association for
Computational Linguistics.
Viktor Tro?n, Pe?ter Hala?csy, Pe?ter Rebrus, Andra?s Rung,
Pe?ter Vajda, and Eszter Simon. 2006. Morphdb.hu:
Hungarian lexical database and morphological gram-
mar. In Proceedings of the 5th International Confer-
ence on Language Resources and Evaluation, pages
1670?1673, Genoa, Italy.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 40?48, Los
Angeles, California, USA. Association for Computa-
tional Linguistics.
Reut Tsarfaty, Djame? Seddah, Yoav Goldberg, Sandra
Ku?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing of morphologically rich languages (SPMRL):
what, how and whither. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 1?12, Los
Angeles, California, USA. Association for Computa-
tional Linguistics.
Yannick Versley, Kathrin Beck, Erhard Hinrichs, and
Heike Telljohann. 2010. A syntax-first approach to
high-quality morphological analysis and lemma dis-
ambiguation for the tba-d/z treebank. In 9th Confer-
ence on Treebanks and Linguistic Theories (TLT9),
pages 233?244.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy
Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of the 7th
Conference on International Language Resources and
Evaluation, pages 1855?1862, Valletta, Malta. Euro-
pean Language Resources Association (ELRA).
Jan Votrubec. 2006. Morphological tagging based on
averaged perceptron. In WDS?06 Proceedings of Con-
tributed Papers, pages 191?195, Praha, Czechia. Mat-
fyzpress, Charles University.
Ja?nos Zsibrita, Veronika Vincze, and Richa?rd Farkas.
2013. magyarlanc 2.0: szintaktikai elemze?s e?s felgy-
ors??tott szo?faji egye?rtelms??te?s. In Attila Tana?cs and
Veronika Vincze, editors, IX. Magyar Sza?m??to?ge?pes
Nyelve?szeti Konferencia, pages 368?374, Szeged,
Hungary.
344
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 57?60,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
A Graphical Interface for Automatic Error Mining in Corpora
Gregor Thiele Wolfgang Seeker Markus G
?
artner Anders Bj
?
orkelund Jonas Kuhn
Institute for Natural Language Processing
University of Stuttgart
{thielegr,seeker,gaertnms,anders,kuhn}@ims.uni-stuttgart.de
Abstract
We present an error mining tool that is de-
signed to help human annotators to find
errors and inconsistencies in their anno-
tation. The output of the underlying al-
gorithm is accessible via a graphical user
interface, which provides two aggregate
views: a list of potential errors in con-
text and a distribution over labels. The
user can always directly access the ac-
tual sentence containing the potential er-
ror, thus enabling annotators to quickly
judge whether the found candidate is in-
deed incorrectly labeled.
1 Introduction
Manually annotated corpora and treebanks are the
primary tools that we have for developing and
evaluating models and theories for natural lan-
guage processing. Given their importance for test-
ing our hypotheses, it is imperative that they are
of the best quality possible. However, manual an-
notation is tedious and error-prone, especially if
many annotators are involved. It is therefore desir-
able to have automatic means for detecting errors
and inconsistencies in the annotation.
Automatic methods for error detection in tree-
banks have been developed in the DECCA
project
1
for several different annotation types, for
example part-of-speech (Dickinson and Meurers,
2003a), constituency syntax (Dickinson and Meur-
ers, 2003b), and dependency syntax (Boyd et al.,
2008). These algorithms work on the assumption
that two data points that appear in identical con-
texts should be labeled in the same way. While
the data points in question, or nuclei, can be single
tokens, spans of tokens, or edges between two to-
kens, context is usually modeled as n-grams over
the surrounding tokens. A nucleus that occurs
1
http://www.decca.osu.edu
multiple times in identical contexts but is labeled
differently shows variation and is considered a po-
tential error.
Natural language is ambiguous and variation
found by an algorithm may be a genuine ambigu-
ity rather than an annotation error. Although we
can support an annotator in finding inconsisten-
cies in a treebank, these inconsistencies still need
to be judged by humans. In this paper, we present
a tool that allows a user to run automatic error de-
tection on a corpus annotated with part-of-speech
or dependency syntax.
2
The tool provides the user
with a graphical interface to browse the variation
nuclei found by the algorithm and inspect their la-
bel distribution. The user can always switch be-
tween high-level aggregate views and the actual
sentences containing the potential error in order to
decide if that particular annotation is incorrect or
not. The interface thus brings together the output
of the error detection algorithm with a direct ac-
cess to the corpus data. This speeds up the pro-
cess of tracking down inconsistencies and errors
in the annotation considerably compared to work-
ing with the raw output of the original DECCA
tools. Several options allow the user to fine-tune
the behavior of the algorithm. The tool is part of
ICARUS (G?artner et al., 2013), a general search
and exploration tool.
3
2 The Error Detection Algorithm
The algorithm, described in Dickinson and Meur-
ers (2003a) for POS tags, works by starting from
individual tokens (the nuclei) by recording their
assigned part-of-speech over an entire treebank.
From there, it iteratively increases the context for
each instance by extending the string to both sides
to include adjacent tokens. It thus successively
builds larger n-grams by adding tokens to the left
2
Generalizing the tool to support any kind of positional
annotation is planned.
3
http://www.ims.uni-stuttgart.de/data/icarus.html
57
Figure 1: The variation n-gram view.
or to the right. Instances are grouped together if
their context is identical, i. e. if their token n-
grams match. Groups where all instances have
the same label do not show variation and are dis-
carded. The algorithm stops when either no vari-
ation nuclei are left or when none of them can be
further extended. All remaining groups that show
variation are considered potential errors. Erro-
neous annotations that do not show variation in the
data cannot be found by the algorithm. This limits
the usefulness of the method for very small data
sets. Also, given the inherent ambiguity of nat-
ural language, the algorithm is not guaranteed to
exclusively output errors, but it achieves very high
precision in experiments on several languages.
The algorithm has been extended to find errors
in constituency and dependency structures (Dick-
inson and Meurers, 2003b; Boyd et al., 2008),
where the definition of a nucleus is changed to
capture phrases and dependency edges. Context
is always modeled using n-grams over surround-
ing tokens, but see, e. g., Boyd et al. (2007) for
extensions.
3 Graphical Error Mining
To start the error mining, a treebank and an error
mining algorithm (part-of-speech or dependency)
must be selected. The algorithm is then executed
on the data to create the variation n-grams. The
user can choose between two views for browsing
the potential errors in the treebank: (1) a view
showing the list of variation n-grams found by the
error detection algorithm and (2) a view showing
label distributions over word forms.
3.1 The Variation N-Gram View
Figure 1 shows a screenshot of the view where the
user is presented with the list of variation n-grams
output by the error detection algorithm. The main
window shows the list of n-grams. When the user
selects one of the n-grams, information about the
nucleus is displayed below the main window. The
user can inspect the distribution over labels (here
part-of-speech tags) with their absolute frequen-
cies. Above the main window, the user can adjust
the length of the presented n-grams, sort them, or
search for specific strings.
For example, Figure 1 shows a part of the vari-
ation n-grams found in the German TiGer corpus
(Brants et al., 2002). The minimum and maximum
length was restricted to four, thus the list contains
only 4-grams. The 4-gram so hoch wie in was se-
lected, which contains wie as its nucleus. In the
lower part, the user can see that wie occurs with
four different part-of-speech tags in the treebank,
namely KOKOM, PWAV, KON, and KOUS. Note
that the combination with KOUS occurs only once
in the entire treebank.
Double clicking on the selected 4-gram in the
list will open up a new tab that displays all sen-
tences that contain this n-gram, with the nucleus
being highlighted. The user can then go through
each of the sentences and decide whether the an-
notated part-of-speech tag is correct. Each time
the user clicks on an n-gram, a new tab will be
created, so that the user can jump back to previous
results without having to recreate them.
A double click on one of the lines in the lower
part of the window will bring up all sentences that
contain that particular combination of word form
58
Figure 2: The label distribution view.
and part-of-speech tag. The fourth line will, for
example, show the one sentence where wie has
been tagged as KOUS, making it easy to quickly
judge whether the tag is correct. In this case, the
annotation is incorrect (it should have been PWAV)
and should thus be marked for correction.
3.2 The Label Distribution View
In addition to the output of the algorithm by Dick-
inson and Meurers (2003a), the tool also provides
a second view, which displays tag distributions of
word forms to the user (see Figure 2). To the left,
a list of unique label combinations is shown. Se-
lecting one of them displays a list of word forms
that occur with exactly these tags in the corpus.
This list is shown below the list of label combina-
tions. To the right, the frequencies of the differ-
ent labels are shown in a bar chart. The leftmost
bar for each label always shows the total frequency
summed over all word forms in the set. Selecting
one or more in the list of word forms adds addi-
tional bars to the chart that show the frequencies
for each selected word form.
As an example, Figure 2 shows the tag combi-
nation [VVINF][VVIZU], which are used to tag in-
finitives with and without incorporated zu in Ger-
man. There are three word forms in the cor-
pus that occur with these two part-of-speech tags:
hinzukommen, aufzul?osen, and anzun?ahern. The
chart on the right shows the frequencies for each
word form and part-of-speech tag, revealing that
hinzukommen is mostly tagged as VVINF but once
as VVIZU, whereas for the other two word forms it
is the other way around. This example is interest-
ing if one is looking for annotation errors in the
TiGer treebank, because the two part-of-speech
tags should have a complementary distribution (a
German verb either incorporates zu or it does not).
Double clicking on the word forms in the list in
the lower left corner will again open up a tab that
shows all sentences containing this word form, re-
gardless of their part-of-speech tag. The user may
then inspect the sentences and decide whether the
annotations are erroneous or not. If the user wants
to see a specific combination, which is more use-
ful if the total number of sentences is large, she
can also click on one of the bars in the chart to get
all sentences matching that combination. In the
example, the one instance of hinzukommen being
tagged as VVIZU is incorrect,
4
and the instances of
the two other verbs tagged as VVINF are as well.
3.3 Dependency Annotation Errors
As mentioned before, the tool also allows the user
to search for errors in dependency structures. The
error mining algorithm for dependency structures
(Boyd et al., 2008) is very similar to the one for
part-of-speech tags, and so is the interface to the
n-gram list or the distribution view. Dependency
edges are therein displayed as triples: the head,
the dependent, and the edge label with the edge?s
direction. As with the part-of-speech tags, the user
can always jump directly to the sentences that con-
tain a particular n-gram or dependency relation.
4
Actually, the word form hinzukommen can belong to two
different verbs, hinzu-kommen and hin-kommen. However,
the latter, which incorporates zu, does not occur in TiGer.
59
4 Error Detection on TiGer
We ran the error mining algorithm for part-of-
speech on the German TiGer Treebank (the de-
pendency version by Seeker and Kuhn (2012)) and
manually evaluated a small sample of n-grams in
order to get an idea of how useful the output is.
We manually checked 115 out of the 207 vari-
ation 6-grams found by the tool, which amounts
to 119 different nuclei. For 99.16% of these nu-
clei, we found erroneous annotations in the asso-
ciated sentences. 95.6% of these are errors where
we are able to decide what the right tag should
be, the remaining ones are more difficult to disam-
biguate because the annotation guidelines do not
cover them.
These results are in line with findings by Dick-
inson and Meurers (2003a) for the Penn Treebank.
They show that even manually annotated corpora
contain errors and an automatic error mining tool
can be a big help in finding them. Furthermore,
it can help annotators to improve their annotation
guidelines by pointing out phenomena that are not
covered by the guidelines, because these phenom-
ena will be more likely to show variation.
5 Related Work
We are aware of only one other graphical tool that
was developed to help with error detection in tree-
banks: Ambati et al. (2010) and Agarwal et al.
(2012) describe a graphical tool that was used in
the annotation of the Hindi Dependency Treebank.
To find errors, it uses a statistical and a rule-based
component. The statistical component is recall-
oriented and learns a MaxEnt model, which is used
to flag dependency edges as errors if their proba-
bility falls below a predefined threshold. In or-
der to increase the precision, the output is post-
processed by the rule-based component, which is
tailored to the treebank?s annotation guidelines.
Errors are presented to the annotators in tables,
also with the option to go to the sentences di-
rectly from there. Unlike the algorithm we im-
plemented, this approach needs annotated training
data for training the classifier and tuning the re-
spective thresholds.
6 Conclusion
High-quality annotations for linguistic corpora are
important for testing hypotheses in NLP and lin-
guistic research. Automatically marking potential
annotation errors and inconsistencies are one way
of supporting annotators in their work. We pre-
sented a tool that provides a graphical interface for
annotators to find and evaluate annotation errors
in treebanks. It implements the error detection al-
gorithms by Dickinson and Meurers (2003a) and
Boyd et al. (2008). The user can view errors from
two perspectives that aggregate error information
found by the algorithm, and it is always easy to
go directly to the actual sentences for manual in-
spection. The tool is currently extended such that
annotators can make changes to the data directly
in the interface when they find an error.
Acknowledgements
We thank Markus Dickinson for his comments.
Funded by BMBF via project No. 01UG1120F,
CLARIN-D, and by DFG via SFB 732, project D8.
References
Rahul Agarwal, Bharat Ram Ambati, and Anil Kumar
Singh. 2012. A GUI to Detect and Correct Errors in
Hindi Dependency Treebank. In LREC 2012, pages
1907?1911.
Bharat Ram Ambati, Mridul Gupta, Samar Husain, and
Dipti Misra Sharma. 2010. A High Recall Error
Identification Tool for Hindi Treebank Validation.
In LREC 2010.
Adriane Boyd, Markus Dickinson, and Detmar Meur-
ers. 2007. Increasing the Recall of Corpus Annota-
tion Error Detection. In TLT 2007, pages 19?30.
Adriane Boyd, Markus Dickinson, and Detmar Meur-
ers. 2008. On Detecting Errors in Dependency
Treebanks. Research on Language and Computa-
tion, 6(2):113?137.
Sabine Brants, Stefanie Dipper, Silvia Hansen-Shirra,
Wolfgang Lezius, and George Smith. 2002. The
TIGER treebank. In TLT 2002, pages 24?41.
Markus Dickinson and W. Detmar Meurers. 2003a.
Detecting Errors in Part-of-Speech Annotation. In
EACL 2003, pages 107?114.
Markus Dickinson and W. Detmar Meurers. 2003b.
Detecting Inconsistencies in Treebanks. In TLT
2003, pages 45?56.
Markus G?artner, Gregor Thiele, Wolfgang Seeker, An-
ders Bj?orkelund, and Jonas Kuhn. 2013. ICARUS
? An Extensible Graphical Search Tool for Depen-
dency Treebanks. In ACL: System Demonstrations,
pages 55?60.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In LREC 2012, pages 3132?3139.
60
Morphological and Syntactic Case in
Statistical Dependency Parsing
Wolfgang Seeker?
University of Stuttgart
Jonas Kuhn??
University of Stuttgart
Most morphologically rich languages with free word order use case systems to mark the gram-
matical function of nominal elements, especially for the core argument functions of a verb. The
standard pipeline approach in syntactic dependency parsing assumes a complete disambiguation
of morphological (case) information prior to automatic syntactic analysis. Parsing experiments
on Czech, German, and Hungarian show that this approach is susceptible to propagating
morphological annotation errors when parsing languages displaying syncretism in their mor-
phological case paradigms. We develop a different architecture where we use case as a possibly
underspecified filtering device restricting the options for syntactic analysis. Carefully designed
morpho-syntactic constraints can delimit the search space of a statistical dependency parser and
exclude solutions that would violate the restrictions overtly marked in the morphology of the
words in a given sentence. The constrained system outperforms a state-of-the-art data-driven
pipeline architecture, as we show experimentally, and, in addition, the parser output comes with
guarantees about local and global morpho-syntactic wellformedness, which can be useful for
downstream applications.
1. Introduction
In statistical parsing, many of the first models were developed and optimized for
English. This is not surprising, given that English is the predominant language for
research in both computational linguistics and linguistics proper. By design, the
statistical parsing approach avoids language-specific decisions built into the model
architecture; models should in principle be trainable on any data following the general
treebank representation scheme. At the same time, it is well known from theoretical
and typological work in linguistics that there is a broad multi-dimensional spectrum
of language types, and that English is in a rather ?extreme? area in that it marks
grammatical relations (subject, object, etc.) strictly with phrase-structural configura-
tions. There are only residues of an inflectional morphology left. In other words, one
? Institut fu?r Maschinelle Sprachverarbeitung, Universita?t Stuttgart, Pfaffenwaldring 5b, D-70569 Stuttgart,
Germany. E-mail: seeker@ims.uni-stuttgart.de.
?? Institut fu?r Maschinelle Sprachverarbeitung, Universita?t Stuttgart, Pfaffenwaldring 5b, D-70569 Stuttgart,
Germany. E-mail: jonas@ims.uni-stuttgart.de.
Submission received: 30 September 2011; revised submission received: 20 May 2012; accepted for publication:
3 August 2012.
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 1
cannot exclude that architectural or representational modeling decisions established
as empirically useful on English data may be favoring the specific language type
of English. Indeed, carrying over successful model architectures from English to
typologically different languages mostly leads to a substantial drop in parsing
accuracy. Linguistically aware representational adjustments can help reduce the
problem significantly, as Collins et al (1999) showed in their pivotal study adjusting a
statistical (constituent) parsing model to a highly inflectional language with free word
order, Czech in that case, pushing the results more than seven percentage points up
to a final 80% dependency accuracy (as compared with 91% accuracy for the English
?source? parser on the Wall Street Journal). Even in recent years, however, a clear gap
has remained between the top parsing architecture for English and morphologically
rich(er) languages.1 The relative hardness of the parsing task, compared with English,
cuts across statistical parsing approaches (constituent or dependency parsing) and
across morphological subtypes, such as languages with a moderately sized remaining
inflectional system (like German), highly inflected languages (like Czech), and
languages in which interactions with derivational morphology make the segmentation
question non-trivial (such as Turkish or Arabic, compare, for example, Eryig?it, Nivre,
and Oflazer [2008]).
Still, it remains hard to pinpoint systematic architectural or representational factors
that explain the empirical picture, although there is a collection of ?recipes? one can
try to tune an approach to a ?hard language.? Of course, there are good reasons
for adjusting a well-proven system rather than developing a more general one from
scratch?given that part of the success of statistical parsing in general lies in subtle
ways of exploiting statistical patterns that reflect inaccessible levels of information in an
indirect way.
This article attempts to do justice to the special status of mature data-driven systems
and still contribute to a systematic clarification, by (1) focusing on a clear-cut aspect
of morphological marking relevant to syntactic parsing (namely, case marking of core
arguments); (2) comparing a selection of languages covering part of the typological
spectrum (Czech, German, and Hungarian); (3) using a state-of-the-art data-driven
parser (Bohnet 2009, 2010) to establish how far the technique of representational ad-
justments may take us; and (4) performing a problem-oriented comparison with an
alternative architecture, which allows us to add constraints motivated from linguistic
considerations.
In a first experiment, we vary the morphological information available to the parser
and examine the errors of the parser with respect to the case-related functions. It
turns out that although the parser is indeed able to learn the case-function mapping
for all three languages, it is susceptible to errors that are propagated through the
pipeline model when parsing languages that show syncretism2 in their morphological
paradigms, in our case Czech and German (e. g., for neuter nouns, nominative and
accusative case have the same surface form). In contrast, due to its mostly unambiguous
case system, we find a much smaller effect for Hungarian. Although the parser itself
profits much from morphological information as our experiments with gold standard
morphology show, errors in automatically predicted morphological information fre-
quently cause errors in the syntactic analysis.
1 Compare, for example, the various Shared Tasks on parsing multiple languages, such as the CoNLL
Shared Tasks 2006, 2007, 2009 (Buchholz and Marsi 2006; Nivre et al 2007a; Hajic? et al 2009), or the PaGe
Shared Task on parsing German (Ku?bler 2008).
2 Two or more different feature values are signaled by the same form.
24
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
In order to better handle syncretism in the morphological description, we then
propose a different way of integrating morphology into the parsing process. We develop
an alternative architecture that circumvents the strict separation of morphological and
syntactic analysis in the pipeline model. We adopt the integer linear programming
(henceforth ILP) approach by Martins, Smith, and Xing (2009), which we augment
with a set of linguistically motivated constraints modeling the morpho-syntactic depen-
dencies in the languages. Case is herein interpreted as an underspecified filtering device
that guides a statistical model by restricting the search space of the parser. Due to the
constraints, the output of the ILP parser is guaranteed to obey all syntactic restrictions
that are marked overtly in the morphological form of the words. Although the restric-
tions are implemented as symbolic constraints, they are applied to the parser during
the search for the best tree, which is driven by a statistical model. We show in a second
experiment that restricting the search space in this way improves the performance on
argument functions (indicated by case morphology) considerably on all three languages
while the performance on all other functions stays stable.
We proceed by first discussing the role of case morphology in syntax (Section 2),
followed by a presentation of the parsing architecture of the Bohnet parser with a
discussion of the relevant aspects for our first experiment (Section 3). Next, we compare
the morphological annotation quality of automatic tools with the gold standard across
languages (Section 4). We then turn to the first experiment in this article where we
examine the performance of the parser with respect to core argument functions on
the three languages (Section 5). In the second experiment (Section 6), we apply an
ILP parser to the data sets augmented with a set of linguistic constraints that integrate
morphological information in an underspecified way into the parsing architecture. We
conclude in Section 7.
2. Challenges of Parsing Morphologically Rich Languages
A characteristic property of most languages commonly referred to as morphologically rich
is that they use morphological means at the word level to encode grammatical relations
within the sentence rather than using the phrase-structural configuration. Whereas in
English or Chinese, placement of a word (or phrase) in a particular position relative
to the verbal head marks its function (e. g., as the subject or object), morphologically
rich languages encode grammatical relations largely by changing the morphological
form of the dependent word, the head word, or both. A correlated phenomenon is the
free word order for which many of these languages allow. Because information about
grammatical relations is marked on the words themselves, it stays available regardless
of their relative position, so word order can be used to mark other information such
as topic-focus structure. The richer the morphological system, the freer the word order
tends to be, or, as Bresnan (2001) puts it, morphology competes with syntax. We thus see that
typologically, morphological and syntactic systems are interdependent and influence
each other. Most languages are located somewhere along a continuum between purely
configurational and purely morphological marking.
In principle, data-driven parsing models with word form sensitive features have the
potential to not only pick up configurational patterns for grammatical relation marking,
but also systematic patterns in the observed variation and co-variation of morphological
word forms. It is, however, not only the interaction between syntax and morphol-
ogy that adds challenges?the marking patterns are also non-trivial to pick up from
surface data.
25
Computational Linguistics Volume 39, Number 1
One of the linguistic challenges is that there are different, overlapping regimes for
morphological marking. One can distinguish head-marking and dependent-marking of
a grammatical relation, depending on where the inflection occurs. In addition, Nichols
(1986, page 58) identifies four ways in which inflection markers may play a role in
signaling syntactic dependency:
Example 1
Hebrew, taken from Nichols (1986, page 58)
be?t
house-of
sefer
book
?school?, lit. ?book house?
First, the morphological marker simply registers the presence of a syntactic depen-
dency. In Example (1), the form of the word be?t signals the presence of a dependent,
without specifying the nature of the relation.
Second, the affix marks not only the presence but also the type of the dependency.
A typical example of the dependent-marking kind is nominal case: Accusative case
on a noun marks it not only as a dependent of a verb, but it also marks the type of
relation, namely, direct object. Verb agreement markers in Indo-European languages
are a head-marking kind of example: They indicate that a noun stands specifically
in the subject relation. Third, a morphological marker may, in addition, index certain
lexical or inflectional categories of the dependent on the head (or vice versa). Subject
agreement often indexes the dependent subject?s gender and number properties on
the head verb; attributive adjectives in Czech, for instance, agree with their noun
heads in case, number, and gender. Fourth, for some affixes, there is a paradigm for
indexing internal properties of the head on the head itself (e. g., tense or mood of
a verb) or properties of the dependent on the dependent (e. g., gender marking on
nouns).
An additional linguistic challenge in learning the patterns from data, which we will
discuss in detail in Section 2.2, comes from the fact that the inflectional paradigms may
contain syncretism. This may interfere with the learning of the previously discussed
patterns. Further challenges we do not address in this article include interactions be-
tween syntax and derivational morphology, which for some languages like Turkish and
Arabic can go along with segmentation issues.
The first Workshop on Statistical Parsing of Morphologically Rich Languages has
set the agenda for developing effective systems by identifying three main types of
technical challenges (Tsarfaty et al 2010, page 2), which we rephrase here from our
system perspective:
Architectural challenges. Should data-driven syntactic parsing be split into subtasks,
and how should they interact? Specifically, should morphological analysis (and likewise
tokenization, part-of-speech tagging, etc.) be performed in a separate (data-driven?)
module and how can error propagation through the pipeline be minimized? Can a joint
model be trained on data that captures two-way interactions between several levels
of representation? Should the same system modularization be used in training and
decoding, or can decoding combine locally trained models, taking into account more
global structural and representational constraints?
Representational challenges. At what technical level should morphological distinc-
tions be represented? Should they (or some of them) be included at the part-of-speech
(POS) level, or at a higher level in the structure? Can some type of representation help
26
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
avoid confusions due to syncretisms? What is the most effective set of dependency
labels for capturing morphological marking of grammatical relations?
Lexical challenges. How can lexical probabilities be estimated reliably? The main
problem for morphologically rich languages is the many different forms for one lexeme,
which is amplified by the often limited amount of training data. How can a parser
analyze unseen word forms and use the information profitably?
2.1 Previous Work and Our Approach
The first two types of technical challenges often go hand in hand, as a change in architec-
ture effectively means a change in the interface representations, and vice versa. Collins
et al (1999) reduce the tag set for the Czech treebank, which consists of a combination of
POS tags and a detailed morphological specification, in order to tackle data sparseness.
A combination of POS and case features turns out to be best for their parsing models.
In statistical constituent parsing, many investigations devise treebank transformations
that allow the parsing models to access morphological information higher in the tree
(Schiehlen 2004; Versley 2005; Versley and Rehbein 2009). These transformations apply
category splits by decorating category symbols with morphological information like
case. Whereas these approaches change traditional models to cope with morphological
information, others approach the problem by devising new models tailored to the
special requirements of morphologically rich languages. Tsarfaty and Sima?an (2008,
2010) introduce an additional layer into the parsing process that directly models the sub-
categorization of a non-terminal symbol without taking word order into consideration.
The parser thus separates the functional subcategorization of a word from its surface
realization, which is not a one-to-one relation in morphologically rich languages with
free word order. In statistical dependency parsing, morphological information is mostly
used as features in the statistical classifier that guides the search for the most probable
tree (Bohnet 2009; Goldberg and Elhadad 2010; Marton, Habash, and Rambow 2010).
The standard way established in the CoNLL Shared Tasks (Nivre et al 2007a; Hajic? et al
2009) is a pipeline approach where POS and morphological information is predicted as
a preprocessing step to the actual parsing. Although Goldberg and Elhadad (2010) and
Marton, Habash, and Rambow (2010) find improvements for hand-annotated (gold)
morphological features, automatically predicted morphological information has none
or even negative effects on their parsing models. Goldberg and Elhadad (2010) also
show that linguistically grounded, carefully designed features (here agreement be-
tween adjectives and nouns in Hebrew) can contribute a considerable improvement,
however. Finally, the pipeline approach itself can be questioned. Cohen and Smith
(2007), Goldberg and Tsarfaty (2008), and Lee, Naradowsky, and Smith (2011) present
joint models where the processes of predicting morphological information and syn-
tactic information are performed at the same time. All three approaches acknowl-
edge the fact that syntax and morphology are heavily intertwined and interact with
each other.
Our attempt at tackling the technical and linguistic challenges can be characterized
as follows: In Section 6, we propose a system architecture that at the basic level follows
a pipeline approach, where local data-driven models are used to predict the highest
scoring output in each step. But this pipeline is complemented with a knowledge-
based component modeling grammatical knowledge about inflectional paradigms and
morphological marking of grammatical relations. Both parts are combined using a set
of global constraints that model the language-specific morpho-syntactic dependencies
to which a syntactic structure in that language has to adhere. These constraints are
27
Computational Linguistics Volume 39, Number 1
used to weed out linguistically implausible structures among the candidate outputs
of the parser. Our architecture thus resides between a strict pipeline approach where
no step can influence previous results, and a full joint model, where several subtasks
are predicted simultaneously. Using the global constraints we can precisely define the
parts of the structure where an interaction between the morphology and the syntax is
allowed to take place.
The key design tasks are of a representational nature: What are the linguistic units
for which hard constraints can (and should) be enforced in a language? (For example,
within Czech and German nominal phrases, indexing of case, number, and gender
follows a strict regime?the values have to co-vary.) What underspecified interface
representation is appropriate to negotiate between the potentially ambiguous output
of one local component and the assumed input of another component? How can we
restrict them as much as possible without sacrificing the correct solution? As it turns out,
the explicit enforcement of conservative linguistic constraints over morphological and
syntactic structures in decoding leads to significantly improved parsing performance
on case-bearing dependents, and also to improved overall performance over a state-
of-the-art data-driven pipeline approach.
2.2 Case Between Morphology and Syntax
In this article, we concentrate on the case feature, which resides at the interface be-
tween morphology and syntax. The case feature overtly marks (when unambiguous)
the syntactic function of a nominal element in a language. Languages show different
sophistication in their case systems. Where German has four different case values,
Hungarian uses a complex system of about 20 different values. In all languages with
a case system, it is used to distinguish and mark the function of the different arguments
of verbs (Blake 2001). Correctly recognizing the argument structure of verbs is one of the
most important tasks in automatic syntactic analysis because verbs and their arguments
encode the core meaning of a sentence and are therefore essential to every subsequent
semantic analysis step.
The three languages investigated in this article, German, Czech, and Hungarian,
belong to the broad category of morphologically rich languages. Syntactically, they all
use a case system to mark the function of the arguments of a verb (and a preposi-
tion). The morphological realization of these case systems show important differences,
however, which have a direct influence on syntactic analysis. Czech and German are
both Indo-European languages, Czech from the Slavonic branch and German from
the Germanic branch. Hungarian, on the other hand, is a Finno-Ugric language of
the Ugric branch. Czech and German both are fusional languages, where nominal
inflection suffixes signal gender, number, and case values simultaneously. Hungar-
ian is an agglutinating language, namely, every morphological feature is signaled
by its own morpheme, which is appended to the word. Whereas Hungarian has a
mostly unambiguous case system, Czech and (more so) German show a considerable
amount of syncretism in their nominal inflection. It is this syncretism that makes it so
much harder for a statistical system to learn the morphological marking patterns of a
language.
Table 1 shows examples of declension paradigms for the fusional languages Czech
and German. Note that these are only examples and cannot represent the entire com-
plexity of the systems. We use them here to exemplify the widespread morphological
syncretism in these two languages. In the masculine animate noun of Czech bratr
(?brother?), ACC/GEN SG, DAT/LOC SG, and ACC/INS PL use the same word forms
28
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
Table 1
Examples of nominal declension paradigms in Czech and German. German never distinguishes
gender in plural.
Czech, masc. animate noun brother Czech, neuter noun city
MASC ANI SG PL NEUT SG PL
NOM bratr bratr?i NOM me?sto me?sta
ACC bratra bratry ACC me?sto me?sta
DAT bratrovi/u bratru?m DAT me?stu me?stu?m
GEN bratra bratru? GEN me?sta me?st
VOC bratr?e ? VOC me?sto ?
LOC bratrovi/u bratrech LOC me?ste?/u me?stech
INS bratrem bratry INS me?stem me?sty
German, definite determiner German, masculine noun German, feminine noun
the dog woman
MASC NEUT FEM PL MASC PL FEM PL
NOM der das die die NOM Hund Hunde NOM Frau Frauen
ACC den das die die ACC Hund Hunde ACC Frau Frauen
DAT dem dem der den DAT Hund Hunden DAT Frau Frauen
GEN des des der der GEN Hundes Hunde GEN Frau Frauen
respectively.3 In the neuter noun me?sto (?city?) we find syncretism in NOM/ACC SG and
PL, and DAT/LOC SG. The NOM/ACC syncretism in neuter nouns is a typical property
of Indo-European languages (Blake 2001). Note also that some inflection morphemes
fill different paradigm cells, for instance bratra is ACC SG, me?sta is NOM PL. To resolve
the ambiguity, gender and number features need to be considered.
Unlike Czech, German has determiners, which are also marked for case and agree
with their head noun in the so-called phi-features (gender, number, case). The declen-
sion patterns of determiners and nouns in German have developed in different ways,
leading to highly case-ambiguous forms for nouns. We see in Table 1 two German
nouns, a masculine one and a feminine one. Although the declension paradigm of the
masculine noun has kept some residual formal marking of case in the GEN SG and the
DAT PL, the declension pattern of the feminine noun does not show case distinction at
all. Both nouns, however, mark the number feature overtly. The paradigm of the de-
terminer is much less ambiguous in the case dimension, but shows syncretism between
different number and gender features. Eisenberg (2006) calls the distribution of different
kinds of syncretism over different parts of the German noun phrase Funktionsteilung
(?function sharing?). It makes the morphological agreement between German nouns
and their dependents extremely important because only by agreement can a mutual
disambiguation take place and reduce the morpho-syntactic ambiguity for the noun
phrase. We will show that for the fusional languages Czech and German, automatic
morphological analyzers have problems predicting the correct case, number, and gen-
der values, whereas for the agglutinating language Hungarian, the unambiguous case
paradigm makes case prediction extremely easy.
3 NOM: nominative, GEN: genitive, DAT: dative, ACC: accusative, LOC: locative, INS: instrumental, SG:
singular, PL: plural, M/MASC: masculine, F/FEM: feminine, N/NEUT: neuter.
29
Computational Linguistics Volume 39, Number 1
?
? ?
?
? ?
Jako
?
as
pr?edkapela
nom
support band
se
acc
themselves
pr?edstav??
?
present
kapela
nom
band
Ambivalency
?
?
AuxY
Obj4 Sb
Atr
Atv
The band Ambivalency performs as support band
Figure 1
A dependency tree from the Czech treebank. Sentence no. 3,159 in the CoNLL 2009 data set.
In order to see the influence of morphology on today?s data-driven systems for
syntactic analysis, we investigate the performance of a state-of-the-art dependency
parser (Bohnet 2009, 2010) on the three languages just described paying special attention
to the handling of the core grammatical functions (i. e., the argument functions of verbs).
Dependency syntax (Hudson 1984; Mel?c?uk 1988) models the syntactic structure of a
sentence by directed labeled links between the words (tokens) of a sentence. Figure 1
shows an example tree for a Czech sentence. Every word of the tree is attached to exactly
one other word (its head) by a labeled arc whose label specifies the nature of the relation.
For instance, kapela is labeled as subject (Sb) of the sentence. Morphologically, the subject
is marked with nominative case (nom) whereas the direct object (Obj4) is marked with
accusative case (acc). We see that the object can precede the verb. Syntactically, Czech
allows for all permutations of subject, object, and verb (Janda and Townsend 2000,
page 86). It is thus a free word order language. Another property of free word order
languages is the higher amount of non-projective structures (compared with English).
Non-projective structures are indicated by crossing branches in the tree structure, as
between kapela and pr?edkapela in Figure 1.
3. Parsing Architecture
In this section, we give a brief description of the parser that we use in the first exper-
iment, where we analyze the performance of the parser with respect to morphological
information. The parser is the state-of-the-art data-driven second-order graph-based
dependency parser presented in Bohnet (2010).4 It is an improved version of the parser
described in Bohnet (2009), which ranked first for German and second for Czech for
syntactic labeled attachment score in the CoNLL 2009 Shared Task (Hajic? et al 2009).
The parser follows the standard pipeline approach. Information about lemma,
POS, and morphology is automatically predicted and fully disambiguated prior to
the parsing step. The CoNLL 2009 Shared Task used a tabbed format where every
token in a sentence is represented by a line of tabulator-separated fields holding
gold standard and predicted information about word position, word form, lemma,
POS, morphology, attachment, and function label. Figure 2 gives an example for the
word se in the sentence in Figure 1. Note that for every type of information, the
4 http://code.google.com/p/mate-tools, version: anna-2.
30
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
3 se se se P P SubPOS=7|Num=X|Cas=4 SubPOS=7|Num=X|Cas=4 4 4 Obj4 Obj4
Figure 2
Example of the CoNLL 2009 dependency format for se. Columns are from left to right:
Position, word form, gold lemma, predicted lemma, gold POS, predicted POS, gold
morphology, predicted morphology, gold head position, predicted head position, gold
function label, predicted function label. Semantic information is not displayed. The gold
standard columns are used for evaluation purposes.
human-annotated gold standard and the predicted value by an automatic tool is rep-
resented. The morphology columns contain several morphological features separated
by a vertical bar.
The parser itself consists of two main modules, the decoder and the feature model.
It is a maximum-spanning-tree5 parser (McDonald et al 2005; McDonald and Pereira
2006) that searches for the best-scoring tree using a chart-based dynamic programming
approach similar to the one proposed by Eisner (1997). The substructures are scored by
a statistical feature model that has been trained on treebank data; the best-scoring tree is
the tree with the highest sum over the scores of all substructures in the tree. The actual
implementation is derived from the decoder by Carreras (2007), which was shown to be
efficient even for very rich feature models (Carreras 2007; Johansson and Nugues 2008;
Bohnet 2009).
The features used in the statistical model are combinations of basic features, namely,
word form, lemma, POS, and morphological features. In addition, the distance between
two nodes, the direction of the edge, and the words between head and dependent are
included. Every feature is combined with the function label on the edge. A detailed
description of the feature model is beyond the scope of this article, but the interested
reader can find it in Bohnet (2009, 2010).
Because we are interested in the way the parser handles morphological information,
we will briefly discuss the inclusion of morphological features as described in Bohnet
(2009, page 3). The parser computes morphological features by combining the part-
of-speech tags (pos) of the head and the dependent with the cross-product of their
morphological feature values. For this, the morphological information (see Figure 2:
columns 7 and 8) is split at the vertical bar and every single morphological feature
value is treated as one morphological feature in the statistical model. The cross-product
then pairs the single feature values of dependent and head creating all combinations.
One single feature computed for the edge between an adjective and a noun in Czech
may then look like (A,N,acc,acc), which states the information that both words have
the accusative case. Other features are created as well, however, that might look like
(A,N,sg,masc), which states that the adjective has singular number and the noun has
masculine gender. So the algorithm does not pay attention to category classes. Further-
more, the whole cross-product is computed for every edge in the tree. All features are
additionally combined with the function label between the head and the dependent,
so in the parsing features, a morphological feature like case is directly combined with
the function label with which it appears together in the treebank. Because of this, the
parser should have direct access to the information about which case value signals
a particular grammatical function. Intuitively, the statistical model should learn that
certain dependent head configurations often occur with certain morphological feature
5 Or graph-based as opposed to transition-based (Nivre et al 2007b; Bohnet 2011).
31
Computational Linguistics Volume 39, Number 1
combinations. For example, a subject edge between a noun and a verb should very often
occur together with morphological features involving nominative case, and a dative
object edge should often occur with a dative feature.
The statistical model is a linear multi-class classifier, trained using an on-line learn-
ing procedure (MIRA [Crammer et al 2003] with a hash kernel [Bohnet 2010]). Learning
is an iterative process where the parser repeatedly tries to recreate the training corpus
sentence by sentence. If the parser makes no mistakes, it proceeds to the next training
instance. Otherwise, the feature weights for the tree that would have been correct and
the feature weights for the tree produced by the parser are compared and the weights
in the feature model are adjusted to favor the correct tree and disfavor the incorrect one.
The parser repeatedly parses the treebank, adjusting its feature model to produce trees
that match the trees in the training data. Because the decoder can only derive projective
trees (without crossing edges), the parser reattaches individual edges in the tree in
a post-processing step to allow for non-projective trees (crossing edges, see Figure 1)
using the algorithm in McDonald and Pereira (2006).
4. Data
Before we turn to our first experiment and its analysis, we briefly describe the data
sets that we used in the experiments and discuss the quality of the morphological
annotation. In a pipeline architecture, where morphological features are fully disam-
biguated prior to parsing, low quality in the predicted morphological information will
have considerable impact on the ability of the parser to learn the mapping between
case and grammatical functions that we want it to learn. Furthermore, the errors made
in the morphological preprocessing are the first observable difference between the two
fusional languages and the agglutinating language and directly reflect this typological
difference. We will thus show that whereas the morphological preprocessing for Czech
and German makes mistakes because of the syncretism in the morphological paradigms,
the morphological preprocessing for Hungarian suffers from a different problem.
All the data sets come from the newspaper domain. The Czech data set is the
CoNLL 2009 Shared Task data set consisting of 38,727 sentences from the Prague
Dependency Treebank (Bo?hmova? et al 2000; Hajic? et al 2006). The German data set
(Seeker and Kuhn 2012) is a semi-automatically corrected recreation of the data set that
was used in the CoNLL 2009 Shared Task (36,017 sentences). It uses the exact6 same raw
(surface) data but contains a different syntactic annotation. It was semi-automatically
derived from the original TIGER treebank (Brants et al 2002) and some time was spent
on manually correcting incorrect function labels and POS tags. The Hungarian data
consist of the general newspaper subcorpus (10,188 sentences) of the Szeged Treebank
(Csendes, Csirik, and Gyimo?thy 2004), which was converted from the original con-
stituent structure annotation to dependency annotation and manually checked by four
trained linguists (Vincze et al 2010). For the experiments in the following sections, we
use the training splits for Czech and German, and the whole set for Hungarian.
For the Czech and the Hungarian data, we kept the predicted information for
lemmata, POS, and morphology that was already provided with the data. For both
6 Except for three sentences that for some reason were missing in the 2006 version of the TiGer treebank,
from which this corpus was derived. The original data set in the CoNLL 2009 Shared Task was derived
from the 2005 version, which still contains these three sentences. The 2005 version also contained
spelling errors in the raw data that had been removed in the 2006 version. These errors were manually
reintroduced in order to recreate the data set as exact as possible.
32
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
languages, this information is predicted in a two-step process where a finite-state
analyzer produces a set of possible annotations for a given verb form, which is then
disambiguated by a statistical model trained on gold-standard data (for Czech, see
Spoustova? et al [2009]; for Hungarian, see Zsibrita, Vincze, and Farkas [2010]). The
German data set was cross-annotated by applying statistical tools7 trained on the gold
standard annotation. Contrary to the Czech and Hungarian data sets, lemma, POS,
and morphological information were annotated in three steps, each building upon the
preceding one.
In preparation for the experiments, we made two changes to the annotation in the
Czech and the Hungarian treebanks in order to allow for a more fine-grained analysis.
First, we copied the SubPOS feature value8 over to the respective POS column (gold
to gold, predicted to predicted). This helps us in doing a more fine-grained evaluation,
which is based on certain POS tags, but it also allows us to formulate linguistic con-
straints in the ILP parser more precisely, as we will see in Section 6.1. The German POS
tag set is already rather specific. We also changed the object labels (Obj) in the Czech
data set by combining it with the case value in the gold standard morphology (creating
Obj1-7). This gives us a more fine-grained object distinction for our analysis and it also
separates the case-marked objects from the clausal objects, which do not have a case
feature and therefore keep the original Obj label.9
In order to learn the mapping between case and grammatical functions, the parser
relies on the automatically predicted morphological information in the data sets. When
the parser is trained on predicted morphology, in principle, it has the chance to adapt
to the errors of an automatic morphological analyzer. We will see in Section 5, however,
that this does not seem to happen very often. Therefore, if we want the parser to perform
well, we need to predict morphological information with high quality. Table 2 shows
the prediction quality of the automatic morphological analyzers in the three data sets.
On the left-hand side, precision and recall are shown for the phi-features for the whole
data set; on the right-hand side, only those words were evaluated where the predicted
POS tag matched the gold standard one. We see that Czech and Hungarian achieve
high scores on all three features, with Czech achieving over 95% for each feature, and
Hungarian over 94% recall and almost 98% precision. In contrast, we find a rather
mediocre annotation in the German data set, where only the number feature can be
predicted with comparable quality,10 and gender and case prediction is rather bad. To a
certain extent, the lower performance for German compared to Czech can be explained
by the more informed annotation tool for Czech. The German data set was annotated
by purely statistical tools whereas the Czech annotation tool uses a finite-state lexicon
to support the statistical disambiguator.
Hungarian shows a big gap between precision and recall (97.83% and 94.11% for
case) when evaluating all words, but the performance on the words with the correct
POS tag is almost perfect (99.22% for case!). The reason lies in the POS recognition.
The Hungarian POS tag set uses a category X as a kind of a catch-all category where
annotators would put tokens they could not assign anywhere else. The precision for this
class is below 10%, because the tool is classifying a considerable amount of proper nouns
(Np) as X. The class X, however, does not get a morphological specification so that about
7 Mate-tools by Bernd Bohnet: http://code.google.com/p/mate-tools.
8 The SubPOS feature distinguishes subcategories inside the main POS categories and is part of the
morphological description (see Figure 2).
9 Prepositional objects headed by prepositions (pos: RR, RF, RV) were also excluded.
10 There are only two values to predict though.
33
Computational Linguistics Volume 39, Number 1
Table 2
Annotation quality of the phi-features (case, gender, and number) for all words and for those
words with a correctly predicted POS tag.
all correct POS
precision recall precision recall
Czech case 95.73 95.63 96.06 96.06
gender 97.59 97.45 98.03 98.03
number 98.18 98.08 98.47 98.47
German case 88.69 88.51 89.26 89.06
gender 90.16 89.99 90.95 90.74
number 96.18 95.63 96.92 96.61
Hungarian case 97.83 94.11 99.22 99.22
number 98.64 95.91 99.88 99.88
3,500 out of 12,500 proper nouns do not receive a case and a number value at all. The
reason for the poor morphological annotation in Hungarian is apparently not a problem
of an ambiguous morphology, it is simply a problem of the POS recognition. We already
know that Hungarian is an agglutinating language. The case paradigm of Hungarian,
although comprising about 20 different case values, does not show syncretic forms with
the exception of a regular genitive-dative syncretism. Whereas in Hungarian, getting
the POS correct effectively means getting case and number correct, the results in Table 2
for Czech and German11 are not much better for words with correctly predicted POS
tags than for all words. In Czech and German, this is a problem of the syncretism in the
morphological paradigms.
The low syncretism in the Hungarian case paradigm is due to the agglutinating
nature of its morphological system. Because every feature (e.g., case) is signaled by
its own morpheme, a syncretism in the system would erase the distinction between
the syncretic forms. Because Hungarian uses the same case paradigm for all words,
a regular syncretism would mean that a certain distinction can no longer be made in
the language.12 In fusional languages, an inflection morpheme signals more than one
feature value. Many syncretisms can thus be disambiguated by the other feature values
or by agreement with dependents, as is done in the German noun phrase. We learn
two things from these findings: First, we may need different approaches for handling
morphology in fusional languages like Czech and German than we do for agglutinating
languages like Hungarian. And second, the category morphologically rich encompasses
11 The fact that for German, precision and recall differ is due to the independency of the POS tagger
and the morphological analyzer. In the German data, the morphological analyzer is not bound
to a certain feature template determined by the POS of the word, so that, in principle, it can
assign case to verbs and tense to nouns. This is not the case for the Czech and Hungarian analyzers.
Precision, recall, and F-score measured over all possible values amount to simple accuracy in those
languages.
12 One of the reviewers pointed out to us that Turkish as an agglutinating language also shows much
morphological ambiguity. That is correct and this also holds for Hungarian. The case paradigm itself
seems to have no syncretism in Turkish, however. The ambiguity rather comes from interaction with
vowel harmony and definiteness marking. The syncretism between genitive and dative case in the
Hungarian case system is more of a puzzle. Our best guess is that the distribution of these cases is
so different that the context can disambiguate them relatively easily.
34
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
languages that are not only different from English but also show important differences
among each other that we should take into account when devising parsing technology.
5. Experiment 1
Having examined the quality of the predicted morphological information in the data
sets, we can now investigate how the parser deals with this information. We proceed
as follows: We train three different models for each language, one using gold standard
morphology, one using predicted morphology, and one using no morphological infor-
mation (henceforth GOLD-M, PRED-M, and NO-M). Comparing the performance of these
three models allows us to see the effect that the morphological information has on the
parsing performance. The model using gold morphology serves as an upper bound
where we can observe the behavior of the parser when it is not disturbed by errors
coming from the automatic morphological analyzers. Note that this model is very unre-
alistic in the sense that syncretisms are fully resolved in the morphological information.
The model using predicted morphology serves as a realistic scenario where we can
observe the problems introduced by imperfect preprocessing and propagated errors
in the pipeline (e.g., due to syncretism). And finally, the model using no morphology
shows us how much non-morphological information contributes to the parsing perfor-
mance. In comparison with the other two models, we can then see the contribution of
morphological information13 to the parsing process. All models use the same predicted
lemma and POS information as discussed in the previous section.
5.1 Experimental Set-up
We performed a five-fold cross annotation14 on the training portions of the data sets of
Czech and German, and on the whole subcorpus of Hungarian, varying the morpho-
logical annotation as described. The overall parsing performance is shown in Table 3,
where the German and the Hungarian scores exclude punctuation and the Czech scores
include them.15
Table 3 gives us the usual picture that has been noticed in several shared tasks
on dependency parsing for multiple languages (e.g., CoNLL-ST 2006, 2007, 2009). The
performance on German is pretty high, although not as high as it would be for English,
and the performance on Czech is rather low. Note the extreme divergence between
labeled (LAS) and unlabeled attachment score (UAS) for Czech.16 For Hungarian, the
performance is comparable to Czech in terms of UAS but the LAS for Hungarian is
better. We also see the expected ordering in performance for the models using dif-
ferent kinds of morphological information. The gold models always outperform the
models using predicted morphology, which in turn outperform the models using no
morphological information. Note, however, that whereas the performance on German
does not degrade very much when using no morphological information, it is very
13 It should be noted that by morphological information we always mean the complete annotation available
in the treebanks. Although we concentrate in the analysis on the phi-features (gender, number, case), the
models using morphological information always use the whole set, including also, for example, verbal
morphology.
14 The number of iterations during training was set to 10.
15 Punctuation in the Czech data set is sometimes used as the head in coordination.
16 This is due to the way the Czech data label certain phenomena, which makes it difficult for the parser to
decide on the correct label. See Boyd, Dickinson, and Meurers (2008, pages 8?9) for examples.
35
Computational Linguistics Volume 39, Number 1
Table 3
Overall performance of the Bohnet parser on the five-fold cross annotation for every language
and different kind of morphological annotation. All results in percent. LAS = labeled attachment
score; UAS = unlabeled attachment score. Results for German and Hungarian are without
punctuation. Best score for Czech on the CoNLL 2009 Shared Task was by Gesmundo et al
(2009), best score for German was by Bohnet (2009), best score for Hungarian on the CoNLL
2007 Shared Task was by Nivre et al (2007a). Best CoNLL 09/07 results were obtained on
different data sets.
Czech German Hungarian
LAS UAS LAS UAS LAS UAS
GOLD-M 82.49 88.61 91.26 93.20 86.70 89.70
PRED-M 81.41 88.13 89.61 92.18 84.33 88.02
NO-M 79.00 86.89 89.18 91.97 78.04 86.02
best on CoNLL 09/07 80.38 ? 87.48 ? 80.27 83.55
harmful for Hungarian to do so (78.04% LAS for NO-M in comparison with 84.33%
LAS for PRED-M). The Czech results lie in between. To give a general impression of
the performance of the parser, the last row shows parsing results for the three languages
reported in the literature. The results have been obtained on different data sets, however,
so a direct comparison would be invalid.
5.2 Analysis
Although the scores in Table 3 reflect the quality of the parser on the complete test
data, we would not expect case morphology to influence all of the functions. We will
therefore go into more detail and concentrate on nominal elements (nouns, pronouns,
adjectives, etc.)17 and core grammatical functions (subjects, objects, nominal predicates,
etc.) because in our three languages, nominal elements carry case morphology to mark
their syntactic function. Core grammatical functions are vital to the interpretation of
a sentence because they mark the participants of a situation. We exclude clausal and
prepositional arguments, which can fill the argument slot of a verb but would not
be marked by case morphology. Table 4 shows the encoding of the core grammatical
functions in the three treebanks.
Table 5 shows the performance of the parsing models for each of the three languages
on the core grammatical functions. As described in Section 4, we split the object function
for Czech according to its associated case value. The results are shown for each of the
three models with GOLD-M on the left, PRED-M in the middle, and NO-M on the right.
The results shown for the NO-M models indicate again that morphology plays a
bigger role in Czech and Hungarian for determining the core grammatical functions
than it does for German. The performance on all grammatical functions except the
rather rare genitive object is generally higher for German, showing that to a large
17 We determine a nominal element by its gold standard POS tag:
Czech: AA, AG, AM, AU, C?, Ca, Cd, Ch, Cl, Cn, Cr, Cw, Cy, NN, P1, P4, P5, P6, P7, P8, P9, PD, PE, PH,
PJ, PK, PL, PP, PQ, PS, PW, PZ.
German: ADJA, ART, NE, NN, PDAT, PDS, PIAT, PIS, PPER, PPOSAT, PPOSS, PRELAT, PRELS, PRF,
PWS, PWAT.
Hungarian: Oe, Oi, Md, Py, Oh, Ps, On, Px, Pq, Mf, Pp, Pg, Mo, Pi, Pr, Pd, Mc, Np, Af, Nc.
36
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
Table 4
Core argument functions and their encoding in the different treebanks. The different object
labels for Czech have been introduced by us. The original function is simply Obj.
PDT 2 (Czech) TiGer (German) HunDep (Hungarian)
subject Sb SB SUBJ
nominal predicate Pnom PD PRED
object Obj1-7 OA, OA2, DA, OG OBJ, DAT
Table 5
Precision, recall, and F-score (LAS) for core grammatical functions marked by case. We omit
locative objects in Czech, and second accusative objects in German, due to their low frequency.
GOLD-M PRED-M NO-M
Czech freq prec rec f prec rec f prec rec f
subject 38,742 89.29 91.18 90.22 83.96 87.01 85.46 74.10 78.82 76.39
obj (acc) 21,137 92.50 93.35 92.93 85.25 83.01 84.12 73.42 72.02 72.71
predicate 6,478 89.07 87.14 88.09 88.24 86.00 87.11 82.34 78.19 80.21
obj (dat) 3,896 83.18 85.68 84.41 80.21 78.88 79.54 74.29 48.05 58.35
obj (instr) 1,579 71.38 66.50 68.85 67.74 62.51 65.02 58.93 35.53 44.33
obj (gen) 1,053 86.69 77.30 81.73 80.42 62.39 70.26 74.60 48.81 59.01
obj (nom) 167 57.63 40.72 47.72 56.97 29.34 38.74 48.67 32.93 39.29
GOLD-M PRED-M NO-M
German freq prec rec f prec rec f prec rec f
subject 45,670 95.11 96.05 95.58 89.95 91.23 90.59 88.32 89.86 89.08
obj (acc) 23,830 93.93 94.80 94.36 84.83 84.89 84.86 82.20 83.35 82.77
obj (dat) 3,864 89.56 87.73 88.64 79.17 64.44 71.05 77.09 50.78 61.23
predicate 2,732 78.07 73.35 75.64 75.80 72.91 74.33 76.20 71.01 73.51
obj (gen) 155 80.25 41.93 55.08 60.66 23.87 34.26 52.94 17.42 26.21
GOLD-M PRED-M NO-M
Hungarian freq prec rec f prec rec f prec rec f
subject 11,816 88.34 91.57 89.93 84.96 88.15 86.53 64.58 66.44 65.50
obj (acc) 9,326 93.63 94.22 93.92 92.36 92.70 92.53 66.23 63.86 65.03
obj (dat) 1,254 80.55 76.95 78.71 75.57 71.53 73.49 58.36 30.62 40.17
predicate 941 81.05 75.45 78.15 77.39 72.37 74.79 72.49 71.41 71.95
extent the parser is able to use information from lexicalization and configurational
information (Seeker and Kuhn 2011). Results for Czech and Hungarian are lower in
the NO-M models. They improve by large margins when switching to predicted mor-
phology. Czech accusative objects improve from 72.71% F-score to 84.12% F-score in
the PRED-M model. In Hungarian, the F-scores for dative objects improve by over 33
percentage points to 73.49% F-score when switching to the PRED-M model. In contrast,
although all the scores improve for German, improvements are generally low when
switching from the NO-M to the PRED-M model. The biggest improvement happens
37
Computational Linguistics Volume 39, Number 1
for dative objects, which increase by about 10 percentage points, but for subjects, the
improvement is just over one percentage point. This is in line with the general idea that
German is a borderline case between morphologically poor configurational languages
like English and morphologically rich non-configurational languages like Czech or
Hungarian. We already saw this general trend in Table 3, but the effect is much larger
if we consider those functions that are directly marked by morphological means in
the language.
If we now turn to the GOLD-M models, we see that in general, German and Czech
benefit more from the gold standard morphological annotation than Hungarian. Know-
ing that Hungarian does not have much form syncretism in its inflectional paradigms,
this is not really surprising. There is, however, still a gain of information because
the effect of the wrong POS tags in Hungarian is eliminated in the GOLD-M model.
An effect that comes out very clearly is the improvement for subjects and accusative
objects for Czech and German when moving from predicted to gold morphology,
because the typical syncretism between nominative and accusative in the neuter gen-
der in Indo-European languages (cf. Table 1) is correctly disambiguated: Comparing
the performance on subjects (marked by nominative case) and accusative objects, we
see a considerable improvement between 5 percentage points for Czech subjects and
almost 10 percentage points for German accusative objects when switching to gold
morphology. This improvement does not happen for Hungarian, where there is no such
syncretism. The gold morphology acts as an oracle here and circumvents the ambiguity
problem that a pipeline approach to predicting morphological information prior to
parsing has.
Another interesting observation related to the way the parser works is that for all
languages, predictions are less accurate for the less frequent functions. The general
order for all three languages from most frequent to least frequent is subjects > accusative
objects > predicates/dative objects > instrumental/genitive objects. For all languages, the
parser?s quality of annotation follows this ordering. This effect comes from the statistical
nature of the parsing system, which will in case of doubt resort to the more frequent
function. A clear sign is that for rare objects, the precision is always higher than the
recall. As an example, notice the performance of the parsing models on dative and
genitive objects. The parser annotates genitive objects if it has strong evidence, hence
the high precision, but it frequently fails to find it in the first place, hence the low recall.
Because the NO-M models do not have morphological information, they can only rely
on lexicalization and contextual information to determine the correct grammatical func-
tion. We can see this ranking in all the models regardless of the amount of morphological
information available, although the differences are much smaller for the more informed
models.
Finally, we see that the benefit from morphological information is comparatively
low for nominal predicates. It seems that the non-morphological context already pro-
vides much useful information (e.g., the copular verbs).
5.3 Analysis of Confusion Errors
We now ask ourselves if the parser utilizes the morphological information, in our case
the case morphology, correctly. In principle, there are two possible scenarios: (1) the
feature model of the parser does not integrate the morphological annotation in a useful
way, so that the parser has difficulties learning the association between case values and
the grammatical functions; (2) There is nothing wrong with the feature model, but the
38
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
morphological annotation is not good enough and causes problems because the parser
gets incorrect information in the features.
To answer this question, we examine the confusion errors made by the parser.
If the parser uses morphological information correctly, we expect it to confuse labels
that can all be signaled by the same case value. For example, if the parser learns the
association between nominative and subject/predicate properly, we would still expect
it to make errors in confusing these two functions. Because the mapping between case
and grammatical function is one-to-many, knowing the case value reduces the number
of possible functions but the final decision between these functions must be made by
non-morphological information. The effect should be strongest in the GOLD-M models
because the morphological information is correctly disambiguated. Consequently, we
expect the same results for the PRED-M models blurred by additional errors introduced
by an imperfect morphological prediction. If, however, the parser does not learn the
mapping or has no access to morphological information, we expect confusion errors all
across the case paradigms.
To start with the last hypothesis, we examine the confusion errors with subjects
made by the parser using the NO-M models. Subjects are marked by nominative case in
all three languages, with Czech allowing for dative and genitive subjects under special
circumstances. The NO-M models do not have access to morphological information
and should therefore mix up functions regardless of the case value that would usually
distinguish them. Table 6 shows the top five confusion errors made by the NO-M models
on the subject function. The values are split for correct and incorrect head selection to
tease apart simple label classification errors from errors involving label classification
and attachment.
Table 6
Top five functions with which subjects were confused when parsing with the NO-M models.
M marks a coordinated function in Czech.
Czech German
NO-M correct head wrong head NO-M correct head wrong head
rank label freq label freq rank label freq label freq
1 Obj4 4,996 Atr 2,644 1 OA 2,680 OA 1,498
2 Pnom 1,261 Obj4 981 2 PD 776 NK 906
3 Adv 811 Sb M 948 3 DA 458 DA 431
4 Obj3 752 Adv 273 4 EP 301 AG 313
5 Obj7 380 Obj M 245 5 MO 219 CJ 296
Hungarian
NO-M correct head wrong head
rank label freq label freq
1 OBL 3,029 ATT 1,116
2 OBJ 1,505 Exd 574
3 PRED 250 COORD 313
4 ATT 185 OBL 311
5 DAT 152 OBJ 139
39
Computational Linguistics Volume 39, Number 1
The results in Table 6 confirm the expectation that confusion errors appear regard-
less of the case value involved, which is no surprise given that the models do not have
access to morphological information: For Czech, when the head was chosen correctly,
Obj4, Obj3, and Obj7 (accusative, dative, and instrumental objects, respectively) are all
signaled by a different case value and their confusion rates follow their frequency in
the data. Pnom (nominal predicates) are expected because they are also signaled by
nominative case as are subjects. If the head was chosen incorrectly, the parser assigns
Obj4 and coordinated subjects and objects (Sb M, Obj M). Adverbial (Adv) and attribu-
tive functions (Atr) are expected as they mark adjunct functions that can be filled by
nominal elements. For German, we see confusions with the object functions (accusative
OA and dative objects DA), predicates (PD), and the EP function marking expletive
pronouns in subject position. Both are marked by nominative case. Furthermore, the
parser makes confusion errors with MO, NK, and AG, which are the three adjunct
functions that can be filled by nominal elements (e.g., AG marks genitive adjuncts).
CJ finally marks coordinated elements, which is an expected error if the head was
chosen incorrectly, but, unlike in the Czech treebank, we cannot tell by the coordination
label the particular function the element would have if it were not coordinated. In
Hungarian, we also have errors across the board, with argument functions not marked
by nominative case (accusative objects OBJ, dative objects DAT), the predicate function
PRED, and all types of adjuncts (ATT [attributives] and OBL [obliques]). Obliques are
especially interesting in Hungarian because the language has only a small number of
prepositions. Most oblique adjunct functions are realized by a particular case (hence
the about 20 different case values), which for a parsing model using no morphologi-
cal information makes it rather difficult to distinguish them from the core argument
functions. In summary, we find the expected picture of confusion errors across the case
paradigms.
Turning now to the GOLD-M models, we can test whether the parser is able to
learn the mapping between case and its associated functions. If so, we expect confusion
errors with functions that are all compatible with the case value of the correct function.
Table 7 shows the top five confusion errors that the GOLD-M models made on the subject
function. Here, we see a completely different picture compared with the NO-M model
errors in Table 6. In all three languages, we find?regardless if the head is correct or
not?confusions only with functions that are compatible with the nominative case. In
Czech, subjects are mostly confused with predicates (Pnom) and coordinated subjects
(Sb M). ExD marks suspended nodes moved because of an elliptical constructions. The
label does not tell whether the node would be a subject with regard to the empty
node but it may be, so it is compatible with nominative case. Atr between nominal
elements may mark close appositions like the one in Figure 1, which would be marked
as nominative by default. ObjX marks objects with no annotated case value (mostly for
foreign words). Of all the functions, only Obj4 cannot be signaled by nominative case.
If one checks those 69 cases, only 22 are annotated with accusative case in the gold
standard, the rest consist mostly of various, high-frequent numerals in neuter gender
and quantifiers, most of which are ambiguous between nominative and accusative. In
these cases, lexicalization seems to overrule the case feature. We get the same picture
for German and Hungarian, both models making errors that are compatible with the
nominative case value. Of the 112 errors with accusative objects (OA) in German, only 36
have the correct case value in the gold standard. Unlike in the Czech and the Hungarian
treebank, the morphological annotation in TiGer contains a considerable number of
errors. We then conclude that for subjects, the parser indeed has no problem learning
that subjects are marked by nominative case.
40
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
Table 7
Top five functions with which subjects were confused when parsing with the GOLD-M models.
M marks a coordinated function in Czech.
Czech German
GOLD-M correct head wrong head GOLD-M correct head wrong head
rank label freq label freq rank label freq label freq
1 Pnom 583 Sb M 1,142 1 PD 773 NK 555
2 ObjX 102 Atr 711 2 EP 323 CJ 245
3 Adv 102 ExD M 162 3 MO 117 PNC 139
4 Obj4 69 ExD 145 4 OA 112 PD 129
5 ExD 45 Pnom 65 5 PH 96 APP 127
Hungarian
GOLD-M correct head wrong head
rank label freq label freq
1 PRED 264 ATT 678
2 Exd 102 Exd 494
3 OBL 94 COORD 249
4 ATT 90 NE 32
5 OBJ 50 DET 22
Next, we examine the accusative objects and compare the performance of the
GOLD-M models with their respective PRED-M counterparts to assess the effect of
predicted morphological information. Table 8 shows the confusion errors for the ac-
cusative objects. On the left, the GOLD-M errors are shown; on the right we see the
PRED-M errors. For the GOLD-M models, the picture is basically the same as with the
subjects, with the small exception that all three languages show confusion with subjects
under the top five.18 Although the effect is not strong, it shows that the statistical
model can sometimes overrule the morphological features even for the gold standard
morphology.
The most interesting effect, however, happens when switching to predicted mor-
phological information. The overall number of errors increases, but the biggest in-
crease occurs for subjects in German (SB) and in Czech (Sb), although the same is not
observable in Hungarian (SUBJ). Of the 2,945 confusion errors in Czech, where the
PRED-M model incorrectly predicts an accusative object, 891 have been classified as
accusative despite being nominative in the gold standard and 1,505 have been classified
as nominative although being accusative. If we check the gender of these instances, we
find the overwhelming majority to be neuter, feminine, or masculine inanimate, exactly
those genders whose inflection paradigms show syncretism between nominative and
accusative forms. We find the same effect in the German errors. The syncretism in
the two languages causes the automatic morphological analyzers to confuse these case
18 The AuxT label in the Czech errors is used to mark certain kinds of reflexive pronouns, which can be in
accusative or dative case. The criterion for deciding whether a reflexive pronoun is labeled AuxT or Obj4
(i.e., accusative object) is whether the governing verb denotes a conscious or unconscious action. This is a
very tough criterion to learn for a dependency parser. In any case, however, AuxT is perfectly compatible
with accusative case.
41
Computational Linguistics Volume 39, Number 1
Table 8
Top five functions with which accusative objects were confused when parsing with the gold
(left) and predicted (right) morphology models. M marks a coordinated function in Czech.
Czech
GOLD-M correct head wrong head PRED-M correct head wrong head
rank label freq label freq rank label freq label freq
1 Adv 274 Obj M 750 1 Sb 2,354 Atr 687
2 AuxT 270 Atr 172 2 Adv 262 Obj M 660
3 Sb 69 ExD M 67 3 AuxT 256 Sb 594
4 ExD 34 Adv 65 4 Obj3 137 Sb M 108
5 AuxR 28 Atv 53 5 Obj2 109 ExD M 94
German
GOLD-M correct head wrong head PRED-M correct head wrong head
rank label freq label freq rank label freq label freq
1 MO 283 NK 357 1 SB 2,176 SB 1,329
2 SB 112 CJ 191 2 DA 610 NK 606
3 DA 55 SB 121 3 MO 308 CJ 365
4 CJ 43 APP 97 4 CJ 46 AG 137
5 EP 25 MO 55 5 EP 40 APP 136
Hungarian
GOLD-M correct head wrong head PRED-M correct head wrong head
rank label freq label freq rank label freq label freq
1 OBL 90 COORD 119 1 OBL 119 COORD 140
2 ATT 60 Exd 81 2 SUBJ 86 Exd 111
3 SUBJ 50 ATT 44 3 ATT 65 ATT 78
4 Exd 23 OBL 18 4 Exd 19 ROOT 18
5 MODE 14 ROOT 13 5 MODE 16 OBL 15
values more often, which subsequently leads to errors in the parser due to the pipeline
architecture. That the parser so frequently falls for incorrect annotation is more proof
that it has learned the mapping between case and its associated grammatical functions.
As expected, we do not find this effect for Hungarian. As we discussed in Section 4, there
is almost no syncretism in the Hungarian case paradigm, which therefore does not lead
to this kind of error propagation. The slight increase in errors in Hungarian is instead
related to the POS errors and their influence on missing morphological information than
the quality of the predicted morphology itself.
For reasons of space and because it would not contribute anything new to the
picture, we will not go into detail for the errors for the remaining grammatical functions.
We conclude that learning the morphological dependencies that hold for a language
(cf. the four types by Nichols [1986]) can be facilitated by a statistical model. When
presented with gold standard morphological information, the parser performance im-
proves considerably over the model without morphological information for all three
42
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
languages. The error analysis shows that the parser learns the mapping between case
and grammatical function, which also shows that the feature model of the parser
integrates the information in a useful way. In the more realistic scenario using pre-
dicted morphology, however, the parser starts making more mistakes for Czech and
German that are caused by errors of the automatic morphological predictors, which
are propagated through the pipeline model. This effect does not occur for Hungarian.
The syncretism in the inflectional paradigms in Czech and German makes the task of
learning the morpho-syntactic rules of a language much more difficult for a statistical
parser in a pipeline architecture. With a high amount of syncretism, it is simply not
sensible to fully disambiguate certain morphological properties of a word (e.g., case)
without taking the syntactic context into account.
6. Case as a Filter
From Experiment 1 we learned that one of the problems when parsing morphologi-
cally rich languages like Czech and German is the propagation of annotation errors
in the processing pipeline and the unreliable morphological information. The problem
is that the parser learns a mapping between case values and grammatical functions
but the predicted morphology delivers the wrong case value. As a solution to this
problem, Lee, Naradowsky, and Smith (2011) have proposed a joint architecture where
the morphological information is predicted simultaneously with the syntactic structure,
so that both processes can inform and influence each other. This puts morphological
prediction and syntactic analysis on the same level. We choose a different approach
here: We keep the basic pipeline architecture, because it works very efficiently. We
support the parser, however, with constraints that model the possibly underspecified
morphological restrictions grounded in the surface forms of the words. Especially for
the core argument functions, a morphological feature like case first and foremost serves
as a morpho-syntactic means to support the syntactic analysis by overtly marking
syntactic relations and thus reducing the choice for the parser. For example, if a word
form morphologically cannot be accusative, the parser should not consider grammatical
functions that are signaled by accusative in the language. Case acts here as a filter on
the available functions for the morphologically marked element. Interpreting the role
of case as a filter, we can use the case feature as a formal device to restrict the search
space of the parser. This is different from the joint model, where morphology and syntax
are predicted at the same time, because the parser will not fully disambiguate a token
with respect to its morphology if the syntactic context does not provide the necessary
information. Another thing that we learned from the first experiment is that although
the predicted morphology is not completely reliable, it is still much better than using
none at all, especially for Czech and Hungarian (see difference between PRED-M and
NO-M models in Table 5). In the following, we will therefore still use the predicted
morphology as features in the statistical model in combination with the filter. In this
architecture, the parser gets statistical information from the feature model to prefer a
particular analysis, but the constraints will block this option if it does not comply with
the morphological specification of the words. The parser then needs to choose a different
option.
In order to implement the constrained parser, we use a parsing approach by
Martins, Smith, and Xing (2009) using integer linear programming. It is related to
the Bohnet parser in the sense that it is also a graph-based approach, but it allows
us to elegantly augment the basic decoder with linguistically motivated constraints
43
Computational Linguistics Volume 39, Number 1
(Klenner 2007; Seeker et al 2010). ILP is a mathematical tool for optimizing linear
functions and was first used in dependency parsing by Riedel and Clarke (2006), who
performed experiments on Dutch using linguistically motivated constraints as we will
do. Martins, Smith, and Xing improved the formulation considerably so that the parser
would output well-formed dependency trees without the need for iterative solving. In
our ILP parser, we use the formulation by Martins, Smith, and Xing extended to labeled
dependency parsing. Like the Bohnet parser, the ILP parser consists of a decoder and
a statistical feature model. Whereas the feature model remains basically the same, the
decoder is implemented using ILP. The formulation represents every possible arc that
might appear in the parse tree as a binary variable (arc indicator), where 1 signals
the presence of the arc in the tree, and 0 signals its absence (see also Figure 3). Each
such arc indicator variable is weighted by a score assigned by the statistical model
that is learned from a treebank. During decoding,19 the parser searches for the highest
scoring combination of arcs that also fulfills the global tree constraints as well as any
other global constraints that may be added to the equations to model, for instance,
linguistic knowledge. The tree constraints ensure that every word in the tree has exactly
one head and that there are no cycles in the tree. Martins, Smith, and Xing use the
single commodity flow formulation by Magnanti and Wolsey (1995) to enforce the tree
structure. The idea is that the root node sends N units of flow through the tree (with N
being the number of words in the sentence) and every node in the tree consumes one
unit. If every node consumes exactly one unit of flow and every node can have only one
parent node, then the tree must be connected and acyclic.
max
?
h?H
?
d?N
?
l?L
?ldha
l
dh (1)
?
h?H
?
l?L
aldh = 1 ?d ? N (2)
|N|
?
l?L
aldh ? fdh ?d ? N,?h ? H (3)
?
h?H
fdh ?
?
g?N
fgd = 1 ?d ? N (4)
?
d?N
fdRoot = |N| (5)
a ? {0, 1}, f ? Z (6)
Let N be the set of words in a sentence, H = N ? {Root} is the set of words plus an
artificial root node, and L is the set of function labels. For every sentence, Equations (1)?
(6) constitute the equation system that the constraint solver has to solve in order to find
the highest scoring dependency tree. Equation (1) shows the objective function, which
is simply the sum over all binary arc indicator variables a ? A = N ? H ? L weighted
by their respective score ?. Equation (2) restricts for every dependent d the number
of incoming arcs to exactly one. It thus makes sure that every word will end up with
19 We use the GUROBI constraint solver: www.gurobi.com, version 4.0.
44
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
d\h 0 1 2 3
1
2
3
a1,0
a2,0
a3,0
a1,1
a2,1
a3,1
a1,2
a2,2
a3,2
a1,3
a2,3
a3,3
d\h 0 1 2 3
1
2
3
f1,0
f2,0
f3,0
f1,1
f2,1
f3,1
f1,2
f2,2
f3,2
f1,3
f2,3
f3,3
arc indicators flow variables
Root John loves Mary
0 1 2 3
Head candidates
for John
a1,0 a1,2
a1,3
Equation (3): flow link (for each pair <a,f>)
= 1
Equation (2):
single head
= 1
Equation (4):
flow consumption
Equation (5): root flow
f1,0 + f2,0 + f3,0 = 3
Figure 3
Schematic description of the unlabeled first-order model for the example sentence John loves
Mary. The constraints are shown for the dependent (d) John. There are three head (h) candidates,
from which the decoder needs to choose one because of the single head constraint (Equation (2)).
Equations (3), (4), and (5) show as an example how the flow constraints are applied to ensure a
tree structure. Equation (3) links each arc indicator to one flow variable making sure that only
active arcs (those that are set to 1) carry flow > 0. Equation (5) sends three units of flow from the
root, one for each other token in the tree. Equation (4) finally forces the flow difference between
the incoming arc (horizontal part) of each node (except root) and the flow on all outgoing arcs
(vertical part) to be exactly 1, thus making sure that each node consumes one unit of flow. To find
the optimal tree, the sum over the weights of all arc indicators that are set to one is maximized.
exactly one head. Equations (3)?(5) model the single commodity flow. A set of integer
variables F = N ? H is introduced to represent the flow on each arc. Equation (3) links
every flow variable that represents the flow between two nodes to the set of arc indicator
variables that can connect these two nodes. If there is no arc between the two nodes (all
indicator variables are 0), the flow must be 0 as well. If one arc indicator is 1, then
the flow variable can take any integer value between 0 and |N|. Equation (4) enforces
the consumption of one unit of flow at each node by requiring the difference between
incoming and outgoing arcs to be exactly one. Equation (5) finally sets the amount of
flow that is sent by the artificial root node to the number of words in the sentence. Note
that this does not force the tree structure to be single-rooted, because the artificial root
node can have multiple dependents. It can be done by an additional constraint that sets
the number of dependents for the root node to one. Figure 3 shows an example for the
basic formulation.
Martins, Smith, and Xing (2009) propose several extensions to the basic model; for
example, second-order features, which introduce new variables for each combination
45
Computational Linguistics Volume 39, Number 1
of two arc indicator variables into the ILP model. For our parser, we implemented
the second-order features that they call all grandchildren and all siblings. They also state
that the use of second-order features in the decoder renders exact decoding intractable,
and they propose several techniques to reduce the complexity, which we also apply
to our parser: (1) Before parsing, the trees are pruned by choosing for each token the
ten most probable heads using a linear classifier that is not restricted by structural
requirements, and (2) The integer constraint is dropped, such that the variables can
now take values between 0 and 1 instead of either 0 or 1. The dropping of the integer
constraint can lead to inexact solutions with fractional values. To arrive at a well-formed
dependency tree, we then use the first-order model in Equations (1)?(6) to get the
maximum spanning tree, this time using the fractional values from the actual solution
as arc weights. Two other techniques that we apply are related to the arc labels: (1) We
use an arc filter (Johansson and Nugues 2008) like the Bohnet parser, which blocks
edges that did not appear in the training data based on the POS tags of the dependent
and the head, and the label, and (2) We do not include labels in the second-order
variables.
The feature set of the ILP parser is similar to but not identical to one in the Bohnet
parser. The ILP parser uses loss-augmented MIRA for training (Taskar et al 2005),
which is similar to the MIRA used in the Bohnet parser. We set the number of training
iterations to 10 as well.
6.1 Morpho-Syntax as Constraints
Using case as a filter for the decoder requires an underspecified symbolic representa-
tion of morphological information that we can use to define constraints. This allows
us to have an exact representation of syncretism controlling the search space of the
parser. The case features of a word are represented in the ILP decoder as a set of
binary variables M for which 1 signals the presence of a particular value and 0 signals
its absence. For Hungarian, we only model the different case values, which leads to
one binary variable for each of the values. For Czech and German, we also include
the gender and the number features which then gives, for each case marked word,
a binary variable for every combination of the case, number, and gender values. The
values of the morphological indicator variables are specified by annotating the data
sets with underspecified morphological descriptions that are obtained from finite-state
morphological analyzers.20 If a certain feature value is excluded by the analyzers, the
value of the indicator variable for this feature is fixed at 0, which then means that the
decoder cannot set it to 1. This way, all morphological values that cannot be marked
by the form of the token (according to the morphological analyzer) are blocked and
thereby also all parser solutions that depend on them. Words unknown to the analyzers
are left completely underspecified so that each of the possible values is allowed (none
of the variables are fixed at 0). The symbolic, grammar-based pre-annotations thus set
some of the morphological indicator variables to 0 where the word form gives enough
information while leaving other variables open to be set by the parser, which can use
syntactic context to make a more informed decision.
We now present three types of constraints that model the morpho-syntactic inter-
actions in the three languages. Their purpose is to help the parser during decoding
20 Czech: http://ufal.mff.cuni.cz/pdt/Morphology and Tagging/Morphology/index.html; German:
Schiller (1994); Hungarian: Tro?n et al (2006).
46
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
to find a linguistically plausible solution. They are inspired by the types of morpho-
syntactic interaction that Nichols (1986) describes and guide the parser by enforcing
them globally in the final structure. It is important to emphasize that these constraints
do not interact with or influence the statistical feature model of the parser. They are
applied during decoding when the parser is searching for the highest-scoring tree and
prevent solutions that violate the constraints.
The first type of constraints that we apply explicitly formulates the mapping be-
tween a function label and the case value that it requires. Equation (7) shows an example
of a case licensing constraint for the DAT label in Hungarian. A dependent d cannot be
attached to a head with label DAT if its morphological indicator variable for dative case
(mdatd ) is zero.
?d :
?
h?H
aDATdh ? mdatd (7)
The second type of constraint models the morphological agreement between de-
pendents and their heads in noun phrases (Equations (8)?(9)), for instance, determiners
and adjectives with their head noun in the noun phrases in Czech and German. In the
treebanks, the relation is marked by NK for German and Atr for Czech.21 The constraints
set the morphological indicators for an adjective and a noun in the following relation:
As long as there is no arc (aNKdh is 0) between the adjective (d) and the noun (h), the two
constraints allow for any value in the morphological indicator variables of both words.
If the arc is established (aNKdh is set to 1), the two constraints form an equivalence forcing
all the morphological indicators to agree on their value (i.e., to be both 1 or both 0). We
additionally require every word to have at least one morphological indicator variable
set to 1. Thus, if there is no solution to the equivalence the arc between the adjective and
the noun cannot be established with this function label.
mdat?pl?femh ? m
dat?pl?fem
d + 1 ? a
NK
dh (8)
mdat?pl?femh ? m
dat?pl?fem
d ? 1 + a
NK
dh (9)
For the third type, Equation (10) shows a constraint that was already proposed
by Riedel and Clarke (2006). It models label uniqueness by forcing label l to appear
at most once on all the dependents of a head (h). Due to the design of the decoder
following Carreras (2007), the Bohnet parser has no means of making sure that a
particular function label is annotated at most once per head. Table 9 shows the number
of times a grammatical function occurs more than once per head in the treebank (TRBK)
and how often it was annotated by the models in the previous experiment. Although
doubly annotated argument functions almost never appear in the treebank, the parser
21 In German and mostly also in Czech, if an adjective is attached to a noun by NK (or Atr), they stand in
an agreement relation. This fortunate circumstance allows us to bind the agreement constraint to these
function labels (and to the involved POS tags). In a (very) small number of cases in the Czech treebank,
however, an adjective is attached to a noun by Atr but there is no agreement. This happens, for example,
if the adjective is actually the head of another noun phrase that stands in attributive relation (Atr) to the
noun. The Atr label was not meant to mark agreement relations, it just happens to coincide for most of
the cases. But it might be worth considering whether morpho-syntactic relations like agreement should
be represented explicitly in syntactic treebanks.
47
Computational Linguistics Volume 39, Number 1
frequently annotates them because it has no way of checking whether the function has
already been annotated (see also Khmylko, Foth, and Menzel [2009]).
?h?l :
?
d?N
aldh ? 1 (10)
The global constraint in Equation (10) allows us to restrict the number of argument
functions and thus implements a very conservative version of subcategorization frame
with which we do not risk coverage problems caused by too restrictive verb frames.
For each language, we automatically counted the number of times a function label
occurred on the direct dependents of each node in the treebank. Labels that occurred
more than once per head with a very low frequency were still counted as appearing
at most once if our linguistic intuition would predict that (see, e.g., German subjects
in Table 9). For each function label l in these lists, the constraint in Equation (10) was
applied.
Table 9
Number of times a core grammatical function was annotated more than once in the treebank
(TRBK) by the model using gold morphology (GOLD-M), and by the model using predicted
morphology (PRED-M).
Czech German Hungarian
TRBK GOLD-M PRED-M TRBK GOLD-M PRED-M TRBK GOLD-M PRED-M
subjects 0 772 1,723 44 1,170 2,403 0 586 670
predicates 7 174 190 6 92 108 1 17 19
obj (dat.) 0 28 46 0 33 46 0 9 5
obj (acc.) 22 284 602 2 364 912 0 182 189
Each individual constraint already reduces the choices that the parser has available
for the syntactic structure. They exclude additional incorrect analyses, however, by
interaction. Figure 4 illustrates the interaction between the three constraints for the
German sentence den Ma?dchen helfen Frauen meaning women help the girls. Each individ-
ual word displays a high degree of syncretism. But when the syntactic structure is de-
cided, many options mutually exclude each other. Constraints (8) and (9) disambiguate
den Ma?dchen for dative plural feminine. The case licensing (Constraint (7)) then restricts
the labels for Ma?dchen to dative object (DA), and Constraint (10) ensures uniqueness
by restricting the choice for Frauen. The parser now has to decide whether Frauen is
subject, accusative object, or something else completely. The constraints are applied on-
line during the decoding process. If the statistical model would strongly prefer Ma?dchen
to be accusative object, the parser could label it with OA. In that case, however, it would
not be able to establish the NK label between den and Ma?dchen, because the agreement
constraint would be violated. So, the constraints filter out incorrect solutions but the
decoder is still driven by the statistical model.
6.2 Experiment 2
In the second experiment, we now apply the ILP parser to the same data sets that we
used in the first experiment, again with a five-fold cross-annotation. We trained two
48
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
?
? ?
?
den
the
??????acc-sg-masc
??????dat-pl-masc
dat-pl-fem
?????dat-pl-neut
Ma?dchen
girls
??????nom-pl-fem
?????acc-pl-fem
dat-pl-fem
?????gen-pl-fem
helfen
help
-
-
-
-
Frauen
women
nom-pl-fem
acc-pl-fem
dat-pl-fem
gen-pl-fem
SB/?DA/OA/...?SB/DA/?OA
NK
?Women help the girls?
Figure 4
Constraint interaction for the German sentence den Ma?dchen helfen Frauen meaning women help
the girls.
Table 10
Overall performance of the Bohnet parser and the ILP parser on the five-fold cross annotation
for every language. All results in percent. LAS = labeled attachment score, UAS = unlabeled
attachment score. Results for German and Hungarian are without punctuation.
Czech German Hungarian
model LAS UAS LAS UAS LAS UAS
GOLD-M 82.49 88.61 91.26 93.20 86.70 89.70
PRED-M 81.41 88.13 89.61 92.18 84.33 88.02
NO-M 79.00 86.89 89.18 91.97 78.04 86.02
ILP NO-C 81.69 88.09 89.30 91.98 84.01 87.12
ILP C 81.91 88.18 89.93 92.25 84.35 87.39
models for each language, one using the constraints (c) and one without the constraints
(no-c). In both cases, we used the predicted morphology in the feature set. Table 10
shows the parsing results for the ILP parsing models in terms of LAS and UAS in
comparison to the results of the Bohnet parser (repeated from Table 3). Both ILP models
should be compared to the PRED-M model because they have the most similar feature
sets. As can be seen from the results, the ILP parser without constraints performs overall
slightly worse than the Bohnet parser and the ILP parser using constraints performs
overall slightly better or equal. This shows that both parsers perform on a similar
level. The differences between the Czech and the German models (ILP C vs. PRED-M)
are statistically significant.22 The interesting results, however, occur for the argument
functions.
Table 11 shows the performance of the unconstrained (no-c) and constrained (c) ILP
models and the PRED-M models of the Bohnet parser on the argument functions. Again,
22 According to a two-tailed t-test for related samples with ? = 0.05.
49
Computational Linguistics Volume 39, Number 1
Table 11
Parsing results for the unconstrained (NO-C) and the constrained (C) ILP models, and the
Bohnet parser in terms of F-score (LAS) for core grammatical functions marked by case.
We omit locative objects in Czech, and second accusative objects in German because of their
extremely low frequency. ? Statistically significant when comparing the performance on a
grammatical function for the C model to the PRED-M model (? = 0.05, two-tailed t-test for
related samples).
Czech German Hungarian
NO-C C PRED-M NO-C C PRED-M NO-C C PRED-M
subject 85.41 87.23* 85.46 90.02 92.91* 90.59 85.05 87.67* 86.53
predicate 87.13 90.09* 87.11 72.86 80.70* 74.33 74.16 78.88* 74.79
obj (nom) 47.48 53.19* 38.74 ? ? ? ? ? ?
obj (gen) 70.15 72.54 70.27 31.41 42.98 34.26 ? ? ?
obj (dat) 79.99 80.42 79.54 65.21 77.78* 71.05 75.33 77.92* 73.49
obj (acc) 84.27 86.79* 84.12 83.74 87.96* 84.86 91.96 93.21* 92.53
obj (instr) 67.36 68.76 65.02 ? ? ? ? ? ?
all arg funcs 84.33 86.37* 84.21 86.27 90.11* 87.24 86.87 89.04* 87.78
all other 81.37 81.37 81.05 89.79 89.88 89.98 82.73 82.86 83.43
we only evaluated those tokens that actually carry case morphology, as we did in the
first experiment. For each language, the best results are in boldface. In addition to the
results for the different argument functions, a total score is computed over all argument
functions (all arg funcs) and another is computed over all tokens that are not included
in the first score (all other). The latter illustrates the performance of the parsing models
on the functions that are not marked by case morphology.
For each language, we get the same basic picture: Although the unconstrained ILP
model performs slightly worse than (German, Hungarian) or equally well as (Czech) the
PRED-M model of the Bohnet parser, the constrained ILP model clearly outperforms both
on the argument functions. On each of them, the constrained ILP model improves over
the other two models, raising the score by 1 percentage point for (for example) subjects
in Hungarian up to 7 percentage points on dative objects in German (compared with
the PRED-M model). What we can see is that, in general, the improvements seem to be
higher on the more infrequent arguments like dative objects and predicates than on the
frequent arguments like subject or accusative object. It is not the case, however, that the
performance of one of the infrequent functions suddenly surpasses the performance of
a more frequent function. Those two effects are to be expected because the ILP parser is
still a data-driven parser. The constraints support it by excluding morpho-syntactically
incorrect analyses but they do not resolve ambiguous cases, which are still decided by
the statistical model.
The main work done by the constraints is to establish interactions between parts
of the parse graph that are not represented in the statistical model. Because the graph-
based approach (in both parsers) factors the graph into first- (and some second-) order
arcs, and because both decoders do not use second-order features with more than
one label, a constraint like label uniqueness (Equation (10)), which is not even directly
related to morphology, is impossible to learn for the statistical model. This is because
it never sees two sister dependents and their labels together and thus does not know if
it has already annotated the current function label. Applying the constraints during
the search makes it impossible for the parser to produce an output that does not
50
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
obey label uniqueness even though the statistical model does not have access to this
information.
It should be stressed that the ILP models in their statistical model still use the same
predicted and fully disambiguated morphological information from the pipeline archi-
tecture as the Bohnet parser. As we saw in the first experiment, using no morphological
information in the statistical model is very harmful to the performance on Czech and
Hungarian, though not so much for German.
One advantage of the proposed architecture is the fact that the ILP parser is still
mainly driven by the statistical model. Krivanek and Meurers (2011) compared a data-
driven, transition-based dependency parser (Nivre et al 2007b) and a constraint-based
dependency parser (Foth and Menzel 2006) on learner and newspaper corpora and
found that whereas the former is better on modifier functions (e.g., PP-attachment),
the latter performs better on argument functions. Their explanation is that where
the data-driven parser has access to lots of data and can pick up statistical effects
in the data like semantic or selectional preferences, the constraint-based parser has
access to deep lexical and grammatical information and is thus able to model argu-
ment structure in a better way. In the ILP parser, we can combine both strengths,
letting the statistical model learn preferences but forcing it via constraints to obey hard
grammatical information. The last row in Table 11 shows that compared to the Bohnet
parser, the ILP models perform comparably well on non-argument functions (maybe
with the exception of Hungarian, where the difference is a bit more distinct). At the
same time, they perform clearly better on the argument functions due to the linguistic
constraints.
Foth and Menzel (2006) (see also Khmylko, Foth, and Menzel 2009) are further
relevant to this work in the sense that our architecture mirrors their approach. In
their work, they use a highly sophisticated rule-based parser, which they equip with
statistical components that model various subtasks like pos tagging, supertagging, or
PP-attachment. They demonstrate that a rule-based parser can benefit from statistical
models that model preferences rather than hard constraints. Our approach comes from
the other side: We equip a statistical parser with hard rules that ensure the linguistic
plausibility of the output. Both approaches prove that proper statistical models and
linguistically motivated rules can work well together to produce syntactic structures of
high quality.
One advantage of applying constraints over the argument structure is that we can
give a guarantee that certain ill-formed trees will not be produced by the parser. For
example, the constraints make sure that there will not be any parser output where there
are two subjects annotated for the same verb. Although this does not mean that the
subject will be the correct one, the formal requirement of not having two subjects is met,
which we believe can be helpful for subsequent semantic analysis/interpretation or, for
example, relation extraction. In the same sense, the constraints will also ensure that mor-
phological agreement and case licensing is correct to the degree that the morphological
analyzer was correct. This feature thus implements a tentative notion of grammaticality
for the statistical model.
7. Conclusion
In this article, we investigated the performance of the state-of-the-art statistical de-
pendency parser by Bohnet (2010) on three morphologically rich languages?Czech,
German, and Hungarian. We concentrated on the core grammatical functions (subject,
51
Computational Linguistics Volume 39, Number 1
object, etc.) that are marked by case morphology in each of the three languages. Our first
experiment shows that apart from small frequency effects due to the statistical nature of
the parser, learning the mapping between a case value and the grammatical functions
signaled by it is not a problem for the parser. We also see, however, that the pipeline
approach, where morphological information is fully disambiguated before being used
by the parser as features in the statistical model, is susceptible to error propagation for
languages that show syncretism in their morphological paradigms. Although we can
show that parsing Hungarian, an agglutinating language without major syncretism in
the case paradigm, is not affected by these problems, parsing the fusional languages
Czech and German frequently suffers from propagated errors due to ambiguous case
morphology. Furthermore, although the predicted morphological information does
not help very much in German, it contributes very much when parsing Czech and
Hungarian, even if it is not completely reliable.
Handling syncretism requires changes in the processing architecture and the rep-
resentation of morphological information. We proposed an augmented pipeline where
the parsing model is restricted by possibly underspecified, morpho-syntactic constraints
exploiting grammatical knowledge about the morphological marking regimes and the
inflectional paradigms. Although the statistical parsing model provides scores for local
substructures during decoding, the symbolic constraints are applied globally to the
entire output structure. A morpho-syntactic feature like case is interpreted as a filter
on the parser output. By modeling phenomena like case-function mapping, agreement,
and function uniqueness as constraints in an ILP decoder for dependency parsing,
we showed in a second experiment that supporting a statistical model with these
constraints helps avoiding parsing errors due to incorrect morphological preprocess-
ing. The advantage of this approach is the combination of local statistical models
and globally enforced hard grammatical knowledge. Whereas some key aspects of the
grammatical structure are ensured by the linguistic knowledge (e.g., overtly marked
case morphology) the underlying data-driven model can still exploit statistical effects
to resolve the remaining ambiguity and model semantic preferences, which are difficult
to model with hard rules.
Morphologically rich languages pose various challenges to the standard parsing
approaches because of their different linguistic properties. As one of them, case systems
are a key device in these languages to encode argument structure and reside at the
brink between morphology and syntax. Paying attention to the role of case in statistical
parsing results in more appropriate models. Morphologically rich, however, is a wide cat-
egory and covers a wide range of languages. Taking the idea of linguistically informed
restrictions over data-driven system components may lead to further improvements on
other phenomena and for other languages.
Acknowledgments
The research reported in this article was
supported by the German Research
Foundation (DFG) in project D8 of SFB 732
Incremental Specification in Context. We would
like to thank Richa?rd Farkas and Veronika
Vincze at the University of Szeged for their
help with the Hungarian corpus and language;
Bernd Bohnet for the help with his parser;
and Anders Bjo?rkelund, Anett Diesner, and
Kyle Richardson for their comments on
earlier drafts of this work.
References
Blake, Barry J. 2001. Case. Cambridge
University Press, Cambridge, MA,
2nd edition.
Bo?hmova?, Alena, Jan Hajic?, Eva Hajic?ova?,
and Barbora Hladka?. 2000. The Prague
Dependency Treebank: A three-level
annotation scenario. In A. Abeille?, editor,
Treebanks: Building and Using Syntactically
Annotated Corpora. Kluwer Academic
Publishers, Amsterdam, chapter 1,
pages 103?127.
52
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
Bohnet, Bernd. 2009. Efficient parsing of
syntactic and semantic dependency
structures. In Proceedings of the 13th
Conference on Computational Natural
Language Learning: Shared Task,
volume 2007, pages 67?72, Boulder, CO.
Bohnet, Bernd. 2010. Very high accuracy
and fast dependency parsing is not
a contradiction. In Proceedings of the
23rd International Conference on
Computational Linguistics, pages 89?97,
Beijing.
Bohnet, Bernd. 2011. Comparing advanced
graph-based and transition-based
dependency. In Proceedings of the
International Conference on Dependency
Linguistics, pages 282?289, Barcelona.
Boyd, Adriane, Markus Dickinson, and
W. Detmar Meurers. 2008. On detecting
errors in dependency treebanks. Research
on Language and Computation, 6(2):113?137.
Brants, Sabine, Stefanie Dipper, Silvia
Hansen-Shirra, Wolfgang Lezius, and
George Smith. 2002. The TIGER treebank.
In Proceedings of the 1st Workshop on
Treebanks and Linguistic Theories,
20?21 September 2002, Sozopol,
Bulgaria, pages 24?41.
Bresnan, Joan. 2001. Lexical-Functional Syntax.
Blackwell Publishers, Oxford.
Buchholz, Sabine and Erwin Marsi. 2006.
CoNLL-X shared task on multilingual
dependency parsing. In Proceedings of the
10th Conference on Computational Natural
Language Learning, pages 149?164,
New York, NY.
Carreras, Xavier. 2007. Experiments with a
higher-order projective dependency parser.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning, pages 957?961, Prague.
Cohen, Shay B. and Noah A. Smith. 2007.
Joint morphological and syntactic
disambiguation. In Proceedings of the
2007 Joint Conference on Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning, pages 208?217, Prague.
Collins, Michael, Jan Hajic?, Lance Ramshaw,
and Christoph Tillmann. 1999. A statistical
parser for Czech. In Proceedings of the
37th Annual Meeting of the Association for
Computational Linguistics, pages 505?512,
College Park, MD.
Crammer, Koby, Ofer Dekel, Shai Shalev-
Shwartz, and Yoram Singer. 2003.
Online passive-aggressive algorithms.
In Proceedings of the 16th Annual
Conference on Neural Information Processing
Systems, volume 7, pages 1217?1224,
Cambridge, MA.
Csendes, Do?ra, Ja?nos Csirik, and Tibor
Gyimo?thy. 2004. The Szeged Corpus:
A POS tagged and syntactically annotated
Hungarian natural language corpus.
In Proceedings of the 5th International
Workshop on Linguistically Interpreted
Corpora, pages 19?23, Geneva.
Eisenberg, Peter. 2006. Grundriss der deutschen
Grammatik: Der Satz. J.B. Metzler, Stuttgart,
3rd edition.
Eisner, Jason. 1997. Bilexical grammars
and a cubic-time probabilistic parser.
In Proceedings of the 5th International
Conference on Parsing Technologies,
pages 54?65, Cambridge, MA.
Eryig?it, Gu?ls?en, Joakim Nivre, and Kemal
Oflazer. 2008. Dependency parsing of
Turkish. Computational Linguistics,
34(3):357?389.
Foth, Kilian A. and Wolfgang Menzel.
2006. Hybrid parsing: Using probabilistic
models as predictors for a symbolic parser.
In Proceedings of the 21st International
Conference on Computational Linguistics
and the 44th annual meeting of the ACL,
pages 321?328, Sidney.
Gesmundo, Andrea, James Henderson,
Paola Merlo, and Ivan Titov. 2009.
A latent variable model of synchronous
syntactic-semantic parsing for multiple
languages. In Proceedings of the 13th
Conference on Computational Natural
Language Learning: Shared Task,
pages 37?42, Boulder, CO.
Goldberg, Yoav and Michael Elhadad. 2010.
Easy first dependency parsing of modern
Hebrew. In Proceedings of the NAACL HLT
2010 First Workshop on Statistical Parsing
of Morphologically-Rich Languages,
pages 103?107, Los Angeles, CA.
Goldberg, Yoav and Reut Tsarfaty. 2008.
A single generative model for joint
morphological segmentation and syntactic
parsing. In Proceedings of the 46th Annual
Meeting of the Association for Computational
Linguistics, pages 371?379, Columbus, OH.
Hajic?, Jan, Massimiliano Ciaramita, Richard
Johansson, Daisuke Kawahara,
Maria Anto`nia Mart??, Llu??s Ma`rquez,
Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan Stepa?nek, Pavel Strana?k, Mihai
Surdeanu, Nianwen Xue, and Yi Zhang.
2009. The CoNLL-2009 shared task:
Syntactic and semantic dependencies in
multiple languages. In Proceedings of the
13th Conference on Computational Natural
53
Computational Linguistics Volume 39, Number 1
Language Learning: Shared Task, pages 1?18,
Boulder, CO.
Hajic?, Jan, Jarmila Panevova?, Eva Hajic?ova?,
Petr Sgall, Petr Pajas, Jan S?te?pa?nek, Jir???
Havelka, and Marie Mikulova?. 2006. Prague
Dependency Treebank 2.0, Linguistic Data
Consortium, Philadelphia, PA.
Hudson, Richard A. 1984. Word Grammar.
Basil Blackwell, Oxford.
Janda, Laura A. and Charles E. Townsend.
2000. Czech. Lincom Europa, Munich.
Johansson, Richard and Pierre Nugues. 2008.
Dependency-based syntactic-semantic
analysis with PropBank and NomBank.
In Proceedings of the 12th Conference on
Computational Natural Language Learning,
pages 183?187, Manchester.
Khmylko, Lidia, Kilian A. Foth, and
Wolfgang Menzel. 2009. Co-parsing with
competitive Models. In Proceedings of the
11th International Conference on Parsing
Technologies, pages 99?107, Paris.
Klenner, Manfred. 2007. Shallow dependency
labeling. In Proceedings of the ACL 2007 Demo
and Poster Sessions, pages 201?204, Prague.
Krivanek, Julia and W. Detmar Meurers.
2011. Comparing rule-based and
datadriven dependency parsing of learner
language. In Proceedings of the International
Conference on Dependency Linguistics,
pages 310?318, Barcelona.
Ku?bler, Sandra. 2008. The PaGe 2008 shared
task on parsing German. In Proceedings
of the Workshop on Parsing German,
pages 55?63, Morristown, NJ.
Lee, John, Jason Naradowsky, and David A.
Smith. 2011. A discriminative model for
joint morphological disambiguation and
dependency parsing. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics, pages 885?894,
Portland, OR.
Magnanti, Thomas and Laurence Wolsey.
1995. Optimal trees. Handbooks in
Operations Research and Management
Science, 7(April):503?615.
Martins, Andre? F. T., Noah A. Smith,
and Eric P. Xing. 2009. Concise integer
linear programming formulations for
dependency parsing. In Proceedings of the
Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint
Conference on Natural Language Processing
of the AFNLP, pages 342?350, Suntec.
Marton, Yuval, Nizar Habash, and
Owen Rambow. 2010. Improving Arabic
dependency parsing with lexical and
inflectional morphological features.
In Proceedings of the NAACL HLT 2010
First Workshop on Statistical Parsing of
Morphologically-Rich Languages,
pages 13?21, Los Angeles, CA.
McDonald, Ryan and Fernando Pereira.
2006. Online learning of approximate
dependency parsing algorithms. In
Proceedings of the 11th Conference of the
European Chapter of the Association for
Computational Linguistics, pages 81?88,
Trento.
McDonald, Ryan, Fernando Pereira,
Kiril Ribarov, and Jan Hajic?. 2005.
Non-projective dependency parsing
using spanning tree algorithms. In
Proceedings of the 2005 Conference on
Human Language Technology and Empirical
Methods in Natural Language Processing,
pages 523?530, Morristown, NJ.
Mel?c?uk, Igor. 1988. Dependency Syntax:
Theory and Practice. SUNY Series in
Linguistics. State University Press of
New York.
Nichols, Joanna. 1986. Head-marking and
dependent-marking grammar. Language,
62(1):56?119.
Nivre, Joakim, Johan Hall, Sandra Ku?bler,
Ryan McDonald, Jens Nilsson, Sebastian
Riedel, and Deniz Yuret. 2007a. The
CoNLL 2007 shared task on dependency
parsing. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural
Language Processing and Computational
Natural Language Learning, pages 915?932,
Prague.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Atanas Chanev, Gu?ls?en Eryig?it,
Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser:
A language-independent system for
data-driven dependency parsing.
Natural Language Engineering,
13(2):95?135.
Riedel, Sebastian and James Clarke. 2006.
Incremental integer linear programming
for non-projective dependency parsing.
In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 129?137, Sydney.
Schiehlen, Michael. 2004. Annotation
strategies for probabilistic parsing
in German. In Proceedings of the 20th
International Conference on Computational
Linguistics, pages 390?397, Geneva.
Schiller, Anne. 1994. Dmor - user?s guide.
Technical report, University of Stuttgart.
Seeker, Wolfgang and Jonas Kuhn. 2012.
Making ellipses explicit in dependency
conversion for a German treebank.
In Proceedings of the 8th International
54
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
Conference on Language Resources and
Evaluation, pages 3132?3139, Istanbul.
Seeker, Wolfgang and Jonas Kuhn. 2011.
On the role of explicit morphological
feature representation in syntactic
dependency parsing for German. In
Proceedings of the 12th International
Conference on Parsing Technologies,
pages 58?62, Dublin.
Seeker, Wolfgang, Ines Rehbein, Jonas Kuhn,
and Josef Van Genabith. 2010. Hard
constraints for grammatical function
labelling. In Proceedings of the 48th Annual
Meeting of the Association for Computational
Linguistics, pages 1087?1097, Uppsala.
Spoustova?, Drahom??ra ?Johanka,? Jan Hajic?,
Jan Raab, and Miroslav Spousta. 2009.
Semi-supervised training for the averaged
perceptron POS tagger. In Proceedings of the
12th Conference of the European Chapter of the
Association for Computational Linguistics,
pages 763?771, Athens.
Taskar, Ben, Vassil Chatalbashev, Daphne
Koller, and Carlos Guestrin. 2005.
Learning structured prediction models:
A large margin approach. In Proceedings
of the 22th Annual International Conference
on Machine Learning, pages 896?903,
Bonn.
Tro?n, Viktor, Pe?ter Hala?csy, Pe?ter Rebrus,
Andra?s Rung, Pe?ter Vajda, and Eszter
Simon. 2006. Morphdb.hu: Hungarian
lexical database and morphological
grammar. In Proceedings of the 5th
International Conference on Language
Resources and Evaluation, pages 1670?1673,
Genoa, Italy.
Tsarfaty, Reut, Djame? Seddah, Yoav
Goldberg, Sandra Ku?bler, Marie Candito,
Jennifer Foster, Yannick Versley, Ines
Rehbein, and Lamia Tounsi. 2010.
Statistical parsing of morphologically
rich languages (SPMRL): What, how and
whither. In Proceedings of the NAACL HLT
2010 First Workshop on Statistical Parsing
of Morphologically-Rich Languages,
pages 1?12, Los Angeles, CA.
Tsarfaty, Reut and Khalil Sima?an. 2008.
Relational-realizational parsing. In
Proceedings of the 22nd International
Conference on Computational Linguistics,
pages 889?896, Manchester.
Tsarfaty, Reut and Khalil Sima?an. 2010.
Modeling morphosyntactic agreement in
constituency-based parsing of Modern
Hebrew. In Proceedings of the NAACL HLT
2010 First Workshop on Statistical Parsing
of Morphologically-Rich Languages,
pages 40?48, Los Angeles, CA.
Versley, Yannick. 2005. Parser evaluation
across text types. In Proceedings of the 4th
Workshop on Treebanks and Linguistic
Theories, pages 209?220, Barcelona.
Versley, Yannick and Ines Rehbein. 2009.
Scalable discriminative parsing for
German. In Proceedings of the 11th
International Conference on Parsing
Technologies, pages 134?137, Paris.
Vincze, Veronika, Do?ra Szauter, Attila
Alma?si, Gyo?rgy Mo?ra, Zolta?n Alexin,
and Ja?nos Csirik. 2010. Hungarian
Dependency Treebank. In Proceedings
of the 7th Conference on International
Language Resources and Evaluation,
pages 1855?1862, Valletta.
Zsibrita, Ja?nos, Veronika Vincze, and
Richa?rd Farkas. 2010. Ismeretlen
kifejeze?sek e?s a szo?faji egye?rtelmu?s??te?s.
In VII. Magyar Sza?m??to?ge?pes Nyelve?szeti
Konferencia, pages 275?283, Szeged.
55

Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1087?1097,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Hard Constraints for Grammatical Function Labelling
Wolfgang Seeker
University of Stuttgart
Institut fu?r Maschinelle Sprachverarbeitung
seeker@ims.uni-stuttgart.de
Ines Rehbein
University of Saarland
Dep. for Comp. Linguistics & Phonetics
rehbein@coli.uni-sb.de
Jonas Kuhn
University of Stuttgart
Institut fu?r Maschinelle Sprachverarbeitung
jonas@ims.uni-stuttgart.de
Josef van Genabith
Dublin City University
CNGL and School of Computing
josef@computing.dcu.ie
Abstract
For languages with (semi-) free word or-
der (such as German), labelling gramma-
tical functions on top of phrase-structural
constituent analyses is crucial for making
them interpretable. Unfortunately, most
statistical classifiers consider only local
information for function labelling and fail
to capture important restrictions on the
distribution of core argument functions
such as subject, object etc., namely that
there is at most one subject (etc.) per
clause. We augment a statistical classifier
with an integer linear program imposing
hard linguistic constraints on the solution
space output by the classifier, capturing
global distributional restrictions. We show
that this improves labelling quality, in par-
ticular for argument grammatical func-
tions, in an intrinsic evaluation, and, im-
portantly, grammar coverage for treebank-
based (Lexical-Functional) grammar ac-
quisition and parsing, in an extrinsic eval-
uation.
1 Introduction
Phrase or constituent structure is often regarded as
an analysis step guiding semantic interpretation,
while grammatical functions (i. e. subject, object,
modifier etc.) provide important information rele-
vant to determining predicate-argument structure.
In languages with restricted word order (e. g.
English), core grammatical functions can often
be recovered from configurational information in
constituent structure analyses. By contrast, sim-
ple constituent structures are not sufficient for less
configurational languages, which tend to encode
grammatical functions by morphological means
(Bresnan, 2001). Case features, for instance, can
be important indicators of grammatical functions.
Unfortunately, many of these languages (including
German) exhibit strong syncretism where morpho-
logical cues can be highly ambiguous with respect
to functional information.
Statistical classifiers have been successfully
used to label constituent structure parser output
with grammatical function information (Blaheta
and Charniak, 2000; Chrupa?a and Van Genabith,
2006). However, as these approaches tend to
use only limited and local context information
for learning and prediction, they often fail to en-
force simple yet important global linguistic con-
straints that exist for most languages, e. g. that
there will be at most one subject (object) per sen-
tence/clause.1
?Hard? linguistic constraints, such as these,
tend to affect mostly the ?core grammatical func-
tions?, i. e. the argument functions (rather than
e. g. adjuncts) of a particular predicate. As these
functions constitute the core meaning of a sen-
tence (as in: who did what to whom), it is impor-
tant to get them right. We present a system that
adds grammatical function labels to constituent
parser output for German in a postprocessing step.
We combine a statistical classifier with an inte-
ger linear program (ILP) to model non-violable
global linguistic constraints, restricting the solu-
tion space of the classifier to those labellings that
comply with our set of global constraints. There
are, of course, many other ways of including func-
tional information into the output of a syntactic
parser. Klein and Manning (2003) show that merg-
ing some linguistically motivated function labels
with specific syntactic categories can improve the
performance of a PCFG model on Penn-II En-
1Coordinate subjects/objects form a constituent that func-
tions as a joint subject/object.
1087
glish data.2 Tsarfaty and Sim?aan (2008) present
a statistical model (Relational-Realizational Pars-
ing) that alternates between functional and config-
urational information for constituency tree pars-
ing and Hebrew data. Dependency parsers like
the MST parser (McDonald and Pereira, 2006) and
Malt parser (Nivre et al, 2007) use function labels
as core part of their underlying formalism. In this
paper, we focus on phrase structure parsing with
function labelling as a post-processing step.
Integer linear programs have already been suc-
cessfully used in related fields including semantic
role labelling (Punyakanok et al, 2004), relation
and entity classification (Roth and Yih, 2004), sen-
tence compression (Clarke and Lapata, 2008) and
dependency parsing (Martins et al, 2009). Early
work on function labelling for German (Brants et
al., 1997) reports 94.2% accuracy on gold data (a
very early version of the TiGer Treebank (Brants
et al, 2002)) using Markov models. Klenner
(2007) uses a system similar to ? but more re-
stricted than ? ours to label syntactic chunks de-
rived from the TiGer Treebank. His research fo-
cusses on the correct selection of predefined sub-
categorisation frames for a verb (see also Klenner
(2005)). By contrast, our research does not involve
subcategorisation frames as an external resource,
instead opting for a less knowledge-intensive ap-
proach. Klenner?s system was evaluated on gold
treebank data and used a small set of 7 dependency
labels. We show that an ILP-based approach can
be scaled to a large and comprehensive set of 42
labels, achieving 97.99% label accuracy on gold
standard trees. Furthermore, we apply the sys-
tem to automatically parsed data using a state-of-
the-art statistical phrase-structure parser with a la-
bel accuracy of 94.10%. In both cases, the ILP-
based approach improves the quality of argument
function labelling when compared with a non-ILP-
approach. Finally, we show that the approach
substantially improves the quality and coverage
(from 93.6% to 98.4%) of treebank-based Lexical-
Functional Grammars for German over previous
work in Rehbein and van Genabith (2009).
The paper is structured as follows: Section 2
presents basic data demonstrating the challenges
presented by German word order and case syn-
cretism for the function labeller. Section 3 de-
2Table 6 shows that for our data a model with merged
category and function labels (but without hard constraints!)
performs slightly worse than the ILP approach developed in
this paper.
scribes the labeller including the feature model of
the classifier and the integer linear program used
to pick the correct labelling. The evaluation part
(Section 4) is split into an intrinsic evaluation mea-
suring the quality of the labelling directly using
the German TiGer Treebank (Brants et al, 2002),
and an extrinsic evaluation where we test the im-
pact of the constraint-based labelling on treebank-
based automatic LFG grammar acquisition.
2 Data
Unlike English, German exhibits a relatively free
word order, i. e. in main clauses, the verb occu-
pies second position (the last position in subor-
dinated clauses) and arguments and adjuncts can
be placed (fairly) freely. The grammatical func-
tion of a noun phrase is marked morphologically
on its constituting parts. Determiners, pronouns,
adjectives and nouns carry case markings and in
order to be well-formed, all parts of a noun phrase
have to agree on their case features. German uses
a nominative?accusative system to mark predicate
arguments. Subjects are marked with nominative
case, direct objects carry accusative case. Further-
more, indirect objects are mostly marked with da-
tive case and sometimes genitive case.
(1) Der Lo?we
NOM
the lion
gibt
gives
dem Wolf
DAT
the wolf
einen Besen.
ACC
a broom
The lion gives a broom to the wolf.
(1) shows a sentence containing the ditransi-
tive verb geben (to give) with its three arguments.
Here, the subject is unambiguously marked with
nominative case (NOM), the indirect object with
dative case (DAT) and the direct object with ac-
cusative case (ACC). (2) shows possible word or-
ders for the arguments in this sentence.3
(2) Der Lo?we gibt einen Besen dem Wolf.
Dem Wolf gibt der Lo?we einen Besen.
Dem Wolf gibt einen Besen der Lo?we.
Einen Besen gibt der Lo?we dem Wolf.
Einen Besen gibt dem Wolf der Lo?we.
Since all permutations of arguments are possi-
ble, there is no chance for a statistical classifier to
decide on the correct function of a noun phrase by
its position alone. Introducing adjuncts to this ex-
ample makes matters even worse.
3Note that although (apart from the position of the finite
verb) there are no syntactic restrictions on the word order,
there are restrictions pertaining to phonological or informa-
tion structure.
1088
Case information for a given noun phrase can
give a classifier some clue about the correct ar-
gument function, since functions are strongly re-
lated to case values. Unfortunately, the German
case system is complex (see Eisenberg (2006) for
a thorough description) and exhibits a high degree
of case syncretism. (3) shows a sentence where
both argument NPs are ambiguous between nom-
inative or accusative case. In such cases, addi-
tional semantic or contextual information is re-
quired for disambiguation. A statistical classifier
(with access to local information only) runs a high
risk of incorrectly classifying both NPs as sub-
jects, or both as direct objects or even as nominal
predicates (which are also required to carry nom-
inative case). This would leave us with uninter-
pretable results. Uninterpretability of this kind can
be avoided if we are able to constrain the number
of subjects and objects globally to one per clause.4
(3) Das Schaf
NOM/ACC
the sheep
sieht
sees
das Ma?dchen.
NOM/ACC
the girl
EITHER The sheep sees the girl
OR The girl sees the sheep.
3 Grammatical Function Labelling
Our function labeller was developed and tested on
the TiGer Treebank (Brants et al, 2002). The
TiGer Treebank is a phrase-structure and gram-
matical function annotated treebank with 50,000
newspaper sentences from the Frankfurter Rund-
schau (Release 2, July 2006). Its overall anno-
tation scheme is quite flat to account for the rel-
atively free word order of German and does not
allow for unary branching. The annotations use
non-projective trees modelling long distance de-
pendencies directly by crossing branches. Words
are lemmatised and part-of-speech tagged with the
Stuttgart-Tu?bingen Tag Set (STTS) (Schiller et al,
1999) and contain morphological annotations (Re-
lease 2). TiGer uses 25 syntactic categories and a
set of 42 function labels to annotate the grammat-
ical function of a phrase.
The function labeller consists of two main com-
ponents, a maximum entropy classifier and an in-
teger linear program. This basic architecture was
introduced by Punyakanok et al (2004) for the
task of semantic role labelling and since then has
been applied to different NLP tasks without signif-
icant changes. In our case, its input is a bare tree
4Although the classifier may, of course, still identify the
wrong phrase as subject or object.
structure (as obtained by a standard phrase struc-
ture parser) and it outputs a tree structure where
every node is labelled with the grammatical rela-
tion it bears to its mother node. For each possi-
ble label and for each node, the classifier assigns
a probability that this node is labelled by this la-
bel. This results in a complete probability distri-
bution over all labels for each node. An integer
linear program then tries to find the optimal over-
all tree labelling by picking for each node the label
with the highest probability without violating any
of its constraints. These constraints implement lin-
guistic rules like the one-subject-per-sentence rule
mentioned above. They can also be used to cap-
ture treebank particulars, such as for example that
punctuation marks never receive a label.
3.1 The Feature Model
Maximum entropy classifiers have been used in a
wide range of applications in NLP for a long time
(Berger et al, 1996; Ratnaparkhi, 1998). They
usually give good results while at the same time
allowing for the inclusion of arbitrarily complex
features. They also have the advantage that they
directly output probability distributions over their
set of labels (unlike e. g. SVMs).
The classifier uses the following features:
? the lemma (if terminal node)
? the category (the POS for terminal nodes)
? the number of left/right sisters
? the category of the two left/right sisters
? the number of daughters
? the number of terminals covered
? the lemma of the left/right corner terminal
? the category of the left/right corner terminal
? the category of the mother node
? the category of the mother?s head node
? the lemma of the mother?s head node
? the category of the grandmother node
? the category of the grandmother?s head node
? the lemma of the grandmother?s head node
? the case features for noun phrases
? the category for PP objects
? the lemma for PP objects (if terminal node)
These features are also computed for the head
of the phrase, determined using a set of head-
finding rules in the style of Magerman (1995)
adapted to TiGer. For lemmatisation, we use Tree-
Tagger (Schmid, 1994) and case features of noun
1089
phrases are obtained from a full German morpho-
logical analyser based on (Schiller, 1994). If a
noun phrase consists of a single word (e. g. pro-
nouns, but also bare common nouns and proper
nouns), all case values output by the analyser are
used to reflect the case syncretism. For multi-word
noun phrases, the case feature is computed by tak-
ing the intersection of all case-bearing words in-
side the noun phrase, i. e. determiners, pronouns,
adjectives, common nouns and proper nouns. If,
for some reason (e.g., due to a bracketing error in
phrase structure parsing), the intersection turns out
to be empty, all four case values are assigned to the
phrase.5
3.2 Constrained Optimisation
In the second step, a binary integer linear pro-
gram is used to select those labels that optimise the
whole tree labelling. A linear program consists of
a linear objective function that is to be maximised
(or minimised) and a set of constraints which im-
pose conditions on the variables of the objective
function (see (Clarke and Lapata, 2008) for a short
but readable introduction). Although solving a lin-
ear program has polynomial complexity, requiring
the variables to be integral or binary makes find-
ing a solution exponentially hard in the worst case.
Fortunately, there are efficient algorithms which
are capable of handling a large number of vari-
ables and constraints in practical applications.6
For the function labeller, we define the set of
binary variables V = N ? L to be the crossprod-
uct of the set of nodes N and the set of labels L.
Setting a variable xn,l to 1 means that node n is
labelled by label l. Every variable is weighted by
the probability wn,l = P (l|f(n)) which the clas-
sifier has assigned to this node-label combination.
The objective function that we seek to optimise is
defined as the sum over all weighted variables:
max
?
n?N
?
l?L
wn,lxn,l (4)
Since we want every node to receive exactly one
5We decided to train the classifier on automatically
assigned and possibly ambiguous morphological informa-
tion instead of on the hand-annotated and manually disam-
biguated morphological information provided by TiGer be-
cause we want the classifier to learn the German case syn-
cretism. This way, the classifier will perform better when pre-
sented with unseen data (e.g. from parser output) for which
no hand-annotated morphological information is available.
6See lpsolve (http://lpsolve.sourceforge.net/) or GLPK
(http://www.gnu.org/software/glpk/glpk.html) for open-
source implementations
label, we add a constraint that for every node n,
exactly one of its variables is set to 1.
?
l?L
xn,l = 1 (5)
Up to now, the whole system is doing exactly
the same as an ordinary classifier that always takes
the most probable label for each node. We will
now add additional global and local linguistic con-
straints.7
The first and most important constraint restricts
the number of each argument function (as opposed
to modifier functions) to at most one per clause.
Let D ? N ? N be the direct dominance rela-
tion between the nodes of the current tree. For ev-
ery node n with category S (sentence) or VP (verb
phrase), at most one of its daughters is allowed
to be labelled SB (subject). The single-subject-
function condition is defined as:
cat(n) ? {S, V P} ??
?
?n,m??D
xm,SB ? 1 (6)
Identical constraints are added for labels OA,
OA2, DA, OG, OP, PD, OC, EP.8
We add further constraints to capture the follow-
ing linguistic restrictions:
? Of all daughters of a phrase, only one is allowed
to be labelled HD (head).
?
?n,m??D
xm,HD ? 1 (7)
? If a noun phrase carries no case feature for nom-
inative case, it cannot be labelled SB, PD or EP.
case(n) 6= nom ??
?
l?{SB,PD,EP}
xn,l = 0
(8)
? If a noun phrase carries no case feature for ac-
cusative case, it cannot be labelled OA or OA2.
? If a noun phrase carries no case feature for da-
tive case, it cannot be labelled DA.
? If a noun phrase carries no case feature for gen-
itive case, it cannot be labelled OG or AG9.
7Note that some of these constraints are language specific
in that they represent linguistic facts about German and do
not necessarily hold for other languages. Furthermore, the
constraints are treebank specific to a certain degree in that
they use a TiGer-specific set of labels and are conditioned on
TiGer-specific configurations and categories.
8SB = subject, OA = accusative object, OA2 = sec-
ond accusative object, DA = dative, OG = genitive object,
OP = prepositional object, PD = predicate, OC = clausal ob-
ject, EP = expletive es
9AG = genitive adjunct
1090
Unlike Klenner (2007), we do not use prede-
fined subcategorization frames, instead letting the
statistical model choose arguments.
In TiGer, sentences whose main verbs are
formed from auxiliary-participle combinations,
are annotated by embedding the participle under
an extra VP node and non-subject arguments are
sisters to the participle. Therefore we add an ex-
tension of the constraint in (6) to the constraint set
in order to also include the daughters of an embed-
ded VP node in such a case.
Because of the particulars of the annotation
scheme of TiGer, we can decide some labels in
advance. As mentioned before, punctuation does
not get a label in TiGer. We set the label for those
nodes to ?? (no label). Other examples are:
? If a node?s category is PTKVZ (separated verb
particle), it is labeled SVP (separable verb par-
ticle).
cat(n) = PTKV Z ?? xn,SV P = 1 (9)
? If a node?s category is APPR, APPRART,
APPO or APZR (prepositions), it is labeled AC
(adpositional case marker).
? All daughters of an MTA node (multi-token
adjective) are labeled ADC (adjective compo-
nent).
These constraints are conditioned on part-of-
speech tags and require high POS-tagging accu-
racy (when dealing with raw text).
Due to the constraints imposed on the classifi-
cation, the function labeller can no longer assign
two subjects to the same S node. Faced with two
nodes whose most probable label is SB, it has to
decide on one of them taking the next best label for
the other. This way, it outputs the optimal solution
with respect to the set of constraints. Note that this
requires the feature model not only to rank the cor-
rect label highest but also to provide a reasonable
ranking of the other labels as well.
4 Evaluation
We conducted a number of experiments using
1,866 sentences of the TiGer Dependency Bank
(Forst et al, 2004) as our test set. The TiGerDB is
a part of the TiGer Treebank semi-automatically
converted into a dependency representation. We
use the manually labelled TiGer trees correspond-
ing to the sentences in the TiGerDB for assessing
the labelling quality in the intrinsic evaluation, and
the dependencies from TiGerDB for assessing the
quality and coverage of the automatically acquired
LFG resources in the extrinsic evaluation.
In order to test on real parser output, the test
set was parsed with the Berkeley Parser (Petrov et
al., 2006) trained on 48k sentences of the TiGer
corpus (Table 1), excluding the test set. Since the
Berkeley Parser assumes projective structures, the
training data and test data were made projective by
raising non-projective nodes in the tree (Ku?bler,
2005).
precision 83.60 recall 82.81
f-score 83.20 tagging acc. 97.97
Table 1: evalb unlabelled parsing scores on test set for Berke-
ley Parser trained on 48,000 sentences (sentence length? 40)
The maximum entropy classifier of the func-
tion labeller was trained on 46,473 sentences of
the TiGer Treebank (excluding the test set) which
yields about 1.2 million nodes as training samples.
For training the Maximum Entropy Model, we
used the BLMVM algorithm (Benson and More,
2001) with a width factor of 1.0 (Kazama and Tsu-
jii, 2005) implemented in an open-source C++ li-
brary from Tsujii Laboratory.10 The integer linear
program was solved with the simplex algorithm in
combination with a branch-and-bound method us-
ing the freely available GLPK.11
4.1 Intrinsic Evaluation
In the intrinsic evaluation, we measured the qual-
ity of the labelling itself. We used the node
span evaluation method of (Blaheta and Char-
niak, 2000) which takes only those nodes into ac-
count which have been recognised correctly by the
parser, i.e. if there are two nodes in the parse and
the reference treebank tree which cover the same
word span. Unlike Blaheta and Charniak (2000)
however, we do not require the two nodes to carry
the same syntactic category label.12
Table 2 shows the results of the node span eval-
uation. The labeller achieves close to 98% label
accuracy on gold treebank trees which shows that
the feature model captures the differences between
the individual labels well. Results on parser output
are about 4 percentage points (absolute) lower as
parsing errors can distort local context features for
the classifier even if the node itself has been parsed
10http://www-tsujii.is.s.u-tokyo.ac.jp/?tsuruoka/maxent/
11http://www.gnu.org/software/glpk/glpk.html
12We also excluded the root node, all punctuation marks
and both nodes in unary branching sub-trees from evaluation.
1091
correctly. The addition of the ILP constraints im-
proves results only slightly since the constraints
affect only (a small number of) argument labels
while the evaluation considers all 40 labels occur-
ring in the test set. Since the constraints restrict the
selection of certain labels, a less probable label has
to be picked by the labeller if the most probable
is not available. If the classifier is ranking labels
sensibly, the correct label should emerge. How-
ever, with an incorrect ranking, the ILP constraints
might also introduce new errors.
label accuracy error red.
without constraints
gold 44689/45691 = 97.81% ?
parser 40578/43140 = 94.06% ?
with constraints
gold 44773/45691 = 97.99%* 8.21%
parser 40593/43140 = 94.10% 0.68%
Table 2: label accuracy and error reduction (all labels) for
node span evaluation, * statistically significant, sign test, ? =
0.01 (Koo and Collins, 2005)
As the main target of the constraint set are argu-
ment functions, we also tested the quality of argu-
ment labels. Table 3 shows the node span evalua-
tion in terms of precision, recall and f-score for ar-
gument functions only, with clear statistically sig-
nificant improvements.
prec. rec. f-score
without constraints
gold standard 92.41 91.86 92.13
parser output 88.14 86.43 87.28
with constraints
gold standard 94.31 92.76 93.53*
parser output 89.51 86.73 88.09*
Table 3: node span results for the test set, argument functions
only (SB, EP, PD, OA, OA2, DA, OG, OP, OC), * statistically
significant, sign test, ? = 0.01 (Koo and Collins, 2005)
For comparison and to establish a highly com-
petitive baseline, we use the best-scoring system
in (Chrupa?a and Van Genabith, 2006), trained and
tested on exactly the same data sets. This purely
statistical labeller achieves accuracy of 96.44%
(gold) and 92.81% (parser) for all labels, and f-
scores of 89.88% (gold) and 84.98% (parser) for
argument labels. Tables 2 and 3 show that our sys-
tem (with and even without ILP constraints) com-
prehensively outperforms all corresponding base-
line scores.
The node span evaluation defines a correct la-
belling by taking only those nodes (in parser out-
put) into account that have a corresponding node
in the reference tree. However, as this restricts at-
tention to correctly parsed nodes, the results are
somewhat over-optimistic. Table 4 provides the
results obtained from an evalb evaluation of the
same data sets.13 The gold standard scores are
high confirming our previous findings about the
performance of the function labeller. However,
the results on parser output are much worse. The
evaluation scores are now taking the parsing qual-
ity into account (Table 1). The considerable drop
in quality between gold trees and parser output
clearly shows that a good parse tree is an impor-
tant prerequisite for reasonable function labelling.
This is in accordance with previous findings by
Punyakanok et al (2008) who emphasise the im-
portance of syntactic parsing for the closely re-
lated task of semantic role labelling.
prec. rec. f-score
without constraints
gold standard 95.94 95.94 95.94
parser output 76.27 75.55 75.91
with constraints
gold standard 96.21 96.21 96.21
parser output 76.36 75.64 76.00
Table 4: evalb results for the test set
4.1.1 Subcategorisation Frames
Early on in the paper we mention that, unlike e. g.
Klenner (2007), we did not include predefined
subcategorisation frames into the constraint set,
but rather let the joint statistical and ILP models
decide on the correct type of arguments assigned
to a verb. The assumption is that if one uses prede-
fined subcategorisation frames which fix the num-
ber and type of arguments for a verb, one runs the
risk of excluding correct labellings due to missing
subcat frames, unless a very comprehensive and
high quality subcat lexicon resource is available.
In order to test this assumption, we run an addi-
tional experiment with about 10,000 verb frames
for 4,508 verbs, which were automatically ex-
tracted from our training section. Following Klen-
ner (2007), for each verb and for each subcat frame
for this verb attested at least once in the training
data, we introduce a new binary variable fn to
the ILP model representing the n-th frame (for the
verb) weighted by its frequency.
We add an ILP constraint requiring exactly one
of the frames to be set to one (each verb has to have
a subcat frame) and replace the ILP constraint in
(6) by:
13Function labels were merged with the category symbols.
1092
??n,m??D
xm,SB ?
?
SB?fi
fi = 0 (10)
This constraint requires the number of subjects
in a phrase to be equal to the number of selected14
verb frames that require a subject. As each verb
is constrained to ?select? exactly one subcat frame
(see additional ILP constraint above), there is at
most one subject per phrase, if the frame in ques-
tion requires a subject. If the selected frame does
not require a subject, then the constraint blocks the
assignment of subjects for the entire phrase. The
same was done for the other argument functions
and as before we included an extension of this con-
straint to cover embedded VPs. For unseen verbs
(i.e. verbs not attested in the training set) we keep
the original constraints as a back-off.
prec. rec. f-score
all labels (cmp. Table 2)
gold standard 97.24 97.24 97.24
parser output 93.43 93.43 93.43
argument functions only (cmp. Table 3)
gold standard 91.36 90.12 90.74
parser output 86.64 84.38 85.49
Table 5: node span results for the test set using constraints
with automatically extracted subcat frames
Table 5 shows the results of the test set node
span evaluation when using the ILP system en-
hanced with subcat frames. Compared to Tables 2
and 3, the results are clearly inferior, and particu-
larly so for argument grammatical functions. This
seems to confirm our assumption that, given our
data, letting the joint statistical and ILP model de-
cide argument functions is superior to an approach
that involves subcat frames. However, and impor-
tantly, our results do not rule out that a more com-
prehensive subcat frame resource may in fact re-
sult in improvements.
4.2 Extrinsic Evaluation
Over the last number of years, treebank-based
deep grammar acquisition has emerged as an
attractive alternative to hand-crafting resources
within the HPSG, CCG and LFG paradigms
(Miyao et al, 2003; Clark and Hockenmaier,
2002; Cahill et al, 2004). While most of the ini-
tial development work focussed on English, more
recently efforts have branched to other languages.
Below we concentrate on LFG.
14The variable representing this frame has been set to 1.
Lexical-Functional Grammar (Bresnan, 2001)
is a constraint-based theory of grammar with min-
imally two levels of representation: c(onstituent)-
structure and f(unctional)-structure. C-structure
(CFG trees) captures language specific surface
configurations such as word order and the hier-
archical grouping of words into phrases, while
f-structure represents more abstract (and some-
what more language independent) grammatical re-
lations (essentially bilexical labelled dependencies
with some morphological and semantic informa-
tion, approximating to basic predicate-argument
structures) in the form of attribute-value struc-
tures. F-structures are defined in terms of equa-
tions annotated to nodes in c-structure trees (gram-
mar rules). Treebank-based LFG acquisition was
originally developed for English (Cahill, 2004;
Cahill et al, 2008) and is based on an f-structure
annotation algorithm that annotates c-structure
trees (from a treebank or parser output) with
f-structure equations, which are read off of the tree
and passed on to a constraint solver producing an
f-structure for the given sentence. The English
annotation algorithm (for Penn-II treebank-style
trees) relies heavily on configurational and catego-
rial information, translating this into grammatical
functional information (subject, object etc.) rep-
resented at f-structure. LFG is ?functional? in the
mathematical sense, in that argument grammatical
functions have to be single valued (there cannot be
two or more subjects etc. in the same clause). In
fact, if two or more values are assigned to a single
argument grammatical function in a local tree, the
LFG constraint solver will produce a clash (i. e.
it will fail to produce an f-structure) and the sen-
tence will be considered ungrammatical (in other
words, the corresponding c-structure tree will be
uninterpretable).
Rehbein (2009) and Rehbein and van Genabith
(2009) develop an f-structure annotation algorithm
for German based on the TiGer treebank resource.
Unlike the English annotation algorithm and be-
cause of the language-particular properties of Ger-
man (see Section 2), the German annotation al-
gorithm cannot rely on c-structure configurational
information, but instead heavily uses TiGer func-
tion labels in the treebank. Learning function la-
bels is therefore crucial to the German LFG an-
notation algorithm, in particular when parsing raw
text. Because of the strong case syncretism in Ger-
man, traditional classification models using local
1093
information only run the risk of predicting mul-
tiple occurences of the same function (subject,
object etc.) at the same level, causing feature
clashes in the constraint solver with no f-structure
being produced. Rehbein (2009) and Rehbein
and van Genabith (2009) identify this as a major
problem resulting in a considerable loss in cov-
erage of the German annotation algorithm com-
pared to English, in particular for parsing raw text,
where TiGer function labels have to be supplied by
a machine-learning-based method and where the
coverage of the LFG annotation algorithm drops
to 93.62% with corresponding drops in recall and
f-scores for the f-structure evaluations (Table 6).
Below we test whether the coverage problems
caused by incorrect multiple assignments of gram-
matical functions can be addressed using the com-
bination of classifier with ILP constraints devel-
oped in this paper. We report experiments where
automatically parsed and labelled data are handed
over to an LFG f-structure computation algorithm.
The f-structures produced are converted into a
dependency triple representation (Crouch et al,
2002) and evaluated against TiGerDB.
cov. prec. rec. f-score
upper bound 99.14 85.63 82.58 84.07
without constraints
gold 95.82 84.71 76.68 80.49
parser 93.41 79.70 70.38 74.75
with constraints
gold 99.30 84.62 82.15 83.37
parser 98.39 79.43 75.60 77.47
Rehbein 2009
parser 93.62 79.20 68.86 73.67
Table 6: f-structure evaluation results for the test set against
TigerDB
Table 6 shows the results of the f-structure
evaluation against TiGerDB, with 84.07% f-score
upper-bound results for the f-structure annotation
algorithm on the original TiGer treebank trees
with hand-annotated function labels. Using the
function labeller without ILP constraints results in
drastic drops in coverage (between 4.5% and 6.5%
points absolute) and hence recall (6% and 12%)
and f-score (3.5% and 9.5%) for both gold trees
and parser output (compared to upper bounds).
By contrast, with ILP constraints, the loss in cov-
erage observed above almost completely disap-
pears and recall and f-scores improve by between
4.4% and 5.5% (recall) and 3% (f-score) abso-
lute (over without ILP constraints). For compar-
ison, we repeated the experiment using the best-
scoring method of Rehbein (2009). Rehbein trains
the Berkeley Parser to learn an extended category
set, merging TiGer function labels with syntactic
categories, where the parser outputs fully-labelled
trees. The results show that this approach suf-
fers from the same drop in coverage as the classi-
fier without ILP constraints, with recall about 7%
and f-score about 4% (absolute) lower than for the
classifier with ILP constraints.
Table 7 shows the dramatic effect of the ILP
constraints on the number of sentences in the test
set that have multiple argument functions of the
same type within the same clause. With ILP con-
straints, the problem disappears and therefore, less
feature-clashes occur during f-structure computa-
tion.
no constraints constraints
gold 185 0
parser 212 0
Table 7: Number of sentences in the test set with doubly an-
notated argument functions
In order to assess whether ILP constraints help
with coverage only or whether they affect the qual-
ity of the f-structures as well, we repeat the experi-
ment in Table 6, however this time evaluating only
on those sentences that receive an f-structure, ig-
noring the rest. Table 8 shows that the impact of
ILP constraints on quality is much less dramatic
than on coverage, with only very small variations
in precison, recall and f-scores across the board,
and small increases over Rehbein (2009).
cov. prec. rec. f-score
no constr. 93.41 79.70 77.89 78.79
constraints 98.39 79.43 77.85 78.64
Rehbein 93.62 79.20 76.43 77.79
Table 8: f-structure evaluation results for parser output ex-
cluding sentences without f-structures
Early work on automatic LFG acquisition and
parsing for German is presented in Cahill et al
(2003) and Cahill (2004), adapting the English
Annotation Algorithm to an earlier and smaller
version of the TiGer treebank (without morpho-
logical information) and training a parser to learn
merged Tiger function-category labels, and report-
ing 95.75% coverage and an f-score of 74.56%
f-structure quality against 2,000 gold treebank
trees automatically converted into f-structures.
Rehbein (2009) uses the larger Release 2 of the
treebank (with morphological information) report-
ing 77.79% f-score and coverage of 93.62% (Ta-
1094
ble 8) against the dependencies in the TiGerDB
test set. The only rule-based approach to German
LFG-parsing we are aware of is the hand-crafted
German grammar in the ParGram Project (Butt
et al, 2002). Forst (2007) reports 83.01% de-
pendency f-score evaluated against a set of 1,497
sentences of the TiGerDB. It is very difficult to
compare results across the board, as individual pa-
pers use (i) different versions of the treebank, (ii)
different (sections of) gold-standards to evaluate
against (gold TiGer trees in TigerDB, the depen-
dency representations provided by TigerDB, auto-
matically generated gold-standards etc.) and (iii)
different label/grammatical function sets. Further-
more, (iv) coverage differs drastically (with the
hand-crafted LFG resources achieving about 80%
full f-structures) and finally, (v) some of the gram-
mars evaluated having been used in the generation
of the gold standards, possibly introducing a bias
towards these resources: the German hand-crafted
LFG was used to produce TiGerDB (Forst et al,
2004). In order to put the results into some per-
spective, Table 9 shows an evaluation of our re-
sources against a set of automatically generated
gold standard f-structures produced by using the
f-structure annotation algorithm on the original
hand-labelled TiGer gold trees in the section cor-
responding to TiGerDB: without ILP constraints
we achieve a dependency f-score of 84.35%, with
ILP constraints 87.23% and 98.89% coverage.
cov. prec. rec. f-score
without constraints
gold 95.24 97.76 90.93 94.22
parser 93.35 88.71 80.40 84.35
with constraints
gold 99.30 97.66 97.33 97.50
parser 98.89 88.37 86.12 87.23
Table 9: f-structure evaluation results for the test set against
automatically generated goldstandard (1,850 sentences)
5 Conclusion
In this paper, we addressed the problem of assign-
ing grammatical functions to constituent struc-
tures. We have proposed an approach to grammat-
ical function labelling that combines the flexibil-
ity of a statistical classifier with linguistic expert
knowledge in the form of hard constraints imple-
mented by an integer linear program. These con-
straints restrict the solution space of the classifier
by blocking those solutions that cannot be correct.
One of the strengths of an integer linear program
is the unlimited context it can take into account
by optimising over the entire structure, providing
an elegant way of supporting classifiers with ex-
plicit linguistic knowledge while at the same time
keeping feature models small and comprehensi-
ble. Most of the constraints are direct formaliza-
tions of linguistic generalizations for German. Our
approach should generalise to other languages for
which linguistic expertise is available.
We evaluated our system on the TiGer corpus
and the TiGerDB and gave results on gold stan-
dard trees and parser output. We also applied
the German f-structure annotation algorithm to
the automatically labelled data and evaluated the
system by measuring the quality of the resulting
f-structures. We found that by using the con-
straint set, the function labeller ensures the inter-
pretability and thus the usefulness of the syntac-
tic structure for a subsequently applied processing
step. In our f-structure evaluation, that means, the
f-structure computation algorithm is able to pro-
duce an f-structure for almost all sentences.
Acknowledgements
The first author would like to thank Gerlof Bouma
for a lot of very helpful discussions. We would
like to thank our anonymous reviewers for de-
tailed and helpful comments. The research was
supported by the Science Foundation Ireland SFI
(Grant 07/CE/I1142) as part of the Centre for
Next Generation Localisation (www.cngl.ie) and
by DFG (German Research Foundation) through
SFB 632 Potsdam-Berlin and SFB 732 Stuttgart.
References
Steven J. Benson and Jorge J. More. 2001. A limited
memory variable metric method in subspaces and
bound constrained optimization problems. Techni-
cal report, Argonne National Laboratory.
Adam L. Berger, Vincent J.D. Pietra, and Stephen A.D.
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational linguis-
tics, 22(1):71.
Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In Proceedings of the
1st North American chapter of the Association for
Computational Linguistics conference, pages 234 ?
240, Seattle, Washington. Morgan Kaufmann Pub-
lishers Inc.
Thorsten Brants, Wojciech Skut, and Brigitte Krenn.
1997. Tagging grammatical functions. In Proceed-
ings of EMNLP, volume 97, pages 64?74.
1095
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, page 2441.
Joan Bresnan. 2001. Lexical-Functional Syntax.
Blackwell Publishers.
Miriam Butt, Helge Dyvik, Tracy Halloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
parallel grammar project. In COLING-02 on Gram-
mar engineering and evaluation-Volume 15, volume
pages, page 7. Association for Computational Lin-
guistics.
Aoife Cahill, Martin Forst, Mairead McCarthy, Ruth
ODonovan, Christian Rohrer, Josef van Genabith,
and Andy Way. 2003. Treebank-based multilingual
unification-grammar development. In Proceedings
of the Workshop on Ideas and Strategies for Multi-
lingual Grammar Development at the 15th ESSLLI,
page 1724.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
van Genabith, and Andy Way. 2004. Long-
distance dependency resolution in automatically ac-
quired wide-coverage PCFG-based LFG approxima-
tions. Proceedings of the 42nd Annual Meeting
on Association for Computational Linguistics - ACL
?04, pages 319?es.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan
Riezler, Josef van Genabith, and Andy Way. 2008.
Wide-Coverage Deep Statistical Parsing Using Au-
tomatic Dependency Structure Annotation. Compu-
tational Linguistics, 34(1):81?124, Ma?rz.
Aoife Cahill. 2004. Parsing with Automatically Ac-
quired, Wide-Coverage, Robust, Probabilistic LFG
Approximations. Ph.D. thesis, Dublin City Univer-
sity.
Grzegorz Chrupa?a and Josef Van Genabith. 2006.
Using machine-learning to assign function labels
to parser output for Spanish. In Proceedings of
the COLING/ACL main conference poster session,
page 136143, Sydney. Association for Computa-
tional Linguistics.
Stephen Clark and Judith Hockenmaier. 2002. Evalu-
ating a wide-coverage CCG parser. In Proceedings
of the LREC 2002, pages 60?66.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression an integer linear
programming approach. Journal of Artificial Intelli-
gence Research, 31:399?429.
Richard Crouch, Ronald M. Kaplan, Tracy Halloway
King, and Stefan Riezler. 2002. A comparison of
evaluation metrics for a broad-coverage stochastic
parser. In Proceedings of LREC 2002 Workshop,
pages 67?74, Las Palmas, Canary Islands, Spain.
Peter Eisenberg. 2006. Grundriss der deutschen
Grammatik: Das Wort. J.B. Metzler, Stuttgart, 3
edition.
Martin Forst, Nu?ria Bertomeu, Berthold Crysmann,
Frederik Fouvry, Silvia Hansen-Shirra, and Valia
Kordoni. 2004. Towards a dependency-based gold
standard for German parsers The TiGer Dependency
Bank. In Proceedings of the COLING Workshop
on Linguistically Interpreted Corpora (LINC ?04),
Geneva, Switzerland.
Martin Forst. 2007. Filling Statistics with Linguistics
Property Design for the Disambiguation of German
LFG Parses. In Proceedings of ACL 2007. Associa-
tion for Computational Linguistics.
Jun?Ichi Kazama and Jun?Ichi Tsujii. 2005. Maxi-
mum entropy models with inequality constraints: A
case study on text categorization. Machine Learn-
ing, 60(1):159194.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL
2003, pages 423?430, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Manfred Klenner. 2005. Extracting Predicate Struc-
tures from Parse Trees. In Proceedings of the
RANLP 2005.
Manfred Klenner. 2007. Shallow dependency label-
ing. In Proceedings of the ACL 2007 Demo and
Poster Sessions, page 201204, Prague. Association
for Computational Linguistics.
Terry Koo and Michael Collins. 2005. Hidden-
variable models for discriminative reranking. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing - HLT ?05, pages 507?514, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Sandra Ku?bler. 2005. How Do Treebank Annotation
Schemes Influence Parsing Results? Or How Not to
Compare Apples And Oranges. In Proceedings of
RANLP 2005, Borovets, Bulgaria.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, page 276283, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics Morristown,
NJ, USA.
Andre? F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
ACL 2009.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, volume 6.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2003. Probabilistic modeling of argument structures
including non-local dependencies. In Proceedings
of the Conference on Recent Advances in Natural
Language Processing RANLP 2003, volume 2.
1096
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135, Januar.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the ACL
- ACL ?06, pages 433?440, Morristown, NJ, USA.
Association for Computational Linguistics.
Vasin Punyakanok, Wen-Tau Yih, Dan Roth, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
the 20th international conference on Computational
Linguistics - COLING ?04, Morristown, NJ, USA.
Association for Computational Linguistics.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The Importance of Syntactic Parsing and Inference
in Semantic Role Labeling. Computational Linguis-
tics, 34(2):257?287, Juni.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania.
Ines Rehbein and Josef van Genabith. 2009. Auto-
matic Acquisition of LFG Resources for German-
As Good as it gets. In Miriam Butt and Tracy Hol-
loway King, editors, Proceedings of LFG Confer-
ence 2009. CSLI Publications.
Ines Rehbein. 2009. Treebank-based grammar acqui-
sition for German. Ph.D. thesis, Dublin City Uni-
versity.
Dan Roth and Wen-Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of CoNNL 2004.
Anne Schiller, Simone Teufel, and Christine Sto?ckert.
1999. Guidelines fu?r das Tagging deutscher
Textcorpora mit STTS (Kleines und gro?es Tagset).
Technical Report August, Universita?t Stuttgart.
Anne Schiller. 1994. Dmor - user?s guide. Technical
report, University of Stuttgart.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, volume 12. Manchester, UK.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
realizational parsing. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics - COLING ?08, pages 889?896, Morristown, NJ,
USA. Association for Computational Linguistics.
1097
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 55?60,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
ICARUS ? An Extensible Graphical Search Tool
for Dependency Treebanks
Markus Ga?rtner Gregor Thiele Wolfgang Seeker Anders Bjo?rkelund Jonas Kuhn
Institut fu?r Maschinelle Sprachverarbeitung
University of Stuttgart
firstname.lastname@ims.uni-stuttgart.de
Abstract
We present ICARUS, a versatile graphi-
cal search tool to query dependency tree-
banks. Search results can be inspected
both quantitatively and qualitatively by
means of frequency lists, tables, or depen-
dency graphs. ICARUS also ships with
plugins that enable it to interface with tool
chains running either locally or remotely.
1 Introduction
In this paper we present ICARUS1 a search and
visualization tool that primarily targets depen-
dency syntax. The tool has been designed such
that it requires minimal effort to get started with
searching a treebank or system output of an auto-
matic dependency parser, while still allowing for
flexible queries. It enables the user to search de-
pendency treebanks given a variety of constraints,
including searching for particular subtrees. Em-
phasis has been placed on a functionality that
makes it possible for the user to switch back and
forth between a high-level, aggregated view of the
search results and browsing of particular corpus
instances, with an intuitive visualization of the
way in which it matches the query. We believe this
to be an important prerequisite for accessing anno-
tated corpora, especially for non-expert users.
Search queries in ICARUS can be constructed
either in a graphical or a text-based manner. Build-
ing queries graphically removes the overhead of
learning a specialized query language and thus
makes the tool more accessible for a wider audi-
ence. ICARUS provides a very intuitive way of
breaking down the search results in terms of fre-
quency statistics (such as the distribution of part-
of-speech on one child of a particular verb against
the lemma of another child). The dimensions for
1Interactive platform for Corpus Analysis and Research
tools, University of Stuttgart
the frequency break-down are simply specified by
using grouping operators in the query. The fre-
quency tables are filled and updated in real time
as the search proceeds through the corpus ? allow-
ing for a quick detection of misassumptions in the
query.
ICARUS uses a plugin-based architecture that
permits the user to write his own plugins and in-
tegrate them into the system. For example, it
comes with a plugin that interfaces with an exter-
nal parser that can be used to parse a sentence from
within the user interface. The constraints for the
query can then be copy-pasted from the resulting
parse visualization. This facilitates example-based
querying, which is particularly helpful for inexpe-
rienced users ? they do not have to recall details
of the annotation conventions outside of their fo-
cus of interests but can go by what the parser pro-
vides.2
ICARUS is written entirely in Java and runs out
of the box without requiring any installation of
the tool itself or additional libraries. This makes
it platform independent and the only requirement
is that a Java Runtime Environment (JRE) is in-
stalled on the host system. It is open-source and
freely available for download.3
As parsers and other Natural Language Pro-
cessing (NLP) tools are starting to find their way
into other sciences such as (digital) humanities or
social sciences, it gets increasingly important to
provide intuitive visualization tools that integrate
seamlessly with existing NLP tools and are easy
to use also for non-linguists. ICARUS interfaces
readily with NLP tools provided as web services
by CLARIN-D,4 the German incarnation of the
European Infrastructure initiative CLARIN.
2This is of course only practical with rather reliable auto-
matic parsers, but in our experience, the state-of-the-art qual-
ity is sufficient.
3www.ims.uni-stuttgart.de/forschung/
ressourcen/werkzeuge/icarus.en.html
4http://de.clarin.eu
55
The remainder of this paper is structured as fol-
lows: In Section 2 we elaborate on the motivation
for the tool and discuss related work. Section 3
presents a running example of how to build queries
and how results are visualized. In Section 4 we
outline the details of the architecture. Section 5
discusses ongoing work, and Section 6 concludes.
2 Background
Linguistically annotated corpora are among the
most important sources of knowledge for empir-
ical linguistics as well as computational modeling
of natural language. Moreover, for most users the
only way to develop a systematic understanding
of the phenomena in the annotations is through a
process of continuous exploration, which requires
suitable and intuitive tools.
As automatic analysis tools such as syntactic
parsers have reached a high quality standard, ex-
ploration of large collections of auto-parsed cor-
pus material becomes more and more common. Of
course, the querying problem is the same no matter
whether some target annotation was added manu-
ally, as in a treebank, or automatically. Yet, the
strategy changes, as the user will try to make sure
he catches systematic parsing errors and develops
an understanding of how the results he is deal-
ing with come about. While there is no guaran-
teed method for avoiding erroneous matches, we
believe that an easy-to-use transparent querying
mechanism that allows the user to look at the same
or similar results from various angles is the best
possible basis for an informed usage: frequency
tables breaking down the corpus distributions in
different dimensions are a good high-level hint,
and the actual corpus instances should be only one
or two mouse clicks away, presented with a con-
cise visualization of the respective instantiation of
the query constraints.
Syntactic annotations are quite difficult to query
if one is interested in specific constructions that
are not directly encoded in the annotation labels
(which is the case for most interesting phenom-
ena). Several tools have been developed to enable
researchers to do this. However, many of these
tools are designed for constituent trees only.
Dependency syntax has become popular as a
framework for treebanking because it lends itself
naturally to the representation of free word order
phenomena and was thus adopted in the creation of
treebanks for many languages that have less strict
word order, such as the Prague Dependency Tree-
bank for Czech (Hajic? et al, 2000) or SynTagRus
for Russian (Boguslavsky et al, 2000).
A simple tool for visualization of dependency
trees is What?s wrong with my NLP? (Riedel,
2008). Its querying functionality is however lim-
ited to simple string-searching on surface forms. A
somewhat more advanced tool is MaltEval (Nils-
son and Nivre, 2008), which offers a number of
predefined search patterns ranging from part-of-
speech tag to branching degree.
On the other hand, powerful tools such as PML-
TQ (Pajas and S?te?pa?nek, 2009) or INESS (Meurer,
2012) offer expressive query languages and can
facilitate cross-layer queries (e.g., involving both
syntactic and semantic structures). They also
accommodate both constituent and dependency
structures.
In terms of complexity in usage and expressiv-
ity, we believe ICARUS constitutes a middle way
between highly expressive and very simple visu-
alization tools. It is easy to use, requires no in-
stallation, while still having rich query and visual-
ization capabilities. ICARUS is similar to PML-
TQ in that it also allows the user to create queries
graphically. It is also similar to the search tool
GrETEL (Augustinus et al, 2012) as it interfaces
with a parser, allowing the user to create queries
starting from an automatic parse. Thus, queries
can be created without any prior knowledge of the
treebank annotation scheme.
As for searching constituent treebanks, there
is a plethora of existing search tools, such
as TGrep2 (Rohde, 2001), TigerSearch (Lezius,
2002), MonaSearch (Maryns, 2009), and Fangorn
(Ghodke and Bird, 2012), among others. They im-
plement different query languages with varying ef-
ficiency and expressiveness.
3 Introductory Example
Before going into the technical details, we show
an example of what you can do with ICARUS.
Assume that a user is interested in passive con-
structions in English, but does not know exactly
how this is annotated in a treebank. As a first step,
he can use a provided plugin that interfaces with
a tool chain5 to parse a sentence that contains a
passive construction (thus adopting the example-
based querying approach laid out in the introduc-
5using mate-tools by Bohnet (2010); available at
http://code.google.com/p/mate-tools
56
tion). Figure 1 shows the parser interface. In the
lower field, the user entered the sentence. The
other two fields show the output of the parser, once
as a graph and once as a feature value description.
Figure 1: Parsing the sentence ?Mary was kissed
by a boy.? with a predefined tool chain.
In the second step, the user can then mark parts
of the output graph by selecting some nodes and
edges, and have ICARUS construct a query struc-
ture from it, following the drag-and-drop scheme
users are familiar with from typical office soft-
ware. The automatically built query can be man-
ually adjusted by the user (relaxing constraints)
and then be used to search for similar structures
in a treebank. The parsing step can of course be
skipped altogether, and a query can be constructed
by hand right away. Figure 2 shows the query
builder, where the user can define or edit search
graphs graphically in the main window, or enter
them as a query string in the lower window.
Figure 2: Query builder for constructing queries.
For the example, Figure 3 shows the query as it
is automatically constructed by ICARUS from the
partial parse tree (3a), and what it might look like
after the user has changed it (3b). The modified
query matches passive constructions in English, as
annotated in the CoNLL 2008 Shared Task data set
(Surdeanu et al, 2008), which we use here.
(a) automatically extracted (b) manually edited
Figure 3: Search graphs for finding passive con-
structions. (a) was constructed automatically from
the parsed sentence, (b) is a more general version.
The search returns 6,386 matches. Note that
the query (Figure 3b) contains a <*>-expression.
This grouping operator groups the results accord-
ing to the specified dimension, in this case by the
lemma of the passivized verb. Figure 4 shows
the result view. On the left, a list of lemmas is
presented, sorted by frequency. Clicking on the
lemma displays the list of matches containing that
particular lemma on the right side. The match-
ing sentences can then be browsed, with the active
sentence also being shown as a tree. Note that the
instantiation of the query constraints is highlighted
in the tree display.
Figure 4: Passive constructions in the treebank
grouped by lemma and sorted by frequency.
The query could be further refined to restrict it
to passives with an overt logical subject, using a
more complex search graph for the by-phrase and
a second instance of the grouping operator. The
results will then also be grouped by the lemma of
the logical subject, and are therefore presented as
a two-dimensional table. Figure 5 shows the new
query and the resulting view. The user is presented
with a frequency table, where each cell contains
the number of hits for this particular combination
of verb lemma and logical subject. Clicking on
the cell opens up a view similar to the right part of
Figure 4 where the user can then again browse the
actual trees.
57
Figure 5: Search graph and result view for passive
constructions with overt logical subjects, grouped
by lemma of the verb and the lemma of the logical
subject.
Finally, we can add a third grouping operator.
Figure 6 shows a further refined query for passives
with an overt logical subject and an object. In the
results, the user is presented with a list of values
for the first grouping operator to the left. Clicking
on one item in that list opens up a table on the right
presenting the other two dimensions of the query.
Figure 6: Search graph and result view for passive
constructions with an overt logical subject and an
object, grouped by lemma of the verb, the logical
subject, and the object.
This example demonstrates a typical use case
for a user that is interested in certain linguistic
constructions in his corpus. Creating the search
graph and interpreting the results does not re-
quire any specialized knowledge other than fa-
miliarity with the annotation of the corpus being
searched. It especially does not require any pro-
gramming skills, and the possibility to graphically
build a query obviates the need to learn a special-
ized query language.
4 Architecture
This section goes into more details about the in-
ner workings of ICARUS. A main component
is the search engine, which enables the user to
quickly search treebanks for whatever he is inter-
ested in. A second important feature of ICARUS
is the plugin-based architecture, which allows for
the definition of custom extensions. Currently,
ICARUS can read the commonly used CoNLL de-
pendency formats, and it is easy to write exten-
sions in order to add additional formats.
4.1 Search Engine and Query Builder
ICARUS has a tree-based search engine for tree-
banks, and includes a graphical query builder.
Structure and appearance of search graphs are sim-
ilar to the design used for displaying dependency
trees (cf. Figure 1), which is realized with the
open-source library JGraph.6 Queries and/or their
results can be saved to disk and later reloaded for
further processing.
Defining a query graphically basically amounts
to drawing a partial graph structure that defines
the type of structure that the user is interested in.
In practice, this is done by creating nodes in the
query builder and connecting them by edges. The
nodes correspond to words in the dependency trees
of the treebank. Several features like word iden-
tity, lemma, part of speech, etc. can be specified
for each node in the search graph in order to re-
strict the query. Dominance and precedence con-
straints over a set of nodes can be defined by sim-
ply linking nodes with the appropriate edge type.
Edges can be further specified for relation type,
distance, direction, projectivity, and transitivity. A
simple example is shown in Figures 2 and 3. The
search engine supports regular expressions for all
string-properties (form, lemma, part of speech, re-
lation). It also supports negation of (existence of)
nodes and edges, and their properties.
As an alternative to the search graph, the user
can also specify the query in a text-based format
by constructing a comma separated collection of
constraints in the form of key=value pairs for a
single node contained within square brackets. Hi-
erarchical structures are expressed by nesting their
textual representation. Figure 7 shows the text-
based form of the three queries used in the exam-
ples in Section 3.
6http://www.jgraph.com/
58
Query 1: [lemma=be[pos=VBN,lemma=<*>,rel=VC]]
Query 2: [lemma=be[pos=VBN,lemma=<*>,rel=VC[form=by,rel=LGS[lemma=<*>,rel=PMOD]]]]
Query 3: [lemma=be[pos=VBN,lemma=<*>,rel=VC[form=by,rel=LGS[lemma=<*>,rel=PMOD]]
[lemma=<*>,rel=OBJ]]]
Figure 7: Text representation of the three queries used in the example in Section 3.
A central feature of the query language is the
grouping operator (<*>), which will match any
value and cause the search engine to group result
entries by the actual instance of the property de-
clared to be grouped. The results of the search
will then be visualized as a list of instances to-
gether with their respective frequencies. Results
can be sorted alphabetically or by frequency (ab-
solute or relative counts) . Depending on the num-
ber of grouping operators used (up to a maximum
of three) the result is structured as a list of fre-
quencies (cf. Figure 4), a table of frequencies for
pairs of instances (cf. Figure 5), or a list where
each item then opens up a table of frequency re-
sults (cf. Figure 6). In the search graph and the
result view, different colors are used to distinguish
between different grouping operators.
The ICARUS search engine offers three differ-
ent search modes:
Sentence-based. Sentence based search stops at
the first successful hit in a sentence and returns
every sentence on a list of results at most once.
Exhaustive sentence-based. The exhaustive
sentence-based search mode extends the sentence
based search by the possibility of processing mul-
tiple hits within a single sentence. Every sentence
with at least one hit is returned exactly once. In the
result view, the user can then browse the different
hits found in one sentence.
Hit-based. Every successful hit is returned sepa-
rately on the corresponding list of results.
When a query is issued, the search results are
displayed on the fly as the search engine is pro-
cessing the treebank. The sentences can be ren-
dered in one of two ways: either as a tree, where
nodes are arranged vertically by depth in the tree,
or horizontally with all the nodes arranged side-
by-side. If a tree does not fit on the screen, part of
it is automatically collapsed but can be expanded
again by the user.
4.2 Extensibility
ICARUS relies on the Java Plugin Framework,7
which provides a powerful XML-based frame-
7http://jpf.sourceforge.net/
work for defining plugins similarly to the engine
used by the popular Eclipse IDE project. The
plugin-based architecture makes it possible for
anybody to write extensions to ICARUS that are
specialized for a particular task. The parser inte-
gration of mate-tools demonstrated in Section 3 is
an example for such an extension.
The plugin system facilitates custom extensions
that make it possible to intercept certain stages
of an ongoing search process and interact with it.
This makes it possible for external tools to pre-
process search data and apply additional annota-
tions and/or filtering, or even make use of exist-
ing indices by using search constraints to limit the
amount of data passed to the search engine. With
this general setup, it is for example possible to eas-
ily extend ICARUS to work with constituent trees.
ICARUS comes with a dedicated plugin that
enables access to web services provided by
CLARIN-D. The project aims to provide tools and
services for language-centered research in the hu-
manities and social sciences. In contrast to the in-
tegration of, e.g., mate-tools, where the tool chain
is executed locally, the user can define a tool chain
by chaining several web services (e.g., lemmatiz-
ers, part-of-speech taggers etc.) together and ap-
ply them to his own data. To do this, ICARUS
is able to read and write the TCF exchange for-
mat (Heid et al, 2010) that is used by CLARIN-D
web services. The output can then be inspected
and searched using ICARUS. As new NLP tools
are added as CLARIN-D web services they can be
immediately employed by ICARUS.
5 Upcoming Extensions
An upcoming release includes the following ex-
tensions:
? Currently, treebanks are assumed to fit into
the executing computer?s main memory.
The new implementation will support asyn-
chronous loading of data, with notifications
passed to the query engine or a plugin when
required data is available. Treebanks with
millions of entries can then be loaded in less
59
memory consuming chunks, thus keeping the
system responsive when access is requested.
? The search engine is being extended with an
operator that allows disjunctions of queries.
This will enable the user to aggregate fre-
quency output over multiple queries.
6 Conclusion
We have presented ICARUS, a versatile and user-
friendly search and visualization tool for depen-
dency trees. It is aimed not only at (computa-
tional) linguists, but also at people from other dis-
ciplines, e.g., the humanities or social sciences,
who work with language data. It lets the user
create queries graphically and returns results (1)
quantitatively by means of frequency lists and ta-
bles as well as (2) qualitatively by connecting the
statistics to the matching sentences and allowing
the user to browse them graphically. Its plugin-
based architecture enables it to interface for exam-
ple with external processing pipelines, which lets
the user apply processing tools directly from the
user interface.
In the future, specialized plugins are planned
to work with different linguistic annotations, e.g.
cross-sentence annotations as used to annotate
coreference chains. Additionally, a plugin is in-
tended that interfaces the search engine with a
database.
Acknowledgments
This work was funded by the Deutsche
Forschungsgemeinschaft (DFG) via the SFB
732 ?Incremental Specification in Context?,
project D8, and by the Bundesministerium fu?r
Bildung und Forschung (BMBF) via project No.
01UG1120F, CLARIN-D center Stuttgart. The
authors are also indebted to Andre? Blessing and
Heike Zinsmeister for reading an earlier draft of
this paper.
References
Liesbeth Augustinus, Vincent Vandeghinste, and
Frank Van Eynde. 2012. Example-based Treebank
Querying. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC?12), Istanbul, Turkey. ELRA.
Igor Boguslavsky, Svetlana Grigorieva, Nikolai Grig-
oriev, Leonid Kreidlin, and Nadezhda Frid. 2000.
Dependency Treebank for Russian: Concept, Tools,
Types of Information. In COLING 2000, pages
987?991, Saarbru?cken, Germany.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In COLING
2010, pages 89?97, Beijing, China.
Sumukh Ghodke and Steven Bird. 2012. Fangorn: A
System for Querying very large Treebanks. In COL-
ING 2012: Demonstration Papers, pages 175?182,
Mumbai, India.
Jan Hajic?, Alena Bo?hmova?, Eva Hajic?ova?, and Barbora
Vidova?-Hladka?. 2000. The Prague Dependency
Treebank: A Three-Level Annotation Scenario. In
Treebanks: Building and Using Parsed Corpora,
pages 103?127. Amsterdam:Kluwer.
Ulrich Heid, Helmut Schmid, Kerstin Eckart, and Er-
hard Hinrichs. 2010. A Corpus Representation For-
mat for Linguistic Web Services: The D-SPIN Text
Corpus Format and its Relationship with ISO Stan-
dards. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta. ELRA.
Wolfgang Lezius. 2002. Ein Suchwerkzeug fu?r syn-
taktisch annotierte Textkorpora. Ph.D. thesis, IMS,
University of Stuttgart.
Hendrik Maryns. 2009. MonaSearch ? A Tool for
Querying Linguistic Treebanks. In Proceedings of
TLT 2009, Groningen.
Paul Meurer. 2012. INESS-Search: A Search System
for LFG (and Other) Treebanks. In Miriam Butt and
Tracy Holloway King, editors, Proceedings of the
LFG2012 Conference. CSLI Publications.
Jens Nilsson and Joakim Nivre. 2008. MaltEval: an
Evaluation and Visualization Tool for Dependency
Parsing. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC?08), Marrakech, Morocco. ELRA.
Petr Pajas and Jan S?te?pa?nek. 2009. System for Query-
ing Syntactically Annotated Corpora. In Proceed-
ings of the ACL-IJCNLP 2009 Software Demonstra-
tions, pages 33?36, Suntec, Singapore. Association
for Computational Linguistics.
Sebastian Riedel. 2008. What?s Wrong With My
NLP?
http://code.google.com/p/
whatswrong/.
Douglas L.T. Rohde. 2001. TGrep2 the next-
generation search engine for parse trees.
http://tedlab.mit.edu/?dr/Tgrep2/.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL 2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In CoNLL 2008,
pages 159?177, Manchester, England.
60
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 7?12,
Baltimore, Maryland USA, June 23-24, 2014.
c
?2014 Association for Computational Linguistics
Visualization, Search, and Error Analysis for Coreference Annotations
Markus G
?
artner Anders Bj
?
orkelund Gregor Thiele Wolfgang Seeker Jonas Kuhn
Institute for Natural Language Processing
University of Stuttgart
{thielegr,seeker,gaertnms,anders,kuhn}@ims.uni-stuttgart.de
Abstract
We present the ICARUS Coreference Ex-
plorer, an interactive tool to browse and
search coreference-annotated data. It can
display coreference annotations as a tree,
as an entity grid, or in a standard text-
based display mode, and lets the user
switch freely between the different modes.
The tool can compare two different an-
notations on the same document, allow-
ing system developers to evaluate errors in
automatic system predictions. It features
a flexible search engine, which enables
the user to graphically construct search
queries over sets of documents annotated
with coreference.
1 Introduction
Coreference resolution is the task of automatically
grouping references to the same real-world entity
in a document into a set. It is an active topic in cur-
rent NLP research and has received considerable
attention in recent years, including the 2011 and
2012 CoNLL shared tasks (Pradhan et al., 2011;
Pradhan et al., 2012).
Coreference relations are commonly repre-
sented by sets of mentions, where all mentions
in one set (or coreference cluster) are considered
coreferent. This type of representation does not
support any internal structure within the clusters.
However, many automatic coreference resolvers
establish links between pairs of mentions which
are subsequently transformed to a cluster by tak-
ing the transitive closure over all links, i.e., placing
all mentions that are directly or transitively classi-
fied as coreferent in one cluster. This is particu-
larly the case for several state-of-the-art resolvers
(Fernandes et al., 2012; Durrett and Klein, 2013;
Bj?orkelund and Kuhn, 2014). These pairwise de-
cisions, which give rise to a clustering, can be ex-
ploited for detailed error analysis and more fine-
grained search queries on data automatically an-
notated for coreference.
We present the ICARUS Coreference Explorer
(ICE), an interactive tool to browse and search
coreference-annotated data. In addition to stan-
dard text-based display modes, ICE features two
other display modes: an entity-grid (Barzilay and
Lapata, 2008) and a tree view, which makes use
of the internal pairwise links within the clusters.
ICE builds on ICARUS (G?artner et al., 2013), a
platform for search and exploration of dependency
treebanks.
1
ICE is geared towards two (typically) distinct
users: The NLP developer who designs corefer-
ence resolution systems can inspect the predic-
tions of his system using the three different dis-
play modes. Moreover, ICE can compare the pre-
dictions of a system to a gold standard annotation,
enabling the developer to inspect system errors in-
teractively. The second potential user is the cor-
pus linguist, who might be interested in brows-
ing or searching a document, or a (large) set of
documents for certain coreference relations. The
built-in search engine of ICARUS now also allows
search queries over sets of documents in order to
meet the needs of this type of user.
2 Data Representation
ICE reads the formats used in the 2011 and 2012
CoNLL shared tasks as well as the SemEval 2010
format (Recasens et al., 2010).
2
Since these for-
mats cannot accommodate pairwise links, an aux-
iliary file with standoff annotation can be pro-
vided, which we call allocation. An allocation is a
list of pairwise links between mentions. Multiple
1
ICE is written in Java and is therefore platform indepen-
dent. It is open source (under GNU GPL) and we provide
both sources and binaries for download on http://www.
ims.uni-stuttgart.de/data/icarus.html
2
These two formats are very similar tabular formats, but
differ slightly in the column representations.
7
allocations can be associated with a single docu-
ment and the user can select one of these for dis-
play or search queries. An allocation can also in-
clude properties on mentions and links. The set
of possible properties is not constrained, and the
user can freely specify properties as a list of key-
value pairs. Properties on mentions may include,
e.g., grammatical gender or number, or informa-
tion status labels. Additionally, a special property
that indicates the head word of a mention can be
provided in an allocation. The head property en-
ables the user to access head words of mentions
for display or search queries.
The motivation for keeping the allocation file
separate from the CoNLL or SemEval files is two-
fold: First, it allows ICE to work without hav-
ing to provide an allocation file, thereby making it
easy to use with the established formats for coref-
erence. The user is still able to introduce addi-
tional structure by the use of the allocation file.
Second, multiple allocation files allow the user to
switch between different allocations while explor-
ing a set of documents. Moreover, as we will see
in Section 3.3, ICE can also compare two different
allocations in order to highlight the differences.
In addition to user-specified allocations, ICE
will always by default provide an internal structure
for the clusters, in which the correct antecedent
of every mention is the closest coreferent mention
with respect to the linear order of the document
(this is equivalent to the training instance creation
heuristic proposed by Soon et al. (2001)). There-
fore, the user is not required to define an allocation
on their own.
3 Display Modes
In this section we describe the entity grid and tree
display modes by means of screenshots. ICE addi-
tionally includes a standard text-based view, sim-
ilar to other coreference visualization tools. The
example document is taken from the CoNLL 2012
development set (Pradhan et al., 2012) and we
use two allocations: (1) the predictions output by
Bj?orkelund and Kuhn (2014) system (predicted)
and (2) a gold allocation that was obtained by
running the same system in a restricted setting,
where only links between coreferent mentions are
allowed (gold). The complete document can be
seen in the lower half of Figure 1.
3.1 Entity grid
Barzilay and Lapata (2008) introduce the entity
grid, a tabular view of entities in a document.
Specifically, rows of the grid correspond to sen-
tences, and columns to entities. The cells of the ta-
ble are used to indicate that an entity is mentioned
in the corresponding sentence. Entity grids pro-
vide a compact view on the distribution of men-
tions in a document and allow the user to see how
the description of an entity changes from mention
to mention.
Figure 1 shows ICE?s entity-grid view for the
example document using the predicted allocation.
When clicking on a cell in the entity grid the im-
mediate textual context of the cell is shown in the
lower pane. In Figure 1, the cell with the blue
background has been clicked, which corresponds
to the two mentions firms from Taiwan and they.
These mentions are thus highlighted in the lower
pane. The user can also right-click on a cell and
jump straight to the tree view, centered around the
same mentions.
3.2 Label Patterns
The information that is displayed in the cells of
the entity grid (and also on the nodes in the tree
view, see Section 3.3) can be fully customized by
the user. The customization is achieved by defin-
ing label patterns. A label pattern is a string that
specifies the format according to which a mention
will be displayed. The pattern can extract infor-
mation on a mention according to three axes: (1)
at the token- level for the full mention, extracting,
e.g., the sequence of surface forms or the part-of-
speech tags of a mention; (2) at the mention- level,
extracting an arbitrary property of a mention as de-
fined in an allocation; (3) token-level information
from the head word of a mention.
Label patterns can be defined interactively
while displaying a document and the three axes are
referenced by dedicated operators. For instance,
the label pattern $form$ extracts the full surface
form of a mention, whereas #form# only extracts
the surface form of the head word of a mention.
All properties defined by the user in the allocation
(see Section 2) are accessible via label patterns.
For example, the allocations we use for Fig-
ure 1 include a number of properties on the
mentions, most of which are internally com-
puted by the coreference system: The TYPE of
a mention, which can take any of the values
8
Figure 1: Entity grid over the predicted clustering in the example document.
{Name, Common, Pronoun} and is inferred from
the part- of-speech tags in the CoNLL file; The
grammatical NUMBER of a mention, which is as-
signed based on the number and gender data com-
piled by Bergsma and Lin (2006) and can take
the values {Sin, Plu, Unknown}. The label pat-
tern for displaying the number property associated
with a mention would be %Number%.
The label pattern used in Figure 1 is defined
as ("$form$" - %Type% - %Number%). This pat-
tern accesses the full surface form of the mentions
($form$), as well as the TYPE (%Type%) and gram-
matical NUMBER (%Number%) properties defined
in the allocation file.
Custom properties and label patterns can be
used for example to display the entity grid in the
form proposed by Barzilay and Lapata (2008): In
the allocation, we assign a coarse-grained gram-
matical function property (denoted GF) to every
mention, where each mention is tagged as either
subject, object, or other (denoted S, O, X, respec-
tively).
3
The label pattern %GF% then displays the
grammatical function of each mention in the entity
grid, as shown in Figure 2.
3.3 Tree view
Pairwise links output by an automatic coreference
system can be treated as arcs in a directed graph.
Linking the first mention of each cluster to an ar-
tificial root node creates a tree structure that en-
codes the entire clustering in a document. This
representation has been used in coreference re-
3
The grammatical function was assigned by converting
the phrase-structure trees in the CoNLL file (which lack
grammatical function information) to Stanford dependencies
(de Marneffe and Manning, 2008), and then extracting the
grammatical function from the head word in each mention.
Figure 2: Example entity grid, using the labels by
Barzilay and Lapata (2008).
solvers (Fernandes et al., 2012; Bj?orkelund and
Kuhn, 2014), but ICE uses it to display links be-
tween mentions introduced by an automatic (pair-
wise) resolver.
Figure 3 shows three examples of the tree view
of the same document as before: The gold allo-
cation (3a), the predicted allocation (3b), as well
as the differential view, where the two allocations
are compared (3c). Each mention corresponds to
a node in the trees and all mentions are directly or
transitively dominated by the artificial root node.
Every subtree under the root constitutes its own
cluster and a solid arc between two mentions de-
notes that the two mentions are coreferent accord-
ing to a coreference allocation. The information
displayed in the nodes of the tree can be cus-
tomized using label patterns.
In the differential view (Figure 3c), solid arcs
correspond to the predicted allocation. Dashed
nodes and arcs are present in the gold allocation,
but not in the prediction. Discrepancies between
the predicted and the gold allocations are marked
9
(a) Tree representing the gold allocation. (b) Tree representing the predicted allocation.
(c) Differential view displaying the difference between the gold and predicted allocations.
Figure 3: Tree view over the example document (gold, predicted, differential).
with different colors denoting different types of er-
rors. The example in Figure 3c contains two errors
made by the system:
1. A false negative mention, denoted by the
dashed red node Shangtou. In the gold
standard (Figure 3a) this mention is clus-
tered with other mentions such as Shantou ?s,
Shantou City, etc. The dashed arc between
Shantou ?s and Shangtou is taken from the
gold allocation, and indicates what the sys-
tem prediction should have been like.
4
2. A foreign antecedent, denoted by the solid
orange arc between Shantou ?s new high level
technology development zone and Shantou.
In this case, the coreference system erro-
neously clustered these two mentions. The
correct antecedent is indicated by the dashed
arc that originates from the document root.
4
This error likely stems from the fact that Shantou is
spelled two different ways within the same document which
causes the resolver?s string-matching feature to fail.
This error is particularly interesting since the
system effectively merges the two clusters
corresponding to Shantou and Shantou? s new
high level technology development zone. The
tree view, however, shows that the error stems
from a single link between these two men-
tions, and that the developer needs to address
this.
Since the tree-based view makes pairwise de-
cisions explicit, the differential view shown in
Figure 3c is more informative to NLP develop-
ers when inspecting errors by automatic system
than comparing a gold standard clustering to a pre-
dicted one. The problem with analyzing the error
on clusterings instead of trees is that the clusters
would be merged, i.e., it is not clear where the ac-
tual mistake was made.
Additional error types not illustrated by Fig-
ure 3c include false positive mentions, where
the system invents a mention that is not part
of the gold allocation. When a false positive
mention is assigned as an antecedent of another
10
mention, the corresponding link is marked as an
invented antecedent. Links that erroneously start
a new cluster when it is coreferent with other men-
tions to the left is marked as false new.
4 Searching
The search engine in ICE makes the annotations
in the documents searchable for, e. g., a corpus lin-
guist who is interested in specific coreference phe-
nomena. It allows the user to express queries over
mentions related through the tree. Queries can ac-
cess the different layers of annotation, both from
the allocation file and the underlying document,
using various constructs such as, e.g., transitivity,
regular expressions, and/or disjunctions. The user
can construct queries either textually (through a
query language) or graphically (by creating nodes
and configuring constraints in dialogues). For a
further discussion of the search engine we refer to
the original ICARUS paper (G?artner et al., 2013).
Figure 4 shows a query that matches cataphoric
pronouns, i.e., pronouns that precede their an-
tecedents. The figure shows the query expressed
as a subgraph (on the left) and the corresponding
results (right) obtained on the development set of
the English CoNLL 2012 data using the manual
annotation represented in the gold allocation.
The query matches two mentions that are di-
rectly or transitively connected through the graph.
The first mention (red node) matches mentions of
the type Pronoun that have to be attached to the
document root node. In the tree formalism we
adopt, this implies that it must be the first men-
tion of its cluster. The second mention (green
node) matches any mention that is not of the type
Pronoun.
(a)
(b)
Figure 4: Example search query and correspond-
ing results.
The search results are grouped along two axes:
the surface form of the head word of the first (red)
node, and the type property of the second mention
(green node), indicated by the special grouping
operator <
*
> inside the boxes. The correspond-
ing results are shown in the right half of Figure 4,
where the first group (surface form) runs verti-
cally, and the second group (mention type) runs
horizontally. The number of hits for each configu-
ration is shown in the corresponding cell. For ex-
ample, the case that the first mention of a chain is
the pronoun I and the closest following coreferent
mention that is not a pronoun is of type Common,
occurs 6 times. By clicking on a cell, the user can
jump straight to a list of the matches, and browse
them using any of the three display modes.
5 Related Work
Two popular annotation and visualization tools
for coreference are PAlinkA (Or?asan, 2003) and
MMAX2 (M?uller and Strube, 2006), which fo-
cus on a (customizable) textual visualization with
highlighting of clusters. The TrED (Pajas and
?
St?ep?anek, 2009) project is a very flexible multi-
level annotation tool centered around tree-based
annotations that can be used to annotate and vi-
sualize coreference. It also features a powerful
search engine. Recent annotation tools include the
web-based BRAT (Stenetorp et al., 2012) and its
extension WebAnno (Yimam et al., 2013). A ded-
icated query and exploration tool for multi-level
annotations is ANNIS (Zeldes et al., 2009).
The aforementioned tools are primarily meant
as annotation tools. They have a tendency of lock-
ing the user into one type of visualization (tree- or
text-based), while often lacking advanced search
functionality. In contrast to them, ICE is not meant
to be yet another annotation tool, but was designed
as a dedicated coreference exploration tool, which
enables the user to swiftly switch between differ-
ent views. Moreover, none of the existing tools
provide an entity-grid view.
ICE is also the only tool that can graphically
compare predictions of a system to a gold standard
with a fine-grained distinction on the types of dif-
ferences. Kummerfeld and Klein (2013) present
an algorithm that transforms a predicted corefer-
ence clustering into a gold clustering and records
the necessary transformations, thereby quantify-
ing different types of errors. However, their algo-
rithm only works on clusterings (sets of mentions),
not pairwise links, and is therefore not able to pin-
point some of the mistakes that ICE can (such as
the foreign antecedent described in Section 3).
11
6 Conclusion
We presented ICE, a flexible coreference visual-
ization and search tool. The tool complements
standard text-based display modes with entity-grid
and tree visualizations. It is also able to dis-
play discrepancies between two different corefer-
ence annotations on the same document, allow-
ing NLP developers to debug coreference sys-
tems in a graphical way. The built-in search en-
gine allows corpus linguists to construct complex
search queries and provide aggregate result views
over large sets of documents. Being based on the
ICARUS platform?s plugin-engine, ICE is extensi-
ble and can easily be extended to cover additional
data formats.
Acknowledgments
This work was funded by the German Federal
Ministry of Education and Research (BMBF) via
CLARIN-D, No. 01UG1120F and the German
Research Foundation (DFG) via the SFB 732,
project D8.
References
Regina Barzilay and Mirella Lapata. 2008. Model-
ing Local Coherence: An Entity-Based Approach.
Computational Linguistics, 34(1):1?34.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In COLING-ACL,
pages 33?40, Sydney, Australia, July.
Anders Bj?orkelund and Jonas Kuhn. 2014. Learning
Structured Perceptrons for Coreference Resolution
with Latent Antecedents and Non-local Features. In
ACL, Baltimore, MD, USA, June.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies
representation. In COLING Workshop on Cross-
framework and Cross-domain Parser Evaluation.
Greg Durrett and Dan Klein. 2013. Easy Victo-
ries and Uphill Battles in Coreference Resolution.
In EMNLP, pages 1971?1982, Seattle, Washington,
USA, October.
Eraldo Fernandes, C??cero dos Santos, and Ruy Milidi?u.
2012. Latent Structure Perceptron with Feature In-
duction for Unrestricted Coreference Resolution. In
EMNLP-CoNLL: Shared Task, pages 41?48, Jeju Is-
land, Korea, July.
Markus G?artner, Gregor Thiele, Wolfgang Seeker, An-
ders Bj?orkelund, and Jonas Kuhn. 2013. ICARUS
? An Extensible Graphical Search Tool for Depen-
dency Treebanks. In ACL: System Demonstrations,
pages 55?60, Sofia, Bulgaria, August.
Jonathan K. Kummerfeld and Dan Klein. 2013. Error-
Driven Analysis of Challenges in Coreference Res-
olution. In EMNLP, pages 265?277, Seattle, Wash-
ington, USA, October.
Christoph M?uller and Michael Strube. 2006. Multi-
level annotation of linguistic data with MMAX2. In
Corpus Technology and Language Pedagogy: New
Resources, New Tools, New Methods, pages 197?
214. Peter Lang.
Constantin Or?asan. 2003. PALinkA: A highly cus-
tomisable tool for discourse annotation. In Akira
Kurematsu, Alexander Rudnicky, and Syun Tutiya,
editors, Proceedings of the Fourth SIGdial Work-
shop on Discourse and Dialogue, pages 39?43.
Petr Pajas and Jan
?
St?ep?anek. 2009. System for
Querying Syntactically Annotated Corpora. In ACL-
IJCNLP: Software Demonstrations, pages 33?36,
Suntec, Singapore.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 Shared Task: Modeling
Unrestricted Coreference in OntoNotes. In CoNLL:
Shared Task, pages 1?27, Portland, Oregon, USA,
June.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling Multilingual Unre-
stricted Coreference in OntoNotes. In EMNLP-
CoNLL: Shared Task, pages 1?40, Jeju Island, Ko-
rea, July.
Marta Recasens, Llu??s M`arquez, Emili Sapena,
M. Ant`onia Mart??, Mariona Taul?e, V?eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
1?8, Uppsala, Sweden, July.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a Web-based Tool for NLP-Assisted
Text Annotation. In EACL: Demonstrations, pages
102?107, April.
Seid Muhie Yimam, Iryna Gurevych, Richard
Eckart de Castilho, and Chris Biemann. 2013.
WebAnno: A Flexible, Web-based and Visually
Supported System for Distributed Annotations. In
ACL: System Demonstrations, pages 1?6, August.
Amir Zeldes, Julia Ritz, Anke L?udeling, and Christian
Chiarcos. 2009. ANNIS: a search tool for multi-
layer annotated corpora. In Proceedings of Corpus
Linguistics.
12
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 135?145,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
(Re)ranking Meets Morphosyntax: State-of-the-art Results
from the SPMRL 2013 Shared Task?
Anders Bjo?rkelund?, O?zlem C?etinog?lu?, Richa?rd Farkas?, Thomas Mu?ller??, and Wolfgang Seeker?
?Institute for Natural Language Processing , University of Stuttgart, Germany
?Department of Informatics, University of Szeged, Hungary
?Center for Information and Language Processing, University of Munich, Germany
{anders,ozlem,muellets,seeker}@ims.uni-stuttgart.de
rfarkas@inf.u-szeged.hu
Abstract
This paper describes the IMS-SZEGED-CIS
contribution to the SPMRL 2013 Shared Task.
We participate in both the constituency and
dependency tracks, and achieve state-of-the-
art for all languages. For both tracks we make
significant improvements through high quality
preprocessing and (re)ranking on top of strong
baselines. Our system came out first for both
tracks.
1 Introduction
In this paper, we present our contribution to the 2013
Shared Task on Parsing Morphologically Rich Lan-
guages (MRLs). MRLs pose a number of interesting
challenges to today?s standard parsing algorithms,
for example a free word order and, due to their rich
morphology, greater lexical variation that aggravates
out-of-vocabulary problems considerably (Tsarfaty
et al, 2010).
Given the wide range of languages encompassed
by the term MRL, there is, as of yet, no clear con-
sensus on what approaches and features are gener-
ally important for parsing MRLs. However, devel-
oping tailored solutions for each language is time-
consuming and requires a good understanding of
the language in question. In our contribution to the
SPMRL 2013 Shared Task (Seddah et al, 2013), we
therefore chose an approach that we could apply to
all languages in the Shared Task, but that would also
allow us to fine-tune it for individual languages by
varying certain components.
?Authors in alphabetical order.
For the dependency track, we combined the n-
best output of multiple parsers and subsequently
ranked them to obtain the best parse. While this
approach has been studied for constituency parsing
(Zhang et al, 2009; Johnson and Ural, 2010; Wang
and Zong, 2011), it is, to our knowledge, the first
time this has been applied successfully within de-
pendency parsing. We experimented with different
kinds of features in the ranker and developed fea-
ture models for each language. Our system ranked
first out of seven systems for all languages except
French.
For the constituency track, we experimented
with an alternative way of handling unknown words
and applied a products of Context Free Grammars
with Latent Annotations (PCFG-LA) (Petrov et al,
2006), whose output was reranked to select the best
analysis. The additional reranking step improved
results for all languages. Our system beats vari-
ous baselines provided by the organizers for all lan-
guages. Unfortunately, no one else participated in
this track.
For both settings, we made an effort to automat-
ically annotate our data with the best possible pre-
processing (POS, morphological information). We
used a multi-layered CRF (Mu?ller et al, 2013) to
annotate each data set, stacking with the information
provided by the organizers when this was beneficial.
The high quality of our preprocessing considerably
improved the performance of our systems.
The Shared Task involved a variety of settings as
to whether gold or predicted part-of-speech tags and
morphological information were available, as well
as whether the full training set or a smaller (5k sen-
135
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
MarMoT 97.38/92.22 97.02/87.08 97.61/90.92 98.10/91.80 97.09/97.67 98.72/97.59 94.03/87.68 98.12/90.84 97.27/97.13
Stacked 98.23/89.05 98.56/92.63 97.83/97.62
Table 1: POS/morphological feature accuracies on the development sets.
tences) training set was used for training. Through-
out this paper we focus on the settings with pre-
dicted preprocessing information with gold segmen-
tation and the full1 training sets. Unless stated other-
wise, all given numbers are drawn from experiments
in this setting. For all other settings, we refer the
reader to the Shared Task overview paper (Seddah et
al., 2013).
The remainder of the paper is structured as fol-
lows: We present our preprocessing in Section 2 and
afterwards describe both our systems for the con-
stituency (Section 3) and for the dependency tracks
(Section 4). Section 5 discusses the results on the
Shared Task test sets. We conclude with Section 6.
2 Preprocessing
We first spent some time on preparing the data sets,
in particular we automatically annotated the data
with high-quality POS and morphological informa-
tion. We consider this kind of preprocessing to be an
essential part of a parsing system, since the quality
of the automatic preprocessing strongly affects the
performance of the parsers.
Because our tools work on CoNLL09 format, we
first converted the training data from the CoNLL06
format to CoNLL09. We thus had to decide whether
to use coarse or fine part-of-speech (POS) tags. In
a preliminary experiment we found that fine tags are
the better option for all languages but Basque and
Korean. For Korean the reason seems to be that the
fine tag set is huge (> 900) and that the same infor-
mation is also provided in the feature column.
We predict POS tags and morphological features
jointly using the Conditional Random Field (CRF)
tagger MarMoT2 (Mu?ller et al, 2013).
MarMoT incrementally creates forward-
backward lattices of increasing order to prune
the sizable space of possible morphological analy-
ses. We use MarMoT with the default parameters.
1Although, for Hebrew and Swedish only 5k sentences were
available for training, and the two settings thus coincide.
2https://code.google.com/p/cistern/
Since morphological dictionaries can improve au-
tomatic POS tagging considerably, we also created
such dictionaries for each language. For this, we an-
alyzed the word forms provided in the data sets with
language-specific morphological analyzers except
for Hebrew and German where we just extracted the
morphological information from the lattice files pro-
vided by the organizers. For the other languages
we used the following tools: Arabic: AraMorph
a reimplementation of Buckwalter (2002), Basque:
Apertium (Forcada et al, 2011), French: an IMS
internal tool,3 Hungarian: Magyarlanc (Zsibrita et
al., 2013), Korean: HanNanum (Park et al, 2010),
Polish: Morfeusz (Wolin?ski, 2006), and Swedish:
Granska (Domeij et al, 2000).
The created dictionaries were shared with the
other Shared Task participants. We used these dic-
tionaries as additional features for MarMoT.
For some languages we also integrated the pre-
dicted tags provided by the organizers into the fea-
ture model. These stacked models gave improve-
ments for Swedish, Polish and Basque (cf. Table 1
for accuracies).
For the full setting the training data was annotated
using 5-fold jackknifing. In the 5k setting, we addi-
tionally added all sentences not present in the parser
training data to the training data sets of the tagger.
This is similar to the predicted 5k files provided by
the organizers, where more training data than the 5k
was also used for prediction.
Table 3 presents a comparison between our graph-
based baseline parser using the preprocessing ex-
plained in this section (denoted mate) and the
preprocessing provided by the organizers (denoted
mate?). Our preprocessing yields improvements
for all languages but Swedish. The worse perfor-
mance for Swedish is due to the fact that the pre-
dictions provided by the organizers were produced
by models that were trained on a much larger data
3The French morphology was written by Zhenxia Zhou,
Max Kisselew and Helmut Schmid. It is an extension of Zhou
(2007) and implemented in SFST (Schmid, 2005).
136
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Berkeley 78.24 69.17 79.74 81.74 87.83 83.90 70.97 84.11 74.50
Replaced 78.70 84.33 79.68 82.74 89.55 89.08 82.84 87.12 75.52
Product 80.30 86.21 81.42 84.56 90.49 89.80 84.15 88.32 79.25
Reranked 81.24 87.35 82.49 85.01 90.49 91.07 84.63 88.40 79.53
Table 2: PARSEVAL scores on the development sets.
set. The comparison with other parsers demonstrates
that for some languages (e.g., Hebrew or Korean)
the improvements due to better preprocessing can
be greater than the improvements due to a better
parser. For instance, for Hebrew the parser trained
on the provided preprocessing is more than three
points (LAS) behind the three parsers trained on
our own preprocessing. However, the difference be-
tween these three parsers is less than a point.
3 Constituency Parsing
The phrase structure parsing pipeline is based on
products of Context Free Grammars with Latent An-
notations (PCFG-LA) (Petrov et al, 2006) and dis-
criminative reranking. We further replace rare words
by their predicted morphological analysis.
We preprocess the treebank trees by removing the
morphological annotation of the POS tags and the
function labels of all non-terminals. We also reduce
the 177 compositional Korean POS tags to their first
atomic tag, which results in a POS tag set of 9 tags.
PCFG-LAs are incrementally built by split-
ting non-terminals, refining parameters using EM-
training and reversing splits that only cause small
increases in likelihood.
Running the Berkeley Parser4 ? the reference im-
plementation of PCFG-LAs ? on the data sets results
in the PARSEVAL scores given in Table 2 (Berke-
ley). The Berkeley parser only implements a simple
signature-based unknown word model that seems to
be ineffective for some of the languages, especially
Basque and Korean.
We thus replace rare words (frequency < 20) by
the predicted morphological tags of Section 2 (or the
true morphological tag for the gold setup). The intu-
ition is that our discriminative tagger has a more so-
phisticated unknown word treatment than the Berke-
ley parser, taking for example prefixes, suffixes and
4http://code.google.com/p/
berkeleyparser/
the immediate lexical context into account. Further-
more, the morphological tag contains most of the
necessary syntactic information. An exception, for
instance, might be the semantic information needed
to disambiguate prepositional attachment. We think
that replacing rare words by tags has an advan-
tage over constraining the pre-terminal layer of the
parser, because the parser can still decide to assign
a different tag, for example in cases were the tag-
ger produces errors due to long-distance dependen-
cies. The used frequency threshold of 20 results
in token replacement rates of 18% (French) to 57%
(Korean and Polish), which correspond to 209 (for
Polish) to 3221 (for Arabic) word types that are not
replaced. The PARSEVAL scores for the described
method are again given in Table 2 (Replaced). The
method yields improvements for all languages ex-
cept for French where we observe a drop of 0.06.
The improvements range from 0.46 for Arabic to
1.02 for Swedish, 3.1 for Polish and more than 10
for Basque and Korean.
To further improve results, we employ the
product-of-grammars procedure (Petrov, 2010),
where different grammars are trained on the same
data set but with different initialization setups. We
trained 8 grammars and used tree-level inference.
In Table 2 (Product) we can see that this leads to
improvements from 0.72 for Hungarian to 3.73 for
Swedish.
On the 50-best output of the product parser,
we also carry out discriminative reranking. The
reranker is trained for the maximum entropy objec-
tive function of Charniak and Johnson (2005) and
use the standard feature set ? without language-
specific feature engineering ? from Charniak and
Johnson (2005) and Collins (2000). We use a
slightly modified version of the Mallet toolkit (Mc-
Callum, 2002) for reranking.
Improvements range from negligible differences
(< .1) for Hebrew and Polish to substantial differ-
ences (> 1.) for Basque, French, and Hungarian.
137
mate parser
best-first
parser
turboparser
merged list
of 50-100 best
trees/sentence
merged list
scored by
all parsers
ranker
ptb trees
Parsing Ranking
IN OUT
scores
scores
scores
features
Figure 1: Architecture of the dependency ranking system.
For our final submission, we used the reranker
output for all languages except French, Hebrew, Pol-
ish, and Swedish. This decision was based on an
earlier version of the evaluation setting provided by
the organizers. In this setup, reranking did not help
or was even harmful for these four languages. The
figures in Table 2 use the latest evaluation script and
are thus consistent with the test set results presented
in Section 5.
After the submission deadline the Shared Task
organizers made us aware that we had surprisingly
low exact match scores for Polish (e.g., 1.22 for
the reranked setup). The reason seems to be that
the Berkeley parser cannot produce unary chains of
length > 2. The gold development set contains 1783
such chains while the prediction of the reranked sys-
tem contains none. A particularly frequent unary
chain with 908 occurences in the gold data is ff ?
fwe ? formaczas. As this chain cannot be pro-
duced the parser leaves out the fwe phrase. Inserting
new fwe nodes between ff and formacszas nodes
raises the PARSEVAL scores of the reranked model
from 88.40 to 90.64 and the exact match scores to
11.34. This suggests that the Polish results could be
improved substantially if unary chains were properly
dealt with, for example by collapsing unary chains.5
4 Dependency Parsing
The core idea of our dependency parsing system
is the combination of the n-best output of several
5Thanks to Slav Petrov for pointing us to the unary chain
length limit.
parsers followed by a ranking step on the com-
bined list. Specifically, we first run two parsers that
each output their 50-best analyses for each sentence.
These 50-best analyses are merged together into one
single n-best list of between 50 and 100 analyses
(depending on the overlap between the n-best lists
of the two parsers). We then use the two parsers
plus an additional one to score each tree in the n-
best lists according to their parsing model, thus pro-
viding us with three different scores for each tree in
the n-best lists. The n-best lists are then given to
a ranker, which ranks the list using the three scores
and a small set of additional features in order to find
the best overall analysis. Figure 1 shows a schematic
of the process.
As a preprocessing step, we reduced the depen-
dency label set for the Hungarian training data.
The Hungarian dependency data set encodes ellipses
through composite edge labels which leads to a pro-
liferation of edge labels (more than 400). Since
many of these labels are extremely rare and thus hard
to learn for the parsers, we reduced the set of edge la-
bels during the conversion. Specifically, we retained
the 50 most frequent labels, while reducing the com-
posite labels to their base label.
For producing the initial n-best lists, we use
the mate parser6 (Bohnet, 2010) and a variant of
the EasyFirst parser (Goldberg and Elhadad, 2010),
which we here call best-first parser.
The mate parser is a state-of-the-art graph-based
dependency parser that uses second-order features.
6https://code.google.com/p/mate-tools
138
The parser works in two steps. First, it uses dy-
namic programming to find the optimal projective
tree using the Carreras (2007) decoder. It then
applies the non-projective approximation algorithm
proposed by McDonald and Pereira (2006) in or-
der to produce non-projective parse trees. The non-
projective approximation algorithm is a greedy hill
climbing algorithm that starts from the optimal pro-
jective parse and iteratively tries to reattach all to-
kens, one at a time, everywhere in the sentence as
long as the tree property holds. It halts when the in-
crease in the score of the tree according to the pars-
ing model is below a certain threshold.
n-best lists are obtained by applying the non-
projective approximation algorithm in a non-greedy
manner, exploring multiple possibilities. All trees
are collected in a list, and when no new trees are
found, or newer trees have a significantly lower
score than the currently best one, search halts. The
n best trees are then retrieved from the list. It
should be noted that, in the standard case, the non-
projective approximation algorithm may find a local
optimum, and that there may be other trees that have
a higher score which were not explored. Thus the
best parse in the greedy case may not necessarily
be the one with the highest score in the n-best list.
Since the parser is trained with the greedy version
of the non-projective approximation algorithm, the
greedily chosen output parse tree is of special in-
terest. We thus flag this tree as the baseline mate
parse, in order to use that for features in the ranker.
The baseline mate parse is also our overall baseline
in the dependency track.
The best-first parser deviates from the EasyFirst
parser in several small respects: The EasyFirst de-
coder creates dependency links between the roots of
adjacent substructures, which gives an O(n log n)
complexity, but restricts the output to projective
trees. The best-first parser is allowed to choose as
head any node of an adjacent substructure instead of
only the root, which increases complexity to O(n2),
but accounts for a big part of possible non-projective
structures. We additionally implemented a swap-
operation (Nivre, 2009; Tratz and Hovy, 2011) to
account for the more complex structures. The best-
first parser relies on a beam-search strategy7 to pur-
7Due to the nature of the decoder, the parser can produce
sue multiple derivations, which we also use to pro-
duce the n-best output.
In the scoring step, we additionally apply the tur-
boparser8 (Martins et al, 2010), which is based on
linear programming relaxations.9 We changed all
three parsers such that they would return a score for
a given tree. We use this to extract scores from each
parser for all trees in the n-best lists. It is impor-
tant to have a score from every parser for every tree,
as previously observed by Zhang et al (2009) in the
context of constituency reranking.
4.1 Ranking
Table 3 shows the performance of the individual
parsers measured on the development sets. It also
displays the oracle scores over the different n-best
lists, i.e., the maximal possible score over an n-best
list if the best tree is always selected.
The mate parser generally performs best followed
by turboparser, while the best-first parser comes last.
But we can see from the oracle scores that the best-
first parser often shows comparable or even higher
oracle scores than mate, and that the combination
of the n-best lists always adds substantial improve-
ments to the oracle scores. These findings show that
the mate and best-first parsers are providing differ-
ent sets of n-best lists. Moreover, all three parsers
rely on different parsing algorithms and feature sets.
For these reasons, we hypothesized that the parsers
contribute different views on the parse trees and that
their combination would result in better overall per-
formance.
In order to leverage the diversity between the
parsers we experimented with ranking10 on the
n-best lists. We used the same ranking model in-
troduced in Section 3 here as well. The model is
trained to select the best parse according to the la-
beled attachment score (LAS). The training data for
the ranker was created by 5-fold jackknifing on the
training sets. The feature sets for the ranker for
spurious ambiguities in the beam. If this occurs, only the one
with the higher score is kept.
8http://www.ark.cs.cmu.edu/TurboParser/
9Ideally we would also extract n-best lists from the tur-
boparser, however time prevented us from making the necessary
modifications.
10We refrain from calling it reranking in this setting, since
we are using merged n-best lists and the initial ranking is not
entirely clear to begin with.
139
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Baseline results for individual parsers
mate? 88.50/83.50 88.18/84.49 92.71/90.85 83.63/75.89 87.07/82.84 86.06/82.39 91.17/85.81 83.65/77.16
mate 87.68/85.42 89.11/84.43 88.30/84.84 93.15/91.46 86.05/79.37 88.03/84.41 87.91/85.76 91.51/86.30 83.53/77.05
bf 87.61/85.32 84.07/75.90 87.45/83.92 92.90/91.10 86.10/79.57 83.85/75.94 86.54/83.97 90.10/83.75 82.27/75.36
turbo 87.82/85.35 88.88/83.84 88.24/84.57 93.59/91.54 85.74/78.95 86.86/82.80 88.35/86.23 90.97/85.55 83.24/76.15
Oracle scores for n-best lists
mate 90.85/88.74 93.39/89.85 90.99/87.81 97.14/95.84 89.05/83.03 91.41/88.19 94.86/92.96 95.19/91.67 87.19/81.66
bf 91.47/89.46 91.68/86.46 91.38/88.68 97.40/96.60 91.04/85.67 87.64/81.79 94.90/92.94 96.25/93.74 87.60/82.46
merged 92.65/90.71 95.15/91.91 92.97/90.43 98.19/97.44 92.39/87.18 92.12/88.76 96.23/94.65 97.28/95.29 89.87/84.96
Table 3: Baseline performance and n-best oracle scores (UAS/LAS) on the development sets. mate? uses the prepro-
cessing provided by the organizers, the other parsers use the preprocessing described in Section 2.
each language were optimized manually via cross-
validation on the training sets. The features used for
each language, as well as a default (baseline) fea-
ture set, are shown in Table 4. We now outline the
features we used in the ranker:
Score from the base parsers ? denoted B, M,
T, for the best-first, mate, and turbo parsers, re-
spectively. We also have indicator features whether
a certain parse was the best according to a given
parser, denoted GB, GM, GT, respectively. Since
the mate parser does not necessarily assign the high-
est score to the baseline mate parse, the GM fea-
ture is a ternary feature which indicates whether a
parse is the same as the baseline mate parse, or bet-
ter, or worse. We also experimented with transfor-
mations and combinations of the scores from the
parsers. Specifically, BMProd denotes the product
of B and M; BMeProd denotes the sum of B and M
in e-space, i.e., eB+M ; reBMT, reBT, reMT denote
the normalized product of the corresponding scores,
where scores are normalized in a softmax fashion
such that all features take on values in the interval
(0, 1).
Projectivity features (Hall et al, 2007) ? the
number of non-projective edges in a tree, denoted
np. Whether a tree is ill-nested, denoted I. Since ill-
nested trees are extremely rare in the treebanks, this
helps the ranker filter out unlikely candidates from
the n-best lists. For a definition and further discus-
sion of ill-nestedness, we refer to (Havelka, 2007).
Constituent features ? from the constituent track
we also have constituent trees of all sentences which
can be used for feature extraction. Specifically, for
every head-dependent pair, we extract the path in the
constituent tree between the nodes, denoted ptbp.
Case agreement ? on head-dependent pairs that
both have a case value assigned among their mor-
phological features, we mark whether it is the same
case or not, denoted case.
Function label uniqueness ? on each training set
we extracted a list of function labels that generally
occur at most once as the dependent of a node, e.g.,
subjects or objects. Features are then extracted from
all nodes that have one or more dependents of each
label aimed at capturing mistakes such as double
subjects on a verb. This template is denoted FL.
In addition to the features mentioned above, we
experimented with a variety of feature templates, in-
cluding features drawn from previous work on de-
pendency reranking (Hall, 2007), e.g., lexical and
POS-based features over edges, ?subcategorization?
frames (i.e., the concatenation of POS-tags that are
headed by a certain node in the tree), etc, although
these features did not seem to help. For German we
created feature templates based on the constraints
used in the constraint-based parser by Seeker and
Kuhn (2013). This includes, e.g., violations in case
or number agreement between heads and depen-
dents, as well as more complex features that con-
sider labels on entire verb complexes. None of these
features yielded any clear improvements though. We
also experimented with features that target some
specific constructions (and specifics of annotation
schemes) which the parsers typically cannot fully
see, such as coordination, however, also here we saw
no clear improvements.
4.2 Effects of Ranking
In Table 5, we show the improvements from using
the ranker, both with the baseline and optimized fea-
tures sets for the ranker. For the sake of comparison,
140
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Baseline 87.68/85.42 89.11/84.43 88.30/84.84 93.15/91.46 86.05/79.37 88.03/84.41 87.91/85.76 91.51/86.30 83.53/77.05
Ranked-dflt 88.54/86.32 89.99/85.43 88.85/85.39 94.06/92.36 87.28/80.44 88.16/84.54 88.71/86.65 92.26/87.12 84.51/77.83
Ranked 88.93/86.74 89.95/85.61 89.37/85.96 94.20/92.68 87.63/81.02 88.38/84.77 89.20/87.12 93.02/87.69 85.04/78.57
Oracle 92.65/90.71 95.15/91.91 92.97/90.43 98.19/97.44 92.39/87.18 92.12/88.76 96.23/94.65 97.28/95.29 89.87/84.96
Table 5: Performance (UAS/LAS) of the reranker on the development sets. Baseline denotes our baseline. Ranked-dflt
and Ranked denote the default and optimized ranker feature sets, respectively. Oracle denotes the oracle scores.
default B, M, T, GB, GM, GT, I
Arabic B, M, T, GB, GM, I, ptbp, reBMT
Basque B, M, T, GB, GM, GT, I, ptbp, I, reMT, case
French B, M, T, GB, GM, GT, I, ptbp
German B, M, T, GM, I, BMProd, FL
Hebrew B, M, T, GB, GM, GT, I, ptbp, FL, BMeProd
Hungarian B, M, T, GB, GM, GT, I, ptbp, reBM, FL
Korean B, M, T, GB, GM, GT, I, ptbp, reMT, FL
Polish B, M, T, GB, GM, GT, I, ptbp, np
Swedish B, M, T, GB, GM, GT, I, ptbp, reBM, FL
Table 4: Feature sets for the dependency ranker for each
language. default denotes the default ranker feature set.
the baseline mate parses as well as the oracle parses
on the merged n-best lists are repeated from Table 3.
We see that ranking clearly helps, both with a tai-
lored feature set, as well as the default feature set.
The improvement in LAS between the baseline and
the tailored ranking feature sets ranges from 1.1%
(French) to 1.6% (Hebrew) absolute, with the excep-
tion of Hungarian, where improvements on the dev
set are more modest (contrary to the test set results,
cf. Section 5). Even with the default feature set, the
improvements range from 0.5% (French) to 1.1%
(Hebrew) absolute, again setting Hungarian aside.
We believe that this is an interesting result consid-
ering the simplicity of the default feature set.
5 Test Set Results
In this section we outline our final results on the test
sets. As previously, we focus on the setting with
predicted tags in gold segmentation and the largest
training set. We also present results on Arabic and
Hebrew for the predicted segmentation setting. For
the gold preprocessing and all 5k settings, we refer
the reader to the Shared Task overview paper (Sed-
dah et al, 2013).11
In Table 7, we present our results in the con-
11Or the results page online: http://www.spmrl.org/
spmrl2013-sharedtask-results.html
stituency track. Since we were the only participat-
ing team in the constituency track, we compare our-
selves with the best baseline12 provided by the or-
ganizers. Our system outperforms the baseline for
all languages in terms of PARSEVAL F1. Follow-
ing the trend on the development sets, reranking is
consistently helping across languages.13 Despite the
lack of other submissions in the shared task, we be-
lieve our numbers are generally strong and hope that
they can serve as a reference for future work on con-
stituency parsing on these data sets.
Table 8 displays our results in the dependency
track. We submitted two runs: a baseline, which
is the baseline mate parse, and the reranked trees.
The table also compares our results to the best per-
forming other participant in the shared task (denoted
Other) as well as the MaltParser (Nivre et al, 2007)
baseline provided by the shared task organizers (de-
noted ST Baseline). We obtain the highest scores
for all languages, with the exception of French. It is
also clear that we make considerable gains over our
baseline, confirming our results on the development
sets reported in Section 4. It is also noteworthy that
our baseline (i.e., the mate parser with our own pre-
processing) outperforms the best other system for 5
languages.
Arabic Hebrew
Other 90.75/8.48 88.33/12.20
Dep. Baseline 91.13/9.10 89.27/15.01
Dep. Ranked 91.74/9.83 89.47/16.97
Constituency 92.06/9.49 89.30/13.60
Table 6: Unlabeled TedEval scores (accuracy/exact
match) for the test sets in the predicted segmentation set-
ting. Only sentences of length ? 70 are evaluated.
12It should be noted that the Shared Task organizers com-
puted 2 different baselines on the test sets. The best baseline
results for each language thus come from different parsers.
13We remind the reader that our submission decisions are not
based on figures in Table 2, cf. Section 3.
141
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
ST Baseline 79.19 74.74 80.38 78.30 86.96 85.22 78.56 86.75 80.64
Product 80.81 87.18 81.83 80.70 89.46 90.58 83.49 87.55 83.99
Reranked 81.32 87.86 82.86 81.27 89.49 91.85 84.27 87.76 84.88
Table 7: Final PARSEVAL F1 scores for constituents on the test set for the predicted setting. ST Baseline denotes the
best baseline (out of 2) provided by the Shared Task organizers. Our submission is underlined.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
ST Baseline 83.18/80.36 79.77/70.11 82.49/77.98 81.51/77.81 76.49/69.97 80.72/70.15 85.72/82.06 82.19/75.63 80.29/73.21
Other 85.78/83.20 89.19/84.25 89.19/85.86 90.80/88.66 81.05/73.63 88.93/84.97 85.84/82.65 88.12/82.56 87.28/80.88
Baseline 86.96/84.81 89.32/84.25 87.87/84.37 90.54/88.37 85.88/79.67 89.09/85.31 87.41/85.51 90.30/85.51 86.85/80.67
Ranked 88.32/86.21 89.88/85.14 88.68/85.24 91.64/89.65 86.70/80.89 89.81/86.13 88.47/86.62 91.75/87.07 88.06/82.13
Table 8: Final UAS/LAS scores for dependencies on the test sets for the predicted setting. Other denotes the highest
scoring other participant in the Shared Task. ST Baseline denotes the MaltParser baseline provided by the Shared Task
organizers.
Table 6 shows the unlabeled TedEval (Tsarfaty et
al., 2012) scores (accuracy/exact match) on the test
sets for the predicted segmentation setting for Ara-
bic and Hebrew. Note that these figures only include
sentences of length less than or equal to 70. Since
TedEval enables cross-framework comparison, we
compare our submissions from the dependency track
to our submission from the constituency track. In
these runs we used the same systems that were used
for the gold segmentation with predicted tags track.
The predicted segmentation was provided by the
Shared Task organizers. We also compare our re-
sults to the best other system from the Shared Task
(denoted Other).
Also here we obtain the highest results for both
languages. However, it is unclear what syntactic
paradigm (dependencies or constituents) is better
suited for the task. All in all it is difficult to assess
whether the differences between the best and second
best systems for each language are meaningful.
6 Conclusion
We have presented our contribution to the 2013
SPMRL Shared Task. We participated in both the
constituency and dependency tracks. In both tracks
we make use of a state-of-the-art tagger for POS and
morphological features. In the constituency track,
we use the tagger to handle unknown words and em-
ploy a product-of-grammars-based PCFG-LA parser
and parse tree reranking. In the dependency track,
we combine multiple parsers output as input for a
ranker.
Since there were no other participants in the con-
stituency track, it is difficult to draw any conclusions
from our results. We do however show that the ap-
plication of product grammars, our handling of rare
words, and a subsequent reranking step outperforms
a baseline PCFG-LA parser.
In the dependency track we obtain the best re-
sults for all languages except French among 7 partic-
ipants. Our reranking approach clearly outperforms
a baseline graph-based parser. This is the first time
multiple parsers have been used in a dependency
reranking setup.
Aside from minor decisions made on the basis
of each language, our approach is language agnos-
tic and does not target morphology in any particu-
lar way as part of the parsing process. We show
that with a strong baseline and with no language
specific treatment it is possible to achieve state-of-
the-art results across all languages. Our architec-
ture for the dependency parsing track enables the use
of language-specific features in the ranker, although
we only had minor success with features that target
morphology. However, it may be the case that ap-
proaches from previous work on parsing MRLs, or
the approaches taken by other teams in the Shared
Task, can be successfully combined with ours and
improve parsing accuracy even more.
Acknowledgments
Richa?rd Farkas is funded by the European Union and
the European Social Fund through the project Fu-
turICT.hu (grant no.: TA?MOP-4.2.2.C-11/1/KONV-
142
2012-0013). Thomas Mu?ller is supported by a
Google Europe Fellowship in Natural Language
Processing. The remaining authors are funded by
the Deutsche Forschungsgemeinschaft (DFG) via
the SFB 732, projects D2 and D8 (PI: Jonas Kuhn).
We also express our gratitude to the treebank
providers for each language: Arabic (Maamouri et
al., 2004; Habash and Roth, 2009; Habash et al,
2009; Green and Manning, 2010), Basque (Aduriz
et al, 2003), French (Abeille? et al, 2003), He-
brew (Sima?an et al, 2001; Tsarfaty, 2010; Gold-
berg, 2011; Tsarfaty, 2013), German (Brants et al,
2002; Seeker and Kuhn, 2012), Hungarian (Csendes
et al, 2005; Vincze et al, 2010), Korean (Choi
et al, 1994; Choi, 2013), Polish (S?widzin?ski and
Wolin?ski, 2010), and Swedish (Nivre et al, 2006).
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.
2003. Building a treebank for french. In Anne
Abeille?, editor, Treebanks. Kluwer, Dordrecht.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. D??az de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In TLT-03, pages 201?204.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (Coling 2010), pages 89?97, Bei-
jing, China, August. Coling 2010 Organizing Commit-
tee.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Erhard Hinrichs and Kiril Simov, edi-
tors, Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT 2002), pages 24?41, So-
zopol, Bulgaria.
Tim Buckwalter. 2002. Buckwalter Arabic Morpholog-
ical Analyzer Version 1.0. Linguistic Data Consor-
tium, University of Pennsylvania, 2002. LDC Catalog
No.: LDC2002L49.
Xavier Carreras. 2007. Experiments with a Higher-
Order Projective Dependency Parser. In Proceedings
of the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 957?961, Prague, Czech Republic, June.
Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 173?180.
Key-Sun Choi, Young S Han, Young G Han, and Oh W
Kwon. 1994. Kaist tree bank project for korean:
Present and future development. In Proceedings of
the International Workshop on Sharable Natural Lan-
guage Resources, pages 7?14. Citeseer.
Jinho D. Choi. 2013. Preparing korean data for
the shared task on parsing morphologically rich lan-
guages. ArXiv e-prints.
Michael Collins. 2000. Discriminative Reranking for
Natural Language Parsing. In Proceedings of the Sev-
enteenth International Conference on Machine Learn-
ing, ICML ?00, pages 175?182.
Do?ra Csendes, Jano?s Csirik, Tibor Gyimo?thy, and Andra?s
Kocsor. 2005. The Szeged treebank. In Va?clav Ma-
tous?ek, Pavel Mautner, and Toma?s? Pavelka, editors,
Text, Speech and Dialogue: Proceedings of TSD 2005.
Springer.
Rickard Domeij, Ola Knutsson, Johan Carlberger, and
Viggo Kann. 2000. Granska-an efficient hybrid sys-
tem for Swedish grammar checking. In In Proceed-
ings of the 12th Nordic Conference in Computational
Linguistics.
Mikel L Forcada, Mireia Ginest??-Rosell, Jacob Nord-
falk, Jim O?Regan, Sergio Ortiz-Rojas, Juan An-
tonio Pe?rez-Ortiz, Felipe Sa?nchez-Mart??nez, Gema
Ram??rez-Sa?nchez, and Francis M Tyers. 2011. Aper-
tium: A free/open-source platform for rule-based ma-
chine translation. Machine Translation.
Yoav Goldberg and Michael Elhadad. 2010. An Ef-
ficient Algorithm for Easy-First Non-Directional De-
pendency Parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 742?750, Los Angeles, California, June.
Association for Computational Linguistics.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
Spence Green and Christopher D. Manning. 2010. Bet-
ter arabic parsing: Baselines, evaluations, and anal-
ysis. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 394?402, Beijing, China, August. Coling 2010
Organizing Committee.
Nizar Habash and Ryan Roth. 2009. Catib: The
columbia arabic treebank. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 221?
224, Suntec, Singapore, August. Association for Com-
putational Linguistics.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
143
Keith Hall, Jiri Havelka, and David A. Smith. 2007.
Log-Linear Models of Non-Projective Trees, k-best
MST Parsing and Tree-Ranking. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 962?966, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Keith Hall. 2007. K-best Spanning Tree Parsing. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 392?399, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Jiri Havelka. 2007. Beyond Projectivity: Multilin-
gual Evaluation of Constraints and Measures on Non-
Projective Structures. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 608?615, Prague, Czech Republic,
June. Association for Computational Linguistics.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown Parsers. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 665?668, Los An-
geles, California, June. Association for Computational
Linguistics.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo Parsers: Depen-
dency Parsing by Approximate Variational Inference.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 34?
44, Cambridge, MA, October. Association for Compu-
tational Linguistics.
Andrew Kachites McCallum. 2002. ?mal-
let: A machine learning for language toolkit?.
http://mallet.cs.umass.edu.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of the 11th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 81?88, Trento, Italy. Asso-
ciation for Computational Linguistics.
Thomas Mu?ller, Helmut Schmid, and Hinrich Schu?tze.
2013. Efficient Higher-Order CRFs for Morphological
Tagging. In In Proceedings of EMNLP.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?ls?en Eryig?it, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13:95?135, 6.
Joakim Nivre. 2009. Non-Projective Dependency Pars-
ing in Expected Linear Time. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
351?359, Suntec, Singapore, August. Association for
Computational Linguistics.
S Park, D Choi, E-k Kim, and KS Choi. 2010. A plug-in
component-based Korean morphological analyzer. In
Proceedings of HCLT2010.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics, pages 433?440. Associa-
tion for Computational Linguistics.
Slav Petrov. 2010. Products of Random Latent Variable
Grammars. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19?27, Los Angeles, California, June. Associa-
tion for Computational Linguistics.
Helmut Schmid. 2005. A programming language for
finite state transducers. In FSMNLP.
Djame? Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie Can-
dito, Jinho Choi, Richa?rd Farkas, Jennifer Foster, Iakes
Goenaga, Koldo Gojenola, Yoav Goldberg, Spence
Green, Nizar Habash, Marco Kuhlmann, Wolfgang
Maier, Joakim Nivre, Adam Przepiorkowski, Ryan
Roth, Wolfgang Seeker, Yannick Versley, Veronika
Vincze, Marcin Wolin?ski, and Alina Wro?blewska.
2013. Overview of the SPMRL 2013 Shared Task: A
Cross-Framework Evaluation of Parsing Morphologi-
cally Rich Languages. In Proceedings of the 4th Work-
shop on Statistical Parsing of Morphologically Rich
Languages: Shared Task, Seattle, WA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation, pages 3132?3139, Istanbul, Turkey. European
Language Resources Association (ELRA).
Wolfgang Seeker and Jonas Kuhn. 2013. Morphological
and Syntactic Case in Statistical Dependency Parsing.
Computational Linguistics, 39(1):23?55.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank for
Modern Hebrew Text. In Traitement Automatique des
Langues.
144
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Text,
Speech and Dialogue: 13th International Conference
(TSD), Lecture Notes in Artificial Intelligence, pages
197?204, Brno, Czech Republic. Springer.
Stephen Tratz and Eduard Hovy. 2011. A Fast, Ac-
curate, Non-Projective, Semantically-Enriched Parser.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1257?1268, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Reut Tsarfaty, Djame? Seddah, Yoav Goldberg, Sandra
Kuebler, Yannick Versley, Marie Candito, Jennifer
Foster, Ines Rehbein, and Lamia Tounsi. 2010. Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL) What, How and Whither. In Proc. of the
SPMRL Workshop of NAACL-HLT, pages 1?12, Los
Angeles, CA, USA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Joint Evaluation of Morphological Segmen-
tation and Syntactic Parsing. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
6?10, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A Unified Morpho-Syntactic
Scheme of Stanford Dependencies. Proceedings of
ACL.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy
Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hun-
garian dependency treebank. In LREC.
Zhiguo Wang and Chengqing Zong. 2011. Parse Rerank-
ing Based on Higher-Order Lexical Dependencies. In
Proceedings of 5th International Joint Conference on
Natural Language Processing, pages 1251?1259, Chi-
ang Mai, Thailand, November. Asian Federation of
Natural Language Processing.
Marcin Wolin?ski. 2006. Morfeusz - A practical tool for
the morphological analysis of Polish. In Intelligent in-
formation processing and web mining, pages 511?520.
Springer.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-Best Combination of Syntactic Parsers.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1552?1560, Singapore, August. Association for Com-
putational Linguistics.
Zhenxia Zhou. 2007. Entwicklung einer franzo?sischen
Finite-State-Morphologie. Diplomarbeit, Institute for
Natural Language Processing, University of Stuttgart.
Ja?nos Zsibrita, Veronika Vincze, and Richa?rd Farkas.
2013. Magyarlanc 2.0: Szintaktikai elemze?s e?s fel-
gyors??tott szo?faji egye?rtelmu?s??te?s. In IX. Magyar
Sza?m??to?ge?pes Nyelve?szeti Konferencia.
145
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146?182,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Overview of the SPMRL 2013 Shared Task:
Cross-Framework Evaluation of Parsing Morphologically Rich Languages?
Djam? Seddaha, Reut Tsarfatyb, Sandra K?blerc,
Marie Canditod, Jinho D. Choie, Rich?rd Farkasf , Jennifer Fosterg, Iakes Goenagah,
Koldo Gojenolai, Yoav Goldbergj , Spence Greenk, Nizar Habashl, Marco Kuhlmannm,
Wolfgang Maiern, Joakim Nivreo, Adam Przepi?rkowskip, Ryan Rothq, Wolfgang Seekerr,
Yannick Versleys, Veronika Vinczet, Marcin Wolin?skiu,
Alina Wr?blewskav, Eric Villemonte de la Cl?rgeriew
aU. Paris-Sorbonne/INRIA, bWeizman Institute, cIndiana U., dU. Paris-Diderot/INRIA, eIPsoft Inc., f,tU. of Szeged,
gDublin City U., h,iU. of the Basque Country, jBar Ilan U., kStanford U., l,qColumbia U., m,oUppsala U., nD?sseldorf U.,
p,u,vPolish Academy of Sciences, rStuttgart U., sHeidelberg U., wINRIA
Abstract
This paper reports on the first shared task on
statistical parsing of morphologically rich lan-
guages (MRLs). The task features data sets
from nine languages, each available both in
constituency and dependency annotation. We
report on the preparation of the data sets, on
the proposed parsing scenarios, and on the eval-
uation metrics for parsing MRLs given dif-
ferent representation types. We present and
analyze parsing results obtained by the task
participants, and then provide an analysis and
comparison of the parsers across languages and
frameworks, reported for gold input as well as
more realistic parsing scenarios.
1 Introduction
Syntactic parsing consists of automatically assigning
to a natural language sentence a representation of
its grammatical structure. Data-driven approaches
to this problem, both for constituency-based and
dependency-based parsing, have seen a surge of inter-
est in the last two decades. These data-driven parsing
approaches obtain state-of-the-art results on the de
facto standard Wall Street Journal data set (Marcus et
al., 1993) of English (Charniak, 2000; Collins, 2003;
Charniak and Johnson, 2005; McDonald et al, 2005;
McClosky et al, 2006; Petrov et al, 2006; Nivre et
al., 2007b; Carreras et al, 2008; Finkel et al, 2008;
?Contact authors: djame.seddah@paris-sorbonne.fr,
reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu
Huang, 2008; Huang et al, 2010; Zhang and Nivre,
2011; Bohnet and Nivre, 2012; Shindo et al, 2012),
and provide a foundation on which many tasks oper-
ating on semantic structure (e.g., recognizing textual
entailments) or even discourse structure (coreference,
summarization) crucially depend.
While progress on parsing English ? the main
language of focus for the ACL community ? has in-
spired some advances on other languages, it has not,
by itself, yielded high-quality parsing for other lan-
guages and domains. This holds in particular for mor-
phologically rich languages (MRLs), where impor-
tant information concerning the predicate-argument
structure of sentences is expressed through word for-
mation, rather than constituent-order patterns as is the
case in English and other configurational languages.
MRLs express information concerning the grammati-
cal function of a word and its grammatical relation to
other words at the word level, via phenomena such
as inflectional affixes, pronominal clitics, and so on
(Tsarfaty et al, 2012c).
The non-rigid tree structures and morphological
ambiguity of input words contribute to the challenges
of parsing MRLs. In addition, insufficient language
resources were shown to also contribute to parsing
difficulty (Tsarfaty et al, 2010; Tsarfaty et al, 2012c,
and references therein). These challenges have ini-
tially been addressed by native-speaking experts us-
ing strong in-domain knowledge of the linguistic
phenomena and annotation idiosyncrasies to improve
the accuracy and efficiency of parsing models. More
146
recently, advances in PCFG-LA parsing (Petrov et al,
2006) and language-agnostic data-driven dependency
parsing (McDonald et al, 2005; Nivre et al, 2007b)
have made it possible to reach high accuracy with
classical feature engineering techniques in addition
to, or instead of, language-specific knowledge. With
these recent advances, the time has come for estab-
lishing the state of the art, and assessing strengths
and weaknesses of parsers across different MRLs.
This paper reports on the first shared task on sta-
tistical parsing of morphologically rich languages
(the SPMRL Shared Task), organized in collabora-
tion with the 4th SPMRL meeting and co-located
with the conference on Empirical Methods in Natural
Language Processing (EMNLP). In defining and exe-
cuting this shared task, we pursue several goals. First,
we wish to provide standard training and test sets for
MRLs in different representation types and parsing
scenarios, so that researchers can exploit them for
testing existing parsers across different MRLs. Sec-
ond, we wish to standardize the evaluation protocol
and metrics on morphologically ambiguous input,
an under-studied challenge, which is also present in
English when parsing speech data or web-based non-
standard texts. Finally, we aim to raise the awareness
of the community to the challenges of parsing MRLs
and to provide a set of strong baseline results for
further improvement.
The task features data from nine, typologically di-
verse, languages. Unlike previous shared tasks on
parsing, we include data in both dependency-based
and constituency-based formats, and in addition to
the full data setup (complete training data), we pro-
vide a small setup (a training subset of 5,000 sen-
tences). We provide three parsing scenarios: one in
which gold segmentation, POS tags, and morphologi-
cal features are provided, one in which segmentation,
POS tags, and features are automatically predicted
by an external resource, and one in which we provide
a lattice of multiple possible morphological analyses
and allow for joint disambiguation of the morpholog-
ical analysis and syntactic structure. These scenarios
allow us to obtain the performance upper bound of
the systems in lab settings using gold input, as well
as the expected level of performance in realistic pars-
ing scenarios ? where the parser follows a morpho-
logical analyzer and is a part of a full-fledged NLP
pipeline.
The remainder of this paper is organized as follows.
We first survey previous work on parsing MRLs (?2)
and provide a detailed description of the present task,
parsing scenarios, and evaluation metrics (?3). We
then describe the data sets for the nine languages
(?4), present the different systems (?5), and empiri-
cal results (?6). Then, we compare the systems along
different axes (?7) in order to analyze their strengths
and weaknesses. Finally, we summarize and con-
clude with challenges to address in future shared
tasks (?8).
2 Background
2.1 A Brief History of the SPMRL Field
Statistical parsing saw initial success upon the avail-
ability of the Penn Treebank (PTB, Marcus et al,
1994). With that large set of syntactically annotated
sentences at their disposal, researchers could apply
advanced statistical modeling and machine learning
techniques in order to obtain high quality structure
prediction. The first statistical parsing models were
generative and based on treebank grammars (Char-
niak, 1997; Johnson, 1998; Klein and Manning, 2003;
Collins, 2003; Petrov et al, 2006; McClosky et al,
2006), leading to high phrase-structure accuracy.
Encouraged by the success of phrase-structure
parsers for English, treebank grammars for additional
languages have been developed, starting with Czech
(Hajic? et al, 2000) then with treebanks of Chinese
(Levy and Manning, 2003), Arabic (Maamouri et
al., 2004b), German (K?bler et al, 2006), French
(Abeill? et al, 2003), Hebrew (Sima?an et al, 2001),
Italian (Corazza et al, 2004), Spanish (Moreno et al,
2000), and more. It quickly became apparent that
applying the phrase-based treebank grammar tech-
niques is sensitive to language and annotation prop-
erties, and that these models are not easily portable
across languages and schemes. An exception to that
is the approach by Petrov (2009), who trained latent-
annotation treebank grammars and reported good
accuracy on a range of languages.
The CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007a) high-
lighted the usefulness of an alternative linguistic for-
malism for the development of competitive parsing
models. Dependency relations are marked between
input tokens directly, and allow the annotation of
147
non-projective dependencies that are parseable effi-
ciently. Dependency syntax was applied to the de-
scription of different types of languages (Tesni?re,
1959; Mel?c?uk, 2001), which raised the hope that in
these settings, parsing MRLs will further improve.
However, the 2007 shared task organizers (Nivre
et al, 2007a) concluded that: "[Performance] classes
are more easily definable via language characteris-
tics than via characteristics of the data sets. The
split goes across training set size, original data for-
mat [...], sentence length, percentage of unknown
words, number of dependency labels, and ratio of
(C)POSTAGS and dependency labels. The class
with the highest top scores contains languages with
a rather impoverished morphology." The problems
with parsing MRLs have thus not been solved by de-
pendency parsing, but rather, the challenge has been
magnified.
The first event to focus on the particular challenges
of parsing MRLs was a dedicated panel discussion
co-located with IWPT 2009.1 Work presented on
Hebrew, Arabic, French, and German made it clear
that researchers working on non-English parsing face
the same overarching challenges: poor lexical cover-
age (due to high level of inflection), poor syntactic
coverage (due to more flexible word ordering), and,
more generally, issues of data sparseness (due to
the lack of large-scale resources). Additionally, new
questions emerged as to the evaluation of parsers in
such languages ? are the word-based metrics used
for English well-equipped to capture performance
across frameworks, or performance in the face of
morphological complexity? This event provoked ac-
tive discussions and led to the establishment of a
series of SPMRL events for the discussion of shared
challenges and cross-fertilization among researchers
working on parsing MRLs.
The body of work on MRLs that was accumulated
through the SPMRL workshops2 and hosting ACL
venues contains new results for Arabic (Attia et al,
2010; Marton et al, 2013a), Basque (Bengoetxea
and Gojenola, 2010), Croatian (Agic et al, 2013),
French (Seddah et al, 2010; Candito and Seddah,
2010; Sigogne et al, 2011), German (Rehbein, 2011),
Hebrew (Tsarfaty and Sima?an, 2010; Goldberg and
1http://alpage.inria.fr/iwpt09/panel.en.
html
2See http://www.spmrl.org/ and related workshops.
Elhadad, 2010a), Hindi (Ambati et al, 2010), Ko-
rean (Chung et al, 2010; Choi and Palmer, 2011) and
Spanish (Le Roux et al, 2012), Tamil (Green et al,
2012), amongst others. The awareness of the model-
ing challenges gave rise to new lines of work on top-
ics such as joint morpho-syntactic processing (Gold-
berg and Tsarfaty, 2008), Relational-Realizational
Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold-
berg, 2011), PLCFRS parsing (Kallmeyer and Maier,
2013), the use of factored lexica (Green et al, 2013),
the use of bilingual data (Fraser et al, 2013), and
more developments that are currently under way.
With new models and data, and with lingering in-
terest in parsing non-standard English data, questions
begin to emerge, such as: What is the realistic per-
formance of parsing MRLs using today?s methods?
How do the different models compare with one an-
other? How do different representation types deal
with parsing one particular language? Does the suc-
cess of a parsing model on a language correlate with
its representation type and learning method? How to
parse effectively in the face of resource scarcity? The
first step to answering all of these questions is pro-
viding standard sets of comparable size, streamlined
parsing scenarios, and evaluation metrics, which are
our main goals in this SPMRL shared task.
2.2 Where We Are At: The Need for
Cross-Framework, Realistic, Evaluation
Procedures
The present task serves as the first attempt to stan-
dardize the data sets, parsing scenarios, and evalu-
ation metrics for MRL parsing, for the purpose of
gaining insights into parsers? performance across lan-
guages. Ours is not the first cross-linguistic task on
statistical parsing. As mentioned earlier, two previ-
ous CoNLL shared tasks focused on cross-linguistic
dependency parsing and covered thirteen different
languages (Buchholz and Marsi, 2006; Nivre et al,
2007a). However, the settings of these tasks, e.g.,
in terms of data set sizes or parsing scenarios, made
it difficult to draw conclusions about strengths and
weaknesses of different systems on parsing MRLs.
A key aspect to consider is the relation between
input tokens and tree terminals. In the standard sta-
tistical parsing setup, every input token is assumed
to be a terminal node in the syntactic parse tree (after
deterministic tokenization of punctuation). In MRLs,
148
morphological processes may have conjoined several
words into a single token. Such tokens need to be seg-
mented and their analyses need to be disambiguated
in order to identify the nodes in the parse tree. In
previous shared tasks on statistical parsing, morpho-
logical information was assumed to be known in ad-
vance in order to make the setup comparable to that
of parsing English. In realistic scenarios, however,
morphological analyses are initially unknown and are
potentially highly ambiguous, so external resources
are used to predict them. Incorrect morphological
disambiguation sets a strict ceiling on the expected
performance of parsers in real-world scenarios. Re-
sults reported for MRLs using gold morphological
information are then, at best, optimistic.
One reason for adopting this less-than-realistic
evaluation scenario in previous tasks has been the
lack of sound metrics for the more realistic scenario.
Standard evaluation metrics assume that the number
of terminals in the parse hypothesis equals the num-
ber of terminals in the gold tree. When the predicted
morphological segmentation leads to a different num-
ber of terminals in the gold and parse trees, standard
metrics such as ParsEval (Black et al, 1991) or At-
tachment Scores (Buchholz and Marsi, 2006) fail
to produce a score. In this task, we use TedEval
(Tsarfaty et al, 2012b), a metric recently suggested
for joint morpho-syntactic evaluation, in which nor-
malized tree-edit distance (Bille, 2005) on morpho-
syntactic trees allows us to quantify the success on
the joint task in realistic parsing scenarios.
Finally, the previous tasks focused on dependency
parsing. When providing both constituency-based
and dependency-based tracks, it is interesting to com-
pare results across these frameworks so as to better
understand the differences in performance between
parsers of different types. We are now faced with
an additional question: how can we compare pars-
ing results across different frameworks? Adopting
standard metrics will not suffice as we would be com-
paring apples and oranges. In contrast, TedEval is
defined for both phrase structures and dependency
structures through the use of an intermediate repre-
sentation called function trees (Tsarfaty et al, 2011;
Tsarfaty et al, 2012a). Using TedEval thus allows us
to explore both dependency and constituency parsing
frameworks and meaningfully compare the perfor-
mance of parsers of different types.
3 Defining the Shared-Task
3.1 Input and Output
We define a parser as a structure prediction function
that maps sequences of space-delimited input tokens
(henceforth, tokens) in a language to a set of parse
trees that capture valid morpho-syntactic structures
in that language. In the case of constituency parsing,
the output structures are phrase-structure trees. In de-
pendency parsing, the output consists of dependency
trees. We use the term tree terminals to refer to the
leaves of a phrase-structure tree in the former case
and to the nodes of a dependency tree in the latter.
We assume that input sentences are represented
as sequences of tokens. In general, there may be a
many-to-many relation between input tokens and tree
terminals. Tokens may be identical to the terminals,
as is often the case in English. A token may be
mapped to multiple terminals assigned their own POS
tags (consider, e.g., the token ?isn?t?), as is the case
in some MRLs. Several tokens may be grouped into
a single (virtual) node, as is the case with multiword
expressions (MWEs) (consider ?pomme de terre? for
?potatoe?). This task covers all these cases.
In the standard setup, all tokens are tree terminals.
Here, the task of a parser is to predict a syntactic
analysis in which the tree terminals coincide with the
tokens. Disambiguating the morphological analyses
that are required for parsing corresponds to selecting
the correct POS tag and possibly a set of morpho-
logical features for each terminal. For the languages
Basque, French, German, Hungarian, Korean, Polish,
and Swedish, we assume this standard setup.
In the morphologically complex setup, every token
may be composed of multiple terminals. In this case,
the task of the parser is to predict the sequence of tree
terminals, their POS tags, and a correct tree associ-
ated with this sequence of terminals. Disambiguating
the morphological analysis therefore requires split-
ting the tokens into segments that define the terminals.
For the Semitic languages Arabic and Hebrew, we
assume this morphologically complex setup.
In the multiword expression (MWEs) setup, pro-
vided here for French only, groupings of terminals
are identified as MWEs (non-terminal nodes in con-
stituency trees, marked heads in dependency trees).
Here, the parser is required to predict how terminals
are grouped into MWEs on top of predicting the tree.
149
3.2 Data Sets
The task features nine languages from six language
families, from Germanic languages (Swedish and
German) and Romance (French) to Slavic (Polish),
Koreanic (Korean), Semitic (Arabic, Hebrew), Uralic
(Hungarian), and the language isolate Basque.
These languages cover a wide range of morpho-
logical richness, with Arabic, Basque, and Hebrew
exhibiting a high degree of inflectional and deriva-
tional morphology. The Germanic languages, Ger-
man and Swedish, have greater degrees of phrasal
ordering freedom than English. While French is not
standardly classified as an MRL, it shares MRLs char-
acteristics which pose challenges for parsing, such as
a richer inflectional system than English.
For each contributing language, we provide two
sets of annotated sentences: one annotated with la-
beled phrase-structure trees, and one annotated with
labeled dependency trees. The sentences in the two
representations are aligned at token and POS levels.
Both representations reflect the predicate-argument
structure of the same sentence, but this information
is expressed using different formal terms and thus
results in different tree structures.
Since some of our native data sets are larger than
others, we provide the training set in two sizes: Full
containing all sentences in the standard training set
of the language, and 5k containing the number of
sentences that is equivalent in size to our smallest
training set (5k sentences). For all languages, the data
has been split into sentences, and the sentences are
parsed and evaluated independently of one another.
3.3 Parsing Scenarios
In the shared task, we consider three parsing scenar-
ios, depending on how much of the morphological
information is provided. The scenarios are listed
below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided
with unambiguous gold morphological segmen-
tation, POS tags, and morphological features for
each input token.
? Predicted: In this scenario, the parser is pro-
vided with disambiguated morphological seg-
mentation. However, the POS tags and mor-
phological features for each input segment are
unknown.
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation sce-
narios. X depicts gold information, ? depicts unknown
information, to be predicted by the system.
? Raw: In this scenario, the parser is provided
with morphologically ambiguous input. The
morphological segmentation, POS tags, and
morphological features for each input token are
unknown.
The Predicted and Raw scenarios require predict-
ing morphological analyses. This may be done using
a language-specific morphological analyzer, or it may
be done jointly with parsing. We provide inputs that
support these different scenarios:
? Predicted: Gold treebank segmentation is given
to the parser. The POS tags assignment and mor-
phological features are automatically predicted
by the parser or by an external resource.
? Raw (1-best): The 1st-best segmentation and
POS tags assignment is predicted by an external
resource and given to the parser.
? Raw (all): All possible segmentations and POS
tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all shown in table 1. For languages
in which terminals equal tokens, only Gold and Pre-
dicted scenarios are considered. For Semitic lan-
guages we further provide input for both Raw (1-
best) and Raw (all) scenarios. 3
3.4 Evaluation Metrics
This task features nine languages, two different repre-
sentation types and three different evaluation scenar-
ios. In order to evaluate the quality of the predicted
structures in the different tracks, we use a combina-
tion of evaluation metrics that allow us to compare
the systems along different axes.
3The raw Arabic lattices were made available later than the
other data. They are now included in the shared task release.
150
In this section, we formally define the different
evaluation metrics and discuss how they support sys-
tem comparison. Throughout this paper, we will be
referring to different evaluation dimensions:
? Cross-Parser Evaluation in Gold/Predicted
Scenarios. Here, we evaluate the results of dif-
ferent parsers on a single data set in the Gold
or Predicted setting. We use standard evalu-
ation metrics for the different types of anal-
yses, that is, ParsEval (Black et al, 1991)
on phrase-structure trees, and Labeled At-
tachment Scores (LAS) (Buchholz and Marsi,
2006) for dependency trees. Since ParsEval is
known to be sensitive to the size and depth of
trees (Rehbein and van Genabith, 2007b), we
also provide the Leaf-Ancestor metric (Samp-
son and Babarczy, 2003), which is less sensitive
to the depth of the phrase-structure hierarchy. In
both scenarios we also provide metrics to evalu-
ate the prediction of MultiWord Expressions.
? Cross-Parser Evaluation in Raw Scenarios.
Here, we evaluate the results of different parsers
on a single data set in scenarios where morpho-
logical segmentation is not known in advance.
When a hypothesized segmentation is not iden-
tical to the gold segmentation, standard evalua-
tion metrics such as ParsEval and Attachment
Scores break down. Therefore, we use TedEval
(Tsarfaty et al, 2012b), which jointly assesses
the quality of the morphological and syntactic
analysis in morphologically-complex scenarios.
? Cross-Framework Evaluation. Here, we com-
pare the results obtained by a dependency parser
and a constituency parser on the same set of sen-
tences. In order to avoid comparing apples and
oranges, we use the unlabeled TedEval metric,
which converts all representation types inter-
nally into the same kind of structures, called
function trees. Here we use TedEval?s cross-
framework protocol (Tsarfaty et al, 2012a),
which accomodates annotation idiosyncrasies.
? Cross-Language Evaluation. Here, we com-
pare parsers for the same representation type
across different languages. Conducting a com-
plete and faithful evaluation across languages
would require a harmonized universal annota-
tion scheme (possibly along the lines of (de
Marneffe and Manning, 2008; McDonald et al,
2013; Tsarfaty, 2013)) or task based evaluation.
As an approximation we use unlabeled TedEval.
Since it is unlabeled, it is not sensitive to label
set size. Since it internally uses function-trees,
it is less sensitive to annotation idiosyncrasies
(e.g., head choice) (Tsarfaty et al, 2011).
The former two dimensions are evaluated on the full
sets. The latter two are evaluated on smaller, compa-
rable, test sets. For completeness, we provide below
the formal definitions and essential modifications of
the evaluation software that we used.
3.4.1 Evaluation Metrics for Phrase Structures
ParsEval The ParsEval metrics (Black et al, 1991)
are evaluation metrics for phrase-structure trees. De-
spite various shortcomings, they are the de-facto stan-
dard for system comparison on phrase-structure pars-
ing, used in many campaigns and shared tasks (e.g.,
(K?bler, 2008; Petrov and McDonald, 2012)). As-
sume that G and H are phrase-structure gold and
hypothesized trees respectively, each of which is rep-
resented by a set of tuples (i, A, j) where A is a
labeled constituent spanning from i to j. Assume
that g is the same as G except that it discards the
root, preterminal, and terminal nodes, likewise for h
and H . The ParsEval scores define the accuracy of
the hypothesis in terms of the normalized size of the
intersection of the constituent sets.
Precision(g, h) = |g?h||h|
Recall(g, h) = |g?h||g|
F1(g, h) = 2?P?RP+R
We evaluate accuracy on phrase-labels ignoring any
further decoration, as it is in standard practices.
Evalb, the standard software that implements Par-
sEval,4 takes a parameter file and ignores the labels
specified therein. As usual, we ignore root and POS
labels. Contrary to the standard practice, we do take
punctuation into account. Note that, as opposed to the
official version, we used the SANCL?2012 version5
modified to actually penalize non-parsed trees.
4http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Evalb
5Modified by Petrov and McDonald (2012) to be less sensi-
tive to punctuation errors.
151
Leaf-Ancestor The Leaf-Ancestor metric (Samp-
son and Babarczy, 2003) measures the similarity be-
tween the path from each terminal node to the root
node in the output tree and the corresponding path
in the gold tree. The path consists of a sequence of
node labels between the terminal node and the root
node, and the similarity of two paths is calculated
by using the Levenshtein distance. This distance is
normalized by path length, and the score of the tree
is an aggregated score of the values for all terminals
in the tree (xt is the leaf-ancestor path of t in tree x).
LA(h, g) =
?
t?yield(g) Lv(ht,gt)/(len(ht)+len(gt))
|yield(g)|
This metric was shown to be less sensitive to dif-
ferences between annotation schemes in (K?bler et
al., 2008), and was shown by Rehbein and van Gen-
abith (2007a) to evaluate trees more faithfully than
ParsEval in the face of certain annotation decisions.
We used the implementation of Wagner (2012).6
3.4.2 Evaluation Metrics for Dependency
Structures
Attachment Scores Labeled and Unlabeled At-
tachment scores have been proposed as evaluation
metrics for dependency parsing in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al, 2007a)
and have since assumed the role of standard metrics
in multiple shared tasks and independent studies. As-
sume that g, h are gold and hypothesized dependency
trees respectively, each of which is represented by
a set of arcs (i, A, j) where A is a labeled arc from
terminal i to terminal j. Recall that in the gold and
predicted settings, |g| = |h| (because the number of
terminals determines the number of arcs and hence it
is fixed). So Labeled Attachment Score equals preci-
sion and recall, and it is calculated as a normalized
size of the intersection between the sets of gold and
parsed arcs.7
Precision(g, h) = |g?h||g|
Recall(g, h) = |g?h||h|
LAS(g, h) = |g?h||g| =
|g?h|
|h|
6The original version is available at
http://www.grsampson.net/Resources.
html, ours at http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Leaf.
7http://ilk.uvt.nl/conll/software.html.
3.4.3 Evaluation Metrics for Morpho-Syntactic
Structures
TedEval The TedEval metrics and protocols have
been developed by Tsarfaty et al (2011), Tsarfaty
et al (2012a) and Tsarfaty et al (2012b) for coping
with non-trivial evaluation scenarios, e.g., comparing
parsing results across different frameworks, across
representation theories, and across different morpho-
logical segmentation hypotheses.8 Contrary to the
previous metrics, which view accuracy as a normal-
ized intersection over sets, TedEval computes the ac-
curacy of a parse tree based on the tree-edit distance
between complete trees. Assume a finite set of (pos-
sibly parameterized) edit operations A = {a1....an},
and a cost function c : A ? 1. An edit script is the
cost of a sequence of edit operations, and the edit dis-
tance of g, h is the minimal cost edit script that turns
g into h (and vice versa). The normalized distance
subtracted from 1 provides the level of accuracy on
the task. Formally, the TedEval score on g, h is de-
fined as follows, where ted is the tree-edit distance,
and the |x| (size in nodes) discards terminals and root
nodes.
TedEval(g, h) = 1?
ted(g, h)
|g|+ |h|
In the gold scenario, we are not allowed to manipu-
late terminal nodes, only non-terminals. In the raw
scenarios, we can add and delete both terminals and
non-terminals so as to match both the morphological
and syntactic hypotheses.
3.4.4 Evaluation Metrics for
Multiword-Expression Identification
As pointed out in section 3.1, the French data set is
provided with tree structures encoding both syntactic
information and groupings of terminals into MWEs.
A given MWE is defined as a continuous sequence of
terminals, plus a POS tag. In the constituency trees,
the POS tag of the MWE is an internal node of the
tree, dominating the sequence of pre-terminals, each
dominating a terminal. In the dependency trees, there
is no specific node for the MWE as such (the nodes
are the terminals). So, the first token of a MWE is
taken as the head of the other tokens of the same
MWE, with the same label (see section 4.4).
8http://www.tsarfaty.com/unipar/
download.html.
152
To evaluate performance on MWEs, we use the
following metrics.
? R_MWE, P_MWE, and F_MWE are recall, pre-
cision, and F-score over full MWEs, in which
a predicted MWE counts as correct if it has the
correct span (same group as in the gold data).
? R_MWE +POS, R_MWE +POS, and F_MWE
+POS are defined in the same fashion, except
that a predicted MWE counts as correct if it has
both correct span and correct POS tag.
? R_COMP, R_COMP, and F_COMP are recall,
precision and F-score over non-head compo-
nents of MWEs: a non-head component of MWE
counts as correct if it is attached to the head of
the MWE, with the specific label that indicates
that it is part of an MWE.
4 The SPMRL 2013 Data Sets
4.1 The Treebanks
We provide data from nine different languages anno-
tated with two representation types: phrase-structure
trees and dependency trees.9 Statistics about size,
average length, label set size, and other character-
istics of the treebanks and schemes are provided in
Table 2. Phrase structures are provided in an ex-
tended bracketed style, that is, Penn Treebank brack-
eted style where every labeled node may be extended
with morphological features expressed. Dependency
structures are provided in the CoNLL-X format.10
For any given language, the dependency and con-
stituency treebanks are aligned at the token and ter-
minal levels and share the same POS tagset and mor-
phological features. That is, any form in the CoNLL
format is a terminal of the respective bracketed tree.
Any CPOS label in the CoNLL format is the pre-
terminal dominating the terminal in the bracketed
tree. The FEATS in the CoNLL format are repre-
sented as dash-features decorated on the respective
pre-terminal node in the bracketed tree. See Fig-
ure 1(a)?1(b) for an illustration of this alignment.
9Additionally, we provided the data in TigerXML format
(Brants et al, 2002) for phrase structure trees containing cross-
ing branches. This allows the use of more powerful parsing
formalisms. Unfortunately, we received no submissions for this
data, hence we discard them in the rest of this overview.
10See http://ilk.uvt.nl/conll/.
For ambiguous morphological analyses, we pro-
vide the mapping of tokens to different segmentation
possibilities through lattice files. See Figure 1(c) for
an illustration, where lattice indices mark the start
and end positions of terminals.
For each of the treebanks, we provide a three-way
dev/train/set split and another train set containing the
first 5k sentences of train (5k). This section provides
the details of the original treebanks and their anno-
tations, our data-set preparation, including prepro-
cessing and data splits, cross-framework alignment,
and the prediction of morphological information in
non-gold scenarios.
4.2 The Arabic Treebanks
Arabic is a morphologically complex language which
has rich inflectional and derivational morphology. It
exhibits a high degree of morphological ambiguity
due to the absence of the diacritics and inconsistent
spelling of letters, such as Alif and Ya. As a conse-
quence, the Buckwalter Standard Arabic Morpholog-
ical Analyzer (Buckwalter, 2004; Graff et al, 2009)
produces an average of 12 analyses per word.
Data Sets The Arabic data set contains two tree-
banks derived from the LDC Penn Arabic Treebanks
(PATB) (Maamouri et al, 2004b):11 the Columbia
Arabic Treebank (CATiB) (Habash and Roth, 2009),
a dependency treebank, and the Stanford version
of the PATB (Green and Manning, 2010), a phrase-
structure treebank. We preprocessed the treebanks
to obtain strict token matching between the treebanks
and the morphological analyses. This required non-
trivial synchronization at the tree token level between
the PATB treebank, the CATiB treebank and the mor-
phologically predicted data, using the PATB source
tokens and CATiB feature word form as a dual syn-
chronized pivot.
The Columbia Arabic Treebank The Columbia
Arabic Treebank (CATiB) uses a dependency repre-
sentation that is based on traditional Arabic grammar
and that emphasizes syntactic case relations (Habash
and Roth, 2009; Habash et al, 2007). The CATiB
treebank uses the word tokenization of the PATB
11The LDC kindly provided their latest version of the Arabic
Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al,
2005), PATB 2 v3.1 (Maamouri et al, 2004a) and PATB 3 v3.3.
(Maamouri et al, 2009)
153
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
train:
#Sents 15,762 7,577 14,759 40,472 8,146 23,010 6,578
#Tokens 589,220 96,368 443,113 719,532 170,141 351,184 68,424
Lex. Size 36,906 25,136 27,470 77,222 40,782 11,1540 22,911
Avg. Length 37.38 12.71 30.02 17.77 20.88 15.26 10.40
Ratio #NT/#Tokens 0.19 0.82 0.34 0.60 0.59 0.60 0.94
Ratio #NT/#Sents 7.40 10.50 10.33 10.70 12.38 9.27 9.84
#Non Terminals 22 12 32 25 16 8 34
#POS tags 35 25 29 54 16 1,975 29
#total NTs 116,769 79,588 152,463 433,215 100,885 213,370 64,792
Dep. Label Set Size 9 31 25 43 417 22 27
train5k:
#Sents 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000
#Tokens 224,907 61,905 150,984 87,841 128,046 109,987 68,336 52,123 76,357
Lex. Size 19,433 18,405 15,480 17,421 15,975 29,009 29,715 18,632 14,110
Avg. Length 44.98 12.38 30.19 17.56 25.60 21.99 13.66 10.42 15.27
Ratio #NT/#Tokens 0.15 0.83 0.34 0.60 0.42 0.57 0.68 0.94 0.58
Ratio #NT/#Sents 7.18 10.33 10.32 10.58 10.97 12.57 9.29 9.87 8.96
#Non Terminals 22 12 29 23 60 16 8 34 8
#POS Tags 35 25 29 51 50 16 972 29 25
#total NTs 35,909 5,1691 51,627 52,945 54,856 62,889 46,484 49,381 44,845
Dep. Label Set Size 9 31 25 42 43 349 20 27 61
dev:
#Sents 1,985 948 1,235 5,000 500 1,051 2,066 821 494
#Tokens 73,932 13,851 38,820 76,704 11,301 29,989 30,480 8,600 9,341
Lex. Size 12,342 5,551 6,695 15,852 3,175 10,673 15,826 4,467 2,690
Avg. Length 37.24 14.61 31.43 15.34 22.60 28.53 14.75 10.47 18.90
Ratio #NT/#Tokens 0.19 0.74 0.33 0.63 0.47 047 0.63 0.94 0.48
Ratio #NT/#Sents 7.28 10.92 10.48 9.71 10.67 13.66 9.33 9.90 9.10
#Non Terminals 21 11 27 24 55 16 8 31 8
#POS Tags 32 23 29 50 47 16 760 29 24
#total NTs 14,452 10,356 12,951 48,560 5,338 14,366 19,283 8,132 4,496
Dep. Label Set Size 9 31 25 41 42 210 22 26 59
test:
#Sents 1959 946 2541 5000 716 1009 2287 822 666
#Tokens 73878 11457 75216 92004 16998 19908 33766 8545 10690
Lex. Size 12254 4685 10048 20149 4305 7856 16475 4336 3112
Avg. Length 37.71 12.11 29.60 18.40 23.74 19.73 14.76 10.39 16.05
Ratio #NT/#Tokens 0.19 0.83 0.34 0.60 0.47 0.62 0.61 0.95 0.57
Ratio #NT/#Sents 7.45 10.08 10.09 11.07 11.17 12.26 9.02 9.94 9.18
#Non Terminals 22 12 30 23 54 15 8 31 8
#POS Tags 33 22 30 52 46 16 809 27 25
#total NTs 14,610 9,537 25,657 55,398 8,001 12,377 20,640 8,175 6,118
Dep. Label Set Size 9 31 26 42 41 183 22 27 56
Table 2: Overview of participating languages and treebank properties. ?Sents? = number of sentences, ?Tokens? =
number of raw surface forms. ?Lex. size? and ?Avg. Length? are computed in terms of tagged terminals. ?NT? = non-
terminals in constituency treebanks, ?Dep Labels? = dependency labels on the arcs of dependency treebanks. ? A more
comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.
154
(a) Constituency Tree
% % every line is a single tree in a bracketed Penn Treebank format
(ROOT (S (NP ( NNP-#pers=3|num=sing# John))(VP ( VB-#pers=3|num=sing# likes)(NP ( NNP-#pers=3|num=sing# Mary)))))
(b) Dependency Tree
%% every line describes a terminal: terminal-id form lemma CPOS FPOS FEATS Head Rel PHead PRel
1 John John NNP NNP pers=3|num=sing 2 sbj _ _
2 likes like VB VB pers=3|num=sing 0 root _ _
3 Mary Mary NNP NNP pers=3|num=sing 2 obj _ _
Input Lattice
0 1 2 3 4 5 6
1:AIF/NN
1:AIF/VB
1:AIF/NNT
2:LA/RB
3:NISH/VB
3:NISH/NN
4:L/PREP
4:LHSTIR/VB
4:HSTIR/VB
5:ZAT/PRP
%% every line describes a terminal: start-id end-id form lemma CPOS FPOS FEATS token-id
0 1 AIF AIF NN NN _ 1
0 1 AIF AIF NNT NNT _ 1
0 1 AIF AIF VB VB _ 1
1 2 LA LA RB RB _ 2
2 3 NISH NISH VB VB _ 3
2 3 NISH NISH NN NN _ 3
3 5 LHSTIR HSTIR VB VB _ 4
3 4 L L PREP PREP _ 4
4 5 HSTIR HSTIR VB VB _ 4
5 6 ZAT ZAT PRP PRP _ 5
Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example.
Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield
of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological
features, in the FEATS CoNLL format.
and employs a reduced POS tagset consisting of six
tags only: NOM (non-proper nominals including
nouns, pronouns, adjectives and adverbs), PROP
(proper nouns), VRB (active-voice verbs), VRB-
PASS (passive-voice verbs), PRT (particles such as
prepositions or conjunctions) and PNX (punctuation).
(This stands in extreme contrast with the Buckwalter
Arabic tagset (PATB official tagset) which is almost
500 tags.) To obtain these dependency trees, we used
the constituent-to-dependency tool (Habash and Roth,
2009). Additional CATiB trees were annotated di-
rectly, but we only use the portions that are converted
from phrase-structure representation, to ensure that
the constituent and dependency yields can be aligned.
The Stanford Arabic Phrase Structure Treebank
In order to stay compatible with the state of the art,
we provide the constituency data set with most of the
pre-processing steps of Green and Manning (2010),
as they were shown to improve baseline performance
on the PATB parsing considerably.12
To convert the original PATB to preprocessed
phrase-structure trees ? la Stanford, we first discard
all trees dominated by X, which indicates errors and
non-linguistic text. At the phrasal level, we collapse
unary chains with identical categories like NP? NP.
We finally remove all traces, but, unlike Green and
Manning (2010), we keep all function tags.
In the original Stanford instance, the pre-terminal
morphological analyses were mapped to the short-
ened Bies tag set provided with the treebank (where
Determiner markers, ?DT?, were added to definite
noun and adjectives, resulting in 32 POS tags). Here
we use the Kulick tagset (Kulick et al, 2006) for
12Both the corpus split and pre-processing code are available
with the Stanford parser at http://nlp.stanford.edu/
projects/arabic.shtml.
155
pre-terminal categories in the phrase-structure trees,
where the Bies tag set is included as a morphological
feature (stanpos) in our PATB instance.
Adapting the Data to the Shared Task We con-
verted the CATiB representation to the CoNLL rep-
resentation and added a ?split-from-previous? and
?split-from-next? markers as in LDC?s tree-terminal
fields.
A major difference between the CATiB treebank
and the Stanford treebank lies in the way they han-
dle paragraph annotations. The original PATB con-
tains sequences of annotated trees that belong to a
same discourse unit (e.g., paragraph). While the
CATiB conversion tool considers each sequence a
single parsing unit, the Stanford pre-processor treats
each such tree structure rooted at S, NP or Frag as
a tree spanning a single sentence. To be compati-
ble with the predicted morphology data which was
bootstrapped and trained on the CATiB interpretation,
we deterministically modified the original PATB by
adding pseudo XP root nodes, so that the Stanford
pre-proprecessor will generate the same tree yields
as the CATiB treebank.
Another important aspect of preprocessing (often-
delegated as a technicality in the Arabic parsing lit-
erature) is the normalization of token forms. Most
Arabic parsing work used transliterated text based on
the schemes proposed by Buckwalter (2002). The
transliteration schemes exhibit some small differ-
ences, but enough to increase the out-of-vocabulary
rate by a significant margin (on top of strictly un-
known morphemes). This phenomenon is evident in
the morphological analysis lattices (in the predicted
dev set there is a 6% OOV rate without normalization,
and half a point reduction after normalization is ap-
plied, see (Habash et al, 2009b; Green and Manning,
2010)). This rate is much lower for gold tokenized
predicted data (with an OOV rate of only 3.66%,
similar to French for example). In our data set, all
tokens are minimally normalized: no diacritics, no
normalization.13
Data Splits For the Arabic treebanks, we use the
data split recommended by the Columbia Arabic and
Dialect Modeling (CADiM) group (Diab et al, 2013).
13Except for the minimal normalization present in MADA?s
back-end tools. This script was provided to the participants.
The data of the LDC first three annotated Arabic Tree-
banks (ATB1, ATB2 and ATB3) were divided into
roughly a 10/80/10% dev/train/test split by word vol-
ume. When dividing the corpora, document bound-
aries were maintained. The train5k files are simply
the first 5,000 sentences of the training files.
POS Tagsets Given the richness of Arabic mor-
phology, there are multiple POS tag sets and tokeniza-
tion schemes that have been used by researchers, (see,
e.g., Marton et al (2013a)). In the shared task, we fol-
low the standard PATB tokenization which splits off
several categories of orthographic clitics, but not the
definite article Al+. On top of that, we consider three
different POS tag sets with different degrees of gran-
ularity: the Buckwalter tag set (Buckwalter, 2004),
the Kulick Reduced Tag set (Kulick et al, 2006), and
the CATiB tag set (Habash et al, 2009a), considering
that granularity of the morphological analyses may
affect syntactic processing. For more information see
Habash (2010).
Predicted Morphology To prepare input for the
Raw scenarios (?3.3), we used the MADA+TOKAN
system (Habash et al, 2009b). MADA is a system
for morphological analysis and disambiguation of
Arabic. It can predict the 1-best tokenization, POS
tags, lemmas and diacritization in one fell swoop.
The MADA output was also used to generate the
lattice files for the Raw-all scenario.
To generate input for the gold token / predicted
tag input scenario, we used Morfette (Chrupa?a et al,
2008), a joint lemmatization and POS tagging model
based on an averaged perceptron. We generated two
tagging models, one trained with the Buckwalter tag
set, and the other with the Kulick tag set. Both were
mapped back to the CATiB POS tag set such that all
predicted tags are contained in the feature field.14
4.3 The Basque Treebank
Basque is an agglutinative language with a high ca-
pacity to generate inflected wordforms, with free
constituent order of sentence elements with respect
to the main verb. Contrary to many other treebanks,
the Basque treebank was originally annotated with
dependency trees, which were later on converted to
constituency trees.
14A conversion script from the rich Buckwalter tagset to
CoNLL-like features was provided to the participants.
156
The Basque Dependency Treebank (BDT) is a
dependency treebank in its original design, due to
syntactic characteristics of Basque such as its free
word order. Before the syntactic annotation, mor-
phological analysis was performed, using the Basque
morphological analyzer of Aduriz et al (2000). In
Basque each lemma can generate thousands of word-
forms ? differing in morphological properties such
as case, number, tense, or different types of subordi-
nation for verbs. If only POS category ambiguity is
resolved, the analyses remain highly ambiguous.
For the main POS category, there is an average of
1.55 interpretations per wordform, which rises to 2.65
for the full morpho-syntactic information, resulting
in an overall 64% of ambiguous wordforms. The
correct analysis was then manually chosen.
The syntactic trees were manually assigned. Each
word contains its lemma, main POS category, POS
subcategory, morphological features, and the la-
beled dependency relation. Each form indicates mor-
phosyntactic features such as case, number and type
of subordination, which are relevant for parsing.
The first version of the Basque Dependency Tree-
bank, consisting of 3,700 sentences (Aduriz et al,
2003), was used in the CoNLL 2007 Shared Task on
Dependency Parsing (Nivre et al, 2007a). The cur-
rent shared task uses the second version of the BDT,
which is the result of an extension and redesign of the
original requirements, containing 11,225 sentences
(150,000 tokens).
The Basque Constituency Treebank (BCT) was
created as part of the CESS-ECE project, where the
main aim was to obtain syntactically annotated con-
stituency treebanks for Catalan, Spanish and Basque
using a common set of syntactic categories. BCT
was semi-automatically derived from the dependency
version (Aldezabal et al, 2008). The conversion pro-
duced complete constituency trees for 80% of the
sentences. The main bottlenecks have been sentence
connectors and non-projective dependencies which
could not be straightforwardly converted into projec-
tive tree structures, requiring a mechanism similar to
traces in the Penn English Treebank.
Adapting the Data to the Shared Task As the
BCT did not contain all of the original non-projective
dependency trees, we selected the set of 8,000 match-
ing sentences in both treebanks for the shared task.15
This implies that around 2k trees could not be gen-
erated and therefore were discarded. Furthermore,
the BCT annotation scheme does not contain attach-
ment for most of the punctuation marks, so those
were inserted into the BCT using a simple lower-left
attachment heuristic. The same goes for some con-
nectors that could not be aligned in the first phase.
Predicted Morphology In order to obtain pre-
dicted tags for the non-gold scenarios, we used the
following pipeline. First, morphological analysis as
described above was performed, followed by a dis-
ambiguation step. At that point, it is hard to obtain a
single interpretation for each wordform, as determin-
ing the correct interpretation for each wordform may
require knowledge of long-distance elements on top
of the free constituency order of the main phrasal el-
ements in Basque. The disambiguation is performed
by the module by Ezeiza et al (1998), which uses
a combination of knowledge-based disambiguation,
by means of Constraint Grammar (Karlsson et al,
1995; Aduriz et al, 1997), and a posterior statistical
disambiguation module, using an HMM.16
For the shared task data, we chose a setting that
disambiguates most word forms, and retains ? 97%
of the correct interpretations, leaving an ambiguity
level of 1.3 interpretations. For the remaining cases
of ambiguity, we chose the first interpretation, which
corresponds to the most frequent option. This leaves
open the investigation of more complex approaches
for selecting the most appropriate reading.17
4.4 The French Treebank
French is not a morphologically rich language per se,
though its inflectional system is richer than that of
English, and it also exhibits a limited amount of word
order variation occurring at different syntactic levels
including the word level (e.g. pre- or post-nominal
15We generated a 80/10/10 split, ? train/dev/test ? The first 5k
sentences of the train set were used as a basis for the train5k.
16Note that the statistical module can be parametrized accord-
ing to the level of disambiguation to trade off precision and
recall. For example, disambiguation based on the main cate-
gories (abstracting over morpho-syntactic features) maintains
most of the correct interpretations but still gives an output with
several interpretations per wordform.
17This is not an easy task. The ambiguity left is the hardest to
solve given that the knowledge-based and statistical disambigua-
tion processes have not been able to pick out a single reading.
157
adjective, pre- or post-verbal adverbs) and the phrase
level (e.g. possible alternations between post verbal
NPs and PPs). It also has a high degree of multi-
word expressions, that are often ambiguous with a
literal reading as a sequence of simple words. The
syntactic and MWE analysis shows the same kind of
interaction (though to a lesser extent) as morphologi-
cal and syntactic interaction in Semitic languages ?
MWEs help parsing, and syntactic information may
be required to disambiguate MWE identification.
The Data Set The French data sets were gener-
ated from the French Treebank (Abeill? et al, 2003),
which consists of sentences from the newspaper Le
Monde, manually annotated with phrase structures
and morphological information. Part of the treebank
trees are also annotated with grammatical function
tags for dependents of verbs. In the SPMRL shared
task release, we used only this part, consisting of
18,535 sentences,18 split into 14,759 sentences for
training, 1,235 sentences for development, and 2,541
sentences for the final evaluation.19
Adapting the Data to the Shared Task The con-
stituency trees are provided in an extended PTB
bracketed format, with morphological features at the
pre-terminal level only. They contain slight, auto-
matically performed, modifications with respect to
the original trees of the French treebank. The syntag-
matic projection of prepositions and complementiz-
ers was normalized, in order to have prepositions and
complementizers as heads in the dependency trees
(Candito et al, 2010).
The dependency representations are projective de-
pendency trees, obtained through automatic conver-
sion from the constituency trees. The conversion pro-
cedure is an enhanced version of the one described
by Candito et al (2010).
Both the constituency and the dependency repre-
sentations make use of coarse- and fine-grained POS
tags (CPOS and FPOS respectively). The CPOS are
the categories from the original treebank. The FPOS
18The process of functional annotation is still ongoing, the
objective of the FTB providers being to have all the 20000 sen-
tences annotated with functional tags.
19The first 9,981 training sentences correspond to the canoni-
cal 2007 training set. The development set is the same and the
last 1235 sentences of the test set are those of the canonical test
set.
are merged using the CPOS and specific morphologi-
cal information such as verbal mood, proper/common
noun distinction (Crabb? and Candito, 2008).
Multi-Word Expressions The main difference
with respect to previous releases of the bracketed
or dependency versions of the French treebank
lies in the representation of multi-word expressions
(MWEs). The MWEs appear in an extended format:
each MWE bears an FPOS20 and consists of a se-
quence of terminals (hereafter the ?components? of
the MWE), each having their proper CPOS, FPOS,
lemma and morphological features. Note though that
in the original treebank the only gold information
provided for a MWE component is its CPOS. Since
leaving this information blank for MWE components
would have provided a strong cue for MWE recog-
nition, we made sure to provide the same kind of
information for every terminal, whether MWE com-
ponent or not, by providing predicted morphological
features, lemma, and FPOS for MWE components
(even in the ?gold? section of the data set). This infor-
mation was predicted by the Morfette tool (Chrupa?a
et al, 2008), adapted to French (Seddah et al, 2010).
In the constituency trees, each MWE corresponds
to an internal node whose label is the MWE?s FPOS
suffixed by a +, and which dominates the component
pre-terminal nodes.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node (a terminal in the
CoNLL format) per MWE component. The first com-
ponent of a MWE is taken as the head of the MWE.
All subsequent components of the MWE depend on
the first one, with the special label dep_cpd. Further-
more, the first MWE component bears a feature mwe-
head equal to the FPOS of the MWE. For instance,
the MWE la veille (the day before) is an adverb, con-
taining a determiner component and a common noun
component. Its bracketed representation is (ADV+
(DET la) (NC veille)), and in the dependency repre-
sentation, the noun veille depends on the determiner
la, which bears the feature mwehead=ADV+.
Predicted Morphology For the predicted mor-
phology scenario, we provide data in which the
mwehead has been removed and with predicted
20In the current data, we did not carry along the lemma and
morphological features pertaining to the MWE itself, though this
information is present in the original trees.
158
FPOS, CPOS, lemma, and morphological features,
obtained by training Morfette on the whole train set.
4.5 The German Treebank
German is a fusional language with moderately free
word order, in which verbal elements are fixed in
place and non-verbal elements can be ordered freely
as long as they fulfill the ordering requirements of
the clause (H?hle, 1986).
The Data Set The German constituency data set
is based on the TiGer treebank release 2.2.21 The
original annotation scheme represents discontinuous
constituents such that all arguments of a predicate
are always grouped under a single node regardless of
whether there is intervening material between them
or not (Brants et al, 2002). Furthermore, punctua-
tion and several other elements, such as parentheses,
are not attached to the tree. In order to make the
constituency treebank usable for PCFG parsing, we
adapted this treebank as described shortly.
The conversion of TiGer into dependencies is a
variant of the one by Seeker and Kuhn (2012), which
does not contain empty nodes. It is based on the same
TiGer release as the one used for the constituency
data. Punctuation was attached as high as possible,
without creating any new non-projective edges.
Adapting the Data to the Shared Task For
the constituency version, punctuation and other
unattached elements were first attached to the tree.
As attachment target, we used roughly the respec-
tive least common ancestor node of the right and
left terminal neighbor of the unattached element (see
Maier et al (2012) for details), and subsequently, the
crossing branches were resolved.
This was done in three steps. In the first step, the
head daughters of all nodes were marked using a
simple heuristic. In case there was a daughter with
the edge label HD, this daughter was marked, i.e.,
existing head markings were honored. Otherwise, if
existing, the rightmost daughter with edge label NK
(noun kernel) was marked. Otherwise, as default, the
leftmost daughter was marked. In a second step, for
each continuous part of a discontinuous constituent,
a separate node was introduced. This corresponds
21This version is available from http://www.ims.
uni-stuttgart.de/forschung/ressourcen/
korpora/tiger.html
to the "raising" algorithm described by Boyd (2007).
In a third steps, all those newly introduced nodes
that did not cover the head daughter of the original
discontinuous node were deleted. For the second
and the third step, we used the same script as for the
Swedish constituency data.
Predicted Morphology For the predicted scenario,
a single sequence of POS tags and morphologi-
cal features has been assigned using the MATE
toolchain via a model trained on the train set via cross-
validation on the training set. The MATE toolchain
was used to provide predicted annotation for lem-
mas, POS tags, morphology, and syntax. In order to
achieve the best results for each annotation level, a
10-fold jackknifing was performed to provide realis-
tic features for the higher annotation levels. The pre-
dicted annotation of the 5k training set were copied
from the full data set.22
4.6 The Hebrew Treebank
Modern Hebrew is a Semitic language, characterized
by inflectional and derivational (templatic) morphol-
ogy and relatively free word order. The function
words for from/to/like/and/when/that/the are prefixed
to the next token, causing severe segmentation ambi-
guity for many tokens. In addition, Hebrew orthogra-
phy does not indicate vowels in modern texts, leading
to a very high level of word-form ambiguity.
The Data Set Both the constituency and the de-
pendency data sets are derived from the Hebrew
Treebank V2 (Sima?an et al, 2001; Guthmann et
al., 2009). The treebank is based on just over 6000
sentences from the daily newspaper ?Ha?aretz?, man-
ually annotated with morphological information and
phrase-structure trees and extended with head infor-
mation as described in Tsarfaty (2010, ch. 5). The
unlabeled dependency version was produced by con-
version from the constituency treebank as described
in Goldberg (2011). Both the constituency and depen-
dency trees were annotated with a set grammatical
function labels conforming to Unified Stanford De-
pendencies by Tsarfaty (2013).
22We also provided a predicted-all scenario, in which we
provided morphological analysis lattices with POS and mor-
phological information derived from the analyses of the SMOR
derivational morphology (Schmid et al, 2004). These lattices
were not used by any of the participants.
159
Adapting the Data to the Shared Task While
based on the same trees, the dependency and con-
stituency treebanks differ in their POS tag sets, as
well as in some of the morphological segmentation
decisions. The main effort towards the shared task
was unifying the two resources such that the two tree-
banks share the same lexical yields, and the same
pre-terminal labels. To this end, we took the layering
approach of Goldberg et al (2009), and included two
levels of POS tags in the constituency trees. The
lower level is lexical, conforming to the lexical re-
source used to build the lattices, and is shared by
the two treebanks. The higher level is syntactic, and
follows the tag set and annotation decisions of the
original constituency treebank.23 In addition, we uni-
fied the representation of morphological features, and
fixed inconsistencies and mistakes in the treebanks.
Data Split The Hebrew treebank is one of the
smallest in our language set, and hence it is provided
in only the small (5k) setting. For the sake of com-
parability with the 5k set of the other treebanks, we
created a comparable size of dev/test sets containing
the first and last 500 sentences respectively, where
the rest serve as the 5k training.24
Predicted Morphology The lattices encoding the
morphological ambiguity for the Raw (all) scenario
were produced by looking up the possible analyses
of each input token in the wide-coverage morpholog-
ical analyzer (lexicon) of the Knowledge Center for
Processing Hebrew (Itai and Wintner, 2008; MILA,
2008), with a simple heuristic for dealing with un-
known tokens. A small lattice encoding the possible
analyses of each token was produced separately, and
these token-lattices were concatenated to produce the
sentence lattice. The lattice for a given sentence may
not include the gold analysis in cases of incomplete
lexicon coverage.
The morphologically disambiguated input files for
the Raw (1-best) scenario were produced by run-
ning the raw text through the morphological disam-
23Note that this additional layer in the constituency treebank
adds a relatively easy set of nodes to the trees, thus ?inflating?
the evaluation scores compared to previously reported results.
To compensate, a stricter protocol than is used in this task would
strip one of the two POS layers prior to evaluation.
24This split is slightly different than the split in previous stud-
ies.
biguator (tagger) described in Adler and Elhadad
(2006; Goldberg et al (2008),Adler (2007). The
disambiguator is based on the same lexicon that is
used to produce the lattice files, but utilizes an extra
module for dealing with unknown tokens Adler et al
(2008). The core of the disambiguator is an HMM
tagger trained on about 70M unannotated tokens us-
ing EM, and being supervised by the lexicon.
As in the case of Arabic, we also provided data
for the Predicted (gold token / predicted morphol-
ogy) scenario. We used the same sequence labeler,
Morfette (Chrupa?a et al, 2008), trained on the con-
catenation of POS and morphological gold features,
leading to a model with respectable accuracy.25
4.7 The Hungarian Treebank
Hungarian is an agglutinative language, thus a lemma
can have hundreds of word forms due to derivational
or inflectional affixation (nominal declination and
verbal conjugation). Grammatical information is typ-
ically indicated by suffixes: case suffixes mark the
syntactic relationship between the head and its argu-
ments (subject, object, dative, etc.) whereas verbs
are inflected for tense, mood, person, number, and
the definiteness of the object. Hungarian is also char-
acterized by vowel harmony.26 In addition, there are
several other linguistic phenomena such as causa-
tion and modality that are syntactically expressed in
English but encoded morphologically in Hungarian.
The Data Set The Hungarian data set used in
the shared task is based on the Szeged Treebank,
the largest morpho-syntactic and syntactic corpus
manually annotated for Hungarian. This treebank
is based on newspaper texts and is available in
both constituent-based (Csendes et al, 2005) and
dependency-based (Vincze et al, 2010) versions.
Around 10k sentences of news domain texts were
made available to the shared task.27 Each word is
manually assigned all its possible morpho-syntactic
25POS+morphology prediction accuracy is 91.95% overall
(59.54% for unseen tokens). POS only prediction accuracy is
93.20% overall (71.38% for unseen tokens).
26When vowel harmony applies, most suffixes exist in two
versions ? one with a front vowel and another one with a back
vowel ? and it is the vowels within the stem that determine which
form of the suffix is selected.
27The original treebank contains 82,000 sentences, 1.2 million
words and 250,000 punctuation marks from six domains.
160
tags and lemmas and the appropriate one is selected
according to the context. Sentences were manu-
ally assigned a constituency-based syntactic struc-
ture, which includes information on phrase structure,
grammatical functions (such as subject, object, etc.),
and subcategorization information (i.e., a given NP
is subcategorized by a verb or an infinitive). The
constituency trees were later automatically converted
into dependency structures, and all sentences were
then manually corrected. Note that there exist some
differences in the grammatical functions applied to
the constituency and dependency versions of the tree-
bank, since some morpho-syntactic information was
coded both as a morphological feature and as dec-
oration on top of the grammatical function in the
constituency trees.
Adapting the Data to the Shared Task Origi-
nally, the Szeged Dependency Treebank contained
virtual nodes for elided material (ELL) and phonolog-
ically covert copulas (VAN). In the current version,
they have been deleted, their daughters have been
attached to the parent of the virtual node, and have
been given complex labels, e.g. COORD-VAN-SUBJ,
where VAN is the type of the virtual node deleted,
COORD is the label of the virtual node and SUBJ is
the label of the daughter itself. When the virtual node
was originally the root of the sentence, its daughter
with a predicative (PRED) label has been selected as
the new root of the sentence (with the label ROOT-
VAN-PRED) and all the other daughters of the deleted
virtual node have been attached to it.
Predicted Morphology In order to provide the
same POS tag set for the constituent and dependency
treebanks, we used the dependency POS tagset for
both treebank instances. Both versions of the tree-
bank are available with gold standard and automatic
morphological annotation. The automatic POS tag-
ging was carried out by a 10-fold cross-validation
on the shared task data set by magyarlanc, a natu-
ral language toolkit for processing Hungarian texts
(segmentation, morphological analysis, POS tagging,
and dependency parsing). The annotation provides
POS tags and deep morphological features for each
input token (Zsibrita et al, 2013).28
28The full data sets of both the constituency and de-
pendency versions of the Szeged Treebank are available at
4.8 The Korean Treebank
The Treebank The Korean corpus is generated by
collecting constituent trees from the KAIST Tree-
bank (Choi et al, 1994), then converting the con-
stituent trees to dependency trees using head-finding
rules and heuristics. The KAIST Treebank consists
of about 31K manually annotated constituent trees
from 97 different sources (e.g., newspapers, novels,
textbooks). After filtering out trees containing an-
notation errors, a total of 27,363 trees with 350,090
tokens are collected.
The constituent trees in the KAIST Treebank29 also
come with manually inspected morphological analy-
sis based on ?eojeol?. An eojeol contains root-forms
of word tokens agglutinated with grammatical affixes
(e.g., case particles, ending markers). An eojeol can
consist of more than one word token; for instance, a
compound noun ?bus stop? is often represented as
one eojeol in Korean, ????????????, which can be
broken into two word tokens,???? (bus) and????????
(stop). Each eojeol in the KAIST Treebank is sepa-
rated by white spaces regardless of punctuation. Fol-
lowing the Penn Korean Treebank guidelines (Han
et al, 2002), punctuation is separated as individual
tokens, and parenthetical notations surrounded by
round brackets are grouped into individual phrases
with a function tag (PRN in our corpus).
All dependency trees are automatically converted
from the constituent trees. Unlike English, which
requires complicated head-finding rules to find the
head of each phrase (Choi and Palmer, 2012), Ko-
rean is a head final language such that the rightmost
constituent in each phrase becomes the head of that
phrase. Moreover, the rightmost conjunct becomes
the head of all other conjuncts and conjunctions in
a coordination phrase, which aligns well with our
head-final strategy.
The constituent trees in the KAIST Treebank do
not consist of function tags indicating syntactic or
semantic roles, which makes it difficult to generate
dependency labels. However, it is possible to gener-
ate meaningful labels by using the rich morphology
in Korean. For instance, case particles give good
the following website: www.inf.u-szeged.hu/rgai/
SzegedTreebank, and magyarlanc is downloadable from:
www.inf.u-szeged.hu/rgai/magyarlanc.
29See Lee et al (1997) for more details about the bracketing
guidelines of the KAIST Treebank.
161
indications of what syntactic roles eojeols with such
particles should take. Given this information, 21
dependency labels were generated according to the
annotation scheme proposed by Choi (2013).
Adapting the Data to the Shared Task All details
concerning the adaptation of the KAIST treebank
to the shared task specifications are found in Choi
(2013). Importantly, the rich KAIST treebank tag set
of 1975 POS tag types has been converted to a list of
CoNLL-like feature-attribute values refining coarse
grained POS categories.
Predicted Morphology Two sets of automatic
morphological analyses are provided for this task.
One is generated by the HanNanum morphological
analyzer.30 The HanNanum morphological ana-
lyzer gives the same morphemes and POS tags as the
KAIST Treebank. The other is generated by the Se-
jong morphological analyzer.31 The Sejong morpho-
logical analyzer gives a different set of morphemes
and POS tags as described in Choi and Palmer (2011).
4.9 The Polish Treebank
The Data Set Sk?adnica is a constituency treebank
of Polish (Wolin?ski et al, 2011; S?widzin?ski and
Wolin?ski, 2010). The trees were generated with
a non-probabilistic DCG parser S?wigra and then
disambiguated and validated manually. The ana-
lyzed texts come from the one-million-token sub-
corpus of the National Corpus of Polish (NKJP,
(Przepi?rkowski et al, 2012)) manually annotated
with morpho-syntactic tags.
The dependency version of Sk?adnica is a re-
sult of an automatic conversion of manually disam-
biguated constituent trees into dependency structures
(Wr?blewska, 2012). The conversion was an entirely
automatic process. Conversion rules were based
on morpho-syntactic information, phrasal categories,
and types of phrase-structure rules encoded within
constituent trees. It was possible to extract dependen-
cies because the constituent trees contain information
about the head of the majority of constituents. For
other constituents, heuristics were defined in order to
select their heads.
30http://kldp.net/projects/hannanum
31http://www.sejong.or.kr
The version of Sk?adnica used in the shared task
comprises parse trees for 8,227 sentences.32
Predicted Morphology For the shared task Pre-
dicted scenario, an automatic morphological an-
notation was generated by the PANTERA tagger
(Acedan?ski, 2010).
4.10 The Swedish Treebank
Swedish is moderately rich in inflections, including
a case system. Word order obeys the verb second
constraint in main clauses but is SVO in subordinate
clauses. Main clause order is freer than in English
but not as free as in some other Germanic languages,
such as German. Also, subject agreement with re-
spect to person and number has been dropped in
modern Swedish.
The Data Set The Swedish data sets are taken
from the Talbanken section of the Swedish Treebank
(Nivre and Megyesi, 2007). Talbanken is a syntacti-
cally annotated corpus developed in the 1970s, orig-
inally annotated according to the MAMBA scheme
(Teleman, 1974) with a syntactic layer consisting
of flat phrase structure and grammatical functions.
The syntactic annotation was later automatically con-
verted to full phrase structure with grammatical func-
tions and from that to dependency structure, as de-
scribed by Nivre et al (2006).
Both the phrase structure and the dependency
version use the functional labels from the original
MAMBA scheme, which provides a fine-grained clas-
sification of syntactic functions with 65 different la-
bels, while the phrase structure annotation (which
had to be inferred automatically) uses a coarse set
of only 8 labels. For the release of the Swedish tree-
bank, the POS level was re-annotated to conform to
the current de facto standard for Swedish, which is
the Stockholm-Ume? tagset (Ejerhed et al, 1992)
with 25 base tags and 25 morpho-syntactic features,
which together produce over 150 complex tags.
For the shared task, we used version 1.2 of the
treebank, where a number of conversion errors in
the dependency version have been corrected. The
phrase structure version was enriched by propagating
morpho-syntactic features from preterminals (POS
32Sk?adnica is available from http://zil.ipipan.waw.
pl/Sklicense.
162
tags) to higher non-terminal nodes using a standard
head percolation table, and a version without crossing
branches was derived using the lifting strategy (Boyd,
2007).
Adapting the Data to the Shared Task Explicit
attribute names were added to the feature field and the
split was changed to match the shared task minimal
training set size.
Predicted Morphology POS tags and morpho-
syntactic features were produced using the Hun-
PoS tagger (Hal?csy et al, 2007) trained on the
Stockholm-Ume? Corpus (Ejerhed and K?llgren,
1997).
5 Overview of the Participating Systems
With 7 teams participating, more than 14 systems for
French and 10 for Arabic and German, this shared
task is on par with the latest large-scale parsing evalu-
ation campaign SANCL 2012 (Petrov and McDonald,
2012). The present shared task was extremely de-
manding on our participants. From 30 individuals or
teams who registered and obtained the data sets, we
present results for the seven teams that accomplished
successful executions on these data in the relevant
scenarios in the given the time frame.
5.1 Dependency Track
Seven teams participated in the dependency track.
Two participating systems are based on MaltParser:
MALTOPTIMIZER (Ballesteros, 2013) and AI:KU
(Cirik and S?ensoy, 2013). MALTOPTIMIZER uses
a variant of MaltOptimizer (Ballesteros and Nivre,
2012) to explore features relevant for the processing
of morphological information. AI:KU uses a combi-
nation of MaltParser and the original MaltOptimizer.
Their system development has focused on the inte-
gration of an unsupervised word clustering method
using contextual and morphological properties of the
words, to help combat sparseness.
Similarly to MaltParser ALPAGE:DYALOG
(De La Clergerie, 2013) also uses a shift-reduce
transition-based parser but its training and decoding
algorithms are based on beam search. This parser is
implemented on top of the tabular logic programming
system DyALog. To the best of our knowledge, this
is the first dependency parser capable of handling
word lattice input.
Three participating teams use the MATE parser
(Bohnet, 2010) in their systems: the BASQUETEAM
(Goenaga et al, 2013), IGM:ALPAGE (Constant et
al., 2013) and IMS:SZEGED:CIS (Bj?rkelund et al,
2013). The BASQUETEAM uses the MATE parser in
combination with MaltParser (Nivre et al, 2007b).
The system combines the parser outputs via Malt-
Blender (Hall et al, 2007). IGM:ALPAGE also uses
MATE and MaltParser, once in a pipeline architec-
ture and once in a joint model. The models are com-
bined via a re-parsing strategy based on (Sagae and
Lavie, 2006). This system mainly focuses on MWEs
in French and uses a CRF tagger in combination
with several large-scale dictionaries to handle MWEs,
which then serve as input for the two parsers.
The IMS:SZEGED:CIS team participated in both
tracks, with an ensemble system. For the depen-
dency track, the ensemble includes the MATE parser
(Bohnet, 2010), a best-first variant of the easy-first
parser by Goldberg and Elhadad (2010b), and turbo
parser (Martins et al, 2010), in combination with
a ranker that has the particularity of using features
from the constituent parsed trees. CADIM (Marton et
al., 2013b) uses their variant of the easy-first parser
combined with a feature-rich ensemble of lexical and
syntactic resources.
Four of the participating teams use exter-
nal resources in addition to the parser. The
IMS:SZEGED:CIS team uses external morpholog-
ical analyzers. CADIM uses SAMA (Graff et al,
2009) for Arabic morphology. ALPAGE:DYALOG
and IGM:ALPAGE use external lexicons for French.
IGM:ALPAGE additionally uses Morfette (Chrupa?a
et al, 2008) for morphological analysis and POS
tagging. Finally, as already mentioned, AI:KU clus-
ters words and POS tags in an unsupervised fashion
exploiting additional, un-annotated data.
5.2 Constituency Track
A single team participated in the constituency parsing
task, the IMS:SZEGED:CIS team (Bj?rkelund et al,
2013). Their phrase-structure parsing system uses a
combination of 8 PCFG-LA parsers, trained using a
product-of-grammars procedure (Petrov, 2010). The
50-best parses of this combination are then reranked
by a model based on the reranker by Charniak and
163
Johnson (2005).33
5.3 Baselines
We additionally provide the results of two baseline
systems for the nine languages, one for constituency
parsing and one for dependency parsing.
For the dependency track, our baseline system is
MaltParser in its default configuration (the arc-eager
algorithm and liblinear for training). Results marked
as BASE:MALT in the next two sections report the
results of this baseline system in different scenarios.
The constituency parsing baseline is based on the
most recent version of the PCFG-LA model of Petrov
et al (2006), used with its default settings and five
split/merge cycles, for all languages.34 We use this
parser in two configurations: a ?1-best? configura-
tion where all POS tags are provided to the parser
(predicted or gold, depending on the scenario), and
another configuration in which the parser performs
its own POS tagging. These baselines are referred to
as BASE:BKY+POS and BASE:BKY+RAW respec-
tively in the following results sections. Note that
even when BASE:BKY+POS is given gold POS tags,
the Berkeley parser sometimes fails to reach a perfect
POS accuracy. In cases when the parser cannot find a
parse with the provided POS, it falls back on its own
POS tagging for all tokens.
6 Results
The high number of submitted system variants and
evaluation scenarios in the task resulted in a large
number of evaluation scores. In the following evalu-
ation, we focus on the best run for each participant,
and we aim to provide key points on the different
dimensions of analysis resulting from our evaluation
protocol. We invite our interested readers to browse
the comprehensive representation of our results on
the official shared-task results webpages.35
33Note that a slight but necessary change in the configuration
of one of our metrics, which occurred after the system submis-
sion deadline, resulted in the IMS:SZEGED:CIS team to submit
suboptimal systems for 4 languages. Their final scores are ac-
tually slightly higher and can be found in (Bj?rkelund et al,
2013).
34For Semitic languages, we used the lattice based PCFG-LA
extension by Goldberg (2011).
35http://www.spmrl.org/
spmrl2013-sharedtask-results.html.
6.1 Gold Scenarios
This section presents the parsing results in gold sce-
narios, where the systems are evaluated on gold seg-
mented and tagged input. This means that the se-
quence of terminals, POS tags, and morphological
features are provided based on the treebank anno-
tations. This scenario was used in most previous
shared tasks on data-driven parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007a; K?bler, 2008). Note
that this scenario was not mandatory. We thank our
participants for providing their results nonetheless.
We start by reviewing dependency-based parsing
results, both on the trees and on multi-word expres-
sion, and continue with the different metrics for
constituency-based parsing.
6.1.1 Dependency Parsing
Full Training Set The results for the gold parsing
scenario of dependency parsing are shown in the top
block of table 3.
Among the six systems, IMS:SZEGED:CIS
reaches the highest LAS scores, not only on aver-
age, but for every single language. This shows that
their approach of combining parsers with (re)ranking
provides robust parsing results across languages with
different morphological characteristics. The second
best system is ALPAGE:DYALOG, the third best sys-
tem is MALTOPTIMIZER. The fact that AI:KU is
ranked below the Malt baseline is due to their sub-
mission of results for 6 out of the 9 languages. Simi-
larly, CADIM only submitted results for Arabic and
ranked in the third place for this language, after the
two IMS:SZEGED:CIS runs. IGM:ALPAGE and
BASQUETEAM did not submit results for this setting.
Comparing LAS results across languages is prob-
lematic due to the differences between languages,
treebank size and annotation schemes (see section 3),
so the following discussion is necessarily tentative. If
we consider results across languages, we see that the
lowest results (around 83% for the best performing
system) are reached for Hebrew and Swedish, the
languages with the smallest data sets. The next low-
est result, around 86%, is reached for Basque. Other
languages reach similar LAS scores, around 88-92%.
German, with the largest training set, reaches the
highest LAS, 91.83%.
Interstingly, all systems have high LAS scores
on the Korean Treebank given a training set size
164
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 89.83 86.68 90.29 91.83 83.87 88.06 89.59 89.58 83.97 88.19
ALPAGE:DYALOG 85.87 80.39 87.69 88.25 80.70 79.60 88.23 86.00 79.80 84.06
MALTOPTIMIZER 87.03 82.07 85.71 86.96 80.03 83.14 89.39 80.49 77.67 83.61
BASE:MALT 82.28 69.19 79.86 79.98 76.61 72.34 88.43 77.70 75.73 78.01
AI:KU 86.39 86.98 79.42 83.67 85.16 78.87 55.61
CADIM 85.56 9.51
2) gold setting / 5k training set
IMS:SZEGED:CIS 87.35 85.69 88.73 87.70 83.87 87.21 83.38 89.16 83.97 86.34
ALPAGE:DYALOG 83.25 79.11 85.66 83.88 80.70 78.42 81.91 85.67 79.80 82.04
MALTOPTIMIZER 85.30 81.40 84.93 83.59 80.03 82.37 83.74 79.79 77.67 82.09
BASE:MALT 80.36 67.13 78.16 76.64 76.61 71.27 81.93 76.64 75.73 76.05
AI:KU 84.98 83.47 79.42 82.84 84.37 78.87 54.88
CADIM 82.67 9.19
3) predicted setting / full training set
IMS:SZEGED:CIS 86.21 85.14 85.24 89.65 80.89 86.13 86.62 87.07 82.13 85.45
ALPAGE:DYALOG 81.20 77.55 82.06 84.80 73.63 75.58 81.02 82.56 77.54 79.55
MALTOPTIMIZER 81.90 78.58 79.00 82.75 73.01 79.63 82.65 79.89 75.82 79.25
BASE:MALT 80.36 70.11 77.98 77.81 69.97 70.15 82.06 75.63 73.21 75.25
AI:KU 72.57 82.32 69.01 78.92 81.86 76.35 51.23
BASQUETEAM 84.25 84.51 88.66 84.97 80.88 47.03
IGM:ALPAGE 85.86 9.54
CADIM 83.20 9.24
4) predicted setting / 5k training set
IMS:SZEGED:CIS 83.66 83.84 83.45 85.08 80.89 85.24 80.80 86.69 82.13 83.53
MALTOPTIMIZER 79.64 77.59 77.56 79.22 73.01 79.00 75.90 79.50 75.82 77.47
ALPAGE:DYALOG 78.65 76.06 80.11 73.07 73.63 74.48 73.79 82.04 77.54 76.60
BASE:MALT 78.48 68.12 76.54 74.81 69.97 69.08 74.87 75.29 73.21 73.37
AI:KU 71.23 79.16 69.01 78.04 81.30 76.35 50.57
BASQUETEAM 83.19 82.65 84.70 84.01 80.88 46.16
IGM:ALPAGE 83.60 9.29
CADIM 80.51 8.95
Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold
show the best results per language and setting.
of approximately 23,000 sentences, which is a little
over half of the German treebank. For German, on
the other hand, only the IMS:SZEGED:CIS system
reaches higher LAS scores than for Korean. This
final observation indicates that more than treebank
size is important for comparing system performance
across treebanks. This is the reason for introducing
the reduced set scenario, in which we can see how the
participating system perform on a common ground,
albeit small.
5k Training Set The results for the gold setting
on the 5k train set are shown in the second block
of Table 3. Compared with the full training, we
see that there is a drop of around 2 points in this
setting. Some parser/language pairs are more sensi-
tive to data sparseness than others. CADIM, for in-
stance, exhibit a larger drop than MALTOPTIMIZER
on Arabic, and MALTOPTIMIZER shows a smaller
drop than IMS:SZEGED:CIS on French. On average,
among all systems that covered all languages, MALT-
OPTIMIZER has the smallest drop when moving to
5k training, possibly since the automatic feature opti-
mization may differ for different data set sizes.
Since all languages have the same number of sen-
tences in the train set, these results can give us limited
insight into the parsing complexity of the different
treebanks. Here, French, Arabic, Polish, and Korean
reach the highest LAS scores while Swedish reaches
165
Team F_MWE F_COMP F_MWE+POS
1) gold setting / full training set
AI:KU 99.39 99.53 99.34
IMS:SZEGED:CIS 99.26 99.39 99.21
MALTOPTIMIZER 98.95 98.99 0
ALPAGE:DYALOG 98.32 98.81 0
BASE:MALT 68.7 72.55 68.7
2) predicted setting / full training set
IGM:ALPAGE 80.81 81.18 77.37
IMS:SZEGED:CIS 79.45 80.79 70.48
ALPAGE:DYALOG 77.91 79.25 0
BASQUE-TEAM 77.19 79.81 0
MALTOPTIMIZER 70.29 74.25 0
BASE:MALT 67.49 71.01 0
AI:KU 0 0 0
3) predicted setting / 5k training set
IGM:ALPAGE 77.66 78.68 74.04
IMS:SZEGED:CIS 77.28 78.92 70.42
ALPAGE:DYALOG 75.17 76.82 0
BASQUETEAM 73.07 76.58 0
MALTOPTIMIZER 65.76 70.42 0
BASE:MALT 62.05 66.8 0
AI:KU 0 0 0
Table 4: Dependency Parsing: MWE results
the lowest one. Treebank variance depends not only
on the language but also on annotation decisions,
such as label set (Swedish, interestingly, has a rela-
tively rich one). A more careful comparison would
then take into account the correlation of data size,
label set size and parsing accuracy. We investigate
these correlations further in section 7.1.
6.1.2 Multiword Expressions
MWE results on the gold setting are found at
the top of Table 4. All systems, with the excep-
tion of BASE:MALT, perform exceedingly well in
identifying the spans and non-head components of
MWEs given gold morphology.36 These almost per-
fect scores are the consequence of the presence of
two gold MWE features, namely MWEHEAD and
PRED=Y, which respectively indicate the node span
of the whole MWE and its dependents, which do not
have a gold feature field. The interesting scenario is,
of course, the predicted one, where these features are
not provided to the parser, as in any realistic applica-
tion.
36Note that for the labeled measure F_MWE+POS, both
MALTOPTIMIZER and ALPAGE:DYALOG have an F-score of
zero, since they do not attempt to predict the MWE label at all.
6.1.3 Constituency Parsing
In this part, we provide accuracy results for phrase-
structure trees in terms of ParsEval F-scores. Since
ParsEval is sensitive to the non-terminals-per-word
ratio in the data set (Rehbein and van Genabith,
2007a; Rehbein and van Genabith, 2007b), and given
the fact that this ratio varies greatly within our data
set (as shown in Table 2), it must be kept in mind that
ParsEval should only be used for comparing parsing
performance over treebank instances sharing the ex-
act same properties in term of annotation schemes,
sentence length and so on. When comparing F-Scores
across different treebanks and languages, it can only
provide a rough estimate of the relative difficulty or
ease of parsing these kinds of data.
Full Training Set The F-score results for the gold
scenario are provided in the first block of Table 5.
Among the two baselines, BASE:BKY+POS fares
better than BASE:BKY+RAW since the latter selects
its own POS tags and thus cannot benefit from the
gold information. The IMS:SZEGED:CIS system
clearly outperforms both baselines, with Hebrew as
an outlier.37
As in the dependency case, the results are not
strictly comparable across languages, yet we can
draw some insights from them. We see consider-
able differences between the languages, with Basque,
Hebrew, and Hungarian reaching F-scores in the low
90s for the IMS:SZEGED:CIS system, Korean and
Polish reaching above-average F-scores, and Ara-
bic, French, German, and Swedish reaching F-scores
below the average, but still in the low 80s. The per-
formance is, again, not correlated with data set sizes.
Parsing Hebrew, with one of the smallest training
sets, obtains higher accuracy many other languages,
including Swedish, which has the same training set
size as Hebrew. It may well be that gold morphologi-
cal information is more useful for combatting sparse-
ness in languages with richer morphology (though
Arabic here would be an outlier for this conjecture),
or it may be that certain treebanks and schemes are
inherently harder to parser than others, as we investi-
gate in section 7.
For German, the language with the largest training
37It might be that the easy layer of syntactic tags benefits from
the gold POS tags provided. See section 4 for further discussion
of this layer.
166
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 82.20 90.04 83.98 82.07 91.64 92.60 86.50 88.57 85.09 86.97
BASE:BKY+POS 80.76 76.24 81.76 80.34 92.20 87.64 82.95 88.13 82.89 83.66
BASE:BKY+RAW 79.14 69.78 80.38 78.99 87.32 81.44 73.28 79.51 78.94 78.75
2) gold setting / 5k training set
IMS:SZEGED:CIS 79.47 88.45 82.25 74.78 91.64 91.87 80.10 88.18 85.09 84.65
BASE:BKY+POS 77.54 74.06 78.07 71.37 92.20 86.74 72.85 87.91 82.89 80.40
BASE:BKY+RAW 75.22 67.16 75.91 68.94 87.32 79.34 60.40 78.30 78.94 74.61
3) predicted setting / full training set
IMS:SZEGED:CIS 81.32 87.86 81.83 81.27 89.46 91.85 84.27 87.55 83.99 85.49
BASE:BKY+POS 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
BASE:BKY+RAW 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
4) predicted setting / 5k training set
IMS:SZEGED:CIS 78.85 86.65 79.83 73.61 89.46 90.53 78.47 87.46 83.99 83.21
BASE:BKY+POS 74.84 72.35 76.19 69.40 85.42 83.82 67.97 87.17 80.64 77.53
BASE:BKY+RAW 74.57 66.75 75.76 68.68 86.96 79.35 58.49 78.38 79.18 74.24
Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in
bold show the best results per language and setting.
set and the highest scores in dependency parsing,
the F-scores are at the lower end. These low scores,
which are obtained despite the larger treebank and
only moderately free word-order, are surprising. This
may be due to case syncretism; gold morphological
information exhibits its own ambiguity and thus may
not be fully utilized.
5k Training Set Parsing results on smaller com-
parable test sets are presented in the second block
of Table 5. On average, IMS:SZEGED:CIS is less
sensitive than BASE:BKY+POS to the reduced size.
Systems are not equally sensitive to reduced training
sets, and the gaps range from 0.4% to 3%, with Ger-
man and Korean as outliers (Korean suffering a 6.4%
drop in F-score and German 7.3%). These languages
have the largest treebanks in the full setting, so it is
not surprising that they suffer the most. But this in
itself does not fully explain the cross-treebank trends.
Since ParsEval scores are known to be sensitive to
the label set sizes and the depth of trees, we provide
LeafAncestor scores in the following section.
6.1.4 Leaf-Ancestor Results
The variation across results in the previous subsec-
tion may have been due to differences across annota-
tion schemes. One way to neutralize this difference
(to some extent) is to use a different metric. We
evaluated the constituency parsing results using the
Leaf-Ancestor (LA) metric, which is less sensitive
to the number of nodes in a tree (Rehbein and van
Genabith, 2007b; K?bler et al, 2008). As shown in
Table 6, these results are on a different (higher) scale
than ParsEval, and the average gap between the full
and 5k setting is lower.
Full Training Set The LA results in gold setting
for full training sets are shown in the first block of Ta-
ble 6. The trends are similar to the ParsEval F-scores.
German and Arabic present the lowest LA scores
(in contrast to the corresponding F-scores, Arabic is
a full point below German for IMS:SZEGED:CIS).
Basque and Hungarian have the highest LA scores.
Hebrew, which had a higher F-score than Basque,
has a lower LA than Basque and is closer to French.
Korean also ranks worse in the LA analysis. The
choice of evaluation metrics thus clearly impacts sys-
tem rankings ? F-scores rank some languages suspi-
ciously high (e.g., Hebrew) due to deeper trees, and
another metric may alleviate that.
5k Training Set The results for the leaf-ancestor
(LA) scores in the gold setting for the 5k training set
are shown in the second block of Table 6. Across
167
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 88.61 94.90 92.51 89.63 92.84 95.01 91.30 94.52 91.46 92.31
BASE:BKY+POS 87.85 91.55 91.74 88.47 92.69 92.52 90.82 92.81 90.76 91.02
BASE:BKY+RAW 87.05 89.71 91.22 87.77 91.29 90.62 87.11 90.58 88.97 89.37
2) gold setting / 5k training set
IMS:SZEGED:CIS 86.68 94.21 91.56 85.74 92.84 94.79 88.87 94.17 91.46 91.15
BASE:BKY+POS 86.26 90.72 89.71 84.11 92.69 92.11 86.75 92.91 90.76 89.56
BASE:BKY+RAW 84.97 88.68 88.74 83.08 91.29 89.94 81.82 90.31 88.97 87.53
3) predicted setting / full training set
IMS:SZEGED:CIS 88.45 94.50 91.79 89.32 91.95 94.90 90.13 94.11 91.05 91.80
BASE:BKY+POS 86.60 90.90 90.96 87.46 89.66 91.72 89.10 92.56 89.51 89.83
BASE:BKY+RAW 86.97 89.91 91.11 87.46 90.77 90.50 86.68 90.48 89.16 89.23
4) predicted setting / 5k training set
IMS:SZEGED:CIS 86.69 93.85 90.76 85.20 91.95 94.05 87.99 93.99 91.05 90.61
BASE:BKY+POS 84.76 89.83 89.18 83.05 89.66 91.24 84.87 92.74 89.51 88.32
BASE:BKY+RAW 84.63 88.50 89.00 82.69 90.77 89.93 81.50 90.08 89.16 87.36
Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.
parsers, IMS:SZEGED:CIS again has a smaller drop
than BASE:BKY+POS on the reduced size. German
suffers the most from the reduction of the training
set, with a loss of approximately 4 points. Korean,
however, which was also severely affected in terms
of F-scores, only loses 1.17 points in the LA score.
On average, the LA seem to reflect a smaller drop
when reducing the training set ? this underscores
again the impact of the choice of metrics on system
evaluation.
6.2 Predicted Scenarios
Gold scenarios are relatively easy since syntactically
relevant morphological information is disambiguated
in advance and is provided as input. Predicted scenar-
ios are more difficult: POS tags and morphological
features have to be automatically predicted, by the
parser or by external resources.
6.2.1 Dependency Parsing
Eight participating teams submitted dependency
results for this scenario. Two teams submitted for a
single language. Four teams covered all languages.
Full Training Set The results for the predicted
scenario in full settings are shown in the third
block of Table 3. Across the board, the re-
sults are considerably lower than the gold sce-
nario. Again, IMS:SZEGED:CIS is the best per-
forming system, followed by ALPAGE:DYALOG and
MALTOPTIMIZER. The only language for which
IMS:SZEGED:CIS is outperformed is French, for
which IGM:ALPAGE reaches higher results (85.86%
vs. 85.24%). This is due to the specialized treatment
of French MWEs in the IGM:ALPAGE system, which
is thereby shown to be beneficial for parsing in the
predicted setting.
If we compare the results for the predicted set-
ting and the gold one, given the full training set,
the IMS:SZEGED:CIS system shows small differ-
ences between 1.5 and 2 percent. The only ex-
ception is French, for which the LAS drops from
90.29% to 85.24% in the predicted setting. The
other systems show somewhat larger differences than
IMS:SZEGED:CIS, with the highest drops for Ara-
bic and Korean. The AI:KU system shows a similar
problem as IMS:SZEGED:CIS for French.
5k Training Set When we consider the predicted
setting for the 5k training set, in the last block of
Table 3, we see the same trends as comparing with
the full training set or when comparing to the gold
setting. Systems suffer from not having gold stan-
dard data, and they suffer from the small training set.
Interestingly, the loss between the different training
set sizes in the predicted setting is larger than in the
168
gold setting, but only marginally so, with a differ-
ence < 0.5. In other words, the predicted setting
adds a challenge to parsing, but it only minimally
compounds data sparsity.
6.2.2 Multiword Expressions Evaluation
In the predicted setting, shown in the second
block of table 4 for the full training set and in the
third block of the same table for the 5k training set,
we see that only two systems, IGM:ALPAGE and
IMS:SZEGED:CIS can predict the MWE label when
it is not present in the training set. IGM:ALPAGE?s
approach of using a separate classifier in combination
with external dictionaries is very successful, reach-
ing an F_MWE+POS score of 77.37. This is com-
pared to the score of 70.48 by IMS:SZEGED:CIS,
which predicts this node label as a side effect of
their constituent feature enriched dependency model
(Bj?rkelund et al, 2013). AI:KU has a zero score
for all predicted settings, which results from an erro-
neous training on the gold data rather than the pre-
dicted data.38
6.2.3 Constituency Parsing
Full Training Set The results for the predicted set-
ting with the full training set are shown in the third
block of table 5. A comparison with the gold setting
shows that all systems have a lower performance in
the predicted scenario, and the differences are in the
range of 0.88 for Arabic and 2.54 for Basque. It is
interesting to see that the losses are generally smaller
than in the dependency framework: on average, the
loss across languages is 2.74 for dependencies and
1.48 for constituents. A possible explanation can be
found in the two-dimensional structure of the con-
stituent trees, where only a subset of all nodes is
affected by the quality of morphology and POS tags.
The exception to this trend is Basque, for which the
loss in constituents is a full point higher than for de-
pendencies. Another possible explanation is that all
of our constituent parsers select their own POS tags
in one way or another. Most dependency parsers ac-
cept predicted tags from an external resource, which
puts an upper-bound on their potential performance.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the bottom
38Unofficial updated results are to to be found in (Cirik and
S?ensoy, 2013)
block of table 5. They show the same trends as the
dependency ones: The results are slightly lower than
the results obtained in gold setting and the ones uti-
lizing the full training set.
6.2.4 Leaf Ancestor Metrics
Full Training Set The results for the predicted sce-
nario with a full training set are shown in the third
block of table 6. In the LA evaluation, the loss
in moving from gold morphology are considerably
smaller than in F-scores. For most languages, the
loss is less than 0.5 points. Exceptions are French
with a loss of 0.72, Hebrew with 0.89, and Korean
with 1.17. Basque, which had the highest loss in
F-scores, only shows a minor loss of 0.4 points. Also,
the average loss of 0.41 points is much smaller than
the one in the ParsEval score, 1.48.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the last
block of table 6. These results, though considerably
lower (around 3 points), exhibit the exact same trends
as observed in the gold setting.
6.3 Realistic Raw Scenarios
The previous scenarios assume that input surface to-
kens are identical to tree terminals. For languages
such as Arabic and Hebrew, this is not always the
case. In this scenario, we evaluate the capacity of a
system to predict both morphological segmentation
and syntactic parse trees given raw, unsegmented
input tokens. This may be done via a pipeline as-
suming a 1-st best morphological analysis, or jointly
with parsing, assuming an ambiguous morpholog-
ical analysis lattice as input. In this task, both of
these scenarios are possible (see section 3). Thus,
this section presents a realistic evaluation of the par-
ticipating systems, using TedEval, which takes into
account complete morpho-syntactic parses.
Tables 7 and 8 present labeled and unlabeled
TedEval results for both constituency and depen-
dency parsers, calculated only for sentence of length
<= 70.39 We firstly observe that labeled TedEval
scores are considerably lower than unlabeled Ted-
Eval scores, as expected, since unlabeled scores eval-
uate only structural differences. In the labeled setup,
39TedEval builds on algorithms for calculating edit distance
on complete trees (Bille, 2005). In these algorithms, longer
sentences take considerably longer to evaluate.
169
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 83.34 1.63 82.54 0.67 56.47 0.67 69.51 69.51
IMS:SZEGED:CIS 89.12 8.37 87.82 5.56 86.08 8.27 86.95 86.95
CADIM 87.81 6.63 86.43 4.21 - - 43.22 86.43
MALTOPTIMIZER 86.74 5.39 85.63 3.03 83.05 5.33 84.34 84.34
ALPAGE:DYALOG 86.60 5.34 85.71 3.54 82.96 6.17 41.48 82.96
ALPAGE:DYALOG (RAW) - - - - 82.82 4.35 41.41 82.82
AI:KU - - - - 78.57 3.37 39.29 78.57
Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.
The upper part refers to constituency results, the lower part refers to dependency results
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 92.06 9.49 91.29 7.13 89.30 13.60 90.30 90.30
IMS:SZEGED:CIS 91.74 9.83 90.85 7.30 89.47 16.97 90.16 90.16
ALPAGE:DYALOG 89.99 7.98 89.46 5.67 88.33 12.20 88.90 88.90
MALTOPTIMIZER 90.09 7.08 89.47 5.56 87.99 11.64 88.73 88.73
CADIM 90.75 8.48 89.89 5.67 - - 44.95 89.89
ALPAGE:DYALOG (RAW) - - - - 87.61 10.24 43.81 87.61
AI:KU - - - - 86.70 8.98 43.35 86.70
Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.
Top upper part refers to constituency results, the lower part refers to dependency results.
the IMS:SZEGED:CIS dependency parser are the
best for both languages and data set sizes. Table 8
shows that their unlabeled constituency results reach
a higher accuracy than the next best system, their
own dependency results. However, a quick look at
the exact match metric reveals lower scores than for
its dependency counterparts.
For the dependency-based joint scenarios, there
is obviously an upper bound on parser performance
given inaccurate segmentation. The transition-based
systems, ALPAGE:DYALOG & MALTOPTIMIZER,
perform comparably on Arabic and Hebrew, with
ALPAGE:DYALOG being slightly better on both lan-
guages. Note that ALPAGE:DYALOG reaches close
results on the 1-best and the lattice-based input set-
tings, with a slight advantage for the former. This is
partly due to the insufficient coverage of the lexical
resource we use: many lattices do not contain the
gold path, so the joint prediction can only as be high
as the lattice predicted path allows.
7 Towards In-Depth Cross-Treebank
Evaluation
Section 6 reported evaluation scores across systems
for different scenarios. However, as noted, these re-
sults are not comparable across languages, represen-
tation types and parsing scenarios due to differences
in the data size, label set size, length of sentences and
also differences in evaluation metrics.
Our following discussion in the first part of this
section highlights the kind of impact that data set
properties have on the standard metrics (label set size
on LAS, non-terminal nodes per sentence on F-score).
Then, in the second part of this section we use the
TedEval cross-experiment protocols for comparative
evaluation that is less sensitive to representation types
and annotation idiosyncrasies.
7.1 Parsing Across Languages and Treebanks
To quantify the impact of treebank characteristics on
parsing parsing accuracy we looked at correlations
of treebank properties with parsing results. The most
highly correlated combinations we have found are
shown in Figures 2, 3, and 4 for the dependency track
and the constituency track (F-score and LeafAnces-
170
21/09/13 03:00SPMRL charts
Page 3 sur 3http://pauillac.inria.fr/~seddah/updated_official.spmrl_results.html
Correlation between label set size, treebank size, and mean LAS
FrP
FrP
GeP
GeP
HuP
HuP
SwP
ArP
ArP
ArG
ArG
BaP
BaP
FrG
FrG
GeG
GeG
HeP
HeG
HuG
HuG
PoP
PoP
PoG
PoG
SwG
BaG
BaG
KoP
KoP
KoG
KoG
10 50 100 500 1 000
72
74
76
78
80
82
84
86
88
90
treebank size / #labels
L
A
S
 
(
%
)
Figure 2: The correlation between treebank size, label set size, and LAS scores. x: treebank size / #labels ; y: LAS (%)
01/10/13 00:43SPMRL charts: all sent.
Page 1 sur 5file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-S?/SPMRL_FINAL/RESULTS/OFFICIAL/official_ptb-all.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, all sent.)
(13/10/01 00:34:34
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean F1
Arabic
Basque
French
German
Hebrew
Hungarian
Korean
Polish
Swedish
ArP
ArG
BaP
BaG
FrP
FrG
GeP
GeG
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
8 9 10
72
74
76
78
80
82
84
86
88
90
92
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 3: The correlation between the non terminals per sentence ratio and F-scores. x: #non terminal/ #sentence ; y:
F1 (%)
171
tor) respectively.
Figure 2 presents the LAS against the average num-
ber of tokens relative to the number of labels. The
numbers are averaged per language over all partici-
pating systems, and the size of the ?bubbles? is pro-
portional to the number of participants for a given
language setting. We provide ?bubbles? for all lan-
guages in the predicted (-P) and gold (-G) setting,
for both training set sizes. The lower dot in terms
of parsing scores always corresponds to the reduced
training set size.
Figure 2 shows a clear correlation between data-
set complexity and parsing accuracy. The simpler
the data set is (where ?simple" here translates into
large data size with a small set of labels), the higher
the results of the participating systems. The bubbles
reflects a diagonal that indicates correlation between
these dimensions. Beyond that, we see two interest-
ing points off of the diagonal. The Korean treebank
(pink) in the gold setting and full training set can be
parsed with a high LAS relative to its size and label
set. It is also clear that the Hebrew treebank (purple)
in the predicted version is the most difficult one to
parse, relative to our expectation about its complexity.
Since the Hebrew gold scenario is a lot closer to the
diagonal again, it may be that this outlier is due to the
coverage and quality of the predicted morphology.
Figure 340 shows the correlation of data complex-
ity in terms of the average number of non-terminals
per sentence, and parsing accuracy (ParsEval F-
score). Parsing accuracy is again averaged over all
participating systems for a given language. In this
figure, we see a diagonal similar to the one in figure 2,
where Arabic (dark blue) has high complexity of the
data (here interpreted as flat trees, low number of
non terminals per sentence) and low F-scores accord-
ingly. Korean (pink), Swedish (burgundy), Polish
(light green), and Hungarian (light blue) follow, and
then Hebrew (purple) is a positive outlier, possibly
due to an additional layer of ?easy" syntactic POS
nodes which increases tree size and inflates F-scores.
French (orange), Basque (red), and German (dark
green) are negative outliers, falling off the diago-
nal. German has the lowest F-score with respect to
40This figure was created from the IMS:SZEGED:CIS
(Const.) and our own PCFG-LA baseline in POS Tagged mode
(BASE:BKY+POS) so as to avoid the noise introduced by the
parser?s own tagging step (BASE:BKY+RAW).
what would be expected for the non-terminals per
sentence ratio, which is in contrast to the LAS fig-
ure where German occurs among the less complex
data set to parse. A possible explanation may be
the crossing branches in the original treebank which
were re-attached. This creates flat and variable edges
which might be hard predict accurately.
Figure 441 presents the correlation between parsing
accuracy in terms the LeafAncestor metrics (macro
averaged) and treebank complexity in terms of the
average number of non-terminals per sentence. As
in the correlation figures, the parsing accuracy is
averaged over the participanting systems for any lan-
guage. The LeafAncestor accuracy is calculated over
phrase structure trees, and we see a similar diago-
nal to the one in Figure 3 showing that flatter tree-
banks are harder (that is, are correlated with lower
averaged scores) But, its slope is less steep than for
the F-score, which confirms the observation that the
LeafAncestor metric is less sensitive than F-score to
the non-terminals-per-sentence ratio.
Similarly to Figure 3, German is a negative outlier,
which means that this treebank is harder to parse ? it
obtains lower scores on average than we would ex-
pect. As for Hebrew, it is much closer to the diagonal.
As it turns out, the "easy" POS layer that inflates the
scores does not affect the LA ratings as much.
7.2 Evaluation Across Scenarios, Languages
and Treebanks
In this section we analyze the results in cross-
scenario, cross-annotation, and cross-framework set-
tings using the evaluation protocols discussed in
(Tsarfaty et al, 2012b; Tsarfaty et al, 2011; Tsarfaty
et al, 2012a).
As a starting point, we select comparable sections
of the parsed data, based on system runs trained on
the small train set (train5k). For those, we selected
subsets containing the first 5,000 tree terminals (re-
specting sentence boundaries) of the test set. We only
used TedEval on sentences up to 70 terminals long,
and projectivized non-projective sentences in all sets.
We use the TedEval metrics to calculate scores on
both constituency and dependency structures in all
languages and all scenarios. Since the metric de-
fines one scale for all of these different cases, we can
41This figure was created under the same condition as the
F-score correlation in figure (Figure 3).
172
04/10/13 23:05SPMRL charts:
Page 1 sur 6file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-SHAREDTASK/SPMRL_FINAL/RESULTS/TESTLEAF.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, )
(13/10/04 23:05:31
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean Leaf Accuracy
ArP
ArG
BaP
BaG
FrP
FrG
GeP
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
GeG
8 9 10
74
76
78
80
82
84
86
88
90
92
94
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non
terminal/ #sentence ; y: Acc.(%)
compare the performance across annotation schemes,
assuming that those subsets are representative of their
original source.42
Ideally, we would be using labeled TedEval scores,
as the labeled parsing task is more difficult, and la-
beled parses are far more informative than unlabeled
ones. However, most constituency-based parsers do
not provide function labels as part of the output, to
be compared with the dependency arcs. Furthermore,
as mentioned earlier, we observed a huge difference
between label set sizes for the dependency runs. Con-
sequently, labeled scores will not be as informative
across treebanks and representation types. We will
therefore only use labels across scenarios for the
same language and representation type.
42We choose this sample scheme for replicability. We first
tried sampling sentences, aiming at the same average sentence
length (20), but that seemed to create artificially difficult test sets
for languages as Polish and overly simplistic ones for French or
Arabic.
7.2.1 Cross-Scenario Evaluation: raw vs. gold
One novel aspect of this shared task is the evalu-
ation on non-gold segmentation in addition to gold
morphology. One drawback is that the scenarios are
currently not using the same metrics ? the metrics
generally applied for gold and predicted scenrios can-
not apply for raw. To assess how well state of the art
parsers perform in raw scenarios compared to gold
scenarios, we present here TedEval results comparing
raw and gold systems using the evaluation protocol
of Tsarfaty et al (2012b).
Table 9 presents the labeled and unlabeled results
for Arabic and Hebrew (in Full and 5k training set-
tings), and Table 10 presents unlabeled TedEval re-
sults (for all languages) in the gold settings. The
unlabeled TedEval results for the raw settings are
substantially lower then TedEval results on the gold
settings for both languages.
When comparing the unlabeled TedEval results for
Arabic and Hebrew on the participating systems, we
see a loss of 3-4 points between Table 9 (raw) and Ta-
ble 10 (gold). In particular we see that for the best per-
173
forming systems on Arabic (IMS:SZEGED:CIS for
both constituency and dependency), the gap between
gold and realistic scenarios is 3.4 and 4.3 points,
for the constituency and the dependency parser re-
spectively. These results are on a par with results
by Tsarfaty et al (2012b), who showed for different
settings, constituency and dependency based, that
raw scenarios are considerably more difficult to parse
than gold ones on the standard split of the Modern
Hebrew treebank.
For Hebrew, the performance gap between unla-
beled TedEval in raw (Table 9) and gold (Table 10)
is even more salient, with around 7 and 8 points of
difference between the scenarios. We can only specu-
late that such a difference may be due to the difficulty
of resolving Hebrew morpho-syntactic ambiguities
without sufficient syntactic information. Since He-
brew and Arabic now have standardized morpholog-
ically and syntactically analyzed data sets available
through this task, it will be possible to investigate
further how cross-linguistic differences in morpho-
logical ambiguity affect full-parsing accuracy in raw
scenarios.
This section compared the raw and gold parsing
results only on unlabeled TedEval metrics. Accord-
ing to what we have seen so far is expected that
for labeled TedEval metrics using the same protocol,
the gap between gold and raw scenario will be even
greater.
7.2.2 Cross-Framework Evaluation:
Dependency vs. Constituency
In this section, our focus is on comparing parsing
results across constituency and dependency parsers
based on the protocol of Tsarfaty et al (2012a) We
have only one submission from IMS:SZEGED:CIS
in the constituency track, and. from the same group,
a submission on the dependency track. We only com-
pare the IMS:SZEGED:CIS results on constituency
and dependency parsing with the two baselines we
provided. The results of the cross-framework evalua-
tion protocol are shown in Table 11.
The results comparing the two variants of the
IMS:SZEGED:CIS systems show that they are very
close for all languages, with differences ranging from
0.03 for German to 0.8 for Polish in the gold setting.
It has often been argued that dependency parsers
perform better than a constituency parser, but we
notice that when using a cross framework protocol,
such as TedEval, and assuming that our test set sam-
ple is representative, the difference between the in-
terpretation of both representation?s performance is
alleviated. Of course, here the metric is unlabeled, so
it simply tells us that both kind of parsing models are
equally able to provide similar tree structures. Said
differently, the gaps in the quality of predicting the
same underlying structure across representations for
MRLs is not as large as is sometimes assumed.
For most languages, the baseline constituency
parser performs better than the dependency base-
line one, with Basque and Korean as an exception,
and at the same time, the dependency version of
IMS:SZEGED:CIS performs slightly better than their
constituent parser for most languages, with the excep-
tion of Hebrew and Hungarian. It goes to show that,
as far as these present MRL results go, there is no
clear preference for a dependency over a constituency
parsing representation, just preferences among par-
ticular models.
More generally, we can say that even if the linguis-
tic coverage of one theory is shown to be better than
another one, it does not necessarily mean that the
statistical version of the formal theory will perform
better for structure prediction. System performance
is more tightly related to the efficacy of the learning
and search algorithms, and feature engineering on
top of the selected formalism.
7.2.3 Cross-Language Evaluation: All
Languages
We conclude with an overall outlook of the Ted-
Eval scores across all languages. The results on the
gold scenario, for the small training set and the 5k
test set are presented in Table 10. We concentrate
on gold scenarios (to avoid the variation in cover-
age of external morphological analyzers) and choose
unlabeled metrics as they are not sensitive to label
set sizes. We emphasize in bold, for each parsing
system (row in the table), the top two languages that
most accurately parsed by it (boldface) and the two
languages it performed the worse on (italics).
We see that the European languages German
and Hungarian are parsed most accurately in the
constituency-based setup, with Polish and Swedish
having an advantage in dependency parsing. Across
all systems, Korean is the hardest to parse, with Ara-
174
Arabic Hebrew AVG1 SOFT AVG Arabic Hebrew AVG2 SOFT AVG2
1) Constituency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS (Bky) 83.59 56.43 70.01 70.01 92.18 88.02 90.1 90.1
2) Dependency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS 88.61 84.74 86.68 86.68 91.41 88.58 90 90
ALPAGE:DYALOG 87.20 81.65 40.83 81.65 90.74 87.44 89.09 89.09
CADIM 87.99 - 44 87.99 91.22 - 45.61 91.22
MALTOPTIMIZER 86.62 81.74 43.31 86.62 90.26 87.00 45.13 90.26
ALPAGE:DYALOG (RAW) - 82.82 41.41 82.82 - 87.43 43.72 87.43
AI:KU - 77.8 38.9 77.8 - 85.87 42.94 85.87
Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.
The upper part refers to constituency parsing and the lower part refers to dependency parsing.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) Constituency Evaluation
IMS:SZEGED:CIS (Bky) 95.35 96.91 95.98 97.12 96.22 97.92 92.91 97.19 96.65
BASE:BKY+POS 95.11 94.69 95.08 97.01 95.85 97.08 90.55 96.99 96.38
BASE:BKY+RAW 94.58 94.32 94.72 96.74 95.64 96.15 87.08 95.93 95.90
2) Dependency Evaluation
IMS:SZEGED:CIS 95.76 97.63 96.59 96.88 96.29 97.56 94.62 98.01 97.22
ALPAGE:DYALOG 93.76 95.72 95.75 96.4 95.34 95.63 94.56 96.80 96.55
BASE:MALT 94.16 95.08 94.21 94.55 94.98 95.25 94.27 95.83 95.33
AI:KU - - 95.46 96.34 95.07 96.53 - 96.88 95.87
MALTOPTIMIZER 94.91 96.82 95.23 96.32 95.46 96.30 94.69 96.06 95.90
CADIM 94.66 - - - - - - - -
Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and
a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing.
For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) gold setting
IMS:SZEGED:CIS (Bky) 95.82 97.30 96.15 97.43 96.37 98.25 94.07 97.22 96.89
IMS:SZEGED:CIS 95.87 98.06 96.61 97.46 96.31 97.93 94.62 98.04 97.24
BASE:BKY+POS 95.61 95.25 95.48 97.31 96.03 97.53 92.15 96.97 96.66
BASE:MALT 94.26 95.76 94.23 95.53 95.00 96.09 94.27 95.90 95.35
2) predicted setting
IMS:SZEGED:CIS (Bky) 95.74 97.07 96.21 97.31 96.10 98.03 94.05 96.92 96.90
IMS:SZEGED:CIS 95.18 97.67 96.15 97.09 96.22 97.63 94.43 97.50 97.02
BASE:BKY+POS 95.03 95.35 97.12 95.36 97.20 91.34 96.92 96.25
BASE:MALT 95.49 93.84 95.39 94.41 95.72 93.74 96.04 95.09
Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k
sentences and tested on 5k terminals.
bic, Hebrew and to some extent French following. It
appears that on a typological scale, Semitic and Asian
languages are still harder to parse than a range of Eu-
ropean languages in terms of structural difficulty and
complex morpho-syntactic interaction. That said,
note that we cannot tell why certain treebanks appear
more challenging to parse then others, and it is still
unclear whether the difficulty is inherent on the lan-
guage, in the currently available models, or because
of the annotation scheme and treebank consistency.43
43The latter was shown to be an important factor orthogonal
to the morphologically-rich nature of the treebank?s language
175
8 Conclusion
This paper presents an overview of the first shared
task on parsing morphologically rich languages. The
task features nine languages, exhibiting different lin-
guistic phenomena and varied morphological com-
plexity. The shared task saw submissions from seven
teams, and results produced by more than 14 different
systems. The parsing results were obtained in dif-
ferent input scenarios (gold, predicted, and raw) and
evaluated using different protocols (cross-framework,
cross-scenario, and cross-language). In particular,
this is the first time an evaluation campaign reports
on the execution of parsers in realistic, morphologi-
cally ambiguous, setting.
The best performing systems were mostly ensem-
ble systems combining multiple parser outputs from
different frameworks or training runs, or integrat-
ing a state-of-the-art morphological analyzer on top
of a carefully designed feature set. This is con-
sistent with previous shared tasks such as ConLL
2007 or SANCL?2012. However, dealing with am-
biguous morphology is still difficult for all systems,
and a promising approach, as demonstrated by AL-
PAGE:DYALOG, is to deal with parsing and morphol-
ogy jointly by allowing lattice input to the parser. A
promising generalization of this approach would be
the full integration of all levels of analysis that are
mutually informative into a joint model.
The information to be gathered from the results of
this shared task is vast, and we only scratched the
surface with our preliminary analyses. We uncov-
ered and documented insights of strategies that make
parsing systems successful: parser combination is
empirically proven to reach a robust performance
across languages, though language-specific strategies
are still a sound avenue for obtaining high quality
parsers for that individual language. The integration
of morphological analysis into the parsing needs to
be investigated thoroughly, and new approaches that
are morphologically aware need to be developed.
Our cross-parser, cross-scenario, and cross-
framework evaluation protocols have shown that, as
expected, more data is better, and that performance
on gold morphological input is significantly higher
than that in more realistic scenarios. We have shown
that gold morphological information is more help-
(Schluter and van Genabith, 2007)
ful to some languages and parsers than others, and
that it may also interact with successful identification
of multiword expressions. We have shown that dif-
ferences between dependency and constituency are
smaller than previously assumed and that properties
of the learning model and granularity of the output
labels are more influential. Finally, we observed
that languages which are typologically farthest from
English, such as Semitic and Asian languages, are
still amongst the hardest to parse, regardless of the
parsing method used.
Our cross-treebank, in-depth analysis is still pre-
liminary, owing to the limited time between the end
of the shared task and the deadline for publication
of this overview. but we nonetheless feel that our
findings may benefit researchers who aim to develop
parsers for diverse treebanks.44
A shared task is an inspection of the state of the
art, but it may also accelerate research in an area
by providing a stable data basis as well as a set of
strong baselines. The results produced in this task
give a rich picture of the issues associated with pars-
ing MRLs and initial cues towards their resolution.
This set of results needs to be further analyzed to be
fully understood, which will in turn contribute to new
insights. We hope that this shared task will provide
inspiration for the design and evaluation of future
parsing systems for these languages.
Acknowledgments
We heartily thank Miguel Ballesteros and Corentin
Ribeire for running the dependency and constituency
baselines. We warmly thank the Linguistic Data Con-
sortium: Ilya Ahtaridis, Ann Bies, Denise DiPersio,
Seth Kulick and Mohamed Maamouri for releasing
the Arabic Penn Treebank for this shared task and
for their support all along the process. We thank
Alon Itai and MILA, the knowledge center for pro-
cessing Hebrew, for kindly making the Hebrew tree-
bank and morphological analyzer available for us,
Anne Abeill? for allowing us to use the French tree-
bank, and Key-Sun Choi for the Kaist Korean Tree-
bank. We thank Grzegorz Chrupa?a for providing
the morphological analyzer Morfette, and Joachim
44The data set will be made available as soon as possible under
the license distribution of the shared-task, with the exception
of the Arabic data, which will continue to be distributed by the
LDC.
176
Wagner for his LeafAncestor implementation. We
finally thank ?zlem ?etinog?lu, Yuval Marton, Benoit
Crabb? and Benoit Sagot who have been nothing but
supportive during all that time.
At the end of this shared task (though watch out
for further updates and analyses), what remains to be
mentioned is our deep gratitude to all people involved,
either data providers or participants. Without all of
you, this shared task would not have been possible.
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Szymon Acedan?ski. 2010. A Morphosyntactic Brill Tag-
ger for Inflectional Languages. In Advances in Natural
Language Processing, volume 6233 of Lecture Notes
in Computer Science, pages 3?14. Springer-Verlag.
Meni Adler and Michael Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological dis-
ambiguation. In Proceedings COLING-ACL, pages
665?672, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolution
of unknown words for full morphological analysis. In
Proceedings of ACL-08: HLT, pages 728?736, Colum-
bus, OH.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev.
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, A D?az
de Ilarraza, et al 1997. Morphosyntactic disambigua-
tion for Basque based on the constraint grammar for-
malism. In Proceedings of RANLP, Tzigov Chark, Bul-
garia.
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki
Alegria, Xabier Arregi, Jose Maria Arriola, Xabier Ar-
tola, Koldo Gojenola, Aitor Maritxalar, Kepa Sarasola,
et al 2000. A word-grammar based morphological
analyzer for agglutinative languages. In Proceedings
of COLING, pages 1?7, Saarbr?cken, Germany.
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola,
Aitziber Atutxa, A Diaz de Ilarraza, Aitzpea Garmen-
dia, and Maite Oronoz. 2003. Construction of a
Basque dependency treebank. In Proceedings of the
2nd Workshop on Treebanks and Linguistic Theories
(TLT), pages 201?204, V?xj?, Sweden.
Zeljko Agic, Danijela Merkler, and Dasa Berovic. 2013.
Parsing Croatian and Serbian by using Croatian depen-
dency treebanks. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Seattle, WA.
I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and
K. Fern?ndez. 2008. From dependencies to con-
stituents in the reference corpus for the processing of
Basque. In Procesamiento del Lenguaje Natural, no
41 (2008), pages 147?154. XXIV edici?n del Congreso
Anual de la Sociedad Espa?ola para el Procesamiento
del Lenguaje Natural (SEPLN).
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: An optimization tool for MaltParser. In Pro-
ceedings of EACL, pages 58?62, Avignon, France.
Miguel Ballesteros. 2013. Effective morphological fea-
ture selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on
Statistical Parsing of Morphologically-Rich Languages,
pages 53?60, Seattle, WA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency parsing
of Basque. In Proceedings of the NAACL/HLT Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010), Los Angeles, CA.
Philip Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1?
3):217?239, 6.
Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking meets morphosyntax: State-of-the-art re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing
of Morphologically-Rich Languages, pages 134?144,
Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Frederick Jelinek, Judith Kla-
vans, Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991. A
procedure for quantitatively comparing the syntactic
coverage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop 1991,
pages 306?311, Pacific Grove, CA.
177
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the EMNLP-CoNLL, pages 1455?1465, Jeju,
Korea.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING, pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
Proceedings of the Linguistic Annotation Workshop,
Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York, NY.
Tim Buckwalter. 2002. Arabic morphological analyzer
version 1.0. Linguistic Data Consortium.
Tim Buckwalter. 2004. Arabic morphological analyzer
version 2.0. Linguistic Data Consortium.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito, Benoit Crabb?, and Pascal Denis. 2010.
Statistical French dependency parsing: Treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNLL,
pages 9?16, Manchester, UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Barcelona,
Spain.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI/IAAI, pages
598?603.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139, Seat-
tle, WA.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In Proceedings of Second Work-
shop on Statistical Parsing of Morphologically Rich
Languages, pages 1?11, Dublin, Ireland.
Jinho D. Choi and Martha Palmer. 2012. Guidelines
for the Clear Style Constituent to Dependency Conver-
sion. Technical Report 01-12, University of Colorado
at Boulder.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings
of the International Workshop on Sharable Natural
Language Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the
shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC, Marrakech, Morocco.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Volkan Cirik and H?sn? S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task: Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 68?75, Seat-
tle, WA.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Matthieu Constant, Marie Candito, and Djam? Seddah.
2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and
dependency parsing. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 46?52, Seattle, WA.
Anna Corazza, Alberto Lavelli, Giogio Satta, and Roberto
Zanoli. 2004. Analyzing an Italian treebank with
state-of-the-art statistical parsers. In Proceedings of
the Third Workshop on Treebanks and Linguistic Theo-
ries (TLT), T?bingen, Germany.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
D?ra Csendes, J?nos Csirik, Tibor Gyim?thy, and Andr?s
Kocsor. 2005. The Szeged treebank. In Proceedings of
the 8th International Conference on Text, Speech and
Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
Eric De La Clergerie. 2013. Exploring beam-based
shift-reduce dependency parsing with DyALog: Re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
178
Morphologically-Rich Languages, pages 81?89, Seat-
tle, WA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mona Diab, Nizar Habash, Owen Rambow, and Ryan
Roth. 2013. LDC Arabic treebanks and associated cor-
pora: Data divisions manual. Technical Report CCLS-
13-02, Center for Computational Learning Systems,
Columbia University.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Department of Linguis-
tics, Ume? University and Department of Linguistics,
Stockholm University.
Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and Mag-
nus ?str?m. 1992. The linguistic annotation system
of the Stockholm?Ume? Corpus project. Technical
Report 33, University of Ume?: Department of Linguis-
tics.
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n
Urizar, and Itziar Aduriz. 1998. Combining stochastic
and rule-based methods for disambiguation in aggluti-
native languages. In Proceedings of COLING, pages
380?384, Montr?al, Canada.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages
959?967, Columbus, OH.
Alexander Fraser, Helmut Schmid, Rich?rd Farkas, Ren-
jing Wang, and Hinrich Sch?tze. 2013. Knowledge
sources for constituent parsing of German, a morpho-
logically rich and less-configurational language. Com-
putational Linguistics, 39(1):57?85.
Iakes Goenaga, Koldo Gojenola, and Nerea Ezeiza. 2013.
Exploiting the contribution of morphological informa-
tion to parsing: the BASQUE TEAM system in the
SPRML?2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 61?67, Seattle, WA.
Yoav Goldberg and Michael Elhadad. 2010a. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of the NAACL/HLT Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Yoav Goldberg and Michael Elhadad. 2010b. An ef-
ficient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of HLT: NAACL, pages
742?750, Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of ACL, Columbus, OH.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, Columbus, OH.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities. In
Proceedings of EALC, pages 327?335, Athens, Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
David Graff, Mohamed Maamouri, Basma Bouziri, Son-
dos Krouna, Seth Kulick, and Tim Buckwalter. 2009.
Standard Arabic Morphological Analyzer (SAMA) ver-
sion 3.1. Linguistic Data Consortium LDC2009E73.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING, pages 394?402, Beijing,
China.
Nathan Green, Loganathan Ramasamy, and Zden?k
?abokrtsk?. 2012. Using an SVM ensemble system for
improved Tamil dependency parsing. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically Rich
Languages, pages 72?77, Jeju, Korea.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2013. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proceedings of the Eighth International Workshop on
Treebanks and Linguistic Theories (TLT), Groningen,
The Netherlands.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of ACL-
IJCNLP, pages 221?224, Suntec, Singapore.
Nizar Habash, Ryan Gabbard, Owen Rambow, Seth
Kulick, and Mitch Marcus. 2007. Determining case in
Arabic: Learning complex linguistic behavior requires
complex linguistic features. In Proceedings of EMNLP-
CoNLL, pages 1084?1092, Prague, Czech Republic.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009a. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009b.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools. Cairo, Egypt.
179
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publishers.
Jan Hajic?, Alena B?hmov?, Eva Hajic?ov?, and Barbora
Vidov?-Hladk?. 2000. The Prague Dependency Tree-
bank: A three-level annotation scenario. In Anne
Abeill?, editor, Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of ACL, pages 209?212, Prague, Czech Republic.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ls?en Eryig?it,
Be?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech Republic.
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Martha
Palmer, and Heejong Yi. 2002. Penn Korean Treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Information
and Computation, Jeju, Korea.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", Anmerkun-
gen ?ber die Theorie der topologischen Felder. In Ak-
ten des Siebten Internationalen Germanistenkongresses
1985, pages 329?340, G?ttingen, Germany.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable grammars.
In Proceedings of EMNLP, pages 12?22, Cambridge,
MA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of ACL,
pages 586?594, Columbus, OH.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-driven
parsing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1).
Fred Karlsson, Atro Voutilainen, Juha Heikkilae, and Arto
Anttila. 1995. Constraint Grammar: a language-
independent system for parsing unrestricted text. Wal-
ter de Gruyter.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430, Sapporo, Japan.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German? In Pro-
ceedings of EMNLP, pages 111?119, Sydney, Australia,
July.
Sandra K?bler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings of LREC, pages 2322?2329, Marrakech,
Morocco.
Sandra K?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63, Columbus, OH.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguistic
Theories Conference, pages 31?42, Prague, Czech Re-
public.
Joseph Le Roux, Benoit Sagot, and Djam? Seddah. 2012.
Statistical parsing of Spanish and data driven lemmati-
zation. In Proceedings of the Joint Workshop on Statis-
tical Parsing and Semantic Processing of Morphologi-
cally Rich Languages, pages 55?61, Jeju, Korea.
Kong Joo Lee, Byung-Gyu Chang, and Gil Chang Kim.
1997. Bracketing Guidelines for Korean Syntactic Tree
Tagged Corpus. Technical Report CS/TR-97-112, De-
partment of Computer Science, KAIST.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL, Sapporo, Japan.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2004a. Arabic Treebank: Part 2 v 2.0.
LDC catalog number LDC2004T02.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004b. The Penn Arabic Treebank:
Building a large-scale annotated Arabic corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2005. Arabic Treebank: Part 1 v 3.0. LDC
catalog number LDC2005T02.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-
deche, Wigdan Mekki, Sondos Krouna, and Basma
Bouziri. 2009. The Penn Arabic Treebank part 3 ver-
sion 3.1. Linguistic Data Consortium LDC2008E22.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing re-
visited: Restricting the fan-out to two. In Proceedings
of the Eleventh International Conference on Tree Ad-
joining Grammars and Related Formalisms (TAG+11),
Paris, France.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference. In
Proceedings of EMNLP, pages 34?44, Cambridge, MA.
180
Yuval Marton, Nizar Habash, and Owen Rambow. 2013a.
Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Yuval Marton, Nizar Habash, Owen Rambow, and Sarah
Alkhulani. 2013b. SPMRL?13 shared task system:
The CADIM Arabic dependency parser. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?80, Seat-
tle, WA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT:NAACL, pages 152?159, New York, NY.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Tackstrom, Claudia Bedini, Nuria Bertomeu Castello,
and Jungmee Lee. 2013. Universal dependency anno-
tation for multilingual parsing. In Proceedings of ACL,
Sofia, Bulgaria.
Igor Mel?c?uk. 2001. Communicative Organization in Nat-
ural Language: The Semantic-Communicative Struc-
ture of Sentences. J. Benjamins.
Knowledge Center for Processing Hebrew
MILA. 2008. Hebrew morphological analyzer.
http://mila.cs.technion.ac.il.
Antonio Moreno, Ralph Grishman, Susana Lopez, Fer-
nando Sanchez, and Satoshi Sekine. 2000. A treebank
of Spanish and its application to parsing. In Proceed-
ings of LREC, Athens, Greece.
Joakim Nivre and Be?ta Megyesi. 2007. Bootstrapping a
Swedish treeebank using cross-corpus harmonization
and annotation projection. In Proceedings of the 6th
International Workshop on Treebanks and Linguistic
Theories, pages 97?102, Bergen, Norway.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 915?932, Prague,
Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In Proceedings
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), a NAACL-HLT 2012
workshop, Montreal, Canada.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, Sydney, Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of HLT: NAACL, pages 19?
27, Los Angeles, CA.
Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa? L. G?rski,
and Barbara Lewandowska-Tomaszczyk, editors. 2012.
Narodowy Korpus Jkezyka Polskiego. Wydawnictwo
Naukowe PWN, Warsaw.
Ines Rehbein and Josef van Genabith. 2007a. Eval-
uating Evaluation Measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
NODALIDA-2007, Tartu, Estonia.
Ines Rehbein and Josef van Genabith. 2007b. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of EMNLP-CoNLL, Prague, Czech Re-
public.
Ines Rehbein. 2011. Data point selection for self-training.
In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?
67, Dublin, Ireland.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of HLT-NAACL, pages
129?132, New York, NY.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology covering
derivation, composition and inflection. In Proceedings
of LREC, Lisbon, Portugal.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
First Workshop on Statistical Parsing of Morphologi-
cally Rich Languages (SPMRL), Los Angeles, CA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a German
181
treebank. In Proceedings of LREC, pages 3132?3139,
Istanbul, Turkey.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of ACL, pages 440?448, Jeju, Korea.
Anthony Sigogne, Matthieu Constant, and Eric Laporte.
2011. French parsing enhanced with a word clustering
method based on a syntactic lexicon. In Proceedings
of the Second Workshop on Statistical Parsing of Mor-
phologically Rich Languages, pages 22?27, Dublin,
Ireland.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann,
and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues,
42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Pro-
ceedings of Text, Speech and Dialogue, pages 197?204,
Brno, Czech Republic.
Ulf Teleman. 1974. Manual f?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
Lucien Tesni?re. 1959. ?l?ments De Syntaxe Structurale.
Klincksieck, Paris.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the First Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing for morphologically rich language (SPMRL):
What, how and whither. In Proceedings of the First
workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP, Edinburgh, UK.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical pars-
ing. In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012b. Joint evaluation for segmentation and parsing.
In Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djam? Seddah, Sandra K?bler, and Joakim
Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme
of Stanford dependencies. In Proceedings of ACL,
Sofia, Bulgaria.
Veronika Vincze, D?ra Szauter, Attila Alm?si, Gy?rgy
M?ra, Zolt?n Alexin, and J?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of LREC,
Valletta, Malta.
Joachim Wagner. 2012. Detecting Grammatical Errors
with Treebank-Induced Probabilistic Parsers. Ph.D.
thesis, Dublin City University.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek
S?widzin?ski. 2011. A preliminary version of
Sk?adnica?a treebank of Polish. In Proceedings of
the 5th Language & Technology Conference, pages
299?303, Poznan?, Poland.
Alina Wr?blewska. 2012. Polish Dependency Bank. Lin-
guistic Issues in Language Technology, 7(1):1?15.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL:HLT, pages 188?193, Portland,
OR.
J?nos Zsibrita, Veronika Vincze, and Rich?rd Farkas.
2013. magyarlanc: A toolkit for morphological and
dependency parsing of Hungarian. In Proceedings of
RANLP, pages 763?771, Hissar, Bulgaria.
182
First Joint Workshop on Statistical Parsing of Morphologically Rich Languages
and Syntactic Analysis of Non-Canonical Languages, pages 97?102 Dublin, Ireland, August 23-29 2014.
Introducing the IMS-Wroc?aw-Szeged-CIS Entry at the SPMRL 2014
Shared Task: Reranking and Morphosyntax Meet Unlabeled Data?
Anders Bjo?rkelund? and O?zlem C?etinog?lu? and Agnieszka Falen?ska,?
Richa?rd Farkas? and Thomas Mu?ller? and Wolfgang Seeker? and Zsolt Sza?nto??
?Institute for Natural Language Processing University of Stuttgart, Germany
Institute of Computer Science, University of Wroc?aw, Poland
?Department of Informatics University of Szeged, Hungary
?Center for Information and Language Processing University of Munich, Germany
{anders,ozlem,muellets,seeker}@ims.uni-stuttgart.de
agnieszka.falenska@cs.uni.wroc.pl
{rfarkas,szantozs}@inf.u-szeged.hu
Abstract
We summarize our approach taken in the SPMRL 2014 Shared Task on parsing morphologically
rich languages. Our approach builds upon our contribution from last year, with a number of
modifications and extensions. Though this paper summarizes our contribution, a more detailed
description and evaluation will be presented in the accompanying volume containing notes from
the SPMRL 2014 Shared Task.
1 Introduction
This paper summarizes the approach of IMS-Wroc?aw-Szeged-CIS taken for the SPMRL 2014 Shared
Task on parsing morphologically rich languages (Seddah et al., 2014). Since this paper is a rough sum-
mary that is written before submission of test runs we refer the reader to the full description paper which
will be published after the shared task (Bjo?rkelund et al., 2014).1
The SPMRL 2014 Shared Task is a direct extension of the SPMRL 2013 Shared Task (Seddah et al.,
2013) which targeted parsing morphologically rich languages. The task involves parsing both depen-
dency and phrase-structure representations of 9 languages: Arabic, Basque, French, German, Hebrew,
Hungarian, Korean, Polish, and Swedish. The only difference between the two tasks is that large amounts
of unlabeled data are additionally available to participants for the 2014 task.
Our contribution builds upon our system from last year (Bjo?rkelund et al., 2013), with additional
features and components that try to exploit the unlabeled data. Given the limited window of time to
participate in this year?s shared task, we only contribute to the setting with predicted preprocessing,
using the largest available training data set for each language.2 We also do not participate in the Arabic
track since the shared task organizers did not provide any unlabeled data at a reasonable time.
2 Review of Last Year?s System
Our current system is based on the system we participated with in the SPMRL 2013 Shared Task. We
summarize the architecture of this system as three different components.
?Authors in alphabetical order
1Due to logistical constraints this paper had to be written before the deadlines for the actual shared task and do thus not contain
a full description of the system, nor the experimental evaluation of the same.
2In other words, no gold preprocessing or smaller training sets.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
97
2.1 Preprocessing
As the initial step of preprocessing we converted the Shared Task data from the CoNLL06 format to
CoNLL09, which required a decision on using coarse or fine grained POS tags. After a set of preliminary
experiments we picked fine POS tags where possible, except Basque and Korean.
We used MarMoT3 (Mu?ller et al., 2013) to predict POS tags and morphological features jointly. We in-
tegrated the output from external morphological analyzers as features to MarMoT. We also experimented
with the integration of predicted tags provided by the organizers and observed that these stacked models
help improve Basque, Polish, and Swedish preprocessing. The stacked models provided additional infor-
mation to our tagger since the provided predictions were coming from models trained on larger training
sets than the shared task training sets.
2.2 Dependency Parsing
The dependency parsing architecture of our SPMRL 2013 Shared Task contribution is summarized in
Figure 1. The first step combines the n-best trees of two parsers, namely the mate parser4 (Bohnet, 2010)
and a variant of the EasyFirst parser (Goldberg and Elhadad, 2010), which we call best-first parser. We
merged the 50-best analyses from these parsers into one n-best list of 50 to 100 trees. We then added
parsing scores to the n-best trees from the two parsers, and additionally from the turboparser5 (Martins
et al., 2010).
mate parser
best-first
parser
turboparser
merged list
of 50-100 best
trees/sentence
merged list
scored by
all parsers
ranker
ptb trees
Parsing Ranking
IN OUT
scores
scores
scores
features
Figure 1: Architecture of the dependency ranking system from (Bjo?rkelund et al., 2013).
The scored trees are fed into the ranking system. The ranker utilizes the parsing scores and fea-
tures coming from both constituency and dependency parses. We specified a default feature set and
experimented with additional features for each language for optimal results. We achieved over 1% LAS
improvement on all languages except a 0.3% improvement on Hungarian.
2.3 Constituency Parsing
The constituency parsing architecture advances in three steps. For all setups we removed the morphologi-
cal annotation of POS tags and the function labels of non-terminals and apply the Berkeley Parser (Petrov
et al., 2006) as our baseline. As the first setup, we replaced words with a frequency < 20 with their pre-
dicted part-of-speech and morphology tags and improved the PARSEVAL scores across languages. The
second setup employed a product grammar (Petrov, 2010), where we combined 8 different grammars
trained on the same data but with different initialization setups. As a result, the scores substantially
improved on all languages.
Finally, we conducted ranking experiments on the 50-best outputs of the product grammars. We used
a slightly modified version of the Mallet toolkit (McCallum, 2002), where the reranker is trained for the
3https://code.google.com/p/cistern/
4https://code.google.com/p/mate-tools
5http://www.ark.cs.cmu.edu/TurboParser/
98
maximum entropy objective function of Charniak and Johnson (2005) and uses the standard feature set
from Charniak and Johnson (2005) and Collins (2000). Hebrew and Polish scores remained almost the
same, whereas Basque, French, and Hungarian highly benefited from reranking.
3 Planned Additions to Last Year?s System
This year we extend our systems for both the constituency and dependency tracks to add additional
information and try to profit from unlabeled data.
3.1 Preprocessing
We use the mate-tools? lemmatizer and MarMoT to preprocess all labeled and unlabeled data. From the
SPMRL 2013 Shared Task, we learned that getting as good preprocessing as possible is an important
part of the overall improvements. Preprocessing consists of predicting lemmas, part-of-speech, and
morphological features. Preprocessing for the training data is done via 5-fold jackknifing to produce
realistic input features for the parsers. This year we do not do stacking on top of provided morphological
analyses since the annotations on the labeled and unlabeled data were inconsistent for some languages.6
3.2 Dependency Parsing
We pursue two different ways of integrating additional information into our system from the SPMRL
2013 Shared Task (Bjo?rkelund et al., 2013): supertags and co-training.
Supertags (Bangalore and Joshi, 1999) are tags that encode more syntactic information than standard
part-of-speech tags. Supertags have been used in deep grammar formalisms like CCG or HPSG to prune
the search space for the parser. The idea has been applied to dependency parsing by Foth et al. (2006)
and recently to statistical dependency parsing (Ouchi et al., 2014; Ambati et al., 2014), where supertags
are used as features rather than to prune the search space. Since the supertag set is dynamically derived
from the gold-standard syntactic structures, we can encode different kinds of information into a supertag,
in particular also morphological information. Supertags are predicted before parsing using MarMoT and
are then used as features in the mate parser and the turboparser.
We will use a variant of co-training (Blum and Mitchell, 1998) by applying two different parsers to
select additional training material from unlabeled data. We use the mate parser and the turboparser to
parse the unlabeled data provided by the organizers. We then select sentences where both parsers agree
on the structure as additional training examples following Sagae and Tsujii (2007). We then train two
more models: one on the labeled training data and the unlabeled data selected by the two parsers, and
one only on the unlabeled data. These two models are then integrated into our parsing system from 2013
as additional scorers to score the n-best list. Their scores are used as features in the ranker.
Before we parse the unlabeled data to obtain the training sentences, we filter it in order to arrive
at a cleaner corpus. Most importantly, we only keep sentences up to length 50, and which contain at
maximum two unknown words (compared to the labeled training data).
3.3 Constituency Parsing
We experiment with two approaches for improving constituency parsing:
Preterminal labelsets play an important role in constituency parsing of morphologically rich lan-
guages (Dehdari et al., 2011). Instead of removing the morphological annotation of POS tags, we use a
preterminal set which carries more linguistic information while still keeping it compact. We follow the
merge procedure for morphological feature values of Sza?nto? and Farkas (2014). This procedure outputs a
clustering of full morphological descriptions and we use the cluster IDs as preterminal labels for training
the Berkeley Parser.
Reranking at the constituency parsing side is enriched by novel features. We define feature tem-
plates exploiting co-occurrence statistics from the unlabeled datasets; automatic dependency parses of
the sentence in question (Farkas and Bohnet, 2012); Brown clusters (Brown et al., 1992); and atomic
morphological feature values (Sza?nto? and Farkas, 2014).
6The organizers later resolved this issue by patching the data, although time constraints prevented us from using the patched
data.
99
4 Conclusion
This paper describes our plans for the SPMRL 2014 Shared Task, most of which are yet to be imple-
mented. For the actual system description and our results, we refer the interested reader to (Bjo?rkelund
et al., 2014) and (Seddah et al., 2014).
Acknowledgements
Agnieszka Falen?ska is funded through the Project International computer science and applied mathemat-
ics for business study programme at the University of Wroc?aw co-financed with European Union funds
within the European Social Fund No. POKL.04.01.01-00-005/13. Richa?rd Farkas and Zsolt Sza?nto? are
funded by the European Union and the European Social Fund through the project FuturICT.hu (grant no.:
TA?MOP-4.2.2.C-11/1/KONV-2012-0013). Thomas Mu?ller is supported by a Google Europe Fellowship
in Natural Language Processing. The remaining authors are funded by the Deutsche Forschungsgemein-
schaft (DFG) via the SFB 732, projects D2 and D8 (PI: Jonas Kuhn).
We also express our gratitude to the treebank providers for each language: Arabic (Maamouri et al.,
2004; Habash and Roth, 2009; Habash et al., 2009; Green and Manning, 2010), Basque (Aduriz et al.,
2003), French (Abeille? et al., 2003), Hebrew (Sima?an et al., 2001; Tsarfaty, 2010; Goldberg, 2011;
Tsarfaty, 2013), German (Brants et al., 2002; Seeker and Kuhn, 2012), Hungarian (Csendes et al., 2005;
Vincze et al., 2010), Korean (Choi et al., 1994; Choi, 2013), Polish (S?widzin?ski and Wolin?ski, 2010),
and Swedish (Nivre et al., 2006).
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel. 2003. Building a treebank for french. In Anne Abeille?,
editor, Treebanks. Kluwer, Dordrecht.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. D??az de Ilarraza, A. Garmendia, and M. Oronoz. 2003.
Construction of a Basque dependency treebank. In TLT-03, pages 201?204.
Bharat Ram Ambati, Tejaswini Deoskar, and Mark Steedman. 2014. Improving dependency parsers using combi-
natory categorial grammar. In Proceedings of the 14th Conference of the European Chapter of the Association
for Computational Linguistics, volume 2: Short Papers, pages 159?163, Gothenburg, Sweden, April. Associa-
tion for Computational Linguistics.
Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational
Linguistics, 25(2):237?265.
Anders Bjo?rkelund, O?zlem C?etinog?lu, Richa?rd Farkas, Thomas Mu?ller, and Wolfgang Seeker. 2013. (re)ranking
meets morphosyntax: State-of-the-art results from the SPMRL 2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 135?145, Seattle, Washington,
USA, October. Association for Computational Linguistics.
Anders Bjo?rkelund, O?zlem C?etinog?lu, Agnieszka Falen?ska, Richa?rd Farkas, Thomas Mu?ller, Wolfgang Seeker,
and Zsolt Sza?nto?. 2014. The IMS-Wroc?aw-Szeged-CIS entry at the SPMRL 2014 Shared Task: Reranking and
Morphosyntax meet Unlabeled Data. In Notes of the SPMRL 2014 Shared Task on Parsing Morphologically-
Rich Languages, Dublin, Ireland, August.
Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of
the Eleventh Annual Conference on Computational Learning Theory, COLT? 98, pages 92?100, New York, NY,
USA. ACM.
Bernd Bohnet. 2010. Top Accuracy and Fast Dependency Parsing is not a Contradiction. In Proceedings of
the 23rd International Conference on Computational Linguistics (Coling 2010), pages 89?97, Beijing, China,
August. Coling 2010 Organizing Committee.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank.
In Erhard Hinrichs and Kiril Simov, editors, Proceedings of the First Workshop on Treebanks and Linguistic
Theories (TLT 2002), pages 24?41, Sozopol, Bulgaria.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18(4):467?479.
100
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.
In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05, pages
173?180.
Key-Sun Choi, Young S Han, Young G Han, and Oh W Kwon. 1994. Kaist tree bank project for korean: Present
and future development. In Proceedings of the International Workshop on Sharable Natural Language Re-
sources, pages 7?14. Citeseer.
Jinho D. Choi. 2013. Preparing korean data for the shared task on parsing morphologically rich languages. CoRR,
abs/1309.1649.
Michael Collins. 2000. Discriminative Reranking for Natural Language Parsing. In Proceedings of the Seven-
teenth International Conference on Machine Learning, ICML ?00, pages 175?182.
Do?ra Csendes, Jano?s Csirik, Tibor Gyimo?thy, and Andra?s Kocsor. 2005. The Szeged treebank. In Va?clav Ma-
tous?ek, Pavel Mautner, and Toma?s? Pavelka, editors, Text, Speech and Dialogue: Proceedings of TSD 2005.
Springer.
Jon Dehdari, Lamia Tounsi, and Josef van Genabith. 2011. Morphological features for parsing morphologically-
rich languages: A case of arabic. In Proceedings of the Second Workshop on Statistical Parsing of Morphologi-
cally Rich Languages, pages 12?21, Dublin, Ireland, October. Association for Computational Linguistics.
Richa?rd Farkas and Bernd Bohnet. 2012. Stacking of dependency and phrase structure parsers. In Proceedings of
COLING 2012, pages 849?866, Mumbai, India, December. The COLING 2012 Organizing Committee.
Kilian A. Foth, Tomas By, and Wolfgang Menzel. 2006. Guiding a constraint dependency parser with supertags.
In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages 289?296, Sydney, Australia, July. Association for Com-
putational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An Efficient Algorithm for Easy-First Non-Directional Dependency
Parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages 742?750, Los Angeles, California, June. Association for
Computational Linguistics.
Yoav Goldberg. 2011. Automatic syntactic processing of Modern Hebrew. Ph.D. thesis, Ben Gurion University of
the Negev.
Spence Green and Christopher D. Manning. 2010. Better arabic parsing: Baselines, evaluations, and analysis. In
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 394?402,
Beijing, China, August. Coling 2010 Organizing Committee.
Nizar Habash and Ryan Roth. 2009. Catib: The columbia arabic treebank. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 221?224, Suntec, Singapore, August. Association for Computational
Linguistics.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009. Syntactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on Arabic Language Resources and Tools, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and Wigdan Mekki. 2004. The Penn Arabic Treebank: Building
a Large-Scale Annotated Arabic Corpus. In NEMLAR Conference on Arabic Language Resources and Tools.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar, and Mario Figueiredo. 2010. Turbo Parsers: Dependency
Parsing by Approximate Variational Inference. In Proceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 34?44, Cambridge, MA, October. Association for Computational
Linguistics.
Andrew Kachites McCallum. 2002. ?mallet: A machine learning for language toolkit?.
http://mallet.cs.umass.edu.
Thomas Mu?ller, Helmut Schmid, and Hinrich Schu?tze. 2013. Efficient Higher-Order CRFs for Morphological
Tagging. In In Proceedings of EMNLP.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Talbanken05: A Swedish treebank with phrase structure and
dependency annotation. In Proceedings of LREC, pages 1392?1395, Genoa, Italy.
101
Hiroki Ouchi, Kevin Duh, and Yuji Matsumoto. 2014. Improving dependency parsers with supertags. In Proceed-
ings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, volume
2: Short Papers, pages 154?158, Gothenburg, Sweden, April. Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable
tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational Linguistics, pages 433?440. Association for Computa-
tional Linguistics.
Slav Petrov. 2010. Products of Random Latent Variable Grammars. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages
19?27, Los Angeles, California, June. Association for Computational Linguistics.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 1044?1050,
Prague, Czech Republic, June. Association for Computational Linguistics.
Djame? Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie Candito, Jinho D. Choi, Richa?rd Farkas, Jennifer Foster, Iakes
Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolf-
gang Maier, Joakim Nivre, Adam Przepio?rkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika
Vincze, Marcin Wolin?ski, Alina Wro?blewska, and Eric Villemonte de la Clergerie. 2013. Overview of the
SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages. In Pro-
ceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 146?182,
Seattle, Washington, USA, October. Association for Computational Linguistics.
Djame? Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie Candito, Jinho Choi, Matthieu Constant, Richa?rd Farkas,
Iakes Goenaga, Koldo Gojenola, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang
Maier, Joakim Nivre, Adam Przepiorkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze,
Marcin Wolin?ski, Alina Wro?blewska, and Eric Villemonte de la Cle?rgerie. 2014. Overview of the SPMRL 2014
shared task on parsing morphologically rich languages. In Notes of the SPMRL 2014 Shared Task on Parsing
Morphologically-Rich Languages, Dublin, Ireland.
Wolfgang Seeker and Jonas Kuhn. 2012. Making Ellipses Explicit in Dependency Conversion for a German
Treebank. In Proceedings of the 8th International Conference on Language Resources and Evaluation, pages
3132?3139, Istanbul, Turkey. European Language Resources Association (ELRA).
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman, and Noa Nativ. 2001. Building a Tree-Bank for Modern
Hebrew Text. In Traitement Automatique des Langues.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards a bank of constituent parse trees for Polish. In Text,
Speech and Dialogue: 13th International Conference (TSD), Lecture Notes in Artificial Intelligence, pages
197?204, Brno, Czech Republic. Springer.
Zsolt Sza?nto? and Richa?rd Farkas. 2014. Special techniques for constituent parsing of morphologically rich lan-
guages. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational
Linguistics, pages 135?144, Gothenburg, Sweden, April. Association for Computational Linguistics.
Reut Tsarfaty. 2010. Relational-Realizational Parsing. Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A Unified Morpho-Syntactic Scheme of Stanford Dependencies. Proceedings of ACL.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hungarian
dependency treebank. In LREC.
102
