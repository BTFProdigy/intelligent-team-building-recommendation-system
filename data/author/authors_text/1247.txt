Automat ic  Senmnt ic  Class i f icat ion for Ch inese Unknown Compound Nouns  
Keh-Jiann Chert & Chao-jan Chen 
Institute of Information Science, Acadeinia Sinica, Taipei 
Abstract 
Tim paper describes a similarity-based model to 
present he morphological rules for Chinese com- 
pound nouns. This representation model serves 
functions of 1) as the morphological rules of the 
compounds, 2) as a mean to evaluate the proper- 
ness of a compound construction, and 3) as a mean 
to disambiguate the semantic ambiguity of the 
nlorphological head of a compound noun. An 
automatic semantic lassil'ication system for Chine- 
se unknown compounds i thus implemented based 
on the model. Experiments and on'or analyses arc 
also presented. 
1. Introduction 
The occurrences of unknown words cause difficul- 
ties in natural language processing. Tile word set of 
a natural anguage is open-ended. There is no way 
of collecting every words of a language, since new 
words will be created for expressing new concepts, 
new inventions. Therefore how to identify new 
words in a text will bc tile most challenging task for 
natural language processing. It is especially true for 
Chinese. Each Chinese morpheme (usually a single 
character) carries meanings and most are polysc- 
incus. New words are easily constructed by com- 
bining lnorphelnes and their meanings are tile se- 
mantic composition of morpheme components. 
Of course there are exceptions of semantically non- 
compositional compounds. In Chinese text, there is 
no blank to mark word boundaries and no inlqec- 
tional markers nor capitalization markers to denote 
the syntactic or selnantic types of new words. 
Hence the unknown word identification for Chinese 
became one of the most difficult and demanding 
research topic. 
The syntactic and semantic categories of 
unknown words in principle can be determined by 
their content and contextual information. However 
many difficult problems have to be solved. First of 
all it is not possible to find a uniforln representa- 
tional schema nd categorization algorithm to han- 
dle different ypes of unknown words, since each 
type of unknown words has very much differeut 
morpho-syntactic structures. Second, the clues for 
identifying different ype of unknown words are 
also different. For instance, identification of 
names of Chinese is very much relied on the 
surnames, which is a limited set of characters. 
The statistical methods are commonly used for 
identifying proper names (Chang et al 1994, 
Sun et al 1994). The identification of general 
compounds is more relied on the morphemes 
and tile semantic relations between morphemes. 
There are co-occurrence restrictions between 
morphemes of compounds, but their relations are 
irregular and mostly due to common sense 
knowledge. The third difficulty is the problems 
of ambiguities, such as structure ambiguities, 
syntactic alnbiguitics and semantic ambiguities. 
For instances, usually a morpheme charac- 
tedword has multiple lneaning and syntactic 
categories. Therefore the ambiguity resolution 
became one of the major tasks. 
Compound nouus are ttle most frequently 
occurred unknown words in Chinese text. 
According to an inspection on tile Sinica corpus 
(Chen etc. 1996), 3.51% of lhe word tokens in 
the corpus are unknown, i.e. they are not listed 
in the CKIP lexicon, which contains about 
80,000 entries. Alnong them, about 51% of the 
word types are compound nouns, 34% are 
compound verbs and 15% are proper names. In 
this paper we locus our attention on the 
identification of the compound nouns. We 
propose a representation model, which will be 
facilitated to identify, to disambiguate and to 
evaluate the structure of a compound noun. In 
fact this model can be extended to handle 
compound verbs also. 
1.1 General properties of compounds and 
their identification strategy 
The semantic ategory and syntactic category 
are closely related. For coarse-grained analysis, 
syntactic ategorization and semantic ategori- 
zation are close related. For instances, nouns 
denote entities; active verbs denote events and 
stative verbs denote states. For fine-grained 
analysis, syntactic and semantic lassifications 
take difl'erent classification criterion, in our 
model the coarse-grained analysis is processed 
first. The syntactic categories of an unknown 
173 
word are predicted first and the possible semantic 
categories will be identified according to its top 
ranked syntactic categories. Different syntactic 
categories require different representational models 
and different fine-grained semantic lassification 
methods. 
The presupposition of automatic se- 
mantic classification for compounds i that the 
meaning of a compound is the semantic om- 
position of its morphemic omponents and the 
head morpheme determines the major semantic 
class of this compound. There are many poly- 
syllabic words of which the property of se- 
mantic composition does not hold, for in- 
stances the transliteration words, those words 
should be listed in the lexicon. Since for the 
majority of compounds the presupposition hold, 
the design of our semantic lassification algo- 
rithm will be based upon this presupposition. 
Therefore the process of identifying semantic 
class of a compound boils down to find and to 
determine the semantic lass of its head mor- 
phen-~e. However ambiguous morphological 
structures cause the difficulties in finding head 
morpheme. For instances, the compound in la) 
has two possible morphological structures, but 
only lb) is the right interpretation. 
1 a) ~2\]..l..'American' 
b) ~ /~.'Amcrica' 'people', 
c) ~ \["~l)\ 'beautiful' 'country-man' 
Once the morphological head is deterlnined, the 
semantic resolution for the head morpheme is the 
next difficulty to be solved. About 51.5% of the 
200 most productive morphemes are polysemous 
and according to the Collocation Dictionary of 
Noun and Measure Words (CDNM), in average 
each ambiguous morpheme carries 3.5 different 
senses (Huang et al 1997). 
2. Representation Models 
Compounds are very productive types of unknown 
words. Nominal and verbal compounds are easily 
coined by combiniug two/many words/characters. 
Since there are more than 5000 commonly used 
Chinese characters and each with idiosyncratic 
syntactic behaviors, it is hm,t to derive a set of 
morphological rules to generate the set of 
Chinese colnpounds without over-generation r 
under-generation. The set of general compounds 
is an open-class. The strategy for automatic 
identification will be relied not only on the 
morpho-syntactic structures but also morpho- 
semantic relations. In general, certain 
interpretable semantic relationships between 
morphemic must be held. However there is no 
absolute lneans to judge whether the semantic 
relations between morphemic omponents are 
acceptable, i,e. the acceptability of such type of 
compounds is not simply 'yes' or 'no'. The 
degree of properness of a compound should 
depend on the logical relation between 
morphemic omponents and their logicalness 
should be judged by cominou sense knowledge. 
It is ahnost ilnpossible to in\]plement a system 
with common sense knowledge. Chen & Chen 
(1998) proposed an example-based measurement 
to evaluate the properness of a newly coined 
compound instead. They postulate that for a 
newly coined compound, if the semantic relation 
of its morphemic omponents i  similar to the 
existing compounds, then it is inore likely that 
this newly coined compound is proper. 
2.1 Example-based similarity nleasure 
Supposed that a compound has the structure of 
XY where X and Y are morphemes and sup- 
posed without loss of generality Y is the head. 
For instance, ~ i<~-  'learn-word-machine' is a
noun compound and the head morphemeY is 
~'machine'  and the modifier X is ~ ' :  'learn- 
word'. In fact the morpheme f~{~ has four differ- 
ent meanings. They are 'machine', 'airplane', 
'secret' and 'opportunity'. How do computers 
judge which one is the right meaning and how is 
the compound construction well-formed or logi- 
cally lneaningful? First of all, the exalnples with 
the head morpheme ~ are extracted from cor- 
pora and dictionaries. The examples are classi- 
fied according to their meaning as shown in the 
Table 1. 
Senses semantic category examples 
( l )  machine <~> Bo0ll 
(2) a i rp lane <~{~> Bo223 
(3) opportunity <{N~> Ca042 
(4) secret <~> Da011 
Table 1. Four senses of tile morpheme ')~' and their respective samples 
174 
Tile meaning of ~l-\]i:i':I~,~- is then determined by 
comparing the similarity between it and each class 
of exalllples. Tile nleauing of the input ul\]kuown 
word will be assigned with the moaning of tile class 
with the most simihu morpho-semantic structures 
with this unknown word. Tile similarity measure is 
based on tile following formula. 
Supl)osed that each class of examples forlllS lhe 
following SOlUalltic relatioll rules. The rules silow 
the possible semantic relations between prel'ix and 
suffix Y and their weight in term ol' ihe frequency 
distribution of each semantic category of the pro- 
fixes in tile class. 
Rules: Semi +Y Freql 
Sere2 + Y Freq2 
: 
Semk + Y Freqk 
( freqi: tile number of the words of the l'orm Semi + 
Y)  
Take sulTix ~- with moaning of 'machine' as 
oxaulple. Igor tile nlorphonle I{} 'ulachine', tile 
extracted COlllpotlllds of the fornl X+~j~-'machine' 
and tile semantic categories of the n3odifiors al'e 
shown il1 Table 2 and the n3orpl~ological rule de- 
rived froill them ix in Table 3. The scnlai/tic types 
alld their hierarchical structure are adopted fro111 
tile Chilin (Moi el al. 1984). The similarity is 
measured between the semantic class of tile prefix 
X o1' tile unknown conlpound and tile prefix se- 
mantic types shown in the rule. ()no ot' the meas- 
uroulonts proposed is: 
SIMILAR (Sere ,Rule)  = {} ; i l , k  lnforl l lati()n- 
Load(Sem(hSemi) * lq-eqi } / Max-vahle 
Where Sere is tile semantic class of X. Max-value 
is tile maxinlal vahle o1' {~\] \[nfornlation Load(S 
~Selni)  * Freqi } for all semantic lasses S. The 
iriax-wllue normalizes tile SIMILAR value to 
0-1. S(hSemi denotes the least common ances- 
tor of S and Semi. For instance, (Hh03(' lHb06) 
= H. Tile Information-Load(S) of a senmntic 
class S ix defined as Entropy(sonlantic sys- 
tem) - Entropy(S). Simply speaking it is the 
anlount of reduced entropy after S is seen. En- 
tropy(S) =~\]i=l,k -p(SemilS) * Log P(SemiJs), 
where {Semi, Sem2 ..... Semk} is lhe set of the 
bottoln level selnantic lasses contained in S. 
J r ,  5 % t,n ~,~-(f~) AoI73 llh031 ,(7J~#~(t~) Bo171 
,~;'.te(7"(,l~) AoI73 I11~032 SII;~(}~)I f083 Ih063 
I~JTl{(\]~) 11c013 11e032 ll~(L, tt~(4~) Ili141 tlj345 
LI~7':~ @~) I 1c()32 (7 P J'-?! ('~{~)Fa221 
I~1 ~)j(4~) Eb342 4~Jl~(4~) Iic071 
'i:\[{'~)2@~) Eb342 /JJ(JC(lJ~) Ih031 
~1~ ,,,:(~) AoI62 117162 ?,3~(4,~) P>e(>51 
gg~'l.;@}) I)k162 1fg191 ~J~!',lZ({~J~) 1h042 
5',J{(,).@{) Be041 ,~':~\[;41@~) 117192 
. - -  I l l  I u ". { :  ILT-)I':(IN) Ca039 Ca041 ~.~pl) (4~) FhOI2 
Os~!l\]Ja(t~) 11c122 IJl:\];';::(45~) 1\[c231 
~t&i I(q~b \]~,12i 4,,i~1;~(.4',2~) Fa212 .hl102 
Table 2. The senlanlic categories ol' modifiers of 
tile COlllpouuds el' X-"f~ ~ math inc" 
Take lhe word l~'~ i :J': I~ 'learning-word- 
inachine' as example. In tile table 3, the results 
show tile estiinated similarity between tile 
X-~J,~ Serui freqi 
Hh031 
Ao 173 
He032 
Eb342 
Ae 162 
Hgl91 
3 
I 
2 
2 
I 
1 
Sem(~.~i!j ":)= Ilg111 
lnl'orrnation-Load( Hgl I 1 f-I Hh031 )=hlfornlation-Load(H)=2.231 
lnfornlation-Load( Hgl 11 N I1e032 )=hlformation-Load(Ii)=2.231 
Inforuultion-l~oad( Itg I 11 I'3 fig 191 )=lnformation-Load(Hg)=5.91 
? i= l,k hfformation-Load(Hgl I 1 f-I Semi) * Freqi 
=2.231"3+2.231"2+5.912"  1 + ...... 
= 104.632 
Max-Vahlel = Z i hffornlation-Load(Hg031 f-I Semi) * Freqi 
= 155.164 
S1MILAP,= (104.632 / 155.164) = 0.6743 
Table 3. The derived morphological rule for tile ulorphenle 'machine' and tile simihu'ity measure of ,~.~l~j?: 
t~"  aS ;_i I1OUn conlpound which denotes a kind of nlachille. 
175 
compound ~-~ and the extracted examples. The 
similarity value is also considered as the logical 
properness value of this compound. In this case is 
0.67, which can be interpreted as that we have 67% 
of confidence to say that ~z~ 'learning-word- 
machine' is a well-formed compound. 
The above representation model serves many func- 
tions. First of all it serves as the morl3hological 
rules of the colnpounds. Second it serves as a mean 
to implement the evaluation function. Third it 
serves as a mean to disambiguate he semantic am- 
biguity of the morphological head of a compound 
noun. For instance, them are four different @. 
Each denotes 'machine', 'airplane', 'opportunity' 
and 'secret' and they are considered as four differ- 
ent morphemes. The example shows that '~  
~}~'denotes a machine not other senses, since the 
evaluation score for matching the rules of '~-'ma- 
chine' has the highest evaluation score among 
theln. 
The above discussion shows the basic concept 
and the base-line model of the example-based 
model. The above similarity measure is called 
over-all-similarity measure, since it takes the equal 
weight on the similarity values of the input com- 
pound with every member in the class. Another 
similarity measum is called maximal-similarity, 
which is defined as follows. It takes the maximal 
value of the similarity between input compound 
and every member in the class as the output. 
SlM2(Word,Rule) = Maxi=l,k{ (Information 
Load(Sem~Semi)) / Max-value2 } 
Both similarity measures are reasonable and have 
their own advantages. The experiment results 
showed that the combination of these two measures 
achieved the best performance on the weights of 
w 1=0.3 and w2=0.7 (Chen & Chen 1998), i.e. SIM 
= SIMI * wl + SIM2 * w2, where wl+w2 = 1. We 
adopt his measure in our experiments. 
It also showed a strong co-relation between the 
similarity scores and the human evaluation scores 
on the properness of testing compounds. The hu- 
man considemd bad compounds howed also low 
similarity scores by computers. 
3. System Implementation 
3.1 Knowledge sources 
To categorize unknown words, the computer sys- 
tem has to equip with the linguistic and semantic 
knowledge about words, morphemes, and word 
formation rules. The knowledge is facilitated to 
identify words, to categorize their semantic and 
syntactic lasses, and to evaluate the properness of 
word formation and the confidence level of 
categorization. In our experiments, the available 
knowledge sources include: 
1) CKIP lexicon: an 80,000 entry Chinese lexi- 
con with syntactic categories for each entry 
(CKIP 1993). 
2) Chilin: a thesaurus of synonym classes, which 
contains about 70,000 words distributed un- 
der 1428 semantic lasses (Mei 1984). 
3) Sinica Corpus: a 5 million word balanced 
Chinese corpus with word segmented and 
part-of-speech tagged (Chen 1996). 
4) the Collocation Dictionary of Noun and 
Measure Words (CDNM) : The CDNM lists 
collocating measure words for nouns. The 
nouns in this dictionary are arranged by their 
ending morpheme, i.e. head morpheme. There 
are 1910 noun ending morphemes and 12,352 
example nouns grouped according to their 
different senses. 
Each knowledge source provides partial data for 
representing morphological rules, which in- 
clndes lists of sample compounds, high frequen- 
cy morphemes and their syntactic and semantic 
information. Unknown words and their frequen- 
cies can be extracted from the Sinica corpus. 
The extracted unknown words produce the 
testing data and the morpheme-category asso- 
ciation-strength which are used in the algorithm 
for the syntactic category prediction for un- 
known words (Chen et al 1997). The CKIP dic- 
tionary provides the syntactic categories for 
morphemes and words. The Chilin provides the 
semantic categories for morpheme and words. 
The CDNM provides the set of high frequency 
noun morphemes and the example compounds 
grouped according to each difference sense. The 
semantic categories for each sense is extracted 
from the Chilin and disambiguated manually. 
The sample compounds for each sense- 
difl'emntiated morpheme xtracted from CDNM 
form the base samples for the morphological 
rules. Additional samples are supplemented from 
the Chilin. 
3.2 Tile algorithm for morphological 
analysis 
The process of morphological analysis for com- 
pound words is very similar to Chinese word 
segmentation process. It requires dictionary 
look-up for matching nlorphemes and resolution 
methods for the inherent ambiguous egmenta- 
176 
tions, such as the exalnples in 1). However con- 
ventional word segmentation algorithms cannot 
apply for the morphological analysis without modi- 
fication, since the nlorpho-syntactic behavior is 
different froth syntactic behavior. Since ihc struc- 
ture of the Chinese COlllpound nOtlllS is head final 
and the most productive morphemes arc monosyl- 
labic, there is a simple and effective algorithm, 
which agrees with these facts. This algorithm seg- 
ments input compounds flom left to right by the 
longest matching criterion (Chcn& Liu 1992). It is 
clear that the loft to right longest lllaiching algo- 
rithm prel'ers shorler head and longer modifier 
structtlres. 
3.3 Senlant ic  categories of morphemes  
The semantic categories of morphemes arc lot- 
lowed from the thesaurus Chilin. This thesaurus is 
a lattice structure of concept taxonomy. Mor- 
phemes/words may have multiple classification due 
to either ambiguous classification or inherent so- 
illantic mnbiguitios. For lhe ambiguous scn'lantic 
categories o1' a morl)hcmo, lhc lower ranking se- 
nmntic categories will be eliminated and leave the 
higher-ranking scnlantic categories to conlpotc 
during the identification process. For instances, in 
the table 2 only the re;tier categories of each exam- 
ple are shown. Since the majority of nlorphemcs 
are unanlbiguous, they will compensate the uncer- 
tainty caused by die semantically ambiguous roof 
phemes. The rank of a semantic category of a mot'- 
phonic depends on the Occurrillg order o1: lhis lilO1- 
plionlo in ils synonyln group, since lhc arrangcincnt 
of the Chilin cilirics is by this natural, hi addition, 
dtlo to limit coverage o1: Chilin, nlally of the ll\]Ol'- 
phemes arc not listed. For the unlisted morphemes, 
we recursivcly apply the currellt algorithm to pre- 
dict their semantic categories. 
4. Semantic Chlssification and Ambiguity 
Resolut ion for  Compound Nouns 
The demand o1" a semantic hlssification system for 
COlllpound nouns was first raised while the task of 
selnantic tagging for Chinese corpus was lriod. The 
Sin|ca corpus is a 5 in|Ilion-word Chinese corpus 
with part-of speech lagging, lit lhis corpus there are 
47,777 word typos tagged with conllllOn nOl.lllS and 
Ollly 12,536 Of tholll are listed ill the Chilin. They 
count only 26.23%. In oilier words the scmandc 
categories for most of the common nouns arc tin- 
known. They will be the target for automatic se- 
mantic classification. 
4.1 Derivation of morphological rules 
A list of' most productive lriorphoinos arc first 
generated from the unknown words extracted 
fl'om the Sinica corpus. The morphological rules 
o1' the sot of the lllOSl productive head mor- 
phonies {llO derived flonl their examples. Both 
the CI)MN all(\] Chilin provide SOlilO oxanlplcs. 
So lhr there are 1910 head morphemes for com- 
pound nouns with examples in the system and 
increasing. They are all monosyllabic mor- 
phemes. For the top 200 most productive mor- 
phenlcs, among them 51.5% are polysemous and 
in average each has 3.5 different meanings. \]'tie 
coverage of ihe ctlrrollt 1910 illorphonlos is 
aboul 71% of ihc uilkiiown noun conlpounds of 
the iesling dala. The rosl 29% tincovorod noun 
nlorphonlos are cilher polysyllabic i-llorpholiies 
or/lie low frequency nlorl~hemes. 
4.2 Semantic classification algorithm 
The unknown compound nouns extracted from 
the Sinica corpus w'cre classified according to 
Ihc morphological representation by the simihtr- 
ity-bascd algoriltnn. The problcms of semantic 
ambiguitics and out-of-coverage morphcmcs 
were two major dilTicultics to be solved during 
the classification stage. The complete scmanlic 
classification algorilhm is as follows: 
I) For each inpu! noun compound, apply mor- 
phological analysis algorilhm lo derive die 
morphemic components of the input com- 
pound. 
2) I)clcrminc the head nlorphenlc and modifiers. 
'flit: dcfaull head illorphclllo is lhc last liior- 
phonic of a conlpound. 
3) Got die synlactic and semantic categories of 
the modifiers. If a modil\]or is also an tin- 
known word, lhen apply this algorilhm rocur- 
sively to idendfy its son-ialltic category. 
4) For lhe head morpholne with the representa- 
tional rules, apply siinilarity illeastlro for each 
possible sornantic chtss and outptlt the so- 
manlic class with lhe highest siinilariiy wthic. 
5) If the head illorphonlo is not covered by tile 
nlorphological rules, search its semantic lass 
from the Chilin. If its semantic lass is not list 
in the Chilin, then no ariswcr can be found, if 
it is polysemous, then the top ranked classes 
will be the output. 
In lhc step I, thc algorithm rcsolvcs the possible 
ambiguities o1' the morphological slrtlcttlrcs of 
the input COlllpound. In the step 3, the selllantic 
categories of the modil'ier arc determined. There 
arc some complications. The firsl complication 
is lhat lhe modifier has nmltiple semantic are- 
177 
gories. In our current process, tile categories of 
lower ranking order will be eliminated. The re- 
maining categories will be processed independently. 
One of the semantic categories of the modifier 
pairing with one of the rule of the head morpheme 
with the category will achieve the maximal simi- 
larity value. The step 4 thus achieves the resolution 
of both semantic ambiguities of the head and tile 
modifier. However only the category of the head is 
our target of resolution. The second complication is
that the modifier is also unknown. If it is a not list- 
ed in the Chilin, there is no way of knowing its 
semantic categories by tile era'rent available re- 
sources. At the step 4, the prediction of semantic 
category of the input compound will depend solely 
on the information about its head morpheme. If the 
head morpheme is unambiguous then output the 
category of the head morpheme as the prediction. 
Otherwise, output he semantic ategory of the top 
rank sense of the head morpheme. The step 5 han- 
dles the cases of exceptions, i.e. no representational 
rule for head morphemes. 
4.3 Experimental  results 
The system classifes the set of unknown common 
nouns extracted from tile Sinica corpus. We ran- 
domly picked two hundred samples from tile output 
for the performance valuation by examining the 
semantic classification manually. The correction 
rate for semantic lassil'ication is 84% and 81% 
for tile frst hundred samples and the second 
hundred samples respectively. We further classi- 
fy tim errors into different ypes. The first type is 
caused by the selection error while disam- 
biguating the polysemous head lnorphemes. The 
second type is caused by the fact that the mean- 
ings of some compounds are not semantic om- 
position of tile meanings of their morphological 
components. Tile third type errors are caused by 
the fact that a few compounds are conjunctive 
structures not assumed head-modifier structure 
by the system. Tile forth type errors are caused 
by the head-initial constructions. Other than tile 
classification errors, there exist 10 unidentifiable 
colnpounds, 4 and 6 in each set, for their head 
morphemes are not listed in tile system nor in 
the Chilin. Among tile 190 identifiable head 
morphemes, 142 of them are covered by the 
morphological rules encoded in the system and 
80 of theln have multiple semantic categories. 
Tile semantic categories of remaining 48 head 
morphemes were found fiom the Chilin. If the 
type 1 selection errors are all caused by the 80 
morphemes with multiple semantic categories, 
then the correction rate of semantic disambigua- 
tion by our similarity-based measure is (80- 
15)/80 = 81%. 
Testing data 1 Testing data 2 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
Total : 100 
error: 12 
type l(semantic selection error): 8 
type2(non-compositional): 2 
type3(conjunction): I 
typre4(head-initial): 1 
unidentified: 4
Total: 100 
error: 13 
type l(semantic selection error): 7 
type3(non-compositional): 5 
type3(con.junction): 0 
typre4(head-initial): 1 
unidentified: 6
Table 5. The performance valuations of 
5. Further Remarks and Conclusions 
In general if an unknown word was extracted from 
corpora, both of its syntactic and semantic atego- 
ries are not known. The syntactic ategories will 
be predicted first according to its prefix-category 
and suffix-category associations as mentioned in 
(Chen et al 1997). According to the top ranked 
syntactic predictions, each respective semantic 
representational rules or models will be applied to 
produce the morpho-semantic plausibility of the 
unknown word of its respective syntactic atego- 
rization. For instance if the predicted syntactic 
the semantic lassification algorithm 
categories are either a common noun or a verb, the 
algorithm present in this paper will be carried out 
to classify its semantic ategory and produce its 
plausibility value for the noun category. Similar 
process should deal with tile case of verbs and 
produce tile plausibility of being a verb. The final 
syntactic and semantic prediction will be based oil 
their plausibility values and its contextual envi- 
ronments (Bai et al 1998, Ide 1998). 
The advantages of tile current representational 
model are: 
1) it is declarative. New examples and new mor- 
178 
phemes can be added into the system withoul 
changing the processing algorilhm, but 111e per- 
formance o1' the system might be increased ue 
to the increlnent of the knowledge. 
2) The representational model not only provides 
the semantic classification of the unknown 
words but also gives the wdue of the phmsibil- 
ity o1' a compound construction. This value 
could be utilized to resolve the alnbiguous 
matching between compeling compound rules. 
3) The representational model can be extended for 
presenting compound verbs. 
4) It acts as one of the major building block of a 
self-learning systeln for linguistic and world 
knowledge acquisition on the lnternel environ- 
l l lel l t .  
Tile classification errors are caused by a) some of 
the testing examples have no semantic omposi- 
tion property, b) some semantic lassifications are 
too much fine-grained. There is no clear cut dif- 
ference between some classes, even Imman judge 
cannot lnake a right classification, c) there are not 
enough samples that causes the simihuity-based 
model does not work on the suffixes with few or 
no sample data. The above classification errors 
can be resolved by collecting the new words, 
which are Selnantically nol>compositional, into 
tile lexicon and by adding new examples for each 
naorphenle. 
Current Selnantic ategorization system only 
roughly classifies the unknown compound nouns 
according to their semantic heads. In the future 
deeper analysis on the semantic relations between 
modifier and head should also be carried otll. 
6. Rel'erences 
Bai, M.H., C.J. Chert & K..I. Chert, 1998, "POS- 
lagging for Chinese Unknown Words by 
Contextual P, ules" Proceedings of 
ROCLING, pp.47-62. 
Chang, .1. S.,S.D. Chert, S. J. Ker, Y. Chert, & J. 
Liu,1994 "A Multiple-Corpus Approach to 
Recognition of Proper Nmnes in Chinese 
Texts", Computer Processing o/" Chinese 
and Oriental Languages, Vo\[. 8, No. 1, pp. 
75-85. 
Chert, C. J., M. It. Bai, K. J. Chert, 1997, "Catego- 
ry Guessing for Chinese Unknown Words." 
Proceedings of the Natural Langttage 
Processing Pac(fi'c Rim Symposimn 1997, 
pp. 35-40. NLPRS '97 Thailand. 
Chert, K. J., C.J. Chert, 1998, "A Corpus-based 
Study on Computational Morphology for 
Mandarin Chinese", in Qlmnlitative and 
Comlmtationa\[ Studies on the Chinese 
lxmgttage Eds by l?,cnjanfin K. Tsou, City 
Univ. of Hong Kong, pp283-306. 
Chert, l<eh-.liann, Ming-Hong Bai, 1997, "Un- 
known Wolzl l)etection for Chinese by a 
Corpus-based Learning Method." Pro- 
ceedings of the lOth Research on Comlm- 
lalional Linguistics International Confer- 
ence, pp 159-174. 
Chert, K.J. & S.II. Liu, 1992,"Word ktentification 
for Mandarin Chinese Sentences," lbv- 
ceedings o.f 14th Colili,g, pp. 101 - 107. 
Chien, Lee-feng, 1999," lWF-tree-based Adaptive 
Keyphrase Extraction for Intelligent Chine- 
se Information Retrieval," Information 
Processing and Management, Vol. 35, pp. 
501-521. 
l;ung P., 1998," Extracting Key Terms from Chi- 
nese and Japanese Texts," Computer Proc- 
essing of Oriental Languages, Vol. 12, #1, 
pp 99-122. 
Huang, C. R. E1 al.,1995,"The lnlroduction of 
Sinica Corpus," Proceedings of ROCIJNG 
VIII, pp. 81-89. 
lhtang, Chu-P, en, Keh-Jiann Chert, and Ching- 
hsiung Lai, 1997, Mandarin 1)ally 
Classification l)ictionary, Taipci: Mandarin 
l)aily Press. 
Ide, Nancy & Jean Veronis, 1998, " Special Issue 
on Word Sense l)isambiguation", 
Computational Linguistics, Vol. 24, # I. 
Lee, J.C. , Y.S. Lee and H.H. Chert, 1994, 
"Identification of Personal Names in 
Chinese Texts." Proceedings of 7th ROC 
Computational Linguistics Conference. 
Lin, M. Y., T. H. Chiang, & K.Y. Su, t993," A 
Preliminary Study on Unknown Word 
Problem in Chinese Word Segmentation" 
Proceedings of Reeling V1, pp 119-137. 
Mei, Gia-Chu etc., 1984Iq * - -  4~ ~q ~q g.(Chil in - 
thesaurus of Chinese words). Hong Kong, 
McDonald 1)., 1996, " Internal and External 
Evidence in the Identification and Semantic 
Categorization of Proper Names", in 
Corpus Processing Jot Lexical Acquisition, 
J. Pustejovsky and B. Boguraev Eds, MIT 
Press 1996. 
Sun, M. S., C.N. Huang, H.Y. Gao, & Jie Fang, 
1994, "Identifying Chinese Names in Unre- 
stricted Texts", Communication of COLIPS, 
Vol.4 No. 2. 113-122. 
179 
Unknown Word Extraction for Chinese Documents 
 
Keh-Jiann Chen 
Institute of Information science,  
Academia Sinica 
kchen@iis.sinica.edu.tw 
Wei-Yun Ma 
Institute of Information science,  
Academia Sinica 
ma@iis.sinica.edu.tw 
 
Abstract  
There is no blank to mark word boundaries in 
Chinese text. As a result, identifying words is 
difficult, because of segmentation ambiguities 
and occurrences of unknown words. 
Conventionally unknown words were extracted 
by statistical methods because statistical 
methods are simple and efficient. However the 
statistical methods without using linguistic 
knowledge suffer the drawbacks of low 
precision and low recall, since character strings 
with statistical significance might be phrases or 
partial phrases instead of words and low 
frequency new words are hardly identifiable by 
statistical methods. In addition to statistical 
information, we try to use as much information 
as possible, such as morphology, syntax, 
semantics, and world knowledge. The 
identification system fully utilizes the context 
and content information of unknown words in 
the steps of detection process, extraction process, 
and verification process. A practical unknown 
word extraction system was implemented which 
online identifies new words, including low 
frequency new words, with high precision and 
high recall rates. 
1 Introduction 
One of the most prominent problems in 
computer processing of Chinese language is 
identification of the word sequences of input 
sentences. There is no blank to mark word 
boundaries in Chinese text. As a result, 
identifying words is difficult, because of 
segmentation ambiguities and occurrences of 
unknown words (i.e. out-of-vocabulary words).  
Most papers dealing with the problem of word 
segmentation focus their attention only on the 
resolution of ambiguous segmentation. The 
problem of unknown word identification is 
considered more difficult and needs to be further 
investigated. According to an inspection on the 
Sinica corpus (Chen etc., 1996), a 5 million 
word Chinese corpus with word segmented, it 
shows that 3.51% of words are not listed in the 
CKIP lexicon, a Chinese lexicon with more than 
80,000 entries. 
Identifying Chinese unknown words from a 
document is difficult; since  
 
1. There is no blank to mark word boundaries; 
2. Almost all Chinese characters and words are also 
morphemes; 
3. Morphemes are syntactic ambiguous and semantic 
ambiguous; 
4. Words with same morpho-syntactic structure might 
have different syntactic categories; 
5. No simple rules can enumerate all types of unknown 
words; 
6. Online identification from a short text is even harder, 
since low frequency unknown words are not 
identifiable by naive statistical methods. 
 
It is difficult to identify unknown words in a 
text since all Chinese characters can either be a 
morpheme or a word and there are no blank to 
mark word boundaries. Therefore without (or 
even with) syntactic or semantic checking, it is 
difficult to tell whether a character in a 
particular context is a part of an unknown word 
or whether it stands alone as a word. Compound 
words and proper names are two major types of 
unknown words. It is not possible to list all of 
the proper names and compounds neither in a 
lexicon nor enumeration by morphological rules. 
Conventionally unknown words were extracted 
by statistical methods for statistical methods are 
simple and efficient. However the statistical 
methods without using linguistic knowledge 
suffer the drawbacks of low precision and low 
recall. Because character strings with statistical 
significance might be phrases or partial phrases 
instead of words and low frequency new words 
are hardly identifiable by statistical methods. 
Common statistical features for unknown 
word extraction are mutual information (Church 
90), entropy (Tung 94), association strength 
(Smadja 93, Wang 95) and dice coefficients 
(Smadja 96) etc. Chang etc. (Chang etc. 97) 
iteratively apply the joint character association 
metric, which is derived by integrating above 
statistical features. Their performance is recall 
rate:81%, precision rate: 72% in disyllabic 
unknown word, recall rate:88%, precision rate: 
39% in trisyllabic unknown word, and recall 
rate:94%, precision rate: 56% in four-syllabic 
unknown word. 
Chang etc. (1994) used statistical methods to 
identify personal names in Chinese text which 
achieved a recall rate of 80% and a precision 
rate of 90%. Chen & Lee (1994) used 
morphological rules and contextual information 
to identify the names of organizations. Since 
organizational names are much more irregular 
than personal names in Chinese, they achieved a 
recall rate of 54.50% and a precision rate of 
61.79%. Lin etc. (1993) made a preliminary 
study of the problem of unknown word 
identification. They used 17 morphological rules 
to recognize regular compounds and a statistical 
model to deal with irregular unknown words, 
such as proper names etc.. With this unknown 
word resolution procedure, an error reduction 
rate of 78.34% was obtained for the word 
segmentation process. Since there is no standard 
reference data, the claimed accuracy rates of 
different papers vary due to different 
segmentation standards. In this paper we use the 
Sinica corpus as a standard reference data. As 
mentioned before, the Sinica corpus is a 
word-segmented corpus based on the Chinese 
word segmentation standard for information 
processing proposed by ROCLING (Huang et al 
1997). Therefore it contains both known words 
and unknown words, which are properly 
segmented. The corpus was utilized for the 
purposes of training and testing. 
From the above discussion, it is known that 
identification of unknown words is difficult and 
need to adopt different methods in identifying 
different types of unknown words. The objective 
of this research is to find methods to extract 
unknown words from a document and identify 
their syntactic and semantic categories. 
Although both processing are interrelated, for 
limiting scope of this paper, we will focus our 
discussion on the extraction process only and 
leave the topics of syntactic and semantic 
category predictions to other papers. 
2 Steps to Identify Unknown Words 
In addition to statistical information, we try to 
use as much information as possible, such as 
morphology, syntax, semantics, and world 
knowledge, to identify unknown words. The 
identification system fully utilizes the context 
and content information of unknown words in 
each three steps of processes, i.e. detection 
process, extraction process, and verification 
process. The detection process detects the 
occurrences of unknown words for better 
focusing, so that on the next step extraction 
process, it needs only focus on the places where 
unknown were detected. In addition, it also 
helps in identifying low frequency unknown 
words, which hardly can be identified by 
conventional statistical extraction methods. The 
extraction process extracts unknown words by 
applying morphological rules and statistical 
rules to match for different types of unknown 
words. As usual, tradeoff would occur between 
recall and precision. Enriching the extraction 
rules might increase recall rates, but it also 
increases the ambiguous and false extractions 
and thus lowers the precision. The final 
verification process comes to rescue. It resolves 
ambiguous and false extractions based on the 
morphological validity, syntactic validity, and 
statistical validity.  
3 Unknown Word Detection 
Conventionally a word segmentation process 
identifies the words in input text by matching 
lexical entries and resolving the ambiguous 
matching (Chen & Liu, 1992, Sproat et al 1996). 
Hence after segmentation process the unknown 
words in the text would be incorrectly 
segmented into pieces of single character word 
or shorter words. If all occurrences of 
monosyllabic words are considered as 
morphemes of unknown words, the recall rate of 
the detection will be about 99%, but the 
precision is as low as 13.4% (Chen & Bai, 1998). 
Hence the complementary problem of unknown 
word detection is the problem of monosyllabic 
known-word detection, i.e. to remove the 
monosyllabic known-words as the candidates of 
unknown morphemes. A corpus-based learning 
method is proposed to derive a set of syntactic 
discriminators for monosyllabic words and 
monosyllabic morphemes (Chen & Bai, 1998).  
The following types of rule patterns were 
generated from the training corpus. Each rule 
contains a key token within curly brackets and 
its contextual tokens without brackets. For some 
rules there may be no contextual dependencies. 
The function of each rule means that in a 
sentence, if a character and its context match the 
key token and the contextual tokens of the rule 
respectively, this character is a proper word (i.e. 
not a morpheme of an unknown word). For 
instance, the rule ?{Dfa} Vh? says that a 
character with syntactic category Dfa is a proper 
word, if it follows a word of syntactic category 
Vh. 
 
Rule type               Example 
================================= 
char   {?}  
word char  ? {?} 
char word  {?} ?? 
category   {T} 
{category} category {Dfa} Vh 
category {category} Na {Vcl} 
char category  {?} VH 
category char  Na {?} 
category category char Na Dfa {?} 
char category category {?} Vh T 
=================================== 
Table1. Rule types and Examples 
 
Rules of the 10 different types of patterns 
above were generated automatically by 
extracting each instance of monosyllabic words 
in the training corpus. Every generated rule 
pattern was checked for applicability and 
accuracy. At the initial stage, 1455633 rules 
were found. After eliminating the low 
applicability rules, i.e. frequency less than 3, 
there are 215817 rules remained. At next stage, 
the rules with accuracy greater than 98% are 
selected for better recall rate. However the 
selected rules may subsume each other. Shorter 
rule patterns are usually more general than the 
longer rules. A further screening process is 
applied to remove the redundant rules. The final 
rule sets contain 45839 rules and were used to 
detect unknown words in the experiment. It 
achieves the detection rate of 96% and the 
precision rates of 60%. Where detection rate 
96% means that for 96% of unknown words in 
the testing data, at least one of its morpheme 
was detected as part of unknown word. However 
the boundaries of unknown words are still not 
known. For more detail discussion, see (Chen & 
Bai 1998). For convenience, hereafter we use (?) 
to mark detected morphemes of unknown words 
and () to mark the words which are not detected 
as morphemes of unknown words. 
4 Unknown Word Extraction 
At detection stages, the contextual rules were 
applied to detect fragments of unknown words, 
i.e. monosyllabic morphemes. The extraction 
rules will be triggered by the detected 
morphemes only. The extraction rules are 
context, content, and statistically constrained. 
Rule-design targets for high recall rate and try to 
maintain high precision at the mean time. It is 
hard to derive a set of morphological rules, 
which exactly cover all types of unknown words. 
Our approach is that if morphological structures 
of certain types of unknown words are well 
established, their fine-grain morphological rules 
will be designed. Otherwise statistical rules are 
designed without differentiate their extracted 
word types. Redundancy is allowed to achieve 
better coverage. Both morphological rules and 
statistical rules use context, content and 
statistical information in their extraction.  
4.1  Morphological rules 
Since there are too many different types of 
unknown words, we cannot go through the detail 
extraction processes for each different type. It 
will be exemplified by the personal name 
extraction to illustrate the idea of using different 
clues in the extraction process. First of all the 
content information is used, each different type 
of unknown words has its own morphological 
structure. For instance, a typical Chinese 
personal name starts with a last name and 
followed by a given name. The set of last names 
is about one hundred. Most of them are common 
characters. Given names are usually one or two 
characters and seldom with bad meaning. Based 
on the above structure information of Chinese 
personal names, the name extraction rules are 
designed as shown in Table 2. Context 
information is used for verification and 
determining the boundary of the extracted word. 
For instance, in the last rule of Table 2, it uses 
context information and statistical information to 
resolve ambiguity of the word boundary. It is 
illustrated by the following examples.  
 
1) after detection   : ?(?) ?(?) ?() ?() ?() ?()? 
  extractnion : ??? ? ? ?? 
             Ming-Zheng Zhang want kill somebody. 
         or  ?? ? ? ? ?? 
             Ming Zhang just want kill somebody.    
     
Rule type                Constraints & Procedure 
========================================== 
(?)  (?)  (?) 21 ++ iii msmsms    combine  )2,1,( ++ iii
(?)  (?)    () 21 ++ iii msmsms    combine       )2,1,( ++ iii
(?)    ()  (?) 21 ++ iii msmsms    combine       )2,1,( ++ iii
()   (?) 1+ii dsms          combine           )1,( +ii
()  (?)  (?) 21 ++ iii psmsms                )1,( +iicombine
()  (?)  (?) 21 ++ iii msmsms      as follows: 
 ( ) 1|  12 <++ iiidocument msmsmsprobif  
    namedisyllabicaasiicombine         )1,( +
( ) 1,,  32 ?++ iicoupus wordmsNAMEfreqelsif  
namedisyllabicaasiicombine         )1,( +
( )3,  + ? coupusicoupus freqwordNAMEfreq
( ) yllabic na as a tris,ii,i 21 ++
  
     combine  
( 2, +imsNAMEelsif
me
)
)
( )
else   namedisyllabicaasiicombine         )1,( +
 
Notes: ms denotes monosyllable. ds denotes disyllable. ps 
denotes polysyllable which consists of more than one 
syllable. word denotes a word which could consist of any 
number of syllable. msi must belong to Common Chinese 
Last Name Set, such as ?, ??etc. 
========================================= 
      Table 2. Rule types of Chinese personal name 
 
In the examples 1), there are two possible 
candidates of personal names, ?? and ???. 
By context information, the bi-gram (NAME, 
?) is less freguent than (NAME, ?) in the 
corpus, so without considering statistical 
constraints, it would suggest that ??? is a 
correct extraction instead of ??. However, the 
locality of the keywords is very important clue 
for identification, since the keywords of a text 
are usually unknown words and they are very 
frequently reoccurred in the text. The statistical 
information is used here for verification. For 
instance, if an another sentence which is like ?
(?) ? (?) ? () ? () occurs in the same 
document, it suggests ??  is the correct 
extraction, since the statistical constraint 
 rejects???. ( 1| <???documentprob
4.2  Statistical Rules 
It is well known that keywords often reoccur in 
a document (Church, 2000) and very possible 
the keywords are also unknown words. 
Therefore statistical extraction methods utilize 
the locality of unknown words. The idea is that 
if two consecutive morphemes are highly 
associated then combine them to form a new 
word. Mutual information-like statistics are very 
often adopted in measuring association strength 
between two morphemes (Church & Merser, 
1993, Sproat et al 1996). However such kind of 
statistic does not work well when the sample 
size is very limited. Therefore we propose to use 
reoccurrence frequency and fan-out numbers to 
characterize words and their boundaries (Chien, 
1999). 12 statistical rules are derived to extract 
unknown words. Each rule is triggered by 
detected morphemes and executed in iteration. 
The boundaries of unknown words might extend 
during iteration until no rule could be applied. 
Following are two examples of statistical rules. 
 
Rule id       Pattern          Statistical constraint  
========================================== 
R1         Lm(?) Rm()               S1  
R2         Lm(?) Rm(?)              S2 
 ( ) ( )
( ) 2  and       
  8.0|  and  8.0| : S1
?
??
LmRmFreq
LmRmPRmLmP
( ) ( )( )
 
( )
( ) ( )( )8.0|  and  8.0|or         
2    8.0|or    8.0| : S2
??
???
LmRmPRmLmP
LmRmFreqandLmRmPRmLmP  
========================================== 
     Table 3. Two examples of statistical rules 
 
The rule R1 says that Lm and Rm will be 
combined, if both conditional probability 
P(Lm|Rm)>=0.8 and P(Rm|Lm)>=0.8 hold and 
the string LmRm occurred more than once in the 
processed document. Conditional probabilities 
constrain the fan-out number on each side of 
morpheme, i.e. the preceding morpheme of Rm 
should almost be limited to Lm only and vice 
versa. The threshold value 0.8 is adjusted 
according to the experimental results, which 
means at least four out of five times the 
preceding morpheme of Rm is Lm and vice 
versa. However the statistical constraints are 
much loose when the right morpheme Rm is also 
a detected morpheme, as exemplified in R2. You 
may notice that it also accepts the unknown 
words occurred only once in the document.  
Conventional statistical extraction methods 
are simple and efficient. However if without 
supporting linguistic evidences the precision of 
extraction is still not satisfactory, since a high 
frequency character string might be a phrase or a 
partial phrase instead of a word. In addition to 
statistical constraint, our proposed statistical 
method requires that a candidate string must 
contain detected morphemes. In other words, the 
statistical rules are triggered by detected 
morphemes only. Furthermore the 
morphological structure of extracted unknown 
word must be valid. A validation process will be 
carried out at the different stages for all 
extracted unknown words. 
5  Verification 
To verify a correct extraction depends on the 
following information. 
 
1. Structure validity: the morphological structure of a 
word should be valid. 
2. Syntactic validity: the syntactic context of an 
 identified new word should be valid. 
3. Local consistency: the identified unknown words  
should satisfy the local statistical constraints, i.e. no 
inconsistent extension on the morphological structures. 
For instance, a new word was identified by the pattern 
rules, but if it violates the statistical constraints, as 
exemplified in 1), will be rejected. 
 
Each extracted candidate will be evaluated 
according to the validity of above three criteria. 
For the candidates extracted by the statistical 
rules, their structure validity and syntactic 
validity are checked after extraction. On the 
other hand, for the unknown words extracted 
according to the morphological rules, their 
structure validity and syntactic validity are 
checked at extraction stage and their local 
statistical consistency is checked after extraction.    
To verify the structure validity and syntactic 
validity of the unknown words extracted by 
statistical methods, their syntactic categories are 
predicted first, since statistical rules do not 
classify unknown word types. The prediction 
method is adopted from (Chen, Bai & Chen, 
1997). They use the association strength 
between morpheme and syntactic category to 
predict the category of a word. The accuracy rate 
is about 80%. Once the syntactic category of an 
unknown word is known its contextual bi-gram 
will be checked. If the bi-grams of (preceding 
word/category, unknown word category) and 
(unknown word category, following 
word/category) are syntactically valid, i.e. the 
bi-gram patterns are commonly occurred in the 
corpus, the extracted word is considered to be a 
valid word. Otherwise this candidate will be 
rejected.  
5.1  Final Selection 
It is possible that the extracted candidates 
conflict each other. For instance, in the 
following example, both candidates are valid. 
????, Bennet? is extracted by name rules and 
????, lawyer-class? is extracted by suffix 
rules. 
 
name  ==>  ?? ?? ?? ??? ? ? 
      An-jan company lawyer Bennett said, 
suffix  ==>  ?? ?? ??? ? ? ? ? 
 An-jan company lawyer-class is pecial said, 
 
The extracted new words will form a word 
lattice. The selection process finds the most 
probable word sequence among word lattice as 
the final result. In the current implementation, 
we used a very simple heuristics of maximizing 
the total weights of words to pick the most 
probable word sequence. The weight of a word 
w is defined to be freq(w)*length(w), where 
freq(w) is the occurrence frequency of w in the 
document and length is the number of characters 
in w. For the above example, ????, Bennett? 
occurred 5 times and ????, lawyer-class? 
occurred twice only in the document. Therefore 
the final result is 
 
??   ??   ??  ? ? ? 
An-jan company lawyer Bennett said , 
?Bennett, the lawyer of An-jan company, said?? 
6  Experimental Results 
In the current implementation, the 
morphological rules include the rules for 
Chinese personal names, foreign transliteration 
names, and compound nouns. In addition to the 
morphological rules, twelve constrained 
statistical rules were implemented to patch the 
under coverage of the morphological rules. 
Although the current implementation is not 
complete, morphological rules of many other 
types of unknown words were not included, such 
as rules for compound verbs. The experiment 
results still show that the proposed methods 
work well and the morphological rules and the 
statistical rules complement each other in the 
extraction and verification. 
The Sinica balanced corpus provides the 
major training and testing data. The training data 
contains 8268 documents with 4.6 million words. 
We use it to train the detection rules and 
morphological rules. We randomly pick 100 
documents from rest of the corpus as the testing 
data, which contain 17585 words and 1160 
unknown word types. 
 A word is considered as an unknown word, 
if neither it is in the CKIP lexicon nor it is 
identified as foreign word (for instance English) 
or a number. The CKIP lexicon contains about 
80000 entries. 
The precision and recall rates are provided. 
The target of our approach is to extract unknown 
words from a document, so we define ?correct 
extractions? as unknown word types correctly 
identified in the document. The precision and 
recall rate formulas are as follows: 
 
idocument in  sextractioncorrect  ofnumber NCi =  
idocument            
in    typesrdunknown wo extracted ofnumber NEi =  
idocument            
in    typesrdunknown wo reference ofnumber NRi =  
 
?
?
=
=
=
== 100
1
i
100
1
i
NE
NC
ratePrecision i
i
i
i      
?
?
=
=
=
== 100
1
i
100
1
i
NR
NC
rate Recall i
i
i
i  
 
  To observe the frequency impact on our 
system, the performance evaluation on both high 
frequency and low frequency unknown word 
identifications are also provided at Table 5 & 6. 
A word occurs more than or equal to 3 times in a 
document is considered a high frequency word. 
There are only 66 high frequency unknown 
words in our testing data. It counts less than 6% 
of the total unknown words. 
 
 Correct# Extract# Precision Recall 
Morphological rules 541 590 92% 47% 
Statistical rules 455 583 78% 39% 
Total system 791 890 89% 68% 
Table 4. Experimental result of total unknown 
word types 
 Correct# Extract# Precision Recall 
Morphological rules 25 26 96% 38% 
Statistical rules 50 60 83% 76% 
Total system 54 64 84% 82% 
Table 5. The performance on the set of unknown 
words with frequency >= 3 in a document 
 
 Correct# Extract# Precision Recall 
Morphological rules 510 564 90% 47% 
Statistical rules 400 523 76% 37% 
Total system 731 826 88% 67% 
Table 6. The performance on the set of unknown 
words with frequency <3 in a document 
 
Recall rate of total unknown word types is not 
very high, because not all of the morphological 
rules were implemented and some of the word 
tokens in the testing data are arguable. The 
experiment results in Table 6 show that the 
proposed methods work well on low frequency 
unknown word identification.  
7  Conclusions and Future Works 
Unknown word extraction is a very hard task. 
In addition to statistical information, it requires 
supporting knowledge of morphological, 
syntactic, semantic, word type specific and 
common sense. One important trend is to look 
harder for sources of knowledge and managing 
knowledge that can support unknown word 
identification. A word segmented and tagged 
corpus is essential for the success of the whole 
research. The corpus provides the major training 
and testing data. It also supports plenty of 
unknown words and their contextual data to 
derive extraction rules. In this work we are 
managing to use the structure information, the 
context environment, and statistical consistency 
of the unknown words and to increase the recall 
and precision of the extraction process. The 
syntactic and semantic classifications for 
unknown words are executed in parallel with the 
extraction process. Both classification processes 
are very hard and need further researches.  
8  References 
Chang J. S.,S.D. Chen, S. J. Ker, Y. Chen, & J. 
Liu,1994 "A Multiple-Corpus Approach to 
Recognition of Proper Names in Chinese Texts", 
Computer Processing of Chinese and Oriental 
Languages, Vol. 8, No. 1, 75-85. 
Chang, Jing-Shin and Keh-Yih Su, 1997a. "An 
Unsupervised Iterative Method for Chinese New 
Lexicon Extraction", International Journal of 
Computational Linguistics & Chinese Language 
Processing, 1997. 
Chen, H.H., & J.C. Lee, 1994,"The Identification of 
Organization Names in Chinese Texts", 
Communication of COLIPS, Vol.4 No. 2, 131-142. 
Chen, K.J. & S.H. Liu, 1992,"Word Identification for 
Mandarin Chinese Sentences," Proceedings of 14th 
Coling, pp. 101-107. 
Chen, K.J., C.R. Huang, L. P. Chang & H.L. Hsu, 
1996,"SINICA CORPUS: Design Methodology for 
Balanced Corpora," Proceedings of PACLIC 11th 
Conference, pp.167-176. 
Chen, C. J., M. H. Bai, K. J. Chen, 1997, ?Category 
Guessing for Chinese Unknown Words.? 
Proceedings of the Natural Language Processing 
Pacific Rim Symposium 1997, pp. 35-40. 
NLPRS ?97 Thailand. 
Chen, K.J. & Ming-Hong Bai, 1998, ?Unknown 
Word Detection for Chinese by a Corpus-based 
Learning Method,? international Journal of 
Computational linguistics and Chinese Language 
Processing, Vol.3, #1, pp.27-44. 
Chen, K.J., Chao-Jan Chen. 1998. ?A Corpus Based 
Study on Computational Morphology for Mandarin 
Chinese?????????????????
???.? Quantitative and Computational Studies 
on the Chinese Language. Benjamin K. T?sou, 
Tom B.Y. Lai, Samuel W. K. Chan, William S-Y. 
Wang, ed. HK: City Univ. of Hong Kong. 
pp.283-306. 
Chiang, T. H., M. Y. Lin, & K. Y. Su, 1992,? 
Statistical Models for Word Segmentation and 
Unknown Word Resolution,? Proceedings of 
ROCLING V, pp. 121-146. 
Chien, Lee-feng, 1999,? PAT-tree-based Adaptive 
Keyphrase Extraction for Intelligent Chinese 
Information Retrieval,? Information Processing 
and Management, Vol. 35, pp. 501-521. 
Church, K. W., & R. L. Mercer, 1993, ?Introduction 
to the Special Issue on Computational Linguistics 
Using Large Corpora.? Computational Linguistics, 
Vol. 19, #1, pp. 1-24 
Church, Kenneth W., 2000,? Empirical Estimates of 
Adaptation: The Chance of Two Noriegas is Closer 
to p/2 than p*p?, Proceedings of Coling 2000, 
pp.180-186. 
Huang, C. R. Et al.,1995,"The Introduction of Sinica 
Corpus," Proceedings of ROCLING VIII, pp. 
81-89. 
Lin, M. Y., T. H. Chiang, &  K. Y. Su, 1993,? A 
Preliminary Study on Unknown Word Problem in 
Chinese Word Segmentation,? Proceedings of 
ROCLING VI, pp. 119-137. 
Smadja, F., 1993,? Retrieving Collocations from 
Text: Xtract,? Computational Linguistics, 
19(1),143-177. 
Smadja, F., K. McKeown, and V. Hatzivassiloglou, 
1996,?Translating Collocations for Bilingual 
Lexicons: A Statistical Approach,? Computational 
Linguistics, 22(1). 
Sproat, R., C. Shih, W. Gale, & N. Chang,1996, "A 
Stochastic Finite-State Word-Segmentation 
Algorithm for Chinese," Computational Linguistics, 
22(3),377-404. 
Sun, M. S., C.N. Huang, H.Y. Gao, & Jie Fang, 1994, 
"Identifying Chinese Names in Unrestricted Texts", 
Communication of COLIPS, Vol.4 No. 2, 113-122. 
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 478?486,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Acquiring Translation Equivalences of Multiword Expressions by 
Normalized Correlation Frequencies 
Ming-Hong Bai1,2 Jia-Ming You1 Keh-Jiann Chen1 Jason S. Chang2 
1 Institute of Information Science, Academia Sinica, Taiwan 
2 Department of Computer Science, National Tsing-Hua University, Taiwan 
mhbai@sinica.edu.tw, swimming@hp.iis.sinica.edu.tw, 
kchen@iis.sinica.edu.tw, jschang@cs.nthu.edu.tw 
 
Abstract 
In this paper, we present an algorithm for ex-
tracting translations of any given multiword 
expression from parallel corpora. Given a 
multiword expression to be translated, the 
method involves extracting a short list of tar-
get candidate words from parallel corpora 
based on scores of normalized frequency, 
generating possible translations and filtering 
out common subsequences, and selecting the 
top-n possible translations using the Dice 
coefficient. Experiments show that our ap-
proach outperforms the word alignment-
based and other naive association-based me-
thods. We also demonstrate that adopting the 
extracted translations can significantly im-
prove the performance of the Moses machine 
translation system. 
1 Introduction 
Translation of multiword expressions (MWEs), 
such as compound words, phrases, collocations 
and idioms, is important for many NLP tasks, 
including the techniques are helpful for dictio-
nary compilation, cross language information 
retrieval, second language learning, and machine 
translation. (Smadja et al, 1996; Gao et al, 2002; 
Wu and Zhou, 2003). However, extracting exact 
translations of MWEs is still an open problem, 
possibly because the senses of many MWEs are 
not compositional (Yamamoto and Matsumoto, 
2000), i.e., their translations are not composi-
tions of the translations of individual words. For 
example, the Chinese idiom ???? should be 
translated as ?turn a blind eye,? which has no 
direct relation with respect to the translation of 
each constituent (i.e., ?to sit?, ?to see? and ?to 
ignore?) at the word level.  
Previous SMT systems (e.g., Brown et al, 
1993) used a word-based translation model 
which assumes that a sentence can be translated 
into other languages by translating each word 
into one or more words in the target language. 
Since many concepts are expressed by idiomatic 
multiword expressions instead of single words, 
and different languages may realize the same 
concept using different numbers of words (Ma et 
al., 2007; Wu, 1997), word alignment based me-
thods, which are highly dependent on the proba-
bility information at the lexical level, are not 
well suited for this type of translation.  
To address the above problem, some methods 
have been proposed for extending word align-
ments to phrase alignments. For example, Och et 
al. (1999) proposed the so-called grow-diag-
final heuristic method for extending word 
alignments to phrase alignments. The method is 
widely used and has achieved good results for 
phrase-based statistical machine translation. 
(Och et al, 1999; Koehn et al, 2003; Liang et al, 
2006). Instead of using heuristic rules, Ma et al 
(2008) showed that syntactic information, e.g., 
phrase or dependency structures, is useful in ex-
tending the word-level alignment. However, the 
above methods still depend on word-based 
alignment models, so they are not well suited to 
extracting the translation equivalences of seman-
tically opaque MWEs due to the lack of word 
level relations between the translational corres-
pondences. Moreover, the aligned phrases are 
not precise enough to be used in many NLP ap-
plications like dictionary compilation, which 
require high quality translations. 
Association-based methods, e.g., the Dice 
coefficient, are widely used to extract transla-
tions of MWEs. (Kupiec, 1993; Smadja et al, 
1996; Kitamura and Matsumoto, 1996; Yama-
moto and Matsumoto, 2000; Melamed, 2001). 
The advantage of such methods is that associa-
tion relations are established at the phrase level 
instead of the lexical level, so they have the po-
tential to resolve the above-mentioned transla-
tion problem. However, when applying associa-
tion-based methods, we have to consider the fol-
lowing complications. The first complication, 
which we call the contextual effect, causes the 
extracted translation to contain noisy words. For 
478
example, translations of the Chinese idiom ??
?? (best of both worlds) extracted by a naive 
association-based method may contain noisy 
collocation words like difficult, try and cannot, 
which are not part of the translation of the idiom. 
They are actually translations of its collocation 
context, such as ??(difficult), ??(try), and 
??(cannot). This problem arises because naive 
association methods do not deal with the effect 
of strongly collocated contexts carefully. If we 
can incorporate lexical-level information to dis-
count the noisy collocation words, the contextual 
effect could be resolved. 
 
English (y) fy fx,y Dice(x,y)
quote out of context 22 19 0.56 
take out of context 17 11 0.35 
interpret out of context 2 2 0.08 
out of context 53 32 0.65 
Table 1. The Dice coefficient tends to select a com-
mon subsequence of translations. (The frequency of
???? ,fx, is 46.) 
 
The second complication, which we call the 
common subsequence problem, is that the Dice 
coefficient tends to select the common subse-
quences of a set of similar translations instead of 
the full translations. Consider the translations of 
???? (quote out of context) shown in the 
first three rows of Table 1. The Dice coefficient 
of each translation is smaller than that of the 
common subsequence ?out of context? in the last 
row. If we can tell common subsequence apart 
from correct translations, the common subse-
quence problem could be resolved. 
In this paper, we propose an improved preci-
sion method for extracting MWE translations 
from parallel corpora. Our method is similar to 
that of Smadja et al (1996), except that we in-
corporate lexical-level information into the asso-
ciation-based method. The algorithm works ef-
fectively for various types of MWEs, such as 
phrases, single words, rigid word sequences (i.e., 
no gaps) and gapped word sequences. Our expe-
riment results show that the proposed translation 
extraction method outperforms word alignment-
based methods and association-based methods. 
We also demonstrate that precise translations 
derived by our method significantly improve the 
performance of the Moses machine translation 
system. 
The remainder of this paper is organized as 
follows. Section 2 describes the methodology for   
extracting translation equivalences of MWEs. 
Section 3 describes the experiment and presents 
the results. In Section 4, we consider the appli-
cation of our results to machine translation. Sec-
tion 5 contains some concluding remarks. 
2 Extracting Translation Equivalences   
Our MWE translation extraction method is simi-
lar to the two-phase approach proposed by 
Smadja et al (1996). The two phases can be 
briefly described as follows:  
Phase 1: Extract candidate words correlated to 
the given MWE from parallel text. 
Phase 2:  
1. Generate possible translations for the 
MWE by combining the candidate words. 
2. Select possible translations by the Dice 
coefficient. 
We propose an association function, called the 
normalized correlation frequency, to extract 
candidate words in the phase 1. This method 
incorporates lexical-level information with asso-
ciation measure to overcome the contextual ef-
fect. In phase 2, we also propose a weighted fre-
quency function to filter out false common sub-
sequences from possible translations. The filter-
ing step is applied before the translation select-
ing step of phase 2.   
Before describing our extraction method, we 
define the following important terms used 
throughout the paper. 
Focused corpus (FC): This is the corpus 
created for each targeted MWE. It is a subset of 
the original parallel corpora, and is comprised of 
the selected aligned sentence pairs that contain 
the source MWE and its translations. 
Candidate word list (CW): A list of extracted 
candidate words for the translations of the 
source MWE. 
2.1 Selecting Candidate Words 
For a source MWE, we try to extract from the 
FC a set of k candidate words CW that are high-
ly correlated to the source MWE. We then as-
sume that the target translation is a combination 
of some words in CW. As noted by Smadja et al 
(1996), this two-step approach drastically reduc-
es the search space. 
However, translations of collocated context 
words in the source word sequence create noisy 
candidate words, which might cause incorrect 
extraction of target translations by naive statis-
tical correlation measures, such as the Dice coef-
479
ficient used by Smadja et al (1996). The need to 
avoid this context effect motivates us to propose 
a candidate word selection method that uses the 
normalized correlation frequency as an associa-
tion measure. 
The rationale behind the proposed method is 
as follows. When counting the word frequency, 
each word in the target corpus normally contri-
butes a frequency count of one. However, we are 
only interested in the word counts correlated to a 
MWE. Therefore, intuitively, we define the 
normalized count of a target word e as the trans-
lation probability of e given the MWE.  
We explain the concept of normalizing the 
correlation count in Section 2.1.1 and the com-
putation of the normalized correlation frequency 
in Section 2.1.2. 
2.1.1 Normalizing Correlation Count 
We propose an association measure called the 
normalized correlation frequency, which ranks 
the association strength of target words with the 
source MWE. For ease of explanation, we use 
the following notations: let f=f1,f2,?,fm and 
e=e1,e2,?,en be a pair of parallel Chinese and 
English sentences; and let w=t1,t2,?,tr be the 
Chinese source MWE. Hence, w is a subse-
quence of f.  
When counting the word frequency, each 
word in the target corpus normally contributes a 
frequency count of one. However, since we are 
interested in the word counts that correlate to w, 
we adopt the concept of the translation model 
proposed by Brown et al(1993). Each word e in 
a sentence e might be generated by some words, 
denoted as r, in the source sentence f. If r is 
non-empty the relation between r and w should 
fit one of the following cases: 
 
1) All words in r belong to w, i.e., wr ? , so 
we say that e is only generated by w. 
2) No words in r belong to w, i.e., wfr ?? , 
so we say that e is only generated by context 
words.  
3) Some words in r belong to w, while others 
are context words. 
 
Intuitively, In Cases 1 and 2, the correlation 
count of an instance e should be 1 and 0 respec-
tively. In Case 3, the normalized count of e is 
the expected frequency generated by w divided 
by the expected frequency generated by f. With 
that in mind, we define the weighted correlation 
count, wcc, as follows:  
 
?
?
??
??
?+
?+=
f
w
f
w
wfe
j
i
f j
f i
fep
fep
ewcc
||)|(
||)|(
),,;( , 
 
where ? is a very small smoothing factor in case 
e is not generated by any word in f. The proba-
bility p(e | f) is the word translation probability 
trained by IBM Model 1 on the whole parallel 
corpus. 
The rationale behind the weighted correlation 
count, wcc, is that if e is part of the translation of 
w, then its association with w should be stronger 
than other words in the context. Hence its wcc 
should be closer to 1. Otherwise, the association 
is weaker and the wcc should be closer to 0. 
2.1.2 Normalized Correlation 
Once the weighted correlation counts wcc is 
computed for each word in FC, we compute the 
normalized correlation frequency for each word 
e as the total sum of the  of all w 
in bilingual sentences (e, f)  in FC. The norma-
lized correlation frequency (ncf) is defined as 
follows: 
),,;( wfeewcc
 
?
=
=
n
i
iiewccencf
1
)()( ),,;();( wfew . 
 
We choose the top-n English words ranked by 
ncf as our candidate words and filter out those 
whose ncf is less than a pre-defined threshold. 
Table 2 shows the candidate words for the Chi-
nese term ???? (quote/take/interpret out of 
context) sorted by their ncf values. To illustrate 
the effectiveness ncf, we also display candidate 
words of the term with their Dice values in 
Tables 3. As shown in the tables, noise words 
such as justify, meaning and unfair are ranked 
lower using ncf than using Dice, while correct 
candidates, such as out, take and remark are 
ranked higher.  We present more experimental 
results in Section 3. 
 
2.2 Generation and Ranking of Candi-
date Translations 
After determining the candidate words, candi-
date translations of w can be generated by mark-
ing the candidate words in each sentence of FC. 
The word sequences marked in each sentence 
are deemed possible translations. At the same 
time, the weakly associated function words,  
480
Candidate words e freq ncf(e,w) 
context 54 31.55 
out 58 24.58 
quote 26 5.84 
take 23 4.81 
remark 8 1.84 
interpret 3 1.38 
piecemeal 1 0.98 
deliberate 3 0.98 
Table 2. Candidate words for the Chinese term 
???? sorted by their global normalized correla-
tion frequencies. 
 
Candidate words e freq dice(e,w) 
context 54 0.0399 
quote 26 0.0159 
deliberate 3 0.0063 
justify 3 0.0034 
interpretation 7 0.0032 
meaning 3 0.0029 
cite 3 0.0025 
unfair 4 0.0023 
Table 3. Candidate words for the Chinese term ??
??  sorted by their Dice coefficient values. 
 
which we fail to select in the candidate word 
selection stage, should be recovered. The rule is 
quite simple: if a function word is adjacent to 
any candidate word, it should be recovered. For 
example, in the following sentence, the function 
word of would be recovered and added to the 
marked sequence: 
 
?The financial secretary has 
been quoted out of context. 
??? ?? ? ?? ? ????.?  
 
 The marked words are shown in boldface.  
2.2.1 Generating Possible Translations 
Although we have selected a reliable candidate 
word list, it may still contain some noisy words 
due to the MWE?s collocation context. Consider 
the following example: 
 
...as quoted in the audit 
report, if taken out of con-
text...  
 
In this instance, quoted is a false positive; there-
fore, the marked word sequence m ?quoted tak-
en out of context? is not the correct translation. 
To avoid such false positives, we include m and 
all its subsequences as possible translations.  
quoted taken out of context 
quoted taken out of 
quoted taken out context 
quoted taken of context 
quoted out of context 
taken out of context 
? 
quoted out 
taken out 
quoted 
taken 
out 
context
Table 4. Example subsequences generated of w and 
add them to the candidate translation list.  
 
Table 4 shows the subsequences of m in the 
above example. The generation process is used 
to increase the coverage of correct translations in 
the candidate list; otherwise, many correct trans-
lations will be lost. However, the process may 
also trigger the side effect of the common sub-
sequence problem described in Section 1.  Since 
all candidates compete for the best translations 
by comparing their association strength with w, 
the common subsequences will have an advan-
tage. 
 
2.2.2 Filtering Common Subsequences 
To resolve the common subsequence effect prob-
lem, we evaluate each candidate translation, in-
cluding its subsequences, by a concept similar to 
the normalized correlation frequency. As men-
tioned in Section 1, the Dice coefficient tends to 
select the common subsequences of some candi-
dates because they have higher frequencies. To 
avoid this problem, we use the normalized corre-
lation frequency to filter out false common sub-
sequences from the candidate translation list. 
Here, we also use the weighted correlation count 
wcc to weight the frequency count of a candidate 
translation. Suppose we have a marked sequence 
in a sentence, m, whose subsequences are gener-
ated in the way described in the previous section. 
If the weighted count of m is assigned the score 
1, the weighted count (wc) of a subsequence t is 
then defined as follows: 
 
?
???
?=
tm
wfewmfet
e
ewccwc )),,;(1(),,,;( . 
 
The underlying concept of wc is that the original 
marked sequence m is supposed to be the most 
481
likely translation of w and the weighted count is 
set to 1. Then, if a subsequence t is generated by 
removing a word e from m, the weighted count 
of the subsequence is reduced by multiplying the 
complement probability of e generated by w. 
Note that the weighted correlation count wcc is 
the probability of the word e generated by w. 
After all  in each sentence of 
the FC have been computed, the weighted fre-
quency for a sequence t can be determined by 
summing the weighted frequencies over FC as 
follows:  
),,,;( wmfetwc
 
?
??
=
FC
wcwf
),(
),,,;();(
fe
wmfetwt . 
 
We compute the wf for each candidate transla-
tion and then sort the candidate translations by 
their wf values. 
Next, we filter out common subsequences 
based on the following rule: for a sequence t, if 
there is a super-sequence t' on the sorted candi-
date translation list and the wf value of t is less 
than that of t', then t is assumed be a common 
subsequence of real translations and removed 
from the list. 
 
candidate translation list freq wf 
quote out of context 19 17.55 
of context 35 15.45 
out of context 32 14.82 
quote of context 19 13.32 
out 35 11.92 
quote 23 11.63 
quote out 19 9.42 
Table 5. Part of the candidate translation list for the 
Chinese idiom, ????, sorted by the wf values. 
 
Table 5 shows an example of the rule?s appli-
cation. The candidate translation list is sorted by 
the translations? wf values. Then, candidates 2-7 
are removed because they are subsequences of 
the first candidate and their wf values are smaller 
than that of the first candidate. 
2.3 Selection of Candidate Translations 
Having removed the common subsequences of 
real translations from the candidate translation 
list of w, we can select the best translations by 
comparing their association strength with w for 
the remaining candidates.  The Dice coefficient 
is a good measure for assessing the association 
strength and selecting translations from the can-
didate list. For a candidate translation t, the Dice 
coefficient is defined as follows: 
 
)()(
),(2
),(
wt
wt
wt
pp
p
Dice += . 
 
Where p(t,w), p(t), p(w) are probabilities of  
(t,w), t, w derived from the training corpus.  
After obtaining the Dice coefficients of the 
candidate translations, we select the top-n candi-
date translations as possible translations of w. 
 
3 Experiments 
In our experiments, we use the Hong Kong Han-
sard and the Hong Kong News parallel corpora 
as training data. The training data was prepro-
cessed by Chinese word segmentation to identify 
words and parsed by Chinese parser to extract 
MWEs. To evaluate the proposed approach, we 
randomly extract 309 Chinese MWEs from 
training data, including dependent word pairs 
and rigid idioms. We then randomly select 103 
of those MWEs as the development set and use 
the other 206 as the test set. The reference trans-
lations of each Chinese MWE are manually ex-
tracted from the parallel corpora. 
 
3.1 Evaluation of Word Candidates 
To evaluate the method for selecting candidate 
words, we use the coverage rate, which is de-
fined as follows: 
 
?
?
?=
w w
ww
||
||1
A
CA
n
coverage , 
 
where n is the number of MWEs in the test set, 
Aw denotes the word set of the reference transla-
tions of w, and Cw denotes a candidate word list 
extracted by the system.  
Table 6 shows the coverage of our method, 
NCF, compared with the coverage of the IBM 
model 1 and the association-based methods MI, 
Chi-square, and Dice. As we can see, the top-10 
candidate words of NCF cover almost 90% of 
the words in the reference translations. Whereas, 
the coverage of the association-based methods 
and IBM model 1 is much lower than 90%. The 
result implies that the candidate extraction me-
thod can extract a more precise candidate set 
than other methods. 
 
482
Method Top10 Top20 Top30 
MI 0.514 0.684 0.760 
Chi-square 0.638 0.765 0.828 
Dice 0.572 0.735 0.803 
IBM 1 0.822 0.900 0.948
NCF 0.899 0.962 0.973 
Table 6. The coverage rates of the candidate words 
extracted by the compared methods 
 
Figure 1 shows the curve diagram of the cov-
erage rate of each method. As the figure shows, 
when the size of the candidate list is increased, 
the coverage rate of using NCF rises rapidly as n 
increases but levels off after n=20. Whereas, the 
coverage rates of other measures grow much 
slowly.  
 
 
Figure 1. The curve diagram of the coverage of 
the candidate word list compiled by each method. 
 
From the evaluation of candidate word selec-
tion, we find that the ncf method, which incorpo-
rates lexical-level information into association-
based measure, can effectively filter out noisy 
words and generates a highly reliable list of can-
didate words for a given MWE. 
 
3.2 Evaluating Extracted Translations 
To evaluate the quality of MWE translations 
extracted automatically, we use the following 
three criteria: 
 
1) Translation accuracy: 
This criterion is used to evaluate the top-n 
translations of the system. It treats each 
translation produced as a string and com-
pares the whole string with the given ref-
erence translations. If any one of the top-n 
hypothesis translations is included in the 
reference translations, it is deemed correct.   
2) WER (word error rate): 
This criterion compares the top-1 hypo-
thesis translation with the reference trans-
lations by computing the edit distance (i.e., 
the minimum number of substitutions, in-
sertions, and deletions) between the hypo-
thesis translation and the given reference 
translations. 
3) PER (position-independent word error 
rate): 
This criterion ignores the word order and 
computes the edit distance between the 
top-1 hypothesis translation and the given 
reference translations. 
 
We also use the MT task to evaluated our me-
thod with other systems. For that, we use the 
GIZA++ toolkit (Och et al, 2000 ) to align the 
Hong Kong Hansard and Hong Kong News pa-
rallel corpora. Then, we extract the translations 
of the given source sequences from the aligned 
corpus as the baseline. We use the following two 
methods to extract translations from the aligned 
results. 
 
1) Uni-directional alignment  
We mark all English words that were 
linked to any constituent of w in the pa-
rallel Chinese-English aligned corpora. 
Then, we extract the marked sequences 
from the corpora and compute the fre-
quency of each sequence. The top-n high 
frequency sequences are returned as the 
possible translations of w. 
2) Bi-directional alignments 
We use the grow-diag-final heuristic (Och 
et al, 1999) to combine the Chinese-
English and English-Chinese alignments, 
and then extract the top-n high frequency 
sequences as described in method 1. 
 
To determine the effect of the common subse-
quence filtering method, FCS, we divide the 
evaluation of our system into two phases: 
 
1) NCF+Dice: 
This system uses the normalized correla-
tion frequency, NCF, to select candidate 
words as described in Section 2.1. It then 
extracts candidate translations (described 
in Section 2.2), but FCS is not used. 
2) NCF+FCS+Dice: 
This is similar to system 1, but it uses 
FCS to filter out common subsequences 
(described in subsection 2.2.2). 
483
Method WER(%) PER(%) 
Uni-directional 4.84 4.02 
Bi-directional 5.84 5.12 
NCF+Dice 3.55 3.24 
NCF+FCS+Dice 2.45 2.23 
Table 7. Translation error rates of the systems. 
 
 
Method Top1 Top2 Top3 
Uni-directional 67.5 79.6 83.0 
Bi-directional 65.5 77.7 81.1 
NCF+Dice 72.8 85.9 88.3 
NCF+FCS+Dice 78.2 89.3 91.7 
Table 8. Translation accuracy rates of the systems. 
(%) 
 
Table 7 shows the word error rates for the 
above systems. As shown in the first and second 
rows, the translations extracted from uni-
directional alignments are better than those ex-
tracted from bi-directional alignments. This 
means that the grow-diag-final heuristic reduces 
the accuracy rate when extracting MWE transla-
tions.  
The results in the third row show that the 
NCF+Dice system outperforms the methods 
based on GIZA++. In other words, the NCF me-
thod can effectively resolve the difficulties of 
extracting MWE translations discussed in Sec-
tion 1. 
In addition, the fourth row shows that the 
NCF+FCS+Dice system also outperforms the 
NCF+Dice system.  Thus, the FCS method can 
resolve the common subsequence problem effec-
tively. 
Table 8 shows the translation accuracy rates 
of each system. The NCF+FCS+Dice system 
achieves the best translation accuracy. Moreover, 
it significantly improves the performance of 
finding MWE translation equivalences. 
 
4 Applying  MWE Translations to MT 
To demonstrate the usefulness of extracted 
MWE translations to existing statistical machine 
translation systems, we use the XML markup 
scheme provided by the Moses decoder, which 
allows the specification of translations for parts 
of a sentence. The procedure for this experiment 
consists of three steps: (1) the extracted MWE 
translations are added to the test set with the 
XML markup scheme, (2) after which the data is 
input to the Moses decoder to complete the 
translation task, (3) the results are evaluated 
 Moses  MWE +Moses
NIST06-sub 23.12 23.49 
NIST06 21.57 21.79 
 Table 9. BLEU scores of the translation results. 
 
using the BLEU metric (Papineni et al, 2002). 
4.1 Experimental Settings 
To train a translation model for Moses, we use 
the Hong Kong Hansard and the Hong Kong 
News parallel corpora as training data 
(2,222,570 sentence pairs). We also use the 
same parallel corpora to extract translations of 
MWEs. The NIST 2008 evaluation data (1,357 
sentences, 4 references) is used as development 
set and NIST 2006 evaluation data (1,664 sen-
tences, 4 references) is used as test set. 
4.2 Selection of MWEs 
Due to the limitation of the XML markup 
scheme, we only consider two types of MWEs: 
continuous bigrams and idioms. Since the goal 
of this experiment is not focus on extraction of 
MWEs, simple methods are applied to extract 
MWEs from the training data: (1) we collect all 
continuous bigrams from Chinese sentences in 
the training data and then simply filter out the 
bigrams by mutual information (MI) with a thre-
shold1, (2) we also extract all idioms from Chi-
nese sentences of the training data by collecting 
all 4-syllables words from the training data and 
filtering out obvious non-idioms, such as deter-
minative-measure words and temporal words by 
their part-of-speeches, because most Chinese 
idioms are 4-syllables words.  
In total, 33,767 Chinese bigram types and 
20,997 Chinese idiom types were extracted from 
training data; and the top-5 translations of each 
MWE were extracted by the method described in 
Section 2. Meanwhile 1,171 Chinese MWEs 
were added to the translations in the test set. The 
Chinese words covered by the MWEs in test 
data set were 2,081 (5.3%). 
 
4.3 Extra Information 
When adding the translations to the test data, 
two extra types of information are required by 
the Moses decoder. The first type comprises the 
function words between the translation and its 
context. For example, if ??  ??/economic 
cooperation is added to the test data, possible  
                                                 
1 We set the threshold at 5. 
484
source sentence ... ????<MWE>????</MWE>????? ... 
Moses ... entered blinded by the colourful community ... 
MWE+Moses ... entered the colourful community ... 
reference ... entered the colorful society ... 
source sentence ... ?????  <MWE>???  ??</MWE> ??? ... 
Moses ... do not want to see an escalation of crisis ... 
MWE+Moses ... do not want to see a further escalation of crisis ... 
reference ... don 't want to see the further escalation of the crisis ... 
source sentence ... ????????<MWE>?????</MWE> ... 
Moses ... the people 's interests ... 
MWE+Moses ... the people of the fundamental interests ... 
reference ... the fundamental interests of the masses ... 
Table 10. Examples of improved translation quality with the MWE translation equivalences. 
 
function words, such as ?in? or ?with?, should be 
provided for the translation. Because the Moses 
decoder does not generate function words that 
are context dependent, it treats a function word 
as a part of the translation. Therefore, we collect 
possible function words for each translation 
from the corpora when the conditional probabili-
ty is larger than a threshold2. 
The second type of information is the phrase 
translation probability and lexical weighting. 
Computing the phrase translation probability is 
trivial in the training corpora, but lexical weight-
ing (Koehn et al, 2003) needs lexical-level 
alignment. For convenience, we assume that 
each word in an MWE links to each word in the 
translations. Under this assumption, the lexical 
weighting is simplified as follows:   
 
??
??= ?= aji ji
n
i
w efpajij
ap
),(1
)|(
|}),(|{|
1
),|( ef
        ? ?
= ??
?
n
i e
ji
j
efp
1
)|(
||
1
ee
. 
 
Then, it is trivial to compute the simplified lexi-
cal weighting of each MWE correspondence 
when the word translation probability table is 
provided. Here, we use the IBM model 1 to learn 
the table from the training data. 
4.4 Evaluation Results 
We trained a model using Moses toolkit (Koehn 
et al, 2007) on the training data as our baseline 
system.  
Table 9 shows the influence of adding the 
MWE translations to the test data. In the first 
row (NIST06-sub), we only consider sentences 
containing MWE translations for BLEU score 
evaluation (726 sentences). In the second row, 
we took the whole NIST 2006 evaluation set 
into consideration (1,664 sentences). The Chi-
nese words covered by the MWEs in NIST06-
sub and NIST06 were 9.9% and 5.3% respec-
tively. 
Adding MWE translations to the test data sta-
tistically significantly lead to better results than 
those of the baseline. Significance was tested 
using a paired bootstrap (Koehn, 2004) with 
1000 samples (p<0.02). Although the improve-
ment in BLEU score seems small, it is actually 
reasonably good given that the MWEs account 
for only 5% of the NIST06 test set. Examples of 
improved translations are shown in Table 10. 
There is still room for improvement of the pro-
posed MWE extraction method in order to pro-
vide more MWE translation pairs or design a 
feasible way to incorporate discontinuous bilin-
gual MWEs to the decoder. 
5 Conclusions and Future Work 
We have proposed a high precision algorithm for 
extracting translations of multiword expressions 
from parallel corpora. The algorithm can be used 
to translate any language pair and any type of 
word sequence, including rigid sequences and 
discontinuous sequences. Our evaluation results 
show that the algorithm can cope with the diffi-
culties caused by indirect association and the 
common subsequence effects, leading to signifi-
cant improvement over the word alignment-
based extraction methods used by the state of the 
art systems and other association-based extrac-
tion methods. We also demonstrate that ex-
tracted translations significantly improve the                                                  
2 We set the threshold at 0.1. 
485
performance of the Moses machine translation 
system. 
In future work, it would be interesting to de-
velop a machine translation model that can be 
integrated with the translation acquisition algo-
rithm in a more effective way. Using the norma-
lized-frequency score to help phrase alignment 
tasks, as the grow-diag-final heuristic, would 
also be interesting direction to explore. 
 
Acknowledgement 
This research was supported in part by the Na-
tional Science Council of Taiwan under the NSC 
Grants: NSC 96-2221-E-001-023-MY3. 
References  
Brown, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, Robert L. Mercer. 1993. The Mathe-
matics of Statistical Machine Translation: Parame-
ter Estimation. Computational Linguistics, 
19(2):263-311.  
Gao, Jianfeng, Jian-Yun Nie, Hongzhao He, Weijun 
Chen, Ming Zhou. 2002. Resolving Query Trans-
lation Ambiguity using a Decaying Co-occurrence 
Model and Syntactic Dependence Relations. In 
Proc. of SIGIR?02. pp. 183 -190. 
Kitamura, Mihoko and Yuji Matsumoto. 1996. Au-
tomatic Extraction of Word Sequence Correspon-
dences in Parallel Corpora. In Proc. of the 4th An-
nual Workshop on Very Large Corpora. pp. 79-87. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In Proc. 
of HLT/NAACL?03. pp. 127-133. 
Koehn, Philipp. 2004. Statistical significance tests for 
machine translation evaluation. In Proc. 
EMNLP?04. pp. 388-395. 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses: 
Open source toolkit for statistical machine transla-
tion. In ACL?07, demonstration session. 
Kupiec, Julian. 1993. An Algorithm for Finding 
Noun Phrase Correspondences in Bilingual Corpo-
ra. In Proc. of ACL?93 . pp. 17-22. 
Liang, Percy, Ben Taskar, Dan Klein. 2006. Align-
ment by Agreement. In Proc. of HLT/NAACL?06. 
pp. 104-111. 
Ma, Yanjun, Nicolas Stroppa, Andy Way. 2007. 
Bootstrapping Word Alignment via Word Packing. 
In Proc. of ACL?07. pp. 304-311. 
Ma, Yanjun, Sylwia Ozdowska, Yanli Sun, and Andy 
Way. 2008. Improving Word Alignment Using 
Syntactic Dependencies. In Proc. of ACL/HLT?08 
Second Workshop on Syntax and Structure in Sta-
tistical Translation. pp. 69-77. 
Melamed, Ilya Dan. 2001. Empirical Methods for 
Exploiting parallel Texts. MIT press. 
Och, Franz Josef and Hermann Ney. 2000. Improved 
Statistical Alignment Models. In Proc. of ACL?00. 
pp. 440-447. 
Och, Franz Josef, Christoph Tillmann, and Hermann 
Ney. 1999. Improved Alignment Models for Sta-
tistical Machine Translation. In Proc. of 
EMNLP/VLC?99. pp. 20-28. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In Proc. 
of ACL?02. pp. 311-318. 
Smadja, Frank, Kathleen R. McKeown, and Vasileios 
Hatzivassiloglou. 1996. Translating Collocations 
for Bilingual Lexicons: A Statistical Approach. 
Computational Linguistics, 22(1):1-38. 
Wu, Dekai. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377-403.  
Wu, Hua, Ming Zhou. 2003. Synonymous Colloca-
tion Extraction Using Translation Information. In 
Proc. of ACL?03. pp. 120-127. 
Yamamoto, Kaoru, Yuji Matsumoto. 2000. Acquisi-
tion of Phrase-level Bilingual Correspondence us-
ing Dependency Structure. In Proc. of COL-
ING?00. pp. 933-939. 
 
486
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 177 ? 187, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Linguistically-Motivated Grammar Extraction, 
Generalization and Adaptation 
Yu-Ming Hsieh, Duen-Chi Yang, and Keh-Jiann Chen 
Institute of Information Science, Academia Sinica, Taipei 
{morris, ydc, kchen}@iis.sinica.edu.tw 
Abstract. In order to obtain a high precision and high coverage grammar, we 
proposed a model to measure grammar coverage and designed a PCFG parser to 
measure efficiency of the grammar. To generalize grammars, a grammar binari-
zation method was proposed to increase the coverage of a probabilistic context-
free grammar. In the mean time linguistically-motivated feature constraints 
were added into grammar rules to maintain precision of the grammar. The gen-
eralized grammar increases grammar coverage from 93% to 99% and bracket-
ing F-score from 87% to 91% in parsing Chinese sentences. To cope with error 
propagations due to word segmentation and part-of-speech tagging errors, we 
also proposed a grammar blending method to adapt to such errors. The blended 
grammar can reduce about 20~30% of parsing errors due to error assignment of 
pos made by a word segmentation system.  
Keywords: Grammar Coverage, Ambiguity, Sentence Parsing, Grammar  
Extraction. 
1   Introduction 
Treebanks provide instances of phrasal structures and their statistical distributions. 
However none of treebanks provide sufficient amount of samples which cover all types 
of phrasal structures, in particular, for the languages without inflectional markers, such 
as Chinese. It results that grammars directly extracted from treebanks suffer low cover-
age rate and low precision [7]. However arbitrarily generalizing applicable rule patterns 
may cause over-generation and increase ambiguities. It may not improve parsing per-
formance [7]. Therefore a new approach of grammar binarization was proposed in this 
paper. The binarized grammars were derived from probabilistic context-free grammars 
(PCFG) by rule binarization. The approach was motivated by the linguistic fact that 
adjuncts could be arbitrarily occurred or not occurred in a phrase. The binarized gram-
mars have better coverage than the original grammars directly extracted from treebank. 
However they also suffer problems of over-generation and structure-ambiguity. Con-
temporary grammar formalisms, such as GPSG, LFG, HPSG, take phrase structure rules 
as backbone for phrase structure representation and adding feature constraints to elimi-
nate illegal or non-logical structures. In order to achieve higher coverage, the backbone 
grammar rules (syntactic grammar) are allowed to be over-generation and the feature 
constraints (semantic grammar for world knowledge) eliminate superfluous structures 
178 Y.-M. Hsieh, D.-C. Yang, and K.-J. Chen 
and increase the precision of grammar representation. Recently, probabilistic prefer-
ences for grammar rules were incorporated to resolve structure-ambiguities and had 
great improvements on parsing performances [2, 6, 10]. Regarding feature constrains, it 
was shown that contexture information of categories of neighboring nodes, mother 
nodes, or head words are useful for improving grammar precision and parsing perform-
ances [1, 2, 7, 10, 12]. However tradeoffs between grammar coverage and grammar 
precision are always inevitable. Excessive grammatical constraints will reduce grammar 
coverage and hence reduce parsing performances. On the other hand, loosely con-
strained grammars cause structure-ambiguities and also reduce parsing performances. In 
this paper, we consider grammar optimization in particular for Chinese language. Lin-
guistically-motivated feature constraints were added to the grammar rules and evaluated 
to maintain both grammar coverage and precision. In section 2, the experimental envi-
ronments were introduced. Grammar generalization and specialization methods were 
discussed in section 3. Grammars adapting to pos-tagging errors were discussed in sec-
tion 4. Conclusions and future researches were stated in the last section. 
2   Research Environments 
The complete research environment, as shown in the figure 1, comprises of the fol-
lowing five modules and functions. 
a) Word segmentation module: identify words including out-of-vocabulary word 
and provide their syntactic categories. 
b) Grammar construction module: extract and derive (perform rule generalization, 
specialization and adaptation processes) probabilistic grammars from tree-
banks. 
c) PCFG parser: parse input sentences. 
d) Evaluation module: evaluate performances of parsers and grammars. 
e) Semantic role assignment module: resolve semantic relations for constituents. 
 
Fig. 1. The system diagram of CKIP parsing environment 
 Linguistically-Motivated Grammar Extraction, Generalization and Adaptation 179 
2.1   Grammar Extraction Module  
Grammars are extracted from Sinica Treebank [4, 5]. Sinica Treebank version 2.0 
contains 38,944 tree-structures and 230,979 words. It provides instances of phrasal 
structures and their statistical distributions. In Sinica Treebank, each sentence is anno-
tated with its syntactic structure and semantic roles for constituents in a dependency 
framework. Figure 2 is an example. 
e.g. ? ? ?? ? ?. 
 Ta  jiao  Li-si  jian  qiu. 
 ?He asked Lisi to pick up the ball.? 
Tree-structure:  
S(agent:NP(Head:Nh:?)|Head:VF:?|goal:NP(Head:Nb:??)|theme:VP(Head:VC:?| 
goal:NP(Head:Na:?))) 
Fig. 2. A sample tree-structure 
Since the Treebank cannot provide sufficient amount of samples which cover all 
types of phrasal structures, it results that grammars directly extracted from treebanks 
suffer low coverage rate [5]. Therefore grammar generalization and specialization 
processes are carried out to obtain grammars with better coverage and precision. The 
detail processes will be discussed in section 3. 
2.2   PCFG Parser and Grammar Performance Evaluation 
The probabilistic context-free parsing strategies were used as our parsing model [2, 6, 
8]. Calculating probabilities of rules from a treebank is straightforward and we use 
maximum likelihood estimation to estimate the rule probabilities, as in [2]. The parser 
adopts an Earley?s Algorithm [8]. It is a top-down left-to-right algorithm. The results 
of binary structures will be normalized into a regular phrase structures by removing 
intermediate nodes, if used grammars are binarized grammars. Grammar efficiency 
will be evaluated according to its parsing performance. 
2.3   Experiments and Performance Evaluation 
Three sets of testing data were used in our performance evaluation. Their basic statis-
tics are shown in Table 1. Each set of testing data represents easy, hard and moderate 
respectively.  
Table 1. Three sets of testing data were used in our experiments 
Testing data Sources hardness
# of short 
sentence 
(1-5 words) 
# of normal 
sentences 
(6-10 words)
# of long 
sentences 
(>11 words) 
Total 
sentences 
Sinica Balanced corpus moderate 612 385 124 1,121 
Sinorama Magazine harder 428 424 104 956 
Textbook Elementary school easy 1,159 566 25 1,750 
180 Y.-M. Hsieh, D.-C. Yang, and K.-J. Chen 
The following parser and grammar performance evaluation indicators were used in 
our experiments: 
z LP(Labeled Precision) 
parser by the labeled phrases of #
parser by the labeled phrasescorrect  of #LP =  
z LR(Labeled Recall) 
data  testing thein phrases of #
parser by the labeled phrasescorrect  of #LR =  
z LF(Labeled F-measure) 
LR  LP
2* LR * LPLF
+
=
 
z BP(Bracketed Precision) 
parser by the made brackets of pairs of #
parser by the madecorrectly  brackets of pairs of #BP =  
z BR(Bracketed Recall) 
data  testing theof standard gold  thein brackets of pairs of #
parser by the madecorrectly  brackets of pairs of #BR =  
z BF(Bracketed F-measure) 
BR  BP
2* BR * BPBF
+
=
 
Additional indicators regarding coverage of grammars?  
z RC-Type?type coverage of rules 
data  testingin  typesrule of #
rulesgrammar  anddata   testingboth in  typesrules of #Type-RC =
 
z RC-Token?token coverage of rules 
data  testingin  tokensrule of #
rulesgrammar  anddata   testingboth in  tokensrules of #Token-RC =
 
The token coverage of a set of rules is the ceiling of parsing algorithm to achieve. 
Tradeoff effects between grammar coverage and parsing F-score can be examined for 
each set of rules. 
3   Grammar Generalization and Specialization 
By using above mentioned research environment, we intend to find out most effec-
tive grammar generalization method and specialization features for Chinese lan-
guage. To extend an existing or extracted grammar, there are several different ap-
proaches. A na?ve approach is to generalize a fine-grained rule to a coarse-grained 
rule. The approach does not generate new patterns. Only the applicable patterns for 
each word were increased. However it was shown that arbitrarily increasing the 
applicable rule patterns does increase the coverage rates of grammars, but degrade 
parsing performance [5]. A better approach is to generalizing and specializing rules 
under linguistically-motivated way. 
 Linguistically-Motivated Grammar Extraction, Generalization and Adaptation 181 
3.1   Binary Grammar Generation, Generalization, and Specialization 
The length of a phrase in Treebank is variable and usually long phrases suffer from 
low probability. Therefore most PCFG approaches adopt the binary equivalence 
grammar, such as Chomsky normal form (CNF). For instance, a grammar rule of S? 
NP Pp Adv V can be replaced by the set of equivalent rules of {S?Np R0, R0?Pp 
R1, R1?Adv V}. The binarization method proposed in our system is different from 
CNF. It generalizes the original grammar to broader coverage. For instance, the above 
rule after performing right-association binarization 1  will produce following three 
binary rules {S?Np S?, S??Pp S?, S??Adv V}. It results that constituents (adjuncts 
and arguments) can be occurred or not occurred at almost any place in the phrase. It 
partially fulfilled the linguistic fact that adjuncts in a phrase are arbitrarily occurred. 
However it also violated the fact that arguments do not arbitrarily occur. Experimental 
results of the Sinica testing data showed that the grammar token coverage increased 
from 92.8% to 99.4%, but the labeling F-score dropped from 82.43% to 82.11% [7]. 
Therefore feature constraints were added into binary rules to limit over-generation 
caused by recursively adding constituents into intermediate-phrase types, such as S? at 
above example. 
Feature attached rules will look like following: 
S?
-left:Adv-head:V? Adv V; 
S?
-left:Pp-head:V?Pp S?-left:Adv-head:V; 
The intermediated node S?
-left:Pp-head:V says that it is a partial S structure with left-
most constituent Pp and a phrasal head V. Here the leftmost feature constraints linear 
order of constituents and the head feature implies that the structure patterns are head 
word dependent. Both constraints are linguistically plausible. Another advantage of 
the feature-constraint binary grammar is that in addition to rule probability it is easy 
to implement association strength of modifier word and head word to evaluate plausi-
bility of derived structures. 
3.2   Feature Constraints for Reducing Ambiguities of Generalized Grammars 
Adding feature constraints into grammar rules attempts to increase precision of gram-
mar representation. However the side-effect is that it also reduces grammar coverage. 
Therefore grammar design is balanced between its precision and coverage. We are 
looking for a grammar with highest coverage and precision. The tradeoff depends on 
the ambiguity resolution power of adopted parser. If the ambiguity resolution power 
of adopted parser is strong and robust, the grammar coverage might be more impor-
tant than grammar precision. On the other hand a weak parser had better to use 
grammars with more feature constraints. In our experiments, we consider grammars 
suited for PCFG parsing. The follows are some of the most important linguistically-
motivated features which have been tested. 
                                                          
1
 The reason for using right-association binarization instead of left-association or head-first 
association binarization is that our parsing process is from left to right. It turns out that pars-
ing speed of right associated grammars is much faster than left-associated grammars for left-
to-right parsing. 
182 Y.-M. Hsieh, D.-C. Yang, and K.-J. Chen 
Head (Head feature): Pos of phrasal head will propagate to all intermediate nodes 
within the constituent. 
Example:S(NP(Head:Nh:?)|S?
-VF(Head:VF:?|S?-VF(NP(Head:Nb:??)| 
VP(Head:VC:?| NP(Head:Na:?))))) 
Linguistic motivations: Constrain sub-categorization frame. 
Left (Leftmost feature): The pos of the leftmost constitute will propagate one?level to 
its intermediate mother-node only. 
Example:S(NP(Head:Nh:?)|S?
-Head:VF(Head:VF:?|S?-NP(NP(Head:Nb:??)| 
VP(Head:VC:?| NP(Head:Na:?))))) 
Linguistic motivation: Constraint linear order of constituents. 
Mother (Mother-node): The pos of mother-node assigns to all daughter nodes. 
Example:S(NP
-S(Head:Nh:?)|S?(Head:VF:?|S?(NP-S(Head:Nb:??)|VP-S(Head:VC:
?| NP
-VP(Head:Na: ? ))))) 
Linguistic motivation: Constraint syntactic structures for daughter nodes. 
Head0/1 (Existence of phrasal head): If phrasal head exists in intermediate node, the 
nodes will be marked with feature 1; otherwise 0. 
Example:S(NP(Head:Nh:? )|S?
-1(Head:VF:? |S?-0(NP(Head:Nb:?? )|VP(Head:VC:
?| NP(Head:Na: ? ))))) 
Linguistic motivation: Enforce unique phrasal head in each phrase. 
Table 2. Performance evaluations for different features 
(a)Binary rules without features (b)Binary+Left 
 Sinica Snorama Textbook Sinica Sinorama Textbook 
RC-Type 95.632 94.026 94.479 95.074 93.823 94.464 
RC-Token 99.422 99.139 99.417 99.012 98.756 99.179 
LP 81.51 77.45 84.42 86.27 80.28 86.67 
LR 82.73 77.03 85.09 86.18 80.00 87.23 
LF 82.11 77.24 84.75 86.22 80.14 86.94 
BP 87.73 85.31 89.66 90.43 86.71 90.84 
BR 89.16 84.91 90.52 90.46 86.41 91.57 
BF 88.44 85.11 90.09 90.45 86.56 91.20 
(c)Binary+Head (d)Binary+Mother 
 Sinica Snorama Textbook Sinica Sinorama Textbook 
RC-Type 94.595 93.474 94.480 94.737 94.082 92.985 
RC-Token 98.919 98.740 99.215 98.919 98.628 98.857 
LP 83.68 77.96 85.52 81.87 78.00 83.77 
LR 83.75 77.83 86.10 82.83 76.95 84.58 
LF 83.71 77.90 85.81 82.35 77.47 84.17 
BP 89.49 85.29 90.17 87.85 85.44 88.47 
BR 89.59 85.15 90.91 88.84 84.66 89.57 
BF 89.54 85.22 90.54 88.34 85.05 89.01 
 Linguistically-Motivated Grammar Extraction, Generalization and Adaptation 183 
Each set of feature constraint added grammar is tested and evaluated. Table 2 
shows the experimental results. Since all features have their own linguistic motiva-
tions, the result feature constrained grammars maintain high coverage and have im-
proving grammar precision. Therefore each feature more or less improves the parsing 
performance and the feature of leftmost daughter node, which constrains the linear 
order of constituents, is the most effective feature. The Left-constraint-added gram-
mar reduces grammar token-coverage very little and significantly increases label and 
bracket f-scores. 
It is shown that all linguistically-motivated features are more or less effective. The 
leftmost constitute feature, which constraints linear order of constituents, is the most 
effective feature. The mother-node feature is the least effective feature, since syntactic 
structures do not vary too much for each phrase type while playing different gram-
matical functions in Chinese. 
Table 3. Performances of grammars with different feature combinations 
(a) Binary+Left+Head1/0 (b) Binary+Left+Head 
 Sinica Sinorama Textbook Sinica Sinorama Textbook 
RC-Type 94.887 93.745 94.381 92.879 91.853 92.324 
RC-Token 98.975 98.740 99.167 98.173 98.022 98.608 
LF 86.54 79.81 87.68 86.00 79.53 86.86 
BF 90.69 86.16 91.39 90.10 86.06 90.91 
LF-1 86.71 79.98 87.73 86.76 79.86 87.16 
BF-1 90.86 86.34 91.45 90.89 86.42 91.22 
Table 4. Performances of the grammar with most feature constraints 
Binary+Left+Head+Mother+Head1/0  
Sinica Sinorama Textbook 
RC-Type 90.709 90.460 90.538 
RC-Token 96.906 96.698 97.643 
LF 86.75 78.38 86.19 
BF 90.54 85.20 90.07 
LF-1 88.56 79.55 87.84 
BF-1 92.44 86.46 91.80 
Since all the above features are effective, we like to see the results of multi-feature 
combinations. Many different feature combinations were tested. The experimental 
results show that none of the feature combinations outperform the binary grammars 
with Left and Head1/0 features, even the grammar combining all features, as shown in 
the Table 3 and 4. Here LF-1 and BF-1 measure the label and bracket f-scores only on 
the sentences with parsing results (i.e. sentences failed of producing parsing results 
are ignored). The results show that grammar with all feature constraints has better LF-
1 and BF-1 scores, since the grammar has higher precision. However the total per-
formances, i.e. Lf and BF scores, are not better than the simpler grammar with feature 
184 Y.-M. Hsieh, D.-C. Yang, and K.-J. Chen 
constraints of Left and Head1/0, since the higher precision grammar losses slight edge 
on the grammar coverage. The result clearly shows that tradeoffs do exist between 
grammar precision and coverage. It also suggests that if a feature constraint can im-
prove grammar precision a lot but also reduce grammar coverage a lot, it is better to 
treat such feature constraints as a soft constraint instead of hard constraint. Probabilis-
tic preference for such feature parameters will be a possible implementation of soft 
constraint.  
3.3   Discussions 
Feature constraints impose additional constraints between constituents for phrase 
structures. However different feature constraints serve for different functions and 
have different feature assignment principles. Some features serve for local constraints, 
such as Left, Head, and Head0/1. Those features are only assigned at local intermedi-
ate nodes. Some features are designed for external effect such as Mother Feature, 
which is assigned to phrase nodes and their daughter intermediate nodes. For in-
stances, NP structures for subject usually are different from NP structures for object 
in English sentences [10]. NP attached with Mother-feature can make the difference. 
NPS rules and NPVP rules will be derived each respectively from subject NP and ob-
ject NP structures. However such difference seems not very significant in Chinese. 
Therefore feature selection and assignment should be linguistically-motivated as 
shown in our experiments. 
In conclusion, linguistically-motivated features have better effects on parsing per-
formances than arbitrarily selected features, since they increase grammar precision, 
but only reduce grammar coverage slightly. The feature of leftmost daughter, which 
constraints linear order of constituents, is the most effective feature for parsing. Other 
sub-categorization related features, such as mother node and head features, do not 
contribute parsing F-scores very much. Such features might be useful for purpose of 
sentence generation instead of parsing. 
4   Adapt to Pos Errors Due to Automatic Pos Tagging 
Perfect testing data was used for the above experiments without considering word 
segmentation and pos tagging errors. However in real life word segmentation and pos 
tagging errors will degenerate parsing performances. The real parsing performances 
of accepting input from automatic word segmentation and pos tagging system are 
shown in the Table 5. 
Table 5. Parsing performances of inputs produced by the automatic word segmentation and  
pos tagging 
Binary+Left+Head1/0  
Sinica Sinorama Textbook 
LF 76.18 64.53 73.61 
BF 84.01 75.95 84.28 
 Linguistically-Motivated Grammar Extraction, Generalization and Adaptation 185 
The na?ve approach to overcome the pos tagging errors was to delay some of the 
ambiguous pos resolution for words with lower confidence tagging scores and leave 
parser to resolve the ambiguous pos until parsing stage. The tagging confidence of 
each word is measured by the following value. 
Confidence value= 
)c(P)c(P
)c(P
w,2w,1
w,1
+
, where P(c1,w) and P(c2,w) are probabilities  
assigned by the tagging model for the best candidate c1,w and the second best candi-
date c2,w. 
The experimental results, Table 6, show that delaying ambiguous pos resolution 
does not improve parsing performances, since pos ambiguities increase structure am-
biguities and the parser is not robust enough to select the best tagging sequence.  The 
higher confidence values mean that more words with lower confidence tagging will 
leave ambiguous pos tags and the results show the worse performances. Charniak et al
[3] experimented with using multiple tags per word as input to a treebank parser, and 
came to a similar conclusion. 
Table 6. Parsing performances for different confidence level of pos ambiguities 
Confidence value=0.5  
Sinica Sinorama Textbook 
LF 75.92 64.14 74.66 
BF 83.48 75.22 83.65 
Confidence value=0.8  
Sinica Sinorama Textbook 
LF 75.37 63.17 73.76 
BF 83.32 74.50 83.33 
Confidence value=1.0  
Sinica Sinorama Textbook 
LF 74.12 61.25 69.44 
BF 82.57 73.17 81.17 
4.1   Blending Grammars 
A new approach of grammar blending method was proposed to cope with pos tagging 
errors. The idea is to blend the original grammar with a newly extracted grammar 
derived from the Treebank in which pos categories are tagged by the automatic pos 
tagger. The blended grammars contain the original rules and the extended rules due to 
pos tagging errors. A 5-fold cross-validation was applied on the testing data to tune 
the blending weight between the original grammar and the error-adapted grammar. 
The experimental results show that the blended grammar of weights 8:2 between the 
original grammar and error-adapted grammar achieves the best results. It reduces 
about 20%~30% parsing errors due to pos tagging errors, shown in the Table 7. The 
pure error-adapted grammar, i.e. 0:10 blending weight, does not improve the parsing 
performance very much 
186 Y.-M. Hsieh, D.-C. Yang, and K.-J. Chen 
Table 7. Performances of the blended grammars 
Error-adapted grammar i.e. 
blending weight (0:10) 
Blending weight 8:2  
Sinica Sinirama Textbook Sinica Sinirama Textbook 
LF 75.99 66.16 71.92 78.04 66.49 74.69 
BF 85.65 77.89 85.04 86.06 77.82 85.91 
5   Conclusion and Future Researches 
In order to obtain a high precision and high coverage grammar, we proposed a model 
to measure grammar coverage and designed a PCFG parser to measure efficiency of 
the grammar. Grammar binarization method was proposed to generalize rules and to 
increase the coverage of context-free grammars. Linguistically-motivated feature 
constraints were added into grammar rules to maintain grammar rule precision. It is 
shown that the feature of leftmost daughter, which constraints linear order of constitu-
ents, is the most effective feature. Other sub-categorization related features, such as 
mother node and head features, do not contribute parsing F-scores very much. Such 
features might be very useful for purpose of sentence generation instead of parsing. 
The best performed feature constraint binarized grammar increases the grammar cov-
erage of the original grammar from 93% to 99% and bracketing F-score from 87% to 
91% in parsing moderate hard testing data. To cope with error propagations due to 
word segmentation and part-of-speech tagging errors, a grammar blending method 
was proposed to adapt to such errors. The blended grammar can reduce about 20~30% 
of parsing errors due to error assignment of a pos tagging system.  
In the future, we will study more effective way to resolve structure ambiguities. In 
particular, consider the tradeoff effect between grammar coverage and precision. The 
balance between soft constraints and hard constraints will be focus of our future re-
searches. In addition to rule probability, word association probability will be another 
preference measure to resolve structure ambiguity, in particular for conjunctive  
structures.  
Acknowledgement 
This research was supported in part by National Science Council under a Center Ex-
cellence Grant NSC 93-2752-E-001-001-PAE and National Digital Archives Program 
Grant NSC93-2422-H-001-0004.  
References 
1. E. Charniak, and G. Carroll, ?Context-sensitive statistics for improved grammatical lan-
guage models.? In Proceedings of the 12th National Conference on Artificial Intelligence, 
AAAI Press, pp. 742-747, Seattle, WA, 1994, 
2. E. Charniak, ?Treebank grammars.? In Proceedings of the Thirteenth National Conference 
on Artificial Intelligence, pp. 1031-1036. AAAI Press/MIT Press, 1996. 
 Linguistically-Motivated Grammar Extraction, Generalization and Adaptation 187 
3. E. Charniak, and G. Carroll, J. Adcock, A. Cassanda, Y. Gotoh, J. Katz, M. Littman, J. 
Mccann, "Taggers for Parsers", Artificial Intelligence, vol. 85, num. 1-2, 1996. 
4. Feng-Yi Chen, Pi-Fang Tsai, Keh-Jiann Chen, and Huang, Chu-Ren, ?Sinica Treebank.? 
Computational Linguistics and Chinese Language Processing, 4(2):87-103, 2000. 
5. Keh-Jiann Chen and, Yu-Ming Hsieh, ?Chinese Treebanks and Grammar Extraction.? the 
First International Joint Conference on Natural Language Processing (IJCNLP-04), March 
2004. 
6. Michael Collins, ?Head-Driven Statistical Models for Natural Language parsing.? Ph.D. 
thesis, Univ. of Pennsylvania, 1999. 
7. Yu-Ming Hsieh, Duen-Chi Yang and Keh-Jiann Chen, ?Grammar extraction, generaliza-
tion and specialization. ( in Chinese)?Proceedings of ROCLING 2004. 
8. Christopher D. Manning and Hinrich Schutze, ?Foundations of Statistical Natural Lan-
guage Processing.? the MIT Press, Cambridge, Massachusetts, 1999. 
9. Mark Johnson, ?PCFG models of linguistic tree representations.? Computational Linguis-
tics, Vol.24, pp.613-632, 1998. 
10. Dan Klein and Christopher D. Manning, ?Accurate Unlexicalized Parsing.? Proceeding of 
the 4lst Annual Meeting of the Association for Computational Linguistics, pp. 423-430, 
July 2003. 
11. Honglin Sun and Daniel Jurafsky, ?Shallow Semantic Parsing of Chinese.? Proceedings of 
NAACL 2004. 
12. 12.Hao Zhang, Qun Liu, Kevin Zhang, Gang Zou and Shuo Bai, ?Statistical Chinese 
Parser ICTPROP.? Technology Report, Institute of Computing Technology, 2003. 
 
Chinese Sketch Engine and 
the Extraction of Grammatical Collocations
Chu-Ren Huang  
Inst. of Linguistics  
Academia Sinica  
churen@sinica.edu.tw
Adam Kilgarriff 
Lexicography MasterClass 
Information Technology  
adam@lexmasterclass.com 
Yiching Wu 
Inst. of Linguistics 
Tsing Hua University 
d898702@oz.nthu.edu.tw 
Chih-Ming Chiu 
Inst. of Information Science 
Academia Sinica 
henning@hp.iis.sinica.edu.tw 
Simon Smith 
Dept. of Applied English 
Ming Chuan University 
ssmith@mcu.edu.tw 
Pavel Rychly 
Faculty of Informatics 
Masaryk University. 
pary@textforge.cz 
Ming-Hong Bai 
Inst. of Information Science 
Academia Sinica 
mhbai@sinica.edu.tw 
Keh-Jiann Chen 
Inst. of Information Science 
Academia Sinica 
kchen@iis.sinica.edu.tw
Abstract. This paper introduces a new 
technology for collocation extraction in Chinese. 
Sketch Engine (Kilgarriff et al, 2004) has 
proven to be a very effective tool for automatic 
description of lexical information, including 
collocation extraction, based on large-scale 
corpus. The original work of Sketch Engine was 
based on BNC. We extend Sketch Engine to 
Chinese based on Gigaword corpus from LDC. 
We discuss the available functions of the 
prototype Chinese Sketch Engine (CSE) as well 
as the robustness of language-independent 
adaptation of Sketch Engine. We conclude by 
discussing how Chinese-specific linguistic 
information can be incorporated to improve the 
CSE prototype.  
1. Introduction 
The accessibility to large scale corpora, at 
one billion words or above, has become both a 
blessing and a challenge for NLP research. How 
to efficiently use a gargantuan corpus is an 
urgent issue concerned by both users and corpora 
designers. Adam Kilgarriff et al (2004) 
developed the Sketch Engine to facilitate 
efficient use of corpora. Their claims are two 
folded: that genuine linguistic generalizations 
can be automatically extracted from a corpus 
with simple collocation information provided 
that the corpus is large enough; and that such a 
methodology is easily adaptable for a new 
language. The first claim was fully substantiated 
with their work on BNC. The current paper deals 
with the second claim by adapting the Sketch 
Engine to Chinese.  
2. Online Chinese Corpora: The State of 
the Arts 
2.1 Chinese Corpora 
The first online tagged Chinese corpus is 
Academia Sinica Balanced Corpus of Modern 
Chinese (Sinica Corpus), which has been 
web-accessible since November, 1996. The 
current version contains 5.2028 million words 
(7.8927 million characters). The corpus data was 
collected between 1990 and 1996 (CKIP, 
1995/1998). Two additional Chinese corpora 
were made available on line in 2003. The first is 
the Sinorama Chinese-English Parallel Text 
Corpus (Sinorama Corpus). The Sinorama 
Corpus is composed of 2,373 parallel texts in 
both Chinese and English that were published 
between 1976 and 2000. There are 103,252 pairs 
of sentences, composed of roughly 3.2 million 
48
English words and 5.3 million Chinese 
characters 1 . The second one is the modern 
Chinese corpus developed by the Center for 
Chinese Linguistics (CCL Corpus) at Peking 
University. It contains eighty-five million 
(85,398,433) simplified Chinese characters 
which were published after 1919 A.D. 
2.2 Extracting Linguistic Information from 
Online Chinese Corpora: Tools and Interfaces 
The Chinese corpora discussed above are 
all equipped with an online interface to allow 
users to extract linguistic generalizations. Both 
Sinica Corpus and CCL Corpus offer 
KWIC-based functions, while Sinorama Corpus 
gives sentence and paragraph aligned output. 
2.2.1 String Matching or Word Matching 
The basic unit of query that a corpus allows 
defines the set of information that can be 
extracted from that corpus. While there is no 
doubt that segmented corpus allows more precise 
linguistic generalizations, string-based 
collocation still afford a corpus of the robustness 
that is not restricted by an arbitrary word-list or 
segmentation algorithm. This robustness is of 
greatest value when extracting neologism or 
sub-lexical collocations. Since CCL Corpus is 
not segmented and tagged, string-based KWIC is 
its main tool for extracting generalizations. This 
comes with the familiar pitfall of word boundary 
ambiguity. For instance, a query of ci.yao ??
?secondary? may yield the intended result (la), as 
well as noise (1b). 
1a. ??????
dan zhe shi ci.yao de 
but this is secondary DE
1http://cio.nist.gov/esd/emaildir/lists/mt_list/msg0003
3.html 
?But this is secondary? 
 b. ????????!
ta ji ci yao.qiu ta da.fu 
he several time ask her answer 
?He had asked her to answer for several times? 
 Sinica Corpus, on the other hand, is fully 
segmented and allows word-based 
generalizations. In addition, Sinica Corpus also 
allows wildcards in its search. Users specify a 
wildcard of arbitrary length (*), or fixed length 
(?). This allows search of a class of words 
sharing some character strings.
2.2.2 Display of Extracted Data 
Formal restriction on the display of 
extracted data also constraints the type of 
information that can be obtained from that 
corpus. Sinica Corpus allows users to change 
window size from about 25 to 57 Chinese 
characters. However, since a Chinese sentence 
may be longer than 57 characters, Sinica Corpus 
cannot guarantee that a full sentence is displayed. 
CCL Corpus, on the other hand, is able to show a 
full output sentence, which may be up to 200 
Chinese characters. However, it does not display 
more than a full sentence. Thus it cannot show 
discourse information. Sinorama Corpus with 
TOTALrecall interface is most versatile in this 
respect. Aligned bilingual full sentences are 
shown with an easy link to the full text. 
In terms of size and completeness of 
extracted data, Sinica Corpus returns all matched 
examples. However, cut and paste must be 
performed for the user to build his/her dataset. 
CCL Corpus, on the other hand, limits data to 
500 lines per page, but allows easy download of 
output data. Lastly, Sinorama/TOTALrecall 
provides choices of 5 to 100 sentences per page. 
49
2.2.3 Refining Extracted Information: Filter 
and Sorter 
Both Sinica Corpus and CCL corpus allows 
users to process extracted information, using 
linguistic and contextual filter or sorter. The CCL 
corpus requires users to remember the rules, 
while Sinica Corpus allows users to fill in blanks 
and/or choose from pull-down menu. In 
particular, Sinica Corpus allows users to refine 
their generalization by quantitatively 
characterizing the left and right contexts. The 
quantitative sorting functions allowed include 
both word and POS frequency, as well as word 
mutual information.  
2.2.4 Extracting Grammatical Information 
Availability of grammatical information 
depends on corpus annotation. CCL and 
Sinorama Corpus do not have POS tags. Sinica 
Corpus is the only Chinese corpus allowing users 
to access an overview of a keyword?s syntactic 
behavior. Users can obtain a list of types and 
distribution of the keyword?s syntactic category. 
In addition, users can find possible collocations 
of the keyword from the output of Mutual 
Information (MI).  
The most salient grammatical information, 
such as grammatical functions (subject, object, 
adjunct etc.) is beyond the scope of the 
traditional corpus interface tools. Traditional 
corpora rely on the human users to arrive at these 
kinds of generalizations.
3. Sketch Engine: A New Corpus-based 
approach to Grammatical Information  
Several existing linguistically annotated 
corpus of Chinese, e.g. Penn Chinese Tree Bank 
(Xia et al, 2000), Sinica Treebank (Chen et al, 
2003), Proposition Bank (Xue and Palmer, 2003, 
2005) and Mandarin VerbNet (Wu and Liu, 
2003), suffer from the same problem. They are 
all extremely labor-intensive to build and 
typically have a narrow coverage. In addition, 
since structural assignment is theory-dependent 
and abstract, inter-annotator consistency is 
difficult to achieve. Since there is also no general 
consensus on the annotation scheme in Chinese 
NLP and linguistics, building an effective 
interface for public use is almost impossible. 
The Sketch Engine offers an answer to the 
above issues.
3.1 Initial Implementation and Design of the 
Sketch Engine 
The Sketch Engine is a corpus processing 
system developed in 2002 (Kilgarriff and 
Tugwell, 2002; Kilgarriff et al, 2004). The main 
components of the Sketch Engine are KWIC 
concordances, word sketches, grammatical 
relations, and a distributional thesaurus. In its 
first implementation, it takes as input basic BNC 
(British National Corpus, (Leech, 1992)) data: 
the annotated corpus, as well as list of lemmas 
with frequencies. In other words, the Sketch 
Engine has a relatively low threshold for the 
complexity of input corpus. 
The Sketch Engine has a versatile query 
system. Users can restrict their query in any 
sub-corpus of BNC. A query string may be a 
word (with or without POS specification), or a 
phrasal segment. A query can also be performed 
using Corpus Query Language (CQL). The 
output display format can be adjusted, and the 
displayed window of a specific item can be 
freely expanded left and right. Most of all, the 
Sketch Engine produces a Word Sketch 
(Kilgarriff and Tugwell, 2002) that is an 
automatically generated grammatical description 
of a lemma in terms of corpus collocations. All 
items in each collocation are linked back to the 
original corpus data. Hence it is similar to a 
50
Linguistic Knowledge Net anchored by a lexicon 
(Huang et al, 2001). 
 A Word Sketch is a one-page list of a 
keyword?s functional distribution and collocation 
in the corpus. The functional distribution 
includes: subject, object, prepositional object, 
and modifier. Its collocations are described by a 
list of linguistically significant patterns in the 
language. Word Sketch uses regular expressions 
over POS-tags to formalize rules of collocation 
patterns, e.g. (2) is used to retrieve the 
verb-object relation in English:
2. 1:?V? ?(DET|NUM|ADJ|ADV|N)?* 2:?N? 
The expression in (2) says: extract the data 
containing a verb followed by a noun regardless 
of how many determiners, numerals, adjectives, 
adverbs and nouns preceding the noun. It can 
extract data containing cook meals and cooking a 
five-course gala dinner, and cooked the/his/two 
surprisingly good meals etc.
The Sketch Engine also produces thesaurus 
lists, for an adjective, a noun or a verb, the other 
words most similar to it in their use in the 
language (Kilgarriff et al 2004). For instance, 
the top five synonym candidates for the verb kill
are shoot (0.249), murder (0.23), injure (0.229), 
attack (0.223), and die (0.212).2 It also provides 
direct links to the Sketch Difference which lists 
the similar and different patterns between a 
keyword and its similar word. For example, both 
kill and murder can occur with objects such as 
people and wife, but murder usually occurs with 
personal proper names and seldom selects animal 
nouns as complement whereas kill can take fox,
whale, dolphin, and guerrilla, etc. as its object. 
 The Sketch Engine adopts Mutual 
2 The similarity is measured and ranked adopting 
Lin?s (1998) mathematics. 
Information (MI) to measure the salience of a 
collocation. Salience data are shown against each 
collocation in Word Sketches and other Sketch 
Engine output. MI provides a measure of the 
degree of association of a given segment with 
others. Pointwise MI, calculated by Equation 3, 
is what is used in lexical processing to return the 
degree of association of two words x and y (a 
collocation).
3. 
)(
)|(log);(
xP
yxPyxI  
3.2 Application to Chinese Corpus 
In order to show the cross-lingual 
robustness of the Sketch Engine as well as to 
propose a powerful tool for collocation 
extraction based on a large scale corpus with 
minimal pre-processing; we constructed Chinese 
Sketch Engine (CSE) by loading the Chinese 
Gigaword to the Sketch Engine (Kilgarriff et al, 
2005). The Chinese Gigaword contains about 
1.12 billion Chinese characters, including 735 
million characters from Taiwan?s Central News 
Agency, and 380 million characters from China?s 
Xinhua News Agency3. Before loading Chinese 
Gigaword into Sketch Engine, all the simplified 
characters were converted into traditional 
characters, and the texts were segmented and 
POS tagged using the Academia Sinica 
segmentation and tagging system (Huang et al, 
1997). An array of machine was used to process 
the 1.12 million characters, which took over 3 
days to perform. All components of the Sketch 
Engine were implemented, including 
Concordance, Word Sketch, Thesaurus and 
Sketch Difference.  
 In our initial in-house testing of this 
prototype of the Chinese Sketch Engine, it does 
3http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2003T09 
51
produce the expected results with an easy to use 
interface. For instance, the Chinese Word Sketch 
correctly shows that the most common and 
salient object of dai.bu ??  ?to arrest? is
xian.fan ??  ?suspect?; the most common 
subject jing.fang ??!?police?; and the most 
common modifier dang.chang??.
 The output data of Thesaurus correctly 
verify the following set of synonyms from the 
Chinese VerbNet Project: that ren.wei ???to
think? behaves most like biao.shi ??  ?to 
express, to state? (salience 0.451), while yi.wei?
? ?to take somebody/something as? is more like 
jue.de?? ?to feel, think? (salience 0.488). The 
synonymous relation can be illustrated by (4) and 
(5).
4a. ????????????????????
?????????????????
ta ren.wei dao hai.wai tou.zi you yi ge guan.nian 
hen zhong.yao, jiu shi yao zhi.dao dang.di de 
you.xi gui.ze 
?He believes that for those investing overseas, 
there is a very important principle-one must know 
the local rules of the game, and accept them.? 
 b. ????????????????????
?????!
zhi.zheng.dang ye biao.shi, you.yu gong.shi 
zheng.yi tai da, kong.pa wu.fa quan.li zhi.chi 
?The KMT also commented that due to the many 
controversies surrounding PTV, it could not 
wholeheartedly support it either.? 
5a. ?????????????????????
????????
he.jia.ju jiu ren.wei??dian.shi you ji.ben yu.yan 
he wen.fa, yao jiang.jiu mai.dian he shi.chang??
?Ho Chia-chu says, "Television has its own 
fundamental language and grammar. You must 
consider selling points and the market."? 
b. ?????????????????????
???????????
ta biao.shi??wo xi.wang fuo.jiao.tu neng liao.jie, 
fu.quan she.hui yu jue.wu de she.hui shi bu 
xiang.he de??
?She says "I hope that followers of Buddhism can 
realize that a patriarchal society is incompatible 
with an enlightened society."? 
The above examples show that ren.wei and 
biao.shi can take both direct and indirect 
quotation. Yi.wei and jue.de, on the other hand, 
can only be used in reportage and cannot 
introduce direct quotation. 
Distinction between near synonymous pairs 
can be obtained from Sketch Difference. This 
function is verified with results from Tsai et al?s 
study on gao.xing?? ?glad? and kuai.le??!
?happy? (Tsai et al, 1998). Gao.xing ?glad? 
specific patterns include the negative imperative 
bie? ?don?t?. It also has a dominant collocation 
with the potentiality complement marker de?
(e.g. ta gao.xing de you jiao you tiao ????
???? ?she was so happy that she cried and 
danced?). In contrast, kuai.le ?happy? has the 
specific collocation with holiday nouns such as 
qiu.jie ??  ?Autumn Festival?. The Sketch 
Difference result is consistent with the account 
that gao.xing/kuai.le contrast is that inchoative 
state vs. homogeneous state. 
4. Evaluation and Future Developments 
An important feature of the prototype of the 
Chinese Sketch Engine is that, in order to test the 
robustness of the Sketch Engine design, the 
original regular expression patterns were adopted 
with minimal modification for Chinese. Even 
though both are SVO languages with similar 
surface word order, it is obvious that they differ 
substantially in terms of assignment of 
grammatical functions. In addition, the Sinica 
tagset is different from the BNC tagset and 
52
actually has much richer functional information. 
These are the two main directions that we will 
pursue in modification and improvement of the 
Chinese Sketch Engine. 
4.1 Word Boundary Representation 
Word breaks are not conventionalized in 
Chinese texts. This poses a challenge in Chinese 
language processing. The Chinese Sketch Engine 
inserted space after segmentation, which helps to 
visualize words. In the future, it will be trivial to 
allow the conventional alternative of no word 
boundary markups. However, it will not be trivial 
to implement fuzzy function to allow searches 
for non-canonical lemmas (i.e. lemmas that are 
segmented differently from the standard corpus). 
4.2 Sub-Corpora Comparison  
The Chinese Gigaword corpus is marked 
with two different genres, story and non-story. A 
still more salient sub-corpus demarcation is the 
one between Mainland China corpus and Taiwan 
corpus. Sketch Difference between lemmas form 
two sub-corpora is being planned. This would 
allow future comparative studies and would have 
wide applications in the localization adaptations 
of language related applications.  
4.3 Collating Frequency Information with 
POS
One of the convenient features of Sketch 
Engine that a frequency ranked word list is 
linked to all major components. This allows a 
very easy and informative reference. Since 
cross-categorical derivation with zero 
morphology is dominant in Chinese, it would 
help the processing greatly if POS information is 
added to the word list. Adding such information 
would also open the possibility of accessing the 
POS ranked frequency information. 
4.5 Fine-tuning Collocation Patterns  
The Sketch Engine relies on collocation 
patterns, such as (2) above, to extract 
collocations. The regular expression format 
allows fast processing of large scale corpora with 
good results. However, these patterns can be 
fine-tuned for better results. We give VN 
collocates with object function as example here. 
In (6), verbs are underlined with a single line, 
and the collocated nouns identified by English 
Word Sketch are underlined with double lines. 
Other nominal objects that the Sketch Engine 
misses are marked with a dotted line. 
6.a. In addition to encouraging kids to ask, think and 
do, parents need to be tolerant and appreciative to 
avoid killing a child's creative sense.
b. Children are taught to love their parents,
classmates, animals, nature . . . . in fact they are 
taught to love just about everything except to 
love China, their mother country. 
c. For example, the government deliberately chose 
not to teach Chinese history and culture, nor 
civics, in the schools. 
d. At the game there will be a lottery drawing for a 
motorcycle! And perhaps you'll catch a foul ball
or a home run.
The sentences in (6) show that the current Sketch 
Engine tend to only identify the first object when 
there are multiple objects. The resultant 
distributional information thus obtained will be 
valid given a sufficiently large corpus. However, 
if the collocation patterns are fine-tuned to allow 
treatment of coordination, richer and more 
precise information can be extracted. 
 A regular expression collocation pattern 
also runs the risk of mis-classification. For 
instance, speech act verbs often allow subject to 
occur in post-verbal positions, and intransitive 
53
verbs can often take temporal nouns in 
post-verbal positions too.  
7. a. ?you can say goodbye to your competitive 
career. 
b. `No,' said Scarlet, `but then I don't notice much.'
8. a. Where did you sleep last night?
  b. ?it arrived Thursday morning.
  c. From Arty's room came the sound of an 
accordion.
9. `I'll look forward to that.' `So will I.' 
Such non-canonical word orders are even more 
prevalent in Chinese. Chinese objects often occur 
in pre-verbal positions in various pre-posing 
constructions, such as topicalization. 
10. ???????????
quan.gu mian.bao, chi le hen jian.kang 
whole-grain bread, eat LE very healthy 
?Eating whole-grain bread is very healthy.? 
11a. ??????????????????
you ren chang.shi yao jiang zhe he.hua fen.lei,
que yue fen yue lei 
someone try to JIANG the lotus classify, but more 
classify more tired 
?People have tried to decide what category the 
lotus belongs in, but have found the effort 
taxing.? 
b. ??????????
wo yi.ding yao ba lao.da chu.diao
I must want BA the oldest (son) get rid of
?I really want to get rid of the older son.?
When objects are pre-posed, they tend to stay 
closer to the verb than the subject. Adding object 
marking information, such as ba?, jiang?, lian
?  would help correctly identify collocating 
pre-posed objects. However, for those unmarked 
pre-posed structures, closeness to the verb may 
not provide sufficient information. Several rules 
will need to be implemented jointly.  
 The above example underlines a critical 
issue. That is, whether relative position alone is 
enough to identify positional information. The 
Sketch Engine is in essence a powerful tool 
extracting generalizations from annotated corpus 
data. We have shown that it can extract useful 
grammatical information with POS tag alone. If 
the corpus is tagged with richer annotation, the 
Sketch Engine should be able to extract even 
richer information. 
 The Sinica Corpus tagset adapts to the fact 
that Chinese has a freer word order than English 
by incorporating semantic information with the 
grammatical category. For instance, locational 
and temporal nouns, proper nouns, and common 
nouns each are assigned a different tag. Verbs are 
sub-categorized according to activity and 
transitivity. Such information is not available in 
the BNC tagset and hence not used in the 
original Sketch Engine design. We will enrich the 
collocation patterns with the annotated linguistic 
information from the Sinica Corpus tagset. In 
particular, we are converting ICG lexical 
subcategorization frames (Chen and Huang 1990) 
to Sketch Engine collocation patters. These ICG 
frames, called Basic Patterns and Adjunct 
Patterns, have already been fully annotated 
lexically and tested on the Sinica Corpus. We 
expect their incorporation to improve Chinese 
Sketch Engine results markedly. 
6. Conclusion 
In this paper, we introduce a powerful tool 
for extraction of collocation information from 
large scale corpora. Our adaptation proved the 
cross-lingual robustness of the Sketch Engine. In 
particular, we show the robustness of the Sketch 
Engine by achieving better results through 
fine-tuning of the collocation patterns via 
integrating available grammatical knowledge. 
54
References 
Chen, Keh-Jiann and Huang, Chu-Ren. 1990.  
Information-based Case Grammar.  
Proceedings of the 13th COLING. Helsinki, 
Finland. 2:54-59. 
Chen, Keh-Jiann, Chu-Ren Huang, Feng-Yi Chen, 
Chi-Ching Luo, Ming-Chung Chang, and 
Chao-Jan Chen. 2003. Sinica Treebank: 
Design Criteria, Representational Issues and 
Implementation. In Anne Abeill?e, (ed.): 
Building and Using Parsed Corpora. Text, 
Speech and Language Technology,
20:231-248. Dordrecht: Kluwer.  
CKIP (Chinese Knowledge Information Processing 
Group). 1995/1998. The Content and 
Illustration of Academica Sinica Corpus.
(Technical Report no 95-02/98-04). Taipei: 
Academia Sinica  
Huang, Chu-Ren, Feng-Ju Lo, Hui-Jun Hsiao, 
Chiu-Jung Lu, and Ching-chun Hsieh. 2001. 
From Language Archives to Digital 
Museums: Synergizing Linguistic Databases. 
Presented at the IRCS workshop on linguistic 
Databases. University of Pennsylvania. 
Huang, Chu-Ren, Keh-Jiann Chen, and Lili Chang. 
1997. Segmentation Standard for Chinese 
Natural Language Processing. 
Computational Linguistics and Chinese 
Language Processing. 2(2):47-62.  
Kilgarriff, Adam and Tugwell, David. Sketching 
Words. 2002. In Marie-H?l?ne Corr?ard (ed.): 
Lexicography and Natural Language 
Processing. A Festschrift in Honour of B.T.S. 
Atkins. 125-137. Euralex.  
Kilgarriff, Adam, Chu-Ren Huang, Pavel Rychl?, 
Simon Smith, and David Tugwell. 2005. 
Chinese Word Sketches. ASIALEX 2005: 
Words in Asian Cultural Context. Singapore.  
Kilgarriff, Adam, Pavel Rychl?, Pavel Smrz and 
David Tugwell. 2004. The Sketch Engine. 
Proceedings of EURALEX, Lorient, France. 
(http://www.sketchengine.co.uk/) 
Leech, Geoffrey. 1992. 100 million words of 
English: the British National Corpus (BNC). 
Language Research 28(1):1-13 
Lin, Dekang. 1998. An Information-Theoretic 
Definition of Similarity. Proceedings of 
International Conference on Machine 
Learning. Madison, Wisconsin. 
(http://www.cs.umanitoba.ca/~lindek/publica
tion.htm) 
Tsai, Mei-Chih, Chu-Ren Huang, Keh-Jiann Chen, 
and Kathleen Ahrens. 1998. Towards a 
Representation of Verbal Semantics--An 
Approach Based on Near Synonyms. 
Computational Linguistics and Chinese 
Language Processing. 3(1): 61-74. 
Wu, Yiching and Liu, Mei-Chun. 2003. The 
Construction and Application of Mandarin 
Verbnet. Proceedings of the Third 
International Conference of Internet Chinese 
Education. 39-48. Taipei, Taiwan. 
Xia, Fei, Martha Palmer, Nianwen Xue, Mary Ellen 
Okurowski, John Kovarik, Fu-Dong Chiou, 
Shizhe Huang, Tony Kroch, and Mitch 
Marcus. 2000. Developing Guidelines and 
Ensuring Consistency for Chinese Text 
Annotation. Proceedings of the second 
International Conference on Language 
Resources and Evaluation (LREC 2000), 
Athens, Greece.  
    (http://www.cis.upenn.edu/~chinese/ctb.html)
Xue, Nianwen and Palmer, Martha. 2003. 
Annotating Propositions in the Penn Chinese 
Treebank. Proceedings of the Second Sighan 
Workshop. Sapporo, Japan. 
     (http://www.cis.upenn.edu/~xueniwen/) 
Xue, Nianwen and Palmer, Martha. 2005. 
Automatic Semantic Role Labeling for 
Chinese Verbs. Proceedings of the 19th 
International Joint Conference on Artificial 
Intelligence. Edinburgh, Scotland. 
     (http://www.cis.upenn.edu/~xueniwen/) 
Websites
Sinica Corpus.  
http://www.sinica.edu.tw/SinicaCorpus/  
British National Corpus (BNC). 
http://www.natcorp.ox.ac.uk/  
Center for Chinese Linguistics, PKU.  
http://ccl.pku.edu.cn/#  
Corpora And NLP (Natural Language Processing) 
for Digital Learning of English (CANDLE). 
http://candle.cs.nthu.edu.tw/candle/     
FrameNet.  
http://www.icsi.berkeley.edu/~framenet/  
Penn Chinese Treebank. 
http://www.cis.upenn.edu/~chinese/ctb.html  
Proposition Bank.  
http://www.cis.upenn.edu/~ace/  
Sinica Treebank.   
http://treebank.sinica.edu.tw/  
Sketch Engine (English).  
http://www.sketchengine.co.uk/     
Sketch Engine (Chinese).  
http://corpora.fi.muni.cz/chinese/  
Sou Wen Jie Zi-A Linguistic KnowledgeNet. 
http://words.sinica.edu.tw/ 
55
1
2
3
4
5
6
Improving Word Alignment by Adjusting Chinese Word Segmentation
Ming-Hong Bai1,2 Keh-Jiann Chen1 Jason S. Chang2
1 Institute of Information Science, Academia Sinica 
2 Department of Computer Science, National Tsing-Hua University 
mhbai@sinica.edu.tw kchen@iis.sinica.edu.tw jschang@cs.nthu.edu.tw
 
Abstract  
Most of the current Chinese word 
alignment tasks often adopt word 
segmentation systems firstly to identify 
words. However, word-mismatching 
problems exist between languages and will 
degrade the performance of word 
alignment. In this paper, we propose two 
unsupervised methods to adjust word 
segmentation to make the tokens 1-to-1 
mapping as many as possible between the 
corresponding sentences. The first method 
is learning affix rules from a bilingual 
terminology bank. The second method is 
using the concept of impurity measure 
motivated by the decision tree. Our 
experiments showed that both of the 
adjusting methods improve the 
performance of word alignment 
significantly. 
1 Introduction 
Word alignment is an important preprocessing task 
for statistical machine translation. There have been 
many statistical word alignment methods proposed 
since the IBM models have been introduced. Most 
existing methods treat word tokens as basic 
alignment units (Brown et al, 1993; Vogel et al, 
1996; Deng and Byrne, 2005), however, many 
languages have no explicit word boundary markers, 
such as Chinese and Japanese. In these languages, 
word segmentation (Chen and Liu, 1992; Chen and 
Bai, 1998; Chen and Ma, 2002; Ma and Chen, 
2003; Gao et al, 2005) is often carried out firstly 
to identify words before word alignment (Wu and 
Xia, 1994). However, the differences in 
lexicalization may degrade word alignment 
performance, for different languages may realize 
the same concept using different numbers of words 
(Ma et al, 2007; Wu, 1997). For instance, Chinese 
multi-syllabic words composed of more than one 
meaningful morpheme which may be translated to 
several English words. For example, the Chinese 
word ??? is composed of two meaning units, 
?? and ?, and is translated to Department of 
Education in English. The morphemes ?? and ? 
have their own meanings and are translated to 
Education and Department respectively. The 
phenomenon of lexicalization mismatch will 
degrade the performance of word alignment for 
several reasons. The first reason is that it will 
reduce the cooccurrence counts of Chinese and 
English tokens. Consider the previous example. 
Since ??? is treated as a single unit, it does not 
contribute to the occurrence counts of Education/
?? and Department/? token pairs. Secondly, the 
rarely occurring compound word may cause the 
garbage collectors effect (Moore, 2004; Liang et 
al., 2006), aligning a rare word in source language 
to too many words in the target language, due to 
the frequency imbalance with the corresponding 
translation words in English (Lee, 2004). Finally, 
the IBM models (Moore, 2004) impose the 
limitation that each word in the target sentence can 
be generated by at most one word in the source 
sentence. In this case, a many-to-one alignment, 
links a phrase in the source sentence to a single 
token in the target sentence, is not allowed, forcing 
most links of a phrase in the source sentence to be 
abolished. As in the previous example, when 
aligning from English to Chinese, ??? can only 
be linked to one of the English words, say 
Education, because of the limitation of the IBM 
model. However for remedy, many of the current 
word alignment methods combine the results of 
both alignment directions, via intersection or 
249
grow-diag-final heuristic, to improve the alignment 
reliability (Koehn et al, 2003; Liang et al, 2006; 
Ayan et al, 2006; DeNero et al, 2007). However 
the many-to-one link limitation will undermine the 
reliability due to the fact that some links are not 
allowed in one of the directions. 
In this paper, we propose two novel methods to 
adjust word segmentation so as to decrease the 
effect of lexicalization differences to improve word 
alignment performance. The main idea of our 
methods is to adjust Chinese word segmentation 
according to their translation derived from parallel 
sentences in order to make the tokens compatible 
to 1-to-1 mapping between the corresponding 
sentences. The first method is based on learning a 
set of affix rules from bilingual terminology bank, 
and adjusting the segmentation according to these 
affix rules when preprocessing the Chinese part of 
the parallel corpus. The second method is based on 
the so-called impurity measure, which was 
motivated by the decision tree (Duda et al, 2001). 
 
2 Related Works 
Our methods are motivated by the translation-
driven segmentation method proposed by Wu 
(1997) to segment words in a way to improve word 
alignment. However, Wu's method needs a 
translation lexicon to filter out the links which 
were not in the lexicon and the result was only 
evaluated on the sentence pairs which were 
covered by the lexicon.  
A word packing method has been proposed by 
Ma et al (2007) to improve the word alignment 
task. Before carrying out word alignment, this 
method packs several consecutive words together 
when those words believed to correspond to a 
single word in the other language. Our basic idea is 
similar to this, but on the contrary, we try to 
unpack words which are translations of several 
words in the other language. Since the word 
packing method treats the packed consecutive 
words as a single token, as we mentioned in the 
previous section, it weakens the association 
strength of translation pairs of their morphemes 
while applying the IBM word alignment model. 
A lot of morphological analysis methods have 
been proposed to improve the performance of word 
alignment for inflectional language (Lee et al, 
2003; Lee, 2004; Goldwater, 2005). They proposed 
to split a word into a morpheme sequence of the 
pattern prefix*-stem-suffix* (* denotes zero or 
more occurrences of a morpheme). Their 
experiments showed that morphological analysis 
can improve the quality of machine translation by 
reducing data sparseness and by making the tokens 
in two languages correspond more 1-to-1. 
However, these segmentation methods were 
developed from the monolingual perspective. 
3 Adjusting Word Segmentation 
The goal of word segmentation adjustment is to 
adjust the segmentation of Chinese words such that 
we have as many 1-to-1 links to the English words 
as possible. In this task, we will face the problem 
of finding the proper morpheme boundaries for 
Chinese words. The challenge is that almost all 
characters of Chinese are morphemes and therefore 
almost every character boundary in a word could 
be the boundary of a morpheme, there is no simple 
rules to find the suitable boundaries of morphemes. 
Furthermore, not all meaningful morphemes need 
to be segmented to meet the requirement of 1-to-1 
mapping. For example, washing machine/???
can be segmented into ?? and ? corresponding 
to washing and machine while heater/??? does 
not need, it depends on their translations.  
In this paper, we have proposed two different 
methods to solve this problem: 1. learning affix 
rules from terminology bank to segment 
morphemes and 2. using impurity measure to 
finding the morpheme boundaries. The detail of 
these methods will be described in the following 
sections. 
4 Affix Rule Method 
The main idea of this method is to segment a 
Chinese word according to some properly designed 
conditional dependent affix rules. As shown in 
Figure 1, each rule is composed of three 
conditional constraints, a) affix condition, b) 
English word condition and c) exception condition. 
In the affix condition, we place a underscore on the 
left of a morpheme, such as _?, to denote a suffix 
and on the right, such as ?_, to denote a prefix. 
The affix rules are applied to each word by 
checking the following three conditions:  
1. The target word has the affix. 
250
2. The English word which is the target of 
translation exists in the parallel sentence. 
3. The target word does not contain the 
morphemes in the exception list (The 
morpheme in the exception list shows an 
alternative segmentation.). 
 
If the target word satisfies all of the above 
conditions of any rule, then the morpheme should 
be separated from the word. The remaining 
problem will be how to derive the set of affix rules. 
 
affix English word exception
_? machine  
_? engine  
?_ vice  
?_ deputy ?? 
_? industry ?? 
Figure 1.  Samples of affix rules. 
 
4.1 Training Data 
We use an unsupervised method to extract affix 
rules from a Chinese-English terminology bank1. 
The bilingual terminology bank a total of 
1,046,058 English terms with Chinese transla-
tions in 63 categories. Among them, 60% or 
629,352 terms are compounds. We take the 
advantage of the terminology bank, that all 
terminologies are 1-to-1 well translated, to find the 
best morpheme segmentation from ambiguous 
segmentations of a Chinese word according to its 
English counterpart. Then we extracted affix rules 
from the word-to-morpheme alignment results of 
terms and translation.  
 
4.2 Word-to-Morpheme Alignment 
The training phase of word-to-morpheme 
alignment is based loosely on word-to-word 
alignment of the IBM model 1. Instead of using 
Chinese words, we considered all the possible 
morphemes. For example, consider the task of 
aligning Department of Education and ??? as 
                                                 
1 The bilingual terminology bank was compiled by the Na-
tional Institute for Compilation and Translation. It is freely 
download at http://terms.nict.gov.tw by registering your in-
formation. 
shown as Figure 2. We use the EM algorithm to 
train the translation probabilities of word-
morpheme pairs based on IBM model 1.  
 
 
Figure 2. Example of word-to-morpheme 
alignment. 
 
In the aligning phase, the original IBM model 1 
does not work properly as we expected. Because 
the English words prefer to link to single character 
and it results that some correct Chinese translations 
will not be linked. The reason is that the 
probability of a morpheme, say p(??|education), 
is always less than its substring, p(?|education), 
since whatever ?? occurs ? and ? always 
occur but not vice versa. So the aligning result will 
be ? /Education and ? /Department, ?  is 
abandoned. To overcome this problem, a constraint 
of alignment is imposed to the model to ensure that 
the aligning result covers every Chinese characters 
of a target word and no overlapped characters in 
the result morpheme sequence. For instances, both
? /Education    ? /Department and ? ?
/Education    ??/Department are not allowed 
alignment sequences. The constraint is applied to 
each possible aligning result. If the alignment 
violates the constraint, it will be rejected.  
Since the new alignment algorithm must 
enumerate all of the possible alignments, the 
process is very time consuming. Therefore, it is 
advantageous to use a bilingual terminology bank 
rather than a parallel corpus. The average length of 
terminologies is short and much shorter than a 
typical sentence in a parallel corpus. This makes 
words to morphemes alignment computationally 
feasible and the results highly accurate (Chang et 
al., 2001; Bai et al, 2006). This makes it possible 
to use the result as pseudo gold standards to 
evaluate affix rules as described in section 4.3. 
 
 
 
 
251
 
air|?? refrigeration|?? machine|? 
building|?? industry|? 
compound|?? steam|?? engine|? 
electronics|?? industry|? 
vice|? chancellor|?? 
Figure 3. Sample of word-to-morpheme alignment. 
4.3 Rule Extraction 
After the alignment task, we will get a word-to-
morpheme aligned terminology bank as shown in 
Figure 3. We can subsequently extract affix rules 
from the aligned terminology bank by the 
following steps: 
 
1) Generate candidates of affix rule: 
For each alignment, we produce all alignment 
links as affix rules. For instance, with 
(electronics|??  industry|? ), we would 
produce two rules: 
 
     (a) ??_, electronics 
     (b) _?, industry 
 
2) Evaluate the rules: 
The precision of each candidate rule is 
estimated by applying the rule to segment the 
Chinese terms. If a Chinese term contains the 
affix shown in the rule, the affix will be 
segmented. The results of segmentation are 
then to compare with the segmentation results 
of the alignments done by the algorithm of the 
section 4.2 as pseudo gold standards. Some 
example results of rule evaluations are shown 
in Figure 4.    
 
affix English word 
Rule 
Applied  
Correct 
segments precision
?_ master 458 378 0.825 
??_ periodic 130 100 0.769 
??_ video 46 40 0.870 
_? chain 147 107 0.728 
_? box 716 545 0.761 
Figure 4. Sample evaluations of candidate rules. 
3) Adding exception condition: 
In the third step, we sort the rules according to 
their precision rates in descending order, 
resulting in rules R1..Rn . And then for each Ri , 
we scan R1 to Ri-1, if there is a rule, Rj, have 
the same English word condition and the affix 
condition of Ri subsume that of Rj, then we 
add affix condition of Rj as exception 
condition of Ri. For example, _? , industry 
and _??, industry are rule candidates in the 
sorted table and have the same English word 
condition. Furthermore, the condition _? 
subsumes that of ??, we add ?? to the 
exception condition of the rule with a shorter 
affix. 
 
4) Reevaluate the rules with exception 
condition: 
After adding the exception conditions, the 
rules are reevaluated with considering the ex-
ception condition to get new evaluation scores. 
 
5) Select rules by scores: 
Finally, filter out the rules with scores lower 
than a threshold2. 
 
The reason of using exception condition is that 
an affix is usually an abbreviation of a word, such 
as _? is an abbreviation of ??. In general, a full 
morpheme is preferred to be segmented than its 
abbreviation while both occurred in a target word. 
For example, when applying rules to ????
/electronic industry, _?? ,industry is preferred 
than _?,industry. However, in the evaluation step, 
precision rate of _?,industry will be reduced when 
applying to full morphemes, such as ????
/electronic industry, and then could be filtered out 
if the precision is lower than the threshold.  
5 Impurity Measure Method 
The impurity measure was used by decision tree 
(Duda et al, 2001) to split the training examples 
into smaller and smaller subsets progressively 
according to features and hope that all the samples 
in each subset is as pure as possible. For 
convenient, they define the impurity function 
rather than the purity function of a subset as 
follows:  
 ??=
j
jj wPwPSimpurity )(log)()( 2  
                                                 
2 We set the threshold as 0.7.  
252
                     
 
(a) impurity value of ????.                   (b) impurity values of ?? and ??. 
Figure 5. Examples of impurity values. 
 
Where P(wj) is the fraction of examples at set S 
that are in category wj. By the well-known 
properties of entropy if all the examples are of the 
same category the impurity is 0; otherwise it is 
positive, with the greatest value occurring when 
the different classes are equal likely.  
5.1 Impurity Measure of Translation 
In our experiment, the impurity measure is used 
to split a Chinese word into two substrings and 
hope that all the characters in a substring are 
generated by the parallel English words as pure as 
possible. Here, we treat a Chinese word as a set of 
characters, the parallel English words as categories 
and the fraction of examples is redefined by the 
expected fraction number of characters that are 
generated by each English word. So we redefine 
the entropy impurity as follows: 
 
);|(log);|();( 2 fe,fe,fe,
e
efcefcfI
e
E ?
??
?=
In which f denotes the target Chinese word, e and f 
denote the parallel English and Chinese sentence 
that f belongs to and   is the expected 
fraction number of characters in f that are 
generated by word e. The expected fraction 
number can be defined as follows: 
);|( fe,efc
??
?
?? ??
??=
e
fe,
e fc
fc
ecp
ecp
efc
)|(
)|(
);|(  
Where p(c | e) denotes the translation probability 
of Chinese character c given English word e. 
 
For example, as shown in Figure 5, the impurity 
value of ????, Figure 5.(a), is much higher 
than values of ?? and ??, Figure 5.(b). Which 
means that the generating relations from English to 
Chinese tokens are purified by breaking ???? 
into ?? and ??.   
The translation probabilities between Chinese 
characters and English word can be trained using 
IBM model 1 by treating Chinese characters as 
tokens. 
5.2 Target Word Selection 
In this experiment, we treat the Chinese words 
which can be segmented into morphemes and 
linked to different English words as target words. 
In order to speedup our impurity method only tar-
get words will be segmented during the process. 
Therefore we investigate the actual distribution of 
target words first, we have tagged 1,573 Chinese 
words manually with target and non-target. It turns 
out that only 6.87% of the Chinese words are 
tagged as target and 94.4% of target words are 
nouns. The results show that most of the Chinese 
words do not need to be re-segmented and their 
POS distribution is very unbalanced. The results 
show that we can filter out the non-target words by 
simple clues. In our experiment, we use three fea-
tures to filter out non-target words: 
 
1) POS: Since 94.4% of the target words are 
nouns, we focus our experiment on nouns 
and filter out words with other POS.  
2) One-to-many alignment in GIZA++:  Only 
Chinese words which are linked to multiple 
English words in the result of GIZA++ are 
considered to be target words. 
3) Impurity measure: the target words are ex-
pected to have high impurity values. So the 
words with a impurity values larger than a 
threshold are selected as target words3. 
                                                 
3 In our experiment, we use 0.3 as our threshold. 
253
5.3 Best Breaking Point and we used these annotated data as our gold 
standard in testing.  The goal of segmentation adjustment using 
impurity is to find the best breaking point of a 
Chinese word according to parallel English words. 
When a word is broken into two substrings, the 
new substrings can be compared to original word 
by the information gain which is defined in terms 
of impurity as follows: 
Because of the modification of Chinese tokens 
caused by the word segmentation adjustment, a 
problem has been created when we wanted to 
compare the results to the copy which did not 
undergo adjustment. Therefore, after the alignment 
was done, we merged the alignment links related to 
tokens that were split up during adjustment. For 
example, the two links of foreign/?? minister/?
? were merged as foreign minister/????. 
),;(
2
1
),;(
2
1
),;(    
),,(
11
11
fefefe niE
i
EE
n
i
i
fIfIfI
fffIG
+
+
??
=
  
The evaluation of word alignment results are 
shown in Table 1, including precision-recall and 
AER evaluation methods. In which the baseline is 
alignment result of the unadjusted data. The table 
shows that after the adjustment of word 
segmentation, both methods obtain significant 
improvement over the baseline, especially for the 
English-Chinese direction and the intersection 
results of both directions. The impurity method in 
particular improves alignment in both English-
Chinese and Chinese-English directions.  
Where i denotes a break point in f,  denotes 
first i characters of f, and  denotes last n-i 
characters of f. If the information gain of a 
breaking point is positive, the result substrings are 
considered to be better, i.e. more pure than original 
word.  
if1
n
if 1+
The goal of finding the best breaking point can 
be achieved by finding the point which maximizes 
the information gain as the following formula: 
The improvement of intersection of both 
directions is important for machine translation. 
Because the intersection result has higher precision, 
a lot of machine translation method relies on 
intersecting the alignment results. The phrase-
based machine translation (Koehn et al, 2003) 
uses the grow-diag-final heuristic to extend the 
word alignment to phrase alignment by using the 
intersection result. Liang (Liang et al, 2006) has 
proposed a symmetric word alignment model that 
merges two simple asymmetric models into a 
symmetric model by maximizing a combination of 
likelihood and agreement between the models. 
This method uses the intersection as the agreement 
of both models in the training time. The method 
has reduced the alignment error significantly over 
the traditional asymmetric models.  
),,(maxarg 111
n
i
i
ni
fffIG +<?  
Note that a word can be separated into two 
substrings each time. If we want to segment a 
complex word composed of many morphemes, just 
split the word again and again like the construction 
of decision tree, until the information gain is 
negative or less than a threshold4. 
6 Experiments 
In order to evaluate the effect of our methods on 
the word alignment task, we preprocessed parallel 
corpus in three ways: First we use a state-of-the-art 
word segmenter to tokenize the Chinese part of the 
corpus. Then, we used the affix rules to adjust 
word segmentation. Finally, we do the same but by 
using the impurity measure method.  We used the 
GIZA++ package (Och and Ney, 2003) as the word 
alignment tool to align tokens on the three copies 
of preprocessed parallel corpora.  
In order to analyze the adjustment results, we 
also manually segment and link the words of 
Chinese sentences to make the alignments 1-to-1 
mapping as many as possible according to their 
translations for the 112 gold standard sentences.  
Table 2 shows the results of our analysis, the 
performance of impurity measure method is also 
slightly better than the affix rules in both recall and 
precision measure. 
We used the first 100,000 sentences of Hong 
Kong News parallel corpus from LDC as our 
training data. And 112 randomly selected parallel 
sentences were aligned manually with sure and 
possible tags, as described in (Och and Ney, 2000), 
                                                 
4 In our experiment, we set 0 as the threshold. 
254
 
 direction Recall precision F-score AER 
English-Chinese 68.3 61.2 64.6 35.7 
Chinese-English 79.6 67.0 72.8 27.8 baseline 
intersection 59.9 92.0 72.6 26.6 
English-Chinese 78.2 64.6 70.8 29.8 
Chinese-English 80.2 68.0 73.6 27.0 affix rules 
intersection 69.1 92.3 79.0 20.2 
English-Chinese 78.1 64.9 70.9 29.7 
Chinese-English 81.4 70.4 75.5 25.0 impurity 
intersection 70.2 91.9 79.6 19.8 
Table 1. Alignment results based on the standard word segmentation data. 
 
 recall precision 
affix rules 82.35 66.66 
impurity 84.31 67.72 
Table 2. Alignment results based on the manual 
word segmentation data. 
 
7 Conclusion 
In this paper, we have proposed two Chinese word 
segmentation adjustment methods to improve word 
alignment. The first method uses the affix rules 
learned from a bilingual terminology bank and 
then applies the rules to the parallel corpus to split 
the compound Chinese words into morphemes ac-
cording to its counterpart parallel sentence. The 
second method uses the impurity method, which 
was motivated by the method of decision tree. The 
experimental results show that both methods lead 
to significant improvement in word alignment per-
formance. 
 
Acknowledgements: This research was supported 
in part by the National Science Council of Taiwan 
under NSC Grants: NSC95-2422-H-001-031. 
References  
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going 
Beyond AER: An Extensive Analysis of Word 
Alignments and Their Impact on MT. In Proceedings 
of ACL 2006, pages 9-16, Sydney, Australia. 
Ming-Hong Bai, Keh-Jiann Chen and Jason S. Chang. 
2006. Sense Extraction and Disambiguation for 
Chinese Words from Bilingual Terminology Bank. 
Computational Linguistics and Chinese Language 
Processing, 11(3):223-244. 
Petter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, Robert L. Mercer. 1993. The 
Mathematics of Machine Translation: Parameter 
Estimation. Computational Linguistics, 19(2):263-
311.  
Jason S Chang, David Yu, Chun-Jun Lee. 2001. Statisti-
cal Translation Model for Phrases(in Chinese). Com-
putational Linguistics and Chinese Language Proc-
essing, 6(2):43-64. 
Keh-Jiann Chen, Ming-Hong Bai. 1998. Unknown 
Word Detection for Chinese by a Corpus-based 
Learning Method. International Journal of 
Computational linguistics and Chinese Language 
Processing, 1998, Vol.3, #1, pages 27-44.  
Keh-Jiann Chen, Wei-Yun Ma. 2002. Unknown Word 
Extraction for Chinese Documents. In Proceedings of 
COLING 2002, pages 169-175, Taipei, Taiwan.  
Keh-Jiann Chen, Shing-Huan Liu. 1992. Word 
Identification for Mandarin Chinese Sentences. In 
Proceedings of 14th COLING, pages 101-107.  
John DeNero, Dan Klein. 2007. Tailoring Word 
Alignments to Syntactic Machine Translation. In 
Proceedings of ACL 2007, pages 17-24, Prague, 
Czech Republic. 
Yonggang Deng, William Byrne. 2005. HMM word and 
phrase alignment for statistical machine translation. 
In Proceedings of HLT-EMNLP 2005, pages 169-176, 
Vancouver, Canada.  
Richard O. Duda, Peter E. Hart, David G. Stork. 2001. 
Pattern Classification. John Wiley & Sons, Inc.  
Jianfeng Gao, Mu Li, Andi Wu and Chang-Ning 
Huang.  2005. Chinese word segmentation and 
named entity recognition: a pragmatic approach. 
Computational Linguistics, 31(4) 
Sharon Goldwater, David McClosky. 2005. Improving 
Statistical MT through Morphological Analysis. In 
255
Proceedings of HLT/EMNLP 2005, pages 676-683, 
Vancouver, Canada.  
Philipp Koehn, Franz J. Och, Daniel Marcu. 2003. Sta-
tistical Phrase-Based Translation. In Proceedings of 
HLT/NAACL 2003, pages 48-54, Edmonton, Canada. 
Young-Suk Lee. 2004. Morphological Analysis for 
Statistical Machine Translation. In Proceedings of 
HLT-NAACL 2004, pages 57-60, Boston, USA.  
Young-Suk Lee, Kishore Papineni, Salim Roukos. 2003. 
Language Model Based Arabic Word Segmentation. 
In Proceedings of ACL 2003, pages 399-406, 
Sapporo, Japan.  
Percy Liang, Ben Taskar, Dan Klein. 2006. Alignment 
by Agreement. In Proceedings of HLT-NAACL 2006, 
pages 104-111, New York, USA.  
Wei-Yun Ma, Keh-Jiann Chen. 2003. A Bottom-up 
Merging Algorithm for Chinese Unknown Word 
Extraction. In Proceedings of ACL 2003, Second 
SIGHAN Workshop on Chinese Language Processing, 
pp31-38, Sapporo, Japan.  
Yanjun Ma, Nicolas Stroppa, Andy Way. 2007. 
Bootstrapping Word Alignment via Word Packing. In 
Proceedings of ACL 2007, pages 304-311, Prague, 
Czech Republic.  
Robert C. Moore. 2004. Improving IBM Word-
Alignment Model 1. In Proceedings of ACL 2004, 
pages 519-526, Barcelona, Spain.  
Franz Josef Och, Hermann Ney. A Systematic 
Comparison of Various Statistical Alignment Models, 
Computational Linguistics, volume 29, number 1, pp. 
19-51 March 2003. 
Franz J. Och, Hermann Ney., Improved Statistical 
Alignment Models, In Proceedings of the 38th An-
nual Meeting of the Association for Computational 
Linguistics, 2000, Hong Kong, pp. 440-447. 
Stefan Vogel, Hermann Ney, Christoph Tillmann. 1996. 
HMM-based word alignment in statistical translation. 
In Proceedings of COLING 1996, pages 836-841, 
Copenhagen, Denmark.  
Dekai Wu, Xuanyin Xia. 1994. Learning an English-
Chinese Lexicon from a Parallel Corpus. In 
Proceedings of AMTA 1994, pages 206-213, 
Columbia, MD.  
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Corpora. 
Computational Linguistics, 23(3):377-403.  
256
Resolving Ambiguities of Chinese Conjunctive Structures by Divide-
and-conquer Approaches 
Duen-Chi Yang, Yu-Ming Hsieh, Keh-Jiann Chen  
Institute of Information Science, Academia Sinica, Taipei 
{ydc, morris, kchen}@iis.sinica.edu.tw 
 
 
Abstract 
This paper presents a method to enhance a 
Chinese parser in parsing conjunctive 
structures. Long conjunctive structures 
cause long-distance dependencies and tre-
mendous syntactic ambiguities. Pure syn-
tactic approaches hardly can determine 
boundaries of conjunctive phrases properly. 
In this paper, we propose a divide-and-
conquer approach which overcomes the dif-
ficulty of data-sparseness of the training 
data and uses both syntactic symmetry and 
semantic reasonableness to evaluate am-
biguous conjunctive structures. In compar-
ing with the performances of the PCFG 
parser without using the divide-and-
conquer approach, the precision of the con-
junctive boundary detection is improved 
from 53.47% to 83.17%, and the bracketing 
f-score of sentences with conjunctive struc-
tures is raised up about 11 %. 
1 Introduction 
Parsing a sentence with long conjunctive structure 
is difficult, since it is inadequate for a context-free 
grammar to represent context-sensitive-like coordi-
nation structures, such as ?a b c? and a? b? c?? ?.  
It causes long-distance dependencies and tremen-
dous syntactic ambiguities (a large number of al-
ternatives). Pure syntactic approaches cannot de-
termine boundaries of conjunctive phrases properly. 
It is obvious that both syntactic and semantic in-
formation are necessary for resolving ambiguous 
boundaries of conjunctive structures. 
Some analysis methods of the detection of con-
junctive structures have been studied for a while. 
Despite of using different resources and tools, these 
methods mainly make use of the similarity of 
words or word categories on both sides of conjunc-
tive structure (Agarwal et al, 1992; Kurohashi et 
al., 1994; Delden, 2002; Steiner 2003). They as-
sumed that two sides of conjuncts should have 
similar syntactic and semantic structures. Some 
papers also suggest that certain key word patterns 
can be used to decide the boundaries (Wu 2003). 
Agarwal et al (1992) used a semantic tagger and a 
syntactic chunker to label syntactic and semantic 
chunks. And then they defined multi-level (cate-
gory to category or semantic type to semantic type) 
similarity matching to find the structure boundaries. 
Delden (2002) included semantic analysis by 
applying WordNet (Miller 1993) information. 
These presented methods used similarity measures 
heuristically according to the property of the lan-
guages. However detecting conjunctive boundaries 
with a similar method in Chinese may meet some 
problems, since a Chinese word may play different 
syntactic functions without inflection. It results that 
syntactic symmetry is not enough to resolve ambi-
guities of conjunctive structures and semantic rea-
sonableness is hard to be evaluated. Therefore we 
propose a divide-and-conquer approach which 
takes the advantage of using structure information 
of partial sentences located at both sides of con-
junction. Furthermore we believe that simple cases 
can be solved by simple methods which are effi-
cient and only complex cases require deep syntac-
tic and semantic analysis. Therefore we develop an 
algorithm to discriminate simple cases and com-
plex cases first. We then use a sophisticated algo-
rithm to handle complex cases only.  
For simple cases, we use conventional pattern 
matching approach to speedup process. For com-
plex conjunctive structures, we propose a divide-
and-conquer approach to resolve the problem. An 
input sentence with complex conjunctive structure 
715
is first divided into two parts, one to the left of the 
conjunctive and one to the right, and then parsed 
independently to detect possible candidates of two 
conjuncts. The particular property of complex con-
junctive structures of Chinese language allows us 
to parse and to produce syntactic structures of two 
partial sentences, since according to our observa-
tions and experiments the syntactic structures of 
partial sentences at either side of a complex con-
junctive construction are grammatical most of the 
times. Figure 1 shows an instance. The parsing re-
sults not only reduce the possible ambiguous 
boundaries but also provide global structural in-
formation for checking the properness of both sides 
of conjunctive structure. Another important point 
worth mentioning is that since the size of available 
Treebank is small, a two-stage approach is pro-
posed to resolve the data sparseness problems in 
evaluating syntactic symmetry and semantic rea-
sonableness. At the first stage, a Conditional Ran-
dom Fields model is trained and used to generate a 
set of candidate boundaries. At the second stage, a 
word-association model is trained from a giga-
word corpus to evaluate the semantic properness of 
candidates. The proposed divide-and-conquer algo-
rithm avoids parsing full complex conjunctive 
structures and handles conjunctive structures with 
deep structural and semantic analysis. 
The extraction method for context-dependent 
rules is described in Section 2 and detail of the di-
vide-and-conquer approach is stated in Section 3. 
In Section 4, we introduce our experimental envi-
ronment and show the results of our experiment. 
We also make some discussions about our observa-
tions in Section 4. Finally, we offer our conclusion 
and future work in Section 5. 
2 Boundary Detection for Simple Con-
junctive Phrases 
The aim of this phase of approach is to determine if 
simple conjunctive phrases exist in input sentences 
and then identify their boundaries by matching 
context-dependent rules. To derive a set of context-
dependent rules for conjunctive phrases, a na?ve 
approach is to extract all conjunctive patterns with 
their contextual constraints from Treebank. How-
ever such a set of extracted rules suffers a low cov-
erage rate, since limited size of training data causes 
zero frequency of long n-gram PoS patterns. 
2.1 Rule extraction and generalization 
Agarwal et al, (1992), Kurohashi et al, (1994), 
and Delden (2002) had shown that the properties of 
likeness and symmetry in both syntactic types and 
lengths for example, exist in most conjunctive 
cases. Hence we use both properties as the condi-
tions in deciding boundaries of conjunctive phrases. 
When we observe Sinica Treebank (Chen et al, 
2003), we also find that this property is more obvi-
ous in simple conjunctive cases than in complex 
cases. 
First, we use a simple algorithm to detect the 
boundaries of completely symmetric conjunctive 
phrases. If PoS patterns of ?A B C and A B C? or 
?A B and A B? occurred in the input sentence, we 
consider patterns of such structures are legitimate 
conjunctive structures regardless whether the PoS 
sequences ?A B C and A B C? or ?A B and A B? 
ever occurred in the Treebank. For other cases we 
use context-dependent rule patterns to determine 
boundaries of conjunctive structures. 
Statistical context-dependent PoS-based rule pat-
terns are extracted automatically from Sinica Tree-
bank. Each rule contains the PoS pattern of a con-
junctive phrase and its left/right contextual con-
straints. The occurrence frequency of the rule and 
its correct identification rate are also associated. e.g. 
[VC] (Na Caa Nc) [DE]1 ;  12; 11 
This rule says that PoS sequence Na Caa Nc 
forms a conjunctive phrase when its left context is 
a VC and its right context is a DE. Such pattern 
occurred 12 times in the training corpus and 11 out 
of 12 times (Na Caa Nc) are correct conjunctive 
phrases. 
Context-dependent rule patterns are generated 
and generalized by the following procedure. 
Rule Generation and Generalization 
For each conjunctive structure in the Treebank, we 
consider a window pattern of at most 9 words. This 
pattern contains conjunction in the center and at 
most 4 words at each side of the conjunction. The 
PoS sequence of these 9 words forms a context-
dependent rule. For instance, the conjunctive struc-
ture shown in Figure 1 will generate the pattern (1). 
(1) [Vc DM] (VH  Na  Caa Neu Na) [DE Na] 
The long pattern has low applicability and hardly 
                                                 
1 Caa is a PoS for coordinate conjunction. Na is a common 
noun; Nc denotes place noun, and Vc is a transitive verb. DE 
denotes the relativizer ???. 
716
can evaluate its precision. Therefore a rule gener-
alization process is applied. Two kinds of generali-
zations are available. One is reducing the length of 
contextual constrains and the other is to reduce a 
fine-grained PoS constraint to a coarse-grained PoS. 
Some instances, shown in (2), are the generalized 
patterns of (1). 
(2)  [DM] (VH  Na  Caa Neu Na) [DE];1;1 
(VH  Na  Caa Neu Na); 10; 5 
[DM] (V  N  Caa N N) [DE]; 3; 2 
Then the applicability and precision of rules higher 
than threshold values will be selected. The threshold 
values for the rule selection are determined by test-
ing results on the development data. 
3 Resolution of Complex Conjunctive 
Structures 
Complex structures are cases whose boundaries can 
not be identified by the pattern matching at phase-1. 
We propose a divide-and-conquer approach to re-
solve the problem. An input sentence with complex 
conjunctive structure was first divided into two 
parts with each part containing one of the conjuncts 
and then parsed independently to produce their 
syntactic structures for detecting possible bounda-
ries of two conjuncts. Then ambiguous candidate 
structures are generated and the best conjunctive 
structure is selected by evaluating syntactic sym-
metry and semantic reasonableness of the candi-
dates. Since the two parts of the partial sentences 
are simple without conjunctive structure and nor-
mally grammatical 2 , hence they can be easily 
parsed by a PCFG parser. 
Here we illustrate the divide-and-conquer algo-
rithm by the following example. For instance, the 
example shown in Figure 1 has complex conjunc-
tive structure and it was first split into two parts (1a) 
and (1b) at conjunction marker ? ??. 
(1a) ?? if (Cbb) ? I (Nh) ?? invent (VC) ?? a 
kind (DM) ? low (VH) ?? pollution (Na) 
(1b) ? null (Neu) ?? accident (Na) ?(DE) ??
car (Na)  
The two parts of partial sentences are then 
parsed to produce their syntactic structures as 
shown in Figure 1. Then a CRF model trained from 
Sinica Treebank for checking syntactic symmetry 
                                                 
2 According to our experiments only 0.8% of the complex 
testing data and development data are failed to parse their 
partial structures at both sides of conjunction. 
was derived to pick the top-N candidates according 
to the syntactic information of both sides of partial 
sentences. Then at the second stage, a semantic 
evaluation model is proposed to select the best 
candidate. The detail of the semantic evaluation 
model is described in the section 3.2. The reason 
for using a two-stage approach is that the size of 
the Treebank is limited, but the semantic evaluation 
model requires the values of association strengths 
between words. The current Treebank cannot pro-
vide enough coverage and reliable values of word-
association strengths.  
3.1  Derive and evaluate possible candidates 
CRF is a well-known probabilistic framework for 
segmenting and labeling sequence data (Lafferty, et 
al. 2001). In our experiments, we regard the prob-
lem of boundary detection as a chunking-like prob-
lem (Lee et al, 2005). Due to this reason, we use 
CRF model to generate candidates and their ranks. 
The features used in CRF model included some 
global syntactic information, such as syntactic 
category of a partial structure and its phrasal head. 
Such global syntactic information is crucial for the 
success of boundary detection and is not available 
if without the step of parsing process. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. The syntactic structures of 5(a) and 5(b) 
produced by a PCFG parser. 
The features used are: 
WL,i ; CL,i; WR,j ; CR,j : The left(i)/right(j) most word 
and its pos category of the left/right conjunct. 
PL, ; PR,: The phrasal category of the left/right con-
junct. 
HwL ; HcL ; HwR ; HcR: The phrasal head and its pos 
category of the left/right conjunct. 
DL ; DR: The length of the left/right conjunct.  
Three types of feature patterns are used for CRF. 
The first type is feature patterns regarding individ-
WL,i+1 (WLi WL,i-1 ?.       WL1    W0   WR1 ?WR,j )WR,j+1, 
 
Some example feature values of the above hypothesis boundaries.  
WLi  = ?; CLi  =Nh; WR,j  =??; CR,j  =Na; 
PL, =S; PR, =NP; 
HwL=??; HcL= VC; HwR =??; HcR=Na; 
DL = 5; DR = 2;
717
ual conjuncts. The second type is feature patterns 
regarding symmetry between two conjuncts. The 
third type is feature patterns regarding contextual 
properness of a conjunctive structure. 
Type1: WLi, WLi-1, WLi+1, CLi, CLi-1, CLi-2, CLi-1CLi-2, CLi+1, CLi+2, 
CLi+1CLi+2, CLiCLi-1CLi-2, CLi-1CLiCLi+1, CLiCLi+1CLi+2, 
WLiHwL, CLiHcL, and WRj, WRj-1, WRj+1, CRj, CRj-1, CRj-2, CRj-
1CRj-2, CRj+1, CRj+2, CRj+1CRj+2, CRjCRj-1CRj-2, CRj-1CRjCRj+1, 
CRjCRj+1CRj+2, WRjHwR, CRjHcR.. 
Type 2: PL PR, HwLHwR, HcLHcR, DLDR. 
Type 3: WL,i+1HwRj, WR,j+1HwLi, WL,1WR,j, WR,1WL,j, 
WL,1WR,j+1, WR,1WL,j+1, WL,1WR,jWR,j+1, 
WR,1WL,iWL,i+1, WL,1WR,j-1WR,j, WR,1WL,i-1WL,i,  
CL,i-1HcRj, CR,j-1HcLi, CL,i+1HcRj, CR,j+1HcLi, 
CL,iCL,i+1HcRj, CR,jCR,j+1HcLi, CL,1CR,j, CR,1CL,j, 
CL,1CR,j+1, CR,1CL,j+1, CL,1CR,jCR,j+1, CR,1CL,iCL,i+1, 
CL,1CR,j-1CR,j, CR,1CL,i-1CL,i. 
A CRF model is trained from the Sinica Tree-
bank and estimated the probabilities of hypothesis 
conjunctive boundary pairs by the feature patterns 
listed above. The top ranked candidates are se-
lected according to the CRF model. In general, for 
further improvement, a final step of semantic 
evaluation will be performed to select the best can-
didate from top-N boundary structures ranked by 
the CRF model, which is described in the next sec-
tion.  
3.2 The word-association evaluation model 
For the purpose of selecting the best candidates of 
complex conjunctive structures, a word association 
evaluation model is adopted (Hsieh et al 2007). 
The word-to-word association data is learned 
automatically by parsing texts from the Taiwan 
Central News Agency corpus (traditional charac-
ters), which contains 735 million characters. The 
syntactically dependent words-pairs are extracted 
from the parsed trees. The word-pairs are phrasal 
heads and their arguments or modifiers. Though the 
data is imperfect (due to some errors produced by 
auto-tagging system and parser), the amount of 
data is large enough to compensate parsing errors 
and reliably exhibit strength between two 
words/concepts. 
37,489,408 sentences in CNA (Central News 
Agency) corpus are successfully parsed and the 
number of extracted word associations is 
221,482,591. The word association probabilities is 
estimated by eq.(1). 
)(
),(
)|(
Headfreq
ModifyHeadfreq
HeadModifyP =        (1) 
?freq(Head)? means Head word frequency in the 
corpus and ?freq(Head,Modify)? is the cooccur-
rence frequency of Head and Modify/Argument.  
The final evaluation is done by combining three 
scores, i.e. (1) the probability produced by PCFG 
parser, (2) the scores of CRF classifier and (3) the 
scores of semantic evaluation. The detail is de-
scribed in Section 4.2. 
4 Experiments 
3,484 sentences of the Sinica Treebank are used as 
training data. The development data and testing 
data are extracted from three different set of cor-
pora the Sinica corpus, Sinorama magazines and 
textbooks of elementary school (Hsieh et al 2005). 
They are totally 202 sentences (244 conjunctions) 
with 6-10 words and 107 sentences (159 conjunc-
tions) with more than 11 words. We only test the 
sentences which contain the coordinate conjunction 
category or categories.  
We adopt the standard PARSEVAL metrics 
(Manning et al, 1999) including bracket f-score to 
evaluate the performance of the tree structures of 
sentences and accuracies of boundary detection of 
conjunction structures. 
4.1 Phase-1 experimental results 
For the phase-1 experiments, the context-
dependent rules are extracted and generalized from 
Sinica treebank. We then use the development data 
to evaluate the performances for different sets of 
rules selected by different threshold values. The 
results show that the threshold values of occurrence 
once and precision 70% performed best. This 
means any context-dependent rule with precision 
greater than or equal to 70% is used for the future 
processes. 39941 rules are in the set. In Table 1, we 
compare the phase-1 result with the baseline model 
on test data. It is shown that the boundary detection 
precision is very high, but the recall rate is com-
paratively low, since the phase-1 process cannot 
handle the complex cases. We also compare the 
processing time between the baseline model and 
the phase-1 parsing processes in Table 2. Marking 
conjunctive boundaries before parsing can limit the 
search range for parser and save processing time. 
The effect is more obvious when parsing long sen-
tences. Because long sentences generate more am-
718
biguous paths than shorter sentences, these surely 
spend much more time. 
6-10 words more than 11 words Test data  
Baseline phase1 Baseline phase1
C-boundary  
f-score 
55.74 84.43 50.0 63.75 
S-bracket        
f-score 
72.67 84.44 71.20 79.40 
Table 1. The comparison between the baseline 
PCFG model and the phase1 parsing process . 
6-10 words more than 11 words unit: second 
Baseline  phase1  Baseline  phase1
development data 14 12 34 23 
test data 14 11 34 24 
Table 2. The comparison of processing time be-
tween the baseline model and the phase1 parsing 
process. 
4.2 Phase-2 experimental results 
Complex cases cannot be matched by context-
dependent rules at the phrase-1 which will be han-
dled by the phase-2 algorithms mentioned in Sec-
tion 3. We use the CRF++ tool (Kudo, 2006) to 
train our CRF model. The CRF model can produce 
the N-best candidates for an input conjunctive sen-
tence. We experiment on the models of Top1-CRF 
and TopN-CRF where the Top1-CRF algorithm 
means that the final output is the best candidate 
produced by CRF model and the TopN-CRF means 
that the final output is the best candidate produced 
by the structure evaluation process described below. 
For each N-best candidate structure, three 
evaluation scores is derived: (a) the probability 
score generated from the PCFG parser, i.e. 
RuleScore, (b) the probability score generated from 
the CRF classifier, i.e. CRF-Score, and (c) the 
word association score, i.e. WA-Score. We normal-
ize each of the three scores by eq.(2): 
minmax
min)(
ScoreScore
ScoreScore
Scorenormal ii ?
?=                 (2) 
Scorei means the score of the i-th candidate, and 
Scoremin and Scoremax mean the worst and the best 
score in the candidate set for a target conjunctive 
sentence. The normalized scores are between 0 and 
1. After normalization, we combine the three 
scores with different weights: 
Total Score = w1*RuleScore + w2*CRF-Score + 
w3*WA-Score                                   (3) 
The w1, w2 and w3 are regarded as the degree of 
importance of the three types of information. We 
use development data to determine the best combi-
nation of w1, w2, w3. Due to limit amount of de-
velopment data, many local maximum and global 
maximum are achieved by different values of w1, 
w2, w3. Therefore we use a clustering algorithm to 
cluster the grid points of (w1, w2, w3) which pro-
duce the best performance. We then pick the larg-
est cluster and calculate its centroid as our final 
weights which are shown at Table 3.  
 Top N w1 w2 w3 
6-10words N = 3 0.11 0.64 0.25 
11- words N = 3 0.18 0.76 0.06 
Table 3. The best weights determined by the devel-
opment data for the sentences with different 
lengths using the best-3 candidates.  
The performance results of the testing data are 
shown in Table 4. In comparing with the results of 
the baseline model shown in Table 1, the conjunc-
tion boundary f-score increased from about 53% to 
83% for the testing data. The processes also im-
prove the overall parsing f-scores from 72% to 
83%. The results of Table 4 also show that the 
evaluation function indeed improves the perform-
ances but marginally. However the experiments are 
done under the condition that the input sentences 
are perfectly word segmented and pos tagged. In 
real practices, parser may accept sentences with 
ambiguous word segmentation and pos tagging to 
avoid the error accumulation due to early commit-
ment on word segmentation and pos tagging. 
Therefore parsers require much more information 
to resolve much more ambiguous conditions. A 
robust evaluation function may play a very impor-
tant role. We will do more researches in the future. 
 Top1CRF TopNCRF 
C-boundary f-score 85.57 89.55 Develop-
ment data S-bracket f-score 80.10 82.34 
C-boundary f-score 82.18 83.17 Test data 
S-bracket f-score 83.15 83.45 
Table 4. The final results of our overall processes.  
Another point worth mentioning, the perform-
ances of ?CRF? (using CRF model without phase-1) 
and ?phase1+CRF? (using CRF model after phase-
1) algorithms are comparable. However ?phase1+ 
CRF? algorithm is much more efficient, since 
?phase1+CRF? algorithm can determine the simple 
conjunctive structures by pattern matching and 
most of conjunctive structures are simple. On the 
other hand, the ?CRF? model requires twice partial 
sentence parsing, generates candidates with CRF 
719
classifier and evaluates structure with three syntac-
tic and semantic scores. 
5 Conclusion 
Conjunctive boundary detection is not a simple 
task. It is not only time consuming but also knowl-
edge intensive. Therefore we propose a context-
dependent rules matching approach to handle sim-
ple cases to get fast returns.  For complex cases, we 
use a knowledge intensive divide-and-conquer ap-
proach. To resolve the problems of inadequate 
knowledge and data sparseness due to limit amount 
of structure annotated training data, we extract 
word/concept associations from CNA corpus.  
In our experiments, the proposed model works 
well. Most conjunctive phrases are simple cases 
and can be matched by context-dependent rules and 
indeed avoid unnecessary calculation. Compared 
with the baseline method of straight forward PCFG 
parsing, the f-score of conjunctive boundary detec-
tion can be raised about 22%. For the complex 
cases, the boundaries f-score is further raised about 
7% after phase-2 processes. The experimental re-
sults show that the method not only works well on 
boundary resolution for conjunctive phrases but 
also improves the total performances of syntactic 
parsing. 
Our solutions include the rule-based method and 
cooperate with semantic and syntactic analyses. 
Therefore in the future we will try to enhance the 
syntactic and semantic analyses. For syntactic 
analysis, we still need to find more effective meth-
ods to improve the performance of our parser. For 
the semantic analysis, we will try to refine the word 
association data and discover a better semantic 
evaluation model. 
Acknowledgements 
This research was supported in part by National 
Digital Archives Program (NDAP, Taiwan) spon-
sored by the National Science Council of Taiwan 
under NSC Grants: NSC95-2422-H-001-031-. 
References 
Agarwal, Rajeev and Boggess, Lois. 1992. A Simple but 
Useful Approach to Conjunct Identification. In Pro-
ceedings of 30th Annual Meeting of Association for 
Computational Linguistics, pages 15-21. 
Chen, Keh-Jiann, Huang, Chu-Ren, Chen, Feng-Yi, Luo, 
Chi-Ching, Chang, Ming-Chung, Chen, Chao-Jan and 
Gao, Zhao-Ming. 2003. Sinica Treebank: design cri-
teria, representational issues and implementation. In 
Anne Abeille, (ed.): Building and Using Parsed Cor-
pora. Text, Speech and Language Technology. 
20:231-248, pages 231-248. 
Hsieh,Yu-Min, Yang, Duen-Chi and Chen, Keh-Jiann. 
2005. Linguistically-motivated grammar extraction, 
generalization and adaptation. In Proceedings of the 
Second International Join Conference on Natural 
Language Processing (IJCNLP2005), pages 177-187, 
Jeju Island, Republic of Korea. 
Hsieh, Yu-Ming, Duen-Chi Yang and Keh-Jiann Chen. 
2007. Improve Parsing Performance by Self-Learning. 
International Journal of Computational Linguistics 
and Chinese Language Processing, Vol. 12, #2, 
pages 195-216. 
Kurohashi, Sadao, and Nagao, Makoto. 1994. A Syntac-
tic Analysis Method of Long Japanese Sentences 
Based on the Detection of Conjunctive Structure. 
Computational Linguistics 20(4), pages 507-534. 
Kudo, Taku. 2006. (software)CRF++: Yet Another CRF 
toolkit http://chasen.org/~taku/software/CRF++/. 
Lafferty, John, McCallum, Andrew, Pereira, Fernando. 
2001. Conditional Random Fields: Probabilistic 
Models for Segmenting and Labeling Sequence Data. 
In Proceedings of the 18th International Conference 
on Machine Learning (ICML-01), pages 282-289. 
Lee, Yong-Hun, Kim, Mi-Young and Lee, Jong-Hyeok. 
2005. Chunking Using Conditional Random Fields in 
Korea Texts. In Proceedings of the Second Interna-
tional Join Conference on Natural Language Proc-
essing (IJCNLP2005), pages 155-164, Jeju Island, 
Republic of Korea. 
Manning, Christopher D., and Schutze, Hinrich. 1999. 
Foundations of Statistical Natural Language process-
ing. The MIT Press, Cambridge, Massachusetts.  
Miller, Geroge, 1993. Introduction to WordNet: An 
Online Lexical Database. Princeton, CSL Report 43. 
Steiner, Ilona. 2003. Parsing Syntactic Redundancies in 
Coordinate Structures. Poster presentation at the 
European Cognitive Science Conference (Euro-
CogSci03). 
Van Delden, Sebastian. 2002. A Hybrid Approach to 
Pre-Conjunct Identification. In Proceedings of the 
2002 Language Engineering Conference (LEC 2002), 
pages 72-77, University of Hyderabad, India. 
Wu, Yunfang. 2003. Contextual Information of Coordi-
nate Structure. Advances on the Research of Machine 
Translation, pages 103-109, Publishing house of 
Electronics Industry. 
720
Knowledge Extraction for Identification of Chinese Organization Names 
Keh-Jiann Chen & Chao-jan Chen 
kchen@iis.siniea.edu.tw fichard@iis.siniea.edu.tw 
Institute of  Information Science, Academia Siniea, Taipei 
ABSTRACT 
In this paper, a knowledge extraction process 
was proposed to extract the knowledge for 
identifying Chinese organization ames. The 
knowledge extraction process utilizes the 
structure property, statistical property as well as 
partial inguistic knowledge of the organization 
names to extract new organizations from domain 
texts. The knowledge xtraction processes were 
experimented on large amount of texts retrieved 
from WWW. With high standard of threshold 
values, new organization names can be 
identified with very high precision. Therefore 
the knowledge extraction processes can be 
carried out automatically to self improve the 
performance in the future. 
1. INTRODUCTION 
The occurrences of unknown words cause 
difficulties in natural language processing. The 
word set of a natural language is open-ended. 
There is no way of collecting every words of a 
language, since new words will be created for 
expressing new concepts, new inventions, 
newborn babies, new organizations. Therefore 
how to identify new words in a text will be the 
most challenging task for natural language 
processing. It is especially true for Chinese. 
Each Chinese morpheme (usually a single 
character) carries meanings and most arc 
polyscmous. New words are easily constructed 
by combining morphemes and their meanings 
are the semantic composition of morpheme 
components. However there are also 
semantically non-compositional compounds, 
such as proper names. In Chinese text, there is 
no blank to mark word boundaries and no 
inflectional markers nor capitalization markers 
to denote the syntactic or semantic types of new 
words. Hence the unknown word identification 
for Chinese became one of the most difficult and 
demanding research topic. 
There are many different types of 
unknown words and each has different morph- 
syntactic and morph-scmantic structures. In 
principle their syntactic and semantic ategories 
can be determined by their content and 
contextual information, but there arc many 
difficult problems have to be solved. First of 
all it is not possible to find a uniform 
representational schema and categorization 
algorithm to handle different types of 
unknown words due to their different morph- 
syntactic structures. Second, the clues for 
identifying different type of unknown words 
are also different. For instance, identification 
of names of Chinese people is very much 
relied on the surnames, which is a limited set 
of characters. The statistical methods are 
commonly used for identifying proper names 
(Chen & Lee 1996, Chang ct al. 1994, Sun et 
al. 1994). The identification of general 
compounds is more relied on the morphemes 
and the semantic relations between 
morphemes (Chcn & Chcn 2000). The third 
difficulty is the problems of ambiguities, 
such as structure ambiguities, syntactic 
ambiguities and semantic ambiguities. For 
instances, usually a morpheme 
character/word has multiple meaning and 
syntactic categories and may play the roles 
of common words or proper names. 
Therefore the ambiguity resolution became 
one of the major tasks. 
In this paper we focus our attention on 
the identification of the organization names. 
It is considered to be a hard task to identify 
organization names in comparing with the 
identification of other types of unknown 
words, because there are not much morph- 
syntactic and morph-sernantic clues to 
indicate an organization name. There is no 
significant preference on the selection of 
morphemes/characters and the semantic of 
the morphemes, which gives no clue leading 
toward the identification. For instance, '~ ,  
micro-soW (Microsoft) has the character by 
character (morpheme by morpheme) 
translation of 'slightly soR" and there is no 
marker, such as capitalization, to indicate 
that it is a proper name. The only reliable 
clue is its context information. However an 
organization's full names usually occur at its 
first mention, unless it is a well-known 
organization. A full name contains its proper 
15 
name and organization type, such as '~  
"~ , Acer Computer-Company'. The 
organization types became the major clue of 
identifying a new organization name. However 
abbreviated shorter names usually will be used, 
such as a) omit part of the organization type, for 
instances '~  ~,  Acer Computer', '~  
~..~J, Acer Company', b) omit the organization 
type totally, for instance '~ .~,  Acer', or c) the 
abbreviation, for instance '~ ,  global-electric 
(Acer-computer)'. Therefore the task became 
not only the identification of organization names 
in different forms but also finding their meaning 
equivalence classes. To achieve the above goal, 
the knowledge of 1) proper names of 
organizations, 2) different lines of the businesses, 
and 3) different organization types, should be 
equipped. Unfortunately there is no well- 
prepared knowledge sources containing the 
above information. Therefore a knowledge 
extraction model is proposed to extract the 
above mentioned knowledge from the dictionary 
and domain texts. 
2. STRUCTURES OF ORGANIZATION 
NAMES 
There is no rigid structure for an organization 
name as mentioned in the previous section. 
Roughly speaking an organization ame is 
composed by two major components. The first 
part is the proper name and the second part is the 
organization type. The second part contains the 
major key words lead toward the identification 
of an organization, since the organization types, 
such as '~ J ,  company', ~ '~"  foundation', 
"/J',~J~ group', '~ \ [ \ ]  enterprise' etc, tells what 
kind of organizations they are. If it is a company, 
to be more informative the line of business 
usually goes with the key word '~\ ]  company', 
for instances '~ff~.t..~J food company', ~  
~ computer company', ' ~ M  ~='~ ~ 
investment consultant company', but in most 
cases the keyword '~B\ ]  company' will be 
ignored, such as ~- -~(  President food). 
Sometimes the line of business and the 
organization type go together to become a single 
word, such as ,qa~ middle school',  ' f , l~  
bank' , '~ :  ~ hospital'. By observing the 
structure of the organization name, it seems that 
once a complete list of the organization types is 
well prepared, then it is not hard to identify the 
organizations by their Rill names. The only 
complication is that abbreviated names occur 
more frequently than fidl names. The identifier 
'.~..~\] company' is usually ignored in real text. 
The lines of business became the major 
identifier for a company and many business lines 
are common words, such as '~1~ food', 
'~JJ~ computer', 'TJ~0~ cement'. Therefore 
it is necessary to make the distinction 
between a common compounds and a 
company name, for examples, '~ J~J~n 
health food" vs. ,~m~ President food', 
'~ fg j~ personal computer' vs. ' ~  
~ Acer computer'. Although they are two- 
way ambiguous, usually they have only one 
preference r ading. 
In conclusion, the types and the proper 
names of organizations will be the major 
clues lead toward the identification of the 
organizations. In addition, it is also better to 
have a list of well known organization 
names, such that the well known company 
names, like '~.~i \ [  MicrosoR', can be 
identified immediately. Most of the 
knowledge preparation works should be 
done by oflline approaches. The prepared 
knowledge would be utilized to online 
identification of newly coined organiT~ations. 
The equivalent classes of the well-known 
organizations are also classified by a 
similarity-based approach. 
3. KNOWLEDGE EXTRACTION 
There are two knowledge sources. One is the 
CKIP Chinese lexicon and another is the 
Chinese text from WWW. The lexicon 
provides a partial list of important 
organizations and the information extracted 
from them will be the initial knowledge of 
the identification system. The texts from 
WWW provide ample of new organization 
names implicitly. The problem is how to 
extract some, if not all, of them from the 
texts. Once we have a list of organization 
names. The proper names for organizations 
and the organization types will be extracted 
by analyzing the morphological structures of 
the organization names. However an 
effective morphological analyzer depends 
upon the availability of the knowledge of the 
organization types, but the lists of the 
organization types are not available yet. 
As we mentioned before the complete 
organization names have two parts. The first 
part is the proper name and the second part 
is the organization type. The number of 
different proper names is unlimited and on 
the other hand the number of different 
organization types is limited. Th/s property 
will be utilized to separate the variable parts, 
i.e. the proper name, and the constant parts, 
i.e. the organization type, from the 
organization names. 
16 
The numbers of organization names in the 
lexicon is very limited, since only the important 
organizations in the common domain will be 
collected. Therefore the initial knowledge 
extracted from lexicon is also very limited. To 
make the sources of knowledge more adequate, 
vast amount of new organization names hould 
be extracted from each different domain corpus. 
Unfortunately none of the existing corpora had 
tagged the organization names. Therefore we are 
going to design a semi-automatic method to 
extract the high frequency organization names 
from text corpora. 
The locality of occurrences of keywords in a 
text will be utilized for keyword extraction. 
Once an organization name occun-ing in a text it 
is very probably reoccurred in the same text. The 
recurrence property had been utilized to extract 
keywords or key-phrases from text (Chien 1999, 
Fung 1998, Smadja 1993). However not all 
keywords arc organization names. The 
knowledge extracted from the lexicon, i.e. the 
list of the organization types will be the initial 
knowledge for identifying organization names. 
In addition to the initial knowledge, the structure 
property of the organization names will be also 
utilized in classifying extracted keywords into 
organization names and non-organization names. 
The extraction processes will be repeated for 
extracting new organizations and therefore 
extracting new organization types. The more 
knowledge would have been extracted the more 
accurate of the organization identification will 
achieve. 
3.1 Morphological Analysis for Organization 
Names 
There are 1391 number of words in the CKIP  
lexicon classified as organizations. Table I 
shows some of the examples. 
Table I. The samples of organizations from the 
CKIP  dictionary 
As we observed, the morphological structure of 
an organization name usually is a compounding 
of a proper name and a organization type. The 
organization type might be a compounding of a 
line of business and a type, for instances i.~JJ~ 
~..~J (computer company), ~\]~,~(bank), qb~ 
(middle school), or simply a line of business, 
for instances AM(food), ~-~J~(computer), 
7~ ~ (cement). The proper names are 
variables, since each organization type may 
have many different institutions with 
different names. The types are constants. 
There is a limited number of constants 
attached with many different proper names 
to form different organization names. 
Therefore to extract the organization types is 
equivalent o extract the high frequency 
ending morphemes. Table 2 shows the top 20 
high frequency ending morphemes xtracted 
from the 1391 organization ames and in 
deed they are organization types. 
52 ~" 38 ~ 36 ~ 30 ~d: 29 ;/q~t 
21 ~ 21 I~ 20 ~ 17 Y~ 16 ~z 
16 ~ 16 ~ 15 ~ 15 ~ 15 "l-\]g 
13 I~ 12 ~ 12 ~\] 12 ~JJ 11 
Table 2. The top 20 organization types 
ranked with their occurrence frequencies 
extracted from the 1391 organization names 
3.2 Automatic Extraction of Organization 
Names 
A Web spider can extract ext from each 
different domain through WWW. Then 
keyword extraction technique is applied on 
domain texts to retrieve possible keywords. 
The kcyword set includes organization 
names, personal names, general compounds, 
and also error extraction. Most of which are 
not organization names. It is supposed that 
the available list of the organization types 
will be the source of knowledge to identify 
candidates oforganizations. However such a 
method only identifies the organizations of 
the known organization types and provides 
new proper names only. It will not identify 
new types of organizations. Therefore we 
use a new method to extract the organization 
names by using the structure property of 
organization names. 
Extraction Algorithm for Organization 
Types: 
Step 1. Using a Web spider to collect 
Chinese texts of a fixed domain, such as 
domain of finance and business, from 
.WWW. 
Step 2. Extract high frequency keywords in 
the text (Smadja 93, Chang & Su 97, Chien 
99). 
Step 3. For the keywords of length 3,4, and 5, 
each keyword is divided into two parts X 
and Y. X is a candidate of proper name and 
17 
Y is a candidate of organization type. The X is 
the initial two-characters of the keyword and Y 
is the remained characters. (Since most proper 
names of organizations have two characters, we 
can extract he organization types of the lengths 
1, 2 and 3 from three different groups of 
keywords with lengths 3, 4 and 5 respectively.) 
Extract the organization type Y, if for some 
keywords X+Y, the following conditions hold. 
a) X satisfies one of the following cases. 
1. X is not in the lexicon, i.e. X is an 
unknown word. 
2. X has the categories of Nb or No, i.e. it is 
a known proper name (bib) or a location 
name (No). 
b) For each Y, assumed to be the organization 
type, there must have more than n number of 
different X, such that X+Y in the extracted 
keyword list. In practice, the threshold value 
n was set to 2. 
In general, Chinese company names like most 
proper names are non-common word (unknown 
words). However sometimes they are place 
names (No), but rarely they are common ouns, 
adjectives, or verbs. Therefore in order to avoid 
too many false alarms, such as "~. ,~, /~ super 
computer", to be considered as a company name, 
the condition a) of step 3 is set. The reason to 
setup the condition b) is that each organization 
type Y should have many different organizations 
which have the same organization type Y, such 
as ' ~ ~  Acer computer',  '~,."~_,~ 
Leo computer', ' ~ ~ ~ ~ Blue-slcy 
computer', ...etc.. The real implementation 
shows the different hreshold value n gives the 
different precision and recall for identification. 
For the first iteration of knowledge 
extraction, we suggest to have higher ecall rate. 
Set the threshold value low and manually select 
the final list of the organization types. For the 
future automatic knowledge xtraction, in order 
to increase the precision of the information 
extraction higher threshold values are suggested. 
4. E~ERIME~ ~S~TS 
The knowledge xtraction processes for Chinese 
org~t ion  names are carried out by different 
stages. At the first stage, the words marked with 
semantic category of organization were accessed 
from the CKIP dictionary. There are 1391 word 
organization types. As mentioned in section 3.1, 
a pseudo morphological analysis process was 
carried out, which try to find the high frequency 
ending morphemes. Since the structure of an 
organization ame is a composition of X+Y, 
where X is a proper name and Y is a 
organization type. There are 546 different 
ending morphemes. The high frequency 
ending morphemes are exactly to be the 
morphemes for common organization types. 
Many of them are monosyllabic words and 
they are polysemous, as shown in Table 1. 
For the future identification, the 
disambiguation process has to be carried out 
for those polysemous ending morphemes 
(Chen & Chen 2000). The extracted 
morphemes and list of organizations will be 
the first collection of the organization types. 
At the second stage, we try to extract 
new organizations names from different 
domain text. Each different domain has 
many new organization types. For instance 
in the domain of finance and business, there 
are many company names, which have 
completely different word strings for the 
organization types as in the extracted list by 
the first stage. 
The algorithm shown in the section 3.2 
was carried out. At the step 1, 31787 texts of 
news of the finance and business domain 
were extracted from http://www.cnyes.com. 
At step 2, 40675 keywords were extracted 
from the news corpus. At step 3, 
organization ames were identified and the 
organization types were extracted. If the 
threshold value n -- 2, 92 types were 
extracted and among them 83 are correct 
organization types. The precision is 90%. If 
the threshold was set to 3, only 56 types 
were extracted and all of them happen to be 
correct. The precision increased to 100%, 
but of course the recall rate dropped. We 
don't know the exact recall rate, since there 
are too many keywords in the training set. 
However the recall rate is not important, 
since the whole knowledge extraction 
process is a recurrent process. The 
knowledge xtraction procedures should be 
repeatedly applied on the different set of text 
and at each iteration more information will 
be extracted. Hence the precision is much 
more important than the recall. The 
knowledge sources for future identification 
of organizations are the accumulated lists of 
the organization names, the proper names of 
organizations and the organization types. 
Table 3 contains the extracted 
organization types while the threshold value 
n=3. The organization types are classified by 
their lengths and sorted by their frequencies 
of uses. Table 4 contains the extracted 
organization types which associated with 
exactly two different names and the last line 
shows the error extractions. Among newly 
extracted organization types only 23 of them 
18 
are already in the old list. 
_~t~ 81 ~ 74 ~:~ 46 ~1~'~ 41 ~---~. 40 {...~j 36 
19 ~-~-~ 18 ~-~ 17 ~b~ 16 ~ 15 ~q~. 15 
~-~ 15 ~ 15 ~ 14 ~?~ 13 ~ II B~ 10 
~ 9 ~ 9 ~ 8 ~ 7 ~=r.~ 6 ~fl~l~ 6 
~f~6 ~I/,~6 )L~5 ~5 ~5 ~5 
Ig-~4 ~32 4 ~_T..4 .T.:~3 'fJfiI 3 ~ i :3  
~/~3 ,~3 ~3 ~3 ~3 ~3 
~? 3 
~\ [ \ [~ 7 ~\ ]~ 6 
Table 3. The extracted organization types 
associated with the number of different names 
>=3 
Table 4. The extracted organization types 
associated with two different names and the last 
lines show the error exWactions. 
4.1 Strategies for On-line Identification of 
Organization Names 
The knowledge about organizations extracted 
from the dictionaries and domain texts will be 
used to identify organization ames at on-line 
sentence processing. During the word 
segmentation process, an organization name is 
either identified immediately (if it is a known 
organization name), or it will be segmented into 
two segments of X+Y or several segments of 
(xl+x2+...+xn)+Y, where X is a proper names, 
Y is the organization type. When the proper 
name X is a new word, it will be segmented into 
shorter segments (xl+x2+...+xn). To simplify 
the experiment process, we  assume the proper 
names X are either the words of categories Nb  
(i.e. proper names) or Nc (i.e. the place names) 
or a two-character unknown word. For the 
identification experiment, a corpus extract from 
a T.V. news (http://www.ttv.com.tw) 
The patterns of X+Y in the testing corpus 
were searched. 117 different organizations were 
identified. Among thern 56 are known 
organizations, i.e. they are in the organization 
name list. 61 of them arc identified by the 
composition of X+Y and 52 of them are correct. 
It counts the precision of 52/61=85% for 
identifying new names. The total performance is 
the precision of 108/I 17=92%. 
The knowledge-based approach for 
identifying organization ames seems very 
promising. It outperforms the reports of the 
precision of 61.79% and the recall of 
54.50% in (Chen & Lee 1996) and the 
experiment was carried out under the 
condition that the knowledge extraction 
process is in its initial stage. We expect hat 
performance of the algorithm will become 
better and better while the knowledge 
extraction process continuously performs. 
4.2 Automatic Extraction of Name 
Equivalent Classes 
The abbreviated names are very frequently 
occurred in the real text especially in the 
domain of the stock market. By observing 
the abbreviation ames, the heuristic rules 
for abbreviating a company name can be 
concluded as follows. 
Abbreviation rule: If the proper name of a 
company is unique, then take the proper 
name as its abbreviation name, such as '~ ,  
Microsoft'. Otherwise the abbreviation will 
be a compound of key-characters from part 
of its proper name and part of its line of 
business, such as ~ is the abbreviation 
o f '~  m~l~,  China petroleum'. 
An  experiment was carried out to find 
the full names of the abbreviations of 
company names shown in the price table of 
the Taiwan stock market. The purposes of 
this experiment are a) to fred the equivalent 
classes of company names and b) to have 
some idea about the recall rate of the current 
knowledge extraction process. 
The matching process between the 
abbreviations and the extracted organization 
name lists is as follows. 
I. For each abbreviation name matches the 
organization names in the organization 
name list. Find all the organization names 
containing the abbreviation name. 
2. Rank the matched organization names 
according to the following criterion. 
The first rank: The proper name of  the 
organization ame is  exactly matched 
with the abbreviation name. 
The second rank: The abbreviation is 
compounding of key-characters f om part 
of the proper name and part of the line of 
business of the matched organization 
names. 
If there are many candidates with the 
same rank, then rank them according to 
their frequencies occurring in the training 
corpus. 
19 
There are 471 abbreviated company names in 
the price list of the stock market. 302 of them 
have matched candidates. Each abbreviation 
name may match many different organization 
names. The recall rate for the top ranked 
candidate is 282/471=60%. The precision of the 
first rank candidate is 282/302=93%. Table 5 
shows some of the results. 
Abbr. Candidates arranged in the order of their anks 
~ ~ 8 ~l.IJ~l~l 3 ~I.L~-T-" 2 
TW~ttt 
~m~4.tt 
11 
I01 ~..~-~; 2
48 m ~  2 
I0 ~rd~;H'f~ 2 ~, f~ I 
5 ~. . J~ l l  22 ~K~....~ 22 
21 
6 
15 ~:~\]~J~ 2 ~\ [~ 2 
11 
67 ~. JW 29 ~,~, J~ 17 
6 ~.~J~gg 2 ~.,~ "~r~ 2 
:~,,~?~ 2 
Table 5. Some examples of the abbreviations 
and the matched candidates (the correct answer 
is highlighted by the boldface characters) 
5. CONCLUSIONS 
The knowledge extraction process will be 
continuously carried on in the future. The 
accumulated knowledge will be utilized for the 
on-line unknown word identification as well as 
for the off-line knowledge extraction. The 
proposed knowledge extraction processing 
model can be generalized to extract other types 
of linguistics or morphological knowledge, for 
instances, to extract the transliterate foreign 
names, to extract the titles of people. 
Some of the errors are caused by that the 
titles of the people are wrongly identified as 
organization types, since the patterns of people's 
name followed by their title are commonly 
occurred in real text. These patterns are similar 
to the sU'uctures of organization ames. Such 
kind of errors can be avoid, if the titles of people 
are known and in fact the titles of people can be 
extracted by the same extraction model except 
that most of people's names have three 
characters instead of two. 
In the future, the knowledge xtraction 
processes will be automatically carried out. We 
expect hat it will be one of the major building 
blocks for automatic learning systems for 
Chinese morphology and sentence processing. 
REFERENCES 
\[1\] Bai, M.H., C.J. Chert & KJ. Chert, 1998, 
"POS tagging for Chinese Unknown 
Words by Contextual Rules" 
Proceedings ofROCLING, pp.47-62. 
\[2\] Chang, J. S.,S.D. Chen, S. J. Ker, Y. 
Chen,& J. Liu,1994 "A Multiple-Corpus 
Approach to Recognition of Proper 
Names in Chinese Texts", Computer 
Processing of Chinese and Oriental 
Languages, Vol. 8, No. 1, pp. 75-85. 
\[3\] Chang, Jing Shin and Keh-Yih Su, 
1997," An Unsupervised Iterative 
Method for Chinese New Lexicon 
Extraction," Computational Linguistics 
and Chinese Language Processing, Vol. 
2 #2, pp97-147. 
\[4\] Chert, Keh-Jiann, Ming-Hong Bai, 1997, 
"Unknown Word Detection for Chinese 
by a Corpus-based Learning Method." 
Proceedings of the l Oth Research on 
Computational Linguistics International 
Conference, pp159-174. 
\[5\] Chen, K.J. & Chao-jan Chen, 2000," 
Automati Semantic Classification for 
Chinese Unknown Compound Nouns," 
Coling 2000. 
\[6\] Chert, K.J. & S.H. Liu, 1992,"Word 
Identification for Mandarin Chinese 
Sentences," Proceedings of14th Coling, 
pp. 101-107. 
\[7\] Chen, Hsin-His & Jen-Chang Lee, 
1996," Identification and Classification 
of Proper Nouns in Chinese Texts," 
Proceedings of Coling-96, Vol. 1., pp. 
222-229. 
\[8\] Chien, Lee-feng, 1999," PAT-tree-based 
Adaptive Keyphrase Extraction for 
Intelligent Chinese Information 
Retrieval," Information Processing and 
Management, Vol. 35, pp. 501-521. 
\[9\] Fung P., 1998," Extracting Key Terms 
from Chinese and Japanese Texts," 
Computer Processing of Oriental 
Languages, Vol. 12, #1, pp 99-122. 
\[10\] Lee, J.C. , Y.S. Lee and H.H. Chen, 
1994, "'Identification of Personal Names 
in Chinese Texts." Proceedings of 7th 
ROC Computational Linguistics 
Conference. 
\[11\] Lin, M. Y., T. H. CMang, & K.Y. Su, 
1993," A Preliminary Study on 
Unknown Word Problem in Chinese 
Word Segmentation" Proceedings of 
Rocling VI, pp 119-137. 
\[12\] McDonald D., 1996, '" Internal and 
20 
External Evidence in the Identification and 
Semantic Categorization f Proper Names", 
in Corpus Processing for Lexical 
Acquisition, J. Pustejovsky and B. Boguraev 
Eds, MIT Press 1996. 
\[13\] Smadja, Frank, 1993,'Retrieving 
Collocations from Text: Xtract," 
Computational Linguistics, vil. 19, #1, pp. 
143-177. 
\[14\] Sun, M. S., C.N. Huang, H.Y. Gao, & Jie 
Fang, 1994, "Identifying Chinese Names in 
Unrestricted Texts", Communication of 
COUPS, Vol.4 No. 2. 113-122. 
21 
Sinica Treebank: 
Design Criteria, Annotation Guidelines, and On-line Interface 
Chu-Ren Huang t, Feng-Yi Chen 2, Keh-Jiann Chen 2, Zhao-ming Gao s, & 
Kuang-Yu Chen 2 
churen(a3,sinica.edu.tw, aoole(~jis.sinica.edu.tw, kchen(~,iis.sinica.edu.tw, 
zmgao@ccms.ntu.edu.tw, sasami@iis.sinica.edu.tw 
=Institute ofLinguistics, Academia Siniea, Taipei, Taiwan 
2Institute of Information Science, Academia Sinica, Taipei, Taiwan 
3Dept. of Foreign Languages & Literatures, National Taiwan University, Taipei, Taiwan 
Abstract 
This paper describes the design 
criteria and annotation guidelines of 
Sinica Treebank. The three design 
criteria are: Maximal Resource Sharing, 
Minimal Structural Complexity, and 
Optimal Semantic Information. One of 
the important design decisions 
following these criteria is the encoding 
of thematic role information. An on-line 
interface facilitating empirical studies of 
Chinese phrase structure is also 
described. 
1. Introduction 
The Penn Treebank (Marcus et al 
1993) initiated a new paradigm in 
corpus-based research. The English. 
Penn Treebank has enabled and 
motivated corpus and computational 
linguistic research based on information 
extractable from structurally annotated 
corpora. Recently, the research has 
focused on the following two issues: 
first, when and how can a structurally 
annotated corpus of language X be 
built? 
Second, what information should or 
can be annotated? A good sample of 
issues in these two directions can be 
found in the papers collected in Abeille 
(1999). 
The construction of the Sinica 
Treebank deals with both issues. First, it 
is one of the first structurally annotated 
corpora in Mandarin Chinese. Second, 
as a design feature, the Sinica Treebank 
annotation includes thematic role 
information in addition to syntactic 
categories. In this paper, we will discuss 
the design criteria and annotation 
guidelines of the Sinica Treebank. We 
will also give a preliminary research 
result based on the Sinica Treebank. 
2. Design Cr i te r ia  
There are three important design 
criteria for the Sinica Treebank: 
maximal resource sharing, minimal 
structural complexity, and optimal 
semantic information. 
First, to achieve maximal resource 
sharing, the construction of the Sinica 
Treebank is bootstrapped from existing 
29 
Chinese computational linguistic 
resources. The textual material is 
extracted from the tagged Sinica Corpus 
(hRp:l/www.sinica.edu.tw/ftms-bin/ 
kiwi.sh, Chen et al 1996). In other 
words, the tasks and issues involving 
tokenization / word segmentation and 
category assignment are previously 
resolved. It is worth noting that the 
segmentation and tagging of Sinica 
Corpus have undergone vigorous 
post-editing. Hence the precision of 
category-assignment is much higher 
than with an automatically tagged 
corpora. In addition, since the same 
research team carried out the tagging of 
Sinica Corpus and annotation of Sinica 
Treebank, consistency of the 
interpretation of texts and tags are 
ensured. For structure-assigument, an 
automatic parser (Chen 1996) is applied 
before human post-editing. 
Second) the criterion of minimal 
structural complexity is motivated to 
ensure that the assigned structural 
information can be shared regardless of 
users' theoretical presupposition. It is 
observed that theory-internal 
motivations often require abstract 
intermediate phrasal levels (such as in 
various versions of the X-bar theory). 
Other theories may also call for an 
abstract covert phrasal category (such as 
INFL in the GB theory for Chinese). In 
either case, although the phrasal 
categories are well-motivated within the 
theory, their significance cannot be 
maintained in the context of other 
theoretical frameworks. Since a primary 
goal of annotated corpora is to serve as 
the empirical base of linguistic 
investigations, it is desirable to annotate 
structure divisions that are the most 
commonly shared among theories. We 
came to the conclusion that the minimal 
basic level structures are the ones that 
are shared by all theories. Thus our 
annotation is designed to achieve 
minimal structural complexity. All 
abstract phrasal levels are eliminated 
and only canonical phrasal categories 
are marked. 
Third) a critical issue involving 
Treebank construction as well as 
theories of NLP is how much semantic 
information, if any, should be 
incorporated. The original Penn 
Treebank took a fairly straightforward 
syntactic approach. A purely semantic 
approach, though tempting in terms of 
theoretical and practical considerations, 
has never been attempted yet. A third 
approach is to annotate partial semantic 
information, especially those pertaining 
to argument-relations. This is an 
approach shared by us and the Prague 
Dependency Treebank (e.g. Bohmova 
and Hajikova 1999). In this approach, 
the thematic relation between a predicate 
and an argument is marked in addition to 
grammatical category. Note that the 
predicate-argument relation is usually 
grammatically instantiated and generally 
considered to be the semantic relation 
that interacts most closely with syntactic 
behavior. This allows optimal semantic 
30 
information to be encoded without going 
too beyond the partially automatic 
process of argument identification. 
3. Annotation Guidelines I: 
Category and Hierarchy 
The basic structure of a tree in a 
treebank is a hierarchy of nodes with 
categorical denotation. As in any 
standard phrase structure grammar, the 
lexieal (i.e. terrninal) symbols are 
defined.by the lexicon (CKIP 1992). 
And following the recent lexicon-driven 
and information=based trends in 
linguistic theory, linguistic information 
will be projected from encoded lexical 
information. Please refer to CKIP (1993) 
for the definition of lexieal categories 
that we followed. We will give below 
the inventory of the restricted set of 
phrasal categories used and their 
interpretation. This set defines the 
domain of expressed syntactic 
information (instead of projected or 
inherited information). Readers can also 
consult Chen et al's (2000) general 
description of how the Siniea Treebank 
is constructed for a more complete list of 
tags as well as explanation i Chinese. 
3.1. Defining Phrasal Categories 
There are only 6 non-terminal 
phrasal categories annotated in the 
Sinica Treebank. 
(1) Phrasal Categories 
1. S: An S is a complete tree headed by a 
predicate (i.e. S is the start symbol). 
2.VP: A VP is a phrase headed by a 
predicate. However, it lacks a 
subject and cannot function alone. 
3. NP: An NP is beaded by an N. 
4.GP: A GP is a phrase headed by 
locational noun or locational adjunct. 
Since the thematic role is often 
determined by the governing 
predicate and not encoded locally; 
nominal phrases are given a tentative 
role of DUMMY so that it can 
inherit he correct role from the main 
predicate. 
5. PP: A PP is headed by a preposition. 
The thematic role of its argument is 
inherited from the mother, hence its 
argument is marked with a 
DUMMY. 
6. XP: A XP is a conjunctive phrase that 
is headed by a conjunction. Its 
syntactic head is the conjunction. 
However, since the actual category 
depends on the interactive 
inheritance from possibly 
non-identical conjoined elements, X 
in XP stands for an under-specified 
category. 
3.2. Defining Inheritance Relations 
Following unification-based 
grammatical theories, categorical 
assignments in Sinica Treebank are both 
lexicon-driven and head-driven. In 
principle, all grammatical information is
lexically encoded. Structurally heads 
indicate the direction of information 
inheritance and define possible 
predicate-argument relations. However, 
since the notion 'head' can have several 
31 
different linguistic definitions, we 
attempt to allow at least the discrepancy 
between syntactic and semantic heads. 
In Sinica Treebank, three different kinds 
of grammatical heads are annotated. 
(2) Heads 
1.Head: indicates a grammatical head in 
an endocentrie phrasal category. 
Unless a different semantic head is 
explicitly marked, a Head marks a 
category that serves simultaneously as
the syntactic and semantic heads of the 
construction. 
2.head: indicates a semantic head which 
does not simultaneously function as a 
syntactic head. For instance in 
constructions involving 
grammatiealized 'particles,' such as in 
the 'VP-de' construction, the 
grammatical head ('de' in this case) 
does not carry any semantic 
information. In these cases, the head 
marks the semantic head ('VP" in this 
case) to indicate the flow of content 
information. 
3. DUMMY: indicates the semantic 
head(s) whose categorical or thematic 
identity cannot be locally determined. 
The two most likely scenarios 
involving DUMMY are (a) in a 
coordination construction, where the 
head category depends on the sum of 
all conjuncts. And (b) in a non-NP 
argument phrase, such as PP, where 
the semantic head carries a thematic 
role assigned not by the immediate 
governing syntactic head ("P" in this 
case), but by a higher predicate. In 
these cases, DUMMY allows a parser 
to determine the correct categorical / 
thematic relation later, while 
maintaining identical local structures. 
3.3. Beyond Simple Inheritance 
When simple inheritance fails, the 
following principles derived from our 
design criteria serve to predict the 
structural assignments of a phrasal 
category: default inheritance, sisters only, 
and left most. 
3.3.1. Default Inheritance 
This principle deals primarily and 
most effectively with coordinations and 
conjunctions. The theoretical motivation 
of this account follows Sag et al's (1985) 
proposal. In essence, the category of a 
conjunctive construction must be 
inherited from its semantic heads. 
However, since conjunctions are not 
restricted to same categories, languages 
must have principled ways to determine 
the categorical identity when different 
semantic heads carry different 
information. 
First, in the trivial case when all 
head daughters are of the same category, 
the mother will inherit hat category. 
Second, when the different head 
daughters are an elaboration of the same 
basic category (e.g. both Nd and Ne are 
elaboration of N), then the basic 
category is the default inheritance 
category for the mother. This can be 
illustrated by (3). 
32 
(3) \[\[\[da4\]VH1 l\[er2\]Caa \[yuan2\]VH13\]\] 
VP 
big 
Third," 
mechanisms 
categorical 
and round 
when other inheritance 
fail to provide a clear 
choice, the default 
inheritance is activated. There are two 
default hierarchies. The first one deals 
with when the head daughters are all 
lexical categories (4a), and the second 
one deals with when they are all phrasal 
categories (4b). If there is a disparity 
between lexical and phrasal categories, 
then a lexical category will be expanded 
to a phrasal category first. 
(4)Default Inheritance Hierarchy for 
Categories 
a) Lexical Categories: V > N > P > Ng 
b) Phrasal Categories: S> VP> NP> 
PP> GP 
When phrasal conjuncts are involved, S 
is the privileged category since it is the 
start symbol of the grammar. VP comes 
next since its structural composition is
identical to that of S. If the structure 
involved is not a predicate (i.e. head of a 
sentence), then it must be a role. For 
argument roles, NP's are more 
privileged than PP's, and PP's are more 
privileged than GP's. (5) is an instance 
of the application of this default 
hierarchy. 
(5) \[\[da41iang4\]Neqa \[ r2\]Caa 
\[feng l sheng4\]VH11 \]V\]VP 
big-quantity and 
bountiful 
"bountiful and of big quantity" 
When lexical conjuncts are involved, the 
same principle is used. The priority is 
given to the predicate head of the 
sentence. Among possible argument 
roles, the nominal category is the default. 
An illustrative xample can be found in 
(6). 
(6) \[\[wei4lan2 de tianlkongl\]NP 
\[yu3\]Caa\[zhul qun2biao l han4\]S\]S 
aqua-blue DE sky 
and people ferocious 
'That the sky being aqua blue and 
that he people being ferocious...' 
3.3.2 Sisters Only 
Following most current linguistic 
theory, argument roles and adjunct 
complements must be sisters of a lexieal 
head. However, driven by our design 
criteria of minimal structural complexity, 
no same level iteration is allowed. Thus 
these arguments and adjuncts can be 
located by the straightforward definition 
of sisterhood: that they share the same 
mother-daughter r lation with the head. 
The result is a flat structure. 
33.3 Left First 
This principle is designed to 
account for possible internal structure 
when there are more than two sisters 
-without having to add on hierarchical 
complexity. Hence, the default 
interpretation of internal structure of 
multiple sisters is that the internal 
association starts from leR to right. 
33 
4. Annotation Guidelines II: 
Structural Annotation of 
Thematic Information 
A thematic relation contains a 
compact bundle of syntactic and 
semantic information. Although 
thematic relations are lexically encoded 
on a predicate, they can only be 
instantiated when that information is 
projected to phrasal arguments. In other 
words, the only empirical evidence for 
the existence of a thematic relation is a 
realized argument. However, a realized 
argument cannot by itself determine the 
thematic relation. The exact nature of 
the relation must be determined based 
on the lexical information fi'om the 
predicate as well as checking of the 
compatibility of that realized argument. 
Since structural information alone 
cannot determine thematic relations, 
prototypical structural annotation, such 
as in the original Penn Treebank, does 
not include thematic roles since they 
contain on-structural information. 
On the other hand, in theories 
where lexical heads drive the structural 
derivation / construction (e.g. ICG and 
HPSG and LFG), thematic relations are 
critical. Hence, we decided to encode 
realized thematic relations on each 
phrasal argument. The list of  thematic 
relations encoded on the head predicate 
is consulted whenever a phrasal 
argument is constructed, and a 
contextually appropriate relation 
sanctioned by the lexical information is 
encoded. It is worth noting that in our 
account., we not only mark the thematic 
relations of a verbal predicate, but we 
also mark the thematic relations 
governed by a deverbal noun, among 
others. Also note that an argument of a 
preposition is marked as a placeholder 
DUMMY. This is because a preposition 
only governs an argument syntactically, 
while its thematic relation is determined 
by a higher verb. 
(7) Thematic Roles: Classification and 
Inventory 
34 
THEMATIC ROLES 
I OUMMY I 
I 
I '~ '  I 
e~edeneer Ioe~on 
I ?*"~'*" t *o.v,~ 
\[ be.erect ~ *errnlemr~ 
\[ ?~mdi~m conjunction 
\[ e~eluaem negae~ 
\[ exrJudon incl~on 
\[ f l~*cy  -{ impera~ 
\[ quamiler quamiol 
\[ s~ndard 
I ~ deg~e 
I dei~$ ma.~0n 
re~.dz uncondJllon 
I hylxnl'~s: oondusion 
I wl '~u~f con~rdon 
I a'uDidanoe puq)ose 
I l- 
I I PR~?''~'?N I I I "?UN I 
o8,o~ I -L - - - t .oM,~T,o . I  I 
I 
I OtJMMY \[ 
5. Current  Status of  the Sinica 
Treebank and On- l ine 
Interface 
Following the above criteria and 
principles, we have already f inished 
Sinica Treebank 1.0. It contains 
annotations of 38,725 Chinese structural 
trees containing 239,532 words. It 
covers ubject areas that include politics, 
traveling, sports, finance, society, etc. 
This version of the Sinica Treebank will 
be released in the near future as soon as 
the licensing documents are cleared by 
the legal dep~,ent  of  Academia Sinica. 
A small subset of  it (1,000 sentences) is
already available for researchers to 
download from the website 
http ://godel.i is.sinica, edu. tw/CKIP/ 
treeslOOO.htm. A searchable interface is 
also being developed and tested for 
researchers o that they can directly 
access the complete treebank 
information. 
As an annotated corpus, one of the 
most important roles that a treebank can 
play is that it can serve as a shared 
35 
source of data for linguistic, especially 
syntactic studies. Following the example 
of the successful Sinica Corpus, we have 
developed an on-line interface for 
extraction of grammatical information 
from the Sinica Treebank. Although the 
users that we have in mind are 
theoretical linguists who do not 
necessarily have computational 
background; we hope that non-linguists 
can also benefit from the ready 
availability of such grammatical 
information. And of course, 
computational linguists should be able 
to use this interface for quick references 
before going into a more in-depth study 
of the annotated corpus. 
Currently, the beta site allows users 
specify a variety of conditions to search 
for structurally annotated sentences. 
Conditions can be specified in terms of 
keywords, grammatical tags (lexical or 
phrasal), thematic relations, or any 
boolean combination of the above 
elements. The search result can be 
presented aseither annotated structure or 
simply the example sentences. Simply 
statistics, based on either straightforward 
frequency count or mutual information, 
are also available. For linguistically 
interesting information, such as the 
heads of various phrasal constructions, a 
user can simply look up the explicitly 
syntactic Head or semantic head; as 
well as DUMMY when it serves as a 
head placeholder. The website of this 
interface, as well as the general release 
of the Sinica Treebank 1.0, is scheduled 
to be announced at the second ACL 
workshop on Chinese Language 
Processing inOctober 2000. 
6. Conclusion 
The construction of the Sinica 
Treebank is only a first step towards 
application of structurally annotated 
corpora. Continuing expansion and 
correction will make this database an 
invaluable resource for linguistic and 
computational studies of Chinese. 
References 
I.ABEILI..E, Anne. 1999. Ed. 
Proceedings ofATALA Workshop - 
Treebanks. Paris, June 18-19, 1999. 
Univ. de Paris VII. 
2.BOHMOVA, Alla and Eva Hajicova. 
1999. How Much of the Underlying 
Syntactic Structure Can be Tagged 
Automatically? In Abeille (Ed). 
1999.31-40. 
3.CHEN, Feng-Yi, Pi-Fang Tsai, 
Keh-Jiann Chen, and Chu-Ren Huang. 
2000. Sinica Treebank. \[in Chinese\] 
Computational Linguistics and 
Chinese Language Processing. 
4.2.87-103. 
4.CHEN, Keh-Jiarm. 1996. A Model for 
Robust Chinese Parser. Computational 
Linguistics and Chinese Language 
Processing. 1.1.183-204. 
5.CHEN, Keh-Jiann, Chu-Ren Huang. 
1996. Information-based Case 
Grammar: A Unification-based 
Formalism for Parsing Chinese. In 
Journal of Chinese Linguistics 
Monograph Series No. 9. Chu-Ren 
Huang, Keh-Jiaun Chen, and 
Benjamin K. T'sou Eds. Readings in 
Chinese Natural Language Processing. 
23-45. Berkeley: JCL. 
6.CHEN, Keh-Jiann, Chu-Ren Huang, 
Li-Ping Chang, Hui-Li Hsu. 1996. 
36 
Sinica Corpus: Design Methodology 
for Balanced Corpora. Proceedings of 
the 11th Pacific Asia Conference on 
Language, Information, and 
Computation (PA CLIC I1). Seoul 
Korea. 167-176. 
7.CHEN, Keh-Jiann and Shing-Huan 
Liu. 1992. Word Identification for 
Mandarin Chinese Sentences. 
Proceedings of COLING-92.101 - 105. 
8.CHEN, Keh- Jiann, Shing-Huan Liu, 
Li-Ping Chang, Yeh-Hao Chin. 1994. 
A Practical Tagger for Chinese 
Corpora." Proceedings of R OCLING 
V/I. 111-126. 
9.CHEN, Keh-Jiann, Chi-Ching Luo, 
Zhao-Ming Gao, Ming-Chung Chang, 
Feng-Yi Chen, and Chao-Ran Chert. 
1999. The CKIP Chinese Treebank: 
Guidelines for Annotation. In Abeille 
(Ed). 1999.85-96. 
I0. CKIP (Chinese Knowledge 
Information Processing). 1993. The 
Categorical Analysis of Chinese. CKIP 
Technical Report 93-05. Nankang: 
Academia Sinica. 
11. HUANG, Chu-Ren, Keh-Jiann 
Chen, Feng-Yi Chen, and Li-Li Chang. 
1997. Segmentation Standard for 
Chinese Natural Language Processing. 
Computational Linguistics and 
Chinese Language Processing. 
2.2.47-62 
12. Lin, Fu-Wen. 1992. Some 
Reflections on the Thematic System of 
Information-based Case Grammar 
(ICG). CKIP Technical Report 92-01. 
Nankang: Academia Sinica. 
13. Marcus, Miteh P., Beatrice 
Santorini, and M. A. Marcinkiewiicz. 
1993. Building a Large Annotated 
Corpus of English: The Peen Treebank. 
Computational Linguistics. 
19.2.313-330. 
14. SAG, Ivan, Gerald Gazdar, Thomas 
Wasow, and Steven Weisler. 1985. 
Coordination and How to Distinguish 
Categories. Natural Language and 
Linguistic Theories. 117-171. 
Appendix 
1. Lexical Categories 
(1) NON-PREDCITIVE ADJVECTIVE: A 
(2) CONJUNCTION: C 
(3) ADVERB: D 
(4) INTERJECTION: I 
(5) NOUN: N 
(6) DETERMINATIVES: Ne 
(7) MEASURE WORD / CLASSIFIER: 
Nf 
(8) POSTPOSITION WORD: Ng 
(9) PRONOUN: Nh 
(10) PREPOSITION: P 
(11) PARTICLES: T 
(12) VERB: V 
2. Sample Sentence and Tree 
nage wanfi de nyuren baifa zhihou 
bian buzai lihui 
? that hair-style DE woman white-hair after 
then never pay-attention 
ting qian tingting .FuR de 
qingcao 
courtyard front slender-ly standing-erect DE 
green-grass 
'After her hair had turned white, that 
coiffured woman ever paid any more 
attention tothe nicely standing green grass 
in the front courtyard." 
S(agent:NP(quantifier:DM:~l 
property:VP- ~j(head:VP(Head:VA4:~-) 
IHead:DE: ~)lHead:Nab:~A.)ltime:GP 
(DUMMY:VP(Head:VI-I 11:~1 ~)1 Head: 
Ng:-~-~.)\[firne:Dd:~l~ I time:Dd: ~"~'1 Head: 
VC2:J~ ~'\[goal:NP (property:VP ? ~(head: 
VP (location:NP(property:Neb:/~.l 
Head:Neda: ~,f)lHead:VH11: ;~ ,~ ~ 2Y_)\[ 
Head:DE: ~)I  Head:Nab: ff ~)) 
37 
Design of Chinese Morphological Analyzer 
 
Huihsin Tseng 
Institute of Information Science 
Academia Sinica, Taipei 
kaori@hp.iis.sinica.edu.tw 
Keh-Jiann Chen 
Institute of Information Science 
Academia Sinica, Taipei 
kchen@iis.sinica.edu.tw 
 
Abstract 
This is a pilot study which aims at the design of a 
Chinese morphological analyzer which is in state 
to predict the syntactic and semantic properties of 
nominal, verbal and adjectival compounds. 
Morphological structures of compound words 
contain the essential information of knowing their 
syntactic and semantic characteristics. In 
particular, morphological analysis is a primary 
step for predicting the syntactic and semantic 
categories of out-of-vocabulary (unknown) words. 
The designed Chinese morphological analyzer 
contains three major functions, 1) to segment a 
word into a sequence of morphemes, 2) to tag the 
part-of-speech of those morphemes, and 3) to 
identify the morpho-syntactic relation between 
morphemes. We propose a method of using 
associative strength among morphemes, 
morpho-syntactic patterns, and syntactic 
categories to solve the ambiguities of 
segmentation and part-of-speech. In our 
evaluation report, it is found that the accuracy of 
our analyzer is 81%. 5% errors are caused by the 
segmentation and 14% errors are due to 
part-of-speech. Once the internal information of a 
compound is known, it would be beneficial for the 
further researches of the prediction of a word 
meaning and its function. 
 
1. Introduction 
This is the first attempt to design a morphological 
analyzer to automatically analyze the 
morphological structures of Chinese compound 
words1. Morphological structures of compound 
words contain the essential information of 
knowing their syntactic and semantic 
characteristics. In particular, morphological 
analysis is a primary step for predicting the 
syntactic and semantic categories of 
out-of-vocabulary (unknown) words. The 
existence of unknown words is a major obstacle in 
Chinese natural language processing. Due to the 
                                                 
1 Compound words here include compounds in 
traditional Chinese linguistics and morphological 
complex words. 
fact that new words are easily coined by 
morphemes in Chinese text, the number of 
unknown words is increasingly large. As a result, 
we cannot collect all the unknown words and 
manually mark their syntactic categories and 
meanings. Our hypothesis to predict the category 
and the meaning of a word is basically based on 
Frege?s principle: ?The meaning of the whole is a 
function of the meanings of the parts?. The 
meanings of morphemes are supposed to make up 
the meanings of the words. However, some words 
like idioms and proper nouns cannot be included 
in the principle. In general, unknown words could 
be divided into two different types: the type that 
has the property of semantic transparency, i.e. the 
words whose meanings can be derived from their 
morphemes and the type without meaning 
transparency, such as proper nouns. In this paper 
we are dealing with the compound words with 
semantic transparency only. For the type of 
compounds without semantic transparency, such 
as proper nouns, their morphemes and 
morphological structures do not provide useful 
information for predicting their syntactic and 
semantic categories. Therefore they are processed 
differently and independently. In addition, some 
regular types of compounds, such as numbers, 
dates, and determinant-measure compounds, are 
easily analyzed by matching their morphological 
structures with their regular expression grammars 
and the result can be used to predict their syntactic 
and semantic properties, so they will be handled 
by matching regular expressions at the stage of 
word segmentation. According to our observation, 
most Chinese compounds have semantic 
transparency except proper nouns, which means 
the meaning of an unknown word can be 
interpreted by their own morpheme components. 
The design of our morphological analyzer will 
focus on processing these compounds, but words 
without semantic transparency are excluded. It 
takes a compound word as input and produces the 
morphological structure of the word. The major 
functions are 1) to segment a word into a sequence 
of morphemes, 2) to tag the part-of-speech of 
those morphemes, and 3) to identify the 
 morpho-syntactic relation between morphemes. 
Once the morpho-syntactic structure of a 
compound is known, the head morpheme provides 
the major clue for determining its syntactic and 
semantic category. 
It seems that a Chinese morphological analyzer is 
similar to a tagging program. Indeed both systems 
have to resolve the segmentation and tagging 
ambiguities. However the major difference is that 
the morphological analyzer does not have 
contextual information of each target word. In 
other words, morphological structures of 
compounds are context independent. We cannot 
apply the same methods, such as n-gram language 
models, to resolve the ambiguities. We proposed a 
method of using the associative strength among 
morphemes, morpho-syntactic patterns, and 
syntactic categories to solve the ambiguities. 
Detail algorithms for morpheme segmentation, 
part-of-speech, and morpho-syntactic relation 
assignment are discussed in Section 2. In the final 
section, we will evaluate the morphological 
analyzer by comparing its results with those 
obtained from the analyses of 5 linguists and 
discussing the categorization of errors found. 
 
2. The Morphological Analyzer 
The morphological analyzer contains three 
functions: to segment a word, to tag the 
part-of-speech (POS) of morphemes, and to 
identify the relation between them.  
2.1 Segmentation  
The goal of this process is to segment a compound 
word into a sequence of morphemes. Since there 
are ambiguous segmentations, simple dictionary 
look-up methods may not work well. For instance, 
the compound of meiguoren (???) could be 
ambiguously segmented into either mei-guoren 
([?[??]] beautiful countryman) or meiguo-ren 
([[ ? ? ] ? ] American people), but only 
meiguo-ren ([[??]?] American-people) is the 
proper segmentation. The left-to-right longest 
matching method is commonly applied to segment 
either words or text. It works well, but there are 
still some small percent of compound words that 
cannot be properly segmented by such a simple 
algorithm. For instance, the word xin-shenghuo 
([?[??]] new life) will be segmented wrongly 
into xinsheng-huo ([[??]?] the life of a new 
student) without considering the priority of 
segmenting the affix xin (?  new) first. In 
particular, words with multi-syllabic suffixes and 
words with reduplication constructions commonly 
cause segmentation errors. Those special types of 
words should be analyzed with other methods. 
 
2.1.1 Affixes and reduplication  
In order to remedy the segmentation error caused 
by the left-to-right longest matching, we observe 
the results of the algorithm and find that there are 
two useful clues to avoid segmentation errors, i.e. 
the information of affix and reduplication.  
A word of a reduplication pattern cannot and need 
not be segmented by the longest left-to-right 
method, since it has special morphological 
structures and the reduplication patterns bring 
enough clues of knowing the syntactic functions 
of the word. Therefore we try to identify words 
that belong to reduplication patterns first. In 
general they fall into the following two types of 
patterns: reduplications and parallel words. Words, 
which do not conform to these patterns, will be 
segmented later.  
Table 1 Special types of patterns and their examples 
Patterns Pattern Maker Note and examples 
Reduplication 
word 
AA,  
AAA, ABB, AAB, 
AxA, ABAB 
AABB, AxAy, 
xByB, 
liang-liang (??), dui-dui-dui 
(???), song-kua-kua (???), 
chi-chi-kan (???),  
xiang-yi-xiang (???). 
yan-jiu-yan-jiu(????),  
chi-chi-he-he(????), 
pao-shang-pao-xia(????),  
yi-nian-zai-nian(????) 
Parallel word A-BC (AC, BC)  zhong-xiao-xue (???) 
 
Reduplication means to duplicate the one or two 
character words into multi-character words. All 
reduplication patterns we used are listed in Table 1. 
If a word belongs to a reduplication pattern, the 
meaning of the word doesn?t change too much. 
The reduplication word?s category can be 
predicted by their patterns. For example, when B 
is not a noun, a word which belongs to the pattern 
AAB is intransitive verb. The category of a word 
that belongs to the pattern of ?parallel word? is 
always a noun. The characteristic of parallel 
words is that both AC and BC are words with 
shared head word C.  
At the next step of the morpheme segmentation, 
we will consider the compounds with affixes. The 
most productive compound construction is the 
structure of a morpheme plus an affix. Hence after 
the affix is identified, it would be easier to 
segment a word into two parts. The segmentation 
algorithm works as follows. A word is segmented 
immediately only if a prefix, infix or suffix 
morpheme is found. The affix table contains 186 
 prefixes, 2 infixes and 648 suffixes. Some affixes 
of the table are multi-syllabic. To segment an affix 
with higher priority will resolve most of the errors 
caused by the left-to-right longest matching 
algorithm. For instance, if tiaoshangqu (??? 
to jump up) is segmented by the left-to-right 
longest matching method, and the result of the 
segmentation is tiaoshang-qu. The left-to-right 
longest matching method might cause error 
segmentation here. However, shangqu is one of 
the suffixes in the affix table, so in our 
morphological analyzer it would be segmented as 
tiao-shangqu. A word containing an infix is also 
not suitable for the general segmentation and it 
would be segmented into single character. There 
are some affixes examples in Table 2: 
Table2 Types of affixes and their examples 
Types of affix Morpheme Examples 
Prefix xin(?) xinsheng-huo (???) 
Infix de(?) suan-de-shang(???) 
Suffix ju(?) 
shangqu (??) 
feizao-ju(???) 
tiao-shangqu (???) 
 
2.1.2 Left-to-right longest matching 
If a word is neither reduplication nor a compound 
with an affix, it should be segmented from left to 
right with longest matching. This general method 
can segment words into morphemes and also 
provide a possible part-of-speech of each 
morpheme by looking it up in the morpheme 
dictionary. 
 
2.2 Tagging  
The work here is to provide the part-of-speech for 
each morpheme and identify a morpho-syntactic 
relation between two morphemes based on the 
information of segmentation and their pos. This is 
the most difficult part of morphological analysis. 
In achieving the goal, we face two obstacles: the 
information insufficiency of morpheme categories 
and morphemes with the multiple categories. 
Since morpheme categories are not the same as 
word categories, it is necessary to assign each 
morpheme with appropriate categories and to 
compile a morpheme dictionary. Once the 
morpheme dictionary is built, the remaining job is 
to resolve the part-of-speech ambiguities of each 
morpheme. Since the part-of-speech of the 
morpheme is independent of its word level context, 
we cannot apply n-gram like language models to 
resolve part-of-speech ambiguity of morphemes. 
Even worse, there is no structure tagging training 
data available either. An EM-like unsupervised 
training on part-of-speech morphological 
structures is also not a sensible solution, since 
morpho-syntactic structure is more sensitive to the 
semantic combination than the syntactic 
combination of morphemes. Therefore we propose 
a method of using morphemes to predict the 
possible syntactic categories of the target 
compound word and selecting the most probable 
consistent result among the candidates of 
part-of-speech structures and the predicted 
categories.  
2.2.1 Preparation of the morpheme 
dictionary 
Before we start to tag morphemes, two steps are 
carried out to resolve the obstacles. That is the 
lack for a morpheme dictionary and morpheme 
ambiguity. First, in order to resolve the lack for 
morpheme categories, it is necessary to edit an 
affix table, as mentioned in Section 2.1, which 
contains prefixes, infixes, suffixes and their 
categories. Most frequently encountered 186 
prefixes and 648 suffixes are listed in this table. 
Basically, if its morpheme has more than 2 
characters, we adopt its categories in the CKIP 
Dictionary. Conversely, if it has fewer than 2 
characters and it functions as a prefix or a suffix, 
we use the categories in the affix table.  
Below we illustrate two examples to explain the 
need of morpheme categories. Both words yu (? 
to speak) and wu (?  to dance) are verbs. 
However they could also function as morphemes. 
When they function as morphemes, they are listed 
as nouns in their category. It is worth noticing that 
the categories of a morpheme are not the same as 
those of a word, even if they are in the same form. 
Therefore, it is important to assign morphemes 
categories properly. 
Table 3 The categories of ?yu and ?wu as a suffix and as a word and 
their examples 
Suffix Category 2  as a 
suffix 
Category as a 
word 
Example 
-yu(?) Na VE ying-yu(??),  
de-yu(??) 
-wu(?) Na VC, Na jueshi-wu(???) 
                                                 
2 The category symbols here are based on CKIP(1993). 
The meaning of each category we adopt here is as 
following: A(non-predicative adjective), Na(common 
noun), Nb(proper noun), Nc(location noun), Nd(time 
noun), VA(active intransitive verb), VB(semi-transitive 
verb), VC(active transitive verb), VCL(active transitive 
verb with locative object), VD(ditransitive verb), 
VE(active transitive verb with sentential object), 
VG(classificatory verb), VH(stative intransitive verb), 
VHC(stative causative verb), VJ(stative transitive verb) 
and so on. 
 Second, in order to resolve the problem of 
morpheme ambiguity, we need a list of 
probabilities which contains all the possible 
combinations of categorical rules and their 
probabilities. For instance, in the list the 
probability P(Na+Na|Na) = 0.4692 means that the 
categorical rule of combining two common nouns 
(Na+Na) to form a common noun (Na) has the 
probability 0.4692. The probability values of each 
categorical rule were estimated from the set of 
11,322 unknown words extracted from Sinica 
Corpus 3.0. The syntactic category of each 
extracted word is known but its structure 
annotation is unknown. Therefore the probability 
of each categorical rule is roughly estimated by 
assuming that every ambiguous categorical 
combination of a word have equal probability. 
The process of computing the possibility of a 
combination is as follows: 
1) We assign morphemes in a word with all their 
possible categories found in the dictionary and the 
affix table; for example, sheyingzhan ([[??]?] 
photography exhibition), which belongs to Na 
category means ?photography exhibition?. 
Sheyingzhan could be segmented as sheying (?? 
photography) and zhan (?  exhibition). After 
segmentation, we found sheying with the 
categories Na and VA, and zhan with the category 
Na. The possible combinations of sheying-zhan 
are ?Na+Na->Na? and ?VA+Na->Na?. However, 
we don?t know which one is correct, so we 
presumably assign a frequency of 0.5 to each 
combination. 
2) After we assign morphemes their categories and 
frequencies, we add up the frequencies of 
identical combinations. A list containing possible 
categorical rules and their probabilities is then 
established. Table 4 shows a part of the 
categorical rules of VHC. 
Table 4 A partial list of categorical rules and their probabilities 
Rule Category Probabilities 
Na+VHC VHC 0.4494 
VH+VHC VHC 0.2303 
Nc+VHC VHC 0.0674 
VHC+VHC VHC 0.0449 
VA+VH VHC 0.0280 
VC+VHC VHC 0.0224 
VJ+VH VHC 0.0224 
Nd+VHC VHC 0.0112 
VC+Na VHC 0.0112 
VC+VC VHC 0.0112 
VC+VHC VHC 0.0112 
 
2.2.2 Part-of-speech 
Once the affix table and the list of categorical 
rules are prepared, we can tackle the problems of 
the obstacles we mentioned in the beginning. 
After morpheme segmentation, each morpheme is 
assigned with their proper categories according to 
the morpheme dictionary and the affix table. 
However, morphemes might be ambiguous, so if 
the category of the target word is known, the most 
probable part-of-speech combination is chosen 
based on the list of categorical rules. However in 
the real implementation, it is assumed that the 
syntactic category of a target word is not known. 
The method mentioned above would not work, 
unless its syntactic category can be predicted. In 
our implementation, we adopted the method 
proposed by Chen, Bai and Chen (1997), by using 
the association strength between morphemes and 
categories to predict the syntactic categories of 
target words. By using the mutual information 
between affixes and categories, the top one 
prediction has the accuracy of 67.00% and the top 
three accuracy of the prediction can reach about 
94.02%. We will then check the consistency 
between predicted the categories and their 
morpho-syntactic structures to make the final 
judgments on both the word category prediction 
and the morpheme category disambiguation.  
The final prediction is based on the maximal value 
of the combined probabilities of the category 
prediction and the categorical rule prediction. 
Since P(Rule|compound) = P(Cat|compound) * 
P(Rule|Cat, compound) ? P(Cat|compound) * 
P(Rule|Cat), we try to find Cat and Rule which 
maximizes P(Cati|compound) * P(Rulej|Cati), for 
all Cati and Rulej. The following is an example 
of .she-ying-zhan. 
 
==================================== 
sheying-zhan (??? photography exhibition) 
 
P(Na|sheying-zhan) *P(Na+Na|Na)= 0.6324*0.4692=0.2967  
---max 
P(Na|sheying-zhan) * P(VA+Na|Na)=0.6324 *0.0865=0.0547 
P(VC|sheying-zhan)* P(Na+Na|VC)=0.3675* 0.0069=0.0025 
P(VC|sheying-zhan)*P(VA+Na|VC)= 0.3675* 0.001=0.0003 
 
sheying-zhan=(Na+Na)->Na 
==================================== 
 
The top1 accuracy of the original category 
prediction for unknown words is 67% by mutual 
information, but after the combination of the 
morphological analyzer, the accuracy of the word 
category prediction is raised to 71%. This is 
because the morphological analyzer will check if 
the categorical combination in a word is in its 
proper category. Therefore, when the original 
unknown word prediction system predicts a word 
in a category which the morphological analyzer 
 finds the probability of its categorical combination 
in the category low, the morphological analyzer 
might reject the category and suggest the 
unknown word prediction system to choose the 
next highest-scoring category in which the 
categorical combination has higher probability. 
In the case that the syntactic category of the 
compound word is known, we will let 
P(Cat|compound) = 1 and the most probable 
part-of-speech combination will simply be the 
categorical rule Rulej such that P(Rulej|Cat) is 
maximized. 
 
2.2.3 Morpho-syntactic relation between 
morphemes 
Once the information of segmentation and 
part-of-speech is ready, the morpho-syntactic 
relation between morphemes can be identified. 
According to Chao (1968) and Li&Thompson 
(1981), there are relations between morphemes in 
compounds such as ?modifier-hear?, ?predicate 
object? and so on. The purpose of knowing 
morpho-syntactic relation between morphemes is 
to help decide the meaning of the target word. The 
morpho-syntactic relation between morphemes is 
grouped into the types listed in Table 5. Generally, 
the relation between morphemes is highly related 
to the category of an unknown word. So the 
relation we assign to morphemes must be based 
on the category of the word. When the unknown 
word is a noun, the relation between its 
morphemes is ?modifier-head?. If it?s a verb, it 
will be more complicated. There are five relation 
types in verbs. The first one is ?verb-object?, such 
as chifan (?? to eat rice).  The first morpheme 
must be a transitive one and the second one should 
be a noun. The second type of the relation is 
?modifier-head?, and it means the second 
morpheme is the semantic head of the word. The 
third type is ?resultative verb?. The second 
morpheme in this type?s word always expresses 
the result of the action. The forth type is 
?head-suffix?. The appearance of the suffix 
changes the augment structure of the head verb, 
but the representing event remains the same. 
These suffixes are ru (? to be similar to), yu (? 
by), wei (? to become), gei (? to give), chu (?
to exit) and cheng (? to become). The fifth type 
of the relation is ?modifier-head?, and there is 
only a morpheme hua (? to transform) which 
belongs to this type. Hua is the head of a word. If 
a non-predicative adjective is an adjective, there 
are two kinds of structure. First, a 
non-predicative adjective has the same structure 
as a noun. The relation between its morphemes is 
also called ?modifier-head?. Second, the relation 
between morphemes for a non-predicative 
adjective which cannot be in the predicate 
position but has verbs structures can be 
?predicate-object? or ?modifier head?. This 
information will be helpful for predicting the core 
meaning of a new word. 
Table 5 The morpho-syntactic relation between morphemes 
 The morpho-syntactic relation between 
morphemes 
Noun Modifier-head 
Verb Verb-object 
Modifier-head 
Resultative Verb (RVC) 
Head-suffix 
Modifier-head(suffix) 
Adjective An: Modifier-head 
Av: verb-object, and modifier-head 
Other directional RVC and reduplication 
 
Once the morpho-syntactic structure of a 
compound is identified, the head morpheme 
provides the major clue for determining its 
syntactic and semantic category. The compound 
word will inherit from the semantic and syntactic 
property of its head and the information will be 
beneficial for the semantic and syntactic 
categorization of new compound words in the 
future.  
 
3. Evaluation and Discussion 
The major functions of the morphological 
analyzer are to segment a word into a sequence of 
morphemes, to tag the part-of-speech of the 
morphemes, and to identify the morpho-syntactic 
relation between morphemes. The work in this 
section is to evaluate the quality of the word 
information which is processed by each function 
of the morphological analyzer. However, it is hard 
to evaluate the accuracy of the morphological 
analyzer automatically, so we compare the results 
generated by the morphological analyzer with 
results generated by human experts, which are 
made out of their language intuition. The answers 
agreed by the majority of the human experts are 
assumed to be the right answers. The closer the 
results of the morphological analyzer are to the 
human experts, the more accurate the 
morphological analyzer is.  
The testing data is the set of unknown words 
extracted from the recently collected text by the 
system of Ma, Hsieh, Yang and Chen (2001). 
There is total 4,566 unknown words in our testing 
data. However, the validity of the morphological 
information is still uncertain; therefore five 
 linguistic specialists have to manually verify the 
morphological structure of unknown words by 
filling out the survey. First, we randomly select 
100 words as a testing set and the following three 
questions are answered by these five specialists. 
 
1) What's the category of the unknown word? 
2) What are the morpheme segmentations of 
the testing words? 
3) What is the syntactic tag of each morpheme? 
 
The definition of our "standard answer" is the 
answer the majority of the subjects give. For 
example, if three out of the five subjects consider 
the category of an unknown word X as VG, the 
standard answer of X would be VG. If five 
subjects think unknown word X belongs to five 
different categories, we would ask one more 
language specialist for opinions to determine the 
category of this unknown word. The standard 
answer we obtained from this survey will be the 
standard answer of the morphological analyzer.  
The morphological analyzer contains three 
functions: to segment a word into morphemes, to 
tag pos, and to identify the relation between 
morphemes. The accuracy we mention here is the 
result from comparing the morphological result 
with the majority answer. 
================================================= 
T=the total number of test set 
R=the total number of being the same with the ?Standard answer? of 
X 
X=Subject1, Subject2, Subject3, Subject4, Subject5, Morphological 
Analyzer 
Accuracy(X)= R(X)/T 
================================================= 
Table 6 The accuracy of five subjects and morphological analyzer 
(MA) 
 S 1 S 2 S 3 S 4 S 5 Average of 5 Ss MA 
Accuracy 89% 94% 94% 86% 83% 89.2% 81% 
 
 
After comparing the result of the morphological 
analyzer with the standard answers obtained from 
the five linguists, we come to the conclusion that 
the accuracy of the morphological analyzer is 81%. 
Out of all the errors, 5% is caused by 
segmentation on proper nouns and loanwords, 
such as bilinshan (??? a name of a mountain), 
dingwan (?? a name of a place), yanyou (?? 
a name of a dynasty), maniuda (??? a name of 
a place), and hongburang (??? home run). 
These words cannot be segmented because they 
only make sense when they are treated as a unit. 
The remaining 14% is caused in the tagging 
process produced by a morpheme table which 
lacks in accuracy. For example, in some cases the 
suffix zhou (? week) is supposed to be listed as 
Nd but is instead listed as Nc. Next, there are no 
proper categories for certain morphemes, such as 
the morpheme lie (? to list as a verb, a row as a 
noun) in the word qinglie (??). In the suffix 
table, the category of lie is only Na, but the 
morpheme lie should have a VC category when 
the meaning of qinglie "to list completely" is 
adopted. Another possible error-causing factor 
would be the choice made by following the 
combination rule. When there is more than one 
possible combination, errors might appear. For 
example, there are two possible combination for 
waizhan (?? to stretch out), ?Ng+Na? and 
?Ng+VC?. Comparing the score of the two 
combinations, the combination of ?Ng+Na? is 
chosen. However, it is not the correct category of 
zhan (? to stretch).  
The best way to resolve these problems mentioned 
above is to revise the morpheme table more often. 
Since the category of the suffix and prefix is fixed, 
it might cause a reduction in morpheme ambiguity. 
We are also interested in the similarity (or the 
range of agreement with language intuition of 
each individual) between those subjects. Since the 
standard answers are the answer of the majority, 
we can compare the standard answer with each 
individual. The average rate of the similarity rate 
is 89.2%. The ten-percentage puzzle might be due 
to the ambiguity of the word and can be 
interpreted that there are indeed some words that 
are not only difficult for a machine to analyze but 
also difficult for human beings to categorize.  
 
Table 7 The error rate and examples 
 Percent-age Examples 
hongburang(???), maniuda(??
?) 
Segmentation 
Error 
5%  
yanyou(??), dingwan(??), 
bilinshan (???) 
Tagging Error 14% mihou(??) tao(?)(Nc), 
zixun(??) zhou(?)(Nc), 
wai(?) zhan(?)(Na,VC) 
 
The evaluation of the identification of the 
morpho-syntactic relation is separated from the 
evaluation of segmentation and tagging, because 
the relation between morphemes is identified 
based on previous information, such as the 
category of a word, segmentation, and the pos. 
Once the essential information is clear, the 
morpho-syntactic relation is known. Nine out of a 
hundred examples are marked by linguists as 
errors of the morpho-syntactic identifier. 
 Furthermore, the reasons causing the error of 
relation identification are 1) the category 
predication?s error, 2) the part-of-speech error, and 
3) the lack of the relation type. Firstly, since the 
relation identifier is based on the result of the 
segmentation and pos, it is understandable that the 
error here is caused by previous functions. The 
category of qipai-jia ([[??]?] initial bidding 
price) is Na, but the system predicts it as an 
intransitive verb VA. So the identifier guesses the 
relation between qipai and jia as ?verb-object? 
based on the previous information. The error of 
the category prediction system might result in 
errors of the relation identifier. Secondly, the 
relation of qing-lie (?? to clearly list) should be 
?modifier-head?, but the identifier marks it as 
?verb-object? relation because lie(?) is tagged as 
Na. When the suffix is a Na, the prefix is a verb, 
and since the category of qing-lie is predicted as a 
verb, the identifier can only predict the relation of 
qing-lie as ?Verb-object?. Therefore, the error of 
part-of-speech might cause the identifier errors. 
Thirdly, the linguists suggest the relation between 
morphemes in nian-song (??  to read) is 
?conjunction relation?. That means that both the 
semantic meaning and syntactic function of nian 
(? to read) and song (? to read) are the same. 
However, we don?t have the ?conjunction 
relation?, because we think the number of words 
which belong to the kind of the relation is very 
limited, and since both morphemes the bring same 
information, there is no difference that enables us 
to mark both of them as heads or only one of them 
as a head for the application of predicating the 
semantic and syntactic property of a word. 
Therefore, in the morphological analyzer the 
words which belong to the ?conjunction? relation 
are identified as ?head-final? relations. 
 
4. Conclusion and future work 
This is a pilot study to design a morphological 
analyzer to analyze the morphological structures 
of Chinese compound words automatically. The 
major functions are 1) to segment a word into a 
sequence of morphemes, 2) to tag the 
part-of-speech of those morphemes, and 3) to 
identify the morpho-syntactic relation between 
morphemes. We evaluate the morphological 
analyzer by comparing 5 linguists? research results 
and discuss the type of errors we find. The more 
similar the results of the morphological analyzer 
compared with the human results, the better the 
morphological analyzer is. It is found that the 
accuracy of our analyzer is 81%. In comparison 
with the performance of human experts resulting 
in an accuracy of 89%, the performance of the 
current morphological analyzer is not bad, but still 
has room for improvement. More, the types and 
the identification of relations of morphemes still 
have much room to be improved. It is also worth 
noticing that the syntactic category prediction for 
general compounds can also be improved by the 
morphological analyzer. Once the internal 
information of a compound is known, it can 
provide clues for prediction of a word meaning 
and its function. The prediction of a word?s 
meaning is very hard and will be one of the main 
themes in our future researches. 
 
5 Reference 
Bosch, Antal van den, Walter Daelemans and Ton 
Weijters. (1996) Morphological Analysis 
Classification: an Inductive-Learning Approach. 
NeMLaP. 
Chao, Yuen Ren. (1968) A grammar of spoken Chinese. 
Berkeley:University of California Press. 
Chen, Chao-jan, Ming-hung Bai and Keh-jiann Chen. 
(1997) Category Guessing for Chinese Unknown 
Words. Proceedings of the Natural Language 
Processing Pacific Rim Symposium 1997, 35-40. 
Chen Yun-chai. (2001) Corpus Analysis of 
Reduplication in Mandarin Chinese. National 
Kaohsiung Normal University: English Department.  
CKIP. (1993) Technical Report no. 93-05: The analysis 
of Chinese category. [??????] CKIP:Nankang 
Creutz, Mathias and Krista Lagus. (2002) 
Unsupervised Discovery of Morphemes. Proceedings 
of Morphological and Phonological Learning 
Workshop of ACL'02. 
Beaney, Michael.(editor) (1997) The Frege Reader. 
Oxfort: Blackwell. 
Li, Charles and Sandra A. Thompson. (1981) Mandarin 
Chinese. Berkeley: University of California Press. 
Ma, Weiyun, Youming Hsieh, Changhua Yang, and 
Keh-jiann Chen. (2001) ?Chinese Corpus 
Development and Management System ? [????
??????????]. Proceedings of Research 
on Computational Linguistics Conference XIV, 
175-191. 
 
A Bottom-up Merging Algorithm for Chinese 
Unknown Word Extraction 
Wei-Yun Ma 
Institute of Information science,  
Academia Sinica 
ma@iis.sinica.edu.tw 
Keh-Jiann Chen 
Institute of Information science,  
Academia Sinica 
kchen@iis.sinica.edu.tw 
 
 
 
 
 
 
 
Abstract 
Statistical methods for extracting Chinese 
unknown words usually suffer a problem 
that superfluous character strings with 
strong statistical associations are extracted 
as well. To solve this problem, this paper 
proposes to use a set of general morpho-
logical rules to broaden the coverage and 
on the other hand, the rules are appended 
with different linguistic and statistical 
constraints to increase the precision of the 
representation. To disambiguate rule ap-
plications and reduce the complexity of 
the rule matching, a bottom-up merging 
algorithm for extraction is proposed, 
which merges possible morphemes recur-
sively by consulting above the general 
rules and dynamically decides which rule 
should be applied first according to the 
priorities of the rules. Effects of different 
priority strategies are compared in our ex-
periment, and experimental results show 
that the performance of proposed method 
is very promising. 
1 Introduction and Related Work 
Chinese sentences are strings of characters with no 
delimiters to mark word boundaries. Therefore the 
initial step for Chinese processing is word 
segmentation. However, occurrences of unknown 
words, which do not listed in the dictionary, 
degraded significantly the performances of most 
word segmentation methods, so unknown word 
extraction became a key technology for Chinese 
segmentation. 
For unknown words with more regular 
morphological structures, such as personal names, 
morphological rules are commonly used for 
improving the performance by restricting the 
structures of extracted words (Chen et. al 1994, 
Sun et. al 1994, Lin et. al 1994). However, it's not 
possible to list  morphological rules for all kinds of 
unknown words, especially those words with very 
irregular structures, which have the characteristics 
of variable lengths and flexible morphological 
structures, such as proper names, abbreviations etc. 
Therefore, statistical approaches usually play 
major roles on irregular unknown word extraction 
in most previous work (Sproat & Shih 1990, 
Chiang et. al 1992, Tung and Lee 1995, Palmer 
1997, Chang et. al 1997, Sun et. al 1998, Ge et. al 
1999). 
For statistical methods, an important issue is 
how to resolve competing ambiguous extractions 
which might include erroneous extractions of 
phrases or partial phrases. They might have 
statistical significance in a corpus as well. Very 
frequently superfluous character strings with 
strong statistic associations are extracted. These 
wrong results are usually hard to be filtered out 
unless deep content and context analyses were 
performed. To solve this problem, the idea of 
unknown word detection procedure prior to 
extraction is proposed. Lin et al (1993) adopt the 
following strategy: First, they decide whether there 
is any unknown word within a detected region with 
fix size in a sentence, and then they extract the 
unknown word from the region by a statistical 
method if the previous answer is "yes". A 
limitation of this method is that it restricts at most 
one unknown word occurs in the detected region, 
so that it could not deal with occurrences of 
consecutive unknown words within a sentence. 
Chen & Ma (2002) adopt another strategy: After an 
initial segmentation process, each monosyllable is 
decided whether it is a common word or a 
morpheme of unknown word by a set of syntactic 
discriminators. The syntactic discriminators are a 
set of syntactic patterns containing monosyllabic, 
words which are learned from a large word 
segmented corpus, to discriminate between 
monosyllabic words and morphemes of unknown 
words. Then more deep analysis can be carried out 
at the detected unknown word morphemes to 
extract unknown words. 
In this paper, in order to avoid extractions of  
superfluous character strings with high frequencies, 
we proposed to use a set of general rules, which is 
formulated as a context free grammar rules of 
composing detected morphemes and their adjacent 
tokens, to match all kinds of unknown words, for 
instance which includes the rule of (UW  UW 
UW). To avoid too much superfluous extractions 
caused by the over general rules, rules are 
appended with linguistic or statistical constraints. 
To disambiguate between rule applications and 
reduce the complexity of the rule matching, a 
bottom-up merging algorithm for extraction is 
proposed, which merges possible morphemes 
recursively by consulting above general rules and 
dynamically decides which rule should be applied 
first according to the priorities of the rules. 
The paper is organized into 7 sections. In the 
next section, we provide an overview of our sys-
tem. Section 3 briefly introduce unknown word 
detection process and makes some analysis for 
helping the derivation of general rules for un-
known words. In section 4, we derive a set of gen-
eral rules to represent all kinds of unknown words, 
and then modify it by appending rules constraints 
and priorities. In section 5, a bottom-up merging 
algorithm is presented for unknown word extrac-
tion. In section 6, the evaluation of extraction is 
presented; we also compare the performances to 
different priority strategies. Finally, in section 7, 
we make the conclusion and propose some future 
works. 
2 System Overview 
The purpose to our unknown word extraction 
system is to online extract all types of unknown 
words from a Chinese text. Figure 1 illustrates the 
block diagram of the system proposed in this paper. 
Initially, the input sentence is segmented by a 
conventional word segmentation program. As a 
result, each unknown word in the sentence will be 
segmented into several adjacent tokens (known 
words or monosyllabic morphemes). At unknown 
word detection stage, every monosyllable is 
decided whether it is a word or an unknown word 
morpheme by a set of syntactic discriminators, 
which are learned from a corpus. Afterward, a 
bottom-up merging process applies the general 
rules to extract unknown word candidates. Finally, 
the input text is re-segmented by consulting the 
system dictionary and the extracted unknown word 
candidates to get the final segmented result. 
 
 
Figure 1. Flowchart of the system 
 
(1)                          
    if    can   increase   gross profit rate 
"if gross profit rate can be increased?" 
 
(2)   after first step word segmentation: 
         
 
                             
        after unknown word detection: 
         
 
               (?)     (?)    (?) 
        after unknown word extraction: 
         
 
                
                   
 
For example, the correct segmentation of (1) is 
shown, but the unknown word ? 	
 ? is 
segmented into three monosyllabic words after the 
first step of word segmentation process as shown 
in (2). The unknown word detection process will 
mark the sentence as ?
  ()   ()   ()   (?)  
(?)   (?)?, where (?) denotes the detected 
monosyllabic unknown word morpheme and () 
denotes common words. During extracting process, 
the rule matching process focuses on the 
morphemes marked with (?) only and tries to 
combine them with left/right neighbors according 
to the rules for unknown words. After that, the 
unknown word ?  ? is extracted. During the 
process, we do not need to take care of other 
superfluous combinations such as ?
 
 ? even 
though they might have strong statistical 
association or co-occurrence too. 
3 Analysis of Unknown Word Detection 
The unknown word detection method proposed by 
(Chen & Bai 1998) is applied in our system. It 
adopts a corpus-based learning algorithm to derive 
a set of syntactic discriminators, which are used to 
distinguish whether a monosyllable is a word or an 
unknown word morpheme after an initial 
segmentation process. If all occurrences of 
monosyllabic words are considered as morphemes 
of unknown words, the recall of the detection will 
be about 99%, but the precision is as low as 13.4%.  
The basic idea in (Chen & Bai 1998) is that the 
complementary problem of unknown word 
detection is the problem of monosyllabic known-
word detection, i.e. to remove the monosyllabic 
known-words as the candidates of unknown word 
morphemes. Chen and Bai (1998) adopt ten types 
of context rule patterns, as shown in table 1, to 
generate rule instances from a training corpus. The 
generated rule instances were checked for 
applicability and accuracy. Each rule contains a 
key token within curly brackets and its contextual 
tokens without brackets. For some rules there may 
be no contextual dependencies. The function of 
each rule means that in a sentence, if a character 
and its context match the key token and the 
contextual tokens of the rule respectively, this 
character is a common word (i.e. not a morpheme 
of unknown word). 
For instance, the rule ?{Dfa} Vh? says that a 
character with syntactic category Dfa is a common 
word, if it follows a word of syntactic category Vh. 
 
 
Rule type               Example 
================================= 
char   {   }  
word char     { } 
char word   {  }   
category   {T} 
{category} category  {Dfa} Vh 
category {category}  Na {Vcl} 
char category  {  } VH 
category char  Na {  } 
category category char Na Dfa { 	 } 
char category category  { 
 } Vh T 
================================= 
 
Table1. Rule types and Examples 
 
The final rule set contains 45839 rules and 
were used to detect unknown words in the ex-
periment. It achieves a detection rate of 96%, and a 
precision of 60%. Where detection rate 96% means 
that for 96% of unknown words in the testing data, 
at least one of its morphemes are detected as part 
of unknown word and the precision of 60% means 
that for 60% of detected monosyllables in the test-
ing data, are actually morphemes. Although the 
precision is not high, most of over-detecting errors 
are ?isolated?, which means there are few situa-
tions that two adjacent detected monosyllabic un-
known morphemes are both wrong at the mean 
time. These operative characteristics are very im-
portant for helping the design of general rules for 
unknown words later. 
4 Rules for Unknown Words 
Although morphological rules work well in regular 
unknown word extraction, it's difficult to induce 
morphological rules for irregular unknown words. 
In this section, we try to represent a common struc-
ture for unknown words from another point of 
view; an unknown word is regarded as the combi-
nation of morphemes which are consecutive mor-
phemes/words in context after segmentation, most 
of which are monosyllables. We adopt context free 
grammar (Chomsky 1956), which is the most 
commonly used generative grammar for modelling 
constituent structures, to express our unknown 
word structure. 
4.1 Rule Derivation 
According to the discussion in section 3, for 96% 
of unknown words, at least one of its morphemes 
are detected as part of unknown word, which 
motivates us to represent the unknown word 
structure with at least one detected morpheme. 
Taking this phenomenon into our consideration, 
the rules for modeling unknown words and an 
unknown word example are presented as follows. 
 
 
  UW     UW UW    (1) 
| ms(?) ms(?)    (2) 
| ms(?) ps()   (3) 
| ms(?) ms()  (4) 
| ps() ms(?)   (5) 
| ms() ms(?)   (6) 
| ms(?) UW   (7) 
| ms() UW (8) 
| ps() UW (9) 
| UW ms(?)  (10) 
| UW ms()   (11) 
| UW ps()   (12) 
 
Notes: There is one non-terminal symbol. ?UW? 
denotes ?unknown word? and is also the start symbol. 
There are three terminal symbols, which includes ms(?), 
which denotes the detected monosyllabic unknown 
word morpheme, ms() , which denotes the monosyllable 
that is not detected as the morpheme, and ps(), which 
denotes polysyllabic (more than one syllable) known 
word. 
 
 
Table 2. General rules for unknown words 
 
 
 
Figure 2. A possible structure for the unknown word 
?   ?(Chen Zhi Ming), which is 
segmented initially and detected as ?   (?) 
(?)  ()?, and ?  ? was marked incorrectly at 
detection stage. 
 
There are three kinds of commonly used meas-
ures applied to evaluate grammars: 1. generality 
(recall), the range of sentences the grammar ana-
lyzes correctly; 2. selectivity (precision), the range 
of non-sentences it identifies as problematic and 3. 
understandability, the simplicity of the grammar 
itself (Allen 1995). For generality, 96% unknown 
words have this kind of structure, so the grammar 
has high generality to generate unknown words. 
But for selectivity, our rules are over-generation. 
Many patterns accepted by the rules are not words. 
The main reason is that rules have to include non-
detected morphemes for high generality. Therefore 
selectivity is sacrificed momentary. In next section, 
rules would be constrained by linguistic and text-
based statistical constraints to compensate the se-
lectivity of the grammar. For understandability, 
you can find each rule in (1)-(12) consists of just 
two right-hand side symbols. The reason for using 
this kind of presentation is that it regards the un-
known word structure as a series of combinations 
of consecutive two morphemes, such that we could 
simplify the analysis of unknown word structure 
by only analyzing its combinations of consecutive 
two morphemes. 
4.2 Appending Constraints 
Since the general rules in table 2 have high 
generality and low selectivity to model unknown 
words, we append some constraints to restrict their 
applications. However, there are tradeoffs between 
generality and selectivity: higher selectivity 
usually results in lower generality. In order to keep 
high generality while assigning constraints, we 
assign different constraints on different rules 
according to their characteristics, such that it is 
only degraded generality slightly but selectivity 
being upgraded significantly. 
The rules in table 2 are classified into two kinds: 
one kind is the rules which both its right-hand side 
symbols consist of detected morphemes, i.e, (1), 
(2), (7), and (10), the others are the rules that just 
one of its right-hand side symbols consists of 
detected morphemes, i.e, (3), (4), (5), (6), (8), (9), 
(11), and (12). The former is regarded as ?strong? 
structure since they are considered to have more 
possibility to compose an unknown word or an 
unknown word morpheme and the latter is 
regarded as ?weak? structure, which means they 
are considered to have less possibility to compose 
an unknown word or an unknown word morpheme. 
The basic idea is to assign more constraint on those 
rules with weak structure and less constraint on 
those rules with strong structure. 
The constraints we applied include word length, 
linguistic and statistical constraints. For statistical 
constraints, since the target of our system is to 
extract unknown words from a text, we use text-
based statistical measure as the statistical 
constraint. It is well known that keywords often 
reoccur in a document (Church 2000) and very 
possible the keywords are also unknown words. 
Therefore the reoccurrence frequency within a 
document is adopted as the constraint. Another 
useful statistical phenomenon in a document is that 
a polysyllabic morpheme is very unlikely to be the 
morphemes of two different unknown words 
within the same text. Hence we restrict the rule 
with polysyllabic symbols by evaluating the 
conditional probability of polysyllabic symbols.  In 
addition, syntactic constraints are also utilized here. 
For most of unknown word morphemes, their 
syntactic categories belong to ?bound?, 
?verb?, ?noun?, and ?adjective? instead of 
?conjunction?, ?preposition??etc. So we restrict 
the rule with non-detected symbols by checking 
whether syntactic categories of its non-detected 
symbols belong to ?bound?, ?verb?, ?noun?, or 
?adjective?. To avoid unlimited recursive rule 
application, the length of matched unknown word 
is restricted unless very strong statistical 
association do occur between two matched tokens. 
The constraints adopted so far are presented in 
table 3. Rules might be restricted by multi-
constraints. 
 
Freqdocu(LR)>=Threshold (3) (4) (5) (6) (8) (9) (11) (12) 
Pdocu(L|R)=1 (1) (3) (7) (8) (9) (12) 
Pdocu(R|L)=1 (1) (5) (9) (10) (11) (12) 
Category(L) is bound, verb, 
noun or adjective (5) (6) (8) (9) 
Category(R) is bound, verb, 
noun or adjective (3) (4) (11) (12) 
 
Notes: L denotes left terminal of right-hand side 
            R denotes right terminal of right-hand side 
                  Threshold is a function of Length(LR) and text 
size. The basic idea is larger amount of length(LR) 
or text size matches larger amount of Threshold.       
 
 Table 3. Constraints for general rules 
4.3 Priority 
To scheduling and ranking ambiguous rule 
matching, each step of rule matching is associated 
with a measure of priority which is calculated by 
the association strength of right-hand side symbols. 
In our extracting algorithm, the priority measure is 
used to help extracting process dynamically decide 
which rule should be derived first. More detail 
discussion about ambiguity problem and complete 
disambiguation process are presented in section 5. 
We regard the possibility of a rule application as 
co-occurrence and association strength of its right-
hand side symbols within a text. In other words, a 
rule has higher priority of application while its 
right-hand side symbols are strongly associated 
with each other, or co-occur frequently in the same 
text. There have been many statistical measures 
which estimate co-occurrence and the degree of 
association in previous researches, such as mutual 
information (Church 1990, Sporat 1990), t-score 
(Church 1991), dice matrix (Smadja 1993, 1996). 
Here, we adopt four well-developed kinds of 
statistical measures as our priority individually: 
mutual information (MI), a variant of mutual 
information (VMI), t-score, and co-occurrence. 
The formulas are listed in table 4. MI mainly 
focuses on association strength, and VMI and t-
score consider both co-occurrence and association 
strength. The performances of these four measures 
are evaluated in our experiments discussed in 
section 6. 
 
==================================== 
),(),( RLfRLoccurenceco =?   
------------------------------------------------------------- 
)()(
),(log),(
RPLP
RLPRLMI =  
------------------------------------------------------------- 
),(),(),( RLMIRLfRLVMI =  
------------------------------------------------------------- 
),(
)()(),(
),(
RLf
N
RfLfRLf
RLscoret
?
=?
  
 
Notes: f(L,R) denotes the number of occurrences of L,R in the 
text; N denotes the number of occurrences of all the 
tokens in the text; length(*) denotes the length of *. 
==================================== 
 
Table 4. Formulas of 4 kinds of priority 
5 Unknown Word Extraction 
5.1 Ambiguity 
Even though the general rules are appended with 
well-designed constraints, ambiguous matchings, 
such as, overlapping and covering, are still existing. 
We take the following instance to illustrate that: 
?   ? (La Fa Yeh), a warship name, occurs 
frequently in the text and is segmented and 
detected as ?   (?)  (?)  (?)?. Although ?  
 ? could be derived as an unknown word ?((   )
 )? by rule 2 and rule 10, ?   ? and ?  ? 
might be also derived as unknown words ?(   )? 
and ?(  )? individually by the rule 2. Hence 
there are total three possible ambiguous unknown 
words and only one is actually correct. 
Several approaches on unsupervised segmenta-
tion of Chinese words were proposed to solve 
overlapping ambiguity to determine whether to 
group ?xyz? as ?xy z? or ?x yz?, where x, y, and z 
are Chinese characters. Sproat and Shih (1990) 
adopt a greedy algorithm: group the pair of adja-
cent characters with largest mutual information 
greater than some threshold within a sentence, and 
the algorithm is applied recursively to the rest of 
the sentence until no character pair satisfies the 
threshold. Sun et al (1998) use various association 
measures such as t-score besides mutual informa-
tion to improve (Sproat & Shih 1990). They devel-
oped an efficient algorithm to solve overlapping 
character pair ambiguity. 
5.2 Bottom-up Merging Algorithm 
Following the greedy strategy of (Sproat & Shih 
1990), here we present an efficient bottom-up 
merging algorithm consulting the general rules to 
extract unknown words. The basic idea is that for a 
segmented sentence, if there are many rule-
matched token pairs which also satisfy the rule 
constraints, the token pair with the highest rule 
priority within the sentence is merged first and 
forms a new token string. Same procedure is then 
applied to the updated token string recursively 
until no token pair satisfied the general rules. It is 
illustrated by the following example: 
 
====================================== 
System environment:  
Co-occurrence priority is adopted. 
Text environment: 
                                  ?    ? (Chen Zhi Qiang), an unknown word, 
occurs three times. 
? 	
 ? (take an electing activity), an unknown 
word, occurs two times. 
?   	
 ? (Chen Zhi Qiang took an electing 
activity), a sentence, occurs one time. 
Input:   	
  
After initial segmentation and detection: 
               (?)   (?)   (?)  	 (?)  
 (?) 
 
                    3        3          1         2                   priority 
After first iteration: 
          (uw)  (?)   	 (?)  
 (?) 
 
3          1            2                    priority 
After second iteration: 
   (uw)  	 (?)  
 (?) 
 
                                               2                         priority 
After third iteration: 
                       (uw) 	
 (uw) 
===================================== 
 
Figure 3. Extraction process of input ?   	
 ?. 
 
By the general rules and greedy strategy, besides 
overlapping character pair ambiguity, the 
algorithm is able to deal with more complex 
overlapping and coverage ambiguity, even which 
result from consecutive unknown words. In finger 
3, input sentence ?  ? is derived as the 
correct two unknown words ?((  )  )? and ?( 
 )? by rule (2), rule (10), and rule (2) in turn. ? 
 ? and ?  ? are not further merged. That is 
because P(  |  )<1 violates the constraint 
of rule (1). Same reason explains why ?  ? 
and ?  ? do not satisfy rule (10) in the third 
iteration. 
By this simple algorithm, unknown words with 
unlimited length all have possibilities to be ex-
tracted. Observing the extraction process of ? Introduction to CKIP Chinese Word Segmentation System for the First 
 International Chinese Word Segmentation Bakeoff 
Wei-Yun Ma 
Institute of Information science,  
Academia Sinica 
ma@iis.sinica.edu.tw 
Keh-Jiann Chen 
Institute of Information science,  
Academia Sinica 
kchen@iis.sinica.edu.tw 
 
 
 
 
 
 
 
Abstract 
In this paper, we roughly described the 
procedures of our segmentation system, 
including the methods for resolving  seg-
mentation ambiguities and identifying un-
known words. The CKIP group of 
Academia Sinica participated in testing on 
open and closed tracks of Beijing Univer-
sity (PK) and Hong Kong Cityu (HK). 
The evaluation results show our system 
performs very well in either HK open 
track or HK closed track and just accept-
able in PK tracks. Some explanations and 
analysis are presented in this paper. 
1 Introduction 
At the first international Chinese Word 
Segmentation Bakeoff, Academia Sinica 
participated in testing on open and closed tracks of 
Beijing University (PK) and Hong Kong Cityu 
(HK). The same segmentation algorithm was 
applied to process these two corpora, except that 
character code conversion from GB to BIG5 for 
PK corpus and few modifications due to different 
segmentation standards had been made. The 
difference between open and closed tracks is that 
while processing the open track, besides of the 
lexicon trained from the specific corpus, we also 
consulted the Academia Sinica lexicon to enhance 
the word collection. 
    It is well known that there are two major 
difficulties in Chinese word segmentation. One is 
resolving the ambiguous segmentation, and the 
other is identifying unknown words. 
Our earlier work mainly focused on the 
resolving of segmentation ambiguities and using 
regular expressions to handle the determinant-
measure and reduplication compounds (Chen & 
Liu 1992, Chen 1999). We adopt a variation of the 
longest matching algorithm with several heuristic 
rules to resolve the ambiguities and achieve 
99.77% of the success rate without counting the 
mistakes occurred due to the existence of unknown 
words. After that, we were paying more attention 
on the problems of extracting and identifying 
unknown words (Chen et.al 1997, Chen & Bai 
1998, Chen & Ma 2002, Tseng & Chen 2002, Ma 
& Chen 2003). The process of unknown word 
extraction could be roughly divided into two steps, 
i.e. detection process and extraction process. The 
detection process detects possible occurrences of 
unknown words (Chen & Bai 1998), so that deeper 
morphological analysis is carried out only at the 
places where unknown word morphemes were 
detected (Chen & Ma 2002). A bottom-up merging 
algorithm was proposed in (Ma & Chen 2003), 
which utilizes hybrid statistical and linguistic 
information to extract unknown words effectively. 
In addition to the bakeoff results evaluated by 
SIGHAN, we also present some other relevant 
experiment results and provide analysis on the 
system performance in the following sections. 
2 System Overview 
Figure 1 illustrates the block diagram of our 
segmentation system used in this contest. The first 
two steps of word segmentation algorithm are 
word matching and resolution for ambiguous 
matches. These two processes were performed in 
parallel. The algorithm reads the input sentences 
from left to right and matches the input character 
string with lexemes. In (Chen & Liu 1992), if an 
ambiguous segmentation does occur, the matching 
algorithm looks ahead two more words, and the 
disambiguation rules for those three word chunks 
is applied afterward. For instance, in (1), the first 
matched word could be '
 
' or '
 
'. Then the 
algorithm will look ahead to take all of the possible 
combinations of three word chunks, as shown in 
(2), into consideration. 
 
 
 
Figure 1. Flowchart of the system 
 
 (1)                             	  
complete  authenticate  report 
"complete the report about authenticating" 
 
(2)                      
 

                  
 

                	  
 
   
The disambiguation algorithm will select the 
first word of the most plausible chunks as the 
solution according to heuristic rules. The first 
heuristic rule is: 
 
Longest Matching Rule: The most plausible seg-
mentation is the three word sequence with the 
longest length. 
In the above example, the longest matched 
three-word chunk is (1). Therefore the first seg-
mented word is '  '. This heuristic rules 
achieves as high as 99.69% accuracy and a high 
applicability of 93.21%, i.e. the 93.21% of the am-
biguities were resolved by this rule. However there 
are still about 6.79% of ambiguities, i.e. the three 
word chunks with the same length but with differ-
ent segmentations, which cannot be resolved by the 
maximal matching rule. The following heuristic 
rules were used for further resolution. 
 
Word Length Rule: Pick the three-word chunk 
that has the smallest standard deviation in length of 
the three words.  
Morphemic Rules: 
(a). Pick the chunk with fewer bound morphemes. 
(b). Pick the chunk with fewer characters in com-
pound words. 
Probability Rule: 
(a). Pick the chunk with the high frequency mono-
syllabic words. 
(b). Pick the chunk with the highest probability 
value. 
 
After disambiguation process, an input sentence 
is segmented into a word sequence. Then for the 
needs of the following unknown word extraction, a 
Pos bi-gram tagging model is applied to tag Pos of 
words. 
It is clear that unknown words in the input text 
will be segmented into several adjacent tokens 
(known words or monosyllabic morphemes). Then 
at unknown word detection stage, every 
monosyllable is decided whether it is a word or an 
unknown word morpheme by a set of syntactic 
discriminators, which are trained from a word 
segmented corpus. 
 
 
(3)                               
    if    can   increase   gross profit rate 
"if gross profit rate can be increased?" 
 
(4)   after first step word segmentation: 
                                        
        after unknown word detection: 
                          (?)     (?)    (?) 
        after unknown word extraction: 
                         	  
 
For example, the correct segmentation of (3) is 
shown, but the unknown word ? 
 ? is 
segmented into three monosyllabic words after the 
first step of word segmentation process. In (4), The 
unknown word detection process will mark the 
sentence as ?   ()   ()   ()   (?)   (?)   (?)?, 
where (?) denotes the detected monosyllabic 
unknown word morpheme and () denotes common 
words. During extracting process, the rule 
matching process focuses on the morphemes 
marked with (?) only and tries to combine them 
with left/right neighbors according to the rules for 
unknown words. After that the unknown word ? 
 ? is extracted. 
We adopt a bottom-up merging algorithm (Ma & 
Chen 2003), which utilizes hybrid statistical and 
linguistic information, to extract unknown words. 
3 Adaptation for Different Tracks 
It is known that different segmentation standards 
could affect the performance of segmentation 
significantly. In this contest, due to limited 
preparing time, we mainly focused on adjusting the 
regular expressions for determinant-measure 
compounds according to the HK and PK 
segmentation standards. 
While processing the PK track, a shortcut 
method of converting GB codes to BIG5 codes was 
adopted to cope with the problem of character 
coding difference. Instead of re-design or re-
implement the GB segmentation system, we 
convert the codes of training and testing PK 
corpora into BIG5 versions and perform the 
segmentation under the BIG5 environment. The 
segmented results are then translated back to GB 
code as the final outputs. In contrast, processing of 
HK corpus is easier for us, because our system was 
designed for the BIG5 environment. 
As for the lexicons, for closed test, both PK and 
HK lexicons are derived from the word sets of 
each respective training corpus. For the open test, 
each lexicon was enhanced by adding the lexical 
entries in the CKIP lexicon. The sizes of lexicons 
are shown in table1.  
 
 HK PK 
# of lexical entries (HK/PK)for 
closed test 
22K 50K 
# of lexical entries (HK/PK join 
CKIP) for open test 
140K 156
K 
 
Note: # lexicon of (CKIP) is 133K 
 
Table 1. The sizes of lexicons 
 
Syntactic categories of a words were utilized in 
unknown word detection and extraction processes. 
We don?t have syntactic categories for words 
which are not in the CKIP lexicon. Therefore, we 
(Chen et.al 1997, Tseng & Chen 2002) use 
association strength between morphemes and 
syntactic categories to predict the category of a 
new word. The accuracy rate is about 80%. 
4 Evaluation Results 
There are several evaluation indexes provided 
by SIGHAN, i.e. test recall (R), test precision (P), 
F score2, the out-of-vocabulary (OOV) rate for the 
test corpus, the recall on OOV words (Roov), and 
the recall on in-vocabulary (Riv) words.  
Tables 2 shows the evaluation results of our sys-
tem in HK closed and open tracks. For both tracks, 
our system achieved the top ranks on F scores. 
 
 R P F OOV Roov Riv 
Closed 0.947 0.934 0.940 0.071 0.625 0.972 
Open 0.958 0.954 0.956 0.071 0.788 0.971 
 
Note: The word count of testing corpus is 34955 
                     
Table 2. Scores for HK 
 
The evaluations of our system in PK closed and 
open tracks are shown in table 3. For PK closed 
track, our system ranks 6th among 10 systems. And 
for PK open track, our system ranks 3rd among 8 
systems. 
 
 R P F OOV Roov Riv 
Closed 0.939 0.934 0.936 0.069 0.642 0.961 
Open 0.939 0.938 0.938 0.069 0.675 0.959 
 
Note: The word count of testing corpus is 34955 
                     
Table 3. Scores for PK 
 
Because Academia Sinica corpora (AS) are 
provided by us, we are not allowed to participate 
any AS track at this contest. Therefore, in this 
report, we still show the performance of our 
system evaluating AS closed track in table 4. Our 
system would have the top rank if the result was 
compared with the other  6 participants of AS 
closed track. 
 
R P F OOV Roov Riv 
0.968 0.966 0.967 0.022 0.657 0.975 
 
Note: The word count of testing corpus is 11985 
 
Table 4. Scores for AS closed 
5 Discussions and Conclusions 
The evaluation results show that our system 
performs very well in either HK closed track or 
HK open track. We think the key to the success is 
our unknown word extraction performs better than 
other participants. This could be observed by the 
results of HK closed track, the 2th and 3th system, 
which have better performance in Riv but worse 
Roov than our system, performs worse than our 
system in f score. Furthermore to have better 
performance, high precision for unknown word 
extraction is necessary, since one identification 
error may cause at least two segmentation errors. 
The performance in PK tracks are not as well as 
HK. An important  reason is that coding 
conversion may cause errors. For instance, in the 
conversion of the GB code of ?   ? (the capital 
of Brazil) to its BIG5 codes, Since GB code to 
BIG5 conversion is a one-to-many mapping, the 
above example is wrongly converted to ?  ?. 
This kind of errors do affect accuracy of the 
segmentation significantly, especially for the 
unkown word processes. To solve this problem, we 
think the best and direct solution is to re-
implement the GB segmentation version without 
any code conversion.  
Variation on the word segmentation standards is 
another reason of causing segmentation errors. 
Some of the standards were even not available to 
the public. It is better to propose a uniform word 
segmentation standard in the future.  
Regarding evaluation index, we suggest that an 
error type of crossing error should be take into 
consideration, since noncrossing errors are more or 
less related to segmentation standards and crossing 
errors are more severe. 
 
6 References 
[1]  Chen, K.J. & S.H. Liu, 1992,"Word Identification 
for Mandarin Chinese Sentences," Proceedings of 
14th Coling, pp. 101-107 
[2]  Chen, C. J., M.H. Bai, & K.J. Chen, 1997,? Cate-
gory Guessing for Chinese Unknown Words,? Pro-
ceedings of the Natural Language Processing 
Pacific Rim Symposium, 35-40, Thailand. 
[3]  Chen, K.J. & Ming-Hong Bai, 1998, ?Unknown 
Word Detection for Chinese by a Corpus-based 
Learning Method,? international Journal of Com-
putational linguistics and Chinese Language 
Processing, Vol.3, #1, pp.27-44 
[4]  Chen, Keh-jiann,1999,?Lexical Analysis for Chi-
nese- Difficulties and Possible Solutions?, Journal 
of Chinese Institute of Engineers, Vol. 22. #5, pp. 
561-571. 
[5]   Chen, K.J. & Wei-Yun Ma, 2002. Unknown Word 
Extraction for Chinese Documents. In Proceedings 
of COLING 2002, pages 169-175 
[6]   Tseng, H.H. & K.J. Chen, 2002. Design of Chinese 
Morphological Analyzer. In Proceedings of 
SIGHAN, pages 49-55 
[7]  Ma Wei-Yun & K.J. Chen, 2003. A bottom-up 
Merging Algorithm for Chinese Unknown Word 
Extraction. In Proceedings of SIGHAN 
 
Automatic Semantic Role Assignment for a Tree Structure 
 
Jia-Ming You 
Institute of Information Science 
Academia Sinica 
swimming@hp.iis.sinica.edu.tw 
Keh-Jiann Chen 
Institute of Information Science 
Academia Sinica 
Kchen@iis.sinica.edu.tw 
 
Abstract 
  We present an automatic semantic roles labeling 
system for structured trees of Chinese sentences. It 
adopts dependency decision making and 
example-based approaches. The training data and 
extracted examples are from the Sinica Treebank, 
which is a Chinese Treebank with semantic role 
assigned for each constituent. It used 74 abstract 
semantic roles including thematic roles, such as 
?agent?; ?theme?, ?instrument?, and secondary roles of 
?location?, ?time?, ?manner? and roles for nominal 
modifiers. The design of role assignment algorithm is 
based on the different decision features, such as 
head-argument/modifier, case makers, sentence 
structures etc. It labels semantic roles of parsed 
sentences. Therefore the practical performance of the 
system depends on a good parser which labels the 
right structures of sentences. The system achieves 
92.71% accuracy in labeling the semantic roles for 
pre-structure- bracketed texts which is considerably 
higher than the simple method using probabilistic 
model of head-modifier relations. 
1. Introduction  
  For natural language understanding, the process of 
fine-grain semantic role assignment is one of the 
prominent steps, which provides semantic relations 
between constituents. The sense and sense relations 
between constituents are core meaning of a sentence. 
  Conventionally there are two kinds of methods 
for role assignments, one is using only statistical 
information (Gildea and Jurafsky, 2002) and the 
other is combining with grammar rules (Gildea and 
Hockenmaier, 2003). However using only grammar 
rules to assign semantic roles could lead to low 
coverage. On the other hand, performance of 
statistical methods relies on significant dependent 
features. Data driven is a suitable strategy for 
semantic roles assignments of general texts. We use 
the Sinica Treebank as information resource because 
of its various domains texts including politics, 
society, literature?etc and it is a Chinese Treebank 
with semantic role assigned for each constituent 
(Chen etc., 2003). It used 74 abstract semantic roles 
including thematic roles, such as ?agent?; ?theme?, 
?instrument?, and secondary roles of ?location?, 
?time?, ?manner? and modifiers of nouns, such as  
 
?quantifier?, ?predication?, ?possessor?, etc. The 
design of role assignment algorithm is based on 
the different decision features, such as 
head-argument/modifier, case makers, sentence 
structures etc. It labels semantic roles of parsed 
sentences by example-based probabilistic models. 
 
1.1 Sinica Treebank 
  The Sinica Treebank has been developed and 
released to public since 2000 by Chinese 
Knowledge Information Processing (CKIP) group 
at Academia Sinica. The Sinica Treebank version 
2.0 contains 38944 structural trees and 240,979 
words in Chinese. Each structural tree is annotated 
with words, part-of-speech of words, syntactic 
structure brackets, and semantic roles. For 
conventional structural trees, only syntactic 
information was annotated. However, it is very 
important and yet difficult for Chinese to identify 
word relations with purely syntactic constraints 
(Xia et al, 2000). Thus, partial semantic 
information, i.e. semantic role for each constituent, 
was annotated in Chinese structural trees. The 
grammatical constraints are expressed in terms of 
linear order of semantic roles and their syntactic 
and semantic restrictions. Below is an example 
sentence of the Sinica Treebank.  
 
Original sentence:  
? ?Ta?? ?yao? ?? ?ZhangSan?? ?jian? ? 
?qiu?? 
He ask Zhang San to pick up the ball. 
 
Parsed tree: 
S(agent:NP(Head:Nhaa:??He?)|Head:VF2:??ask? 
|goal:NP(Head:Nba:???Zhang San?) 
|theme:VP(Head:VC2:??pick?|goal:NP(Head:Nab:?
'?ball?))) 
Figure 1: An example sentence of Sinica Treebank 
  In the Sinica Treebank, not only the semantic 
relations of a verbal predicate but also the 
modifier head relations were marked. There are 74 
different semantic roles, i.e. the task of semantic 
role assignment has to establish the semantic 
relations among phrasal heads and their 
arguments/modifiers within 74 different choices. 
The set of semantic roles used in the Sinica Treebank 
is listed in the appendix.  
 
2.  Example-based Probabilistic Models for 
Assigning Semantic Roles 
  The idea of example-based approaches is that 
semantic roles are preserved for the same event 
frames. For a target sentence, if we can find same 
examples in the training corpus, we can assign the 
same semantic role for each constituent of the target 
sentence as the examples. However reoccurrence of 
exact same surface structures for a sentence is very 
rare, i.e. the probability of finding same example 
sentences in a corpus is very low. In fact, by 
observing structures of parsed trees, we find that 
most of semantic roles are uniquely determined by 
semantic relations between phrasal heads and their 
arguments/modifiers and semantic relations are 
determined by syntactic category, semantic class of 
related words. For example:  
 
Original sentence:  
?? ?wo men? ? ?du? ?? ?xi huan? ?? ?hu 
die?? 
 
We all like butterflies.  
 
Parsed tree:  
S(experiencer:NP(Head:Nhaa:?? 
?we? )|quantity:Dab:? ?all? |Head:VK1:?? ?like? 
|goal:NP(Head:Nab:?? ?butterflies?))? 
Figure 2: The illustration of the parsed tree.  
 
  In Figure2, ?? ?like? is the sentential head; ?
? ?we? and ?? ?butterflies? are the arguments; 
? ?all? is the modifier. As a result, the semantic role 
?experiencer? of ??  ?we? is deduced from the 
relation between ?? ?we? and?? ?like?, since 
the event frame of?? ?like? has the two arguments 
of experiencer and goal and the experiencer usually 
takes the subject position. The semantic roles of ?
? ?butterflies? and ? ?all? are assigned by the 
same way. For the task of automatic role 
assignment, once phrase boundaries and phrasal 
head are known, the semantic relations will be 
resolved by looking for similar 
head-argument/modifier pairs in training data. 
 
2.1  Example Exaction  
  To extract head-argument/modifier examples 
from the Sinica Treebank is trivial, since phrase 
boundaries and semantic roles, including phrasal 
head, are labeled. The extracted examples are 
pairs of head word and target word. The target 
word is represented by the head of the 
argument/modifier, since the semantic relations 
are established between the phrasal head and the 
head of argument/modifier phrase. An extracted 
word pair includes the following features. 
 
Target word:  
  The head word of argument/modifier.  
 
Target POS:  
  The part-of-speech of the target word. 
 
Target semantic role:  
  Semantic role of the constituent contains the 
target word as phrasal head. 
 
Head word:  
  The phrasal head. 
 
Head POS:  
  The part-of-speech of the head word. 
 
Phrase type:  
  The phrase which contains the head word and 
the constituent containing target word. 
 
Position:  
  Shows whether target word appears before or 
after head word. 
 
The examples we extracted from Figure 2 are 
listed below.  
 
Table 1: The three head-argument/modifier pairs 
extracted from Figure 2. 
 
 
S
goal
NP
experiencer
NP
Head
VK1
quantity
Dab
Head
Nhaa
Head
Nab
??
W e
?
all
??
like
??
Butterflies.
 
 
 
 
Table 2: Coverage and accuracy of different features combinations 
 
2.2 Probabilistic Model for Semantic Role 
Assignment
  It is possible that conflicting examples (or 
ambiguous role assignments) occur in the training 
data. We like to assign the most probable roles. The 
probability of each semantic role in a constituent 
with different features combinations are estimated 
from extract examples. 
 
 
position) pt,  t_pos, t,h_pos, h,(#
position) pt,  t_pos,, t h_pos, h,(r, #
) position pt,t_pos, t,h_pos, h, |r (
)|(
=
= P
tconstituenrP
1 
 
  Due to the sparseness of the training data, it?s not 
possible to have example feature combinations 
matched all input cases. Therefore the similar 
examples will be matched. A back off process will 
be carried out to reduce feature constraints during 
the example matching. We will evaluate 
performances for various features combinations to 
see which features combinations are best suited for 
semantic roles assignments. 
  We choose four different feature combinations. 
Each has relatively high accuracy. The four 
classifiers will be back off in sequence. If none of 
the four classifiers is applicable, a baseline model 
of assigning the most common semantic role of 
target word is applied. 
 
if # of (h,h_pos,t,t_pos,pt,position) > threshold  
P(r|constituent)=P(r|h,h_pos,t,t_pos,pt,position) 
 
Else 
if # of (h_pos,t,t_pos,pt,position) > threshold 
P(r|constituent)=P(r|h_pos,t,t_pos,pt,position) 
 
Else  
                                                 
1  r: semantic role; h: the head word; 
   h_pos: part-of-speech of head word;  
   t: the target word;  
   t_pos: part-of-speech of target word; 
   pt: the phrase type. 
if # of (h,h_pos,t_pos,pt,position) > threshold  
P(r|constituent)=P(r|h,h_pos,t_pos,pt,position) 
 
Else 
if # of (h_pos,t_pos,pt,position) > threshold 
P(r|constituent)=P(r|h_pos,t_pos,pt,position) 
 
Else 
Baseline model: 
P(r|constituent)=P(r| t, t_pos,pt) 
 
3. Experiments  
  We adopt the Sinica Treebank as both training 
and testing data. It contains about 40,000 parsed 
sentences. We use 35,000 sentences as training data 
and the rest 5,000 as testing data. The table 2 shows 
the coverage of each classifier, their accuracies, and 
performance of each individual classifier without 
back off process. The table 3 shows combined 
performance of the four classifiers after back off 
processes in sequence. The baseline algorithm is the 
simple unigram approach to assign the most 
common role for the target word. Because the 
accuracy of the four classifiers is considerably high, 
instead of using linear probability combinations we 
will rather use the most reliable classifier for each 
different features combination. 
 
 
 
 
 
 
Table 3: The accuracy of our backoff method and 
the base line (the most common semantic roles) 
 
3.1 Error Analyses 
  Although the accuracy of back off model is 
relatively high to the baseline model, it still has 
quite a room for improvement. After analyzed the 
errors, we draw following conclusions. 
 
Method Accuracy 
Backoff 90.29% 
Baseline:  68.68% 
a) Semantic head vs. syntactic head 
 
  A semantic role for a prepositional phrase (PP) is 
mainly determined by the syntactic head of PP, i.e. 
preposition, and the semantic head of PP, i.e. the 
head word of the DUMMY-argument of PP. For 
example, in Figure 3, the two sentences are almost 
the same, only the contents of PP are different. 
Obviously, the semantic roles of PP (? ?in? ?? 
?Indonesia?) is location, and the semantic role of PP 
(? ?in? ?? ?this year?) is time. Therefore the 
semantic roles of the two PPs should be determined 
only within the scope of PP and not relevant to 
matrix verb.  
 
 
S
Head
VC31
agent
NP
Head
Nca
Head
P21
Head
Nca
manner
Dh
?? ? ?? ?? ??
Taipei  speed-up  the  investments   in  Indonesia.
Dummy
NP
location
PP
S
Head
VC31
agent
NP
Head
Nca
Head
P21
Head
Nca
manner
Dh
?? ? ?? ?? ??
Taipei  speed-up   the  investments   this year.
Dummy
NP
time
PP
 
Figure 3: Parsed trees of ??????????? 
and ??????????? 
 
b) Structure-dependent semantic roles assignments 
 
  Complex structures are always the hardest part of 
semantic roles assignments. For example, the 
sentences with passive voice are the typical 
complex structures. In Figure 4, the semantic role 
of ?? ?Butterflies? is not solely determined by 
the head verb ?? ?attracted? and itself. Instead 
we should inspect the existence of passive voice 
and then reverse the roles of subject and object. 
 
 
 
 
Figure 4: A parsed tree of passive sentence ????
?????? 
  
4 Refined Models 
  Chen & Huang (1996) had studied the task of 
semantic assignment during Chinese sentence 
parsing. They concluded that semantic roles are 
determined by the following 4 parameters. 
 
1. Syntactic and semantic categories of the target 
word, 
2. Case markers, i.e. prepositions and 
postpositions 
3. Phrasal head, and 
4. Sub-categorization frame and its syntactic 
patterns. 
   
  Therefore head-modifier/argument examples 
only resolve most of semantic role assignments. 
Some of complex cases need other parameters to 
determine their semantic roles. For instance, the 
argument roles of Bei sentences (passive sentence) 
should be determined by all four parameters.   
  The refined model contain two parts, one is the 
refinements of features data which provide more 
precisely information and the other is the 
improvements of back off process to deal with 
special semantic roles assignments. 
 
4.1 Refinement of Features Extractions 
  The refinements of features extractions focus on 
two different cases, one is the features extractions 
of case-marked structures, such as PP and GP 
(postpositional phrases), and the other is the general 
semantic class identifications of synonyms.  
  The features of PP/GP include two different 
S
Head
VJ1
goal
NP
Head
P02
evaluation
Dbb
Head
Nab
Head
Nad
?? ? ? ?? ??
Butterflies   are   also   be   attracted   by   the voice.
Dummy
NP
theme
PP
feature types: the internal and the external features. 
The internal features of phrases compose of phrasal 
head and Dummy-head; the external features are 
heads (main verbs) of the target phrases. 
 
 
Figure 5: A parsed tree for demonstrating features 
extractions of PP 
 
Table 4: The internal/external relations of Figure 5.  
 
  The semantic class identifications of synonyms 
are crucial for solving data sparseness problems. 
Some type of  words are very productive, such as 
numbers, DM (determinative measurement), proper 
names. They need to be classified into different 
semantic classes. We use some tricks to classify 
them into specific word classes. For example we 
label  1 ??  ?one kilogram?, 2 ??  ?two 
kilograms? as their canonical form ???  ?n 
kilograms?; ??? ?the first day?, ??? ?the 
second day ? as ???  ?the nth days?;  ?
?  ?Zhang San?, ??  ?Li Si? as a personal 
name?etc. With this method, we can increase the 
number of matched examples and resolve the 
problem of occurrences of unknown words in a 
large scale.   
 
4.2  Dependency Decisions and Refined Back 
off Processes 
  The refined back off model aimed to solve 
semantic roles assignments for certain special 
structures. Using only head-modifier features could 
result into decision making with insufficient 
information. As illustrated before, the semantic role 
of ?? ?butterflies? in Figure 4 is ?agent? observed 
from the head-argument feature.  But in fact the 
feature of passive voice ? ?passive? tells us that 
the subject role of ?? ?butterflies? should be the 
semantic role ?goal? instead of the usual role of 
?agent?. 
  Therefore we enhanced our back off process by 
adding some dependency decisions. The 
dependency conditions include special grammar 
usage like passive form, quotation, topical 
sentences? etc. In the refined back off process, 
first we have to detect which dependency condition 
is happened and resolved it by using dependency 
features. For example, if the feature word ? 
?passive? occurs in a sentence, we realize that the 
subjective priority of semantic roles should be 
reversed. For instance, ?goal? will take subject 
position instead of ?agent? (?goal? appears before 
?agent?). 
4.3 Experiment Results 
  The experiments were carried out for the refined 
back off model with the same set of training data 
and testing data as in the previous experiments. 
Table 5 shows that the refined back off model gains 
2.4 % accuracy rate than the original back off 
model. However most of the improvement is due to 
the refinements of features extractions and 
canonical representation for certain classes of words. 
A few improvements were contributed to the 
decision making on the cases of structure 
dependency.   
 
 
Method Accuracy 
Refined Backoff  92.71% 
Backoff 90.29% 
Baseline 68.68% 
Table 5: Role assignment accuracies of refined 
backoff, backoff, and baseline models. 
 
5  Conclusion and Future Works 
  Semantic roles are determined by the following 4 
parameters. 
 
1. Syntactic and semantic categories of the target 
word, 
2. Case markers, i.e. prepositions and 
postpositions, 
3. Phrasal head, and 
4. Sub-categorization frame and its syntactic 
patterns. 
 
S
Head
VC31
agent
NP
Head
Nca
Head
P21
Head
Nca
manner
Dh
?? ? ?? ?? ??
Taipei  speed-up  the  investments   in  Indonesia.
Dummy
NP
location
PP
  We present an automatic semantic roles labeling 
system. It adopts dependency decision making and 
example-based approaches, which makes decision 
on the amount of parameters by observing the 
occurrence of dependency features and to utilize the 
minimal amount of feature combinations to assign 
semantic roles. It labels semantic roles of parsed 
sentences. Therefore the practical performance of 
the system depends on a good parser which labels 
the right structures of sentences. The system 
achieves 92.71% accuracy in labeling the semantic 
roles for pre-structure- bracketed texts which is 
considerably higher than the simple method using 
probabilistic model of head-modifier relations. 
In the future, we will consider fine-grain semantic 
role assignment problems. The current semantic 
roles assignment is focus on one sentence. However, 
the occurrences of frame elements are not limited to 
a single sentence. For instance, ?John bought the 
books from Mary?. The semantic roles of ?John? 
and ?Mary? are agent and theme respectively. 
According to Fillmore?s FrameNet, the frame 
element assignment for the above sentence should 
be ?John? the buyer, ?Mary? the seller, ?the books? 
the goods. The precondition of buy-frame says that 
the seller should be the owner of the goods. 
Therefore after the sentence parsing and logical 
reasoning, the following semantic relations should 
be established. 
 
Event frame: Commerce-buy 
Buyer: John 
Seller: Mary 
Goods: books 
Additional frame: Own 
 
Before the buy event 
Owner: Mary 
Possession: books 
After the buy event 
Owner: John 
Possession: books 
 
  The semantic roles assignment is a process of 
crossing phrasal and sentential boundaries. Some 
semantic roles of an event might occur at left or 
right context. Therefore we have to analyze the 
relation between two consecutive events. The 
relations include causal relation, temporal relation, 
resultant relation, etc. How to resolve the above 
problems will be our future studies. 
 
References 
 
Chen, Keh-Jiann, Chu-Ren Huang. 1996.  
Information-based Case Grammar: A 
Unification-based Formalism for Parsing Chinese. 
Journal of Chinese Linguistics Monograph Series 
No. 9.  
 
Chen, Keh-Jiann, Chu-Ren Huang, Feng-Yi Chen, 
Chi-Ching Luo,Ming-Chung Chang, Chao-Jan Chen, 
and Zhao-Ming Gao, 2003. Sinica Treebank: Design 
Criteria, Representational Issues and 
Implementation. In Anne Abeille (Ed.) Treebanks 
Building and Using Parsed Corpora. Language and 
Speech series. Dordrecht:Kluwer, pp231-248. 
Chu-Ren Huang, Keh-Jiann Chen, and Benjamin K. 
T?sou Eds. Readings in Chinese Natural Language 
Processing. 23-45. Berkeley: JCL. 
 
Daniel Gildea and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational 
Linguistics, 28(3):245-288 
 
Daniel Gildea and Julia Hockenmaier. 2003. Identifying 
Semantic Roles Using Combinatory Categorial 
Grammar. Conference on Empirical Methods in 
Natural Language Processing (EMNLP). 
 
Xia, Fei, 2000, The Part-of-Speech Tagging Guidelines 
for the Penn Chinese Treebank (3.0). IRCS Report 
00-07. Philadelphia, PA: University of Pennsylvania. 
Appendix: 
 
  Figure 6: The detail classification of semantic roles in the Sinica Treebank 
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 1?8,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Improving Context Vector Models by Feature Clustering for Auto-
matic Thesaurus Construction 
 
Jia-Ming You 
Institute of Information Science 
Academia Sinica 
swimming@hp.iis.sinica.edu.tw  
Keh-Jiann Chen 
Institute of Information Science 
Academia Sinica 
kchen@iis.sinica.edu.tw  
Abstract 
Thesauruses are useful resources for NLP; 
however, manual construction of thesau-
rus is time consuming and suffers low 
coverage. Automatic thesaurus construc-
tion is developed to solve the problem. 
Conventional way to automatically con-
struct thesaurus is by finding similar 
words based on context vector models 
and then organizing similar words into 
thesaurus structure. But the context vec-
tor methods suffer from the problems of 
vast feature dimensions and data sparse-
ness. Latent Semantic Index (LSI) was 
commonly used to overcome the prob-
lems. In this paper, we propose a feature 
clustering method to overcome the same 
problems. The experimental results show 
that it performs better than the LSI mod-
els and do enhance contextual informa-
tion for infrequent words. 
1 Introduction 
Thesaurus is one of the most useful linguistic 
resources. It provides information more than just 
synonyms. For example, in WordNet (Fellbaum, 
1998), it also builds up relations between syno-
nym sets, such as hyponym, hypernym. There are 
two Chinese thesauruses Cilin(1983) and 
Hownet1. Cilin provides synonym sets with sim-
ple hierarchical structure. Hownet uses some 
primitive senses to describe word meanings. The 
common primitive senses provide additional re-
lations between words implicitly. However, 
many words occurred in contemporary news cor-
pora are not covered by Chinese thesauruses.  
 
                                                 
1 http://www.HowNet.com(Dong Zhendong, Dong 
Qiang:HowNet) 
 
Therefore, we intend to create a thesaurus 
based on contemporary news corpora. The com-
mon steps to automatically construct a thesaurus 
include a) contextual information extraction, b) 
finding synonym words and c) organizing syno-
nym words into a thesaurus. The approach is 
based upon the fact that word meaning lays on its 
contextual behavior. If words act similarly in 
context, they may share the same meaning.  
However, the method can only handle frequent 
words rather than infrequent ones. In fact most of 
vocabularies occur infrequently, one has to dis-
cover extend information to overcome the data 
sparseness problem. We will introduce the con-
ventional approaches for automatic thesaurus 
construction in section 2. Follow a discussion 
about the problems and solutions of context vec-
tor models in section 3.  In section 4, we use two 
performance evaluation metrics, i.e. discrimina-
tion and nonlinear interpolated precision, to 
evaluate our proposed method. 
2 Conventional approaches for auto-
matic thesaurus construction  
The conventional approaches for automatic the-
saurus construction include three steps: (1) Ac-
quire contextual behaviors of words from cor-
pora. (2) Calculate the similarity between words. 
(3) Finding similar words and then organizing 
into a thesaurus structure.  
2.1 Acquire word sense knowledge 
One can model word meanings by their co-
occurrence context. The common ways to extract 
co-occurrence contextual words include simple 
window based and syntactic dependent based 
(You, 2004). Obviously, syntactic dependent 
relations carry more accurate information than 
window based. Also, it can bring additional in-
formation, such as POS (part of speech) and se-
mantic roles etc. To extract the syntactic de-
1
;1 )(log)()rdentropy(wo ?= -=
m
k
i
kwordp
i
kwordpi
pended relation, a raw text has to be segmented, 
POS tagged, and parsed. Then the relation ex-
tractor identifies the head-modifier relations 
and/or head-argument relations. Each relation 
could be defined as a triple (w, r, c), where w is 
the thesaurus term, c is the co-occurred context 
word and r is the relation between w and c.  
Then context vector of a word is represented 
differently by different models, such as: tf, 
weight-tf, Latent Semantic Indexing (LSI) 
(Deerwester, S.,et al, 1990) and Probabilistic 
LSI (Hofmann, 1999). The context vectors of 
word x can be express by:     
 
a) tf model: word x = }tf...,,2tf,1{tf xnxx ,where xitf is 
the term frequency of the ith context word when 
given word x.  
 
b) weight-tf model: assume there are n contex-
tual words and m target words. word x= 
 
,where weighti,  we used here, is defined as  
[logm-entropy(wordi)]/logm 
   
)(wordikp      
is the co-occurrence probability of wordk when 
given wordi. 
 
c) LSI or PLSI models: using tf or weighted-tf 
co-occurrence matrix and by adopting LSI or 
PLSI to reduce the dimension of the matrix. 
 
2.2 Similarity between words  
The common similarity functions include  
a) Adopting simple frequency feature, such as 
cosine, which computes the angle between two 
context vectors;  
 
b) Represent words by the probabilistic distribu-
tion among contexts, such as Kull-Leiber diver-
gence (Cover and Thomas, 1991). 
The first step is to convert the co-occurrence 
matrix into a probabilistic matrix by simple for-
mula.  
? =
==
? =
===
= n 1k yktf
y
itfxiq},
x
n,...q
x
2q,
x
1{q wordy 
n
1k
x
ktf
x
itfxip},
x
n,...p
x
2p,
x
1{pwordx
 q
p
 
 
Then calculate the distance between probabil-
istic vectors by sums up the all probabilistic dif-
ference among each context word so called cross 
entropy. 
 
Due to the original KL distance is asymmetric 
and is not defined when zero frequency occurs. 
Some enhanced KL models were developed to 
prevent these problems such as Jensen-Shannon 
(Jianhua, 1991), which introducing a probabilis-
tic variable m, or ? -Skew Divergence (Lee, 
1999), by adopting adjustable variable ?. Re-
search shows that Skew Divergence achieves 
better performance than other measures. (Lee, 
2001) 
 
))1(||(yxS  rgence)D(SkewDive yxxKL aaa -+==  
 
2/)(                                  
,2/)}||()||({y)x,(JS Shannon)-D(Jensen
yxm
myKLmxKL
+=
+==
 
To convert distance to similarity value, we 
adopt the formula inspired by Mochihashi, and 
Matsumoto 2002.   
 
 
2.3 Organize similar words into thesaurus  
 
There are several clustering methods can be used 
to cluster similar words. For example, by select-
ing N target words as the entries of a thesaurus, 
then extract top-n similar words for each entry; 
adopting HAC(Hierarchical agglomerative clus-
tering, E.M. Voorhees,1986) method to cluster 
the most similar word pairs in each clustering 
loop. Eventually, these similar words will be 
formed into synonyms sets.  
 
3 Difficulties and Solutions 
There are two difficulties of using context vector 
models. One is the enormous dimensions of con-
}weight...tf2weight2tf,1weight1{tf n
xxx
n ???
yx
xy)cos(x,
?
?= y
)},distance(exp{wordy)(wordx,similarity yx?-= l
i
i
21 q
plog)(q)KL(p, :Distance KL ??
=
= ipn
i
2
textual words, and the other is data sparseness 
problem. Conventionally LSI or PLSI methods 
are used to reduce feature dimensions by map-
ping literal words into latent semantic classes. 
The researches show that it?s a promising 
method (April Kontostathis, 2003). However the 
latent semantic classes also smooth the informa-
tion content of feature vectors. Here we proposed 
a different approach to cope with the feature re-
duction and data sparseness problems. 
 
3.1 Feature Clustering 
Reduced feature dimensions and data sparseness 
cause the problem of inaccurate contextual in-
formation. In general, one has to reduce the fea-
ture dimensions for computational feasibility and 
also to extend the contextual word information to 
overcome the problem of insufficient context 
information. 
In our experiments, we took the clustered-
feature approaches instead of LSI to cope with 
these two problems and showed better perform-
ances. The idea of clustered-feature approaches 
is by adopting the classes of clustering result of 
the frequent words as the new set of features 
which has less feature dimensions and context 
words are naturally extend to their class mem-
bers. We followed the steps described in section 
2 to develop the synonyms sets. First, the syntac-
tic dependent relations were extracted to create 
the context vectors for each word. We adopted 
the skew divergence as the similarity function, 
which is reported to be the suitable similarity 
function (Masato, 2005), to measure the distance 
between words.  
 
We used HAC algorithm to develop the syno-
nyms classes, which is a greedy method, simply 
to cluster the most similar word pairs at each 
clustering iteration.  
 
The HAC clustering process: 
 
While  the similarity of the most similar word pair 
(wordx, wordy) is greater than a threshold ? 
 
then cluster wordx, wordy together and replace it with 
the centroid   between wordx and wordy 
 
Recalculate the similarity between other words and 
the  centroid   
 
3.2 Clustered-Feature Vectors 
We obtain the synonyms sets S from above HAC 
method. Let the extracted synonyms sets S = { S1, 
S2,?SR} which contains R synonym classes; 
i
jS stands for the jth element of the ith synonym 
class;  the ith synonym class Si contains Qi ele-
ments.  
 
 
The feature extension processing transforms 
the coordination from literal words to synonyms 
sets. Assume there are N contextual words 
{C1,C2,?CN}, and the first step is to transform 
the context vector of of Ci to the distribution vec-
tor among S. Then the new feature vector is the 
summation of the distribution vectors among S 
of its all contextual words. 
 
The new feature vector of wordj = 
?= ?
N
i 1
 jitf Distribution_Vector_among_S( iC ) 
,where jitf  is the term frequency of the context 
word Ci occurs with wordj.  
Distribution_Vector_among_S( iC )= { }RSiPSiPSiP ,..., 21 ,
.S synonyms  at the  of rdscontext wo
 ofon distributi  themeans,(Ci)
1
),(
  where,
jjthCi
freq
Qj
q
CijqSfreqS
iP
j
?
==
 
Due to the transformed coordination no longer 
stands for either frequency or probability, we use 
simple cosine function to measure the similarity 
between these transformed clustered-feature vec-
tors.  
4 Evaluation  
To evaluate the performance of the feature clus-
tering method, we had prepared two sets of test-
ing data with high and low frequency words re-
spectively. We want to see the effects of feature 
reduction and feature extension for both frequent 
and infrequent words. 
 
??
??
?
?
?
??
??
?
?
?
=
R
QR
RR
Q
Q
SSS
SSS
SSS
...
............
...
...
 S 
21
2
2
2
2
2
1
1
1
1
2
1
1
3
4.1 Discrimination Rates  
The discrimination rate is used to examine the 
capability of distinguishing the correlation be-
tween words. Given a word pair (wordi,wordj), 
one has to decide whether the word pair is simi-
lar or not. Therefore, we will arrange two differ-
ent word pair sets, related and unrelated, to esti-
mate the discrimination. By given the formula 
below  
 
 
,where Na and Nb are respectively the numbers 
of synonym word pairs and unrelated word pairs. 
As well as, na and nb are the numbers of correct 
labeled pairs in synonyms and unrelated words.   
4.2 Nonlinear interpolated precision  
 
The Nap evaluation is used to measure the per-
formance of restoring words to taxonomy, a 
similar task of restoring words in WordNet 
(Dominic Widdows, 2003).  
The way we adopted Nap evaluation is to re-
construct a partial Chinese synonym set, and 
measure the structure resemblance between 
original synonyms and the reconstructed one. By 
doing so, one has to prepare certain number of 
synonyms sets from Chinese taxonomy, and try 
to reclassify these words.  
Assume there are n testing words distributed 
in R synonyms sets.  Let i1R stands for the repre-
sented word of the ith synonyms set. Then we 
will compute the similarity ranking between each 
represented word and the rest n-1 testing words. 
By given formula  
 
i
jS  represents the jth similar word of i1R  among 
the rest n-1 words 
 
??
?
??
?= 0
  synonym are R and  S if,1 i1iji
jZ  
 
The NAP value means how many percent 
synonyms can be identified. The maximum value 
of NAP is 1, means the extracted similar words 
are exactly match to the synonyms.  
5 Experiments 
The context vectors were derived from a 10 
year news corpus from The Central News 
Agency. It contains nearly 33 million sentences, 
234 million word tokens, and we extracted 186 
million syntactic relations from this corpus. Due 
to the low reliability of infrequent data, only the 
relation triples (w, r, c), which occurs more than 
3 times and POS of w and c must be noun or 
verb, are used. It results that nearly 30,000 high 
frequent nouns and verbs are used as the contex-
tual features. And with feature clustering2, the 
contextual dimensions were reduced from 30,988 
literal words to 12,032 semantic classes. 
In selecting testing data, we consider the 
words that occur more than 200 times as high 
frequent words and the frequencies range from 
40 to 200 as low frequent words.    
 
Discrimination  
 
For the discrimination experiments, we randomly 
extract high frequent word pairs which include 
500 synonym pairs and 500 unrelated word pairs 
from Cilin (Mei et. al, 1983). At the mean time, 
we also prepare equivalent low frequency data.  
We use a mathematical technique Singular 
Value Decomposition (SVD) to derive principal 
components and to implement LSI models with 
respect to different feature dimensions from 100 
to 1000. We compare the performances of differ-
ent models. The results are shown in the follow-
ing figures. 
 
Figure1.  Discrimination for high frequent words 
 
The result shows that for the high frequent 
data, although the feature clustering method did 
not achieve the best performance, it perform-
ances better at related data and a balanced per-
formance at unrelated data. The tradeoffs be-
                                                 
2 Some feature clustering results are listed in the Ap-
pendix  
??
???
? += Nb
nb
Na
na
2
1ratetion Discrimina
,1R
1NAP
1
1
1n
1j
R
1i
???
?
???
? += ??? -
=
-
==
j
k
i
k
i
j Zj
Z
4
tween related recalls and unrelated recalls are 
clearly shown. Another observation is that no 
matter of using LSI or literal word features (tf or 
weight_tf), the performances are comparable. 
Therefore, we could simply use any method to 
handle the high frequent words.  
 
Figure2 Discrimination for low frequent word 
 
For the infrequent words experiments, neither 
LSI nor weighted-tf performs well due to insuffi-
cient contextual information. But by introducing 
feature clustering method, one can gain more 6% 
accuracy for the related data. It shows feature 
clustering method could help gather more infor-
mation for the infrequent words.  
 
Nonlinear interpolated precision 
 
For the Nap evaluation, we prepared two testing 
data from Cilin and Hownet. In the high frequent 
words experiments, we extract 1311 words 
within 352 synonyms sets from Cilin and 2981 
words within 570 synonyms sets from Hownet.  
 
Figure 3. Nap performance for high frequent words 
 
In high frequent experiments, the results show 
that the models retaining literal form perform 
better than dimension reduction methods. It 
means in the task of measuring similarity of high 
frequent words using literal contextual feature 
vectors is more precise than using dimension 
reduction feature vectors. 
In the infrequent words experiments, we can 
only extract 202 words distributed in 62 syno-
nyms sets from Cilin and 1089 words within 222 
synonyms sets. Due to fewer testing words, LSI 
was not applied in this experiment. 
 
Figure 4. Nap performance for low frequent words 
 
It shows with insufficient contextual informa-
tion, the feature clustering method could not help 
in recalling synonyms because of dimensional 
reduction.  
 
6. Error Analysis and Conclusion  
 
Using context vector models to construct thesau-
rus suffers from the problems of large feature 
dimensions and data sparseness. We propose a 
feature clustering method to overcome the prob-
lems. The experimental results show that it per-
forms better than the LSI models in distinguish-
ing related/unrelated pairs for the infrequent data, 
and also achieve relevant scores on other evalua-
tions.  
Feature clustering method could raise the abil-
ity of discrimination, but not robust enough to 
improve the performance in extracting synonyms. 
It also reveals the truth that it?s easy to distin-
guish whether a pair is related or unrelated once 
the word pair shares the same sense in their 
senses. However, it?s not the case when seeking 
synonyms. One has to discriminate each sense 
for each word first and then compute the similar-
ity between these senses to achieve synonyms.  
Because feature clustering method lacks the abil-
ity of senses discrimination of a word, the 
method can handle the task of distinguishing cor-
relation pairs rather than synonyms identification. 
 
Also, after analyzing discrimination errors 
made by context vector models, we found that 
some errors are not due to insufficient contextual 
information. Certain synonyms have dissimilar 
contextual contents for different reasons. We 
observed some phenomenon of these cases:  
 
5
a) Some senses of synonyms in testing data are 
not their dominant senses.  
 
Take guang1hua2 (??) for example, it has a 
sense of ?splendid? which is similar to the sense 
of guang1mang2 (?? ). Guang1hua2 and 
guang1mang2 are certainly mutually changeable 
in a certain degree, guang1hua2jin4shi4 (???
?) and guang1mang2jin4shi4 (????), or 
xi2ri4guang1hua2 ( ? ? ? ? ) and 
xi2ri4guang1mang2 (????). However, the 
dominated contextual sense of guang1hua2 is 
more likely to be a place name, like 
guang1hua2shi4chang3( ? ? ? ? ) or 
hua1lian2guang1hua2 (????) etc3.  
 
b) Some synonyms are different in usages for 
pragmatic reasons.  
 
Synonyms with different contextual vectors 
could be result from different perspective views. 
For example, we may view wai4jie4 (??) as a 
container image with viewer inside, but on the 
other hand, yi3wai4 (??) is an omnipotence 
perspective. This similar meaning but different 
perspective makes distinct grammatical usage 
and different collocations.  
 
 
 Similarly, zhong1shen1 (??) and sheng1ping2 
( ? ? ) both refer to ?life-long time?. 
zhong1shen1 explicates things after a time point, 
which differs from sheng1ping2, showing mat-
ters before a time point.  
 
 
 
 
 
c) Domain specific usages.  
 
 For example, in medical domain news ,wa1wa1 
(??) occurs frequently with bo1li2 (??) refer 
                                                 
3 This may due to different genres. In newspapers the 
proper noun usage of guang1hua2 is more common 
than in a literature text. 
to kind of illness. Then the corpus reinterpret 
wa1wa1 (??) as a sick people, due to it occurs 
with medical term. But the synonym of wa1wa1 
(?? ), xiao3peng2you3(??? ) stands for 
money in some finance news.  Therefore, the 
meanings of words change from time to time. It?s 
hard to decide whether meaning is the right an-
swer when finding synonyms.  
  
With above observations, our future researches 
will be how to distinguish different word senses 
from its context features. Once we could distin-
guish the corresponding features for different 
senses, it will help us to extract more accurate 
synonyms for both frequent and infrequent words.  
 
 
References 
 
April Kontostathis,  William M. Pottenger 2003. ,  A 
Framework for Understanding LSI Performance ,  In 
the Proceedings of the ACM SIGIR Workshop on 
Mathematical/Formal Methods in Information Re-
trieval, Annual International SIGIR Conference, 2003. 
 
Christiance Fellbaum, editor 1998,WordNet: An alec-
tronic lwxical database. MIT press, Cambrige MA.  
 
Deerwester, S.,et al 1990 Indexing by Latent Seman-
tic Analysis. Jorunal of the American Society for In-
formation Science, 41(6):391-407 
 
Dominic Widdows. 2003. Unsupervised methods for 
developing taxonomies by combining syntactic and 
statistical information. In Proceeding of HLT-NAACL 
2003 Main papers, pp, 197-204.  
 
E.M. Voorhees, ?Implement agglomerative hierarchi-
cal clustering algorithm for use in document re-
trieval?, Information Processing & Management. , no. 
22 pp.46-476,1986 
 
Hofmann, T.1999. Probabilistic Latent  Semantic 
Indexing. Proc.of the 22nd International conference on 
Research and Development in Information Retrieval 
(SIGIR?99),50-57 
 
James R.Curran and Marc Moens. 2002. Improve-
ments in Automatic Thesaurus Extraction. Proceed-
ings of the Workshop of the ACL Special Interest 
Group on the Lexicon (SIGLEX), pp. 59-66 
 
Jia-Ming You and Keh-Jiann Chen, 2004 Automatic 
Semantic Role Assignment for a Tree Structure, Pro-
 
 
Wai4jie4  
?? 
               Yi3wai4?? 
 
 
 
Omnipotence viewer 
? 
Zhong1shen1
?? 
? 
sheng1ping2
?? 
viewer 
6
ceedings of 3rd ACL SIGHAN Workshop 
 
Jiahua Lin. 1991. Divergence measures based on the 
Shannon Entropy. IEEE transactions on Information 
Theory, 37(1): 145-151 
 
Lillian Lee. 2001. On the effectiveness of the skew 
divergence for statistical language analysis. In Artifi-
cial Intelligence and Statistics 2001, page 65-72. 
 
Lillian Lee. 1999. Measure of distributional similarity. 
In Proceeding of the 37th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-1999), 
page 23-32.  
 
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko 
Toyama. 2005. PLSI Utilization for Automatic The-
saurus Construction. IJCNLP 2005, LNAI 651, pp. 
334-345.  
 
Mei,Jiaju,Yiming Lan, Yunqi Gao, Yongxian Ying 
(1983) ?????   [ A Dictionary of Syno-
nyms],Shanghai Cishu Chubanshe.  
 
Mochihashi, D., Matsumoto, Y.2002. Probabilistic 
Representation of Meanings. IPSJ SIG Notes Natural 
Language, 2002-NL-147:77-84. 
 
T.Cover and J.Thomas, 1991. Element of Information 
Theory. Wiley & sons, New York 
 
 
 
 
 
7
Appendix: 
Some feature clustering results 
?? ???  
??? ??? ??? ???  
?? ???  
??? ??? ??  
?? ?? ??? ??  
??? ?? ???  
?? ?? ?? ??  
???? ???? ????  
?? ?? ?? ??? ?? ?? ?? ?? ?? 
?? ??  
??? ?? ??  
???? ??  
???? ?? ?? ?? ??  
?? ?? ?? ?? ??  
??? ???  
??? ???  
?? ?? ??  
?? ?? ??? ???  
?? ??? ??  
???? ?? ?? ?? ??  
?? ???  
?? ??? ?? ??  
?? ?? ?? ?? ??? ??  
?? ?? ?? ????  
?? ??  
??? ?? ??? ?? ??? ?? ???  
?? ?? ?? ?? ?? ??  
?? ??  
?? ??  
?? ?? ?? ??? ??? ?? ??? ?? 
?? ?? ?? ?? ?? ?? ?? ??  
?? ?? ??? ??? ???  
 
??? ??? ??? ?? ??? ??? ?? ?
?? ??? ??? ???  
?? ??? ?? ??? ?? ?? ??  
??? ???  
??? ??? ??  
???? ??????? ?? ??  
?? ?? ?? ???  
?? ?? ??  
?? ?? ??  
?? ??  
?? ?? ?? ??  
?? ???  
???? ?? ??? ?? ???? ?? ??? 
??? ?? ???  
?? ???? ??? ??  
?? ?? ?? ?? ??? ?? ??  
?? ?? ?? ?? ?? ?? ?? ??  
?? ??  
?? ?? ??  
?? ?? ??  
??? ??? ?? ??  
?? ?? ?? ??  
?? ?? ??  
?? ?? ??? ??? ?? ?? ?? ?? ?
? ?? ??? ?? ??  
?? ?? ?? ???? ??? ?? ?? ??  
?? ?? ?? ??  
??? ???  
?? ?? ???  
?? ?? ??  
??? ??? ??  
?? ??  
??? ??  
?? ?? ?? ???  
??? ???? ?? ?? ???  
?? ??? ?? ?? ?? ?? ???  
?? ?? ??  
?? ?? ??? ?? ?? ??  
8
Coling 2010: Demonstration Volume, pages 45?48,
Beijing, August 2010
E-HowNet and Automatic Construction of a Lexical Ontology
Wei-Te Chen, Su-Chu Lin, Shu-Ling Huang, You-Shan Chung, and Keh-Jiann Chen
     Institute of Information Science, Academia Sinica
weitehchen@gmail.com,
{jess, yosieh, yschung, kchen}@iis.sinica.edu.tw
Abstract
In this paper, we propose a lexical senses
representation system called E-HowNet,
in which the lexical senses are defined by
basic concepts. As a result, the mean-
ings of expressions are more specific than
those derived by using primitives. We also
design an ontology to express the taxo-
nomic relations between concepts and the
attributes of concepts. To establish the
taxonomic relations between word senses,
we introduce a strategy that constructs the
E-HowNet ontology automatically. We
then implement the lexical ontology as a
Web application1 to demonstrate the tax-
onomy and the search functions for query-
ing key-terms and E-HowNet expressions
in the lexicon, which contains more than
88,000 lexical senses.
1 Introduction
E-HowNet, an evolution and extension of HowNet
(Dong & Dong, 2006), is an entity-relation rep-
resentation model for lexical senses. Under the
framework, word senses are defined by basic
concepts as well as conceptual relations called
attribute-values. The following is an example of
lexical sense representation in E-HowNet.
(1) ?? ?|carefully choose? is expressed
(or defined) by the expression ?{choose|?
?:manner={cautious|?}}?.
In the representation, the meaning of ???? is
comprised of two primitive concepts, ?choose|?
?? and ?cautious|??, and the conceptual rela-
1available at http://ckip.iis.sinica.edu.tw/?wtchen/taxonomy/
tion between the primitives is explained by the se-
mantic role ?manner?. For further details, readers
may refer to the E-HowNet technical report (CKIP
2009).
With a well-established entity-relation model,
semantic composition is applicable from the mor-
phological level to the sentential level in E-
HowNet. Semantic compositionality, together
with syntactic information, contributes enor-
mously to natural language understanding.
The remainder of this paper is organized as
follows. We describe the major features of E-
HowNet in Section 2 and introduce the E-HowNet
ontology in Section 3. Then, we present our on-
line E-HowNet system in Section 4. Section 5
contains some concluding remarks.
To achieve the goal of semantic compositional-
ity and to extend the advantage from HowNet, the
following features are implemented in E-HowNet.
a) Multi-level definitions and semantic decom-
position: Word senses (concepts) can be defined
(expressed) by primitives as well as by any well-
defined concepts and conceptual relations. How-
ever, using only primitives to express word senses,
as in HowNet, causes information degradation and
important ontological relations between concepts
may be missed.
b) Uniform sense representation and seman-
tic compositionality: To achieve semantic com-
positionality, it is necessary to encode the senses
of both content words and function words in a
uniform framework. HowNet performs well for
defining content words, but it does not provide
a well-form representational framework for ex-
pression the sense of function words, which in-
dicate semantic relations. In contrast, E-HowNet
45
provides uniform representations for the senses
of content/function words and the senses of sen-
tences/phrases. For example, the passive sense
of the preposition ?? by? introduces an agent
role (relation) and the conjunction ??? because?
links the relation of reason between two events.
The functional representation and semantic com-
positionality are illustrated by the following ex-
ample:
(2) Because of the rain, the clothes are all wet.
???????????
Table 1: The function representation and semantic
compositionality for example sentence
Word POS E-HowNet
Definition
?? Cb reason ={ }(conjunction)
?? VA {rain|??}(intransitive verb)
?? Na {clothing|??}(common noun)
? Da Quantity=(adverb) {complete|?}
? VH {wet|?}(state verb)
? Ta aspect=(particle) {Vachieve|??}
Suppose that the following dependency struc-
ture and semantic relations are derived by parsing
sentence (2) as follows:
(3) S(reason:VP(Head:Cb:??|dummy:VA:?
?)|theme:NP(Head:Na:??) | quantity: Da:? |
Head:Vh:?|particle:Ta:?)?
The semantic composition in (4) is the result of
unifying the features of the lexical representations
shown in the above table. The dependency daugh-
ters have become feature attributes of the senten-
tial head ?wet|??.
(4) def:{wet|?:
theme={clothing|??},
aspect={Vachieve|??},
quantity={complete|?},
reason={rain|??}}.
c) Taxonomy for both entities and relations: To
achieve automatic feature unification, E-HowNet
organizes entities and relations (attributes) in a hi-
erarchical structure that relates entities taxonomi-
cally. Further details are provided in the next sec-
tion.
2 Ontology
We adopt and extend approximately 2,600 prim-
itives from HowNet to form the top-level ontol-
ogy of E-HowNet, which includes two types of
subtrees: entities and relations. The entities are
comprised of events, objects, and attribute-values;
while the relations are comprised of semantic-
roles and functions. Entities indicate concepts that
have substantial content, whereas relations link
the semantic relations between entities (Chen et
al., 2004; Chen et al, 2005; Chen et al, 2005;
Huang et al 2008). The taxonomic structure is or-
ganized by hypernym-hyponym relations; there-
fore, it forms an inheritable system, i.e., the hy-
ponym concepts inherit the properties of hyper-
nym concepts. The proposed approach facilitates
the adoption of knowledge represented by other
frameworks, such as FrameNet, and HowNet; and
it allows concepts to be represented with vary-
ing degrees of specificity. Another advantage is
that conceptual similarities can be modeled by
their relational distances in the hierarchy (Resnik,
1999), and the taxonomic relations between lexi-
cal senses can be captured from their E-HowNet
expressions automatically.
2.1 Automatic Construction of Ontology
With E-HowNet expressions, lexical senses are
defined as entities and relations. Thus, all the tax-
onomic relations of lexical senses can be iden-
tified according to their E-HowNet definitions.
Synonyms are identified by their identical E-
HowNet expressions, and hyponymy relations are
identified by the subsumption of attribute-values.
(Note that only near-synonym classes are iden-
tified due to the coarse-grained expressions of
the lexical senses in the current version of E-
HowNet.) Furthermore, new categories are iden-
tified by common attribute-values. For instance,
pandas and zebras can be categorized as animals
with the same feature: black and white markings.
To construct a complete lexical taxonomy, we use
46
Figure 1: The E-HowNet ontology system
a strategy that categorizes concepts automatically.
Starting with a manually created top-level on-
tology of primitive concepts, the following strat-
egy classifies the lexicon into hierarchical sub-
categories:
(1) Attach lexical senses. Words and associ-
ated sense expressions are first attached to the top-
level ontology nodes according to their head con-
cepts. For instance, the head concept of the ex-
pression ?{choose|??:manner={cautious|?}}?
is?choose|???.
(2) Sub-categorization by attribute-values. Lex-
ical concepts with the same semantic head are fur-
ther sub-categorized according to their attribute-
values. Lexicons that have the same attribute-
values share specific characteristics; therefore fur-
ther sub-categorization is performed based on the
distinct attribute-values of the lexicons.
(3) Repeat step (2) if there are too many lexical
concepts in one category. Although the lexicons
are classified after step (2), some sub-categories
might still contain too many lexicons. In this
situation, we further classify the lexicons in the
sub-category with other attribute-values until all
sub-categories contain fewer members than a pre-
defined threshold, or all members of a category
are synonyms.
3 Overview of the On-line System
The current E-HowNet ontology is an on-line ver-
sion of the automatically constructed taxonomic
structure of E-HowNet expressions, which con-
tain more than 88,000 lexical senses. This sec-
tion provides an overview of the ontology and the
functions of the on-line web browsing system.
Figure 2: Key-Term Search Box
Figure 1 shows the E-HowNet ontology system
and tree structure.
The tree structure of hyponymy relations al-
lows users to browse the entire tree by expanding
and hiding sub-trees. Although the classification
strategy enables the number of entities under each
node to be limited and viewed easily, a more effec-
tive function is essential for exploring more than
88 thousand items of data in E-HowNet. There-
fore, we provide a search function that allows
users to query lexical senses in two ways:
Key-Term Search: The first way is key-term
search, which is shown in Figure 2. The syntax
of the query interface is like that used by conven-
tional search engines. By inputting the key-term
???? , the system will search all the taxonomy
nodes, sub-categories, and lexical nodes. Then,
the results for the taxonomy node ?object|???
and the lexical word ???? will be displayed in
47
Figure 3: E-HowNet Expression Search Box
the respective columns.
E-HowNet Expression Search: To search
a class of words with specific attribute-values,
we provide another query syntax for exploring
data in E-HowNet Expression. For instance, to
find all expressions about wooden objects in-
volves finding E-HowNet data items containing
the entity ?object???? and the attribute-value
?material={wood|?}?. The expressions are en-
tered on the form shown in Figure 3 and submitted
to the system. The results of word senses denoting
wooden objects are then returned.
4 Conclusion
E-HowNet sense representations are incremental.
Hence, lexical sense expressions can be updated
and refined at anytime. In addition, logical rela-
tions and the taxonomic structure can be rebuilt
automatically based on the refined expressions.
New categories in the taxonomy can be identi-
fied and characterized by their specific attribute-
values. Uniform representations of function
words and content words facilitate semantic com-
position and decomposition, and allow users to
derive sense representations of phrases/sentences
from the composition of lexical senses. Further-
more, because of E-HowNet?s semantic decom-
position capability, the primitive representations
for surface sentences with the same deep seman-
tics are nearly canonical. We have implemented
the E-HowNet ontology online to demonstrate the
taxonomy, sub-categories, and lexicons in a hier-
archical tree structure. In addition, we provide
search functions for querying key-terms and E-
HowNet expressions.
References
Keh-Jiann Chen, Shu-Ling Huang, Yueh-Yin Shih and
Yi-Jun Chen. 2004. Multi-level Definitions and
Complex Relations in Extended-HowNet. In Pro-
ceedings of the Fifth Workshop on Chinese Lexical
Semantics 2004, Beijing University. (in Chinese)
Keh-Jiann Chen, Shu-Ling Huang and Yueh-Yin Shih,
Yi-Jun Chen. 2005. Extended-HowNet- A Repre-
sentational Framework for Concepts. In Proceed-
ings of OntoLex 2005, Jeju Island, South Korea.
Yi-Jun Chen, Shu-Ling Huang, Yueh-Yin Shih and
Keh-Jiann Chen. 2005. Semantic Representation
and Definitions for Function Words in Extended-
HowNet. In Proceedings of the Sixth Workshop on
Chinese Lexical Semantics 2005, Xiamen Univer-
sity.
Z. D. Dong and Q. Dong 2006. HowNet and the Com-
putation of Meaning. World Scientific Publishing
Co. Pte. Ltd.
Shu-Ling Huang, Shih Yueh-Yin and Keh-Jiann Chen
2008. Knowledge Representation for Comparison
Words in Extended-HowNet. Language and Lin-
guistics, vol. 9(2), pp. 395-414.
Philip Resnik. 1999. Semantic similarity in a Taxon-
omy: An information-based measure and its appli-
cation to problems of ambiguity in natural language.
Journal of Artifical Intelligence Research, vol. 11,
pp. 95-130.
CKIP. 2009. Lexical Semantic Representation and Se-
mantic Composition: An Introduction to E-HowNet
(E-HowNet Technical Report). Academia Sinica,
Taipei.
48
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 928?937,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Ambiguity Resolution for Vt-N Structures in Chinese 
 
 
Yu-Ming Hsieh1,2       Jason S. Chang2       Keh-Jiann Chen1 
1 Institute of Information Science, Academia Sinica, Taiwan 
2 Department of Computer Science, National Tsing-Hua University, Taiwan 
morris@iis.sinica.edu.tw, jason.jschang@gmail.com 
kchen@iis.sinica.edu.tw 
 
  
 
Abstract 
The syntactic ambiguity of a transitive 
verb (Vt) followed by a noun (N) has 
long been a problem in Chinese parsing. 
In this paper, we propose a classifier to 
resolve the ambiguity of Vt-N structures. 
The design of the classifier is based on 
three important guidelines, namely, 
adopting linguistically motivated features, 
using all available resources, and easy in-
tegration into a parsing model. The lin-
guistically motivated features include 
semantic relations, context, and morpho-
logical structures; and the available re-
sources are treebank, thesaurus, affix da-
tabase, and large corpora. We also pro-
pose two learning approaches that resolve 
the problem of data sparseness by auto-
parsing and extracting relative 
knowledge from large-scale unlabeled 
data. Our experiment results show that 
the Vt-N classifier outperforms the cur-
rent PCFG parser. Furthermore, it can be 
easily and effectively integrated into the 
PCFG parser and general statistical pars-
ing models. Evaluation of the learning 
approaches indicates that world 
knowledge facilitates Vt-N disambigua-
tion through data selection and error cor-
rection. 
1 Introduction 
In Chinese, the structure of a transitive verb (Vt) 
followed by a noun (N) may be a verb phrase 
(VP), a noun phrase (NP), or there may not be a 
dependent relation, as shown in (1) below. In 
general, parsers may prefer VP reading because a 
transitive verb followed by a noun object is nor-
mally a VP structure. However, Chinese verbs 
can also modify nouns without morphological 
inflection, e.g., ?? /farming ? /pond. Conse-
quently, parsing Vt-N structures is difficult be-
cause it is hard to resolve such ambiguities with-
out prior knowledge. The following are some 
typical examples of various Vt-N structures:  
1) 
??/solve ??/problem ? VP 
??/solving ??/method ? NP 
??/solve ??/mankind (??/problem)?None 
To find the most effective disambiguation fea-
tures, we need more information about the Vt-N 
? NP construction and the semantic relations 
between Vt and N. Statistical data from the Sini-
ca Treebank (Chen et al., 2003) indicates that 
58% of Vt-N structures are verb phrases, 16% 
are noun phrases, and 26% do not have any de-
pendent relations. It is obvious that the semantic 
relations between a Vt-N structure and its con-
text information are very important for differen-
tiating between dependent relations. Although 
the verb-argument relation of VP structures is 
well understood, it is not clear what kind of se-
mantic relations result in NP structures. In the 
next sub-section, we consider three questions: 
What sets of nouns accept verbs as their modifi-
ers? Is it possible to identify the semantic types 
of such pairs of verbs and nouns? What are their 
semantic relations? 
1.1 Problem Analysis 
Analysis of the instances of NP(Vt-N) structures 
in the Sinica Treebank reveals the following four 
types of semantic structures, which are used in 
the design of our classifier. 
 
Type 1. Telic(Vt) + Host(N): Vt denotes the 
telic function (purpose) of the head noun N, e.g., 
928
?? /research ?? /tool; ?? /explore ?
/machine; ?/gamble ?/house; ??/search ?
?/program. The telic function must be a salient 
property of head nouns, such as tools, buildings, 
artifacts, organizations and people. To identify 
such cases, we need to know the types of nouns 
which take telic function as their salient property. 
Furthermore, many of the nouns are monosyl-
labic words, such as ?/people, ?/instruments, 
?/machines. 
Type 2. Host-Event(Vt) + Attribute(N): 
Head nouns are attribute nouns that denote the 
attributes of the verb, e.g., ??/research ??
/method (method of research); ??/attack ??
/strategy (attacking strategy); ??/write ??
/context (context of writing); ?/gamble ?/rule 
(gambling rules). An attribute noun is a special 
type of noun. Semantically, attribute nouns de-
note the attribute types of objects or events, such 
as weight, color, method, and rule. Syntactically, 
attribute nouns do not play adjectival roles (Liu, 
2008). By contrast, object nouns may modify 
nouns. The number of attributes for events is 
limited. If we could discover all event-attribute 
relations, then we can solve this type of construc-
tion. 
Type 3. Agentive + Host: There is only a lim-
ited number of such constructions and the results 
of the constructions are usually ambiguous, e.g., 
??/fried rice (NP), ??/shouting sound. The 
first example also has the VP reading. 
Type 4. Apposition + Affair: Head nouns are 
event nouns and modifiers are verbs of apposi-
tion events, e.g. ??/collide ??/accident, ?
? /destruct ?? /movement, ?? /hate ??
/behavior. There is finite number of event nouns.  
 
Furthermore, when we consider verbal modi-
fiers, we find that verbs can play adjectival roles 
in Chinese without inflection, but not all verbs 
play adjectival roles. According to Chang et al. 
(2000) and our observations, adjectival verbs are 
verbs that denote event types rather than event 
instances; that is, they denote a class of events 
which that are concepts in an upper-level ontolo-
gy. One important characteristic of adjectival 
verbs is that they have conjunctive morphologi-
cal structures, i.e., the words are conjunct with 
two nearly synonymous verbs, e.g., ?/study ?
/search (research), ? /explore ? /detect (ex-
plore), and ?/search ?/find (search). Therefore, 
we need a morphological classifier that can de-
tect the conjunctive morphological structure of a 
verb by checking the semantic parity of two 
morphemes of the verb. 
Based on our analysis, we designed a Vt-N 
classifier that incorporates the above features to 
solve the problem. However, there is a data 
sparseness problem because of the limited size of 
the current Treebank. In other words, Treebank 
cannot provide enough training data to train a 
classifier properly. To resolve the problem, we 
should mine useful information from all availa-
ble resources. 
The remainder of this paper is organized as 
follows. Section 2 provides a review of related 
works. In Section 3, we describe the disambigua-
tion model with our selected features, and intro-
duce a strategy for handling unknown words. We 
also propose a learning approach for a large-
scale unlabeled corpus. In Section 4, we report 
the results of experiments conducted to evaluate 
the proposed Vt-N classifier on different feature 
combinations and learning approaches. Section 5 
contains our concluding remarks. 
2 Related Work 
Most works on V-N structure identification focus 
on two types of relation classification: modifier-
head relations and predicate-object relations (Wu, 
2003; Qiu, 2005; Chen, 2008; Chen et al., 2008; 
Yu et al., 2008). They exclude the independent 
structure and conjunctive head-head relation, but 
the cross-bracket relation does exist between two 
adjacent words in real language. For example, if 
???/all over  ??/world ? was included in the 
short sentence ???/all over  ??/world ??
/countries?, it would be an independent structure. 
A conjunctive head-head relation between a verb 
and a noun is rare. However, in the sentence ??
? ?? ? ? ??? (Both service and equip-
ment are very thoughtful.), there is a conjunctive 
head-head relation between the verb ? ?
/service and the noun ??/equipment. Therefore, 
we use four types of relations to describe the V-
N structures in our experiments. The symbol 
?H/X? denotes a predicate-object relation; ?X/H? 
denotes a modifier-head relation; ?H/H? denotes 
a conjunctive head-head relation; and ?X/X? de-
notes an independent relation. 
Feature selection is an important task in V-N 
disambiguation. Hence, a number of studies have 
suggested features that may help resolve the am-
biguity of V-N structures (Zhao and Huang, 1999; 
Sun and Jurafsky, 2003; Chiu et al., 2004; Qiu, 
2005; Chen, 2008). Zhao and Huang used lexi-
cons, semantic knowledge, and word length in-
929
formation to increase the accuracy of identifica-
tion. Although they used the Chinese thesaurus 
CiLin (Mei et al., 1983) to derive lexical seman-
tic knowledge, the word coverage of CiLin is 
insufficient. Moreover, none of the above papers 
tackle the problem of unknown words. Sun and 
Jurafsky exploit the probabilistic rhythm feature 
(i.e., the number of syllables in a word or the 
number of words in a phrase) in their shallow 
parser. Their results show that the feature im-
proves the parsing performance, which coincides 
with our analysis in Section 1.1. Chiu et al.?s 
study shows that the morphological structure of 
verbs influences their syntactic behavior. We 
follow this finding and utilize the morphological 
structure of verbs as a feature in the proposed Vt-
N classifier. Qiu?s approach uses an electronic 
syntactic dictionary and a semantic dictionary to 
analyze the relations of V-N phrases. However, 
the approach suffers from two problems: (1) low 
word coverage of the semantic dictionary and (2) 
the semantic type classifier is inadequate. Finally, 
Chen proposed an automatic VN combination 
method with features of verbs, nouns, context, 
and the syllables of words. The experiment re-
sults show that the method performs reasonably 
well without using any other resources. 
Based on the above feature selection methods, 
we extract relevant knowledge from Treebank to 
design a Vt-N classifier. However we have to 
resolve the common problem of data sparseness. 
Learning knowledge by analyzing large-scale 
unlabeled data is necessary and proved useful in 
previous works (Wu, 2003; Chen et al., 2008; Yu 
et al., 2008). Wu developed a machine learning 
method that acquires verb-object and modifier-
head relations automatically. The mutual infor-
mation scores are then used to prune verb-noun 
whose scores are below a certain threshold. The 
author found that accurate identification of the 
verb-noun relation improved the parsing perfor-
mance by 4%. Yu et al. learned head-modifier 
pairs from parsed data and proposed a head-
modifier classifier to filter the data. The filtering 
model uses the following features: a PoS-tag pair 
of the head and the modifier; the distance be-
tween the head and the modifier; and the pres-
ence or absence of punctuation marks (e.g., 
commas, colons, and semi-colons) between the 
head and the modifier. Although the method im-
proves the parsing performance by 2%, the filter-
ing model obtains limited data; the recall rate is 
only 46.35%. The authors also fail to solve the 
problem of Vt-N ambiguity. 
Our review of previous works and the obser-
vations in Section 1.1 show that lexical words, 
semantic information, the syllabic length of 
words, neighboring PoSs and the knowledge 
learned from large-scale data are important for 
Vt-N disambiguation. We consider more features 
for disambiguating Vt-N structures than previous 
studies. For example, we utilize (1) four relation 
classification in a real environment, including 
?X/H?, ?H/X?, ?X/X? and ?H/H? relations; (2) un-
known word processing of Vt-N words (includ-
ing semantic type predication and morph-
structure predication); (3) unsupervised data se-
lection (a simple and effective way to extend 
knowledge); and (4) supervised knowledge cor-
rection, which makes the extracted knowledge 
more useful. 
3 Design of the Disambiguation Model 
The disambiguation model is a Vt-N relation 
classifier that classifies Vt-N relations into ?H/X? 
(predicate-object relations), ?X/H? (modifier-
head relations), ?H/H? (conjunctive head-head 
relations), or ?X/X? (independent relations). We 
use the Maximum Entropy toolkit (Zhang, 2004) 
to construct the classifier. The advantage of us-
ing the Maximum Entropy model is twofold: (1) 
it has the flexibility to adjust features; and (2) it 
provides the probability values of the classifica-
tion, which can be easily integrated into our 
PCFG parsing model. 
In the following sections, we discuss the de-
sign of our model for feature selection and ex-
traction, unknown word processing, and world 
knowledge learning. 
3.1 Feature Selection and Extraction 
We divide the selected features into five groups: 
PoS tags of Vt and N, PoS tags of the context, 
words, semantics, and additional information. 
Table 1 shows the feature types and symbol nota-
tions. We use symbols of t1 and t2 to denote the 
PoS of Vt and N respectively. The context fea-
ture is neighboring PoSs of Vt and N: the sym-
bols of t-2 and t-1 represent its left PoSs, and the 
symbol t3 and t4 represent its right PoSs. The se-
mantic feature is the lexicon?s semantic type ex-
tracted from E-HowNet sense expressions 
(Huang et al., 2008). For example, the E-
HowNet expression of ? ? ? /vehicles? is 
{LandVehicle|? :quantity={mass|? }}, so its 
semantic type is {LandVehicle|?}. We discuss 
the model?s performance with different feature 
combinations in Section 4. 
930
Feature  Feature Description 
PoS PoS of Vt and N 
t1; t2 
Context Neighboring PoSs 
t-2; t-1; t3; t4 
Word Lexical word 
w1; w2 
Semantic Semantic type of word 
st1; st2 
Additional 
Information 
Morphological structure of verb 
Vmorph 
 Syllabic length of noun 
Nlen 
 
Table 1. The features used in the Vt-N classifier 
 
The example in Figure 1 illustrates feature la-
beling of a Vt-N structure. First, an instance of a 
Vt-N structure is identified from Treebank. Then, 
we assign the semantic type of each word with-
out considering the problem of sense ambiguity 
for the moment. This is because sense ambigui-
ties are partially resolved by PoS tagging, and 
the general problem of sense disambiguation is 
beyond the scope of this paper. Furthermore, 
Zhao and Huang (1999) demonstrated that the 
retained ambiguity does not have an adverse im-
pact on identification. Therefore, we keep the 
ambiguous semantic type for future processing. 
 
zhe        zaochen     xuexi  zhongwen    DE    fongchao 
this         cause        learn     Chinese                    trend 
?This causes the trend of learning Chinese.? 
 
Figure 1. An example of a tree with a Vt-N struc-
ture 
 
Table 2 shows the labeled features for ???
/learn  ??/Chinese? in Figure 1. The column x  
and y describe relevant features in ???/learn? 
and ???/Chinese? respectively. Some features 
are not explicitly annotated in the Treebank, e.g., 
the semantic types of words and the morphologi-
cal structure of verbs. We propose labeling 
methods for them in the next sub-section. 
Feature Type x y 
Word w1=?? w2=?? 
PoS t1=VC t2=Na 
Semantic st1=study|?? st2=language|?? 
Context t-2=Nep; t-1=VK; t3=DE; t4=Na 
Additional 
Information 
Vmorph=VV Nlen=2 
Relation Type  rt = H/X 
 
Table 2. The feature labels of Vt-N pair in Figure 
1 
3.2 Unknown Word Processing 
In Chinese documents, 3% to 7% of the words 
are usually unknown (Sproat and Emerson, 
2003). By ?unknown words?, we mean words not 
listed in the dictionary. More specifically, in this 
paper, unknown words means words without se-
mantic type information (i.e., E-HowNet expres-
sions) and verbs without morphological structure 
information. Therefore, we propose a method for 
predicting the semantic types of unknown words, 
and use an affix database to train a morph-
structure classifier to derive the morphological 
structure of verbs. 
 
Morph-Structure Predication of Verbs: We 
use data analyzed by Chiu et al. (2004) to devel-
op a classifier for predicating the morphological 
structure of verbs. There are four types of mor-
phological structures for verbs: the coordinating 
structure (VV), the modifier-head structure (AV), 
the verb-complement structure (VR), and the 
verb-object structure (VO). To classify verbs 
automatically, we incorporate three features in 
the proposed classifier, namely, the lexeme itself, 
the prefix and the suffix, and the semantic types 
of the prefix and the suffix. Then, we use train-
ing data from the affix database to train the clas-
sifier. Table 3 shows an example of the unknown 
verb ???? /disseminate? and the morph-
structure classifier shows that it is a ?VR? type. 
 
Feature Feature Description 
Word=??? Lexicon 
PW=?? Prefix word 
PWST={disseminate|??} Semantic Type of 
Prefix Word ?? 
SW=? Suffix Word 
SWST={Vachieve|??} Semantic Type of 
Suffix Word ? 
 
Table 3. An example of an unknown verb and 
feature templates for morph-structure predication 
931
 
Semantic Type Provider: The system ex-
ploits WORD, PoS, affix and E-HowNet infor-
mation to obtain the semantic types of words (see 
Figure 2). If a word is known and its PoS is giv-
en, we can usually find its semantic type by 
searching the E-HowNet database. For an un-
known word, the semantic type of its head mor-
pheme is its semantic type; and the semantic type 
of the head morpheme is obtained from E-
HowNet1. For example, the unknown word ??
?? /disseminate?, its prefix word is ???
/disseminate? and we learn that its semantic type 
is {disseminate|??} from E-HowNet. There-
fore, we assign {disseminate|??} as the se-
mantic type of ???? /disseminate?. If the 
word or head morpheme does not exist in the 
affix database, we assign a general semantic type 
based on its PoS, e.g., nouns are {thing|??} 
and verbs are {act|??}. In this matching pro-
cedure, we may encounter multiple matching 
data of words and affixes. Our strategy is to keep 
the ambiguous semantic type for future pro-
cessing. 
 
Input: WORD, PoS 
Output: Semantic Type (ST) 
procedure STP(WORD, PoS) 
 (* Initial Step *) 
 ST := null; 
 (* Step 1: Known word *) 
 if WORD already in E-HowNet then 
  ST := EHowNet(WORD, PoS); 
 else if WORD in Affix database then 
  ST := EHowNet(affix of WORD, PoS); 
 (* Step 2 : Unknown word *) 
 if ST is null and PoS is ?Vt? then 
  ST := EHowNet(prefix of WORD, PoS);  
 else if ST is null and PoS is ?N? then 
  ST := EHowNet(suffix of WORD, PoS);  
 (* Step 3 : default *) 
 if ST is null and PoS is ?Vt? then 
  ST := ?act|???; 
 else if ST is null and PoS is ?N? then 
  ST := ?thing|??? 
 (* Finally *) 
 STP := ST; 
end; 
 
Figure 2. The Pseudo-code of the Semantic Type 
Predication Algorithm. 
 
1 The E-HowNet function in Figure 2 will return a null ST 
value where words do not exist in E-HowNet or Affix data-
base. 
3.3 Learning World Knowledge 
Based on the features discussed in the previous 
sub-section, we extract prior knowledge from 
Treebank to design the Vt-N classifier. However, 
the training suffers from the data sparseness 
problem. Furthermore most ambiguous Vt-N 
relations are resolved by common sense 
knowledge that makes it even harder to construct 
a well-trained system. An alternative way to ex-
tend world knowledge is to learn from large-
scale unlabeled data (Wu, 2003; Chen et al., 
2008; Yu et al., 2008). However, the unsuper-
vised approach accumulates errors caused by 
automatic annotation processes, such as word 
segmentation, PoS tagging, syntactic parsing, 
and semantic role assignment. Therefore, how to 
extract useful knowledge accurately is an im-
portant issue. 
To resolve the error accumulation problem, we 
propose two methods: unsupervised NP selection 
and supervised error correction. The NP selec-
tion method exploits the fact that an intransitive 
verb followed by a noun can only be interpreted 
as an NP structure, not a VP structure. It is easy 
to find such instances with high precision by 
parsing a large corpus. Based on the selection 
method, we can extend contextual knowledge 
about NP(V+N) and extract nouns that take ad-
jectival verbs as modifiers. The error correction 
method involves a small amount of manual edit-
ing in order to make the data more useful and 
reduce the number of errors in auto-extracted 
knowledge. The rationale is that, in general, high 
frequency Vt-N word-bigram is either VP or NP 
without ambiguity. Therefore, to obtain more 
accurate training data, we simply classify each 
high frequency Vt-N word bigram into a unique 
correct type without checking all of its instances. 
We provide more detailed information about the 
method in Section 4.3. 
4 Experiments and Results 
4.1 Experimental Setting 
We classify Vt-N structures into four types of 
syntactic structures by using the bracketed in-
formation (tree structure) and dependency rela-
tion (head-modifier) to extract the Vt-N relations 
from treebank automatically. The resources used 
in the experiments as follows. 
Treebank: The Sinica Treebank contains 
61,087 syntactic tree structures with 361,834 
words. We extracted 9,017 instances of Vt-N 
structures from the corpus. Then, we randomly 
                                                 
932
selected 1,000 of the instances as test data and 
used the remainder (8,017 instances) as training 
data. Labeled information of word segmentation 
and PoS-tagging were retained and utilized in the 
experiments. 
E-HowNet: E-HowNet contains 99,525 lexi-
cal semantic definitions that provide information 
about the semantic type of words. We also im-
plement the semantic type predication algorithm 
in Figure 2 to generate the semantic types of all 
Vt and N words, including unknown words. 
Affix Data: The database includes 13,287 ex-
amples of verbs and 27,267 examples of nouns, 
each example relates to an affix. The detailed 
statistics of the verb morph-structure categoriza-
tion are shown in Table 4. The data is used to 
train a classifier to predicate the morph-structure 
of verbs. We found that verbs with a conjunctive 
structure (VV) are more likely to play adjectival 
roles than the other three types of verbs. The 
classifier achieved 87.88% accuracy on 10-fold 
cross validation of the above 13,287 verbs. 
 
 VV VR AV VO 
Prefix 920 2,892 904 662 
Suffix 439 7,388 51 31 
 
Table 4. The statistics of verb morph-structure 
categorization 
 
Large Corpus: We used a Chinese parser to 
analyze sentence structures automatically. The 
auto-parsed tree structures are used in Experi-
ment 2 (described in the Sub-section 4.3). We 
obtained 1,262,420 parsed sentences and derived 
237,843 instances of Vt-N structure as our da-
taset (called as ASBC). 
4.2 Experiment 1: Evaluation of the Vt-N 
Classifier 
In this experiment, we used the Maximum En-
tropy Toolkit (Zhang, 2004) to develop the Vt-N 
classifier. Based on the features discussed in Sec-
tion 3.1, we designed five models to evaluate the 
classifier?s performance on different feature 
combinations.  
The features and used in each model are de-
scribed below. The feature values shown in 
brackets refer to the example in Figure 1. 
? M1 is the baseline model. It uses PoS-tag 
pairs as features, such as (t1=VC, t2=Na). 
? M2 extends the M1 model by adding con-
text features of (t-1=VK, t1=VC), (t2=Na, 
t3=DE), (t-2=Nep, t-1=VK, t1=VC), (t2=Na, 
t3=DE, t4=Na) and (t-1=VK, t3=DE). 
? M3 extends the M2 model by adding lexi-
con features of (w1=??, t1=VK, w2=?
?, t2=Na), (w1???, w2=??), (w1=?
?) and (w2=??). 
? M4 extends the M3 model by adding se-
mantic features of (st1=study|??, t1=VK , 
st2=language|?? , t2=Na), (st1=study|?
? , t1=VK) and (st2=language| ? ? , 
t2=Na). 
? M5 extends the M4 model by adding two 
features: the morph-structure of verbs; and 
the syllabic length of nouns 
(Vmorph=?VV?) and (Nlen=2). 
Table 5 shows the results of using different 
feature combinations in the models. The symbol 
P1(%) is the 10-fold cross validation accuracy of 
the training data, and the symbol P2(%) is the 
accuracy of the test data. By adding contextual 
features, the accuracy rate of M2 increases from 
59.10% to 72.30%. The result shows that contex-
tual information is the most important feature 
used to disambiguate VP, NP and independent 
structures. The accuracy of M2 is approximately 
the same as the result of our PCFG parser be-
cause both systems use contextual information. 
By adding lexical features (M3), the accuracy 
rate increases from 72.30% to 80.20%. For se-
mantic type features (M4), the accuracy rate in-
creases from 80.20% to 81.90%. The 1.7% in-
crease in the accuracy rate indicates that seman-
tic generalization is useful. Finally, in M5, the 
accuracy rate increases from 81.90% to 83.00%. 
The improvement demonstrates the benefits of 
using the verb morph-structure and noun length 
features. 
 
Models Feature for Vt-N P1(%) P2(%) 
M1 (t1,t2) 61.94 59.10 
M2 + (t-1,t1) (t2,t3) (t-2,t-
1,t1) (t2,t3,t4) (t-1,t3) 
76.59 72.30 
M3 + (w1,t1,w2,t2) (w1,w2) 
(w2) (w1) 
83.55 80.20 
M4 + (st1,t1,st2,t2) (st1,t1) 
(st2, t2) 
84.63 81.90 
M5 + (Vmorph) (Nlen) 85.01 83.00 
 
Table 5. The results of using different feature 
combinations 
 
933
Next, we consider the influence of unknown 
words on the Vt-N classifier. The statistics shows 
that 17% of the words in Treebank lack semantic 
type information, e.g., ??/StayIn, ??/fill, ?
?/posted, and ??/tied. The accuracy of the 
Vt-N classifier declines by 0.7% without seman-
tic type information for unknown words. In other 
words, lexical semantic information improves the 
accuracy of the Vt-N classifier. Regarding the 
problem of unknown morph-structure of words, 
we observe that over 85% of verbs with more 
than 2 characters are not found in the affix data-
base. If we exclude unknown words, the accura-
cy of the Vt-N prediction decreases by 1%. 
Therefore, morph-structure information has a 
positive effect on the classifier. 
4.3 Experiment 2: Using Knowledge Ob-
tained from Large-scale Unlabeled Data 
by the Selection and Correction Meth-
ods. 
In this experiment, we evaluated the two 
methods discussed in Section 3, i.e., unsuper-
vised NP selection and supervised error correc-
tion. We applied the data selection method (i.e., 
distance=1, with an intransitive verb (Vi) fol-
lowed by an object noun (Na)) to select 46,258 
instances from the ASBC corpus and compile a 
dataset called Treebank+ASBC-Vi-N. Table 6 
shows the performance of model 5 (M5) on the 
training data derived from Treebank and Tree-
bank+ASBC-Vi-N. The results demonstrate that 
learning more nouns that accept verbal modifiers 
improves the accuracy. 
 
 
Treebank+ 
ASBC-Vi-N 
Treebank 
size of training 
instances 
46,258 8,017 
M5 - P2(%) 83.90 83.00 
 
Table 6. Experiment results on the test data for 
various knowledge sources 
 
We had also try to use the auto-parsed results 
of the Vt-N structures from the ASBC corpus as 
supplementary training data for train M5. It de-
grades the model?s performance by too much 
error when using the supplementary training data. 
To resolve the problem, we utilize the supervised 
error correction method, which manually correct 
errors rapidly because high frequency instances 
(w1, w2) rarely have ambiguous classifications in 
different contexts. So we designed an editing tool 
to correct errors made by the parser in the classi-
fication of high frequency Vt-N word pairs. After 
the manual correction operation, which takes 40 
man-hours, we assign the correct classifications 
(w1, t1, w2, t2, rt) for 2,674 Vt-N structure types 
which contains 10,263 instances to creates the 
ASBC+Correction dataset. Adding the corrected 
data to the original training data increases the 
precision rate to 88.40% and reduces the number 
of errors by approximately 31.76%, as shown in 
the Treebank+ASBC+Correction column of Ta-
ble 7. 
 
 
Treebank+ 
ASBC+Correction 
Treebank+ 
ASBC-Vi-N 
Treebank 
size of train-
ing instances 
56,521 46,258 8,017 
M5 - P2(%) 88.40 83.90 83.00 
 
Table 7. Experiment results of classifiers with 
different training data 
 
We also used the precision and recall rates to 
evaluate the performance of the models on each 
type of relation. The results are shown in Table 8. 
Overall, the Treebank+ASBC+Correction meth-
od achieves the best performance in terms of the 
precision rate. The results for Treebank+ASBC-
Vi-N show that the unsupervised data selection 
method can find some knowledge to help identi-
fy NP structures. In addition, the proposed mod-
els achieve better precision rates than the PCFG 
parser. The results demonstrate that using our 
guidelines to design a disambiguation model to 
resolve the Vt-N problem is successful. 
 
 H/X X/H X/X 
Treebank 
R(%) 91.11 67.90 74.62 
P(%) 84.43 78.57 81.86 
Treebank+ 
ASBC-Vi-N 
R(%) 91.00 72.22 71.54 
P(%) 84.57 72.67 85.71 
Treebank+ 
ASBC+Correction 
R(%) 98.62 60.49 83.08 
P(%) 86.63 88.29 93.51 
PCFG 
R(%) 90.54 23.63 80.21 
P(%) 78.24 73.58 75.00 
 
Table 8. Performance comparison of different 
classification models. 
 
4.4 Experiment 3: Integrating the Vt-N 
classifier with the PCFG Parser 
Identifying Vt-N structures correctly facilitates 
statistical parsing, machine translation, infor-
934
mation retrieval, and text classification. In this 
experiment, we develop a baseline PCFG parser 
based on feature-based grammar representation 
by Hsieh et al. (2012) to find the best tree struc-
tures (T) of a given sentence (S). The parser then 
selects the best tree according to the evaluation 
score Score(T,S) of all possible trees. If there are 
n PCFG rules in the tree T, the Score(T,S) is the 
accumulation of the logarithmic probabilities of 
the i-th grammar rule (RPi). Formula 1 shows the 
baseline PCFG parser. 
 
?
=
=
n
i
iRPSTScore
1
)(),(  (1)
 
 
The Vt-N models can be easily integrated into 
the PCFG parser. Formula 2 represents the inte-
grated structural evaluation model. We combine 
RPi and VtNPi with the weights w1 and w2 re-
spectively, and set the value of w2 higher than 
that of w1. VtNPi is the probability produced by 
the Vt-N classifier for the type of the relation 
between Vt-N bigram determined by the PCFG 
parsing. The classifier is triggered when a [Vt, N] 
structure is encountered; otherwise, the Vt-N 
model is not processed. 
 
?
=
?+?=
n
i
ii VtNPwRPwSTScore
1
21 )(),(  (2)
 
 
The results of evaluating the parsing model in-
corporated with the Vt-N classifier (see Formula 
2) are shown in Table 9 and Table 10. The P2 is 
the accuracy of Vt-N classification on the test 
data. The bracketed f-score (BF2) is the parsing 
performance metric. Based on these results, the 
integrated model outperforms the PCFG parser in 
terms of Vt-N classification. Because the Vt-N 
classifier only considers sentences that contain 
Vt-N structures, it does not affect the parsing 
accuracies of other sentences.  
 
 
PCFG +  
M5 (Treebank) PCFG 
P2(%) 80.68 77.09 
BF(%) 83.64 82.80 
 
Table 9. The performance of the PCFG parser 
with and without model M5 from Treebank. 
 
2 The evaluation formula is (BP*BR*2) / (BP+BR), where 
BP is the precision and BR is the recall. 
 
PCFG +  
M5 (Treebank+ASBC+Correction) PCFG 
P2(%) 87.88 77.09 
BF(%) 84.68 82.80 
 
Table 10. The performance of the PCFG parser 
with and without model M5 from Tree-
bank+ASBC+Correction data set. 
 
4.5 Experiment 4: Comparison of Various 
Chinese Parsers 
In this experiment, we give some comparison 
results in various parser: ?PCFG Parser? (base-
line), ?CDM Parser? (Hsieh et al., 2012), and 
?Berkeley Parser? (Petrov et al., 2006). The CDM 
parser achieves the best score in Traditional Chi-
nese Parsing task of SIGHAN Bake-offs 2012 
(Tseng et al., 2012). Petrov?s parser (as Berkeley, 
version is 2009 1.1) is the best PCFG parser for 
non-English language and it is an open source. In 
our comparison, we use the same training data 
for training models and parse the same test da-
taset based on the gold standard word segmenta-
tion and PoS tags. We have already discussed the 
PCFG parser in Section 4.4. As for CDM parser, 
we retrain relevant model in our experiments. 
And since Berkeley parser take different tree 
structure (Penn Treebank format), we transform 
the experimental data to Berkeley CoNLL format 
and re-train a new model with parameters ?-
treebank CHINESE -SMcycles 4? 3 from training 
data. Moreover we use ?-useGoldPOS? parame-
ters to parse test data and further transform them 
to Sinica Treebank style from the Berkeley par-
ser?s results. The different tree structure formats 
of Sinica Treebank and Penn Treebank are as 
follow: 
 
Sinica Treebank:  
S(NP(Head:Nh:??)|Head:VC:??
|NP(Head:Na:??)) 
 
Penn Treebank:  
( (S (NP (Head:Nh (Nh ??))) (Head:VC 
(VC ??)) (NP (Head:Na (Na ??))))) 
 
The evaluation results on the testing data, i.e. 
in P2 metric, are as follows. The accuracy of 
PCFG parser is 77.09%; CDM parser reaches 
78.45% of accuracy; and Berkeley parser is 
70.68%. The results show that the problem of Vt-
3 The ?-treebank CHINESE -SMcycles 4? is the best train-
ing parameter in Traditional Chinese Parsing task of 
SIGHAN Bake-offs 2012. 
                                                 
                                                 
935
N cannot be well solved by any general parser 
including CDM parser and Berkeley?s parser. It 
is necessary to have a different approach aside 
from the general model. So we set the target for a 
better model for Vt-N classification which can be 
easily integrated into the existing parsing model. 
So far our best model achieved the P2 accuracy 
of 87.88%.  
5 Concluding Remarks 
We have proposed a classifier to resolve the am-
biguity of Vt-N structures. The design of the 
classifier is based on three important guidelines, 
namely, adopting linguistically motivated fea-
tures, using all available resources, and easy in-
tegration into parsing model. After analyzing the 
Vt-N structures, we identify linguistically moti-
vated features, such as lexical words, semantic 
knowledge, the morphological structure of verbs, 
neighboring parts-of-speech, and the syllabic 
length of words. Then, we design a classifier to 
verify the usefulness of each feature. We also 
resolve the technical problems that affect the 
prediction of the semantic types and morph-
structures of unknown words. In addition, we 
propose a framework for unsupervised data se-
lection and supervised error correction for learn-
ing more useful knowledge. Our experiment re-
sults show that the proposed Vt-N classifier sig-
nificantly outperforms the PCFG Chinese parser 
in terms of Vt-N structure identification. Moreo-
ver, integrating the Vt-N classifier with a parsing 
model improves the overall parsing performance 
without side effects. 
In our future research, we will exploit the pro-
posed framework to resolve other parsing diffi-
culties in Chinese, e.g., N-N combination. We 
will also extend the Semantic Type Predication 
Algorithm (Figure 2) to deal with all Chinese 
words. Finally, for real world knowledge learn-
ing, we will continue to learn more useful 
knowledge by auto-parsing to improve the pars-
ing performance. 
Acknowledgments 
We thank the anonymous reviewers for their val-
uable comments. This work was supported by 
National Science Council under Grant NSC99-
2221-E-001-014-MY3. 
Reference 
Li-li Chang, Keh-Jiann Chen, and Chu-Ren Huang. 
2000. Alternation Across Semantic Fields: A Study 
on Mandarin Verbs of Emotion. Internal Journal of 
Computational Linguistics and Chinese Language 
Processing (IJCLCLP), 5(1):61-80. 
Keh-Jiann Chen, Chu-Ren Huang, Chi-Ching Luo, 
Feng-Yi Chen, Ming-Chung Chang, Chao-Jan 
Chen, , and Zhao-Ming Gao. 2003. Sinica Tree-
bank: Design Criteria, Representational Issues and 
Implementation. In (Abeille 2003) Treebanks: 
Building and Using Parsed Corpora, pages 231-
248. Dordrecht, the Netherlands: Kluwer. 
Li-jiang Chen. 2008. Autolabeling of VN Combina-
tion Based on Multi-classifier. Journal of Comput-
er Engineering, 34(5):79-81. 
Wenliang Chen, Daisuke Kawahara, Kiyotaka 
Uchimoto, Yujjie Zhang, and Hitoshi Isahara. 2008. 
Dependency Parsig with Short Dependency Rela-
tions in Unlabeled Data.  In Proceedings of the 
third International Joint Conference on Natural 
Language Processing (IJCNLP). pages 88-94.. 
Chih-ming Chiu, Ji-Chin Lo, and Keh-Jiann Chen. 
2004. Compositional Semantics of Mandarin Affix 
Verbs. In Proceedings of the Research on Compu-
tational Linguistics Conference (ROCLING), pages 
131-139. 
Yu-Ming Hsieh, Ming-Hong Bai, Jason S. Chang, and 
Keh-Jiann Chen. 2012. Improving PCFG Chinese 
Parsing with Context-Dependent Probability Re-
estimation, In Proceedings of the Second CIPS-
SIGHAN Joint Conference on Chinese Language 
Processing, pages 216?221. 
Shu-Ling Huang, You-Shan Chung, Keh-Jiann Chen. 
2008. E-HowNet: the Expansion of HowNet. In 
Proceedings of the First National HowNet work-
shop, pages 10-22, Beijing, China. 
Chunhi Liu, Xiandai Hanyu Shuxing Fanchou Yianjiu 
(??????????). Chengdu: Bashu Books, 
2008. 
Jiaju Mei, Yiming Lan, Yunqi Gao, and Yongxian 
Ying. 1983. A Dictionary of Synonyms. Shanghai 
Cishu Chubanshe.  
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proceesings of 
COLING/ACL, pages 433-400. 
Likun Qiu. 2005. Constitutive Relation Analysis for 
V-N Phrases. Journal of Chinese Language and 
Computing, 15(3):173-183. 
Richard Sproat and Thomas Emerson, 2003. The first 
International Chinese Word Segmentation Bakeoff. 
In Proceedings of the Second SIGHAN Workshop 
on Chinese Language Processing, pages 133-143. 
Honglin Sun and Dan Jurafsky. 2003. The Effect of 
Rhythm on Structural Disambiguation in Chinese. 
In Proceedings of the Second SIGHAN Workshop 
on Chinese Language Processing, pages 39-46. 
936
Yuen-Hsieh Tseng, Lung-Hao Lee, and Liang-Chih 
Yu. 2012. Tranditional Chinese Parsing Evaluation 
at SIGHAN Bake-offs 2012. In Proceedings of the 
Second CIPS-SIGHAN Joint Conference on Chi-
nese Language Processing, pages 199-205. 
Andi Wu. 2003. Learning Verb-Noun Relations to 
Improve Parsing. In Proceedings of the Second 
SIGHAN workshop on Chinese Language Pro-
cessing, pages 119-124. 
Kun Yu, Daisuke Kawahara, and Sadao Kurohashi. 
2008. Chinese Dependency Parsing with Large 
Scale Automatically Constructed Case Structures, 
In Proceedings of the 22nd International Confer-
ence on Computational Linguistics (COLING2008), 
pages 1049-1056. 
Jun Zhao and Chang-ning Huang. 1999. The Com-
plex-feature-based Model for Acquisition of VN-
construction Structure Templates. Journal of Soft-
ware, 10(1):92-99. 
Le Zhang. 2004. Maximum Entropy Modeling 
Toolkit for Python and C++. Reference Manual. 
937
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 352?356,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
TransAhead: A Computer-Assisted Translation and Writing Tool 
G
G
*Chung-chi Huang      +Ping-che Yang **Keh-jiann Chen       ++Jason S. Chang 
  
*ISA, NTHU, HsinChu, Taiwan, R.O.C. **IIS, Academia Sinica, Taipei, Taiwan, R.O.C. 
+III, Taipei, Taiwan, R.O.C. ++CS, NTHU, HsinChu, Taiwan, R.O.C. 
{*u901571,+maciaclark,++jason.jschang}@gmail.com; **kchen@iis.sinica.edu.tw 
  
 
Abstract 
We introduce a method for learning to predict 
text completion given a source text and partial 
translation. In our approach, predictions are 
offered aimed at alleviating users? burden on 
lexical and grammar choices, and improving 
productivity. The method involves learning 
syntax-based phraseology and translation 
equivalents. At run-time, the source and its 
translation prefix are sliced into ngrams to 
generate and rank completion candidates, 
which are then displayed to users. We present 
a prototype writing assistant, TransAhead, that 
applies the method to computer-assisted 
translation and language learning. The 
preliminary results show that the method has 
great potentials in CAT and CALL with 
significant improvement in translation quality 
across users. 
1 Introduction 
More and more language workers and learners use 
the MT systems on the Web for information 
gathering and language learning. However, web 
translation systems typically offer top-1 
translations (which are usually far from perfect) 
and hardly interact with the user. 
Text translation could be achieved more 
interactively and effectively if a system considered 
translation as a collaborative between the machine 
generating suggestions and the user accepting or 
overriding on those suggestions, with the system 
adapting to the user?s action. 
Consider the source sentence ?????????
????????? (We play an important role in 
closing this deal). The best man-machine 
interaction is probably not the one used by typical 
existing MT systems. A good working 
environment might be a translation assistant that 
offers suggestions and gives the user direct control 
over the target text. 
We present a system, TransAhead1, that learns 
to predict and suggest lexical translations (and 
their grammatical patterns) likely to follow the 
ongoing translation of a source text, and adapts to 
the user?s choices. Example responses of 
TransAhead to the source sentence ????????
?????????? and two partial translations  
are shown in Figure 1. The responses include text 
and grammatical patterns (in all-cap labels 
representing parts-of-speech). TransAhead 
determines and displays the probable subsequent 
grammatical constructions and partial translations 
in the form of parts-of-speech and words (e.g., 
?IN[in] VBG[close,?]? for keywords ?play role? 
where lexical items in square brackets are lemmas 
of potential translations) in a pop-up. TransAhead 
learns these constructs and translations during 
training. 
At run-time, TransAhead starts with a source 
sentence, and iterates with the user, making 
predictions on the grammar patterns and lexical 
translations, while adapting to the user?s 
translation choices to resolve ambiguities in the 
source sentence related to word segmentation and 
word sense. In our prototype, TransAhead 
mediates between users and suggestion modules to 
translation quality and  productivity. 
2 Related Work 
Computer Assisted Translation (CAT) has been an 
area of active research. We focus on offering 
suggestions during the  translation process with  an 
                                                           
1
 http://140.114.214.80/theSite/TransAhead/ (Chrome only) 
352
 Figure 1. Example TransAhead responses to a source text under the translation (a) ?we? and (b) ?we play an 
important role?. Note that the grammar/text predictions of (a) and (b) are not placed directly under the caret (current 
input focus) for space limit. (c) and (d) depict predominant grammar constructs which follow and (e) summarizes 
the confident translations of the source?s character-based ngrams. The frequency of grammar pattern is shown in 
round brackets while the history (i.e., keyword) based on the user input is shown in shades. 
 
emphasis on language learning. Specifically, our 
goal is to build a translation assistant to help 
translator (or learner-translator) with inline 
grammar help and translation. Unlike recent 
research focusing on professional (e.g., Brown and 
Nirenburg, 1990), we target on both professional 
and student translators. 
More recently, interactive MT (IMT) systems 
have begun to shift the user?s role from post-
editing machine output to collaborating with the 
machine to produce the target text. Foster et al
(2000) describe TransType, a pioneering system 
that supports next word predictions. Along the 
similar line, Koehn (2009) develops caitra which 
predicts and displays phrasal translation 
suggestions one phrase at a time. The main 
difference between their systems and TransAhead 
is that we also display grammar patterns to provide 
the general patterns of predicted translations so a 
student translator can learn and become more 
proficient. 
Recent work has been done on using fully-
fledged statistical MT systems to produce target 
hypotheses completing user-validated translation 
prefix in IMT paradigm. Barrachina et al (2008) 
investigate the applicability of different MT 
kernels within IMT framework. Nepveu et al 
(2004) and Ortiz-Martinez et al (2011) further 
exploit user feedbacks for better IMT systems and 
user experience. Instead of triggered by user 
correction, our method is triggered by word 
delimiter and assists both translation and learning 
the target language. 
In contrast to the previous CAT research, we 
present a writing assistant that suggests grammar 
constructs as well as lexical translations following 
users? partial translation, aiming to provide users 
with choice to ease mental burden and enhance 
performance. 
3 The TransAhead System 
3.1 Problem Statement 
We focus on predicting a set of grammar patterns 
with lexical translations likely to follow the current 
partial target translation of a source text. The 
predictions will be examined by a human user 
directly. Not to overwhelm the user, our goal is to 
return a reasonable-sized set of predictions that 
contain suitable word choices and grammatical 
patterns to choose and learn from. Formally, 
Problem Statement: We are given a target-
language reference corpus Ct, a parallel corpus Cst, 
a source-language text S, and its translation prefix 
Tp. Our goal is to provide a set of predictions based 
on Ct and Cst likely to further translate S in terms of 
grammar and text. For this, we transform S and Tp 
into sets of ngrams such that the predominant 
grammar constructs with suitable translation 
options following Tp are likely to be acquired. 
(b) 
Source text: ???????????????? 
(a) 
Pop-up predictions/suggestions: 
we MD VB[play, act, ..]  (41369), ? 
we VBP[play, act, ..] DT  (13138), ? 
we VBD[play, act, ..] DT  (8139), ? 
Pop-up predictions/suggestions: 
play role IN[in] VBG[close, end, ..] (397), ? 
important role IN[in] VBG[close, end, ..]  (110), ? 
role IN[in] VBG[close, end, ..] (854), ? 
(c) 
(d) 
(e) 
Patterns for ?we?: 
we MD VB (41369), ?, 
we VBP DT (13138), ?, 
we VBD DT (8139), ? 
Patterns for ?we play an important role?: 
play role IN[in] DT (599), 
play role IN[in] VBG (397), ?, 
important role IN[in] VBG (110), ?, 
role IN[in] VBG (854), ? 
Translations for the source text: 
????: we, ?; ????: close, end, ?;  ?; ????: 
play, ?; ????: critical, ?; ?; ???: act, ?; ?; 
???: heavy, ?; ???: will, wish, ?; ???: cents, ?; 
???: outstanding, ? 
Input your source text and start to interact with TransAhead! 
353
3.2 Learning to Find Pattern and Translation 
In the training stage, we find and store syntax-
based phraseological tendencies and translation 
pairs. These patterns and translations are intended 
to be used in a real-time system to respond to user 
input speedily. 
First, we part of speech tag sentences in Ct. 
Using common phrase patterns (e.g., the 
possessive noun one?s in ?make up one?s mind?) 
seen in grammar books, we resort to parts-of-
speech (POS) for syntactic generalization. Then, 
we build up inverted files of the words in Ct for the 
next stage (i.e., pattern grammar generation). Apart 
from sentence and position information, a word?s 
lemma and POS are also recorded. 
Subsequently, we use the procedure in Figure 2 
to generate grammar patterns following any given 
sequence of words, either contiguous or skipped. 
 
 
Figure 2. Automatically generating pattern grammar. 
 
The algorithm first identifies the sentences 
containing the given sequence of words, query. 
Iteratively, Step (3) performs an AND operation on 
the inverted file, InvList, of the current word wi and 
interInvList, a previous intersected results. 
After that, we analyze query?s syntax-based 
phraseology (Step (5)). For each element of the 
form ([wordPosi(w1),?, wordPosi(wn)], sentence 
number) denoting the positions of query?s words in 
the sentence, we generate grammar pattern 
involving replacing words in the sentence with 
POS tags and words in wordPosi(wi) with lemmas, 
and extracting fixed-window 2  segments 
surrounding query from the transformed sentence. 
The result is a set of grammatical patterns (i.e., 
syntax-based phraseology) for the query. The 
procedure finally returns top N predominant 
                                                           
2
 Inspired by (Gamon and Leacock, 2010). 
syntactic patterns of the query. Such patterns 
characterizing the query?s word usages in the spirit 
of pattern grammar in (Hunston and Francis, 2000) 
and are collected across the target language. 
In the fourth and final stage, we exploit Cst for 
bilingual phrase acquisition, rather than a manual 
dictionary, to achieve better translation coverage 
and variety. We obtain phrase pairs through a 
number of steps, namely, leveraging IBM models 
for bidirectional word alignments, grow-diagonal-
final heuristics to extract phrasal equivalences 
(Koehn et al, 2003). 
3.3 Run-Time Grammar and Text Prediction 
Once translation equivalents and phraseological 
tendencies are learned, they are stored for run-time 
reference. TransAhead then predicts/suggests the 
following grammar and text of a translation prefix 
given the source text using the procedure in Figure 
3. 
 
 
Figure 3. Predicting pattern grammar and 
translations at run-time. 
 
We first slice the source text S into character-
level ngrams, represented by {si}. We also find the 
word-level ngrams of the translation prefix Tp. But 
this time we concentrate on the ngrams, may 
skipped, ending with the last word of Tp (i.e., 
pivoted on the last word) since these ngrams are 
most related to the subsequent grammar patterns. 
Step (3) and (4) retrieve translations and patterns 
learned from Section 3.2. Step (3) acquires the 
target-language active vocabulary that may be used 
to translate the source. To alleviate the word 
boundary issue in MT (Ma et al (2007)), the word 
boundary in our system is loosely decided. Initially, 
TransAhead non-deterministically segments the 
source text using character ngrams for translations 
and proceeds with collaborations with the user to 
obtain the segmentation for MT and to complete 
the translation. Note that Tp may reflect some 
translated segments, reducing the size of the active 
vocabulary, and that a user vocabulary of 
preference (due to users? domain knowledge or 
procedure PatternFinding(query,N,Ct) (1)  interInvList=findInvertedFile(w1 of query) 
for each word wi in query except for w1 (2)     InvList=findInvertedFile(wi) (3a)   newInterInvList= ? ; i=1; j=1 
(3b)   while i<=length(interInvList) and j<=lengh(InvList) 
(3c)      if interInvList[i].SentNo==InvList[j].SentNo 
(3d)         Insert(newInterInvList, interInvList[i],InvList[j]) 
else 
(3e)         Move i,j accordingly 
(3f)    interInvList=newInterInvList 
(4) Usage= ?  
for each element in interInvList 
(5)     Usage+={PatternGrammarGeneration(element,Ct)} (6) Sort patterns in Usage in descending order of frequency 
(7) return the N patterns in Usage with highest frequency 
procedure MakePrediction(S,Tp) 
(1) Assign sliceNgram(S) to {si} (2) Assign sliceNgramWithPivot(Tp) to {tj} (3) TransOptions=findTranslation({si},Tp) (4) GramOptions=findPattern({tj}) (5) Evaluate translation options in TransOptions 
           and incorporate them into GramOptions (6) Return GramOptions 
354
errors of the system) may be exploited for better 
system performance. In addition, Step (4) extracts 
patterns preceding with the history ngrams of {tj}. 
In Step (5), we first evaluate and rank the 
translation candidates using linear combination: 
( ) ( )( ) ( )1 1 1 2 2   i i pP t s P s t P t T? ?? + + ?  
where ?i is combination weight, P1 and P2 are 
translation and language model respectively, and t 
is one of the translation candidates under S and Tp. 
Subsequently, we incorporate the lemmatized 
translation candidates according to their ranks into 
suitable grammar constituents in GramOptions. 
For example, we would include ?close? in pattern 
?play role IN[in] VBG? as ?play role IN[in] 
VBG[close]?. 
At last, the algorithm returns the representative 
grammar patterns with confident translations 
expected to follow the ongoing translation and 
further translate the source. This algorithm will be 
triggered by word delimiter to provide an 
interactive CAT and CALL environment. Figure 1 
shows example responses of our working prototype. 
4 Preliminary Results 
In developing TransAhead, we used British 
National Corpus and Hong Kong Parallel Text as 
target-language reference corpus and parallel 
training corpus respectively, and deployed GENIA 
tagger for lemma and POS analyses. 
To evaluate TransAhead in CAT and CALL, we 
introduced it to a class of 34 (Chinese) college 
freshmen learning English as foreign language. We 
designed TransAhead to be accessible and intuitive, 
so the user training tutorial took only one minute. 
After the tutorial, the participants were asked to 
translate 15 Chinese texts from (Huang et al, 2011) 
(half with TransAhead assistance called experi-
mental group, and the other without any system 
help whatsoever called control group). The 
evaluation results show that the experimental 
group achieved much better translation quality than 
the control group with an average BLEU score 
(Papineni et al, 2002) of 35.49 vs. 26.46. 
Admittedly, the MT system Google Translate 
produced translations with a higher BLEU score of 
44.82. 
Google Translate obviously has much more 
parallel training data and bilingual translation 
knowledge. No previous work in CAT uses Google 
Translate for comparison. Although there is a 
difference in average translation quality between 
the experimental TransAhead group and the 
Google Translate, it is not hard for us to notice the 
source sentences were better translated by 
language learners with the help of TransAhead. 
Take the sentence  ????????????????
?? for example. A total of 90% of the participants 
in the experimental group produced more 
grammatical and fluent translations (see Figure 4) 
than that (?We conclude this transaction plays an 
important role?) by Google Translate. 
 
 
Figure 4. Example translations with 
TransAhead assistance. 
 
Post-experiment surveys indicate that (a) the 
participants found Google Translate lack human-
computer interaction while TransAhead is intuitive 
to collaborate with in translation/writing; (b) the 
participants found TransAhead grammar and 
translation predictions useful for their immediate 
task and for learning; (c) interactivity made the 
translation and language learning a fun process 
(like image tagging game of (von Ahn and Dabbish, 
2004)) and the participants found TransAhead very 
recommendable and would like to use it again in 
future translation tasks. 
5 Summary 
We have introduced a method for learning to offer 
grammar and text predictions expected to assist the 
user in translation and writing. We have 
implemented and evaluated the method. The 
preliminary results are encouragingly promising. 
As for the further work, we intend to evaluate and 
improve our system further in learner productivity 
in terms of output quality, typing speed, and the 
amount of using certain keys such as delete and 
backspace. 
Acknowledgement 
This study is conducted under the ?Project Digital 
Convergence Service Open Platform? of the 
Institute for Information Industry which is 
subsidized by the Ministry of Economy Affairs of 
the Republic of China. 
1. we play(ed) a critical role in closing this/the deal. 
2. we play(ed) a critical role in sealing this/the deal. 
3. we play(ed) an important role in ending this/the deal. 
4. we play(ed) an important role in closing this/the deal. 
355
References 
S. Barrachina, O. Bender, F. Casacuberta, J. Civera, E. 
Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Tomas, E. 
Vidal, and J.-M. Vilar. 2008. Statistical approaches 
to computer-assisted translation. Computational 
Linguistics, 35(1): 3-28. 
R. D. Brown and S. Nirenburg. 1990. Human-computer 
interaction for semantic disambiguation. In 
Proceedings of COLING, pages 42-47. 
G. Foster, P. Langlais, E. Macklovitch, and G. Lapalme. 
2002. TransType: text prediction for translators. In 
Proceedings of ACL Demonstrations, pages 93-94. 
M. Gamon and C. Leacock. 2010. Search right and thou 
shalt find ? using web queries for learner error 
detection. In Proceedings of the NAACL Workshop. 
C.-C. Huang, M.-H. Chen, S.-T. Huang, H.-C. Liou, and 
J. S. Chang. 2011. GRASP: grammar- and syntax-
based pattern-finder in CALL. In Proceedings of 
ACL Workshop. 
S. Hunston and G. Francis. 2000. Pattern Grammar: A 
Corpus-Driven Approach to the Lexical Grammar of 
English. Amsterdam: John Benjamins. 
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical 
phrase-based translation. In Proceedings of NAACL. 
P. Koehn. 2009. A web-based interactive computer 
aided translation tool. In Proceedings of ACL. 
Y. Ma, N. Stroppa, and A. Way. 2007. Bootstrapping 
word alignment via word packing. In Proceedings of 
ACL. 
L. Nepveu, G. Lapalme, P. Langlais, and G. Foster. 
2004. Adaptive language and translation models for 
interactive machine translation. In Proceedings of 
EMNLP. 
D. Ortiz-Martinez, L. A. Leiva, V. Alabau, I. Garcia-
Varea, and F. Casacuberta. 2011. An interactive 
machine translation system with online learning. In 
Proceedings of ACL System Demonstrations, pages 
68-73. 
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002. Bleu: 
a method for automatic evaluation of machine 
translation. In Proceedings of ACL, pages 311-318. 
L. von Ahn and L. Dabbish. 2004. Labeling images with 
a computer game. In Proceedings of CHI. 
356
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 55?60,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
DOMCAT: A Bilingual Concordancer for Domain-Specific Computer 
Assisted Translation 
Ming-Hong Bai1,2 Yu-Ming Hsieh1,2 Keh-Jiann Chen1 Jason S. Chang2 
1 Institute of Information Science, Academia Sinica, Taiwan 
2 Department of Computer Science, National Tsing-Hua University, Taiwan 
mhbai@sinica.edu.tw, morris@iis.sinica.edu.tw, 
kchen@iis.sinica.edu.tw, jason.jschang@gmail.com 
 
Abstract 
In this paper, we propose a web-based 
bilingual concordancer, DOMCAT 1 , for 
domain-specific computer assisted 
translation. Given a multi-word expression 
as a query, the system involves retrieving 
sentence pairs from a bilingual corpus, 
identifying translation equivalents of the 
query in the sentence pairs (translation 
spotting) and ranking the retrieved sentence 
pairs according to the relevance between 
the query and the translation equivalents. 
To provide high-precision translation 
spotting for domain-specific translation 
tasks, we exploited a normalized 
correlation method to spot the translation 
equivalents. To ranking the retrieved 
sentence pairs, we propose a correlation 
function modified from the Dice coefficient 
for assessing the correlation between the 
query and the translation equivalents. The 
performances of the translation spotting 
module and the ranking module are 
evaluated in terms of precision-recall 
measures and coverage rate respectively. 
1 Introduction 
A bilingual concordancer is a tool that can retrieve 
aligned sentence pairs in a parallel corpus whose 
source sentences contain the query and the 
translation equivalents of the query are identified 
in the target sentences. It helps not only on finding 
translation equivalents of the query but also 
presenting various contexts of occurrence. As a 
result, it is extremely useful for bilingual 
                                                          
1 http://ckip.iis.sinica.edu.tw/DOMCAT/ 
lexicographers, human translators and second 
language learners (Bowker and Barlow 2004; 
Bourdaillet et al, 2010; Gao 2011).  
Identifying the translation equivalents, 
translation spotting, is the most challenging part of 
a bilingual concordancer. Recently, most of the 
existing bilingual concordancers spot translation 
equivalents in terms of word alignment-based 
method. (Jian et al, 2004; Callison-Burch et al, 
2005; Bourdaillet et al, 2010). However, word 
alignment-based translation spotting has some 
drawbacks. First, aligning a rare (low frequency) 
term may encounter the garbage collection effect 
(Moore, 2004; Liang et al, 2006) that cause the 
term to align to many unrelated words. Second, the 
statistical word alignment model is not good at 
many-to-many alignment due to the fact that 
translation equivalents are not always correlated in 
lexical level. Unfortunately, the above effects will 
be intensified in a domain-specific concordancer 
because the queries are usually domain-specific 
terms, which are mostly multi-word low-frequency 
terms and semantically non-compositional terms. 
Wu et al (2003) employed a statistical 
association criterion to spot translation equivalents 
in their bilingual concordancer. The association-
based criterion can avoid the above mentioned 
effects. However, it has other drawbacks in 
translation spotting task. First, it will encounter the 
contextual effect that causes the system incorrectly 
spot the translations of the strongly collocated 
context. Second, the association-based translation 
spotting tends to spot the common subsequence of 
a set of similar translations instead of the full 
translations. Figure 1 illustrates an example of 
contextual effect, in which ?Fan K'uan? is 
incorrectly spotted as part of the translation of the 
query term ?????? ? (Travelers Among 
Mountains and Streams), which is the name of the 
55
painting painted by ?Fan K'uan/?? ? since the 
painter?s name is strongly collocated with the 
name of the painting. 
 
Sung , Travelers Among Mountains and Streams , Fan 
K'uan 
???????? 
Figure 1. ?Fan K'uan? may be incorrectly spotted as 
part of the translation of ???????, if pure 
association method is applied. 
 
Figure 2 illustrates an example of common 
subsequence effect, in which ??????? (the 
River During the Qingming Festival/ Up the River 
During Qingming) has two similar translations as 
quoted, but the Dice coefficient tends to spot the 
common subsequences of the translations. 
(Function words are ignored in our translation 
spotting.) 
 
Expo 2010 Shanghai-Treasures of Chinese Art Along 
the River During the Qingming Festival 
2010?????????????????? 
Oversized Hanging Scrolls and Handscrolls Up the 
River During Qingming 
???????????? 
Figure 2. The Dice coefficient tends to spot the common 
subsequences ?River During Qingming?. 
Bai et al (2009) proposed a normalized 
frequency criterion to extract translation 
equivalents form sentence aligned parallel corpus. 
This criterion takes lexical-level contexture effect 
into account, so it can effectively resolve the above 
mentioned effect. But the goal of their method is to 
find most common translations instead of spotting 
translations, so the normalized frequency criterion 
tends to ignore rare translations. 
In this paper, we propose a bilingual 
concordancer, DOMCAT, for computer assisted 
domain-specific term translation. To remedy the 
above mentioned effects, we extended the 
normalized frequency of Bai et al (2009) to a 
normalized correlation criterion to spot translation 
equivalents. The normalized correlation inherits 
the characteristics of normalized frequency and is 
adjusted for spotting rare translations. These 
characteristics are especially important for a 
domain-specific bilingual concordancer to spot 
translation pairs of low-frequency and semantically 
non-compositional terms.  
The remainder of this paper is organized as 
follows.  Section 2 describes the DOMCAT system. 
In Section 3, we describe the evaluation of the 
DOMCAT system. Section 4 contains some 
concluding remarks. 
2 The DOMCAT System 
Given a query, the DOMCAT bilingual 
concordancer retrieves sentence pairs and spots 
translation equivalents by the following steps: 
 
1. Retrieve the sentence pairs whose source 
sentences contain the query term. 
2. Extract translation candidate words from the 
retrieved sentence pairs by the normalized 
correlation criterion. 
3. Spot the candidate words for each target 
sentence and rank the sentences by 
normalized the Dice coefficient criterion. 
 
In step 1, the query term can be a single word, a 
phrase, a gapped sequence and even a regular 
expression. The parallel corpus is indexed by the 
suffix array to efficiently retrieve the sentences.  
The step 2 and step 3 are more complicated and 
will be described from Section 2.1 to Section 2.3. 
2.1 Extract Translation Candidate Words 
After the queried sentence pairs retrieved from the 
parallel corpus, we can extract translation 
candidate words from the sentence pairs. We 
compute the local normalized correlation with 
respect to the query term for each word e in each 
target sentence. The local normalized correlation 
is defined as follows: 
 
?
?
??
??
??
???
f
q
f
qfeq
j
i
f j
f i
fep
fepelnc ||)|(
||)|(),,;(      (1) 
 
where q denotes the query term, f denotes the 
source sentence and e denotes the target sentence,  
? is a small smoothing factor. The probability p(e|f) 
is the word translation probability derived from the 
entire parallel corpus by IBM Model 1 (Brown et 
al., 1993). The sense of local normalized 
correlation of e can be interpreted as the 
probability of word e being part of translation of 
the query term q under the condition of sentence 
pair (e, f). 
56
Once the local normalized correlation is 
computed for each word in retrieved sentences, we 
compute the normalized correlation on the 
retrieved sentences. The normalized correlation is 
the average of all lnc values and defined as follows:  
 
?
?
?
n
i
iielncnenc 1
)()( ),,;(1);( feqq            (2) 
 
where n is the number of retrieved sentence pairs.  
After the nc values for the words of the retrieved 
target sentences are computed, we can obtain a 
translation candidate list by filtering out the words 
with lower nc values. 
To compare with the association-based method, 
we also sorted the word list by the Dice coefficient 
defined as follows: 
 
)()(
),(2),( q
qq freqefreq
efreqedice ??           (3) 
 
where freq is frequency function which  computes 
frequencies from the parallel corpus. 
 
Candidate words NC 
mountain 0.676 
stream 0.442 
traveler 0.374 
among 0.363 
sung 0.095 
k'uan 0.090 
Figure 3(a). Candidate words sorted by nc values. 
 
Candidate words Dice 
traveler 0.385 
reduced 0.176 
stream 0.128 
k'uan 0.121 
fan 0.082 
among 0.049 
mountain 0.035 
Figure 3(b). Candidate words sorted by Dice coefficient 
values. 
 
Figure 3(a) and (b) illustrate examples of 
translation candidate words of the query term ??
???? ? (Travelers Among Mountains and 
Streams) sorted by the nc values, NC, and the Dice 
coefficients respectively. The result shows that the 
normalized correlation separated the related words 
from unrelated words much better than the Dice 
coefficient. 
The rationale behind the normalized correlation 
is that the nc value is the strength of word e 
generated by the query compared to that of 
generated by the whole sentence. As a result, the 
normalized correlation can easily separate the 
words generated by the query term from the words 
generated by the context. On the contrary, the Dice 
coefficient counts the frequency of a co-occurred 
word without considering the fact that it could be 
generated by the strongly collocated context.  
 
2.2 Translation Spotting 
Once we have a translation candidate list and 
respective nc values, we can spot the translation 
equivalents by the following spotting algorithm. 
For each target sentence, first, spot the word with 
highest nc value. Then extend the spotted sequence 
to the neighbors of the word by checking their nc 
values of neighbor words but skipping function 
words. If the nc value is greater than a threshold ?, 
add the word into spotted sequence. Repeat the 
extending process until no word can be added to 
the spotted sequence. 
The following is the pseudo-code for the 
algorithm: 
 
S is the target sentence 
H is the spotted word sequence 
?is the threshold of translation candidate words 
 
Initialize: 
H? ?
emax?S[0] Foreach ei in S: If nc(ei) > nc(emax):  
emax ??ei 
If nc(emax )??:?
add?emax?to?H 
Repeat until no word add to H 
ej?left?neighbor?of?H?
If?nc(ej?)??:?
? ???add?ej?to?H?
ek?right?neighbor?of?H?
If nc(?ek?)???:?
? ???add?ek?to?H?
Figure 4: Pseudo-code of translation spotting process. 
 
57
2.3 Ranking 
The ranking mechanism of a bilingual 
concordancer is used to provide the most related 
translation of the query on the top of the outputs 
for the user. So, an association metric is needed to 
evaluate the relations between the query and the 
spotted translations. The Dice coefficient is a 
widely used measure for assessing the association 
strength between a multi-word expression and its 
translation candidates. (Kupiec, 1993; Smadja et 
al., 1996; Kitamura and Matsumoto, 1996; 
Yamamoto and Matsumoto, 2000; Melamed, 2001)  
The following is the definition of the Dice 
coefficient: 
 
)()(
),(2),( qt
qtqt freqfreq
freqdice ??            (4) 
 
where q denotes a multi-word expression to be 
translated, t denotes a translation candidate of q. 
However, the Dice coefficient has the common 
subsequence effect (as mentioned in Section 1) due 
to the fact that the co-occurrence frequency of the 
common subsequence is usually larger than that of 
the full translation; hence, the Dice coefficient 
tends to choose the common subsequence. 
To remedy the common subsequence effect, we 
introduce a normalized frequency for a spotted 
sequence defined as follows: 
 
?
?
?
n
i
iilnfnf
1
)()( ),,;(),( feqtqt            (5) 
 
where lnf is a function which compute normalized 
frequency locally in each sentence. The following 
is the definition of lnf: 
 
?
???
??
tH
feqfeqt
e
elnclnf )),,;(1(),,;(      (6) 
 
where H is the spotted sequence of the sentence 
pair (e,f), H-t are the words in H but not in t. The 
rationale behind lnf function is that: when counting 
the local frequency of t in a sentence pair, if t is a 
subsequence of H, then the count of t should be 
reasonably reduced by considering the strength of 
the correlation between the words in H-t and the 
query. 
Then, we modify the Dice coefficient by 
replacing the co-occurrence frequency with 
normalized frequency as follows: 
 
)()(
),(2),( qt
qtqt freqfreq
nfnf_dice ??        (7) 
 
The new scoring function, nf_dice(t,q), is 
exploited as our criterion for assessing the 
association strength between the query and the 
spotted sequences. 
3 Experimental Results 
3.1 Experimental Setting 
We use the Chinese/English web pages of the 
National Palace Museum 2  as our underlying 
parallel corpus. It contains about 30,000 sentences 
in each language. We exploited the Champollion 
Toolkit (Ma et al, 2006) to align the sentence pairs. 
The English sentences are tokenized and 
lemmatized by using the NLTK (Bird and Loper, 
2004) and the Chinese sentences are segmented by 
the CKIP Chinese segmenter (Ma and Chen, 2003). 
To evaluate the performance of the translation 
spotting, we selected 12 domain-specific terms to 
query the concordancer. Then, the returned spotted 
translation equivalents are evaluated against a 
manually annotated gold standard in terms of recall 
and precision metrics. We also build two different 
translation spotting modules by using the GIZA++ 
toolkit (Och and Ney, 2000) with the 
intersection/union of the bidirectional word 
alignment as baseline systems. 
To evaluate the performance of the ranking 
criterion, we compiled a reference translation set 
for each query by collecting the manually 
annotated translation spotting set and selecting 1 to 
3 frequently used translations. Then, the outputs of 
each query are ranked by the nf_dice function and 
evaluated against the reference translation set. We 
also compared the ranking performance with the 
Dice coefficient. 
3.2 Evaluation of Translation Spotting 
We evaluate the translation spotting in terms of the 
Recall and Precision metrics defined as follows: 
 
                                                          
2 http://www.npm.gov.tw 
58
||
||
1
)(
1
)()(
?
?
?
? ?? n
i
i
g
n
i
ii
g
H
HHRecall                     (8) 
||
||
1
)(
1
)()(
?
?
?
? ?? n
i
i
n
i
ii
g
H
HHPrecision                     (9) 
 
where i denotes the index of the retrieved 
sentence, )(iH  is the spotted sequences of the ith 
sentence returned by the concordancer,  and )(igH is 
the gold standard spotted sequences of the ith 
sentence. Table 1 shows the evaluation of 
translation spotting for normalized correlation, NC, 
compared with the intersection and union of 
GIZA++ word alignment. The F-score of the 
normalized correlation is much higher than that of 
the word alignment methods. It is noteworthy that 
the normalized correlation increased the recall rate 
without losing the precision rate. This may indicate 
that the normalized correlation can effectively 
conquer the drawbacks of the word alignment-
based translation spotting and the association-
based translation spotting mentioned in Section 1. 
 
 Recall Precision F-score 
Intersection 0.4026 0.9498 0.5656 
Union 0.7061 0.9217 0.7996 
NC 0.8579 0.9318 0.8933 
Table 1. Evaluation of the translation spotting 
queried by 12 domain-specific terms. 
 
We also evaluate the queried results of each 
term individually (as shown in Table 2). As it 
shows, the normalized correlation is quite stable 
for translation spotting. 
 
Query terms GIZA Intersection GIZA Union NC R P F R P F R P F 
??? (Maogong cauldron) 0.27 0.86 0.41 0.87 0.74 0.80  0.92 0.97 0.94 
????(Jadeite cabbage) 0.48 1.00 0.65 1.00 0.88 0.94  0.98 0.98 0.98 
?????(Travelers Among Mountains and Streams) 0.28 0.75 0.41 1.00 0.68 0.81 0.94 0.91 0.92
?????(Up the River During Qingming) 0.22 0.93 0.35 0.97 0.83 0.89  0.99 0.91 0.95
???(Ching-te-chen) 0.50 0.87 0.63 0.73 0.31 0.44 1.00 0.69 0.82
??(porcelain) 0.53 0.99 0.69 0.93 0.64 0.76 0.78 0.96 0.86
??(cobalt blue glaze) 0.12 1.00 0.21 0.85 0.58 0.69 0.94 0.86 0.90
??(inscription) 0.20 0.89 0.32 0.71 0.34 0.46  0.88 0.95 0.91
????(Three Friends and a Hundred Birds) 0.58 0.99 0.73 1.00 0.97 0.99 1.00 0.72 0.84
??(wild cursive script) 0.42 1.00 0.59 0.63 0.80 0.71 0.84 1.00 0.91
???(Preface to the Orchid Pavilion Gathering) 0.33 0.75 0.46 0.56 0.50 0.53 0.78 1.00 0.88
????(Latter Odes to the Red Cliff) 0.19 0.50 0.27 0.75 0.46 0.57 0.94 0.88 0.91
Table 2. Evaluation of the translation spotting for each term
3.3 Evaluation of Ranking 
To evaluate the performance of a ranking function, 
we ranked the retrieved sentences of the queries by 
the function. Then, the top-n sentences of the 
output are evaluated in terms of the coverage rate 
defined as follows: 
?coverage  
queries of #
top-nin on  translatia findcan  queries of #   (10) 
 
The meaning of the coverage rate can be 
interpreted as: how many percent of the query can 
find an acceptable translation in the top-n results.  
We use the reference translations, as described in 
Section 3.1, as acceptable translation set for each 
query of our experiment. Table 3 shows the 
coverage rate of the nf_dice function compared 
with the Dice coefficient. As it shows, in the 
outputs ranked by the Dice coefficient, uses 
usually have to look up more than 3 sentences to 
find an acceptable translation; while in the outputs 
ranked by the nf_dice function, users can find an 
acceptable translation in top-2 sentences. 
 
59
 
 dice nf_dice 
top-1 0.42  0.92 
top-2 0.75  1.00 
top-3 0.92  1.00 
Table 3. Evaluation of the ranking criteria. 
4 Conclusion and Future Works 
In this paper, we proposed a bilingual 
concordancer, DOMCAT, designed as a domain-
specific computer assisted translation tool. We 
exploited a normalized correlation which 
incorporate lexical level information into 
association-based method that effectively avoid the 
drawbacks of the word alignment-based translation 
spotting as well as the association-based translation 
spotting. 
In the future, it would be interesting to extend 
the parallel corpus to the internet to retrieve more 
rich data for the computer assisted translation. 
References  
Bai, Ming-Hong, Jia-Ming You, Keh-Jiann Chen, Jason 
S. Chang. 2009. Acquiring Translation Equivalences 
of Multiword Expressions by Normalized Correlation 
Frequencies. In Proceedings of EMNLP, pages 478-
486. 
Bird, Steven and Edward Loper. 2004. NLTK: The 
Natural Language Toolkit. In Proceedings of ACL, 
pages 214-217. 
Bourdaillet, Julien, St?phane Huet, Philippe Langlais 
and Guy Lapalme. 2010. TRANSSEARCH: from a 
bilingual concordancer to a translation finder. 
Machine Translation, 24(3-4): 241?271. 
Bowker, Lynne, Michael Barlow. 2004. Bilingual 
concordancers and translation memories: A 
comparative evaluation. In Proceedings of the 
Second International Workshop on Language 
Resources for Translation Work, Research and 
Training , pages. 52-61. 
Brown, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2):263-311. 
Callison-Burch, Chris, Colin Bannard and Josh 
Schroeder. 2005. A Compact Data Structure for 
Searchable Translation Memories. In Proceedings of 
EAMT. 
Gao, Zhao-Ming. 2011. Exploring the effects and use of 
a Chinese?English parallel concordancer. Computer-
Assisted Language Learning 24.3 (July 2011): 255-
275. 
Jian, Jia-Yan, Yu-Chia Chang and Jason S. Chang. 2004. 
TANGO: Bilingual Collocational Concordancer. In 
Proceedings of ACL, pages 166-169. 
Kitamura, Mihoko and Yuji Matsumoto. 1996. 
Automatic Extraction of Word Sequence 
Correspondences in Parallel Corpora. In Proceedings 
of WVLC-4 pages 79-87. 
Kupiec, Julian. 1993. An Algorithm for Finding Noun 
Phrase Correspondences in Bilingual Corpora. In 
Proceedings of ACL, pages 17-22. 
Liang, Percy, Ben Taskar, Dan Klein. 2006. Alignment 
by Agreement. In Proceedings of HLT-NAACL 2006, 
pages 104-111, New York, USA. 
Ma, Wei-Yun and Keh-Jiann Chen. 2003. Introduction 
to CKIP Chinese word segmentation system for the 
first international Chinese word segmentation 
bakeoff. In Proceedings of the second SIGHAN 
workshop on Chinese language processing, pages 
168-171. 
Ma, Xiaoyi. 2006. Champollion: A Robust Parallel Text 
Sentence Aligner. In Proceedings of the Fifth 
International Conference on Language Resources 
and Evaluation. 
Melamed, Ilya Dan. 2001. Empirical Methods for 
Exploiting parallel Texts. MIT press. 
Moore, Robert C. 2004. Improving IBM Word-
Alignment Model 1. In Proceedings of ACL, pages 
519-526, Barcelona, Spain. 
Och, Franz J., Hermann Ney., 2000, Improved 
Statistical Alignment Models, In Proceedings of ACL, 
pages 440-447. Hong Kong. 
Smadja, Frank, Kathleen R. McKeown, and Vasileios 
Hatzivassiloglou. 1996. Translating Collocations for 
Bilingual Lexicons: A Statistical Approach. 
Computational Linguistics, 22(1):1-38. 
Wu, Jian-Cheng, Kevin C. Yeh, Thomas C. Chuang, 
Wen-Chi Shei, Jason S. Chang.  2003. TotalRecall: A 
Bilingual Concordance for Computer Assisted 
Translation and Language Learning. In Proceedings 
of ACL, pages 201-204. 
Yamamoto, Kaoru, Yuji Matsumoto. 2000. Acquisition 
of Phrase-level Bilingual Correspondence using 
Dependency Structure. In Proceedings of COLING, 
pages 933-939. 
60
