Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 889?896
Manchester, August 2008
Relational-Realizational Parsing
Reut Tsarfaty and Khalil Sima?an
Institute for Logic, Language and Computation, University of Amsterdam
Plantage Muidergracht 24, 1018TV, Amsterdam, The Netherlands
{rtsarfat,simaan}@science.uva.nl
Abstract
State-of-the-art statistical parsing models
applied to free word-order languages tend
to underperform compared to, e.g., pars-
ing English. Constituency-based mod-
els often fail to capture generalizations
that cannot be stated in structural terms,
and dependency-based models employ a
?single-head? assumption that often breaks
in the face of multiple exponence. In this
paper we suggest that the position of a con-
stituent is a form manifestation of its gram-
matical function, one among various pos-
sible means of realization. We develop the
Relational-Realizational approach to pars-
ing in which we untangle the projection
of grammatical functions and their means
of realization to allow for phrase-structure
variability and morphological-syntactic in-
teraction. We empirically demonstrate
the application of our approach to pars-
ing Modern Hebrew, obtaining 7% error
reduction from previously reported results.
1 Introduction
Many broad-coverage statistical parsers to date are
constituency-based with a Probabilistic Context-
Free Grammar (PCFG) or a Stochastic Tree Sub-
stitution Grammar (STSG) at their backbone. The
majority of such models belong to a Head-Driven
paradigm, in which a head constituent is gen-
erated first, providing a positional anchor for
subsequent (e.g., Markovian) sisters? generation.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Constituency-based models, lexicalized and un-
lexicalized alike, demonstrate state-of-the-art per-
formance for parsing English (Charniak, 1997;
Collins, 2003; Klein and Manning, 2003; Bod,
2003), yet a direct application of such models to
parsing less configurational languages often fails
to yield comparable results. The parameters of
such parsers capture generalizations that are eas-
ily stated in structural terms (e.g., subjects linearly
precede predicates, VPs dominate objects, etc.)
which may not be adequate for parsing languages
with less configurational character.
A different vein of research explores data-driven
dependency-based parsing methods (e.g., (Mc-
Donald et al, 2005)) which seem to be intuitively
more adequate for the task. It turns out, how-
ever, that even such models fail to provide the
desired remedy. Recent reports by (Nivre, 2007)
delineated a class of richly-inflected languages
with relatively free word-order (including Greek,
Basque, and Modern Standard Arabic) for which
the parsers performed poorly, regardless of the
parsing method used. The need for parsing meth-
ods that can effectively cope with such phenomena
doesn?t seem to have been eliminated by depen-
dency parsing ? perhaps quite the contrary.
The essential argument we promote here is that
in order to deal with the kind of variation that
is empirically observed cross-linguistically an al-
ternative view of the generation process is re-
quired. Our Relational-Realizational parsing pro-
posal, strongly inspired by Relational Grammar
(Perlmutter, 1982), takes grammatical relations
such as ?Subject? and ?Predicate? as central, primi-
tive notions of the syntactic representation, and re-
tains a distinction between the projection of such
relations and the means by which they are real-
ized. The grammar we develop here, formally
889
represented as a PCFG, articulates two alternating
generation phases: a Relational phase, in which a
clause-level category projects a monostratal Rela-
tional Network (RN) representation for the clause,
and a Realizational phase, in which the projected
relations are realized in a certain surface config-
uration. Paradigmatic morphosyntactic represen-
tations are constructed for all non-terminal nodes,
allowing for morphosyntactic interaction at vari-
ous levels of the syntactic parse tree.
We illustrate the application of our theoretical
reconstruction to the representation of clause-level
categories in Modern Hebrew (MH) and their in-
teraction with a handful of morphological features.
The treebank grammar resulting from our applica-
tion yields 13% error reduction relative to a tree-
bank PCFG which uses the same information in
the form of state-splits, and our best result shows
a 7% error reduction over the best parsing results
for MH so far. Through a quantitative and quali-
tative analysis we illustrate the advantages of the
Relational-Realizational approach and its poten-
tial promise for parsing other ?exotic? languages.
2 Background
Recent decades have seen a surge of interest in sta-
tistical models using a body of annotated text for
learning the distributions of grammatically mean-
ingful structures, in order to assign the most likely
ones to unseen sentences. Probabilistic Context
Free Grammars (PCFGs) have become popular in
the articulation of such models, and unlexicalized
treebank grammars (or representational variations
thereof) were shown to perform reasonably well on
English benchmark corpora (Johnson, 1998; Klein
and Manning, 2003).
A major leap in the performance of PCFG-based
statistical parsers has been introduced by the move
towards a Head-Driven paradigm (Collins, 2003;
Charniak, 1997), in which syntactic categories are
enriched with head information percolated up the
tree. The head-driven generation process allows
one to model the relation between the information
content of a constituent and the information con-
tent of its head-marked sister. At the same time,
such models introduce a bias with respect to the
positioning of a non-head constituent relative to its
head-marked sister. The vast improvement in pars-
ing results came about not without modeling costs,
e.g., additional ad-hoc modifications for capturing
complex structures such as conjunction.
An inherent difficulty with the application of
constituency-based parsing models is the implicit
assumption that the relation between the posi-
tion of a constituent and its grammatical func-
tion is fully predictable. For languages with
relatively free word-order, this assumption often
breaks down. Distinguishing, e.g., ?left? and
?right? distributions for constituents of the same
?sort? implicitly takes the position of a constituent
to be a primitive syntactic notion, and their gram-
matical function to be a secondary, derived one.
Theoretical accounts show that this may be insuf-
ficient (Perlmutter, 1982). A subsequent difficulty
with the head-driven paradigm, also shared by
dependency-based parsing methods, is the stipu-
lation that all grammatically relevant properties of
a phrase are recovered from a single head. In fact,
it is typologically established that grammatically
meaningful properties of a constituent may jointly
emerge from different surface forms dominated by
it (co-heads or multiple exponence (Zwicky, 1993;
Blevins, 2008)).1
The task we undertake here is to suggest a sta-
tistical generative parsing method which is linguis-
tically plausible as well as technologically viable
for parsing languages with relatively free word-
order and variable means of realization. In what
follows we remain within the computationally ef-
ficient framework of PCFGs, and propose a varia-
tion that draws on insights from syntactic and mor-
phological theories that have been explored cross-
linguistically.
3 Approach
3.1 Relational Grammars (RGs)
Relational Grammars (RGs) were introduced in the
early 80?s when attempts to find a universal def-
inition for notions such as a ?Subject? in terms
of various ?behavioral properties? seemed to have
failed (Perlmutter, 1982). The unsuccessful at-
tempts to recover an adequate definition of gram-
matical functions in structural terms led to a revival
of a view in which grammatical relations such as
?Subject? and ?Object? are primitive notions by
1We refrain here from referring to the increasingly popular
approach of discriminative parsing, firstly, because we are in-
terested in a generative parsing model that assigns a probabil-
ity distribution to all sentence-structure pairs in the language,
essentially allowing it to be used as a language model (e.g.,
in SR or SMT applications). Secondly, so far the features that
have been explored in these frameworks are mainly those eas-
ily stated in structural terms, with not much effort towards
modeling morphosyntactic interactions systematically.
890
which syntactic structures are defined (Postal and
Perlmutter, 1977). This view proved useful for de-
scriptive purposes, influencing the design of for-
malisms such as Arc-Pair grammars (Postal, 1982)
and LFG (Bresnan, 1979).
The two main primitive elements used in RGs
are (a) a set of nodes representing linguistic ele-
ments (which we refer to using upper case letters),
and (b) a set of names of grammatical relations
(which we refer to as gr
1
...gr
n
). RGs represent
the fact that a linguistic element bears a certain re-
lation to another element using a structure called
an ?Arc?, represented as [gr
i
(A,B)]. Arcs are rep-
resented as arrows, with A the head of the Arc and
B its tail, and a Relational Network (RN) is defined
to be a set of Arcs that share a single head.2
Now, a few theoretical observations are due.
Firstly, the essential difference between RGs and
dependency-based grammars is that RNs take the
linguistic element at the head of a network to be a
clause-level category, not a particular surface form.
The corresponding tails are then the various nom-
inals bearing the different grammatical relations
to the clause (including a ?Predicate?, a ?Subject?,
an ?Object?, etc.). In addition, RNs abstract away
from elements such as auxiliary verbs and particles
which do not have their own arc representation.
RGs also differ from phrase-structure grammars
in that their RNs are unordered. Therefore, linear
precedence need not play a role in stating general-
izations. RGs differ from both constituency- and
dependency-based formalisms in that they do not
weigh heavily the ?single-head? assumption ? RNs
may delineate a whole chunk as bearing a certain
grammatical relation to the clause.
The set-theoretic notion of RNs in RGs abstracts
away from surface phenomena crucial for generat-
ing phrase-structure trees. Thus, we next turn to
modeling how grammatical relations are realized.
3.2 Form, Function and Separation
Morphological phenomena such as suprasegmen-
tation, interdigitation, reduplication, subtractive
morphology, templatic morphology, and methathe-
sis demonstrate that it is sometimes impossible to
find a direct correspondence between a certain part
2RGs also define the notion of a stratum, a single level
of syntactic representation, and for the current discussion we
assume a monostratal representation. We do not claim that
our framework is capable of dealing with the full range of
phenomena multistratal RNs were shown to account for, yet
there is nothing in our proposal that excludes extending the
representation into a multistratal framework that does so.
of a word (a ?morpheme?) and the function it has
in altering the word?s meaning (Anderson, 1992).
Attempts to model such morphological phenom-
ena brought forward the hypothesis that ?form? and
?function? need not stand in one-to-one correspon-
dence, and that one is not necessarily immediately
predicted by the other. This hypothesis is known as
the ?Separation Hypothesis? (Beard, 1988). The
problem of modeling certain surface phenomena
then boils down to modeling form and function
correlations, bearing in mind that these may be
quite complex.
Bringing this general notion of separation into
the syntactic derivation, we propose to view the
position of a constituent in a phrase as its form and
the articulated grammatical relation as its function.
The task of learning the position of different con-
stituents realizing the grammatical relations in an
RN is now delegated to a statistical component. A
set of parameters which we refer to as ?configu-
ration? determines the syntactic position in which
each of the grammatical relations is to be realized.
3.3 Morphosyntactic Representations
In order to connect the abstract RN representation
with the constituents that syntactic parse trees are
?made of? we propose to view the internal nodes
of a tree as Morphosyntactic Paradigms. Our
morphosyntactic representation for constituents,
loosely inspired by (Anderson, 1992), is a struc-
tured representation of morphological and syntac-
tic properties for an internal node in the parse tree.
In our model, the morphological features asso-
ciated with a syntactic constituent are percolated
from its dominated surface forms, and we allow
the specification of head (PoS tag) information and
structural features such as vertical markovization.
Given the grammatical relation an element bears
to a clause, it is statistically feasible to learn the
morphosyntactic paradigm by which it is realized.
4 The Model
4.1 The Generative Model
Let S
p
? ?S
c
1
-gr
1
. . . S
c
n
-gr
n
? be a context-free
rule where S
p
is the morphosyntactic representa-
tion of a parent constituent, gr
1
...gr
n
are the gram-
matical relations forming its RN, and ?S
c
1
. . . S
c
n
?
are ordered morphosyntactic representations of the
child constituents bearing the respective relations
to the parent. Our grammar then conceptualizes
the generation of such a rule in three phases:
891
? Projection:
S
p
? {gr
i
}
n
i=1
@S
p
? Configuration:
{gr
i
}
n
i=1
@S
p
? ?gr
1
@S
p
. . . gr
n
@S
p
?
? Realization:
{gr
i
@S
p
? S
c
i
}
n
i=1
In the projection stage we generate the set of gram-
matical relations in the RN of a constituent. In
the configuration stage we order these grammat-
ical relations, and in realization we generate the
morphosyntactic representation of each child con-
stituent given the relation to its parent. Figure (1)
shows the application of this process to two clauses
bearing identical RNs that are in turn realized in
different possible configurations.
This three-step process does not generate func-
tional elements (such as auxiliary verbs and
special-purpose particles) that are outside of con-
stituents? RNs. We thus let the configuration stage
place obligatory or optional ?realizational slots?
between the ordered elements (marked gr
i
: gr
j
),
signalling periphrastic adpositions and/or modifi-
cation. Note that modification may introduce more
than one constituent, to be generated in realization.
? Projection:
S
p
? {gr
i
}
n
i=1
@S
p
? Configuration:
{gr
i
}
n
i=1
@S
p
?
?gr
0
: gr
1
@S
p
gr
1
@S
p
. . . gr
n
: gr
n+1
@S
p
?
? Realization:
{gr
i
@S
p
? S
c
i
}
n
i=1
{gr
i
: gr
i+1
@S
p
? ?S
c
i
1
...S
c
i
m
i
?}
n
i=0
In figure (2), the configuration stage reserves a slot
for an obligatory punctuation mark at the end of an
affirmative sentence. It further reserves a slot for
an optional adverbial modifier at a position com-
monly employed in MH for interjections.
In the current framework, grammatical rela-
tions may be realized in a certain surface position
via configuration, or using explicit morphological
marking per grammatical relation independently of
linear context. Figure (3) demonstrates how the
realization phase models the correlation between
grammatical relations and morphological informa-
tion percolated from dominated surface forms. In
particular, our model can capture the interaction
between marked features, e.g., the ?exclusive or?
relation between definiteness and accusativity in
marking direct objects in MH (Danon, 2001).
Finally, a conjunction structure in our model
is simply an RN representation of multiple mor-
phosyntactically equivalent conjuncts, as illus-
trated in figure (4). This modeling choice avoids
the need to stipulate a single head for such struc-
tures (cf. head-driven processes) and allows the
different conjuncts to share a realization distribu-
tion ? essentially implying homogeneity in the
assignment of heads and morphosyntactic features
across conjuncts.
4.2 The Probabilistic Model
Our probablistic model is a PCFG, where CFG
rules capture the three stages of generation. Ev-
ery time we apply our projection-configuration-
realization cycle we replace the rule probability
with the probabilities of the three stages, multi-
plied (n +?n
i=0
m
i
daugthers , gr
0
=gr
n+1
=null).
P (?S
c
i
1
, .., S
c
i
m
i
S
c
i
-gr
i
, S
c
i+1
1
, .., S
c
i+1
m
i+1
?
n
i=0
|S
p
) =
P ({gr
i
}
n
i=1
|S
p
)?
P (?gr
0
: gr
1
, g
1
, . . .?|{gr
i
}
n
i=1
, S
p
)?
?
n
i=1
P (S
c
i
|gr
i
, S
p
)?
P (?S
c
0
1
, ..., S
c
0
m
0
?|gr
0
: gr
1
, S
p
)?
?
n
i=1
P (?S
c
i
1
, ..., S
c
i
m
i
?|gr
i
: gr
i+1
, S
p
)
The multiplication implements the independence
assumption between form and function underlying
the Separation Hypothesis, and the conditioning
we articulate captures one possible way to model a
systematic many-to-many correspondence.
4.3 The Grammar
We use a probabilistic treebank grammar in which
the different parameters and their probability dis-
tributions are read off and estimated from the
treebank trees. Clause-level (or clause-like) con-
stituents such as S, SQ, FRAG, FRAGQ, inter-
nally complex VPs and a small number NPs can
head RNs. For the rest we use flat CFG rules.
We use a limited set of grammatical relations,
namely, ?Predicate?, ?Subject?, ?Object? and ?Com-
plement? ? making the distinction between a
nominal complement and a verbal (infinitival) one.
Our linguistic elements are morphosyntactic rep-
resentations of labeled non-terminal constituents,
where the morphosyntactic representations of con-
stituents incorporate morphological information
percolated from surface forms and syntactic infor-
mation about the constituent?s environment.
892
(a) S
NP-SBJ VP-PRD NP-OBJ
S
{PRD,OBJ,SBJ}@S
SBJ@S
NP
PRD@S
VP
OBJ@S
NP
(b) S
VP-PRD NP-SBJ NP-OBJ
S
{PRD,OBJ,SBJ}@S
PRD@S
VP
SBJ@S
NP
OBJ@S
NP
Figure 1: Generating Canonical and Non-Canonical Configurations: The CF depictions of the S level constituents at the
LHS of (a) and (b) are distinct, whereas the RR-CFG representations at the RHS of (a) and (b) share the projection of GRs and
differs in their configuration ? while (a) generates an SVO order, (b) generates a so-called Verb-Initial (VI) construction.
(c) S
VP-PRD ADVP NP-SBJ NP-OBJ DOT
S
{PRD,OBJ,SBJ}@S
PRD@S
VP
PRD:SBJ@S
ADVP
SBJ@S
NP
OBJ@S
NP
OBJ:@S
DOT
Figure 2: Generating Adjunction and Periphrastic Configurations: The CF depiction of S at the LHS of (c) generates
complements, adjuncts, and punctuation in one go, whereas the RR-CFG representation at the RHS generates first the projec-
tion of core grammatical elements and then the configuration of a modified affirmative sentence in which they are realized.
(Similarly, realising a question configuration using inversion in, e.g., English, naturally follows).
(d) S
VP-PRD
VB
NP
[Def+,Acc+]
-OBJ
AT
[Acc+]
NP
[Def+]
NNT NN
[Def+]
NP
[Def+]
-SBJ
NN
[Def+]
S
{PRD,OBJ,SBJ}@S
PRD@S
VP
VB
OBJ@S
NP
[Def+,Acc+]
AT
[Acc+]
NP
[Def+]
NNT NN
[Def+]
SBJ@S
NP
[Def+]
NN
[Def+]
Figure 3: Realizing Grammatical Relations with bounded and unbounded Morphemes: The CF depiction of the S level
constituent at the LHS of (d) shows a strong dependence between the position of syntactic constituents and the morphologically
realized features percolated from lower surface forms. In the RR-CFG representation at the RHS the feature distribution among
sub constituents is dependent on grammatical relations, independently of their positioning. The realization stage generates a
morphosyntactic paradigm in one go, allowing to capture meaningful collocations and idiosyncrasies, e.g., the Xor relation of
the Acc+ and def+ features when marking direct objects in MH (Danon, 2001).
S-CNJ
S S CC S DOT
S
{SCNJ,SCNJ,SCNJ}@S
SCNJ@S
S
SCNJ@S
S
SCNJ:SCNJ@S
CC
SCNJ@S
S
SCNJ:@S
DOT
Figure 4: Generating a Conjunction Structure: The conjunction structure in the LHS of (e) is generated by the RR-CFG
on the RHS in three stages. First, a relational network of finite number of conjuncts is generated, then a configuration for the
conjuncts and conjunction markers (in MH, a CC before the last conjunct) is proposed, and finally the different conjuncts are
generated conditioned on the same grammatical relation and the same parent. (Note that the possibility of different means for
realizing conjunction, e.g., using morphemes, punctuation or multiple adpositions, falls out naturally from this setup.)
893
5 Experiments
Data The data we use is taken from the Modern
Hebrew Treebank (MHTB) (Sima?an et al, 2001)
which consists of 6501 sentences from the newspa-
per ?haaretz? annotated with phrase-structure trees
and decorated with various morphological and
functional features. We use version 2.0 of the tree-
bank3 which we processed and head-annotated as
in (Tsarfaty and Sima?an, 2007). We experimented
with sentences 1?500 (development set) and sen-
tences 501?6001 (training set), and used sentences
6001-6501 (test set) for confirming our best result.
Models Our Plain models use the coarse-level
MHTB category-labels enriched with various mor-
phological features. Our morphological represen-
tation Base varies with respect to the use of the per-
colated features definiteness Def and accusativity
Acc. Constituents? morphosyntactic representa-
tions enriched with their head PoS tag are referred
to as Head and grand-parent encodings as Parent.
For each combination of morphological and syn-
tactic features we experimented with a state-split
PCFG and with our RR-PCFG implementation.
Procedure We read off our models? parame-
ters from the decorated phrase-structure trees in
the MHTB, and use relative frequency estimation
to instantiate their probability distributions. We
smooth lexical rules using a PoS-tags distribution
we learn for rare-words, where the ?rare? threshold
is set to 1. We then use BitPar, a general purpose
chart parser,4 to find the most likely structures,
and we extract the corresponding coarse-grained
tree-skeletons for the purpose of evaluation.5 We
use PARSEVAL measures to quantitatively evalu-
ate our models and perform a qualitative analysis
of the resulting parse trees.
Results Table 1 shows the average F-Measure
value for all sentences of length ?40 in our
development set with/without punctuation. The
na??ve baseline implementation for our experi-
ments, the BasePlain PCFG, performs at the level
of 67.61/68.67 (comparable to the baseline re-
ported in (Tsarfaty and Sima?an, 2007)). For all
3http://www.mila.cs.technion.ac.il/
english/resources/corpora/treebank/ver2.
0/index.html
4
:http://www.ims.uni-stuttgart.de/tcl/
SOFTWARE/BitPar.html
5Our setup is comparable to English, which means that our
surface forms are segmented per PoS tag without specifying
their respective PoS tags and morphological features.
Syntax Plain Head Parent ParentHead
Morphology
Base (PCFG) 67.61/68.77 71.01/72.48 73.56/73.79 73.44/73.61
(RR-PCFG) 65.86/66.86 71.84/72.76 74.06/74.28 75.13/75.29
BaseDef (PCFG) 67.68/68.86 71.17/72.47 74.13/74.39 72.54/72.79
(RR-PCFG) 66.65/67.86 73.09/74.13 74.59/74.59 76.05/76.34
BaseDefAcc (PCFG) 68.11/69.30 71.50/72.75 74.16/ 74.41 72.77/73.01
(RR-PCFG) 67.13/68.01 73.63/74.69 74.65/74.79 76.15/ 76.43
Table 1: Parsing Results for Sentences of Length < 40
in the Development Set: Averaged F-Measure With/Without
Punctuation. Base refers to coarse syntactic categories, Def
indicates percolating definiteness values, Acc indicated per-
colating accusativity marking. The underlined results repli-
cate previously reported results in similar settings.
models in the Plain column the simple PCFG out-
performs the RR-variety. Yet, the contribution of
percolated morphological features is higher with
the RR-PCFG than with the simple PCFG.
Moving to the Head column, we see that all RR-
models already outperform their enriched PCFG
counterparts. Again, morphological information
contributes more to the RR-variety. The best result
for this column, achieved by the BaseDefAccHead
RR-model (63.73/64.69), outperforms its PCFG
counterpart as well as all two-dimensional models
reported by (Tsarfaty and Sima?an, 2007). In the
Parent column), our RR-variety continues to out-
perform the PCFG albeit in an insignificant rate.
(Both results are at the same level as the best model
of (Tsarfaty and Sima?an, 2007).)
Finally, for all models in the ParentHead col-
umn the RR-models outperform their PCFG coun-
terparts to a significant degree. Similarly to the
Head column, the more morphological informa-
tion is added, the greater the improvement is. Our
best RR-model, BaseDefAccParentHead, scores
almost 10pt (25% error reduction) more than the
Plain PCFG, it is about 3.5pt better (13% error re-
duction) than a state-split PCFG using the same
information, and almost 2pt (7% error reduction)
more than the best results reported for MH so far.
We confirmed the results of our best model on
our test set, for which our baseline (BasePlain)
obtained 69.63/70.31. The enriched PCFG
of DaseDefAccHeadParent yields 73.66/73.86
whereas the RR-PCFG yields 75.83/75.89. The
overall performance for PCFGs is higher on this
set, yet the RR-model shows a notable improve-
ment (about 9% error reduction).
6 Analysis and Discussion
The trends in our quantitative analysis suggest that
the RR-models are more powerful in exploiting
different sorts of information encoded in parse
894
(a) S
NP
CDT
EFRWT
tens-of
NP
NN
ANFIM
people
VP
VB
MGIEIM
arrive
PP
IN
M
from
NP
NNP
TAILND
Thailand
PP
IN
L
to
NP
NNP
IFRAL
Israel
...
(b) S
NP
CDT
EFRWT
tens-of
NP
NN
ANFIM
people
VP
VB
MGIEIM
arrive
PP
IN
M
from
NP
NP
NNP
TAILND
Thailand
PP
IN
L
to
NP
NNP
IFRAL
Israel
....
Figure 5: Qualitative Analysis of Sentence (Fragment)
#1: (a) is the gold tree fragment, correctly predicted by our
best RR-PCFG model. (b) is the tree fragment predicted by
the PCFG corresponding to previously reported results.
trees, be it morphological information coming
from dominated surface forms or functional infor-
mation on top of syntactic categories.
We have shown that head information, which
has very little contribution to parsing accuracy as
a mere state-split, turns out to have crucial ef-
fects within the RR-models. For state-splits based
PCFGs, adding head information brings about
a category fragmentation and decreasing perfor-
mance. The separation between form and function
we articulate in the RR-approach allows us to cap-
ture generalizations concerning the distribution of
syntactic constituents under heads based on their
grammatical function, and use fine-grained fea-
tures to predict their morphosyntactic behaviour.
We have further shown that morphological in-
formation contributes a substantial improvement
when adopting the RR-approach, which is inline
with the linguistic insight that there is a correlation
between morphological marking on top of surface
forms and the grammatical function their domi-
nating constituents realize. Morphological infor-
mation is particularly useful in the presence of
heads. Taken together, head and percolated fea-
tures implement a rather complete conceptualiza-
tion of multiple exponence.
To wrap up the discussion, we leave numbers
aside and concentrate on the kind of structures pre-
dicted by our best model in comparison to the
ones suggested by previously reported unlexical-
ized PCFGs ((Tsarfaty and Sima?an, 2007), un-
derlined in our table). Due to lack of space we
(a) S
PP
MCD FNI
on the
other hand
VP
VB
MTIR
allows
NP
NNP
MSRD HEBWDH WHRWWXH
the ministry of...
VP
VB
LHESIK
to-
employ
NP
EWBDIM ZRIM
foreign
workers
PP
B..
in..
(b) S
PP
IN
M
from
NP
NNT
CD
side
NP
CDT
FNI
two
NP
NNT
MTIR
allows
NP
NNP
MSRD HEBWDH WHRWWXH
the ministry of...
VP
LHESIK
to-
employ
NP
NP
EWBDIM
workers
ADJP
ZRIM
foreigners
PP
B..
in..
Figure 6: Qualitative Analysis of Sentence (Fragment)
#4: (a) is the gold tree fragment, correctly predicted by our
best RR-PCFG model. (b) is the tree fragment predicted by
the PCFG corresponding to previously reported results.
only discuss errors found within the first 10 parsed
sentence, yet we note that the qualitative trend
we describe here persists throughout our develop-
ment set. Figures (5) and (6) show a gold tree
(a fragment of sentence #1) correctly predicted
by our best RR-model (a) in comparison with the
one predicted by the respective PCFG (b). The
tree fragment in figure (5) shows that the RR-
grammar bracketed and attached correctly all the
constituents that bear grammatical relations to the
S clause (5a). The corresponding PCFG conflated
the ?to? and ?from? phrases to a rather meaning-
less prepositional phrase (5b). For (a fragment of)
sentence #4 in our set (figure 6) the RR-model re-
covered all grammatically meaningful constituents
under the S clause and under the internal VP (6a).
Notably, the PCFG in (6b) recovered none of them.
Both grammars make attachment mistakes internal
to complex NPs, but the RR-model is better at iden-
tifying higher level constituents that correlate with
meaningful grammatical functions.
Our qualitative analysis suggests that our model
is even more powerful than our quantitative analy-
sis indicates, yet we leave the discussion of better
ways to quantify this for future research.
A Note on Related Work Studies on parsing
MH to date concentrate mostly on spelling out
the integration of a PCFG parser with a mor-
phological disambiguation component (e.g., (Tsar-
faty, 2006; Goldberg and Tsarfaty, 2008)). On a
setup identical to ours (gold segmentation, no PoS)
the latter obtained 70pt. (Tsarfaty and Sima?an,
895
2007) examined the contribution of horizontal
and vertical conditioning to an unlexicalized MH
parser and concluded that head-driven Markoviza-
tion performs below the level of vertical condi-
tioning enriched with percolated features. We do
not know of existing dependency-parsers applied
to parsing MH or mildly-context-sensitive broad-
coverage parsers applied to parsing a Semitic lan-
guage.6 To the best of our knowledge, this is the
first fully generative probabilistic framework that
models explicitly morpho-syntactic interaction to
enhance parsing for non-configrational languages.
7 Conclusion
Projection and Realization are two sides of the
same coin. Projection determines which gram-
matical relations appear in the syntactic represen-
tation, and Realization determines how such rela-
tions are realized. We suggest that the Relational-
Realizational (RR) approach is adequate for pars-
ing languages characteristically different from En-
glish, and we illustrate it with an application to
parsing MH. We show that our approach to mod-
eling the interaction between syntactic categories
and a handful of percolated features already yields
a notable improvement in parsing accuracy and
substantially improves the quality of suggested
parses. Incorporating additional functional and
morphological information, we expect, will help
bridging the gap in performance between Hebrew
and configurational languages such as English.
Acknowledgements We thank Remko Scha,
Jelle Zuidema, Yoav Goldberg, and three anony-
mous reviewers for comments on earlier drafts.
The first author wishes to thank Jim Blevins, Julia
Hockenmeir, Mark Johnson, Kevin Knight, Chris
Manning, Joakim Nivre and Gerald Penn for stim-
ulating discussion. Errors are our own. The work
of the first author is funded by the Dutch Science
Foundation (NWO), grant number 017.001.271.
References
Anderson, S. R. 1992. A-Morphus Morphology. Cam-
bridge University Press.
Beard, R. 1988. The Separation of Derivation and
Affixation: Toward a Lexeme-Morpheme Base Mor-
phology. Quarderni di semantica, pages 277?287.
6Parsing MSA has been explored with a treebank three
times as large as ours using a head-driven lexicalized parser
obtaining around 78% accuracy (http://papers.ldc.
upenn.edu/). The input setup assumes gold segmentation
as well as PoS tags information and some diacritization.
Blevins, J. P. 2008. Periphrasis as Syntactic Ex-
ponence. In Ackerman, F., J.P. Blevins, and G.S.
Stump, editors, Patterns in Paradigms. CSLI.
Bod, R. 2003. An Efficient Implementation of a New
Dop Model. In Proceedings of EACL.
Bresnan, Joan. 1979. A Theory of Grammatical Repre-
sentation. Duplicated Lecture Notes. Department of
Linguistics and Philosophy, MIT.
Charniak, E. 1997. Statistical Parsing with a Context-
Free Grammar and Word Statistics. In AAAI/IAAI.
Collins, M. 2003. Head-Driven Statistical Models for
Natural Language Parsing. Comp. Linguistics.
Danon, G. 2001. Syntactic Definiteness in the Gram-
mar of Modern Hebrew. Linguistics, 6(39).
Goldberg, Y. and R. Tsarfaty. 2008. A Single Gener-
ative Framework for Joint Morphological Segmenta-
tion and Syntactic Parsing. In Proceedings of ACL.
Johnson, M. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24(4).
Klein, D. and C. Manning. 2003. Accurate Unlexical-
ized Parsing. In Proceedings of ACL.
McDonald, R., F. Pereira, K. Ribarov, and J. Hajic?.
2005. Non-Projective Dependency Parsing using
Spanning Tree Algorithms. In Proceedings of HLT.
Nivre, J. 2007. Data-driven Dependency Parsing
Across Languages and Domains; Perspectives from
the CoNLL-2007 Shared Task. In Proceedings of
IWPT.
Perlmutter, D. M. 1982. Syntactic Representation,
Syntactic levels, and the Notion of Subject. In Ja-
cobson, Pauline and Geoffrey Pullum, editors, The
Nature of Syntactic Representation. Springer.
Postal, P. M. and D. M. Perlmutter. 1977. Toward
a Universal Characterization of Passivization. In
BLS3.
Postal, P. M. 1982. Some Arc-Pair Grammar Decrip-
tions. In Jacobson, P. and G. K. Pullum, editors, The
Nature of Syntactic Representation. Dordrecht.
Sima?an, K., A. Itai, Y. Winter, A. Altman, and N. Na-
tiv. 2001. Building a Tree-Bank for Modern Hebrew
Text. In Traitement Automatique des Langues.
Tsarfaty, R. and K. Sima?an. 2007. Three-Dimensional
Parametrization for Parsing Morphologically Rich
Languages. In Proceedings of IWPT.
Tsarfaty, R. 2006. Integrated Morphological and Syn-
tactic Disambiguation for Modern Hebrew. In Pro-
ceeding of ACL-SRW.
Zwicky, A. M. 1993. Heads, Bases, and Functors. In
Corbett, G.G., N. Fraser, and S. McGlashan, editors,
Heads in Grammatical Theory. Cambridge.
896
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 630?639,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Phrase Translation Probabilities with ITG Priors
and Smoothing as Learning Objective
Markos Mylonakis
Language and Computation, ILLC
Faculty of Science
University of Amsterdam
m.mylonakis@uva.nl
Khalil Sima?an
Language and Computation, ILLC
Faculty of Science
University of Amsterdam
k.simaan@uva.nl
Abstract
The conditional phrase translation probabil-
ities constitute the principal components of
phrase-based machine translation systems.
These probabilities are estimated using a
heuristic method that does not seem to opti-
mize any reasonable objective function of the
word-aligned, parallel training corpus. Ear-
lier efforts on devising a better understood
estimator either do not scale to reasonably
sized training data, or lead to deteriorating
performance. In this paper we explore a new
approach based on three ingredients (1) A
generative model with a prior over latent
segmentations derived from Inversion Trans-
duction Grammar (ITG), (2) A phrase ta-
ble containing all phrase pairs without length
limit, and (3) Smoothing as learning ob-
jective using a novel Maximum-A-Posteriori
version of Deleted Estimation working with
Expectation-Maximization. Where others
conclude that latent segmentations lead to
overfitting and deteriorating performance,
we show here that these three ingredients
give performance equivalent to the heuristic
method on reasonably sized training data.
1 Motivation
A major component in phrase-based statistical Ma-
chine translation (PBSMT) (Zens et al, 2002;
Koehn et al, 2003) is the table of conditional prob-
abilities of phrase translation pairs. The pervading
method for estimating these probabilities is a sim-
ple heuristic based on the relative frequency of the
phrase pair in the multi-set of the phrase pairs ex-
tracted from the word-aligned corpus (Koehn et al,
2003). While this heuristic estimator gives good em-
pirical results, it does not seem to optimize any intu-
itively reasonable objective function of the (word-
aligned) parallel corpus (see e.g., (DeNero et al,
2006)) The mounting number of efforts attacking
this problem over the last few years (DeNero et al,
2006; Marcu and Wong, 2002; Birch et al, 2006;
Moore and Quirk, 2007; Zhang et al, 2008) exhibits
its difficulty. So far, none has lead to an alternative
method that performs as well as the heuristic on rea-
sonably sized data (approx. 1000k sentence pair).
Given a parallel corpus, an estimator for phrase-
tables in PBSMT involves two interacting decisions
(1) which phrase pairs to extract, and (2) how to as-
sign probabilities to the extracted pairs. The heuris-
tic estimator employs word-alignment (Giza++)
(Och and Ney, 2003) and a few thumb rules for
defining phrase pairs, and then extracts a multi-set
of phrase pairs and estimates their conditional prob-
abilities based on the counts in the multi-set. Us-
ing this method for extracting a set of phrase pairs,
(DeNero et al, 2006; Moore and Quirk, 2007) aim
at defining a better estimator for the probabilities.
Generally speaking, both efforts report deteriorating
translation performance relative to the heuristic.
Instead of employing word-alignment to guide
phrase pair extraction, it is theoretically more ap-
pealing to aim at phrase alignment as part of the esti-
mation process (Marcu and Wong, 2002; Birch et al,
2006). This way, phrase pair extraction goes hand-
in-hand with estimating the probabilities. How-
ever, in practice, due to the huge number of possi-
ble phrase pairs, this task is rather challenging, both
computationally and statistically. It is hard to define
630
both a manageable phrase pair translation model and
a well-founded training regime that would scale up
to reasonably sized parallel corpora (see e.g., (Birch
et al, 2006)). It remains to be seen whether this the-
oretically interesting approach will lead to improved
phrase probability estimates.
In this paper we also start out from a stan-
dard phrase extraction procedure based on word-
alignment and aim solely at estimating the condi-
tional probabilities for the phrase pairs and their
reverse translation probabilities. Unlike preceding
work, we extract all phrase pairs from the training
corpus and estimate their probabilities, i.e., without
limit on length. We present a novel formulation of
a conditional translation model that works with a
prior over segmentations and a bag of conditional
phrase pairs. We use binary Synchronous Context-
Free Grammar (bSCFG), based on Inversion Trans-
duction Grammar (ITG) (Wu, 1997; Chiang, 2005a),
to define the set of eligible segmentations for an
aligned sentence pair. We also show how the num-
ber of spurious derivations per segmentation in this
bSCFG can be used for devising a prior probabil-
ity over the space of segmentations, capturing the
bias in the data towards monotone translation. The
heart of the estimation process is a new smoothing
estimator, a penalized version of Deleted Estima-
tion, which averages the temporary probability es-
timates of multiple parallel EM processes at each
joint iteration.
For evaluation we use a state-of-the-art baseline
system (Moses) (Hoang and Koehn, 2008) which
works with a log-linear interpolation of feature func-
tions optimized by MERT (Och, 2003). We sim-
ply substitute our own estimates for the heuristic
phrase translation estimates (both directions and the
phrase penalty score) and compare the two within
the Moses decoder. While our estimates differ sub-
stantially from the heuristic, their performance is on
par with the heuristic estimates. This is remark-
able given the fact that comparable previous work
(DeNero et al, 2006; Moore and Quirk, 2007) did
not match the performance of the heuristic estima-
tor using large training sets. We find that smooth-
ing is crucial for achieving good estimates. This
is in line with earlier work on consistent estimation
for similar models (Zollmann and Sima?an, 2006),
and agrees with the most up-to-date work that em-
ploys Bayesian priors over the estimates (Zhang et
al., 2008).
2 Related work
Marcu and Wong (Marcu and Wong, 2002) realize
that the problem of extracting phrase pairs should
be intertwined with the method of probability esti-
mation. They formulate a joint phrase-based model
in which a source-target sentence pair is generated
jointly. However, the huge number of possible
phrase-alignments prohibits scaling up the estima-
tion by Expectation-Maximization (EM) (Dempster
et al, 1977) to large corpora. Birch et al(Birch et
al., 2006) provide soft measures for including word-
alignments in the estimation process and obtain im-
proved results only on small data sets.
Coming up-to-date, (Blunsom et al, 2008) at-
tempt a related estimation problem to (Marcu and
Wong, 2002), using the expanded phrase pair set
of (Chiang, 2005a), working with an exponential
model and concentrating on marginalizing out the
latent segmentation variable. Also most up-to-date,
(Zhang et al, 2008) report on a multi-stage model,
without a latent segmentation variable, but with a
strong prior preferring sparse estimates embedded in
a Variational Bayes (VB) estimator and concentrat-
ing the efforts on pruning both the space of phrase
pairs and the space of (ITG) analyses. The latter two
efforts report improved performance, albeit again on
a limited training set (approx. 140k sentences up to
a certain length).
DeNero et al(2006) have explored estimation us-
ing EM of phrase pair probabilities under a con-
ditional translation model based on the original
source-channel formulation. This model involves a
hidden segmentation variable that is set uniformly
(or to prefer shorter phrases over longer ones). Fur-
thermore, the model involves a reordering compo-
nent akin to the one used in IBM model 3. De-
spite this, the heuristic estimator remains superior
because ?EM learns overly determinized segmen-
tations and translation parameters, overfitting the
training data and failing to generalize?. More re-
cently, (Moore and Quirk, 2007) devise a estimator
working with a model that does not include a hid-
den segmentation variable but works with a heuris-
tic iterative procedure (rather than MLE or EM). The
631
translation results remain inferior to the heuristic but
the authors note an interesting trade-off between de-
coding speed and the various settings of this estima-
tor.
Our work expands on the general approach taken
by (DeNero et al, 2006; Moore and Quirk, 2007)
but arrives at insights similar to those of the most
recent work (Zhang et al, 2006), albeit in a com-
pletely different manner. The present work differs
from all preceding work in that it employs the set
of all phrase pairs during training. It differs from
(Zhang et al, 2008) in that it does postulate a la-
tent segmentation variable and puts the prior di-
rectly over that variable rather than over the ITG
synchronous rule estimates. Our method neither
excludes phrase pairs before estimation nor does it
prune the space of possible segmentations/analyses
during training/estimation. As well as smoothing,
we find (in the same vein as (Zhang et al, 2008))
that setting effective priors/smoothing is crucial for
EM to arrive at better estimates.
3 The Translation Model
Given a word-aligned parallel corpus of source-
target sentences, it is common practice to extract a
set of phrase pairs using extraction heuristics (cf.
(Koehn et al, 2003; Och and Ney, 2004)). These
heuristics define a phrase pair to consist of a source
and target ngrams of a word-aligned source-target
sentence pair such that if one end of an alignment
is in the one ngram, the other end is in the other
ngram (and there is at least one such alignment)
(Och and Ney, 2004; Koehn et al, 2003). For ef-
ficiency and sparseness, the practitioners of PBSMT
constrain the length of the source phrase to a certain
maximum number of words.
An All Phrase Pairs Model: In this work we train
a phrase-translation table that consists of all phrase-
pairs that can be extracted from the word-aligned
training data according to the standard phrase ex-
traction heuristic. After training, we can still limit
the set of phrase pairs to those selected by a cut-off
on phrase length. The reason for using all phrase
pairs during training is that it gives a clear point of
reference for an estimator, without implicit, acciden-
tal biases that might emerge due to length cut-off1.
The Generative Model: Given a word-aligned
source-target sentence pair ?f , e,a?, the generative
story underlying our model goes as follows:
1. Abiding by the word-alignments in a, segment
the source-target sentence pair ?f , e? into a se-
quence of I containers ?I1 , and a bag of I
phrase pairs ?I1(f , e) = {?fj , ej?}Ij=1. Each
container ?j = ?lf , rf , le, re? consists of the
start lf and end rf positions2 for a phrase in
f and the start le and end re positions for an
aligned phrase in e.
2. For a given segmentation ?I1 , for every con-
tainer ?j (1 ? j ? I) generate the phrase-pair
?fj, ej?, independently from all other phrase-
pairs.
This leads to the following probabilistic model:
P (f | e;a) =
?
?I1??(a)
P (?I1)
?
?fj ,ej???I1 (f ,e)
P (fj | ej) (1)
Where ?(a) is the set of binarizable segmenta-
tions (defined next) that are eligible according to the
word-alignments a between f and e. These segmen-
tations into bilingual containers (where segmenta-
tions are taken inside the containers) are different
from the monolingual segmentations used in earlier
comparable conditional models (e.g., (DeNero et al,
2006)) which must generate the alignment on top of
the segmentations. Note how the different phrase
pairs ?fj, ej? are generated from their bilingual con-
tainers in the given segmentation ?I1 . We will dis-
cuss our choice of prior probability over segmenta-
tions P (?I1) after we discuss the definition of the bi-
narizable segmentations ?(a).
3.1 Binarizable segmentations ?(a)
Following (Zhang et al, 2006; Huang et al, 2008),
every sequence of phrase alignments can be viewed
1For example, if the cut-off on phrase pairs is ten words, all
sentence pairs smaller than ten words in the training data will
be included as phrase pairs as well. These sentences are treated
differently from longer sentences, which are not allowed to be
phrase pairs.
2The NULL alignments (word-to-NULL) in the training
data can also be marked with actual positions on both sides in
order to allow for this definition of containers.
632
as a sequence of integers 1, . . . I together with a
permuted version of this sequence pi(1), . . . , pi(I),
where the two copies of an integer in the two se-
quences are assumed aligned/paired together. For
example, possible permutations of {1, 2, 3, 4} are
{2, 1, 3, 4} and {2, 4, 1, 3}. Because a segmenta-
tion ?I1 of a sentence pair is also a sequence of
aligned phrases, it also constitutes a permuted se-
quence. A binarizable permutation x is either of
length one, or can be properly split into two binariz-
able sub-sequences y and z such that either3 z < y
or y < z. For example, one way to binarize the
permutation {2, 1, 3, 4} is to introduce a proper split
into {2, 1; 3, 4}, then recursively another proper split
of {2, 1} into {2; 1} and {3, 4} into {3; 4}. In con-
trast, the permutation {2, 4, 1, 3} is non-binarizable.
<>
<>
2 1
[]
3 4
[]
[]
<>
2 1
3
4
Figure 1: Multiple ways to binarize a permutation
Graphically speaking, the recursive definition of
binarizable permutations can be depicted as a bi-
nary tree structure where the nodes correspond to
recursive proper splits of the permutation, and the
leaves are decorated with the naturals. Figure 1 ex-
hibits two possible binarizations of the same permu-
tation where <> and [] denote inverted and mono-
tone proper splits respectively. Note that the number
of possible binarizations of a binarizable permuta-
tion is a recursive function of the number of possi-
ble proper splits and reaches its maximum for fully
monotone permutations (all binary trees, which is a
factorial function of the length of the permutation).
By definition (cf. (Zhang et al, 2006; Huang et
al., 2008)), a binarizable segmentation/permutation
can be recognized by a binarized Synchronous
Context-Free Grammar (SCFG), i.e., an SCFG in
which the right hand sides of all non-lexical rules
constitute binarizable permutations. In particular,
this holds for the SCFG implementing Inversion
3For two sequences of numbers, the notation y < z stands
for ?y ? y,?z ? z : y < z.
Transduction Grammar (Wu, 1997). This SCFG
(Chiang, 2005b) has two binary synchronous rules
that correspond resp. to the contiguous monotone
and inverted alignments:
XP ? XP 1 XP 2 , XP 1 XP 2 (2)
XP ? XP 1 XP 2 , XP 2 XP 1
The boxed integers in the superscripts on the non-
terminal XP denote synchronized rewritings. In
this work, we employ a binary SCFG (bSCFG)
working with these two synchronous rules to-
gether with a set of lexical rules {XP ?
f, e | ?f, e? is a phrase pair}.
In this bSCFG, every derivation corresponds to a
binarization of a segmentation of the input. Note
that the bSCFG defined in equation 2 generates all
possible binarizations for every segmentation of the
input. It is possible to constrain this bSCFG such
that it generates a single, canonical derivation per
segmentation. However, in section 3.2 we show that
the number of such derivations is a good measure of
phrase pair productivity.
It is well known that there are alignments and
segmentations that this bSCFG does not cover (see
(Huang et al, 2008)). Recently, strong evidence
emerged (e.g., (Huang et al, 2008)) showing that
most word-alignments of actual parallel corpora can
be covered by a binarized SCFG of the ITG type.
Furthermore, because our model employs the set of
all phrase-pairs that can be extracted from a given
training set, it will always find segmentations that
cover every sentence pair in the training data4. This
implies that while our model might discard non-
binarizable segmentations for certain complex word
alignments, we do manage to train the model on the
binarizable segementations of all sentence pairs.
Up to the prior over segmentations (see next), we
implement the above model using a weighted ver-
sion of the binary SCFG as follows:
? The weight for lexical rules is given by
P (XP ? f, e) := P (f | e), where ?f, e? is
a phrase-pair. These are the trainable parame-
ters of our model.
4In the worst case the whole sentence pair is a phrase pair
with a trivial segmentation.
633
11
5
5
1
1
5
52 3 4
3 4 23 4 2
2 43
Figure 2: Two segmentations of an align-
ment/permutation. Both segmentations have the
same number of binarizations despite differences in
container sizes.
? The weights for the two non-lexical rules in
equation 2 are fixed at 1.0. These weights are
not trained at all.
Where we use the notation P (.) for the weight of a
synchronous rule.
3.2 Prior over segmentations
As it has been found out by (DeNero et al, 2006),
it is not easy to come up with a simple, effec-
tive prior distribution over segmentations that al-
lows for improved phrase pair estimates. Within a
Maximum-Likelihood estimator, preference for seg-
mentations ?I1 consisting of longer containers could
lead to overfitting as we will explain in section 4.
Alternatively, it is tempting to have preference for
segmentations ?I1 that consist of shorter contain-
ers, because (generally speaking) shorter contain-
ers have higher expected coverage of new sentence
pairs. However, mere bias for shorter containers
will not give better estimates as observed by (DeN-
ero et al, 2006). One case where this bias clearly
fails is the case of a contiguous sequence of con-
tainers with a complex alignment structure (cross-
ing alignments). For example (see figure 2), for
the alignment {1, 3, 4, 2, 5} there is a segmentation
into five containers {1; 3; 4; 2; 5}, and another into
three {1; 3, 4, 2; 5}. The first segmentation involves
shorter containers that have crossing brackets among
them, while the second one consists of three con-
tainers including a longer container {3, 4, 2}. In
the first segmentation, due to their crossing align-
ments, each of the containers {3}, {4} and {2} will
not combine with the surrounding context ({1} and
{5}) on its own, i.e., without the other two contain-
ers. Furthermore, there is only a single binariza-
tion of {3, 4, 2}. Hence, while the first segmen-
tation involves shorter containers than the second
one, these shorter containers are as productive as
the large container {3, 4, 2}, i.e., they combine with
surrounding containers in the same number of ways
as the large container. In such and similar cases,
there are no grounds for the bias towards shorter
phrases/containers.
The notion of container productivity (the num-
ber of ways in which it combines with surrounding
containers during training) seems to correlate with
the expected number of ways a container can be
used during decoding, which should be correlated
with expected coverage. During training, contain-
ers that are often surrounded by other, monotoni-
cally aligned containers are expected to be more pro-
ductive than alternative containers that are often sur-
rounded by crossing alignments. Hence, the num-
ber of binarizations that a segmentation has under
the bSCFG is a direct function of the ways in which
the containers combine among themselves (mono-
tone vs. inverted/crossing) within segmentations,
and provides a more accurate measure of container
productivity than container length. Hence, the final
model we employ is the following:
P (f | e;a) =
?
?I1??(a)
N(?I1)
Z(?(a))
?
?fj ,ej???I1(f ,e)
P (fj | ej) (3)
Where N(?I1) is the number of binary deriva-
tions/trees that ?I1 has in the binary SCFG (bSCFG),
and Z(?(a)) = ??J1 ??(a) N(?
J
1 ), i.e., this prior is
the ratio of number of derivations of ?I1 to the to-
tal number of derivations that ?f , e,a? has under the
bSCFG.
3.3 Contrast with similar models:
In contrast with the model of (DeNero et al, 2006),
who define the segmentations over the source sen-
tence f alone, our model employs bilingual con-
tainers thereby segmenting both source and target
sides simultaneously. Therefore, unlike (DeNero
et al, 2006), our model does not need to gener-
ate the word-alignments explicitly, as these are em-
bedded in the segmentations. Similarly, our model
does not include explicit penalty terms for reorder-
634
ing/inversion but includes a related bias in the prior
probabilities over segmentations P (?I1).
In a way, the segmentations and bilingual contain-
ers we use can be viewed as similar to the concepts
used in the Joint Model of Marcu and Wong (Marcu
and Wong, 2002). Unlike (Marcu and Wong, 2002),
however, our model works with conditional proba-
bilities and starts out from the word-alignments.
The novel aspects of our model are three (1) It de-
fines the set of segmentations using a bSCFG, (2) It
includes a novel, refined prior probability over seg-
mentations, and (3) It employs all phrase pairs that
can be extracted from a word-aligned training par-
allel corpus. For these novel elements to produce
reasonable estimates, we devise our own estimator.
4 Estimation by Smoothing
In principle, we are dealing here with a translation
model that employs all phrase pairs (of unbounded
size), extracted from a word-aligned parallel cor-
pus. Under this model, where a phrase pair and
its sub-phrase pairs are included in the model, the
MLE can be expected to overfit the data5 unless a
suitable prior probability over segmentations is em-
ployed. Indeed, the prior over segmentations defined
in the preceding section prevents the MLE from
completely overfitting the training data. However,
we find empirical evidence that this prior is insuffi-
cient for avoiding overfitting.
Our model behaves like a memory-based model
because it memorizes all extractable phrase pairs
found in the training data including the training sen-
tence pairs themselves. Such memory-based mod-
els are related to nonparametric models such as
K-NN and kernel methods (Hastie et al, 2001).
For memory-based models, consistent estimation for
novel instances proceeds by local density estimation
from the surroundings of the instance, which is akin
to smoothing for parametric models. Hence, next we
describe our own version of a smoothed Maximum-
Likelihood estimator for phrase translation probabil-
5One trivial MLE solution would give the longest container,
consisting of the longest phrase pairs, a probability of one, at
the cost of all shorter alternatives. A similar problem arises in
Data-Oriented Parsing, see (Sima?an and Buratto, 2003; Zoll-
mann and Sima?an, 2006). Note that models that employ an
upperbound on phrase pair length will still risk overfitting train-
ing sentences of lengths that fall within this upperbound.
???????????????????-
INPUT: Word-aligned parallel training data T
OUTPUT: Estimates pi for all P (f | e)
{
Split training data T into equal parts H1, . . . ,H10.
For 1 ? i ? 10 do
Extract from Ei = ?j 6=iHj all phrase pairs pii
Initialize p?i0i to uniform conditional probs
Let j = 0
Repeat
Let j = j + 1 // EM iteration counter
For 1 ? i ? 10 do
E-step: calculate expected counts for pairs
in piji on Hi using counts from p?i
j?1
i .
M-step: calculate probabilities for pairs in
piji from the expected counts
For 1 ? i ? 10 do p?iji := 110
?10
i=1 piji
Until pi := {p?ij1, . . . , p?i
j
10} has converged
}
???????????????????-
Figure 3: Penalized Deleted Estimation
ities.
For a latent variable model, it is usually common
to employ Expectation-Maximization (EM) (Demp-
ster et al, 1977) as a search method for a (local)
maximum-likelihood estimate (MLE) of the train-
ing data. Instead of mere EM we opt for a smoothed
version: we present a new method, that combines
Deleted Estimation (Jelinek and Mercer, 1980) with
the Jackknife (Duda et al, 2001) as the core estima-
tor.
Figure 3 shows the pseudo-code for our estima-
tor. Like in Deleted Estimation, we split the training
data into ten equal portions. This way we create ten
different splits of extraction/heldout sets of respec-
tively 90%/10% of the training set. For every split
1 ? i ? 10, we extract a set of phrase pairs pii from
the extraction set Ei and train it (under our model)
on the heldout set Hi. Naturally, the phrase pair sets
pii (1 ? i ? 10) are subsets of (or equal to) the set
of phrase pairs pi = ?ipii extracted from the total
training data (i.e., pi is the set of model parameters).
The training of the different pii?s, each on its corre-
sponding heldout set Hi, is done by ten separate EM
processes, which are synchronized in their initializa-
635
tion, their iterations as well as stop condition. The
EM processes start out from uniform conditional es-
timates of the phrase pairs in all pii. After every EM
iteration j, when the M-step has finished, the esti-
mates in all piji (1 ? i ? 10) are set to the average
(over 1 ? i ? 10) of the estimates in piji leading to
p?iji (following the Jackknife method). The resulting
averaged probabilities in p?iji are then used as the cur-
rent phrase pair estimates, which feed into the next
iteration j + 1 of the different EM processes (each
working on a different heldout set Hi with a differ-
ent set of phrase pairs pii).
There are two special boundary cases which de-
mand special attention during estimation:
Sparse distributions: For a phrase e that does oc-
cur both in Hi and Ei, there could be a phrase
pair ?f, e? that does occur in Hi but does not
occur in pii. To prevent EM from giving the
extra probability mass to all other pairs ?f, e??
unjustifiably, we apply smoothing. We add the
missing pair ?f, e? to pii and set its probability
to a fixed number 10?5?len, where len is the
length of the phrase pair. In effect, we backoff
our model (equation 1) to a word-level model
with fixed word translation probability (10?5).
Zero distributions: When a phrase e does not oc-
cur in Hi, all its pairs ?f, e? in pii will have
zero counts. During each EM iteration, when
the M-step is applied, the distribution P (? | e)
is undefined by MLE, since it is irrelevant for
the likelihood of Hi. In this case any choice
of proper distribution P (? | e) will constitute an
MLE solution. We choose to set this case to a
uniform distribution every time again.
Since our model and estimator are implemented
within the bSCFG framework, we use a bilingual
CYK parser (Younger, 1967) under the grammar
in equation 2. This parser builds for every input
?f ,a, e? all binarizations/derivations for every seg-
mentation in ?(a). For implementing EM, we em-
ploy the Inside-Outside algorithm (Lari and Young,
1990; Goodman, 1998). During estimation, because
the input, output and word-alignment are known
in advance, the time and space requirements re-
main manageable despite the worst-case complexity
O(n6) in target sentence length n.
Penalized Deleted Estimation: In contrast with
our method, Deleted Estimation sums the expected
counts (rather than probabilities) obtained from
the different splits before applying the M-step
(normalization). While the rationale behind Deleted
Estimation comes from MLE over the original
training data, our method has a smoothing objective
(inspired by the Jackknife ): generally speaking, the
averages over different heldout sets (under different
subsets of the model) give less sharp estimates than
MLE. By averaging the different heldout estimates,
this estimator employs a penalty term that depends
on the marginal count of e in the heldout set6.
Interestingly, when the phrase e is very frequent7,
it will approximately occur almost as often in the
different heldout sets. In this case, our method
reduces to Deleted Estimation, where it effectively
sums the counts8. Yet, when the target phrase e
does occur only very few times, it is likely that its
count in some splits will be zero. In our method, at
every EM iteration, during the Maximization step,
we set such cases back to uniform. By averaging the
probabilities from the different splits over many EM
iterations, setting these cases to uniform constitutes
a kind of prior that prevents the final estimates
from falling too far from uniform. In contrast, in
Deleted Interpolation the zero counts are simply
summed with the other corresponding counts of the
same phrase pair, which leads to sharper probability
distributions. In all experiments that we conducted,
our method (which we call Penalized Deleted
Estimation) gave more successful estimates than
mere Deleted Estimation.
On the theoretical side, the choice for a fixed
6Define county(x) to be the count of event x
in data y. The Deleted Estimation (DE) estimate is?
H countH (f, e)/countT (e), which can be written as?
H [countH (f, e)/countH (e)][countH(e)/countT (e)] =?
H piH(f |e)[countH (e)/countT (e)] where piH(f |e) is the
estimate from heldout set H . Hence, DE linearly interpolated
piH with factors countH (e)/countT (e). Our estimator em-
ploys uniform interpolation factors instead, thereby penalizing
the DI counts (hence Penalized DI).
7Theoretically speaking, when the training data is unbound-
edly large, our estimator will converge to the same estimates
as the Deleted Estimation. When the data is still sparse, our
estimator is biased, unlike the MLE which will overfit.
8When calculating the conditional probabilities, the denom-
inators used are approximately equal to one another.
636
prior over segmentations (ITG prior) implies that our
model cannot be estimated to converge (in proba-
bility) to the relative frequency estimates (RFE) of
source-target sentence pairs in the limit of the train-
ing data (a sufficiently large parallel corpus). A prior
probability over segmentations that would allow our
estimator to converge in the limit to the RFE must
gradually prefer segmentations consisting of larger
containers as the data grows large. We set the de-
sign and estimation of such a prior aside for future
work.
5 Empirical experiments
Decoding and Baseline Model: In this work
we employ an existing decoder, Moses (Hoang
and Koehn, 2008), which defines a log-linear
model interpolating feature functions, with interpo-
lation scores ?f e? = argmaxe
?
f?? ?fHf (f , e).
The ?f are optimized by Minimum-Error Training
(MERT) (Och, 2003). The set ? consists of the
following feature functions (see (Hoang and Koehn,
2008)): a 5-gram target language model, the stan-
dard reordering scores, the word and phrase penalty
scores, the conditional lexical estimates obtained
from the word-alignment in both directions, and the
conditional phrase translation estimates in both di-
rections P (f | e) and P (e | f). Keeping the other
five feature functions fixed, we compare our esti-
mates of P (f | e) and P (e | f) (and the phrase
penalty) to the commonly used heuristic estimates.
Because our model employs a latent segmenta-
tion variable, this variable should be marginalized
out during decoding to allow selecting the highest
probability translation given the input. This turns
out crucial for improved results (cf. (Blunsom et al,
2008)). However, such a marginalization can be NP-
Complete, in analogy to a similar problem in Data-
Oriented Parsing (Sima?an, 2002)9. We do not have
a decoder yet that can approximate this marginaliza-
tion efficiently and we employ the standard Moses
decoder for this work.
Experimental Setup: The training, development
and test data all come from the French-English
translation shared task of the ACL 2007 Second
9A reduction of simple instances of the first problem to in-
stances of the latter problem should be possible.
Phrases System BLEU
? 7 Baseline PBSMT 33.03
? 10 Baseline PBSMT 33.03
All Baseline PBSMT 33.00
? 7 EM + ITG Prior 32.50
? 7 EM + Del. Est. 32.67
? 7 EM + Del. Est. + ITG Prior 32.73
? 7 EM + Pen. Del. Est. + ITG Prior 33.02
? 10 EM + Pen. Del. Est. + ITG Prior 33.14
All EM + Pen. Del. Est. + ITG Prior 32.98
Table 1: Results: data from ACL07 2nd Wkshp on SMT
Workshop on Statistical Machine Translation 10. Af-
ter pruning sentence pairs with word length more
than 40 on either side, we are left with 949K sen-
tence pairs for training. The development and test
data are composed of 2K sentence pairs each. All
data sets are lower-cased.
For both the baseline system and our method,
we produce word-level alignments for the parallel
training corpus using GIZA++. We use 5 iterations
of each IBM Model 1 and HMM alignment mod-
els, followed by 3 iterations of each Model 3 and
Model 4. From this aligned training corpus, we ex-
tract the phrase pairs according to the heuristics in
(Koehn et al, 2003). The baseline system extracts
all phrase-pairs upto a certain maximum length on
both sides and employs the heuristic estimator. The
language model used in all systems is a 5-gram lan-
guage model trained on the English side of the paral-
lel corpus. Minimum-Error Rate Training (MERT)
is applied on the development set to obtain opti-
mal log-linear interpolation weights for all systems.
Performance is measured by computing the BLEU
scores (Papineni et al, 2002) of the system?s trans-
lations, when compared against a single reference
translation per sentence.
Results: We compare different versions of our
system against the baseline system using the heuris-
tic estimator. We observe the effects of the ITG prior
in the translation model as well as the method of es-
timation (Deleted Estimation vs. Penalized Deleted
Estimation).
Table 1 exhibits the BLEU scores for the sys-
10http://www.statmt.org/wmt07
637
tems. Our own system (with ITG prior and Pe-
nalized Deleted Estimation and maximum phrase-
length ten words) scores (33.14), slightly outper-
forming the best baseline system (33.03). When us-
ing straight Deleted Estimation over EM, this leads
to deterioration (32.73). When also the ITG prior is
excluded (by having a single derivation per segmen-
tation) this leads to further deterioration (32.67). By
using mere EM with an ITG prior, performance goes
down to 32.50, exhibiting the crucial role of the es-
timation by smoothing. Clearly, Penalized Deleted
Estimation and the ITG prior are important for the
improved phrase translation estimates.
As table 1 shows we also varied the phrase length
cutoff (seven, ten or none=all phrase pairs). The
length cutoff pertains to both sides of a phrase-pair.
For our estimator, we always train all phrase pairs,
applying the length cutoff only after training (no re-
normalization is applied at that point).
Interestingly, we find out that the heuristic estima-
tor cannot benefit performance by including longer
phrase pairs. Our estimator does benefit perfor-
mance by including phrase pairs of length upto ten
words, but then it degrades again when including
all phrase pairs. We take the latter finding to sig-
nal remaining overfitting that proved resistant to the
smoothing applied by our estimator. The heuristic
estimator exhibits a similar degradation.
We also tried to vary the treatment of Sparse Dis-
tributions (section 4, page 7) during heldout estima-
tion from fixed word-translation probabilities to the
lexical model probabilities. This lead to slight dete-
rioration of results (32.94). It is unclear whether this
deterioration is meaningful or not. We did not ex-
plore mere EM without any smoothing or ITG prior,
as we expect it will directly overfit the training data
as reported by (DeNero et al, 2006).
We note that for French-English translation it is
hard to outperform the heuristic within the PBSMT
framework, since it already performs very well.
Preliminary, most recent experiments on German-
English (also WMT07 data) exhibit that our estima-
tor outperforms the heuristic.
6 Discussion and Future Research
The most similar efforts to ours, mainly (DeNero
et al, 2006), conclude that segmentation variables
in the generative translation model lead to overfit-
ting while attaining higher likelihood of the train-
ing data than the heuristic estimator. Based on this
advise (Moore and Quirk, 2007) exclude the latent
segmentation variables and opt for a heuristic train-
ing procedure. In this work we also start out from a
generative model with latent segmentation variables.
However, we find out that concentrating the learning
effort on smoothing is crucial for good performance.
For this, we devise ITG-based priors over segmenta-
tions and employ a penalized version of Deleted Es-
timation working with EM at its core. The fact that
our results (at least) match the heuristic estimates on
a reasonably sized data set (947k parallel sentence
pairs) is rather encouraging.
The work in (Zhang et al, 2008) has a simi-
lar flavor to our work, yet the two differ substan-
tially. Both depart from Maximum-Likelihood to-
wards non-overfitting estimators. Where Zhang et al
choose for sparse priors (leading to sharp phrase dis-
tributions) and put the smoothing burden on the ITG
rule parameters and a pruning strategy, we choose
for a prior over segmentations determined by the
ITG derivation space and smooth the MLE directly
with a penalized version of Deleted Estimation. It
remains to be seen how the two biases compare to
one another on the same task.
There are various strands of future research.
Firstly, we plan to explore our estimator on other
language pairs in order to obtain more evidence on
its behavior. Secondly, as (Blunsom et al, 2008)
show, marginalizing out the different segmentations
during decoding leads to improved performance. We
plan to build our own decoder (based on ITG) where
different ideas can be tested including tractable ways
for achieving a marginalization effect. Apart from a
new decoder, it will be worthwhile adapting the prior
probability in our model to allow for consistent es-
timation. Finally, it would be interesting to study
properties of the penalized Deleted Estimation used
in this paper.
Acknowledgments: Both authors are supported
by a VIDI grant (nr. 639.022.604) from The Nether-
lands Organization for Scientific Research (NWO).
David Chiang and Andy Way are acknowledged for
stimulating discussions on machine translation and
parsing.
638
References
A. Birch, Ch. Callison-Burch, M. Osborne, and Ph.
Koehn. 2006. Constraining the phrase-based, joint
probability statistical translation model. In Proceed-
ings on the Workshop on Statistical Machine Trans-
lation, pages 154?157. Association for Computational
Linguistics.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-
inative latent variable model for statistical machine
translation. In Proceedings of ACL-08: HLT, pages
200?208. Association for Computational Linguistics.
D. Chiang. 2005a. A hierarchical phrase-based model
for statistical machine translation. In In Proceedings
of ACL 2005, pages 263?270.
D. Chiang. 2005b. An introduction to synchronous
grammars. Technical report, Univeristy of Maryland.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the em al-
gorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?38.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why generative phrase models underperform surface
heuristics. In Proceedings on the Workshop on Sta-
tistical Machine Translation, pages 31?38, New York
City. Association for Computational Linguistics.
R.O. Duda, P.E. Hart, and D.G. Stork. 2001. Pattern
Classification. John Wiley & Sons, NY, USA.
J.T. Goodman. 1998. Parsing Inside-Out. PhD thesis,
Departement of Computer Science, Harvard Univer-
sity, Cambridge, Massachusetts.
T. Hastie, R. Tibshirani, and J. H. Friedman. 2001. The
Elements of Statistical Learning. Springer.
H. Hoang and Ph. Koehn. 2008. Design of the moses de-
coder for statistical machine translation. In ACL Work-
shop on Software engineering, testing, and quality as-
surance for NLP 2008.
L. Huang, H. Zhang, D. Gildea, and K. Knight.
2008. Binarization of synchronous context-free
grammars. Submitted to Computational Linguistics.
http://www.cis.upenn.edu/ lhuang3/opt.pdf.
F. Jelinek and R. L. Mercer. 1980. Interpolated estima-
tion of markov source parameters from sparse data. In
In Proceedings of the Workshop on Pattern Recogni-
tion in Practice.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In HLT-NAACL.
K. Lari and S.J. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer, Speech and Language, 4:35?56.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proceedings of Empirical methods in natural lan-
guage processing, pages 133?139. Association for
Computational Linguistics.
R. Moore and Ch. Quirk. 2007. An iteratively-trained
segmentation-free phrase translation model for statisti-
cal machine translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
112?119, Prague, Czech Republic. Association for
Computational Linguistics.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In ACL, pages 311?318.
K. Sima?an and L. Buratto. 2003. Backoff Parame-
ter Estimation for the DOP Model. In H. Blockeel
N. Lavra ?C, D. Gamberger and L. Todorovski, editors,
Proceedings of the 14th European Conference on Ma-
chine Learning (ECML?03), Lecture Notes in Artifi-
cial Intelligence (LNAI 2837), pages 373?384, Cavtat-
Dubrovnik, Croatia. Springer.
K. Sima?an. 2002. Computational complexity of proba-
bilistic disambiguation. Grammars, 5(2):125?151.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403.
D.H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189?208.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In Matthias Jarke, Jana
Koehler, and Gerhard Lakemeyer, editors, KI 2002:
Advances in Artificial Intelligence, 25th Annual Ger-
man Conference on AI (KI 2002), volume 2479 of
Lecture Notes in Computer Science, pages 18?32.
Springer.
H. Zhang, L. Huang, D. Gildea, and K. Knight. 2006.
Synchronous binarization for machine translation. In
HLT-NAACL.
H. Zhang, Ch. Quirk, R. C. Moore, and D. Gildea.
2008. Bayesian learning of non-compositional phrases
with synchronous parsing. In Proceedings of ACL-08:
HLT, pages 97?105, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
A. Zollmann and K. Sima?an. 2006. An efficient and
consistent estimator for data-oriented parsing. Journal
of Automata, Languages and Combinatorics (JALC),
10 (2005) Number 2/3:367?388.
639
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 842?851,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
An Alternative to Head-Driven Approaches for
Parsing a (Relatively) Free Word-Order Language
Reut Tsarfaty Khalil Sima?an Remko Scha
Institute for Logic Language and Computation
University of Amsterdam
{r.tsarfaty,k.simaan,r.scha}@uva.nl
Abstract
Applying statistical parsers developed for
English to languages with freer word-
order has turned out to be harder than
expected. This paper investigates the
adequacy of different statistical parsing
models for dealing with a (relatively)
free word-order language. We show
that the recently proposed Relational-
Realizational (RR) model consistently
outperforms state-of-the-art Head-Driven
(HD) models on the Hebrew Treebank.
Our analysis reveals a weakness of HD
models: their intrinsic focus on configu-
rational information. We conclude that the
form-function separation ingrained in RR
models makes them better suited for pars-
ing nonconfigurational phenomena.
1 Introduction
Parsing technology has come a long way since
Charniak (1996) demonstrated that a simple tree-
bank PCFG performs better than any other parser
(with F
1
75 accuracy) on parsing the WSJ Penn
treebank (Marcus et al, 1993). Treebank Gram-
mars (Scha, 1990; Charniak, 1996) trained on
large corpora nowadays present the best available
means to parse natural language text.
The performance curve for parsing the WSJ was
a steep one at first, as the incorporation of no-
tions such as head, distance, subcategorization
(Charniak, 1997; Collins, 1999) brought about
a dramatic increase in parsing accuracy to the
level of F
1
88. Discriminative approaches, Data-
Oriented Parsing (?all-subtrees?) approaches, and
self-training techniques brought further improve-
ments, and recent results are starting to level off at
around F
1
92.1 (McClosky et al, 2008).
As the interest of the NLP community grows
to encompass more languages, we observe efforts
towards adapting an English parser for parsing
other languages (e.g., (Collins et al, 1999)), or
towards designing a language-independent frame-
work based on principles underlying the mod-
els for parsing English (Bikel, 2002). The per-
formance curve for parsing other languages with
these models looks rather different. A case in point
is Modern Standard Arabic. Since the initial ef-
fort of (Bikel, 2002) to parse the Arabic treebank
(Maamouri et al, 2004), which yielded F
1
75 ac-
curacy, four years and successive revisions have
led to no more than F
1
79 (Maamouri et al, 2008).
This pattern from Arabic is not peculiar. The
level of state-of-the-art results for other languages
still lags behind those for English, even after
putting considerable effort into the adaptation.1
Given that these languages are inherently differ-
ent from English and from one another, it appears
that we cannot avoid a question concerning the ad-
equacy of the models used to parse them. That is,
given the properties of a language, which model-
ing strategy would be appropriate for parsing it?
Until recently, there has been practically
no computationally affordable alternative to the
Head-Driven (HD) approach in the development
of phrase-structure based statistical parsing mod-
els. Recently, we proposed the Relational-
Realizational (RR) approach that rests upon differ-
ent premises (Tsarfaty and Sima?an, 2008). The
question of how the RR model fares against the
HD models that have so far been predominantly
used has never been tackled. Yet, it is precisely
such a comparison that can shed new light on the
question of adequacy we posed above.
Empirically quantifying the effects of differ-
ent modeling choices has been addressed for En-
glish by, e.g., (Johnson, 1998; Klein and Manning,
2003), and for German by, e.g., (Dubey, 2004;
1Consider, e.g., ?The PaGe shared task on parsing Ger-
man? (Kubler, 2008), reporting F
1
75, F
1
79, F
1
83 for the
participating parsers.
842
Rafferty and Manning, 2008). This paper provides
an empirical systematic comparison of conceptu-
ally different modeling strategies with respect to
parsing Hebrew. This comparison is intended to
provide a first answer to the question of parser ad-
equacy in the face of word-order freedom.
Our two empirical results are unequivocal.
Firstly, RR models significantly outperform HD
models (about 2 points absolute improvement in
F
1
) in parsing the Modern Hebrew treebank. In
particular, RR models show better performance
in identifying the constituents for which syntactic
positions are relatively free. Secondly, we show
a novel variation of the HD model, incorporating
the Relational notions of the RR model, on the hy-
pothesis that this might bridge the gap. The RR
model remains superior.
Our post-experimental analysis shows that HD
modeling is inherently problematic for parsing a
language with freer word-order because of the
hard-wiring of notions such as left, right and dis-
tance from the head. RR models, taking a prin-
cipled approach towards capturing variable form-
function correspondence patterns, are better suited
for parsing nonconfigurational phenomena.
2 The Data
This section describes some properties of Modern
Hebrew (henceforth, Hebrew) that make it signifi-
cantly different from English. These properties af-
fect the syntactic representations found in the He-
brew Treebank and the kind of syntactic phenom-
ena a parser for Hebrew has to cope with.
Modern Hebrew is a Semitic language with a
canonical SVO word-order pattern,2 yet it allows
considerable freedom in the placement of syntac-
tic constituents in a clause. For example, linguistic
elements of any kind may be fronted, triggering
an inversion familiar from Germanic languages
as in (1b) (Triggered Inversion (TI) in (Shlonsky,
1997)). Under some information structuring con-
ditions, Verb Initial (VI) constructions are also al-
lowed, as in (1c) (Melnik, 2002). All sentences
in (1) thus mean ?Dani gave the present to Dina?,
despite their different word-ordering.
(1) a. dani natan et hamatana ledina
Dani gave ACC the-present to-Dina
b. et hamatana natan dani ledina
ACC the-present gave Dani to-Dina
2SVO is an abbreviation for the Subject-Verb-Object type
in the basic word-order typology of (Greenberg, 1963).
Word Order Frequency Relative Frequency
SV 1612 41%
VS 1144 29%
No S 624 16%
No V 550 14%
Table 1: Modern Hebrew Predicative Clause-
Types in 3930 Predicative Matrix Clauses in the
Training Set of the Modern Hebrew Treebank.
c. natan dani et hamatana ledina
gave Dani ACC the-present to-Dina
A corpus study we conducted on a fragment of
the Modern Hebrew treebank reveals that although
there is a significant number of subjects preceding
verbs in simple (matrix) clauses (41%), there are
also a fair number of sentences for which this or-
der is reversed (29%), and there is evidence for
other configurations, such as empty realization of
subjects (16%) and non-verbal realization of pred-
icates (14%).
In the face of such lack of consistency in its
configurational position, the grammatical function
Object in Hebrew is indicated by Differential Ob-
ject Marking (DOM) (Aissen, 2003). NP objects
in Hebrew are marked for accusativity (using the
marker et) if they are also marked for definiteness
(indicated by the prefix ha). So, in contrast with
(2a)-(2b), the indefinite object renders (2c) un-
grammatical, and the missing accusativity renders
(2d) awkward. The fact that marking NP objects
involves the joint contribution of multiple surface
elements (et, ha) contributing features to the NP
constituent is referred to as extended exponence
(Matthews, 1993, p. 182).
(2) a. dani natan matana ledina
Dani gave present to-Dina
?Dani gave a present to Dina?
b. dani natan et hamatana ledina
Dani gave ACC the-present to-Dina
?Dani gave the present to Dina?
c. *dani natan et matana ledina
Dani gave ACC present to-Dina
d. ??dani natan hamatana ledina
Dani gave the-present to-Dina
These data pose a challenge to generative pars-
ing models, as they would be required to gener-
ate alternative word-order patterns while maintain-
ing a coherent pattern of object marking, encom-
843
passing the contribution of multiple surface expo-
nents. The question this paper addresses is there-
fore what kind of modeling approach would be ad-
equate for modeling the interplay between syntax
and morphology in marking grammatical relations
in Hebrew, as reflected by the sentence-pair (3).
They both mean, roughly, ?Dani gave the present
to Dina yesterday; their word-order vary, but the
pattern of object marking is retained.
(3) a. dani natan etmol et hamatana ledina
Dani gave yesterday ACC the-present
to-Dina
b. et hamatana natan etmol dani ledina
ACC the-present gave yesterday dani
to-dina
3 The Models
The different models we experiment with are all
trained on syntactic structures annotated in the
Modern Hebrew Treebank (Sima?an et al, 2001).
The native representation of clause-level cate-
gories in the Treebank employs flat structures.
This choice was made due to the lack of empirical
evidence in Hebrew for grouping freely positioned
syntactic elements to form a constituent.3 In order
to compensate for the ambiguity in the interpreta-
tion of flat structures, additional information such
as morphological marking and grammatical func-
tion labels is added to the phrase-structure trees.
3.1 The State-Splits Approach
The simplest way to encode grammatical func-
tions information on top of the phrase-structure
representation in the treebank is by decorating
non-terminal nodes with morphological or func-
tional features, similarly to the rich representation
format of syntactic categories in GPSG. This is
the approach taken by the annotators of the He-
brew treebank in which information about mor-
phological marking appears at multiple levels of
constituency (Guthmann et al, 2009), and func-
tional features (such as subject, object, etc.) deco-
rate phrase-level constituent labels (Sima?an et al,
2001). The S-level representation of our example
sentences (3a)?(3b) then would be as we depict
in figure 1, which can be read off as feature-rich
3Such clauses are defined formally as exocentric in for-
mal theories of syntax, and are used to describe syntactic
structures in, e.g., Tagalog, Hungarian and Warlpiri (Bres-
nan, 2001, page 110). This flat representation format is char-
acteristic of treebanks for other languages with relatively-free
word-order as well, such as German (cf. (Kubler, 2008)).
PCFG productions. We refer to this approach as
the State-Splits (SP) approach, which serves as the
baseline for the rest of our investigation.
3.2 The Head-Driven Approach
Following the linguistic wisdom that the inter-
nal organization of syntactic constituents revolves
around their heads, Head-Driven (HD) models
have been proposed by (Magerman, 1995; Char-
niak, 1997; Collins, 1999). In a generative HD
model, the head daughter is generated first, con-
ditioned on properties of the mother node. Then,
sisters of the head daughter are generated condi-
tioned on the head, typically by left and right gen-
eration processes. Overall, HD processes have the
modeling advantage that they capture structurally-
marked positions that characterize the argument
structure of the sentence. The simplest possible
process uses unigram probabilities, but (Klein and
Manning, 2003) show that using vertical and hori-
zontal Markovization improves parsing accuracy.4
An unlexicalized generative HD model will
generate our two example sentences as we illus-
trate in figure 2. The generation of the context-
free events in figure 1 is then broken down to
seven different context-free parameters each, en-
coding head-parent and head-sister structural rela-
tionships ? the latter mediated with a structurally-
marked delta function (?
i
). The rich morpho-
logical representation of phrase-level NP objects
(+def/acc), for instance, is conditioned on the
head sister, its direction, and the distance from the
head (check, e.g., nodes ?
L
1
,?
R
2
).
3.3 The Relational-Realizational Approach
The Relational-Realizational (RR) parsing model
of (Tsarfaty and Sima?an, 2008) similarly decom-
poses the generation of the context-free events in
figure 1 into multiple independent parameters, but
does so in a conceptually different way. Instead of
decomposing a context-free event to head and sis-
ters, the RR model is best viewed as a generative
grammar that decomposes it to form and function.
The RR grammar first generates a set of gram-
matical functions depicting the Relational Net-
work (RN) (Perlmutter, 1982) of the clause. This
4The success of Head-Driven models (Charniak, 1997;
Collins, 2003) was initially attributed to the fact that they
were fully lexicalized, but (Klein and Manning, 2003) show
that an unlexicalized model combining Head-Driven Marko-
vian processes with linguistically motivated state-splits can
approach the performance of fully lexicalized models.
844
(3a) S
NP-SBJ
Dani
VP-PRD
natan
gave
ADVP
etmol
yesterday
NP
+D+ACC
-OBJ
et-hamatana
the-present
PP-COM
le-dina
to-Dina
(3b) S
NP
+D+ACC
-OBJ
et-ha-matana
the-present
VP-PRD
natan
gave
ADVP
etmol
yesterday
NP-SBJ
Dani
Dani
PP-COM
le-dina
to-Dina
Figure 1: The State-Splits Approach for Ex. (3)
(3a) S
V P@S
L,?
L
1
, V P@S
NP
Dani
Dani
HEAD,V P@S
VP
natan
gave
R,?
R
1
, V P@S
ADVP
etmol
yesterday
R,?
R
2
, V P@S
NP
+D+ACC
et-ha-matana
the-Present
R,?
R
3
, V P@S
PP
le-dina
to-Dina
(3b) S
V P@S
L,?
L1
, V P@S
NP
+D+ACC
et-ha-matana
the-present
HEAD,V P@S
VP
natan
gave
R,?
R
1
, V P@S
ADVP
etmol
yesterday
R,?
R
2
, V P@S
NP
Dani
Dani
R,?
R
3
, V P@S
PP
le-dina
to-Dina
Figure 2: The Head-Driven Approach for Ex. (3)
(3a) S
{SBJ,PRD,OBJ,COM}@S
SBJ@S
NP
Dani
Dani
PRD@S
VP
natan
gave
PRD : OBJ@S
ADVP
etmol
yesterday
OBJ@S
NP
+D+ACC
et-hamatana
the-present
COM@S
PP
le-dina
to-Dina
(3b) S
{SBJ,PRD,OBJ,COM}@S
OBJ@S
NP
+D+ACC
et-ha-matana
the-Present
PRD@S
VP
natan
gave
PRD : SBJ@S
ADVP@S
etmol
yesterday
SBJ@S
NP
Dani
Dani
COM@S
PP
le-dina
to-Dina
Figure 3: The Relational-Realizational Approach
RN provides an abstract set-theoretic representa-
tion of the argument structure of the clause.5 This
is called the projection phase. Then an ordering
of the grammatical relations is generated, includ-
ing reserved contextual slots for adjunction and/or
punctuation marks. This is called the configura-
tion phase. Finally, each of the grammatical func-
tion labels and adjunction slots gets realized as a
morphosyntactic representation (a category label
plus dominated morphological features) of the re-
spective daughter constituent. This is called the
realization phase.6
Figure 3 shows the generation of sentences
(3a)?(3b) following the projection, configuration
and realization phases corresponding to the top-
down context-free layers of the tree. In both
cases, the same relational network is generated,
capturing the fact that they have the same argu-
ment structure. Then the different orderings of
the grammatical elements are generated, reserving
an adjunction slot for sentential modification (la-
beled by short context). Interestingly, the HD/RR
models for our sentences are of comparable size
(seven parameters) but the parameter types en-
code radically different notions. Illustrative of the
difference is the realization of a morphologically
marked NP object. In the RR model this is con-
ditioned on a grammatical relation (check, for in-
stance, node OBJ@S) and in the HD model it is
conditioned on linear ordering or configurational
notions such as left, right and distance.
4 Experiments
Goal We set out to compare the performance
of the different modeling approaches for pars-
ing Modern Hebrew. Considerable effort was de-
voted to making the models strictly comparable,
in terms of preparing the data, defining statistical
events, and unifying the rules determining cross-
cutting linguistic notions (e.g., heads and predi-
cates, grammatical functions and subcat sets). We
spell out some of the setup considerations below.
Data We use the Modern Hebrew treebank
(MHTB) (Sima?an et al, 2001) consisting of 6501
sentences from news-wire texts, morphologically
analyzed and syntactically annotated as phrase-
5Unlike in HD models or dependency grammars, the head
predicative element has no distinguished status here.
6Realization of adjunction slots (but not of function la-
bels) may generate multiple sisters adjoining at a single
position.
845
GF Description Applicable to. . .
PRD Predicative Elements VP, PREDP
SBJ Grammatical Subjects NP, SBAR
OBJ Direct Objects NP
COM Indirect Objects NP, PP
FInite Complements SBAR
IC Infinitival Complements VP
CNJ A Conjunct within
a Conjunction Structure All
Table 2: Grammatical Functions in the MHTB
SP-PCFG Expansion P(C
l
n
, . . . , C
h
, . . . , C
r
n
|P )
HD-PCFG Head P(C
h
|P )
Left Branch? P(L:?
l
1
, H:?
h
|C
h
, P )
Right Branch? P(C
h
, R:?
r
1
|?
h
, C
h
, P )
Left Arg/Mod P(C
l
i
,?
l
i+1
| L ,?
l
i
, C
h
, P )
Right Arg/Mod P(C
r
i
,?
r
i+1
| R ,?
r
i
, C
h
, P )
Left Final? P(C
1
| L ,?
l
n?1
, C
h
, P )
Right Final? P(C
n
| R ,?
r
n?1
, C
h
, P )
RR-PCFG Projection P({gr
1
, . . . , gr
m
}|P )
Configuration P(?gr
1
, . . . , gr
m
?|{gr
1
, . . . , gr
m
}P )
Realization P(C
j
|gr
j
, P )
Adjunction P(C
j
1
, . . . , C
j
n
|gr
j
: gr
j+1
, P )
Table 3: PCFG Parameter Classes for All Models
structure trees. In our version of the MHTB, def-
initeness and accusativity features are percolated
from the PoS-tags level to phrase-level categories,
extending the procedure of (Guthmann et al,
2009). For all models, we applied non-terminal
state-splits distinguishing finite from non-finite
verb forms and possessive from non-possessive
noun phrases. We head-annotated the treebank,
and based on the ?subject?, ?object?, ?complement?
and ?conjunction? labels in the MHTB we devised
an automatic procedure to annotate all the gram-
matical functions indicated in table 2.7
Procedure For all models, we learn a PCFG by
reading off the parameters described in table 3,
in accordance with the trees depicted in figures
1?3.8 For all models, we use relative frequency
estimates. For lexical parameters, we use a sim-
ple smoothing procedure assigning probability to
unknown words using the per-tag distribution of
rare words (?rare? threshold set to < 2). The in-
put to our parser consists of morphologically seg-
mented surface forms, and the parser has to as-
7The enhanced corpus will be available at www.
science.uva.nl/
?
rtsarfat/resources.htm.
8Our training procedure is strictly equivalent to the
transform-detransform methodology of (Johnson, 1998), but
we implement a tree-traverse procedure as in (Bikel, 2002)
collecting all parameters per event at once.
sign the syntactic as well as morphological anal-
ysis to the surface segments.9 We use the stan-
dard development/training/test split as in (Tsarfaty
and Sima?an, 2008). Since our goal is a detailed
comparison and fine-grained analysis of the results
we concentrate on the development set. We use
a general-purpose CKY parser (Schmid, 2004) to
exhaustively parse the sentences, and we strip off
all model-specific information prior to evaluation.
Evaluation We use standard Parseval measures
calculated for the original, flat, canonical repre-
sentation of the parse trees.10 We report Pre-
cision/Recall for the coarse-grained non-terminal
categories. In addition to overall Parseval scores
we report the accuracy results Per Syntactic Cate-
gory. We further report model size in terms of the
number of parameters. As is well known in Ma-
chine Learning, models with more parameters re-
quire more data to learn, and are more vulnerable
to sparseness. In our evaluation we thus follow the
rule of thumb that (all else being equal) for mod-
els of equal size the better performing model is
preferred, and for models with equal performance,
the smaller one is preferred.
5 Results and Analysis
5.1 Overall Results
Table 4 shows the parsing results for the State-
Split (SP) PCFG, the Head-Driven (HD) PCFG
and the Relational-Realizational (RR) PCFG
models on parsing the Modern Hebrew Treebank,
with definiteness and accusativity marked on PoS-
tags as well as phrase-level categories. For all
models, we experiment with grandparent encod-
ing. For non-HD models, we also examine the
utility of a head-category split.11
9This setup is more difficult than, e.g., the Arabic parsing
setup of (Bikel, 2002), as they assume gold-standard pos-tags
as input. Yet it is easier than the setup of (Tsarfaty, 2006;
Goldberg and Tsarfaty, 2008) which uses unsegmented sur-
face forms as input. The decision to use segmented and un-
tagged forms was made to retain a realistic scenario. Mor-
phological analysis is known to be ambiguous, and we do
not assume that morphological features are known up front.
Morphological segmentation is also ambiguous, but for our
purposes it is unavoidable. When comparing different mod-
els on an individual sentence they may propose segmenta-
tion to sequences of different lengths, for which accuracy re-
sults cannot be faithfully compared. See (Tsarfaty, 2006) for
discussion.
10The flat canonical representation also allows for a fair
comparison that is not biased by the differing branching fac-
tors of the different models.
11In HD models, a head-tag is already assumed in the con-
ditioning context for sister nodes (Klein and Manning, 2003).
846
SP-PCFG
Grand-Parent ? ? + +
Head-Tag ? + ? +
Prec/Rec 70.05/72.40 71.14/72.03 74.66/74.35 71.99/72.17
(#Params) (4995) (8366) (7385) (11633)
HD-PCFG
Grand-Parent ? ? + +
Markov 0 1 0 1
Prec/Rec 66.87/71.64 70.40/74.35 73.04/71.94 73.52/74.84
(#Params) (6678) (10015) (19066) (21399)
RR-PCFG
Grand-Parent ? ? + +
Head Tag ? + ? +
Prec/Rec 69.90/73.96 72.96/75.73 74.19/75.03 76.32/76.51
(#Params) (3791) (7546) (7611) (13618)
Table 4: The Performance of Different Models
in Parsing Hebrew: Parsing Results Prec/Recall
for Sentences of Length ? 40.
For all models, grandparent encoding is help-
ful. For HD models, a higher Markovian order im-
proves performance. This shows that even in He-
brew there are linear-precedence tendencies that
help steer the disambiguation in the right direc-
tion, which is in line with our observation that
word-order patterns in Modern Hebrew are not
completely free (cf. table 1).
The best SP model performs equally or better
than all HD models. This might be due to the
smaller size of SP grammars, resulting in more ro-
bust estimates. But it is remarkable that, given the
feature-rich representation, such a simple treebank
grammar provides better disambiguation capacity
than linguistically articulated HD models. We at-
tribute this to the fact that parent-daughter rela-
tions have a stronger association with grammati-
cal functions than relations between neighbouring
nodes. For Hebrew, such adjacency relations may
be arbitrary due to word-order variability.
Overall, RR models show the best performance
for the set of all models with parent encoding, and
for the set of all models without. Our best RR
model shows 6.6%/8.4% Prec/Rec error reduction
from the best SP model. The Recall improvement
shows that the RR model is much better in gener-
alizing, recovering successfully more of the con-
stituents found in the gold representation. The
best RR model also outperforms HD models with
8.7%/6.7% Prec/Rec error reduction from the best
In our SP or RR models, head-information is used as yet an-
other feature-value pair rather than an object with a distin-
guished status during generation.
Model / SP-PCFG HD-PCFG RR-PCFG
Category
NP 77.39 / 74.32 77.94 / 73.75 78.96 / 76.11
PP 71.78 / 71.14 71.83 / 69.24 74.4 / 72.02
SBAR 55.73 / 59.71 53.79 / 57.49 57.97 / 61.67
ADVP 71.37 / 77.01 72.52 / 73.56 73.57 / 77.59
ADJP 79.37 / 78.96 78.47 / 77.14 78.69 / 78.18
S 73.25 / 79.07 71.07 / 76.49 72.37 / 78.33
SQ 36.00 / 32.14 30.77 / 14.29 55.56 / 17.86
PREDP 36.31 / 39.63 44.74 / 39.63 44.51 / 46.95
VP 76.34 / 80.81 77.33 / 82.51 78.59 / 81.18
Table 5: Per-Category Evaluation of Parsing
Performance for Different Models: Prec/Rec
Per Category Calculated for All Sentences.
HD model. The resulting precision improvement
of the RR relative to HD is larger than the im-
provement relative to SP, and the Recall improve-
ment pattern is reversed. So it seems that the HD
model generalizes better than the SP model, but
also gets generalizations wrong more often than
the SP model.
The RR model combines the generalization
advantage of breaking down context-free events
while it maintains the coherence advantage of
learning flat trees (cf. (Johnson, 1998)). The best
RR model obtains the best performance among
all models: F
1
76.41. To put this result in con-
text, for the setting in which the Arabic parser of
(Maamouri et al, 2008) obtains F
1
78.1, ? i.e.,
with gold standard feature-rich tags ? the best
RR model obtains F
1
83.3 accuracy which is the
best parsing result reported for a Semitic language
so far. RR models also have the advantage of re-
sulting in more compact grammars, which makes
learning and parsing with them much more com-
putationally efficient.
5.2 Per-Category Break-Down Analysis
To understand better the merits of the different
models we conducted a break-down analysis of
performance-per-category for the best performing
models of each kind. The break-down results are
shown in table 5. We divided the table into three
sets of categories: those for which the RR model
gave the best performance, those for which the SP
model gave the best performance, and those for
which there is no clear trend.
The most striking outcome is that the RR model
identifies at higher accuracy precisely those syn-
tactic elements that are freely positioned with re-
847
spect to the head: NPs, PPs, ADVPs and SBARs.
Adjectives, in contrast, have clear ordering con-
straints ? they always appear after the noun. S
level elements, when embedded, always appear
immediately after a conjunction or a relativizer.
In particular, NPs and PPs realize arguments and
adjuncts that may occupy different positions rela-
tive to the head. The RR model is better than the
other models in identifying those elements partly
because morphological information helps to dis-
ambiguate syntactically relevant chunks and make
correct attachment decisions about them.
Remarkably, predicative (verb-less) phrases
(PREDP), which are characteristic of Semitic lan-
guages, are hard to parse, but here too the RR does
slightly better than the other two, as it allows for
variability in the means to realize a (verbal or verb-
less) predicate. Both RR and HD models outper-
form SP for VPs, which is due to the specific na-
ture of VPs in the MHTB ? they exist only for
complement phrases with strict linear ordering.
6 Distances, Functions and
Subcategorization Frames
Markovian processes to the left and to the right of
the head provide a first approximation of the pred-
icate?s argument structure, as they capture trends
in the co-occurrences of constituents reflected in
their pattern of positioning and adjacency. But
as our results so far show, such an approxima-
tion is empirically less rewarding for a language
in which grammatical relations are not tightly cor-
related with structural notions.12
Collins (2003) attempted a more abstract for-
mulation of argument-structure by articulating left
and right subcat-sets. Each set represents those
arguments that are expected to occur at each side
of the head. Argument sisters (?complements?)
are generated if and only if they are required, and
their generation ?cancels? the requirement in the
set. Adjuncts (?modifiers?) may be freely gener-
ated at any position.
At first glance, such a dissociation of configura-
tional positions and subcategorization sets seems
to be more adequate for parsing Hebrew, because
it allows for some variability in the order of gen-
eration. But here too, since the model uses sets of
12Conditioning based on adjacency and distance is also
common inside dependency parsing models, and we conjec-
ture that this is one of the reasons for their difficulty in coping
with freer word-order languages, a difficulty pointed out in
(Nivre et al, 2007).
(3a) S
V P@S
L,{SBJ}, V P@S
NP
Dani
Dani
H,V P@S
VP
natan
gave
R,{OBJ,COM}, V P@S
ADVP
etmol
yesterday
R,{OBJ,COM}, V P@S
NP
+D+ACC
et-ha-matana
the-Present
R,{COM}, V P@S
PP
le-dina
to-Dina
(3b) S
V P@S
L,{OBJ}, V P@S
NP
+D+ACC
et-ha-matana
the-present
H,V P@S
VP
natan
gave
R,{SBJ,COM}, V P@S
ADVP
etmol
yesterday
R,{SBJ,COM}, V P@S
NP
Dani
Dani
R,{COM}, V P@S
PP
le-dina
to-Dina
Figure 4: The Relational Head-Driven Approach
constituent labels, it disambiguates the grammati-
cal functions of an NP solely based on the direc-
tion of the head, which is adequate for English but
not for Hebrew. In order to relax this association
further, we propose to replace constituent labels
in the subcat-sets with grammatical relations iden-
tical to the functional elements in the relational
network of the RR. This provides means to medi-
ate the cancellation of constituents in the sets with
their functions and correlate it with morphology.
To get an idea of the implications of such a
modeling strategy, let us consider our example
sentences in such a Relational-HD model as de-
picted in figure 4. Both representations share
the event of generating the verbal head. Sisters
are generated conditioned on the head and the
functional elements remaining to be ?cancelled?.
Each of the two trees consists of an event real-
izing an ?object?, one for an NP to the right of
the head, and the other for an NP to its left. In
both cases, an object constituent will be generated
jointly with the morphological features associated
with it. Evidently, when using sets of grammatical
relations instead of constituent-labels, correlation
of morphology and grammatical functions is more
straight-forward to maintain.
848
Model SP-PCFG HD-PCFG HD-PCFG HD-PCFG HD-PCFG RR-PCFG
Type of Distance ? Phrase-Level Intervening Left and Right Left and Right Left and Right Subcat Sets
or Subcategorization State-Splits Verb/Punc #Constituents Constituent Labels Function Labels Configuration
Precision/Recall 70.95/70.32 72.39 / 71.97 72.70 / 74.46 72.42 / 74.29 72.84/74.62 76.32/76.51
(#Params) (13884) (11650) (18058) (16334) (16460) (13618)
Table 6: Incorporating Distance and Grammatical Functions into Head-Driven Parsing Models
Reporting Precison/Recall (#Parameters) for Sentences Length < 40.
6.1 Results and Analysis
Table 6 reports the results of experimenting with
HD models with different instantiations of a dis-
tance function, starting from the standard notion
of (Collins, 2003) and ending with our proposed,
relational, function sets. For all HD models, we
retain the head, left and right generation cycle and
only change the conditioning context (?
i
) for sis-
ter generation.
As a baseline, we show the results of adding
grammatical function information as state-splits
on top of an SP-PCFG.13 This SP model presents
much lower performance than the RR model al-
though they are almost of the same size and they
start off with the same information. This result
shows that sophisticated modeling can blunt the
claws of the sparseness problem. One may ob-
tain the same number of parameters for two dif-
ferent models, but correlate them with more pro-
found linguistic notions in one model than in the
other. In our case, there is more statistical evi-
dence in the data for, e.g., case marking patterns,
than for association of grammatical relations with
structurally-marked positions.
For all HD variations, the RR model contin-
ues to outperform HD models. The function-set
variation performs slightly (but not significantly)
better than the category-set. What seems to be
still standing in the way of getting useful dis-
ambiguation cues for HD models is the fact that
the left and right direction of realization is hard-
wired in their representation. This breaks down a
coherent distribution over morphosyntactic repre-
sentations realizing grammatical relations to arbi-
trary position-dependent fragments, which results
in larger grammars and inferior performance.14
13The startegy of adding grammatical functions as state-
splits is used in, e.g., German (Rafferty and Manning, 2008).
14Due to the difference in the size of the grammars, one
could argue that smoothing will bridge the gap between
the HD and RR modeling strategies. However, the better
size/accuracy trade-off shown here for RR models suggests
that they provide a good bias/variance balancing point, es-
pecially for feature-rich models characterizing morphologi-
7 A Typological Detour
Hebrew, Arabic and other Semitic Languages are
known to be substantially different from English
in that English is strongly configurational. In
configurational languages word-order is fixed, and
information about the grammatical functions of
constituents (e.g., subject or object) is often cor-
related with structurally-marked positions inside
highly-nested constituency structures. Nonconfig-
urational languages (Hale, 1983), in contrast, al-
low for freedom in their word-ordering and infor-
mation about grammatical relations between con-
stituents is often marked by means of morphology.
Configurationality is hardly a clear-cut notion.
The difference in the configurationality level of
different languages is often conceived as depicted
in figure 7. In linguistic typology, the branch
of linguistics that studies the differences between
languages (Song, 2001), the division of labor be-
tween linear ordering and morphological marking
in the realization of grammatical relations is of-
ten viewed as a continuum. Common wisdom has
it that the lower a language is on the configura-
tionality scale, the more morphological marking
we expect to be used (Bresnan, 2001, page 6).
For a statistical parser to cope with nonconfig-
urational phenomena as observed in, for instance,
Hebrew or German, it should allow for flexibil-
ity in the form of realization of the grammati-
cal functions within the phrase-structure represen-
tation of trees. Recent morphological theories
employ Form-Function separation as a widely-
accepted practice for enhancing the adequacy of
models describing variability in the realization of
grammatical properties. Our results suggest that
the adequacy of syntactic processing models is re-
lated to such typological insights as well, and is
enhanced by adopting a similar form-function sep-
aration for expressing grammatical relations.
cally rich languages. A promising strategy then would be to
smooth or split-and-merge (Petrov et al, 2006)) RR-based
models rather than to add an elaborate smoothing component
to configurationally-based HD models.
849
configurational ?????? nonconfigurational
Chinese>English>{German,Hebrew}>Warlpiri
Figure 5: The Configurationality Scale
The HD assumptions take the function of a con-
stituent to be transparently related to its formal
position, which entails word-order rigidity. Such
transparent relations between configurational po-
sitions and grammatical functions are assumed by
other kinds of parsing frameworks such as the ?all-
subtrees? approach of Data-Oriented Parsing, and
the distinction between left and right application
in CCG-based parsers.
The RR modeling strategy stipulates a strict
separation between form ? parametrizing explic-
itly basic word-order (Greenberg, 1963) and mor-
phological realization (Greenberg, 1954) ? and
function ? parametrizing relational networks bor-
rowed from (Perlmutter, 1982) ? which makes
it possible to statistically learn complex form-
function mapping reflected in the data. This is
an adequate means to capture, e.g., morphosyn-
tactic interactions, which characterize the less-
configurational languages on the scale.
8 Conclusion
In our comparison of the HD and RR modeling
approaches, the RR approach is shown to be em-
pirically superior and typologically more adequate
for parsing a language exhibiting word-order vari-
ation interleaved with extended morphology. HD
models are less accurate and more vulnerable to
sparseness as they assume transparent mappings
between form and function, based on left and right
decompositions hard-wired in the HD representa-
tion. RR models, in contrast, employ form and
function separation which allows the statistical
model to learn complex correspondance patterns
reflected in the data. In the future we plan to in-
vestigate how the different models fare against one
another in parsing different languages. In particu-
lar we wish to examine whether parsing different
languages should be pursued by different models,
or whether the RR strategy can effectively cope
with different languages types. Finally, we wish
to explore the implications of RR modeling for
applications that consider the form of expression
in multiple languages, for instance Statistical Ma-
chine Translation (SMT).
9 Acknowledgements
We thank Jelle Zuidema, Inbal Tsarfati, David
McCloskey and Yoav Golderg for excellent com-
ments on earlier versions. We also thank Miles
Osborne and Tikitu de Jager for comments on the
camera-ready draft. All errors are our own. The
work of the first author is funded by the Dutch Sci-
ence Foundation (NWO) grant 017.001.271.
References
J. Aissen. 2003. Differential Object Marking: Iconic-
ity vs. Economy. Natural Language and Linguistic
Theory, 21.
D. M. Bikel. 2002. Design of a Multi-lingual, Parallel-
processing Statistical Parsing Engine. In Proceed-
ings of HLT.
J. Bresnan. 2001. Lexical-Functional Syntax. Black-
well Textbooks in Linguistics. Blackwell.
E. Charniak. 1996. Tree-Bank Grammars. In
AAAI/IAAI, Vol. 2.
E. Charniak. 1997. Statistical Parsing with a Context-
Free Grammar and Word Statistics. In AAAI/IAAI.
M. Collins, J. Hajic?, E. Brill, L. Ramshaw, and C. Till-
mann. 1999. A Statistical Parser of Czech. In Pro-
ceedings ACL.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. Collins. 2003. Head-Driven Statistical Models for
Natural Language Parsing. Computational Linguis-
tics.
A. Dubey. 2004. Statistical Parsing for German:
Modeling syntactic properties and annotation differ-
ences. Ph.D. thesis, Saarland University, Germany.
Y. Goldberg and R. Tsarfaty. 2008. A Single Frame-
work for Joint Morphological Segmentation and
Syntactic Parsing. In Proceedings of ACL.
J.H. Greenberg. 1954. A Quantitative Approach to
the Morphological Typology of Language. In R. F.
Spencer, editor, Method and Perspective in Anthro-
pology. University of Minessota Press.
J. H. Greenberg. 1963. Some Universals of Grammar
with Particular Reference to the Order of Meaning-
ful Elements. In Joseph H. Greenberg, editor, Uni-
versals of Language. MIT Press.
N. Guthmann, Y. Krymolowski, A. Milea, and Y. Win-
ter. 2009. Automatic Annotation of Morpho-
Syntactic Dependencies in a Modern Hebrew Tree-
bank. In Proceedings of TLT.
850
K. L. Hale. 1983. Warlpiri and the Grammar of Non-
Configurational Languages. Natural Language and
Linguistic Theory, 1(1).
M. Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24(4).
D. Klein and C. Manning. 2003. Accurate Unlexical-
ized Parsing. In Proceedings of ACL.
S. Kubler. 2008. The PaGe Shared task on Parsing
German. In ACL Workshop on Parsing German.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The Penn Arabic Treebank: Building a Large-
Scale Annotated Arabic Corpus. In Proceedings of
NEMLAR.
M. Maamouri, A. Bies, and S. Kulick. 2008. Enhanced
Annotation and Parsing of the Arabic treebank. In
Proceedings of INFOS.
D. M. Magerman. 1995. Statistical Decision-Tree
Models for Parsing. In Proceedings of ACL.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics.
P. H. Matthews. 1993. Morphology. Cambridge.
D. McClosky, E. Charniak, and M. Johnson. 2008.
When is self-training effective for parsing? In Pro-
ceedings of CoLing.
N. Melnik. 2002. Verb-Initial Constructions in Mod-
ern Hebrew. Ph.D. thesis, Berkeley, California.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The Shared Task on Dependency Pars-
ing. In Proceedings of the CoNLL Shared Task.
D. M. Perlmutter. 1982. Syntactic Representation,
Syntactic Levels, and the Notion of a Subject. In
Pauline Jacobson and Geoffrey Pullum, editors, The
Nature of Syntactic Representation. Springer.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning Accurate, Compact, and Interpretable Tree
Annotation. In Proceedings of ACL.
A. Rafferty and C. D. Manning. 2008. Parsing Three
German Treebanks: Lexicalized and Unlexicalized
Baselines. In ACL WorkShop on Parsing German.
R. Scha. 1990. Language Theory and Language Tech-
nology; Competence and Performance. In Q. A. M.
de Kort and G. L. J. Leerdam, editors, Computer-
toepassingen in de Neerlandistiek. Almere: LVVN.
H. Schmid. 2004. Efficient Parsing of Highly Am-
biguous Context-Free Grammars with Bit vectors.
In Proceedings of COLING.
U. Shlonsky. 1997. Clause Structure and Word Order
in Hebrew and Arabic. Oxford University Press.
K. Sima?an, A. Itai, Y. Winter, A. Altman, and N. Na-
tiv. 2001. Building a Tree-Bank for Modern He-
brew Text. In Traitement Automatique des Langues.
J. J. Song. 2001. Linguistic Typology: Morphology
and Syntax. Pearson Education Limited, Edinbrugh.
R. Tsarfaty and K. Sima?an. 2008. Relational-
Realizational Parsing. In Proceedings of CoLing.
R. Tsarfaty. 2006. Integrated Morphological and Syn-
tactic Disambiguation for Modern Hebrew. In Pro-
ceeding of ACL-SRW.
851
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1182?1191,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
A Syntactified Direct Translation Model with Linear-time Decoding
Hany Hassan
Cairo TDC
IBM
Cairo, Egypt
hanyh@eg.ibm.com
Khalil Sima?an
Language and Computation
University of Amsterdam
Amsterdam, The Netherlands
k.simaan@uva.nl
Andy Way
School of Computing
Dublin City University
Dublin, Ireland
away@computing.dcu.ie
Abstract
Recent syntactic extensions of statisti-
cal translation models work with a syn-
chronous context-free or tree-substitution
grammar extracted from an automatically
parsed parallel corpus. The decoders ac-
companying these extensions typically ex-
ceed quadratic time complexity.
This paper extends the Direct Transla-
tion Model 2 (DTM2) with syntax while
maintaining linear-time decoding. We
employ a linear-time parsing algorithm
based on an eager, incremental interpre-
tation of Combinatory Categorial Gram-
mar (CCG). As every input word is pro-
cessed, the local parsing decisions resolve
ambiguity eagerly, by selecting a single
supertag?operator pair for extending the
dependency parse incrementally. Along-
side translation features extracted from
the derived parse tree, we explore syn-
tactic features extracted from the incre-
mental derivation process. Our empiri-
cal experiments show that our model sig-
nificantly outperforms the state-of-the art
DTM2 system.
1 Introduction
Syntactic structure is gradually showing itself to
constitute a promising enrichment of state-of-the-
art Statistical Machine Translation (SMT) models.
However, it would appear that the decoding algo-
rithms are bearing the brunt of this improvement in
terms of time and space complexity. Most recent
extensions work with a synchronous context-free
or tree-substitution grammar extracted from an au-
tomatically parsed parallel corpus. While attrac-
tive in many ways, the decoders that are needed
for these types of grammars usually have time
and space complexities that are far beyond linear.
Leaving pruning aside, there is a genuine ques-
tion as to whether syntactic structure necessarily
implies more complex decoding algorithms. This
paper shows that this need not necessarily be the
case.
In this paper we extend the Direct Translation
Model (DTM2) (Ittycheriah and Roukos, 2007)
with target language syntax while maintaining
linear-time decoding. With this extension we
make three novel contributions to SMT. Our first
contribution is to define a linear-time syntactic
parser that works as incrementally as standard
SMT decoders (Tillmann and Ney, 2003; Koehn,
2004a). At every word position in the target lan-
guage string, this parser spans at most a single
parse-state to augment the translation states in
the decoder. The parse state summarizes previ-
ous parsing decisions and imposes constraints on
the set of valid future extensions such that a well-
formed sequence of parse states unambiguously
defines a dependency structure. This approach
is based on an incremental interpretation of the
mechanisms of Combinatory Categorial Grammar
(CCG) (Steedman, 2000).
Our second contribution lies in extending the
DMT2 model with a novel set of syntactically-
oriented feature functions. Crucially, these feature
functions concern the derived (partial) dependency
structure as well as local aspects of the derivation
process, including such information as the CCG
lexical categories (supertag), the CCG operators
and the intermediate parse states. This accom-
plishment is interesting both from a linguistic and
technical point of view.
Our third contribution is the extension of the
standard phrase-based decoder with the syntactic
structure and definition of new grammar-specific
pruning techniques that control the size of the
search space. Interestingly, because it is eager,
the incremental parser used in this work is hard
pushed to perform at a parsing level close to state-
1182
of-the-art cubic-time parsers. Nevertheless, the
parsing information it provides allows for signif-
icant improvement in translation quality.
We test the new model, called the Dependency-
based Direct Translation Model (DDTM), on stan-
dard Arabic?English translation tasks used in the
community, including LDC and GALE data. We
show that our DDTM system provides significant
improvements in BLEU (Papineni et al, 2002) and
TER (Snover et al, 2006) scores over the already
extremely competitive DTM2 system. We also
provide results of manual, qualitative analysis of
the system output to provide insight into the quan-
titative results.
This paper is organized as follows. Section 2
reviews the related work. Section 3 discusses the
DTM2 baseline model. Section 4 presents the gen-
eral workings of the incremental CCG parser lay-
ing the foundations for integrating it into DTM2.
Section 5 details our own DDTM, the dependency-
based extension of the DTM2 model. Section 6
reports on extensive experiments and their results.
Section 7 provides translation output to shed fur-
ther detailed insight into the characteristics of the
systems. Finally, Section 8 concludes, and dis-
cusses future work.
2 Related Work
In (Marcu et al, 2006), it is demonstrated that
?syntactified? target language phrases can im-
prove translation quality for Chinese?English. A
stochastic, top-down transduction process is em-
ployed that assigns a joint probability to a source
sentence and each of its alternative syntactified
translations; this is done by specifying a rewrit-
ing process of the target parse-tree into a source
sentence.
Likewise, the model in (Zollmann and Venu-
gopal, 2006) extends (Chiang, 2005) by augment-
ing the hierarchical phrases with syntactic cate-
gories derived from parsing the target side of a
parallel corpus. They use an existing parser to
parse the target side of the parallel corpus in or-
der to extract a syntactically motivated, bilingual
synchronous grammar as in (Chiang, 2005).
The above-mentioned approaches for incor-
porating syntax into Phrase-based SMT (Marcu
et al, 2006; Zollmann and Venugopal, 2006)
share common drawbacks. Firstly, they are
based on syntactic phrase-structure parse trees
incorporated into a Synchronous CFG or Tree-
Substitution Grammar, which makes for a diffi-
cult match with non-constituent phrases that are
common within Phrase-based SMT. These ap-
proaches usually resort to ad hoc solutions to
enrich the non-constituent phrases with syntactic
structures. Secondly, they deploy chart-based de-
coders with a high computational cost compared
with the phrase-based beam search decoders, e.g.,
(Tillmann and Ney, 2003; Koehn, 2004a). Thirdly,
due to the large parse space, some of the pro-
posed approaches are forced to employ small lan-
guage models compared to what is usually used
in phrase-based systems. To circumvent these
computational limitations, various pruning tech-
niques are usually needed, e.g., (Huang and Chi-
ang, 2007).
Other recent approaches, e.g., (Birch et al,
2007; Hassan et al, 2007; Hassan et al, 2008a)
incorporate a linear-time supertagger into SMT to
take the role of a syntactic language model along-
side the standard language model. While these ap-
proaches share with our work the use of lexical-
ized grammars, they never seek to build a full de-
pendency tree or employ syntactic features in or-
der to directly influence the reordering probabili-
ties in the decoder. In the current work, we ex-
pand our previous work in (Hassan et al, 2007;
Hassan et al, 2008a) to introduce the capabilities
of building a full dependency structure and em-
ploying syntactic features to influence the decod-
ing process.
Recently, (Shen et al, 2008) introduced an ap-
proach for incorporating a dependency-based lan-
guage model into SMT. They proposed to extract
String-to-Dependency trees from the parallel cor-
pus. As the dependency trees are not constituents
by nature, they handle non-constituent phrases as
well. While this work is in the same general
direction as our work, namely aiming at incor-
porating dependency parsing into SMT, there re-
main three major differences. Firstly, (Shen et al,
2008) resorted to heuristics to extract the String-
to-Dependency trees, whereas our approach em-
ploys the well formalized CCG grammatical the-
ory. Secondly, their decoder works bottom-up
and uses a chart parser with a limited language
model capability (3-grams), while we build on the
efficient, linear-time decoder commonly used in
phrase-based SMT. Thirdly, (Shen et al, 2008)
deploys the dependency language model to aug-
ment the lexical language model probability be-
1183
tween two head words but never seek a full de-
pendency graph. In contrast, our approach inte-
grates an incremental parsing capability, that pro-
duces the partial dependency structures incremen-
tally while decoding, and thus provides for better
guidance for the search of the decoder for more
grammatical output. To the best of our knowledge,
our approach is the first to incorporate incremental
dependency parsing capabilities into SMT while
maintaining the linear-time and -space decoder.
3 Baseline: Direct Translation Model 2
The Direct Translation Model (DTM) (Papineni
et al, 1997) employs the a posteriori conditional
distribution P (T |S) of a target sentence T given
a source sentence S, instead of the common in-
version into P (S|T ) based on the source chan-
nel approach (Brown et al, 1990). DTM2, in-
troduced in (Ittycheriah and Roukos, 2007), ex-
presses the phrase-based translation task in a uni-
fied log-linear probabilistic framework consisting
of three components: (i) a prior conditional dis-
tribution P
0
(.|S), (ii) a number of feature func-
tions ?
i
() that capture the translation and language
model effects, and (iii) the weights of the features
?
i
that are estimated under MaxEnt (Berger et al,
1996), as in (1):
P (T |S) =
P
0
(T, J |S)
Z
exp
?
i
?
i
?
i
(T, J, S) (1)
Here J is the skip reordering factor for the phrase
pair captured by ?
i
() and represents the jump from
the previous source word, and Z is the per source
sentence normalization term. The prior probabil-
ity P
0
is the prior distribution for the phrase prob-
ability which is estimated using the phrase nor-
malized counts commonly used in conventional
Phrase?based SMT systems, e.g., (Koehn et al,
2003).
DTM2 differs from other Phrase?based SMT
models in that it extracts from a word-aligned par-
allel corpus only a non-redundant set of minimal
phrases in the sense that no two phrases overlap
with each other.
Baseline DTM2 Features: The baseline em-
ploys the following five types of features (beside
the language model):
? Lexical Micro Features examining source
and target words of the phrases,
? Lexical Context Features encoding the
source and target phrase context (i.e. previ-
ous and next source and previous target),
? Source Morphological Features encoding
morphological and segmentation characteris-
tics of source words.
? Part-of-Speech Features encoding source and
target POS tags as well as the POS tags of the
surrounding contexts of phrases.
The DTM2 approach based on MaxEnt provides
a flexible framework for incorporating other avail-
able feature types as we demonstrate below.
DTM2 Decoder: The decoder for the baseline is
a beam search decoder similar to decoders used in
standard phrase-based log-linear systems such as
(Tillmann and Ney, 2003) and (Koehn, 2004a).
The main difference between the DTM2 decoder
and the standard Phrase?based SMT decoders is
that DTM2 deploys Maximum Entropy probabilis-
tic models to obtain the translation costs and var-
ious feature costs by deploying the features de-
scribed above in a discriminative MaxEnt fashion.
In the rest of this paper we adopt the DTM2 for-
malization of translation as a discriminative task,
and we describe the CCG-based incremental de-
pendency parser that we use for extending the
DTM2 decoder, and then list a new set of syntac-
tic dependency feature functions that extend the
DTM2 feature set. We also discuss pruning and
other details of the approach.
4 The Incremental Dependency Parser
As it processes an input sentence left-to-right
word-by-word, the incremental dependency model
builds?for each prefix of the input sentence?a
partial parse that is a subgraph of the partial parse
that it builds for a longer prefix. The dependency
graph is constructed incrementally, in that the sub-
graph constructed at a preceding step is never al-
tered or revised in any later steps. The following
schematic view in (2) exhibits the general work-
ings of this parser:
S
0
o
1
w
1
,st
1
//
S
1
o
2
w
2
,st
2
//
S
2
S
i
o
i
w
i
,st
i
//
S
i+1
S
n
(2)
The syntactic process is represented by a sequence
of transitions between adjacent syntactic states S
i
.
1184
A transition from state S
i?1
to S
i
scans the cur-
rent word w
i
and stochastically selects a com-
plex lexical descriptor/category st
i
and an oper-
ator o
i
given the local context in the transition se-
quence. The syntactic state S
i
summarizes all the
syntactic information about fragments that have
already been processed and registers the syntac-
tic arguments which are to be expected next. Only
an impoverished deterministic procedure (called a
?State-Realizer?) is needed in order to compose a
state S
i
with the previous states S
0
. . . S
i?1
in or-
der to obtain a fully connected intermediate depen-
dency structure at every position in the input.
To implement the incremental parsing scheme
described above we use the parser described in
(Hassan et al, 2008b; Hassan et al, 2009), which
is based on Combinatory Categorial Grammar
(CCG) (Steedman, 2000). We only briefly de-
scribe this parser as its full description is beyond
the scope of this paper. The notions of a supertag
as a lexical category and the process of supertag-
ging are both crucial here (Bangalore and Joshi,
1999). Fortunately, CCG specifies the desired kind
of lexical categories (supertags) st
i
for every word
and a small set of combinatory operators o
i
that
combine the supertag st
i
with a previous parse
state S
i?1
into the next parse state S
i
. In terms
of CCG representations, the parse state is a CCG
composite category which specifies either a func-
tor and the arguments it expects to the right of the
current word, or is itself an argument for a functor
that will follow it to the right. At the first word in
the sentence, the parse state consists solely of the
supertag of that word.
Attacks rocked Riyadh
S
0
NP (S\NP)/NP NP
> NOP
S
1
: NP
> TRFC
S
2
: S/NP
> FA
S
3
: S
Figure 1: A sentence and possible supertag-,
operator- and state-sequences. NOP: No Oper-
ation; TRFC: Type Raise-Forward Composition;
FA: Forward Application. The CCG operators
used show that Attacks and Riyadh are both
dependents of rocked.
Figure 1 exhibits an example of the workings of
this parser. Practically speaking, after POS tag-
ging the input sentence, the parser employs two
components:
? A Supertag-Operator Tagger which proposes
a supertag?operator pair for the current word,
? A deterministic State-Realizer, which real-
izes the current state by applying the current
operator to the previous state and the current
supertag.
The Supertag-Operator Tagger is a probablistic
component while the State-Realizer is a determin-
istic component. The generative model underlying
this component concerns the probability P (W,S)
of a word sequence W = wn
1
and a parse-state
sequence S = Sn
1
, with associated supertag se-
quence ST = stn
1
and operator sequence O = on
1
,
which represents a possible derivation. Note that
given the choice of supertags st
i
and operator o
i
,
the state S
i
is calculated deterministically by the
State-Realizer.
A generative version of this model is described
in (3):
P (W,S) =
n
?
i=1
Word Predictor
? ?? ?
P (w
i
|W
i?1
S
i?1
)
.
Supertagger
? ?? ?
P (st
i
|W
i
) .
Operator Tagger
? ?? ?
P (o
i
|W
i
, S
i?1
, ST
i
) (3)
In (3):
? P (W,S) represents the product of the pro-
duction probabilities at each parse-state and
is similar to the structured language model
representation introduced in (Chelba, 2000).
? P (w
i
|W
i?1
S
i?1
) is the probability of w
i
given the previous sequence of words W
i?1
and the previous sequence of states S
i?1
,
? P (st
i
|W
i
): is the supertag st
i
probability
given the word sequence W
i
up to the cur-
rent position. Basically, this represents a se-
quence tagger (a ?supertagger?).
? P (o
i
|W
i
, S
i?1
, ST
i
) represents the probabil-
ity of the operator o
i
given the previous
words, supertags and state sequences up to
the current position. This represents a CCG
operator tagger.
The different local conditional components (for
every i) in (3) are estimated as discriminative
MaxEnt submodels trained on a corpus of incre-
mental CCG derivations. This corpus was ex-
tracted from the CCGbank (Hockenmaier, 2003)
1185
by transforming every normal form derivation into
strictly left-to-right CCG derivations, with the
CCG operators only slightly redesigned to allow
incrementality while still satisfying the dependen-
cies in the CCGbank (cf. (Hassan et al, 2008b;
Hassan et al, 2009)).
As mentioned before, the State-Realizer is a
deterministic function. Starting at the first word
with (obviously) a null previous state, the realizer
performs the following deterministic steps for
each word in turn: (i) set the current supertag
and operator to those of the current word; (ii) at
the current state, apply the current operator to the
previous state and current supertag; (iii) add edges
to the dependency graphs between words that are
linked as CCG arguments; and (iv) if not at the
end of the sentence, set the previous state to the
current one, then set the current word to the next
one, and iterate from (i).
It is worth noting that the proposed dependency
parser is deterministic in the sense that it maintains
only one parse state per word. This characteris-
tic is crucial for its incorporation into a large-scale
SMT system to avoid explosion of the translation
space during decoding.
5 Dependency-based DTM (DDTM)
In this section we extend the DTM2 model with
incremental target dependency-based syntax. We
call the resulting model the Dependency-based Di-
rect Translation Model (DDTM). This extension
takes place by (i) extracting syntactically enriched
minimal phrase pairs, (ii) including a new set of
syntactic feature functions among the exponen-
tial model features, and (iii) adapting the decoder
for dealing with syntax, including various pruning
strategies and enhancements. Next we describe
each extension in turn.
5.1 Phrase Table: Incremental Syntax
The target-side sentences in the word-aligned par-
allel corpus used for training are parsed using
the incremental dependency parser described in
section 4. This results in a word-aligned par-
allel corpus where the words of the target sen-
tences are tagged with supertags and operators.
From this corpus we extract the set of minimal
phrase pairs using the method described in (Itty-
cheriah and Roukos, 2007), extracting along with
every target phrase the associated sequences of su-
pertags and operators. As shown in (4), a source
phrase s
1
, . . . , s
n
translates into a target phrase
w
1
, . . . , w
m
where every word w
i
is labeled with
a supertag st
i
, and a possible parsing operator o
i
appearing with it in the parsed parallel corpus:
s
1
...s
n
//
[w
1
, st
1
, o
1
]...[w
m
, st
m
, o
m
] (4)
Hence, our phrase table associates with every
target phrase an incremental parsing subgraph.
These subgraphs along with their probabilities
represent our phrase table augmented with incre-
mental dependency parsing structure.
This representation turns the complicated prob-
lem of MT with incremental parsing into a sequen-
tial classification problem in which the classifier
deploys various features from the source sentence
and the candidate target translations to specify a
sequence of decisions that finally results in an out-
put target string along with its associated depen-
dency graph. The classification decisions are per-
formed in sequence step-by-step while traversing
the input string to provide decisions on possible
words, supertags, operators and states. A beam
search decoder simultaneously decides which se-
quence is the most probable.
5.2 DDTM Features
The exponential model and the MaxEnt frame-
work used in DTM2 and DDTM enabled us to ex-
plore the utility of incremental syntactic parsing
within a rich feature space. In our DDTM sys-
tem, we implemented a set of features alongside
the baseline DTM2 features that were discussed in
Section 3. The features described here encode all
the probabilistic components in (3) within a log
linear interpretation along with some more empir-
ically intuitive features.
? Supertag-Word features: these features ex-
amine the target phrase words with their as-
sociated supertags and is related to the Su-
pertagger component in (3).
? Supertag sequence features: these features
encode n-gram supertags (equivalent to the n-
gram supertags Language Model). This fea-
ture is related to the supertagger component
as well.
? Supertag-Operator features: these features
encode supertags and associated operators
which is related to the Operator Tagger com-
ponent in (3).
1186
? Supertag-State features: these features regis-
ter state and supertag co-occurrences.
? State sequence features: these features en-
code n-gram state features and are equiva-
lent to an n-gram Language Model over parse
state sequences which is related to the multi-
plication in (3).
? Word-State sequence features: these fea-
tures encode words and states co-occurrences
which is related to the Word Predictor com-
ponent in (3).
The exponential model and the MaxEnt frame-
work used in DTM2 and DDTM enable us to ex-
plore the utility of incremental syntactic parsing
with the use of minimal phrases within a rich fea-
ture space.
5.3 DDTM Decoder
In order to support incremental dependency pars-
ing, we extend the DTM2 decoder in three ways:
firstly, by constructing the syntactic states during
decoding; secondly, by extending the hypothesis
structures to incorporate the syntactic states and
the partial dependency derivations; and thirdly, by
modifying the pruning strategy to handle the large
search space.
At decoding time, each hypothesis state is as-
sociated with a parse-state which is constructed
while decoding using the incremental parsing ap-
proach introduced in ((Hassan et al, 2008b; Has-
san et al, 2009)). The previous state, the se-
quences of supertags and CCG incremental opera-
tors are deployed in a deterministic manner to re-
alize the parse-states as well as the intermediate
dependency graphs between words.
Figure 2 shows the DDTM decoder while de-
coding a sentence with the English translation ?At-
tacks rocked Riyadh?. Each hypothesis is asso-
ciated with a parse-state S
i
and a partial depen-
dency graph (shown for some states only). More-
over, each transition is associated with an opera-
tor o that combines the previous state and the cur-
rent supertag st to construct the next state S
i
. The
decoder starts from a null state S
1
and then pro-
ceeds with a possible expansion with the word ?at-
tacks?, supertag NP and operator NOP to pro-
duce the next hypothesis with state S
2
and cate-
gory NP . Further expansion for that path with the
verb ?rocked?, supertag ?(S\NP )/NP and oper-
ator TRFC will produce the state S
5
with cat-
egory S/NP . The partial dependency graph for
state S
5
is shown above the state where a depen-
dency relation between the two words is estab-
lished. Furthermore, another expansion with the
word ?Riyadh?, supertag NP and operator FA
produces state S
7
with category S and a completed
dependency graph as shown above the state. An-
other path which spans the states S
1
, S
3
, S
6
and
S
8
ends with a state category S/NP and a partial
dependency graph as shown under state S
8
where
the dependency graph is still missing its object
(e.g. ?Riyadh attacks rocked the Saudi Govt.?).
The addition of parse-states may result in a very
large search space due to the fact that the same
phrase/word may have many possible supertags
and many possible operators. Moreover, the same
word sequences may have many parse-state se-
quences and, therefore, many hypotheses that rep-
resent the same word sequence. The search space
is definitely larger than the baseline search space.
We adopt the following three pruning heuristics to
limit the search space.
5.3.1 Grammatical Pruning
Any hypothesis which does not constitute a valid
parse-state is discarded, i.e. if the previous parse-
state and the current supertag sequence cannot
construct a valid state using the associated oper-
ator sequence, then the expansion is discarded.
Therefore, this pruning strategy maintains only
fully connected graphs and discards any partially
connected graphs that might result during the de-
coding process.
As shown in Figure 2, the expansion from state
S
1
to state S
4
(with the dotted line) is pruned and
not expanded further because the proposed expan-
sion is the verb ?attacks?, supertag (S\NP )/NP
and operator TRFC . Since the previous state is
NULL, it cannot be combined with the verb using
the TRFC operator. This would produce an un-
defined state and thus the hypothesis is discarded.
5.3.2 Supertags and Operators Threshold
We limit the supertag and operator variants per tar-
get phrase to a predefined number of alternatives.
We tuned this on the MT03 DevSet for the best
accuracy while maintaining a manageable search
space. The supertags limit was set to four alterna-
tives while the operators limit was set to three.
As shown in Figure 2, each word can have many
alternatives with different supertags. In this exam-
ple the word ?attacks? has two forms, namely a
1187
e:
a : --------
P:1
S1:NULL
e: attacks
a: *----
P:=.162
ST=NP
S2=NP
e: attacks
a: *-------
P:=.092
ST=(S\NP)/NP
S4= UNDEF
O:TRFC
e: Riyadh
a: -*------
P:=.142
ST=NP/NP
S3=NP/NP
e: rocked
a: --*--
P:=.083
ST=(S\NP)/NP
S5=S/NP
O:NOP
O:NOP
O:TRFC
e: rocked
a: --*------
P:=.01
ST=(S\NP)/NP
S8=S/NP
attacks
attacks rocked
e: Riyadh
a: --*--
P:=.04
ST=NP
S7=S
O:FC
attacks rocked Riyadh
e: attacks
a: *-------
P:=.07
ST=NP
S6=NP
O:TRFC
Riyadh attacks rocked
O:FA
Figure 2: DDTM Decoder: each hypothesis has a parse state and a partial dependency structure.
noun and a verb, with different supertags and op-
erators. The proposed thresholds limit the possible
alternatives to a reasonable number.
5.3.3 Merging Hypotheses
Standard Phrase?based SMT decoders merge
translation hypotheses if they cover the same
source words and share the same n-gram lan-
guage model history. Similarly, DDTM decoder
merges translation hypotheses if they cover the
same source words, share the same n-gram lan-
guage model history and share the same parse-
state history. This helps in reducing the search
space by merging paths that will not constitute a
part of the best path.
6 Experiments
We conducted experiments on an Arabic-to-
English translation task using LDC parallel data
and GALE parallel data. We used the UN paral-
lel corpus and LDC news corpus together with the
GALE parallel corpus, totaling 7.8M parallel sen-
tences. The 5-gram Language Model was trained
on the English Gigaword Corpus and the English
part of the parallel corpus. Our baseline system is
similar to the system described in (Ittycheriah and
Roukos, 2007). We report results on NIST MT05
and NIST MT06 evaluations test sets using BLEU
and TER as automatic evaluation metrics.
To train the DDTM model, we use the incre-
mental parser introduced in (Hassan et al, 2008b;
Hassan et al, 2009) to parse the target side of the
parallel training data. Each sentence is associated
with supertag, operator and parse-state sequences.
We then train models with different feature sets.
Results: We compared the baseline DTM2 (It-
tycheriah and Roukos, 2007) with our DDTM sys-
tem with the features listed above. We examine
the effect of all features on system performance.
In this set of experiments we used LDC parallel
data only which is composed of 3.7M sentences
and the results are reported on MT05 test set. Each
of the examined systems deploys DTM2 features
in addition to a number of newly added syntactic
features. The systems examined are:
? DTM2: Direct Translation model 2 baseline.
? D-SW: DTM2 + Supertag-Word features.
? D-SLM: DTM2 + Supertag-Word and su-
pertag n-gram features.
? D-SO: DTM2+ Supertag-Operator features.
? D-SS : DTM2 + supertags and states features
with parse-state construction.
? D-WS : DTM2 + words and states features
with parse-state construction.
? D-STLM: DTM2 + state n-gram features
with parse-state construction.
? DDTM: fully fledged system with all fea-
tures that proved useful above which are:
Supertag-Word features, supertag n-gram
1188
features, supertags and states features and
state n-gram features .
System BLEU Score on MT05
DTM2-Baseline 52.24
D-SW 52.28
D-SLM 52.29
D-SO 52.01
D-SS 52.39
D-WS 52.03
D-STLM 52.53
DDTM 52.61
Table 1: DDTM Results with various features.
As shown in Table 1, the DTM baseline system
demonstrates a very high BLEU score, unsurpris-
ingly given its top-ranked performance in two re-
cent major MT evaluation campaigns. Among the
features we tried, supertags and n-gram supertags
systems (D-SW and D-SLM systems) give slight
yet statistically insignificant improvements. On
the other hand, the states n-gram sequence features
(D-SS and DDTM systems) give small yet statis-
tically significant improvements (as calculated via
bootstrap resampling (Koehn, 2004b)). The D-WS
system shows a small degradation in performance,
probably due to the fact that the states-words inter-
actions are quite sparse and could not be estimated
with good evidence. Similarly, the D-SO system
shows a small degradation in performance. When
we investigated the features types, we found out
that all features that deploy the operators had bad
effect on the model. We think this is due to the fact
that the operator set is a small set with high evi-
dence in many training instances such that it has
low discriminative power on it is own. However,
it implicitly helps in producing the state sequence
which proved useful.
System DTM2-Baseline DDTM
MT05 (BLEU) 55.28 55.66
MT05 (TER) 38.79 38.48
MT06 (BLEU) 43.56 43.91
MT06 (TER) 49.08 48.65
Table 2: DDTM Results on MT05 and MT06.
We examined a combination of the best fea-
tures in our DDTM system on a larger training
data comprising 7.8M sentences from both NIST
and GALE parallel corpora. Table 2 shows the
results on both MT05 and MT06 test sets. As
shown, DDTM significantly outperforms the state-
of-the-art baseline system. It is worth noting that
DDTM outperforms this baseline even when very
large amounts of training data are used. Despite
the fact that the actual scores are not so different,
we found that the baseline translation output and
the DDTM translation outout are significantly dif-
ferent. We measured this by calculating the TER
between the baseline translation and the DDTM
translation for the MT05 test set, and found this
to be 25.9%. This large difference has not been
realized by the BLEU or TER scores in compari-
son to the baseline. We believe that this is due to
the fact that most changes that match the syntac-
tic constraints do not bring about the best match
where the automatic evaluation metrics are con-
cerned. Accordingly, in the next section we de-
scribe the outcome of a detailed manual analysis
of the output translations.
7 Manual Analysis of Results
Although the BLEU score does not mark a large
improvement by the dependency-based system
over the baseline system, human inspection of the
data gives us important insights into the pros and
cons of the dependency-based model. We ana-
lyzed a randomly selected set of 100 sentences
from the MT05 test set. In this sample, the base-
line and the DDTM system perform similarly in
68% of the sentences. The outputs of both system
are similar though not identical. In these cases,
the systems may choose equivalent paraphrases.
However, the translations using syntactic struc-
tures are rather similar. It is worth noting that the
DDTM system tends to produce more concise sys-
ntactic structures which may lead to less BLUE
score due to penalizing the translation length al-
though the translation might be equivelent to the
baseline if not better.
In 28% of the sentences, the DDTM system pro-
duces remarkably better translations. The exam-
ples here illustrate the behaviour of the baseline
and the DDTM systems which can be observed
consistently throughout the test set. We only high-
light some of the examples for illustration pur-
poses. DDTM manages to insert verbs which are
deleted by any standard phrase-based SMT sys-
tem. DDTM prefers to deploy verbs since they
have complex and more detailed syntactic struc-
tures which give better and more likely state se-
1189
quences. Furthermore, the DDTM system avoids
longer noun phrases and instead uses some prepo-
sitions in-between. Again, this is probably due to
the fact that like verbs, prepositions have a com-
plex syntactic description that give rise to more
likely state sequences.
On the other hand, the baseline produced better
translation in 8% of the analysis sample. We ob-
served that the baseline is doing better mainly in
two cases. The first when the produced translation
is very poor and producing poor sysntatctic struc-
ture due to out of vocabularies or hard to trans-
late sentences. The second case is with sentences
with long noun phrases, in such cases the DDTM
system prefres to introduce verbs or prepositions
in the middle of long noun phrase and thus the
baseline would produce better translations. This
is maybe due to the fact that noun phrases have
relatively simple structure in CCG such that it did
not help in constructing long noun phrases.
Source: ??Q???  ZAJ
.
? Yg A

?Qk
.


HA??j
	
?? ??
	
X Y?K
.
?
	
?
	
k?
Reference: He then underwent medical examinations by a po-
lice doctor .
Baseline: He was subjected after that tests conducted by doc-
tors of the police .
DDTM: Then he underwent tests conducted by doctors of the
police .
Source: 	?



KPA J


?
.
	
?A??j
.
? ?J


?  ZA??
	
?AK


Q ?
	
Q? Y

??
	
?




J
	
j
	
j
	
??
Reference: Riyadh was rocked tonight by two car bomb at-
tacks..
Baseline: Riyadh rocked today night attacks by two booby -
trapped cars.
DDTM: Attacks rocked Riyadh today evening in two car
bombs.
Figure 3: DDTM provides better syntactic struc-
ture with more concise translations.
Figure 3 shows two examples where DDTM
provides better and more concise syntactic struc-
ture. As we can see, there is not much agree-
ment between the reference and the proposed
translation. However, longer translations enhance
the possibility of picking more common n-gram
matches via the BLEU score and so increases the
chance of better scores. This well-known bias
does not favour the more concise output derived
by our DDTM system, of course.
8 Conclusion and Future Work
In this paper, we presented a novel model of de-
pendency phrase-based SMT which integrates in-
cremental dependency parsing into the transla-
tion model while retaining the linear decoding as-
sumed in conventional Phrase?based SMT sys-
tems. To the best of our knowledge, this model
constitutes the first effective attempt at integrating
a linear-time dependency parser that builds a con-
nected tree incrementally into SMT systems with
linear-time decoding. Crucially, it turns out that
incremental dependency parsing based on lexical-
ized grammars such as CCG and LTAG can pro-
vide valuable incremental parsing information to
the decoder even if their output is imperfect. We
believe this robustness in the face of imperfect
parser output to be a property of the probabilistic
formulation and statistical estimation used in the
Direct Translation Model. A noteworthy aspect of
our proposed approach is that it integrates features
from the derivation process as well as the derived
tree. We think that this is possible due to the im-
portance of the notion of a derivation in linguistic
frameworks such as CCG and LTAG.
Future work will attempt further extensions of
our DDTM system to allow for the exploitation
of long-range aspects of the dependency struc-
ture. We will work on expanding the features
set of DDTM system to leverage features from
the constructed dependency structure itself. Fi-
nally, we will work on enabling the deployment
of source side dependency structures to influence
the construction of the target dependency structure
based on a bilingually enabled dependency pars-
ing mechanism using the discriminative modeling
capabilities.
Acknowledgments
We would like to thank Salim Roukos, IBM TJ
Watson Research Center, for fruitful, insightful
discussions and for his support during this work.
We also would like to thank Abe Ittycheriah, IBM
TJ Watson Research Center, for providing the
DTM2 baseline and for his support during de-
veloping this system. Finally, we would like to
thank the anonymous reviewers for their helpful
and constructive comments.
1190
References
Bangalore, S. and Joshi, A. (1999). ?Supertagging: An
Approach to Almost Parsing?, Computational Lin-
guistics 25(2):237?265, 1999.
Berger, A. and Della Pietra, S. and Della Pietra, V.J.
(1996). Maximum Entropy Approach to Natural
Language Processing Computational Linguistics,
22(1): 39?71, 1996.
Birch, A., Osborne, M. and Koehn, P. (2007). CCG Su-
pertags in Factored Statistical Machine Translation.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, ACL-07, pp.9?16, 2007.
Brown, P., Cocke,J., Della Pietra, S., Jelinek, F., Della
Pietra, V.J. Lafferty, R. Mercer and Roossin, P. ?A
Statistical Approach to Machine Translation? Com-
putational Linguistics 16(2):79?85, 1990.
Chelba, C. (2000). Exploiting Syntactic Structure for
Natural Language Modeling. PhD thesis, Johns
Hopkins University, Baltimore, MD.
Chiang, D. (2005). A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL05), pp.263?270, Ann Arbor,
MI.
Hassan, H., Sima?an, K., and Way, A.. (2009). Lex-
icalized Semi-Incremental Dependency Parsing. In
Proceedings of RANLP 2009, the International Con-
ference on Recent Advances in Natural Language
Processing, Borovets, Bulgaria (to appear).
Hassan, H., Sima?an, K., and Way, A. (2008a). Syntac-
tically Lexicalized Phrase-Based Statistical Transla-
tion. IEEE Transactions on Audio, Speech and Lan-
guage Processing, 6(7):1260?1273.
Hassan, H., Sima?an, K., and Way, A.. (2008b). A Syn-
tactic Language Model Based on Incremental CCG
Parsing. In Proceedings IEEE Workshop on Spoken
Language Technology (SLT) 2008, Goa, India.
Hassan, H., Sima?an, K., and Way, A. (2007). Inte-
grating Supertags into Phrase-based Statistical Ma-
chine Translation. In Proceedings of the ACL-2007,
Prague, Czech Republic, pp.288?295, 2007.
Hockenmaier, J. (2003). Data and Models for Statisti-
cal Parsing with Combinatory Categorial Grammar,
Ph.D Thesis, University of Edinburgh, UK, 2003.
Huang, L. and Chiang, D. (2007). Forest Rescoring:
Faster Decoding with Integrated Language Models.
In Proceedings of the ACL-2007, Prague, Czech Re-
public, 2007.
Ittycheriah, A. and Roukos, S. (2007). Direct trans-
lation model 2. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pp.57?64,
Rochester, NY.
Koehn, P. (2004a). Pharaoh: A Beam Search De-
coder for phrase-based Statistical Machine Transla-
tion Models. Machine Translation: From Real Users
to Research. In Proceedings of 6th Conference of the
Association for Machine Translation in the Ameri-
cas, AMTA 2004, pp.115?124, Washington, DC.
Koehn, P. (2004b). Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pp.388?395,
Edmonton, AB, Canada.
Koehn, P. Och, F.J. and Marcu, D. (2003). Statisti-
cal phrase-based translation. In Proceedings of the
Joint Human Language Technology Conference and
the Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL 2003), pp.127?133, Edmonton, AL,
Canada.
Marcu, D., Wang, W., Echihabi, A., and Knight, K.
(2006). SPMT: Statistical Machine Translation with
Syntactified Target Language Phrases. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2006),
pp.44?52, Sydney, Australia.
Papineni, K., Roukos, S., and Ward, T. (1997).
Feature-Based Language Understanding. In Pro-
ceedings of 5th European Conference on Speech
Communication and Technology EUROSPEECH
?97 , pp.1435?1438, Rhodes, Greece.
Papineni, K., Roukos, S., Ward, T. and Zhu, W-J.
(2002). BLEU: a Method for Automatic Evalua-
tion of Machine Translation. In 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?02), pp.311?318, Philadelphia, PA.
Shen, L., Xu, J., and Weischedel, R. (2008). A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Pro-
ceedings of ACL-08: HLT, pp.577?585, Columbus,
OH.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L. and
Makhoul, J. (2006) A Study of Translation Edit Rate
with Targeted Human Annotation. In AMTA 2006:
Proceedings of the 7th Conference of the Association
for Machine Translation in the Americas, pp.223?
231, Cambridege, MA.
Steedman, M. (2000). The Syntactic Process. MIT
Press, Cambridge, MA.
Tillmann, C. and Ney, H. (2003). Word reordering and
a dynamic programming beam search algorithm for
statistical machine translation. Computational Lin-
guistics, 29(1):97?133.
Zollmann, A. and Venugopal, A. (2006). Syntax aug-
mented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, HLT/NAACL, pp.138?141, New York,
NY.
1191
Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 288?295,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Supertagged Phrase-Based Statistical Machine Translation
Hany Hassan
School of Computing,
Dublin City University,
Dublin 9, Ireland
hhasan@computing.dcu.ie
Khalil Sima?an
Language and Computation,
University of Amsterdam,
Amsterdam, The Netherlands
simaan@science.uva.nl
Andy Way
School of Computing,
Dublin City University,
Dublin 9, Ireland
away@computing.dcu.ie
Abstract
Until quite recently, extending Phrase-based
Statistical Machine Translation (PBSMT)
with syntactic structure caused system per-
formance to deteriorate. In this work we
show that incorporating lexical syntactic de-
scriptions in the form of supertags can yield
significantly better PBSMT systems. We de-
scribe a novel PBSMT model that integrates
supertags into the target language model
and the target side of the translation model.
Two kinds of supertags are employed: those
from Lexicalized Tree-Adjoining Grammar
and Combinatory Categorial Grammar. De-
spite the differences between these two ap-
proaches, the supertaggers give similar im-
provements. In addition to supertagging, we
also explore the utility of a surface global
grammaticality measure based on combina-
tory operators. We perform various experi-
ments on the Arabic to English NIST 2005
test set addressing issues such as sparseness,
scalability and the utility of system subcom-
ponents. Our best result (0.4688 BLEU)
improves by 6.1% relative to a state-of-the-
art PBSMT model, which compares very
favourably with the leading systems on the
NIST 2005 task.
1 Introduction
Within the field of Machine Translation, by far the
most dominant paradigm is Phrase-based Statistical
Machine Translation (PBSMT) (Koehn et al, 2003;
Tillmann &Xia, 2003). However, unlike in rule- and
example-based MT, it has proven difficult to date to
incorporate linguistic, syntactic knowledge in order
to improve translation quality. Only quite recently
have (Chiang, 2005) and (Marcu et al, 2006) shown
that incorporating some form of syntactic structure
could show improvements over a baseline PBSMT
system. While (Chiang, 2005) avails of structure
which is not linguistically motivated, (Marcu et al,
2006) employ syntactic structure to enrich the en-
tries in the phrase table.
In this paper we explore a novel approach towards
extending a standard PBSMT system with syntactic
descriptions: we inject lexical descriptions into both
the target side of the phrase translation table and the
target language model. Crucially, the kind of lexical
descriptions that we employ are those that are com-
monly devised within lexicon-driven approaches to
linguistic syntax, e.g. Lexicalized Tree-Adjoining
Grammar (Joshi & Schabes, 1992; Bangalore &
Joshi, 1999) and Combinary Categorial Grammar
(Steedman, 2000). In these linguistic approaches, it
is assumed that the grammar consists of a very rich
lexicon and a tiny, impoverished1 set of combina-
tory operators that assemble lexical entries together
into parse-trees. The lexical entries consist of syn-
tactic constructs (?supertags?) that describe informa-
tion such as the POS tag of the word, its subcatego-
rization information and the hierarchy of phrase cat-
egories that the word projects upwards. In this work
we employ the lexical entries but exchange the al-
gebraic combinatory operators with the more robust
1These operators neither carry nor presuppose further lin-
guistic knowledge beyond what the lexicon contains.
288
and efficient supertagging approach: like standard
taggers, supertaggers employ probabilities based on
local context and can be implemented using finite
state technology, e.g. Hidden Markov Models (Ban-
galore & Joshi, 1999).
There are currently two supertagging approaches
available: LTAG-based (Bangalore & Joshi, 1999)
and CCG-based (Clark & Curran, 2004). Both the
LTAG (Chen et al, 2006) and the CCG supertag
sets (Hockenmaier, 2003) were acquired from the
WSJ section of the Penn-II Treebank using hand-
built extraction rules. Here we test both the LTAG
and CCG supertaggers. We interpolate (log-linearly)
the supertagged components (language model and
phrase table) with the components of a standard
PBSMT system. Our experiments on the Arabic?
English NIST 2005 test suite show that each of the
supertagged systems significantly improves over the
baseline PBSMT system. Interestingly, combining
the two taggers together diminishes the benefits of
supertagging seen with the individual LTAG and
CCG systems. In this paper we discuss these and
other empirical issues.
The remainder of the paper is organised as fol-
lows: in section 2 we discuss the related work on en-
riching PBSMT with syntactic structure. In section
3, we describe the baseline PBSMT system which
our work extends. In section 4, we detail our ap-
proach. Section 5 describes the experiments carried
out, together with the results obtained. Section 6
concludes, and provides avenues for further work.
2 Related Work
Until very recently, the experience with adding syn-
tax to PBSMT systems was negative. For example,
(Koehn et al, 2003) demonstrated that adding syn-
tax actually harmed the quality of their SMT system.
Among the first to demonstrate improvement when
adding recursive structure was (Chiang, 2005), who
allows for hierarchical phrase probabilities that han-
dle a range of reordering phenomena in the correct
fashion. Chiang?s derived grammar does not rely on
any linguistic annotations or assumptions, so that the
?syntax? induced is not linguistically motivated.
Coming right up to date, (Marcu et al, 2006)
demonstrate that ?syntactified? target language
phrases can improve translation quality for Chinese?
English. They employ a stochastic, top-down trans-
duction process that assigns a joint probability to
a source sentence and each of its alternative trans-
lations when rewriting the target parse-tree into a
source sentence. The rewriting/transduction process
is driven by ?xRS rules?, each consisting of a pair
of a source phrase and a (possibly only partially)
lexicalized syntactified target phrase. In order to
extract xRS rules, the word-to-word alignment in-
duced from the parallel training corpus is used to
guide heuristic tree ?cutting? criteria.
While the research of (Marcu et al, 2006) has
much in common with the approach proposed here
(such as the syntactified target phrases), there re-
main a number of significant differences. Firstly,
rather than induce millions of xRS rules from par-
allel data, we extract phrase pairs in the standard
way (Och & Ney, 2003) and associate with each
phrase-pair a set of target language syntactic struc-
tures based on supertag sequences. Relative to using
arbitrary parse-chunks, the power of supertags lies
in the fact that they are, syntactically speaking, rich
lexical descriptions. A supertag can be assigned to
every word in a phrase. On the one hand, the cor-
rect sequence of supertags could be assembled to-
gether, using only impoverished combinatory opera-
tors, into a small set of constituents/parses (?almost?
a parse). On the other hand, because supertags are
lexical entries, they facilitate robust syntactic pro-
cessing (using Markov models, for instance) which
does not necessarily aim at building a fully con-
nected graph.
A second major difference with xRS rules is that
our supertag-enriched target phrases need not be
generalized into (xRS or any other) rules that work
with abstract categories. Finally, like POS tagging,
supertagging is more efficient than actual parsing or
tree transduction.
3 Baseline Phrase-Based SMT System
We present the baseline PBSMT model which we
extend with supertags in the next section. Our
baseline PBSMT model uses GIZA++2 to obtain
word-level alignments in both language directions.
The bidirectional word alignment is used to obtain
phrase translation pairs using heuristics presented in
2http://www.fjoch.com/GIZA++.html
289
(Och & Ney, 2003) and (Koehn et al, 2003), and the
Moses decoder was used for phrase extraction and
decoding.3
Let t and s be the target and source language
sentences respectively. Any (target or source) sen-
tence x will consist of two parts: a bag of elements
(words/phrases etc.) and an order over that bag. In
other words, x = ??x, Ox?, where ?x stands for the
bag of phrases that constitute x, andOx for the order
of the phrases as given in x (Ox can be implemented
as a function from a bag of tokens ?x to a set with a
finite number of positions). Hence, we may separate
order from content:
argmax
t
P (t|s) = argmax
t
P (s | t)P (t) (1)
= arg max
??t,Ot?
TM
? ?? ?
P (?s | ?t)
distortion
? ?? ?
P (Os | Ot)
LM
? ?? ?
Pw(t) (2)
Here, Pw(t) is the target language model, P (Os|Ot)
represents the conditional (order) linear distortion
probability, and P (?s|?t) stands for a probabilis-
tic translation model from target language bags of
phrases to source language bags of phrases using a
phrase translation table. As commonly done in PB-
SMT, we interpolate these models log-linearly (us-
ing different ?weights) together with a word penalty
weight which allows for control over the length of
the target sentence t:
arg max
??t,Ot?
P (?s | ?t) P (Os | Ot)
?o
Pw(t)
?lm exp|t|?w
For convenience of notation, the interpolation factor
for the bag of phrases translation model is shown in
formula (3) at the phrase level (but that does not en-
tail any difference). For a bag of phrases ?t consist-
ing of phrases ti, and bag ?s consisting of phrases
si, the phrase translation model is given by:
P (?s | ?t) =
Y
si
ti
P (si|ti)
P (si| ti) = Pph(si|ti)
?t1Pw(si|ti)
?t2Pr(ti|si)
?t3 (3)
where Pph and Pr are the phrase-translation proba-
bility and its reverse probability, and Pw is the lexi-
cal translation probability.
3http://www.statmt.org/moses/
4 Our Approach: Supertagged PBSMT
We extend the baseline model with lexical linguis-
tic representations (supertags) both in the language
model as well as in the phrase translation model. Be-
fore we describe how our model extends the base-
line, we shortly review the supertagging approaches
in Lexicalized Tree-Adjoining Grammar and Com-
binatory Categorial Grammar.
4.1 Supertags: Lexical Syntax
NP
D
The
NP
NP
N
purchase
NP
NP
N
price
S
NP VP
V
includes
NP
NP
N
taxes
Figure 1: An LTAG supertag sequence for the sen-
tence The purchase price includes taxes. The sub-
categorization information is most clearly available
in the verb includes which takes a subject NP to its
left and an object NP to its right.
Modern linguistic theory proposes that a syntactic
parser has access to an extensive lexicon of word-
structure pairs and a small, impoverished set of oper-
ations to manipulate and combine the lexical entries
into parses. Examples of formal instantiations of this
idea include CCG and LTAG. The lexical entries are
syntactic constructs (graphs) that specify informa-
tion such as POS tag, subcategorization/dependency
information and other syntactic constraints at the
level of agreement features. One important way of
portraying such lexical descriptions is via the su-
pertags devised in the LTAG and CCG frameworks
(Bangalore & Joshi, 1999; Clark & Curran, 2004).
A supertag (see Figure 1) represents a complex,
linguistic word category that encodes a syntactic
structure expressing a specific local behaviour of a
word, in terms of the arguments it takes (e.g. sub-
ject, object) and the syntactic environment in which
it appears. In fact, in LTAG a supertag is an elemen-
tary tree and in CCG it is a CCG lexical category.
Both descriptions can be viewed as closely related
functional descriptions.
The term ?supertagging? (Bangalore & Joshi,
1999) refers to tagging the words of a sentence, each
290
with a supertag. When well-formed, an ordered se-
quence of supertags can be viewed as a compact
representation of a small set of constituents/parses
that can be obtained by assembling the supertags
together using the appropriate combinatory opera-
tors (such as substitution and adjunction in LTAG
or function application and combination in CCG).
Akin to POS tagging, the process of supertagging
an input utterance proceeds with statistics that are
based on the probability of a word-supertag pair
given their Markovian or local context (Bangalore
& Joshi, 1999; Clark & Curran, 2004). This is the
main difference with full parsing: supertagging the
input utterance need not result in a fully connected
graph.
The LTAG-based supertagger of (Bangalore &
Joshi, 1999) is a standard HMM tagger and consists
of a (second-order) Markov language model over su-
pertags and a lexical model conditioning the proba-
bility of every word on its own supertag (just like
standard HMM-based POS taggers).
The CCG supertagger (Clark & Curran, 2004) is
based on log-linear probabilities that condition a su-
pertag on features representing its context. The CCG
supertagger does not constitute a language model
nor are the Maximum Entropy estimates directly in-
terpretable as such. In our model we employ the
CCG supertagger to obtain the best sequences of su-
pertags for a corpus of sentences from which we ob-
tain language model statistics. Besides the differ-
ence in probabilities and statistical estimates, these
two supertaggers differ in the way the supertags are
extracted from the Penn Treebank, cf. (Hocken-
maier, 2003; Chen et al, 2006). Both supertaggers
achieve a supertagging accuracy of 90?92%.
Three aspects make supertags attractive in the
context of SMT. Firstly, supertags are rich syntac-
tic constructs that exist for individual words and so
they are easy to integrate into SMT models that can
be based on any level of granularity, be it word-
or phrase-based. Secondly, supertags specify the
local syntactic constraints for a word, which res-
onates well with sequential (finite state) statistical
(e.g. Markov) models. Finally, because supertags
are rich lexical descriptions that represent under-
specification in parsing, it is possible to have some
of the benefits of full parsing without imposing the
strict connectedness requirements that it demands.
4.2 A Supertag-Based SMT model
We employ the aforementioned supertaggers to en-
rich the English side of the parallel training cor-
pus with a single supertag sequence per sentence.
Then we extract phrase-pairs together with the co-
occuring English supertag sequence from this cor-
pus via the same phrase extraction method used in
the baseline model. This way we directly extend
the baseline model described in section 3 with su-
pertags both in the phrase translation table and in
the language model. Next we define the probabilistic
model that accompanies this syntactic enrichment of
the baseline model.
Let ST represent a supertag sequence of the same
length as a target sentence t. Equation (2) changes
as follows:
argmax
t
?
ST
P (s | t, ST )PST (t, ST ) ?
arg max
?t,ST ?
TM w.sup.tags
? ?? ?
P (?s | ?t,ST )
distortion
? ?? ?
P (Os | Ot)
?o
LM w.sup.tags
? ?? ?
PST (t, ST )
word?penalty
? ?? ?
exp|t|?w
The approximations made in this formula are of two
kinds: the standard split into components and the
search for the most likely joint probability of a tar-
get hypothesis and a supertag sequence cooccuring
with the source sentence (a kind of Viterbi approach
to avoid the complex optimization involving the sum
over supertag sequences). The distortion and word
penalty models are the same as those used in the
baseline PBSMT model.
Supertagged Language Model The ?language
model? PST (t, ST ) is a supertagger assigning prob-
abilities to sequences of word?supertag pairs. The
language model is further smoothed by log-linear
interpolation with the baseline language model over
word sequences.
Supertags in Phrase Tables The supertagged
phrase translation probability consists of a combina-
tion of supertagged components analogous to their
counterparts in the baseline model (equation (3)),
i.e. it consists of P (s | t, ST ), its reverse and
a word-level probability. We smooth this proba-
bility by log-linear interpolation with the factored
291
John bought quickly sharesNNP_NN VBD_(S[dcl]\NP)/NP RB|(S\NP)\(S\NP) NNS_N
2 Violations
Figure 2: Example CCG operator violations: V = 2
and L = 3, and so the penalty factor is 1/3.
backoff version P (s | t)P (s | ST ), where we im-
port the baseline phrase table probability and ex-
ploit the probability of a source phrase given the tar-
get supertag sequence. A model in which we omit
P (s | ST ) turns out to be slightly less optimal than
this one.
As in most state-of-the-art PBSMT systems, we
use GIZA++ to obtain word-level alignments in both
language directions. The bidirectional word align-
ment is used to obtain lexical phrase translation pairs
using heuristics presented in (Och & Ney, 2003) and
(Koehn et al, 2003). Given the collected phrase
pairs, we estimate the phrase translation probability
distribution by relative frequency as follows:
P?ph(s|t) =
count(s, t)
?
s count(s, t)
For each extracted lexical phrase pair, we extract the
corresponding supertagged phrase pairs from the su-
pertagged target sequence in the training corpus (cf.
section 5). For each lexical phrase pair, there is
at least one corresponding supertagged phrase pair.
The probability of the supertagged phrase pair is es-
timated by relative frequency as follows:
Pst(s|t, st) =
count(s, t, st)
?
s count(s, t, st)
4.3 LMs with a Grammaticality Factor
The supertags usually encode dependency informa-
tion that could be used to construct an ?almost parse?
with the help of the CCG/LTAG composition oper-
ators. The n-gram language model over supertags
applies a kind of statistical ?compositionality check?
but due to smoothing effects this could mask cru-
cial violations of the compositionality operators of
the grammar formalism (CCG in this case). It is
interesting to observe the effect of integrating into
the language model a penalty imposed when formal
compostion operators are violated. We combine the
n-gram language model with a penalty factor that
measures the number of encountered combinatory
operator violations in a sequence of supertags (cf.
Figure 2). For a supertag sequence of length (L)
which has (V ) operator violations (as measured by
the CCG system), the language model P will be ad-
justed as P? = P ? (1 ? VL ). This is of course no
longer a simple smoothed maximum-likelihood es-
timate nor is it a true probability. Nevertheless, this
mechanism provides a simple, efficient integration
of a global compositionality (grammaticality) mea-
sure into the n-gram language model over supertags.
Decoder The decoder used in this work is Moses,
a log-linear decoder similar to Pharaoh (Koehn,
2004), modified to accommodate supertag phrase
probabilities and supertag language models.
5 Experiments
In this section we present a number of experiments
that demonstrate the effect of lexical syntax on trans-
lation quality. We carried out experiments on the
NIST open domain news translation task from Ara-
bic into English. We performed a number of ex-
periments to examine the effect of supertagging ap-
proaches (CCG or LTAG) with varying data sizes.
Data and Settings The experiments were con-
ducted for Arabic to English translation and tested
on the NIST 2005 evaluation set. The systems were
trained on the LDC Arabic?English parallel corpus;
we use the news part (130K sentences, about 5 mil-
lion words) to train systems with what we call the
small data set, and the news and a large part of
the UN data (2 million sentences, about 50 million
words) for experiments with large data sets.
The n-gram target language model was built us-
ing 250M words from the English GigaWord Cor-
pus using the SRILM toolkit.4 Taking 10% of the
English GigaWord Corpus used for building our tar-
get language model, the supertag-based target lan-
guage models were built from 25M words that were
supertagged. For the LTAG supertags experiments,
we used the LTAG English supertagger5 (Bangalore
4http://www.speech.sri.com/projects/srilm/
5http://www.cis.upenn.edu/?xtag/gramrelease.html
292
& Joshi, 1999) to tag the English part of the parallel
data and the supertag language model data. For the
CCG supertag experiments, we used the CCG su-
pertagger of (Clark & Curran, 2004) and the Edin-
burgh CCG tools6 to tag the English part of the par-
allel corpus as well as the CCG supertag language
model data.
The NIST MT03 test set is used for development,
particularly for optimizing the interpolation weights
using Minimum Error Rate training (Och, 2003).
Baseline System The baseline system is a state-
of-the-art PBSMT system as described in sec-
tion 3. We built two baseline systems with two
different-sized training sets: ?Base-SMALL? (5 mil-
lion words) and ?Base-LARGE? (50 million words)
as described above. Both systems use a trigram lan-
guage model built using 250 million words from
the English GigaWord Corpus. Table 1 presents the
BLEU scores (Papineni et al, 2002) of both systems
on the NIST 2005 MT Evaluation test set.
System BLEU Score
Base-SMALL 0.4008
Base-LARGE 0.4418
Table 1: Baseline systems? BLEU scores
5.1 Baseline vs. Supertags on Small Data Sets
We compared the translation quality of the baseline
systems with the LTAG and CCG supertags systems
(LTAG-SMALL and CCG-SMALL). The results are
System BLEU Score
Base-SMALL 0.4008
LTAG-SMALL 0.4205
CCG-SMALL 0.4174
Table 2: LTAG and CCG systems on small data
given in Table 2. All systems were trained on the
same parallel data. The LTAG supertag-based sys-
tem outperforms the baseline by 1.97 BLEU points
absolute (or 4.9% relative), while the CCG supertag-
based system scores 1.66 BLEU points over the
6http://groups.inf.ed.ac.uk/ccg/software.html
baseline (4.1% relative). These significant improve-
ments indicate that the rich information in supertags
helps select better translation candidates.
POS Tags vs. Supertags A supertag is a complex
tag that localizes the dependency and the syntax in-
formation from the context, whereas a normal POS
tag just describes the general syntactic category of
the word without further constraints. In this experi-
ment we compared the effect of using supertags and
POS tags on translation quality. As can be seen
System BLEU Score
Base-SMALL 0.4008
POS-SMALL 0.4073
LTAG-SMALL .0.4205
Table 3: Comparing the effect of supertags and POS
tags
in Table 3, while the POS tags help (0.65 BLEU
points, or 1.7% relative increase over the baseline),
they clearly underperform compared to the supertag
model (by 3.2%).
The Usefulness of a Supertagged LM In these
experiments we study the effect of the two added
feature (cost) functions: supertagged translation and
language models. We compare the baseline system
to the supertags system with the supertag phrase-
table probability but without the supertag LM. Ta-
ble 4 lists the baseline system (Base-SMALL), the
LTAG system without supertagged language model
(LTAG-TM-ONLY) and the LTAG-SMALL sys-
tem with both supertagged translation and language
models. The results presented in Table 4 indi-
System BLEU Score
Base-SMALL 0.4008
LTAG-TM-ONLY 0.4146
LTAG-SMALL .0.4205
Table 4: The effect of supertagged components
cate that the improvement is a shared contribution
between the supertagged translation and language
models: adding the LTAG TM improves BLEU
score by 1.38 points (3.4% relative) over the base-
line, with the LTAG LM improving BLEU score by
293
a further 0.59 points (a further 1.4% increase).
5.2 Scalability: Larger Training Corpora
Outperforming a PBSMT system on small amounts
of training data is less impressive than doing so on
really large sets. The issue here is scalability as well
as whether the PBSMT system is able to bridge the
performance gap with the supertagged system when
reasonably large sizes of training data are used. To
this end, we trained the systems on 2 million sen-
tences of parallel data, deploying LTAG supertags
and CCG supertags. Table 5 presents the compari-
son between these systems and the baseline trained
on the same data. The LTAG system improves by
1.17 BLEU points (2.6% relative), but the CCG sys-
tem gives an even larger increase: 1.91 BLEU points
(4.3% relative). While this is slightly lower than
the 4.9% relative improvement with the smaller data
sets, the sustained increase is probably due to ob-
serving more data with different supertag contexts,
which enables the model to select better target lan-
guage phrases.
System BLEU Score
Base-LARGE 0.4418
LTAG-LARGE 0.4535
CCG-LARGE 0.4609
Table 5: The effect of more training data
Adding a grammaticality factor As described in
section 4.3, we integrate an impoverished grammat-
icality factor based on two standard CCG combi-
nation operations, namely Forward and Backward
Application. Table 6 compares the results of the
baseline, the CCG with an n-gram LM-only system
(CCG-LARGE) and CCG-LARGE with this ?gram-
maticalized? LM system (CCG-LARGE-GRAM).
We see that bringing the grammaticality tests to
bear onto the supertagged system gives a further im-
provement of 0.79 BLEU points, a 1.7% relative
increase, culminating in an overall increase of 2.7
BLEU points, or a 6.1% relative improvement over
the baseline system.
5.3 Discussion
A natural question to ask is whether LTAG and CCG
supertags are playing similar (overlapping, or con-
System BLEU Score
Base-LARGE 0.4418
CCG-LARGE 0.4609
CCG-LARGE-GRAM 0.4688
Table 6: Comparing the effect of CCG-GRAM
flicting) roles in practice. Using an oracle to choose
the best output of the two systems gives a BLEU
score of 0.441, indicating that the combination pro-
vides significant room for improvement (cf. Ta-
ble 2). However, our efforts to build a system that
benefits from the combination using a simple log-
linear combination of the two models did not give
any significant performance change relative to the
baseline CCG system. Obviously, more informed
ways of combining the two could result in better per-
formance than a simple log-linear interpolation of
the components.
Figure 3 shows some example system output.
While the baseline system omits the verb giving ?the
authorities that it had...?, both the LTAG and CCG
found a formulation ?authorities reported that? with
a closer meaning to the reference translation ?The
authorities said that?. Omitting verbs turns out to
be a problem for the baseline system when trans-
lating the notorious verbless Arabic sentences (see
Figure 4). The supertagged systems have a more
grammatically strict language model than a standard
word-level Markov model, thereby exhibiting a pref-
erence (in the CCG system especially) for the inser-
tion of a verb with a similar meaning to that con-
tained in the reference sentence.
6 Conclusions
SMT practitioners have on the whole found it dif-
ficult to integrate syntax into their systems. In this
work, we have presented a novel model of PBSMT
which integrates supertags into the target language
model and the target side of the translation model.
Using LTAG supertags gives the best improve-
ment over a state-of-the-art PBSMT system for a
smaller data set, while CCG supertags work best on
a large 2 million-sentence pair training set. Adding
grammaticality factors based on algebraic composi-
tional operators gives the best result, namely 0.4688
BLEU, or a 6.1% relative increase over the baseline.
294
Reference: The authorities said he was allowed to contact family members by phone from the armored vehicle he was in.
Baseline: the authorities that it had allowed him to communicate by phone with his family of the armored car where
LTAG: authorities reported that it had allowed him to contact by telephone with his family of armored car where
CCG: authorities reported that it had enabled him to communicate by phone his family members of the armored car where
Figure 3: Sample output from different systems
Source: wmn AlmErwf An Al$Eb AlSyny mHb llslAm . Ref: It is well known that the Chinese people are peace loving .
Baseline: It is known that the Chinese people a peace-loving .
LTAG: It is known that the Chinese people a peace loving . CCG: It is known that the Chinese people are peace loving .
Figure 4: Verbless Arabic sentence and sample output from different systems
This result compares favourably with the best sys-
tems on the NIST 2005 Arabic?English task. We
expect more work on system integration to improve
results still further, and anticipate that similar in-
creases are to be seen for other language pairs.
Acknowledgements
We would like to thank Srinivas Bangalore and
the anonymous reviewers for useful comments on
earlier versions of this paper. This work is par-
tially funded by Science Foundation Ireland Princi-
pal Investigator Award 05/IN/1732, and Netherlands
Organization for Scientific Research (NWO) VIDI
Award.
References
S. Bangalore and A. Joshi, ?Supertagging: An Ap-
proach to Almost Parsing?, Computational Linguistics
25(2):237?265, 1999.
J. Chen, S. Bangalore, and K. Vijay-Shanker, ?Au-
tomated extraction of tree-adjoining grammars
from treebanks?. Natural Language Engineering,
12(3):251?299, 2006.
D. Chiang, ?A Hierarchical Phrase-Based Model for Sta-
tistical Machine Translation?, in Proceedings of ACL
2005, Ann Arbor, MI., pp.263?270, 2005.
S. Clark and J. Curran, ?The Importance of Supertagging
for Wide-Coverage CCG Parsing?, in Proceedings of
COLING-04, Geneva, Switzerland, pp.282?288, 2004.
J. Hockenmaier, Data and Models for Statistical Parsing
with Combinatory Categorial Grammar, PhD thesis,
University of Edinburgh, UK, 2003.
A. Joshi and Y. Schabes, ?Tree Adjoining Grammars and
Lexicalized Grammars? in M. Nivat and A. Podelski
(eds.) Tree Automata and Languages, Amsterdam, The
Netherlands: North-Holland, pp.409?431, 1992.
P. Koehn, ?Pharaoh: A Beam Search Decoder for phrase-
based Statistical Machine TranslationModels?, in Pro-
ceedings of AMTA-04, Berlin/Heidelberg, Germany:
Springer Verlag, pp.115?124, 2004.
P. Koehn, F. Och, and D. Marcu, ?Statistical Phrase-
Based Translation?, in Proceedings of HLT-NAACL
2003, Edmonton, Canada, pp.127?133, 2003.
D. Marcu, W. Wang, A. Echihabi and K. Knight, ?SPMT:
Statistical Machine Translation with Syntactified Tar-
get Language Phrases?, in Proceedings of EMNLP,
Sydney, Australia, pp.44?52, 2006.
D. Marcu andW.Wong, ?A Phrase-Based, Joint Probabil-
ity Model for Statistical Machine Translation?, in Pro-
ceedings of EMNLP, Philadelphia, PA., pp.133?139,
2002.
F. Och, ?Minimum Error Rate Training in Statistical Ma-
chine Translation?, in Proceedings of ACL 2003, Sap-
poro, Japan, pp.160?167, 2003.
F. Och and H. Ney, ?A Systematic Comparison of Var-
ious Statistical Alignment Models?, Computational
Linguistics 29:19?51, 2003.
K. Papineni, S. Roukos, T. Ward and W-J. Zhu, ?BLEU:
A Method for Automatic Evaluation of Machine
Translation?, in Proceedings of ACL 2002, Philadel-
phia, PA., pp.311?318, 2002.
L. Rabiner, ?A Tutorial on Hidden Markov Models and
Selected Applications in Speech Recognition?, in A.
Waibel & F-K. Lee (eds.) Readings in Speech Recog-
nition, San Mateo, CA.: Morgan Kaufmann, pp.267?
296, 1990.
M. Steedman, The Syntactic Process. Cambridge, MA:
The MIT Press, 2000.
C. Tillmann and F. Xia, ?A Phrase-based Unigram Model
for Statistical Machine Translation?, in Proceedings of
HLT-NAACL 2003, Edmonton, Canada. pp.106?108,
2003.
295
BioGrapher: Biography Questions as a
Restricted Domain Question Answering Task
Oren Tsur
Text and Data Mining Group
Bar Ilan University
tsuror@cs.biu.ac.il
Maarten de Rijke
Informatics Institute
University of Amsterdam
mdr@science.uva.nl
Khalil Sima?an
Institute for Logic, Language
and Computation
University of Amsterdam
simaan@science.uva.nl
Abstract
We address Question Answering (QA) for biograph-
ical questions, i.e., questions asking for biographi-
cal facts about persons. The domain of biographical
documents differs from other restricted domains in
that the available collections of biographies are in-
herently incomplete: a major challenge is to answer
questions about persons for whom biographical in-
formation is not present in biography collections.
We present BioGrapher, a biographical QA system
that addresses this problem by machine learning al-
gorithms for biography classification. BioGrapher
first attempts to answer a question by searching in
a given collection of biographies, using techniques
tailored for the restricted nature of the domain. If
a biography is not found, BioGrapher attempts to
find an answer on the web: it retrieves documents
using a web search engine, filters these using the bi-
ography classifier, and then extracts answers from
documents classified as biographies. Our empirical
results show that biographical classification, prior to
answer extraction, improves the results.
1 Introduction
Although most current research in question answer-
ing (QA) is oriented towards open domains, as
witnessed by evaluation exercises such as TREC,
CLEF, and NTCIR, various significant applications
concern restricted domains, e.g., software manuals.
In restricted domains, a QA system faces questions
and documents that exhibit less variation in lan-
guage use (e.g., words and fixed phrases, more spe-
cific terminology) than in an open domain, and it
could access high-quality knowledge sources that
cover the entire domain. Open domain QA as it
is assessed at TREC, CLEF, and NTCIR concerns
a broad variety of fairly restricted question types,
such as location questions, monetary questions, bi-
ography questions, questions that ask for concept
definitions, etc. How useful or effective is it to
adopt a restricted domain approach to some of these
question types? In this paper we explore so-called
biographical questions, e.g., Who was Algar Hiss?
or Who is Sir John Hale?, i.e., questions that de-
mand answers consisting of biograpical key facts
that are typically found in biographies and that typ-
ically involve fixed phrases about, e.g., birthdates,
education, societal roles. This type of questions was
found to be quite frequent in search engine logs. We
believe that biographical questions can be usefully
viewed as defining a restricted domain for QA: the
domain of biographical information as represented
by biographies.
Ideally, biographical questions are answered by
retrieving a biography from some existing collec-
tion of biographies (such as biography.com)
and extracting snippets from it. Such resources,
however, have a limited coverage. There will al-
ways be people whose biographical information is
not contained in any of the existing collections.
This necessitates retrieval of ?biography-like? doc-
uments, i.e., documents with biographical informa-
tion. The problem of identifying biography-like
documents by machine learning algorithms turns
out to be a challenging but rewarding task as we will
see below.
In this paper we address the problem of question
answering within the biographical domain. We de-
scribe BioGrapher, a restricted domain QA system
for answering bibliographical questions in which a
baseline approach, that exploits biography collec-
tions, is extended with a trainable biography classi-
fier operating on the web in order to enhance cov-
erage. The baseline system helps us understand the
usefulness of existing high quality biography col-
lections within a QA system. The extension of our
baseline approach concerns the problem of identi-
fying biography-like documents, and the extraction
from such documents of answers for questions that
could not be answered using biography collections.
A main challenge lies in constructing an algorithm
for identifying documents containing usefull bio-
graphical information that may provide an answer
for a given question. To addresss this challenge, we
explore two machine learning algorithms: Ripper
(Cohen and Singer, 1996) and Support-Vector Ma-
chines (Joachims, 1998).
Section 2 provides some background, and in Sec-
tion 3 we briefly describe our baseline QA sys-
tem, based on external knowledge sources comple-
mented with a naive approach to retrieving biogra-
phy snippets using a web search engine. In Sec-
tion 4 we prepare the ground for our text classifica-
tion experiments. In Section 5 we present two clas-
sifiers: one loosely based on the Ripper algorithm
and the other based on an SVM classifier. In Sec-
tion 6 we compare the performanc of the baseline
against versions of the system integrated with the
two classifiers. Section 7 discusses the results and
considers the possibility of applying our approach
to other restricted domains. We conclude in Sec-
tion 8.
2 Related Work
Two kinds of related work are relevant to this pa-
per: question answering against external knowledge
sources, and genre detection (using classifiers). We
briefly discuss both.
Many QA research groups employed External
Knowledge Sources in order to improve perfor-
mance. For instance, (Chu-Carroll and Prager,
2002) used WordNet to answer what is questions,
using the isa hierarchy supported by WordNet.
(Hovy et al, 2002; Lin, 2002) used dictionaries
such as WordNet and web search results to re-rank
answers. (Yang et al, 2003) preformed structure
analysis of the knowledge obtained from WordNet
and the Web in order to further improve perfor-
mance.
We refer to (Sebastiani, 2002) for extensive re-
view about machine learning in automated text clas-
sification. (Lewis, 1992) were among the first to
use machine learning for genre detection trying to
categorize Reuters articles to predefined categories.
Probabilistic classifiers were used by many groups
(Lewis, 1998). Much current text classification re-
search is focused on Support Vector Machines, first
used for genre detection by (Joachims, 1998).
3 A Na??ve Baseline
In this section we describe our baseline QA system.
This system was used at TREC 2003, to produce an-
swers to so-called person definition questions (Jij-
koun et al, 2004; Voorhees, 2004). We present the
results and give a short analysis of the system?s per-
formance; as we will see, this provides further mo-
tivation for the use of text classification for identi-
fying biography-like documents.
Definition questions at TREC 2003
The QA track at TREC 2003 featured a subtask
devoted to definition questions. The latter came
in three flavors: person definitions (e.g., Who is
Colin Powell?), organization definitions (e.g., What
is the U.N.?), and concept definitions (e.g., What is
goth?). Here, we are only interested person defini-
tions.
In response to a definition question, systems had
to return an unordered set of snippets; each snippet
was supposed to be a facet in the definition of the
target. There were no limits placed on either the
length of an individual answer string or on the num-
ber of snippets in the list, although systems were
penalized for retrieving extraneous information.
As our primary strategy for handling person def-
inition questions, we consulted external resources.
The main resource used is biography.com.
While such resources contain biographies of many
historical and well-known people, they often lack
biographies of contemporary people that are not too
well-known. To be able to deal with such cases we
backed-off to using a web search engine (Google),
and applied a na??ve heuristic approach. We hand-
crafted a set of features (such as ?born?, ?gradu-
ated?, ?suffered?, etc.) that we felt would trigger
for biography-like snippets. Various subsets of the
large feature set, together with the target of the def-
inition question, were combined to form queries for
the web search engine.
Given a set of candidate answer snippets, we per-
formed two filtering steps before presenting the final
answer: we separated non-relevant snippets from
valuable snippets and we identified semantically-
close snippets. We addressed the first step by ana-
lyzing the distances between query terms submitted
to the search engine and the sets of features, and by
means of shallow syntactic aspects of the different
features such as sentence boundaries. To address
the second step we developed a snippet similarity
metric based on stemming, stopword removal and
keyword overlap by sorting and calculating the Lev-
enshtein distance measure of similarity.1. An ex-
ample of the snippets filtering can be found in Ta-
ble 1. The table presents 3 of the returned snippets
for the question Who is Sir John Hale?. The first
and third snippet are filtered out, the first one for
non-relevancy and the third for its semantic similar-
ity with the second, shorter, snippet.
1The Levenshtein measure is a measure of the similarity be-
tween two strings, which are refered to as the source string s
and the target string t. The distance is the number of deletions,
insertions, or substitutions required to transform s into t
1 Sir Malcolm Bradbury (writer/teacher) Dead.
Heart trouble. . . . Heywood Hale Broun (com-
mentator, writer ) ? Dead. John Brunner (au-
thor) Dead. Stroke. . . . Description: Debunks
the rumor of his death. . .
2 . . . Professor Sir John Hale woke up, had . . . For
her birthday in 1996, he wrote on the . . . John
Hale died in his sleep - possibly following an-
other stroke . . .
3 Observer On 29 July 1992, Professor Sir John
Hale woke up . . . her birthday in 1996, he wrote
on the . . . John Hale died in his sleep - possibly
following another stroke.
Table 1: Snippets filtering for Who is Sir John Hale?
Evaluation
Evaluation of individual person definition questions
was done using the F-measure: F = (?2 + 1)P ?
R)/(?2P + R), where P is precision (to be de-
fined shortly), R is recall (to be defined shortly),
and ? was set to 5, indicating that precision was
more important than recall. Length was used as a
crude approximation to precision; it gives a system
an allowance of 100 (non-white-space) characters
for each correct snippet it retrieved. The precision
score is set to one if the response is no longer than
this allowance, otherwise it is downgraded using the
function P = 1? ((length ? allowance)/length).
As to recall, for each question, the TREC asses-
sors marked some snippets as vital and the remain-
der as non-vital. The non-vital snippets act as ?don?t
care? condition. That is, systems should be penal-
ized for not retrieving vital nuggets, and for retriev-
ing snippets that are not in the assessors? snippet
lists at all, but should be neither penalized nor re-
warded for returning a non-vital snippet. To im-
plement the ?don?t care? condition, snippet recall is
computed only over vital snippets (Voorhees, 2004).
In total, 30 person definition questions were eval-
uated at the TREC 2003 QA track. The overall F
score of a run was obtained by averaging over all
the individual questions.
Results and Analysis
The F score obtained by the naive system described
in this section, on the TREC 2003 person definition
questions, was 0.392. An analysis of the results
shows that, for questions that could be answered
from external biography resources, the baseline sys-
tem obtains an F score of 0.586.
In post-submission experiments we changed the
subsets of features we use in the queries sent to
Google as well as the number of queries/subsets we
use. The snippet similarity threshold was also tuned
in order to filter out more snippets. This resulted in
fewer unanswered questions, while the average an-
swer length was decreased as well, by close to 50%.
All in all, an informal evaluation showed increase in
recall, precision and in the overall F score.
From our experience with our baseline system
we learned the following important lesson: having
a (relatively) small number of high quality biogra-
phy sources as a basis for each question?s answer
is far better than using a broad and large variety of
snippets returned by a web search engine. While
extending available biography resources so as to se-
riously boost their coverage is not a feasible option,
we want to do the next best thing: make sure we
identify good biography-like documents online, so
that we can use these to mine snippets from; to this
end we will use text classification.
4 Preparing for Text Classification
In the previous section we suggested that using a
text classifier might improve the performance of bi-
ography QA. Using text classifiers, we aim to iden-
tify biography-like documents from which we can
extract answers. In this section we detail the docu-
ment representations on which we will operate.
Document and Text Representation
Text classifiers represent a document as a set of fea-
tures d = {f1, f2,. . . , fn} where n is the number of
active features, that is, features that occur in the doc-
ument. A feature f can be a word, a set of words, a
stem or any phrasal structure, depending on its text
representation. Each feature has a weight, usually
representing the number of occurrences of this fea-
ture in the document.
What is a suitable abstract representation of doc-
uments for our biography domain? We have defined
7 clusters, groups of words (terms/tokens) with a
high degree of pair-wise semantic relatedness. Each
cluster has a meta-tag symbol (as can be seen in Ta-
ble 2) and all occurrences of members of a cluster
were substituted by the cluster?s meta-tag. An ex-
ample of a document abstraction can be found in Ta-
ble 3. This abstraction captures typical similarities
between biographical strings; e.g., for the two sen-
tences John Kennedy was born in 1917 and William
Shakespeare was born in 1564 we get the same ab-
straction <NAME> <NAME> was born in <YEAR>.
It is worth noting that some of the clusters,
such as <CAP> and <PLACE>, <CAP> and <PN>
and others may overlap. Looking at the exam-
ple in Table 3, we see that Abbey was born in
Chicago, Illinois, but the automatic abstractor mis-
interpreted the token ?Ill.,? marking it is <CAP> for
capitalized (possibly meaningful) word, but not as
<NAME> the name of the subject of the biography
<YEAR> four digits surrounded by white space,
probably a year
<D> sequence of number of digits other than
four digits, can be part of a date, age etc.
<CAP> a capitalized word in the middle of a
sentence that wasn?t substituted by any
other tag
<PN> a proper name that is not the subject of
the biography It substitutes any name
out of a list of thousand names
<PLACE> denotes a name of a place, city or coun-
try out of a list of more than thousand
places
<MONTH> denotes one of the twelve months
<NOM> denotes a nominative
Table 2: Seven meta-tags used for document ab-
straction
<PLACE>. A similar thing happens with the name
?Wooldridge? that is not very common; it should
have been <PN> instead of <CAP>.
All procedures described below are preformed on
abstract-meta-tagged documents.
5 Identifying Biography Documents
Given a document, the task of a biography classifier
is to decide whether a given document is a biogra-
phy or not. In this section we address the problem
of acquiring biography classifiers by training ma-
chine learning algorithms on data. We present two
biography classification algorithms: a naive classi-
fier based on Ripper (Cohen and Singer, 1996), and
another based on SVM (Joachims, 1998). The two
methods differ radically both in the way they rep-
resent the training data (i.e., document representa-
tion), and in their learning approaches. The naive
classifier is obtained by a repetitive rule learning al-
Original Lincoln, Abbey (b. Anna Marie
Wooldridge) 1930 ? Jazz singer, com-
poser/arranger, movie actress; born in
Chicago, Ill. While a teenager she
sang at school and church functions
and then toured locally with a dance
band.
Abstraction <NAME>, <NAME> ( b . <PN> <CAP>
<CAP> ) <YEAR> - <CAP> singer ,
composer/arranger , movie actress ;
born in <PLACE> <CAP> . While
a teenager <NOM> sang at school and
church functions and then toured lo-
cally with a dance band .
Table 3: Abstraction of jazz singer Abbey Wool-
ridge?s biography
gorithm. We modified this algorithm to specifically
fit the task of identifying biographies. The SVM
learns ?linear decision boundaries? between the dif-
ferent classes. We employ here the implementation
of SVMs by (Joachims, 1998). Next we discuss the
details of how each algorithm was used for learning
a biography classifier.
Naive Classifier
We employ this algorithm for its simplicity and scal-
ability. This algorithm learns user-friendly rules,
i.e., human-readable conjunctions of propositions,
which can be converted to queries for a Boolean
search engine. Furthermore, it is known to exhibit
relatively good results across a wide variety of class-
fication problems, including tasks that involve large
collections of noisy data, similar to the large doc-
ument collections that we face in definitional QA.
The naive classifier consists of two main stages:
(1) Rules building. This is similar to Ripper?s first
stage of building an initial rule set. Our algorithm
deviates from standard implementations of Ripper
in that the terms that serve as the literals in the
rules are n-grams of various lengths. We feel that
n-grams, as opposed to individual literals (as in (Co-
hen and Singer, 1996), better capture contextual ef-
fects, which could be crucial in text classification.
Our learner learns the rules as follows. The set of
k-most frequent n-grams representing the training
documents is split into two frequency-ordered lists:
TLP (term-list-positive) containing the positive ex-
ample set and TLN (term-list-negative) containing
the negative examples set. The vector ~w is initial-
ized to be TLP/(TLP ? TLN), i.e., the most fre-
quent n-grams extracted from the positive set that
are not top frequent in the negative set.
(2) Rule optimization. Instead of Ripper?s rule
pruning stage, our algorithm assigns a weight to
each rule/n-gram r in the rules vector according
to the formula g(n)?f(r)C , where g(n) is an increas-
ing function in the length of the n-gram (longer n-
grams receive higher weights), f(r) is the ratio of
the frequency of r in the positive examples to its
frequency in the negative examples, and C is the
size of the training set. The normalization by C
is merely for the purpose of tracking variations of
the weights in different sizes of training sets. The
preference for longer n-grams can be justified by
the intuition that longer n-grams are more informa-
tive as they stand for stricter contexts. For example,
the string ?(<NAME> , <NAME> born in <YEAR>?
seems more informative than the shorter string in
<YEAR>).
Training material. The corpus we used as our
training set is a collection of 350 biographies. Most
of the biographies were randomly sampled from
biography.com, while the rest were collected
from the web. About 130 documents from the New
York Times (NYT) 2000 collection were randomly
selected as negative example set. The volumes of
the positive and negative sets are equal.
Various considerations played a role in building
this corpus. The biographies from biography.
com are ?clean? in the sense that all of them were
written as biographies. To enable the learning of
features of informal biographies, some other ?noisy?
biographies such as biography-like newspaper re-
views were added. Furthermore, a small number of
different biographies of the same person were man-
ually added in order to enforce variation in style.
We also added a small number of biographies from
other different sources to avoid any bias towards the
biography.com domain.
Validation and tuning. We tuned the naive algo-
rithm on a separate validation set of documents. The
validation set was collected in the same way as the
training set. It contained 60 biographies, of which
40 were randomly sampled from biography.
com, 10 ?clean? biographies were collected from
various online sources, 10 other documents were
noisy biographies such as newspaper reviews. In
addition, another 40 non-biographical documents
were randomly retrieved from the web.
The vector ~w is now used to rank the documents
of the validation set V in order to set a threshold
? that minimizes the false-positive and the false-
negative errors. Each document dj ? V in the val-
idation set is represented by a vector ~x, where xi
counts the occurrences of wi in dj . The score of the
document is the normalized inner product of ~x and
~w given by the function score(dj) = ~x?~wlength(dj) .
In the validation stage some heuristic modifica-
tions were applied by the algorithm. For example,
when the person name tag is absent, the document
gets the score of zero even though other parameters
of the vector may be present. We also normalized
document scores by document length.
Support Vector Machines (SVMs)
Now we describe the learning of a biography clas-
sifier using SVMs. Unlike many other classifiers,
SVMs are capable of learning classification even of
non-linearly-separable classes. It is assumed that
classes that are non-linearly separable in one di-
mension may be linearly separable in higher di-
mension. SVMs offer two important advantages
for text classification (Joachims, 1998; Sebastiani,
2002): (1) Term selection is often not needed, as
SVMs tend to be fairly robust to overfitting and
can scale up to considerable dimensionalities, and
(2) No human and machine effort in parameter tun-
ing on a validation set is needed, as there is a the-
oretically motivated ?default? choice of parameter
settings which have been shown to provide best re-
sults.
The key idea behind SVMs is to boost the dimen-
sion of the representation vectors and then to find
the best line or hyper-plane from the widest set of
parallel hyper-planes. This hyper-plane maximizes
the distance between two elements in the set. The
elements in the set are the support vectors. Theo-
retically, the classifier is determined by a very small
number of examples defining the category frontier,
the support vectors. Practically, finding the support
vectors is not a trivial task.
Training SVMs. The implementation used is
SVM-light v.5 (Joachims, 1999). The classifier was
run with its default setting, with linear kernel func-
tion and no kernel optimization tricks. The SVM-
light was trained on the very same (meta-tagged)
training corpus the naive classifier was trained on.
Since SVM is supposed to be robust and to fit big
and noisy collections, no feature selection method
was applied. The special feature underlying SVMs
is the high dimensional representation of a docu-
ment, allowing categorization by a hyper-plane of
high dimension; therefore each document was rep-
resented by the vector of its stems. The dimen-
sion was boosted to include all the stems from the
positive set. The boosted vector dimension was
7935, the number of different stems in the collec-
tion. The number of support vectors discovered was
17, which turned out to be too small for this task.
Testing this model on the test set (the same test set
used to test the naive classifier from previous sec-
tion) yielded very poor results. It seemed that the
classification was totally random. Testing the classi-
fier on smaller subsets of the training set (200, 300,
400 documents) exposed signs of convergence, sug-
gesting the training set is too sparse for the SVM.
To overcome sparse data, more documents were
needed. The size of the training set was more than
doubled. A total of 9968 documents was used as
the training set. Just like the original training set,
most of the biographies were randomly extracted
from biography.com, while a few dozen bi-
ographies were manually extracted from various
online sources to correct for a possible bias in
biography.com. Training SVM-light on the
new training set yielded 232 support vectors, which
seems enough to perform this classification tasks.
6 Experimental Results
In order to test the effectiveness of the biography
classifiers in improving question answering, we in-
tegrated each one of them with the naive baseline
biographical QA system and tested the integrated
system, called BioGrapher (Figure 1). Before dis-
cussing the results of this experiment, we briefly
mention how the two classifiers performed on the
pure classification task. For this purpose, we cre-
ated a test set including 47 documents that were re-
trieved from the web. The evaluation measure was
the accuracy of the classifiers in recognizing biogra-
phies. The Ripper-based algorithm achieved 89%
success, outranking the SVM which achieved 83%.
A discussion of this difference is beyond the scope
of this paper (see (Tsur, 2003) for details).
We tested BioGrapher on 11 out of the 30 bio-
graphical questions in the TREC 2003 QA track.
Those 11 questions were chosen as a test set be-
cause the baseline system (Section 3) scored poorly
on them, suggesting that our baseline heuristics are
incapable of effectively dealing with this type of
questions.
Question
Question 
analyzer
Snippets filter
Answer 
snippets
Biography 
collection
Web
Retrieval 
engine
Document 
classifier
Figure 1: BioGrapher system overview
Two experiments were carried out, one for the
Ripper-based classifier and another for the SVM-
based one. For each definitional question Biogra-
pher submits two simple queries to a web search
engine (e.g., Sir John Hale and Sir John Hale biog-
raphy). It retrieves the top 20 documents returned
by the search engine, thus obtaining, for each ques-
tion, 40 documents amongst which it should find
a biography. BioGrapher then classifies the doc-
uments into biographies and non-biographic doc-
uments. The distribution of documents that were
classified as biographies can be found in Table 4.
To simplify the experiments, and especially the
Question Naive Classifier SVM
Who is Alberto Tomba? 2 4
Who is Albert Ghiorso? 8 2
Who is Alexander Pope? 13 11
Who is Alice Rivlin? 3 2
Who is Absalom? 2 3
Who is Nostradamus? 1 1
Who is Machiavelli? 3 6
Who is Andrea Bocceli? 1 1
Who is Al Sharpton? 2 6
Who is Aga Khan? 4 1
Who is Ben Hur? 2 1
Table 4: Distribution of documents retrieved (in-
cluding false-positive)
error analysis, we set up BioGrapher to return an-
swer snippets from a single biography or biography-
like document only. Recall, the test questions
were such that there were no biographies for the
question targets in the biography collection we
used (biography.com): the biographies used
were ones that BioGrapher identified on the web.
We evaluated BioGrapher in the following manner.
The assessor first determines whether the document
from which BioGrapher extracts answer snippets is
a proper biography or not. In case the document is
not a pure biography the F-score given to this ques-
tion is zero. Otherwise, the F-score was determined
in the manner described in Section 3.2
BioGrapher with the Ripper-based Classifier
The total number of documents that were classi-
fied as biographies is 41 (out of 440 retrieved docu-
ments). However, analysis of the results reveals that
the false positive ratio is high; only 20 of the 41 cho-
sen documents were proper biography-like pages,
the other ?biographies? were very noisy.
For 4 out of the 11 test questions, a proper
biography was returned as the top ranking docu-
ment. While all 4 questions scored 0 at the origi-
nal TREC evaluation, now their average F-score is
0.732, improving the average F-score over all biog-
raphy questions by 9.6% to 0.4659.
BioGrapher with the SVM Classifier
The total number of documents that were classi-
fied as biographies is 38 (out of 440 retrieved docu-
2Obviously, the F-score for snippets extracted from doc-
uments incorrectly classified as biographies could be higher
than zero because these documents could still contain valuable
pieces of biographical information that would contribute to the
answer?s F-score. However, we decided to compute precision
and recall only for snippets extracted from documents correctly
classified as biographies as we think of the biography classi-
fier as a means to identify (?on-the-fly?) quality documents that
could in principle be added to a biography collection.
ments). However, just like in the case of the Ripper-
based classifier, an analysis of the results reveals
that the false positive ratio is high; only 18 of the
38 chosen documents were biography-like.
The SVM classifier managed to return proper bi-
ographies (as top ranking documents) for 5 out of
11 questions. The average F-score for those ques-
tions is 0.674 instead of the original zero, improving
the average F-score over all biography questions by
9.7% to 0.4665.
No biographies at all were retrieved for 4 of the
11 definition targets in the test set, the same four
definition targets for which the Ripper-based classi-
fier did not find biographies. A closer look reveals
the same problems as with the Ripper-based classi-
fier: a relatively high false-positive error ratio and
weak ranking of the classified biographies.
7 Discussion
The results of the experiments using both classifiers
are quite similar. The system integrated with the
SVM-based classifier achieved a slightly higher F-
score but it still falls within the standard deviation.
Our experiment serves as a proof of concept for the
hypothesis that using text classification methods im-
proves the baseline QA system in a principled way.
In spite of the major improvement in the system?s
performance, we have found two main problems
with the classifiers. First, although the classifiers
managed to identify biography-like documents, they
have a high false-positive ratio and too many er-
rors in filtering out some of the non-pure-biography
documents. This happens when the documents re-
trieved by the web search engine simply cannot be
regarded as clean biographies by human assessors,
although they do contain many biographic details.
Second, most of the definition targets had biogra-
phies retrieved and even classified as biographies,
but the biographies were ranked below other noisy
biographical documents, therefore the best biogra-
phy was not presented as a source from which to
extract answer snippets. There are various obvi-
ous paths to improve over the current system: (1)
Improve the classifiers by better training and other
classification algorithms; (2) Enable the extraction
of answers from ?noisy? biography-like documents
in such a way that the gain in recall is not reversed
by a loss of precision; and (3) Allow for the extrac-
tion of answer snippets from multiple biography-
like documents, while avoiding to return overlap-
ping snippets.
8 Conclusion
In this paper we have addressed the problem of bi-
ographical question answering. The main challenge
in this restricted domain is the fact that the available
collections of biography documents are (unavoid-
ably) too small to admit answering all biographi-
cal questions. We use the web as a backoff source
for finding biography-like documents from which to
extract answers in case a given biography collec-
tion does not contain information about the ques-
tion target. We demonstrated the benefits of inte-
grating a text classifier into a restricted domain QA
system as a filter to web retrieval. Finally, we be-
lieve that the use of text classifiers can be benefi-
cial for definitional QA, especially for identifying
documents from which the final answer should be
extracted. Future work will address the weakness
of the current implementation: improved biography
classification and improved answer extraction from
biography-like documents.
Acknowledgments
Maarten de Rijke was supported by the Nether-
lands Organization for Scientific Research (NWO)
under project numbers 612-13-001, 365-20-
005, 612.069.006, 612.000.106, 220-80-001,
612.000.207, and 612.066.302.
References
J. Chu-Carroll and J. Prager. 2002. Use of Word-
Net hypernyms for answering what-is questions.
In Proceedings of the Tenth Text REtrieval Con-
ference (TREC 2001). NIST Special Publication
500-250.
W. Cohen and Y. Singer. 1996. Context sensitive
learning methods. In Proceedings of the 19th
ACM International Conference on Research and
Development in Information Retrieval (SIGIR-
96), pages 307?315. ACM Press.
E. Hovy, U. Hermjakob, and C.Y. Lin. 2002. The
use of external knowledge in factoid QA. In Pro-
ceedings of the Tenth Text REtrieval Conference
(TREC 2001). NIST Special Publication 500-
250.
V. Jijkoun, G. Mishne, C. Monz, M. de Rijke,
S. Schlobach, and O. Tsur. 2004. The Univer-
sity of Amsterdam at the TREC 2003 Question
Answering Track. In E.M. Voorhees, editor, Pro-
ceedings TREC 2003. NIST Special Publication
SP 500-255.
T. Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many rele-
vant features. In Proceedings of ECML-98, 10th
European Conference on Machine Learning.
T. Joachims. 1999. Svm-light v.5, making large-
scale svm learning practical. advances in kernel
methods - support vector learning. B. Scholkopf
and C. Burges and A. Smola (ed.) MIT-Press.
D.D. Lewis. 1992. Representation and learning
in information retrieval. Ph.D. thesis, Graduate
School of the University of Maassachusetts.
D.D. Lewis. 1998. Naive (bayes) at forty: The in-
dependence assumption in information retrieval.
In Proceedings of the 10th European Conference
on Machine Learning, pages 137?142. Springer-
Verlag.
C.Y. Lin. 2002. The effectiveness of dictionary and
web based answer reranking. In The 19th Inter-
national Conference on Computational Linguis-
tics (COLING 2002).
F. Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Computing Sur-
veys, 34(1):1?47.
O. Tsur. 2003. Definitional question answering
using trainable text classifiers. Master?s thesis,
ILLC, University of Amsterdam.
E.M. Voorhees. 2004. Overview of the TREC 2003
question answering track. In Text REtrieval Con-
ference (TREC 2004). NIST Special Publication:
SP 500-255.
H. Yang, T.-S. Chua, S. Wang, and C.-K. Koh.
2003. Structured use of external knowledge for
event-based open domain question answering.
In Proceedings of the 26th annual international
ACM SIGIR conference on Research and de-
velopment in informaion retrieval, pages 33?40.
ACM Press.
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 39?46,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Choosing an Optimal Architecture for Segmentation and POS-Tagging of
Modern Hebrew
Roy Bar-Haim
Dept. of Computer Science
Bar-Ilan University
Ramat-Gan 52900, Israel
barhair@cs.biu.ac.il
Khalil Sima?an
ILLC
Universiteit van Amsterdam
Amsterdam, The Netherlands
simaan@science.uva.nl
Yoad Winter
Dept. of Computer Science
Technion
Haifa 32000, Israel
winter@cs.technion.ac.il
Abstract
A major architectural decision in de-
signing a disambiguation model for seg-
mentation and Part-of-Speech (POS) tag-
ging in Semitic languages concerns the
choice of the input-output terminal sym-
bols over which the probability distribu-
tions are defined. In this paper we de-
velop a segmenter and a tagger for He-
brew based on Hidden Markov Models
(HMMs). We start out from a morpholog-
ical analyzer and a very small morpholog-
ically annotated corpus. We show that a
model whose terminal symbols are word
segments (=morphemes), is advantageous
over a word-level model for the task of
POS tagging. However, for segmentation
alone, the morpheme-level model has no
significant advantage over the word-level
model. Error analysis shows that both
models are not adequate for resolving a
common type of segmentation ambiguity
in Hebrew ? whether or not a word in a
written text is prefixed by a definiteness
marker. Hence, we propose a morpheme-
level model where the definiteness mor-
pheme is treated as a possible feature of
morpheme terminals. This model exhibits
the best overall performance, both in POS
tagging and in segmentation. Despite the
small size of the annotated corpus avail-
able for Hebrew, the results achieved us-
ing our best model are on par with recent
results on Modern Standard Arabic.
1 Introduction
Texts in Semitic languages like Modern Hebrew
(henceforth Hebrew) and Modern Standard Ara-
bic (henceforth Arabic), are based on writing sys-
tems that allow the concatenation of different lexi-
cal units, called morphemes. Morphemes may be-
long to various Part-of-Speech (POS) classes, and
their concatenation forms textual units delimited by
white space, which are commonly referred to as
words. Hence, the task of POS tagging for Semitic
languages consists of a segmentation subtask and
a classification subtask. Crucially, words can be
segmented into different alternative morpheme se-
quences, where in each segmentation morphemes
may be ambiguous in terms of their POS tag. This
results in a high level of overall ambiguity, aggra-
vated by the lack of vocalization in modern Semitic
texts.
One crucial problem concerning POS tagging of
Semitic languages is how to adapt existing methods
in the best way, and which architectural choices have
to be made in light of the limited availability of an-
notated corpora (especially for Hebrew). This paper
outlines some alternative architectures for POS tag-
ging of Hebrew text, and studies them empirically.
This leads to some general conclusions about the op-
timal architecture for disambiguating Hebrew, and
(reasonably) other Semitic languages as well. The
choice of tokenization level has major consequences
for the implementation using HMMs, the sparseness
of the statistics, the balance of the Markov condi-
39
tioning, and the possible loss of information. The
paper reports on extensive experiments for compar-
ing different architectures and studying the effects
of this choice on the overall result. Our best result
is on par with the best reported POS tagging results
for Arabic, despite the much smaller size of our an-
notated corpus.
The paper is structured as follows. Section 2 de-
fines the task of POS tagging in Hebrew, describes
the existing corpora and discusses existing related
work. Section 3 concentrates on defining the dif-
ferent levels of tokenization, specifies the details of
the probabilistic framework that the tagger employs,
and describes the techniques used for smoothing the
probability estimates. Section 4 compares the differ-
ent levels of tokenization empirically, discusses their
limitations, and proposes an improved model, which
outperforms both of the initial models. Finally, sec-
tion 5 discusses the conclusions of our study for seg-
mentation and POS tagging of Hebrew in particular,
and Semitic languages in general.
2 Task definition, corpora and related
work
Words in Hebrew texts, similar to words in Ara-
bic and other Semitic languages, consist of a stem
and optional prefixes and suffixes. Prefixes include
conjunctions, prepositions, complementizers and the
definiteness marker (in a strict well-defined order).
Suffixes include inflectional suffixes (denoting gen-
der, number, person and tense), pronominal comple-
ments with verbs and prepositions, and possessive
pronouns with nouns.
By the term word segmentation we henceforth re-
fer to identifying the prefixes, the stem and suffixes
of the word. By POS tag disambiguation we mean
the assignment of a proper POS tag to each of these
morphemes.
In defining the task of segmentation and POS tag-
ging, we ignore part of the information that is usu-
ally found in Hebrew morphological analyses. The
internal morphological structure of stems is not an-
alyzed, and the POS tag assigned to stems includes
no information about their root, template/pattern, in-
flectional features and suffixes. Only pronominal
complement suffixes on verbs and prepositions are
identified as separate morphemes. The construct
state/absolute,1 and the existence of a possessive
suffix are identified using the POS tag assigned to
the stem, and not as a separate segment or feature.
Some of these conventions are illustrated by the seg-
mentation and POS tagging of the word wfnpgfnw
(?and that we met?, pronounced ve-she-nifgashnu):2
w/CC: conjunction
f /COM: complementizer
npgfnw/VB: verb
Our segmentation and POS tagging conform with
the annotation scheme used in the Hebrew Treebank
(Sima?an et al, 2001), described next.
2.1 Available corpora
The Hebrew Treebank (Sima?an et al, 2001) con-
sists of syntactically annotated sentences taken from
articles from the Ha?aretz daily newspaper. We ex-
tracted from the treebank a mapping from each word
to its analysis as a sequence of POS tagged mor-
phemes. The treebank version used in the current
work contains 57 articles, which amount to 1,892
sentences, 35,848 words, and 48,332 morphemes.
In addition to the manually tagged corpus, we have
access to an untagged corpus containing 337,651
words, also originating from Ha?aretz newspaper.
The tag set, containing 28 categories, was ob-
tained from the full morphological tagging by re-
moving the gender, number, person and tense fea-
tures. This tag set was used for training the POS
tagger. In the evaluation of the results, however, we
perform a further grouping of some POS tags, lead-
ing to a reduced POS tag set of 21 categories. The
tag set and the grouping scheme are shown below:
{NN}, {NN-H}, {NNT}, {NNP}, {PRP,AGR}, {JJ}, {JJT},
{RB,MOD}, {RBR}, {VB,AUX}, {VB-M}, {IN,COM,REL},
{CC}, {QW}, {HAM}, {WDT,DT}, {CD,CDT}, {AT}, {H},
{POS}, {ZVL}.
2.2 Related work on Hebrew and Arabic
Due to the lack of substantial tagged corpora, most
previous corpus-based work on Hebrew focus on the
1The Semitic construct state is a special form of a word
that participates in compounds. For instance, in the Hebrew
compound bdiqt hjenh (?check of the claim?), the word bdiqt
(?check of?/?test of?) is the construct form of the absolute form
bdiqh (?check?/?test?).
2In this paper we use Latin transliteration for Hebrew letters
following (Sima?an et al, 2001).
40
development of techniques for learning probabilities
from large unannotated corpora. The candidate anal-
yses for each word were usually obtained from a
morphological analyzer.
Levinger et al (1995) propose a method for
choosing a most probable analysis for Hebrew
words using an unannotated corpus, where each
analysis consists of the lemma and a set of morpho-
logical features. They estimate the relative frequen-
cies of the possible analyses for a given word w by
defining a set of ?similar words? SW (A) for each
possible analysis A of w. Each word w? in SW (A)
corresponds to an analysis A? which differs from A
in exactly one feature. Since each set is expected to
contain different words, it is possible to approximate
the frequency of the different analyses using the av-
erage frequency of the words in each set, estimated
from the untagged corpus.
Carmel and Maarek (1999) follow Levinger et
al. in estimating context independent probabilities
from an untagged corpus. Their algorithm learns fre-
quencies of morphological patterns (combinations
of morphological features) from the unambiguous
words in the corpus.
Several works aimed at improving the ?similar
words? method by considering the context of the
word. Levinger (1992) adds a short context filter that
enforces grammatical constraints and rules out im-
possible analyses. Segal?s (2000) system includes,
in addition to a somewhat different implementation
of ?similar words?, two additional components: cor-
rection rules a` la Brill (1995), and a rudimentary de-
terministic syntactic parser.
Using HMMs for POS tagging and segmenting
Hebrew was previously discussed in (Adler, 2001).
The HMM in Adler?s work is trained on an untagged
corpus, using the Baum-Welch algorithm (Baum,
1972). Adler suggests various methods for perform-
ing both tagging and segmentation, most notable are
(a) The usage of word-level tags, which uniquely de-
termine the segmentation and the tag of each mor-
pheme, and (b) The usage of a two-dimensional
Markov model with morpheme-level tags. Only the
first method (word-level tags) was tested, resulting
in an accuracy of 82%. In the present paper, both
word-level tagging and morpheme-level tagging are
evaluated.
Moving on to Arabic, Lee et al (2003) describe a
word segmentation system for Arabic that uses an n-
gram language model over morphemes. They start
with a seed segmenter, based on a language model
and a stem vocabulary derived from a manually seg-
mented corpus. The seed segmenter is improved it-
eratively by applying a bootstrapping scheme to a
large unsegmented corpus. Their system achieves
accuracy of 97.1% (per word).
Diab et al (2004) use Support Vector Machines
(SVMs) for the tasks of word segmentation and POS
tagging (and also Base Phrase Chunking). For seg-
mentation, they report precision of 99.09% and re-
call of 99.15%, when measuring morphemes that
were correctly identified. For tagging, Diab et al
report accuracy of 95.49%, with a tag set of 24 POS
tags. Tagging was applied to segmented words, us-
ing the ?gold? segmentation from the annotated cor-
pus (Mona Diab, p.c.).
3 Architectures for POS tagging Semitic
languages
Our segmentation and POS tagging system consists
of a morphological analyzer that assigns a set of
possible candidate analyses to each word, and a dis-
ambiguator that selects from this set a single pre-
ferred analysis per word. Each candidate analysis
consists of a segmentation of the word into mor-
phemes, and a POS tag assignment to these mor-
phemes. In this section we concentrate on the ar-
chitectural decisions in devising an optimal disam-
biguator, given a morphological analyzer for He-
brew (or another Semitic language).
3.1 Defining the input/output
An initial crucial decision in building a disambigua-
tor for a Semitic text concerns the ?tokenization? of
the input sentence: what constitutes a terminal (i.e.,
input) symbol. Unlike English POS tagging, where
the terminals are usually assumed to be words (de-
limited by white spaces), in Semitic texts there are
two reasonable options for fixing the kind of termi-
nal symbols, which directly define the correspond-
ing kind of nonterminal (i.e., output) symbols:
Words (W): The terminals are words as they ap-
pear in the text. In this case a nonterminal a
that is assigned to a word w consists of a se-
quence of POS tags, each assigned to a mor-
41
pheme of w, delimited with a special segmenta-
tion symbol. We henceforth refer to such com-
plex nonterminals as analyses. For instance,
the analysis IN-H-NN for the Hebrew word
bbit uniquely encodes the segmentation b-h-bit.
In Hebrew, this unique encoding of the segmen-
tation by the sequence of POS tags in the anal-
ysis is a general property: given a word w and
a complex nonterminal a = [t1 . . . tp] for w, it
is possible to extend a back to a full analysis
a? = [(m1, t1) . . . (mp, tp)], which includes the
morphemes m1 . . .mp that make out w. This is
done by finding a match for a in Analyses(w),
the set of possible analyses of w. Except for
very rare cases, this match is unique.
Morphemes (M): In this case the nonterminals are
the usual POS tags, and the segmentation is
given by the input morpheme sequence. Note
that information about how morphemes are
joined into words is lost in this case.
Having described the main input-output options for
the disambiguator, we move on to describing the
probabilistic framework that underlies their work-
ings.
3.2 The probabilistic framework
Let wk1 be the input sentence, a sequence of words
w1 . . . wk. If tokenization is per word, then the
disambiguator aims at finding the nonterminal se-
quence ak1 that has the highest joint probability with
the given sentence wk1 :
argmax
ak1
P (wk1 ,a
k
1) (1)
This setting is the standard formulation of proba-
bilistic tagging for languages like English.
If tokenization is per morpheme, the disambigua-
tor aims at finding a combination of a segmentation
mn1 and a tagging tn1 for mn1 , such that their joint
probability with the given sentence, wk1 , is maxi-
mized:
argmax
(mn1 ,t
n
1 )?ANALY SES(w
k
1 )
P (wk1 ,m
n
1 , t
n
1 ), (2)
where ANALY SES(wk1) is the set of possible
analyses for the input sentence wk1 (output by the
morphological analyzer). Note that n can be dif-
ferent from k, and may vary for different segmen-
tations. The original sentence can be uniquely re-
covered from the segmentation and the tagging.
Since all the ?mn1 , tn1 ? pairs that are the input for
the disambiguator were derived from wk1 , we have
P (wk1 |m
n
1 , t
n
1 ) = 1, and thus P (wk1 ,mn1 , tn1 ) =
P (tn1 ,m
n
1 ). Therefore, Formula (2) can be simpli-
fied as:
argmax
(mn1 ,t
n
1 )?ANALY SES(w
k
1 )
P (mn1 , t
n
1 ) (3)
Formulas (1) and (3) can be represented in a unified
formula that applies to both word tokenization and
morpheme tokenization:
argmax
(en1 ,A
n
1 )?ANALY SES(w
k
1 )
P (en1 , A
n
1 ) (4)
In Formula (4) en1 represents either a sequence of
words or a sequence of morphemes, depending on
the level of tokenization, and An1 are the respective
nonterminals ? either POS tags or word-level anal-
yses. Thus, the disambiguator aims at finding the
most probable ?terminal sequence, nonterminal
sequence? for the given sentence, where in the
case of word-tokenization there is only one possible
terminal sequence for the sentence.
3.3 HMM probabilistic model
The actual probabilistic model used in this work for
estimating P (en1 , An1 ) is based on Hidden Markov
Models (HMMs). HMMs underly many successful
POS taggers , e.g. (Church, 1988; Charniak et al,
1993).
For a k-th order Markov model (k = 1 or k = 2),
we rewrite (4) as:
argmax
en1 ,A
n
1
P (en1 , A
n
1 ) ?
argmax
en1 ,A
n
1
n?
i=1
P (Ai | Ai?k, . . . , Ai?1)P (ei | Ai)
(5)
For reasons of data sparseness, actual models we use
work with k = 2 for the morpheme level tokeniza-
tion, and with k = 1 for the word level tokenization.
42
For these models, two kinds of probabilities need
to be estimated: P (ei | Ai) (lexical model) and
P (Ai |Ai?k, . . . , Ai?1) (language model). Because
the only manually POS tagged corpus that was avail-
able to us for training the HMM was relatively small
(less than 4% of the Wall Street Journal (WSJ) por-
tion of the Penn treebank), it is inevitable that major
effort must be dedicated to alleviating the sparseness
problems that arise. For smoothing the nonterminal
language model probabilities we employ the stan-
dard backoff smoothing method of Katz (1987).
Naturally, the relative frequency estimates of
the lexical model suffer from more severe data-
sparseness than the estimates for the language
model. On average, 31.3% of the test words do
not appear in the training corpus. Our smooth-
ing method for the lexical probabilities is described
next.
3.4 Bootstrapping a better lexical model
For the sake of exposition, we assume word-level
tokenization for the rest of this subsection. The
method used for the morpheme-level tagger is very
similar.
The smoothing of the lexical probability of a word
w given an analysis a, i.e., P (w | a) = P (w,a)P (a) ,
is accomplished by smoothing the joint probability
P (w,a) only, i.e., we do not smooth P (a).3 To
smooth P (w,a), we use a linear interpolation of
the relative frequency estimates from the annotated
training corpus (denoted rf tr(w,a)) together with
estimates obtained by unsupervised estimation from
a large unannotated corpus (denoted emauto(w,a)):
P (w,a) = ? rf tr(w,a)+(1??) emauto(w,a)
(6)
where ? is an interpolation factor, experimentally set
to 0.85.
Our unsupervised estimation method can be
viewed as a single iteration of the Baum-Welch
(Forward-Backward) estimation algorithm (Baum,
1972) with minor differences. We apply this method
to the untagged corpus of 340K words. Our method
starts out from a naively smoothed relative fre-
3the smoothed probabilities are normalized so that?
w P (w,a) = P (a)
quency lexical model in our POS tagger:
PLM0(w|a) =
{
(1 ? p0) rf tr(w,a) ftr(w) > 0
p0 otherwise
(7)
Where ftr(w) is the occurrence frequency of w in
the training corpus, and p0 is a constant set experi-
mentally to 10?10. We denote the tagger that em-
ploys a smoothed language model and the lexical
model PLM0 by the probability distribution Pbasic
(over analyses, i.e., morpheme-tag sequences).
In the unsupervised algorithm, the model Pbasic
is used to induce a distribution of alternative analy-
ses (morpheme-tag sequences) for each of the sen-
tences in the untagged corpus; we limit the num-
ber of alternative analyses per sentence to 300. This
way we transform the untagged corpus into a ?cor-
pus? containing weighted analyses (i.e., morpheme-
tag sequences). This corpus is then used to calcu-
late the updated lexical model probabilities using
maximum-likelihood estimation. Adding the test
sentences to the untagged corpus ensures non-zero
probabilities for the test words.
3.5 Implementation4
The set of candidate analyses was obtained from Se-
gal?s morphological analyzer (Segal, 2000). The
analyzer?s dictionary contains 17,544 base forms
that can be inflected. After this dictionary was ex-
tended with the tagged training corpus, it recog-
nizes 96.14% of the words in the test set.5 For each
train/test split of the corpus, we only use the training
data for enhancing the dictionary. We used SRILM
(Stolcke, 2002) for constructing language models,
and for disambiguation.
4 Evaluation
In this section we report on an empirical comparison
between the two levels of tokenization presented in
the previous section. Analysis of the results leads to
an improved morpheme-level model, which outper-
forms both of the initial models.
Each architectural configuration was evaluated in
5-fold cross-validated experiments. In a train/test
4http://www.cs.technion.ac.il/?barhaim/MorphTagger/
5Unrecognized words are assumed to be proper nouns, and
the morphological analyzer proposes possible segmentations for
the word, based on the recognition of possible prefixes.
43
split of the corpus, the training set includes 1,598
sentences on average, which on average amount to
28,738 words and 39,282 morphemes. The test set
includes 250 sentences. We estimate segmentation
accuracy ? the percentage of words correctly seg-
mented into morphemes, as well as tagging accu-
racy ? the percentage of words that were correctly
segmented for which each morpheme was assigned
the correct POS tag.
For each parameter, the average over the five folds
is reported, with the standard deviation in parenthe-
ses. We used two-tailed paired t-test for testing the
significance of the difference between the average
results of different systems. The significance level
(p-value) is reported.
The first two lines in Table 1 detail the results ob-
tained for both word (W) and morpheme (M) lev-
els of tokenization. The tagging accuracy of the
Accuracy per word (%)
Tokenization Tagging Segmentation
W 89.42 (0.9) 96.43 (0.3)
M 90.21 (1.2) 96.25 (0.5)
M+h 90.51 (1.0) 96.74 (0.5)
Table 1: Level of tokenization - experimental results
morpheme tagger is considerably better than what
is achieved by the word tagger (difference of 0.79%
with significance level p = 0.01). This is in spite of
the fact that the segmentation achieved by the word
tagger is a little better (and a segmentation error im-
plies incorrect tagging). Our hypothesis is that:
Morpheme-level taggers outperform
word-level taggers in their tagging ac-
curacy, since they suffer less from data
sparseness. However, they lack some
word-level knowledge that is required for
segmentation.
This hypothesis is supported by the number of
once-occurring terminals in each level: 8,582 in the
word level, versus 5,129 in the morpheme level.
Motivated by this hypothesis, we next consider
what kind of word-level information is required for
the morpheme-level tagger in order to do better in
segmentation. One natural enhancement for the
morpheme-level model involves adding information
about word boundaries to the tag set. In the en-
hanced tag set, nonterminal symbols include addi-
tional features that indicate whether the tagged mor-
pheme starts/ends a word. Unfortunately, we found
that adding word boundary information in this way
did not improve segmentation accuracy.
However, error analysis revealed a very common
type of segmentation errors, which was found to be
considerably more frequent in morpheme tagging
than in word tagging. This kind of errors involves
a missing or an extra covert definiteness marker ?h?.
For example, the word bbit can be segmented either
as b-bit (?in a house?) or as b-h-bit (?in the house?),
pronounced bebayit and babayit, respectively. Un-
like other cases of segmentation ambiguity, which
often just manifest lexical facts about spelling of He-
brew stems, this kind of ambiguity is productive: it
occurs whenever the stem?s POS allows definiteness,
and is preceded by one of the prepositions b/k/l. In
morpheme tagging, this type of error was found on
average in 1.71% of the words (46% of the segmen-
tation errors). In word tagging, it was found only
in 1.36% of the words (38% of the segmentation er-
rors).
Since in Hebrew there should be agreement be-
tween the definiteness status of a noun and its related
adjective, this kind of ambiguity can sometimes be
resolved syntactically. For instance:
?bbit hgdwl? implies b-h-bit (?in the big house?)
?bbit gdwl? implies b-bit (?in a big house?)
By contrast, in many other cases both analyses
are syntactically valid, and the choice between them
requires consideration of a wider context, or some
world knowledge. For example, in the sentence
hlknw lmsibh (?we went to a/the party?), lmsibh
can be analyzed either as l-msibh (indefinite,?to a
party?) or as l-h-mbsibh (definite,?to the party?).
Whether we prefer ?the party? or ?a party? depends
on contextual information that is not available for
the POS tagger.
Lexical statistics can provide valuable informa-
tion in such situations, since some nouns are more
common in their definite form, while other nouns are
more common as indefinite. For example, consider
the word lmmflh (?to a/the government?), which can
be segmented either as l-mmflh or l-h-mmflh. The
44
Tokenization Analysis
W (lmmflh IN-H-NN)
M (IN l) (H h) (NN mmflh)
M+h (IN l) (H-NN hmmflh)
Table 2: Representation of l-h-mmflh in each level
of tokenization
stem mmflh (?government?) was found 25 times in
the corpus, out of which only two occurrences were
indefinite. This strong lexical evidence in favor of
l-h-mmflh is completely missed by the morpheme-
level tagger, in which morphemes are assumed to
be independent. The lexical model of the word-
level tagger better models this difference, since it
does take into account the frequencies of l-mmflh
and l-h-mmlh, in measuring P(lmmflh|IN-NN) and
P(lmmflh|IN-H-NN). However, since the word tag-
ger considers lmmflh, hmmflh (?the government?),
and mmflh (?a government?) as independent words,
it still exploits only part of the potential lexical evi-
dence about definiteness.
In order to better model such situations, we
changed the morpheme-level model as follows. In
definite words the definiteness article h is treated
as a manifestation of a morphological feature of the
stem. Hence the definiteness marker?s POS tag (H)
is prefixed to the POS tag of the stem. We refer by
M+h to the resulting model that uses this assump-
tion, which is rather standard in theoretical linguistic
studies of Hebrew. The M+h model can be viewed as
an intermediate level of tokenization, between mor-
pheme and word tokenization. The different analy-
ses obtained by the three models of tokenization are
demonstrated in Table 2.
As shown in Table 1, the M+h model shows
remarkable improvement in segmentation (0.49%,
p < 0.001) compared with the initial morpheme-
level model (M). As expected, the frequency of seg-
mentation errors that involve covert definiteness (h)
dropped from 1.71% to 1.25%. The adjusted mor-
pheme tagger also outperforms the word level tagger
in segmentation (0.31%, p = 0.069). Tagging was
improved as well (0.3%, p = 0.068). According to
these results, tokenization as in the M+h model is
preferable to both plain-morpheme and plain-word
tokenization.
5 Conclusion
Developing a word segmenter and POS tagger for
Hebrew with less than 30K annotated words for
training is a challenging task, especially given the
morphological complexity and high degree of am-
biguity in Hebrew. For comparison, in English a
baseline model that selects the most frequent POS
tag achieves accuracy of around the 90% (Charniak
et al, 1993). However, in Hebrew we found that a
parallel baseline model achieves only 84% using the
available corpus.
The architecture proposed in this paper addresses
the severe sparseness problems that arise in a num-
ber of ways. First, the M+h model, which was
found to perform best, is based on morpheme-
level tokenization, which suffers of data sparse-
ness less than word tokenization, and makes use of
multi-morpheme nonterminals only in specific cases
where it was found to be valuable. The number of
nonterminal types found in the corpus for this model
is 49 (including 11 types of punctuation marks),
which is much closer to the morpheme-level model
(39 types) than to the word-level model (205 types).
Second, the bootstrapping method we present ex-
ploits additional resources such as a morphological
analyzer and an untagged corpus, to improve lexi-
cal probabilities, which suffer from data sparseness
the most. The improved lexical model contributes
1.5% to the tagging accuracy, and 0.6% to the seg-
mentation accuracy (compared with using the basic
lexical model), making it a crucial component of our
system.
Among the few other tools available for POS tag-
ging and morphological disambiguation in Hebrew,
the only one that is freely available for extensive
training and evaluation as performed in this paper
is Segal?s ((Segal, 2000), see section 2.2). Com-
paring our best architecture to the Segal tagger?s re-
sults under the same experimental setting shows an
improvement of 1.5% in segmentation accuracy and
4.5% in tagging accuracy over Segal?s results.
Moving on to Arabic, in a setting comparable to
(Diab et al, 2004), in which the correct segmenta-
tion is given, our tagger achieves accuracy per mor-
pheme of 94.9%. This result is close to the re-
45
sult reported by Diab et al, although our result was
achieved using a much smaller annotated corpus.
We therefore believe that future work may benefit
from applying our model, or variations thereof, to
Arabic and other Semitic languages.
One of the main sources for tagging errors in our
model is the coverage of the morphological analyzer.
The analyzer misses the correct analysis of 3.78% of
the test words. Hence, the upper bound for the accu-
racy of the disambiguator is 96.22%. Increasing the
coverage while maintaining the quality of the pro-
posed analyses (avoiding over-generation as much
as possible), is crucial for improving the tagging re-
sults.
It should also be mentioned that a new version of
the Hebrew treebank, now containing approximately
5,000 sentences, was released after the current work
was completed. We believe that the additional an-
notated data will allow to refine our model, both in
terms of accuracy and in terms of coverage, by ex-
panding the tag set with additional morpho-syntactic
features like gender and number, which are prevalent
in Hebrew and other Semitic languages.
Acknowledgments
We thank Gilad Ben-Avi, Ido Dagan and Alon Itai
for their insightful remarks on major aspects of this
work. The financial and computational support of
the Knowledge Center for Processing Hebrew is
gratefully acknowledged. The first author would like
to thank the Technion for partially funding his part
of the research. The first and third authors are grate-
ful to the ILLC of the University of Amsterdam for
its hospitality while working on this research. We
also thank Andreas Stolcke for his devoted technical
assistance with SRILM.
References
Meni Adler. 2001. Hidden Markov Model for Hebrew
part-of-speech tagging. Master?s thesis, Ben Gurion
University, Israel. In Hebrew.
Leonard Baum. 1972. An inequality and associated max-
imization technique in statistical estimation for proba-
bilistic functions of a Markov process. In Inequalities
III:Proceedings of the Third Symposium on Inequali-
ties, University of California, Los Angeles, pp.1-8.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistic, 21:784?789.
David Carmel and Yoelle Maarek. 1999. Morphological
disambiguation for Hebrew search systems. In Pro-
ceedings of the 4th international workshop,NGITS-99.
Eugene Charniak, Curtis Hendrickson, Neil Jacobson,
and Mike Perkowitz. 1993. Equations for part-of-
speech tagging. In National Conference on Artificial
Intelligence, pages 784?789.
K. W. Church. 1988. A stochastic parts program and
noun phrase parser for unrestricted text. In Proc. of
the Second Conference on Applied Natural Language
Processing, pages 136?143, Austin, TX.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic tagging of Arabic text: From raw text to
base phrase chunks. In HLT-NAACL 2004: Short Pa-
pers, pages 149?152.
S.M. Katz. 1987. Estimation of probabilities from sparse
data from the language model component of a speech
recognizer. IEEE Transactions of Acoustics, Speech
and Signal Processing, 35(3):400?401.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language
model based Arabic word segmentation. In ACL,
pages 399?406.
M. Levinger, U. Ornan, and A. Itai. 1995. Morphological
disambiguation in Hebrew using a priori probabilities.
Computational Linguistics, 21:383?404.
Moshe Levinger. 1992. Morphological disambiguation
in Hebrew. Master?s thesis, Computer Science Depart-
ment, Technion, Haifa, Israel. In Hebrew.
Erel Segal. 2000. Hebrew morphological ana-
lyzer for Hebrew undotted texts. Master?s the-
sis, Computer Science Department, Technion,
Haifa, Israel. http://www.cs.technion.ac.il/-
?erelsgl/bxi/hmntx/teud.html.
K. Sima?an, A. Itai, Y. Winter, A. Altman, and N. Nativ.
2001. Building a tree-bank of Modern Hebrew text.
Traitment Automatique des Langues, 42:347?380.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In ICSLP, pages 901?904, Denver,
Colorado, September.
46
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 97?103,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Smoothing a Lexicon-based POS Tagger for Arabic and Hebrew 
 
 
Saib Mansour Khalil Sima'an Yoad Winter 
Computer Science, Technion ILLC Computer Science, Technion 
Haifa, 32000, Israel  Universiteit van Amsterdam Haifa, 32000, Israel  
 Amsterdam, The Netherlands and Netherlands Institute for Ad-
vanced Study 
Wassenaar, The Netherlands 
saib@cs.technion.ac.il simaan@science.uva.nl winter@cs.technion.ac.il 
 
  
Abstract 
We propose an enhanced Part-of-Speech 
(POS) tagger of Semitic languages that 
treats Modern Standard Arabic (hence-
forth Arabic) and Modern Hebrew 
(henceforth Hebrew) using the same 
probabilistic model and architectural set-
ting. We start out by porting an existing 
Hidden Markov Model POS tagger for 
Hebrew to Arabic by exchanging a mor-
phological analyzer for Hebrew with 
Buckwalter's (2002) morphological ana-
lyzer for Arabic. This gives state-of-the-
art accuracy (96.12%), comparable to Ha-
bash and Rambow?s (2005) analyzer-
based POS tagger on the same Arabic 
datasets. However, further improvement 
of such analyzer-based tagging methods is 
hindered by the incomplete coverage of 
standard morphological analyzer (Bar 
Haim et al, 2005). To overcome this cov-
erage problem we supplement the output 
of Buckwalter's analyzer with syntheti-
cally constructed analyses that are pro-
posed by a model which uses character 
information (Diab et al, 2004) in a way 
that is similar to Nakagawa's (2004) sys-
tem for Chinese and Japanese. A version 
of this extended model that (unlike Naka-
gawa) incorporates synthetically con-
structed analyses also for known words 
achieves 96.28% accuracy on the standard 
Arabic test set. 
 
1 Introduction 
Part-of-Speech tagging for Semitic languages has 
been an active topic of research in recent years. 
(Diab et al, 2004; Habash and Rambow, 2005; 
Bar-Haim et al, 2005) are some examples for this 
line of work on Modern Standard Arabic and Mod-
ern Hebrew. POS tagging systems aim at classify-
ing input sequences of lexemes by assigning each 
such sequence a corresponding sequence of most 
probable POS tags. It is often assumed that for 
each input lexeme there is a set of a priori possible 
POS tag categories, or a probability function over 
them, and the tagger has to choose from this lim-
ited set of candidate categories. We henceforth use 
the term lexicon to refer to the set of lexemes in a 
language and the mapping that assigns each of 
them candidate POS tags, possibly with additional 
probabilities.  
Two ways to obtain a lexicon can be distin-
guished in recent works on POS tagging in Semitic 
languages. Data-driven approaches like (Diab et al 
2004) employ the lexicon only implicitly when 
extracting features on possible POS tags from an-
notated corpora that are used for training the POS 
tagger. Lexicon-based approaches (Habash and 
Rambow, 2005; Bar-Haim et al, 2005) use a lexi-
con that is extracted from a manually constructed 
morphological analyzer (Buckwalter 2002 and 
Segal 2001 respectively).  
In this paper we show that although lexicon-
based taggers for Arabic and Hebrew may initially 
outperform data-driven taggers, they do not ex-
haust the advantages of data-driven approaches. 
97
 Consequently, we propose a hybrid model of data-
driven methods and lexicon-based methods, and 
show its advantages over both models, in a way 
that is reminiscent of Nakagawa's (2004) results 
for Chinese and Japanese.  
As a first step, we develop a Part-of-Speech tag-
ger that treats Arabic and Hebrew using the same 
probabilistic model and architectural setting. We 
start out from MorphTagger, a lexicon-based tag-
ger for Hebrew developed by Bar-Haim et al 
(2005), which uses standard Hidden Markov 
Model techniques. We port the existing 
MorphTagger implementation to Arabic by ex-
changing Segal's (2001) morphological analyzer 
with Buckwalter's (2002) morphological analyzer, 
and then training the tagger on the Arabic Tree-
bank (Maamouri et al, 2001). Remarkably, this 
gives state-of-the-art accuracy (96.12%) on the 
same Arabic datasets as Habash and Rambow 
(2005). To the best of our knowledge, this is the 
first time the same POS tagging architecture is 
used both for Arabic and Hebrew texts with com-
parable accuracy.  
Despite the initial advantages of this setting, our 
empirical study shows that in both languages, fur-
ther improvement in accuracy is hindered by the 
incompleteness of the morphological analyzer. By 
"incompleteness" we refer not only to the well-
studied problem of unknown words (out-of-
vocabulary). Our results show that for both Arabic 
and Hebrew, a more serious problem involves 
words for which the analyzer provides a set of 
analyses that does not contain the correct one. We 
find out that this is the case for 3% of the words in 
the development set. This obviously sets an upper 
bound on tagger accuracy using methods that are 
purely based on a manually constructed lexicon. 
We refer to this problem as the "incomplete lexi-
con" problem.  
We focus on devising a solution to the incom-
plete lexicon problem by smoothing. We supple-
ment the output of Buckwalter's analyzer with 
synthetically constructed analyses that are pro-
posed by a model which uses character information 
(Diab et al, 2004) in a way that is similar to Naka-
gawa's (2004) system for Japanese. Unlike Naka-
gawa's method, however, our smoothing method 
incorporates synthetically constructed analyses 
also for known words, though only when all avail-
able taggings of the sentence have low probabili-
ties according to our model. A version of this 
extended model achieves a modest improvement 
(96.28%) in accuracy over the baseline on the 
standard Arabic test set. 
This paper is structured as follows. In section  2 
we start with a brief discussion of previous work. 
Section  3 describes our adaptation of Bar Haim et 
al.?s POS tagging system to Arabic. In section  4 
we show that an architecture like Bar Haim et al?s, 
which relies on a morphological analyzer, is likely 
to suffer from coverage problems under any con-
figuration where it is used as a stand-alone. In sec-
tion  5 we present our new architecture and the 
method of combining the models. Section  6 con-
cludes. 
2 Relation to Previous Works 
Quite a few works have dealt with extending a 
given POS tagger, mainly by smoothing it using 
extra-information about untreated words. For ex-
ample, (Church, 1988) uses the simple heuristic of 
predicting proper nouns from capitalization. This 
method is not applicable to Arabic and Hebrew, 
which lack typographical marking of proper nouns. 
More advanced methods like those described by 
Weischedel et al (1993) incorporate the treatment 
of unknown words within the probability model. 
Weischedel et al use derivational and inflectional 
endings to infer POS tags of unknown words. Na-
kagawa (2004) addresses the problem of unknown 
words for Japanese and Chinese, and uses a hybrid 
method of word-level and character-level informa-
tion. In his model, Nakagawa uses character in-
formation (only) when handling unknown words, 
claiming that in word-level methods information 
about known words helps to achieve higher accu-
racy compared to character-level models. On the 
other hand, when it comes to unknown words, Na-
kagawa uses a character-level method, which is 
hypothesized to be more robust in such cases than 
word-level methods. 
Virtually all works that dealt with coverage 
problems of POS taggers have concentrated on the 
problem of ?unknown? words ? words that have no 
analysis in the initial tagging system. However, in 
the context of analyzer-based tagging systems, we 
also have to deal with the problem of ?known? 
words that miss the correct analysis in the morpho-
logical analyzer. In the Arabic and Hebrew data-
sets we have examined, this problem is more 
severe than the unknown words problem. Unlike 
98
 previous works, we propose to smooth the word-
segment driven model also for ?known? words. To 
avoid overgeneration, this is done only when all 
taggings of the sentence have low probability. 
3 Adapting a Hebrew POS-tagger to 
Arabic 
Bar Haim et al's (2005) POS tagging system, 
MorphTagger, was developed initially for Hebrew. 
Our work is mainly developed for Arabic and 
tested over Arabic data. Due to the similarity in the 
morphological processes in Hebrew and Arabic 
and the generality of Bar Haim et al's architecture, 
the adaptation process was fairly simple. However, 
as far as we know this is the first implementation 
of a unified model for Arabic and Hebrew that 
achieves state-of-the-art accuracy. MorphTagger 
requires two components: a morphological ana-
lyzer to produce a set of analyses for every lexeme, 
and a POS tagged corpus for acquiring an HMM 
disambiguator. The HMM disambiguator assigns a 
probability to every pair xxxxx, where 
n
n www ...11 =  is a sentence and nn ttt ...11 =  a corre-
sponding sequence of POS tags hypothesized by 
the analyzer. This probability is approximated in a 
standard HMM fashion: 
1 1 1 1 1 1 2
1
( , ) ( ) ( | ) ( | , ) ( | )
n
n n n n n
i i i i i
i
P w t P t P w t P t t t P w t? ?
=
= =?  
For an input sentence nw1 , the pair  xxxxx with 
the highest probability is selected. The language 
( ),|( 21 ?? iii tttP ) and lexical ( )|( ii twP ) models' 
parameters are estimated from the tagged corpus 
by Maximum-Likelihood Estimator (MLE) fol-
lowed by Katz backoff smoothing for the language 
model and Add-? smoothing for the lexical model, 
where a small ?=1 count is given to analyzes pro-
vided by the analyzer but not found in the training 
corpus. Furthermore, MorphTagger employs an 
array of other smoothing techniques explained in 
Bar Haim et al (2005).  
Our implementation of MorphTagger for Arabic 
was developed using Buckwalter?s (2002) Mor-
phological Analyzer v1.0 (BMA1.0), and the Ara-
bic Treebank part 1 v2.0 (ATB1), Part 2 v2.0 
(ATB2) and Part 3 v1.0 (ATB3). The ATB was 
chosen not only because of its size and comprehen-
siveness, but also because Buckwalter?s analyzer 
was developed in accordance with the ATB, which 
makes the task of combining information from 
both sources easier. In all our experiments we use a 
tag-set of 24 tags which was mapped from the 
original tag-set (191 tags in ATB1) using the map-
ping script of the ATB distribution. 
To check the ambiguity level and the difficulty 
of the task at hand, we ran BMA1.0 over a testing 
set extracted from ATB1. The average number of 
analyses per word is 1.83, and the average number 
of segmentations per word is 1.2, however, the task 
of disambiguating Arabic is still not easy, as 46% 
of the data is ambiguous. Those results are compa-
rable to the results of Bar Haim et al for Hebrew, 
according to which the average number of analyses 
per word is 2.17 with 1.25 segmentations on aver-
age per word, and 54% of the words are ambigu-
ous. 
The performance of MorphTagger over Arabic 
was measured using the same test settings of Diab 
et al (2004). Habash and Rambow (2005) use a 
different test setting drawn from ATB1. Although 
we could not reproduce the exact setting of Habash 
and Rambow, comparison to their reported accu-
racy is still quite telling due to the similarity of the 
data. The comparison between the accuracy of the 
three systems is summarized in Table 1. The re-
sults in this table were obtained using the correct 
(?gold?) segmentation and applying the standard F-
measure for POS tagging accuracy. The result of 
Diab et al was reproduced on their setting, and the 
result of Habash and Rambow is as reported in 
their paper. 
 
System Tagging accuracy 
MorphTagger 96.12 
Diab et al 95.81 
Habash and Rambow 97.5 
Table 1 - Comparison between systems over ATB1 
 
The result achieved by MorphTagger slightly 
exceeds Diab et al?s result (on the same test set-
ting) and is slightly inferior to Habash and Ram-
bow?s reported result. Overall, it is an encouraging 
result that the MorphTagger system that was de-
veloped for Hebrew could be easily ported to Ara-
bic and yield state-of-the-art results. 
In Table 2, we present the accuracies achieved 
for MorphTagger on a cross validated, 10-fold test, 
including the standard deviation results in paren-
theses. The results are reported both for gold-
segmentation (GS) and without GS.  
nn tw 11 ,
nn tw 11 ,
99
 Test setting Accuracy per word (%) F?=1 per Word-segment (%) 
 Segmentation Tagging Segmentation Tagging 
GS 100 
 
94.89 
(0.62) 
 
100 
 
95.436 
(0.53) 
 
without GS 99.015 (0.24) 
 
94.374 
(0.64) 
98.854 (0.28) 
 
94.727 
(0.56) 
 
Table 2 - MorphTagger performance cross validated 
 
Note that by tagging accuracy per word we 
mean the percentage of words correctly segmented 
and tagged. The tagging F-measure is calculated in 
the standard way, counting the correctly tagged 
word-segments and dividing it by the number of 
"gold" word-segments for recall, and further by the 
number of outputted word-segments for precision. 
Analyzing the POS tagging errors of MorphTag-
ger, we found that about 2.8% of the words in 
ATB1 were not correctly analyzed by the morpho-
logical analyzer. Such ?incomplete lexicon? prob-
lems inevitably lead to tagging errors in 
MorphTagger?s architecture. This problem is more 
serious still on data taken from ATB2 and ATB3, 
where respectively 4.5% and 5.3% of the data led 
to ?incomplete lexicon? problems. We conclude 
that a morphological analyzer can be used to im-
prove upon Diab et al?s results, as done in Habash 
and Rambow and in our straightforward applica-
tion of MorphTagger to Arabic. However, this 
method still suffers from considerable coverage 
problems, which are discussed in the following 
section. 
4 Coverage of Morphological Analysis 
for Arabic 
In order to analyze the coverage problem, we 
tested the coverage of BMA1.0 over parts of the 
ATB which were composed from articles taken on 
different periods of times. The results are summa-
rized in Table 3. The schema of the table includes, 
for each part of the ATB: (i) the number of tokens 
that include at least one Arabic character (hence-
forth ?Arabic words?1); (ii) Out-of-Vocabulary 
(OOV) words, unanalyzed by BMA1.0; (iii) the 
percentage of proper nouns (NNP) out of the OOV 
words; (iv) the number of ?no correct? words ? 
                                                          
1 This definition of Arabic words is taken from Buckwalter's 
analyzer. 
words for which BMA1.0 found at least one solu-
tion but the correct analysis according to the ATB 
was not among them; and (v,vi,vii) the number of 
proper nouns (NNP), nouns (NN) and adjectives 
(JJ) from "no correct". A problem that is unique to 
the ATB is that some words in the corpus were not 
manually annotated and were given the NO_FUNC 
tag. Those words are counted as Arabic words, but 
are ignored in the rest of the statistics of Table 3. 
The noticeable difference in OOV words be-
tween ATB1 and ATB2/ATB3 is expected, be-
cause the lexicon of BMA1.0 was developed using 
information extracted from ATB1. ATB2 and 
ATB3, which were developed after BMA1.0 was 
released (using a more advanced version of Buck-
walter's analyzer), show a different picture. In 
those two parts the OOV problem is not too hard: a 
heuristic that would assign NNP to each OOV 
word would be sufficient in most of the cases. 
However, the ?No Correct? problem is more diffi-
cult: NNPs account for 5% in ATB2 and 18% in 
ATB3 of these words, which are mostly dominated 
by missing adjectives and missing nouns (54% 
jointly in ATB2 and 37% jointly in ATB3).  
Taken together, the OOV problem and the ?No 
Correct? problem mean that more than 5% of the 
words in ATB2 and ATB3 cannot be tagged cor-
rectly using BMA1.0 unless further data are added 
to those provided by the morphological analyzer. A 
similar coverage result was reached for Hebrew by 
Bar Haim et al, using a morphological analyzer for 
Hebrew (Segal, 2001). Bar Haim et al report that 
for about 4% of the Hebrew words in their corpus, 
the correct analysis was missing. From these data 
we conclude that on top of systems like the ones 
proposed by Bar Haim et al and Habash and Ram-
bow,   we   need   to   enhance   the   morphological 
analyzer using additional analyses.
100
 ATB 
part 
Arabic 
words 
OOV NNP of 
OOV 
No Correct NNP of No 
Correct 
NN of No 
Correct 
JJ of No 
Correct 
1 123798 126   
(0.11%) 
21     
(16.67%) 
3369  
 (2.82%) 
0 517 
(15.35%) 
980 
(29.09%) 
2 125729 958   
(0.77%) 
497   
(51.88%) 
5663   
(4.53%) 
282 
(4.98%) 
1254 
(22.14%) 
1818 
(32.1%) 
3 293026 6405 
(2.2%) 
5241 
(81.83%) 
15484 
(5.32%) 
2864 
(18.5%) 
2238 
(14.45%) 
3494 
(22.57%) 
Table 3 - Coverage of Buckwalter's Analyzer 
 
5 Smoothing Using a Data-driven Charac-
ter-based Model 
So far we have shown that POS tagging models 
that use a morphological analyzer achieve high 
accuracy but suffer from coverage problems that 
can not be solved by a simple heuristic. On the 
other hand, models that use character-based infor-
mation are likely to make relatively good predic-
tions for words that are out of the vocabulary of the 
morphological analyzer. We hypothesize that this 
may be especially true for Semitic languages, due 
to their rich and systematic pattern (template) para-
digms. Such patterns add constant characters to 
root characters, and features of substrings of words 
may therefore help in predicting POS tags from 
those patterns. 
Our baseline models for the experiments are 
MorphTagger with a NNP heuristic (MorphTag-
ger+NNP) and ArabicSVM (Diab et al's system). 
As we have already reported in section  3, 
MorphTagger+NNP achieved 96.12% tagging ac-
curacy and ArabicSVM achieved 95.87% over the 
same testing data used by Diab et al One simple 
hybrid model would be adding the analyses pro-
duced by the SVM to the morphological analyzer 
analyses and disambiguate these analyses using 
MorphTagger's HMM. This system has improved 
accuracy ? it achieved accuracy of 96.18%, higher 
than both of the base models. 
The problem with such model is over-generation 
of the SVM: when checked over ATB1 and ATB2, 
40% of the new analyses introduced by the SVM 
are correct analyses, and 60% are wrong. To avoid 
this problem, we suggest conditioning the addition 
of SVM analyses on the sentence's tagging prob-
ability calculated by the HMM model. This is justi-
fied due to the fact that there is correlation between 
the probability of the tagging of a sentence given 
by a language model and the accuracy of the tag-
ging. The relation is shown in Figure 1. 
75
80
85
90
95
100
-800 -700 -600 -500 -400 -300 -200 -100 0
normalized log(P(s))
ac
cu
ra
cy
 
Figure 1 Probability VS Accuracy 
 
Figure 1 shows the relation between the accu-
racy of the tagging and the normalized logarithmic 
probability of the tagging. We normalize the prob-
ability of the tagging by the sentence length as 
longer sentences usually have lower probabilities.  
Following the previous conclusions, we propose 
a hybrid model which adds the analyses of the 
SVM only in cases where the tagging probability 
by the basic MorphTagger system is lower than an 
empirically calculated threshold. If the HMM is 
confident about the tagging it produces, the prob-
ability of the tagging will be high enough to pass 
the threshold, and then the tagging will be output-
ted without adding the SVM analyses which might 
add  noise  to  the  morphological  analyzer  output. A  
general algorithm is shown in Figure 2.
 
101
  
Figure 2 - Enhanced Tagging Algorithm 
 
Note that in the algorithm, a new (word, tag) 
pair introduced by the morphological analyzer or 
by the character model does not appear in the 
tagged corpus, therefore a small count ?=1 is given 
in such cases. This method can be improved fur-
ther, especially for the analyses produced by the 
data-driven character-based method. 
The accuracy we obtained using this system was 
96.28% which shows slight improvement over the 
previous simple hybrid system. Examining the er-
rors in the simple hybrid method and the condi-
tioned method, we see that the improvement is not 
smooth: the conditioned model includes errors 
which did not exist in the simple model. These er-
rors occur when correct analyses of the character-
based model were discarded. In general, however, 
the conditioned method chooses more correct 
analyses. It should be noted that adding the charac-
ter-based model analyses boosted the coverage 
from 97% to 98%, but the accuracy did not im-
prove to the same level. The main cause for this is 
the weak relation between the probability of a sen-
tence and the accuracy. As it is difficult to model 
this relation, we believe that more time should be 
invested to improve the HMM probabilities espe-
cially for the character model analyses, which can 
boost the chances of choosing good analyses. 
6 Conclusions and Future Work 
This paper demonstrates that it is possible to suc-
cessfully port a POS tagger originally built for He-
brew to Arabic using a morphological analyzer and 
a tagged corpus. The POS tagger (called 
MorphTagger) achieves state-of-the-art results 
both on Hebrew and Arabic. Despite this positive 
result we find that further improvement of accu-
racy is hindered by the coverage of the morpho-
logical analyzer. Contrary to earlier work on POS 
tagging, the problem turns out not so much in un-
known (OOV) lexemes as much as in known lex-
emes for which the correct tag is missing. We 
showed empirical evidence that this problem arises 
for the available treebanks and morphological ana-
lyzers for both Arabic and Hebrew. We propose an 
approach that smoothes a given lexical model (ob-
tained from a morphological analyzer and an anno-
tated corpus) by adding synthetically constructed 
analyses, obtained from a POS tagger that com-
bines character-level information. Unlike earlier 
work, we apply this smoothing only when the 
probabilistic model assigns probabilities lower 
than a threshold to all possible POS taggings of the 
input sentence. This way we obtain moderate im-
provement in Arabic POS tagging. 
The problem of missing lexeme-POS pairs in 
POS taggers for Semitic languages is more severe 
than in languages like English. We conjecture that 
this is because of the more complex morphology of 
Semitic languages. 
In future work it might be worthwhile to con-
sider morphological processes that are more com-
plex than the standard affixation 
(suffixing/prefixing) processes in order to general-
ize better over cases in the training data. Such a 
generalization may provide better coverage of lex-
eme-POS pairs and would increase the upper 
bound on accuracy. 
Given a sentence s, perform the following steps: 
1. Produce analyses for each word in s using the morphological analyzer 
combined with the corpus analyses. 
2. Calculate lexical and contextual probabilities using available annotated 
corpora (using Maximum Likelihood Estimation). 
3. Run Viterbi's Algorithm for HMM disambiguation, and calculate a rank 
of the tagging which is composed from the probability given by the 
model and the length of the sentence. 
4. If [rank>threshold] output tagging. 
4'. [Otherwise] run the character based model over the sentence and add the 
new analyses generated.  
5'. Combine the analyses generated by the morphological analyzer and the 
character-based model, update the lexical probabilities and rerun the 
model. 
102
 References  
Roy Bar Haim, Khalil Sima?an and Yoad Winter. 2005. 
Choosing an Optimal Architecture for Segmentation 
and POS-Tagging of Modern Hebrew. ACL Work-
shop on Computational Approaches to Semitic Lan-
guages. A revised and extended version to appear in 
Journal of Natural Language Engineering. 
Eric Brill. 1995. Transformation-based error-driven 
learning and natural language processing: a case 
study in part of speech tagging. In Computational 
Linguistics 21, pages 543-565. 
Tim Buckwalter. 2002. Arabic Morphological Analyzer 
Version 1.0. Linguistic Data Consortium, University 
of Pennsylvania. 
Kenneth W. Church. 1988. A stochastic parts program 
and noun phrase parser for unrestricted text. Pro-
ceedings of the second conference on Applied natural 
language processing, Pages 136-143. 
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004. 
Automatic Tagging of Arabic Text: From Raw Text to 
Base Phras e Chunks. In HLT-NAACL: Short Pa-
pers, pages 149-152. 
Nizar Habash and Owen Rambow. 2005. Arabic To-
kenization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In 
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 573-
580, Ann Arbor. 
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language 
model based Arabic word segmentation. In ACL, 
pages 399-406. 
Mohamed Maamouri and Ann Bies. 2004. Developing 
an Arabic treebank: Methods, guidelines, proce-
dures, and tools. In Proceedings of the Workshop on 
Computational Approaches to Arabic Script-based 
Languages (COLING), Geneva. 
Christopher D. Manning and Hinrich Sch?tze. 1999. 
Foundations of Statistical Natural Language Proc-
essing. The MIT press, Cambridge, Massachusetts. 
Tetsuji Nakagawa. 2004. Chinese and Japanese word 
segmentation using word-level and character-level 
information. In Proceedings of the 20th International 
Conference on Computational Linguistics, pages 
466-472, Geneva. 
Erel Segal. 2001. Hebrew morphological analyzer for 
Hebrew undotted texts. Master's thesis, Computer 
Science Department, Technion, Haifa, Israel. 
Ralph Weischedel, Marie Meteer, Richard Schwartz, 
Lance Ramshaw and Jeff Palmucci. 1993. Coping 
with Ambiguity and Unknown Words through Prob-
abilistic Models. Computational Linguistics (Special 
issue on using large corpora: II) volume 19, pages 
361-382. 
 
103
Proceedings of the 10th Conference on Parsing Technologies, pages 156?167,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Three-Dimensional Parametrization for Parsing
Morphologically Rich Languages
Reut Tsarfaty and Khalil Sima?an
Institute for Logic, Language and Computation
University of Amsterdam
Plantage Muidergracht 24, 1018TV Amsterdam, The Netherlands
{rtsarfat,simaan}@science.uva.nl
Abstract
Current parameters of accurate unlexical-
ized parsers based on Probabilistic Context-
Free Grammars (PCFGs) form a two-
dimensional grid in which rewrite events
are conditioned on both horizontal (head-
outward) and vertical (parental) histories.
In Semitic languages, where arguments
may move around rather freely and phrase-
structures are often shallow, there are ad-
ditional morphological factors that govern
the generation process. Here we pro-
pose that agreement features percolated up
the parse-tree form a third dimension of
parametrization that is orthogonal to the pre-
vious two. This dimension differs from
mere ?state-splits? as it applies to a whole
set of categories rather than to individual
ones and encodes linguistically motivated
co-occurrences between them. This paper
presents extensive experiments with exten-
sions of unlexicalized PCFGs for parsing
Modern Hebrew in which tuning the param-
eters in three dimensions gradually leads to
improved performance. Our best result in-
troduces a new, stronger, lower bound on the
performance of treebank grammars for pars-
ing Modern Hebrew, and is on a par with
current results for parsing Modern Standard
Arabic obtained by a fully lexicalized parser
trained on a much larger treebank.
1 Dimensions of Unlexicalized Parsing
Probabilistic Context Free Grammars (PCFGs) are
the formal backbone of most high-accuracy statisti-
cal parsers for English, and a variety of techniques
was developed to enhance their performance rela-
tive to the na??ve treebank implementation ? from
unlexicalized extensions exploiting simple category
splits (Johnson, 1998; Klein and Manning, 2003)
to fully lexicalized parsers that condition events be-
low a constituent upon the head and additional lexi-
cal content (Collins, 2003; Charniak, 1997). While
it is clear that conditioning on lexical content im-
proves the grammar?s disambiguation capabilities,
Klein and Manning (2003) demonstrate that a well-
crafted unlexicalized PCFG can close the gap, to a
large extent, with current state-of-the-art lexicalized
parsers for English.
The factor that sets apart vanilla PCFGs (Char-
niak, 1996) from their unlexicalized extensions pro-
posed by, e.g., (Johnson, 1998; Klein and Manning,
2003), is the choice for statistical parametrization
that weakens the independence assumptions implicit
in the treebank grammar. Studies on accurate unlex-
icalized parsing models outline two dimensions of
parametrization. The first, proposed by (Johnson,
1998), is the annotation of parental history, and the
second encodes a head-outward generation process
(Collins, 2003). Johnson (1998) augments node la-
bels with the label of their parent, thus incorporat-
ing a dependency on the node?s grandparent. Collins
(2003) proposes to generate the head of a phrase first
and then generate its sisters using Markovian pro-
cesses, thereby exploiting head/sister-dependencies.
156
Klein and Manning (2003) systematize the dis-
tinction between these two forms of parametrization
by drawing them on a horizontal-vertical grid: par-
ent encoding is vertical (external to the rule) whereas
head-outward generation is horizontal (internal to
the rule). By varying the value of the parame-
ters along the grid, Klein and Manning (2003) tune
their treebank grammar to achieve improved perfor-
mance. This two-dimensional parametrization has
been instrumental in devising parsing models that
improve disambiguation capabilities for English as
well as other languages, such as German (Dubey and
Keller, 2003) Czech (Collins et al, 1999) and Chi-
nese (Bikel and Chiang, 2000). However, accuracy
results for parsing languages other than English still
lag behind.1
We propose that for various languages includ-
ing the Semitic family, e.g. Modern Hebrew (MH)
and Modern Standard Arabic (MSA), a third di-
mension of parametrization is necessary for encod-
ing linguistic information relevant for breaking false
independence assumptions. In Semitic languages,
arguments may move around rather freely and the
phrase-structure of clause-level categories is often
shallow. For such languages agreement features play
a role in disambiguation at least as important as the
vertical and horizontal conditioning. We propose a
third dimension of parameterizations that encodes
morphological features such as those realizing syn-
tactic agreement. These features are percolated from
surface forms in a bottom-up fashion and express
information that is complementary to the horizon-
tal and vertical generation histories proposed before.
Such morphological information refines syntactic
categories based on their morpho-syntactic role, and
captures linguistically motivated co-occurrences and
dependencies manifested via, e.g., morpho-syntactic
agreement.
This work aims at parsing MH and explores the
empirical contribution of the three dimensions of
parameters specified above. We present extensive
experiments that gradually lead to improved perfor-
mance as we extend the degree to which the three
dimensions are exploited. Our best model uses all
three dimensions of parametrization, and our best re-
1The learning curves over increasing training data (e.g., for
German (Dubey and Keller, 2003)) show that treebank size can-
not be the sole factor to account for the inferior performance.
sult is on a par with those achieved for MSA using a
fully lexicalized parser and a much larger treebank.
The remainder of this document is organized as fol-
lows. In section 2 we review characteristic aspects
of MH (and other Semitic languages) and illustrate
the special role of morphology and dependencies
displayed by morpho-syntactic processes using the
case of syntactic definiteness in MH. In section 3 we
define our three-dimensional parametrization space.
In section 4 we spell out the method and procedure
for the empirical evaluation of one, two and three
parametrization dimensions, and in section 5 we re-
port and analyze results for different parametrization
choices. Finally, section 6 discusses related work
and in section 7 we summarize and conclude.
2 Dimensions of Modern Hebrew Syntax
Parsing MH is in its infancy. Although a syntacti-
cally annotated corpus has been available for quite
some time (Sima?an et al, 2001), we know of only
two studies attempting to parse MH using statistical
methods (see section 6). One reason for the sparse-
ness in this field is that the adaptation of existing
models to parsing MH is technically involved yet
does not guarantee to yield comparable results as
the processes that license grammatical structures of
phrases and sentences in MH differ from those as-
sumed for English. This section outlines differences
between English and MH and discusses their reflec-
tion in the MH treebank annotation scheme. We
argue that on top of syntactic processes exploited
by current parsers there is an orthogonal morpho-
syntactic dimension which is invaluable for syntac-
tic disambiguation, and it can be effectively learned
using simple treebank grammars.
2.1 Modern Hebrew Structure
Phrases and sentences in MH, as well as in Arabic
and other Semitic languages, have a relatively flexi-
ble phrase structure. Subjects, verbs and objects can
be inverted and prepositional phrases, adjuncts and
verbal modifiers can move around rather freely. The
factors that affect word-order in the language are not
exclusively syntactic and have to do with rhetorical
and pragmatic factors as well.2
2See, for instance, (Melnik, 2002) for an Information
Structure-syntactic account of verb initial sentences.
157
(a) S
NP.MP-SBJ
CD.MP
sni
two.MP
N.MP
hildim
the-children.MP
VP.MP
V.MP
aklw
ate.MP
NP.FS-OBJ
N.FS
ewgh
cake.FS
(b) S
NP.FS-OBJ
N.FS
ewgh
cake.FS
VP.MP
V.MP
aklw
ate.MP
NP.MP-SBJ
CD.MP
sni
two.MP
N.MP
hildim
the-children.MP
Figure 1: Word Order and Agreement Features in MH
Phrases: Agreement on MP features reveals the subject-
predicate dependency between surface forms and their dom-
inating constituents in a variable phrase-structure (marking
M(asculine), F(eminine), S(ingular), P(lural).)
It would be too strong a claim, however, to clas-
sify MH (and similar languages) as a free-word-
order language in the canonical sense. The level of
freedom in the order and number of internal con-
stituents varies between syntactic categories. Within
a verb phrase or a sentential clause, for instance,
the order of constituents obeys less strict rules than
within, e.g., a noun phrase.3 Figure 1 illustrates two
syntactic structures that express the same grammat-
ical relations yet vary in their internal order of con-
stituents. Within the noun phrase constituents, how-
ever, determiners always precede nouns.
Within the flexible phrase structure it is typically
morphological information that provides cues for the
grammatical relations between surface forms. In
figure 1, for example, it is agreement on gender
and number that reveals the subject-predicate depen-
dency between surface forms. Figure 1 also shows
that agreement features help to reveal such relations
between higher levels of constituents as well.
Determining the child constituents that contribute
each of the features is not a trivial matter either. To
illustrate the extent and the complexity of that matter
let us consider definiteness in MH, which is morpho-
logically marked (as an h prefix to the stem, glossed
here explicitly as ?the-?) and behaves as a syntactic
3See (Wintner, 2000) and (Goldberg et al, 2006) for formal
and statistical accounts (respectively) of noun phrases in MH.
(a) NP.FS.D
NP.FS.D
sganit hmnhl
deputy.FS the-manager.MS.D
ADJP.FS.D
hmswrh
the-dedicated.FS.D
(a) S
NP.FS.D
sganit hmnhl
deputy.FS the-manager.MS.D
PREDP.FS
mswrh
dedicated.FS
Figure 2: Definiteness in MH as a Phrase-Level Agreement
Feature: Agreement on definiteness helps to determine the in-
ternal structure of a higher level NP (a), and the absence thereof
helps to determine the attachment to a predicate in a verb-less
sentence (b) (marking D(efiniteness))
(a) S
NP.FS.D
NNT.FS
sganit
deputy.FS
N.MS.D
hmnhl
the-manager.MS.D
VP.FS
V.FS
htpjrh
resigned.FS
(b) S?V?
NP?NNT?.FS.D
NNT.FS
sganit
deputy.FS
N.MS.D
hmnhl
the-manager.MS.D
VP?V?).FS
V.FS
htpjrh
resigned.FS
Figure 3: Phrase-Level Agreement Features and Head-
Dependencies in MH: The direction of percolating definiteness
in MH is distinct of that of the head (marking ?head-tag?)
property (Danon, 2001). Definite noun-phrases ex-
hibit agreement with other modifying phrases, and
such agreement helps to determine the internal struc-
ture, labels, and the correct level of attachment as
illustrated in figure 2. The agreement on definite-
ness helps to determine the internal structure of noun
phrases 2(a), and the absence thereof helps in de-
termining the attachment to predicates in verb-less
sentences, as in 2(b). Finally, definiteness may be
percolated from a different form than the one deter-
mining the gender and number of a phrase. In figure
3(a), for instance, the definiteness feature (marked
as D) percolates from ?hmnhl ? (the-manager.MS.D)
while the gender and number are percolated from
?sganit ? (deputy.FS). The direction of percolation
of definiteness may be distinct of that of percolat-
ing head information, as can be seem in figure 3(b).
(The direction of head-dependencies in MH typi-
cally coincides with that of percolating gender.)
To summarize, agreement features are helpful in
analyzing and disambiguating syntactic structures in
MH, not only at the lexical level, but also at higher
levels of constituency. In MH, features percolated
from different surface forms jointly determine the
features of higher-level constituents, and such fea-
tures manifest multiple dependencies, which in turn
cannot be collapsed onto a single head.
158
2.2 The Modern Hebrew Treebank Scheme
The annotation scheme of version 2.0 of the MH
treebank (Sima?an et al, 2001)4 aims to capture the
morphological and syntactic properties of MH just
described. This results in several aspects that dis-
tinguish the MH treebank from, e.g., the WSJ Penn
treebank annotation scheme (Marcus et al, 1994).
The MH treebank is built over word segments.
This means that the yields of the syntactic trees do
not correspond to space delimited words but rather
to morphological segments that carry distinct syn-
tactic roles, i.e., each segment corresponds to a sin-
gle POS tag. (This in turn means that prefixes
marking determiners, relativizers, prepositions and
definite articles are segmented away and appear as
leaves in a syntactic parse tree.) The POS categories
assigned to segmented words are decorated with fea-
tures such as gender, number, person and tense, and
these features are percolated higher up the tree ac-
cording to pre-defined syntactic dependencies (Kry-
molowski et al, 2007). Since agreement features
of non-terminal constituents may be contributed by
more than one child, the annotation scheme defines
multiple dependency labels that guide the percola-
tion of the different features higher up the tree. Def-
initeness in the MH treebank is treated as a segment
at the POS tags level and as a feature at the level of
non-terminals. As any other feature, it is percolated
higher up the tree according to marked dependency
labels. Table 1 lists the features and values annotated
on top of syntactic categories and table 2 describes
the dependencies according to which these features
are percolated from child constituents to their par-
ents.
In order to comply with the flexible phrase struc-
ture in MH, clausal categories (S, SBAR and FRAG
and their corresponding interrogatives SQ, SQBAR
and FRAGQ) are annotated as flat structures. Verbs
(VB tags) always attach to a VP mother, however
only non-finite VBs can accept complements un-
der the same VP parent, meaning that all inflected
verb forms are represented as unary productions
under an inflected VP. NP and PP are annotated
4Version 2.0 of the MH treebank is publicly available
at http://mila.cs.technion.ac.il/english/
index.html along with a complete overview of the MH
annotation scheme and illustrative examples (Krymolowski et
al., 2007).
Feature:Value Value Encoded
gender:Z masculine
gender:N feminine
gender:B both
number:Y singular
number:R plural
number:B both
definiteness:H definite
definiteness:U underspecified
Table 1: Features and Values in the MH Treebank
Dependency Type Features Percolated
DEP HEAD all
DEP MAJOR at least gender
DEP NUMBER number
DEP DEFINITE definiteness
DEP ACCUSATIVE case
DEP MULTIPLE all (e.g., conjunction)
Table 2: Dependency Labels in the MH Treebank
as nested structures capturing the recursive struc-
ture of construct-state nouns, numerical expressions
and possession. An additional category, PREDP, is
added in the treebank scheme to account for sen-
tences in MH that lack a copular element, and it may
also be decorated with inflectional features agreeing
with the subject. The MH treebank scheme also fea-
tures null elements that mark traces and additional
labels that mark functional features (e.g., SBJ,OBJ)
which we strip off and ignore throughout this study.
Morphological features percolated up the tree
manifest dependencies that are marked locally yet
have a global effect. We propose to learn treebank
grammars in which the syntactic categories are aug-
mented with morphological features at all levels of
the hierarchy. This allows to learn finer-grained
categories with subtle differences in their syntactic
behavior and to capture non-independence between
certain parts of the syntactic parse-tree.
3 Refining the Parameter Space
(Klein and Manning, 2003) argue that parent en-
coding on top of syntactic categories and RHS
markovization of CFG productions are two instances
of the same idea, namely that of encoding the gener-
ation history of a node to a varying degree. They
subsequently describe two dimensions that define
their parameters? space. The vertical dimension (v),
capturing the history of the node?s ancestors in a top-
159
down generation process (e.g., its parent and grand-
parent), and the horizontal dimension (h), capturing
the previously generated horizontal ancestors of a
node (effectively, its sisters) in a head-outward gen-
eration process. By varying the value of h and v
along this two-dimensional grid they improve per-
formance of their induced treebank grammar.
Formally, the probability of a parse tree pi is cal-
culated as the probability of its derivation, the se-
quential application of rewrite rules. This in turn
is calculated as the product of rules? probabilities,
approximated by assuming independence between
them P (pi) = ?i P (ri|r1 ? ... ? ri?1) ?
?
i P (ri).
The vertical dimension v can be thought of as a func-
tion ?0 selecting features from the generation his-
tory of the constituent thus restoring selected depen-
dencies:
P (ri) = P (ri|?0(r1 ? .. ? ri?1))
The horizontal dimension h can be thought of as two
functions ?1,?2 over decomposed rules, where ?1
selects hidden internal features of the parent, and
?2 selects previously generated sisters in a head-
outward Markovian process (we retain here the as-
sumption that the head child H always matters).
P (ri) = Ph(H|?1(LHS(ri)))
?
?
C?RHS(ri)?H
PC(C|?2(RHS(ri)),H)
The fact that the default notion of a treebank
grammar takes v = 1 (i.e., ?0(r1 ? .. ? ri?1) = ?)
and h = ? (RHS cannot decompose) is, according
to Klein and Manning (2003), a historical accident.
We claim that languages with freeer word order
and richer morphology call for an additional dimen-
sion of parametrization. The additional parameter
shows to what extent morphological features en-
coded in a specialized structure back up the deriva-
tion of the tree. This dimension can be thought of
as a function ?3 selecting aspects of morphological
orthogonal analysis of the rules, where MA denotes
morphological analysis of the syntactic categories in
both LHS and RHS of the rule.
P (ri) = P (ri|?3(MA(ri)))
The fact that in current parsers ?3(MA(ri)) = ? is,
we claim, another historical accident. Parsing En-
glish is quite remarkable in that it can be done with
Figure 4: The Three-Dimensional Parametrization Space
impoverished morphological treatment, but for lan-
guages in which morphological processes are more
pertinent, we argue, bi-dimensional parametrization
shall not suffice.
The emerging picture is as follows. Bare-category
skeletons reside in a bi-dimensional parametrization
space (figure 3(a)) in which the vertical (figure 3(b))
and horizontal (figure 3(c)) parameter instantiations
elaborate the generation history of a non-terminal
node. Specialized structures enriched with (an in-
creasing amount of) morphological features reside
deeper along a third dimension we refer to as depth
(d). Figure 4 illustrates an instantiation of d = 1
with a single definiteness feature. Higher d values
would imply adding more (accumulating) features.
Klein and Manning (2003) view the vertical
and horizontal parametrization dimensions as im-
plementing external and internal annotation strate-
gies respectively. External parameters indicate fea-
tures of the external environment that influence the
node?s expansion possibilities, and internal parame-
ters mark aspects of hidden internal content which
influence constituents? external distribution. We
view the third dimension of parametrization as im-
plementing a relational strategy of annotation en-
coding the way different constituents may combine
to form phrases and sentences. In a bottom up pro-
cess this annotation strategy imposes soft constraints
on a the top-down head-outward generation process.
Figure 6(a) focuses on a selected NP node high-
lighted in figure 4 and shows its expansion possibil-
ities in three dimensions. Figure 6(b) illustrates how
the depth expansion interacts with both parent anno-
160
(a) The horizontal/vertical Grid (b) The vertical dimension (c) The horizontal dimension
Figure 5: The Two-Dimensional Space: The horizontal and vertical dimensions outlined by (Klein and Manning, 2003)
tation and neighbor dependencies thereby affecting
both distributions.
3.1 A Note on State-Splits
Recent studies (Klein and Manning, 2003; Mat-
suzaki et al, 2005; Prescher, 2005; Petrov et al,
2006) suggest that category-splits help in enhanc-
ing the performance of treebank grammars, and a
previous study on MH (Tsarfaty, 2006) outlines spe-
cific POS-tags splits that improve MH parsing ac-
curacy. Yet, there is a major difference between
category-splits, whether manually or automatically
acquired, and the kind of state-splits that arise from
agreement features that refine phrasal categories.
While category-splits aim at each category in iso-
lation, agreement features apply to a whole set
of categories all at once, thereby capturing refine-
ment of the categories as well as linguistically mo-
tivated co-occurrences between them. Individual
category-splits are viewed as taking place in a two-
dimensional space and it is hard to analyze and em-
pirically evaluate their interaction with other annota-
tion strategies. Here we propose a principled way to
statistically model the interaction between different
linguistic processes that license grammatical struc-
tures and empirically contrast their contribution.
3.2 A Note on Stochastic AV grammars
The practice of having morphological features or-
thogonal to a constituency structure is not a new
one and is familiar from formal theories of syntax
such as HPSG (Sag et al, 2003) and LFG (Ka-
plan and Bresnan, 1982). Here we propose to re-
frame systematic morphological decoration of syn-
tactic categories at all levels of the hierarchy as
(a) (b)
Figure 6: The Expansion Possibilities of a Non-Terminal
Node: Expanding the NP from figure 4 in a three-dimensional
parameterization Space
an additional dimension of statistical estimation for
learning unlexicalized treebank PCFGs. Our pro-
posal deviates from various stochastic extensions of
such constraints-based grammatical formalisms (cf.
(Abney, 1997)) and has the advantage of elegantly
bypassing the issue of loosing probability mass to
failed derivations due to unification failures. To the
best of our knowledge, this proposal has not been
empirically explored before.
4 Experimental Setup
Our goal is to determine the optimal strategy for
learning treebank grammars for MH and to contrast
it with bi-dimensional strategies explored for En-
glish. The methodology we use is adopted from
(Klein and Manning, 2003) and our procedure is
identical to the one described in (Johnson, 1998).
We define transformations over the treebank that ac-
cept as input specific points in the (h, v, d) space de-
picted in figure 7. We use the transformed training
sets for learning different treebank PCFGs which we
then used to parse unseen sentences, and detrans-
form the parses for the purpose of evaluation.5
5Previous studied on MH used different portions of the tree-
bank and its annotation scheme due to its gradual development
161
Data We use version 2.0 of the MH treebank
which consists of 6501 sentences from the daily
newspaper ?Ha?aretz?. We employ the syntactic cat-
egories, POS categories and morphological features
annotated therein. The data set is split into 13 sec-
tions consisting of 500 sentences each. We use the
first section (section 0) as our development set and
the last section (section 12) as our test set. The re-
maining sentences (sections 1?11) are all used for
training. After removing empty sentences, sentences
with uneven bracketing and sentences that do not
match the annotation scheme6 we remain with a de-
vset of 483 sentences (average length in word seg-
ments 48), a trainset of 5241 sentences (53) and
a testset of 496 sentences (58). Since this work
is only the first step towards the development of a
broad-coverage statistical parser for MH (and other
Semitic languages) we use the development set for
parameter-tuning and error analysis and use the test
set only for confirming our best results.
Models The models we implement use one-, two-
or three-dimensional parametrization and different
instantiation of values thereof. (Due to the small
size of our data set we only use the values {0, 1}
as possible instantiations.)
The v dimension is implemented using a trans-
form as in (Johnson, 1998) where v = 0 corresponds
to bare syntactic categories and v = 1 augments
node labels with the label of their parent node.
The h dimension is peculiar in that it distinguishes
PCFGs (h = ?), where RHS cannot decompose,
from their head-driven unlexicalized variety. To im-
plement h 6= ? we use a PCFG transformation em-
ulating (Collins, 2003)?s first model, in which sisters
are generated conditioned on the head tag and a sim-
ple ?distance? function (Hageloh, 2007).7 The in-
process. As the MH treebank is approaching maturity we feel
that the time is ripe to standardize its use for MH statistical
parsing. The software we implemented will be made available
for non-commercial use upon request to the author(s) and the
feature percolation software by (Krymolowski et al, 2007) is
publicly available through the Knowledge Center for Process-
ing Hebrew. By this we hope to increase the interest in MH
within the parsing community and to facilitate the application
of more sophisticated models by cutting down on setup time.
6Marked as ?NO MATCH? in the treebank.
7A formal overview of the transformation and its corre-
spondence to (Collins, 2003)?s models is available at (Hageloh,
2007). We use the distance function defined therein, marking
the direction and whether it is the first node to be generated.
stantiated value of h then selects the number of pre-
viously generated (non-head) sisters to be taken into
account when generating the next sister in a Marko-
vian process (?2 in our formal exposition).
The d dimension we proposed is implemented us-
ing a transformation that augments syntactic cate-
gories with morphological features percolated up the
tree. We use d = 0 to select bare syntactic cate-
gories and instantiate d = 1 with the definiteness
feature. The decision to select definiteness (rather
than, e.g., gender or number) is rather pragmatic as
its direction of percolation may be distinct of head
information and the question remains whether the
combination of such non-overlapping dependencies
is instrumental for parsing MH.
Our baseline model is a vanilla treebank PCFG
as described in (Charniak, 1996) which we locate
on the (?, 0, 0) point of our coordinates-system.
In a first set of experiments we implement simple
PCFG extensions of the treebank trees based on se-
lected points on the (?, v, d) plain. In a second
set of experiments we use an unlexicalized head-
driven baseline a` la (Collins, 2003) located on the
(0, 0, 0) coordinate. We transform the treebank trees
in correspondence with different points in the three-
dimensional space defined by (h, v, d). The models
we implement are marked in the coordinate-system
depicted in figure 7. The implementation details of
the transformations we use are spelled out in tables
3?4.
Procedure We implement different models that
correspond to different instantiations of h, v and d.
For each instantiation we transform the training set
and learn a PCFG using Maximum Likelihood es-
timates, and we use BitPar (Schmidt, 2004), an ef-
ficient general-purpose parser, to parse unseen sen-
tences. The input to the parser is a sequence of word
segments where each segment corresponds to a sin-
gle POS tag, possibly decorated with morphologi-
cal features. This setup assumes partial morpholog-
ical disambiguation (namely, segmentation) but cru-
cially we do not disambiguate their respective POS
categories. This setup is more appropriate for us-
ing general-purpose parsing tools and it makes our
results comparable to studies in other languages.8
8Our working assumption is that better performance of a
parsing model in our setup will improve performance also
162
Transliterate The lexical items (leaves) in the MH treebank are written left-to-write and are encoded
in utf8. A transliteration software is used to convert the utf encoding into Latin characters and to reverse
their order, essentially allowing for standard left-to-right processing.
Correct The manual annotation resulted in unavoidable errors in the annotation scheme, such as typos
(e.g., SQBQR instead of SQBAR) wrong delimiters (e.g., ?-? instead of ? ?) or wrong feature order (e.g.,
number-gender instead of gender-number). We used an automatic script to detect these error, we manually
determine their correction. Then we created an automatic script to apply all fixes (57 errors in 1% sentences).
Re-attach VB elements are attached by convention to a VP which inherits its morphological features.
9 VB instances in the treebank are mistakenly attached to an S parent without an intermediate VP level.
Our software re-attaches those VB elements to a VP parent and percolates its morphological features.
Disjoint Due to recursive processes of generating noun phrases and numerical expression (smixut)
in MH the sets of POS and syntactic categories are not disjoint. This is a major concern for PCFG parsers
that assume disjoint sets of pre- and non-terminals. The overlap between the sets also introduces additional
infinite derivations to which we loose probability mass. Our software takes care to decorate POS categories
used as non-terminal with an additional ?P?, creating a new set of categories encoding partial derivations.
Lexicalize A pre-condition for applying horizontal parameterizations a` la Collins is the annotation of
heads of syntactic phrases. The treebank provided by the knowledge center does not define unique heads,
but rather, mark multiple dependencies for some categories and none for others. Our software uses rules
for choosing the syntactic head according to specified dependencies and a head table when none are specified.
Linearize In order to implement the head-outward constituents? generation process we use software made
available to us by (Hageloh, 2007) which converts PCFG production such as the generation of a head is followed by left and right
markovized derivation processes. We used two versions of Markovization, one which conditions only on the
head and a distance function, and another which conditions also on immediately neighboring sister(s).
Decorate Our software implements an additional general transform which selects the features that are to be
annotated on top of syntactic categories to implement various parametrization decisions. This transform can be
used for, e.g., displaying parent information, selecting morphological features, etc.
Table 3: Transforms over the MH Treebank: We clean and correct the treebank using Transliterate, Correct, Re-attach and
Disjoint, and transform the training set according to certain parametrization decisions using Lexicalize, Linearize and Decorate.
Smoothing pre-terminal rules is done explicitly by
collecting statistics on ?rare word? occurrences and
providing the parser with possible open class cat-
egories and their corresponding frequency counts.
The frequency threshold defining ?rare words? was
tuned empirically and set to 1. The resulting test
parses are detransformed and to skeletal constituent
structures, and are compared against the gold parses
to evaluate parsing accuracy.
Evaluation We evaluate our models using EVALB
in accordance with standard PARSEVAL evaluation
metrics. The evaluation of all models focuses on
Labeled Precision and Recall considering bare syn-
tactic categories (stripping off all morphological or
parental features and removing intermediate nodes
for linearization). We report the average F-measure
for sentences of length up to 40 and for all sentences
(F?40 and FAll respectively). We report the results
within an integrated model for morphological and syntactic dis-
ambiguation in the spirit of (Tsarfaty, 2006). We conjecture
that the kind of models developed here which takes into account
morphological information is more appropriate for the morpho-
logical disambiguation task defined therein.
for two evaluation options, once including punctua-
tion marks (WP ) and once excluding them (WOP ).
5 Results
Our baseline for the first set of experiments is
a vanilla PCFG as described in (Charniak, 1996)
(without a preceding POS tagging phase and without
right branching corrections). We transform the tree-
bank trees based on various points in the (?, v, d)
two-dimensional space to evaluate the performance
of the resulting PCFG extensions.
Table 5 reports the accuracy results for all models
on section 0 (devset) of the treebank. The accuracy
results for the vanilla PCFG are approximately 10%
lower than reported by (Charniak, 1996) for English
demonstrating that parsing MH using the currently
available treebank is a harder task. For all unlexical-
ized extensions learned from the transfromed tree-
banks, the resulting grammars show enhanced dis-
ambiguation capabilities and improved parsing ac-
curacy. We observe that the vertical dimension con-
tributes the most from both one-dimensional mod-
163
Name Params Description Transforms used
DIST h = 0 0-order Markov process Lexicalize(category), Linearize(distance)
MRK h = 1 1-order Markov process Lexicalize(category), Linearize(distance, neighbor)
PA v = 1 Parent Annotation Decorate(parent)
DEF d = 1 Definiteness feature percolation Decorate(definiteness)
Table 4: Implementing Different Parametrization Options using Transforms
Implementation (h, v, d) FALL F?40 FALL F?40
WP WP WOP WOP
PCFG (?, 0, 0) 65.17 66.63 66.17 67.7
PA (?, 0, 1) 70.6 71.96 70.96 72.18
DEF (?, 1, 0) 67.53 68.78 68.82 70.06
PA+DEF (?, 1, 1) 72.63 73.89 73.01 74.11
Table 5: PCFG Two-Dimensional Extensions: Accuracy re-
sults for parsing the devest (section 0)
els. A qualitative error analysis reveals that parent
annotation strategy distinguishes effectively various
kinds of distributions clustered together under a sin-
gle category. For example, S categories that appear
under TOP tend to be more flat than S categories ap-
pearing under SBAR (SBAR clauses typically gen-
erate a non-finite VP node under which additional
PP modifiers can be attached).
Orthogonal morphological marking provide addi-
tional information that is indicative of the kind of
dependencies that exist between a category and its
various child constituents, and we see that the d di-
mension instantiated with definiteness not only con-
tribute more than 2% to the overall parsing accuracy
of a vanilla PCFG, but also contributes as much to
the improvement obtained from a treebank already
annotated with the vertical dimension. The contribu-
tions are thus additive providing preliminary empir-
ical support to our claim that these two dimensions
provide information that is complementary.
In our next set of experiments we evaluate the
contribution of the depth dimension to extensions of
the head-driven unlexicalized variety a` la (Collins,
2003). We set our baseline at the (0, 0, 0) coordi-
nate and evaluate models that combine one, two and
three dimensions of parametrization. Table 6 shows
the accuracy results for parsing section 0 using the
resulting models.
The first outcome of these experiments is that our
new baseline improves on the accuracy results of
a simple treebank PCFG. This result indicates that
head-dependencies which play a role in determin-
ing grammatical structures in English are also in-
strumental for parsing MH. However, the marginal
contribution of the head-driven variation is surpris-
ingly low. Next we observe that for one-dimensional
models the vertical dimension still contributes the
most to parsing accuracy. However, morphologi-
cal information represented by the depth dimension
contributes more to parsing accuracy than informa-
tion concerning immediately preceding sisters on
the horizontal dimension. This outcome is consis-
tent with our observation that the grammar of MH
puts less significance on the position of constituents
relative to one others and that morphological in-
formation is more indicative of the kind of syntac-
tic relations that appear between them. For two-
dimensional models, incorporating the depth dimen-
sion (orthogonal morphological marking) is better
than not doing so, and relying solely on horizon-
tal/vertical parameters performs slightly worse than
the vertical/depth combination. The best performing
model for two-dimensional head-driven extensions
is the one combining vertical history and morpho-
logical depth. This is again consistent with the prop-
erties of MH highlighted in section 2 ? parental in-
formation gives cues about the possible expansion
on the current node, and morphological information
indicates possible interrelation between child con-
stituents that may be generated in a flexible order.
Our second set of experiments shows that a three-
dimensional annotation strategy strikes the best bal-
ance between bias and variance and achieves the best
accuracy results among all models. Different dimen-
sions provide different sorts of information which
are complementary, resulting in a model that is ca-
pable of generalizing better. The total error reduc-
tion from a plain PCFG is more than 20%, and our
best result is on a par with those achieved for other
languages (e.g., 75% for MSA).
164
Implementation Params FALL F?40 FALL F?40
(h, v, d) WP WP WOP WOP
DIST (0, 0, 0) 66.56 68.20 67.59 69.24
MRK (1, 0, 0) 66.69 68.14 67.93 69.37
PA (0, 1, 0) 68.87 70.48 69.64 70.91
DEF (0, 0, 1) 68.85 69.92 70.42 71.45
PA+MRK (1, 1, 0) 69.97 71.48 70.69 71.98
MRK+DEF (1, 0, 1) 69.46 70.79 71.05 72.37
PA+DEF (0, 1, 1) 71.15 72.34 71.98 72.91
PA+MRK+DEF (1, 1, 1) 72.34 73.63 73.27 74.41
Table 6: Head-Driven Three-Dimensional Extensions: Ac-
curacy results for parsing the devest (section 0)
Implementation Params FALL F?40 FALL F?40
(h, v, d) WP WP WOP WOP
PCFG (?, 0, 0) 65.08 67.31 65.82 68.22
PCFG+PA+DEF (?, 1, 1) 72.26 74.46 72.42 74.52
DIST (0, 0, 0) 66.33 68.79 67.06 69.47
PA+MRK+DEF (1, 1, 1) 72.64 74.64 73.21 75.25
Table 7: PCFG and Head-Driven Unlexicalized Models:
Accuracy Results for parsing the testst (section 12)
Figure 8 shows the FAll(WOP ) results for all
models we implemented. In general, we see that for
parsing MH higher dimensionality is better. More-
over, we see that for all points on the (v, h, 0) plain
the corresponding models on the (v, h, 1) plain al-
ways perform better. We further see that the contri-
bution of the depth dimension to a parent annotated
PCFG can compensate, to a large extent on the lack
of head-dependency information. These accumula-
tive results, then, provide empirical evidence to the
importance of morphological and morpho-syntactic
processes such as definiteness for syntactic analysis
and disambiguation as argued for in section 2.
We confirm our results on the testset and report
in table 7 our results on section 12 of the treebank.
The performance has slightly increased and we ob-
tain better results for our best strategy. We retain the
high error-reduction rate and propose our best result,
75.25% for sentences of length ? 40, as an empiri-
cally established string baseline on the performance
of treebank grammars for MH.
6 Related Work
The MH treebank (Sima?an et al, 2001), a mor-
phologically and syntactically annotated corpus, has
been successfully used for various NLP tasks such as
morphological disambiguation, POS tagging (Bar-
Haim et al, 2007) and NP chunking (Goldberg et
al., 2006). However its use for statistical parsing has
been more scarce and less successful. The only pre-
vious studies attempting to parse MH we know of
are (Sima?an et al, 2001), applying a variation of the
DOP tree-gram model to 500 sentences, and (Tsar-
faty, 2006), using a treebank PCFG in an integrated
system for morphological and syntactic disambigua-
tion.9 The adaptation of state-of-the-art parsing
models to MH is not immediate as the flat variable
structures of phrases are hard to parse and a plen-
tiful of morphological features that would facilitate
disambiguation are not exploited by currently avail-
able parsers. Also, the MH treebank is much smaller
than the ones for, e.g., English (Marcus et al, 1994)
and Arabic (Maamouri and Bies, 2004), making it
hard to apply data-intensive methods such as the all-
subtrees approach (Bod, 1992) or full lexicalization
(Collins, 2003). Our best performing model incor-
porates three dimensions of parametrization and our
best result (75.25%) is similar to the one obtained
by the parser of (Bikel, 2004) for Modern Standard
Arabic (75%) using a fully lexicalized model and
a training corpus about three times as large as our
newest MH treebank.
This work has shown that devising an adequate
baseline for parsing MH requires more than sim-
ple category-splits and sophisticated head-driven ex-
tensions, and our results provide preliminary evi-
dence for the variation in performance of different
parametrization strategies relative to the properties
and structure of a given language. The compari-
son with parsing accuracy for MSA suggests that
parametrizing an orthogonal depth dimension may
be able to compensate, to some extent, on the lack
of sister-dependencies, lexical information, and per-
haps even the lack of annotated data, but establish-
ing empirically its contribution to parsing MSA is a
matter for further research. In the future we intend
to further investigate the significance of the depth di-
mension by extending our models to include more
morphological features, more variation in the pa-
9Both studies acheived between 60%?70% accuracy, how-
ever the results are not comparable to our study because of the
use of different training sets, different annotation conventions,
and different evaluation schemes.
165
Figure 7: All Models: Locating Unlexicalized Parsing Models
in a Three-Dimensional Parametrization Space
Figure 8: All Results: Parsing Results for Unlexicalized Mod-
els in a Three-Dimensional Parametrization Space
rameter space, and applications to more languages.
7 Conclusion
Morphologically rich languages introduce a new di-
mension into the expansion possibilities of a non-
terminal node in a syntactic parse tree. This di-
mension is orthogonal to the vertical (Collins, 2003)
and horizontal (Johnson, 1998) dimensions previ-
ously outlined by Klein and Manning (2003), and
it cannot be collapsed into any one of the previous
two. These additional dependencies exist alongside
the syntactic head dependency and are attested using
morphosyntactic phenomena such as long distance
agreement. We demonstrate using syntactic defi-
niteness in MH that incorporating morphologically
marked features as a third, orthogonal dimension
for annotating syntactic categories is invaluable for
weakening the independence assumptions implicit
in a treebank PCFG and increasing the model?s dis-
ambiguation capabilities. Using a three-dimensional
model we establish a new, stronger, lower bound on
the performance of unlexicalized parsing models for
Modern Hebrew, comparable to those achieved for
other languages (Czech, Chinese, German and Ara-
bic) with much larger corpora.
Tuning the dimensions and value of the parame-
ters for learning treebank grammars is largely an em-
pirical matter, and we do not wish to claim here that
a three-dimensional annotation strategy is the best
for any given language. Rather, we argue that for
different languages different optimal parametriza-
tion strategies may apply. MH is not a free-word-
order language in the canonical sense, and our qual-
itative analysis shows that all dimensions contribute
to the models? disambiguation capabilities. Orthog-
onal dimensions provide complementary informa-
tion that is invaluable for the parsing process to the
extent that the relevant linguistic phenomena license
grammatical structures in the language. Our results
point out a principled way to quantitatively charac-
terizing differences between languages, thus guid-
ing the selection of parameters for the development
of annotated resources, custom parsers and cross-
linguistic robust parsing engines.
Acknowledgments We thank the Knowledge
Center for Processing Hebrew and Dalia Bojan for
providing us with the newest version of the MH
treebank. We are particularly grateful to the devel-
opment team of version 2.0, Adi Mile?a and Yuval
Krymolowsky, supervised by Yoad Winter for con-
tinued collaboration and technical support. We fur-
ther thank Felix Hageloh for allowing us to use the
software resulting from his M.Sc. thesis work. We
also like to thank Remko Scha, Jelle Zuidema, Yoav
Seginer and three anonymous reviewers for helpful
comments on the text, and Noa Tsarfaty for techni-
cal help in the graphical display. The work of the
first author is funded by the Netherlands Organiza-
tion for Scientific Research (NWO), grant number
017.001.271, for which we are grateful.
166
References
S. Abney. 1997. Stochastic Attribute-Value Grammars.
Computational Linguistics, 23 (4):597?618.
R. Bar-Haim, K. Sima?an, and Y. Winter. 2007. Part-of-
Speech Tagging of Modern Hebrew Text. Journal of
Natural Language Engineering.
D. Bikel and D. Chiang. 2000. Two Statistical Parsing
Models Applied to the Chinese Treebank. In Second
Chinese Language Processing Workshop, Hong Kong.
D. Bikel. 2004. Intricacies of Collins? Parsing Model.
Computational Linguistics, 4(30).
R. Bod. 1992. Data Oriented Parsing. In Proceedings of
COLING.
E. Charniak. 1996. Tree-Bank Grammars. In
AAAI/IAAI, Vol. 2, pages 1031?1036.
E. Charniak. 1997. Statistical Parsing with a Context-
Free Grammar and Word Statistics. In AAAI/IAAI,
pages 598?603.
M. Collins, J. Hajic, L. Ramshaw, and C. Tillmann. 1999.
A Statistical Parser for Czech. In Proceedings of ACL,
College Park, Maryland.
M. Collins. 2003. Head-Driven Statistical Models for
Natural Language Parsing. Computational Linguis-
tics, 29(4).
G. Danon. 2001. Syntactic Definiteness in the Grammar
of Modern Hebrew. Linguistics, 6(39):1071?1116.
A. Dubey and F. Keller. 2003. Probabilistic Parsing for
German using Sister-Head Dependencies. In Proceed-
ings of ACL.
Y. Goldberg, M. Adler, and M. Elhadad. 2006. Noun
Phrase Chunking in Hebrew: Influence of Lexical and
Morphological Features. In Proceedings of COLING-
ACL.
F. Hageloh. 2007. Parsing using Transforms over Tree-
banks. Master?s thesis, University of Amsterdam.
M. Johnson. 1998. PCFG Models of Linguistic
Tree Representations. Computational Linguistics,
24(4):613?632.
R. Kaplan and J. Bresnan. 1982. Lexical-Functional
Grammar: A formal system for grammatical represen-
tation. In J. Bresnan, editor, The Mental Representa-
tion of Grammatical Relations, Cambridge, MA. The
MIT Press.
D. Klein and C. Manning. 2003. Accurate Unlexicalized
Parsing. In Proceedings of ACL, pages 423?430.
Y. Krymolowski, Y. Adiel, N. Guthmann, S. Kenan,
A. Milea, N. Nativ, R. Tenzman, and P. Veisberg.
2007. Treebank Annotation Guide. MILA, Knowl-
edge Center for Hebrew Processing.
M. Maamouri and A. Bies. 2004. Developing an Ara-
bic Treebank: Methods, Guidelines, Procedures, and
Tools. In Proceedings of COLING.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,
A. Bies, M. Ferguson, K. Katz, and B. Schasberger.
1994. The Penn Treebank: Annotating Predicate-
Argument Structure.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with Latent Annotations. In Proceedings of
ACL?05.
N. Melnik. 2002. Verb-Initial Constructions in Modern
Hebrew. Ph.D. thesis, Berkeley University of Califor-
nia.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning Accurate, Compact, and Interpretable Tree
Annotation. In Proceedings of ACL-COLING, pages
433?440, Sydney, Australia, July.
D. Prescher. 2005. Head-Driven PCFGs with Latent-
Head Statistics. In In Proceedings of the International
Workshop on Parsing Technologies.
I. A. Sag, T. Wasow, and E. M. Bender. 2003. Syntactic
Theory: A Formal Introduction. CSLI Publications,
address, second edition.
H. Schmidt. 2004. Efficient Parsing of Highly Ambigu-
ous Context-Free Grammars with Bit Vectors. In Pro-
ceedings of COLING, Geneva, Switzerland.
K. Sima?an, A. Itai, Y. Winter, A. Altman, and N. Nativ.
2001. Building a Tree-Bank of Modern Hebrew Text.
In Traitment Automatique des Langues.
R. Tsarfaty. 2006. Integrated Morphological and Syntac-
tic Disambiguation for Modern Hebrew. In Proceed-
ing of SRW COLING-ACL.
S. Wintner. 2000. Definiteness in the Hebrew Noun
Phrase. Journal of Linguistics, 36:319?363.
167
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 214?217,
Paris, October 2009. c?2009 Association for Computational Linguistics
Smoothing fine-grained PCFG lexicons
Tejaswini Deoskar
ILLC
University of Amsterdam
t.deoskar@uva.nl
Mats Rooth
Dept. of Linguistics and CIS
Cornell University
mr249@cornell.edu
Khalil Sima?an
ILLC
University of Amsterdam
k.simaan@uva.nl
Abstract
We present an approach for smoothing
treebank-PCFG lexicons by interpolating
treebank lexical parameter estimates with
estimates obtained from unannotated data
via the Inside-outside algorithm. The
PCFG has complex lexical categories,
making relative-frequency estimates from
a treebank very sparse. This kind of
smoothing for complex lexical categories
results in improved parsing performance,
with a particular advantage in identify-
ing obligatory arguments subcategorized
by verbs unseen in the treebank.
1 Introduction
Lexical scarcity is a problem faced by all sta-
tistical NLP applications that depend on anno-
tated training data, including parsing. One way
of alleviating this problem is to supplement super-
vised models with lexical information from unla-
beled data. In this paper, we present an approach
for smoothing the lexicon of a treebank PCFG
with frequencies estimated from unannotated data
with Inside-outside estimation (Lari and Young,
1990). The PCFG is an unlexicalised PCFG, but
contains complex lexical categories (akin to su-
pertags in LTAG (Bangalore and Joshi, 1999) or
CCG (Clark and Curran, 2004)) encoding struc-
tural preferences of words, like subcategorization.
The idea behind unlexicalised parsing is that the
syntax and lexicon of a language are largely inde-
pendent, being mediated by ?selectional? proper-
ties of open-class words. This is the intuition be-
hind lexicalised formalisms like CCG: here lexical
categories are fine-grained and syntactic in nature.
Once a word is assigned a lexical category, the
word itself is not taken into consideration further
in the syntactic analysis. Fine-grained categories
imply that lexicons estimated from treebanks will
be extremely sparse, even for a language like En-
glish with a large treebank resource like the Penn
Treebank (PTB) (Marcus et al, 1993). Smoothing
a treebank lexicon with an external wide-coverage
lexicon is problematic due to their respective rep-
resentations being incompatible and without an
obvious mapping, assuming that the external lexi-
con is probabilistic to begin with. In this paper, we
start with a treebank PCFG with fine-grained lex-
ical categories and re-estimate its parameters on a
large corpus of unlabeled data. We then use re-
estimates of lexical parameters (i.e. pre-terminal
to terminal rule probabilities) to smooth the orig-
inal treebank lexical parameters by interpolation
between the two. Since the treebank PCFG itself is
used to propose analyses of new data, the mapping
problem is inherently taken care of. The smooth-
ing procedure takes into account the fact that unsu-
pervised estimation has benefits for unseen or low-
frequency lexical items, but the treebank relative-
frequency estimates are more reliable in the case
of high-frequency items.
2 Treebank PCFG
In order to have fine-grained and linguistic lexi-
cal categories (like CCG) within a simple formal-
ism with well-understood estimation methods, we
first build a PCFG containing such categories from
the PTB. The PCFG is unlexicalised (with lim-
ited lexicalization of certain function words, like
in Klein and Manning (2003)). It is created by
first transforming the PTB (Johnson, 1998) in an
appropriate way and then extracting a PCFG from
the transformed trees (Deoskar and Rooth, 2008).
All functional tags in the PTB (such as NP-SBJ,
PP-TMP, etc.) are maintained, as are all empty
categories, making long-distance dependencies re-
coverable. The PCFG is trained on the standard
training sections of the PTB and performs at the
state-of-the-art level for unlexicalised PCFGs, giv-
ing 86.6% f-score on Sec. 23.
214
VP
VB.np
add
NP
four more
Boeings
PP-TMP
by 1994
PP-CLR
to the
two units.
(a) An NP PP subcategorization frame marked on the
verb ?add? as np. Note that the arguments NP and PP-
CLR are part of the subcategorization frame and are
represented locally on the verb but the adjunct PP-
TMP is not.
VP
VBG.s.e.to
seeking
S.e.to
+E-NP+ VP.to
TO
to
VP
avoid..
(b) An S frame on the verb ?seeking?: +E-
NP+ represents the empty subject of the
S. Note that structure internal to S is also
marked on the verb.
VP
Vb.sb
think
SBAR
+C+ S
the consumer
is right
(c) An SBAR frame: +C+ is the
empty complementizer.
Figure 1: Subcategorized structures are marked as features on the verbal POS category.
An important feature of our PCFG is that pre-
terminal categories for open-class items like verbs,
nouns and adverbs are more complex than PTB
POS tags. They encode information about the
structure selected by the lexical item, in effect,
its subcategorization frame. A pre-terminal in our
PCFG consists of the standard PTB POS tag, fol-
lowed by a sequence of features incorporated into
it. Thus, each PTB POS tag can be considered to
be divided into multiple finer-grained ?supertags?
by the incorporated features. These features en-
code the structure selected by the words. We fo-
cus on verbs in this paper, as they are important
structural determiners. A sequence of one or more
features forms the ?subcategorization frame? of a
verb: three examples are shown in Figure 1. The
features are determined by a fully automated pro-
cess based on PTB tree structure and node labels.
There are 81 distinct subcategorization frames for
verbal categories. The process can be repeated for
other languages with a treebank annotated in the
PTB style which marks arguments like the PTB.
3 Unsupervised Re-estimation
Inside-outside (henceforth I-O) (Lari and Young,
1990), an instance of EM, is an iterative estima-
tion method for PCFGs that, given an initial model
and a corpus of unannotated data, produces mod-
els that assign increasingly higher likelihood to
the corpus at each iteration. I-O often leads to
sub-optimal grammars, being subject to the well-
known problem of local maxima, and dependence
on initial conditions (de Marcken, 1995) (although
there have been positive results using I-O as well,
for e.g. Beil et al (1999)). More recently, Deoskar
(2008) re-estimated an unlexicalised PTB PCFG
using unlabeled Wall Street Journal data. They
compared models for which all PCFG parameters
were re-estimated from raw data to models for
which only lexical parameters were re-estimated,
and found that the latter had better parsing results.
While it is common to constrain EM either by
good initial conditions or by heuristic constraints,
their approach used syntactic parameters from a
treebank model to constrain re-estimation of lex-
ical parameters. Syntactic parameters are rela-
tively well-estimated from a treebank, not being as
sparse as lexical parameters. At each iteration, the
re-estimated lexicon was interpolated with a tree-
bank lexicon, ensuring that re-estimated lexicons
did not drift away from the treebank lexicon.
We follow their methodology of constrained
EM re-estimation. Using the PCFG with fine
lexical categories (as described in ?2) as the ini-
tial model, we re-estimate its parameters from an
unannotated corpus. The lexical parameters of
the re-estimated PCFG form its probabilistic ?lex-
icon?, containing the same fine-grained categories
as the original treebank PCFG. We use this re-
estimated ?lexicon? to smooth the lexical proba-
bilities in the treebank PCFG.
4 Smoothing based on a POS tagger : the
initial model.
In order to use the treebank PCFG as an initial
model for unsupervised estimation, new words
from the unannotated training corpus must be in-
cluded in it ? if not, parameter values for new
words will never be induced. Since the treebank
model contains no information regarding correct
feature sequences for unseen words, we assign all
possible sequences that have occurred in the tree-
bank model with the POS tag of the word. We
assign all possible sequences to seen words as
215
well ? although the word is seen, the correct fea-
ture sequence for a structure in a training sentence
might still be unseen with that word. This is done
as follows: a standard POS-tagger (TreeTagger,
(Schmid, 1994)) is used to tag the unlabeled cor-
pus. A frequency table cpos(w, ?) consisting of
words and POS-tags is extracted from the result-
ing corpus, where w is the word and ? its POS
tag. The frequency cpos(w, ?) is split amongst all
possible feature sequences ? for that POS tag in
proportion to treebank marginals t(?, ?) and t(?)
cpos(w, ?, ?) = t(?, ?)t(?) cpos(w, ?) (1)
Then the treebank frequency t(w, ?, ?) and the
scaled corpus frequency are interpolated to get a
smoothed model tpos. We use ?=0.001, giving a
small weight initially to the unlabeled corpus.
tpos(w, ?, ?) = (1? ?)t(w, ?, ?) + ?cpos(w, ?, ?)
(2)
The first term will be zero for words unseen in the
treebank: their distribution in the smoothed model
will be the average treebank distribution over all
possible feature sequences for a POS tag. For
seen words, the treebank distribution over feature
sequence is largely maintained, but a small fre-
quency is assigned to unseen sequences.
5 Smoothing based on EM re-estimation
After each iteration i of I-O, the expected counts
cemi(w, ?, ?) under the model instance at itera-
tion (i ? 1) are obtained. A smoothed treebank
lexicon temi is obtained by linearly interpolating
the smoothed treebank lexicon tpos(w, ?, ?) and a
scaled re-estimated lexicon c?emi(w, ?, ?).
temi(w, ?, ?) = (1??)tpos(w, ?, ?)+?c?emi (w, ?, ?)
(3)
where 0 < ? < 1. The term c?emi(w, ?, ?) is ob-
tained by scaling the frequencies cemi(w, ?, ?) ob-
tained by I-O, ensuring that the treebank lexicon is
not swamped with the large training corpus1.
c?emi(w, ?, ?) = t(?, ?)?
w cemi(w, ?, ?)
cemi(w, ?, ?)
(4)
? determines the relative weights given to the
treebank and re-estimated model for a word. Since
parameters of high-frequency words are likely
to be more accurate in the treebank model, we
parametrize ? as ?f according to the treebank fre-
quency f = t(w, ?).
1Note that in Eq. 4, the ratio of the two terms involving
cemi is the conditional, lexical probability Pemi(w|?, ?).
6 Experiments
The treebank PCFG is trained on sections 0-22 of
the PTB, with 5000 sentences held-out for evalu-
ation. We conducted unsupervised estimation us-
ing Bitpar (Schmid, 2004) with unannotated Wall
Street Journal data of 4, 8 and 12 million words,
with sentence length <25 words. The treebank
and re-estimated models are interpolated with ? =
0.5 (in Eq. 3). We also parametrize ? for treebank
frequency of words ? optimizing over a develop-
ment set gives us the following values of ?f for
different ranges of treebank word frequencies.
if t(w, ?) <= 5 , ?f = 0.5
if 5 < t(w, ?) <= 15 , ?f = 0.25
if 15 < t(w, ?) <= 50 , ?f = 0.05
if t(w, ?) > 50 , ?f = 0.005
(5)
Evaluations are on held-out data from the PTB
by stripping all PTB annotation and obtaining
Viterbi parses with the parser Bitpar. In addition
to standard PARSEVAL measures, we also eval-
uate parses by another measure specific to sub-
categorization2 : the POS-tag+feature sequence on
verbs in the Viterbi parse is compared against the
corresponding tag+feature sequence on the trans-
formed PTB gold tree, and errors are counted. The
tag-feature sequence correlates to the structure se-
lected by the verb, as exemplified in Fig. 1.
7 Results
There is a statistically significant improvement3
in labeled bracketing f-score on Sec. 23 when
the treebank lexicon is smoothed with an EM-re-
estimated lexicon. In Table 1, tt refers to the base-
line treebank model, smoothed using the POS-
tag smoothing method (from ?4) on the test data
(Sec. 23) in order to incorporate new words from
the test data4. tpos refers to the initial model for
re-estimation, obtained by smoothed the treebank
model with the POS-tag smoothing method with
the large unannotated corpus (4 million words).
This model understandably does not improve over
tt for parsing Sec. 23. tem1,?=0.5 is the model
obtained by smoothing with an EM-re-estimated
model with a constant interpolation factor ? =
0.5. This model gives a statistically significant im-
provement in f-score over both tt and tpos. The
last model tem1,?f is obtained by smoothing with
2PARSEVAL measures are known to be insensitive to sub-
categorization (Carroll et al, 1998).
3A randomized version of a paired-sample t-test is used.
4This is always done before parsing test data.
216
tt tpos tem1,?=0.5 tem1,?f
Recall 86.48 86.48 86.72 87.44
Precision 86.61 86.63 86.95 87.15
f-score 86.55 86.56 *86.83 *87.29
Table 1: Labeled bracketing F-score on section 23.
an interpolation factor as in Eq. 5 : this is the best
model with a statistically significant improvement
in f-score over tt, tpos and tem1,?=0.5.
Since we expect that smoothing will be advanta-
geous for unseen or low-frequency words, we per-
form an evaluation targeted at identifying struc-
tures subcategorized by unseen verbs. Table 2
shows the error reduction in identifying subcat.
frames in Viterbi parses, of unseen verbs and also
of all verbs (seen and unseen) in the testset. A
breakup of error by frame type for unseen verbs is
also shown (here, only frames with >10 token oc-
currences in the test data are shown). In all cases
(unseen verbs and all verbs) we see a substantial
error reduction. The error reduction improves with
larger amounts of unannotated training data.
8 Discussion and Conclusions
We have shown that lexicons re-estimated with I-
O can be used to smooth unlexicalised treebank
PCFGs, with a significant increase in f-score even
in the case of English with a large treebank re-
source. We expect this method to have more
impact for languages with a smaller treebank or
richer tag-set. An interesting aspect is the substan-
tial reduction in subcategorization error for un-
seen verbs for which no word-specific information
about subcategorization exists in the unsmoothed
or POS-tag-smoothed lexicon. The error reduction
in identifying subcat. frames implies that some
constituents (such as PPs) are not only attached
correctly but also identified correctly as arguments
(such as PP-CLR) rather than as adjuncts.
There have been previous attempts to use POS-
tagging technologies (such as HMM or maximum-
entropy based taggers) to enhance treebank-
trained grammars (Goldberg et al (2009) for He-
brew, (Clark and Curran, 2004) for CCG). The re-
estimation method we use builds full parse-trees,
rather than use local features like taggers do, and
hence might have a benefit over such methods. An
interesting option would be to train a ?supertag-
ger? on fine-grained tags from the PTB and to su-
pertag a large corpus to harvest lexical frequen-
Frame # tokens %Error %Error %Error
(test) tpos tem1 Reduc.
All unseen (4M words) 1258 33.47 22.81 31.84
All unseen (8M words) 1258 33.47 22.26 33.49
All unseen (12M words) 1258 33.47 21.86 34.68
transitive 662 23.87 18.73 21.52
intransitive 115 38.26 33.91 11.36
NP PP-CLR 121 34.71 32.23 7.14
PP-CLR 73 27.4 20.55 25
SBAR 124 12.1 12.1 0
S 12 83.33 58.33 30
NP NP 10 90 80 11.11
PRT NP 21 38.1 33.33 12.5
s.e.to (see Fig.1b) 50 16 12 25
NP PP-DIR 11 63.64 54.55 14.28
All verbs (4M) 11710 18.5 16.84 8.97
Table 2: Subcat. error for verbs in Viterbi parses.
cies. This would form another (possibly higher)
baseline for the I-O re-estimation approach pre-
sented here and is the focus of our future work.
References
S. Bangalore and A. K. Joshi. 1999. Supertagging: An Ap-
proach to Almost Parsing. Computational Linguistics,
25:237?265.
F. Beil, G. Carroll, D. Prescher, S. Riezler, and M. Rooth.
1999. Inside-outside estimation of a lexicalized PCFG for
German. In ACL 37.
J. Carroll, G. Minnen, and E. Briscoe. 1998. Can subcate-
gorization probabilities help parsing. In 6th ACL/SIGDAT
Workshop on Very Large Corpora.
S. Clark and J. R. Curran. 2004. The Importance of Supertag-
ging for Wide-Coverage CCG Parsing. In 22nd COLING.
Carl de Marcken. 1995. On the unsupervised induction of
Phrase Structure grammars. In Proceedings of the 3rd
Workshop on Very Large Corpora.
T. Deoskar. 2008. Re-estimation of Lexical Parameters for
Treebank PCFGs. In 22nd COLING.
Tejaswini Deoskar and Mats Rooth. 2008. Induction of
Treebank-Aligned Lexical Resources. In 6th LREC.
Y. Goldberg, R. Tsarfaty, M. Adler, and M. Elhadad. 2009.
Enhancing Unlexicalized Parsing Performance using a
Wide Coverage Lexicon, Fuzzy Tag-set Mapping, and
EM-HMM-based Lexical Probabilities. In EACL-09.
M. Johnson. 1998. PCFG models of linguistic tree represen-
tations. Computational Linguistics, 24(4).
D. Klein and C. Manning. 2003. Accurate unlexicalized pars-
ing. In ACL 41.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the Inside-Outside algo-
rithm. Computer Speech and Language, 4:35?56.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging Us-
ing Decision Trees. In International Conference on New
Methods in Language Processing.
H. Schmid. 2004. Efficient Parsing of Highly Ambiguous
CFGs with Bit Vectors. In 20th COLING.
217
 	
 

ffProceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1928?1939, Dublin, Ireland, August 23-29 2014.
Latent Domain Translation Models in Mix-of-Domains Haystack
Hoang Cuong and Khalil Sima?an
Institute for Logic, Language and Computation
University of Amsterdam
Science Park 107, 1098 XG Amsterdam, The Netherlands
Abstract
This paper addresses the problem of selecting adequate training sentence pairs from a mix-of-
domains parallel corpus for a translation task represented by a small in-domain parallel corpus.
We propose a novel latent domain translation model which includes domain priors, domain-
dependent translation models and language models. The goal of learning is to estimate the
probability of a sentence pair in mix-domain corpus to be in- or out-domain using in-domain
corpus statistics as prior. We derive an EM training algorithm and provide solutions for esti-
mating out-domain models (given only in- and mix-domain data). We report on experiments in
data selection (intrinsic) and machine translation (extrinsic) on a large parallel corpus consisting
of a mix of a rather diverse set of domains. Our results show that our latent domain invitation
approach outperforms the existing baselines significantly. We also provide analysis of the merits
of our approach relative to existing approaches.
Large parallel corpora are important for training statistical MT systems. Besides size, the relevance
of a parallel training corpus to the translation task at hand can be decisive for system performance, cf.
(Axelrod et al., 2011; Koehn and Haddow, 2012). In this paper we look at data selection where we
have access to a large parallel data repository C
mix
, representing a rather varied mix of domains, and
we are given a sample of in-domain parallel data C
in
, exemplifying a target translation task. Simply
concatenating C
in
with C
mix
does not always deliver best performance, because including irrelevant
sentences might be more harmful than beneficial, cf. (Axelrod et al., 2011). To make the best of
available data, we must select sentences from C
mix
for their relevance to translating sentences from C
in
.
Axelrod et al. (2011) and follow-up work, e.g., (Haddow and Koehn, 2012; Koehn and Haddow,
2012), select sentence pairs in C
mix
using the cross-entropy difference between in- and mix-domain lan-
guage models, both source and target sides, a modification of the Moore and Lewis method (Moore and
Lewis, 2010). In the translation context, however, often a source phrase has different senses/translations
in different domains, which cannot be distinguished with monolingual language models. The depen-
dence of translation choice on domain suggests that the word alignments themselves can better be con-
ditioned on domain information. However, in the data selection setting, corpus C
mix
often does not
contain useful domain markers, and C
in
contains only a small sample of in-domain sentence pairs.
In this paper we present a latent domain translation model which weights every sentence pair ?f , e? ?
C
mix
with a probability P (D | f , e) for being in-domain (D
1
) or out-domain (D
0
). Our model defines
P (e, f) =
?
D?{D
1
,D
0
}
P (D)P (e, f | D), using a latent domain variable D ? {D
0
, D
1
}. Using bi-
directional translation models, this leads to a domain prior P (D), domain-dependent translation models
P
t
(? |?, D) and language models P
lm
(? | D) as in Equation 1:
P (e, f | D) =
1
2
? {P
lm
(e | D)P
t
(f | e, D) + P
lm
(f | D)P
t
(e | f , D)} (1)
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1928
For efficiency we assume IBM Model I alignments a and translation tables t(?), e.g., P
t
(e |f , D) ?
?
a
?
i
t(e
i
|f
a
i
, D). Language models (LMs) P
lm
are trained separately, albeit one problem not ad-
dressed by earlier work is how to train out-domain LMs given only in- and mix-domain data?
In our model, initially both the translation and LM probabilities estimated from C
in
serve as priors for
weighting sentence pairs in C
mix
as being more relevant for in-domain translation than not. This initial
weighting reveals pseudo out-domain data in C
mix
, which we use to train out-domain language models
as well as initialize out-domain word alignment tables.
1
With these sharpened translation and language
models, training commences using a version of EM (Dempster et al., 1977). Because the potentially
relevant data in C
mix
might be a superset of any in-domain data, the estimates from C
in
serve merely
as initial model estimates. Metaphorically, iterative EM training resembles party invitations
on social networks (hence, the Invitation model): if initially in/out-domain sentence pairs (the hosts)
invite some sentence pairs from C
mix
, in the next iteration the new pseudo in/out-domain sentences
help invite more sentence pairs. In EM, sentence pairs receive weighted, rather than absolute, invitations
from in- and out-domain models.
We present extensive experiments on a rather difficult selection task exploiting a large mix-domain
corpus of 4.61M sentence pairs. Initially we conduct intrinsic evaluation on the mix-domain corpus
where we also hide in-domain data and seek to retrieve it. Subsequently we conduct full MT experiments
over the task. The results show that our Invitation model gives far better selections as well as translation
performance than the baseline trained on the large data C
mix
.
1 Invitation models of weighting and selection
By now training data selection from large mix-domain data is an accepted necessity, e.g., (Axelrod et
al., 2011; Gasc?o et al., 2012; Haddow and Koehn, 2012; Banerjee et al., 2012; Irvine et al., 2013).
Data selection has a different (but complementary) goal than domain adaptation, which aims at adapting
an existing out-domain system by focusing on, e.g., translation model (Koehn and Schroeder, 2007;
Foster and Kuhn, 2007; Sennrich, 2012), reordering model (Chen et al., 2013) and/or language model
adaptation (Eidelman et al., 2012). Our setting is in line with data selection approaches (Moore and
Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), and is somewhat related to phrase pair weighting
(Matsoukas et al., 2009; Foster et al., 2010). In this paper we explicitly draw attention to the special
case of a mix-domain parallel corpus consisting of a large and rather diverse set of domains.
Our model assigns to every sentence pair ?f , e? ? C
mix
a probability as in Equation 2:
P (D | f , e) =
P (f , e, D)
?
D?{D
1
,D
0
}
P (f , e, D)
(2)
P (f , e, D) =
1
2
? P (D)? {P
lm
(e | D)P
t
(f | e, D) + P
lm
(f | D)P
t
(e | f , D)}
Viewed as learning two latent corpora C
1
and C
0
, the task is to assign every ?f , e? ? C
mix
an expected
count P (D
x
| f , e) that it is in C
x
? {C
0
, C
1
}. Next we discuss the model components each in turn.
The domain-dependent translation models P
t
(? |D) can be viewed as modeling the probability that e
translates as f in domain D ? {D
0
, D
1
}. Given f = f
1
, f
2
, . . . , f
m
and e = e
1
, e
2
, . . . , e
l
, we assume
(hidden) alignments a = a
1
, a
2
, . . . , a
m
akin to IBM Model I (Brown et al., 1993):
P
t
(f ,a | e, D) =

(l + 1)
m
?
m
j=1
t(f
j
|e
a
j
, D) (3)
P
t
(f | e, D) =
?
a
P
t
(f ,a|e, D) =

(l + 1)
m
?
m
j=1
?
l
i=0
t(f
j
|e
i
, D). (4)
1
Earlier work on data selection exploits the contrast between in-domain and mix-domain instead of (pseudo) out-domain
language models. However, the mix-domain language models trained on a mix of rather diverse set of domains could be
considered kind of wide-coverage, which makes for a rather weak contrast with the in-domain language models.
1929
where t(f
j
|e
a
j
, D) is the domain-dependent lexical probability of f
j
given e
a
j
with respect to D. One
crucial aspect about model inspired by IBM-Model-I is that P
t
(f | e, D) can be estimated efficiently, as
in Equation 4. This makes the training particularly efficient as detailed in Section 2
The in-/out-domain source and target language models are not the same as in previous work, e.g.,
(Axelrod et al., 2011), which employ in-/mix-domain language models. This makes explicit the difficulty
in finding data to train out-domain language models, and we present a solution in Section 2.
The domain priors P (D
1
) and P (D
0
) represent the percentage of the pairs that are in-/ and out
domain respectively in C
mix
learned by our model. Their estimate during training might be a reasonable
selection cut-off threshold. However, we found that it is not entirely clear whether these cut-off criteria
might exclude other relevant/irrelevant pairs that are not exactly in-domain. We leave this extension for
future work.
2
Finally, it should be noted that the domain-dependent word alignment model, t(f |e,D) is a gener-
alization of the standard (domain-independent) word alignment model, t(f |e), in which, t(f |e,D) =
t(f |e)t(D|f,e)?
f
t(f |e)t(D|f,e)
. Here, t(D|f, e) can be thought of as the latent word-relevance models, i.e., the proba-
bility that a word pair is relevant for in- (D
1
) or out-domain (D
0
). Empirical results (beyond the scope
of this work) show that training the latent in-domain alignment model, t(f |e,D
1
) often gives better
translation systems than training the standard (domain-independent) alignment model, t(f |e).
2 Training
With all language models trained separately, our selection model can be viewed to have two sets of
domain-dependent parameters ? = {?
D
0
,?
D
1
}. The parameters ?
D
consist of the domain-dependent
lexical parameters (e.g., t
?
D
(f |e,D), t
?
D
(e|f,D)) and the domain prior parameter (e.g., P
?
D
(D)).
Our training procedure seeks the parameters ? that maximize the log-likelihood of C
mix
:
L =
?
f ,e
logP
?
(f , e) =
?
f ,e
log
?
D
?
a
P
?
D
(a, D, f , e) (5)
Because of the latent variables a and D, there is no closed form solution and the model is fit using the
EM algorithm (Dempster et al., 1977). EM can be seen to maximize L via block-coordinate ascent on a
lower bound F(q,?) using an auxiliary distribution over the latent variables q(a, D|f , e)
L ? F(q,?) =
?
f ,e
?
D
?
a
q(a, D | f , e) log
P
?
D
(a, D, f , e)
q(a, D | f , e)
(6)
where the inequality results from log being concave and Jensen?s inequality. We rewrite the Free energy
F(q,?) (Neal and Hinton, 1999) as follows:
F(q,?) =
?
f ,e
?
D
?
a
q(a, D | f , e) log
P
?
D
(a, D, f , e)
q(a, D | f , e)
=
?
f ,e
?
D,a
q(a, D | f , e) log
P
?
D
(a, D | f , e)
q(a, D | f , e)
+
?
f ,e
?
D,a
q(a, D | f , e) logP
?
(f , e)
=
?
f ,e
logP
?
(f , e)?KL[q(a, D | f , e) || P
?
D
(a, D|f , e)] (7)
where KL[?||?] is the KL-divergence. To find q
?
(a, D|f , e) that maximizes F(q,?):
q
?
(a, D|f , e) = argmax
q(a,D|f ,e)
F(q,?) = argmin
q(a,D|f ,e)
KL[q(a, D|f , e)||P
?
D
(a, D|f , e)]
= P
?
D
(a, D|f , e) = P
?
D
(D|f , e)P
?
D
(a|f , e, D). (8)
2
We especially thank an anonymous reviewer who gave valuable comments related to this point.
1930
Here
P
?
D
(a|f , e, D) =
P
?
D
(f ,a|e,D)
P
?
D
(f |e,D)
=
?
m
j=1
t(f
j
|e
a
j
, D)
?
m
j=1
?
l
i=0
t(f
j
|e
i
, D)
(9)
The distribution q
?
(a, D|f , e) together with q
?
(D|f , e) =
?
a
q
?
(a, D|f , e) = P
?
D
(D|f , e) can be
used to softly fill in the values of a and D respectively to estimate model parameters.
We now state our derived EM update formulas. We use the notation P
(c)
and t
(c)
for current iteration
estimates, and P
(+)
and t
(+)
for the re-estimates. We denote the expected counts that e aligns to f in
the translation (f |e) with respect to a domain D with c(f |e; f , e, D). Similarly, we denote the expected
count of (f |e) with respect to a domain D by c(D; f , e).
E-step ?D ? {D
0
, D
1
} do
c(D; f , e) = P
(c)
(D | f , e)
c(f |e; f , e, D) = P
(c)
(D | f , e)
t
(c)
(f | e,D)
?
l
i=0
t
(c)
(f | e
i
, D)
?
m
j=1
?(f, f
j
)
?
l
i=0
?(e, e
i
)
M-step ?D ? {D
0
, D
1
} do
t
(+)
(f |e,D) =
?
f ,e
c(f |e; f , e, D)
?
f
?
f ,e
c(f |e; f , e, D)
P
(+)
(D) =
?
f ,e
c(D; f , e)
?
D
?
f ,e
c(D; f , e)
To re-estimate P (D | f , e) we substitute the M-step estimates into Equations 3, 4 and 2. We initial-
ize translation tables t(f |e,D
1
) and t(e|f,D
1
) with non-zero estimates obtained from applying IBM
model I to in-domain corpus C
in
.
3
Before EM training starts we must train the LMs. The in-domain
LMs P
lm
(e|D
1
) and P
lm
(f |D
1
) are trained on the source and target sides of C
in
respectively. For the
out-domain LMs P
lm
(e|D
0
) and P
lm
(f |D
0
) we need an out-domain data set to train them. It would also
be reasonable to use the set to train the out-domain tables, t(? | ?, D
0
). This raises an hitherto unattended
question regarding how to construct such an out-domain data set.
Inspired by burn-in in sampling, initially we isolate all LMs from our model to train the translation
models for a single EM iteration; we initialize the model with a translation table constructed on C
in
and uniform otherwise. Using the re-estimates, we score sentence pairs in C
mix
with P (D
1
|f , e) and
select a burn-in subset of smallest scoring pairs as pseudo out-domain data which can be used to train
P
lm
(e|D
0
) and P
lm
(f |D
0
). Choosing the optimal size of this subset is difficult, but in practice, we
usually choose a subset that has similar size (number of words) to the given in-domain corpus. The
rationale behind this choice is to avoid the risk that pseudo out-domain models would dominate the in-
domain models during further training. We observe that choosing the same size for a pseudo out-domain
corpus is not guaranteed to always give optimal performance, and this point deserves further study.
Finally, once the domain-dependent LMs have been trained, the domain-dependent LM probabilities
stay fixed during EM. Crucially, it is important to scale the probabilities of the four LMs to make them
comparable: we normalize the probability that a LM assigns to a sentence by the total probability this
LM assigns to all sentences in C
mix
.
3 Experimental setting
We carry out experiments in data selection (intrinsic) as well as in machine translation (extrinsic). We
build an English-Spanish mix-domain corpus consisting of a large and rather varied set of domains (a
3
Note that in practice, we usually use only one iteration to train IBM Model I. To simplify the implementation, we ignore
factor

(l+1)
m
in the model (Equation 3), which serves a minor role. It should be also noted that we set a (small) threshold,
e.g., t(?|?, ?) = 0.0001 for all word pairs that do not occur in the in-domain corpus to avoid over-fitting.
1931
haystack) in a way that allows us to directly measure selection quality. Starting out from a general-
domain corpus C
g
consisting of 4.51M sentence pairs, collected from multiple resources including
EuroParl (Koehn, 2005), Common Crawl Corpus, UN Corpus, News Commentary, TAUS Software,
TAUS Hardware, and TAUS Pharmacy, and a 177K in-domain (TAUS Legal) sentence pairs.
We create C
mix
by selecting an arbitrary 100K pairs of in-domain set and adding them to C
g
; the
remaining 77K in-domain pairs constitute C
in
. We think of this as hiding in-domain data in C
mix
so
we can evaluate our ability to retrieve it; in this setting we can evaluate selection directly using pseudo-
precision/recall defined as the percentage of selected in-domain pairs to the total selected or to the hidden
100K pairs respectively.
Table 1 summarizes the data and the translation task. It should be noted that a mix-domain corpus,
that contains a large and rather varied set of domains, frequently contains subsets with a vocabulary that
is close to the in-domain adaptation task; in this case, e.g., Europarl and TAUS Legal share big portions
of their source vocabulary, whereas their translations could differ. This makes the selection task far more
difficult than assumed by previous approaches as we will show next.
Task Corpora English Spanish
Mix-Domain Corpus (4.51M sents) 125, 339, 057 139, 655, 311
TAUS Legal
In-Domain Corpus (77K sents) 1, 555, 342 1, 733, 370
Dev (2K sents) 27, 983 30, 501
Test (2K sents) 45, 736 48, 999
Table 1: The data preparation - training, dev and testing corpora (size in words). Note that the dev set
contains sentences of 10-25 words, while the test set contains sentences that vary substantially in length,
from 5-10 words up to 45-50 words.
Our Invitation model takes 3 EM-iterations to train.
4
We then weigh sentence pairs under our model
with P (D
1
| e, f). We test various baseline models, including the bilingual cross-entropy difference
model, and the two cross-entropy difference models (on the source language and on the target lan-
guage).
5
We report pseudo-precision/recall at the sentence-level using a range of cut-off criteria for
selecting the top scoring instances in the mix-domain corpus.
We use Moses (Koehn et al., 2007) with GIZA++ (Och and Ney, 2003) and k-best batch MIRA
(Cherry and Foster, 2012). Final MT systems use the same non-adapted language models trained on
2.2M English Europarl sentences plus 248.8K sentences from News Commentary Corpus (WMT 2013).
We report BLEU (Papineni et al., 2002), METEOR 1.4 (Denkowski and Lavie, 2011) and TER
(Snover et al., 2006). Statistical significance uses 95% confidence intervals using paired bootstrap
re-sampling (Press et al., 1992; Koehn, 2004). The k-best batch MIRA optimizer (Cherry and Foster,
2012) was run at least three times to optimize any SMT system to avoid instability (Clark et al., 2011).
6
4 Results
Table 2 presents the results showing substantial improvement in selection performance compared to all
the baselines. Subsequently we build SMT systems over the selected subsets. We report the transla-
tion yielded by these systems over the task in Table 2 as well. It can be easily seen that the baseline
approaches that simply train on in- and mix-domain data do not work that well for a difficult selection
task from a mix-domain corpus consisting of a large and rather diverse set of domains. The SMT sys-
4
To train the LM probs, we construct interpolated 4-gram Kneser-Ney language models using BerkeleyLM (Pauls and
Klein, 2011). This setting for training language models is used for all experiments in this work.
5
The script we use to train these models is developed by Luke Orland and available at: https://github.com/
lukeorland/moore\_and\_lewis\_data\_selection.
6
Note that metric scores for the systems are averages over multiple runs.
1932
Cut-off Model
In-domain
Pairs
pseudo-
Precision
pseudo-
Recall
BLEU METEOR TER
50K
CE Difference (source side) 370 0.74 0.37 20.5 28.0 62.3
CE Difference (target side) 375 0.75 0.38 19.3 26.8 63.3
Bilingual CE Difference 413 0.83 0.41 18.7 26.3 64.3
Invitation 19156 38.31 19.16 36.5 36.4 47.1
100K
CE Difference (source side) 592 0.59 0.59 24.8 30.8 57.8
CE Difference (target side) 572 0.57 0.57 22.1 29.7 60.1
Bilingual CE Difference 649 0.65 0.65 23.1 30.0 58.9
Invitation 30474 30.47 30.47 37.1 36.9 47.0
150K
CE Difference (source side) 753 0.50 0.75 26.4 32.0 56.2
CE Difference (target side) 742 0.49 0.74 23.9 31.2 58.8
Bilingual CE Difference 793 0.53 0.79 24.4 30.9 58.1
Invitation 38424 25.62 38.42 37.1 37.0 46.7
200K
CE Difference (source side) 874 0.44 0.87 26.6 32.4 56.0
CE Difference (target side) 888 0.44 0.88 25.8 32.1 57.2
Bilingual CE Difference 932 0.93 0.65 25.7 32.0 57.0
Invitation 44392 22.17 44.39 37.5 37.4 46.2
250K
CE Difference (source side) 994 0.40 0.99 27.3 32.8 55.4
CE Difference (target side) 997 0.40 0.10 26.3 32.4 56.3
Bilingual CE Difference 1062 0.42 1.06 26.6 32.7 55.6
Invitation 49419 19.77 49.42 37.3 37.3 46.1
300K
CE Difference (source side) 1122 0.37 1.12 28.2 33.4 54.5
CE Difference (target side) 1093 0.36 1.09 26.4 32.7 56.0
Bilingual CE Difference 1169 0.39 1.17 27.8 33.3 54.9
Invitation 53892 17.96 53.89 37.7 37.5 46.0
Table 2: Systematic comparison between selection models.
tems trained on the selection of our model perform significantly and consistently better (with p-value
= 0.0001 for all cases) than the others trained on the selection of the baselines.
Sentences
Bilingual CE Difference
1
by assisting in the placement and financing of used and end-of-lease aircraft , atr asset management has helped broaden
atr ?s customer base , notably in emerging markets , by providing quality reconditioned aircraft at attractive prices and
has helped maintain residual values of used aircraft .
al participar en la colocaci?on y en la financiaci?on de los aviones usados al final del per??odo de arrendamiento , atr
gesti?on de activos ha podido ampliar la base de su clientela , en particular en los pa??ses de econom??as emergentes , al
proporcionar aparatos entregados en buen estado a precios interesantes y ha contribuido a mantener el valor residual
de los aviones usados .
2
in contrast , recent improvements in western europe are not expected to be reversed significantly .
en cambio no se espera que las recientes mejoras en europa occidental se inviertan significativamente .
3
creating xml file ...
creando el archivo xml ...
Invitation Model
1
as she has said , the harmonisation of the requirements for information to appear on the invoice will mean that traders
operating within the single market will be subject to a single legislation , while until now they have had to know , comply
with and apply fifteen different legislations .
como ella ha dicho , la armonizaci?on de los requisitos de informaci?on que deben constar en la factura permitir?a a los
comerciantes que operen en el mercado interior sujetarse a una sola legislaci?on , mientras que hasta ahora ten??an que
conocer , sujetarse y aplicar quince legislaciones diferentes .
2
the solicitation documents shall specify the estimated period of time following dispatch of the notice of acceptance that
will be required to obtain the approval .
en el pliego de condiciones se indicar?a el plazo de tiempo previsto , a partir de la expedici?on del aviso de aceptaci?on ,
que ser?a requerido para obtener la aprobaci?on .
3
there is no doubt that disadvantages will result for the consumer and for the manufacturer of branded goods , for example
with regard to consumer health protection .
ello generar?a , sin duda alguna , desventajas para el consumidor y el productor de art??culos de marca , entre otros
aspectos tambi?en en lo que se refiere a la protecci?on de la salud del consumidor .
Table 3: Top pairs from mix-domain corpus with highest scores according to models.
Table 3 presents some random top ranked sentence pairs from the bilingual cross-entropy difference
1933
Cut-off: 50K Cut-off: 100K Cut-off: 200K
Model English Spanish English Spanish English Spanish
CE Difference (source side) 8.65 8.70 11.92 12.21 15.50 16.22
CE Difference (target side) 8.14 10.09 11.61 14.13 15.45 18.50
Bilingual CE Difference 7.03 8.16 10.38 11.96 14.34 16.43
Invitation 40.16 44.70 37.30 41.59 34.32 38.32
Table 4: Average words in selected sentences.
model against our Invitation model for the task. This shows clearly more relevant pairs for our selection
model than for the baselines. It should be noted that the baseline models tend to prefer shorter sentences,
while our model suffers less from this kind of bias. Table 4 presents the average length (in words) of
selected sentences selected by different models over various cut-offs.
Cut-off Model
In-domain
Pairs
pseudo-
Precision
pseudo-
Recall
BLEU METEOR TER
300K
Without Translation Model 34156 11.39 34.16 35.8 36.6 47.3
Without Language Model 51991 17.33 51.99 37.4 37.4 46.6
Full model 53892 17.96 53.89 37.7 37.5 46.0
Table 5: Experiments exploring the roles of individual components in our model.
Which component type (language or translation models) contributes more to performance? We neu-
tralize each component in turn and build a selection system with the remaining model parameters. Ta-
ble 5 shows translation models are crucial for performance, while domain-dependent LMs make a small,
yet noteworthy contribution. It should also be noted that using the LMs derived separately from in- and
out-domain data yields far better performance than the LMs derived from in- and mix-domain data for
this task.
System Phrases BLEU METEOR TER
Large data C
mix
236.74M 36.8 37.2 47.1
Subset of 300K 22.47M 37.7 37.5 46.0
Table 6: Translation accuracy comparison.
Finally, we compare a system trained on a selection of the top scored 300K sentences to a baseline
large-scale SMT system trained on C
mix
(4.61M sentences). The baseline trained on C
mix
works with
236.74M phrase pairs, whereas the Invitation trained system employs a small table of 22.47M phrases.
Tabel 6 shows the results. It is interesting that the small MT system trained by Invitation performs
significantly better (with p-value = 0.0001 for all metrics) than the large-scale system baseline trained
on all of C
mix
.
Input
cada estado miembro supervisar?a la categor??a cient??fica de la evaluaci?on y las actividades de los miembros
de los comit?es y de los expertos que haya designado, pero se abstendr?a de darles instrucciones incompatibles
con las funciones que les competen.
Reference
each member state shall monitor the scientific level of the evaluation carried out and supervise the activities
of members of the committees and the experts it nominates, but shall refrain from giving them any instruction
which is incompatible with the tasks incumbent upon them.
Large C
mix
each member state will oversee the category scientific assessment and the activities of members of the com-
mittees and experts which designated, but abstain of instruct incompatible with their regulatory functions.
Subset 300K
each member state will monitor the scientific category of the evaluation and the activities of the members
of the committees and of experts who has designated, but refrain from giving them instructions incompatible
with the required functions assumed.
Table 7: Translation example yielded by systems.
To give a sense of the improvement in translation, we present an example in Table 7. The example
is indeed illuminating because it shows the difference in choice between the mix-domain system and
1934
our selection-trained system. The example shows different translation pairs: ?supervisar?a-monitor? vs.
?supervisar?a-oversee?, ?evaluaci?on-evaluation? vs. ?evaluaci?on-assessment?, and ?abstendr?a de-refrain
from? vs. ?abstendr?a de-abstain?. Table 8 presents phrase table entries, i.e., p(e | f) and p(f | e), for the
pairs of words in each system.
supervisar
?
a evaluaci
?
on abstendr
?
a de
System Entry monitor oversee evaluation assessment refrain from abstain
Large data C
mix
?(e|f) 0.002 0.020 0.579 0.429 0.002 0.013
?(f |e) 0.119 0.081 0.391 0.403 0.014 0.060
Subset of 300K
?(e|f) 0.012 0.024 0.487 0.357 0.015 ?
?(f |e) 0.203 0.072 0.338 0.417 0.143 ?
Table 8: Phrase entry examples. Note that the system trained on the subset of top 300K pairs of sentences
does not contain the phrase pair ?refrain from-abstain?.
5 Final Machine Translation experiments: Putting all data together
For final adaptation evaluations we follow (Koehn and Schroeder, 2007; Nakov, 2008) and (Axelrod et
al., 2011; Sennrich, 2012), by passing multiple phrase tables directly to the Moses decoder and tuning
a system using these different tables together. Table 9 presents the result, showing the consistent im-
provement of adaptation with Invitation model compared to the baselines (with p-value = 0.0001 for all
cases) over the mixture data C
mix
.
Data System BLEU METEOR TER
In-domain 36.66 37.19 44.76
50K
+ CE Difference (source side) 37.1 36.7 48.1
+ CE Difference (target side) 37.1 36.6 48.2
+ Bilingual CE Difference 37.1 36.6 48.2
+ Invitation 38.0 37.2 47.3
100K
+ CE Difference (source side) 37.3 36.8 47.9
+ CE Difference (target side) 37.2 36.8 48.0
+ Bilingual CE Difference 37.2 36.8 48.0
+ Invitation 38.4 37.4 46.9
150K
+ CE Difference (source side) 37.1 36.9 48.2
+ CE Difference (target side) 37.3 36.9 47.9
+ Bilingual CE Difference 37.0 36.8 48.1
+ Invitation 38.6 37.5 46.6
200K
+ CE Difference (source side) 37.3 36.9 47.7
+ CE Difference (target side) 37.3 36.9 47.9
+ Bilingual CE Difference 37.3 36.9 47.8
+ Invitation 38.4 37.6 46.7
250K
+ CE Difference (source side) 37.4 36.9 47.7
+ CE Difference (target side) 37.3 37.0 47.7
+ Bilingual CE Difference 37.3 37.0 47.8
+ Invitation 38.6 37.7 46.5
300K
+ CE Difference (source side) 37.3 37.0 47.8
+ CE Difference (target side) 37.1 37.0 48.0
+ Bilingual CE Difference 37.3 36.9 47.8
+Invitation 38.9 37.9 46.3
Table 9: Translation results from our domain-adapted SMT systems.
Finally, we also test the adaptation evaluations between the system trained on the small selection of
top 300K sentences against the large-scale SMT system trained on C
mix
when combined with the in-
domain trained system. Table 10 presents the results, revealing comparable translation performance,
although they are trained on data sets that are significantly different in size.
1935
System BLEU METEOR TER
In-domain + Large data C
mix
39.0 38.0 46.3
In-domain + Subset of 300K 38.9 37.9 46.3
Table 10: Translation results from our domain-adapted SMT system and the large-scale SMT system.
Note that the baseline is slightly better than our domain-adapted SMT system under BLEU and ME-
TEOR, however, not statistically significant.
6 Final notes on mix-domain data selection
The specific data selection scenario studied in this paper brings up different aspects that did not receive
(sufficient) attention in earlier work on data selection and domain adaptation:
? The mix-domain parallel corpus C
mix
contains a large variety of domains that overlap and but also
differ in lexical choice and translation. This is radically different from the in-/out-domain setting
usually assumed in adaptation and constitutes a major challenge for existing selection approaches.
? The way the small in-domain corpus relates to the large mix-domain corpus is also challenging
because translation performance often depends on selecting relevant sentence pairs, aside from
those that are clearly in-domain.
? The lack of out-domain data in a realistic mix-domain scenario, suggests that efforts are needed
at finding data that contrasts enough with the in-domain data. In this work we propose an initial
training period (burn-in) for isolating pseudo out-domain data. But it might be that relevance-
related approaches could also turn out more effective for this.
In our current model we implement the P (e | D) and P (f | D) as language models, inspired by the
approaches based on the contrast between the cross-entropies of in- and mix-domain language models
(Moore and Lewis, 2010; Axelrod et al., 2011). However, P (e | D) and P (f | D) should work with
relevance models, i.e., assessing the relevance of sentences to domain D. Relevance is a different con-
cept than fluency as embodied by language models, and this aspects demands special attention in future
work.
7
In ongoing large-scale experiments, we now explore the behavior of our Invitation model on a variety
of different data settings and compare that to a range of alternative existing approaches. We are also
exploring new variations of our Invitation model to find out what the optimal settings might be for
different mixes of domains. So far we find that the burn-in and size of pseudo out-domain selection
after burn-in can be important in certain situations. We also observe that estimating the suitable size
of the selection set is also a topic that demands more attention because the estimate of P (D
1
) with
the interpretation percentage of relevant data in C
mix
like likely to demand suitable relevance models
instead of language models.
We observe that the present Invitation model could be approached from a discriminative perspective,
which could be effective for specific data settings. Finally, it is theoretically not clear whether a single
approach will be most effective for all practical data scenarios.
7 Conclusions
This work looks at modeling the relevance of sentence pairs from the mix-domain corpus to a task repre-
sented by an in-domain sample. In contrast with previous work we cast this as a translation problem with
a latent domain variable. Our Invitation model based on iterative weighted Invitations using EM, offers
a new view on data selection for MT. Our model also offers principled cut-off points for selecting in-
domain and other relevant subsets. Experiments on the in-domain task shows our approach outperforms
the existing data selection for such a very complex mixture training data.
7
We thank Amir Kamran for bringing this difference to our attention through ongoing joint experimental work.
1936
The high accuracy in our experiments in this kind of data compared to the baseline suggests that our
model might also offer good estimates that can be used for data weighting. In future work we aim to test
the Invitation model for instance weighting and explore avenues for using it for selecting and weighting
sub-sentential translation pairs (e.g., phrase pairs) that can be used directly for building SMT systems.
A further issue is to improve the quality of word alignments induced for mix-domain corpora. We also
aim at exploring a discriminative learning approach in conjunction with our model.
Acknowledgements
The first author is supported by the EXPERT (EXPloiting Empirical appRoaches to Translation) Initial
Training Network (ITN) of the European Union?s Seventh Framework Programme. We thank Transla-
tion Automation Society (TAUS.com) for providing us with suitable data for the mix-domain scenario.
We also thank Amir Kamran and Bart Mellebeek for help and collaboration on experiments related to
data selection and domain adaptation. We thank Milo?s Stanojevi?c and three anonymous reviewers for
their valuable comments on earlier versions.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages
355?362, Stroudsburg, PA, USA. Association for Computational Linguistics.
Pratyush Banerjee, Sudip Kumar Naskar, Johann Roturier, Andy Way, and Josef van Genabith. 2012. Translation
quality-based supplementary data selection by incremental update of translation models. In Martin Kay and
Christian Boitet, editors, COLING 2012, 24th International Conference on Computational Linguistics, Pro-
ceedings of the Conference: Technical Papers, 8-15 December 2012, Mumbai, India, pages 149?166. Indian
Institute of Technology Bombay.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: parameter estimation. Comput. Linguist., 19:263?311, June.
Boxing Chen, George Foster, and Roland Kuhn. 2013. Adaptation of reordering models for statistical machine
translation. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, pages 938?946, Atlanta, Georgia, June. Association
for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings
of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, NAACL HLT ?12, pages 427?436, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT
?11, pages 176?181, Stroudsburg, PA, USA. Association for Computational Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the em
algorithm. JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B, 39(1):1?38.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation
of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation,
WMT ?11, pages 85?91, Stroudsburg, PA, USA. Association for Computational Linguistics.
Kevin Duh, Graham Neubig, Katsuhito Sudoh, and Hajime Tsukada. 2013. Adaptation data selection using
neural language models: Experiments in machine translation. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers), pages 678?683, Sofia, Bulgaria, August.
Association for Computational Linguistics.
1937
Vladimir Eidelman, Jordan Boyd-Graber, and Philip Resnik. 2012. Topic models for dynamic translation model
adaptation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:
Short Papers - Volume 2, ACL ?12, pages 115?119, Stroudsburg, PA, USA. Association for Computational
Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-model adaptation for smt. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT ?07, pages 128?135, Stroudsburg, PA, USA. Association for
Computational Linguistics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation
in statistical machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 451?459, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Guillem Gasc?o, Martha-Alicia Rocha, Germ?an Sanchis-Trilles, Jes?us Andr?es-Ferrer, and Francisco Casacuberta.
2012. Does more data always yield better translations? In Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguistics, EACL ?12, pages 152?161, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Barry Haddow and Philipp Koehn. 2012. Analysing the effect of out-of-domain data on smt systems. In Proceed-
ings of the Seventh Workshop on Statistical Machine Translation, WMT ?12, pages 422?432, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ann Irvine, John Morgan, Marine Carpuat, Hal Daume III, and Dragos Munteanu. 2013. Measuring machine
translation errors in new domains. Transactions of the Association for Computational Linguistics (TACL).
Philipp Koehn and Barry Haddow. 2012. Towards effective use of training data in statistical machine transla-
tion. In Proceedings of the Seventh Workshop on Statistical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ?07, pages 224?227,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra Constantin, and
Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ?07, pages 177?180,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages 388?395, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceed-
ings: the tenth Machine Translation Summit, pages 79?86, Phuket, Thailand. AAMT, AAMT.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for
machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Pro-
cessing, pages 708?717, Singapore, August. Association for Computational Linguistics.
Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings
of the ACL 2010 Conference Short Papers, ACLShort ?10, pages 220?224, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Preslav Nakov. 2008. Improving english-spanish statistical machine translation: Experiments in domain adapta-
tion, sentence paraphrasing, tokenization, and recasing. In Proceedings of the Third Workshop on Statistical
Machine Translation, StatMT ?08, pages 147?150, Stroudsburg, PA, USA. Association for Computational Lin-
guistics.
Radford M. Neal and Geoffrey E. Hinton. 1999. A view of the em algorithm that justifies incremental, sparse,
and other variants. In Michael I. Jordan, editor, Learning in Graphical Models, pages 355?368. MIT Press,
Cambridge, MA, USA.
1938
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Comput. Linguist., 29(1):19?51, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational
Linguistics, ACL ?02, pages 311?318, Stroudsburg, PA, USA. Association for Computational Linguistics.
Adam Pauls and Dan Klein. 2011. Faster and smaller n-gram language models. In Proceedings of the 49th
Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1,
HLT ?11, pages 258?267, Stroudsburg, PA, USA. Association for Computational Linguistics.
William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 1992. Numerical Recipes in C
(2Nd Ed.): The Art of Scientific Computing. Cambridge University Press, New York, NY, USA.
Rico Sennrich. 2012. Perplexity minimization for translation model domain adaptation in statistical machine
translation. In Proceedings of the 13th Conference of the European Chapter of the Association for Computa-
tional Linguistics, EACL ?12, pages 539?549, Stroudsburg, PA, USA. Association for Computational Linguis-
tics.
Matthew Snover, Bonnie Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas, pages
223?231.
1939
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 202?206,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Fitting Sentence Level Translation Evaluation
with Many Dense Features
Milo
?
s Stanojevi
?
c and Khalil Sima?an
Institute for Logic, Language and Computation
University of Amsterdam
Science Park 107, 1098 XG Amsterdam, The Netherlands
{m.stanojevic,k.simaan}@uva.nl
Abstract
Sentence level evaluation in MT has turned out
far more difficult than corpus level evaluation.
Existing sentence level metrics employ a lim-
ited set of features, most of which are rather
sparse at the sentence level, and their intricate
models are rarely trained for ranking. This pa-
per presents a simple linear model exploiting
33 relatively dense features, some of which are
novel while others are known but seldom used,
and train it under the learning-to-rank frame-
work. We evaluate our metric on the stan-
dard WMT12 data showing that it outperforms
the strong baseline METEOR. We also ana-
lyze the contribution of individual features and
the choice of training data, language-pair vs.
target-language data, providing new insights
into this task.
1 Introduction
Evaluating machine translation (MT) output at the sen-
tence/ segment level has turned out far more challeng-
ing than corpus/ system level. Yet, sentence level
evaluation can be useful because it allows fast, fine-
grained analysis of system performance on individual
sentences.
It is instructive to contrast two widely used metrics,
METEOR (Michael Denkowski and Alon Lavie, 2014)
and BLEU (Papineni et al., 2002), on sentence level
evaluation. METEOR constantly shows better corre-
lation with human ranking than BLEU (Papineni et
al., 2002). Arguably, this shows that sentence level
evaluation demands finer grained and trainable models
over less sparse features. Ngrams, the core of BLEU,
are sparse at the sentence level, and a mismatch for
longer ngrams implies that BLEU falls back on shorter
ngrams. In contrast, METEOR has a trainable model
and incorporates a small, yet wider set of features that
are less sparse than ngrams. We think that METEOR?s
features and its training approach only suggest that sen-
tence level evaluation should be treated as a modelling
challenge. This calls for questions such as what model,
what features and what training objective are better
suited for modelling sentence level evaluation.
We start out by explicitly formulating sentence level
evaluation as the problem of ranking a set of compet-
ing hypothesis. Given data consisting of human ranked
system outputs, the problem then is to formulate an
easy to train model for ranking. One particular exist-
ing approach (Ye et al., 2007) looks especially attrac-
tive because we think it meshes well with a range of
effective techniques for learning-to-rank (Li, 2011).
We deliberately select a linear modelling approach
inspired by RankSVM (Herbrich et al., 1999), which is
easily trainable for ranking and allows analysis of the
individual contributions of features. Besides presenting
a new metric and a set of known, but also a set of novel
features, we target three questions of interest to the MT
community:
? What kind of features are more helpful for sen-
tence level evaluation?
? How does a simple linear model trained for rank-
ing compare to the well-developed metric ME-
TEOR on sentence level evaluation?
? Should we train the model for each language pair
separately or for a target language?
Our new metric dubbed BEER
1
outperforms ME-
TEOR on WMT12 data showing the effectiveness of
dense features in a learning-to-rank framework. The
metric and the code are available as free software
2
.
2 Model
Our model is a linear combination of features trained
for ranking similar to RankSVM (Herbrich et al., 1999)
or, to readers familiar with SMT system tuning, to PRO
tuning (Hopkins and May, 2011):
score(sys) = ~w ? ~x
sys
where ~w represents a weight vector and ~x
sys
a vec-
tor of feature values for system output sys. Look-
ing at evaluation as a ranking problem, we con-
trast (at least) two system translations good and
bad for the same source sentence. Assuming that
humanRank(good) > humanRank(bad) as ranked
1
BEER participated on WMT14 evaluation metrics task
where it was the highest scoring sentence level evaluation
metric on average over all language pairs (Stanojevi?c and
Sima?an, 2014)
2
https://github.com/stanojevic/beer
202
by human judgement, we expect metric score(?) to ful-
fill score(good) > score(bad):
~w ? ~x
good
> ~w ? ~x
bad
?
~w ? ~x
good
? ~w ? ~x
bad
> 0 ?
~w ? (~x
good
? ~x
bad
) > 0 ?
~w ? (~x
bad
? ~x
good
) < 0
The two feature vectors (~x
good
? ~x
bad
) and (~x
bad
?
~x
good
) can be considered as positive and negative in-
stances for training our linear classifier. For training
this model, we use Logistic Regression from the Weka
toolkit (Hall et al., 2009).
3 Features
Generally speaking we identify adequacy and fluency
features. For both types we devise far less sparse fea-
tures than word ngrams.
Adequacy features We use precision P , recallR and
F1-score F as follows:
P
func
, R
func
, F
func
on matched function words
P
cont
, R
cont
, F
cont
on matched content words
P
all
, R
all
, F
all
on matched words of any type
P
char
, R
char
, F
char
matching of the char ngrams
By differentiating between function and non-function
words, our metric weighs each kind of words accord-
ing to importance for evaluation. Matching character
ngrams, originally proposed in (Yang et al., 2013), re-
wards certain translations even if they did not get the
morphology completely right. Existing metrics use
stemmers for this, but using character ngrams is inde-
pendent of the availability of a good quality stemmer.
Higher-order character ngrams have less risk of sparse
counts than word ngrams. In our experiments we used
char ngrams for n up to 6, which makes the total num-
ber of adequacy features 27.
Fluency features To evaluate word order we follow
(Isozaki et al., 2010; Birch and Osborne, 2010) in rep-
resenting reordering as a permutation pi over [1..n] and
then measuring the distance to the ideal monotone per-
mutation ?1, 2, ? ? ? , n?. We present a novel approach
based on factorization into permutation trees (PETs)
(Zhang and Gildea, 2007), and contrast it with Kendall
? (Birch and Osborne, 2010; Isozaki et al., 2010). PETs
are factorizations of permutations, which allows for an
abstract and less sparse view of word order as exempli-
fied next. Kendall score was regularly shown to have
high correlation with human judgment on distant lan-
guage pairs (Isozaki et al., 2010; Birch and Osborne,
2010).
Features based on PETs We informally review
PETs in order to exploit them for novel ordering fea-
tures. We refer the reader to (Zhang and Gildea, 2007)
and (Maillette de Buy Wenniger and Sima?an, 2011)
for a formal treatment of PETs and efficient factoriza-
tion algorithms.
A PET of permutation pi is a tree organization of pi?s
unique, atomic building blocks, called operators. Ev-
ery operator on a PET node is an atomic permutation
(not factorizing any further),
3
and it stands for the per-
mutation of the direct children of that node. Figure 1a
shows an example PET that has one 4-branching node
with operator ?2, 4, 1, 3?, two binary branching nodes
of which one decorated with the inverted operator ?2, 1?
and another with the monotone ?1, 2?.
PETs have two important properties making them at-
tractive for measuring order difference: firstly, order
difference is measured on the operators ? the atomic
reordering building blocks of the permutation, and sec-
ondly, the operators on higher level nodes capture hid-
den ordering patterns that cannot be observed without
factorization. Statistics over ordering patterns in PETs
are far less sparse than word or character ngram statis-
tics.
Intuitively, among the atomic permutations, the bi-
nary monotone operator ?1, 2? signifies no ordering dif-
ference at all, whereas the binary inverted ?2, 1? signi-
fies the shortest unit of order difference. Operators of
length four like ?2, 4, 1, 3? (Wu, 1997) are presumably
more complex than ?2, 1?, whereas operators longer
than four signify even more complex order difference.
Therefore, we devise possible branching feature func-
tions over the operator length for the nodes in PETs:
? factor 2 - with two features: ?
[ ]
and ?
<>
(there
are no nodes with factor 3 (Wu, 1997))
? factor 4 - feature ?
=4
? factor bigger than 4 - feature ?
>4
Consider permutations ?2, 1, 4, 3? and ?4, 3, 2, 1?, none
of which has exactly matching ngrams beyond uni-
grams. Their PETs are in Figures 1b and 1c. Intuitively,
?2, 1, 4, 3? is somewhat less scrambled than ?4, 3, 2, 1?
because it has at least some position in correct order.
These ?abstract ngrams? pertaining to correct order-
ing of full phrases could be counted using ?
[ ]
which
would recognize that on top of the PET in 1b there is
a binary monotone node, unlike the PET in Figure 1c
which has no monotone nodes at all.
Even though the set of operators that describe a per-
mutation is unique for the given permutation, the ways
in which operators are combined (the derivation tree)
is not unique. For example, for the fully monotone
3
For example ?2, 4, 1, 3? is atomic whereas ?4, 3, 2, 1? is
not. The former does not contain any contiguous sub-ranges
of integers whereas the latter contains sub-range {2, 3, 4} in
reverse order ?4, 3, 2?, which factorizes into two binary in-
verting nodes cf. Fig. 1c.
203
?2, 4, 1, 3?
2 ?2, 1?
?1, 2?
5 6
4
1 3
(a) Complex PET
?1, 2?
?2, 1?
2 1
?2, 1?
4 3
(b) PET with inversions
?2, 1?
?2, 1?
?2, 1?
4 3
2
1
(c) Canonical fully
inverted PET
?2, 1?
?2, 1?
4 ?2, 1?
3 2
1
(d) Alternative fully
inverted PET
?2, 1?
?2, 1?
4 3
?2, 1?
2 1
(e) Alternative fully
inverted PET
?2, 1?
4 ?2, 1?
?2, 1?
3 2
1
(f) Alternative fully
inverted PET
?2, 1?
4 ?2, 1?
3 ?2, 1?
2 1
(g) Alternative fully
inverted PET
Figure 1: Examples of PETs
permutation ?4, 3, 2, 1? there are 5 possible derivations
(PETs) presented in Figures 1c, 1d, 1e, 1f and 1g. The
features on PETs that we described so far look at the
operators independently (they treat a derivation as a
set of operators) so differenct derivations do not influ-
ence the score?whichever derviation we use we will
get the same feature score. However, the number of
derivations might say something about the goodness of
the permutation. Similar property of permutations was
found to be helpful earlier in (Mylonakis and Sima?an,
2008) as an ITG prior for learning translation rule prob-
abilities.
Permutations like ?3, 2, 1, 4? and ?2, 4, 3, 1? have the
same set of operators, but the former factorizes into
more PETs than the latter because ?4, 3? must group
first before grouping it with 2 and then 1 in ?2, 4, 3, 1?.
The ?freedom to bracket? in different ways could be a
signal of better grouping of words (even if they have
inverted word order). Hence we exploit one more fea-
ture:
?
count
the ratio between the number of alternative
PETs for the given permutation, to the number of
PETs that could be built if permutation was per-
fectly grouped (fully monotone or fully inverted).
Finding the number of PETs that could be built does
not require building all PETs or encoding them in the
chart. The number can be computed directly from the
canonical left-branching PET. Since multiple different
PETs appear only in cases when there is a sequence of
more than one node that is either ?1, 2? or ?2, 1? (Zhang
et al., 2008), we can use these sequences to predict the
number of PETs that could be built. Let X represent a
set of sequences of the canonical derivation. The num-
ber of PETs is computed in the following way:
#PETs =
?
x?X
Cat(|x|) (1)
Cat(n) =
1
n+ 1
(
2n
n
)
(2)
whereCat(?) is a Catalan number. The proof for this
formula is beyond the scope of this paper. The reader
can consider the example of the PET in Figure 1c. That
derivation has one sequence of monotone operators of
length 3. So the number of PETs that could be built is
Cat(3) = 5.
4 Experiments
We use human judgments from the WMT tasks:
WMT13 is used for training whereas WMT12 for test-
ing. The baseline is METEOR?s latest version (Michael
Denkowski and Alon Lavie, 2014), one of the best met-
rics on sentence level. To avoid contaminating the re-
sults with differences with METEOR due to resources,
we use the same alignment, tokenization and lower-
casing (-norm in METEOR) algorithms, and the same
tables of function words, synonyms, paraphrases and
stemmers.
Kendall ? correlation is borrowed from WMT12
(Callison-Burch et al., 2012):
? =
#concordant?#discordant?#ties
#concordant+ #discordant+ #ties
#concordant represents the number of pairs or-
dered in the same way by metric and by human,
#discordant the number of opposite orderings and
#ties the number of tied rankings by metric.
Beside testing our full metric BEER, we perform ex-
periments where we remove one kind of the following
features at a time:
1. char n-gram features (P, R and F-score)
2. all word features (P, R and F-score for all, function
and content words),
3. all function and content words features
4. all F-scores (all words, function words, content
words, char ngrams)
204
metric en-cs en-fr en-de en-es cs-en fr-en de-en es-en avg ?
BEER without char features 0.124 0.178 0.168 0.149 0.121 0.17 0.179 0.078 0.146
BEER without all word features 0.184 0.237 0.223 0.217 0.192 0.209 0.243 0.199 0.213
BEER without all F-scores 0.197 0.243 0.219 0.22 0.177 0.227 0.254 0.211 0.219
METEOR 0.156 0.252 0.173 0.202 0.208 0.249 0.273 0.246 0.22
BEER without PET features 0.202 0.248 0.243 0.225 0.198 0.249 0.268 0.234 0.233
BEER without function words 0.2 0.245 0.231 0.227 0.189 0.268 0.267 0.253 0.235
BEER without fluency features 0.201 0.248 0.236 0.223 0.202 0.257 0.283 0.243 0.237
BEER without Kendall ? 0.205 0.246 0.244 0.227 0.202 0.257 0.282 0.248 0.239
BEER full 0.206 0.245 0.244 0.23 0.198 0.263 0.283 0.245 0.239
Table 1: Kendall ? scores on WMT12 data
5. PET features
6. Kendall ? features
7. all fluency features (PET and Kendall ? )
Table 1 shows the results sorted by their average
Kendall ? correlation with human judgment.
5 Analysis
Given these experimental results, we are coming back
to the questions we asked in the introduction.
5.1 What kind of features are more helpful for
sentence level evaluation?
Fluency vs. Adequacy The fluency features play a
smaller role than adequacy features. Apparently, many
SMT systems participating in this task have rather sim-
ilar reordering models, trained on similar data, which
makes the fluency features not that discriminative rel-
ative to adequacy features. Perhaps in a different ap-
plication, for example MT system tuning, the reorder-
ing features would be far more relevant because ignor-
ing them would basically imply disregarding the im-
portance of the reordering model in MT.
Character vs. Word features We observe that, pre-
cision, recall and F-score on character ngrams are cru-
cial. We think that this shows that less sparse features
are important for sentence level evaluation. The sec-
ond best features are word features. Without word
features, BEER scores just below METEOR, which
suggests that word boundaries play a role as well. In
contrast, differentiating between function and content
words does not seem to be important.
PETs vs. Kendall ? Despite the smaller role for
reordering features we can make a few observations.
Firstly, while PETs and Kendall seem to have simi-
lar effect on English-Foreign cases, in all four cases of
Foreign-English PETs give better scores. We hypoth-
esize that the quality of the permutations (induced be-
tween system output and reference) is better for English
than for the other target languages. Discarding PET
features has far larger impact than discarding Kendall.
Most interestingly, for de-en it makes the difference
in outperforming METEOR. In many cases discarding
Kendall ? improves the BEER score, likely because it
conflicts with the PET features that are found more ef-
fective.
5.2 Is a linear model sufficient?
A further insight, from our perspective, is that F-score
features constitute a crucial set of features, even when
the corresponding precision and recall features are in-
cluded. Because our model merely allows for linear in-
terpolation, whereas F-score is a non-linear function of
precision and recall, we think this suggests that a non-
linear interpolation of precision and recall is useful.
4
By formulating the evaluation as a ranking problem it is
relatively easy to ?upgrade? for using non-linear mod-
els while using the same (or larger) set of features.
5.3 Train for the language pair or only for the
target language?
All our models were trained for each language pair.
This is not the case with many other metrics which
train their models for each target language instead of
language pair. We contrast these two settings in Table
2. Training for each language pair separately does not
give significant improvement over training for the tar-
get language only. A possible reason could be that by
training for the target language we have more training
data (in this case four times more).
Train for cs-en fr-en de-en es-en avg ?
target lang 0.199 0.257 0.273 0.248 0.244
lang pair 0.198 0.263 0.283 0.245 0.247
Table 2: Kendall ? scores on WMT12 for different
training data
5.4 BEER vs. METEOR
The results across individual language pairs are mostly
consistent with the averages with a few exceptions.
BEER outperforms METEOR in five out of eight lan-
guage pairs, ties at one (the difference is only 0.001 on
es-en) and loses in two (en-fr and cs-en). In some cases
BEER is better than METEOR by a large margin (see,
e.g., en-cs, en-de).
4
Interestingly, METEOR tunes ? in F
?
.
205
6 Conclusion
In this work we show that combining less sparse fea-
tures at the sentence level into a linear model that is
trained on ranking we can obtain state-of-the-art re-
sults. The analysis of the results shows that features on
character ngrams are crucial, besides the standard word
level features. The reordering features, while rather
important, are less effective within this WMT task, al-
beit the more abstract PET features have larger impact
than the often used Kendall. Good performance of F-
score features leads to the conclusion that linear models
might not be sufficient for modeling human sentence
level ranking and to learn the right relation between
precision and recall it could be worthwhile exploring
non-linear models.
Acknowledgments
This work is supported by STW grant nr. 12271 and
NWO VICI grant nr. 277-89-002. We also thank TAUS
and the other DatAptor project User Board members.
References
Alexandra Birch and Miles Osborne. 2010. LRscore
for Evaluating Lexical and Reordering Quality in
MT. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 327?332, Uppsala, Sweden, July. Association
for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An
Update. SIGKDD Explor. Newsl., 11(1):10?18,
November.
Ralf Herbrich, Thore Graepel, and Klaus Obermayer.
1999. Support Vector Learning for Ordinal Regres-
sion. In In International Conference on Artificial
Neural Networks, pages 97?102.
Mark Hopkins and Jonathan May. 2011. Tuning as
Ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1352?1362, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
Evaluation of Translation Quality for Distant Lan-
guage Pairs. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?10, pages 944?952, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Hang Li. 2011. Learning to Rank for Information Re-
trieval and Natural Language Processing. Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool Publishers.
Gideon Maillette de Buy Wenniger and Khalil Sima?an.
2011. Hierarchical Translation Equivalence over
Word Alignments. In ILLC Prepublication Series,
PP-2011-38. University of Amsterdam.
Michael Denkowski and Alon Lavie. 2014. Meteor
Universal: Language Specific Translation Evalua-
tion for Any Target Language. In Proceedings of the
ACL 2014 Workshop on Statistical Machine Transla-
tion.
Markos Mylonakis and Khalil Sima?an. 2008.
Phrase Translation Probabilities with ITG Priors and
Smoothing as Learning Objective. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 630?639, Honolulu,
USA, October. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Milo?s Stanojevi?c and Khalil Sima?an. 2014. BEER:
BEtter Evaluation as Ranking. In Proceedings of the
Ninth Workshop on Statistical Machine Translation,
pages 414?419, Baltimore, Maryland, USA, June.
Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377?403.
Muyun Yang, Junguo Zhu, Sheng Li, and Tiejun Zhao.
2013. Fusion of Word and Letter Based Metrics
for Automatic MT Evaluation. In Proceedings of
the Twenty-Third International Joint Conference on
Artificial Intelligence, IJCAI?13, pages 2204?2210.
AAAI Press.
Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sen-
tence Level Machine Translation Evaluation As a
Ranking Problem: One Step Aside from BLEU. In
Proceedings of the Second Workshop on Statistical
Machine Translation, StatMT ?07, pages 240?247,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Hao Zhang and Daniel Gildea. 2007. Factorization of
synchronous context-free grammars in linear time.
In In NAACL Workshop on Syntax and Structure in
Statistical Translation (SSST.
Hao Zhang, Daniel Gildea, and David Chiang.
2008. Extracting Synchronous Grammar Rules
From Word-Level Alignments in Linear Time. In
Proceedings of the 22nd International Conference
on Computational Linguistics (COLING-08), pages
1081?1088, Manchester, UK.
206
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 566?576,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Latent Domain Phrase-based Models for Adaptation
Hoang Cuong and Khalil Sima?an
Institute for Logic, Language and Computation
University of Amsterdam
Science Park 107, 1098 XG Amsterdam, The Netherlands
{c.hoang,k.simaan}@uva.nl
Abstract
Phrase-based models directly trained
on mix-of-domain corpora can be
sub-optimal. In this paper we equip
phrase-based models with a latent domain
variable and present a novel method for
adapting them to an in-domain task rep-
resented by a seed corpus. We derive an
EM algorithm which alternates between
inducing domain-focused phrase pair
estimates, and weights for mix-domain
sentence pairs reflecting their relevance
for the in-domain task. By embedding
our latent domain phrase model in a
sentence-level model and training the
two in tandem, we are able to adapt all
core translation components together
? phrase, lexical and reordering. We
show experiments on weighing sentence
pairs for relevance as well as adapting
phrase-based models, showing significant
performance improvement in both tasks.
1 Mix vs. Latent Domain Models
Domain adaptation is usually perceived as utiliz-
ing a small seed in-domain corpus to adapt an ex-
isting system trained on an out-of-domain corpus.
Here we are interested in adapting an SMT sys-
tem trained on a large mix-domain corpus C
mix
to an in-domain task represented by a seed paral-
lel corpus C
in
. The mix-domain scenario is in-
teresting because often a large corpus consists of
sentence pairs representing diverse domains, e.g.,
news, politics, finance, sports, etc.
At the core of a standard state-of-the-art phrase-
based system (Och and Ney, 2004) is a phrase
table {?e?,
?
f?} extracted from the word-aligned
training data together with estimates for P
t
(e? |
?
f)
and P
t
(
?
f | e?). Because the translations of
words often vary across domains, it is likely
that in a mix-domain corpus C
mix
the translation
ambiguity will increase with the domain diver-
sity. Furthermore, the statistics in C
mix
will re-
flect translation preferences averaged over the di-
verse domains. In this sense, phrase-based mod-
els trained on C
mix
can be considered domain-
confused. This often leads to suboptimal perfor-
mance (Gasc?o et al., 2012; Irvine et al., 2013).
Recent adaptation techniques can be seen as
mixture models, where two or more phrase ta-
bles, estimated from in- and mix-domain corpora,
are combined together by interpolation, fill-up, or
multiple-decoding paths (Koehn and Schroeder,
2007; Bisazza et al., 2011; Sennrich, 2012; Raz-
mara et al., 2012; Sennrich et al., 2013). Here
we are interested in the specific question how to
induce a phrase-based model from C
mix
for in-
domain translation? We view this as in-domain
focused training on C
mix
, a complementary adap-
tation step which might precede any further com-
bination with other models, e.g., in-, mix- or
general-domain.
The main challenge is how to induce from C
mix
a phrase-based model for the in-domain task,
given only C
in
as evidence? We present an ap-
proach whereby the contrast between in-domain
prior distributions and ?out-domain? distributions
is exploited for softly inviting (or recruiting) C
mix
phrase pairs to either camp. To this end we in-
566
troduce a latent domain variable D to signify in-
(D
1
) and out-domain (D
0
) respectively.
1
With the introduction of the latent variables, we
extend the translation tables in phrase-based mod-
els from generic P
t
(e? |
?
f) to domain-focused by
conditioning them on D, i.e., P
t
(e? |
?
f,D) and de-
composing them as follows:
P
t
(e? |
?
f,D) =
P
t
(e? |
?
f)P(D | e?,
?
f)
?
e?
P
t
(e? |
?
f)P(D | e?,
?
f)
. (1)
Where P(D | e?,
?
f) is viewed as the latent phrase-
relevance models, i.e., the probability that a
phrase pair is in- (D
1
) or out-domain (D
0
). In the
end, our goal is to replace the domain-confused
tables, P
t
(e? |
?
f) and P
t
(
?
f | e?), with the in-domain
focused ones, P
t
(e? |
?
f,D
1
) and P
t
(
?
f | e?, D
1
).
2
Note how P
t
(e? |
?
f,D
1
) and P
t
(
?
f | e?, D
1
) contains
P
t
(e? |
?
f) and P
t
(
?
f | e?) as special case.
Eq. 1 shows that the key to training the latent
phrase-based translation models is to train the la-
tent phrase-relevance models, P (D | e?,
?
f). Our
approach is to embed P (D | e?,
?
f) in asymmetric
sentence-level models P (D | e, f) and train them
on C
mix
. We devise an EM algorithm where at
every iteration, in- or out-domain estimates pro-
vide full sentence pairs ?e, f? with expectations
{P (D | e, f) | D ? {0, 1}}. Once these ex-
pectation are in C
mix
, we induce re-estimates for
the latent phrase-relevance models, P (D | e?,
?
f).
Metaphorically, during each EM iteration the cur-
rent in- or out-domain phrase pairs compete on
inviting C
mix
sentence pairs to be in- or out-
domain, which bring in new (weights for) in- and
out-domain phrases. Using the same algorithm we
also show how to adapt all core translation com-
ponents in tandem, including also lexical weights
and lexicalized reordering models.
Next we detail our model, the EM-based invita-
tion training algorithm and provide technical so-
lutions to a range of difficulties. We report exper-
1
Crucially, the lack of explicit out-domain data in C
mix
is
a major technical difficulty. We follow (Cuong and Sima?an,
2014) and in the sequel present a relatively efficient solution
based on a kind of ?burn-in? procedure.
2
It is common to use these domain-focused models as
additional features besides the domain-confused features.
However, here we are more interested in replacing the
domain-confused features rather than complementing them.
This distinguishes this work from other domain adaptation
literature for MT.
iments showing good instance weighting perfor-
mance as well as significantly improved phrase-
based translation performance.
2 Model and training by invitation
Eq. 1 shows that the key to training the latent
phrase-based translation models is to train the la-
tent phrase-relevance models, P (D | e?,
?
f). As
mentioned, for training P (D | e?,
?
f) on parallel
sentences in C
mix
we embed them in two asym-
metric sentence-level models {P (D | e, f) | D ?
{0, 1}}.
2.1 Domain relevance sentence models
Intuitively, sentence models for domain relevance
P (D | e, f) are somewhat related to data selec-
tion approaches (Moore and Lewis, 2010; Axel-
rod et al., 2011). The dominant approach to data
selection uses the contrast between perplexities
of in- and mix-domain language models.
3
In the
translation context, however, often a source phrase
has different senses/translations in different do-
mains, which cannot be distinguished with mono-
lingual language models (Cuong and Sima?an,
2014). Therefore, our proposed latent sentence-
relevance model includes two major latent com-
ponents - monolingual domain-focused relevance
models and domain-focused translation models
derives as follows:
P (D | e, f) =
P (e, f, D)
?
D?{D
1
,D
0
}
P (e, f, D)
, (2)
where P (e, f, D) can be decomposed as:
P (f, e, D) =
1
2
(
P (D)P
lm
(e | D)P
t
(f | e, D)
+ P (D)P
lm
(f | D)P
t
(e | f, D)
)
.
(3)
Here
? P
t
(e|f, D) and similarly P
t
(f|e, D): the latent
domain-focused translation models aim at cap-
turing the faithfulness of translation with re-
spect to different domains. We simplify this as
3
Note that earlier work on data selection exploits the con-
trast between in- and mix-domain. In (Cuong and Sima?an,
2014), we present the idea of using the language and transla-
tion models derived separately from in- and out-domain data,
and show how it helps for data selection.
567
?bag-of-possible-phrases? translation models:
4
P
t
(e|f, D) :=
?
?e?,
?
f??A(e,f)
P
t
(e?|
?
f,D)
c(e?,
?
f)
,
(4)
where A(e, f) is the multiset of phrases in
?e, f? and c(?) denotes their count. Sub-model
P
t
(e?|
?
f,D) is given by Eq. 1.
? P
lm
(e|D), P
lm
(f|D): the latent monolingual
domain-focused relevance models aim at cap-
turing the relevance of e and f for identifying
domain D but here we consider them language
models (LMs).
5
As mentioned, the out-domain
LMs differ from previous works, e.g., (Axel-
rod et al., 2011), which employ mix-domain
LMs. Here, we stress the difficulty in finding
data to train out-domain LMs and present a so-
lution based on identifying pseudo out-domain
data.
? P (D): the domain priors aim at modeling
the percentage of relevant data that the learn-
ing framework induces. It can be estimated
via phrase-level parameters but here we prefer
sentence-level parameters:
6
P (D) :=
?
?e,f??C
mix
P (D | e, f)
?
D
?
?e,f??C
mix
P (D | e, f)
(5)
2.2 Training by invitation
Generally, our model can be viewed to have latent
parameters ? = {?
D
0
,?
D
1
}. The training pro-
cedure seeks ? that maximize the log-likelihood
of the observed sentence pairs ?e, f? ? C
mix
:
L =
?
?e,f??C
mix
log
?
D
P
?
D
(D, e, f). (6)
It is obvious that there does not exist a closed-form
solution for Equation 6 because of the existence of
4
We design our latent domain translation models with ef-
ficiency as our main concern. Future extensions could in-
clude the lexical and reordering sub-models (as suggested by
an anonymous reviewer.)
5
Relevance for identification or retrieval could be differ-
ent from frequency or fluency. We leave this extension for
future work.
6
It should be noted that in most phrase-based SMT sys-
tems bilingual phrase probabilities are estimated heuristically
from word alignmened data which often leads to overfitting.
Estimating P (D) from sentence-level parameters rather than
from phrase-level parameters helps us avoid the overfitting
which often accompanies phrase extraction.
the log-term log
?
. The EM algorithm (Dempster
et al., 1977) comes as an alternative solution to fit
the model. It can be seen to maximizeL via block-
coordinate ascent on a lower bound F(q,?) using
an auxiliary distribution q(D | e, f)
F(q,?) =
?
?e,f?
?
D
q(D | e, f) log
P
?
D
(D, e, f)
q(D | e, f)
(7)
where the inequality results, i.e., L ? F(q,?),
derived from log being concave and Jensen?s in-
equality. We rewrite the Free Energy F(q,?)
(Neal and Hinton, 1999) as follows:
F =
?
?e,f?
?
D
q(D | e, f) log
P
?
D
(D | e, f)
q(D | e, f)
+
?
?e,f?
?
D
q(D | e, f) logP
?
(e, f)
=
?
?e,f?
logP
?
(e, f) (8)
?KL[q(D | e, f) || P
?
D
(D | e, f)],
where KL[? || ?] is the KL-divergence.
With the introduction of the KL-divergence, the
alternating E and M steps for our EM algorithm
are easily derived as
E-step : q
t+1
(9)
argmax
q(D | e,f)
F(q,?
t
) =
argmin
q(D | e,f)
KL[q(D|e, f) || P
?
t
D
(D|e, f)]
= P
?
t
D
(D | e, f)
M-step : ?
t+1
(10)
argmax
?
F(q
t+1
,?) =
argmax
?
?
?e,f?
?
D
q(D | e, f) logP
?
D
(D, e, f)
The iterative procedure is illustrated in Fig-
ure 1.
7
At the E-step, a guess for P (D | e?,
?
f) can
be used to update P
t
(
?
f | e?, D) and P
t
(e? |
?
f,D)
(i.e., using Eq. 1) and consequently P
t
(f | e, D)
and P
t
(e | f, D) (i.e., using Eq. 4). These resulting
table estimates, together with the domain-focused
LMs and the domain priors are served as expected
counts to update P (D | e, f).
8
At the M-step,
7
For simplicity, we ignore the LMs and prior models in
the illustration in Fig. 1.
8
Since we only use the in-domain corpus as priors to ini-
tilize the EM parameters, in technical perspective we do not
want P (D | e, f) parameters to go too far off from the initial-
ization. We therefore prefer the averaged style in practice,
i.e., at the iteration n we update the P (D |e, f) parameters,
P
(n)
(D|e, f) as
1
n
(P
(n)
(D | e, f) +
?
n?1
i=1
P
(i)
(D | e, f)).
568
P (e?|
?
f,D)
P (
?
f |e?, D)
P (e|f, D)
P (f|e, D)
P (f, e, D)
P (D|e?,
?
f)
P (D|e, f)
Phrase-level Sentence-level
Re-update phrase-level parameters
Update sentence-level parameters
Figure 1: Our probabilistic invitation framework.
the new estimates for P (D | e, f) can be used to
(softly) fill in the values of hidden variable D and
estimate parameters P (D | e?,
?
f) and P (D). The
EM is guaranteed to converge to a local maximum
of the likelihood under mild conditions (Neal and
Hinton, 1999).
Before EM training starts we must provide a
?reasonable? initial guess for P (D | e?,
?
f). We
must also train the out-domain LMs, which needs
the construction of pseudo out-domain data.
9
One simple way to do that is inspired by burn-
in in sampling, under the guidance of an in-
domain data set, C
in
as prior. At the begin-
ning, we train P
t
(e? |
?
f,D
1
) and P
t
(
?
f | e?, D
1
)
for all phrases learned from C
in
. We also train
P
t
(e? |
?
f) and P
t
(
?
f | e?) for all phrases learned
from C
mix
. During burn-in we assume that the
out-domain phrase-based models are the domain-
confused phrase-based models, i.e., P
t
(e? |
?
f,D
0
)
? P
t
(e? |
?
f) and P
t
(
?
f | e?, D
0
) ? P
t
(
?
f | e?). We
isolate all the LMs and the prior models from our
model, and apply a single EM iteration to update
P (D | e, f) based on those domain-focused mod-
els P
t
(e? |
?
f,D) and P
t
(
?
f | e?, D).
In the end, we use P (D | e, f) to fill in the val-
ues of hidden variable D in C
mix
, so it provides
us with an initialization for P (D | e?,
?
f). Subse-
quently, we also rank sentence pairs in C
mix
with
P (D
1
| e, f) and select a subset of smallest scor-
ing pairs as a pseudo out-domain subset to train
P
lm
(e | D
0
) and P
lm
(f | D
0
). Once the latent
domain-focused LMs have been trained, the LM
probabilities stay fixed during EM. Crucially, it
9
The in-domain LMs P
lm
(e | D
1
) and P
lm
(f | D
1
) can
be simply trained on the source and target sides of C
in
re-
spectively.
is important to scale the probabilities of the four
LMs to make them comparable: we normalize the
probability that a LM assigns to a sentence by the
total probability this LM assigns to all sentences
in C
mix
.
3 Intrinsic evaluation
We evaluate the ability of our model to retrieve
?hidden? in-domain data in a large mix-domain
corpus, i.e., we hide some in-domain data in a
large mix-domain corpus. We weigh sentence
pairs under our model with P (D
1
| e?,
?
f) and
P (D
1
| e, f) respectively. We report pseudo-
precision/recall at the sentence-level using a
range of cut-off criteria for selecting the top
scoring instances in the mix-domain corpus. A
good relevance model expects to score higher for
the hidden in-domain data.
Baselines Two standard perplexity-based se-
lection models in the literature have been
implemented as the baselines: cross-entropy
difference (Moore and Lewis, 2010) and bilingual
cross-entropy difference (Axelrod et al., 2011),
investigating their ability to retrieve the hiding
data as well. Training them over the data to learn
the sentences with their relevance, we then rank
the sentences to select top of pairs to evaluate the
pseudo-precision/recall at the sentence-level.
Results We use a mix-domain corpus C
g
of 770K
sentence pairs of different genres.
10
There is also
a Legal corpus of 183K pairs that serves as the
in-domain data. We create C
mix
by selecting an
arbitrary 83K pairs of in-domain pairs and adding
them to C
g
(the hidden in-domain data); we use
the remaining 100k in-domain pairs as C
in
.
To train the baselines, we construct interpo-
lated 4-gram Kneser-Ney LMs using BerkeleyLM
(Pauls and Klein, 2011). Training our model on
the data takes six EM-iterations to converge.
11
10
Count of sentence pairs: European Parliament (Koehn,
2005): 183, 793; Pharmaceuticals: 190, 443, Software:
196, 168, Hardware: 196, 501.
11
After the fifth EM iteration we do not observe any sig-
nificant increase in the likelihood of the data. Note that we
use the same setting as for the baselines to train the latent
domain-focused LMs for use in our model ? interpolated 4-
gram Kneser-Ney LMs using BerkeleyLM. This training set-
ting is used for all experiments in this work.
569
05
1015
2025
3035
4045
5055
6065
7075
8085
9095
100
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Pseu
do-P
reci
sion
 (Se
nten
ce-L
evel
) 
Top Percentage 
(a): Pseudo-Precision (Sentence-Level) 
Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5
0
5
10
15
20
25
30
35
40
45
50
55
60
65
70
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Pse
udo
-Rec
all 
(Se
nte
nce
-Le
vel
) 
Top Percentage 
(b): Pseudo -Recall (Sentence -Level) 
Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5
Figure 2: Intrinsic evaluation.
Fig. 2 helps us examine how the pseudo sen-
tence invitation are done during each EM iter-
ation. For later iterations we observe a better
pseudo-precision and pseudo-recall at sentence-
level (Fig. 2(a), Fig. 2(b)). Fig. 2 also reveals
a good learning capacity of our learning frame-
work. Nevertheless, we observe that the baselines
do not work well for this task. This is not new,
as pointed out in our previous work (Cuong and
Sima?an, 2014).
Which component type contributes more to the
performance, the latent domain language models
or the latent domain translation models? Further
experiments have been carried on to neutralize
each component type in turn and build a selection
system with the rest of our model parameters. It
turns out that the latent domain translation mod-
els are crucial for performance for the learning
framework, while the latent domain LMs make a
far smaller yet substantial contribution. We refer
readers to our previous work (Cuong and Sima?an,
2014), which provides detail analysis of the data
selection problem.
4 Translation experiments: Setting
Data We use a mix-domain corpus consisting of
4M sentence pairs, collected from multiple re-
sources including EuroParl (Koehn, 2005), Com-
mon Crawl Corpus, UN Corpus, News Commen-
tary. As in-domain corpus we use ?Consumer
and Industrial Electronics? manually collected
by Translation Automation Society (TAUS.com).
The corpus statistics are summarized in Table 1.
System We train a standard state-of-the-art
English Spanish
C
mix
Sents 4M
Words 113.7M 127.1M
Domain:
Electronics
C
in
Sents 109K
Words 1, 485, 558 1, 685, 716
Dev
Sents 984
Words 13130 14, 955
Test
Sents 982
Words 13, 493 15, 392
Table 1: The data preparation.
phrase-based system, using it as the baseline.
12
There are three main kinds of features for the
translation model in the baseline - phrase-based
translation features, lexical weights (Koehn et al.,
2003) and lexicalized reordering features (Koehn
et al., 2005).
13
Other features include the penal-
ties for word, phrase and distance-based reorder-
ing.
The mix-domain corpus is word-aligned using
GIZA++ (Och and Ney, 2003) and symmetrized
with grow(-diag)-final-and (Koehn et al., 2003).
We limit phrase length to a maximum of seven
words. The LMs are interpolated 4-grams with
Kneser-Ney, trained on 2.2M English sentences
from Europarl augmented with 248.8K sentences
from News Commentary Corpus (WMT 2013).
We tune the system using k-best batch MIRA
(Cherry and Foster, 2012). Finally, we use Moses
12
We use Stanford Phrasal - a standard state-of-the-art
phrase-based translation system developed by Cer et al.
(2010).
13
The lexical weights and the lexical reordering features
will be described in more detail in Section 6.
570
19.91 
20.48 20.5 
20.64 
20.51 20.52 
19.8
20
20.2
20.4
20.6
20.8
21
21.2
Baseline Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5
Electrics (Training Data: 1 Million)  
Figure 3: BLEU averaged over multiple runs.
(Koehn et al., 2007) as decoder.
14
We report BLEU (Papineni et al., 2002), ME-
TEOR 1.4 (Denkowski and Lavie, 2011) and TER
(Snover et al., 2006), with statistical significance
at 95% confidence interval under paired bootstrap
re-sampling (Press et al., 1992). For every system
reported, we run the optimizer at least three times,
before running MultEval (Clark et al., 2011) for
resampling and significance testing.
Outlook In Section 5 we examine the effect of
training only the latent domain-focused phrase ta-
ble using our model. In Section 6 we proceed fur-
ther to estimate also latent domain-focused lexical
weights and lexicalized reordering models, exam-
ining how they incrementally improve the transla-
tion as well.
5 Adapting phrase table only
Here we investigate the effect of adapting the
phrase table only; we will delay adapting the
lexical weights and lexicalized reordering fea-
tures to Section 6. We build a phrase-based sys-
tem with the usual features as the baseline, in-
cluding two bi-directional phrase-based models,
plus the penalties for word, phrase and distortion.
We also build a latent domain-focused phrase-
based system with the two bi-directional latent
phrase-based models, and the standard penalties
described above.
We explore training data sizes 1M , 2M
and 4M sentence pairs. Three baselines are
trained yielding 95.77M , 176.29M and 323.88M
phrases respectively. We run 5 EM iterations to
14
While we implement the latent domain phrase-based
models using Phrasal for some advantages, we prefer to use
Moses for decoding.
train our learning framework. We use the pa-
rameter estimates for P (D | e?,
?
f) derived at each
EM iteration to train our latent domain-focused
phrase-based systems. Fig. 3 presents the results
(in BLEU) at each iteration in detail for the case of
1M sentence pairs. Similar improvements are ob-
served for METEOR and TER. Here, we consis-
tently observe improvements at p-value = 0.0001
for all cases.
It should be noted that when doubling the train-
ing data to 2M and 4M , we observe the similar
results.
Finally, for all cases we report their best result
in Table 2. Here, note how the improvement could
be gained when doubling the training data.
Data
System Avg ? p-value
1M
Baseline 19.91 ? ?
Our System 20.64 +0.73 0.0001
2M
Baseline 20.54 ? ?
Our System 21.41 +0.87 0.0001
4M
Baseline 21.44 ? ?
Our System 22.62 +1.18 0.0001
Table 2: BLEU averaged over multiple runs.
It is also interesting to consider the average
entropy of phrase table entries in the domain-
confused systems, i.e.,
?
?
?e?,
?
f?
p
t
(e?|
?
f) log p
t
(e?|
?
f)
number of phrases?e?,
?
f?
against that in the domain-focused systems
?
?
?e?,
?
f?
p
t
(e?|
?
f,D
1
) log p
t
(e?|
?
f,D
1
)
number of phrases?e?,
?
f?
.
Following (Hasler et al., 2014) in Table 3 we also
show that the entropy decreases significantly in
571
the adapted tables in all cases, which indicates that
the distributions over translations of phrases have
become sharper.
Baseline Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5
0.210 0.187 0.186 0.185 0.185 0.184
Table 3: Average entropy of distributions.
In practice, the third iteration systems usually
produce best translations. This is somewhat ex-
pected because as EM invites more pseudo in-
domain pairs in later iterations, it sharpens the
estimates of P (D
1
| e?,
?
f), making pseudo out-
domain pairs tend to 0.0. Table 4 shows the per-
centage of entries with P (D
1
| e?,
?
f) < 0.01 at
every iteration, e.g., 34.52% at the fifth iteration.
This induced schism in C
mix
diminishes the dif-
ference between the relevance scores for certain
sentence pairs, limiting the ability of the latent
phrase-based models to further discriminate in the
gray zone.
Entries P (D
1
|
?
f, e?) < 0.01
Iter. 1 22.82%
Iter. 2 27.06%
Iter. 3 30.07%
Iter. 4 32.47%
Iter. 5 34.52%
Table 4: Phrase analyses.
Finally, to give a sense of the improvement
in translation, we (randomly) select cases where
the systems produce different translations and
present some of them in Table 5. These ex-
amples are indeed illuminating, e.g., ?can repro-
duce signs of audio?/?can play signals audio?,
?password teacher?/?password master?, reveal-
ing thoroughly the benefit derived from adapting
the phrase models from being domain-confused to
being domain-focused. Table 6 presents phrase ta-
ble entries, i.e., p
t
(e | f) and p
t
(e | f,D
1
), for the
?can reproduce signs of audio?/?can play signals
audio? example.
6 Fully adapted translation model
The preceding experiments reveal that adapting
the phrase tables significantly improves transla-
tion performance. Now we also adapt the lexical
se
?
nales reproducir
Entries signals signs play reproduce
Baseline 0.29 0.36 0.15 0.20
Iter. 1 0.36 0.23 0.29 0.16
Iter. 2 0.37 0.19 0.32 0.17
Iter. 3 0.37 0.17 0.34 0.16
Iter. 4 0.37 0.16 0.36 0.16
Iter. 5 0.37 0.15 0.37 0.16
Table 6: Phrase entry examples.
and reordering components. The result is a fully
adapted, domain-focused, phrase-based system.
Briefly, the lexical weights provide smooth es-
timates for the phrase pair based on word trans-
lation scores P (e | f) between pairs of words
?e, f?, i.e., P (e | f) =
c(e,f)?
e
c(e,f)
(Koehn et
al., 2003). Our latent domain-focused lexical
weights, on the other hand, are estimated ac-
cording to P (e | f, D
1
), i.e., P (e | f, D
1
) =
P (e | f)P (D
1
| e, f)?
f
P (e | f)P (D
1
| e, f)
.
The lexicalized reordering models with orien-
tation variable O, P (O | e?,
?
f), model how likely
a phrase ?e?,
?
f? directly follows a previous phrase
(monotone), swaps positions with it (swap), or
is not adjacent to it (discontinous) (Koehn et al.,
2005). We make these domain-focused:
P (O | e?,
?
f,D
1
) =
P (O | e?,
?
f)P (D
1
| O, e?,
?
f)
?
O
P (O | e?,
?
f)P (D
1
| O, e?,
?
f)
(11)
Estimating P (D
1
| O, e?,
?
f) and P (D
1
| e, f) is
similar to estimating P (D
1
| e?,
?
f) and hinges on
the estimates of P (D
1
| e, f) during EM.
The baseline for the following experiments is a
standard state-of-the-art phrase-based system, in-
cluding two bi-directional phrase-based transla-
tion features, two bi-directional lexical weights,
six lexicalized reordering features, as well as the
penalties for word, phrase and distortion. We de-
velop three kinds of domain-adapted systems that
are different at their adaptation level to fit the task.
The first (Sys. 1) adapts only the phrase-based
models, using the same lexical weights, lexical-
ized reordering models and other penalties as the
baseline. The second (Sys. 2) adapts also the lex-
ical weights, fixing all other features as the base-
line. The third (Sys. 3) adapts both the phrase-
based models, lexical weights and lexicalized re-
572
Translation Examples
Input El reproductor puede reproducir se?nales de audio grabadas en mix-mode cd, cd-g, cd-extra y cd text.
Reference The player can play back audio signals recorded in mix-mode cd, cd-g, cd-extra and cd text.
Baseline The player can reproduce signs of audio recorded in mix-mode cd, cd-g, cd-extra and cd text.
Our System The player can play signals audio recorded in mix-mode cd, cd-g, cd-extra and cd text.
Input Se puede crear un archivo autodescodificable cuando el archivo codificado se abre con la contrase?na maestra.
Reference A self-decrypting file can be created when the encrypted file is opened with the master password.
Baseline To create an file autodescodificable when the file codified commenced with the password teacher.
Our System You can create an archive autodescodificable when the file codified opens with the password master.
Input Repite todas las pistas (?unicamente cds de v??deo sin pbc)
Reference Repeat all tracks (non-pbc video cds only)
Baseline Repeated all avenues (only cds video without pbc)
Our System Repeated all the tracks (only cds video without pbc)
Table 5: Translation examples yielded by a domain-confused phrase-based system (the baseline) and a
domain-focused phrase-based system (our system).
ordering models
15
, fixing other penalties as the
baseline.
Metric
System Avg ? p-value
Consumer and Industrial Electronics
(In-domain: 109K pairs; Dev: 982 pairs; Test: 984 pairs)
BLEU
Baseline 22.9 ? ?
Sys. 1 23.4 +0.5 0.008
Sys. 2 23.9 +1.0 0.0001
Sys. 3 24.0 +1.1 0.0001
METEOR
Baseline
30.0
? ?
Sys. 1 30.4 +0.4 0.0001
Sys. 2 30.8 +0.8 0.0001
Sys. 3 30.9 +0.9 0.0001
TER
Baseline 59.5 ? ?
Sys. 1 58.8 -0.7 0.0001
Sys. 2 58.0 -1.5 0.0001
Sys. 3 57.9 -1.6 0.0001
Table 7: Metric scores for the systems, which are
averages over multiple runs.
Table 7 presents results for training data size
of 4M parallel sentences. It shows that the fully
domain-focused system (Sys. 3) significantly im-
proves over the baseline. The table also shows
that the latent domain-focused phrase-based mod-
els and lexical weights are crucial for the im-
proved performance, whereas adapting the re-
ordering models makes a far smaller contribution.
Finally we also apply our approach to other
15
We run three EM iterations to train our invitation frame-
work, and then use the parameter estimates for P (D
1
| e?,
?
f),
P (D
1
| e, f) and P (D
1
| O, e?,
?
f) to train these domain-
focused features. We adopt this training setting for all other
different tasks in the sequel.
tasks where the relation between their in-domain
data and the mix-domain data varies substantially.
Table 8 presents their in-domain, tuning and test
data in detail, as well as the translation results
over them. It shows that the fully domain-focused
systems consistently and significantly improve the
translation accuracy for all the tasks.
7 Combining multiple models
Finally, we proceed further to test our latent
domain-focused phrase-based translation model
on standard domain adaptation. We conduct ex-
periments on the task ?Professional & Business
Services? as an example.
16
For standard adap-
tation we follow (Koehn and Schroeder, 2007)
where we pass multiple phrase tables directly to
the Moses decoder and tune them together. For
baseline we combine the standard phrase-based
system trained on C
mix
with the one trained on
the in-domain data C
in
. We also combine our la-
tent domain-focused phrase-based system with the
one trained on C
in
. Table 9 presents the results
showing that combining our domain-focused sys-
tem adapted from C
mix
with the in-domain model
outperforms the baseline.
16
We choose this task for additional experiments because
it has very small in-domain data (23K). This is supposed
to make adaptation difficult because of the robust large-scale
systems trained on C
mix
.
573
Metric
System Avg ? p-value
Professional & Business Services
(In-domain: 23K pairs; Dev: 1, 000 pairs; Test: 998 pairs)
BLEU
Baseline 22.0 ? ?
Our System 23.1 +1.1 0.0001
METEOR
Baseline 30.8 ? ?
Our System 31.4 +0.6 0.0001
TER
Baseline 58.0 ? ?
Our System 56.6 -1.4 0.0001
Financials
(In-domain: 31K pairs; Dev: 1, 000 pairs; Test: 1, 000 pairs)
BLEU
Baseline 31.1 ? ?
Our System 31.8 +0.7 0.0001
METEOR
Baseline 36.3 ? ?
Our System 36.6 +0.3 0.0001
TER
Baseline 48.8 ? ?
Our System 48.3 -0.5 0.0001
Computer Hardware
(In-domain: 52K pairs; Dev: 1, 021 pairs; Test: 1, 054 pairs)
BLEU
Baseline 24.6 ? ?
Our System 25.3 +0.7 0.0001
METEOR
Baseline 32.4 ? ?
Our System 33.1 +0.7 0.0001
TER
Baseline 56.4 ? ?
Our System 55.0 -1.4 0.0001
Computer Software
(In-domain: 65K pairs; Dev: 1, 100 pairs; Test: 1, 000 pairs)
BLEU
Baseline 27.4 ? ?
Our System 28.3 +0.9 0.0001
METEOR
Baseline 34.0 ? ?
Our System 34.7 +0.7 0.0001
TER
Baseline 51.7 ? ?
Our System 50.6 -1.1 0.0001
Pharmaceuticals & Biotechnology
(In-domain: 85K pairs; Dev: 920 pairs; Test: 1, 000 pairs)
BLEU
Baseline 31.6 ? ?
Our System 32.4 +0.8 0.0001
METEOR
Baseline 34.0 ? ?
Our System 34.4 +0.4 0.0001
TER
Baseline 51.4 ? ?
Our System 50.6 -0.8 0.0001
Table 8: Metric scores for the systems, which are
averages over multiple runs.
8 Related work
A distantly related, but clearly complementary,
line of research focuses on the role of docu-
ment topics (Eidelman et al., 2012; Zhang et al.,
2014; Hasler et al., 2014). An off-the-shelf Latent
Dirichlet Allocation tool is usually used to infer
document-topic distributions. On one hand, this
setting may not require in-domain data as prior.
On the other hand, it requires meta-information
(e.g., document information).
Part of this work (the latent sentence-relevance
models) relates to data selection (Moore and
Lewis, 2010; Axelrod et al., 2011), where
sentence-relevance weights are used for hard-
Metric
System Avg ? p-value
Professional & Business Services
(In-domain: 23K pairs; Dev: 1, 000 pairs; Test: 998 pairs)
BLEU
In-domain 46.5 ? ?
+ Mix-domain 46.6 ? ?
+ Our system 47.9 +1.3 0.0001
METEOR
In-domain 39.8 ? ?
+ Mix-domain 40.1 ? ?
+ Our System 41.1 +1.0 0.0001
TER
In-domain 38.2 ? ?
+ Mix-domain 38.0 ? ?
+ Our System 36.9 -1.1 0.0001
Table 9: Domain adaptation experiments. Metric
scores for the systems, which are averages over
multiple runs.
filtering rather than weighting. The idea of using
sentence-relevance estimates for phrase-relevance
estimates relates to Matsoukas et al. (2009) who
estimate the former using meta-information over
documents as main features. In contrast, our work
overcomes the mutual dependence of sentence and
phrase estimates on one another by training both
models in tandem.
Adaptation using small in-domain data has
a different but complementary goal to another
line of research aiming at combining a domain-
adapted system with the another trained on the in-
domain data (Koehn and Schroeder, 2007; Bisazza
et al., 2011; Sennrich, 2012; Razmara et al., 2012;
Sennrich et al., 2013). Our work is somewhat re-
lated to, but markedly different from, phrase pair
weighting (Foster et al., 2010). Finally, our latent
domain-focused phrase-based models and invita-
tion training paradigm can be seen to shift atten-
tion from adaptation to making explicit the role of
domain-focused models in SMT.
9 Conclusion
We present a novel approach for in-domain fo-
cused training of a phrase-based system on a
mix-of-domain corpus by using prior distributions
from a small in-domain corpus. We derive an EM
training algorithm for learning latent domain rel-
evance models for the phrase- and sentence-levels
in tandem. We also show how to overcome the
difficulty of lack of explicit out-domain data by
bootstrapping pseudo out-domain data.
In future work, we plan to explore generative
Bayesian models as well as discriminative learn-
ing approaches with different ways for estimat-
574
ing the latent domain relevance models. We hy-
pothesize that bilingual, but also monolingual, rel-
evance models can be key to improved perfor-
mance.
Acknowledgements
We thank Ivan Titov for stimulating discussions,
and three anonymous reviewers for their com-
ments on earlier versions. The first author is sup-
ported by the EXPERT (EXPloiting Empirical ap-
pRoaches to Translation) Initial Training Network
(ITN) of the European Union?s Seventh Frame-
work Programme. The second author is sup-
ported by VICI grant nr. 277-89-002 from the
Netherlands Organization for Scientific Research
(NWO). We thank TAUS for providing us with
suitable data.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 355?362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus interpolation methods for
phrase-based smt adaptation. In IWSLT, pages 136?
143.
Daniel Cer, Michel Galley, Daniel Jurafsky, and
Christopher D. Manning. 2010. Phrasal: A toolkit
for statistical machine translation with facilities for
extraction and incorporation of arbitrary model fea-
tures. In Proceedings of the NAACL HLT 2010
Demonstration Session, HLT-DEMO ?10, pages 9?
12, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ?12, pages 427?436, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: Short Pa-
pers - Volume 2, HLT ?11, pages 176?181, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Hoang Cuong and Khalil Sima?an. 2014. La-
tent domain translation models in mix-of-domains
haystack. In Proceedings of COLING 2014, the
25th International Conference on Computational
Linguistics: Technical Papers, pages 1928?1939,
Dublin, Ireland, August. Dublin City University and
Association for Computational Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. JOURNAL OF THE ROYAL STATIS-
TICAL SOCIETY, SERIES B, 39(1):1?38.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ?11, pages 85?91, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic transla-
tion model adaptation. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Short Papers - Volume 2, ACL
?12, pages 115?119, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 451?459, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Guillem Gasc?o, Martha-Alicia Rocha, Germ?an
Sanchis-Trilles, Jes?us Andr?es-Ferrer, and Francisco
Casacuberta. 2012. Does more data always yield
better translations? In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL ?12, pages
152?161, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry
Haddow. 2014. Dynamic topic adaptation for
phrase-based mt. In Proceedings of the 14th Con-
ference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 328?337,
Gothenburg, Sweden, April. Association for Com-
putational Linguistics.
Ann Irvine, John Morgan, Marine Carpuat, Daume Hal
III, and Dragos Munteanu. 2013. Measuring ma-
chine translation errors in new domains. pages 429?
440.
575
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT
?07, pages 224?227, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Philipp Koehn, Amittai Axelrod, Alexandra Birch,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
International Workshop on Spoken Language Trans-
lation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Conference Pro-
ceedings: the tenth Machine Translation Summit,
pages 79?86, Phuket, Thailand. AAMT, AAMT.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight es-
timation for machine translation. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2 - Volume
2, EMNLP ?09, pages 708?717, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 220?224, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Radford M. Neal and Geoffrey E. Hinton. 1999.
Learning in graphical models. chapter A View
of the EM Algorithm That Justifies Incremental,
Sparse, and Other Variants, pages 355?368. MIT
Press, Cambridge, MA, USA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Comput. Linguist., 30(4):417?449, Decem-
ber.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?11, pages 258?267, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 1992. Numerical
Recipes in C (2Nd Ed.): The Art of Scientific Com-
puting. Cambridge University Press, New York,
NY, USA.
Majid Razmara, George Foster, Baskaran Sankaran,
and Anoop Sarkar. 2012. Mixing multiple trans-
lation models in statistical machine translation. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers
- Volume 1, ACL ?12, pages 940?949, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Rico Sennrich, Holger Schwenk, and Walid Aransa.
2013. A multi-domain translation model frame-
work for statistical machine translation. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 832?840, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the 13th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, EACL ?12,
pages 539?549, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Matthew Snover, Bonnie Dorr, R. Schwartz, L. Micci-
ulla, and J. Makhoul. 2006. A study of translation
edit rate with targeted human annotation. In Pro-
ceedings of Association for Machine Translation in
the Americas, pages 223?231.
Min Zhang, Xinyan Xiao, Deyi Xiong, and Qun Liu.
2014. Topic-based dissimilarity and sensitivity
models for translation rule selection. Journal of Ar-
tificial Intelligence Research, 50(1):1?30.
576
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 642?652,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning Hierarchical Translation Structure with Linguistic Annotations
Markos Mylonakis
ILLC
University of Amsterdam
m.mylonakis@uva.nl
Khalil Sima?an
ILLC
University of Amsterdam
k.simaan@uva.nl
Abstract
While it is generally accepted that many trans-
lation phenomena are correlated with linguis-
tic structures, employing linguistic syntax for
translation has proven a highly non-trivial
task. The key assumption behind many ap-
proaches is that translation is guided by the
source and/or target language parse, employ-
ing rules extracted from the parse tree or
performing tree transformations. These ap-
proaches enforce strict constraints and might
overlook important translation phenomena
that cross linguistic constituents. We propose
a novel flexible modelling approach to intro-
duce linguistic information of varying gran-
ularity from the source side. Our method
induces joint probability synchronous gram-
mars and estimates their parameters, by select-
ing and weighing together linguistically moti-
vated rules according to an objective function
directly targeting generalisation over future
data. We obtain statistically significant im-
provements across 4 different language pairs
with English as source, mounting up to +1.92
BLEU for Chinese as target.
1 Introduction
Recent advances in Statistical Machine Translation
(SMT) are widely centred around two concepts:
(a) hierarchical translation processes, frequently
employing Synchronous Context Free Grammars
(SCFGs) and (b) transduction or synchronous
rewrite processes over a linguistic syntactic tree.
SCFGs in the form of the Inversion-Transduction
Grammar (ITG) were first introduced by (Wu, 1997)
as a formalism to recursively describe the trans-
lation process. The Hiero system (Chiang, 2005)
utilised an ITG-flavour which focused on hierarchi-
cal phrase-pairs to capture context-driven translation
and reordering patterns with ?gaps?, offering com-
petitive performance particularly for language pairs
with extensive reordering. As Hiero uses a single
non-terminal and concentrates on overcoming trans-
lation lexicon sparsity, it barely explores the recur-
sive nature of translation past the lexical level. Nev-
ertheless, the successful employment of SCFGs for
phrase-based SMT brought translation models as-
suming latent syntactic structure to the spotlight.
Simultaneously, mounting efforts have been di-
rected towards SMT models employing linguistic
syntax on the source side (Yamada and Knight,
2001; Quirk et al, 2005; Liu et al, 2006), target
side (Galley et al, 2004; Galley et al, 2006) or both
(Zhang et al, 2008; Liu et al, 2009; Chiang, 2010).
Hierarchical translation was combined with target
side linguistic annotation in (Zollmann and Venu-
gopal, 2006). Interestingly, early on (Koehn et al,
2003) exemplified the difficulties of integrating lin-
guistic information in translation systems. Syntax-
based MT often suffers from inadequate constraints
in the translation rules extracted, or from striving to
combine these rules together towards a full deriva-
tion. Recent research tries to address these issues,
by re-structuring training data parse trees to bet-
ter suit syntax-based SMT training (Wang et al,
2010), or by moving from linguistically motivated
synchronous grammars to systems where linguistic
plausibility of the translation is assessed through ad-
ditional features in a phrase-based system (Venu-
gopal et al, 2009; Chiang et al, 2009), obscuring
the impact of higher level syntactic processes.
While it is assumed that linguistic structure does
correlate with some translation phenomena, in this
642
work we do not employ it as the backbone of trans-
lation. In place of linguistically constrained trans-
lation imposing syntactic parse structure, we opt for
linguistically motivated translation. We learn latent
hierarchical structure, taking advantage of linguistic
annotations but shaped and trained for translation.
We start by labelling each phrase-pair span in the
word-aligned training data with multiple linguisti-
cally motivated categories, offering multi-grained
abstractions from its lexical content. These phrase-
pair label charts are the input of our learning al-
gorithm, which extracts the linguistically motivated
rules and estimates the probabilities for a stochastic
SCFG, without arbitrary constraints such as phrase
or span sizes. Estimating such grammars under
a Maximum Likelihood criterion is known to be
plagued by strong overfitting leading to degener-
ate estimates (DeNero et al, 2006). In contrast,
our learning objective not only avoids overfitting
the training data but, most importantly, learns joint
stochastic synchronous grammars which directly
aim at generalisation towards yet unseen instances.
By advancing from structures which mimic lin-
guistic syntax, to learning linguistically aware latent
recursive structures targeting translation, we achieve
significant improvements in translation quality for 4
different language pairs in comparison with a strong
hierarchical translation baseline.
Our key contributions are presented in the fol-
lowing sections. Section 2 discusses the weak in-
dependence assumptions of SCFGs and introduces
a joint translation model which addresses these is-
sues and separates hierarchical translation structure
from phrase-pair emission. In section 3 we consider
a chart over phrase-pair spans filled with source-
language linguistically motivated labels. We show
how we can employ this crucial input to extract and
train a hierarchical translation structure model with
millions of rules. Section 4 demonstrates decoding
with the model by constraining derivations to lin-
guistic hints of the source sentence and presents our
empirical results. We close with a discussion of re-
lated work and our conclusions.
2 Joint Translation Model
Our model is based on a probabilistic Synchronous
CFG (Wu, 1997; Chiang, 2005). SCFGs define a
SBAR ? [WHNP SBAR\WHNP] (a)
SBAR\WHNP ? ?VP/NPL NPR? (b)
NPR ? [NP PP] (c)
WHNP ? WHNPP (d)
WHNPP ? which / der (e)
VP/NPL ? VP/NPLP (f)
VP/NPLP ? is / ist (g)
NPR ? NPRP (h)
NPRP ? the solution / die Lo?sung (i)
NP ? NPP (j)
NPP ? the solution / die Lo?sung (k)
PP ? PPP (l)
PPP ? to the problem / fu?r das Problem (m)
Figure 1: English-German SCFG rules for the relative
clause(s) ?which is the solution (to the problem) / der die
Lo?sung (fu?r das Problem) ist?, [ ] signify monotone trans-
lation, ? ? a swap reordering.
language over string pairs, which are generated be-
ginning from a start symbol S and recursively ex-
panding pairs of linked non-terminals across the two
strings using the grammar?s rule set. By crossing the
links between the non-terminals of the two sides re-
ordering phenomena are captured. We employ bi-
nary SCFGs, i.e. grammars with a maximum of two
non-terminals on the right-hand side. Also, for this
work we only used grammars with either purely lexi-
cal or purely abstract rules involving one or two non-
terminal pairs. An example can be seen in Figure 1,
using an ITG-style notation and assuming the same
non-terminal labels for both sides.
We utilise probabilistic SCFGs, where each rule
is assigned a conditional probability of expanding
the left-hand side symbol with the rule?s right-hand
side. Phrase-pairs are emitted jointly and the over-
all probabilistic SCFG is a joint model over parallel
strings.
2.1 SCFG Reordering Weaknesses
An interesting feature of all probabilistic SCFGs
(i.e. not only binary ones), which has received sur-
prisingly little attention, is that the reordering pat-
643
tern between the non-terminal pairs (or in the case
of ITGs the choice between monotone and swap ex-
pansion) are not conditioned on any other part of a
derivation. The result is that, the reordering pattern
with the highest probability will always be preferred
(e.g. in the Viterbi derivation) over the rest, irre-
spective of lexical or abstract context. As an ex-
ample, a probabilistic SCFG will always assign a
higher probability to derivations swapping or mono-
tonically translating nouns and adjectives between
English and French, only depending on which of the
two rules NP ? [NN JJ ], NP ? ?NN JJ?
has a higher probability. The rest of the (sometimes
thousands of) rule-specific features usually added to
SCFG translation models do not directly help either,
leaving reordering decisions disconnected from the
rest of the derivation.
While in a decoder this is somehow mitigated by
the use of a language model, we believe that the
weakness of straightforward applications of SCFGs
to model reordering structure at the sentence level
misses a chance to learn this crucial part of the
translation process during grammar induction. As
(Mylonakis and Sima?an, 2010) note, ?plain? SCFGs
seem to perform worse than the grammars described
next, mainly due to wrong long-range reordering de-
cisions for which the language model can hardly
help.
2.2 Hierarchical Reordering SCFG
We address the weaknesses mentioned above by re-
lying on an SCFG grammar design that is similar to
the ?Lexicalised Reordering? grammar of (Mylon-
akis and Sima?an, 2010). As in the rules of Fig-
ure 1, we separate non-terminals according to the
reordering patterns in which they participate. Non-
terminals such as BL, CR take part only in swap-
ping right-hand sides ?BL CR? (with BL swap-
ping from the source side?s left to the target side?s
right, CR swapping in the opposite direction), while
non-terminals such as B, C take part solely in mono-
tone right-hand side expansions [B C]. These non-
terminal categories can appear also on the left-hand
side of a rule, as in rule (c) of Figure 1.
In contrast with (Mylonakis and Sima?an, 2010),
monotone and swapping non-terminals do not emit
phrase-pairs themselves. Rather, each non-terminal
NT is expanded to a dedicated phrase-pair emit-
A ? [B C] A ? ?BL CR?
AL ? [B C] AL ? ?BL CR?
AR ? [B C] AR ? ?BL CR?
A ? AP AP ? ? / ?
AL ? ALP A
L
P ? ? / ?
AR ? ARP A
R
P ? ? / ?
Figure 2: Recursive Reordering Grammar rule cate-
gories; A, B, C non-terminals; ?, ? source and target
strings respectively.
ting non-terminal NTP, which generates all phrase-
pairs for it and nothing more. In this way, the pref-
erence of non-terminals to either expand towards
a (long) phrase-pair or be further analysed recur-
sively is explicitly modelled. Furthermore, this set
of pre-terminals allows us to separate the higher or-
der translation structure from the process that emits
phrase-pairs, a feature we employ next.
In (Mylonakis and Sima?an, 2010) this grammar
design mainly contributed to model lexical reorder-
ing preferences. While we retain this function, for
the rich linguistically-motivated grammars used in
this work this design effectively propagates reorder-
ing preferences above and below the current rule ap-
plication (e.g. Figure 1, rules (a)-(c)), allowing to
learn and apply complex reordering patterns.
The different types of grammar rules are sum-
marised in abstract form in Figure 2. We will subse-
quently refer to this grammar structure as Hierarchi-
cal Reordering SCFG (HR-SCFG).
2.3 Generative Model
We arrive at a probabilistic SCFG model which
jointly generates source e and target f strings, by
augmenting each grammar rule with a probability,
summing up to one for every left-hand side. The
probability of a derivation D of tuple ?e, f? begin-
ning from start symbol S is equal to the product of
the probabilities of the rules used to recursively gen-
erate it.
We separate the structural part of the derivation
D, down to the pre-terminals NTP, from the phrase-
emission part. The grammar rules pertaining to the
644
X, SBAR, WHNP+VP, WHNP+VBZ+NP
X, VBZ+NP, VP, SBAR\WHNP
X, SBAR/NN, WHNP+VBZ+DT
X, VBZ+DT, VP/NN
X, WHNP+VBZ, X, NP,
SBAR/NP VP\VBZ
X, WHNP, X, VBZ, X, DT, X, NN,
SBAR/VP VP/NP NP/NN NP\DT
which is the problem
Figure 3: The label chart for the source fragment ?which
is the problem?. Only a sample of the entries is listed.
structural part and their associated probabilities de-
fine a model p(?) over the latent variable ? de-
termining the recursive, reordering and phrase-pair
segmenting structure of translation, as in Figure 4.
Given ?, the phrase-pair emission part merely gener-
ates the phrase-pairs utilising distributions from ev-
ery NTP to the phrase-pairs that it covers, thereby
defining a model over all sentence-pairs generated
given each translation structure. The probabilities of
a derivation and of a sentence-pair are then as fol-
lows:
p(D) =p(?)p(e, f |?) (1)
p(e, f) =
?
D:D
?
??e,f?
p(D) (2)
By splitting the joint model in a hierarchical struc-
ture model and a lexical emission one we facilitate
estimating the two models separately. The following
section discusses this.
3 Learning Translation Structure
3.1 Phrase-Pair Label Chart
The input to our learning algorithm is a word-
aligned parallel corpus. We consider as phrase-
pair spans those that obey the word-alignment con-
straints of (Koehn et al, 2003). For every train-
ing sentence-pair, we also input a chart containing
one or more labels for every synchronous span, such
as that of Figure 3. Each label describes differ-
ent properties of the phrase pair (syntactic, semantic
etc.), possibly in relation to its context, or supply-
ing varying levels of abstraction (phrase-pair, deter-
miner with noun, noun-phrase, sentence etc.). We
aim to induce a recursive translation structure ex-
plaining the joint generation of the source and target
sentence taking advantage of these phrase-pair span
labels.
For this work we employ the linguistically mo-
tivated labels of (Zollmann and Venugopal, 2006),
albeit for the source language. Given a parse of the
source sentence, each span is assigned the following
kind of labels:
Phrase-Pair All phrase-pairs are assigned the X
label
Constituent Source phrase is a constituent A
Concatenation of Constituents Source phrase la-
belled A+B as a concatenation of constituents A and
B, similarly for 3 constituents.
Partial Constituents Categorial grammar (Bar-
Hillel, 1953) inspired labels A/B, A\B, indicating
a partial constituent A missing constituent B right or
left respectively.
An important point is that we assign all applica-
ble labels to every span. In this way, each label set
captures the features of the source side?s parse-tree
without being bounded by the actual parse structure,
as well as provides a coarse to fine-grained view of
the source phrase.
3.2 Grammar Extraction
From every word-aligned sentence-pair and its la-
bel chart, we extract SCFG rules as those of Figure
2. Binary rules are extracted from adjoining syn-
chronous spans up to the whole sentence-pair level,
with the non-terminals of both left and right-hand
side derived from the label names plus their reorder-
ing function (monotone, left/right swapping) in the
span examined. A single unary rule per non-terminal
NT generates the phrase-pair emitting NTP. Unary
rules NTP ? ? / ? generating the phrase-pair are
created for all the labels covering it.
While we label the phrase-pairs similarly to (Zoll-
mann and Venugopal, 2006), the extracted grammar
is rather different. We do not employ rules that are
grounded to lexical context (?gap? rules), relying in-
stead on the reordering-aware non-terminal set and
related unary and binary rules. The result is a gram-
mar which can both capture a rich array of trans-
lation phenomena based on linguistic and lexical
grounds and explicitly model the balance between
645
SBAR
WHNP
WHNPP
which
der
< SBAR\WHNP >
VP/NPL
VP/NPLP
is
ist
NPR
NP
NPP
the solution
die Lo?sung
PP
PPP
to the problem
fu?r das Problem
Figure 4: A derivation of a sentence fragment with the
grammar of Figure 1.
memorising long phrase-pairs and generalising over
yet unseen ones, as shown in the next example.
The derivation in Figure 4 illustrates some of the
formalism?s features. A preference to reorder based
on lexical content is applied for is / ist. Noun phrase
NPR is recursively constructed with a preference to
constitute the right branch of an order swapping non-
terminal expansion. This is matched with VP/NPL
which reorders in the opposite direction. The labels
VP/NP and SBAR\WHNP allow linguistic syntax
context to influence the lexical and reordering trans-
lation choices. Crucially, all these lexical, attach-
ment and reordering preferences (as encoded in the
model?s rules and probabilities) must be matched to-
gether to arrive at the analysis in Figure 4.
3.3 Parameter Estimation
We estimate the parameters for the phrase-emission
model p(e, f |?) using Relative Frequency Estima-
tion (RFE) on the label charts induced for the train-
ing sentence-pairs, after the labels have been aug-
mented by the reordering indications. In the RFE
estimate, every rule NTP ? ? / ? receives a prob-
ability in proportion with the times that ? / ? was
covered by the NT label.
On the other hand, estimating the parameters un-
der Maximum-Likelihood Estimation (MLE) for the
latent translation structure model p(?) is bound to
overfit towards memorising whole sentence-pairs as
discussed in (Mylonakis and Sima?an, 2010), with
the resulting grammar estimate not being able to
generalise past the training data. However, apart
from overfitting towards long phrase-pairs, a gram-
mar with millions of structural rules is also liable to
overfit towards degenerate latent structures which,
while fitting the training data well, have limited ap-
plicability to unseen sentences.
We avoid both pitfalls by estimating the grammar
probabilities with the Cross-Validating Expectation-
Maximization algorithm (CV-EM) (Mylonakis and
Sima?an, 2008; Mylonakis and Sima?an, 2010). CV-
EM is a cross-validating instance of the well known
EM algorithm (Dempster et al, 1977). It works it-
eratively on a partition of the training data, climb-
ing the likelihood of the training data while cross-
validating the latent variable values, considering for
every training data point only those which can be
produced by models built from the rest of the data
excluding the current part. As a result, the estima-
tion process simulates maximising future data likeli-
hood, using the training data to directly aim towards
strong generalisation of the estimate.
For our probabilistic SCFG-based translation
structure variable ?, implementing CV-EM boils
down to a synchronous version of the Inside-Outside
algorithm, modified to enforce the CV criterion. In
this way we arrive at cross-validated ML estimate of
the ? parameters while keeping the phrase-emission
parameters of p(e, f |?) fixed. The CV-criterion,
apart from avoiding overfitting, results in discarding
the structural rules which are only found in a single
part of the training corpus, leading to a more com-
pact grammar while still retaining millions of struc-
tural rules that are more hopeful to generalise.
Unravelling the joint generative process, by mod-
elling latent hierarchical structure separately from
phrase-pair emission, allows us to concentrate our
inference efforts towards the hidden, higher-level
translation mechanism.
4 Experiments
4.1 Decoding Model
The induced joint translation model can be used
to recover argmaxe p(e|f), as it is equal to
argmaxe p(e, f). We employ the induced proba-
bilistic HR-SCFGG as the backbone of a log-linear,
feature based translation model, with the derivation
probability p(D) under the grammar estimate being
646
one of the features. This is augmented with a small
number n of additional smoothing features ?i for
derivation rules r: (a) conditional phrase translation
probabilities, (b) lexical phrase translation probabil-
ities, (c) word generation penalty, and (d) a count
of swapping reordering operations. Features (a), (b)
and (c) are applicable to phrase-pair emission rules
and features for both translation directions are used,
while (d) is only triggered by structural rules.
These extra features assess translation quality past
the synchronous grammar derivation and learning
general reordering or word emission preferences
for the language pair. As an example, while our
probabilistic HR-SCFG maintains a separate joint
phrase-pair emission distribution per non-terminal,
the smoothing features (a) above assess the condi-
tional translation of surface phrases irrespective of
any notion of recursive translation structure.
The final feature is the language model score
for the target sentence, mounting up to the follow-
ing model used at decoding time, with the feature
weights ? trained by Minimum Error Rate Training
(MERT) (Och, 2003) on a development corpus.
p(D
?
? ?e, f?) ? p(e)?lmpG(D)
?G
n?
i=1
?
r?D
?i(r)
?i
4.2 Decoding Modifications
We use a customised version of the Joshua SCFG
decoder (Li et al, 2009) to translate, with the fol-
lowing modifications:
Source Labels Constraints As for this work the
phrase-pair labels used to extract the grammar are
based on the linguistic analysis of the source side,
we can construct the label chart for every input sen-
tence from its parse. We subsequently use it to con-
sider only derivations with synchronous spans which
are covered by non-terminals matching one of the
labels for those spans. This applies both for the non-
terminals covering phrase-pairs as well as the higher
level parts of the derivation.
In this manner we not only constrain the trans-
lation hypotheses resulting in faster decoding time,
but, more importantly, we may ground the hypothe-
ses more closely to the available linguistic informa-
tion of the source sentence. This is of particular
interest as we move up the derivation tree, where
an initial wrong choice below could propagate to-
wards hypotheses wildly diverging from the input
sentence?s linguistic annotation.
Per Non-Terminal Pruning The decoder uses a
combination of beam and cube-pruning (Huang and
Chiang, 2007). As our grammar uses non-terminals
in the hundreds of thousands, it is important not
to prune away prematurely non-terminals covering
smaller spans and to leave more options to be con-
sidered as we move up the derivation tree.
For this, for every cell in the decoder?s chart, we
keep a separate bin per non-terminal and prune to-
gether hypotheses leading to the same non-terminal
covering a cell. This allows full derivations to be
found for all input sentences, as well as avoids ag-
gressive pruning at an early stage. Given the source
label constraint discussed above, this does not in-
crease running times or memory demands consid-
erably as we allow only up to a few tens of non-
terminals per span.
Expected Counts Rule Pruning To compact the
hierarchical structure part of the grammar prior to
decoding, we prune rules that fail to accumulate
10?8 expected counts during the last CV-EM iter-
ation. For English to German, this brings the struc-
tural rules from 15M down to 1.2M. Note that we
do not prune the phrase-pair emitting rules. Over-
all, we consider this a much more informed pruning
criterion than those based on probability values (that
are not comparable across left-hand sides) or right-
hand side counts (frequent symbols need many more
expansions than a highly specialised one).
4.3 Experimental Setting & Baseline
We evaluate our method on four different lan-
guage pairs with English as the source language
and French, German, Dutch and Chinese as tar-
get. The data for the first three language pairs are
derived from parliament proceedings sourced from
the Europarl corpus (Koehn, 2005), with WMT-
07 development and test data for French and Ger-
man. The data for the English to Chinese task is
composed of parliament proceedings and news arti-
cles. For all language pairs we employ 200K and
400K sentence pairs for training, 2K for develop-
ment and 2K for testing (single reference per source
sentence). Both the baseline and our method decode
647
Training English to
French German Dutch Chinese
set size BLEU NIST BLEU NIST BLEU NIST BLEU NIST
200K
josh-base 29.20 7.2123 18.65 5.8047 21.97 6.2469 22.34 6.5540
lts 29.43 7.2611** 19.10** 5.8714** 22.31* 6.2903* 23.67** 6.6595**
400K
josh-base 29.58 7.3033 18.86 5.8818 22.25 6.2949 23.24 6.7402
lts 29.83 7.4000** 19.49** 5.9374** 22.92** 6.3727** 25.16** 6.9005**
Table 1: Experimental results for training sets of 200K and 400K sentence pairs. Statistically significant score im-
provements from the baseline at the 95% confidence level are labelled with a single star, at the 99% level with two.
with a 3-gram language model smoothed with modi-
fied Knesser-Ney discounting (Chen and Goodman,
1998), trained on around 1M sentences per target
language. The parses of the source sentences em-
ployed by our system during training and decod-
ing are created with the Charniak parser (Charniak,
2000).
We compare against a state-of-the-art hierarchi-
cal translation (Chiang, 2005) baseline, based on the
Joshua translation system under the default training
and decoding settings (josh-base). Apart of eval-
uating against a state-of-the-art system, especially
on the English-Chinese language pair, the compar-
ison has an added interesting aspect. The heuristi-
cally trained baseline takes advantage of ?gap rules?
to reorder based on lexical context cues, but makes
very limited use of the hierarchical structure above
the lexical surface. In contrast, our method induces
a grammar with no such rules, relying on lexical
content and the strength of a higher level translation
structure instead.
4.4 Training & Decoding Details
To train our Latent Translation Structure (LTS) sys-
tem, we used the following settings. CV-EM cross-
validated on a 10-part partition of the training data
and performed 10 iterations. The structural rule
probabilities were initialised to uniform per left-
hand side.
The decoder does not employ any ?glue grammar?
as is usual with hierarchical translation systems to
limit reordering up to a certain cut-off length. In-
stead, we rely on our LTS grammar to reorder and
construct the translation output up to the full sen-
tence length.
In summary, our system?s experimental pipeline is
as follows. All input sentences are parsed and label
charts are created from these parses. The Hierarchi-
cal Reordering SCFG is extracted and its parame-
ters are estimated employing CV-EM. The structural
rules of the estimate are pruned according to their
expected counts and smoothing features are added to
all rules. We train the feature weights under MERT
and decode with the resulting log-linear model.
The overall training and decoding setup is appeal-
ing also regarding computational demands. On an
8-core 2.3GHz system, training on 200K sentence-
pairs demands 4.5 hours while decoding runs on 25
sentences per minute.
4.5 Results
Table 1 presents the results for the baseline and our
method for the 4 language pairs, for training sets of
both 200K and 400K sentence pairs. Our system
(lts) outperforms the baseline for all 4 language
pairs for both BLEU and NIST scores, by a margin
which scales up to +1.92 BLEU points for English to
Chinese translation when training on the 400K set.
In addition, increasing the size of the training data
from 200K to 400K sentence pairs widens the per-
formance margin between the baseline and our sys-
tem, in some cases considerably. All but one of the
performance improvements are found to be statis-
tically significant (Koehn, 2004) at the 95% confi-
dence level, most of them also at the 99% level.
We selected an array of target languages of
increasing reordering complexity with English as
source. Examining the results across the target lan-
guages, LTS performance gains increase the more
challenging the sentence structure of the target lan-
guage is in relation to the source?s, highlighted when
translating to Chinese. Even for Dutch and German,
which pose additional challenges such as compound
words and morphology which we do not explicitly
treat in the current system, LTS still delivers signif-
icant improvements in performance. Additionally,
648
System 200K 400K
(a)
lts-nolabels 22.50 24.24
lts 23.67** 25.16**
(b)
josh-base-lm4 23.81 24.77
lts-lm4 24.48** 26.35**
Table 2: Additional experiments for English to Chi-
nese translation examining (a) the impact of the linguis-
tic annotations in the LTS system (lts), when com-
pared with an instance not employing such annotations
(lts-nolabels) and (b) decoding with a 4th-order
language model (-lm4). BLEU scores for 200K and
400K training sentence pairs.
the robustness of our system is exemplified by deliv-
ering significant performance increases for all lan-
guage pairs.
For the English to Chinese translation task, we
performed further experiments along two axes. We
first investigate the contribution of the linguistic
annotations, by comparing our complete system
(lts) with an otherwise identical implementation
(lts-nolabels) which does not employ any lin-
guistically motivated labels. The latter system then
uses a labels chart as that of Figure 3, which however
labels all phrase-pair spans solely with the generic
X label. The results in Table 2(a) indicate that a
large part of the performance improvement can be
attributed to the use of the linguistic annotations ex-
tracted from the source parse trees, indicating the
potential of the LTS system to take advantage of
such additional annotations to deliver better trans-
lations.
The second additional experiment relates to the
impact of employing a stronger language model dur-
ing decoding, which may increase performance but
slows down decoding speed. Notably, as can be seen
in Table 2(b), switching to a 4-gram LM results in
performance gains for both the baseline and our sys-
tem and while the margin between the two systems
decreases, our system continues to deliver a con-
siderable and significant improvement in translation
BLEU scores.
5 Related Work
In this work, we focus on the combination of
learning latent structure with syntax and linguistic
annotations, exploring the crossroads of machine
learning, linguistic syntax and machine translation.
Training a joint probability model was first dis-
cussed in (Marcu and Wong, 2002). We show that
a translation system based on such a joint model
can perform competitively in comparison with con-
ditional probability models, when it is augmented
with a rich latent hierarchical structure trained ade-
quately to avoid overfitting.
Earlier approaches for linguistic syntax-based
translation such as (Yamada and Knight, 2001; Gal-
ley et al, 2006; Huang et al, 2006; Liu et al, 2006)
focus on memorising and reusing parts of the struc-
ture of the source and/or target parse trees and con-
straining decoding by the input parse tree. In con-
trast to this approach, we choose to employ lin-
guistic annotations in the form of unambiguous syn-
chronous span labels, while discovering ambiguous
translation structure taking advantage of them.
Later work (Marton and Resnik, 2008; Venugopal
et al, 2009; Chiang et al, 2009) takes a more flex-
ible approach, influencing translation output using
linguistically motivated features, or features based
on source-side linguistically-guided latent syntactic
categories (Huang et al, 2010). A feature-based ap-
proach and ours are not mutually exclusive, as we
also employ a limited set of features next to our
trained model during decoding. We find augment-
ing our system with a more extensive feature set an
interesting research direction for the future.
An array of recent work (Chiang, 2010; Zhang et
al., 2008; Liu et al, 2009) sets off to utilise source
and target syntax for translation. While for this work
we constrain ourselves to source language syntax
annotations, our method can be directly applied to
employ labels taking advantage of linguistic annota-
tions from both sides of translation. The decoding
constraints of section 4.2 can then still be applied on
the source part of hybrid source-target labels.
For the experiments in this paper we employ a la-
bel set similar to the non-terminals set of (Zollmann
and Venugopal, 2006). However, the synchronous
grammars we learn share few similarities with those
that they heuristically extract. The HR-SCFG we
adopt allows capturing more complex reordering
phenomena and, in contrast to both (Chiang, 2005;
Zollmann and Venugopal, 2006), is not exposed to
the issues highlighted in section 2.1. Nevertheless,
our results underline the capacity of linguistic anno-
649
tations similar to those of (Zollmann and Venugopal,
2006) as part of latent translation variables.
Most of the aforementioned work does concen-
trate on learning hierarchical, linguistically moti-
vated translation models. Cohn and Blunsom (2009)
sample rules of the form proposed in (Galley et al,
2004) from a Bayesian model, employing Dirich-
let Process priors favouring smaller rules to avoid
overfitting. Their grammar is however also based
on the target parse-tree structure, with their system
surpassing a weak baseline by a small margin. In
contrast to the Bayesian approach which imposes
external priors to lead estimation away from degen-
erate solutions, we take a data-driven approach to
arrive to estimates which generalise well. The rich
linguistically motivated latent variable learnt by our
method delivers translation performance that com-
pares favourably to a state-of-the-art system.
Mylonakis and Sima?an (2010) also employ the
CV-EM algorithm to estimate the parameters of an
SCFG, albeit a much simpler one based on a hand-
ful of non-terminals. In this work we employ some
of their grammar design principles for an immensely
more complex grammar with millions of hierarchi-
cal latent structure rules and show how such gram-
mar can be learnt and applied taking advantage of
source language linguistic annotations.
6 Conclusions
In this work we contribute a method to learn and
apply latent hierarchical translation structure. To
this end, we take advantage of source-language lin-
guistic annotations to motivate instead of constrain
the translation process. An input chart over phrase-
pair spans, with each cell filled with multiple lin-
guistically motivated labels, is coupled with the HR-
SCFG design to arrive at a rich synchronous gram-
mar with millions of structural rules and the capacity
to capture complex linguistically conditioned trans-
lation phenomena. We address overfitting issues by
cross-validating climbing the likelihood of the train-
ing data and propose solutions to increase the effi-
ciency and accuracy of decoding.
An interesting aspect of our work is delivering
competitive performance for difficult language pairs
such as English-Chinese with a joint probability
generative model and an SCFG without ?gap rules?.
Instead of employing hierarchical phrase-pairs, we
invest in learning the higher-order hierarchical syn-
chronous structure behind translation, up to the full
sentence length. While these choices and the related
results challenge current MT research trends, they
are not mutually exclusive with them. Future work
directions include investigating the impact of hierar-
chical phrases for our models as well as any gains
from additional features in the log-linear decoding
model.
Smoothing the HR-SCFG grammar estimates
could prove a possible source of further perfor-
mance improvements. Learning translation and re-
ordering behaviour with respect to linguistic cues
is facilitated in our approach by keeping separate
phrase-pair emission distributions per emitting non-
terminal and reordering pattern, while the employ-
ment of the generic X non-terminals already allows
backing off to more coarse-grained rules. Neverthe-
less, we still believe that further smoothing of these
sparse distributions, e.g. by interpolating them with
less sparse ones, could in the future lead to an addi-
tional increase in translation quality.
Finally, we discuss in this work how our method
can already utilise hundreds of thousands of phrase-
pair labels and millions of structural rules. A fur-
ther promising direction is broadening this set with
labels taking advantage of both source and target-
language linguistic annotation or categories explor-
ing additional phrase-pair properties past the parse
trees such as semantic annotations.
Acknowledgments
Both authors are supported by a VIDI grant (nr.
639.022.604) from The Netherlands Organization
for Scientific Research (NWO). The authors would
like to thank Maxim Khalilov for helping with
experimental data and Andreas Zollmann and the
anonymous reviewers for their valuable comments.
References
Yehoshua Bar-Hillel. 1953. A quasi-arithmetical nota-
tion for syntactic description. Language, 29(1):47?58.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the North American Asso-
ciation for Computational Linguistics (HLT/NAACL),
Seattle, Washington, USA, April.
650
Stanley Chen and Joshua Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Harvard University, Au-
gust.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 218?226, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263?270.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model
of syntax-directed tree to string grammar induction.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
352?361, Singapore, August. Association for Compu-
tational Linguistics.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the em al-
gorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?38.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proceedings on the Workshop
on Statistical Machine Translation, pages 31?38, New
York City. Association for Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
273?280, Boston, Massachusetts, USA, May. Associ-
ation for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 961?
968, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144?151,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
Conference of the Association for Machine Translation
in the Americas (AMTA), Boston, MA, USA.
Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 138?147, Cambridge, MA, October. Associa-
tion for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL 2003.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In MT Summit 2005.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev
Khudanpur, Lane Schwartz, Wren Thornton, Jonathan
Weese, and Omar Zaidan. 2009. Joshua: An open
source toolkit for parsing-based machine translation.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 135?139, Athens, Greece,
March. Association for Computational Linguistics.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 609?616, Sydney, Australia, July. Associa-
tion for Computational Linguistics.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 558?566, Suntec, Singapore, August.
Association for Computational Linguistics.
Daniel Marcu andWilliamWong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of Empirical methods in natural
language processing, pages 133?139. Association for
Computational Linguistics.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-08: HLT, pages 1003?1011,
651
Columbus, Ohio, June. Association for Computational
Linguistics.
Markos Mylonakis and Khalil Sima?an. 2008. Phrase
translation probabilities with ITG priors and smooth-
ing as learning objective. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 630?639, Honolulu, USA,
October.
Markos Mylonakis and Khalil Sima?an. 2010. Learn-
ing probabilistic synchronous CFGs for phrase-based
translation. In Fourteenth Conference on Computa-
tional Natural Language Learning, Uppsala, Sweden,
July.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167, Sapporo, Japan,
July. Association for Computational Linguistics.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of 43rd Annual Meeting
of the Association for Computational Linguistics, Ann
Arbor, Michigan, USA, June.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars: Soft-
ening syntactic constraints to improve statistical ma-
chine translation. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 236?244, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation. Com-
putational Linguistics, 36(2):247?277.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 523?530, Toulouse, France, July.
Association for Computational Linguistics.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings of ACL-08: HLT, pages 559?567, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation, pages 138?141, New York City, June. As-
sociation for Computational Linguistics.
652
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 40?48,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Modeling Morphosyntactic Agreement
in Constituency-Based Parsing of Modern Hebrew
Reut Tsarfaty?and Khalil Sima?an
Institute for Logic, Language and Computation
University of Amsterdam
{r.tsarfaty,k.simaan}@uva.nl
Abstract
We show that na??ve modeling of morphosyn-
tactic agreement in a Constituency-Based
(CB) statistical parsing model is worse than
none, whereas a linguistically adequate way
of modeling inflectional morphology in CB
parsing leads to improved performance. In
particular, we show that an extension of the
Relational-Realizational (RR) model that in-
corporates agreement features is superior to
CB models that treat morphosyntax as state-
splits (SP), and that the RR model benefits
more from inflectional features. We focus on
parsing Hebrew and report the best result to
date, F184.13 for parsing off of gold-tagged
text, 5% error reduction from previous results.
1 Introduction
Agreement is defined by linguists as the system-
atic covariance of the grammatical properties of one
linguistic element to reflect the semantic or formal
properties of another (Corbett, 2001). Morpholog-
ically marked agreement features such as gender,
number and person are used to realize grammat-
ical relations between syntactic constituents, and
such patterns are abundantly found in (less- or) non-
configurational languages (Hale, 1983) where the
order of words is known to be (relatively) free.
Agreement features encompass information con-
cerning the functional relations between constituents
in the syntactic structure, but whether incorporat-
ing agreement features in a statistical parsing model
leads to improved performance has so far remained
an open question and saw contradictory results.
?The first author is currently a researcher at the department
of Linguistics and Philology at Uppsala University.
Taking Semitic languages as an example, it was
shown that an SVM-based shallow parser (Gold-
berg et al, 2006) does not benefit from includ-
ing agreement features for NP chunking in Hebrew.
Phrase-structure based parsers for Arabic system-
atically discard morphological features from their
label-set and never parametrize agreement explic-
itly (Maamouri et al, 2008). Models based on deep
grammars such as CCG (Hockenmaier and Steed-
man, 2003) and HPSG (Miyao and Tsujii, 2008)
could in principle use inflectional morphology, but
they currently rely on functional information mainly.
For formalisms that do incorporate morphology,
generative models are may leak probability due to
unification failures (Abney, 1997). Even results
from dependency parsing remain inconclusive. It
was shown for dependency parsing that case, defi-
niteness and animacy features are useful to enhance
parsing (e.g., (?vrelid and Nivre, 2007)), agreement
patterns are often excluded. When agreement fea-
tures were included as features in dependency parser
for Hebrew in (Goldberg and Elhadad, 2009) for He-
brew they obtained tiny-to-no improvement.
A question thus emerges whether there are any
benefits in explicitly incorporating morphosyntactic
agreement patterns into our models. This question is
a manifestation of a greater issue, namely, whether
it is beneficial to represent complex patterns of mor-
phology in the statistical parsing model, or whether
configurational information subsume the relevant
patterns, as it is commonly assumed in constituency-
based parsing. Here we claim that agreement fea-
tures are useful for statistical parsing provided that
they are represented and parametrized in a way that
reflects their linguistic substance; to express func-
tional information orthogonal to configuration.
40
We do so by extending the Relational-
Realizational (RR) model we presented in (Tsarfaty
and Sima?an, 2008) to explicitly encode agreement
features in its native representation (RR-AGR). In
the RR model, a joint distribution over grammatical
relations is firstly articulated in the projection phase.
The grammatical relations may be spelled out by
positioning them with respect to one another in the
configuration phase, through the use of morphology
in the realization phase, or both. This paper shows
that, for Hebrew, this RR-AGR strategy signifi-
cantly outperforms a constituency-based model that
treats agreement features as internally structured
non-terminal state-splits (SP-AGR). As we accumu-
late morphological features, the performance gap
between the RR and SP models becomes larger.
The best result we report for the RR-AGR model,
F184.13, is the best result reported for Hebrew to
date for parsing gold PoS-tagged segments, with
5% error reduction from previous results. This
result is also significantly higher than all parsing
results reported so far for Arabic, a Semitic lan-
guage with similar morphosyntactic phenomena.1
The RR approach is shown to be an adequate way
to model complex morphosyntactic patterns for im-
proving constituency-based parsing of a morpholog-
ically rich, free word order language. Because the
RR model is also proper and generative, it may also
embed as a language model to enhance more com-
plex NLP tasks, e.g., statistical Machine Translation.
2 The Data
The grammar of nonconfigurational languages al-
lows for freedom in word ordering and discontinu-
ities of syntactic constituents (Hale, 1983). Such
languages do not rely on configurational information
such as position and adjacency in marking grammat-
ical relations such as subject and object, but instead
they use word-level morphology. One way to encode
grammatical relations in the form of words is by us-
ing morphological case, that is, explicitly marking
an argument (e.g. nominative, accusative) with re-
spect to its grammatical function. In (Tsarfaty et
al., 2009) we showed that incorporating case indeed
leads to improved performance for constituency-
based, Relational-Realizational parsing of Hebrew.
1In (Maamouri et al, 2008), F178.1 for gold standard input.
A more involved way to morphologically encode
grammatical relations is by making explicit refer-
ence to the properties of multiple linguistic ele-
ments. This is the general pattern of agreement, i.e.,
?[A] systematic covariance between a se-
mantic or a formal property of one ele-
ment and a formal property of another.?
(Steele, adapted from (Corbett, 2001))
Describing agreement patterns involves explicit
reference to the following four components; the el-
ement which determines the agreement properties
is the Controller of the agreement, the element
whose properties are determined by agreement is
the Target, the syntactic environment in which the
agreement occurs is the Domain of agreement, and
the properties with respect to which they agree are
agreement Features (Corbett, 2001). Agreement is
an inherently asymmetrical relation. Combination
of features displayed by controllers has to be ac-
commodated by the inflectional features of the tar-
get, but there is no opposite requirement. Let us il-
lustrate the formal description of agreement through
Subject-Verb agreement familiar from English (1).
(1) a. Subject-Verb Agreement in English:
Controller: NP
Target: V
Domain: S
Features: number, person
b. Example:
i. They like the paper
ii. *They likes the paper
The agreement target (the verb) in English has a
rich enough inflectional paradigm that reflects the
person and number features inherent in controllers
? the nouns that realize subjects. (But nouns in En-
glish need not reflect, say, tense.) Had the subject
been an NP, e.g., the phrase ?the committee?, the
agreement pattern would have had to be determined
by the features of the entire NP, and in English the
features of the phrase would be determined by the
lexical head ?committee?. The controller of the
agreement (noun) does not coincide with the head of
the lexical dependency (the verb), which means that
the direction of morphological dependencies need
not coincide with that of lexical dependencies.
41
The Semitic LanguageModernHebrew Modern
Hebrew, (henceforth, Hebrew) is a Semitic language
with a flexible word order and rich morphological
structure. Hebrew nouns morphologically reflect
their inherent gender and number. Pronouns also
reflect person features. Hebrew verbs are inflected
to reflect gender, number, person and tense. Adjec-
tives are inflected to reflect the inherent properties of
nouns, and both nouns and adjectives are inflected
for definiteness. The Hebrew grammar uses this ar-
senal of properties to implement a wide variety of
agreement patterns realizing grammatical relations.
Agreement in Hebrew S Domains Hebrew man-
ifests different patterns of agreement in its S do-
main. Verbal predicates (the target) in matrix sen-
tences (the domain) agree with their nominal sub-
jects (the controller) on the agreement features gen-
der, number and person. This occurs regardless of
their configurational positions, as illustrated in (2b).
(2) a. Agreement in Verbal Sentences:
Controller: NP
Target: V
Domain: S
Features: number, person, gender
b. i. ????? ???? ??? ???
dani
Dani.3MS
natan
gave.3MS
matana
present
ledina
to-Dina
Dani gave a present to Dina (SVO)
ii. ????? ??? ??? ????
matana
present
natan
gave.3MS
dani
Dani.3MS
ledina
to-Dina
Dani gave a present to Dina (VI)
Subject-Predicate agreement relations are not
only independent of surface positions, but are also
orthogonal to the syntactic distributional type of
the constituent realizing the predicate. Semitic lan-
guages allow for predicates to be realized as an
NP, an ADJP or a PP clause (3b) lacking a verb
altogether. (In the Hebrew treebank, such pred-
icates are marked as PREDP). In all such cases,
agreement feature-bundles realized as pronominals,
which (Doron, 1986) calls Pron, are optionally
placed after the subject. The position of Pron el-
ement with respect to the subject and predicate is
fixed.2 The role of these Pron elements is to indicate
the argument-structure of a nominal sentence that is
not projected by a verb. In the Hebrew treebank they
are subsumed under predicative phrases (PREDPs).
If a PREDP head is of type NP or ADJP it must be
inflected to reflect the features of the subject con-
troller, as is illustrated in examples (3b-i)?(3b-ii).
(3) a. Agreement in Nominal Sentences:
Controller: NP
Target: Pron
Domain: S
Features: number, gender,person
b. i. ????? (???) ????
dina
Dina.FS
(hi)
(Pron.3FS)
cayeret
painter.FS
Dina is a painter
ii. ?????? (???) ????
Dina
Dina.FS
(hi)
(Pron.3FS)
muchsheret
talented.FS
Dina is talented
iii. ???? (???) ????
Dina
Dina.FS
(hi)
(Pron.3FS)
babayit
in-the-house
Dina is at home
c. i. ????? ???? *(???) (hi)* dina cayeret
(Pron.3FS)* Dina.FS painter.FS
The pronominal features gender, number, person
are also a part the inflectional paradigm of the verb
??? (be), which is extended to include tense features.
These inflected elements are used as AUX which
function as co-heads together with the main (nom-
inal or verbal) predicate. AUX elements that take a
nominal predicate as in (4b) agree with their subject,
and so do auxiliaries that take a verbal complement,
e.g., the modal verb in (4c). The nominal predicate
in (4b) also agrees with the subject ? and so does
the modal verb in (4c). Agreement of AUX with the
2Doron (1986) shows that these Pron elements can not be
considered the present tense supplements of AUX elements in
Hebrew since their position with respect to the subject and pred-
icate is fixed, whereas AUX can change position, see (4) below.
42
verbal or nominal predicates is again independent of
their surface positions.
(4) a. Subject-AUX Agreement in Hebrew:
Controller: NP
Target: AUX
Domain: S
Features: number, person, gender
b. i. ????? ??? ???? ???
hi
she.3FS
hayta
was.3FS
be?avar
in-past
cayeret
painter.FS
She was a painter in the past
ii. ????? ??? ???? ???
be?avar
in-past
hayta
was.3FS
hi
she.3FS
cayeret
painter.FS
She was a painter in the past?
c. i. ??? ????? ???? ???
hi
She.3FS
hayta
was.3FS
amura
supposed.FS
lehagi?a
to-arrive
She was supposed to arrive
ii. ??? ???? ????? ???
hi
She.3FS
amura
supposed.FS
hayta
was.3FS
lehagi?a
to-arrive
She was supposed to arrive
Agreement in Construct State Nouns Semitic
languages allow for the creation of noun compounds
by phonologically marking their lexical head and
adding a genitive complement. These constructions
are called Construct-State Nouns (CSN) (Danon,
2008) and an example of a CSN is provided in (5a).3
(5) a. ????? ??
bat
child.FS.CSN
ha-cayar
Def-painter.MS
The painter?s daughter
3Also known as iDaFa constructions in Arabic.
In such cases, all the agreement features are taken
from the head of the CSN, the noun ?daughter? in (5).
Since CSNs may be embedded in other CSNs, the
constructions may be arbitrarily long. When short
or long, CSNs themselves may be modified by ad-
jectives that agree with the CSN as a whole. This
gives rise to multiple patterns of agreement within
a single complex CSN. Consider, for instance, the
modified CSN in (6a).
(6) a. ??????? ????? ??
bat
child.FS.CSN
ha-cayar
Def-painter.MS
ha-muchsheret
Def-talented.FS
The talented daughter of the painter
The features Def, F, S of the adjective ?talented?
agree with the inherent properties of the CSN head
?child.FS? and with the definiteness status of the em-
bedded genitive Def-painter. This phenomenon is
called by Danon (2008) definiteness-spreading, and
what is important about such spreading is to observe
that it is not always the case that all agreement fea-
tures of a phrase are contributed by its lexical head.4
Interim Summary The challenges of model-
ing agreement inside constituency-based statistical
models can be summarized as follows. The models
are required to assign probability mass to alternating
sequences of constituents while retaining equivalent
feature distributions that capture agreement. Agree-
ment is (i) orthogonal to the position of constituents
(ii), orthogonal to their distributional types, and (iii)
orthogonal to features? distributions among domi-
nated subconstituents. Yet, from a functional point
of view their contribution is entirely systematic.
3 The Models
The strong version of the well-known Lexicalist
Hypothesis (LH) states that ?syntactic rules cannot
make reference to any aspect of word internal struc-
ture? (Chomsky, 1970). Anderson (1982) argues
that syntactic processes operating within configura-
tional structures can often manipulate, or have ac-
cess to, formal and inherent properties of individ-
ual words. Anderson (1982) argues that a model
4Examples for non-overlapping contribution of features by
multiple dependencies can be found in (Guthmann et al, 2009).
43
that is well-equipped to capture such phenomena is
one that retains a relaxed version of the LH, that is,
one in which syntactic processes do not make refer-
ence to aspects of word-internal structure other than
morphologically marked inflectional features. What
kind of parsing model would allow us to implement
this relaxed version of the Lexicalist Hypothesis?
The Morphosyntatctic State-Splits (SP) Model
One way to maintain a relaxed version of the LH
in syntax is to assume a constituency-based rep-
resentation in which the morphological features of
words are percolated to the level of constituency
in which they are syntactically relevant. This ap-
proach is characteristic of feature-based grammars
(e.g., GPSG (Gazdar et al, 1985) and follow-up
studies). These grammars assume a feature geom-
etry that defines the internal structure of node labels
in phrase-structure trees.5
Category-label state-splits can reflect the different
morphosyntactic behavior of different non-terminals
of the same type. Using such supervised, linguis-
tically motivated, state-splits, based on the phrase-
level marking of morphological information is one
may build an efficient implementation of a PCFG-
based parsing model that takes into account mor-
phological features. State-split models were shown
to obtain state-of-the-art performance with little
computational effort. Supervised state-splits for
constituency-based unlexicalized parsing in (Klein
and Manning, 2003) in an accurate English parser.
For the pair of Hebrew sentences (2b), the morpho-
logical state-split context-free representation of the
domain S is as described at the top of figure 1.6
The Relational-Realizational (RR) Model A dif-
ferent way to implement a syntactic model that con-
form to the relaxed LH is by separating the inflec-
tional features of surface words from their grammat-
ical functions in the syntactic representation and let-
5While agreement patterns in feature-rich grammars give
rise to re-entrancies that break context-freeness, GPSG shows
that using feature-percolation we can get quite far in modeling
morphosyntactic dependencies and retaining context-freeness.
6Horizontal markovization a` la (Klein and Manning, 2003)
would be self-defeating here. Markovization of constituents
conditions inflectional features on configurational positions,
which is inadequate for free word-order languages as Hebrew.
This is already conjectured in the PhD thesis of Collins, and it
is verified empirically for Hebrew in (Tsarfaty et al, 2009).
ting the model learn systematic form-function corre-
spondence patterns between them.
The Relational-Realizational (RR) model (Tsar-
faty and Sima?an, 2008) takes such a ?separational-
ist? approach which is constituent-based. Grammat-
ical relations are separated from their morphologi-
cal or syntactic means of realization, which are in
turn also distinguished. The easiest way to describe
the RR model is via a three-phase generative process
encompassing the projection, configuration and re-
alization phases. In the projection phase, a clause-
level syntactic category generates a Relational Net-
work (RN), i.e., a set of grammatical function-labels
representing the argument-structure of the clause. In
the configuration phase, linear ordering is generated
for the function-labels and optional realization slots
are reserved for elements such as punctuation, auxil-
iaries and adjuncts. The realization phase spells out
a rich morphosyntactic representation (MSR) ? a
syntactic label plus morphological features ? real-
izing each grammatical function and each of the re-
served slots. The process repeats as necessary until
MSRs of pre-terminals are mapped to lexical items.
In (Tsarfaty et al, 2009) we have shown that
the RR model makes beneficial use of morpholog-
ical patterns involving case marking, but did not
study the incorporation of inflectional agreement
features such as gender. Since agreement features
such as gender, number and case-related informa-
tion such accusativity, definiteness are determined
by non-overlapping subconstituents, it remains an
open question whether an addition of agreement fea-
tures into the model can be down in a linguistically
adequate and statistically sound way, and whether or
not they further improve performance.
We claim that the Relational-Realizational model
of (Tsarfaty et al, 2009) has all the necessary ingre-
dients to seamlessly migrate RR representations to
ones that encode agreement explicitely. In order to
explain how we do so let us recapitulate the empir-
ical facts. Agreement is an asymmetric relation de-
fined for a certain domain, in which the agreement
properties of a target co-vary with the inherent prop-
erties of the controller. Consider the two sentences
in (2b) in which the formal means to differentiate the
subject from the object is by the pattern of an agree-
ing predicate. The RR representations of the domain
S are given at the bottom of figure 1.
44
The agreement targets and agreement controllers
are easy to recognize; controllers are the syntac-
tic constituents that realize subjects, parametrized
as Prealization(V B|PRD@S), and targets are the
ones that realize predicates, parametrized as
Prealization(NP |SBJ@S). Now, if we take the
predicted labels of controllers and targets to in-
clude reference to inflectional features, we get
the following parameterization of the realization
parameters Prealization(V B?FEATSi?|PRD@S) and
Prealization(NP ?FEATSj?|SBJ@S) with ?FEATSi?,
?FEATSj? the inflectional features indicated in their
morphosyntactic representation. Now, we only need
to make sure that ?FEATSi?, ?FEATSj? indeed agree,
regardless of their position under S.
We do so by explicitly marking the domain
of agreement, the S category, with the features
of the syntactically most prominent participant in
the situation, the subject (this is where the non-
symmetrical nature of agreement comes into play).
The realization distributions take the following
forms Prealization(V B?FEATSj?|PRD@S?FEATSi?)
and Prealization(NP ?FEATSi?|SBJ@S?FEATSi?). In
the former, NP ?FEATSi? reflects the inherent fea-
tures of the SBJ and in the latter V B?FEATSj? re-
flects the agreement features of the PRD. Now, re-
gardless of word order, and regardless of the inter-
nal structure of NPs, the parameters capturing agree-
ment would be the same for examples (2b i-ii). The
only parameters that differ are the configuration pa-
rameters (boxed), reflecting word-order alternation.
For the sake of completeness we include here also
the SP vs. RR representation of S domains involv-
ing auxiliaries in figure 2. Here the sentences vary
only in the position of the AUX element relative to
the subject with which it agrees. Subjects, predi-
cates, and slots that have been reserved for AUX
elements, all reflect the same pattern of agreement
through their conditioning on the rich representa-
tion of the domain.7 More parameters that vary here
(boxed) are AUX placement and realization param-
eters. Since Pron elements endow PREDPs with
agreement features, agreement with verbless (nomi-
nal) predicates under S analogously follows.
7In Hebrew, even some adverbial modifiers reflect pat-
terns of agreement, e.g., ????? (literally, ?I am still?, glossed
?still.1S?). This solution caters for all such patterns in which
non-obligatory elements exhibit agreement.
4 Experiments
We aim to examine whether the explicit incorpora-
tion of agreement features helps Hebrew parsing,
and if so, which of the two modeling strategies is
better for utilizing the disambiguation cues provided
by morphosyntactic agreement.
Data We use the Hebrew treebank v2.0 with the
extended annotation of (Guthmann et al, 2009),
which adds inflectional properties to non-terminal
categories such as NP and VP. We head-annotate
the corpus and systematically add the agreement fea-
tures of Domains throughout the treebank. We fur-
ther distinguish finite from non-finite verb forms,
and cliticized from non-cliticized nouns, as in
(Goldberg and Tsarfaty, 2008; Tsarfaty et al, 2009).
On top of the treebank labels SBJ subject, OBJ ob-
ject, COM complement and CNJ conjunction we
add PRD predicates and IC infinitival complements.
Procedure We devised a procedure to read-off
treebank grammars based on (i) GPSG-like, states-
plit context-free parameters (SP-AGR), and (ii) RR-
AGR parameters in which context-free rules capture
the projection, configuration and realization phases.
In each model the multiplication provides the prob-
ability of the generation. We use relative frequency
estimates and exhaustively parse gold pos-tagged in-
put8 using a general-purpose CKY parser. We use
the same data split as in (Goldberg and Tsarfaty,
2008; Tsarfaty et al, 2009) (training on sentences
501-6000 and parsing sentences 1-500) and we con-
vert all trees to the flat, coarse-grained, original tree-
bank representation for the purpose of evaluation.
Setup We experiment with bare constituent labels,
grand-parent decorated labels (gp), and labels deco-
rated with grand-parent and head-tag labels (gp,hd).
We use increasingly richer subsets of the {gender,
definiteness, accusativity} set.9
8This choice to parse gold-tagged sentences is meant to alle-
viate the differences in the model?s morphological disambigua-
tion capacity. We want to evaluate the contribution of morpho-
logical features for syntactic disambiguation, and if the models
will disambiguate the morphological analyses differently, the
syntactic analysis will be assigned to different yields and the
accuracy results would be strictly incomparable. But see (Gold-
berg and Tsarfaty, 2008) for a way to combine the two.
9We deliberately choose features that have non-overlapping
behavior, to see whether their contribution is accumulative.
45
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
gave
NP-OBJ
matana
present
PP-COM
ledian
to-dina
SMS
NP-OBJ
matana
present
VPMS-PRD
natan
gave
NPMS-SBJ
dani
Dani
PP-COM
ledian
to-dina
P(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
natan
gave
OBJ@SMS
NP-OBJ
matana
present
COM@SMS
PP-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
present
PRD@SMS
VPMS
natan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SMS
PP-COM
ledian
to-dina
Pprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )
Pconfiguration(?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).
SFS
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)
Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
n tan
gave
NP-OBJ
m tana
present
P-COM
ledian
to-dina
SMS
NP-OBJ
m tana
present
VPMS-PRD
n tan
gave
NPMS-SBJ
dani
Dani
P-COM
ledian
to-dina
P(NPMS-SBJ,VPMS-PRD,NP-OBJ, P-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ, P-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
n tan
gave
OBJ@SMS
NP-OBJ
m tana
present
COM@SMS
P-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
m tana
present
PRD@SMS
VPMS
n tan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SMS
P-COM
ledian
to-dina
Pprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )
Pconfiguration(?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization( P | COM@ SMS ) Prealization( P | COM@ SMS )
Figure 1: The SP-AGR (top) and R-AGR representations of sentences (2b-i) (left) and (2b-ii).
SFS
NPF -SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
SFS
NPF -SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, P, PREDPFS-PRD? | SFS ) P(? P, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)
Figure 2: The SP-AGR (top) and R-AGR representation of sentences (4c-i) (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
gave
NP-OBJ
matana
present
PP-COM
ledian
to-dina
SMS
NP-OBJ
matana
present
VPMS-PRD
natan
gave
NPMS-SBJ
dani
Dani
PP-COM
ledian
to-dina
P(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
natan
gave
OBJ@SMS
NP-OBJ
matana
present
COM@SMS
PP-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
present
PRD@SMS
VPMS
natan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SMS
PP-COM
ledian
to-dina
Pprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )
Pconfiguration(?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).
SFS
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)
Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
gave
NP-OBJ
matana
present
PP-COM
ledian
to-dina
SMS
NP-OBJ
matana
present
VPMS-PRD
natan
gave
NPMS-SBJ
dani
Dani
PP-COM
ledian
to-dina
P(NPMS-SBJ,VPMS PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
natan
gave
OBJ@SMS
NP-OBJ
matana
present
COM@SMS
PP-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
present
PRD@SMS
VPMS
natan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SMS
PP-COM
ledian
to-dina
projection({SBJ,PRD,OBJ,COM} | SMS ) projection({SBJ,PRD,OBJ,COM} | SMS )
Pconfiguration(?S,P ,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR repres tations of s n ences (2b- (left) and (2b-ii).
SFS
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | SFS ) (?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@ FS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
mura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD S+FS
PP
mura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
projection({SBJ,PRD,COM} | SFS ) projection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )
Prealization(AUX | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)
Figure 2: The SP-AGR (top) and RR-AGR representation of s n ences (4c- (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
gave
NP-OBJ
matana
present
PP-COM
ledian
to-dina
SMS
NP-OBJ
matana
present
VPMS-PRD
atan
gave
NPMS-SBJ
dani
Dani
PP-COM
ledian
to-dina
P(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
natan
gave
OBJ@SMS
NP-OBJ
matana
present
COM@SMS
PP-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
present
PRD@SMS
VPMS
atan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SMS
PP-COM
ledian
to-dina
Pprojection({SBJ,PRD,OBJ,COM} | S S ) Pprojection({SBJ,PRD,OBJ,COM} | S S )
Pconfiguration(?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).
SFS
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)
Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).
S S
NP S-SBJ
dani
Dani
VP S-PRD
natan
gave
NP-OBJ
matana
present
PP-CO
ledian
to-dina
S S
NP-OBJ
matana
present
VP S-PRD
natan
gave
NP S-SBJ
dani
Dani
PP-CO
ledian
to-dina
P( P S-SBJ, P S-PR , P- BJ,PP-C | S S ) P( P- BJ, P S-PR , P S-SBJ,PP-C | S S )
S S
{SBJ,PRD,OBJ,CO } S S
SBJ S S
NP S
dani
Dani
PRD S S
VP S
natan
gave
OBJ S S
NP-OBJ
matana
present
CO S S
PP-CO
ledian
to-dina
S S
{SBJ,PRD,OBJ,CO } S S
OBJ S S
NP-OBJ
matana
present
PRD S S
VP S
natan
gave
SBJ S S
NP S
dani
Dani
CO S S
PP-CO
ledian
to-dina
Pprojection({SBJ,PR , BJ,C } | S S ) Pprojection({SBJ,PR , BJ,C } | S S )
Pconfiguration(?S,P, ,C? | {SBJ,PR , BJ,C } S S) Pconfiguration(? ,P,S,C? | {SBJ,PR , BJ,C } S S)
Prealization( P S | SBJ S S ) Prealization( P S | SBJ S S )
Prealization( B S | PR S S ) Prealization( B S | PR S S )
Prealization( P | BJ S S ) Prealization( P | BJ S S )
Prealization(PP | C S S ) Prealization(PP | C S S )
igure 1: The SP- (top) and - representations of sentences (2b-i) (left) and (2b-ii).
FS
NPFS-SBJ
hi
she
AUXFS
hayta
was
DFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
FS
NPFS-SBJ
hi
she
DFS-PRD
amura
supposed
AUXFS
hayt
was
VPINF-COM
lehagia
to-arrive
P(? PFS-SBJ, FS, PP, PRE PFS-PR ? | SFS ) P(?PP, FS, PFS-SBJ, PRE PFS-PR ? | SFS )
FS
{SBJ,PRD,CO } FS
SBJ FS
NPFS
hi
she
SBJ:PRD FS
AUXFS
hayta
was
PRD S+FS
PP
amura
supposed
CO SINF
PREDPFS
lehagia
to-arrive
FS
{SBJ,PRD,CO } FS
SBJ FS
NPFS
hi
she
PRD S+FS
PP
amura
supposed
PRD:CO FS
AUXFS
hayta
was
CO SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PR ,C } | SFS ) Pprojection({SBJ,PR ,C } | SFS )
Pconfiguration( ?SBJ, SBJ:PR , PR , C ? | {SBJ,PR ,C } SFS) Pconfiguration( ?SBJ, PR , PR :C , C ? | {SBJ,PR ,C } SFS)
Prealization( PFS | SBJ SFS ) Prealization( PFS | SBJ SFS )
Prealization( FS | SBJ:PR SFS ) Prealization( FS | PR :C SFS )
Prealization( FS | PR SFS ) Prealization( FS | PR SFS )
Prealization( P | C SFS) Prealization( P | C SFS)
igure 2: The SP- (top) and - representation of sentences (4c-i) (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
gave
NP-OBJ
matana
present
PP-COM
ledian
to-dina
SMS
NP-OBJ
matana
present
VPMS-PRD
natan
gave
NPMS-SBJ
dani
Dani
PP-C
ledian
to-dina
P(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
P @SMS
VPMS
natan
gave
BJ@SMS
NP-OBJ
matana
present
COM@SMS
PP-CO
ledian
to-dina
SM
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
p esent
PRD@SMS
VPMS
natan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SM
PP-COM
ledian
to-dina
Pprojection({S J,PRD,OBJ,COM} | SMS ) Pproject on({SBJ,PRD, J,CO } | SMS )
Pconfiguration(?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfigur ti (?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealiz tion(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,P , }@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | BJ@SFS ) Prealizat on(NPFS | BJ@ FS )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@ FS )
realization(VP | COM@SFS) Prealization(VP | COM@ FS)
Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
V -PRD
n tan
gave
NP-OBJ
matana
pres nt
P COM
ledian
to-di a
SMS
NP-OBJ
matana
present
VPMS-PRD
natan
gave
NPMS-SBJ
dani
Dani
P-COM
ledian
to-dina
P(NPMS-SBJ,V -PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS PRD,NP S-SBJ,PP-CO | SMS )
S S
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@ MS
VPMS
natan
gave
OBJ@SMS
N -OBJ
matana
present
COM@SMS
PP-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
present
PRD@SMS
VPMS
natan
gave
SBJ S S
NPMS
d i
Dani
COM@SMS
PP-COM
ledian
to-d na
Pprojection({SBJ,PRD,OBJ,CO } | S ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )
Pconfiguration(?S,P,O,C? | { BJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii .
SFS
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
FS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
le gi
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | S ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | S )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
ayta
was
PRD@S+
PP
mura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@ +FS
PP
amura
upposed
P :COM@SFS
AUXFS
hayta
was
SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, J:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM COM? | {SBJ,PRD,COM}@SFS)
lization(NPFS | SBJ@S ) Prealization(N FS | SBJ@S )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)
Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii .
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
g ve
N -OBJ
matana
prese t
PP-COM
ledian
to- na
SMS
NP-OBJ
matana
prese t
VPMS- RD
tan
g ve
NPMS-SBJ
dani
Dani
PP-COM
ledian
to- na
P(NPMS-SBJ,VPMS-PRD,N OBJ,PP-COM | SMS ) P(N -OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,O COM}@SMS
BJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
natan
g ve
OBJ@S S
NP-OBJ
matana
prese t
COM@SMS
PP-COM
ledian
to- na
SMS
{SBJ,PRD,O OM}@SMS
OBJ@SMS
NP-OBJ
matana
prese t
PRD@SMS
VPMS
tan
g ve
BJ@ S
NPMS
dani
Dani
COM@SMS
PP-COM
ledian
to- na
Pprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )
Pconfigurati (?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfigurati (?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@ MS ) Pre lization(NPMS | SBJ@ MS )
Prealization(VB | PRD S ) Prealization(VB | PR S )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM ) Prealization(PP | CO )
Figur 1: The SP-AGR (top) and R-AGR representatio of sentence (2b-i) (left) and (2b-ii).
SFS
NPFS-SBJ
hi
she
AUXFS
hayt
was
MD -PRD
amura
supposed
VPINF-COM
lehagia
t -arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amur
supposed
AUXFS
hayta
was
VPIN -COM
leh gia
to-arrive
P(?NPFS-SBJ, AUXFS, P , PREDPFS-PR ? | FS ) P(?PP, AUXFS, NPFS- BJ, PREDPFS-PR ? | FS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@ FS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
REDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@ INF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfigurati ( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfigurati ( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | BJ@ FS ) Prealization(NPFS | BJ@ FS )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PR @SFS ) Prealization(MDFS | PR @SFS )
Prealization(VP | COM@SFS) realization(VP | COM@SFS)
Figur 2: The SP-AGR (top) and R-AGR representation of sentence (4c-i) (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
gave
NP-OBJ
matana
present
PP-COM
ledian
to-dina
SMS
NP-OBJ
matana
present
VPMS-PRD
natan
gave
NPMS-SBJ
dani
Dani
PP-COM
ledian
to-dina
P(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
natan
gave
OBJ@SMS
NP-OBJ
matana
present
COM@SMS
PP-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
present
PRD@SMS
VPMS
natan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SMS
PP-COM
ledian
to-dina
Pprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )
Pconfiguration(?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).
SFS
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
o-arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@ FS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
PREDPF
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PR ,COM} | SFS )
Pc nfiguration( ?SBJ, S J:PRD, PRD, C M? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
realization(NPFS | SBJ@SFS ) Pre lization(NPFS | SBJ@SFS )
Prealization(AUXFS | BJ:PRD SFS ) Prealization(AUXFS | PRD:C @SFS )
Prealization(MDFS | F ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS realization(VP | COM@ FS)
Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).
46
Model { gender def+acc gender+def+acc
SP-AGR 79.77 79.55 80.13 80.26
(3942) (7594) (4980) (8933)
RR-AGR 80.23 81.09 81.48 82.64
(3292) (5686) (3772) (6516)
SP-AGR (gp) 83.06 82.18 79.53 80.89
(5914) (10765) (12700) (11028)
RR-AGR (gp) 83.49 83.70 83.66 84.13
(6688) (10063) (12383) (12497)
SP-AGR (gp,hd) 76.61 64.07 75.12 61.69
(10081) (16721) (11681) (18428)
RR-AGR (gp,hd) 83.40 81.19 83.33 80.45
(12497) (22979) (13828) (24934)
Table 1: F-score (#params) measure for all models on
the Hebrew treebank dev-set for Sentences Length < 40
5 Results and Discussion
Table 1 shows the standard F1 scores (and #param-
eters) for all models. Throughout, the RR-AGR
model outperforms the SP-AGR models that use the
same category set and the same morphological fea-
tures as state splits. For RR-AGR and RR-AGR (gp)
models, adding agreement features to case features
improves performance. The accumulative contribu-
tion is significant. For SP-AGR and SP-AGR (gp)
models, adding more features either remains at the
same level of performance or becomes detrimental.
Since the SP/RR-AGR and SP/RR-AGR (gp)
models are of comparable size for each feature-set,
it is unlikely that the differences in performance are
due to the lack of training data. A more reason-
able explanation if that the RR parameters repre-
sent functional generalizations orthogonal to config-
uration for which statistical evidence is more easily
found in the data. The robust realization distribu-
tions which cut across ordering alternatives can steer
the disambiguation in the right direction.
The RR-AGR (gp) +gen+def+acc model yields
the best result for parsing Hebrew to date (F1 84.13),
improving upon our best model in (Tsarfaty et al,
2009) (F1 83.33, underlined) in a pos-tagged set-
ting. For this setting, Arabic parsing results are F1
78.1. Given the similar morphosyntactic phenomena
(agreement, MaSDaR, iDaFa) it would be interest-
ing to see if the model enhances parsing for Arabic.
For (gp,hd) models (a configuration which was
shown to give the best results in (Tsarfaty et al,
2009)) there is a significant decrease in accuracy
with the gender feature, but there is a lesson to be
learned. Firstly, while the RR-AGR (gp,hd) model
shows moderate decrease with gender, the decrease
in performance of SP-AGR (gp,hd) for the same
feature-set is rather dramatic, which is consistent
with the observation that the RR model is less vul-
nerable to sparseness and that it makes better use of
the statistics of functional relations in the data.
Consulting the size of the different grammars, the
combination of RR-AGR (gp, hd) with gender fea-
tures indeed results in substantially larger grammars,
and it is possible that at this point we indeed need to
incorporate smoothing. At the same time there may
be an alternative explanation for the decreased per-
formance. It might be that the head-tag does not add
informative cues beyond the contribution of the fea-
tures which are spread inside the constituent, and are
already specified. This is a reasonable hypothesis
since gender in Hebrew always percolates through
the head as opposed to def/acc that percolate from
other forms. Incorporating head-tag in (Tsarfaty et
al., 2009) might have led to improvement only due
to the lack of agreement features which subsume
the relevant pattern. This suggests that incorporat-
ing all co-heads and functional elements that con-
tribute morphological features spread inside the con-
stituent, is more adequate for modeling morphosyn-
tax than focusing on the features of a single head.
6 Conclusion
We show that morphologically marked agreement
features can significantly improve parsing perfor-
mance if they are represented and parametrized in
a way that reflects their linguistic substance: relat-
ing form-and-function in a non-linear fashion. We
have so far dealt with the adequacy of representa-
tion and we plan to test whether more sophisticated
estimation (e.g., split-merge-smooth estimation as in
(Petrov et al, 2006)) can obtain further improve-
ments from the explicit representation of agreement.
At the same time, the state-of-the-art results we
present render the RR model promising for further
exploration with morphologically rich languages.
Acknowledgements The work of the first author
has been funded by NWO, grant 017.001.271. We
wish to thank Joakim Nivre and three anonymous
reviewers for helpful comments on earlier drafts.
47
References
Steven Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597?618.
Stephen R. Anderson. 1982. Where?s morphology? Lin-
guistic Inquiry.
Noam Chomsky. 1970. Remarks on nominalization. In
R. Jacobs and P. Rosenbaum, editors, Reading in En-
glish Transformational Grammar. Waltham: Ginn.
Greville G. Corbett. 2001. Agreement: Terms and
boundaries. In SMG conference papers.
Gabi Danon. 2008. Definiteness spreading in the hebrew
construct-state. Lingua, 118(7):872?906.
Edit Doron. 1986. The pronominal ?copula? as agree-
ment clitic. Syntax and Semantics, (19):313?332.
Gerald Gazdar, Ewan Klein, Geoffrey K. Pullum, and
Ivan A. Sag. 1985. Generalised phrase structure
grammar. Blackwell, Oxford, England.
Yoav Goldberg and Michael Elhadad. 2009. Hebrew de-
pendency parsing: Initial results. In Proceedings of
IWPT.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syn-
tactic parsing. In Proceedings of ACL.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2006.
Noun phrase chunking in hebrew: Influence of lex-
ical and morphological features. In Proceedings of
COLING-ACL.
Nomie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew treebank.
In Frank Van Eynde, Anette Frank, Koenraad De
Smedt, and Gertjan van Noord, editors, Proceedings
of TLT.
Kenneth L. Hale. 1983. Warlpiri and the grammar of
non-configurational languages. Natural Language and
Linguistic Theory, 1(1).
Julia Hockenmaier and Mark Steedman. 2003. Parsing
with generative models of predicate-argument struc-
ture. In Proceedings of ACL.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
Enhanced annotation and parsing of the arabic tree-
bank. In Proceedings of INFOS.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature-forest
models for probabilistic hpsg parsing. Computational
Linguistics, 34(1):35?80.
Lilja ?vrelid and Joakim Nivre. 2007. Swedish depen-
dency parsing with rich linguistic features. In Pro-
ceeding of RANLP.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of ACL.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
realizational parsing. In Proceedings of CoLing.
Reut Tsarfaty, Khalil Sima?an, and Remko Scha. 2009.
An alternative to head-driven approaches for parsing a
(relatively) free word order language. In Proceedings
of EMNLP.
48
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 117?125,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Learning Probabilistic Synchronous CFGs for Phrase-based Translation
Markos Mylonakis
ILLC
University of Amsterdam
m.mylonakis@uva.nl
Khalil Sima?an
ILLC
University of Amsterdam
k.simaan@uva.nl
Abstract
Probabilistic phrase-based synchronous
grammars are now considered promis-
ing devices for statistical machine transla-
tion because they can express reordering
phenomena between pairs of languages.
Learning these hierarchical, probabilistic
devices from parallel corpora constitutes a
major challenge, because of multiple la-
tent model variables as well as the risk
of data overfitting. This paper presents
an effective method for learning a family
of particular interest to MT, binary Syn-
chronous Context-Free Grammars with in-
verted/monotone orientation (a.k.a. Bi-
nary ITG). A second contribution con-
cerns devising a lexicalized phrase re-
ordering mechanism that has complimen-
tary strengths to Chiang?s model. The
latter conditions reordering decisions on
the surrounding lexical context of phrases,
whereas our mechanism works with the
lexical content of phrase pairs (akin to
standard phrase-based systems). Surpris-
ingly, our experiments on French-English
data show that our learning method ap-
plied to far simpler models exhibits per-
formance indistinguishable from the Hiero
system.
1 Introduction
A fundamental problem in phrase-based machine
translation concerns the learning of a probabilistic
synchronous context-free grammar (SCFG) over
phrase pairs from an input parallel corpus. Chi-
ang?s Hiero system (Chiang, 2007) exemplifies
the gains to be had by combining phrase-based
translation (Och and Ney, 2004) with the hierar-
chical reordering capabilities of SCFGs, particu-
larly originating from Binary Inversion Transduc-
tion Grammars (BITG) (Wu, 1997). Yet, exist-
ing empirical work is largely based on successful
heuristic techniques, and the learning of Hiero-like
BITG/SCFG remains an unsolved problem,
The difficulty of this problem stems from the
need for simultaneously learning of two kinds of
preferences (see Fig.1) (1) lexical translation prob-
abilities (P (?e, f? | X)) of source (f ) and target
(e) phrase pairs, and (2) phrase reordering prefer-
ences of a target string relative to a source string,
expressed in synchronous productions probabil-
ities (for monotone or switching productions).
Theoretically speaking, both kinds of preferences
may involve latent structure relative to the paral-
lel corpus. The mapping between source-target
sentence pairs can be expressed in terms of la-
tent phrase segmentations and latent word/phrase-
alignments, and the hierarchical phrase reorder-
ing can be expressed in terms of latent binary
synchronous hierarchical structures (cf. Fig. 1).
But each of these three kinds of latent structures
may be made explicit using external resources:
word-alignment could be considered solved us-
ing Giza++ (Och and Ney, 2003)), phrase pairs
can be obtained from these word-alignments (Och
and Ney, 2004), and the hierarchical synchronous
structure can be grown over source/target linguis-
tic syntactic trees output by an existing parser.
The Joint Phrase Translation Model (Marcu and
Wong, 2002) constitutes a specific case, albeit
without the hierarchical, synchronous reordering
Start S ? X 1 / X 1 (1)
Monotone X ? X 1 X 2 /X 1 X 2 (2)
Switching X ? X 1 X 2 /X 2 X 1 (3)
Emission X ? e / f (4)
Figure 1: A phrase-pair SCFG (BITG)
117
component. Other existing work, e.g. (Chiang,
2007), assumes the word-alignments are given in
the parallel corpus, but the problem of learning
phrase translation probabilities is usually avoided
by using surface counts of phrase pairs (Koehn et
al., 2003). The problem of learning the hierar-
chical, synchronous grammar reordering rules is
oftentimes addressed as a learning problem in its
own right assuming all the rest is given (Blunsom
et al, 2008b).
A small number of efforts has been dedicated
to the simultaneous learning of the probabilities
of phrase translation pairs as well as hierarchi-
cal reordering, e.g., (DeNero et al, 2008; Zhang
et al, 2008; Blunsom et al, 2009). Of these,
some concentrate on evaluating word-alignment,
directly such as (Zhang et al, 2008) or indirectly
by evaluating a heuristically trained hierarchical
translation system from sampled phrasal align-
ments (Blunsom et al, 2009). However, very
few evaluate on actual translation performance of
induced synchronous grammars (DeNero et al,
2008). In the majority of cases, the Hiero system,
which constitutes the yardstick by which hierar-
chical systems are measured, remains superior in
translation performance, see e.g. (DeNero et al,
2008).
This paper tackles the problem of learning gen-
erative BITG models as translation models assum-
ing latent segmentation and latent reordering: this
is the most similar setting to the training of Hiero.
Unlike all other work that heuristically selects a
subset of phrase pairs, we start out from an SCFG
that works with all phrase pairs in the training set
and concentrate on the aspects of learning. This
learning problem is fraught with the risks of over-
fitting and can easily result in inadequate reorder-
ing preferences (see e.g. (DeNero et al, 2006)).
Almost instantly, we find that the translation
performance of all-phrase probabilistic SCFGs
learned in this setting crucially depends on the in-
terplay between two aspects of learning:
? Defining a more constrained parameter
space, where the reordering productions
are phrase-lexicalised and made sensitive to
neighbouring reorderings, and
? Defining an objective function that effec-
tively smoothes the maximum-likelihood cri-
terion.
One contribution of this paper is in devis-
ing an effective, data-driven smoothed Maximum-
Likelihood that can cope with a model working
with all phrase pair SCFGs. This builds upon
our previous work on estimating parameters of a
?bag-of-phrases? model for Machine Translation
(Mylonakis and Sima?an, 2008). However, learn-
ing SCFGs poses significant novel challenges, the
core of which lies on the hierarchical nature of a
stochastic SCFG translation model and the rele-
vant additional layer of latent structure. We ad-
dress these issues in this work. Another important
contribution is in defining a lexicalised reorder-
ing component within BITG that captures order
divergences orthogonal to Chiang?s model (Chi-
ang, 2007) but somewhat akin to Phrase-Based
Statistical Machine Translation reordering models
(Koehn et al, 2003).
Our analysis shows that the learning difficul-
ties can be attributed to a rather weak generative
model. Yet, our best system exhibits Hiero-level
performance on French-English Europarl data us-
ing an SCFG-based decoder (Li et al, 2009). Our
findings should be insightful for others attempting
to make the leap from shallow phrase-based sys-
tems to hierarchical SCFG-based translation mod-
els using learning methods, as opposed to heuris-
tics.
The rest of the paper is structured as follows.
Section 2 briefly introduces the SCFG formalism
and discusses its adoption in the context of Statis-
tical Machine Translation (SMT). In section 3, we
consider some of the pitfalls of stochastic SCFG
grammar learning and address them by introduc-
ing a novel learning objective and algorithm. In
the section that follows we browse through latent
translation structure choices, while in section 5 we
present our empirical experiments on evaluating
the induced stochastic SCFGs on a translation task
and compare their performance with a hierarchical
translation baseline. We close with a comparison
of related work and a final discussion including fu-
ture research directions.
2 Synchronous Grammars for Machine
Translation
Synchronous Context Free Grammars (SCFGs)
provide an appealing formalism to describe the
translation process, which explains the generation
of parallel strings recursively and allows capturing
long-range reordering phenomena. Formally, an
SCFG G is defined as the tuple (N,E, F,R, S),
118
where N is the finite set of non-terminals with
S ? N the start symbol, F and E are finite sets
of words for the source and target language and R
is a finite set of rewrite rules. Every rule expands
a left-hand side non-terminal to a right-hand side
pair of strings, a source language string over the
vocabulary F?N and a target language string over
E ? N . The number of non-terminals in the two
strings is equal and the rule is complemented with
a mapping between them.
String pairs in the language of the SCFG are
those with a valid derivation, consisting of a se-
quence of rule applications, starting from S and
recursively expanding the linked non-terminals at
the right-hand side of rules. Stochastic SCFGs
augment every rule in R with a probability, under
the constraint that probabilities of rules with the
same left-hand side sum up to one. The probabil-
ity of each derived string pair is then the product
of the probabilities of rules used in the derivation.
Unless otherwise stated, for the rest of the paper
when we refer to SCFGs we will be pointing to
their stochastic extension.
The rank of an SCFG is defined as the maxi-
mum number of non-terminals in a grammar?s rule
right-hand side. Contrary to monolingual Context
Free Grammars, there does not always exist a con-
version of an SCFG of a higher rank to one of a
lower rank with the same language of string pairs.
For this, most machine translation applications fo-
cus on SCFGs of rank two (binary SCFGs), or
binarisable ones witch can be converted to a bi-
nary SCFG, given that these seem to cover most
of the translation phenomena encountered in lan-
guage pairs (Wu, 1997) and the related processing
algorithms are less demanding computationally.
Although SCFGS were initially introduced for
machine translation as a stochastic word-based
translation process in the form of the Inversion-
Transduction Grammar (Wu, 1997), they were ac-
tually able to offer state-of-the-art performance in
their latter phrase-based implementation by Chi-
ang (Chiang, 2005). Chiang?s Hiero hierarchi-
cal translation system is based on a synchronous
grammar with a single non-terminal X covering
all learned phrase-pairs. Beginning from the start
symbol S, an initial phrase-span structure is con-
structed monotonically using a simple ?glue gram-
mar?:
S ?S 1 X 2 / S 1 X 2
S ?X 1 / X 1
The true power of the system lies in expanding
these initial phrase-spans with a set of hierarchi-
cal translation rules, which allow conditioning re-
ordering decisions based on lexical context. For
the French to English language pair, some exam-
ples would be:
S ? X 1 e?conomiques / financial X 1
S ? cette X 1 de X 2 / this X 1 X 2
S ? politique X 1 commune de X 2 /
X 2
? s common X 1 policy
Further work builds on the Hiero grammar to ex-
pand it with constituency syntax motivated non-
terminals (Zollmann and Venugopal, 2006).
3 Synchronous Grammar Learning
The learning of phrase-based stochastic SCFGs
with a Maximum Likelihood objective is exposed
to overfitting as other all-fragment models such as
Phrase-Based SMT (PBSMT) (Marcu and Wong,
2002; DeNero et al, 2006) and Data-Oriented
Parsing (DOP) (Bod et al, 2003; Zollmann and
Sima?an, 2006). Maximum Likelihood Estima-
tion (MLE) returns degenerate grammar estimates
that memorise well the parallel training corpus but
generalise poorly to unseen data.
The bias-variance decomposition of the gener-
alisation error Err sheds light on this learning
problem. For an estimator p? with training data D,
Err can be expressed as the expected Kullback-
Leibler (KL) divergence between the target distri-
bution q and that the estimate p?. This error decom-
poses into bias and variance terms (Heskes, 1998):
Err =
bias
? ?? ?
KL(q, p?)+
variance
? ?? ?
EDKL(p?, p?) (5)
Bias is the KL-divergence between q and the mean
estimate over all training data p? = EDp?(D). Vari-
ance is the expected divergence between the av-
erage estimate and the estimator?s actual choice.
MLE estimators for all-fragment models are zero-
biased with zero divergence between the average
estimate and the true data distribution. In contrast,
their variance is unboundedly large, leading to un-
bounded generalisation error on unseen cases.
119
3.1 Cross Validated MLE
A well-known method for estimating generalisa-
tion error is k-fold Cross-Validation (CV) (Hastie
et al, 2001). By partitioning the training data D
into k parts Hk1 , we estimate Err as the expected
error over all 1 ? i ? k, when testing on Hi with
a model trained by MLE on the rest of the data
D?i = ?j 6=iHj .
Here we use CV to leverage the bias-variance
trade-off for learning stochastic all-phrase SCFGs.
Given an input all-phrase SCFG grammar with
phrase-pairs extracted from the training data, we
maximise training data likelihood (MLE) subject
to CV smoothing: for each data part Hi (1 ? i ?
k), we consider only derivations which employ
grammar rules extracted from the rest of the data
D?i. Other work (Mylonakis and Sima?an, 2008)
has also explored MLE under CV for a ?bag-of-
phrases model? that does not deal with reordering
preferences, does not employ latent hierarchical
structure and works with a non-hierarchical de-
coder, and partially considers the sparsity issues
that arise within CV training. The present paper
deals with these issues.
Because of the latent segmentation and hi-
erarchical variables, CV-smoothed MLE cannot
be solved analytically and we devise a CV in-
stance of the Expectation-Maximization (EM) al-
gorithm, with an implementation based on a syn-
chronous version of the Inside-Outside algorithm
(see Fig. 2). For each word-aligned sentence pair
in a partitionHi, the set of eligible derivations (de-
noted D?i) are those that can be built using only
phrase-pairs and productions found inD?i. An es-
sential part of the learning process involves defin-
ing the grammar extractor G(D), a function from
data to an all-phrase SCFG. We will discuss vari-
ous extractors in section 4.
Our CV-EM algorithm is an EM instance, guar-
anteeing convergence and a non-decreasing CV-
smoothed data likelihood after each iteration. The
running time remains O(n6), where n is input
length, but by considering only derivation spans
which do not cross word-alignment points, this
runs in reasonable times for relatively large cor-
pora.
3.2 Bayesian Aspects of CV-MLE
Beside being an estimator, the CV-MLE learning
algorithm has the added value of being a grammar
learner focusing on reducing generalisation error,
INPUT: Word-aligned parallel training data D
Grammar extractor G
The number of parts k to partition D
OUTPUT: SCFG G with rule probabilities p?
Partition training data D into parts H1, . . . ,Hk.
For 1 ? i ? k do
Extract grammar rules set Gi = G(Hi)
Initialise G = ?iGi, p?0 uniform
Let j = 0
Repeat
Let j = j + 1
E-step:
For 1 ? i ? k do
Calculate expected counts given G, p?j?1,
for derivations D?i of Hi
using rules from ?k 6=iG(k)
M-step: set p?j to ML estimate given
expected counts
Until convergence
Figure 2: The CV Expectation Maximization al-
gorithm
in the sense that probabilities of grammar produc-
tions should reflect the frequency with which these
productions are expected to be used for translating
future data. Additionally, since the CV criterion
prohibits for every data point derivations that use
rules only extracted from the same data part, such
rules are assigned zero probabilities in the final es-
timate and are effectively excluded from the gram-
mar. In this way, the algorithm ?shapes? the input
grammar, concentrating probability mass on pro-
ductions that are likely to be used with future data.
One view point of CV-MLE is that each par-
tition D?i and Hi induces a prior probability
Prior(pi; D?i) on every parameter assignment pi,
obtained from D?i. This prior assigns zero prob-
ability to all pi parameter sets with non-zero prob-
abilities for rules not in G(D?i), and uniformly
distributes probability to the rest of the parameter
sets. In light of this, the CV-MLE objective can be
written as follows:
argmax
pi
?
i
Prior(pi; D?i)? P (Hi | pi) (6)
This data-driven prior aims to directly favour pa-
rameter sets which are expected to better gener-
alise according to the CV criterion, without rely-
ing on arbitrary constraints such as limiting the
120
length of phrase pairs in the right-hand side of
grammar rules. Furthermore, other frequently em-
ployed priors such as the Dirichlet distribution and
the Dirichlet Process promote better generalising
rule probability distributions based on externally
set hyperparameter values, whose selection is fre-
quently sensitive in terms of language pairs, or
even the training corpus itself. In contrast, the CV-
MLE prior aims for a data-driven Bayesian model,
focusing on getting information from the data, in-
stead of imposing external human knowledge on
them (see also (Mackay and Petoy, 1995)).
3.3 Smoothing the Model
One remaining wrinkle in the CV-EM scheme is
the treatment of boundary cases. There will often
be sentence-pairs in Hi, that cannot be fully de-
rived by the grammar extracted from the rest of the
data D?i either because of (1) ?unknown? words
(i.e. not appearing in other parts of the CV parti-
tion) or (2) complicated combinations of adjacent
word-alignments. We employ external smoothing
of the grammar, prior to learning.
Our solution is to extend the SCFG extracted
fromD?i with new emission productions deriving
the ?unknown? phrase-pairs (i.e., found in Hi but
not in D?i). Crucially, the probabilities of these
productions are drawn from a fixed smoothing dis-
tribution, i.e., they remain constant throughout es-
timation. Our smoothing distribution of phrase-
pairs for all pre-terminals considers source-target
phrase lengths drawn from a Poisson distribution
with unit mean, drawing subsequently the words
of each of the phrases uniformly from the vocab-
ulary of each language, similar to (Blunsom et al,
2009).
psmooth(f/e) =
ppoisson(|f |; 1) ppoisson(|e|; 1)
V |f |f V
|e|
e
Since the smoothing distribution puts stronger
preference on shorter phrase-pairs and avoids
competing with the ?known? phrase-pairs, it leads
the learner to prefer using as little as possible such
smoothing rules, covering only the phrase-pairs
required to complete full derivations.
4 Parameter Spaces and Grammar
Extractors
A Grammar Extractor (GE) plays a major role in
our probabilistic SCFG learning pipeline. A GE is
a function from a word-aligned parallel corpus to a
probabilistic SCFG model. Together with the con-
straints that render a proper probabilistic SCFG1,
this defines the parameter space.
The extractors used in this paper create SCFGs
productions of two different kinds: (a) hierarchi-
cal synchronous productions that define the space
of possible derivations up to the level of the SCFG
pre-terminals, and (2) the phrase-pair emission
rules that expand the pre-terminals to phrase-pairs
of varying lengths. Given the word-alignments,
the set of phrase-pairs extracted is the set of all
translational equivalents (without length upper-
bound) under the word-alignment as defined in
(Och and Ney, 2004; Koehn et al, 2003).
Below we focus on the two grammar extrac-
tors employed in our experiments. We start out
from the most generic, BITG-like formulation,
and aim at incremental refinement of the hierar-
chical productions in order to capture relevant,
content-based phrase-pair reordering preferences
in the training data.
Single non-terminal SCFG This is a phrase-
based binary SCFG grammar employing a single
non-terminal X covering each extracted phrase-
pair. The other productions consist of monotone
and switching expansions of phrase-pair spans
covered by X . Finally, the whole sentence-pair is
considered to be covered by X . We will call this
?plain SCFG? extractor. See Fig. 1.
Lexicalised Reordering SCFG One weakness
of the plain SCFG is that the reordering deci-
sions in the derivations are made without reference
to lexical content of the phrases; this is because
all phrase-pairs are covered by the same non-
terminal. As a refinement, we propose a gram-
mar extractor that aims at modelling the reordering
behaviour of phrase-pairs by taking their content
into account. This time, the X non-terminal is re-
served for phrase-pairs and spans which will take
part in monotonic productions only. Two fresh
non-terminals, XSL and XSR, are used for cov-
ering phrase-pairs that participate in order switch-
ing with other, adjacent phrase-pairs. The non-
terminal XSL covers phrase-pairs which appear
first in the source language order, and the latter
those which follow them. The grammar rules pro-
duced by this GE, dubbed ?switch grammar?, are
listed in Fig. 3.
1The sum of productions that have the same left-hand la-
bel must be one.
121
Start S ? X 1 /X 1
Monotone Expansion
X ? X 1 X 2 /X 1 X 2
XSL ? X 1 X 2 / X 1 X 2
XSR ? X 1 X 2 /X 1 X 2
Switching Expansion
X ? XSL 1 XSR 2 /XSR 2 XSL 1
XSL ? XSL 1 XSR 2 /XSR 2 XSL 1
XSR ? XSL 1 XSR 2 /XSR 2 XSL 1
Phrase-Pair Emission
X ? e/f
XSL ? e / f
XSR ? e / f
Figure 3: Lexicalised-Reordering SCFG
The reordering information captured by the
switch grammar is in a sense orthogonal to that
of Hiero-like systems utilising rules such as those
listed in section 2. Hiero rules encode hier-
archical reordering patterns based on surround-
ing context. In contrast, the switch grammar
models the reordering preferences of the phrase-
pairs themselves, similarly to the monotone-swap-
discontinuous reordering models of Phrase-based
SMT models (Koehn et al, 2003). Furthermore, it
strives to match pairs of such preferences, combin-
ing together phrase-pairs with compatible reorder-
ing preferences.
5 Experiments
In this section we proceed to integrate our esti-
mates within an SCFG-based decoder. We subse-
quently evaluate our performance in relation to a
state-of-the-art Hiero baseline on a French to En-
glish translation task.
5.1 Decoding
The joint model of bilingual string derivations pro-
vided by the learned SCFG grammar can be used
for translation given a input source sentence, since
argmaxe p(e|f) = argmaxe p(e, f). We use our
learned stochastic SCFG grammar with the decod-
ing component of the Joshua SCFG toolkit (Li
et al, 2009). The full translation model inter-
polates log-linearly the probability of a grammar
derivation together with the language model prob-
ability of the target string. The model is further
smoothed, similarly to phrase-based models and
the Hiero system, with smoothing features ?i such
as the lexical translation scores of the phrase-pairs
involved and rule usage penalties. As usual with
statistical translation, we aim for retrieving the tar-
get sentence e corresponding to the most probable
derivation D
?
? (f, e) with rules r, with:
p(D) ? p(e)?lmpscfg(e, f)
?scfg
?
i
?
r?D
?i(r)
?i
The interpolation weights are tuned using Mini-
mum Error Rate Training (Och, 2003).
5.2 Results
We test empirically the learner?s output gram-
mars for translating from French to English, us-
ing k = 5 for the Cross Validation data partition-
ing. The training material is a GIZA++ word-
aligned corpus of 200K sentence-pairs from the
Europarl corpus (Koehn, 2005), with our devel-
opment and test parallel corpora of 2K sentence-
pairs stemming from the same source. Train-
ing the grammar parameters until convergence de-
mands around 6 hours on an 8-core 2.26 GHz Intel
Xeon system. Decoding employs a 4-gram lan-
guage model, trained on English Europarl data of
19.5M words, smoothed using modified Kneser-
Ney discounting (Chen and Goodman, 1998), and
lexical translation smoothing features based on the
GIZA++ alignments.
In a sense, the real baseline to which we might
compare against should be a system employing the
MLE estimate for the grammar extracted from the
whole training corpus. However, as we have al-
ready discussed, this assigns zero probability to all
sentence-pairs outside of the training data and is
subsequently bound to perform extremely poorly,
as decoding would then completely rely on the
smoothing features. Instead, we opt to compare
against a hierarchical translation baseline provided
by the Joshua toolkit, trained and tuned on the
same data as our learning algorithm. The grammar
used by the baseline is much richer than the ones
learned by our algorithm, also employing rules
which translate with context, as shown in section
2. Nevertheless, since it is not clear how the re-
ordering rules probabilities of a grammar similar
to the ones we use could be trained heuristically,
we choose to relate the performance of our learned
stochastic SCFG grammars to the particular, state-
of-the-art in SCFG-based translation, system.
Table 1 presents the translation performance re-
sults of our systems and the baseline. On first
122
System
Lexical
BLEU
Smoothing
joshua-baseline No 27.79
plain scfg No 28.04
switch scfg No 28.48
joshua-baseline Yes 29.96
plain scfg Yes 29.75
switch scfg Yes 29.88
Table 1: Empirical results, with and without addi-
tional lexical translation smoothing features dur-
ing decoding
observation, it is evident that our learning algo-
rithm outputs stochastic SCFGs which manage to
generalise, avoiding the degenerate behaviour of
plain MLE training for these models. Given the
notoriety of the estimation process, this is note-
worthy on its own. Having a learning algorithm
at hand which realises in a reasonable extent the
potential of each stochastic grammar design (as
implemented in the relevant grammar extractors),
we can now compare between the two grammar
extractors used in our experiments. The results
table highlights the importance of conditioning
the reordering process on lexical grounds. The
plain grammar with the single phrase-pair non-
terminal cannot accomplish this and achieves a
lower BLEU score. On the other hand, the switch
SCFG allows such conditioning. The learner takes
advantage of this feature to output a grammar
which performs better in taking reordering deci-
sions, something that is reflected in both the actual
translations as well as the BLEU score achieved.
Furthermore, our results highlight the impor-
tance of the smoothing decoding features. The
unsmoothed baseline system itself scores consid-
erably less when employing solely the heuristic
translation score. Our unsmoothed switch gram-
mar decoding setup improves on the baseline by
a considerable difference of 0.7 BLEU. Subse-
quently, when adding the smoothing lexical trans-
lation features, both systems record a significant
increase in performance, reaching comparable lev-
els of performance.
The degenerate behaviour of MLE for SCFGs
can be greatly limited by constraining ourselves
to grammars employing minimal phrase-pairs
; phrase-pairs which cannot be further broken
down into smaller ones according to the word-
alignment. One could argue that it is enough to
perform plain MLE with such minimal phrase-pair
SCFGs, instead of using our more elaborate learn-
ing algorithm with phrase-pairs of all lengths. To
investigate this, for our final experiment we used
a plain MLE estimate of the switch grammar to
translate, limiting the grammar?s phrase-pair emis-
sion rules to only those which involve minimal
phrase-pairs. The very low score of 17.82 BLEU
(without lexical smoothing) not only highlights
the performance gains of using longer phrase-pairs
in hierarchical translation models, but most impor-
tantly provides a strong incentive to address the
overfitting behaviour of MLE estimators for such
models, instead of avoiding it.
6 Related work
Most learning of phrase-based models, e.g.,
(Marcu and Wong, 2002; DeNero et al, 2006;
Mylonakis and Sima?an, 2008), works without hi-
erarchical components (i.e., not based on the ex-
plicit learning of an SCFG/BITG). These learning
problems pose other kinds of learning challenges
than the ones posed by explicit learning of SCFGs.
Chiang?s original work (Chiang, 2007) is also re-
lated. Yet, the learning problem is not expressed in
terms of an explicit objective function because sur-
face heuristic counts are used. It has been very dif-
ficult to match the performance of Chiang?s model
without use of these heuristic counts.
A somewhat related work, (Blunsom et al,
2008b), attempts learning new non-terminal labels
for synchronous productions in order to improve
translation. This work differs substantially from
our work because it employs a heuristic estimate
for the phrase pair probabilities, thereby concen-
trating on a different learning problem: that of re-
fining the grammar symbols. Our approach might
also benefit from such a refinement but we do not
attempt this problem here. In contrast, (Blunsom
et al, 2008a) works with the expanded phrase pair
set of (Chiang, 2005), formulating an exponential
model and concentrating on marginalising out the
latent segmentation variables. Again, the learning
problem is rather different from ours. Similarly,
the work in (Zhang et al, 2008) reports on a multi-
stage model, without a latent segmentation vari-
able, but with a strong prior preferring sparse esti-
mates embedded in a Variational Bayes (VB) esti-
mator. This work concentrates the efforts on prun-
ing both the space of phrase pairs and the space of
(ITG) analyses.
123
To the best of our knowledge, this work is the
first to attempt learning probabilistic phrase-based
BITGs as translation models in a setting where
both a phrase segmentation component and a hi-
erarchical reordering component are assumed la-
tent variables. Like this work, (Mylonakis and
Sima?an, 2008; DeNero et al, 2008) also employ
an all-phrases model. Our paper shows that it is
possible to train such huge grammars under itera-
tive schemes like CV-EM, without need for sam-
pling or pruning. At the surface of it, our CV-
EM estimator is also a kind of Bayesian learner,
but in reality it is a more specific form of regu-
larisation, similar to smoothing techniques used in
language modelling (Chen and Goodman, 1998;
Mackay and Petoy, 1995).
7 Discussion and Future Research
Phrase-based stochastic SCFGs provide a rich for-
malism to express translation phenomena, which
has been shown to offer competitive performance
in practice. Since learning SCFGs for machine
translation has proven notoriously difficult, most
successful SCFGmodels for SMT rely on rules ex-
tracted from word-alignment patterns and heuris-
tically computed rule scores, with the impact and
the limits imposed by these choices yet unknown.
Some of the reasons behind the challenges of
SCFG learning can be traced back to the introduc-
tion of latent variables at different, competing lev-
els: word and phrase-alignment as well as hier-
archical reordering structure, with larger phrase-
pairs reducing the need for extensive reordering
structure and vice versa. While imposing priors
such as the often used Dirichlet distribution or the
Dirichlet Process provides a method to overcome
these pitfalls, we believe that the data-driven reg-
ularisation employed in this work provides an ef-
fective alternative to them, focusing more on the
data instead of importing generic external human
knowledge.
We believe that this work makes a significant
step towards learning synchronous grammars for
SMT. This is an objective not only worthy be-
cause of promises of increased performance, but,
most importantly, also by increasing the depth of
our understanding on SCFGs as vehicles of latent
translation structures. Our usage of the induced
grammars directly for translation, instead of an in-
termediate task such as phrase-alignment, aims ex-
actly at this.
While the latent structures that we explored
in this paper were relatively simple in compar-
ison with Hiero-like SCFGs, they take a differ-
ent, content-driven approach on learning reorder-
ing preferences than the context-driven approach
of Hiero. We believe that these approaches are not
merely orthogonal, but could also prove comple-
mentary. Taking advantage of the possible syner-
gies between content and context-driven reorder-
ing learning is an appealing direction of future re-
search. This is particularly promising for other
language pairs, such as Chinese to English, where
Hiero-like grammars have been shown to perform
particularly well.
Acknowledgments: Both authors are supported
by a VIDI grant (nr. 639.022.604) from The
Netherlands Organization for Scientific Research
(NWO).
References
P. Blunsom, T. Cohn, and M. Osborne. 2008a. A dis-
criminative latent variable model for statistical ma-
chine translation. In Proceedings of ACL-08: HLT,
pages 200?208. Association for Computational Lin-
guistics.
Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008b. Bayesian synchronous grammar induction.
In Advances in Neural Information Processing Sys-
tems 21, Vancouver, Canada, December.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles
Osborne. 2009. A gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
47th Annual Meeting of the Association of Compu-
tational Linguistics, Singapore, August. Association
for Computational Linguistics.
R. Bod, R. Scha, and K. Sima?an, editors. 2003. Data
Oriented Parsing. CSLI Publications, Stanford Uni-
versity, Stanford, California, USA.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Tech-
nical Report TR-10-98, Harvard University, August.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why generative phrase models underperform sur-
face heuristics. In Proceedings on the Workshop on
Statistical Machine Translation, pages 31?38, New
York City. Association for Computational Linguis-
tics.
124
John DeNero, Alexandre Bouchard-Co?te?, and Dan
Klein. 2008. Sampling alignment structure un-
der a Bayesian translation model. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 314?323, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
T. Hastie, R. Tibshirani, and J. H. Friedman. 2001. The
Elements of Statistical Learning. Springer.
Tom Heskes. 1998. Bias/variance decompositions for
likelihood-based estimators. Neural Computation,
10:1425?1433.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In HLT-NAACL 2003.
P. Koehn. 2005. Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In MT Summit 2005.
Zhifei Li, Chris Callison-Burch, Chris Dyer, San-
jeev Khudanpur, Lane Schwartz, Wren Thornton,
Jonathan Weese, and Omar Zaidan. 2009. Joshua:
An open source toolkit for parsing-based machine
translation. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, pages 135?139,
Athens, Greece, March. Association for Computa-
tional Linguistics.
David J. C. Mackay and Linda C. Bauman Petoy. 1995.
A hierarchical dirichlet language model. Natural
Language Engineering, 1:1?19.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proceedings of Empirical methods in natural lan-
guage processing, pages 133?139. Association for
Computational Linguistics.
Markos Mylonakis and Khalil Sima?an. 2008. Phrase
translation probabilities with itg priors and smooth-
ing as learning objective. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 630?639, Honolulu, USA,
October.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403.
H. Zhang, Ch. Quirk, R. C. Moore, and D. Gildea.
2008. Bayesian learning of non-compositional
phrases with synchronous parsing. In Proceedings
of ACL-08: HLT, pages 97?105, Columbus, Ohio,
June. Association for Computational Linguistics.
A. Zollmann and K. Sima?an. 2006. An efficient
and consistent estimator for data-oriented parsing.
Journal of Automata, Languages and Combinatorics
(JALC), 10 (2005) Number 2/3:367?388.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York City,
June. Association for Computational Linguistics.
125
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 92?100,
COLING 2010, Beijing, August 2010.
A Discriminative Syntactic Model for Source Permutation
via Tree Transduction
Maxim Khalilov and Khalil Sima?an
Institute for Logic, Language and Computation
University of Amsterdam
{m.khalilov,k.simaan}@uva.nl
Abstract
A major challenge in statistical machine
translation is mitigating the word or-
der differences between source and tar-
get strings. While reordering and lexical
translation choices are often conducted in
tandem, source string permutation prior
to translation is attractive for studying re-
ordering using hierarchical and syntactic
structure. This work contributes an ap-
proach for learning source string permu-
tation via transfer of the source syntax
tree. We present a novel discriminative,
probabilistic tree transduction model, and
contribute a set of empirical upperbounds
on translation performance for English-
to-Dutch source string permutation under
sequence and parse tree constraints. Fi-
nally, the translation performance of our
learning model is shown to outperform the
state-of-the-art phrase-based system sig-
nificantly.
1 Introduction
From its beginnings, statistical machine transla-
tion (SMT) has faced a word reordering challenge
that has a major impact on translation quality.
While standard mechanisms embedded in phrase-
based SMT systems, e.g. (Och and Ney, 2004),
deal efficiently with word reordering within a lim-
ited window of words, they are still not expected
to handle all possible reorderings that involve
words beyond this relatively narrow window, e.g.,
(Tillmann and Ney, 2003; Zens and Ney, 2003;
Tillman, 2004). More recent work handles word
order differences between source and target lan-
guages using hierarchical methods that draw on
Inversion Transduction Grammar (ITG), e.g., (Wu
and Wong, 1998; Chiang, 2005). In principle,
the latter approach explores reordering defined by
the choice of swapping the order of sibling sub-
trees under each node in a binary parse-tree of the
source/target sentence.
An alternative approach aims at minimizing the
need for reordering during translation by permut-
ing the source sentence as a pre-translation step,
e.g., (Collins et al, 2005; Xia and McCord, 2004;
Wang et al, 2007; Khalilov, 2009). In effect,
the translation process works with a model for
source permutation (s ? s?) followed by trans-
lation model (s? ? t), where s and t are source
and target strings and s? is the target-like permuted
source string. In how far can source permutation
reduce the need for reordering in conjunction with
translation is an empirical question.
In this paper we define source permutation as
the problem of learning how to transfer a given
source parse-tree into a parse-tree that minimizes
the divergence from target word-order. We model
the tree transfer ?s ? ?s? as a sequence of local,independent transduction operations, each trans-
forming the current intermediate tree ?s?i into thenext intermediate tree ?s?i+1 , with ?s0 = ?s and
?s?n = ?s? . A transduction operation merely per-mutes the sequence of n > 1 children of a single
node in an intermediate tree, i.e., unlike previous
work, we do not binarize the trees. The number
of permutations is factorial in n, and learning a
sequence of transductions for explaining a source
permutation can be computationally rather chal-
lenging (see (Tromble and Eisner, 2009)). Yet,
92
from the limited perspective of source string per-
mutation (s ? s?), another challenge is to inte-
grate a figure of merit that measures in how far s?
resembles a plausible target word-order.
We contribute solutions to these challenging
problems. Firstly, we learn the transduction
operations using a discriminative estimate of
P (pi(?x) |Nx, ?x, contextx), where Nx is the la-
bel of node (address) x, Nx ? ?x is the context-
free production under x, pi(?x) is a permutation of
?x and contextx represents a surrounding syntac-
tic context. As a result, this constrains {pi(?x)}
only to those found in the training data, and it
conditions the transduction application probabil-
ity on its specific contexts. Secondly, in every se-
quence s?0 = s, . . . , s?n = s? resulting from a tree
transductions, we prefer those local transductions
on ?s?i?1 that lead to source string permutation s
?
i
that are closer to target word order than s?i?1; we
employ s? language model probability ratios as a
measure of word order improvement.
In how far does the assumption of source per-
mutation provide any window for improvement
over a phrase-based translation system? We con-
duct experiments on translating from English into
Dutch, two languages which are characterized
by a number of systematic divergences between
them. Initially, we conduct oracle experiments
with varying constraints on source permutation
to set upperbounds on performance relative to
a state-of-the-art system. Translating the oracle
source string permutation (obtained by untangling
the crossing alignments) offers a large margin of
improvement, whereas the oracle parse tree per-
mutation provides a far smaller improvement. A
minor change to the latter to also permute con-
stituents that include words aligned with NULL,
offers further improvement, yet lags bahind bare
string permutation. Subsequently, we present
translation results using our learning approach,
and exhibit a significant improvement in BLEU
score over the state-of-the-art baseline system.
Our analysis shows that syntactic structure can
provide important clues for reordering in trans-
lation, especially for dealing with long distance
cases found in, e.g., English and Dutch. Yet, tree
transduction by merely permuting the order of sis-
ter subtrees might turn out insufficient.
2 Baseline: Phrase-based SMT
Given a word-aligned parallel corpus, phrase-
based systems (Och and Ney, 2002; Koehn et al,
2003) work with (in principle) arbitrarily large
phrase pairs (also called blocks) acquired from
word-aligned parallel data under a simple defi-
nition of translational equivalence (Zens et al,
2002). The conditional probabilities of one phrase
given its counterpart are interpolated log-linearly
together with a set of other model estimates:
e?I1 = argmaxeI1
{ M?
m=1
?mhm(eI1, fJ1 )
}
(1)
where a feature function hm refer to a system
model, and the corresponding ?m refers to the rel-
ative weight given to this model. A phrase-based
system employs feature functions for a phrase pair
translation model, a language model, a reordering
model, and a model to score translation hypothesis
according to length. The weights ?m are usually
optimized for system performance (Och, 2003) as
measured by BLEU (Papineni et al, 2002). Two
reordering methods are widely used in phrase-
based systems.
Distance-based A simple distance-based re-
ordering model default for Moses system is the
first reordering technique under consideration.
This model provides the decoder with a cost lin-
ear to the distance between words that should be
reordered.
MSD A lexicalized block-oriented data-driven
reordering model (Tillman, 2004) considers three
different orientations: monotone (M), swap (S),
and discontinuous (D). The reordering probabili-
ties are conditioned on the lexical context of each
phrase pair, and decoding works with a block se-
quence generation process with the possibility of
swapping a pair of blocks.
3 Related Work on Source Permutation
The integration of linguistic syntax into SMT
systems offers a potential solution to reordering
problem. For example, syntax is successfully
integrated into hierarchical SMT (Zollmann and
93
Venugopal, 2006). Similarly, the tree-to-string
syntax-based transduction approach offers a com-
plete translation framework (Galley et al, 2006).
The idea of augmenting SMT by a reordering
step prior to translation has often been shown to
improve translation quality. Clause restructuring
performed with hand-crafted reordering rules for
German-to-English and Chinese-to-English tasks
are presented in (Collins et al, 2005) and (Wang
et al, 2007), respectively. In (Xia and McCord,
2004; Khalilov, 2009) word reordering is ad-
dressed by exploiting syntactic representations of
source and target texts.
Other reordering models operate provide the
decoder with multiple word orders. For ex-
ample, the MaxEnt reordering model described
in (Xiong et al, 2006) provides a hierarchi-
cal phrasal reordering system integrated within
a CKY-style decoder. In (Galley and Manning,
2008) the authors present an extension of the fa-
mous MSD model (Tillman, 2004) able to handle
long-distance word-block permutations. Coming
up-to-date, in (PVS, 2010) an effective application
of data mining techniques to syntax-driven source
reordering for MT is presented.
Recently, Tromble and Eisner (2009) define
source permutation as learning source permuta-
tions; the model works with a preference matrix
for word pairs, expressing preference for their two
alternative orders, and a corresponding weight
matrix that is fit to the parallel data. The huge
space of permutations is then structured using a
binary synchronous context-free grammar (Binary
ITG) with O(n3) parsing complexity, and the per-
mutation score is calculated recursively over the
tree at every node as the accumulation of the
relative differences between the word-pair scores
taken from the preference matrix. Application to
German-to-English translation exhibits some per-
formance improvement.
Our work is in the general learning direction
taken in (Tromble and Eisner, 2009) but differs
both in defining the space of permutations, using
local probabilistic tree transductions, as well as in
the learning objective aiming at scoring permuta-
tions based on a log-linear interpolation of a lo-
cal syntax-based model with a global string-based
(language) model.
4 Pre-Translation Source Permutation
Given a word-aligned parallel corpus, we define
the source string permutation as the task of learn-
ing to unfold the crossing alignments between
sentence pairs in the parallel corpus. Let be given
a source-target sentence pair s ? t with word
alignment set a between their words. Unfold-
ing the crossing instances in a should lead to as
monotone an alignment a? as possible between a
permutation s? of s and the target string t. Con-
ducting such a ?monotonization? on the parallel
corpus gives two parallel corpora: (1) a source-
to-permutation parallel corpus (s ? s?) and
(2) a source permutation-to-target parallel corpus
(s? ? t). The latter corpus is word-aligned au-
tomatically again and used for training a phrase-
based translation system, while the former corpus
is used for training our model for pre-translation
source permutation via parse tree transductions.
Figure 1: Example of crossing alignments and
long-distance reordering using a source parse tree.
In itself, the problem of permuting the source
string to unfold the crossing alignments is compu-
tationally intractable (see (Tromble and Eisner,
2009)). However, different kinds of constraints
can be made on unfolding the crossing alignments
in a. A common approach in hierarchical SMT is
to assume that the source string has a binary parse
tree, and the set of eligible permutations is defined
by binary ITG transductions on this tree. This de-
fines permutations that can be obtained only by
at most inverting pairs of children under nodes of
the source tree. Figure 1 exhibits a long distance
reordering of the verb in English-to-Dutch transla-
tion: inverting the order of the children under the
VP node would unfold the crossing alignment.
94
4.1 Oracle Performance
As has been shown in the literature (Costa-jussa`
and Fonollosa, 2006; Khalilov and Sima?an, 2010;
Wang et al, 2007), source and target texts mono-
tonization leads to a significant improvement in
terms of translation quality. However it is not
known how many alignment crossings can be un-
folded under different parse tree conditions. In or-
der to gauge the impact of corpus monotonization
on translation system performance, we trained a
set of oracle translation systems, which create
target sentences that follow the source language
word order using the word alignment links and
various constraints.
(a) Word alignment.
(b) Parse tree and corre-
sponding alignment.
(c) Word alignment
and ADJP span.
Figure 2: Reordering example.
The set-up of our experiments and corpus char-
acteristics are detailed in Section 5. Table 1 re-
ports translation scores of the oracle systems. No-
tice that all the numbers are calculated on the re-
aligned corpora. Baseline results are provided for
informative purposes.
String permutation The first oracle system
under consideration is created by traversing
the string from left to right and unfolding all
crossing alignment links (we call this system
oracle-string). For example in Figure 2(a),
the oracle-string system generates a string ?do
so gladly? swapping the words ?do? and
?gladly? without considering the parse tree.
The first line of the table shows the performance
of the oracle-string system with monotone source
and target portions of the corpus.
Oracle under tree constraint We use a syntac-
tic parser for parsing the English source sentences
that provide n-ary constituency parses. Now we
constrain unfolding crossing alignments only to
those alignment links which agree with the struc-
ture of the source-side parse tree and consider the
constituents which include aligned tokens only.
Unfolding a crossing alignment is modeled as per-
muting the children of a node in the parse tree. We
refer to this oracle system as oracle-tree. For ex-
ample provided in Figure 2(b), there is no way to
construct a monotonized version of the sentence
since the word ?so? is aligned to NULL and im-
pedes swapping the order of VB and ADJP under
the VP.
Oracle under relaxed tree constraint The
oracle-tree system does not permute the words
which are both (1) not found in the alignment and
(2) are spanned by the sub-trees sibling to the re-
ordering constituents. Now we introduce a re-
laxed version of the parse tree constraint: the or-
der of the children of a node is permuted when
the node covers the reordering constituents and
also when the frontier contains leaf nodes aligned
with NULL (oracle-span). For example, in Fig-
ure 2(c) the English word ?so? is not aligned, but
according to the relaxed version, must move to-
gether with the word ?gladly? since they share
a parent node (ADJP).
Source BLEU NIST
baseline dist 24.04 6.29
baseline MSD 24.04 6.28
oracle? string 27.02 6.51
oracle? tree 24.09 6.30
oracle? span 24.95 6.37
Table 1: Translation scores of oracle systems.
The main conclusion which can be drawn from
the oracle results is that there is a possibility for
relatively big (?3 BLEU points) improvement
with complete unfolding of crossing alignments
and very limited (?0.05 BLEU points) with the
same done under the parse tree constraint. A tree-
based system that allows for permuting unaligned
words that are covered by a dominating parent
node shows more improvement in terms of BLEU
and NIST scores (?0.9 BLEU points).
The gap between oracle-string and oracle-tree
performance is due to alignment crossings which
95
cannot be unfolded under trees (illustrated in Fig-
ure 3), but possibly also due to parse and align-
ment errors.
Figure 3: Example of alignment crossing that does
not agree with the parse tree.
4.2 Source Permutation via Syntactic
Transfer
Given a parallel corpus with string pairs s ? t
with word alignment a, we create a source per-
muted parallel corpus s ? s? by unfolding the
crossing alignments in a: this is done by scanning
the string s from left to right and moving words in-
volved in crossing alignments to positions where
the crossing alignments are unfolded). The source
strings s are parsed, leading to a single parse tree
?s per source string.
Our model aims at learning from the source
permuted parallel corpus s ? s? a probabilistic
optimization argmaxpi(s) P (pi(s) | s, ?s). We as-
sume that the set of permutations {pi(s)} is de-
fined through a finite set of local transductions
over the tree ?s. Hence, we view the permutations
leading from s to s? as a sequence of local tree
transductions ?s?0 ? . . . ? ?s?n , where s
?
0 = s
and s?n = s? , and each transduction ?s?i?1 ? ?s?iis defined using a tree transduction operation that
at most permutes the children of a single node in
?s?i?1 as defined next.
A local transduction ?s?i?1 ? ?s?i is modelledby an operation that applies to a single node with
address x in ?s?i?1 , labeled Nx, and may permutethe ordered sequence of children ?x dominated by
node x. This constitutes a direct generalization of
the ITG binary inversion transduction operation.
We assign a conditional probability to each such
local transduction:
P (?s?i | ?s?i?1) ? P (pi(?x) | Nx ? ?x, Cx) (2)
where pi(?x) is a permutation of ?x (the ordered
sequence of node labels under x) and Cx is a local
tree context of node x in tree ?s?i?1 . One wrin-kle in this definition is that the number of possi-
ble permutations of ?x is factorial in the length
of ?x. Fortunately, the source permuted training
data exhibits only a fraction of possible permuta-
tions even for longer ?x sequences. Furthermore,
by conditioning the probability on local context,
the general applicability of the permutation is re-
strained.
Given this definition, we define the probabil-
ity of the sequence of local tree transductions
?s?0 ? . . . ? ?s?n as
P (?s?0 ? . . . ? ?s?n) =
n?
i=1
P (?s?i | ?s?i?1) (3)
The problem of calculating the most likely per-
mutation under this transduction model is made
difficult by the fact that different transduction se-
quences may lead to the same permutation, which
demands summing over these sequences. Fur-
thermore, because every local transduction condi-
tions on local context of an intermediate tree, this
quickly risks becoming intractable (even when we
use packed forests). In practice we take a prag-
matic approach and greedily select at every inter-
mediate point ?s?i?1 ? ?s?i the single most likelylocal transduction that can be conducted on any
node of the current intermediate tree ?s?i?1 usingan interpolation of the term in Equation 2 with
string probability ratios as follows:
P (pi(?x) | Nx ? ?x, Cx)?
P (s?i?1)
P (s?i)
The rationale behind this log-linear interpolation
is that our source permutation approach aims at
finding the optimal permutation s? of s that can
serve as input for a subsequent translation model.
Hence, we aim at tree transductions that are syn-
tactically motivated that also lead to improved
string permutation. In this sense, the tree trans-
duction definitions can be seen as an efficient and
96
syntactically informed way to define the space of
possible permutations.
We estimate the string probabilities P (s?i) us-
ing 5-gram language models trained on the s?
side of the source permuted parallel corpus s ?
s? . We estimate the conditional probability
P (pi(?x) | Nx ? ?x, Cx) using a Maximum-
Entropy framework, where feature functions are
defined to capture the permutation as a class, the
node label Nx and its head POS tag, the child
sequence ?x together with the corresponding se-
quence of head POS tags and other features corre-
sponding to different contextual information.
We were particularly interested in those linguis-
tic features that motivate reordering phenomena
from the syntactic and linguistic perspective. The
features that were used for training the permuta-
tion system are extracted for every internal node
of the source tree that has more than one child:
? Local tree topology. Sub-tree instances that
include parent node and the ordered se-
quence of child node labels.
? Dependency features. Features that deter-
mine the POS tag of the head word of the cur-
rent node, together with the sequence of POS
tags of the head words of its child nodes.
? Syntactic features. Three binary features
from this class describe: (1) whether the par-
ent node is a child of the node annotated with
the same syntactic category, (2) whether the
parent node is a descendant of the node an-
notated with the same syntactic category, and
(3) if the current subtree is embedded into a
?SENT-SBAR? sub-tree. The latter feature in-
tends to model the divergence in word order
in relative clauses between Dutch and En-
glish which is illustrated in Figure 1.
In initial experiments we piled up all feature func-
tions into a single model. Preliminary results
showed that the system performance increases if
the set of patterns is split into partial classes con-
ditioned on the current node label. Hence, we
trained four separate MaxEnt models for the cate-
gories with potentially high number of crossing
alignments, namely VP, NP, SENT, and SBAR.
For combinatory models we use the following no-
tations: M4 = ?i?[ NP, VP, SENT, SBAR] Mi and M2 =?
i?[VP, SENT] Mi.
5 Experiments and results
The SMT system used in the experiments was
implemented within the open-source MOSES
toolkit (Koehn et al, 2007). Standard train-
ing and weight tuning procedures which were
used to build our system are explained in details
on the MOSES web page1. The MSD model
was used together with a distance-based reorder-
ing model. Word alignment was estimated with
GIZA++ tool2 (Och, 2003), coupled with mk-
cls3 (Och, 1999), which allows for statistical word
clustering for better generalization. An 5-gram
target language model was estimated using the
SRI LM toolkit (Stolcke, 2002) and smoothed
with modified Kneser-Ney discounting. We use
the Stanford parser4 (Klein and Manning, 2003)
as a source-side parsing engine. The parser was
trained on the English treebank set provided with
14 syntactic categories and 48 POS tags. The
evaluation conditions were case-sensitive and in-
cluded punctuation marks. For Maximum En-
tropy modeling we used the maxent toolkit5.
Data The experiment results were obtained us-
ing the English-Dutch corpus of the European Par-
liament Plenary Session transcription (EuroParl).
Training corpus statistics can be found in Table 2.
Dutch English
Sentences 1.2 M 1.2 M
Words 32.9 M 33.0 M
Average sentence length 27.20 27.28
Vocabulary 228 K 104 K
Table 2: Basic statistics of the English-Dutch Eu-
roParl training corpus.
The development and test datasets were ran-
domly chosen from the corpus and consisted of
1http://www.statmt.org/moses/
2code.google.com/p/giza-pp/
3http://www.fjoch.com/mkcls.html
4http://nlp.stanford.edu/software/
lex-parser.shtml
5http://homepages.inf.ed.ac.uk/
lzhang10/maxent_toolkit.html
97
500 and 1,000 sentences, respectively. Both were
provided with one reference translation.
Results Evaluation of the system performance
is twofold. In the first step, we analyze the qual-
ity of reordering method itself. In the next step
we look at the automatic translation scores and
evaluate the impact which the choice of reorder-
ing strategy has on the translation quality. In
both stages of evaluation, the results are con-
trasted with the performance shown by the stan-
dard phrase-based SMT system (baseline) and
with oracle results.
Source reordering analysis Table 3 shows the
parameters of the reordered system allowing to as-
sess the effectiveness of reordering permutations,
namely: (1) a total number of crossings found in
the word alignment (#C), (2) the size of the re-
sulting phrase table (PT), (3) BLEU, NIST, and
WER scores obtained using monotonized parallel
corpus (oracle) as a reference.
All the numbers are calculated on the re-aligned
corpora. Calculations are done on the basis of the
100,000 line extraction from the corpus6 and cor-
responding alignment matrix. The baseline rows
show the number of alignment crossings found in
the original (unmonotonized) corpus.
System #C PT ScoresBLEU NIST WER
Oracle
string 54.6K 48.4M - - -
tree 187.3K 30.3M 71.73 17.01 16.77
span 146.9K 33.0M 73.41 17.11 15.73
Baselines
baselines 187.0K 29.8M 71.70 17.07 16.55
Category models
MNP 188.9K 29.7M 71.63 17.07 16.52
MV P 168.1K 29.8M 73.17 17.16 15.99
MSENT 171.0K 29.8M 73.08 17.08 16.10
MSBAR 188.6K 29.8M 72.89 16.90 16.41
Combinatory models
M4 193.2K 29.1M 70.98 16.85 16.78
M2 165.4K 29.9M 73.07 16.92 15.88
Table 3: Main parameters of the tree-based re-
ordering system.
6A smaller portion of the corpus is used for analysis in
order to reduce evaluation time.
Translation scores The evaluation results for
the development and test corpora are reported in
Table 4. They include two baseline configurations
(dist and MSD), oracle results and contrasts them
with the performance shown by different combi-
nations of single-category tree-based reordering
models. Best scores within each experimental sec-
tion are placed in cells filled with grey.
System Dev TestBLEU BLEU NIST
baseline dist 23.88 24.04 6.29
baseline MSD 24.07 24.04 6.28
oracle-string 26.28 27.02 6.50
oracle-tree 23.84 24.09 6.30
oracle-span 24.79 24.95 6.35
MNP 23.79 23.81 6.27
MV P 24.16 24.55 6.29
MSENT 24.27 24.56 6.32
MSBAR 23.99 24.12 6.27
M4 23.50 23.86 6.29
M2 24.28 24.64 6.33
Table 4: Experimental results.
Analysis The number of crossings
found in word alignment intersection and
BLEU/NIST/WER scores estimated on reordered
data vs. monotonized data report the reordering
algorithm effectiveness. A big gap between num-
ber of crossings and total number of reorderings
per corpus found in oracle-string system7 and
baseline systems demonstrates the possible reduc-
tion of system?s non-monotonicity. The difference
in number of crossings and BLEU/NIST/WER
scores between the oracle-span and the best
performing MaxEnt models (namely, M2) shows
the level of performance of the prediction module.
A number of distinct phrase translation pairs in
the translation table implicitly reveals the general-
ization capabilities of the translation system since
it simplifies the translation task. From the other
hand, increased number of shorter phrases can add
noise in the reordered data and makes decoding
more complex. Hence, the size of phrase table it-
self can not be considered as a robust indicator of
its translation potential.
7The number of crossings for oracle configuration is not
zero since this parameter is calculated on the re-aligned cor-
pus.
98
Table 4 shows that three of six MaxEnt re-
ordering systems outperform baseline systems by
about 0.5-0.6 BLEU points, that is statistically
significant8. The combination of NP, NP, SENT,
and SBAR models do not show good performance
possibly due to increased sparseness of reorder-
ing patterns. However, the system that consider
only the MV P and MSENT models achieves 0.62
BLEU score gain over the baseline configurations.
The main conclusion which can be drawn from
analysis of Tables 3 and 4 is that there is an
evident correlation between characteristics of re-
ordering system and performance demonstrated
by the translation system trained on the corpus
with reordered source part.
Example Figure 4 exemplifies the sentences
that presumably benefits from the monotonization
of the source part of the parallel corpus. The ex-
ample demonstrates a pervading syntactic distinc-
tion between English and Dutch: the reordering of
verb-phrase subconstituents VP NP PP within the
relative clause into PP NP VP.
6 Conclusions and future work
We introduced a tree-based reordering model that
aims at monotonizing the word order of source
8All statistical significance calculations are done for a
95% confidence interval and 1 000 resamples, following the
guidelines from (Koehn, 2004).
and target languages as a pre-translation step. Our
model avoids complete generalization of reorder-
ing instances by using tree contexts and limit-
ing the permutations to data instances. From a
learning perspective, our work shows that navigat-
ing a large space of intermediate tree transforma-
tions can be conducted effectively using both the
source-side syntactic tree and a language model
of the idealized (target-like) source-permuted lan-
guage.
We have shown the potential for translation
quality improvement when target sentences are
created following the source language word or-
der (?3 BLEU points over the standard phrase-
based SMT) and under parse tree constraint (?0.9
BLEU points). As can be seen from these re-
sults, our model exhibits competitive translation
performance scores compared with the standard
distance-based and lexical reordering.
The gap between the oracle and our system?s
results leaves room for improvement. We intend
to study extensions of the current tree transfer
model to narrow this performance gap. As a first
step we are combining isolated models for con-
crete syntactic categories and aggregating more
features into the MaxEnt model. Algorithmic im-
provements, such as beam-search and chart pars-
ing, could allow us to apply our method to full
parse-forests as opposed to a single parse tree.
(a) Original parse tree. (b) Reordered parse tree.
Src: that ... to lead the Commission during the next five-year term
Ref.: dat ... om de komende vijf jaar de Commissie te leiden
Baseline MSD: dat ... om het voortouw te nemen in de Commissie tijdens de komende vijf jaar
Rrd src: that ... during the next five-year term the Commission to lead
M2 : dat ... om de Commissie tijdens de komende vijf jaar te leiden
(c) Translations.
Figure 4: Example of tree-based monotonization.
99
References
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings
of ACL?05, pages 263?270.
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause
restructuring for statistical machine translation. In
Proceedings of ACL?05, pages 531?540.
M. R. Costa-jussa` and J. A. R. Fonollosa. 2006.
Statistical machine reordering. In Proceedings of
HLT/EMNLP?06, pages 70?76.
M. Galley and Ch. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
Proceedings of EMNLP?08, pages 848?856.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. De-
Neefe, W. Wang, and I. Thaye. 2006. Scalable in-
ference and training of context-rich syntactic trans-
lation models. In Proc. of COLING/ACL?06, pages
961?968.
M. Khalilov and K. Sima?an. 2010. Source reordering
using maxent classifiers and supertags. In Proc. of
EAMT?10, pages 292?299.
M. Khalilov. 2009. New statistical and syntactic mod-
els for machine translation. Ph.D. thesis, Universi-
tat Polite`cnica de Catalunya, October.
D. Klein and C. Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of the 41st Annual
Meeting of the ACL?03, pages 423?430.
Ph. Koehn, F. Och, and D. Marcu. 2003. Statistical
phrase-based machine translation. In Proceedings
of the HLT-NAACL 2003, pages 48?54.
Ph. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: open-source
toolkit for statistical machine translation. In Pro-
ceedings of ACL 2007, pages 177?180.
Ph. Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP?04, pages 388?395.
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Proceedings of ACL?02, pages
295?302.
F. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
F. Och. 1999. An efficient method for determin-
ing bilingual word classes. In Proceedings of ACL
1999, pages 71?76.
F. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of ACL?03,
pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL?02, pages 311?
318.
A. PVS. 2010. A data mining approach to
learn reorder rules for SMT. In Proceedings of
NAACL/HLT?10, pages 52?57.
A. Stolcke. 2002. SRILM: an extensible language
modeling toolkit. In Proceedings of SLP?02, pages
901?904.
C. Tillman. 2004. A unigram orientation model for
statistical machine translation. In Proceedings of
HLT-NAACL?04, pages 101?104.
C. Tillmann and H. Ney. 2003. Word reordering and
a dynamic programming beam search algorithm for
statistical machine translation. Computational Lin-
guistics, 1(29):93?133.
R. Tromble and J. Eisner. 2009. Learning linear order-
ing problems for better translation. In Proceedings
of EMNLP?09, pages 1007?1016.
C. Wang, M. Collins, and Ph. Koehn. 2007. Chinese
syntactic reordering for statistical machine transla-
tion. In Proceedings of EMNLP-CoNLL?07, pages
737?745.
D. Wu and H. Wong. 1998. Machine translation wih a
stochastic grammatical channel. In Proceedings of
ACL-COLING?98, pages 1408?1415.
F. Xia and M. McCord. 2004. Improving a statistical
MT system with automatically learned rewrite pat-
terns. In Proceedings of COLING?04, pages 508?
514.
D. Xiong, Q. Liu, and S. Lin. 2006. Maximum en-
tropy based phrase reordering model for statistical
machine translation. In Proceedings of ACL?06,
pages 521?528.
R. Zens and H. Ney. 2003. A comparative study on
reordering constraints in statistical machine transla-
tion. In Proceedings of ACL?03, pages 144?151.
R. Zens, F. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In Proceedings of KI:
Advances in Artificial Intelligence, pages 18?32.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In
Proceedings of NAACL?06, pages 138?141.
100
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 413?419,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
ILLC-UvA translation system for EMNLP-WMT 2011
Maxim Khalilov and Khalil Sima?an
Institute for Logic, Language and Computation
University of Amsterdam
P.O. Box 94242
1090 GE Amsterdam, The Netherlands
{m.khalilov,k.simaan}@uva.nl
Abstract
In this paper we describe the Institute for
Logic, Language and Computation (Uni-
versity of Amsterdam) phrase-based statisti-
cal machine translation system for English-
to-German translation proposed within the
EMNLP-WMT 2011 shared task. The main
novelty of the submitted system is a syntax-
driven pre-translation reordering algorithm
implemented as source string permutation via
transfer of the source-side syntax tree.
1 Introduction
For the WMT 2011 shared task, ILLC-UvA submit-
ted two translations (primary and secondary) for the
English-to-German translation task. This year, we
directed our research toward addressing the word
order problem for statistical machine translation
(SMT) and discover its impact on output translation
quality. We reorder the words of a sentence of the
source language with respect to the word order of
the target language and a given source-side parse
tree. The difference from the baseline Moses-based
translation system lies in the pre-translation step, in
which we introduce a discriminative source string
permutation model based on probabilistic parse tree
transduction.
The idea here is to permute the order of the source
words in such a way that the resulting permutation
allows as monotone a translation process as possible
is not new. This approach to enhance SMT by using
a reordering step prior to translation has proved to be
successful in improving translation quality for many
translation tasks, see (Genzel, 2010; Costa-jussa` and
Fonollosa, 2006; Collins et al, 2005), for example.
The general problem of source-side reordering is
that the number of permutations is factorial in n, and
learning a sequence of transductions for explaining
a source permutation can be computationally rather
challenging. We propose to address this problem by
defining the source-side permutation process as the
learning problem of how to transfer a given source
parse tree into a parse tree that minimizes the diver-
gence from target word order.
Our reordering system is inspired by the direction
taken in (Tromble and Eisner, 2009), but differs in
defining the space of permutations, using local prob-
abilistic tree transductions, as well as in the learn-
ing objective aiming at scoring permutations based
on a log-linear interpolation of a local syntax-based
model with a global string-based (language) model.
The reordering (novel) and translation (standard)
components are described in the following sections.
The rest of this paper is structured as follows. After a
brief description of the phrase-based translation sys-
tem in Section 2, we present the architecture and de-
tails of our reordering system (Section 3), Section 4
reviews related work, Section 5 reports the experi-
mental setup, details the submissions and discusses
the results, while Section 6 concludes the article.
2 Baseline system
2.1 Statistical machine translation
In SMT the translation problem is formulated as se-
lecting the target translation t with the highest prob-
ability from a set of target hypothesis sentences for
413
the source sentence s: t? = argmaxt { p(t|s) } =
argmaxt { p(s|t) ? p(t) }.
2.2 Phrase-based translation
While first systems following this approach per-
formed translation on the word level, modern state-
of-the-art phrase-based SMT systems (Och and Ney,
2002; Koehn et al, 2003) start-out from a word-
aligned parallel corpus working with (in principle)
arbitrarily large phrase pairs (also called blocks) ac-
quired from word-aligned parallel data under a sim-
ple definition of translational equivalence (Zens et
al., 2002).
The conditional probabilities of one phrase given
its counterpart is estimated as the relative frequency
ratio of the phrases in the multiset of phrase-pairs
extracted from the parallel corpus and are interpo-
lated log-linearly together with a set of other model
estimates:
e?I1 = argmaxeI1
{ M?
m=1
?mhm(eI1, fJ1 )
}
(1)
where a feature function hm refer to a system model,
and the corresponding ?m refers to the relative
weight given to this model.
A phrase-based system employs feature func-
tions for a phrase pair translation model, a lan-
guage model, a reordering model, and a model
to score translation hypothesis according to length.
The weights ?m are optimized for system perfor-
mance (Och, 2003) as measured by BLEU (Papineni
et al, 2002).
Apart from the novel syntax-based reordering
model, we consider two reordering methods that
are widely used in phrase-based systems: a simple
distance-based reordering and a lexicalized block-
oriented data-driven reordering model (Tillman,
2004).
3 Architecture of the reordering system
We approach the word order challenge by including
syntactic information in a pre-translation reordering
framework. This section details the general idea of
our approach and details the reordering model that
was used in English-to-German experiments.
3.1 Pre-translation reordering framework
Given a word-aligned parallel corpus, we define the
source string permutation as the task of learning
to unfold the crossing alignments between sentence
pairs in the parallel corpus. Let be given a source-
target sentence pair s ? t with word alignment set
a between their words. Unfolding the crossing in-
stances in a should lead to as monotone an align-
ment a? as possible between a permutation s? of s
and the target string t. Conducting such a ?mono-
tonization? on the parallel corpus gives two par-
allel corpora: (1) a source-to-permutation parallel
corpus (s ? s?) and (2) a source permutation-to-
target parallel corpus (s? ? t). The latter corpus is
word-aligned automatically again and used for train-
ing a phrase-based translation system, while the for-
mer corpus is used for training our model for pre-
translation source permutation via parse tree trans-
ductions.
In itself, the problem of permuting the source
string to unfold the crossing alignments is com-
putationally intractable (see (Tromble and Eisner,
2009)). However, different kinds of constraints can
be made on unfolding the crossing alignments in a.
A common approach in hierarchical SMT is to as-
sume that the source string has a binary parse tree,
and the set of eligible permutations is defined by bi-
nary ITG transductions on this tree. This defines
permutations that can be obtained only by at most
inverting pairs of children under nodes of the source
tree.
3.2 Conditional tree reordering model
Given a parallel corpus with string pairs s ? t with
word alignment a, the source strings s are parsed,
leading to a single parse tree ?s per source string. We
create a source permuted parallel corpus s ? s? by
unfolding the crossing alignments in a without/with
syntactic tree to provide constraints on the unfold-
ing.
Our model aims at learning from the source per-
muted parallel corpus s ? s? a probabilistic op-
timization argmaxpi(s) P (pi(s) | s, ?s). We as-
sume that the set of permutations {pi(s)} is defined
through a finite set of local transductions over the
tree ?s. Hence, we view the permutations leading
from s to s? as a sequence of local tree transduc-
414
tions ?s?0 ? . . . ? ?s?n , where s
?
0 = s and s
?
n = s
? ,
and each transduction ?s?i?1 ? ?s?i is defined using a
tree transduction operation that at most permutes the
children of a single node in ?s?i?1 as defined next.A local transduction ?s?i?1 ? ?s?i is modelled byan operation that applies to a single node with ad-
dress x in ?s?i?1 , labeled Nx, and may permute theordered sequence of children ?x dominated by node
x. This constitutes a direct generalization of the ITG
binary inversion transduction operation. We assign a
conditional probability to each such local transduc-
tion:
P (?s?i | ?s?i?1) ? P (pi(?x) | Nx ? ?x, Cx) (2)
where pi(?x) is a permutation of ?x (the ordered
sequence of node labels under x) and Cx is a lo-
cal tree context of node x in tree ?s?i?1 . One wrin-kle in this definition is that the number of possible
permutations of ?x is factorial in the length of ?x.
Fortunately, the source permuted training data ex-
hibits only a fraction of possible permutations even
for longer ?x sequences. Furthermore, by condition-
ing the probability on local context, the general ap-
plicability of the permutation is restrained.
In principle, if we would disregard the computa-
tional cost, we could define the probability of the se-
quence of local tree transductions ?s?0 ? . . . ? ?s?nas
P (?s?0 ? . . . ? ?s?n) =
n?
i=1
P (?s?i | ?s?i?1) (3)
The problem of calculating the most likely permu-
tation under this kind of transduction probability
is intractable because every local transduction con-
ditions on local context of an intermediate tree1.
Hence, we disregard this formulation and in practice
we take a pragmatic approach and greedily select at
every intermediate point ?s?i?1 ? ?s?i the single mostlikely local transduction that can be conducted on
any node of the current intermediate tree ?s?i?1 . The
1Note that a single transduction step on the current tree
?s?i?1 leads to a forest of trees ?s?i because there can be mul-
tiple alternative transduction rules. Hence, this kind of a model
demands optimization over many possible sequences of trees,
which can be packed into a sequence of parse-forests with trans-
duction links between them.
individual steps are made more effective by interpo-
lating the term in Equation 2 with string probability
ratios:
P (pi(?x) | Nx ? ?x, Cx)?
(
P (s?i?1)
P (s?i)
)
(4)
The rationale behind this interpolation is that our
source permutation approach aims at finding the op-
timal permutation s? of s that can serve as input for
a subsequent translation model. Hence, we aim at
tree transductions that are syntactically motivated
that also lead to improved string permutations. In
this sense, the tree transduction definitions can be
seen as an efficient and syntactically informed way
to define the space of possible permutations.
We estimate the string probabilities P (s?i) using
5-gram language models trained on the s? side of
the source permuted parallel corpus s ? s? . We es-
timate the conditional probability P (pi(?x) | Nx ?
?x, Cx) using a Maximum-Entropy framework,
where feature functions are defined to capture the
permutation as a class, the node label Nx and its
head POS tag, the child sequence ?x together with
the corresponding sequence of head POS tags and
other features corresponding to different contextual
information.
We were particularly interested in those linguistic
features that motivate reordering phenomena from
the syntactic and linguistic perspective. The features
that were used for training the permutation system
are extracted for every internal node of the source
tree that has more than one child:
? Local tree topology. Sub-tree instances that in-
clude parent node and the ordered sequence of
child node labels.
? Dependency features. Features that determine
the POS tag of the head word of the current
node, together with the sequence of POS tags
of the head words of its child nodes.
? Syntactic features. Two binary features from
this class describe: (1) whether the parent node
is a child of the node annotated with the same
syntactic category, (2) whether the parent node
is a descendant of a node annotated with the
same syntactic category.
415
4 Related work
The integration of linguistic syntax into SMT sys-
tems offers a potential solution to reordering prob-
lem. For example, syntax is successfully integrated
into hierarchical SMT (Zollmann and Venugopal,
2006). In (Yamada and Knight, 2001), a set of tree-
string channel operations is defined over the parse
tree nodes, while reordering is modeled by permuta-
tions of children nodes. Similarly, the tree-to-string
syntax-based transduction approach offers a com-
plete translation framework (Galley et al, 2006).
The idea of augmenting SMT by a reordering step
prior to translation has often been shown to improve
translation quality. Clause restructuring performed
with hand-crafted reordering rules for German-to-
English and Chinese-to-English tasks are presented
in (Collins et al, 2005) and (Wang et al, 2007), re-
spectively. In (Xia and McCord, 2004; Khalilov,
2009) word reordering is addressed by exploiting
syntactic representations of source and target texts.
In (Costa-jussa` and Fonollosa, 2006) source and
target word order harmonization is done using well-
established SMT techniques and without the use of
syntactic knowledge. Other reordering models op-
erate provide the decoder with multiple word or-
ders. For example, the MaxEnt reordering model
described in (Xiong et al, 2006) provides a hierar-
chical phrasal reordering system integrated within
a CKY-style decoder. In (Galley and Manning,
2008) the authors present an extension of the famous
MSD model (Tillman, 2004) able to handle long-
distance word-block permutations. Coming up-to-
date, in (PVS, 2010) an effective application of data
mining techniques to syntax-driven source reorder-
ing for MT is presented.
Different syntax-based reordering systems can be
found in (Genzel, 2010). In this system, reorder-
ing rules capable to capture many important word
order transformations are automatically learned and
applied in the preprocessing step.
Recently, Tromble and Eisner (Tromble and Eis-
ner, 2009) define source permutation as the word-
ordering learning problem; the model works with a
preference matrix for word pairs, expressing pref-
erence for their two alternative orders, and a cor-
responding weight matrix that is fit to the parallel
data. The huge space of permutations is then struc-
tured using a binary synchronous context-free gram-
mar (Binary ITG) with O(n3) parsing complexity,
and the permutation score is calculated recursively
over the tree at every node as the accumulation of
the relative differences between the word-pair scores
taken from the preference matrix. Application to
German-to-English translation exhibits some perfor-
mance improvement.
5 Experiments and submissions
Design, architecture and configuration of the trans-
lation system that we used in experimentation co-
incides with the Moses-based translation system
(Baseline system) described in details on the
WMT 2011 web page2.
This section details the experiments carried out to
evaluate the proposed reordering model, experimen-
tal set-up and data.
5.1 Data
In our experiments we used EuroParl v6.0 German-
English parallel corpus provided by the organizers
of the evaluation campaign.
A detailed statistics of the training, development,
internal (test int.) and official (test of.) test datasets
can be found in Table 1. The development corpus
coincides with the 2009 test set and for internal test-
ing we used the test data proposed to the participants
of WMT 2010.
?ASL? stands for average sentence length. All the
sets were provided with one reference translation.
Data Sent. Words Voc. ASL
train En 1.7M 46.0M 121.3K 27.0
train Ge 1.7M 43.7M 368.5K 25.7
dev En 2.5K 57.6K 13.2K 22.8
test int. En 2.5K 53.2K 15.9K 21.4
test of. En 3.0K 74.8K 11.1K 24.9
Table 1: German-English EuroParl corpus (version 6.0).
Apart from the German portion of the EuroParl
parallel corpus, two additional monolingual corpora
from news domain (the News Commentary corpus
(NC) and the News Crawl Corpus 2011 (NS)) were
2http://www.statmt.org/wmt11/baseline.
html
416
used to train a language model for German. The
characteristics of these datasets can be found in Ta-
ble 2. Notice that the data were not de-duplicated.
Data Sent. Words Voc. ASL
NC Ge 161.8M 3.9G 136.7M 23.9
NS Ge 45.3M 799.4M 3.0M 17.7
Table 2: Monolingual German corpora used for target-
side language modeling.
5.2 Experimental setup
Moses toolkit (Koehn et al, 2007) in its standard
setting was used to build the SMT systems:
? GIZA++/mkcls (Och, 2003; Och, 1999) for
word alignment.
? SRI LM (Stolcke, 2002) for language model-
ing. A 3-gram target language model was es-
timated and smoothed with modified Kneser-
Ney discounting.
? MOSES (Koehn et al, 2007) to build an un-
factored translation system.
? the Stanford parser (Klein and Manning,
2003) was used as a source-side parsing en-
gine3.
? For maximum entropy modeling we used the
maxent toolkit4.
The discriminative syntactic reordering model is
applied to reorder training, development, and test
corpora. A Moses-based translation system (corpus
realignment included5) is then trained using the re-
ordered input.
5.3 Internal results and submissions
The outputs of two translation system were submit-
ted. First, we piled up all feature functions into a sin-
gle model as described in Section 3. It was our ?sec-
ondary? submission. However, our experience tells
3The parser was trained on the English treebank set provided
with 14 syntactic categories and 48 POS tags.
4http://homepages.inf.ed.ac.uk/lzhang10/
maxent_toolkit.html
5Some studies show that word re-alignment of a mono-
tonized corpus gives better results than unfolding of alignment
crossings (Costa-jussa` and Fonollosa, 2006).
that the system performance can increase if the set
of patterns is split into partial classes conditioned on
the current node label (Khalilov and Sima?an, 2010).
Hence, we trained three separate MaxEnt models for
the categories with potentially high reordering re-
quirements, namely NP , SENT and SBAR(Q).
It was defines as our ?primary? submission.
The ranking of submission was done according to
the results shown on internal testing, shown in Ta-
ble 3.
System BLEU dev BLEU test NIST test
Baseline 11.03 9.78 3.78
Primary 11.07 10.00 3.79
Secondary 10.92 9.91 3.78
Table 3: Internal testing results.
5.4 Official results and discussion
Unfortunately, the results of our participation this
year were discouraging. The primary submission
was ranked 30th (12.6 uncased BLEU-4) and the
secondary 31th (11.2) out of 32 submitted systems.
It turned out that our preliminary idea to extrapo-
late the positive results of English-to-Dutch transla-
tion reported in (Khalilov and Sima?an, 2010) to the
WMT English-to-German translation task was not
right.
Analyzing the reasons of negative results during
the post-evaluation period, we discovered that trans-
lation into German differs from English-to-Dutch
task in many cases. In contrast to English-to-Dutch
translation, the difference in terms of automatic
scores between the internal baseline system (without
external reordering) and the system enhanced with
the pre-translation reordering is minimal. It turns
out that translating into German is more complex
in general and discriminative reordering is more ad-
vantageous for English-to-Dutch than for English-
to-German translation.
A negative aspect influencing is the way how the
rules are extracted and applied according to our ap-
proach. Syntax-driven reordering, as described in
this paper, involves large contextual information ap-
plied cumulatively. Under conditions of scarce data,
alignment and parsing errors, it introduces noise to
the reordering system and distorts the feature prob-
417
ability space. At the same time, many reorderings
can be performed more efficiently based on fixed
(hand-crafted) rules (as it is done in (Collins et al,
2005)). A possible remedy to this problem is to
combine automatically extracted features with fixed
(hand-crafted) rules. Our last claims are supported
by the observations described in (Visweswariah et
al., 2010).
During post-evaluation period we analyzed the
reasons why the system performance has slightly
improved when separate MaxEnt models are ap-
plied. The outline of reordered nodes for
each of syntactic categories considered (SENT ,
SBAR(Q) and NP ) can be found in Table 4 (the
size of the corpus is 1.7 M of sentences).
Category # of applications
NP 497,186
SBAR(Q) 106,243
SENT 221,568
Table 4: Application of reorderings for separate syntactic
categories.
It is seen that the reorderings for NP nodes is
higher than for SENT and SBAR(Q) categories.
While SENT and SBAR(Q) reorderings work anal-
ogously for Dutch and German, our intuition is that
German has more features that play a role in reorder-
ing of NP structures than Dutch and there is a need
of more specific features to model NP permutations
in an accurate way.
6 Conclusions
This paper presents the ILLC-UvA translation sys-
tem for English-to-German translation task pro-
posed to the participants of the EMNLP-WMT 2011
evaluation campaign. The novel feature that we
present this year is a source reordering model in
which the reordering decisions are conditioned on
the features from the source parse tree.
Our system has not managed to outperform the
majority of the participating systems, possibly due
to its generic approach to reordering. We plan to in-
vestigate why our approach works well for English-
to-Dutch and less well for the English-to-German
translation in order to discover more generic ways
for learning discriminative reordering rules. One
possible explanation of the bad results is a high
sparseness of automatically extracted rules that does
not allow for sufficient generalization of reordering
instances.
In the future, we plan (1) to perform deeper anal-
ysis of the dissimilarity between English-to-Dutch
and English-to-German translations from SMT
perspective, and (2) to investigate linguistically-
motivated ideas to extend our model such that we
can bring about some improvement to English-to-
German translation.
7 Acknowledgements
Both authors are supported by a VIDI grant (nr.
639.022.604) from The Netherlands Organization
for Scientific Research (NWO).
References
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause re-
structuring for statistical machine translation. In Pro-
ceedings of ACL?05, pages 531?540.
M. R. Costa-jussa` and J. A. R. Fonollosa. 2006.
Statistical machine reordering. In Proceedings of
HLT/EMNLP?06, pages 70?76.
M. Galley and Ch. D. Manning. 2008. A simple and ef-
fective hierarchical phrase reordering model. In Pro-
ceedings of EMNLP?08, pages 848?856.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thaye. 2006. Scalable inference and
training of context-rich syntactic translation models.
In Proc. of COLING/ACL?06, pages 961?968.
D. Genzel. 2010. Aumotatically learning source-side re-
ordering rules for large scale machine translation. In
Proc. of COLING?10, pages 376?384, Beijing, China.
M. Khalilov and K. Sima?an. 2010. A discriminative
syntactic model for source permutation via tree trans-
duction. In Proc. of the Fourth Workshop on Syn-
tax and Structure in Statistical Translation (SSST-4) at
COLING?10, pages 92?100, Beijing (China), August.
M. Khalilov. 2009. New statistical and syntactic mod-
els for machine translation. Ph.D. thesis, Universitat
Polite`cnica de Catalunya, October.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of the 41st Annual Meeting of
the ACL?03, pages 423?430.
Ph. Koehn, F. Och, and D. Marcu. 2003. Statistical
phrase-based machine translation. In Proceedings of
the HLT-NAACL 2003, pages 48?54.
Ph. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
418
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open-source toolkit for
statistical machine translation. In Proceedings of ACL
2007, pages 177?180.
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proceedings of ACL?02, pages 295?
302.
F. Och. 1999. An efficient method for determining bilin-
gual word classes. In Proceedings of ACL 1999, pages
71?76.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proceedings of ACL?03, pages
160?167.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL?02, pages 311?
318.
A. PVS. 2010. A data mining approach to learn reorder
rules for SMT. In Proceedings of NAACL/HLT?10,
pages 52?57.
A. Stolcke. 2002. SRILM: an extensible language mod-
eling toolkit. In Proceedings of SLP?02, pages 901?
904.
C. Tillman. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL?04, pages 101?104.
R. Tromble and J. Eisner. 2009. Learning linear order-
ing problems for better translation. In Proceedings of
EMNLP?09, pages 1007?1016.
K. Visweswariah, J. Navratil, J. Sorensen, V. Chenthama-
rakshan, and N. Kambhatla. 2010. Syntax based
reordering with automatically derived rules for im-
proved statistical machine translation. In Proc. of
COLING?10, pages 1119?1127, Beijing, China.
C. Wang, M. Collins, and Ph. Koehn. 2007. Chinese
syntactic reordering for statistical machine translation.
In Proceedings of EMNLP-CoNLL?07, pages 737?745.
F. Xia and M. McCord. 2004. Improving a statistical MT
system with automatically learned rewrite patterns. In
Proceedings of COLING?04, pages 508?514.
D. Xiong, Q. Liu, and S. Lin. 2006. Maximum entropy
based phrase reordering model for statistical machine
translation. In Proceedings of ACL?06, pages 521?
528.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proceedings of ACL?01,
pages 523?530.
R. Zens, F. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In Proceedings of KI: Ad-
vances in Artificial Intelligence, pages 18?32.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In Pro-
ceedings of NAACL?06, pages 138?141.
419
Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 19?28,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Hierarchical Alignment Decomposition Labels for Hiero Grammar Rules
Gideon Maillette de Buy Wenniger
Institute for Logic,
Language and Computation
University of Amsterdam
Science Park 904, 1098 XH Amsterdam
The Netherlands
gemdbw AT gmail.com
Khalil Sima?an
Institute for Logic,
Language and Computation
University of Amsterdam
Science Park 904, 1098 XH Amsterdam
The Netherlands
k.simaan AT uva.nl
Abstract
Selecting a set of nonterminals for the syn-
chronous CFGs underlying the hierarchical
phrase-based models is usually done on the
basis of a monolingual resource (like a syntac-
tic parser). However, a standard bilingual re-
source like word alignments is itself rich with
reordering patterns that, if clustered some-
how, might provide labels of different (pos-
sibly complementary) nature to monolingual
labels. In this paper we explore a first ver-
sion of this idea based on a hierarchical de-
composition of word alignments into recursive
tree representations. We identify five clus-
ters of alignment patterns in which the chil-
dren of a node in a decomposition tree are
found and employ these five as nonterminal la-
bels for the Hiero productions. Although this
is our first non-optimized instantiation of the
idea, our experiments show competitive per-
formance with the Hiero baseline, exemplify-
ing certain merits of this novel approach.
1 Introduction
The Hiero model (Chiang, 2007; Chiang, 2005)
formulates phrase-based translation in terms of a
synchronous context-free grammar (SCFG) limited
to the inversion transduction grammar (ITG) (Wu,
1997) family. While the original Hiero approach
works with a single nonterminal label (X) (besides
the start nonterminal S ), more recent work is dedi-
cated to devising methods for extracting more elab-
orate labels for the phrase-pairs and their abstrac-
tions into SCFG productions, e.g., (Zollmann and
Venugopal, 2006; Li et al, 2012; Almaghout et al,
2011). All labeling approaches exploit monolin-
gual parsers of some kind, e.g., syntactic, seman-
tic or sense-oriented. The rationale behind mono-
lingual labeling is often to make the probability dis-
tributions over alternative synchronous derivations
of the Hiero model more sensitive to linguistically
justified monolingual phrase context. For example,
syntactic target-language labels in many approaches
are aimed at improved target language modeling
(fluency, cf. Hassan et al (2007); Zollmann and
Venugopal (2006)), whereas source-language labels
provide suitable context for reordering (see Mylon-
akis and Sima?an (2011)). It is usually believed
that the monolingual labels tend to stand for clus-
ters of phrase pairs that are expected to be inter-
substitutable, either syntactically or semantically
(see Marton et al (2012) for an illuminating discus-
sion).
While we believe that monolingual labeling
strategies are sound, in this paper we explore the
complementary idea that the nonterminal labels
could also signify bilingual properties of the phrase
pair, particularly its characteristic word alignment
patterns. Intuitively, an SCFG with nonterminal la-
bels standing for alignment patterns should put more
preference on synchronous derivations that mimic
the word alignment patterns found in the training
corpus, and thus, possibly allow for better reorder-
ing. It is important to stress the fact that these word
alignment patterns are complementary to the mono-
lingual linguistic patterns and it is conceivable that
the two can be combined effectively, but this remains
beyond the scope of this article.
The question addressed in this paper is how to se-
lect word alignment patterns and cluster them into
bilingual nonterminal labels? In this paper we ex-
plore a first instantiation of this idea starting out
from the following simplifying assumptions:
19
? The labels come from the word alignments
only,
? The labels are coarse-grained, pre-defined clus-
ters and not optimized for performance,
? The labels extend the binary set of ITG oper-
ators (monotone and inverted) into five such
labels in order to cover non-binarizable align-
ment patterns.
Our labels are based on our own tree decomposi-
tions of word alignments (Sima?an and Maillette de
Buy Wenniger, 2011), akin to Normalized Decom-
position Trees (NDTs) (Zhang et al, 2008). In this
first attempt we explore a set of five nonterminal la-
bels that characterize alignment patterns found di-
rectly under nodes in the NDT projected for every
word alignment in the parallel corpus during train-
ing. There is a range of work that exploits the mono-
tone and inverted orientations of binary ITG within
hierarchical phrase-based models, either as feature
functions of lexicalized Hiero productions (Chiang,
2007; Zollmann and Venugopal, 2006), or as labels
on non-lexicalized ITG productions, e.g., (Mylon-
akis and Sima?an, 2011). As far as we are aware,
this is the first attempt at exploring a larger set of
such word alignment derived labels in hierarchical
SMT. Therefore, we expect that there are many vari-
ants that could improve substantially on our strong
set of assumptions.
2 Hierarchical SMT models
Hierarchical SMT usually works with weighted in-
stantiations of Synchronous Context-Free Gram-
mars (SCFGs) (Aho and Ullman, 1969). SCFGs
are defined over a finite set of nonterminals (start
included), a finite set of terminals and a finite set
of synchronous productions. A synchronous pro-
duction in an SCFG consists of two context-free
productions (source and target) containing the same
number of nonterminals on the right-hand side, with
a bijective (1-to-1 and onto) function between the
source and target nonterminals. Like the standard
Hiero model (Chiang, 2007), we constrain our work
to SCFGs which involve at most two nonterminals
in every lexicalized production.
Given an SCFG G, a source sentence s is trans-
lated into a target sentence t by synchronous deriva-
tions d, each is a finite sequence of well-formed
substitutions of synchronous productions from G,
see (Chiang, 2006). Standardly, for complexity rea-
sons, most models used make the assumption that
the probability P(t | s) can be optimized through as
single best derivation as follows:
arg max
t
P(t | s) = arg max
t
?
d?G
P(t,d | s) (1)
? arg max
d?G
P(t,d | s) (2)
This approximation can be notoriously problematic
for labelled Hiero models because the labels tend
to lead to many more derivations than in the orig-
inal model, thereby aggravating the effects of this
assumption. This problem is relevant for our work
and approaches to deal with it are Minimum Bayes-
Risk decoding (Kumar and Byrne, 2004; Tromble et
al., 2008), Variational Decoding (Li et al, 2009) and
soft labeling (Venugopal et al, 2009; Marton et al,
2012; Chiang, 2010).
Given a derivation d, most existing phrase-
based models approximate the derivation probabil-
ity through a linear interpolation of a finite set of
feature functions (?(d)) of the derivation d, mostly
working with local feature functions ?i of individ-
ual productions, the target side yield string t of d
(target language model features) and other heuristic
features discussed in the experimental section:
arg max
d?G
P(t,d | s) ? arg max
d?G
|?(d)|?
i=1
?i ? ?i (3)
Where ?i is the weight of feature ?i optimized over
a held-out parallel corpus by some direct error-
minimization procedure like MERT (Och, 2003).
3 Baseline: Hiero Grammars (single label)
Hiero Grammars (Chiang, 2005; Chiang, 2007) are
a particular form of SCFGs that generalize phrase-
based translation models to hierarchical phrase-
based Translation models. They allow only up to
two (pairs of) nonterminals on the right-hand-side of
rules. Hierarchical rules are formed from fully lex-
icalized base rules (i.e. phrase pairs) by replacing a
sub-span of the phrase pair that corresponds itself to
a valid phrase pair with variable X called ?gap?. Two
20
gaps may be maximally introduced in this way1, la-
beled as X1 and X2 respectively for distinction. The
types of permissible Hiero rules are:
X ? ??, ?? (4a)
X ? ?? X1 ?, ? X1 ?? (4b)
X ? ?? X1 ? X2 ? , ? X1 ? X2 ? ? (4c)
X ? ?? X1 ? X2 ? , ? X2 ? X1 ? ? (4d)
Here ?, ?, ?, ?, ?, ? are terminal sequences, pos-
sibly empty. Equation 4a corresponds to a normal
phrase pair, 4b to a rule with one gap and 4c and 4d
to the monotone- and inverting rules respectively.
An important extra constraint used in the original
Hiero model is that rules must have at least one pair
of aligned words, so that translation decisions are al-
ways based on some lexical evidence. Furthermore
the sum of terminals and nonterminals on the source
side may not be greater than five, and nonterminals
are not allowed to be adjacent on the source side.
4 Alignment Labeled Grammars
Labeling the Hiero grammar productions makes
rules with gaps more restricted about what broad
categories of rules are allowed to substitute for the
gaps. In the best case this prevents overgeneraliza-
tion, and makes the translation distributions more
accurate. In the worst case, however, it can also lead
to too restrictive rules, as well as sparse translation
distributions. Despite these inherent risks, a number
of approaches based on syntactically inspired labels
has succeeded to improve the state of the art by
using monolingual labels, e.g., (Zollmann and
Venugopal, 2006; Zollmann, 2011; Almaghout et
al., 2011; Chiang, 2010; Li et al, 2012).
Unlabeled Hiero derivations can be seen as recur-
sive compositions of phrase pairs. A single transla-
tion may be generated by different derivations (see
equation 1), each standing for a choice of com-
position rules over a choice of a segmentation of
the source-target sentence pair into a bag of phrase
pairs. However, a synchronous derivation also in-
duces an alignment between the different segments
1The motivation for this restriction to two gaps is mainly a
practical computational one, as it can be shown that translation
complexity grows exponentially with the number of gaps.
that it composes together. Our goal here is to la-
bel the Hiero rules in order to exploit aspects of the
alignment that a synchronous derivation induces.
We exploit the idea that phrase pairs can be ef-
ficiently grouped into maximally decomposed trees
(normalized decomposition trees ? NDTs) (Zhang
et al, 2008). In an NDT every phrase pair is re-
cursively decomposed at every level into the mini-
mum number of its phrase constituents, so that the
resulting structure is maximal in that it contains the
largest number of nodes. In Figure 1 left we show
an example alignment and in Figure 1 right its as-
sociated NDT. The NDT shows pairs of source and
target spans of (sub-) phrase pairs, governed at dif-
ferent levels of the tree by their parent node. In
our example the root node splits into three phrase
pairs, but these three phrase pairs together do not
manage to cover the entire phrase pair of the par-
ent because of the discontinuous translation struc-
ture ?owe, sind ... schuldig?. Consequently, a par-
tially lexicalized structure with three children corre-
sponding to phrase pairs and lexical items covering
the words left by these phrase pairs is required.
During grammar extraction we determine an
Alignment Label for every left-hand-side and gap of
every rule we extract. This is done by looking at the
NDT that decomposes their corresponding phrase
pairs, and determining the complexity of the rela-
tion with their direct children in this tree. Complex-
ity cases are ordered by preference, where the more
simple label corresponding to the choice of maximal
decomposition is preferred. We distinguish the fol-
lowing five cases, ordered by increasing complexity:
1. Monotonic: If the alignment can be split into
two monotonically ordered parts.
2. Inverted: If the alignment can be split into two
inverted parts.
3. Permutation: If the alignment can be factored
as a permutation of more than 3 parts.2
4. Complex: If the alignment cannot be factored
as a permutation of parts, but the phrase does
contain at least one smaller phrase pair (i.e., it
is composite).
5. Atomic: If the alignment does not allow the ex-
istence of smaller (child) phrase pairs.
2Permutations of just 3 parts never occur in a NDT, as they
can always be further decomposed as a sequence of two binary
nodes.
21
1
we
2
owe
3
this
4
to
5
our
6
citizens
das
1
sind
2
wir
3
unsern
4
burgern
5
schuldig
6
([1, 6], [1, 6])
([5, 6], [4, 5])
([6, 6], [5, 5])([5, 5], [4, 4])
([3, 3], [1, 1])([1, 1], [3, 3])
Figure 1: Example of complex word alignment, taken from Europarl data English-German (left) and its associated
Normalized Decomposition Tree (Zhang et al, 2008) (right).
We show examples of each of these cases in Figure
2. Furthermore, in Figure 3 we show an example
of an alignment labeled Hiero rule based on one of
these alignment examples.
Our kind of labels has a completely different fla-
vor from monolingual labels in that they cannot be
seen as identifying linguistically meaningful clus-
ters of phrase pairs. These labels are mere latent
bilingual clusters and the translation model must
marginalize over them (equation 1) or use Minimum
Bayes-Risk decoding.
4.1 Features : Relations over labels
In this section we describe the features we use in
our experiments. To be unambiguous we first need
to introduce some terminology. Let r be a transla-
tion rule. We use p? to denote probabilities estimated
using simple relative frequency estimation from the
word aligned sentence pairs of the training corpus.
Then src(r) is the source side of the rule, includ-
ing the source side of the left-hand-side label. Simi-
larly tgt(r) is the target side of the rule, including the
target side of the left-hand-side label. Furthermore
un(src(r)) is the source side without any nontermi-
nal labels, and analogous for un(tgt(r)).
4.1.1 Basic Features
We use the following phrase probability features:
? p?(tgt(r)|src(r)): Phrase probability target side
given source side
? p?(src(r)|tgt(r)): Phrase probability source side
given target side
We reinforce those by the following phrase prob-
ability smoothing features:
? p?(tgt(r)|un(src(r)))
? p?(un(src(r))|tgt(r))
? p?(un(tgt(r))|src(r))
? p?(src(r)|un(tgt(r)))
? p?(un(tgt(r))|un(src(r)))
? p?(un(src(r))|un(tgt(r)))
We also add the following features:
? p?w(tgt(r)|src(r)), p?w(src(r)|tgt(r)): Lexical
weights based on terminal symbols as for
phrase-based and hierarchical phrase-based
MT.
? p?(r|lhs(r)) : Generative probability of a rule
given its left-hand-side label
We use the following set of basic binary features,
with 1 values by default, and a value exp(1) if the
corresponding condition holds:
? ?Glue(r): exp(1) if rule is a glue rule
? ?lex(r): exp(1) if rule has only terminals on
right-hand side
? ?abs(r): exp(1) if rule has only nonterminals on
right-hand side
? ?st w tt(r): exp(1) if rule has terminals on the
source side but not on the target side
? ?tt w st(r): exp(1) if rule has terminals on the
target side but not on the source side
? ?mono(r): exp(1) if rule has no inverted pair of
nonterminals
Furthermore we use the :
? ?ra(r): Phrase penalty, exp(1) for all rules.
? exp(?wp(r)): Word penalty, exponent of the
number of terminals on the target side
? ?rare(r): exp( 1#(?r??C ?rr? ) ) : Rarity penalty, with
#(
?
r??C ?rr?) being the count of rule r in the cor-
pus.
4.1.2 Binary Reordering Features
Besides the basic features we want to use extra
sets of binary features that are specially designed
to directly learn the desirability of certain broad
classes of reordering patterns, beyond the way this
is already implicitly learned for particular lexical-
ized rules by the introduction of reordering labels.3
These features can be seen as generalizations of the
most simple feature that penalizes/rewards mono-
3We did some initial experiments with such features in
Joshua, but haven?t managed yet to get them working in Moses
with MBR. Since these experiments are inconclusive without
MBR we leave them out here.
22
this is an important matter
das ist ein wichtige angelegenheit
1
1
2
2
Monotone
we all agree on this
das sehen wir alle
1
1
2
2
Inversion
i want to stress two points
auf zwei punkte mo?chte ich hinweisen
1
1
2
2
3
3
4
4
Permutation
we owe this to our citizens
das sind wir unsern burgern schuldig
1
1
2
2
3
3
Complex
it would be possible
kann mann
1
1
Atomic
Figure 2: Different types of Alignment Labels
tone order ?mono(r) from our basic feature set. The
new features we want to introduce ?fire? for a spe-
cific combination of reordering labels on the left
hand side and one or both gaps, plus optionally the
information whether the rule itself invert its gaps and
whether or not it is abstract.
5 Experiments
We evaluate our method on one language pair using
German as source and English as target. The data is
derived from parliament proceedings sourced from
the Europarl corpus (Koehn, 2005), with WMT-07
development and test data. We used a maximum
sentence length of 40 for filtering. We employ ei-
ther 200K or (approximately) 1000K sentence pairs
for training, 1K for development and 2K for test-
ing (single reference per source sentence). Both the
baseline and our method decode with a 3-gram lan-
guage model smoothed with modified Knesser-Ney
discounting (Chen and Goodman, 1998), trained on
the target side of the full original training set (ap-
proximately 1000K sentences).
We compare against state-of-the-art hierarchi-
cal translation (Chiang, 2005) baselines, based on
the Joshua (Ganitkevitch et al, 2012) and Moses
(Hoang et al, 2007) translation systems with default
decoding settings. We use our own grammar extrac-
we owe this to our citizens
das sind wir unsern burgern schuldig
X Complex
X Atomic1
X Atomic1
X Monotone2
X Monotone2
X Complex
Figure 3: Example of a labeled Hiero rule
X Complex? ?we owe X Atomic1 to X Monotone2 ,
X Atomic1 sind wir X Monotone2 schuldig ?
extracted from the Complex example in Figure 2 by re-
placing the phrase pairs ?this, das? and ?our citizens , un-
sern burgern? with (labeled) variables.
tor for the generation of all grammars, including the
baseline Hiero grammars. This enables us to use the
same features (as far as applicable given the gram-
mar formalism) and assure true comparability of the
grammars under comparison.
5.1 Training and Decoding Details
In this section we discuss the choices and settings
we used in our experiments. Our initial experiments
4We later discovered we needed to add the flag ??return-
best-dev? in Moses to actually get the weights from the best
development run, our initial experiments had not used this. This
explains the somewhat unfortunate drop in performance in our
Analysis Experiments.
23
Decoding
Type
System
Name 200K
Lattice
MBR
Hiero 26.44
Hiero-RL 26.72
Viterbi Hiero 26.23Hiero-RL-PPL 26.16
Table 1: Initial Results. Lowercase BLEU results for
German-English trained on 200K sentence pairs.4
Top rows display results for our experiments using Moses
(Hoang et al, 2007) with Lattice Minimum Bayes-Risk
Decoding5 (Tromble et al, 2008) in combination with
Batch Mira (Cherry and Foster, 2012) for tuning. Below
are results for experiments with Joshua (Ganitkevitch et
al., 2012) using Viterbi decoding (i.e. no MBR) and PRO
(Hopkins and May, 2011) for tuning.
were done on Joshua (Ganitkevitch et al, 2012),
using the Viterbi best derivation. The second set
of experiments was done on Moses (Hoang et al,
2007) using Lattice Minimum Bayes-Risk Decod-
ing5 (Tromble et al, 2008) to sum over derivations.
5.1.1 General Settings
To train our system we use the following settings.
We use the standard Hiero grammar extraction
constraints (Chiang, 2007) but for our reordering
labeled grammars we use them with some modifi-
cations. In particular, while for basic Hiero only
phrase pairs with source spans up to 10 are allowed,
and abstract rules are forbidden, we allow extraction
of fully abstract rules, without length constraints.
Furthermore we allow their application without
length constraints during decoding. Following
common practice, we use simple relative frequency
estimation to estimate the phrase probabilities,
lexical probabilities and generative rule probability
respectively.6
5After submission we were told by Moses support that in
fact neither normal Minimum Bayes-Risk (MBR) nor Lattice
MBR are operational in Moses Chart.
6Personal correspondence with Andreas Zollmann further
reinforced the authors appreciation of the importance of this
feature introduced in (Zollmann and Venugopal, 2006; Zoll-
mann, 2011). Strangely enough this feature seems to be un-
available in the standard Moses (Hoang et al, 2007) and Joshua
(Ganitkevitch et al, 2012) grammar extractors, that also imple-
ment SAMT grammar extraction
5.1.2 Specific choices and settings Joshua
Viterbi experiments
Based on experiments reported in (Mylonakis and
Sima?an, 2011; Mylonakis, 2012) we opted to not
label the (fully lexicalized) phrase pairs, but instead
label them with a generic PhrasePair label and use
a set of switch rules from all other labels to the
PhrasePair label to enable transition between Hiero
rules and phrase pairs.
We train our systems using PRO (Hopkins and
May, 2011) implemented in Joshua by Ganitkevitch
et al (2012). We use the standard tuning, where all
features are treated as dense features.We allow up to
30 tuning iterations. We further follow the PRO set-
tings introduced in (Ganitkevitch et al, 2012) but
use 0.5 for the coefficient ? that interpolates the
weights learned at the current with those from the
previous iteration. We use the final learned weights
for decoding with the log-linear model and report
Lowercase BLEU scores for the tuned test set.
5.1.3 Specific choices and settings Moses
Lattice MBR experiments
As mentioned before we use Moses (Hoang et
al., 2007) for our second experiment, in combina-
tion with Lattice Minimum Bayes-Risk Decoding5
(Tromble et al, 2008). Furthermore we use Batch
Mira (Cherry and Foster, 2012) for tuning with max-
imum 10 tuning iterations of the 200K training set,
and 30 for the 1000K training set.7
For our Moses experiments we mainly worked
with a uniform labeling policy, labeling phrase pairs
in the same way with alignment labels as normal
rules. This is motivated by the fact that since we are
using Minimum Bayes-Risk decoding, the risks of
sparsity from labeling are much reduced. And label-
ing everything does have the advantage that reorder-
7We are mostly interested in the relative performance of our
system in comparison to the baseline for the same settings. Nev-
ertheless, it might be that the labeled systems, which have more
smoothing features, are relatively suffering more from too lit-
tle tuning iterations than the baseline which does not have these
extra features and thus may be easier to tune. This was one of
the reasons to increase the number of tuning iterations from 10
to 30 in our later experiments on 1000K. Usage of Minimum
Bayes-Risk decoding or not is crucial as we have explained be-
fore in section 1. The main reason we opted for Batch Mira over
PRO is that it is more commonly used in Moses systems, and in
any case at least superior to MERT (Och, 2003) in most cases.
24
ing information can be fully propagated in deriva-
tions starting from the lowest (phrase) level. We also
ran experiments with the generic phrase pair label-
ing, since there were reasons to believe this could
decrease sparsity and potentially lead to better re-
sults.8
5.2 Initial Results
We report Lowercase BLEU scores for experi-
ments with and without Lattice Minimum Bayes-
Risk (MBR) decoding (Tromble et al, 2008). Ta-
ble 1 bottom shows the results of our first experi-
ments with Joshua, using the Viterbi derivation and
no MBR decoding to sum over derivations. We
display scores for the Hiero baseline (Hiero) and
the (partially) alignment labeled system (Hiero-AL-
PPL) which uses alignment labels for Hiero rules
and PhrasePair to label all phrase pairs. Scores are
around 26.25 BLEU for both systems, with only
marginal differences. In summary our labeled sys-
tems are at best comparable to the Hiero baseline.
Table 1 top shows the results of our second ex-
periments with Moses and Lattice MBR5. Here
our (fully) alignment labeled system (Hiero-AL)
achieves a score of 26.72 BLEU, in comparison to
26.44 BLEU for the Hiero baseline (Hiero). A small
improvement of 0.28 BLEU point.
5.3 Advanced experiments
We now report Lowercase BLEU scores for more
detailed analysis experiments with and without Lat-
tice Minimum Bayes-Risk5 (MBR) decoding, where
we varied other training and decoding parameters in
the Moses environment. Particularly, in this set of
experiments we choose the best tuning parameter
settings over 30 Batch Mira iterations (as opposed
to the weights returned by default ? used in the pre-
vious experiments). We also explore varieties in tun-
ing with a decoder that works with Viterbi/MBR,
and final testing with Viterbi/MBR.
In Table 2, the top rows show the results of our ex-
periments using MBR decoding. We display scores
8We discovered that the Moses chart decoder does not allow
fully abstract unary rules in the current implementation, which
makes direct usage of unary (switch) rules not possible. Switch
rules and other unaries can still be emulated though, by adapt-
ing the grammar, using multiple copies of rules with different
labels. This blows up the grammar a bit, but at least works in
practice.
Decoding
Type
System
Name 200K 1000K
Lattice
MBR
Hiero 27.19 28.39
Hiero-AL 26.61 28.32
Hiero-AL-PPL 26.89 28.41
Viterbi Hiero 26.80 28.57Hiero-AL 28.36
Table 2: Analysis Results. Lowercase BLEU results for
German-English trained on 200K and 1000K sentence
pairs using Moses (Hoang et al, 2007) in combination
with Batch Mira (Cherry and Foster, 2012) for tuning.
Top rows display results for our experiments with Lattice
Minimum Bayes-Risk Decoding5 (Tromble et al, 2008).
Below are results for experiments using Viterbi decoding
(i.e. no MBR) for tuning. Results on 200K were run with
10 tuning iterations, results on 1000K with 30 tuning it-
erations.
for the Hiero baseline (Hiero) and the fully/partially
alignment labeled systems Hiero-AL and Hiero-AL-
PPL. In the preceding set of experiments MBR de-
coding clearly showed improved performance over
Viterbi, particularly for our labelled system.
On the small training set of 200K we observe
that the Hiero baseline achieves 27.19 BLEU and
thus beats the labeled systems Hiero-AL with 26.61
BLEU and 26.89 BLEU by a good margin. On the
bigger dataset of 1000K and with more tuning iter-
ations (3), all systems perform better. When using
Lattice MBR Hiero achieving 28.39 BLEU, Hiero-
AL 28.32 BLEU and finally Hiero-AL-PPL achieves
28.41. These are insignificant differences in perfor-
mance between the labelled and unlabeled systems.
Table 1 bottom also shows the results of our
second set of experiments with Viterbi decoding.
Here, the baseline Hiero system for 200K training
set achieves a score of 26.80 BLEU on the small
training set. We also conducted another set of
experiments on the larger training set of 1000K, this
time with Viterbi decoding. The Hiero baseline with
Viterbi scores 28.57 BLEU while Hiero-AL scores
28.36 BLEU under the same conditions.
It is puzzling that Hiero Viterbi (for 1000k) per-
forms better than the same system with MBR decod-
ing systems. But after submission we were told by
Moses support that neither normal MBR nor Lattice
MBR are operational in Moses Chart. This means
that in fact the effect of MBR on our labels remains
still undecided, and more work is still needed in this
direction. The small decrease in performance for the
25
labelled system relative to Hiero (in Viterbi) is possi-
bly the result of the labelled system being more brit-
tle and harder to tune than the Hiero system. This
hypothesis needs further exploration.
While a whole set of experimental questions re-
mains open, we think that based on this preliminary
but nevertheless considerable set of experiments, it
seems that our labels do not always improve perfor-
mance compared with the Hiero baseline. It is possi-
ble that these labels, under a more advanced imple-
mentation via soft constraints (as opposed to hard la-
beling), could provide the empirical evidence to our
theoretical choices. A further concern regarding the
labels is that our current choice (5 labels) is heuristic
and not optimized for the training data. It remains to
be seen in the future if proper learning of these labels
as latent variables optimized for the training data or
the use of soft constraints can shed more light on the
use of alignment labels in hierarchical SMT.
5.4 Analysis
While we did not have time to do a deep compara-
tive analysis of the properties of the grammars, a few
things can be said based on the results. First of all
we have seen that alignment labels do not always im-
prove over the Hiero baseline. In earlier experiments
we observed some improvement when the labelled
grammar was used in combination with Minimum
Bayes-Risk Decoding but not without it. In later ex-
periments with different tuning settings (Mira), the
improvements evaporated and in fact, the Viterbi Hi-
ero baseline turned out, surprisingly, the best of all
systems.
Our use of MBR is theoretically justified by the
importance of aggregating over the derivations of the
output translations when labeling Hiero variables:
statistically speaking, if the labels split the occur-
rences of the phrase pairs, they will lead to multiple
derivations per Hiero derivation with fractions of the
scores. This is in line with earlier work on the ef-
fect of spurious ambiguity, e.g. Variational Decod-
ing (Li et al, 2009). Yet, in the case of our model,
there is also a conceptual explanation for the need to
aggregate over different derivations of the same sen-
tence pair. The decomposition of a word alignment
into hierarchical decomposition trees has a interest-
ing property: the simpler (less reordering) a word
alignment, the more (binary) decomposition trees ?
and in our model derivations ? it will have. Hence,
aggregating over the derivations is a way to gather
evidence for the complexity of alignment patterns
that our model can fit in between a given source-
target sentence pair. However, in the current exper-
imental setting, where final tuning with Mira is cru-
cial, and where the use of MBR within Moses is still
not standard, we cannot reap full benefit of our the-
oretical analysis concerning the fit of MBR for our
models? alignment labels.
6 Conclusion
We presented a novel method for labeling Hiero
variables with nonterminals derived from the hierar-
chical patterns found in recursive decompositions of
word alignments into tree representations. Our ex-
periments based on a first instantiation of this idea
with a fixed set of labels, not optimized to the train-
ing data, show promising performance. Our early
experiments suggested that these labels have merit,
whereas later experiments with more varied training
and decoder settings showed these results to be un-
stable.
Empirical results aside, our approach opens up a
whole new line of research to improve the state of
the art of hierarchical SMT by learning these la-
tent alignment labels directly from standard word
alignments without special use of syntactic or other
parsers. The fact that such labels are in principle
complementary with monolingual information is an
exciting perspective which we might explore in fu-
ture work.
Acknowledgements
This work is supported by The Netherlands Organi-
zation for Scientific Research (NWO) under grant nr.
612.066.929. This work was sponsored by the BIG
Grid project for the use of the computing and storage
facilities, with financial support from the Nether-
lands Organization of Scientific Research (NWO)
under grant BG-087-12. The authors would like to
thank the people from the Joshua team at John Hop-
kins University, in particular Yuan Cao, Jonathan
Weese, Matt Post and Juri Ganitkevitch, for their
helpful replies to questions regarding Joshua and its
PRO and Packing implementations.
26
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax
directed translations and the pushdown assembler. J.
Comput. Syst. Sci., 3(1):37?56.
Hala Almaghout, Jie Jiang, and Andy Way. 2011. Ccg
contextual labels in hierarchical phrase-based smt. In
Proceedings of the 15th Annual Conference of the Eu-
ropean Association for Machine Translation (EAMT-
2011), May.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard University.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In HLT-
NAACL, pages 427?436.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the ACL, pages 263?270,
June.
David Chiang. 2006. An introduction to synchronous
grammars.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post,
and Chris Callison-Burch. 2012. Joshua 4.0: Pack-
ing, pro, and paraphrases. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
pages 283?291, Montre?al, Canada, June. Association
for Computational Linguistics.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proceedings of ACL 2007, page 288295.
Hieu Hoang, Alexandra Birch, Chris Callison-burch,
Richard Zens, Rwth Aachen, Alexandra Constantin,
Marcello Federico, Nicola Bertoldi, Chris Dyer,
Brooke Cowan, Wade Shen, Christine Moran, and
Ondrej Bojar. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, pages 177?180.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1352?1362.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In HLT-NAACL, page 16917.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2 - Volume 2, pages 593?
601.
Junhui Li, Zhaopeng Tu, Guodong Zhou, and Josef van
Genabith. 2012. Using syntactic head information in
hierarchical phrase-based translation. In Proceedings
of the Seventh Workshop on Statistical Machine Trans-
lation, pages 232?242.
Yuval Marton, David Chiang, and Philip Resnik. 2012.
Soft syntactic constraints for arabic?english hierar-
chical phrase-based translation. Machine Translation,
26(1-2):137?157.
Markos Mylonakis and Khalil Sima?an. 2011. Learning
hierarchical translation structure with linguistic anno-
tations. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 642?652.
Markos Mylonakis. 2012. Learning the Latent Struc-
ture of Translation. Ph.D. thesis, University of Ams-
terdam.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, pages 160?167.
Khalil Sima?an and Gideon Maillette de Buy Wenniger.
2011. Hierarchical translation equivalence over word
alignments. Technical Report PP-2011-38, Institute
for Logic, Language and Computation.
Roy W. Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice minimum bayes-risk
decoding for statistical machine translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 620?629.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars: soft-
ening syntactic constraints to improve statistical ma-
chine translation. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 236?244.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
Hao Zhang, Daniel Gildea, and David Chiang. 2008. Ex-
tracting synchronous grammar rules from word-level
alignments in linear time. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics - Volume 1, pages 1081?1088.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
27
NAACL 2006 - Workshop on statistical machine trans-
lation, June.
Andreas Zollmann. 2011. Learning Multiple-
Nonterminal Synchronous Grammars for Statistical
Machine Translation. Ph.D. thesis, Carnegie Mellon
University.
28
Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 58?67,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
A Formal Characterization of Parsing Word Alignments by Synchronous
Grammars with Empirical Evidence to the ITG Hypothesis
Gideon Maillette de Buy Wenniger?
University of Amsterdam
gemdbw@gmail.com
Khalil Sima?an?
University of Amsterdam
k.simaan@uva.nl
Abstract
Deciding whether a synchronous grammar
formalism generates a given word alignment
(the alignment coverage problem) depends on
finding an adequate instance grammar and
then using it to parse the word alignment. But
what does it mean to parse a word align-
ment by a synchronous grammar? This is for-
mally undefined until we define an unambigu-
ous mapping between grammatical deriva-
tions and word-level alignments. This pa-
per proposes an initial, formal characteriza-
tion of alignment coverage as intersecting two
partially ordered sets (graphs) of translation
equivalence units, one derived by a gram-
mar instance and another defined by the word
alignment. As a first sanity check, we report
extensive coverage results for ITG on auto-
matic and manual alignments. Even for the
ITG formalism, our formal characterization
makes explicit many algorithmic choices of-
ten left underspecified in earlier work.
1 Introduction
The training data used by current statistical machine
translation (SMT) models consists of source and
target sentence pairs aligned together at the word
level (word alignments). For the hierarchical and
syntactically-enriched SMT models, e.g., (Chiang,
2007; Zollmann and Venugopal, 2006), this training
data is used for extracting statistically weighted Syn-
chronous Context-Free Grammars (SCFGs). For-
mally speaking, a synchronous grammar defines a
set of (source-target) sentence pairs derived syn-
chronously by the grammar. Contrary to common
? Institute for Logic, Language and Computation.
belief, however, a synchronous grammar (see e.g.,
(Chiang, 2005; Satta and Peserico, 2005)) does not
accept (or parse) word alignments. This is because
a synchronous derivation generates a tree pair with
a bijective binary relation (links) between their non-
terminal nodes. For deciding whether a given word
alignment is generated/accepted by a given syn-
chronous grammar, it is necessary to interpret the
synchronous derivations down to the lexical level.
However, it is formally defined yet how to unam-
biguously interpret the synchronous derivations of
a synchronous grammar as word alignments. One
major difficulty is that synchronous productions, in
their most general form, may contain unaligned ter-
minal sequences. Consider, for instance, the rela-
tively non-complex synchronous production
?X ? ? X(1) ? X(2) ? X(3), X ? ? X(2) ? X(1) ? X(3)?
where superscript (i) stands for aligned instances
of nonterminal X and all Greek symbols stand for
arbitrary non-empty terminals sequences. Given a
word aligned sentence pair it is necessary to bind
the terminal sequence by alignments consistent with
the given word alignment, and then parse the word
alignment with the thus enriched grammar rules.
This is not complex if we assume that each of the
source terminal sequences is contiguously aligned
with a target contiguous sequence, but difficult if we
assume arbitrary alignments, including many-to-one
and non-contiguously aligned chunks.
One important goal of this paper is to propose
a formal characterization of what it means to syn-
chronously parse a word alignment. Our formal
characterization is borrowed from the ?parsing as in-
tersection" paradigm, e.g., (Bar-Hillel et al, 1964;
Lang, 1988; van Noord, 1995; Nederhof and Satta,
58
2004). Conceptually, our characterization makes use
of three algorithms. Firstly, parse the unaligned sen-
tence pair with the synchronous grammar to obtain a
set of synchronous derivations, i.e., trees. Secondly,
interpret a word alignment as generating a set of
synchronous trees representing the recursive trans-
lation equivalence relations of interest1 perceived in
the word alignment. And finally, intersect the sets
of nodes in the two sets of synchronous trees to
check whether the grammar can generate (parts of)
the word alignment. The formal detail of each of
these three steps is provided in sections 3 to 5.
We think that alignment parsing is relevant for
current research because it highlights the differ-
ence between alignments in training data and align-
ments accepted by a synchronous grammar (learned
from data). This is useful for literature on learn-
ing from word aligned parallel corpora (e.g., (Zens
and Ney, 2003; DeNero et al, 2006; Blunsom et al,
2009; Cohn and Blunsom, 2009; Riesa and Marcu,
2010; Mylonakis and Sima?an, 2011; Haghighi et
al., 2009; McCarley et al, 2011)). A theoretical,
formalized characterization of the alignment pars-
ing problem is likely to improve the choices made in
empirical work as well. We exemplify our claims by
providing yet another empirical study of the stability
of the ITG hypothesis. Our study highlights some of
the technical choices left implicit in preceding work
as explained in the next section.
2 First application to the ITG hypothesis
A grammar formalism is a whole set/family of syn-
chronous grammars. For example, ITG (Wu, 1997)
defines a family of inversion-transduction gram-
mars differing among them in the exact set of syn-
chronous productions, terminals and non-terminals.
Given a synchronous grammar formalism and an
input word alignment, a relevant theoretical ques-
tion is whether there exists an instance synchronous
grammar that generates the word alignment exactly.
We will refer to this question as the alignment cover-
age problem. In this paper we propose an approach
to the alignment coverage problem using the three-
step solution proposed above for parsing word align-
1The translation equivalence relations of interest may vary
in kind as we will exemplify later. The known phrase pairs are
merely one possible kind.
ments by arbitrary synchronous grammars.
Most current use of synchronous grammars is
limited to a subclass using a pair of nonterminals,
e.g., (Chiang, 2007; Zollmann and Venugopal, 2006;
Mylonakis and Sima?an, 2011), thereby remain-
ing within the confines of the ITG formalism (Wu,
1997). On the one hand, this is because of computa-
tional complexity reasons. On the other, this choice
relies on existing empirical evidence of what we will
call the ?ITG hypothesis", freely rephrased as fol-
lows: the ITG formalism is sufficient for represent-
ing a major percentage of reorderings in translation
data in general.
Although checking whether a word alignment can
be generated by ITG is far simpler than for arbi-
trary synchronous grammars, there is a striking vari-
ation in the approaches taken in the existing litera-
ture, e.g., (Zens and Ney, 2003; Wellington et al,
2006; S?gaard and Wu, 2009; Carpuat and Wu,
2007; S?gaard and Kuhn, 2009; S?gaard, 2010).
S?gaard and Wu (S?gaard and Wu, 2009) observe
justifiably that the literature studying the ITG align-
ment coverage makes conflicting choices in method
and data, and reports significantly diverging align-
ment coverage scores. We hypothesize here that
the major conflicting choices in method (what to
count and how to parse) are likely due to the ab-
sence of a well-understood, formalized method for
parsing word alignments even under ITG. In this pa-
per we apply our formal approach to the ITG case,
contributing new empirical evidence concerning the
ITG hypothesis.
For our empirical study we exemplify our ap-
proach by detailing an algorithm dedicated to ITG in
Normal-Form (NF-ITG). While our algorithm is in
essence equivalent to existing algorithms for check-
ing binarizability of permutations, e.g.,(Wu, 1997;
Huang et al, 2009), the formal foundations pre-
ceding it concern nailing down the choices made
in parsing arbitrary word alignments, as opposed to
(bijective) permutations. The formalization is our
way to resolve some of the major points of differ-
ences in existing literature.
We report new coverage results for ITG parsing
of manual as well as automatic alignments, showing
the contrast between the two kinds. While the latter
seems built for phrase extraction, trading-off preci-
sion for recall, the former is heavily marked with id-
59
iomatic expressions. Our coverage results make ex-
plicit a relevant dilemma. To hierarchically parse the
current automatic word alignments exactly, we will
need more general synchronous reordering mecha-
nisms than ITG, with increased risk of exponential
parsing algorithms (Wu, 1997; Satta and Peserico,
2005). But if we abandon these word alignments,
we will face the exponential problem of learning re-
ordering arbitrary permutations, cf. (Tromble and
Eisner, 2009). Our results also exhibit the impor-
tance of explicitly defining the units of translation
equivalence when studying (ITG) coverage of word
alignments. The more complex the choice of trans-
lation equivalence relations, the more difficult it is to
parse the word alignments.
3 Translation equivalence in MT
In (Koehn et al, 2003), a translation equivalence
unit (TEU) is a phrase pair: a pair of contiguous
substrings of the source and target sentences such
that the words on the one side align only with words
on the other side (formal definitions next). The hier-
archical phrase pairs (Chiang, 2005; Chiang, 2007)
are extracted by replacing one or more sub-phrase
pairs, that are contained within a phrase pair, by
pairs of linked variables. This defines a subsumption
relation between hierarchical phrase pairs (Zhang et
al., 2008). Actual systems, e.g., (Koehn et al, 2003;
Chiang, 2007) set an upperbound on length or the
number of variables in the synchronous productions.
For the purposes of our theoretical study, these prac-
tical limitations are irrelevant.
We give two definitions of translation equivalence
for word alignments.2 The first one makes no as-
sumptions about the contiguity of TEUs, while the
second does require them to be contiguous sub-
strings on both sides (i.e., phrase pairs).
As usual, s = s1...sm and t = t1...tn are source and
target sentences respectively. Let s? be the source
word at position ? in s and t? be the target word at
position ? in t. An alignment link a ? a in a word
alignment a is a pair of positions ??, ?? such that 1 ?
2Unaligned words tend to complicate the formalization un-
necessarily. As usual we also require that unaligned words must
first be grouped with aligned words adjacent to them before
translation equivalence is defined for an alignment. This stan-
dard strategy allows us to informally discuss unaligned words
in the following without loss of generality.
? ? m and 1 ? ? ? n. For the sake of brevity, we
will often talk about alignments without explicitly
mentioning the associated source and target words,
knowing that these can be readily obtained from the
pair of positions and the sentence pair ?s, t?. Given
a subset a? ? a we define wordss(a?) = {s? | ?X :
??, X? ? a?} and wordst(a?) = {t? | ?X : ?X, ?? ? a?}.
Now we consider triples (s?, t?, a?) such that
a? ? a, s? = wordss(a?) and t? = wordst(a?). We
define the translation equivalence units (TEUs) in
the set TE(s, t, a) as follows:
Definition 3.1 (s?, t?, a?) ? TE(s, t, a) iff ??, ?? ? a?
? (for all X, if ??, X? ? a then ??, X? ? a?) ? (for
all X, if ?X, ?? ? a then ?X, ?? ? a?)
In other words, if some alignment involving source
position ? or ? is included in a?, then all alignments
in a containing that position are in a? as well. This
definition allows a variety of complex word align-
ments such as the so-called Cross-serial Discontigu-
ous Translation Units and Bonbons (S?gaard and
Wu, 2009).
We also define the subsumption relation (partial
order) <a as follows:
Definition 3.2 A TEU u2 = (s2, t2, a2) subsumes
(<a) a TEU u1 = (s1, t1, a1) iff a1 ? a2. The sub-
sumption order will be represented by u1 <a u2.
Based on the subsumption relation we can par-
tition TE(s, t, a) into two disjoint sets : atomic
TEAtom(s, t, a) and composed TEComp(s, t, a).
Definition 3.3 u1 ? TE(s, t, a) is atomic iff @ u2 ?
TE(s, t, a) such that (u2 <a u1).
Now the set TEAtom(s, t, a) is simply the set
of all atomic translation equivalents, and
the set of composed translation equivalents
TEComp(s, t, a) = (TE(s, t, a) \ TEAtom(s, t, a)).
Based on the general definition of translation
equivalence, we can now give a more restricted
definition that allows only contiguous translation
equivalents (phrase pairs):
Definition 3.4 (s?, t?, a?) constitutes a contiguous
translation equivalent iff:
1. (s?, t?, a?) ? TE(s, t, a) and
60
2. Both s? and t? are contiguous substrings of s
and t? respectively.
This set of translation equivalents is the unlimited
set of phrase pairs known from phrase-based ma-
chine translation (Koehn et al, 2003). The relation
<a as well as the division into atomic and composed
TEUs can straightforwardly be adapted to contigu-
ous translation equivalents.
4 Grammatical translation equivalence
The derivations of a synchronous grammar can be
interpreted as deriving a partially ordered set of
TEUs as well. A finite derivation S ?+ ?s, t, aG?
of an instance grammar G is a finite sequence of
term-rewritings, where at each step of the sequence a
single nonterminal is rewritten using a synchronous
production of G. The set of the finite derivations
of G defines a language, a set of triples ?s, t, aG?
consisting of a source string of terminals s, a target
string of terminals t and an alignment between their
grammatical constituents. Crucially, the alignment
aG is obtained by recursively interpreting the align-
ment relations embedded in the synchronous gram-
mar productions in the derivation for all constituents
and concerns constituent alignments (as opposed to
word alignments).
Grammatical translation equivalents TEG(s, t)
A synchronous derivation S ?+ ?s, t, aG? can be
viewed as a deductive proof that ?s, t, aG? is a gram-
matical translation equivalence unit (grammatical
TEU). Along the way, a derivation also proves other
constituent-level (sub-sentential) units as TEUs.
We define a sub-sentential grammatical TEU of
?s, t, aG? to consist of a triple ?sx, tx, ax?, where sx
and tx are two subsequences3 (of s and t respec-
tively), derived synchronously from the same con-
3A subsequence of a string is a subset of the word-position
pairs that preserves the order but do not necessarily constitute
contiguous substrings.
Figure 2: Alignment with both contiguous and dis-
contiguous TEUs (example from Europarl En-Ne).
stituent X in some non-empty ?tail" of a derivation
S ?+ ?s, t, aG?; importantly, by the workings of G,
the alignment ax ? aG fulfills the requirement that a
word in sx or in tx is linked to another by aG iff it is
also linked that way by ax (i.e., no alignments start
out from terminals in sx or tx and link to terminals
outside them). We will denote with TEG(s, t) the set
of all grammatical TEUs for the sentence pair ?s, t?
derived by G.
Subsumption relation <G(s,t) Besides deriving
TEUs, a derivation also shows how the different
TEUs compose together into larger TEUs according
to the grammar. We are interested in the subsump-
tion relation: one grammatical TEU/constituent (u1)
subsumes another (u2) (written u2 <G(s,t) u1) iff the
latter (u2) is derived within a finite derivation of the
former (u1).4
The set of grammatical TEUs for a finite set of
derivations for a given sentence pair is the union of
the sets defined for the individual derivations. Simi-
larly, the relation between TEU?s for a set of deriva-
tions is defined as the union of the individual rela-
tions.
5 Alignment coverage by intersection
Let a word aligned sentence pair ?s, t, a? be given,
and let us assume that we have a definition of an or-
dered set TE(s, t, a) with partial order <a. We will
say that a grammar formalism covers a iff there ex-
ists an instance grammar G that fulfills two intersec-
tion equations simultaneously:5
(1) TE(s, t, a) ? TEG(s, t) = TE(s, t, a)
(2) <a ? <G(s,t)=<a
In the second equation, the intersection of partial or-
ders is based on the standard view that these are in
essence also sets of ordered pairs. In practice, it
is sufficient to implement an algorithm that shows
4Note that we define this relation exhaustively thereby defin-
ing the set of paths in synchronous trees derived by the grammar
for ?s, t?. Hence, the subsumption relation can be seen to define
a forest of synchronous trees.
5In this work we have restricted this definition to full cover-
age (i.e., subset) version but it is imaginable that other measures
can be based on the cardinality (size) of the intersection in terms
of covered TEUs, in following of measures found in (S?gaard
and Kuhn, 2009; S?gaard and Wu, 2009). We leave this to fu-
ture work.
61
Figure 1: Alignment with only contiguous TEUs (example from LREC En-Fr).
that G derives every TEU in TE(s, t, a), and that
the subsumption relation <a between TEUs in a
must be realized by the derivations of G that de-
rive TE(s, t, a). In effect, this way every TEU that
subsumes other TEUs must be derived recursively,
while the minimal, atomic units (not subsuming any
others) must be derived using the lexical produc-
tions (endowed with internal word alignments) of
NF-ITG. Again, the rationale behind this choice is
that the atomic units constitute fixed translation ex-
pressions (idiomatic TEUs) which cannot be com-
posed from other TEUs, and hence belong in the lex-
icon. We will exhibit coverage algorithms for doing
so for NF-ITG for the two kinds of semantic inter-
pretations of word alignments.
A note on dedicated instances of NF-ITG Given
a translation equivalence definition over word align-
ments TE(s, t, a), the lexical productions for a ded-
icated instance of NF-ITG are defined6 by the set
{X ? u | u ? TEAtom(s, t, a)}. This means that the
lexical productions have atomic TEUs at the right-
hand side including alignments between the words
of the source and target terminals. In the sequel, we
will only talk about dedicated instances of NF-ITG
and hence we will not explicitly repeat this every
time.
Given two grammatical TEUs u1 and u2, an NF-
ITG instance allows their concatenation either in
monotone [] or inverted <> order iff they are ad-
jacent on the source and target sides. This fact
implies that for every composed translation equiv-
alent u ? TE(s, t, a) we can check whether it is
derivable by a dedicated NF-ITG instance by check-
ing whether it recursively decomposes into adjacent
pairs of TEUs down to the atomic TEUs level. Note
that by doing so, we are also implicitly checking
6Unaligned words add one wrinkle in this scheme: infor-
mally, we consider a TEU u formed by attaching unaligned
words to an atomic TEU also as atomic iff u is absolutely needed
to cover the aligned sentence pair.
whether the subsumption order between the TEUs
in TE(s, t, a) is realized by the grammatical deriva-
tion (i.e, <G(s,t)?<a). Formally, an aligned sentence
pair ?s, t, a? is split into a pair of TEUs ?s1, t1, a1?
and ?s2, t2, a2? that can be composed back using
the [] and <> productions. If such a split exists,
the splitting is conducted recursively for each of
?s1, t1, a1? and ?s2, t2, a2? until both are atomic TEUs
in TE(s, t, a). This recursive splitting is the check
of binarizability and an algorithm is described in
(Huang et al, 2009).
6 A simple algorithm for ITG
We exemplify the grammatical coverage for (nor-
mal form) ITG by employing a standard tabular al-
gorithm based on CYK (Younger, 1967). The al-
gorithm works in two phases creating a chart con-
taining TEUs with associated inferences. In the ini-
tialization phase (Algorithm 1), for all source spans
that correspond to translation equivalents and which
have no smaller translation equivalents they contain,
atomic translation equivalents are added as atomic
inferences to the chart. In the second phase, based
on the atomic inferences, the simple rules of NF-
ITG are applied to add inferences for increasingly
larger chart entries. An inference is added (Algo-
rithms 2 and 3) iff a chart entry can be split into two
sub-entries for which inferences already exist, and
furthermore the union of the sets of target positions
for those two entries form a consecutive range.7 The
addMonotoneInference and addInvertedInference in
Algorithm 3 mark the composit inferences by mono-
tone and inverted productions respectively.
7We are not treating unaligned words formally here. For un-
aligned source and target words, we have to generate the differ-
ent inferences corresponding to different groupings with their
neighboring aligned words. Using pre-processing we set aside
the unaligned words, then parse the remaining word alignment
fully. After parsing, by post-processing, we introduce in the
parse table atomic TEUs that include the unaligned words.
62
InitializeChart
Input : ?s, t, a?
Output: Initialized chart for atomic units
for spanLength? 2 to n do
for i? 0 to n ? spanLength + 1 do
j? i + spanLength ? 1
u? {?X,Y? : X ? {i... j}}
if (u ? TEAtom(s, t, a)) then
addAtomicIn f erence(chart[i][ j],u)
end
end
end
Algorithm 1: Algorithm that initializes the Chart
with atomic sub-sentential TEUs. In order to be
atomic, a TEU may not contain smaller TEUs that
consist of a proper subset of the alignments (and
associated words) of the TEU.
ComputeTEUsNFITG
Input : ?s, t, a?
Output: TRUE/FALSE for coverage
InitializeChart(chart)
for spanLength? 2 to n do
for i? 0 to n ? spanLength + 1 do
j? i + spanLength ? 1
if chart[i][ j] ? TE(s, t, a) then
continue
end
for splitPoint ? i + 1 to j do
a? ? (chart[i][k ? 1] ? chart[k][ j])
if (chart[i][k ? 1] ? TE(s, t, a)) ?
(chart[k][ j] ? TE(s, t, a)) ?
(a? ? TE(s, t, a)) then
addT EU(chart, i, j, k, a?)
end
end
end
if (chart[0][n ? 1] , ?) then
return TRUE
else
return FALSE
end
end
Algorithm 2: Algorithm that incrementally builds
composite TEUs using only the rules allowed by
NF-ITG
addTEU
Input :
chart - the chart
i,j,k - the lower, upper and split point indices
a? - the TEU to be added
Output: chart with TEU a? added in the
intended entry
if MaxYt ({Yt : ?Xs,Yt? ? chart[i][k ? 1]})
< MaxYt ({Yt : ?Xs,Yt? ? chart[k][ j]}) then
addMonotoneIn f erence(chart[i][ j], a?)
else
addInvertedIn f erence(chart[i][ j], a?)
end
Algorithm 3: Algorithm that adds a TEU and as-
sociated Inference to the chart
7 Experiments
Data Sets We use manually and automatically
aligned corpora. Manually aligned corpora come
from two datasets. The first (Grac?a et al,
2008) consists of six language pairs: Portuguese?
English, Portuguese?French, Portuguese?Spanish,
English?Spanish, English?French and French?
Spanish. These datasets contain 100 sentence pairs
each and distinguish Sure and Possible alignments.
Following (S?gaard and Kuhn, 2009), we treat these
two equally. The second manually aligned dataset
(Pad? and Lapata, 2006) contains 987 sentence pairs
from the English-German part of Europarl anno-
tated using the Blinker guidelines (Melamed, 1998).
The automatically aligned data comes from Europarl
(Koehn, 2005) in three language pairs (English?
Dutch, English?French and English?German). The
corpora are automatically aligned using GIZA++
(Och and Ney, 2003) in combination with the grow-
diag-final-and heuristic. With sentence length cut-
off 40 on both sides these contain respectively 945k,
949k and 995k sentence pairs.
Grammatical Coverage (GC) is defined as the
percentage word alignments (sentence pairs) in a
parallel corpus that can be covered by an instance
of the grammar (NF-ITG) (cf. Section 5). Clearly,
GC depends on the chosen semantic interpretation
of word alignments: contiguous TE?s (phrase pairs)
or discontiguous TE?s.
63
Alignments Set GC contiguous TEs GC discontiguous TEs
Hand aligned corpora
English?French 76.0 75.0
English?Portuguese 78.0 78.0
English?Spanish 83.0 83.0
Portuguese?French 78.0 74.0
Portuguese?Spanish 91.0 91.0
Spanish?French 79.0 74.0
LREC Corpora Average 80.83?5.49 79.17?6.74
English?German 45.427 45.325
Automatically aligned Corpora
English?Dutch 45.533 43.57
English?French 52.84 49.95
English?German 45.59 43.72
Automatically aligned corpora average 47.99?4.20 45.75?3.64
Table 1: The grammatical coverage (GC) of NF-ITG for different corpora dependent on the interpretation
of word alignments: contiguous Translation Equivalence or discontiguous Translation Equivalence
Results Table 1 shows the Grammatical Coverage
(GC) of NF-ITG for the different corpora depen-
dent on the two alternative definitions of translation
equivalence. The first thing to notice is that there
is just a small difference between the Grammatical
Coverage scores for these two definitions. The dif-
ference is in the order of a few percentage points,
the largest difference is seen for Portuguese?French
(79% v.s 74% Grammatical Coverage), for some
language pairs there is no difference. For the au-
tomatically aligned corpora the absolute difference
is on average about 2%. We attribute this to the fact
that there are only very few discontiguous TEUs that
can be covered by NF-ITG in this data.
The second thing to notice is that the scores are
much higher for the corpora from the LREC dataset
than they are for the manually aligned English?
German corpus. The approximately double source
and target length of the manually aligned English?
German corpus, in combination with somewhat less
dense alignments makes this corpus much harder
than the LREC corpora. Intuitively, one would
expect that more alignment links make alignments
more complicated. This turns out to not always be
the case. Further inspection of the LREC alignments
also shows that these alignments often consist of
parts that are completely linked. Such completely
linked parts are by definition treated as atomic
TEUs, which could make the alignments look sim-
pler. This contrasts with the situation in the man-
ually aligned English?German corpus where on av-
erage less alignment links exist per word. Exam-
ples 1 and 2 show that dense alignments can be sim-
pler than less dense ones. This is because sometimes
the density implies idiomatic TEUs which leads to
rather flat lexical productions. We think that id-
iomatic TEUs reasonably belong in the lexicon.
When we look at the results for the automati-
cally aligned corpora at the lowest rows in the ta-
ble, we see that these are comparable to the results
for the manually aligned English?German corpus
(and much lower than the results for the LREC cor-
pora). This could be explained by the fact that the
manually aligned English?German is not only Eu-
roparl data, but possibly also because the manual
alignments themselves were obtained by initializa-
tion with the GIZA++ alignments. In any case, the
manually and automatically acquired alignments for
this data are not too different from the perspective of
NF-ITG. Further differences might exist if we would
employ another class of grammars, e.g., full SCFGs.
One the one hand, we find that manual align-
ments are well but not fully covered by NF-ITG.
On the other, the automatic alignments are not cov-
ered well but NF-ITG. This suggests that these au-
tomatic alignments are difficult to cover by NF-ITG,
and the reason could be that these alignments are
built heuristically by trading precision for recall cf.
64
(Och and Ney, 2003). Sogaard (S?gaard, 2010) re-
ports that full ITG provides a few percentage points
gains over NF-ITG.
Overall, we find that our results for the LREC data
are far higher Sogaard?s (S?gaard, 2010) results but
lower than the upperbounds of (S?gaard and Wu,
2009). A similar observation holds for the English?
German manually aligned EuroParl data, albeit the
maximum length (15) used in (S?gaard and Wu,
2009; S?gaard, 2010) is different from ours (40). We
attribute the difference between our results and So-
gaard?s approach to our choice to adopt lexical pro-
ductions of NF-ITG that contain own internal align-
ments (the detailed version) and determined by the
atomic TEUs of the word alignment. Our results
differ substantially from (S?gaard and Wu, 2009)
who report upperbounds (indeed our results still fall
within these upperbounds for the LREC data).
8 Related Work
The array of work described in (Zens and Ney,
2003; Wellington et al, 2006; S?gaard and Wu,
2009; S?gaard and Kuhn, 2009; S?gaard, 2010) con-
centrates on methods for calculating upperbounds
on the alignment coverage for all ITGs, including
NF-ITG. Interestingly, these upperbounds are deter-
mined by filtering/excluding complex alignment phe-
nomena known formally to be beyond (NF-)ITG.
None of these earlier efforts discussed explicitly the
dilemmas of instantiating a grammar formalism or
how to formally parse word alignments.
The work in (Zens and Ney, 2003; S?gaard and
Wu, 2009), defining and counting TEUs, provides
a far tighter upperbound than (Wellington et al,
2006), who use the disjunctive interpretation of
word alignments, interpreting multiple alignment
links of the same word as alternatives. We adopt the
conjunctive interpretation of word alignments like a
majority of work in MT, e.g., (Ayan and Dorr, 2006;
Fox, 2002; S?gaard and Wu, 2009; S?gaard, 2010).
In deviation from earlier work, the work in (S?-
gaard and Kuhn, 2009; S?gaard and Wu, 2009;
S?gaard, 2010) discusses TEUs defined over word
alignments explicitly, and defines evaluation metrics
based on TEUs. In particular, Sogaard (S?gaard,
2010) writes that he employs "a more aggressive
search" for TEUs than earlier work, thereby leading
to far tighter upperbounds on hand aligned data. Our
results seem to back this claim but, unfortunately, we
could not pin down the formal details of his proce-
dure.
More remotely related, the work described in
(Huang et al, 2009) presents a binarization algo-
rithm for productions of an SCFG instance (as op-
posed to formalism). Although somewhat related,
this is different from checking whether there exists
an NF-ITG instance (which has to be determined)
that covers a word alignment.
In contrast with earlier work, we present the align-
ment coverage problem as an intersection of two par-
tially ordered sets (graphs). The partial order over
TEUs as well as the formal definition of parsing as
intersection in this work are novel elements, mak-
ing explicit the view of word alignments as automata
generating partially order sets.
9 Conclusions
In this paper we provide a formal characterization
for the problem of determining the coverage of a
word alignment by a given grammar formalism as
the intersection of two partially ordered sets. These
partially ordered set of TEUs can be formalized in
terms of hyper-graphs implementing forests (packed
synchronous trees), and the coverage as the intersec-
tion between sets of synchronous trees generalizing
the trees of (Zhang et al, 2008).
Practical explorations of our findings for the bene-
fit of models of learning reordering are underway. In
future work we would like to investigate the exten-
sion of this work to other limited subsets of SCFGs.
We will also investigate the possibility of devising
ITGs with explicit links between terminal symbols
in the productions, exploring different kinds of link-
ing.
Acknowledgements We thank reviewers for their
helpful comments, and thank Mark-Jan Nederhof for
illuminating discussions on parsing as intersection.
This work is supported by The Netherlands Orga-
nization for Scientific Research (NWO) under grant
nr. 612.066.929.
65
References
Nacip Ayan and Bonnie Dorr. 2006. Going beyond AER:
an extensive analysis of word alignments and their im-
pact on MT. In Proc. of the 21st International Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting of the ACL, pages 9?16, Morristown, NJ,
USA.
Yehoshua Bar-Hillel, Micha Perles, and Eli Shamir.
1964. On formal properties of simple phrase struc-
ture grammars. In Y. Bar-Hillel, editor, Language and
Information: Selected Essays on their Theory and Ap-
plication, chapter 9, pages 116?150. Addison-Wesley,
Reading, Massachusetts.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In ACL/AFNLP, pages 782?790.
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proc. of the Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
2007), page 61?72.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the ACL, pages 263?270,
June.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Trevor Cohn and Phil Blunsom. 2009. A bayesian model
of syntax-directed tree to string grammar induction. In
EMNLP, pages 352?361.
John DeNero, Daniel Gillick, James Zhang, and Dan
Klein. 2006. Why generative phrase models underper-
form surface heuristics. In Proceedings of the work-
shop on SMT, pages 31?38.
Heidi J. Fox. 2002. Phrasal cohesion and statistical
machine translation. In Proceedings of the ACL-02
conference on Empirical methods in natural language
processing - Volume 10, Proceedings of EMNLP,
pages 304?311, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Joao Grac?a, Joana Pardal, Lu?sa Coheur, and Diamantino
Caseiro. 2008. Building a golden collection of paral-
lel multi-language word alignment. In LREC?08, Mar-
rakech, Morocco. European Language Resources As-
sociation (ELRA).
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
itg models. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 923?931, Suntec, Singa-
pore, August. Association for Computational Linguis-
tics.
Liang Huang, Hao Zhang, Daniel Gildea, and Kevin
Knight. 2009. Binarization of synchronous
context-free grammars. Computational Linguistics,
35(4):559?595.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of the Human Language Technology Conference, HLT-
NAACL, May.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proc. of MT Summit.
Bernard Lang. 1988. Parsing incomplete sentences. In
Proceedings of COLING, pages 365?371.
J. Scott McCarley, Abraham Ittycheriah, Salim Roukos,
Bing Xiang, and Jian-Ming Xu. 2011. A correc-
tion model for word alignments. In Proceedings of
EMNLP, pages 889?898.
Dan Melamed. 1998. Annotation style guide for the
blinker project, version 1.0. Technical Report IRCS
TR #98-06, University of Pennsylvania.
Markos Mylonakis and Khalil Sima?an. 2011. Learning
hierarchical translation structure with linguistic anno-
tations. In Proceedings of the HLT/NAACL-2011.
Mark-Jan Nederhof and Giorgio Satta. 2004. The lan-
guage intersection problem for non-recursive context-
free grammars. Inf. Comput., 192(2):172?184.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Sebastian Pad? and Mirella Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic pro-
jection. In ACL-COLING?06, ACL-44, pages 1161?
1168, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
search for word alignment. In Proceedings of ACL,
pages 157?166.
Giorgio Satta and Enoch Peserico. 2005. Some com-
putational complexity results for synchronous context-
free grammars. In Proceedings of Human Language
Technology Conference and Conference on Empirical
Methods i n Natural Language Processing, pages 803?
810, Vancouver, British Columbia, Canada, October.
Association for Computational Linguistics.
Anders S?gaard and Jonas Kuhn. 2009. Empirical lower
bounds on alignment error rates in syntax-based ma-
chine translation. In SSST ?09, pages 19?27, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Anders S?gaard and Dekai Wu. 2009. Empirical lower
bounds on translation unit error rate for the full class
of inversion transduction grammars. In Proceedings of
the 11th International Workshop on Parsing Technolo-
gies (IWPT-2009), 7-9 October 2009, Paris, France,
66
pages 33?36. The Association for Computational Lin-
guistics.
Anders S?gaard. 2010. Can inversion transduction
grammars generate hand alignments? In Proccedings
of the 14th Annual Conference of the European Asso-
ciation for Machine Translation (EAMT).
Roy Tromble and Jason Eisner. 2009. Learning linear or-
dering problems for better translation. In Proceedings
of EMNLP?09, pages 1007?1016, Singapore.
Gertjan van Noord. 1995. The intersection of finite state
automata and definite clause grammars. In Proceed-
ings of ACL, pages 159?165.
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the com-
plexity of translational equivalence. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 3(23):377?403.
D.H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189?208.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In Proceedings of the Annual Meeting of
the ACL, pages 144?151.
Hao Zhang, Daniel Gildea, and David Chiang. 2008. Ex-
tracting synchronous grammar rules from word-level
alignments in linear time. In Proceedings of COLING,
pages 1081?1088.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the North-American Chapter of the
ACL (NAACL?06), pages 138?141.
67
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 414?419,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
BEER: BEtter Evaluation as Ranking
Milo
?
s Stanojevi
?
c
ILLC
University of Amsterdam
mstanojevic@uva.nl
Khalil Sima?an
ILLC
University of Amsterdam
k.simaan@uva.nl
Abstract
We present the UvA-ILLC submission of
the BEER metric to WMT 14 metrics task.
BEER is a sentence level metric that can
incorporate a large number of features
combined in a linear model. Novel con-
tributions are (1) efficient tuning of a large
number of features for maximizing corre-
lation with human system ranking, and (2)
novel features that give smoother sentence
level scores.
1 Introduction
The quality of sentence level (also called segment
level) evaluation metrics in machine translation is
often considered inferior to the quality of corpus
(or system) level metrics. Yet, a sentence level
metrics has important advantages as it:
1. provides an informative score to individual
translations
2. is assumed by MT tuning algorithms (Hop-
kins and May, 2011).
3. facilitates easier statistical testing using sign
test or t-test (Collins et al., 2005)
We think that the root cause for most of the diffi-
culty in creating a good sentence level metric is the
sparseness of the features often used. Consider the
n-gram counting metrics (BLEU (Papineni et al.,
2002)): counts of higher order n-grams are usu-
ally rather small, if not zero, when counted at the
individual sentence level. Metrics based on such
counts are brittle at the sentence level even when
they might be good at the corpus level. Ideally we
should have features of varying granularity that we
can optimize on the actual evaluation task: relative
ranking of system outputs.
Therefore, in this paper we explore two kinds of
less sparse features:
Character n-grams are features at the sub-word
level that provide evidence for translation ad-
equacy - for example whether the stem is cor-
rectly translated,
Abstract ordering patterns found in tree factor-
izations of permutations into Permutation
Trees (PETs) (Zhang and Gildea, 2007), in-
cluding non-lexical alignment patterns.
The BEER metric combines features of both kinds
(presented in Section 2).
With the growing number of adequacy and or-
dering features we need a model that facilitates ef-
ficient training. We would like to train for opti-
mal Kendall ? correlation with rankings by human
evaluators. The models in the literature tackle this
problem by
1. training for another similar objective ? e.g.,
tuning for absolute adequacy and fluency
scores instead on rankings, or
2. training for rankings directly but with meta-
heuristic approaches like hill-climbing, or
3. training for pairwise rankings using learning-
to-rank techniques
Approach (1) has two disadvantages. One is the
inconsistency between the training and the testing
objectives. The other, is that absolute rankings are
not reliable enough because humans are better at
giving relative than absolute judgments (see WMT
manual evaluations (Callison-Burch et al., 2007)).
Approach (2) does not allow integrating a large
number of features which makes it less attractive.
Approach (3) allows integration of a large num-
ber of features whose weights could be determined
in an elegant machine learning framework. The
output of learning in this approach can be either a
function that ranks all hypotheses directly (global
ranking model) or a function that assigns a score
414
to each hypothesis individually which can be used
for ranking (local ranking model) (Li, 2011). Lo-
cal ranking models are preferable because they
provide absolute distance between hypotheses like
most existing evaluation metrics.
In this paper we follow the learning-to-rank ap-
proach which produces a local ranking model in a
similar way to PRO MT systems tuning (Hopkins
and May, 2011).
2 Model
Our model is a fairly simple linear interpolation of
feature functions, which is easy to train and simple
to interpret. The model determines the similarity
of the hypothesis h to the reference translation r
by assigning a weight w
i
to each feature ?
i
(h, r).
The linear scoring function is given by:
score(h, r) =
?
i
w
i
? ?
i
(h, r) = ~w ?
~
?
2.1 Adequacy features
The features used are precision P , recall R and
F1-score F for different counts:
P
function
, R
function
, F
function
on matched func-
tion words
P
content
, R
content
, F
content
on matched content
words (all non-function words)
P
all
, R
all
, F
all
on matched words of any type
P
char n?gram
, R
char n?gram
, F
char n?gram
matching of the character n-grams
By differentiating function and non-function
words we might have a better estimate of which
words are more important and which are less. The
last, but as we will see later the most important,
adequacy feature is matching character n-grams,
originally proposed in (Yang et al., 2013). This
can reward some translations even if they did not
get the morphology completely right. Many met-
rics solve this problem by using stemmers, but us-
ing features based on character n-grams is more
robust since it does not depend on the quality
of the stemmer. For character level n-grams we
can afford higher-order n-grams with less risk of
sparse counts as on word n-grams. In our exper-
iments we used character n-grams for size up to
6 which makes the total number of all adequacy
features 27.
2.2 Ordering features
To evaluate word order we follow (Isozaki et al.,
2010; Birch and Osborne, 2010) in representing
reordering as a permutation and then measuring
the distance to the ideal monotone permutation.
Here we take one feature from previous work ?
Kendall ? distance from the monotone permuta-
tion. This metrics on the permutation level has
been shown to have high correlation with human
judgment on language pairs with very different
word order.
Additionally, we add novel features with an
even less sparse view of word order by exploiting
hierarchical structure that exists in permutations
(Zhang and Gildea, 2007). The trees that represent
this structure are called PETs (PErmutation Trees
? see the next subsection). Metrics defined over
PETs usually have a better estimate of long dis-
tance reorderings (Stanojevi?c and Sima?an, 2013).
Here we use simple versions of these metrics:
?
count
the ratio between the number of different
permutation trees (PETs) (Zhang and Gildea,
2007) that could be built for the given per-
mutation over the number of trees that could
be built if permutation was completely mono-
tone (there is a perfect word order).
?
[ ]
ratio of the number of monotone nodes in
a PET to the maximum possible number of
nodes ? the lenght of the sentence n.
?
<>
ratio of the number of inverted nodes to n
?
=4
ratio of the number of nodes with branching
factor 4 to n
?
>4
ratio of the number of nodes with branching
factor bigger than 4 to n
2.3 Why features based on PETs?
PETs are recursive factorizations of permutations
into their minimal units. We refer the reader to
(Zhang and Gildea, 2007) for formal treatment of
PETs and efficient algorithms for their construc-
tion. Here we present them informally to exploit
them for presenting novel ordering metrics.
A PET is a tree structure with the nodes deco-
rated with operators (like in ITG) that are them-
selves permutations that cannot be factorized any
further into contiguous sub-parts (called opera-
tors). As an example, see the PET in Figure 1a.
This PET has one 4-branching node, one inverted
415
?2, 4, 1, 3?
2 ?2, 1?
?1, 2?
5 6
4
1 3
(a) Complex PET
?1, 2?
?2, 1?
2 1
?2, 1?
4 3
(b) PET with inversions
?2, 1?
?2, 1?
?2, 1?
4 3
2
1
(c) Fully inverted PET
Figure 1: Examples of PETs
node and one monotone. The nodes are decorated
by operators that stand for a permutation of the
direct children of the node.
PETs have two important properties that make
them attractive for observing ordering: firstly, the
PET operators show the minimal units of ordering
that constitute the permutation itself, and secondly
the higher level operators capture hidden patterns
of ordering that cannot be observed without fac-
torization. Statistics over patterns of ordering us-
ing PETs are non-lexical and hence far less sparse
than word or character n-gram statistics.
In PETs, the minimal operators on the node
stand for ordering that cannot be broken down any
further. The binary monotone operator is the sim-
plest, binary inverted is the second in line, fol-
lowed by operators of length four like ?2, 4, 1, 3?
(Wu, 1997), and then operators longer than four.
The larger the branching factor under a PET node
(the length of the operator on that node) the more
complex the ordering. Hence, we devise possi-
ble branching feature functions over the operator
length for the nodes in PETs:
? factor 2 - with two features: ?
[ ]
and ?
<>
(there are no nodes with factor 3 (Wu, 1997))
? factor 4 - feature ?
=4
? factor bigger than 4 - feature ?
>4
All of the mentioned PETs node features, except
?
[ ]
and ?
count
, signify the wrong word order but
of different magnitude. Ideally all nodes in a PET
would be binary monotone, but when that is not
the case we are able to quantify how far we are
from that ideal binary monotone PET.
In contrast with word n-grams used in other
metrics, counts over PET operators are far less
sparse on the sentence level and could be more
reliable. Consider permutations 2143 and 4321
and their corresponding PETs in Figure 1b and
1c. None of them has any exact n-gram matched
(we ignore unigrams now). But, it is clear that
2143 is somewhat better since it has at least some
words in more or less the right order. These ?ab-
stract n-grams? pertaining to correct ordering of
full phrases could be counted using ?
[ ]
which
would recognize that on top of the PET in 1b there
is the monotone node unlike the PET in 1c which
has no monotone nodes at all.
3 Tuning for human judgment
The task of correlation with human judgment on
the sentence level is usually posed in the following
way (Mach?a?cek and Bojar, 2013):
? Translate all source sentences using the avail-
able machine translation systems
? Let human evaluators rank them by quality
compared to the reference translation
? Each evaluation metric should do the same
task of ranking the hypothesis translations
? The metric with higher Kendall ? correlation
with human judgment is considered better
Let us take any pair of hypotheses that have the
same reference r where one is better (h
good
) than
the other one (h
bad
) as judged by human evaluator.
In order for our metric to give the same ranking as
human judges do, it needs to give the higher score
to the h
good
hypothesis. Given that our model is
linear we can derive:
score(h
good
, r) > score(h
bad
, r)?
~w ?
~
?
good
> ~w ?
~
?
bad
?
~w ?
~
?
good
? ~w ?
~
?
bad
> 0?
~w ? (
~
?
good
?
~
?
bad
) > 0
~w ? (
~
?
bad
?
~
?
good
) < 0
The most important part here are the last two
equations. Using them we formulate ranking prob-
lem as a problem of binary classification: the pos-
itive training instance would have feature values
416
~?
good
?
~
?
bad
and the negative training instance
would have feature values
~
?
bad
?
~
?
good
. This trick
was used in PRO (Hopkins and May, 2011) but for
the different task:
? tuning the model of the SMT system
? objective function was an evaluation metric
Given this formulation of the training instances
we can train the classifier using pairs of hypothe-
ses. Note that even though it uses pairs of hypothe-
ses for training in the evaluation time it uses only
one hypothesis ? it does not require the pair of hy-
potheses to compare them. The score of the classi-
fier is interpreted as confidence that the hypothesis
is a good translation. This differs from the major-
ity of earlier work which we explain in Section 6.
4 Experiments on WMT12 data
We conducted experiments for the metric which
in total has 33 features (27 for adequacy and 6
for word order). Some of the features in the
metric depend on external sources of informa-
tion. For function words we use listings that are
created for many languages and are distributed
with METEOR toolkit (Denkowski and Lavie,
2011). The permutations are extracted using ME-
TEOR aligner which does fuzzy matching using
resources such as WordNet, paraphrase tables and
stemmers. METEOR is not used for any scoring,
but only for aligning hypothesis and reference.
For training we used the data from WMT13 hu-
man evaluation of the systems (Mach?a?cek and Bo-
jar, 2013). Before evaluation, all data was low-
ercased and tokenized. After preprocessing, we
extract training examples for our binary classifier.
The number of non-tied human judgments per lan-
guage pair are shown in Table 1. Each human
judgment produces two training instances : one
positive and one negative. For learning we use
regression implementation in the Vowpal Wabbit
toolkit
1
.
Tuned metric is tested on the human evaluated
data from WMT12 (Callison-Burch et al., 2012)
for correlation with the human judgment. As base-
line we used one of the best ranked metrics on the
sentence level evaluations from previous WMT
tasks ? METEOR (Denkowski and Lavie, 2011).
The results are presented in the Table 2. The pre-
sented results are computed using definition of
1
https://github.com/JohnLangford/
vowpal_wabbit
language pair #comparisons
cs-en 85469
de-en 128668
es-en 67832
fr-en 80741
ru-en 151422
en-cs 102842
en-de 77286
en-es 60464
en-fr 100783
en-ru 87323
Table 1: Number of human judgments in WMT13
language
pair
BEER
with
paraphrases
BEER
without
paraphrases
METEOR
en-cs 0.194 0.190 0.152
en-fr 0.257 0.250 0.262
en-de 0.228 0.217 0.180
en-es 0.227 0.235 0.201
cs-en 0.215 0.213 0.205
fr-en 0.270 0.254 0.249
de-en 0.290 0.271 0.273
es-en 0.267 0.249 0.247
Table 2: Kendall ? correleation on WMT12 data
Kendall ? from the WMT12 (Callison-Burch et
al., 2012) so the scores could be compared with
other metrics on the same dataset that were re-
ported in the proceedings of that year (Callison-
Burch et al., 2012).
The results show that BEER with and without
paraphrase support outperforms METEOR (and
almost all other metrics on WMT12 metrics task)
on the majority of language pairs. Paraphrase sup-
port matters mostly when the target language is
English, but even in language pairs where it does
not help significantly it can be useful.
5 WMT14 evaluation task results
In Table 4 and Table 3 you can see the results of
top 5 ranked metrics on the segment level evalua-
tion task of WMT14. In 5 out of 10 language pairs
BEER was ranked the first, on 4 the second best
and on one third best metric. The cases where it
failed to win the first place are:
? against DISCOTK-PARTY-TUNED on * - En-
glish except Hindi-English. DISCOTK-
PARTY-TUNED participated only in evalua-
tion of English which suggests that it uses
some language specific components which is
not the case with the current version of BEER
? against METEOR and AMBER on English-
Hindi. The reason for this is simply that we
417
Direction en-fr en-de en-hi en-cs en-ru
BEER .295 .258 .250 .344 .440
METEOR .278 .233 .264 .318 .427
AMBER .261 .224 .286 .302 .397
BLEU-NRC .257 .193 .234 .297 .391
APAC .255 .201 .203 .292 .388
Table 3: Kendall ? correlations on the WMT14 hu-
man judgements when translating out of English.
Direction fr-en de-en hi-en cs-en ru-en
DISCOTK-PARTY-TUNED .433 .381 .434 .328 .364
BEER .417 .337 .438 .284 .337
REDCOMBSENT .406 .338 .417 .284 .343
REDCOMBSYSSENT .408 .338 .416 .282 .343
METEOR .406 .334 .420 .282 .337
Table 4: Kendall ? correlations on the WMT14
human judgements when translating into English.
did not have the data to tune our metric for
Hindi. Even by treating Hindi as English we
manage to get high in the rankings for this
language.
From metrics that participated in all language
pairs on the sentence level on average BEER has
the best correlation with the human judgment.
6 Related work
The main contribution of our metric is a linear
combination of features with far less sparse statis-
tics than earlier work. In particular, we employ
novel ordering features over PETs, a range of char-
acter n-gram features for adequancy, and direct
tuning for human ranking.
There are in the literature three main approaches
for tuning the machine translation metrics.
Approach 1 SPEDE (Wang and Manning, 2012),
metric of (Specia and Gim?enez, 2010),
ROSE-reg (Song and Cohn, 2011), ABS met-
ric of (Pad?o et al., 2009) and many oth-
ers train their regression models on the data
that has absolute scores for adequacy, fluency
or post-editing and then test on the ranking
problem. This is sometimes called pointwise
approach to learning-to-rank. In contrast our
metric is trained for ranking and tested on
ranking.
Approach 2 METEOR is tuned for the ranking
and tested on the ranking like our metric but
the tuning method is different. METEOR has
a non-linear model which is hard to tune with
gradient based methods so instead they tune
their parameters by hill-climbing (Lavie and
Agarwal, 2008). This not only reduces the
number of features that could be used but also
restricts the fine tuning of the existing small
number of parameters.
Approach 3 Some methods, like ours, allow
training of a large number of parameters for
ranking. Global ranking models that di-
rectly rank hypotheses are used in ROSE-
rank (Song and Cohn, 2011) and PAIR met-
ric of (Pad?o et al., 2009). Our work is more
similar to the training method for local rank-
ing models that give score directly (as it is
usually expected from an evaluation metric)
which was originally proposed in (Ye et al.,
2007) and later applied in (Duh, 2008) and
(Yang et al., 2013).
7 Conclusion and future plans
We have shown the advantages of combining
many simple features in a tunable linear model
of MT evaluation metric. Unlike majority of the
previous work we create a framework for training
large number of features on human rankings and at
the same time as a result of tuning produce a score
based metric which does not require two (or more)
hypotheses for comparison. The features that we
used are selected for reducing sparseness on the
sentence level. Together the smooth features and
the learning algorithm produce the metric that has
a very high correlation with human judgment.
For future research we plan to investigate some
more linguistically inspired features and also ex-
plore how this metric could be tuned for better tun-
ing of statistical machine translation systems.
Acknowledgments
This work is supported by STW grant nr. 12271
and NWO VICI grant nr. 277-89-002.
References
Alexandra Birch and Miles Osborne. 2010. LRscore
for Evaluating Lexical and Reordering Quality in
MT. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 327?332, Uppsala, Sweden, July. Association
for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
418
(Meta-) Evaluation of Machine Translation. In
Proceedings of the Second Workshop on Statistical
Machine Translation, StatMT ?07, pages 136?158,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, ACL ?05, pages 531?540, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
Kevin Duh. 2008. Ranking vs. Regression in Ma-
chine Translation Evaluation. In Proceedings of the
Third Workshop on Statistical Machine Translation,
StatMT ?08, pages 191?194, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
Ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
Evaluation of Translation Quality for Distant Lan-
guage Pairs. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?10, pages 944?952, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Alon Lavie and Abhaya Agarwal. 2008. METEOR:
An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments. In
Proceedings of the ACL 2008 Workshop on Statisti-
cal Machine Translation.
Hang Li. 2011. Learning to Rank for Information Re-
trieval and Natural Language Processing. Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool Publishers.
Matou?s Mach?a?cek and Ond?rej Bojar. 2013. Results
of the WMT13 Metrics Shared Task. In Proceed-
ings of the Eighth Workshop on Statistical Machine
Translation, pages 45?51, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Sebastian Pad?o, Michel Galley, Dan Jurafsky, and
Christopher D. Manning. 2009. Textual Entail-
ment Features for Machine Translation Evaluation.
In Proceedings of the Fourth Workshop on Statis-
tical Machine Translation, StatMT ?09, pages 37?
41, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Xingyi Song and Trevor Cohn. 2011. Regression and
Ranking based Optimisation for Sentence Level MT
Evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 123?129,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Lucia Specia and Jes?us Gim?enez. 2010. Combining
Confidence Estimation and Reference-based Metrics
for Segment-level MT Evaluation. In Ninth Confer-
ence of the Association for Machine Translation in
the Americas, AMTA-2010, Denver, Colorado.
Milo?s Stanojevi?c and Khalil Sima?an. 2013. Eval-
uating Long Range Reordering with Permutation-
Forests. In ILLC Prepublication Series, PP-2013-
14. University of Amsterdam.
Mengqiu Wang and Christopher D. Manning. 2012.
SPEDE: Probabilistic Edit Distance Metrics for MT
Evaluation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, WMT ?12,
pages 76?83, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377?403.
Muyun Yang, Junguo Zhu, Sheng Li, and Tiejun Zhao.
2013. Fusion of Word and Letter Based Metrics
for Automatic MT Evaluation. In Proceedings of
the Twenty-Third International Joint Conference on
Artificial Intelligence, IJCAI?13, pages 2204?2210.
AAAI Press.
Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sen-
tence Level Machine Translation Evaluation As a
Ranking Problem: One Step Aside from BLEU. In
Proceedings of the Second Workshop on Statistical
Machine Translation, StatMT ?07, pages 240?247,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Hao Zhang and Daniel Gildea. 2007. Factorization of
synchronous context-free grammars in linear time.
In In NAACL Workshop on Syntax and Structure in
Statistical Translation (SSST.
419
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 11?21,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Bilingual Markov Reordering Labels for Hierarchical SMT
Gideon Maillette de Buy Wenniger and Khalil Sima?an
Institute for Logic, Language and Computation
University of Amsterdam
Science Park 107, 1098 XG Amsterdam, The Netherlands
gemdbw AT gmail.com, k.simaan AT uva.nl
Abstract
Earlier work on labeling Hiero grammars
with monolingual syntax reports improved
performance, suggesting that such label-
ing may impact phrase reordering as well
as lexical selection. In this paper we ex-
plore the idea of inducing bilingual labels
for Hiero grammars without using any
additional resources other than original
Hiero itself does. Our bilingual labels
aim at capturing salient patterns of phrase
reordering in the training parallel corpus.
These bilingual labels originate from hier-
archical factorizations of the word align-
ments in Hiero?s own training data. In this
paper we take a Markovian view on syn-
chronous top-down derivations over these
factorizations which allows us to extract
0
th
- and 1
st
-order bilingual reordering la-
bels. Using exactly the same training
data as Hiero we show that the Marko-
vian interpretation of word alignment fac-
torization offers major benefits over the
unlabeled version. We report extensive
experiments with strict and soft bilingual
labeled Hiero showing improved perfor-
mance up to 1 BLEU points for Chinese-
English and about 0.1 BLEU points for
German-English.
Phrase reordering in Hiero (Chiang, 2007) is mod-
elled with synchronous rules consisting of phrase
pairs with at most two nonterminal gaps, thereby
embedding ITG permutations (Wu, 1997) in lexi-
cal context. It is by now recognized that Hiero?s
reordering can be strengthened either by labeling
(e.g., (Zollmann and Venugopal, 2006)) or by sup-
plementing the grammar with extra-grammatical
reordering models, e.g., (Xiao et al., 2011; Huck
et al., 2013; Nguyen and Vogel, 2013). In this
paper we concentrate on labeling approaches.
Conceptually, labeling Hiero rules aims at in-
troducing preference in the SCFG derivations for
frequently occurring lexicalized ordering constel-
lations over rare ones which also affects lexical se-
lection. In this paper, we present an approach for
distilling phrase reordering labels directly from
alignments (hence bilingual labels).
To extract bilingual labels from word
alignments we must first interpret the alignments
as a hierarchy of phrases. Luckily, every
word alignment factorizes into Normalized
Decomposition Trees (NDTs) (Zhang et al.,
2008), showing explicitly how the word alignment
recursively decomposes into phrase pairs. Zhang
et al. (2008) employ NDTs for extracting Hiero
grammars. In this work, we extend NDTs
with explicit phrase permutation operators also
extracted from the original word alignment
(Sima?an and Maillette de Buy Wenniger, 2013);
Every node in the NDT is equipped with a
node operator that specifies how the order of
the target phrases (children of this node) is
produced from the corresponding source phrases.
Subsequently, we cluster the node operators
in these enriched NDTs according to their
complexity, e.g., monotone (straight), inverted,
non-binary but one-to-one, and the more complex
case of discontinuous (Maillette de Buy Wenniger
and Sima?an, 2013).
Inspired by work on parsing (Klein and Man-
ning, 2003), we explore a vertical Markovian
labeling approach: intuitively, 0
th
-order labels
signify the reordering of the sub-phrases inside the
phrase pair (Zhang et al., 2008), 1
st
-order labels
signify reordering aspects of the direct context
(an embedding, parent phrase pair) of the phrase
pair, and so on. Like the phrase orientation
models this labeling approach does not employ
external resources (e.g., taggers, parsers) beyond
the training data used by Hiero.
We empirically explore this bucketing for 0
th
-
11
and 1
st
-order labels both as hard and soft labels.
In experiments on German-English and Chinese-
English we show that this extension of Hiero of-
ten significantly outperforms the unlabeled model
while using no external data or monolingual la-
beling mechanisms. This suggests the viability
of automatically inducing bilingual labels follow-
ing the Markov labeling approach on operator-
labelled NDTs as proposed in this paper.
1 Hierarchical models and related work
Hiero SCFGs (Chiang, 2005; Chiang, 2007) allow
only up to two (pairs of) nonterminals on the right-
hand-side (RHS) of synchronous rules. The types
of permissible Hiero rules are:
X ? ??, ?? (1)
X ? ?? X
1
?, ? X
1
?? (2)
X ? ?? X
1
? X
2
? , ? X
1
? X
2
? ? (3)
X ? ?? X
1
? X
2
? , ? X
2
? X
1
? ? (4)
Here ?, ?, ?, ?, ?, ? are terminal sequences, possi-
bly empty. Equation 1 corresponds to a normal
phrase pair, 2 to a rule with one gap and 3 and 4
to the monotone- and inverting rules respectively.
Given an Hiero SCFG G, a source sentence s is
translated into a target sentence t by synchronous
derivations d, each is a finite sequence of well-
formed substitutions of synchronous productions
from G, see (Chiang, 2006). Existing phrase-
based models score a derivation der with linear
interpolation of a finite set of feature functions
(?(d)) of the derivation d, mostly working with
local feature functions ?
i
of individual produc-
tions, the target side yield string t of d (target
language model features) and other features (see
experimental section): arg max
d?G
P(t,d | s) ?
arg max
d?G
?
|?(d)|
i=1
?
i
? ?
i
. The parameters {?
i
} are
optimized on a held-out parallel corpus by direct
error-minimization (Och, 2003).
A range of (distantly) related work exploits
syntax for Hiero models, e.g. (Liu et al., 2006;
Huang et al., 2006; Mi et al., 2008; Mi and
Huang, 2008; Zollmann and Venugopal, 2006;
Wu and Hkust, 1998). In terms of labeling
Hiero rules, SAMT (Zollmann and Venugopal,
2006; Mylonakis and Sima?an, 2011) exploits a
?softer notion? of syntax by fitting the CCG-like
syntactic labels to non-constituent phrases. The
work of (Xiao et al., 2011) adds a lexicalized
orientation model to Hiero, akin to (Tillmann,
2004) and achieves significant gains. The work
of (Huck et al., 2013; Nguyen and Vogel, 2013)
overcomes technical limitations of (Xiao et al.,
2011), making necessary changes to the decoder,
which involves delayed (re-)scoring at hypernodes
up in the derivation of nodes lower in the chart
whose orientations are affected by them. This
goes to show that phrase-orientation models are
not mere labelings of Hiero.
Soft syntactic constraints has been around for
some time now (Zhou et al., 2008; Venugopal et
al., 2009; Chiang, 2010). In (Zhou et al., 2008)
Hiero is reinforced with a linguistically motivated
prior. This prior is based on the level of syntactic
homogeneity between pairs of non-terminals
and the associated syntactic forests rooted at
these nonterminals, whereby tree-kernels are
applied to efficiently measure the amount of
overlap between all pairs of sub-trees induced
by the pairs of syntactic forests. Crucially, the
syntactic prior encourages derivations that are
more syntactically coherent but does not block
derivations when they are not. In (Venugopal
et al., 2009) the authors associate distributions
over compatible syntactic labelings with grammar
rules, and combine these preference distributions
during decoding, thus achieving a summation
rather than competition between compatible label
configurations. The latter approach requires
significant changes to the decoder and comes at a
considerable computational cost. An alternative
approach (Chiang, 2010) uses labels similar to
(Zollmann and Venugopal, 2006) together with
boolean features for rule-label and substituted-
label combinations; using discriminative training
(MIRA) it is learned what combinations are
associated with better translations.
The labeling approach presented next differs
from existing approaches. It is inspired by soft
labeling but employs novel, non-linguistic bilin-
gual labels. And it shares the bilingual intuition
with phrase orientation models but it is based on
a Markov approach for SCFG labeling, thereby
remaining within the confines of Hiero SCFG,
avoiding the need to make changes inside the
decoder.
1
1
Soft constraint decoding can easily be implemented
without adapting the decoder, through a smart application of
?label bridging? unary rules. In practice however, adapting
the decoder turns out to be computationally more efficient,
therefore we used this solution in our experiments.
12
13
76
2
54
1
we
2
should
3
tailor
4
our
5
policy
6
accordingly
darauf
1
m?usen
2
wir
3
unsere
4
politik
5
ausrichten
6
Figure 1: Example alignment from Europarl
([1, 6], [1, 6], 1 )
([1, 2], [2, 3], 2 )
([1, 1], [3, 3], 4 ) ([2, 2], [2, 2], 5 )
([4, 5], [4, 5], 3 )
([4, 4], [4, 4], 6 ) ([5, 5], [5, 5], 7 )
Figure 2: Normalized Decomposition Tree (Zhang
et al., 2008) extended with pointers to original
alignment structure from Figure 1
2 Bilingual reordering labels for Hiero
Figure 1 shows an alignment from Europarl
German-English (Koehn, 2005) along with a tree
showing corresponding maximally decomposed
phrase pairs. Phrase pairs can be grouped into a
maximally decomposed tree (called Normalized
Decomposition Tree ? NDT) (Zhang et al., 2008).
Figure 2 shows the NDT for Figure 1, extended
with pointers to the original alignment structure
in Figure 2. The numbered boxes indicate how
the phrases in the two representations correspond.
In an NDT every phrase pair is recursively split
up at every level into a minimum number (two
or greater) of contiguous parts. In this example
the root node splits into three phrase pairs, but
these phrase pairs together do not cover the entire
parent phrase pair because of the discontinuity:
?tailor ... accordingly/ darauf ... ausrichten?.
Following (Zhang et al., 2008), we use the
NDT factorizations of word alignments in the
training data for extracting phrases. Every NDT
shows the hierarchical structuring into phrases
embedded in larger phrases, which together with
the context of the original alignment exposes the
reordering complexity of every phrase (Sima?an
and Maillette de Buy Wenniger, 2013). We will
exploit these elaborate distinctions based on the
complexity of reordering for Hiero rule labels as
explained next.
Phrase-centric (0
th
-order) labels are based on
the view of looking inside a phrase pair to see
how it decomposes into sub-phrase pairs. The op-
erator signifying how the sub-phrase pairs are re-
ordered (target relative to source) is bucketted into
a number of ?permutation complexity? categories.
Straightforwardly, we can start out by using the
two well known cases of Inversion Transduction
Grammars (ITG) {Monotone, Inverted} and label
everything
2
that falls outside these two category
with a default label ?X? (leaving some Hiero
nodes unlabeled). This leads to the following
coarse phrase-centric labeling scheme, which we
name 0
th
ITG+
: (1) Monotonic(Mono): binarizable,
fully monotone plus non-decomposable phrases
(2) Inverted(Inv): binarizable, fully inverted (3) X:
decomposable phrases that are not binarizable.
A clear limitation of the above ITG-like label-
ing approach is that all phrase pairs that decom-
pose into complex non-binarizable reordering pat-
terns are not further distinguished. Furthermore,
non-decomposable phrases are lumped together
with decomposable monotone phrases, although
they are in fact quite different. To overcome these
problems we extend ITG in a way that further
distinguishes the non-binarizable phrases and also
distinguishes non-decomposable phrases from the
rest. This gives a labeling scheme we will call
simply 0
th
-order labeling, abbreviated 0
th
, con-
sisting of a more fine-grained set of five cases,
ordered by increasing complexity (see examples
in Figure 4): (1) Atomic: non-decomposable
phrases, (2) Monotonic(Mono): binarizable, fully
monotone, (3) Inverted(Inv): binarizable, fully
inverted (4) Permutation(Perm): factorizes into a
permutation of four or more sub-phrases (5) Com-
plex(Comp): does not factorize into a permutation
and contains at least one embedded phrase.
In Figure 3, we show a phrase-complexity la-
beled derivation for the example of Figure 1.
Observe how the phrase-centric labels reflect the
relative reordering at the node. For example, the
2
Non-decomposable phrases will still be grouped
together with Monotone, since they are more similar to this
category than to the catchall ?X? category.
13
S0
accordingly
policy
our
tailor
should
we
COMPLEX TOP
1
INVERTED E.F.D.
2
ATOMIC R.B.I.
4
MONO E.F.D.
3
ATOMIC L.B.M.
7
S
0
ausrichten
politik
unsere
wir
m?ussen
darauf
COMPLEX TOP
1
INVERTED E.F.D.
2
ATOMIC R.B.I.
4
MONO E.F.D.
3
ATOMIC L.B.M.
7
Figure 3: Synchronous trees (implicit derivations end results) based on differently labelled Hiero
grammars. The figure shows alternative labeling for every node: Phrase-Centric (0
th
-order) (light gray)
and Parent-Relative (1
st
-order) (dark gray).
this is an important matter
das ist ein wichtige angelegenheit
1
1
2
2
Monotone
we all agree on this
das sehen wir alle
1
1
2
2
Inversion
i want to stress two points
auf zwei
punkte
m?ochte ich hinweisen
1
1
2
2
3
3
4
4
Permutation
we owe this to our citizens
das sind wir unsern burgern schuldig
1
1
2
2
3
3
Complex
it would be possible
kann mann
1
1
Atomic
Figure 4: Different types of Phrase-Centric Alignment Labels
Inverted label of node-pair 2 corresponds to the
inversion in the alignment of ?we should, m?usen
wir?; in contrast, node-pair 1 is complex and
discontinuous and the label is Complex.
Parent-relative (1
st
-order) labels capture the re-
ordering that a phrase undergoes relative to an
embedding parent phrase.
1. For a binarizable mother phrase with orien-
tation X
o
? {Mono, Inv}, the phrase itself can
either group to the left only Left-Binding-
X
o
, right only Right-Binding-X
o
, or with both
sides (Fully-X
o
).
2. Fully-Discontinuous: Any phrase within
a non-binarizable permutation or complex
alignment containing discontinuity.
3. Top: phrases that span the entire aligned
sentence pair.
In cases were multiple labels are applicable, the
simplest applicable label is chosen according to
the following preference order:
{Fully-Monotone, Left/Right-Binding-Monotone,
Fully-Inverted, Left/Right-Binding-Inverted,
Fully-Discontinuous, TOP}.
In Figure 3 the parent-relative labels in the
derivation reflect the reordering taking place at the
phrases with respect to their parent node. Node 4
has a parent node that inverts the order and the
sibling node it binds is on the right, therefore it
14
is labeled ?right-binding inverted? (R.B.I.); E.F.D.
and L.B.M. are similar abbreviations for ?embed-
ded fully discontinuous? and ?left-binding mono-
tone? respectively. As yet another example node
7 in Figure 3 is labeled ?left-binding monotone?
(L.B.M.) since it is monotone, but the alignment
allows it only to bind to the left at the parent node,
as opposed to only to the right or to both sides
which cases would have yielded ?right-binding
monotone? R.B.M. and ?(embedded) fully mono-
tone? (E.F.M.) parent-relative reordering labels
respectively.
Note that for parent-relative labels the binding
direction of monotone and inverted may not be
informative. We therefore also form a set of
coarse parent-relative labels (?1
st
Coarse
?) by col-
lapsing the label pairs Left/Right-Binding-Mono
and Left/Right-Binding-Inverted into single labels
One-Side-Binding-Mono and One-Side-Binding-
Inv
3
.
3 Features for soft bilingual labeling
Labels used in hierarchical Statistical Machine
Translation (SMT) are typically adapted from ex-
ternal resources such as taggers and parsers. Like
in our case, these labels are typically not fitted to
the training data ? with very few exceptions e.g.,
(Mylonakis and Sima?an, 2011; Mylonakis, 2012;
Hanneman and Lavie, 2013). Unfortunately this
means that the labels will either overfit or underfit,
and when they are used as strict constraints on
SCFG derivations they are likely to underperform.
Experience with mismatch between syntactic la-
bels and the data is abundant (Venugopal et al.,
2009; Marton et al., 2012; Chiang, 2010), and
using soft constraint decoding with suitable label
substitution features has been shown to be an
effective workaround solution. The intuition be-
hind soft constraint decoding is that even though
heuristic labels are not perfectly tailored to the
data, they do provide useful information provided
the model is ?allowed to learn? to use them only
in as far as they can improve the final evaluation
metric (usually BLEU).
3
We could also further coarsen the 1
st
labels by
removing entirely all sub-distinctions of binding-type for
the binarizable cases, but that would make the labeling
essentially equal to the earlier mentioned 0
th
ITG+
except for
looking at the reordering occurring at the parent rather than
inside the phrase itself. We did not explore this variant in this
work, as the high similarity to the already explored 0
th
ITG+
variant made it not seem to add much extra information.
???
LHS
10
N1
11
N2
12
GAP1
11
GAP2
12
Substituting rule
Decoder chart
Label Substitution Features
Figure 5: Label substitution features, schematic
view. Labels/Gaps with same filling in the figures
correspond to the situation of a nonterminal/gap
whose labels correspond (for N1/GAP1). Fillings
of different shades (as for N2/GAP2 on the right
in the two figures) indicates the situation were the
label of the nonterminal and the gap is different.
Next we introduce the set of label substitution
features used in our experiments.
Label substitution features consist of a unique
feature for every pair of labels ?L
?
, L
?
? in the
grammar, signifying a rule with left-hand-side
label L
?
substituting on a gap labeled L
?
. These
features are combined with two more coarse
features, ?Match? and ?Nomatch?, indicating if
the substitution involves labels that match or not.
Figure 5 illustrates the concept of label substi-
tution features schematically. In this figure the
substituting rule is substituted onto two gaps in
the chart, which induces two label substitution
features indicated by the two ellipses. The sit-
uation is analogous for rules with just one gap.
To make things concrete, lets assume that both
the first nonterminal of the rule N1 as well as
the first gap it is substituted onto GAP1 have
label MONO. Furthermore lets assume the second
nonterminal N2 has label COMPLEX while the
label of the gap GAP2 it substitutes onto is INV .
This situation results in the following two specific
label substitution features:
? subst(MONO,MONO)
? subst(INV ,COMPLEX)
Canonical labeled rules. Typically when la-
beling Hiero rules there can be many different
labeled variants of every original Hiero rule. With
soft constraint decoding this leads to prohibitive
computational cost. This also has the effect of
making tuning the features more difficult. In
practice, soft constraint decoding usually exploits
15
Systen Name Matching Type Label Order Label Granularity
Hiero-0
th
ITG+
Strict 0
th
order Coarse
Hiero-0
th
Strict 0
th
order Fine
Hiero-1
st
Coarse
Strict 1
th
order Coarse
Hiero-1
st
Strict 1
th
order Fine
Hiero-0
th
ITG+
-Sft Soft 0
th
order Coarse
Hiero-0
th
-Sft Soft 0
th
order Fine
Hiero-1
st
Coarse
-Sft Soft 1
th
order Coarse
Hiero-1
st
-Sft Soft 1
th
order Fine
Table 1: Experiment names legend
System Name
DEV TEST
BLEU ? METEOR ? TER ? KRS ? BLEU ? METEOR ? TER ? KRS ?
German-English
Hiero 27.90 32.69 58.22 66.37 28.39 32.94 58.01 67.44
SAMT 27.76 32.67 58.05 66.84
N
28.32 32.88 57.70
NN
67.63
Hiero-0
th
ITG+
27.85 32.70 58.04
NN
66.27 28.36 32.90
H
57.83
NN
67.30
Hiero-0
th
27.82 32.75 57.92
NN
66.66 28.39 33.03
NN
57.75
NN
67.55
Hiero-1
st
Coarse
27.86 32.66 58.23 66.37 28.22
H
32.90 57.93 67.47
Hiero-1
st
27.74
H
32.60
HH
58.11 66.44 28.27 32.80
HH
57.95 67.39
Chinese-English
Hiero 31.70 30.72 61.21 58.28 31.63 30.56 59.28 58.03
Hiero-0
th
ITG+
31.54 30.97
NN
62.79
HH
59.54
NN
31.94
NN
30.84
NN
60.76
HH
59.45
NN
Hiero-0
th
31.66 30.95
NN
62.20
HH
60.00
NN
31.90
NN
30.79
NN
60.11
HH
59.68
NN
Hiero-1
st
Coarse
31.64 30.75 61.37 59.48
NN
31.57 30.57 59.58
HH
59.13
NN
Hiero-1
st
31.74 30.79 61.94
HH
60.22
NN
31.77 30.62 60.13
HH
59.89
NN
Table 2: Mean results bilingual labels with strict matching.
4
a single labeled version per Hiero rule, which
we call the ?canonical labeled rule?. Following
(Chiang, 2010), this canonical form is the most
frequent labeled variant.
4 Experiments
We evaluate our method on two language pairs:
using German/Chinese as source and English as
target. In all experiments we decode with a
4-gram language model smoothed with modified
Knesser-Ney discounting (Chen and Goodman,
1998). The data used for training the language
models differs per language pair, details are given
in the next paragraphs. All data is lowercased as
a last pre-processing step. In all experiments we
use our own grammar extractor for the generation
of all grammars, including the baseline Hiero
grammars. This enables us to use the same
features (as far as applicable given the grammar
formalism) and assure true comparability of the
grammars under comparison.
German-English
4
Statistical significance is dependent on variance of
resampled scores, and hence sometimes different for same
mean scores across different systems.
The data for our German-English experiments
is derived from parliament proceedings sourced
from the Europarl corpus (Koehn, 2005), with
WMT-07 development and test data. We used a
maximum sentence length of 40 for filtering the
training data. We employ 1M sentence pairs for
training, 1K for development and 2K for test-
ing (single reference per source sentence). Both
source and target of all datasets are tokenized
using the Moses(Hoang et al., 2007) tokenization
script. For these experiments both the baseline
and our method use a language model trained
on the target side of the full original training set
(approximately 1M sentences).
Chinese-English
The data for our Chinese-English experiments is
derived from a combination of MultiUn(Eisele
and Chen, 2010; Tiedemann, 2012)
5
data and
Hong Kong Parallel Text data from the Linguistic
Data Consortium
6
. The Hong Kong Parallel Text
data is in traditional Chinese and is thus first
converted to simplified Chinese to be compatible
5
Freely available and downloaded from
http://opus.lingfil.uu.se/
6
The LDC catalog number of this dataset is LDC2004T08
16
System Name
DEV TEST
BLEU ? METEOR ? TER ? KRS ? BLEU ? METEOR ? TER ? KRS ?
German-English
Hiero 27.90 32.69 58.22 66.37 28.39 32.94 58.01 67.44
SAMT 27.76 32.67 58.05 66.84
N
28.32 32.88 57.70
NN
67.63
Hiero-0
th
ITG+
-Sft 28.00
N
32.76
NN
57.90
NN
66.17 28.48 32.98 57.79
NN
67.32
Hiero-0
th
-Sft 28.01
N
32.71 57.95
NN
66.24 28.45 32.98 57.73
NN
67.51
Hiero-1
st
Coarse
-Sft 27.94 32.69 57.91
NN
66.26 28.45
N
32.94 57.75
NN
67.36
Hiero-1
st
-Sft 28.13
NN
32.80
NN
57.92
NN
66.32 28.45 33.00
N
57.79
NN
67.45
Chinese-English
Hiero 31.70 30.72 61.21 58.28 31.63 30.56 59.28 58.03
Hiero-0
th
ITG+
-Sft 31.88
N
30.46
HH
60.64
NN
57.82
H
31.93
NN
30.37
HH
58.86
NN
57.60
H
Hiero-0
th
-Sft 32.04
NN
30.90
NN
61.47
HH
59.36
NN
32.20
NN
30.74
NN
59.45
H
58.92
NN
Hiero-1
st
Coarse
-Sft 32.39
NN
31.02
NN
61.56
HH
59.51
NN
32.55
NN
30.86
NN
59.57
HH
59.03
NN
Hiero-1
st
-Sft 32.63
NN
31.22
NN
62.00
HH
60.43
NN
32.61
NN
30.98
NN
60.19
HH
59.84
NN
Table 3: Mean results bilingual labels with soft matching.
4
with the rest of the data
7
. We used a maximum
sentence length of 40 for filtering the training
data. The combined dataset has 7.34M sentence
pairs. The MulitUN dataset contains translated
documents from the United Nations, similar in
genre to the parliament domain. The Hong Kong
Parallel Text in contrast contains a richer mix
of domains, namely Hansards, Laws and News.
For the dev and test set we use the Multiple-
Translation Chinese datasets from LDC, part 1-4
8
,
which contain sentences from the News domain.
We combined part 2 and 3 to form the dev set
(1813 sentence pairs) and part 1 and 4 to form the
test set (1912 sentence pairs). For both develop-
ment and testing we use 4 references. The Chinese
source side of all datasets is segmented using the
Stanford Segmenter(Chang et al., 2008)
9
. The
English target side of all datasets is tokenized
using the Moses tokenization script.
For these experiments both the baseline and
our method use a language model trained on
5.4M sentences of domain specific
10
news data
taken from the ?Xinhua? subcorpus of the English
Gigaword corpus of LDC.
11
7
Using a simple conversion script downloaded from
http://www.mandarintools.com/zhcode.html
8
LDC catalog numbers: LDC2002T01, DC2003T17,
LDC2004T07 and LDC2004T07
9
Downloaded from
http://nlp.stanford.edu/software/segmenter.shtml
10
For Chinese-English translation the different domain of
the train data (mainly parliament) and dev/test data (news)
requires usage of a domain specific language model to get
optimal results. For German-English, all data is from the
the parliament domain, so a language model trained on the
(translation model) training data is already domain-specific.
11
The LDC catalog number of this dataset is LDC2003T05
4.1 Experimental Structure
In our experiments we explore the influence of
three dimensions of bilingual reordering labels on
translation accuracy. These dimensions are:
? label granularity : granularity of the labeling
{Coarse,Fine}
? label order : the type/order of the labeling
{0
th
, 1
st
}
? matching type : the type of label matching
performed during decoding {Strict,Soft}
Combining these dimensions gives 8 different
reordering labeled systems per language pair.
On top of that we use two baseline systems,
namely Hiero and Syntax Augmented Machine
Translation (SAMT) to measure these systems
against. An overview of the naming of our
reordering labeled systems is given in Table 1.
Training and decoding details Our experiments
use Joshua (Ganitkevitch et al., 2012) with Viterbi
best derivation. Baseline experiments use nor-
mal decoding whereas soft labeling experiments
use soft constraint decoding. For training we
use standard Hiero grammar extraction constraints
(Chiang, 2007) (phrase pairs with source spans
up to 10 words; abstract rules are forbidden).
During decoding maximum span 10 on the source
side is maintained. Following common practice,
we use relative frequency estimates for phrase
probabilities, lexical probabilities and generative
rule probability.
We train our systems using (batch-kbest) Mira
as borrowed by Joshua from the Moses codebase,
allowing up to 30 tuning iterations. Following
17
standard practice, we tune on BLEU, and after
tuning we use the configuration with the highest
scores on the dev set with actual (corpus level)
BLEU evaluation. We report lowercase BLEU
(Papineni et al., 2002), METEOR (Denkowski
and Lavie, 2011) and TER (Snover et al., 2006)
scores for the tuned test set and also for the tuned
dev set, the latter mainly to observe any possible
overfitting. We use Multeval version 0.5.1.
12
for
computing these metrics. We also use MultEval?s
implementation of statistical significance testing
between systems, which is based on multiple
optimizer runs and approximate randomization.
Multeval (Clark et al., 2011) randomly swaps
outputs between systems and estimates the prob-
ability that the observed score difference arose by
chance. Differences that are statistically signif-
icant and correspond to improvement/worsening
with respect to the baseline are marked with
N
/
H
at
the p ? .05 level and
NN
/
HH
at the p ? .01 level. We
also report the Kendall Reordering Score (KRS),
which is the reordering-only variant of the LR-
score (Birch and Osborne, 2010) (without the
optional interpolation with BLEU) and which is
a sentence-level score. For the computation of
statistical significance of this metric we use our
own implementation of the sign test
13
(Dixon and
Mood, 1946), as also described in (Koehn, 2010).
In our experiments we repeated each experi-
ment three times to counter unreliable conclusions
due to optimizer variance. Scores are averages
over three runs of tuning plus testing. Scores
marked with
N
are significantly better than the
baseline, those marked with
H
are significantly
worse; according to the resampling test of Mul-
teval (Clark et al., 2011).
Preliminary experiment with strict matching
Initial experiments concerned 0
th
-order reorder-
ing labels in a strict matching approach (no soft
constraints). The results are shown in Table 2 for
both language pairs. The results for the Hiero and
SAMT
14
baselines (Hiero and SAMT) are shown
in the first rows. Below it results for the 0
th
-order
(phrase-centric) bilingual labeled systems with
either the Coarse (Hiero-0
th
ITG+
) or Fine label
12
https://github.com/jhclark/multeval
13
To make optimal usage of the 3 runs we computed
equally weighted improvement/worsening counts for all
possible 3 ? 3 baseline output / system output pairs and use
those weighted counts in the sign test.
14
SAMT could only be ran for German-English and not
for Chinese-English, due to memory constraints.
variant (Hiero-0
th
) are shown, followed by the
results for Coarse and Fine variant of the 1
th
-order
(parent-relative) bilingual labeled systems (Hiero-
1
st
Coarse
and Hiero-1
st
). All these systems use the
default decoding with strict label matching.
For German-English the effect of strict bilin-
gual labels is mostly positive: although we have
no improvement for BLEU we do achieve sig-
nificant improvements for METEOR and TER
on the test set. For Chinese-English, overall
Hiero-0
th
ITG+
shows the biggest improvements,
namely significant improvements of +0.31 BLEU,
+0.28 METEOR and +1.42 KRS. TER is the
only metric that worsens, and considerably so
with +1.48 point. Hiero-1
st
achieves the highest
improvement of KRS, namely 1.86 point higher
than the Hiero baseline. Overall, this preliminary
experiment shows that strict labeling sometimes
gives improvements over Hiero, but sometimes it
leads to worsening in terms of some of the metrics.
Results with soft bilingual constraints Our ini-
tial experiments with strict bilingual labels in
combination with strict matching by the decoder
gave some hope such constraints could be useful.
At the same time the results showed no stable
improvements across language pairs, and thus
does not allow us to draw definite conclusions
about the merit of bilingual labels.
Results for experiments with soft bilingual la-
beling are shown in Table 3. Here Hiero corre-
sponds to the Hiero baseline. Below it are shown
the systems that use soft constraint decoding (
SCD). Hiero-0
th
ITG+
-Sft and Hiero-0
th
-Sft using
phrase-centric labels (0
th
-order) in Coarse or Fine
form. Similarly, Hiero-1
st
Coarse
-Sft and Hiero-
1
st
-Sft correspond to the analog systems with
1
st
-order, parent-relative labels. For German-
English there are only minor improvements for
BLEU and METEOR, with somewhat bigger im-
provements for TER. For Chinese-English how-
ever the improvements are considerable, +0.98
BLEU improvement over the Hiero baseline for
Hiero-1
st
-Sft as well as +0.42 METEOR and
+1.81 KRS. TER is worsening with +0.85 for this
system. For Chinese-English the Fine version of
the labels gives overall superior results for both
0
th
-order and 1
st
-order labels.
Discussion Our best soft bilingual labeling system
for German-English shows small but significant
improvements of METEOR and TER while im-
18
proving BLEU and KRS as well, but not signifi-
cantly. The results with soft-constraint matching
are better than those for strict-matching in general,
while there is no clear winner between the Coarse
and Fine variant of labels.
For Chinese-English we see considerable
improvements and overall the best results for
the combination of soft-constraint matching,
with the Fine 1
st
-order variant of the labeled
systems (Hiero-1
st
-Sft). For Chinese-English the
improvement of the word-order is also particularly
clear as indicated by the +1.81 KRS improvement
for this best system. Furthermore the negative
effects in terms of worsening of TER are also
reduced in the soft-matching setting, dropping
from +1.48 TER to +0.85 TER. The results for
Hiero-0
th
-Sft are also competitive, since though
it gives somewhat lower improvements of BLEU
and METEOR, it gives an improvement of +1.89
KRS, while TER only worsens by +0.17 for this
system.
We conclude that bilingual Markov labels can
make a big difference in improvement of hier-
archical SMT. We observe that going beyond
the basic reordering labels of ITG, refining the
cases not captured by ITG and even more ef-
fective: taking a 1
st
-order rather than o
th
-order
perspective on reordering are major factors for
the success of including reordering information to
hierarchical SMT through labeling. Crucial to the
success of this undertaking is also the usage of
a soft-constraint approach to label matching, as
opposed to strict-matching. Finally, comparison
of the German-English results with results for
Syntax-Augmented Machine Translation (SAMT)
reveals that SAMT loses performance compared
to the Hiero baseline for BLEU, the metric upon
which tuning is done, as well as METEOR, while
only TER and KRS show improvement. Since
the best bilingual labeled system for German-
English (Hiero-1
st
-Sft) improves METEOR and
TER significantly, while also improving BLEU
and KRS, though not significant, we believe our
labeling is highly competitive with syntax-based
labeling approaches, without the need for any
additional resources in the form of parsers or
taggers, as syntax-based systems require. Likely
complementarity of reordering information, and
(target) syntax, which improves fluency, makes
combining both a promising possibility we would
like to explore in future work.
5 Conclusion
We presented a novel method to enrich Hierarchi-
cal Statistical Machine Translation with bilingual
labels that help to improve the translation quality.
Considerable and significant improvements of the
BLEU, METEOR and KRS are achieved simul-
taneously for Chinese-English translation while
tuning on BLEU, where the Kendall Reordering
Score is specifically designed to measure im-
provement of reordering in isolation. For German-
English more modest, statistically significant im-
provements of METEOR and TER (simultane-
ously) or BLEU (separately) are achieved. Our
work differs from related approaches that use
syntactic or part-of-speech information in the for-
mation of reordering constraints in that it needs no
such additional information. It also differs from
related work on reordering constraints based on
lexicalization in that it uses no such lexicaliza-
tion but instead strives to achieve more globally
coherent translations, afforded by global, holistic
constraints that take the local reordering history
of the derivation directly into account. Our exper-
iments also once again reinforce the established
wisdom that soft, rather than strict constraints,
are a necessity when aiming to include new in-
formation to an already strong system without the
risk of effectively worsening performance through
constraints that have not been directly tailored
to the data through a proper learning approach.
While lexicalized constraints on reordering have
proven to have great potential, un-lexicalized soft
bilingual constraints, which are more general and
transcend the rule level have their own place in
providing another agenda of improving transla-
tion which focusses more on the global coher-
ence direction by directly putting soft alignment-
informed constraints on the combination of rules.
Finally, while more research is necessary in this
direction, there are strong reasons to believe that
in the right setup these different approaches can be
made to further reinforce each other.
Acknowledgements
This work is supported by The Netherlands Or-
ganization for Scientific Research (NWO) under
grant nr. 612.066.929. The authors would like to
thank Matt Post and Juri Ganitkevitch, for their
support with respect to the integration of Fuzzy
Matching Decoding into the Joshua codebase.
19
References
Alexandra Birch and Miles Osborne. 2010. Lrscore
for evaluating lexical and reordering quality in mt.
In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 327?332.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing chinese word
segmentation for machine translation performance.
In Proceedings of the Third Workshop on Statistical
Machine Translation, pages 224?232.
Stanley F. Chen and Joshua T. Goodman. 1998.
An empirical study of smoothing techniques for
language modeling. Technical Report TR-10-98,
Computer Science Group, Harvard University.
David Chiang. 2005. A hierarchical phrase-
based model for statistical machine translation. In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 263?270, June.
David Chiang. 2006. An introduction to synchronous
grammars.
David Chiang. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2):201?
228.
David Chiang. 2010. Learning to translate with
source and target syntax. In Proceedings of
the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1443?1452.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: Controlling
for optimizer instability. In Proceedings of
the 49th Annual Meeting of the Association
for Computational Linguistics: HLTTechnologies:
Short Papers - Volume 2, pages 176?181.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization
and evaluation of machine translation systems. In
Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 85?91.
W. J. Dixon and A. M. Mood. 1946. The statistical
sign test. Journal of the American Statistical
Association, pages 557?566.
Andreas Eisele and Yu Chen. 2010. Multiun: A
multilingual corpus from united nation documents.
In Proceedings of the 7th International Conference
on Language Resources and Evaluation (LREC
2010), pages 2868?2872.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt
Post, and Chris Callison-Burch. 2012. Joshua
4.0: Packing, pro, and paraphrases. In Proceedings
of the Seventh Workshop on Statistical Machine
Translation, pages 283?291, Montr?eal, Canada,
June. Association for Computational Linguistics.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In HLT-NAACL, pages 288?297.
Hieu Hoang, Alexandra Birch, Chris Callison-burch,
Richard Zens, Rwth Aachen, Alexandra Constantin,
Marcello Federico, Nicola Bertoldi, Chris Dyer,
Brooke Cowan, Wade Shen, Christine Moran, and
Ondrej Bojar. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the 41st Annual Meeting on Association for
Computational Linguistics - Volume 1, pages 177?
180.
Liang Huang, Kevin Knight, and Aravind Joshi.
2006. A syntax-directed translator with extended
domain of locality. In Proceedings of the Workshop
on Computationally Hard Problems and Joint
Inference in Speech and Language Processing,
pages 1?8.
Matthias Huck, Joern Wuebker, Felix Rietig, and
Hermann Ney. 2013. A phrase orientation
model for hierarchical machine translation. In
ACL 2013 Eighth Workshop on Statistical Machine
Translation, pages 452?463.
Dan Klein and Christopher D. Manning. 2003.
Accurate unlexicalized parsing. In Proceedings
of the 41st Annual Meeting on Association for
Computational Linguistics - Volume 1, pages 423?
430.
P. Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proc. of MT
Summit.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st International
Conference on Computational Linguistics and
the 44th Annual Meeting of the Association for
Computational Linguistics, pages 609?616.
Gideon Maillette de Buy Wenniger and Khalil
Sima?an. 2013. Hierarchical alignment decomposi-
tion labels for hiero grammar rules. In Proceedings
of the Seventh Workshop on Syntax, Semantics and
Structure in Statistical Translation, pages 19?28.
Yuval Marton, David Chiang, and Philip Resnik.
2012. Soft syntactic constraints for arabic?english
hierarchical phrase-based translation. Machine
Translation, 26(1-2):137?157.
Haitao Mi and Liang Huang. 2008. Forest-based
translation rule extraction. In Proceedings of
EMNLP.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT,
June.
20
Markos Mylonakis and Khalil Sima?an. 2011.
Learning hierarchical translation structure with
linguistic annotations. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, pages 642?652.
Markos Mylonakis. 2012. Learning the Latent
Structure of Translation. Ph.D. thesis, University
of Amsterdam.
ThuyLinh Nguyen and Stephan Vogel. 2013.
Integrating phrase-based reordering features into a
chart-based decoder for machine translation. In
Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1587?1596.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings
of the 41st Annual Meeting on Association for
Computational Linguistics - Volume 1, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for
Computational Linguistics, pages 311?318.
Khalil Sima?an and Gideon Maillette de Buy Wen-
niger. 2013. Hierarchical alignment trees:
A recursive factorization of reordering in word
alignments with empirical results. Internal Report.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted human
annotation. In In Proceedings of Association for
Machine Translation in the Americas, pages 223?
231.
Jrg Tiedemann. 2012. Parallel data, tools and
interfaces in opus. In Proceedings of the 8th
International Conference on Language Resources
and Evaluation (LREC 2012), pages 2868?2872.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In
Proceedings of HLT-NAACL 2004: Short Papers,
pages 101?104.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars:
softening syntactic constraints to improve statistical
machine translation. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 236?244.
Dekai Wu and Hongsing Wong Hkust. 1998. Machine
translation with a stochastic grammatical channel.
In Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics and
17th International Conference on Computational
Linguistics - Volume 2, pages 1408?1415.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
Xinyan Xiao, Jinsong Su, Yang Liu, Qun Liu, and
Shouxun Lin. 2011. An orientation model
for hierarchical phrase-based translation. In
Proceedings of the 2011 International Conference
on Asian Language Processing, pages 165?168.
Hao Zhang, Daniel Gildea, and David Chiang.
2008. Extracting synchronous grammar rules
from word-level alignments in linear time. In
Proceedings of the 22nd International Conference
on Computational Linguistics - Volume 1, pages
1081?1088.
Bowen Zhou, Bing Xiang, Xiaodan Zhu, and Yuqing
Gao. 2008. Prior derivation models for formally
syntax-based translation using linguistically syntac-
tic parsing and tree kernels. In Proceedings of
the ACL-08: HLT Second Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages
19?27.
Andreas Zollmann and Ashish Venugopal. 2006.
Syntax augmented machine translation via chart
parsing. In NAACL 2006 - Workshop on statistical
machine translation, June.
21
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 138?147,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Evaluating Word Order Recursively over Permutation-Forests
Milo
?
s Stanojevi
?
c and Khalil Sima?an
Institute for Logic, Language and Computation
University of Amsterdam
Science Park 107, 1098 XG Amsterdam, The Netherlands
{m.stanojevic,k.simaan}@uva.nl
Abstract
Automatically evaluating word order of
MT system output at the sentence-level is
challenging. At the sentence-level, ngram
counts are rather sparse which makes it
difficult to measure word order quality ef-
fectively using lexicalized units. Recent
approaches abstract away from lexicaliza-
tion by assigning a score to the permuta-
tion representing how word positions in
system output move around relative to a
reference translation. Metrics over per-
mutations exist (e.g., Kendal tau or Spear-
man Rho) and have been shown to be
useful in earlier work. However, none
of the existing metrics over permutations
groups word positions recursively into
larger phrase-like blocks, which makes it
difficult to account for long-distance re-
ordering phenomena. In this paper we ex-
plore novel metrics computed over Per-
mutation Forests (PEFs), packed charts
of Permutation Trees (PETs), which are
tree decompositions of a permutation into
primitive ordering units. We empirically
compare PEFs metric against five known
reordering metrics on WMT13 data for ten
language pairs. The PEFs metric shows
better correlation with human ranking than
the other metrics almost on all language
pairs. None of the other metrics exhibits
as stable behavior across language pairs.
1 Introduction
Evaluating word order (also reordering) in MT is
one of the main ingredients in automatic MT eval-
uation, e.g., (Papineni et al., 2002; Denkowski
and Lavie, 2011). To monitor progress on eval-
uating reordering, recent work explores dedicated
reordering evaluation metrics, cf. (Birch and Os-
borne, 2011; Isozaki et al., 2010; Talbot et al.,
2011). Existing work computes the correlation be-
tween the ranking of the outputs of different sys-
tems by an evaluation metric to human ranking, on
e.g., the WMT evaluation data.
For evaluating reordering, it is necessary to
word align system output with the correspond-
ing reference translation. For convenience, a 1:1
alignment (a permutation) is induced between the
words on both sides (Birch and Osborne, 2011),
possibly leaving words unaligned on either side.
Existing work then concentrates on defining mea-
sures of reordering over permutations, cf. (Lap-
ata, 2006; Birch and Osborne, 2011; Isozaki et al.,
2010; Talbot et al., 2011). Popular metrics over
permutations are: Kendall?s tau, Spearman, Ham-
ming distance, Ulam and Fuzzy score. These met-
rics treat a permutation as a flat sequence of inte-
gers or blocks, disregarding the possibility of hier-
archical grouping into phrase-like units, making it
difficult to measure long-range order divergence.
Next we will show by example that permutations
also contain latent atomic units that govern the re-
cursive reordering of phrase-like units. Account-
ing for these latent reorderings could actually be
far simpler than the flat view of a permutation.
Isozaki et al. (2010) argue that the conventional
metrics cannot measure well the long distance
reordering between an English reference sentence
?A because B? and a Japanese-English hypothesis
translation ?B because A?, where A and B are
blocks of any length with internal monotonic
alignments. In this paper we explore the idea of
factorizing permutations into permutation-trees
(PETs) (Gildea et al., 2006) and defining new
138
?2,4,1,3?
2 ?1,2?
4 ?1,2?
5 6
1 3
Figure 1: A permutation tree for ?2, 4, 5, 6, 1, 3?
tree-based reordering metrics which aims at
dealing with this type of long range reorderings.
For the Isozaki et al. (2010) Japanese-English
example, there are two PETs (when leaving A and
B as encapsulated blocks):
?2,1?
A ?2,1?
because B
?2,1?
?2,1?
A because
B
Our PET-based metrics interpolate the scores over
the two inversion operators ?2, 1? with the internal
scores for A and B, incorporating a weight
for subtree height. If both A and B are large
blocks, internally monotonically (also known as
straight) aligned, then our measure will not count
every single reordering of a word in A or B,
but will consider this case as block reordering.
From a PET perspective, the distance of the
reordering is far smaller than when looking at a
flat permutation. But does this hierarchical view
of reordering cohere better with human judgement
than string-based metrics?
The example above also shows that a permuta-
tion may factorize into different PETs, each corre-
sponding to a different segmentation of a sentence
pair into phrase-pairs. In this paper we introduce
permutation forests (PEFs); a PEF is a hypergraph
that compactly packs the set of PETs that factorize
a permutation.
There is yet a more profoud reasoning behind
PETs than only accounting for long-range reorder-
ings. The example in Figure 1 gives the flavor of
PETs. Observe how every internal node in this
PET dominates a subtree whose fringe
1
is itself a
permutation over an integer sub-range of the orig-
inal permutation. Every node is decorated with a
permutation over the child positions (called oper-
ator). For example ?4, 5, 6? constitutes a contigu-
ous range of integers (corresponding to a phrase
pair), and hence will be grouped into a subtree;
1
Ordered sequence of leaf nodes.
which in turn can be internally re-grouped into a
binary branching subtree. Every node in a PET is
minimum branching, i.e., the permutation factor-
izes into a minimum number of adjacent permuta-
tions over integer sub-ranges (Albert and Atkin-
son, 2005). The node operators in a PET are
known to be the atomic building blocks of all per-
mutations (called primal permutations). Because
these are building atomic units of reordering, it
makes sense to want to measure reordering as a
function of the individual cost of these operators.
In this work we propose to compute new reorder-
ing measures that aggregate over the individual
node-permutations in these PETs.
While PETs where exploited rather recently for
extracting features used in the BEER metric sys-
tem description (Stanojevi?c and Sima?an, 2014) in
the official WMT 2014 competition, this work is
the first to propose integral recursive metrics over
PETs and PEFs solely for measuring reordering
(as opposed to individual non-recursive features in
a full metric that measures at the same time both
fluency and adequacy). We empirically show that
a PEF-based evaluation measure correlates better
with human rankings than the string-based mea-
sures on eight of the ten language pairs in WMT13
data. For the 9
th
language pair it is close to best,
and for the 10
th
(English-Czech) we find a likely
explanation in the Findings of the 2013 WMT (Bo-
jar et al., 2013). Crucially, the PEF-based mea-
sure shows more stable ranking across language
pairs than any of the other measures. The metric
is available online as free software
2
.
2 Measures on permutations: Baselines
In (Birch and Osborne, 2010; Birch and Osborne,
2011) Kendall?s tau and Hamming distance are
combined with unigram BLEU (BLEU-1) leading
to LRscore showing better correlation with human
judgment than BLEU-4. Birch et al. (2010) ad-
ditionally tests Ulam distance (longest common
subsequence ? LCS ? normalized by the permu-
tation length) and the square root of Kendall?s tau.
Isozaki et al. (2010) presents a similar approach
to (Birch and Osborne, 2011) additionally test-
ing Spearman rho as a distance measure. Talbot
et al. (2011) extracts a reordering measure from
METEOR (Denkowski and Lavie, 2011) dubbed
Fuzzy Reordering Score and evaluates it on MT
reordering quality.
2
https://github.com/stanojevic/beer
139
For an evaluation metric we need a function
which would have the standard behaviour of evalu-
ation metrics - the higher the score the better. Bel-
low we define the baseline metrics that were used
in our experiments.
Baselines A permutation over [1..n] (subrange
of the positive integers where n > 1) is a bijective
function from [1..n] to itself. To represent permu-
tations we will use angle brackets as in ?2, 4, 3, 1?.
Given a permutation pi over [1..n], the notation pi
i
(1 ? i ? n) stands for the integer in the i
th
posi-
tion in pi; pi(i) stands for the index of the position
in pi where integer i appears; and pi
j
i
stands for the
(contiguous) sub-sequence of integers pi
i
, . . . pi
j
.
The definitions of five commonly used met-
rics over permutations are shown in Figure 2.
In these definitions, we use LCS to stand for
Longest Common Subsequence, and Kronecker
?[a] which is 1 if (a == true) else zero, and
A
n
1
= ?1, ? ? ? , n? which is the identity permuta-
tion over [1..n]. We note that all existing metrics
kendall(pi) =
?
n?1
i=1
?
n
j=i+1
?[pi(i) < pi(j)]
(n
2
? n)/2
hamming(pi) =
?
n
i=1
?[pi
i
== i]
n
spearman(pi) = 1?
3
?
n
i=1
(pi
i
? i)
2
n(n
2
? 1)
ulam(pi) =
LCS(pi,A
n
1
)? 1
n? 1
fuzzy(pi) = 1?
c? 1
n? 1
where c is # of monotone sub-permutations
Figure 2: Five commonly used metrics over per-
mutations
are defined directly over flat string-level permuta-
tions. In the next section we present an alternative
view of permutations are compositional, recursive
tree structures.
3 Measures on Permutation Forests
Existing work, e.g., (Gildea et al., 2006), shows
how to factorize any permutation pi over [1..n]
into a canonical permutation tree (PET). Here we
will summarize the relevant aspects and extend
PETs to permutation forests (PEFs).
A non-empty sub-sequence pi
j
i
of a permutation
pi is isomorphic with a permutation over [1..(j ?
i + 1)] iff the set {pi
i
, . . . , pi
j
} is a contiguous
range of positive integers. We will use the term
a sub-permutation of pi to refer to a subsequence
of pi that is isomorphic with a permutation. Note
that not every subsequence of a permutation pi is
necessarily isomorphic with a permutation, e.g.,
the subsequence ?3, 5? of ?1, 2, 3, 5, 4? is not a
sub-permutation. One sub-permutation pi
1
of pi is
smaller than another sub-permutation pi
2
of pi iff
every integer in pi
1
is smaller than all integers in
pi
2
. In this sense we can put a full order on non-
overlapping sub-permutations of pi and rank them
from the smallest to the largest.
For every permutation pi there is a minimum
number of adjacent sub-permutations it can be fac-
torized into (see e.g., (Gildea et al., 2006)). We
will call this minimum number the arity of pi and
denote it with a(pi) (or simply a when pi is un-
derstood from the context). For example, the arity
of pi = ?5, 7, 4, 6, 3, 1, 2? is a = 2 because it can
be split into a minimum of two sub-permutations
(Figure 3), e.g. ?5, 7, 4, 6, 3? and ?1, 2? (but alter-
natively also ?5, 7, 4, 6? and ?3, 1, 2?). In contrast,
pi = ?2, 4, 1, 3? (also known as the Wu (1997) per-
mutation) cannot be split into less than four sub-
permutations, i.e., a = 4. Factorization can be
applied recursively to the sub-permutations of pi,
resulting in a tree structure (see Figure 3) called a
permutation tree (PET) (Gildea et al., 2006; Zhang
and Gildea, 2007; Maillette de Buy Wenniger and
Sima?an, 2011).
Some permutations factorize into multiple alter-
native PETs. For pi = ?4, 3, 2, 1? there are five
PETs shown in Figure 3. The alternative PETs
can be packed into an O(n
2
) permutation forest
(PEF). For many computational purposes, a sin-
gle canonical PET is sufficient, cf. (Gildea et al.,
2006). However, while different PETs of pi exhibit
the same reordering pattern, their different binary
branching structures might indicate important dif-
ferences as we show in our experiments.
A permutation forest (akin to a parse forest)
F for pi (over [1..n]) is a data structure consisting
of a subset of {[[i, j, I
j
i
, O
j
i
]] | 0 ? i ? j ? n},
where I
j
i
is a (possibly empty) set of inferences
(sets of split points) for pi
j
i+1
and O
j
i
is an oper-
ator shared by all inferences of pi
j
i+1
. If pi
j
i+1
is
a sub-permutation and it has arity a ? (j ? (i +
140
?2,1?
?2,1?
?2,4,1,3?
5 7 4 6
3
?1,2?
1 2
?2,1?
4 ?2,1?
3 ?2,1?
2 1
?2,1?
4 ?2,1?
?2,1?
3 2
1
?2,1?
?2,1?
4 3
?2,1?
2 1
?2,1?
?2,1?
?2,1?
4 3
2
1
?2,1?
?2,1?
4 ?2,1?
3 2
1
Figure 3: A PET for pi = ?5, 7, 4, 6, 3, 1, 2?. And five different PETs for pi = ?4, 3, 2, 1?.
1)), then each inference consists of a a ? 1-tuple
[l
1
, . . . , l
a?1
], where for each 1 ? x ? (a? 1), l
x
is a ?split point? which is given by the index of the
last integer in the x
th
sub-permutation in pi. The
permutation of the a sub-permutations (?children?
of pi
j
i+1
) is stored in O
j
i
and it is the same for all
inferences of that span (Zhang et al., 2008).
?2,1?
4
3 2 1
?2,1?
4 3 2 1
?2,1?
4 3 2
1
Figure 4: The factorizations of pi = ?4, 3, 2, 1?.
Let us exemplify the inferences on pi =
?4, 3, 2, 1? (see Figure 4) which factorizes into
pairs of sub-permutations (a = 2): a split point
can be at positions with index l
1
? {1, 2, 3}.
Each of these split points (factorizations) of pi will
be represented as an inference for the same root
node which covers the whole of pi (placed in entry
[0, 4]); the operator of the inference here consists
of the permutation ?2, 1? (swapping the two ranges
covered by the children sub-permutations) and in-
ference consists of a? 1 indexes l
1
, . . . , l
a?1
sig-
nifying the split points of pi into sub-permutations:
since a = 2 for pi, then a single index l
1
?
{1, 2, 3} is stored with every inference. For the
factorization ((4, 3), (2, 1)) the index l
1
= 2 sig-
nifying that the second position is a split point into
?4, 3? (stored in entry [0, 2]) and ?2, 1? (stored in
entry [2, 4]). For the other factorizations of pi sim-
ilar inferences are stored in the permutation forest.
Figure 5 shows a simple top-down factorization
algorithm which starts out by computing the ar-
ity a using function a(pi). If a = 1, a single leaf
node is stored with an empty set of inferences. If
a > 1 then the algorithm computes all possible
factorizations of pi into a sub-permutations (a se-
quence of a? 1 split points) and stores their infer-
ences together as I
j
i
and their operator O
j
i
asso-
ciated with a node in entry [[i, j, I
j
i
, O
j
i
]]. Subse-
quently, the algorithm applies recursively to each
sub-permutation. Efficiency is a topic beyond
the scope of this paper, but this naive algorithm
has worst case time complexity O(n
3
), and when
computing only a single canonical PET this can be
O(n) (see e.g., (Zhang and Gildea, 2007)).
Function PEF (i, j, pi,F);
# Args: sub-perm. pi over [i..j] and forest F
Output: Parse-Forest F(pi) for pi;
begin
if ([[i, j, ?]] ? F) then return F ; #memoization
a := a(pi);
if a = 1 return F := F ? {[[i, j, ?]]};
For each set of split points {l
1
, . . . , l
a?1
} do
O
j
i
:= RankListOf(pi
l
1
(l
0
+1)
, pi
l
2
(l
1
+1)
, . . . , pi
l
a
(l
a?1
+1)
);
I
j
i
:= I
j
i
? [l
1
, . . . , l
a?1
];
For each pi
v
? {pi
l
1
l
0
+1
, pi
l
2
(l
1
+1)
, . . . , pi
l
a
(l
a?1
+1)
} do
F := F ? PermForest(pi
v
);
F := F ? {[[i, j, I
j
i
, O
j
i
]]};
Return F ;
end;
Figure 5: Pseudo-code of permutation-forest fac-
torization algorithm. Function a(pi) returns the ar-
ity of pi. Function RankListOf(r
1
, . . . , r
m
) re-
turns the list of rank positions (i.e., a permutation)
of sub-permutations r
1
, . . . , r
m
after sorting them
smallest first. The top-level call to this algorithm
uses pi, i = 0, j = n and F = ?.
Our measure (PEFscore) uses a function
opScore(p) which assigns a score to a given oper-
ator, which can be instantiated to any of the exist-
ing scoring measures listed in Section 2, but in this
case we opted for a very simple function which
gives score 1 to monotone permutation and score
0 to any other permutation.
Given an inference l ? I
j
i
where l =
[l
1
, . . . , l
a?1
], we will use the notation l
x
to refer
to split point l
x
in l where 1 ? x ? (a ? 1), with
the convenient boundary assumption that l
0
= i
and l
a
= j.
141
PEFscore(pi) = ?
node
(0, n, PEF (pi))
?
node
(i, j,F) =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
if (I
j
i
== ?) then 1
else if (a(pi
j
i+1
) = j ? i) then opScore(O
j
i
)
else ? ? opScore(O
j
i
) + (1? ?)?
?
l?I
j
i
?
inf
(l,F ,a(pi
j
i+1
))
|I
j
i
|
? ?? ?
Avg. inference score over I
j
i
?
inf
(l,F , a) =
?
a
x=1
?[l
x
?l
x?1
>1]??
node
(l
(x?1)
,l
x
,F)
?
a
x=1
?[l
x
?l
(x?1)
>1]
? ?? ?
Avg. score for non-terminal children
opScore(p) =
{
if (p == ?1, 2?) then 1
else 0
Figure 6: The PEF Score
The PEF-score, PEFscore(pi) in Figure 6,
computes a score for the single root node
[[0, n, I
n
0
, O
n
0
]]) in the permutation forest. This
score is the average inference score ?
inf
over all
inferences of this node. The score of an inference
?
inf
interpolates (?) between the opScore of the
operator in the current span and (1? ?) the scores
of each child node. The interpolation parameter ?
can be tuned on a development set.
The PET-score (single PET) is a simplification
of the PEF-score where the summation over all in-
ferences of a node
?
l?I
j
i
in ?
node
is replaced by
?Select a canonical l ? I
j
i
?.
4 Experimental setting
Data The data that was used for experiments are
human rankings of translations from WMT13 (Bo-
jar et al., 2013). The data covers 10 language pairs
with a diverse set of systems used for translation.
Each human evaluator was presented with 5 differ-
ent translations, source sentence and a reference
translation and asked to rank system translations
by their quality (ties were allowed).
3
Meta-evaluation The standard way for doing
meta-evaluation on the sentence level is with
Kendall?s tau correlation coefficient (Callison-
Burch et al., 2012) computed on the number of
times an evaluation metric and a human evaluator
agree (and disagree) on the rankings of pairs of
3
We would like to extend our work also to English-
Japanese but we do not have access to such data at the mo-
ment. In any case, the WMT13 data is the largest publicly
available data of this kind.
translations. We extract pairs of translations from
human evaluated data and compute their scores
with all metrics. If the ranking assigned by a met-
ric is the same as the ranking assigned by a hu-
man evaluator then that pair is considered concor-
dant, otherwise it is a discordant pair. All pairs
which have the same score by the metric or are
judged as ties by human evaluators are not used
in meta-evaluation. The formula that was used for
computing Kendall?s tau correlation coefficient is
shown in Equation 1. Note that the formula for
Kendall tau rank correlation coefficient that is used
in meta-evaluation is different from the Kendall
tau similarity function used for evaluating permu-
tations. The values that it returns are in the range
[?1, 1], where ?1 means that order is always op-
posite from the human judgment while the value 1
means that metric ranks the system translations in
the same way as humans do.
? =
#concordant pairs?#discordant pairs
#concordant pairs+#discordant pairs
(1)
Evaluating reordering Since system transla-
tions do not differ only in the word order but also
in lexical choice, we follow Birch and Osborne
(2010) and interpolate the score given by each re-
ordering metric with the same lexical score. For
lexical scoring we use unigram BLEU. The param-
eter that balances the weights for these two metrics
? is chosen to be 0.5 so it would not underesti-
mate the lexical differences between translations
(?  0.5) but also would not turn the whole met-
ric into unigram BLEU (?  0.5). The equation
142
for this interpolation is shown in Equation 2.
4
FullMetric(ref, sys) = ? lexical(ref, sys) +
(1? ?)? bp(|ref |, |pi|)? ordering(pi) (2)
Where pi(ref, sys) is the permutation represent-
ing the word alignment from sys to ref . The ef-
fect of ? on the German-English evaluation is vis-
ible on Figure 7. The PET and PEF measures have
an extra parameter ? that gives importance to the
long distance errors that also needs to be tuned. On
Figure 8 we can see the effect of ? on German-
English for ? = 0.5. For all language pairs for
? = 0.6 both PETs and PEFs get good results so
we picked that as value for ? in our experiments.
Figure 7: Effect of ? on German-English evalua-
tion for ? = 0.6
Choice of word alignments The issue we did
not discuss so far is how to find a permutation
from system and reference translations. One way
is to first get alignments between the source sen-
tence and the system translation (from a decoder
or by automatically aligning sentences), and also
alignments between the source sentence and the
reference translation (manually or automatically
aligned). Subsequently we must make those align-
ments 1-to-1 and merge them into a permutation.
That is the approach that was followed in previ-
ous work (Birch and Osborne, 2011; Talbot et al.,
4
Note that for reordering evaluation it does not make
sense to tune ? because that would blur the individual contri-
butions of reordering and adequacy during meta evaluation,
which is confirmed by Figure 7 showing that ?  0.5 leads
to similar performance for all metrics.
Figure 8: Effect of ? on German-English evalua-
tion for ? = 0.5
2011). Alternatively, we may align system and ref-
erence translations directly. One of the simplest
ways to do that is by finding exact matches be-
tween words and bigrams between system and ref-
erence translation as done in (Isozaki et al., 2010).
The way we align system and reference transla-
tions is by using the aligner supplied with ME-
TEOR (Denkowski and Lavie, 2011) for finding
1-to-1 alignments which are later converted to a
permutation. The advantage of this method is that
it can do non-exact matching by stemming or us-
ing additional sources for semantic similarity such
as WordNets and paraphrase tables. Since we will
not have a perfect permutation as input, because
many words in the reference or system transla-
tions might not be aligned, we introduce a brevity
penalty (bp(?, ?) in Equation 2) for the ordering
component as in (Isozaki et al., 2010). The brevity
penalty is the same as in BLEU with the small
difference that instead of taking the length of sys-
tem and reference translation as its parameters, it
takes the length of the system permutation and the
length of the reference.
5 Empirical results
The results are shown in Table 1 and Table 2.
These scores could be much higher if we used
some more sophisticated measure than unigram
BLEU for the lexical part (for example recall is
very useful in evaluation of the system translations
(Lavie et al., 2004)). However, this is not the issue
here since our goal is merely to compare different
ways to evaluate word order. All metrics that we
tested have the same lexical component, get the
same permutation as their input and have the same
value for ?.
143
E
n
g
l
i
s
h
-
C
z
e
c
h
E
n
g
l
i
s
h
-
S
p
a
n
i
s
h
E
n
g
l
i
s
h
-
G
e
r
m
a
n
E
n
g
l
i
s
h
-
R
u
s
s
i
a
n
E
n
g
l
i
s
h
-
F
r
e
n
c
h
Kendall 0.16 0.170 0.183 0.193 0.218
Spearman 0.157 0.170 0.181 0.192 0.215
Hamming 0.150 0.163 0.168 0.187 0.196
FuzzyScore 0.155 0.166 0.178 0.189 0.215
Ulam 0.159 0.170 0.181 0.189 0.221
PEFs 0.156 0.173 0.185 0.196 0.219
PETs 0.157 0.165 0.182 0.195 0.216
Table 1: Sentence level Kendall tau scores for
translation out of English with ? = 0.5 and ? =
0.6
C
z
e
c
h
-
E
n
g
l
i
s
h
S
p
a
n
i
s
h
-
E
n
g
l
i
s
h
G
e
r
m
a
n
-
E
n
g
l
i
s
h
R
u
s
s
i
a
n
-
E
n
g
l
i
s
h
F
r
e
n
c
h
-
E
n
g
l
i
s
h
Kendall 0.196 0.265 0.235 0.173 0.223
Spearman 0.199 0.265 0.236 0.173 0.222
Hamming 0.172 0.239 0.215 0.157 0.206
FuzzyScore 0.184 0.263 0.228 0.169 0.216
Ulam 0.188 0.264 0.232 0.171 0.221
PEFs 0.201 0.265 0.237 0.181 0.228
PETs 0.200 0.264 0.234 0.174 0.221
Table 2: Sentence level Kendall tau scores for
translation into English with ? = 0.5 and ? = 0.6
5.1 Does hierarchical structure improve
evaluation?
The results in Tables 1, 2 and 3 suggest that the
PEFscore which uses hierarchy over permutations
outperforms the string based permutation metrics
in the majority of the language pairs. The main
exception is the English-Czech language pair in
which both PETs and PEFs based metric do not
give good results compared to some other met-
rics. For discussion about English-Czech look at
the section 6.1.
5.2 Do PEFs help over one canonical PET?
From Figures 9 and 10 it is clear that using all
permutation trees instead of only canonical ones
makes the metric more stable in all language pairs.
Not only that it makes results more stable but it
metric avg rank avg Kendall
PEFs 1.6 0.2041
Kendall 2.65 0.2016
Spearman 3.4 0.201
PETs 3.55 0.2008
Ulam 4 0.1996
FuzzyScore 5.8 0.1963
Hamming 7 0.1853
Table 3: Average ranks and average Kendall
scores for each tested metrics over all language
pairs
Figure 9: Plot of scaled Kendall tau correlation for
translation from English
also improves them in all cases except in English-
Czech where both PETs and PEFs perform badly.
The main reason why PEFs outperform PETs is
that they encode all possible phrase segmentations
of monotone and inverted sub-permutations. By
giving the score that considers all segmentations,
PEFs also include the right segmentation (the one
perceived by human evaluators as the right seg-
mentation), while PETs get the right segmentation
only if the right segmentation is the canonical one.
5.3 Is improvement consistent over language
pairs?
Table 3 shows average rank (metric?s position af-
ter sorting all metrics by their correlation for each
language pair) and average Kendall tau correlation
coefficient over the ten language pairs. The table
shows clearly that the PEFs metric outperforms all
other metrics. To make it more visible how met-
rics perform on the different language pairs, Fig-
ures 9 and 10 show Kendall tau correlation co-
efficient scaled between the best scoring metric
for the given language (in most cases PEFs) and
144
Figure 10: Plot of scaled Kendall tau correlation
for translation into English
the worst scoring metric (in all cases Hamming
score). We can see that, except in English-Czech,
PEFs are consistently the best or second best (only
in English-French) metric in all language pairs.
PETs are not stable and do not give equally good
results in all language pairs. Hamming distance
is without exception the worst metric for evalua-
tion since it is very strict about positioning of the
words (it does not take relative ordering between
words into account). Kendall tau is the only string
based metric that gives relatively good scores in
all language pairs and in one (English-Czech) it is
the best scoring one.
6 Further experiments and analysis
So far we have shown that PEFs outperform the
existing metrics over the majority of language
pairs. There are two pending issues to discuss.
Why is English-Czech seemingly so difficult?
And does preferring inversion over non-binary
branching correlate better with human judgement.
6.1 The results on English-Czech
The English-Czech language pair turned out to
be the hardest one to evaluate for all metrics.
All metrics that were used in the meta-evaluation
that we conducted give much lower Kendall tau
correlation coefficient compared to the other lan-
guage pairs. The experiments conducted by other
researchers on the same dataset (Mach?a?cek and
Bojar, 2013), using full evaluation metrics, also
get far lower Kendall tau correlation coefficient
for English-Czech than for other language pairs.
In the description of WMT13 data that we used
(Bojar et al., 2013), it is shown that annotator-
agreement for English-Czech is a few times lower
than for other languages. English-Russian, which
is linguistically similar to English-Czech, does
not show low numbers in these categories, and is
one of the language pairs where our metrics per-
form the best. The alignment ratio is equally high
between English-Czech and English-Russian (but
that does not rule out the possibility that the align-
ments are of different quality). One seemingly
unlikely explanation is that English-Czech might
be a harder task in general, and might require a
more sophisticated measure. However, the more
plausible explanation is that the WMT13 data for
English-Czech is not of the same quality as other
language pairs. It could be that data filtering, for
example by taking only judgments for which many
evaluators agree, could give more trustworthy re-
sults.
6.2 Is inversion preferred over non-binary
branching?
Since our original version of the scoring function
for PETs and PEFs on the operator level does not
discriminate between kinds of non-monotone op-
erators (all non-monotone get zero as a score) we
also tested whether discriminating between inver-
sion (binary) and non-binary operators make any
difference.
E
n
g
l
i
s
h
-
C
z
e
c
h
E
n
g
l
i
s
h
-
S
p
a
n
i
s
h
E
n
g
l
i
s
h
-
G
e
r
m
a
n
E
n
g
l
i
s
h
-
R
u
s
s
i
a
n
E
n
g
l
i
s
h
-
F
r
e
n
c
h
PEFs ? = 0.0 0.156 0.173 0.185 0.196 0.219
PEFs ? = 0.5 0.157 0.175 0.183 0.195 0.219
PETs ? = 0.0 0.157 0.165 0.182 0.195 0.216
PETs ? = 0.5 0.158 0.165 0.183 0.195 0.217
Table 4: Sentence level Kendall tau score for
translation out of English different ? with ? = 0.5
and ? = 0.6
Intuitively, we might expect that inverted binary
operators are preferred by human evaluators over
non-binary ones. So instead of assigning zero as a
score to inverted nodes we give them 0.5, while for
non-binary nodes we remain with zero. The ex-
periments with the inverted operator scored with
0.5 (i.e., ? = 0.5) are shown in Tables 4 and 5.
The results show that there is no clear improve-
ment by distinguishing between the two kinds of
145
C
z
e
c
h
-
E
n
g
l
i
s
h
S
p
a
n
i
s
h
-
E
n
g
l
i
s
h
G
e
r
m
a
n
-
E
n
g
l
i
s
h
R
u
s
s
i
a
n
-
E
n
g
l
i
s
h
F
r
e
n
c
h
-
E
n
g
l
i
s
h
PEFs ? = 0.0 0.201 0.265 0.237 0.181 0.228
PEFs ? = 0.5 0.201 0.264 0.235 0.179 0.227
PETs ? = 0.0 0.200 0.264 0.234 0.174 0.221
PETs ? = 0.5 0.202 0.263 0.235 0.176 0.224
Table 5: Sentence level Kendall tau score for
translation into English for different ? with ? =
0.5 and ? = 0.6
non-monotone operators on the nodes.
7 Conclusions
Representing order differences as compact permu-
tation forests provides a good basis for develop-
ing evaluation measures of word order differences.
These hierarchical representations of permutations
bring together two crucial elements (1) grouping
words into blocks, and (2) factorizing reorder-
ing phenomena recursively over these groupings.
Earlier work on MT evaluation metrics has of-
ten stressed the importance of the first ingredient
(grouping into blocks) but employed it merely in a
flat (non-recursive) fashion. In this work we pre-
sented novel metrics based on permutation trees
and forests (the PETscore and PEFscore) where
the second ingredient (factorizing reordering phe-
nomena recursively) plays a major role. Permuta-
tion forests compactly represent all possible block
groupings for a given permutation, whereas per-
mutation trees select a single canonical grouping.
Our experiments with WMT13 data show that our
PEFscore metric outperforms the existing string-
based metrics on the large majority of language
pairs, and in the minority of cases where it is not
ranked first, it ranks high. Crucially, the PEFs-
core is by far the most stable reordering score over
ten language pairs, and works well also for lan-
guage pairs with long range reordering phenom-
ena (English-German, German-English, English-
Russian and Russian-English).
Acknowledgments
This work is supported by STW grant nr. 12271
and NWO VICI grant nr. 277-89-002. We thank
TAUS and the other DatAptor project User Board
members. We also thank Ivan Titov for helpful
comments on the ideas presented in this paper.
References
Michael H. Albert and Mike D. Atkinson. 2005. Sim-
ple permutations and pattern restricted permutations.
Discrete Mathematics, 300(1-3):1?15.
Alexandra Birch and Miles Osborne. 2010. LRscore
for Evaluating Lexical and Reordering Quality in
MT. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 327?332, Uppsala, Sweden, July. Association
for Computational Linguistics.
Alexandra Birch and Miles Osborne. 2011. Reorder-
ing Metrics for MT. In Proceedings of the Associ-
ation for Computational Linguistics, Portland, Ore-
gon, USA. Association for Computational Linguis-
tics.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating re-
ordering. Machine Translation, pages 1?12.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
Daniel Gildea, Giorgio Satta, and Hao Zhang. 2006.
Factoring Synchronous Grammars by Sorting. In
ACL.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?10, pages 944?952, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Mirella Lapata. 2006. Automatic Evaluation of In-
formation Ordering: Kendall?s Tau. Computational
Linguistics, 32(4):471?484.
146
Alon Lavie, Kenji Sagae, and Shyamsundar Jayara-
man. 2004. The significance of recall in auto-
matic metrics for MT evaluation. In Proceedings of
the Sixth Conference of the Association for Machine
Translation in the Americas.
Matou?s Mach?a?cek and Ond?rej Bojar. 2013. Results
of the WMT13 Metrics Shared Task. In Proceed-
ings of the Eighth Workshop on Statistical Machine
Translation, pages 45?51, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Gideon Maillette de Buy Wenniger and Khalil Sima?an.
2011. Hierarchical Translation Equivalence over
Word Alignments. In ILLC Prepublication Series,
PP-2011-38. University of Amsterdam.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL?02, pages 311?318, Philadelphia, PA, USA.
Milo?s Stanojevi?c and Khalil Sima?an. 2014. BEER:
BEtter Evaluation as Ranking. In Proceedings of the
Ninth Workshop on Statistical Machine Translation,
pages 414?419, Baltimore, Maryland, USA, June.
Association for Computational Linguistics.
David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Jason
Katz-Brown, Masakazu Seno, and Franz Och. 2011.
A Lightweight Evaluation Framework for Machine
Translation Reordering. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
12?21, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 3(23):377?403.
Hao Zhang and Daniel Gildea. 2007. Factorization
of Synchronous Context-Free Grammars in Linear
Time. In NAACL Workshop on Syntax and Structure
in Statistical Translation (SSST), pages 25?32.
Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting synchronous grammar rules from word-
level alignments in linear time. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics-Volume 1, pages 1081?1088. As-
sociation for Computational Linguistics.
147
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 157?165,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
How Synchronous are Adjuncts in Translation Data?
Sophie Arnoult
ILLC
University of Amsterdam
s.i.arnoult@uva.nl
Khalil Sima?an
ILLC
University of Amsterdam
k.simaan@uva.nl
Abstract
The argument-adjunct distinction is cen-
tral to most syntactic and semantic the-
ories. As optional elements that refine
(the meaning of) a phrase, adjuncts are
important for recursive, compositional ac-
counts of syntax, semantics and transla-
tion. In formal accounts of machine trans-
lation, adjuncts are often treated as modi-
fiers applying synchronously in source and
target derivations. But how well can the
assumption of synchronous adjunction ex-
plain translation equivalence in actual par-
allel data? In this paper we present the
first empirical study of translation equiva-
lence of adjuncts on a variety of French-
English parallel corpora, while varying
word alignments so we can gauge the ef-
fect of errors in them. We show that for
proper measurement of the types of trans-
lation equivalence of adjuncts, we must
work with non-contiguous, many-to-many
relations, thereby amending the traditional
Direct Correspondence Assumption. Our
empirical results show that 70% of manu-
ally identified adjuncts have adjunct trans-
lation equivalents in training data, against
roughly 50% for automatically identified
adjuncts.
1 Introduction
Most syntactic and semantic theories agree on the
argument-adjunct distinction, although they vary
on the specifics of this distinction. Common to
these theories is that adjunction is a central de-
vice for language recursion, as adjunction modi-
fies initial but complete sentences by adding op-
tional phrases; adjunction also contributes to se-
mantic compositionality, albeit in various ways,
as syntactic adjuncts may take different seman-
tic roles. Shieber and Schabes (1990) transfer the
role of adjuncts from monolingual syntax (Joshi
et al., 1975) to the realm of translation equiva-
lence using a Synchronous Tree Adjoining Gram-
mars (STAG), and propose to view adjunction as
a synchronous operation for recursive, composi-
tional translation. STAG therefore relies substan-
tially on what Hwa (2002) calls the Direct Corre-
spondence Assumption, the notion that semantic
or syntactic relations correspond across a bitext.
We know from various works?notably by Hwa et
al. (2002) for dependency relations, Arnoult and
Sima?an (2012) for adjuncts, and Pad?o and Lap-
ata (2009) and Wu and Fung (2009) for semantic
roles?that the Direct Correspondence Assumption
does not always hold.
A question that has not received much atten-
tion is the degree to which the assumption of
synchronous adjunction is supported in human
translation data. This is crucial for the succes-
ful application of linguistically-motivated STAG,
but attempts at answering this question empirically
are hampered by a variety of difficulties. Lin-
guistic structures may diverge between languages
(Dorr, 1994), translations may be more or less lit-
eral, and annotation resources may be inaccurate,
when they are available at all. Besides, automatic
word alignments are known to be noisy and man-
ual alignments are rather scarse. The work of
Arnoult and Sima?an (2012) reports lower and up-
per bounds of one-to-one adjunct correspondence,
using rather limited resources to identify French
adjuncts making their results not directly applica-
ble for measuring the stability of the synchronous
adjunction assumption.
In this paper we aim at redefining the transla-
tion equivalence of adjuncts in ways that allow us
to report far more accurate bounds on their cross-
linguistic correspondence. In particular, we are in-
terested in measuring adjunct correspondence ro-
bustly, in training data.
Consider for example the sentence pair of Fig-
157
ure 1. Most adjuncts in each sentence translate
as adjuncts in the other sentence, but one of these
translation equivalences appears to be many-to-
many, because of parsing mismatches across the
bitext; both parses and adjunct labellers on both
sides of the bitext must be on par for adjunct trans-
lation equivalences to be established. Besides, one
generally establishes translation equivalence using
word alignments, which may be noisy. Another
factor is that of the degree of translation equiva-
lence in the data in general; while parallel bitexts
express the same meaning, meaning may diverge
locally.
I think that the time A
e1
A
e6
has been A
e7
long
taken A
e2
, for example , too
in handling A
e3
applications A
e4
routine for changes of facilities A
e5
along a pipeline
je crois qu?il a pris A
f1
de temps A
f2
trop
`a ?etudier des demandes A
f3
de changements d?installations A
f4
, A
f5
courantes le long d?un pipe-line par exemple
Figure 1: Example sentence pair
This paper contributes the first study to mea-
sure the degree of adjunction synchronicity: we
derive many-to-many pairings between adjuncts
across a bitext, thus supporting a generic view
of translation equivalence, where meaning can
be expressed by distinct entities and redistributed
freely in translation; practically, this also allows
us to capture equivalence in spite of mismatched
parses. We abstract away from word alignments
to a certain degree, as we directly pair adjuncts
across a bitext, but we still use word alignments?
namely the overlap of adjunct projections with tar-
get adjuncts?to decide on these pairings. We fur-
ther distinguish between adjunct pairings that are
bijective through the word alignment, and other
pairings, where the translation equivalence does
not exactly agree with the word alignment; we
qualify these pairings as weakly equivalent.
Under this new view of adjunct translation
equivalence, we perform measures in French-
English data. We show that adjunction is pre-
served in 40% to 50% of the cases with automati-
cally labelled adjuncts, with differences between
data sets, word aligners and sentence length;
about 25% more adjuncts form weakly translation-
equivalent pairings. With gold adjunct annota-
tions, the proportion of translation-equivalent ad-
juncts increases to 70%.
These results show that adjunct labelling accu-
racy on both sides of the data is crucial for adjunct
alignment, while suggesting that applications that
exploit adjunction can gain from decreasing their
dependence on word alignments and idealized ex-
perimental conditions , and identifying favorable
contexts for adjunct preservation.
2 Alignment-based role pairing
How can one find translation-equivalent adjuncts
using word alignments, without being too con-
strained by the latter? Obviously, adjunct pairs
that are consistent with the word alignments are
translation equivalent, but we also want to be able
to identify translation-equivalent adjuncts that are
not exactly aligned to each other, and also to ac-
cept many-to-many pairings; not only to get lin-
guistically justified discontinuous pairs, as with
the French double negation particle, but also for
robustness with regard to dissimilar attachments
in the French and English parses.
2.1 Translation equivalence under the
alignment-consistency constraint
Consider for instance Figure 2, which represents
a word alignment for part of the sentence pair of
Figure 1. We would like to match
?
f
2
to e?
2
and e?
6
,
?
f
3
to e?
3
,
?
f
4
to e?
5
, and
?
f
5
to e?
6
. If one only pairs
adjuncts that are consistent with the word align-
ment, one obtains only half of these adjunct pairs:
?
?
f
3
, e?
3
? and ?
?
f
4
, e?
5
?; one cannot pair up
?
f
5
and
e?
6
because the latter is also aligned outside of the
former; and one can also not find the equivalence
between
?
f
2
on one hand and e?
2
and e?
6
on the other
hand if one assumes one-to-one correspondence
between adjuncts.
2.2 Translation equivalence through
projection
We align adjuncts across the bitext by projecting
them through the word alignment and finding, for
each adjunct, the shortest adjunct or sequence of
adjuncts that overlaps the most with that adjunct?s
158
`a
?etudier
de
les
demandes
courantes
de
changements
de
installations
le
long
de
un
pipe
-
line
,
par
exemple
i
n
h
a
n
d
l
i
n
g
r
o
u
t
i
n
e
a
p
p
l
i
c
a
t
i
o
n
s
f
o
r
c
h
a
n
g
e
s
o
f
f
a
c
i
l
i
t
i
e
s
a
l
o
n
g
a p
i
p
e
l
i
n
e
, f
o
r
e
x
a
m
p
l
e
,
?
f
2
?
f
3
?
f
4
?
f
5
e?
2
e?
3
e?
4
e?
5
e?
6
Figure 2: Example with word alignment
projection. To prevent source adjuncts from be-
ing aligned to the first target adjunct that sub-
sumes their projection, we also enforce that only
non-overlapping source adjuncts may be aligned
to a same target sequence, as explained in sec-
tion 2.2.1.
This procedure results in a many-to-many align-
ment between adjuncts on either side. We distin-
guish several types of adjunct pairings through this
alignment, which we interpret as divergent, equiv-
alent or weakly equivalent, as described in sec-
tion 2.2.2.
We perform this alignment in both source-target
and target-source directions to measure the pro-
portion of source, respectively target, adjuncts that
fall in each category.
2.2.1 Adjunct pairing procedure
We define the projection of an adjunct ? as the
unique tuple of maximal, non-overlapping phrases
?
n
1
that are aligned to ? through the word align-
ment. Each phrase ?
i
in this tuple is understood
as being extended with possible surrounding un-
aligned positions?phrases are primarily identified
by the aligned positions they cover. And each ?
i
is maximal as any larger phrase distinct from ?
i
would also include (aligned) positions not aligned
to ?. Let I(?
i
) be the set of aligned positions
in each ?
i
, and I(?
n
1
) the set of aligned positions
covered by ?
n
1
.
We align ? to the non-overlapping sequence
of target adjuncts ?
m
1
that has the smallest set of
aligned positions while having the largest over-
lap with ?
n
1
; the overlap of a projection and a tar-
get sequence is the intersection of their respective
sets of aligned positions. For instance in Figure 2,
the projection of
?
f
4
is maximally covered by e?
2
,
e?
4
, and e?
5
; we align the latter to
?
f
4
as it covers
the least aligned positions. In practice, we search
through the tree of target adjuncts for adjuncts that
overlap with ?
n
1
, and for each such adjunct ? we
compare its overlap with ?
n
1
to that of the sequence
of its children ?
k
1
to determine which (of ? or ?
k
1
)
should be part of the final target sequence.
We perform a similar selection on overlapping
source adjuncts that point to the same target se-
quence. For each source adjunct ?, we determine
if its target sequence ?
m
1
is also aligned to adjuncts
dominated by ?, in which case we compare the
overlap of ??s projection with ?
n
1
to that of its chil-
dren in the source adjunct tree to determine which
should be aligned to ?
m
1
. For instance in Figure 2,
e?
4
is aligned to
?
f
2
(when projecting from English
to French), but so is e?
2
; as e?
2
?s projection overlaps
more with
?
f
2
, we discard the alignment between
e?
4
and
?
f
2
.
The final alignments for our example are repre-
sented in Table 1.
Table 1: Adjunct pairings for the alignment of
Figure 2
f ? e e? f
?
f
2
e?
2
, e?
6
e?
2
?
f
2
?
f
3
e?
3
e?
3
?
f
3
?
f
4
e?
5
e?
4
-
?
f
5
e?
6
e?
5
?
f
4
e?
6
?
f
2
2.2.2 Types of adjunct pairings
We distinguish three main classes of adjunct
translation equivalence: divergent, equivalent and
weakly equivalent. We further subdivide each
class into two types, as shown in Table 2. Ad-
junct pairings fall into one of these types depend-
ing on their configuration (unaligned, one-to-one
or many-to-many) and their agreement with the
word alignments. Equivalent types notably differ
from weakly equivalent ones by being bijectively
159
aligned; With the notations of section 2.2.1, two
adjunct sequences ?
n
1
and ?
m
1
with respective pro-
jections ?
n
?
1
and ?
m
?
1
are translation equivalent iff
I(?
n
?
1
) = I(?
m
1
) and I(?
m
?
1
) = I(?
n
1
).
Table 2: Adjunct pairing types
divergent
null empty projection
div no aligned target adjuncts
weakly equivalent
we-nm many-to-many non-bijective
we-11 one-to-one non-bijective
equivalent
eq-nm many-to-many bijective
eq-11 one-to-one bijective
In Table 1, e?
4
?s translation is divergent as it is
not aligned to any adjunct;
?
f
5
and e?
6
are weakly
equivalent as the projection of
?
f
5
does not cover
all the aligned positions of e?
6
. The pairing from
?
f
2
to e?
2
, e?
6
is many-to-many equivalent, and so are
the pairings from e?
2
and e?
6
to
?
f
2
; the remaining
pairings are one-to-one equivalent.
As Table 3 shows, the divergent types null and
div regroup untranslated adjuncts (Example 1)
and divergent adjuncts: Examples (2) and (3) show
cases of conflational divergence (Dorr, 1994), that
appear in different types because of the underly-
ing word alignments; in Example (4), the prepo-
sitional phrase with this task has been wrongly
labelled as an adjunct, leading to a falsely diver-
gent pairing. The weakly-equivalent types we-nm
and we-11 regroup both divergent and equiva-
lent pairings: the adjuncts of Examples (5) and (8)
are aligned by our method to adjuncts that are not
their translation equivalent, the adjunct in Exam-
ple (6) cannot be aligned to its equivalent because
of a parsing error, and the equivalences in Exam-
ples (7) and (9) cannot be identified because of a
word-alignment error. Finally, we show a number
of equivalent pairings (eq-nm and eq-11): in
Example (10), an attachment error in the French
parse induces a many-to-one equivalence where
there should be two one-to-one equivalences; Ex-
amples (11) to (13) show a number of true many-
to-many equivalences, while Examples (14) and
(15) show that adjuncts may be equivalent across a
bitext while belonging to a different syntactic cate-
gory and modifying a different type of phrase (15).
3 Adjunct identification
We identify adjuncts in dependency trees obtained
by conversion from phrase-structure trees: we map
modifier labels to adjuncts, except when the de-
pendent is a closed-class word. For English, we
use the Berkeley parser and convert its output with
the pennconverter (Johansson and Nugues, 2007;
Surdeanu et al., 2008); for French, we use the
Berkeley parser and the functional role labeller of
Candito et al. (2010). The pennconverter with de-
fault options and the French converter make sim-
ilar structural choices concerning the representa-
tion of coordination and the choice of heads.
3.1 English adjuncts
We first identify closed-class words by their POS
tag: CC, DT, EX, IN, POS, PRP, PRP$, RP, SYM,
TO, WDT, WP, WP$, WRB. Punctuation marks,
identified by the P dependency relation, and name
dependencies, identified by NAME, POSTHON, or
TITLE, are also treated as closed-class words.
Adjuncts are identified by the dependency rela-
tion: ADV, APPO, NMOD (except determiners, pos-
sessives and ?of? complements), PRN, AMOD (ex-
cept when the head is labeled with ADV) and PMOD
left of its head. Cardinals, identified by the CD
POS tag, and remaining dependents are classified
as arguments.
3.2 French adjuncts
Closed-class words are identified by the (coarse)
POS tags: C, D, CL, P, PONCT, P+D, PRO. Aux-
iliary verbs, identified by the dependency relations
aux tps and aux pass, are also included.
Adjuncts are identified by the dependency re-
lations mod rel and mod (except if the depen-
dent?s head is a cardinal number, identified by the
s=card label).
3.3 Evaluation
We evaluate adjunct identification accuracy using
a set of 100 English and French sentences, drawn
randomly from the Europarl corpus. A single an-
notator marked adjuncts in both sets, identifying
slightly more than 500 adjuncts in both sets. We
find F scores of 71.3 and 72.2 for English and
French respectively, as summarized in Table 4. We
find that about a quarter of errors are related to
parse attachment, yielding scores of 77.7 and 78.6
if one corrects them.
160
Table 3: Examples of adjunct pairing types
null
(1) it is indeed a great honour vous me faites un grand honneur
(2) the polling booths les isoloirs
div
(3) the voting stations les isoloirs
(4) to be entrusted with this task en me confiant cette t?ache
we-nm
(5) reforms to the Canadian military r?eformes des forces [arm
?
ees] [canadiennes]
(6) an even greater country un pays [encore] [plus] magnifique
(7) in safe communities [en s
?
ecurit
?
e] [dans nos communaut
?
es]
we-11
(8) across the land de tout le pays
(9) strong opinions des opinions bien arr
?
et
?
ees
eq-nm
(10) a proud moment for Canada un moment heureux pour le Canada
(11) we have used the wrong process nous ne suivons pas le bon processus
(12) our common space and our common means un espace et des moyens communs
(13) the [personal] [protected] files les dossiers confidentiels et prot
?
eg
?
es
eq-11
(14) the names just announced les noms que je viens de mentionner
(15) one in three Canadian jobs au Canada , un emploi sur trois
Table 4: Adjunct identification F scores
prec. recall F
En
auto. 66.2 77.2 71.3
corr. 72.3 84.0 77.7
Fr
auto. 68.1 76.7 72.2
corr. 74.7 83.0 78.6
4 Experiments
4.1 Experimental set-up
We measure adjunct translation equivalence in
four data sets: the manually-aligned Canadian
Hansards corpus (Och and Ney, 2003), contain-
ing 447 sentence pairs, the house and senate train-
ing data of the Canadian Hansards (1.13M sen-
tence pairs), the French-English Europarl training
set (1.97M sentence pairs) and the Moses news-
commentaries corpus (156k sentence pairs). Be-
sides, we randomly selected 100 sentence pairs
from the Europarl set to measure adjunct identi-
fication accuracy as reported in section 3 and ad-
junct correspondence with gold adjunct annota-
tions.
All four corpora except the manual Hansards
are preprocessed to keep sentences with up to
80 words, and all four data sets are used jointly
to train unsupervised alignments, both with the
Berkeley aligner (Liang et al., 2006) and GIZA++
(Brown et al., 1993; Och and Ney, 2003) through
mgiza (Gao and Vogel, 2008), using 5 iterations of
Model 1 and 5 iterations of HMM for the Berkeley
aligner, and 5 iterations of Model 1 and HMM and
3 iterations of Model 3 and Model 4 for GIZA++.
The GIZA++ alignments are symmetrized using
the grow-diag-final heuristics. Besides, the man-
ual Hansards corpus is aligned with Sure Only
(SO) and Sure and Possible (SP) manual align-
ments.
4.2 Measurements with gold adjunct
annotations
We compared adjunct translation equivalence of
automatically identified adjuncts and gold anno-
tations using 100 manually annotated sentence
pairs from the Europarl corpus; adjuncts were
aligned automatically, using the Berkeley word
alignments. We also measured adjunct equiv-
alence using automatic adjunct annotations cor-
rected for parse attachment errors, as introduced
161
in section 3.3. Table 5 reports harmonic mean fig-
ures (m
h
) for each adjunct projection type. For
information, we also report their decomposition in
the case of gold annotations, showing some depen-
dence on the projection direction.
Table 5: Translation equivalence of auto-
matic, rebracketed and gold adjuncts
auto. corr. gold
m
h
m
h
ef fe m
h
null 7.6 7.7 8.1 7.3 7.7
div 22.3 22.5 14.7 12.0 13.2
we-nm 10.8 9.6 2.7 4.6 3.4
we-11 12.5 10.8 7.4 8.5 7.9
eq-nm 3.5 2.2 2.5 3.3 2.9
eq-11 41.8 45.8 64.5 64.3 64.4
About two thirds of manually identified ad-
juncts form equivalent pairs, representing a gain
of 20 points with regard to automatically identi-
fied adjuncts. This is accompanied by a halving of
divergent pairings and of weakly equivalent ones.
Further, we find that about half of the remaining
weak equivalences can be interpreted as transla-
tion equivalent (to compare to an estimated third
for automatically identified adjuncts), allowing us
to estimate to 70% the degree of translation equiv-
alence given Berkeley word alignments in the Eu-
roparl corpus.
4.3 Measurements with manual and
automatic alignments
We aligned adjuncts in the manual Hansards cor-
pus using all four word alignments. Table 6
presents the mean proportions for each category
of adjunct projection.
Table 6: Translation-equivalence of adjuncts
in the manual Hansards
SO SP bky giza
null 32.1 2.8 8.7 3.3
div 19.7 29.3 27.1 30.3
we-nm 3.4 14.6 8.5 11.4
we-11 5.7 13.8 13.5 15.3
eq-nm 4.1 7.3 4.1 4.2
eq-11 33.7 31.8 37.6 35.3
Comparing the mean proportions per type be-
tween the four alignments, we see that a third of
adjuncts on either side are not aligned at all with
the sure-only manual alignments. In the example
of Figure 2 for instance, these alignments do not
link
?
f
3
to e?
3
. On the other hand, the sure and
possible manual alignments lead to many diver-
gent or weakly equivalent pairings, a result of their
dense phrasal alignments. In comparison, the au-
tomatic alignments connect more words than the
sure-only alignments, leading to a mixed result for
the adjunct pairings: one gains more translation-
equivalent, but also more divergent and weakly
equivalent pairs. In this, the Berkeley aligner ap-
pears less noisy than GIZA++, as it captures more
translation equivalent pairs and less weakly equiv-
alent ones. This is confirmed in the other data sets
too, as Table 7 shows.
Table 7: Mean proportions of adjunct-pairing
types in automatically aligned data
hans-hst europarl news
bky giza bky giza bky giza
null 7.5 2.7 6.3 2.3 8.3 3.3
div 28.1 30.8 21.8 24.2 21.0 23.9
we-nm 10.4 12.2 11.0 12.7 10.6 12.6
we-11 13.4 15.5 12.4 14.6 11.7 14.2
eq-nm 3.2 4.0 3.2 4.0 3.1 3.8
eq-11 37.1 34.6 45.0 42.0 44.9 41.8
Comparing figures between the different data
sets, we see that the Europarl and the News
data have more translation-equivalent and less di-
vergent adjuncts than the Hansards training data
(hans-hst). Taking the harmonic mean for both
equivalent types (eq-nm and eq-11), we find
that 48.2% of adjuncts have an adjunct translation
equivalent in the Europarl data (with the Berke-
ley aligner) and 48.0% in the News corpus, against
40.3% the Hansards training set and 41.6% in the
manual Hansards set. This suggests that transla-
tions in the Hansards data are less literal than in
the Europarl or the News corpus.
4.4 Effect of sentence length
We explore the relation between sentence length
and translation equivalence by performing mea-
surements in bucketed data. We bucket the data
using the length of the English sentences. Mea-
surements are reported in Table 8 for the Hansards
162
Table 8: Adjunct translation equivalence with the Berkeley aligner in bucketed
data
hans-man hansard-hst europarl
1-15 16-30 1-15 16-30 31-50 51-80 1-15 16-30 31-50 51-80
null 9.3 8.5 6.5 7.6 7.8 8.0 6.4 6.0 6.2 6.6
div 28.1 25.9 39.5 25.3 23.5 22.6 25.3 22.2 21.2 20.6
we-nm 6.1 9.4 5.3 10.1 13.6 16.7 5.0 9.3 12.5 14.9
we-11 11.8 14.1 12.2 13.4 14.2 14.8 10.0 11.7 13.0 13.9
eq-nm 3.1 4.5 2.8 3.4 3.3 3.1 3.4 3.3 3.1 2.9
eq-11 40.6 36.3 32.5 39.6 37.3 34.4 49.1 47.1 43.7 40.7
and the Europarl sets (the News set yields similar
results to the Europarl data).
All data sets show a dramatic increase of the
proportion of adjuncts involved in many-to-many,
and to a lesser extent one-to-one weakly equiva-
lent translations. This increase is accompanied by
a decrease of all other adjunct-pairing types (un-
aligned adjuncts excepted), and is likely to result
from increased word-alignment and parsing errors
with sentence length.
A rather surprising result is the high proportion
of divergent adjunct translations in the shorter sen-
tences of the Hansards training set; we find the
same phenomenon with the GIZA++ alignment.
We attribute this effect to the Hansards set having
less literal translations than the other sets. That
we see this effect mostly in shorter sentences may
result from translation mismatches being mostly
local. As sentence length increases however, word
and adjunct alignment errors are also likely to link
more unrelated adjuncts, resulting in a drop of di-
vergent adjuncts.
4.5 Simplifying alignments
We perform a simple experiment to test the effect
of word-alignment simplification of adjunct trans-
lation equivalence. For this we remove alignment
links between function words (as defined in sec-
tion 3) on both sides of the data, and we realign
adjuncts using these simplified alignments. Ta-
ble 9 shows that this simplification (column ?-fw?)
slightly decreases the proportion of weakly equiv-
alent pairings with regard to the standard align-
ment (?std?), mostly to the benefit of translation-
equivalent pairings. This suggests that further
gains may be obtained with better alignments.
Table 9: Effect of alignment simplification
on adjunct translation equivalence in the Eu-
roparl data
bky giza
std -fw std -fw
null 6.3 7.5 2.3 3.1
div 21.8 21.5 24.2 24.0
we-nm 11.0 9.1 12.7 10.8
we-11 12.4 10.0 14.6 13.2
eq-nm 3.2 4.0 4.0 4.8
eq-11 45.0 47.5 42.0 43.7
5 Related work
While adjunction is a formal operation that may be
applied to non-linguistic adjuncts in STAG, De-
Neefe and Knight (2009) restrict it to syntactic
adjuncts in a Synchronous Tree Insertion Gram-
mar. They identify complements using (Collins,
2003)?s rules, and regard all other non-head con-
stituents as adjuncts. Their model is able to gen-
eralize to unseen adjunction patterns, and to beat a
string-to-tree baseline in an Arabic-English trans-
lation task.
Arnoult and Sima?an (2012) exploit adjunct op-
tionality to generate new training data for a phrase-
based model, by removing phrase pairs with an
English adjunct from the training data. They iden-
tify adjuncts using syntactic heuristics in phrase-
structure parses. They found that few of the gener-
ated phrase pairs were actually used at decoding,
leading to marginal improvement over the base-
line in a French-English task. They also report
163
figures of role preservation for different categories
of adjuncts, with lower bounds between 29% and
65% and upper bounds between 61% and 78%, in
automatically aligned Europarl data. The upper
bounds are limited by discontinuous adjunct pro-
jections, while the estimation of lower bounds is
limited by the lack of adjunct-identification means
for French.
There has been a growing body of work on ex-
ploiting semantic annotations for SMT. In many
cases, predicate-argument structures are used to
provide source-side contextual information for
lexical selection and/or reordering (Xiong et al.,
2012; Li et al., 2013), without requiring cross-
linguistic correspondence. When correspondence
between semantic roles is required, predicates are
commonly aligned first. For instance, Lo et al.
(2012) use a maximum-weighted bipartite match-
ing algorithm to align predicates with a lexical-
similarity measure to evaluate semantic-role corre-
spondence. Pad?o and Lapata (2009) use the same
algorithm with a similarity measure based on con-
stituent overlap to project semantic roles from En-
glish to German.
6 Conclusion
In this paper we presented the first study of trans-
lation equivalence of adjuncts on a variety of
French-English parallel corpora and word align-
ments. We use a method based on overlap to de-
rive many-to-many adjunct pairings, that are inter-
pretable in terms of translation equivalence.
We found through measurements in French-
English data sets that 40% to 50% of adjuncts?
depending on the data?are bijectively aligned
across a bitext, whereas about 25% more adjuncts
align to adjuncts, albeit not bijectively. We esti-
mate that a third of these weakly equivalent links
represent true, adjunct translation equivalences.
With manually identified adjuncts, we found
that about 70% have adjunct translation-
equivalents in automatically aligned data.
These are fairly low results if one considers that
French and English are relatively close syntacti-
cally. So while they show that adjunct labelling
accuracy on both sides of the data is crucial for
adjunct alignment, and that applications that
exploit adjunction can gain from decreasing their
dependence on word alignments and idealized
experimental conditions, they call for better
understanding of the factors behind translation
divergence.
In fact, as a remaining quarter of adjuncts have
divergent translations, it would be interesting to
determine, for instance, the degree to which diver-
gence is caused by lexical conflation, or reflects
non-literal translations.
Acknowledgments
We thank the anonymous reviewers for their perti-
nent comments. This research is part of the project
?Statistical Translation of Novel Constructions?,
which is supported by NWO VC EW grant from
the Netherlands Organisation for Scientific Re-
search (NWO).
References
Sophie Arnoult and Khalil Sima?an. 2012. Adjunct
Alignment in Translation Data with an Applica-
tion to Phrase-Based Statistical Machine Transla-
tion. In Proceedings of the 16th Annual Conference
of the European Association for Machine Transla-
tion, pages 287?294.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311.
M.-H. Candito, B. Crabb?e, and P. Denis. 2010. Statisti-
cal French dependency parsing: treebank conversion
and first results. In Proceedings of The seventh in-
ternational conference on Language Resources and
Evaluation (LREC).
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Lin-
guistics, 29(4):589?637.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
Tree Adjoining Machine Translation. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 727?736.
Bonnie J. Dorr. 1994. Machine Translation Diver-
gences: A Formal Description and Proposed Solu-
tion. Computational Linguistics, 20(4):597?633.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Software En-
gineering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49?57, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating Translational Cor-
respondence Using Annotation Projection. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, ACL ?02, pages 392?
399.
164
Richard Johansson and Pierre Nugues. 2007. Ex-
tended Constituent-to-dependency Conversion for
English. In Proceedings of NODALIDA 2007, pages
105?112, Tartu, Estonia, May 25-26.
Aravind K. Joshi, Leon S. Levy, and Masako Taka-
hashi. 1975. Tree adjunct grammars. Journal of
Computer and System Sciences, 10(1):136?163.
Junhui Li, Philip Resnik, and Hal Daum?e III. 2013.
Modeling Syntactic and Semantic Structures in Hi-
erarchical Phrase-based Translation. In Proceed-
ings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
540?549, Atlanta, Georgia.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by Agreement. In Proceedings of the Main
Conference on Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation of Computational Linguistics, HLT-NAACL
?06, pages 104?111.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
2012. Fully Automatic Semantic MT Evaluation. In
Proceedings of the Seventh Workshop on Statistical
Machine Translation, WMT ?12, pages 243?252.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29:19?51.
Sebastian Pad?o and Mirella Lapata. 2009. Cross-
lingual Annotation Projection for Semantic Roles.
Journal of Artificial Intelligence Research, 36:307?
340.
Stuart Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In Handbook of Formal
Languages, pages 69?123. Springer.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In CoNLL 2008:
Proceedings of the Twelfth Conference on Natural
Language Learning, pages 159?177, Manchester,
United Kingdom.
Dekai Wu and Pascale Fung. 2009. Can Semantic Role
Labeling Improve SMT? In Proceedings of the 13th
Annual Conference of the European Association for
Machine Translation, pages 218?225.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Mod-
eling the Translation of Predicate-Argument Struc-
ture for SMT. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics, pages 902?911.
165
