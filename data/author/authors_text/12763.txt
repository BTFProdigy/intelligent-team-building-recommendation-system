Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 400?409, Prague, June 2007. c?2007 Association for Computational Linguistics
Flexible, Corpus-Based Modelling of Human Plausibility Judgements
Sebastian Pad? and Ulrike Pad?
Computational Linguistics
Saarland University
Saarbr?cken, Germany
{pado,ulrike}@coli.uni-sb.de
Katrin Erk
Dept. of Linguistics
University of Texas at Austin
Austin, Texas
katrin.erk@mail.utexas.edu
Abstract
In this paper, we consider the computational
modelling of human plausibility judgements
for verb-relation-argument triples, a task
equivalent to the computation of selectional
preferences. Such models have applications
both in psycholinguistics and in computa-
tional linguistics.
By extending a recent model, we obtain
a completely corpus-driven model for this
task which achieves significant correlations
with human judgements. It rivals or exceeds
deeper, resource-driven models while exhibit-
ing higher coverage. Moreover, we show that
our model can be combined with deeper mod-
els to obtain better predictions than from ei-
ther model alone.
1 Introduction
One fundamental and intuitive finding in experimen-
tal psycholinguistics is that humans judge the plau-
sibility of a verb-argument pair vastly differently de-
pending on the semantic relation in the pair. Table 1
lists example human judgements which McRae et
al. (1998) elicited by asking about the plausibility of,
e.g., a hunter shooting (relation agent) or being shot
(relation patient). McRae et al found that ?hunter? is
judged to be a very plausible agent of ?shoot? and
an implausible patient, while the reverse is true for
?deer?. In linguistics, this phenomenon is explained
by selectional preferences on verbs? argument po-
sitions; we use plausibility and fit with selectional
preferences interchangeably.
Verb Relation Noun Plausibility
shoot agent hunter 6.9
shoot patient hunter 2.8
shoot agent deer 1.0
shoot patient deer 6.4
Table 1: Verb-relation-noun triples with plausibility
judgements on a 7-point scale (McRae et al, 1998)
In this paper, we consider computational mod-
els that predict human plausibility ratings, or the
fit of selectional preferences and argument, for
such (verb, relation, argument), in short, (v, r, a),
triples. Being able to model this type of data is rel-
evant in a number of ways. From the point of view
of psycholinguistics, selectional preferences have an
important effect in human sentence processing (e.g.,
McRae et al (1998), Trueswell et al (1994)), and
models of selectional preferences are therefore nec-
essary to inform models of this process (Pad? et al,
2006). In computational linguistics, a multitude of
tasks is sensitive to selectional preferences, such as
the resolution of ambiguous attachments (Hindle and
Rooth, 1993), word sense disambiguation (McCarthy
and Carroll, 2003), semantic role labelling (Gildea
and Jurafsky, 2002), or testing the applicability of
inference rules (Pantel et al, 2007).
A number of approaches has been proposed to
model selectional preference data (Pad? et al, 2006;
Resnik, 1996; Clark and Weir, 2002; Abe and Li,
1996). These models generally operate by general-
ising from seen (v, r, a) triples to unseen ones. By
relying on resources like corpora with semantic role
annotation or the WordNet ontology, these models
400
generally share two problems: (a), limited coverage;
and (b), the resource (at least partially) predetermines
the generalisations that they can make.
In this paper, we investigate whether it is possi-
ble to predict the plausibility of (v, r, a) triples in
a completely corpus-driven way. We build on a re-
cent selectional preference model (Erk, 2007) that
bases its generalisations on word similarity in a vec-
tor space. While that model relies on corpora with
semantic role annotation, we show that it is possible
to predict plausibility ratings solely on the basis of a
parsed corpus, by using shallow cues and a suitable
vector space specification.
For evaluation, we use two balanced data sets of
human plausibility judgements, i.e., datasets where
each verb is paired both with a good agent and a good
patient, and where both nouns are presented in either
semantic relation (as in Table 1). Using balanced test
data is a particularly difficult task, since it forces
the models to account reliably both for the influence
of the semantic relation (agent/patient) and of the
argument head (?hunter?/?deer?).
We obtain three main results: (a), our model is able
to match the superior performance of the model pro-
posed by Pad? et al (2006), while retaining the high
coverage of the model proposed by Resnik (1996);
(b), using parsing as a preprocessing step improves
the model?s performance significantly; and (c), a com-
bination of our model with the Pad? model exceeds
both individual models in accuracy.
Plan of the paper. In Section 2, we give an
overview of existing selectional preferences and vec-
tor space models. Section 3 introduces our model and
discusses its parameters. Sections 4 and 5 present our
experimental setup and results. Section 6 concludes.
2 Related Work
Modelling Selectional Preferences with Gram-
matical Functions. The idea of inducing selec-
tional preferences from corpora was introduced by
Resnik (1996). He approximated the semantic verb-
argument relations in (v, r, a) triples by grammatical
functions, which are readily available for large train-
ing corpora. His basic two-step procedure was fol-
lowed by all later approaches: (1), extract argument
headwords for a given predicate and relation from
a corpus; (2), generalise to other, similar words us-
ing the WordNet noun hierarchy. Other models also
relying on the WordNet resource include Abe and
Li (1996) and Clark and Weir (2002).
We present Resnik?s model in some detail, since
we will use it for comparison below. Resnik first
computes the overall selectional preference strength
for each verb-relation pair, i.e. the degree of ?con-
strainedness? of each relation. This quantity is esti-
mated as the difference (in terms of the Kullback-
Leibler divergence D) between the distribution over
WordNet argument classes given the relation, p(c|r),
and the distribution of argument classes given the
current verb-relation combination, p(c|v, r). The in-
tuition is that a verb-relation pair that only allows
for a limited range of argument heads will have a
probability distribution over argument classes that
strongly diverges from the prior distribution.
Next, the selectional association of the triple,
A(v, r, c), is computed as the ratio of the selectional
preference strength for this particular class, divided
by the overall selectional preference strength of the
verb-relation pair. This is shown in Equation 1.
A(v, r, c) =
p(c|v, r)log p(c|v,r)p(c|r)
D(p(c|r)||p(c|v, r))
(1)
Finally, the selectional preference between a verb,
a relation, and an argument head is taken to be the
selectional association of the verb and relation with
the most strongly associated WordNet ancestor class
of the argument.
WordNet-based approaches however face two
problems. One is a coverage problem due to the lim-
ited size of the resource (see the task-based evalu-
ation in Gildea and Jurafsky (2002)). The other is
that the shape of the WordNet hierarchy determines
the generalisations that the models make. These are
not always intuitive. For example, Resnik (1996) ob-
serves that (answer, obj, tragedy) receives a high
preference because ?tragedy? in WordNet is a type
of written communication, which is a preferred argu-
ment class of ?answer?.
Rooth et al (1999) present a fundamentally dif-
ferent approach to selectional preference induction
which uses soft clustering to form classes for general-
isation and does not take recourse to any hand-crafted
resource. We will argue in Section 6 that our model
allows more control over the generalisations made.
401
Modelling Selectional Preferences with Thematic
Roles. Pad? et al (2006) present a deeper model
for the plausibility of (v, r, a) triples that approxi-
mates the relations with thematic roles. It estimates
the selectional preferences of a verb-role pair with
a generative probability model that equates the plau-
sibility of a (v, r, a) triple with the joint probability
of seeing the thematic role with the verb-argument
pair. In addition, the model also considers the verb?s
sense s and the grammatical function gf of the ar-
gument; however, since the model is generative, it
can make predictions even when not all variables are
instantiated. The final model is shown in Equation 2.
Plausibilityv,r,a = P (v, s, r, a, gf ) (2)
The induction of this model from the FrameNet cor-
pus of semantically annotated training data (Fillmore
et al, 2003) encounters a serious sparse data prob-
lem, which is approached by the application of word-
class-based and Good-Turing re-estimation smooth-
ing. The resulting model?s plausibility predictions
are significantly correlated to human judgements, but
because of the use of verb-specific thematic roles,
the model?s coverage is still restricted by the verb
coverage of the training corpus.
Vector Space Models. Another class of models
that has found wide application in lexical semantics
is the family of vector space models. In a vector space
model, each target word is represented as a vector,
typically constructed from co-occurrence counts with
context words in a large corpus (the so-called basis
elements). The underlying assumption is that words
with similar meanings occur in similar contexts, and
will be assigned similar vectors. Thus, the distance
between the vectors of two target words, as given by
some distance measure (e.g., Cosine or Jaccard), is a
measure of their semantic similarity.
Vector space models are simple to construct, and
the semantic similarity they provide has found a wide
range of applications. Examples in NLP include in-
formation retrieval (Salton et al, 1975), automatic
thesaurus extraction (Grefenstette, 1994), and pre-
dominant sense identification (McCarthy et al, 2004).
In cognitive science, they have been used to account
for the influence of context on human lexical pro-
cessing (McDonald and Brew, 2004), and to model
lexical priming (Lowe and McDonald, 2000).
A drawback of vector space models is the diffi-
culty of interpreting what some degree of ?generic
semantic similarity? between two target words means
in linguistic terms. In particular, this similarity is
not sensitive to selectional preferences over specific
semantic relations, and thus cannot model the plau-
sibility data we are interested in. The next section
demonstrates how the integration of ideas from se-
lectional preference induction makes this distinction
possible.
3 The Vector Similarity Model:
Corpus-Based Modelling of Plausibility
3.1 Model Architecture
Our model builds on the architecture of Erk (2007). It
combines the idea underlying the selectional prefer-
ence models from Section 2, namely to predict plau-
sibility by generalising over head words, with vector
space similarity. The fundamental idea of our model
is to model the plausibility of the triple (v, r, a) by
comparing the argument head a to other headwords
a? which we have already seen in a corpus for the
same verb-relation pair (v, r), and which we there-
fore assume to be plausible. We write Seenr(v) for
the set of seen headwords. Our intuition is that if a
is similar to the words in Seenr(v), then the triple
(v, r, a) is plausible; conversely, if it is very dissimi-
lar, then the triple is implausible.
Concretely, we judge the plausibility of the triple
by averaging over the similarity of the vector for a to
all vectors for the seen headwords in Seenr(v):
Pl(v, r, a) =
?
a??Seenr(v)
w(a?) ? sim(a, a?)
|Seenr(v)|
(3)
where w is a weight factor specific to each a?. w can
be used to implement different weighting schemes
that encode prior knowledge, e.g., about the reliabil-
ity of different words in Seenr(v). In this paper, we
only consider a very simple weighting factor, namely
the frequency of the seen headwords. This encodes
the assumption that similarity to frequent head words
is more important than similarity to infrequent ones:
Pl(v, r, a) =
?
a??Seenr(v)
f(a?) ? sim(a, a?)
|Seenr(v)|
(4)
402
deer
lion
hunter
poacher
director
seen patients
of "shoot"
seen agents
of "shoot"
Figure 1: A vector space for estimating the
plausibilities of (shoot, agent, hunter) and
(shoot, patient, hunter).
This model can be seen as a straightforward imple-
mentation of the selectional preference induction pro-
cess of generalising from seen headwords to other,
similar words. By using vector space representations
to judge the similarity of words, we obtain a com-
pletely corpus-driven model that does not require any
additional resources and is very flexible. A comple-
mentary view on this model is as a generalisation of
traditional vector space models that computes simi-
larity not between two vectors, but between a vector
and a set of other vectors. By using the vectors for
seen headwords of a given relation as this set, the
similarity we compute is specific to this relation.
Example. Figure 1 shows an example vector space.
Consider v = ?shoot?, r = agent, and a = ?hunter?.
In order to judge whether a hunter is a plausible agent
of ?shoot?, the vector space representation of ?hunter?
is compared to all representations of known agents
of "shoot?, namely ?poacher? and ?director?. Due
to the nearness of the vector for ?hunter? to these
two vectors, ?hunter? will be judged a fairly good
agent of ?shoot?. Compare this with the result for the
role patient : ?hunter? is further away from ?lion? and
?deer?, and will therefore be found to be a rather bad
patient of ?shoot?. However, ?hunter? is still more
plausible as a patient of ?shoot? than e.g., ?director?.
3.2 Instantiating the Model: Unparsed
vs. Parsed Corpora
The two major tasks which need to be addressed to
obtain an instance of this model are (a), determining
the sets of seen head words Seenr(v), and (b), the
construction of a vector space. Erk (2007) extracted
the set of seen head words from corpora with se-
mantic role annotation, and used only a single vector
space representation. In this paper, we eliminate the
reliance on special annotation by considering shallow
approximations of the semantic relations in question.
In addition, we discuss in detail which properties of
the vector space are crucial for the prediction of plau-
sibility ratings, a much more fine-grained task than
the pseudo-word disambiguation task presented in
Erk (2007) that is more closely related to semantic
role labelling. The goal of our exposition is thus to
develop a model that can use more training data, and
represent the corpus information optimally in order
to obtain superior coverage.
In fact, tasks (a) and (b) can be solved on the basis
of unparsed corpora, but we would expect the results
to be rather noisy. Fortunately, the state of the art in
broad-coverage (Lin, 1993) and unsupervised (Klein
and Manning, 2004) dependency parsing allows us to
treat dependency parsing merely as a preprocessing
step. We therefore describe two instantiations of our
model: one based on an unprocessed corpus, and one
based on a dependency-based parsed corpus. By com-
paring the models, we can gauge whether syntactic
preprocessing improves model performance. In the
following, we describe the strategies the two models
adopt for (a) and (b).
Identifying seen head words for relations. Re-
call that the set Seenr(v) is supposed to contain
known head words a that are observed in the corpus
as triples (v, r, a). In a parsed corpus, we can approx-
imate the relation agent by the dependency relation
of subject provided by the parser, and the relation
patient by the dependency relation of object. In
an unparsed corpus, these grammatical relations are
unavailable, and the only straightforward evidence
we can use is word order. In this case, we assume
that words directly adjacent to the left of a predicate
are subjects, and therefore agents, whereas words
directly to its right are objects, and thus patients.
Vector space topology. The success of our method
depends directly on the topology of the vector space.
More specifically, two words should only be assigned
similar vectors if they are in fact of similar plausibil-
ity. If this is not the case, there is no guarantee that a
word a that is similar to the words in Seenr(v) forms
403
``````````````Basis elements
Target
deer hunter
shoot 10 10
escape 12 12
``````````````Basis elements
Target
deer hunter
shoot-SUBJ 0 8
shoot-OBJ 10 2
escape-SUBJ 10 5
escape-OBJ 2 7
Figure 2: Two vector spaces, using as basis elements
either context words (above) or words paired with
grammatical functions (below)
a plausible triple (v, r, a) itself (cf. Figure 1).
The topology, in turn, is related to the choice of
basis elements. Traditional vector space models use
context words as basis elements of the space. The
top table in Figure 2 illustrates our intuition that such
spaces are problematic: ?deer? and ?hunter? receive
identical vectors, even though they show complemen-
tary plausibility ratings (cf. Table 1). The reason is
that ?deer? and ?hunter? often co-occur quite closely
to one another (e.g., in the vicinity of ?shoot?), and
thus show a very similar profile in terms of context
words. In preliminary experiments, we found that vec-
tor spaces with context words as basis elements are
in fact unable to distinguish such word pairs reliably.
In contrast, the bottom table in Figure 2 indicates
that this problem can be alleviated by using context
words combined with the grammatical relation to
the target word as basis elements. Target words now
receive different representations, depending on the
grammatical function in which they occur with con-
text words. In consequence, resulting spaces can dis-
tinguish, for example, between ?hunter? and ?deer?.
We adopt word-function pairs as basis elements for
the vector spaces in all our models. In a dependency-
parsed corpus, the basis elements can be directly read
off the syntactic structure. In an unparsed corpus, we
again fall back on word order, appending to each
context word its relative position to the target word.
4 Experimental Setup
Experimental Materials. In order to make our
evaluation comparable to the earlier modelling study
by Pad? et al (2006), we present evaluations on the
two plausibility judgement datasets used there.1
The first dataset consists of 100 data points from
McRae et al (1998). Our example in Table 1, which
is taken from this dataset, demonstrates its balanced
structure: 25 verbs are paired with two arguments
and two relations each, such that each argument is
highly plausible in one relation, but implausible in
the other. The resulting distribution of ratings is thus
highly bimodal. Models can only reliably predict the
human ratings in this data set if they can capture the
difference between verb argument slots as well as as
between individual fillers.
The second, larger dataset is less strictly balanced,
since its triples are constructed on the basis of corpus
co-occurrences (Pad? et al, 2006). 18 verbs are com-
bined with the three most frequent subjects and ob-
jects from both the Penn Treebank and the FrameNet
corpus. Each verb-argument pair was rated both as
an agent and as a patient, which leads to a total of
24 rated triples per verb. The dataset contains ratings
for a total of 414 triples, due to overlap between cor-
pora. The resulting judgements show a more even
distribution of ratings than the McRae data.
Vector Similarity Models. Following our exposi-
tion in the last section, we construct two instantia-
tions of our vector similarity model, one using un-
parsed and one parsed data. Both are trained on the
complete British National Corpus (Burnard, 1995,
BNC) with more than six million sentences.
The unparsed model (Unparsed) uses the BNC
without any pre-processing. We first construct the
set of known headwords, Seenr(v), as follows: All
words up to 2 words to the left of instances of v
are assumed to be subjects, and thus agents; vice
versa for patients to the right. Then, we construct
semantic space representations for the experimental
arguments and known headwords, adopting optimal
parameter settings from the literature (Pad? and Lap-
ata, 2007). This means a context window of 5 words
to either side and 2,000 basis elements (dimensions),
which are formed by the most frequent 1,000 words
1We are grateful to Ken McRae for his dataset.
404
in the BNC, combined with each of the relations
agent and patient. All counts are log-likelihood trans-
formed (Lowe, 2001).
To construct the parsed model (Parsed), we
dependency-parsed the BNC with Minipar (Lin,
1993). We first obtain the seen headwords Seenr(v)
by using all subjects and objects of v as agents and pa-
tients, respectively. We then construct a vector space
for the experimental arguments and known head-
words.2 We use 2,000 dimensions again, but adopt the
most frequent (head , grammatical function) pairs
in the BNC as basis elements. The context window
is formed by subject and object dependencies.
All counts are log-likelihood transformed.
We experiment with two distance measures to com-
pute vector similarity, namely the Jaccard Coefficient
and Cosine Distance, both of which have been shown
to yield good performance in NLP tasks (Lee, 1999;
McDonald and Lowe, 1998).
Evaluation Procedure. We evaluate our models
by correlating the predicted plausibility values with
the human judgements, which range between 1 and
7. Since the human judgement data is not normally
distributed, we use Spearman?s ?, a non-parametric
rank-order test. We determine the statistical signif-
icance of differences in correlation strength using
the method described in Raghunathan (2003). This
method can deal with missing values and thus allows
us to compare models with different coverage.
It is difficult to specify a straightforward baseline
for our correlation-based evaluation. In contrast to
classification tasks, where models choose one out of
a fixed number of classes, our model predicts contin-
uous data. This task is more difficult to approximate,
e.g., using frequency information.
With respect to upper bounds, we hold that au-
tomatic models of plausibility cannot be expected
to surpass the typical agreement on the plausibility
judgement task between human participants. Thus,
we assume an upper bound of ? ? 0.7.
Comparison against Other Models. We compare
our performance to two models from the literature dis-
cussed in Section 2. The first model (Pado) is the the-
2This space was computed using the
DependencyVectors software described in Pad? and
Lapata (2007). This software can be downloaded from http:
//www.coli.uni-saarland.de/~pado/dv.html.
Model Coverage Spearman?s ?
Unparsed Cosine 90% 0.023, ns
Unparsed Jaccard 90% 0.044, ns
Parsed Cosine 91% 0.218, *
Parsed Jaccard 91% 0.129, ns
Resnik 94% 0.028, ns
Pado 56% 0.415, **
Table 2: Model performance on McRae data.
*: p < 0.05, **: p < 0.01
matic role-based model by Pad? et al (2006) trained
on the FrameNet (Fillmore et al, 2003) release 1.2 ex-
ample sentences, a subset of the BNC annotated with
semantic roles. This corpus contains about 57,000
sentences, which corresponds to roughly 1% of the
BNC data.
The second model (Resnik) is the WordNet-based
selectional preference model by Resnik (1996),
trained on the dependency-parsed BNC (see above).
5 Experimental Evaluation
The McRae Dataset. Table 2 summarises our re-
sults on the McRae dataset. The upper part shows
the results for our two vector similarity models
(Parsed/Unparsed), combined with the two distance
measures (Cosine/Jaccard). The lower part shows the
two resource-based models we use for comparison.
We find that all vector similarity models exhibit
high coverage (above 90%), and one model (Parsed
Cosine) can predict human judgements with a signifi-
cant correlation. The instantiation of the model has
a significant impact on the performance: The Parsed
models clearly outperform the Unparsed models. The
effect of the distance measure is less clear-cut, since
the Unparsed models perform better with Jaccard,
while the Parsed models prefer Cosine.
The deep semantic plausibility model (Pado)
makes predictions only for slightly more than half of
the data. This low coverage is a direct result of the
small overlap in verbs between the McRae dataset
and the FrameNet corpus. However, on the data
points it covers, it achieves a significant correlation
to human judgements. The correlation coefficient is
numerically much higher than that of the Parsed Co-
sine model, but due to the large coverage difference,
the two models are not statistically distinguishable.
405
Model Coverage Spearman?s ?
Unparsed Cosine 98% 0.117, *
Unparsed Jaccard 98% 0.149, **
Parsed Cosine 98% 0.479, ***
Parsed Jaccard 98% 0.120, *
Resnik 98% 0.237, ***
Pado 97% 0.515, ***
Table 3: Model performance on Pado data.
*: p < 0.05, **: p < 0.01, ***: p < 0.001
Resnik?s WordNet-based model shows a coverage
that is comparable to the vector similarity models,
but does not achieve a significant correlation to the
human judgements.
The Pado Dataset. Table 3 summarises the results
for the Pado dataset. Since all verbs in this dataset are
covered in FrameNet, the deep Pado model shows a
coverage comparable to all other models, at >95%.
The main difference to the McRae dataset lies in
the models? performance. We find that all models,
including the Unparsed vector models and Resnik,
manage to achieve significant correlations with the
human judgements. Within the vector similarity mod-
els, the same trends hold as for the McRae dataset:
Parsed outperforms Unparsed, and the best combina-
tion is Parsed Cosine. The models fall into two clearly
separated groups: The Pado and Parsed Cosine mod-
els achieve a highly significant correlation, and are
statistically indistinguishable. They significantly out-
perform the second group (p < 0.001), formed by
all other models. Within this second group, Resnik is
numerically the best model and shows a significant
correlation with human data; nevertheless, the differ-
ence to the first group is evident from its substantially
lower correlation coefficient.
The construction of the Pado dataset alows a fur-
ther analysis. As mentioned in Section 4, the dataset
consists of verb-argument pairs drawn from two dif-
ferent corpora. Therefore, each verb is combined
both with some arguments that are seen in FrameNet,
and some that are not. Our hypothesis is that the
FrameNet-trained Pado model performs consider-
ably better on the 216 ?FN-Seen? data points (verb-
argument pairs observed in FrameNet in at least one
relation) than on the 198 ?FN-Unseen? data points
(verb-argument pairs unseen in both relations).
Table 4 shows the results of this analysis for the
best-performing models. We observe a pattern corre-
sponding to our expectations: The performance of the
Pado model is clearly worse for FN-Unseen than for
FN-Seen, while the Resnik and Parsed Cosine mod-
els perform more evenly across both datasets. While
the Pado model is significantly better on the FN-Seen
dataset, it is numerically outperformed by the Parsed
Cosine model for the FN-Unseen data points. We
conclude that the deep model is more accurate within
the coverage of its resources, but loses its advantage
when it has to resort to smoothing.
Model combination. Our last analysis indicates
that the models have complementary strengths: the
thematic role-based Pado model is the best plausi-
bility predictor on the data points it has seen, while
the Parsed cosine model overall predicts human data
only numerically worse, and with better coverage.
We therefore suggest to combine the predictions of
the two models to combine their respective strengths.
For the moment, we only consider a naive backoff
scheme: For each data point, we use the prediction
of the Pado model if the data point is ?FN-Seen? (cf.
the last paragraph), and the prediction of the Parsed
Cosine model otherwise. Note that this criterion does
not consider the predictions of the models themselves,
only properties of the underlying training set.
The actual combination requires a normalisation
of the respective predictions, since one of the models
(Pado) is probabilistic, while the other one (Parsed
Cosine) is similarity-based, and their predictions are
not directly comparable. We perform a simple nor-
malisation by z-transforming the complete predic-
tions of each model.3 The combination of the scaled
predictions in fact results in an improved correlation
with the human data. The correlation coefficient of
?=0.552 numerically exceeds either base model, and
the coverage of 98% corresponds to the coverage of
the more robust Parsed Cosine model.
We take this result as evidence that even a simple
combination technique can lead to improved predic-
tions. Unfortunately, our naive backoff scheme does
not directly carry over to the McRae dataset, where
only 2 out of 100 data points are ?FN-Seen?, and the
Pado model would thus hardly contribute.
3The z transformation scales a dataset to a mean of 0 and a
standard deviation of 1.
406
Model FN-Seen Data FN-Unseen Data
Parsed Cosine 94% 0.426, *** 100% 0.461, ***
Resnik 96% 0.217, ** 100% 0.263, ***
Pado 97% 0.569, *** 96% 0.383, ***
Table 4: Performance on data points seen and unseen in FrameNet (Pado dataset). **: p < 0.01 ***: p < 0.001
Discussion. We have verified experimentally that
our vector similarity model is able to match the per-
formance of a deep plausibility model, exceeding it
in coverage, and to outperform a WordNet-based se-
lectional preference model. We conclude that a com-
pletely corpus-driven approach constitutes a viable
alternative to resource-based models.
One insight from our experiments is that vec-
tor similarity models constructed from dependency-
parsed corpora perform significantly better than un-
parsed models. This indicates that dependency rela-
tions like subject and object are reliable syntac-
tic correlates of semantic relations like agent and pa-
tient, but that their approximation in terms of word or-
der introduces considerable noise. The Parsed models
are best combined with Cosine Distance. We surmise
that Cosine, which tends to consider low-frequency
words more than Jaccard, is more susceptible to the
additional noise in unparsed corpora.
Furthermore, the choice of basis elements for the
vector space is vital: Plausibilities could only be pre-
dicted successfully with word-relation pairs as basis
elements. This is in contrast to recent results on pre-
dominant sense acquisition, the task of identifying
the most frequent sense for a given word in an un-
supervised manner (McCarthy et al, 2004). On that
task, Pad? and Lapata (2007) found vector spaces
with words as basis elements are in fact competitive
with models using word-relation pairs. This diver-
gence underlines an interesting difference between
the two tasks. Evidently, predominant senses identi-
fication, as a WSD-related task, can succeed on the
basis of topical information, which is represented
well in word-based spaces. In contrast, plausibility
judgments can only be predicted by a space based
on word-relation pairs which can represent the finer-
grained distinctions arising from different relations
between verb and noun.
A second important finding is that the relative per-
formance of the different models is the same on the
McRae and Pado datasets. The Pado model performs
best, followed by our Parsed Cosine vector similarity
model, followed by the Unparsed and Resnik models.
The McRae dataset, however, is much more diffi-
cult to account for than the Pado data, independent of
the model. This effect was already noted by Pad? et
al. (2006), who attributed it to the very limited over-
lap between the McRae dataset and FrameNet. While
this explanation can account for the difference for the
Pado model, we observe the same pattern across all
models. This suggests that a more general frequency
effect is at work here: The median frequency of the
hand-selected McRae nouns is 1,356 in the BNC, as
opposed to 8,184 for the corpus-derived Pado nouns.
The resulting sparseness affects all model families,
since all ultimately rely on co-occurrences.
The performance difference between the two
datasets is particularly large for the WordNet-based
selectional preference model (Resnik). A further
analysis of the model?s predictions shows that
the model has difficulty in distinguishing between
verb-relation-argument triples that differ only in
the argument, such as (shoot, agent, hunter) and
(shoot, agent, deer). Recall that it is crucial for the
prediction of the McRae data to make this distinc-
tion, since the arguments for each relation are cho-
sen to differ widely in plausibility. The reason for
the Resnik model?s difficulty is that arguments are
mapped onto WordNet synsets, and whenever two
arguments are mapped onto closely related synsets,
their plausibility ratings are similar. This problem is
graver for the McRae test set, where all arguments are
animates, and thus more similar in terms of WordNet,
than for the Pado set, which also contains a portion of
inanimate arguments with animate counterparts. This
analysis highlights again the fundamental problem
of resource-based models, where design decisions of
the underlying resource may limit, or even mislead,
the models? generalisations.
Finally, we have shown in a first experiment that
407
the syntax-based vector similarity model can be com-
bined with the role-base model to obtain a combined
model that performs superior to both. In this com-
bined model, the shallowmodel?s better coverage sup-
plements the accurate predictions of the deep model.
6 Conclusions
In this paper, we have considered the computational
modelling of human plausibility judgements for verb-
relation-argument triples, a task equivalent to the
computation of selectional preferences. We have ex-
tended a recent proposal (Erk, 2007) which com-
bines ideas from selectional preference induction and
vector space models. Our model can be constructed
from a large corpus with partial syntactic information
(specifically, subject and object relations) from which
it builds an optimally informative vector space.
We have demonstrated that the successful evalua-
tion of the model in Erk (2007) on the coarse-grained
pseudo-word disambiguation task carries over to the
prediction of human plausibility judgments which re-
quires relatively fine-grained, relation-based distinc-
tions. Our model is competitive with existing ?deep?
models while exhibiting a higher coverage. We have
also shown that our vector similarity model can be
combined with a ?deep? model so that the combined
model outperforms both base models. A thorough
investigation of strategies for prediction combination
and scaling remains future work.
The strategy of our model to derive generalisations
directly from corpus data, without recourse to re-
sources, is similar to another family of corpus-driven
selectional preference models, namely EM-based
clustering models (Rooth et al, 1999). However, we
believe that our model has a number of advantages.
(1), It is conceptually simple and implements the
intuition behind selectional preference models, ?gen-
eralise from known headwords to unknown ones?,
particularly directly through the comparison of new
headwords to known ones according to a given defini-
tion of similarity. (2), The separation of the similarity
computation and the acquisition of seen headwords
gives the experimenter fine-grained control over the
types and sources of information which inform the
construction of the model. (3), The instantiation of
the similarity computation with a vector space makes
it possible to integrate additional linguistic informa-
tion beyond verb-argument co-occurrences into the
model, building on a large body of work in vector
space construction. In sum, our modular model pro-
vides a higher degree of control than one-step models
like the EM-based proposal.
An important avenue of further research is the
ability of the vector plausibility model to model finer-
grained distinctions between semantic relations be-
yond the agent/patient dichotomy, as thematic role-
based models are able to. Excluding the direct use of
role-annotated corpora like FrameNet for coverage
reasons, the most promising strategy is to extend our
present scheme of approximating semantic relations
by grammatical realisations. How much noise this
approximation introduces when finer role sets are
used is an open research question.
Acknowledgments. The work presented in this pa-
per was supported by the financial support of DFG
(grants Pi-154/9-2 and IRTG ?Language Technology
and Cognitive Systems?).
References
Naoki Abe and Hang Li. 1996. Learning word associa-
tion norms using tree cut pair models. In Proceedings
of ICML 1996, pages 3?11.
Lou Burnard, 1995. User?s guide for the British National
Corpus. British National Corpus Consortium, Oxford
University Computing Services.
Stephen Clark and David Weir. 2002. Class-based prob-
ability estimation using a semantic hierarchy. Compu-
tational Linguistics, 28(2):187?206.
Katrin Erk. 2007. A simple, similarity-based model for
selectional preferences. In Proceedings of the 45th
ACL, Prague, Czech Republic.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to FrameNet.
International Journal of Lexicography, 16:235?250.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
19(1):103?120.
408
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the 42th
ACL, pages 478?485, Barcelona, Spain.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th ACL, pages 25?32, College
Park, MA.
Dekang Lin. 1993. Principle-based parsing without over-
generation. In Proceedings of the 31st ACL, pages
112?120, Columbus, OH.
Will Lowe and Scott McDonald. 2000. The direct route:
Mediated priming in semantic space. In Proceedings
of the 22nd CogSci, pages 675?680, Philadelphia, PA.
Will Lowe. 2001. Towards a theory of semantic space.
In Proceedings of the 23rd CogSci, pages 576?581, Ed-
inburgh, UK.
Diana McCarthy and John Carroll. 2003. Disambiguat-
ing nouns, verbs and adjectives using automatically ac-
quired selectional preferences. Computatinal Linguis-
tics, 29(4):639?654.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42th ACL, pages
279?286, Barcelona, Spain.
Scott McDonald and Chris Brew. 2004. A distributional
model of semantic context effects in lexical process-
ing. In Proceedings of the 42th ACL, pages 17?24,
Barcelona, Spain.
Scott McDonald and Will Lowe. 1998. Modelling func-
tional priming and the associative boost. In Proceed-
ings of the 20th CogSci, pages 675?680, Madison, WI.
Ken McRae, Michael Spivey-Knowlton, and Michael
Tanenhaus. 1998. Modeling the influence of thematic
fit (and other constraints) in on-line sentence compre-
hension. Journal of Memory and Language, 38:283?
312.
Sebastian Pad? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2).
Ulrike Pad?, Frank Keller, and Matthew W. Crocker.
2006. Combining syntax and thematic fit in a proba-
bilistic model of sentence processing. In Proceedings
of the 28th CogSci, pages 657?662, Vancouver, BC.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Tim-
othy Chklovski, and Eduard Hovy. 2007. ISP: Learn-
ing inferential selectional preferences. In Proceedings
of NAACL 2007, Rochester, NY.
Trivellore Raghunathan. 2003. An approximate test for
homogeneity of correlated correlations. Quality and
Quantity, 37:99?110.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational re-
alization. Cognition, 61:127?159.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing an semanti-
cally annotated lexicon via EM-based clustering. In
Proceedings of the 37th ACL, pages 104?111, College
Park, MA.
Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975.
A vector-space model for information retrieval. Jour-
nal of the American Society for Information Science,
18:613?620.
John Trueswell, Michael Tanenhaus, and Susan Garnsey.
1994. Semantic influences on parsing: Use of the-
matic role information in syntactic ambiguity resolu-
tion. Journal of Memory and Language, 33:285?318.
409
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 668?675, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Analyzing models for semantic role assignment using confusability
Katrin Erk and Sebastian Pad?
Computational Linguistics
Saarland University
Saarbr?cken, Germany
{erk,pado}@coli.uni-sb.de
Abstract
We analyze models for semantic role
assignment by defining a meta-model
that abstracts over features and learning
paradigms. This meta-model is based on
the concept of role confusability, is de-
fined in information-theoretic terms, and
predicts that roles realized by less specific
grammatical functions are more difficult
to assign. We find that confusability is
strongly correlated with the performance
of classifiers based on syntactic features,
but not for classifiers including semantic
features. This indicates that syntactic fea-
tures approximate a description of gram-
matical functions, and that semantic fea-
tures provide an independent second view
on the data.
1 Introduction
Semantic roles have become a focus of research in
computational linguistics during the recent years.
The driving force behind this interest is the prospect
that semantic roles, as a shallow meaning represen-
tation, can improve many NLP applications, while
still being amenable to automatic analysis. The
benefit of semantic roles has already been demon-
strated for a number of tasks, among others for ma-
chine translation (Boas, 2002), information extrac-
tion (Surdeanu et al, 2003), and question answer-
ing (Narayanan and Harabagiu, 2004).
Robust and accurate automatic semantic role as-
signment, a prerequisite for the wide-range use of
semantic roles in NLP, has been investigated in a
number of studies and shared tasks. Typically, role
assignment has been modeled as a classification
task, with models being estimated from large cor-
pora (Gildea and Jurafsky, 2002; Moschitti, 2004;
Xue and Palmer, 2004; Surdeanu et al, 2003; Prad-
han et al, 2004; Litkowski, 2004; Carreras and
M?rquez, 2005).
Within this framework, there is a number of archi-
tectural parameters which lend themselves to opti-
mization: the machine learning framework, the fea-
ture set, pre- and postprocessing, each of which has
been investigated in the context of semantic role as-
signment. The current paper concentrates on feature
engineering, since the feature set is a pivotal com-
ponent of any kind of machine learning system, and
allows us to incorporate and test linguistic intuitions
on the role assignment task.
We approach feature engineering not by directly
optimizing system performance. Instead, we pro-
ceed by error analysis, like Pado and Boleda (2004).
Our aim is to form a global hypothesis that explains
the distribution of errors across classes. Insofar as
the model does not contain model-specific infor-
mation, following this methodology can provide a
meta-model of a model family which abstracts over
concrete features and over the learning paradigm.
The concrete global hypothesis we test is: (1) All
features of current models approximate a descrip-
tion of grammatical functions, and the complete sys-
tems approximate an assignment based on grammat-
ical functions. (2) System performance for a given
role depends on how easily it is confused with other
roles. We will give this concept of role confusability
a formal, information-theoretic definition.
The present study specifically analyzes mod-
els for semantic role assignment in the FrameNet
668
paradigm (Fillmore et al, 2003). We are going to
show that our hypothesis indeed holds for a variety
of models ? but only models that comprise exclu-
sively syntactic features. We conclude that syntactic
features approximate a description of grammatical
functions, but that semantic features model a dif-
ferent aspect of the role assignment mapping. To-
gether with the reasonable performance of a solely
semantics-based system, this leads us to suggest a
closer investigation of semantic features ? and in
particular, a co-training approach with syntactic and
semantic features as different views on the role as-
signment data.
Plan of the paper. In Section 2, we give a
brief introduction to FrameNet, the semantic role
paradigm and corpus we are using in this study. Our
first experiment, described in Section 3, establishes
that there is a high variance in performance across
roles, and that this variance is itself stable across
models and learners. In Section 4, we state our hy-
pothesis, namely that this variance can be explained
through role confusability, and formalize the con-
cept . In Section 5, we perform detailed correlation
tests to verify our hypothesis and discuss our find-
ings. Section 6 concludes the paper.
2 FrameNet
This section presents the semantic role paradigm and
the role-annotated corpus on which the present study
is based. FrameNet1 is a lexical resource based on
Fillmore?s Frame Semantics (Fillmore, 1985). It de-
scribes frames, representations of prototypical situa-
tions. Each frame provides its set of semantic roles,
the entities or concepts pertaining to the prototypi-
cal situation. Each frame is further associated with a
set of target predicates (nouns, verbs or adjectives),
occurrences of which can introduce the frame.
FrameNet provides manually annotated examples
for each predicate, sampled from the British Na-
tional Corpus (Burnard, 1995). The size of this cor-
pus exceeds 135,000 sentences. The following sen-
tences are examples for verbs in the IMPACT frame,
which describes a situation in which typically ?an
IMPACTOR makes sudden, forcible contact with the
IMPACTEE, or two IMPACTORS both ... [make]
forcible contact?:
1http://www.icsi.berkeley.edu/~framenet/
(1) [Impactee His car] was struck [Impactor by a
third vehicle].
(2) [Impactor The door] slammed [Result shut].
(3) [Impactors Their vehicles] collided [Place at
Pond Hill].
FrameNet manual annotation also comprises a layer
of grammatical functions: For example, the subject
of finite verbs is labeled Ext, and Mod is a label
used for modifiers of heads, e.g. an adjective mod-
ifying a noun. The grammatical functions used in
FrameNet are listed in Fillmore and Petruck (2003).
Note that the frame-specificity of semantic roles
in FrameNet has important consequences for seman-
tic role assignment, since there is no direct way
to generalize role assignments across frames, and
learning has to proceed frame-wise. This com-
pounds the data sparseness problem, and automatic
assignment for frames with no training data is very
difficult (Gildea and Jurafsky, 2002).
3 Experiment 1: Variance in role
assignment
Several studies have established that there is con-
siderable variance in semantic role assignment per-
formance across different semantic roles within sys-
tems (Carreras and M?rquez, 2004; Carreras and
M?rquez, 2005; Pado and Boleda Torrent, 2004).
However, these studies used either the PropBank
semantic role paradigm (Carreras and M?rquez)
or a limited of experimental conditions (Pado and
Boleda). For this reason, we perform a first experi-
ment to replicate this phenomenon in our setting.
Note that the vast majority of participant sys-
tems in recent shared tasks divides semantic role as-
signment into multiple sequential steps. The max-
imal decomposition is as follows: preprocessing,
e.g. removal of unlikely argument candidates; ar-
gument recognition, the distinction between role-
bearing and non-role-bearing instances; argument
labeling, the actual classification of role-bearing in-
stances; and postprocessing, e.g. by inference over
probable role sequences.
Following this distinction, we concentrate in this
study on the argument labeling step, i.e. distinguish-
ing between roles, rather than distinguishing roles
669
from non-roles. This is justified by earlier empiri-
cal results, namely that the argument labeling step
requires more training data than argument recogni-
tion (Fleischmann and Hovy, 2003), and that it calls
for more sophisticated feature construction (Xue and
Palmer, 2004). We take this as evidence that the
quality of the argument labeling step is central to a
good semantic role assignment system.
In order to isolate the effects of argument label-
ing, we assume perfect argument recognition by us-
ing gold standard role boundaries; however, we do
not use gold standard parse trees, but rather automat-
ically computed ones, which realistically introduces
some noise (see the following paragraph).
Data and preprocessing. As experimental mate-
rial, we used the same data that was used in the
Senseval-3 semantic role assignment task: 40 frames
from FrameNet version 1.1, comprising 66,777 in-
stances. The number of roles per frame ranged from
2 to 22, and the number of role instances ranged
from 593 to 8,378. The data was randomly split into
training (90%) and test instances (10%).
The data was parsed with the Collins
model 3 (1996) parser; in addition, all tokens
were lemmatized with TreeTagger (Schmid, 1994).
Modeling. We model role assignment as a clas-
sification task, with parse tree constituents as in-
stances to be classified. We repeated the classifica-
tion with two different learners: The first learner,
TiMBL (Daelemans et al, 2003) is an implementa-
tion of nearest-neighbor classification algorithms in
the memory-based learning paradigm2. The second
learner, Malouf?s probabilistic maximum entropy
(Maxent) system (Malouf, 2002), uses the LMVM
algorithm to estimate log-linear models. We did not
perform smoothing.
Table 5 shows the features we use. Here as in the
system setup, we keep close to current existing mod-
els for semantic role assignment in order to make our
results as representative as possible. We investigate
different feature sets in order to verify our results. In
Exp. 1, we limit ourselves to two feature sets, Syn
(syntactic features) and Sem (lexical features) from
the bottom of Table 5. The feature sets were exactly
the same for both learners.
2TiMBL was set to k-NN classification, using the MVDM
distance metric and 5 neighbors.
Syn/Sem Syn
MBL 87.1 ? 12.7 82.2 ? 17.8
Maxent 87.5 ? 13.4 82.4 ? 18. 2
Table 1: Exp. 1: Overall results (F-scores and stan-
dard deviation across roles).
Syn/Sem Syn
Role FMBL FMaxent FMBL FMaxent
Frame: CHANGE_POSITION_ON_A_SCALE
ATTR 79.0 80.7 57.6 66.1
CO_VAR 55.6 64.0 22.2 31.6
DIFF 87.1 84.9 75.0 66.7
ITEM 68.6 70.3 48.0 61.3
VALUE_1 88.0 91.7 78.3 72.7
VALUE_2 93.3 90.9 89.3 85.2
Frame: KINSHIP
ALTER 87.0 89.2 87.8 87.4
EGO 96.7 98.8 96.7 95.5
Frame: PART_ORIENTATIONAL
PART 98.2 96.4 97.6 97.0
WHOLE 100 100 98.2 100
Frame: TRAVEL
AREA 31.6 52.6 25.0 45.5
GOAL 74.4 71.4 68.3 62.2
MODE 46.2 72.7 12.5 15.4
PATH 66.7 53.3 50.0 40.0
SOURCE 66.7 72.7 66.7 66.7
TIME 77.8 66.7 15.4 40.0
TRAVELER 90.9 90.6 90.9 90.6
Table 2: Exp. 1: Role-specific figures of system per-
formance for four example frames.
Results. Table 1 shows the systems? overall F-
scores and standard deviation across roles. Table 2
illustrates the differences in performance across
roles on four frames: It lists all roles with ? 5 oc-
currences for each frame. PART_ORIENTATIONAL
shows very little variance, while the roles of
CHANGE_POSITION_ON_A_SCALE and especially
TRAVEL differ widely. For KINSHIP, the system
shows good performance for both roles, but the F-
scores still differ by around 9 points.
Discussion. Table 1 shows that there is consider-
able variance across roles, with a standard devia-
tion in the range of 18% for the syntax-only model.
We note that the deviation decreases to 13% for the
combined syntax-semantics model. Table 2 con-
firms that this is not purely between-frames, but
also within-frames variance. This confirms the phe-
nomenon described at the beginning of this section.
670
fr frame
fe role (frame element)
fes(fr) roles of a frame
gfs(fr) gramm. functions of a frame
gfsfr (fe) gramm. functions realizing a role in
a frame
Table 3: Notation summary
4 A meta-model for role assignment:
Confusability
The experiment of the previous section has shown a
considerable variance in system performance across
roles. The aim of this section is to develop a meta-
model which can explain this variance.
The models we have explored in Exp. 1 rely
mainly on syntactic features: Even in the combined
syntax-semantics model, 24 of the 31 features de-
scribe syntactic structure. This predominance of
syntactic features can be observed in many current
models for semantic role assignment. Accordingly,
our meta-model focuses on the uniformity of the
mapping from syntactic structure to semantic roles.
We formalize the variance in this mapping by the
confusability of a semantic role. It implements the
following hypothesis:
(1) The semantic role assignment systems we study
approximate role assignment through gram-
matical functions.
(2) System performance for a given role depends on
the role?s confusability: A role is highly con-
fusable if the grammatical functions that in-
stantiate it often also instantiate other roles.
By using the ideal, manually assigned grammat-
ical functions that are available from the FrameNet
data ? and which are not passed on to the learner ?
our meta-model abstracts over concrete feature sets.
Our definition of confusability proceeds in two
steps. First we model the informativity of a gram-
matical function by the entropy of semantic roles
that it maps to. Then we compute the confusabil-
ity of a role as a weighted average of the entropies
of the grammatical functions that realize it.
Grammatical function entropy. Viewing a gram-
matical function as a random variable with semantic
Grammatical function entropy
GF DEG THM DEP LOC H
Mod 69 43 24 0 1.46
Comp 18 491 12 41 0.72
Ext 0 17 0 561 0.16
Head 0 0 0 273 0.0
Obj 0 0 0 3 0.0
Role Confusability
Role Mod Comp Ext Head Obj Conf
DEG 69 18 0 0 0 1.31
THM 43 491 17 0 0 0.76
DEP 24 12 0 0 0 1.22
LOC 0 41 561 273 3 0.16
Table 4: Grammatical function entropy and role con-
fusability for the frame ABUNDANCE
roles as values, we define the entropy of a grammat-
ical function gf within the frame fr as
Hfr (gf ) =
?
fe?fes(fr)
?p(fe|gf ) log p(fe|gf )
where p(fe|gf ) = f(gf ,fe)f(gf ) is the conditional proba-
bility of roles fe given gf (cf. the notation in Table 3).
Role confusability. The confusability of a role
is the sum of its grammatical function entropies,
weighted by the conditional probabilities p(gf |fe) =
f(gf ,fe)
f(fe) of grammatical functions gf given fe.
cfr (fe) =
?
gf ?gfs(fr)
p(gf |fe)Hfr (gf )
An example. Table 4 shows the grammatical func-
tion entropies and role confusabilities for the frame
ABUNDANCE, both computed on the training data.
The upper part of Table 4 lists the entropies of
the grammatical functions Mod, Comp, Ext,
Head and Obj3 and the counts f(gf, fe) of occur-
rences of the grammatical functions together with
the roles DEGREE (DEG), THEME (THM), DEPIC-
TIVE (DEP) and LOCATION (LOC). The entropy of
Mod, with similar numbers of occurrences for three
different roles, is relatively high, while Ext occurs
almost exclusively for one role and has a much lower
entropy. The lower part of Table 4 shows the confus-
ability for the same set of roles. The confusability of
3See Fillmore and Petruck (2003) for a glossary of
FrameNet?s grammatical functions.
671
DEGREE is relatively high even though it is mostly
realized by Mod because Mod has a high entropy, i.e.
it indicates multiple roles; LOCATION on the other
hand is not very confusable even though it occurs
frequently as both Ext and Head, since both gram-
matical functions indicate this role.
Related work. Our approach is similar to Pado
and Boleda (2004) in that they also use the unifor-
mity of linking as an explanation for performance
variations in semantic role assignment. However,
their analysis is located at the frame level. We ex-
amine individual roles, which allows us to derive a
simpler and more intuitive formalization of linking
uniformity. Also, our model will ultimately lead us
to a different conclusion: the uniformity of linking
is a good predictor of the performance of role as-
signment systems, but only for exclusively syntactic
models (see Section 5).
5 Experiment 2: Relating confusability
and system performance
In this section, we test the validity of our meta-
model. We assess whether confusability, defined in
Section 4, can explain the variance in role assign-
ment that we have found in Section 3, by testing the
correlation between the two variables.
Experimental setup. We use the same data set
(Senseval-3) and the same two classifiers (memory-
based and maximum entropy classification) as in
Exp. 1. To cover a wider range of models and thus
increase the validity of our analysis, we split up the
Syn feature set from Exp. 1 into the four smaller
sets described in the upper part of Table 5. We use
these sets individually, combined, and together with
the lexical features in the Sem set. This results in a
total of 20 different models (10 for each classifier),
for which we computed role-specific F-scores.
In parallel, we estimated the confusability as de-
scribed in Section 4, with FrameNet?s manually as-
signed grammatical functions as a basis, using only
the training portion of our data. We did not smooth,
but omitted roles occurring less than 5 times to
avoid sparse and thus unreliable data points. Re-
call that confusability does not vary with the feature
set, since its central asset is to abstract over concrete
model parameters and feature sets.
Feature set FMBL FMaxent
Path0 70.9 71.3
Path 73.3 72.6
Pt 78.8 79.0
Path/Pt 80.8 79.8
Path/Sibling 76.7 76.6
Pt/Sibling 78.8 79.1
Syn 82.2 82.4
Sem 80.3 80.7
Syn/Sem 87.1 87.5
Table 6: Exp. 2: Results for different feature sets
Results. The F-scores for the subdivided Syn fea-
ture set are shown in the upper part of Table 6, with
the complete Syn and Sem sets and their combina-
tion below. There is a clear relationship between
features and F-score: additional features are consis-
tently rewarded with higher performance. Interest-
ingly, phrase type information appears to be a better
role predictor than path (compare models Path and
Pt). Also, the semantic feature set alne (Sem) per-
forms at over 80% F-Score, slightly better any of the
individual syntactic feature groups.
The high F-score variance between individual
roles which we have shown for the feature sets Syn
and Syn/Sem in Exp. 1 generalizes to the other fea-
ture sets; all individual syntactic feature sets exhibit
a higher variance than Syn, and Sem shows a higher
variance than the Syn/Sem combination. This does
not come as a surprise, since the two models of
Exp. 1 use the two richest feature sets, and we would
expect less robust behavior for weaker models. An-
other point to note is that the performance of the two
learners is remarkably similar.
The high variance in the F-scores is mirrored in
the confusability figures; we obtain an average con-
fusability for our semantic roles of 1.79 with a high
standard deviation of 0.84. A scatter plot of F-scores
against confusability figures (Fig.1) suggests a linear
correlation analysis.
Analysis 1: Correlating confusability and F-
score. Since the data does not appear to be nor-
mally distributed, we apply Kendall?s nonparamet-
ric rank test. The results, which are listed in Table 7,
show an extremely significant negative correlation
between confusability and F-score: higher confus-
672
Path0 These are features centered around the path from the target lemma to the constituent: the path
itself, its length, partial path up to the lowest common ancestor, the grammatical rule that
expands the target predicate?s parent, relative position of constituent to target
Path Feature set Path, plus target lemma
Pt These are features related to phrase type and part of speech: the phrase type of the constituent
and its parent, the POS of the constituent first word, last word and head as well as the POS of
an informative content word of the constituent (for PP and SBar constituents only: the head of
the head?s complement), as well as the target lemma
Sibling Phrase type and POS of the head of the left and right sibling constituent, and the Collins parser?s
judgment on the argumenthood of the constituent
Syn This set combines Path, Sibling and Pt. Additional features are: target voice; the constituent?s
preposition; a feature combining path with target voice and target POS; and two rule-based
features judging argumenthood and grammatical function of the constituent
Sem These are lexical features: Head words of the constituent and of its left and right siblings;
leftmost and rightmost word of the constituent; informative content word lemma (see set Pt for
details); and the governing verb of the target predicate
Table 5: Feature groups used in the experiments
Figure 1: Scatter plot: F-score against confusability
(Feature set Syn).
ability appears to be related to lower F-score.
However, note that the correlation is extremely
significant even for the model which only uses se-
mantic features. This is unexpected at best and
makes a strong interpretation of this correlation
doubtful: it is rather likely that there is a third vari-
able with which both F-score and confusability are
correlated. The most obvious candidate for such a
confounding variable is the size of the training set ?
clearly, we expect our models to perform better with
larger training sets. In order to get a more realistic
MBL MaxEnt
Feature set z p z p
Path0 -11.72 10?15 -11.76 10?15
Path -12.29 10?15 -11.23 10?15
Pt -10.64 10?15 -11.12 10?15
Path/Pt -11.19 10?15 -10.45 10?15
Path/Sibling -12.65 10?15 -11.76 10?15
Pt/Sibling -10.58 10?15 -9.90 10?15
Syn -9.47 10?15 -9.38 10?15
Sem -6.90 10?11 -8.23 10?15
Syn/Sem -8.30 10?15 -8.29 10?15
Table 7: Exp. 2, Analysis 1: Correlation between F-
Score and confusability. z: Kendall?s tau coefficient,
p: significance level
assessment of the relationship between confusabil-
ity and F-score, we perform an additional analysis
to disconfound confusability and frequency.
Analysis 2: Disconfounding confusability and
frequency. One way of factoring out the influ-
ence of a confounding variable is to perform a par-
tial correlation analysis, which explicitly removes
the effects of a third variable when determining the
strength of a correlation between two variables. Like
a normal correlation analysis, it yields a partial cor-
relation coefficient.
673
MBL MaxEnt
Features rc rf rc rf
Path0 -.29??? -.03 -.29??? -.03
Path -.30??? -.02 -.27??? -.07??
Pt -.19??? -.11?? -.21??? -.12??
Path/Pt -.22??? -.07? -.19??? -.16???
Path/Sibl -.31??? +.01 -.28??? -.06?
Pt/Sibl -.20??? -.10?? -.18??? -.16???
Syn -.10? -.17??? -.12? -.19???
Sem +.01 -.27??? -.02 -.24???
Syn/Sem +.02 -.25??? -.01 -.25???
Table 8: Exp. 2, Analysis 2: Partial correlation
coefficients. rc: correlation between F-score and
confusability, controlling for training set size. rf :
correlation between F-score and training set size,
controlling for confusability. Significance levels:
???: p<0.001; ??: p<0.01; ?: p<0.05.
We first compute partial correlation coefficients
between F-score and confusability, controlling for
training set size. The results, which indicate the
?true? relationship between performance and con-
fusability, are shown in the rc columns of Table 8.
For both learners, confusability is significantly cor-
related with F-score for all syntactic feature sets, but
not for the semantic feature set and for the combined
set Syn/Sem.
We also compute the partial correlation coeffi-
cients between F-score and training set size, control-
ling for confusability. These figures are reported in
the rf columns of Table 8 and show the ?true? rela-
tionship between performance and training set size.
There is no significant correlation between training
set size and performance for simple syntax based-
models, but the correlation is highly significant for
complex syntactic models and all semantic models.
Discussion. The partial correlation analysis con-
firms that confusability is a meta-model that can ex-
plain the performance of a range of different models
for semantic role assignment, namely those models
which rely exclusively on syntactic features. Since
we used the gold standard features provided by
FrameNet and did not introduce implementation- or
feature-specific knowledge, this points to a general
limitation of syntax-based models. In contrast, se-
mantic features behave completely differently; their
contribution is not limited by a role?s confusabil-
ity. At the very least, it cannot be captured by
our current meta-model, but the absolute increase in
performance indicates that integrating semantics is
the way forward, which is surprising given that the
purely lexical features we use the present study are
usually extremely sparse.
The analysis of the partial correlation between F-
score and training set size also allows interesting
conclusions. The correlation is not significant for
small syntactic feature sets like Path, indicating that
models for such features can be learned satisfacto-
rily from relatively small training sets (but which are
also limited in expressivity). This is markedly dif-
ferent for richer feature sets. Arguably, these feature
sets are sparser and can therefore profit more from
an increased amount of training data. Again, the ef-
fect is most pronounced for the semantic feature set.
6 Conclusion
In this paper, we have formulated a meta-model for
semantic role assignment. We have used the confus-
ability of roles to predict classification performance
independently of the classification framework and
feature sets used. We have defined role confusability
in two steps: First, we have formalized the certainty
with which we can predict a semantic role from a
given grammatical function with grammatical func-
tion entropy. Then, we have defined the confusabil-
ity of a role as a weighted sum of grammatical func-
tion entropies.
We have found that role confusability is highly
significantly correlated with system performance for
models based solely on syntactic features. We con-
clude that syntactic features approximate a descrip-
tion of grammatical functions, but that semantic fea-
tures model a different aspect of the world.
Much of current research in semantic role assign-
ment is centered on the refinement of syntactic fea-
tures. Our study suggests that it may be worth-
while to explore the refinement of semantic fea-
tures as well. The most obvious choice is to in-
vestigate features related to selectional preferences.
Possible features include goodness of fit relative to
pre-computed preferences (Baldewein et al, 2004),
named entities (Pradhan et al, 2004), or broad on-
tological classes like ?animate? or ?artifact?. Fol-
674
lowing up on this idea, a natural continuation of the
present study would be to create a meta-model that
subsumes semantic features. Such a model could
use optimal selectional restrictions as a predictor.
The next step would then be to construct a combined
meta-model that describes the behavior of systems
with both syntactic and semantic features.
Another interesting research direction that our
study suggests is the combination of syntactic and
semantic models in co-training. Co-training can
be sensibly applied only when conditional indepen-
dence holds for the two target functions and the dis-
tribution (Blum and Mitchell, 1998), i.e. when it
uses two independent views on the instance set. By
pointing out a highly significant distinction between
syntactic and semantic features with respect to role
confusability, our study provides empirical evidence
that syntactic and semantic features model different
aspects of the role assignment mapping, and that co-
training may be feasible by using syntactic and se-
mantic features as views.
Acknowledgments. We are grateful to the
Deutsche Forschungsgemeinschaft (DFG) for
funding the SALSA-II project (grant PI-154/9-2).
References
U. Baldewein, K. Erk, S. Pado, D. Prescher. 2004. Se-
mantic role labelling with similarity-based generali-
sation using em-based clustering. In Proceedings of
SENSEVAL-3.
A. Blum, T. Mitchell. 1998. Combining labeled and un-
labeled data with co-training. In COLT: Proceedings
of the Workshop on Computational Learning Theory,
Morgan Kaufmann Publishers.
H. C. Boas. 2002. Bilingual framenet dictionaries for
machine translation. In Proceedings of LREC 2002,
1364?1371, Las Palmas, Canary Islands.
L. Burnard, 1995. User?s guide for the British National
Corpus. British National Corpus Consortium, Oxford
University Computing Services, 1995.
X. Carreras, L. M?rquez. 2004. Introduction to the
CoNLL-2004 shared task: semantic role labeling. In
Proceedings of CoNLL 2004, Boston, MA.
X. Carreras, L. M?rquez. 2005. Introduction to the
CoNLL-2005 shared task: semantic role labeling. In
Proceedings of CoNLL 2005, Ann Arbor, MI.
M. J. Collins. 1996. A new statistical parser based on
bigram lexical dependencies. In A. Joshi, M. Palmer,
eds., Proceedings of the Thirty-Fourth Annual Meeting
of the Association for Computational Linguistics, 184?
191, San Francisco. Morgan Kaufmann Publishers.
W. Daelemans, J. Zavrel, K. van der Sloot, A. van den
Bosch. 2003. Timbl: Tilburg memory based
learner, version 5.0, reference guide. Technical Re-
port ILK 03-10, Tilburg University, 2003. Available
from http://ilk.uvt.nl/downloads/pub/
papers/ilk0310.ps.gz.
C. J. Fillmore, M. R. Petruck. 2003. FrameNet glossary.
International Journal of Lexicography, 16:359?361.
C. J. Fillmore, C. R. Johnson, M. R. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16:235?250.
C. J. Fillmore. 1985. Frames and the semantics of under-
standing. Quaderni di Semantica, IV(2).
M. Fleischmann, E. Hovy. 2003. A maximum en-
tropy approach to framenet tagging. In Proceedings
of HLT/NAACL 2003, Edmonton, Canada.
D. Gildea, D. Jurafsky. 2002. Automatic labeling of se-
mantic roles. Computational Linguistics, 28(3):245?
288.
K. Litkowski. 2004. Senseval-3 task: Automatic label-
ing of semantic roles. In R. Mihalcea, P. Edmonds,
eds., Proceedings of Senseval-3: The Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, Barcelona, Spain.
R. Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proceedings of
CoNLL 2002, Taipei, Taiwan.
A. Moschitti. 2004. A study on convolution kernel for
shallow semantic parsing. In Proceedings of the ACL
2004, Barcelona, Spain.
S. Narayanan, S. Harabagiu. 2004. Question answering
based on semantic structures. In Proceedings of COL-
ING 2004, Geneva, Switzerland.
S. Pado, G. Boleda Torrent. 2004. The influence of ar-
gument structure on semantic role assignment. In Pro-
ceedings of EMNLP 2004, Barcelona, Spain.
S. Pradhan, W. Ward, K. Hacioglu, J. H. Martin, D. Ju-
rafsky. 2004. Shallow semantic parsing using sup-
port vector machines. In Proceedings of HLT/NAACL
2004, Boston, MA.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of NeMLaP 1994.
M. Surdeanu, S. Harabagiu, J. Williams, P. Aarseth.
2003. Using predicate-argument structures for infor-
mation extraction. In Proceedings of ACL 2003, Sap-
poro, Japan.
N. Xue, M. Palmer. 2004. Calibrating features for se-
mantic role labeling. In Proceedings of EMNLP 2004,
Barcelona, Spain.
675
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 859?866, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Cross-linguistic Projection of Role-Semantic Information
Sebastian Pad?
Computational Linguistics
Saarland University
Saarbr?cken, Germany
pado@coli.uni-sb.de
Mirella Lapata
School of Informatics
University of Edinburgh
Edinburgh, UK
mlap@inf.ed.ac.uk
Abstract
This paper considers the problem of auto-
matically inducing role-semantic annota-
tions in the FrameNet paradigm for new
languages. We introduce a general frame-
work for semantic projection which ex-
ploits parallel texts, is relatively inexpen-
sive and can potentially reduce the amount
of effort involved in creating semantic re-
sources. We propose projection models
that exploit lexical and syntactic informa-
tion. Experimental results on an English-
German parallel corpus demonstrate the
advantages of this approach.
1 Introduction
Shallow semantic parsing, the task of automatically
identifying the semantic roles conveyed by senten-
tial constituents, has recently attracted much atten-
tion, partly because of its increasing importance for
potential applications. For instance, information ex-
traction (Surdeanu et al, 2003), question answer-
ing (Narayanan and Harabagiu, 2004) and machine
translation (Boas, 2002) could stand to benefit from
broad coverage semantic processing.
The FrameNet project (Fillmore et al, 2003)
has played a central role in this endeavour by
providing a large lexical resource based on se-
mantic roles. In FrameNet, meaning is represented
by frames, schematic representations of situations.
Semantic roles are frame-specific, and are called
frame elements. The database associates frames with
lemmas (verbs, nouns, adjectives) that can evoke
them (called frame-evoking elements or FEEs), lists
the possible syntactic realisations of their seman-
tic roles, and provides annotated examples from the
British National Corpus (Burnard, 1995). The avail-
ability of rich annotations for the surface realisation
of semantic roles has triggered interest in semantic
parsing and enabled the development of data-driven
models (e.g., Gildea and Jurafsky, 2002).
Frame: DEPARTING
THEME The officer left the house.
The plane leaves at seven.
His departure was delayed.
SOURCE We departed from New York.
He retreated from his opponent.
The woman left the house.Fr
am
e
E
le
m
en
ts
F
E
E
s abandon.v, desert.v, depart.v, departure.n,
emerge.v, emigrate.v, emigration.n, escape.v,
escape.n, leave.v, quit.v, retreat.v, retreat.n,
split.v, withdraw.v, withdrawal.n
Table 1: Example of FrameNet frame
Table 1 illustrates an example from the FrameNet
database, the DEPARTING frame. It has two roles, a
THEME which is the moving object and a SOURCE
expressing the initial position of the THEME. The
frame elements are realised by different syntactic ex-
pressions. For instance, the THEME is typically an
NP, whereas the SOURCE is often expressed by a
prepositional phrase (see the expressions in boldface
in Table 1). The DEPARTING frame can be evoked
by abandon, desert, depart, and several other verbs
as well as nouns (see the list of FEEs in Table 1).
Although recent advances in semantic parsing1
have greatly benefited from the availability of the
English FrameNet, unfortunately such resources are
largely absent for other languages. The English
FrameNet (Version 1.1) contains 513 frames cov-
ering 7,125 lexical items and has been under de-
velopment for approximately six years. Although
FrameNets are currently under construction for Ger-
man, Spanish, and Japanese, these resources are still
in their infancy and of limited value for modelling
purposes. Methods for acquiring FrameNets from
corpora automatically would greatly reduce the hu-
man effort involved and facilitate their development
for new languages.
In this paper, we propose a method which em-
ploys parallel corpora for acquiring frame elements
1Approaches to modelling semantic parsing are too numer-
ous to list; see Carreras and M?rquez (2005) for an overview.
859
and their syntactic realisations (see the upper half of
Table 1) for new languages. Our method leverages
the existing English FrameNet to overcome the re-
source shortage in other languages by exploiting the
translational and structural equivalences present in
aligned data. The idea underlying our approach can
be summarised as follows: (1) given a pair of sen-
tences E (English) and L (new language) that are
translations of each other, annotate E with seman-
tic roles; and then (2) project these roles onto L. In
this manner, we induce semantic structure on the L
side of the parallel text, which can then serve as data
for training a statistical semantic parser for L that is
independent of the parallel corpus.
We first assess if the main assumption of semantic
projection is warranted (Section 3), namely whether
frames and semantic roles exhibit a high degree of
parallelism across languages. Then we propose two
broad classes of projection models that utilise lexi-
cal and syntactic information (Section 4), and show
experimentally that roles can be projected from En-
glish onto German with high accuracy (Section 5).
We conclude the paper by discussing the implica-
tions of our results and future work (Section 6).
2 Related work
A number of recent studies exploit parallel cor-
pora for cross-linguistic knowledge induction. In
this paradigm, annotations for resource-rich lan-
guages like English are projected onto another lan-
guage through aligned parallel texts. Yarowsky et
al. (2001) propose several projection algorithms for
deriving monolingual tools (ranging from part-of-
speech taggers, to chunkers and morphological anal-
ysers) without additional annotation cost. Hwa et
al. (2002) assess the degree of syntactic parallelism
in dependency relations between English and Chi-
nese. Their results show that, although assuming di-
rect correspondence is often too restrictive, syntactic
projection yields good enough annotations to train
a dependency parser. Smith and Smith (2004) ex-
plore syntactic projection further by proposing an
English-Korean bilingual parser integrated with a
word translation model.
Previous work has primarily focused on the pro-
jection of morphological and grammatico-syntactic
information. Inducing semantic resources from low
density languages still poses a significant challenge
to data-driven methods. The challenge is recognised
by Fung and Chen (2004) who construct a Chinese
FrameNet by mapping English FrameNet entries to
concepts listed in HowNet2, an on-line ontology for
Chinese, however without exploiting parallel texts.
The present work extends previous approaches on
annotation projection by inducing FrameNet seman-
tic roles from parallel corpora. Analogously to Hwa
et al (2002), we investigate whether there are indeed
semantic correspondences between two languages,
since there is little hope for projecting meaningful
annotations in nonparallel semantic structures. Sim-
ilarly to Fung and Chen (2004) we automatically in-
duce semantic role annotations for a target language.
In contrast to them, we resort to parallel corpora as a
source of semantic equivalence. Thus, we avoid the
need for a target concept dictionary in addition to the
English FrameNet. We propose a general framework
for semantic projection that can incorporate different
knowledge sources. To our knowledge, the frame-
work and its application to semantic role projection
are novel.
3 Creation of a Gold Standard Corpus
Sample Selection. To evaluate the output of our
projection algorithms, we created a gold standard
corpus of English-German sentence pairs with man-
ual FrameNet frame and role annotations. The sen-
tences were sampled from Europarl (Koehn, 2002),
a corpus of professionally translated proceedings of
the European Parliament. Europarl is available in
11 languages with up to 20 million words per lan-
guage aligned at the document and sentence level.
Recall that frame projection is only meaningful if
the same frame is appropriate for both sentences in
a projection pair. This constrains sample selection
for two reasons: first, FrameNet is as yet incom-
plete with respect to its coverage. So, a randomly
selected sentence pair may evoke novel frames or
novel senses of already existing frames (e.g., the
?greeting? sense of hail which is currently not listed
in FrameNet). Second, due to translational variance,
there is no a priori guarantee that words which are
mutual translations evoke the same frame. For ex-
ample, the English verb finish is often translated
in German by the adverb abschlie?end, which ar-
guably cannot have a role set identical to finish. Re-
lying solely on the English FrameNet database for
sampling would yield many sentence pairs which
are either inappropriate for the present study (be-
cause they do not evoke the same frames) or simply
problematic for annotation since they are outside the
2See http://www.keenage.com/zhiwang/e_zhiwang.
html.
860
present coverage of the database.
For the above reasons, our sample selection pro-
cedure was informed by two existing resources,
the English FrameNet and SALSA, a FrameNet-
compatible database for German currently under de-
velopment (Erk et al, 2003). We first used the pub-
licly available GIZA++ (Och and Ney, 2003) soft-
ware to induce English-German word alignments.
Next, we gathered all German-English sentences
in the corpus that had at least one pair of aligned
words (we,wg), which were listed in FrameNet and
SALSA, respectively, and had at least one frame
in common. These sentences exemplify 83 frame
types, 696 lemma pairs, and 265 unique English and
178 unique German lemmas. Sentence pairs were
grouped into three bands according to their frame
frequency (High, Medium, Low). We randomly se-
lected 380 pairs from each band. The total sample
consisted of ,140 sentence pairs.
This procedure produces a realistic corpus sample
for the role projection task; similar samples can be
drawn for new language pairs using either existing
bilingual dictionaries (Fung and Chen, 2004) or au-
tomatically constructed semantic lexicons (Pad? and
Lapata, 2005).
Annotation. Two annotators, with native-level
proficiency in German and English, manually la-
belled the parallel corpus with semantic information.
Their task was to identify the frame for a given pred-
icate in a sentence, and assign the corresponding
roles. They were provided with detailed guidelines
that explained the task using multiple examples.
During annotation, they had access to parsed ver-
sions of the sentences in question (see Section 5 for
details), and to the English FrameNet and SALSA.
The annotation proceeded in three phases: a train-
ing phase (40 sentences), a calibration phase (100
sentences), and a production mode phase (1000 sen-
tences). In the calibration phase, sentences were
doubly annotated to assess inter-annotator agree-
ment. In production mode, sentences were split into
two distinct sets, each of which was annotated by a
single coder. We ensured that no annotator saw both
parts of any sentence pair to guarantee independent
annotation of the bilingual data. Each coder anno-
tated approximately the same amount of data in En-
glish and German.
Table 2 shows the results of our inter-annotator
agreement study. In addition to the widely used
Kappa statistic, we computed a number of different
agreement measures: the ratio of frames common
Measure English German All
Frame Match 0.90 0.87 0.88
Role Match 0.95 0.95 0.95
Span Match 0.85 0.83 0.84
Kappa 0.86 0.90 0.87
Table 2: Monolingual inter-annotation agreement on
the calibration set
Measure Precision Recall F-score
Frame Match 0.72 0.72 0.72
Role Match 0.91 0.92 0.91
Table 3: Cross-lingual semantic parallelism between
English and German
between two sentences (Frame Match), the ratio of
common roles (Role Match), and the ratio of roles
with identical spans (Span Match). As can be seen,
annotators tend to agree in frame assignment; dis-
agreements are mainly due to fuzzy distinctions be-
tween frames (e.g., between AWARENESS and CER-
TAINTY). As can be seen from Table 2, annotators
agree in what roles to assign (Role Match is 0.95 for
both English and German); agreeing on their exact
spans is a harder problem.
Semantic Parallelism. Since we obtained par-
allel FrameNet annotations for English and German,
we were able to investigate the degree of semantic
parallelism between the two languages. More specif-
ically, we treated the German annotation as gold
standard against which we compared the English an-
notations. To facilitate comparisons with the output
of our automatic projection methods (see Section 4),
we measured parallelism using precision and recall.
Frames and frame roles were counted as matching if
they were annotated in a sentence, regardless of their
spans. The results are shown in Table 3.
The cross-lingual data exhibit more than twice the
amount of frame differences than monolingual data
(compare Tables 2 and 3). This indicates that frame
disambiguation methods must be employed in auto-
matic role projection to ensure that two aligned to-
kens evoke the same frame. However, frame disam-
biguation is outside the scope of the present paper.
On the positive side, role agreement is rela-
tively high (0.91 F-score). This indicates that in
cases where frames match across languages, seman-
tic roles could be accurately transferred (provided
that these languages diverge little in their argument
structure). This observation offers support for the
861
projection approach put forward in this paper. Note,
however, that a practical projection system could at-
tain this level of performance only if it could employ
an oracle to recover annotators? decisions about the
span of roles. We can obtain a more realistic upper
bound for an automatic system from the monolin-
gual Role Span agreement figure (F-score 0.84). The
latter represents a ceiling for the agreement we can
expect from sentences annotated by different anno-
tators.
4 Projection of Semantic Information
In this section, we formalise the semantic projection
task and give the details of our modelling approach.
All models discussed here project semantic annota-
tions from a source language to a target language.
As explained earlier, our present study is only con-
cerned with the projection of roles between match-
ing frames.
4.1 Problem Formulation
We assume that we are provided with source and tar-
get sentences represented as sets of entities es ? Es
and et ? Et . These entities can be words, con-
stituents, phrases, or other groupings. In addition,
we are given the semantic annotation of the source
sentences from which we can directly read off the
source semantic role assignment as : R? 2Es , where
R is the set of semantic roles. The goal of the pro-
jection is to specify the target semantic role assign-
ments at : R? 2Et , which are unknown.3
Clearly, effecting the projection requires estab-
lishing some form of match between the source and
target entities. We therefore formalise projection as
a function which maps the source role assignment
and a set of matches M ? Es?Et onto a new target
role assignment:
pro j : (As?M)? (R? 2
Et ) (1)
By way of currying, we can state the new target role
assignment as a function which directly computes a
set of target entities, given the source role assign-
ment, a set of entity matches, and a role:
at : (As?M?R)? 2
Et (2)
According to this formalisation, the crucial part of
semantic projection is to identify a correct and ex-
haustive set of entity matches. Obviously, this raises
3Without loss of generality, we limit ourselves to one frame
per sentence, as does FrameNet.
r ? R Semantic role
ts ? Ts, tt ? Tt Source, target tokens
al ? Al : Ts ? 2Tt Word alignment
as ? As : R? 2Ts Source role assignment
at : (As?Al?R)? 2Tt Projected target role as-
signment
Table 4: Notation and signature summary for word-
based projection
the question of what linguistic information is appro-
priate for establishingM. Unfortunately, any attempt
to compute a match based on categorical data de-
rived from linguistic analyses (e.g., parts of speech,
phrase types or grammatical relations), needs to em-
pirically derive cross-linguistic similarities between
categories, a task which must be repeated for every
new language pair, and requires additional data.
Rather than postulating an ad hoc similarity func-
tion, we use word alignments to derive informa-
tion about semantic roles in the target language. Our
first model family (Section 4.2) relies exclusively
on this knowledge source. Although potentially use-
ful as a proxy for semantic equivalence, automati-
cally induced alignments are often noisy, thus lead-
ing to errors in annotation projection (Yarowsky et
al., 2001). For example, function words commonly
diverge across languages and are systematically mis-
aligned; furthermore, alignments are restricted to
single words rather than word combinations. This
observation motivates a second model family with a
bias towards linguistically meaningful entities (Sec-
tion 4.3). Such entities can be constituents derived
from the output of a parser or non-recursive syntac-
tic structures (i.e., chunks).
In this paper we compare simple word align-
ment models against more resource intensive models
that utilise constituent-based information and exam-
ine whether syntactic knowledge significantly con-
tributes to semantic projection.
4.2 Word-based Projection Model
The first model family uses source and target word
tokens as entities for projection. In this framework,
projection models can be defined by deriving the set
of matches M directly from word alignments. The
resulting signatures are shown in Table 4.
Our first projection model assigns to each role
r with source span s(r) the set of all target tokens
which are aligned to a token in the source span:
aw(as,al,r) =
[
ts?as(r)
al(ts) (3)
862
John and Mary left
Johann und Maria gingen
Departing
Departing
Figure 1:Word alignment-based semantic projection
of Role THEME (shadowed), Frame DEPARTING
The main shortcoming of this model is that it cannot
capture an important linguistic property of semantic
roles, namely that they almost always cover contigu-
ous stretches of text. We can repair non-contiguous
projections by applying a ?convex complementing?
heuristic to the output of (3), which fills all holes
in a sequence of tokens, without explicit recourse to
syntactic information. We define the convex comple-
menting heuristic as:
acw(as,al,r) = {tt | min(i(at1))? i(tt)
?max(i(at1))}
(4)
where i returns the index of a token t.
The two models just described are illustrated in
Figure 1. The frame DEPARTING is introduced by
left and gingen in English and German, respectively.
For simplicity, we only show the edges correspond-
ing to the THEME role. In English, the THEME is re-
alised by the words John and Mary. The dotted lines
show the available word alignments. The projection
of the THEME role according to (3) consists only
of the tokens {Johann, Maria} (shown by the plain
black lines); the convex complementing heuristic in
model (4) adds the token und, resulting in the (cor-
rect) convex set {Johann, und, Maria}.
4.3 Constituent-based Projection Model
Our second model family attempts to make up for
errors in the word alignment by projecting from and
to constituents. In this study, our constituents are ob-
tained from full parse trees (see Section 5 for de-
tails). Models which use non-recursive structures are
also possible; however, we leave this to future work.
The main difference from word-based projection
models is the introduction of constituent information
as an intermediate level; we thus construct a con-
stituent alignment for which only a subset of word
alignments has to be accurate. The appropriate sig-
natures and notation for constituent-based projection
are summarised in Table 5.
In order to keep the model as flexible as pos-
sible, and to explore the influence of different de-
sign decisions, we model constituent-based projec-
tion as two independently parameterisable subtasks:
first we compute a real-valued similarity function
between source and target constituents; then, we em-
ploy the similarity function to align relevant con-
stituents and project the role information.
Similarity functions. In principle, any function
which matches the signature in Table 5 could be
used. In practice, the use of linguistic knowledge
runs into the problem of defining similarity between
category-based representations discussed above. For
this reason, we limit ourselves to two simple similar-
ity functions based on word overlap: Given source
and target constituents cs and ct , we define the word
overlap ow of cs with ct as the proportion of tokens
within ct aligned to tokens within cs. Let yield(c)
denote the set of tokens in the yield of a constituent
c, then:
ow(cs,ct) =
|(
S
ts?yield(cs) al(ts))? yield(ct)|
|yield(ct)|
(5)
Since the asymmetry of this overlap measure leads
to high overlap scores for small target constituents,
we define word overlap similarity, as the product of
two constituents? mutual overlap:
sim(cs,ct) = o(cs,ct) ?o(ct ,cs) (6)
Simple word-based overlap has one undesired char-
acteristic: larger constituents tend to be less similar
because of missing alignments (e.g., between func-
tion words). Since content words are arguably more
important for the role projection task, we define a
second overlap measure, content word overlap owc,
which takes only nouns, verbs and adjectives into
account. Let yieldc(c) denote the set of tokens in the
yield of c that are content words, then:
owc(cs,ct) =
|(
S
ts?yieldc(cs) al(ts))? yieldc(ct)|
|yieldc(ct)|
(7)
Constituent alignment. Considerable latitude
is available in interpreting a similarity function to
derive a constituent alignment. Due to space limita-
tions, we demonstrate two basic models.
Our first forward constituent alignment model
(a f c), aligns source constituents that form the span
863
r ? R Semantic role
cs ?Cs,ct ?Ct Source and target con-
stituents
yield :C ? T Yield of a constituent
yieldc :C ? T Content word yield of a
constituent
al ? Al : Ts ? 2Tt Word alignment
as ? As : R? 2Cs Source role assignment
sim :Cs?Ct ? R+ Constituent similarity
at : As?Sim?R? 2Ct Projected target role as-
signment
Table 5: Notation and signature summary for
constituent-based projection
of a role to a single target constituent. We compute
the similarity of a target constituent ct to a set of
source constituents cs ? as(r) by taking the product
similarity for each source and target constituent pair:
a f c(as,sim,r) = argmax
ct?Ct
?
cs?as(r)
sim(cs,ct) (8)
This projection model forces the target role assign-
ment to be a function, i.e., it makes the somewhat
simplifying assumption that each role corresponds
to a single target constituent.
Our second backward constituent alignment
model (abc) proceeds in the opposite direction: it it-
erates over target constituents and attempts to de-
termine their most similar source constituent for
each ct . If the aligned source constituent is labelled
with a role, it is projected onto ct :
abc(as,sim,r) = {ct |(argmax
cs?Cs
sim(cs,ct)) ? as(r)}
(9)
In general, abc allows for more flexible role pro-
jection: it will sometimes decide not to project a
role at all (if the source constituents are dissimilar
to any target constituents), or it can assign a role
to more than one target constituent; however, this
means that there is less control over what is pro-
jected, and wrong alignments can lead to wrong re-
sults more easily.
Finally, if no word alignments are found for
complete source or target constituents, the maxi-
mal similarity rating in abc or ab f will be zero.
This is often the case for semantically weak single-
word constituents such as demonstrative pronouns
(e.g., [That] is right./ [Das] ist richtig.). When we
observe this phenomenon, we heuristically skip un-
aligned constituents (zero skipping).
Figure 2 contrasts the two constituent-based pro-
jection models using the frame QUESTIONING as
He asked all of them
Er fragte alle von ihnen
NP
3
PP
2
NP
1
NP
4
PP
5
NP
6
Questioning
Questioning
NP1 PP2 NP3
NP4 0.33 0.5 1
PP5 0.67 1 0.5
NP6 0.33 0 0
Figure 2: Constituent-based semantic projection of
role ADDRESSEE (shadowed), frame QUESTION-
ING. Below: Constituent similarity matrix.
an example. Again, we only show one role, AD-
DRESSEE, indicated by the shadowed box in Fig-
ure 2. Note that the object NP in German was mis-
parsed as an NP and a PP, a relatively frequent er-
ror. The difference between the two decision proce-
dures can be explained straightforwardly by look-
ing at the table below the graph, which shows the
similarity matrix for the constituents according to
equation (6). In this table, the source constituents
(indices 1?3) correspond to columns, and the tar-
get constituents (indices 4?6) to rows. The align-
ment model in (8) iterates over labelled source con-
stituents (here only NP1) and chooses the row with
the highest value as the target constituent for a can-
didate role. In our case, this is the PP5 (cell in bold-
face). In contrast, model (9) iterates over all target
constituents (i.e., rows) and checks if the most sim-
ilar source constituent bears a role label. Since NP1
is the most similar constituent for NP6 (underlined
cell), (9) assigns the QUESTIONING role to NP6.
5 Experiments
Evaluation Framework. We implemented the
models described in the previous section and used
them to project semantic information from En-
glish onto German. For the constituent-based mod-
els, constituent information was obtained from the
output of Collins? parser (1997) for English and
Dubey?s parser (2004) for German. Words were
864
Model Precision Recall F-score
w 0.41 0.40 0.41
cw 0.46 0.45 0.46
Upper bound 0.85 0.84 0.84
Table 6: Results for word-based projection models
aligned using the default setting4 of GIZA++ (Och
and Ney, 2003), a publicly available implementa-
tion of the IBM models and HMM word alignment
models. We evaluated the projected roles against the
?gold standard? roles obtained from the manual an-
notation (see Section 3). We also compared our re-
sults to the upper bound given by the inter-annotator
agreement on the calibration data set.
Results. Table 6 shows our results for the word-
based projection models. The simplest word-based
model (aw), obtains an F-score of 0.41. This is a
good result considering that the model does not ex-
ploit any linguistic information (e.g., parts of speech
or syntactic structure). It also supports our hypothe-
sis that word alignments are useful for the role pro-
jection task. The convex complementing heuristic
(acw) delivers an F-score increase of five points over
the ?words only? model, simply by making up for
holes in the word alignment.
We evaluated eight instantiations of the
constituent-based projection models; the results are
shown in Table 7. The best model (in boldface) uses
forward constituent alignment, content word-based
overlap similarity, and zero skipping. We observe
that backward constituent alignment-based models
(1?4) perform similarly to word-based projection
models (the F-score ranges between 0.40 and 0.45).
However, they obtain considerably higher precision
(albeit lower recall) than the word-based models.
This may be an advantage if the projected data
is destined for training target-language semantic
parsers. This precision/recall pattern appears to be
a direct result of abc, which only projects a role
from cs to ct if cs ?wins? against all other source
constituents, thus resulting in reliable, but overly
cautious projections, which cannot not be further
improved by zero skipping.
The forward constituent alignment models (5?8)
show consistently higher performance than word-
based models and models 1?4, indicating that the
stronger assumptions made by forward alignment
4The training scheme involved five iterations of Model 1,
five iterations of the HMM model, five iterations of Model 3,
and five iterations of Model 4.
Model al o 0-skip Precision Recall F-score
1 bc w no 0.70 0.33 0.45
2 bc w yes 0.70 0.33 0.45
3 bc wc no 0.65 0.32 0.42
4 bc wc yes 0.65 0.32 0.42
5 f c w no 0.61 0.60 0.60
6 f c w yes 0.66 0.60 0.63
7 f c wc no 0.62 0.60 0.61
8 fc wc yes 0.70 0.60 0.65
Upper bound 0.85 0.84 0.84
Table 7: Results for constituent-based projection
models (al: constituent alignment model; o: overlap
measure; 0-skip: zero skipping)
are justified in the data. In addition, we also find
that we can increase precision by concentrating on
reliable alignments. This is achieved by using the
zero skipping heuristic (compare the odd vs. even-
numbered models in Table 7) and by computing
overlap on content words (compare Models 6 vs. 8,
and 5 vs. 7).
We used the ?2 test to examine whether the dif-
ferences observed between the two classes of mod-
els are statistically significant. The best constituent-
based model significantly outperforms the best
word-based model both in terms of precision
(?2 = 114.47, p < 0.001) and recall (?2 = 400.40,
p < 0.001). Both projection models perform signifi-
cantly worse than humans (p < 0.001).
Discussion. Our results confirm that constituent
information is important for the semantic projection
task. Our best model adopts a conservative strat-
egy which enforces a one-to-one correspondence be-
tween roles and target constituents. This strategy
leads to high precision, however recall lags behind
(see Model 8 in Table 7). Manual inspection of the
projection output revealed that an important source
of missing roles are word alignments gaps. Such
gaps are not only due to noisy alignments, but also
reflect genuine structural differences between trans-
lated sentences. Consider the following (simplified)
example for the STATEMENT frame (introduced by
say) and its semantic role STATEMENT (introduced
by we):
(10) We
Wir
claim
behaupten
X
X
and
und
we
?
say
sagen
Y
Y
The word alignment correctly aligns the German
pronoun wir with the first English we and leaves
865
the second occurrence unaligned. Since there is no
corresponding German word for the second we, pro-
jection of the SPEAKER role fails. In future work,
this problem could be handled with explicit identi-
fication of empty categories (see Dienes and Dubey,
2003).
6 Conclusions
In this paper, we argue that parallel corpora show
promise in relieving the lexical acquisition bottle-
neck for low density languages. We proposed se-
mantic projection as a means of obtaining FrameNet
annotations automatically without additional human
effort. We examined semantic parallelism, a prereq-
uisite for accurate projection, and showed that se-
mantic roles can be successfully projected for pred-
icate pairs with matching frame assignments. Sim-
ilarly to previous work (Hwa et al, 2002), we find
that some mileage can be gained by assuming di-
rect correspondence between two languages. How-
ever, linguistic knowledge is key in obtaining mean-
ingful projections. Our experiments show that the
use of constituent information yields substantial im-
provements over relying on word alignment alone.
Nevertheless, the word-based models offer a good
starting point for low-density languages for which
parsers are not available. Their output could be fur-
ther post-processed manually or automatically using
bootstrapping techniques (Riloff and Jones, 1999).
We have presented a general, flexible framework
for semantic projection which can be easily applied
to other languages. An important direction for fu-
ture work lies in the assessment of more shallow
syntactic information (i.e., chunks) which can be ob-
tained more easily for new languages, and generally
in the integration of more linguistic knowledge to
guide projection. Finally, we will incorporate into
our projection approach automatic semantic role an-
notations for the source language and investigate the
potential of the projected annotations for training se-
mantic parsers for the target language.
Acknowledgements. The authors acknowledge
the support of DFG (Pad?; grant PI-154/9-2) and
EPSRC (Lapata; grant GR/T04540/01). Thanks to
B. Kouchnir and P. Kreischer for their annotation.
References
H. C. Boas. 2002. Bilingual framenet dictionaries for
machine translation. In Proceedings of LREC 2002,
1364?1371, Las Palmas, Canary Islands.
L. Burnard, 1995. The Users Reference Guide for the
British National Corpus. British National Corpus
Consortium, Oxford University Computing Service,
1995.
X. Carreras, L. M?rquez, eds. 2005. Proceedings of the
CoNLL shared task: Semantic role labelling, 2005.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of ACL/EACL
1997, 16?23, Madrid, Spain.
P. Dienes, A. Dubey. 2003. Antecedent recovery: Exper-
iments with a trace tagger. In Proceedings of EMNLP
2003, 33?40, Sapporo, Japan.
A. Dubey. 2004. Statistical parsing for German: Mod-
elling syntactic properties and annotation differences.
Ph.D. thesis, Saarland University.
K. Erk, A. Kowalski, S. Pad?, M. Pinkal. 2003. Towards
a resource for lexical semantics: A large German cor-
pus with extensive semantic annotation. In Proceed-
ings of ACL 2003, 537?544, Sapporo, Japan.
C. J. Fillmore, C. R. Johnson, M. R. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16:235?250.
P. Fung, B. Chen. 2004. BiFrameNet: Bilingual frame
semantics resources construction by cross-lingual in-
duction. In Proceedings of COLING 2004, 931?935,
Geneva, Switzerland.
D. Gildea, D. Jurafsky. 2002. Automatic labeling of se-
mantic roles. Computational Linguistics, 28(3):245?
288.
R. Hwa, P. Resnik, A. Weinberg, O. Kolak. 2002. Eval-
uation translational correspondance using annotation
projection. In Proceedings of ACL 2002, 392?399,
Philadelphia, PA.
P. Koehn. 2002. Europarl: A multilingual corpus for
evaluation of machine translation. Draft.
S. Narayanan, S. Harabagiu. 2004. Question answering
based on semantic structures. In Proceedings of COL-
ING 2004, 693?701, Geneva, Switzerland.
F. J. Och, H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?52.
S. Pad?, M. Lapata. 2005. Cross-lingual bootstrapping
for semantic lexicons. In Proceedings of AAAI 2005,
Pittsburgh, PA.
E. Riloff, R. Jones. 1999. Learning dictionaries for in-
formation extraction by multi-level bootstrapping. In
Proceedings of AAAI 1999, Orlando, FL.
D. A. Smith, N. A. Smith. 2004. Bilingual parsing with
factored estimation: Using English to parse Korean.
In Proceedings of EMNLP 2004, 49?56, Barcelona,
Spain.
M. Surdeanu, S. Harabagiu, J. Williams, P. Aarseth.
2003. Using predicate-argument structures for infor-
mation extraction. In Proceedings of ACL 2003, 8?15,
Sapporo, Japan.
D. Yarowsky, G. Ngai, R. Wicentowski. 2001. Inducing
multilingual text analysis tools via robust projection
across aligned corpora. In Proceedings of HLT 2001,
161?168.
866
Constructing Semantic Space Models from Parsed Corpora
Sebastian Pad?
Department of Computational Linguistics
Saarland University
PO Box 15 11 50
66041 Saarbr?cken, Germany
pado@coli.uni-sb.de
Mirella Lapata
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello Street
Sheffield S1 4DP, UK
mlap@dcs.shef.ac.uk
Abstract
Traditional vector-based models use word
co-occurrence counts from large corpora
to represent lexical meaning. In this pa-
per we present a novel approach for con-
structing semantic spaces that takes syn-
tactic relations into account. We introduce
a formalisation for this class of models
and evaluate their adequacy on two mod-
elling tasks: semantic priming and auto-
matic discrimination of lexical relations.
1 Introduction
Vector-based models of word co-occurrence have
proved a useful representational framework for a
variety of natural language processing (NLP) tasks
such as word sense discrimination (Sch?tze, 1998),
text segmentation (Choi et al, 2001), contextual
spelling correction (Jones and Martin, 1997), auto-
matic thesaurus extraction (Grefenstette, 1994), and
notably information retrieval (Salton et al, 1975).
Vector-based representations of lexical meaning
have been also popular in cognitive science and
figure prominently in a variety of modelling stud-
ies ranging from similarity judgements (McDonald,
2000) to semantic priming (Lund and Burgess, 1996;
Lowe and McDonald, 2000) and text comprehension
(Landauer and Dumais, 1997).
In this approach semantic information is extracted
from large bodies of text under the assumption that
the context surrounding a given word provides im-
portant information about its meaning. The semantic
properties of words are represented by vectors that
are constructed from the observed distributional pat-
terns of co-occurrence of their neighbouring words.
Co-occurrence information is typically collected in
a frequency matrix, where each row corresponds to
a unique target word and each column represents its
linguistic context.
Contexts are defined as a small number of words
surrounding the target word (Lund and Burgess,
1996; Lowe and McDonald, 2000) or as entire para-
graphs, even documents (Landauer and Dumais,
1997). Context is typically treated as a set of
unordered words, although in some cases syntac-
tic information is taken into account (Lin, 1998;
Grefenstette, 1994; Lee, 1999). A word can be
thus viewed as a point in an n-dimensional semantic
space. The semantic similarity between words can
be then mathematically computed by measuring the
distance between points in the semantic space using
a metric such as cosine or Euclidean distance.
In the variants of vector-based models where no
linguistic knowledge is used, differences among
parts of speech for the same word (e.g., to drink
vs. a drink ) are not taken into account in the con-
struction of the semantic space, although in some
cases word lexemes are used rather than word sur-
face forms (Lowe and McDonald, 2000; McDonald,
2000). Minimal assumptions are made with respect
to syntactic dependencies among words. In fact it is
assumed that all context words within a certain dis-
tance from the target word are semantically relevant.
The lack of syntactic information makes the build-
ing of semantic space models relatively straightfor-
ward and language independent (all that is needed is
a corpus of written or spoken text). However, this
entails that contextual information contributes indis-
criminately to a word?s meaning.
Some studies have tried to incorporate syntactic
information into vector-based models. In this view,
the semantic space is constructed from words that
bear a syntactic relationship to the target word of in-
terest. This makes semantic spaces more flexible,
different types of contexts can be selected and words
do not have to physically co-occur to be considered
contextually relevant. However, existing models ei-
ther concentrate on specific relations for construct-
ing the semantic space such as objects (e.g., Lee,
1999) or collapse all types of syntactic relations
available for a given target word (Grefenstette, 1994;
Lin, 1998). Although syntactic information is now
used to select a word?s appropriate contexts, this in-
formation is not explicitly captured in the contexts
themselves (which are still represented by words)
and is therefore not amenable to further processing.
A commonly raised criticism for both types of se-
mantic space models (i.e., word-based and syntax-
based) concerns the notion of semantic similarity.
Proximity between two words in the semantic space
cannot indicate the nature of the lexical relations be-
tween them. Distributionally similar words can be
antonyms, synonyms, hyponyms or in some cases
semantically unrelated. This limits the application
of semantic space models for NLP tasks which re-
quire distinguishing between lexical relations.
In this paper we generalise semantic space models
by proposing a flexible conceptualisation of context
which is parametrisable in terms of syntactic rela-
tions. We develop a general framework for vector-
based models which can be optimised for different
tasks. Our framework allows the construction of se-
mantic space to take place over words or syntactic
relations thus bridging the distance between word-
based and syntax-based models. Furthermore, we
show how our model can incorporate well-defined,
informative contexts in a principled way which re-
tains information about the syntactic relations avail-
able for a given target word.
We first evaluate our model on semantic prim-
ing, a phenomenon that has received much attention
in computational psycholinguistics and is typically
modelled using word-based semantic spaces. We
next conduct a study that shows that our model is
sensitive to different types of lexical relations.
2 Dependency-based Vector Space Models
Once we move away from words as the basic con-
text unit, the issue of representation of syntactic in-
formation becomes pertinent. Information about the
dependency relations between words abstracts over
word order and can be considered as an intermediate
layer between surface syntax and semantics. More
Det
a
N
lorry
Aux
might
V
carry
A
sweet
N
apples
subj
det
aux
obj
mo
d
Figure 1: A dependency parse of a short sentence
formally, dependencies are asymmetric binary rela-
tionships between a head and a modifier (Tesni?re,
1959). The structure of a sentence can be repre-
sented by a set of dependency relationships that form
a tree as shown in Figure 1. Here the head of the sen-
tence is the verb carry which is in turn modified by
its subject lorry and its object apples.
It is the dependencies in Figure 1 that will form
the context over which the semantic space will be
constructed. The construction mechanism sets out
by identifying the local context of a target word,
which is a subset of all dependency paths starting
from it. The paths consist of the dependency edges
of the tree labelled with dependency relations such
as subj, obj, or aux (see Figure 1). The paths can be
ranked by a path value function which gives differ-
ent weight to different dependency types (for exam-
ple, it can be argued that subjects and objects convey
more semantic information than determiners). Tar-
get words are then represented in terms of syntactic
features which form the dimensions of the seman-
tic space. Paths are mapped to features by the path
equivalence relation and the appropriate cells in the
matrix are incremented.
2.1 Definition of Semantic Space
We assume the semantic space formalisation pro-
posed by Lowe (2001). A semantic space is a matrix
whose rows correspond to target words and columns
to dimensions which Lowe calls basis elements:
Definition 1. A Semantic Space Model is a matrix
K = B?T , where bi ? B denotes the basis element
of column i, t j ? T denotes the target word of row j,
and Ki j the cell (i, j).
T is the set of words for which the matrix con-
tains representations; this can be either word types
or word tokens. In this paper, we assume that co-
occurrence counts are constructed over word types,
but the framework can be easily adapted to represent
word tokens instead.
In traditional semantic spaces, the cells Ki j of
the matrix correspond to word co-occurrence counts.
This is no longer the case for dependency-based
models. In the following we explain how co-
occurrence counts are constructed.
2.2 Building the Context
The first step in constructing a semantic space from
a large collection of dependency relations is to con-
struct a word?s local context.
Definition 2. The dependency parse p of a sentence
s is an undirected graph p(s) = (Vp,Ep). The set of
nodes corresponds to words of the sentence: Vp =
{w1, . . . ,wn}. The set of edges is Ep ?Vp ?Vp.
Definition 3. A class q is a three-tuple consisting
of a POS-tag, a relation, and another POS-tag. We
write Q for the set of all classes Cat ?R?Cat. For
each parse p, the labelling function Lp : Ep ? Q as-
signs a class to every edge of the parse.
In Figure 1, the labelling function labels the left-
most edge as Lp((a, lorry)) = ?Det,det,N?. Note that
Det represents the POS-tag ?determiner? and det the
dependency relation ?determiner?.
In traditional models, the target words are sur-
rounded by context words. In a dependency-based
model, the target words are surrounded by depen-
dency paths.
Definition 4. A path ? is an ordered tuple of edges
?e1, . . . ,en? ? Enp so that
? i : (ei?1 = (v1,v2) ? ei = (v3,v4)) ? v2 = v3
Definition 5. A path anchored at a word w is a path
?e1, . . . ,en? so that e1 = (v1,v2) and w = v1. Write
?w for the set of all paths over Ep anchored at w.
In words, a path is a tuple of connected edges in
a parse graph and it is anchored at w if it starts at w.
In Figure 1, the set of paths anchored at lorry 1 is:
{?(lorry,carry)?,?(lorry,carry),(carry,apples)?,
?(lorry,a)?,?(lorry,carry),(carry,might)?, . . .}
The local context of a word is the set or a subset of
its anchored paths. The class information can always
be recovered by means of the labelling function.
Definition 6. A local context of a word w from a
sentence s is a subset of the anchored paths at w. A
function c : W ? 2?w which assigns a local context
to a word is called a context specification function.
1For the sake of brevity, we only show paths up to length 2.
The context specification function allows to elim-
inate paths on the basis of their classes. For exam-
ple, it is possible to eliminate all paths from the set
of anchored paths but those which contain immedi-
ate subject and direct object relations. This can be
formalised as:
c(w) = {? ? ?w |? = ?e??
(Lp(e) = ?V,obj,N??Lp(e) = ?V,subj,N?)}
In Figure 1, the labels of the two edges which
form paths of length 1 and conform to this context
specification are marked in boldface. Notice that the
local context of lorry contains only one anchored
path (c(lorry) = {?(lorry,carry)?}).
2.3 Quantifying the Context
The second step in the construction of the
dependency-based semantic models is to specify the
relative importance of different paths. Linguistic in-
formation can be incorporated into our framework
through the path value function.
Definition 7. The path value function v assigns a
real number to a path: v : ? ? R.
For instance, the path value function could pe-
nalise longer paths for only expressing indirect re-
lationships between words. An example of a length-
based path value function is v(?) = 1
n
where ? =
?e1, . . . ,en?. This function assigns a value of 1 to the
one path from c(lorry) and fractions to longer paths.
Once the value of all paths in the local context
is determined, the dimensions of the space must be
specified. Unlike word-based models, our contexts
contain syntactic information and dimensions can
be defined in terms of syntactic features. The path
equivalence relation combines functionally equiva-
lent dependency paths that share a syntactic feature
into equivalence classes.
Definition 8. Let ? be the path equivalence relation
on ?. The partition induced by this equivalence re-
lation is the set of basis elements B.
For example, it is possible to combine all paths
which end at the same word: A path which starts
at wi and ends at w j, irrespectively of its length and
class, will be the co-occurrence of wi and w j. This
word-based equivalence function can be defined in
the following manner:
?(v1,v2), . . . ,(vn?1,vn)? ? ?(v?1,v?2), . . . ,(v?m?1,v?m)?
iff vn = v?m
This means that in Figure 1 the set of basis elements
is the set of words at which paths end. Although co-
occurrence counts are constructed over words like in
traditional semantic space models, it is only words
which stand in a syntactic relationship to the target
that are taken into account.
Once the value of all paths in the local context
is determined, the local observed frequency for the
co-occurrence of a basis element b with the target
word w is just the sum of values of all paths ? in
this context which express the basis element b. The
global observed frequency is the sum of the local
observed frequencies for all occurrences of a target
word type t and is therefore a measure for the co-
occurrence of t and b over the whole corpus.
Definition 9. Global observed frequency:
?f (b, t) = ?
w?W (t)
?
??C(w)???b
v(?)
As Lowe (2001) notes, raw frequency counts are
likely to give misleading results. Due to the Zip-
fian distribution of word types, words occurring
with similar frequencies will be judged more similar
than they actually are. A lexical association func-
tion can be used to explicitly factor out chance co-
occurrences.
Definition 10. Write A for the lexical association
function which computes the value of a cell of the
matrix from a co-occurrence frequency:
Ki j = A( ?f (bi, t j))
3 Evaluation
3.1 Parameter Settings
All our experiments were conducted on the British
National Corpus (BNC), a 100 million word col-
lection of samples of written and spoken language
(Burnard, 1995). We used Lin?s (1998) broad cover-
age dependency parser MINIPAR to obtain a parsed
version of the corpus. MINIPAR employs a man-
ually constructed grammar and a lexicon derived
from WordNet with the addition of proper names
(130,000 entries in total). Lexicon entries con-
tain part-of-speech and subcategorization informa-
tion. The grammar is represented as a network of
35 nodes (i.e., grammatical categories) and 59 edges
(i.e., types of syntactic (dependency) relationships).
MINIPAR uses a distributed chart parsing algorithm.
Grammar rules are implemented as constraints asso-
ciated with the nodes and edges.
Cosine distance cos(~x,~y) = ?i xiyi?
?i x2i
?
?i y2i
Skew divergence s?(~x,~y) = ?i xi log xi?xi+(1??)yi
Figure 2: Distance measures
The dependency-based semantic space was con-
structed with the word-based path equivalence func-
tion from Section 2.3. As basis elements for our se-
mantic space the 1000 most frequent words in the
BNC were used. Each element of the resulting vec-
tor was replaced with its log-likelihood value (see
Definition 10 in Section 2.3) which can be consid-
ered as an estimate of how surprising or distinctive
a co-occurrence pair is (Dunning, 1993).
We experimented with a variety of distance mea-
sures such as cosine, Euclidean distance, L1 norm,
Jaccard?s coefficient, Kullback-Leibler divergence
and the Skew divergence (see Lee 1999 for an
overview). We obtained the best results for co-
sine (Experiment 1) and Skew divergence (Experi-
ment 2). The two measures are shown in Figure 2.
The Skew divergence represents a generalisation of
the Kullback-Leibler divergence and was proposed
by Lee (1999) as a linguistically motivated distance
measure. We use a value of ? = .99.
We explored in detail the influence of different
types and sizes of context by varying the context
specification and path value functions. Contexts
were defined over a set of 23 most frequent depen-
dency relations which accounted for half of the de-
pendency edges found in our corpus. From these,
we constructed four context specification functions:
(a) minimum contexts containing paths of length 1
(in Figure 1 sweet and carry are the minimum con-
text for apples), (b) np context adds dependency in-
formation relevant for noun compounds to minimum
context, (c) wide takes into account paths of length
longer than 1 that represent meaningful linguistic re-
lations such as argument structure, but also prepo-
sitional phrases and embedded clauses (in Figure 1
the wide context of apples is sweet, carry, lorry, and
might ), and (d) maximum combined all of the above
into a rich context representation.
Four path valuation functions were used: (a) plain
assigns the same value to every path, (b) length
assigns a value inversely proportional to a path?s
length, (c) oblique ranks paths according to the
obliqueness hierarchy of grammatical relations
(Keenan and Comrie, 1977), and (d) oblength
context specification path value function
1 minimum plain
2 minimum oblique
3 np plain
4 np length
5 np oblique
6 np oblength
7 wide plain
8 wide length
9 wide oblique
10 wide oblength
11 maximum plain
12 maximum length
13 maximum oblique
14 maximum oblength
Table 1: The fourteen models
combines length and oblique . The resulting 14
parametrisations are shown in Table 1. Length-
based and length-neutral path value functions are
collapsed for the minimum context specification
since it only considers paths of length 1.
We further compare in Experiments 1 and 2 our
dependency-based model against a state-of-the-art
vector-based model where context is defined as a
?bag of words?. Note that considerable latitude is
allowed in setting parameters for vector-based mod-
els. In order to allow a fair comparison, we se-
lected parameters for the traditional model that have
been considered optimal in the literature (Patel et al,
1998), namely a symmetric 10 word window and
the most frequent 500 content words from the BNC
as dimensions. These parameters were similar to
those used by Lowe and McDonald (2000) (symmet-
ric 10 word window and 536 content words). Again
the log-likelihood score is used to factor out chance
co-occurrences.
3.2 Experiment 1: Priming
A large number of modelling studies in psycholin-
guistics have focused on simulating semantic prim-
ing studies. The semantic priming paradigm pro-
vides a natural test bed for semantic space models
as it concentrates on the semantic similarity or dis-
similarity between a prime and its target, and it is
precisely this type of lexical relations that vector-
based models capture.
In this experiment we focus on Balota and Lorch?s
(1986) mediated priming study. In semantic priming
transient presentation of a prime word like tiger di-
rectly facilitates pronunciation or lexical decision on
a target word like lion. Mediated priming extends
this paradigm by additionally allowing indirectly re-
lated words as primes ? like stripes, which is only
related to lion by means of the intermediate concept
tiger. Balota and Lorch (1986) obtained small medi-
ated priming effects for pronunciation tasks but not
for lexical decision. For the pronunciation task, re-
action times were reduced significantly for both di-
rect and mediated primes, however the effect was
larger for direct primes.
There are at least two semantic space simulations
that attempt to shed light on the mediated priming
effect. Lowe and McDonald (2000) replicated both
the direct and mediated priming effects, whereas
Livesay and Burgess (1997) could only replicate di-
rect priming. In their study, mediated primes were
farther from their targets than unrelated words.
3.2.1 Materials and Design
Materials were taken form Balota and Lorch
(1986). They consist of 48 target words, each paired
with a related and a mediated prime (e.g., lion-tiger-
stripes). Each related-mediated prime tuple was
paired with an unrelated control randomly selected
from the complement set of related primes.
3.2.2 Procedure
One stimulus was removed as it had a low cor-
pus frequency (less than 100), which meant that
the resulting vector would be unreliable. We con-
structed vectors from the BNC for all stimuli with
the dependency-based models and the traditional
model, using the parametrisations given in Sec-
tion 3.1 and cosine as a distance measure. We calcu-
lated the distance in semantic space between targets
and their direct primes (TarDirP), targets and their
mediated primes (TarMedP), targets and their unre-
lated controls (TarUnC) for both models.
3.2.3 Results
We carried out a one-way Analysis of Variance
(ANOVA) with the distance as dependent variable
(TarDirP, TarMedP, TarUnC). Recall from Table 1
that we experimented with fourteen different con-
text definitions. A reliable effect of distance was
observed for all models (p < .001). We used the
?2 statistic to calculate the amount of variance ac-
counted for by the different models. Figure 3 plots
?2 against the different contexts. The best result
was obtained for model 7 which accounts for 23.1%
of the variance (F(2,140) = 20.576, p < .001) and
corresponds to the wide context specification and
the plain path value function. A reliable distance
effect was also observed for the traditional vector-
based model (F(2,138) = 9.384, p < .001).
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 1  2  3  4  5  6  7  8  9  10  11  12  13  14
e
ta
 s
qu
ar
ed
model
TarDirP -- TarMedP -- TarUnC
TarDirP -- TarUnC
 TarMedP -- TarUnC
Figure 3: ?2 scores for mediated priming materials
Model TarDirP ? TarUnC TarMedP ? TarUnC
Model 7 F = 25.290 (p < .001) F = .001 (p = .790)
Traditional F = 12.185 (p = .001) F = .172 (p = .680)
L & McD F = 24.105 (p < .001) F = 13.107 (p < .001)
Table 2: Size of direct and mediated priming effects
Pairwise ANOVAs were further performed to ex-
amine the size of the direct and mediated priming ef-
fects individually (see Table 2). There was a reliable
direct priming effect (F(1,94) = 25.290, p < .001)
but we failed to find a reliable mediated priming
effect (F(1,93) = .001, p = .790). A reliable di-
rect priming effect (F(1,92) = 12.185, p = .001)
but no mediated priming effect was also obtained for
the traditional vector-based model. We used the ?2
statistic to compare the effect sizes obtained for the
dependency-based and traditional model. The best
dependency-based model accounted for 23.1% of
the variance, whereas the traditional model ac-
counted for 12.2% (see also Table 2).
Our results indicate that dependency-based mod-
els are able to model direct priming across a wide
range of parameters. Our results also show that
larger contexts (see models 7 and 11 in Figure 3) are
more informative than smaller contexts (see mod-
els 1 and 3 in Figure 3), but note that the wide con-
text specification performed better than maximum. At
least for mediated priming, a uniform path value as
assigned by the plain path value function outper-
forms all other functions (see Figure 3).
Neither our dependency-based model nor the tra-
ditional model were able to replicate the mediated
priming effect reported by Lowe and McDonald
(2000) (see L & McD in Table 2). This may be
due to differences in lemmatisation of the BNC,
the parametrisations of the model or the choice of
context words (Lowe and McDonald use a spe-
cial procedure to identify ?reliable? context words).
Our results also differ from Livesay and Burgess
(1997) who found that mediated primes were fur-
ther from their targets than unrelated controls, us-
ing however a model and corpus different from the
ones we employed for our comparative studies. In
the dependency-based model, mediated primes were
virtually indistinguishable from unrelated words.
In sum, our results indicate that a model which
takes syntactic information into account outper-
forms a traditional vector-based model which sim-
ply relies on word occurrences. Our model is able
to reproduce the well-established direct priming ef-
fect but not the more controversial mediated prim-
ing effect. Our results point to the need for further
comparative studies among semantic space models
where variables such as corpus choice and size as
well as preprocessing (e.g., lemmatisation, tokeni-
sation) are controlled for.
3.3 Experiment 2: Encoding of Relations
In this experiment we examine whether dependency-
based models construct a semantic space that encap-
sulates different lexical relations. More specifically,
we will assess whether word pairs capturing differ-
ent types of semantic relations (e.g., hyponymy, syn-
onymy) can be distinguished in terms of their dis-
tances in the semantic space.
3.3.1 Materials and Design
Our experimental materials were taken from
Hodgson (1991) who in an attempt to investigate
which types of lexical relations induce priming col-
lected a set of 142 word pairs exemplifying the fol-
lowing semantic relations: (a) synonymy (words
with the same meaning, value and worth ), (b) su-
perordination and subordination (one word is an in-
stance of the kind expressed by the other word, pain
and sensation), (c) category coordination (words
which express two instances of a common super-
ordinate concept, truck and train), (d) antonymy
(words with opposite meaning, friend and enemy),
(e) conceptual association (the first word subjects
produce in free association given the other word,
leash and dog), and (f) phrasal association (words
which co-occur in phrases private and property).
The pairs were selected to be unambiguous exam-
ples of the relation type they instantiate and were
matched for frequency. The pairs cover a wide range
of parts of speech, like adjectives, verbs, and nouns.
 0.14
 0.15
 0.16
 0.17
 0.18
 0.19
 0.2
 0.21
 1  2  3  4  5  6  7  8  9  10  11  12  13  14
e
ta
 s
qu
ar
ed
model
Hodgson skew divergence
Figure 4: ?2 scores for the Hodgson materials
Mean PA SUP CO ANT SYN
CA 16.25 ? ? ? ?
PA 15.13 ? ?
SUP 11.04
CO 10.45
ANT 10.07
SYN 8.87
Table 3: Mean skew divergences and Tukey test re-
sults for model 7
3.3.2 Procedure
As in Experiment 1, six words with low fre-
quencies (less than 100) were removed from the
materials. Vectors were computed for the re-
maining 278 words for both the traditional and
the dependency-based models, again with the
parametrisations detailed in Section 3.1. We calcu-
lated the semantic distance for every word pair, this
time using Skew divergence as distance measure.
3.3.3 Results
We carried out an ANOVA with the lexical rela-
tion as factor and the distance as dependent variable.
The lexical relation factor had six levels, namely the
relations detailed in Section 3.3.1. We found no ef-
fect of semantic distance for the traditional semantic
space model (F(5,141) = 1.481, p = .200). The ?2
statistic revealed that only 5.2% of the variance was
accounted for. On the other hand, a reliable effect
of distance was observed for all dependency-based
models (p < .001). Model 7 (wide context specifi-
cation and plain path value function) accounted for
the highest amount of variance in our data (20.3%).
Our results can be seen in Figure 4.
We examined whether there are any significant
differences among the six relations using Post-hoc
Tukey tests. The pairwise comparisons for model 7
are given in Table 3. The mean distances for concep-
tual associates (CA), phrasal associates (PA), super-
ordinates/subordinates (SUP), category coordinates
(CO), antonyms (ANT), and synonyms (SYN) are
also shown in Table 3. There is no significant differ-
ence between PA and CA, although SUP, CO, ANT,
and SYN, are all significantly different from CA (see
Table 3, where ? indicates statistical significance,
a = .05). Furthermore, ANT and SYN are signifi-
cantly different from PA.
Kilgarriff and Yallop (2000) point out that man-
ually constructed taxonomies or thesauri are typ-
ically organised according to synonymy and hy-
ponymy for nouns and verbs and antonymy for ad-
jectives. They further argue that for automatically
constructed thesauri similar words are words that
either co-occur with each other or with the same
words. The relations SYN, SUP, CO, and ANT can be
thought of as representing taxonomy-related knowl-
edge, whereas CA and PA correspond to the word
clusters found in automatically constructed thesauri.
In fact an ANOVA reveals that the distinction be-
tween these two classes of relations can be made
reliably (F(1,136) = 15.347, p < .001), after col-
lapsing SYN, SUP, CO, and ANT into one class and
CA and PA into another.
Our results suggest that dependency-based vector
space models can, at least to a certain degree, dis-
tinguish among different types of lexical relations,
while this seems to be more difficult for traditional
semantic space models. The Tukey test revealed that
category coordination is reliably distinguished from
all other relations and that phrasal association is re-
liably different from antonymy and synonymy. Tax-
onomy related relations (e.g., synonymy, antonymy,
hyponymy) can be reliably distinguished from con-
ceptual and phrasal association. However, no reli-
able differences were found between closely associ-
ated relations such as antonymy and synonymy.
Our results further indicate that context encoding
plays an important role in discriminating lexical re-
lations. As in Experiment 1 our best results were
obtained with the wide context specification. Also,
weighting schemes such as the obliqueness hierar-
chy length again decreased the model?s performance
(see conditions 2, 5, 9, and 13 in Figure 4), show-
ing that dependency relations contribute equally to
the representation of a word?s meaning. This points
to the fact that rich context encodings with a wide
range of dependency relations are promising for cap-
turing lexical semantic distinctions. However, the
performance for maximum context specification was
lower, which indicates that collapsing all depen-
dency relations is not the optimal method, at least
for the tasks attempted here.
4 Discussion
In this paper we presented a novel semantic space
model that enriches traditional vector-based models
with syntactic information. The model is highly gen-
eral and can be optimised for different tasks. It ex-
tends prior work on syntax-based models (Grefen-
stette, 1994; Lin, 1998), by providing a general
framework for defining context so that a large num-
ber of syntactic relations can be used in the construc-
tion of the semantic space.
Our approach differs from Lin (1998) in three
important ways: (a) by introducing dependency
paths we can capture non-immediate relationships
between words (i.e., between subjects and objects),
whereas Lin considers only local context (depen-
dency edges in our terminology); the semantic
space is therefore constructed solely from isolated
head/modifier pairs and their inter-dependencies are
not taken into account; (b) Lin creates the semantic
space from the set of dependency edges that are rel-
evant for a given word; by introducing dependency
labels and the path value function we can selectively
weight the importance of different labels (e.g., sub-
ject, object, modifier) and parametrize the space ac-
cordingly for different tasks; (c) considerable flexi-
bility is allowed in our formulation for selecting the
dimensions of the semantic space; the latter can be
words (see the leaves in Figure 1), parts of speech
or dependency edges; in Lin?s approach, it is only
dependency edges (features in his terminology) that
form the dimensions of the semantic space.
Experiment 1 revealed that the dependency-based
model adequately simulates semantic priming. Ex-
periment 2 showed that a model that relies on rich
context specifications can reliably distinguish be-
tween different types of lexical relations. Our re-
sults indicate that a number of NLP tasks could
potentially benefit from dependency-based models.
These are particularly relevant for word sense dis-
crimination, automatic thesaurus construction, auto-
matic clustering and in general similarity-based ap-
proaches to NLP.
References
Balota, David A. and Robert Lorch, Jr. 1986. Depth of au-
tomatic spreading activation: Mediated priming effects in
pronunciation but not in lexical decision. Journal of Ex-
perimental Psychology: Learning, Memory and Cognition
12(3):336?45.
Burnard, Lou. 1995. Users Guide for the British National Cor-
pus. British National Corpus Consortium, Oxford University
Computing Service.
Choi, Freddy, Peter Wiemer-Hastings, and Johanna Moore.
2001. Latent Semantic Analysis for text segmentation. In
Proceedings of EMNLP 2001. Seattle, WA.
Dunning, Ted. 1993. Accurate methods for the statistics of sur-
prise and coincidence. Computational Linguistics 19:61?74.
Grefenstette, Gregory. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Academic Publishers.
Hodgson, James M. 1991. Informational constraints on pre-
lexical priming. Language and Cognitive Processes 6:169?
205.
Jones, Michael P. and James H. Martin. 1997. Contextual
spelling correction using Latent Semantic Analysis. In Pro-
ceedings of the ANLP 97.
Keenan, E. and B. Comrie. 1977. Noun phrase accessibility and
universal grammar. Linguistic Inquiry (8):62?100.
Kilgarriff, Adam and Colin Yallop. 2000. What?s in a thesaurus.
In Proceedings of LREC 2000. pages 1371?1379.
Landauer, T. and S. Dumais. 1997. A solution to Platos prob-
lem: the latent semantic analysis theory of acquisition, in-
duction, and representation of knowledge. Psychological Re-
view 104(2):211?240.
Lee, Lillian. 1999. Measures of distributional similarity. In
Proceedings of ACL ?99. pages 25?32.
Lin, Dekang. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING-ACL 1998. Montr?al,
Canada, pages 768?511.
Lin, Dekang. 2001. LaTaT: Language and text analysis tools.
In J. Allan, editor, Proceedings of HLT 2001. Morgan Kauf-
mann, San Francisco.
Livesay, K. and C. Burgess. 1997. Mediated priming in high-
dimensional meaning space: What is "mediated" in mediated
priming? In Proceedings of COGSCI 1997. Lawrence Erl-
baum Associates.
Lowe, Will. 2001. Towards a theory of semantic space. In Pro-
ceedings of COGSCI 2001. Lawrence Erlbaum Associates,
pages 576?81.
Lowe, Will and Scott McDonald. 2000. The direct route: Medi-
ated priming in semantic space. In Proceedings of COGSCI
2000. Lawrence Erlbaum Associates, pages 675?80.
Lund, Kevin and Curt Burgess. 1996. Producing high-
dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instruments, and Computers
28:203?8.
McDonald, Scott. 2000. Environmental Determinants of Lexical
Processing Effort. Ph.D. thesis, University of Edinburgh.
Patel, Malti, John A. Bullinaria, and Joseph P. Levy. 1998. Ex-
tracting semantic representations from large text corpora. In
Proceedings of the 4th Neural Computation and Psychology
Workshop. London, pages 199?212.
Salton, G, A Wang, and C Yang. 1975. A vector-space model
for information retrieval. Journal of the American Society
for Information Science 18(613?620).
Sch?tze, Hinrich. 1998. Automatic word sense discrimination.
Computational Linguistics 24(1):97?124.
Tesni?re, Lucien. 1959. Elements de syntaxe structurale.
Klincksieck, Paris.
Towards a Resource for Lexical Semantics:
A Large German Corpus with Extensive Semantic Annotation
Katrin Erk and Andrea Kowalski and Sebastian Pado? and Manfred Pinkal
Department of Computational Linguistics
Saarland University
Saarbru?cken, Germany
{erk, kowalski, pado, pinkal}@coli.uni-sb.de
Abstract
We describe the ongoing construction of
a large, semantically annotated corpus
resource as reliable basis for the large-
scale acquisition of word-semantic infor-
mation, e.g. the construction of domain-
independent lexica. The backbone of the
annotation are semantic roles in the frame
semantics paradigm. We report expe-
riences and evaluate the annotated data
from the first project stage. On this ba-
sis, we discuss the problems of vagueness
and ambiguity in semantic annotation.
1 Introduction
Corpus-based methods for syntactic learning and
processing are well-established in computational
linguistics. There are comprehensive and carefully
worked-out corpus resources available for a num-
ber of languages, e.g. the Penn Treebank (Marcus et
al., 1994) for English or the NEGRA corpus (Skut
et al, 1998) for German. In semantics, the sit-
uation is different: Semantic corpus annotation is
only in its initial stages, and currently only a few,
mostly small, corpora are available. Semantic an-
notation has predominantly concentrated on word
senses, e.g. in the SENSEVAL initiative (Kilgarriff,
2001), a notable exception being the Prague Tree-
bank (Hajic?ova?, 1998) . As a consequence, most
recent work in corpus-based semantics has taken an
unsupervised approach, relying on statistical meth-
ods to extract semantic regularities from raw cor-
pora, often using information from ontologies like
WordNet (Miller et al, 1990).
Meanwhile, the lack of large, domain-
independent lexica providing word-semantic
information is one of the most serious bottlenecks
for language technology. To train tools for the
acquisition of semantic information for such lexica,
large, extensively annotated resources are necessary.
In this paper, we present current work of the
SALSA (SAarbru?cken Lexical Semantics Annota-
tion and analysis) project, whose aim is to provide
such a resource and to investigate efficient methods
for its utilisation. In the current project phase, the
focus of our research and the backbone of the an-
notation are semantic role relations. More specif-
ically, our role annotation is based on the Berke-
ley FrameNet project (Baker et al, 1998; Johnson
et al, 2002). In addition, we selectively annotate
word senses and anaphoric links. The TIGER corpus
(Brants et al, 2002), a 1.5M word German newspa-
per corpus, serves as sound syntactic basis.
Besides the sparse data problem, the most seri-
ous problem for corpus-based lexical semantics is
the lack of specificity of the data: Word meaning is
notoriously ambiguous, vague, and subject to con-
textual variance. The problem has been recognised
and discussed in connection with the SENSEVAL
task (Kilgarriff and Rosenzweig, 2000). Annotation
of frame semantic roles compounds the problem as
it combines word sense assignment with the assign-
ment of semantic roles, a task that introduces vague-
ness and ambiguity problems of its own.
The problem can be alleviated by choosing a suit-
able resource as annotation basis. FrameNet roles,
which are local to particular frames (abstract sit-
uations), may be better suited for the annotation
task than the ?classical? thematic roles concept with
a small, universal and exhaustive set of roles like
agent, patient, theme: The exact extension of the
role concepts has never been agreed upon (Fillmore,
1968). Furthermore, the more concrete frame se-
mantic roles may make the annotators? task easier.
The FrameNet database itself, however, cannot be
taken as evidence that reliable annotation is pos-
sible: The aim of the FrameNet project is essen-
tially lexicographic and its annotation not exhaus-
tive; it comprises representative examples for the use
of each frame and its frame elements in the BNC.
While the vagueness and ambiguity problem may
be mitigated by the using of a ?good? resource, it
will not disappear entirely, and an annotation format
is needed that can cope with the inherent vagueness
of word sense and semantic role assignment.
Plan of the paper. In Section 2 we briefly intro-
duce FrameNet and the TIGER corpus that we use
as a basis for semantic annotation. Section 3 gives
an overview of the aims of the SALSA project, and
Section 4 describes the annotation with frame se-
mantic roles. Section 5 evaluates the first annotation
results and the suitability of FrameNet as an anno-
tation resource, and Section 6 discusses the effects
of vagueness and ambiguity on frame semantic role
annotation. Although the current amount of anno-
tated data does not allow for definitive judgements,
we can discuss tendencies.
2 Resources
SALSA currently extends the TIGER corpus by se-
mantic role annotation, using FrameNet as a re-
source. In the following, we will give a short
overview of both resources.
FrameNet. The FrameNet project (Johnson et al,
2002) is based on Fillmore?s Frame Semantics. A
frame is a conceptual structure that describes a situ-
ation. It is introduced by a target or frame-evoking
element (FEE). The roles, called frame elements
(FEs), are local to particular frames and are the par-
ticipants and props of the described situations.
The aim of FrameNet is to provide a comprehen-
sive frame-semantic description of the core lexicon
of English. A database of frames contains the
frames? basic conceptual structure, and names and
descriptions for the available frame elements. A
lexicon database associates lemmas with the frames
they evoke, lists possible syntactic realizations of
FEs and provides annotated examples from the
BNC. The current on-line version of the frame
database (Johnson et al, 2002) consists of almost
400 frames, and covers about 6,900 lexical entries.
Frame: REQUEST
FE Example
SPEAKER Pat urged me to apply for the job.
ADDRESSEE Pat urged me to apply for the job.
MESSAGE Pat urged me to apply for the job.
TOPIC Kim made a request about changing the title.
MEDIUM Kim made a request in her letter.
Frame: COMMERCIAL TRANSACTION (C T)
BUYER Jess bought a coat.
GOODS Jess bought a coat.
SELLER Kim sold the sweater.
MONEY Kim paid 14 dollars for the ticket.
PURPOSE Kim bought peppers to cook them.
REASON Bob bought peppers because he was hungry.
Figure 1: Example frame descriptions.
Figure 1 shows two frames. The frame REQUEST
involves a FE SPEAKER who voices the request,
an ADDRESSEE who is asked to do something, the
MESSAGE, the request that is made, the TOPIC that
the request is about, and the MEDIUM that is used to
convey the request. Among the FEEs for this frame
are the verb ask and the noun request. In the frame
COMMERCIAL TRANSACTION (henceforth C T), a
BUYER gives MONEY to a SELLER and receives
GOODS in exchange. This frame is evoked e.g. by
the verb pay and the noun money.
The TIGER Corpus. We are using the TIGER
Corpus (Brants et al, 2002), a manually syntacti-
cally annotated German corpus, as a basis for our
annotation. It is the largest available such cor-
pus (80,000 sentences in its final release compared
to 20,000 sentences in its predecessor NEGRA)
and uses a rich annotation format. The annotation
scheme is surface oriented and comparably theory-
neutral. Individual words are labelled with POS
information. The syntactic structures of sentences
are described by relatively flat trees providing in-
formation about grammatical functions (on edge la-
bels), syntactic categories (on node labels), and ar-
gument structure of syntactic heads (through the
use of dependency-oriented constituent structures,
which are close to the syntactic surface). An exam-
ple for a syntactic structure is given in Figure 2.
3 Project overview
The aim of the SALSA project is to construct a large
semantically annotated corpus and to provide meth-
ods for its utilisation.
Corpus construction. In the first phase of the
project, we annotate the TIGER corpus in part man-
Figure 2: A sentence and its syntactic structure.
ually, in part semi-automatically, having tools pro-
pose tags which are verified by human annotators.
In the second phase, we will extend these tools for
the weakly supervised annotation of a much larger
corpus, using the TIGER corpus as training data.
Utilisation. The SALSA corpus is designed to
be utilisable for many purposes, like improving sta-
tistical parsers, and extending methods for informa-
tion extraction and access. The focus in the SALSA
project itself is on lexical semantics, and our first
use of the corpus will be to extract selectional pref-
erences for frame elements.
The SALSA corpus will be tagged with the fol-
lowing types of semantic information:
FrameNet frames. We tag all FEEs that oc-
cur in the corpus with their appropriate frames, and
specify their frame elements. Thus, our focus is
different from the lexicographic orientation of the
FrameNet project mentioned above. As we tag all
corpus instances of each FEE, we expect to en-
counter a wider range of phenomena. which Cur-
rently, FrameNet only exists for English and is still
under development. We will produce a ?light ver-
sion? of a FrameNet for German as a by-product
of the annotation, reusing as many as possible of
the semantic frame descriptions from the English
FrameNet database. Our first results indicate that
the frame structure assumed for the description of
the English lexicon can be reused for German, with
minor changes and extensions.
Word sense. The additional value of word sense
disambiguation in a corpus is obvious. However,
exhaustive word sense annotation is a highly time-
consuming task. Therefore we decided for a selec-
tive annotation policy, annotating only the heads of
frame elements. GermaNet, the German WordNet
version, will be used as a basis for the annotation.
            request         conversation
SPKR
FEE
ADD
MSG
FEE FEE
TOPIC
INTLC_1
Figure 3: Frame annotation.
Coreference. Similarly, we will selectively anno-
tate coreference. If a lexical head of a frame element
is an anaphor, we specify the antecedent to make the
meaning of the frame element accessible.
4 Frame Annotation
Annotation schema. To give a first impression of
frame annotation, we turn to the sentence in Fig. 2:
(1) SPD fordert Koalition zu Gespra?ch u?ber Re-
form auf.
(SPD requests that coalition talk about reform.)
Fig. 3 shows the frame annotation associated with
(1). Frames are drawn as flat trees. The root node is
labelled with the frame name. The edges are labelled
with abbreviated FE names, like SPKR for SPEAKER,
plus the tag FEE for the frame-evoking element. The
terminal nodes of the frame trees are always nodes
of the syntactic tree. Cases where a semantic unit
(FE or FEE) does not form one syntactic constituent,
like fordert . . . auf in the example, are represented
by assignment of the same label to several edges.
Sentence (1), a newspaper headline, contains at
least two FEEs: auffordern and Gespra?ch. auf-
fordern belongs to the frame REQUEST (see Fig. 1).
In our example the SPEAKER is the subject NP SPD,
the ADDRESSEE is the direct object NP Koalition,
and the MESSAGE is the complex PP zu Gespra?ch
u?ber Reform. So far, the frame structure follows the
syntactic structure, except for that fact that the FEE,
as a separable prefix verb, is realized by two syntac-
tic nodes. However, it is not always the case that
frame structure parallels syntactic structure. The
second FEE Gespra?ch introduces the frame CON-
VERSATION. In this frame two (or more) groups
talk to one another and no participant is construed
as only a SPEAKER or only an ADDRESSEE. In
our example the only NP-internal frame element is
the TOPIC (?what the message is about?) u?ber Re-
form, whereas the INTERLOCUTOR-1 (?the promi-
nent participant in the conversation?) is realized by
the direct object of auffordern.
As shown in Fig. 3, frames are annotated as trees
of depth one. Although it might seem semantically
more adequate to admit deeper frame trees, e.g. to
allow the MSG edge of the REQUEST frame in Fig.
3 to be the root node of the CONVERSATION tree,
as its ?real? semantic argument, the representation
of frame structure in terms of flat and independent
semantic trees seems to be preferable for a number
of practical reasons: It makes the annotation process
more modular and flexible ? this way, no frame an-
notation relies on previous frame annotation. The
closeness to the syntactic structure makes the an-
notators? task easier. Finally, it facilitates statistical
evaluation by providing small units of semantic in-
formation that are locally related to syntax.
Difficult cases. Because frame elements may
span more than one sentence, like in the case of
direct speech, we cannot restrict ourselves to an-
notation at sentence level. Also, compound nouns
require annotation below word level. For ex-
ample, the word ?Gagenforderung? (demand for
wages) consists of ?-forderung? (demand), which in-
vokes the frame REQUEST, and a MESSAGE element
?Gagen-?. Another interesting point is that one word
may introduce more than one frame in cases of co-
ordination and ellipsis. An example is shown in (2).
In the elliptical clause only one fifth for daughters,
the elided bought introduces a C T frame. So we let
the bought in the antecedent introduce two frames,
one for the antecedent and one for the ellipsis.
(2) Ein Viertel aller Spielwaren wu?rden fu?r So?hne
erworben, nur ein Fu?nftel fu?r To?chter.
(One quarter of all toys are bought for sons, only one fifth
for daughters.)
Annotation process. Frame annotation proceeds
one frame-evoking lemma at a time, using subcor-
pora containing all instances of the lemma with
some surrounding context. Since most FEEs are
polysemous, there will usually be several frames rel-
evant to a subcorpus. Annotators first select a frame
for an instance of the target lemma. Then they assign
frame elements.
At the moment the annotation uses XML tags on
bare text. The syntactic structure of the TIGER-
sentences can be accessed in a separate viewer. An
annotation tool is being implemented that will pro-
vide a graphical interface for the annotation. It will
display the syntactic structure and allow for a graph-
ical manipulation of semantic frame trees, in a simi-
lar way as shown in Fig. 3.
Extending FrameNet. Since FrameNet is far
from being complete, there are many word senses
not yet covered. For example the verb fordern,
which belongs to the REQUEST frame, additionally
has the reading challenge, for which the current ver-
sion of FrameNet does not supply a frame.
5 Evaluation of Annotated Data
Materials. Compared to the pilot study we previ-
ously reported (Erk et al, 2003), in which 3 annota-
tors tagged 440 corpus instances of a single frame,
resulting in 1,320 annotation instances, we now dis-
pose of a considerably larger body of data. It con-
sists of 703 corpus instances for the two frames
shown in Figure 1, making up a total of 4,653 an-
notation instances. For the frame REQUEST, we
obtained 421 instances with 8-fold and 114 with
7-fold annotation. The annotated lemmas com-
prise auffordern (to request), fordern, verlangen (to
demand), zuru?ckfordern (demand back), the noun
Forderung (demand), and compound nouns ending
with -forderung. For the frame C T we have 30, 40
and 98 instances with 5-, 3-, and 2-fold annotation
respectively. The annotated lemmas are kaufen (to
buy), erwerben (to acquire), verbrauchen (to con-
sume), and verkaufen (to sell).
Note that the corpora we are evaluating do not
constitute a random sample: At the moment, we
cover only two frames, and REQUEST seems to be
relatively easy to annotate. Also, the annotation re-
sults may not be entirely predictive for larger sam-
ple sizes: While the annotation guidelines were be-
ing developed, we used REQUEST as a ?calibration?
frame to be annotated by everybody. As a result, in
some cases reliability may be too low because de-
tailed guidelines were not available, and in others
it may be too high because controversial instances
were discussed in project meetings.
Results. The results in this section refer solely to
the assignment of fully specified frames and frame
elements. Underspecification is discussed at length
frames average best worst
REQUEST 96.83% 100% 90.73%
COMM. 97.11% 98.96% 88.71%
elements average best worst
REQUEST 88.86% 95.69% 66.57%
COMM. 74.25% 90.30% 69.33%
Table 1: Inter-annotator agreement on frames (top)
and frame elements (below).
in Section 6. Due to the limited space in this pa-
per, we only address the question of inter-annotator
agreement or annotation reliability, since a reliable
annotation is necessary for all further corpus uses.
Table 1 shows the inter-annotator agreement on
frame assignment and on frame element assignment,
computed for pairs of annotators. The ?average?
column shows the total agreement for all annotation
instances, while ?best? and ?worst? show the fig-
ures for the (lemma-specific) subcorpora with high-
est and lowest agreement, respectively. The upper
half of the table shows agreement on the assignment
of frames to FEEs, for which we performed 14,410
pairwise comparisons, and the lower half shows
agreement on assigned frame elements (29,889 pair-
wise comparisons). Agreement on frame elements is
?exact match?: both annotators have to tag exactly
the same sequence of words. In sum, we found that
annotators agreed very well on frames. Disagree-
ment on frame elements was higher, in the range of
12-25%. Generally, the numbers indicated consider-
able differences between the subcorpora.
To investigate this matter further, we computed
the Alpha statistic (Krippendorff, 1980) for our an-
notation. Like the widely used Kappa, ? is a chance-
corrected measure of reliability. It is defined as
? = 1 ? observed disagreement
expected disagreement
We chose Alpha over Kappa because it also indi-
cates unreliabilities due to unequal coder preference
for categories. With an ? value of 1 signifying total
agreement and 0 chance agreement, ? values above
0.8 are usually interpreted as reliable annotation.
Figure 4 shows single category reliabilities for
the assignment of frame elements. The graphs
shows that not only did target lemmas vary in
their difficulty, but that reliability of frame ele-
ment assignment was also a matter of high varia-
tion. Firstly, frames introduced by nouns (Forderung
and -forderung) were more difficult to annotate than
verbs. Secondly, frame elements could be assigned
to three groups: frame elements which were al-
ways annotated reliably, those whose reliability was
highly dependent on the FEE, and the third group
whose members were impossible to annotate reli-
ably (these are not shown in the graphs). In the
REQUEST frames, SPEAKER, MESSAGE and AD-
DRESSEE belong to the first group, at least for verbal
FEEs. MEDIUM is a member of the second group,
and TOPIC was annotated at chance level (? ? 0).
In the COMMERCE frame, only BUYER and GOODS
always show high reliability. SELLER can only be re-
liably annotated for the target verkaufen. PURPOSE
and REASON fall into the third group.
5.1 Discussion
Interpretation of the data. Inter-annotator agree-
ment on the frames shown in Table 1 is very high.
However, the lemmas we considered so far were
only moderately ambiguous, and we might see lower
figures for frame agreement for highly polysemous
FEEs like laufen (to run).
For frame elements, inter-annotator agreement
is not that high. Can we expect improvement?
The Prague Treebank reported a disagreement of
about 10% for manual thematic role assignment
( ?Zabokrtsky?, 2000). However, in contrast to our
study, they also annotated temporal and local modi-
fiers, which are easier to mark than other roles.
One factor that may improve frame element
agreement in the future is the display of syntactic
structure directly in the annotation tool. Annotators
were instructed to assign each frame element to a
single syntactic constituent whenever possible, but
could only access syntactic structure in a separate
viewer. We found that in 35% of pairwise frame ele-
ment disagreements, one annotator assigned a single
syntactic constituent and the other did not. Since a
total of 95.6% of frame elements were assigned to
single constituents, we expect an increase in agree-
ment when a dedicated annotation tool is available.
As to the pronounced differences in reliability be-
tween frame elements, we found that while most
central frame elements like SPEAKER or BUYER
were easy to identify, annotators found it harder to
agree on less frequent frame elements like MEDIUM,
PURPOSE and REASON. The latter two with their
 0.6
 0.8
 1
auffordern fordern verlangen Forderung -forderung
a
lp
ha
 v
al
ue
addressee
medium
message
speaker
 0.6
 0.8
 1
erwerben kaufen verkaufen
a
lp
ha
 v
al
ue
buyer
seller
money
goods
Figure 4: Alpha values for frame elements. Left: REQUEST. Right: COMMERCIAL TRANSACTION.
particularly low agreement (? < 0.8) contribute to-
wards the low overall inter-annotator agreement of
the C T frame. We suspect that annotators saw too
few instances of these elements to build up a reli-
able intuition. However, the elements may also be
inherently difficult to distinguish.
How can we interpret the differences in frame el-
ement agreement across target lemmas, especially
between verb and noun targets? While frame ele-
ments for verbal targets are usually easy to identify
based on syntactic factors, this is not the case for
nouns. Figure 3 shows an example: Should SPD
be tagged as INTERLOCUTOR-2 in the CONVERSA-
TION frame? This appears to be a question of prag-
matics. Here it seems that clearer annotation guide-
lines would be desirable.
FrameNet as a resource for semantic role an-
notation. Above, we have asked about the suitabil-
ity of FrameNet for semantic role annotation, and
our data allow a first, though tentative, assessment.
Concerning the portability of FrameNet to other
languages than English, the English frames worked
well for the German lemmas we have seen so far.
For C T a number of frame elements seem to be
missing, but these are not language-specific, like
CREDIT (for on commission and in installments).
The FrameNet frame database is not yet complete.
How often do annotators encounter missing frames?
The frame UNKNOWN was assigned in 6.3% of the
instances of REQUEST, and in 17.6% of the C T in-
stances. The last figure is due to the overwhelm-
ing number of UNKNOWN cases in verbrauchen, for
which the main sense we encountered is ?to use up
a resource?, which FrameNet does not offer.
Is the choice of frame always clear? And can
frame elements always be assigned unambiguously?
Above we have already seen that frame element as-
signment is problematic for nouns. In the next sec-
tion we will discuss problematic cases of frame as-
signment as well as frame element assignment.
6 Vagueness, Ambiguity and
Underspecification
Annotation Challenges. It is a well-known prob-
lem from word sense annotation that it is often im-
possible to make a safe choice among the set of pos-
sible semantic correlates for a linguistic item. In
frame annotation, this problem appears on two lev-
els: The choice of a frame for a target is a choice
of word sense. The assignment of frame elements to
phrases poses a second disambiguation problem.
An example of the first problem is the Ger-
man verb verlangen, which associates with both the
frame REQUEST and the frame C T. We found sev-
eral cases where both readings seem to be equally
present, e.g. sentence (3). Sentences (4) and (5) ex-
emplify the second problem. The italicised phrase in
(4) may be either a SPEAKER or a MEDIUM and the
one in (5) either a MEDIUM or not a frame element
at all. In our exhaustive annotation, these problems
are much more virulent than in the FrameNet corpus,
which consists mostly of prototypical examples.
(3) Gleichwohl versuchen offenbar Assekuranzen,
[das Gesetz] zu umgehen, indem sie von Nicht-
deutschen mehr Geld verlangen.
(Nonetheless insurance companies evidently try to cir-
cumvent [the law] by asking/demanding more money
from non-Germans.)
(4) Die nachhaltigste Korrektur der Programmatik
fordert ein Antrag. . .
(The most fundamental policy correction is requested by
a motion. . . )
(5) Der Parteitag billigte ein Wirtschaftskonzept, in
dem der Umbau gefordert wird.
(The party congress approved of an economic concept in
which a change is demanded.)
Following Kilgarriff and Rosenzweig (2000), we
distinguish three cases where the assignment of a
single semantic tag is problematic: (1), cases in
which, judging from the available context informa-
tion, several tags are equally possible for an ambigu-
ous utterance; (2), cases in which more than one tag
applies at the same time, because the sense distinc-
tion is neutralised in the context; and (3), cases in
which the distinction between two tags is systemati-
cally vague or unclear.
In SALSA, we use the concept of underspecifica-
tion to handle all three cases: Annotators may assign
underspecified frame and frame element tags. While
the cases have different semantic-pragmatic status,
we tag all three of them as underspecified. This is in
accordance with the general view on underspecifica-
tion in semantic theory (Pinkal, 1996). Furthermore,
Kilgarriff and Rosenzweig (2000) argue that it is im-
possible to distinguish those cases
Allowing underspecified tags has several advan-
tages. First, it avoids (sometimes dubious) decisions
for a unique tag during annotation. Second, it is use-
ful to know if annotators systematically found it hard
to distinguish between two frames or two frame ele-
ments. This diagnostic information can be used for
improving the annotation scheme (e.g. by removing
vague distinctions). Third, underspecified tags may
indicate frame relations beyond an inheritance hier-
archy, horizontal rather than vertical connections. In
(3), the use of underspecification can indicate that
the frames REQUEST and C T are used in the same
situation, which in turn can serve to infer relations
between their respective frame elements.
Evaluating underspecified annotation. In the
previous section, we disregarded annotation cases
involving underspecification. In order to evalu-
ate underspecified tags, we present a method of
computing inter-annotator agreement in the pres-
ence of underspecified annotations. Represent-
ing frames and frame elements as predicates that
each take a sequence of word indices as their
argument, a frame annotation can be seen as a
pair (CF,CE) of two formulae, describing the
frame and the frame elements, respectively. With-
out underspecification, CF is a single predicate
and CE is a conjunction of predicates. For the
CONVERSATION frame of sentence (1), CF has
the form CONVERSATION(Gespra?ch)1 , and CE is
INTLC 1(Koalition) ? TOPIC(u?ber Reform). Un-
derspecification is expressed by conjuncts that are
disjunctions instead of single predicates. Table 2
shows the admissible cases. For example, the CE
of (4) contains the conjunct SPKR(ein Antrag) ?
MEDIUM(ein Antrag). Our annotation scheme guar-
antees that every FE name appears in at most one
conjunct of CE. Exact agreement means that ev-
ery conjunct of annotator A must correspond to a
conjunct by annotator B, and vice versa. For partial
agreement, it suffices that for each conjunct of A,
one disjunct matches a disjunct in a conjunct of B,
and conversely.
frame annotation
F(t) single frame: F is assigned to t
(F1(t)?F2(t)) frame disjunction: F1 or F2 is
assigned to t
frame element annotation
E(s) single frame element: E is as-
signed to s
(E1(s)?E2(s)) frame element disjunction: E1
or E2 is assigned to s
(E(s)?NOFE(s)) optional element: E1 or no
frame element is assigned to s
(E(s)?E(s1ss2)) underspecified length: frame
element E is assigned to s
or the longer sequence s1ss2,
which includes s
Table 2: Types of conjuncts. F is a frame name, E
a frame element name, and t and s are sequences of
word indices (t is for the target (FEE))
Using this measure of partial agreement, we now
evaluate underspecified annotation. The most strik-
ing result is that annotators made little use of under-
specification. Frame underspecification was used in
0.4% of all frames, and frame element underspecifi-
cation for 0.9% of all frame elements. The frame el-
ement MEDIUM, which was rarely assigned outside
1We use words instead of indices for readability.
underspecification, accounted for roughly half of all
underspecification in the REQUEST frame. 63% of
the frame element underspecifications are cases of
optional elements, the third class in the lower half of
Table 2. (Partial) agreement on underspecified tags
was considerably lower than on non-underspecified
tags, both in the case of frames (86%) and in the
case of frame elements (54%). This was to be ex-
pected, since the cases with underspecified tags are
the more difficult and controversial ones. Since un-
derspecified annotation is so rare, overall frame and
frame element agreement including underspecified
annotation is virtually the same as in Table 1.
It is unfortunate that annotators use underspecifi-
cation only infrequently, since it can indicate inter-
esting cases of relatedness between different frames
and frame elements. However, underspecification
may well find its main use during the merging of
independent annotations of the same corpus. Not
only underspecified annotation, also disagreement
between annotators can point out vague and ambigu-
ous cases. If, for example, one annotator has as-
signed SPEAKER and the other MEDIUM in sentence
(4), the best course is probably to use an underspec-
ified tag in the merged corpus.
7 Conclusion
We presented the SALSA project, the aim of which
is to construct and utilize a large corpus reliably
annotated with semantic information. While the
SALSA corpus is designed to be utilizable for many
purposes, our focus is on lexical semantics, in or-
der to address one of the most serious bottlenecks
for language technology today: the lack of large,
domain-independent lexica.
In this paper we have focused on the annotation
with frame semantic roles. We have presented the
annotation scheme, and we have evaluated first an-
notation results, which show encouraging figures for
inter-annotator agreement. We have discussed the
problem of vagueness and ambiguity of the data and
proposed a representation for underspecified tags,
which are to be used both for the annotation and the
merging of individual annotations.
Important next steps are: the design of a tool for
semi-automatic annotation, and the extraction of se-
lectional preferences from the annotated data.
Acknowledgments. We would like to thank the
following people, who helped us with their sugges-
tions and discussions: Sue Atkins, Collin Baker,
Ulrike Baldewein, Hans Boas, Daniel Bobbert,
Sabine Brants, Paul Buitelaar, Ann Copestake,
Christiane Fellbaum, Charles Fillmore, Gerd Flied-
ner, Silvia Hansen, Ulrich Heid, Katja Markert and
Oliver Plaehn. We are especially indebted to Maria
Lapata, whose suggestions have contributed to the
current shape of the project in an essential way. Any
errors are, of course, entirely our own.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceedings of
COLING-ACL, Montreal, Canada.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lez-
ius, and George Smith. 2002. The TIGER treebank. In
Proceedings of the Workshop on Treebanks and Linguistic
Theories, Sozopol, Bulgaria.
Katrin Erk, Andrea Kowalski, and Manfred Pinkal. 2003. A
corpus resource for lexical semantics. In Proceedings of
IWCS5, pages 106?121, Tilburg, The Netherlands.
Charles J. Fillmore. 1968. The case for case. In Bach and
Harms, editors, Universals in Linguistic Theory, pages 1?88.
Holt, Rinehart, and Winston, New York.
Eva Hajic?ov a?. 1998. Prague Dependency Treebank: From An-
alytic to Tectogrammatical Annotation. In Proceedings of
TSD?98, pages 45?50, Brno, Czech Republic.
C. R. Johnson, C. J. Fillmore, M. R. L. Petruck, C. F. Baker,
M. Ellsworth, J. Ruppenhofer, and E. J. Wood. 2002.
FrameNet: Theory and Practice. http://www.icsi.
berkeley.edu/?framenet/book/book.html.
Adam Kilgarriff and Joseph Rosenzweig. 2000. Framework
and results for English Senseval. Computers and the Hu-
manities, 34(1-2).
Adam Kilgarriff, editor. 2001. SENSEVAL-2, Toulouse.
Klaus Krippendorff. 1980. Content Analysis. Sage.
M. Marcus, G. Kim, M.A. Marcinkiewicz, R. MacIntyre,
A. Bies, M. Gerguson, K. Katz, and B. Schasberger. 1994.
The Penn Treebank: Annotating predicate argument struc-
ture. In Proceedings of the ARPA HLT Workshop.
G. Miller, R. Beckwith, C. Fellbaum, D. Gros, and K. Miller.
1990. Introduction to WordNet: An on-line lexical database.
International Journal of Lexicography, 3(4):235?44.
Manfred Pinkal. 1996. Vagueness, ambiguity, and underspeci-
fication. In Proceedings of SALT?96, pages 185?201.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and Hans
Uszkoreit. 1998. A linguistically interpreted corpus of Ger-
man newspaper text. In Proceedings of LREC?98, Granada.
Zdene?k ?Zabokrtsk y?. 2000. Automatic functor assignment
in the Prague Dependency Treebank. In Proceedings of
TSD?00, Brno, Czech Republic.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1161?1168,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Optimal Constituent Alignment with Edge Covers for Semantic Projection
Sebastian Pad?
Computational Linguistics
Saarland University
Saarbr?cken, Germany
pado@coli.uni-sb.de
Mirella Lapata
School of Informatics
University of Edinburgh
Edinburgh, UK
mlap@inf.ed.ac.uk
Abstract
Given a parallel corpus, semantic projec-
tion attempts to transfer semantic role an-
notations from one language to another,
typically by exploiting word alignments.
In this paper, we present an improved
method for obtaining constituent align-
ments between parallel sentences to guide
the role projection task. Our extensions
are twofold: (a) we model constituent
alignment as minimum weight edge cov-
ers in a bipartite graph, which allows us to
find a globally optimal solution efficiently;
(b) we propose tree pruning as a promising
strategy for reducing alignment noise. Ex-
perimental results on an English-German
parallel corpus demonstrate improvements
over state-of-the-art models.
1 Introduction
Recent years have witnessed increased interest in
data-driven methods for many natural language
processing (NLP) tasks, ranging from part-of-
speech tagging, to parsing, and semantic role la-
belling. The success of these methods is due partly
to the availability of large amounts of training data
annotated with rich linguistic information. Unfor-
tunately, such resources are largely absent for al-
most all languages except English. Given the data
requirements for supervised learning, and the cur-
rent paucity of suitable data for many languages,
methods for generating annotations (semi-)auto-
matically are becoming increasingly popular.
Annotation projection tackles this problem by
leveraging parallel corpora and the high-accuracy
tools (e.g., parsers, taggers) available for a
few languages. Specifically, through the use of
word alignments, annotations are transfered from
resource-rich languages onto low density ones.
The projection process can be decomposed into
three steps: (a) determining the units of projection;
these are typically words but can also be chunks
or syntactic constituents; (b) inducing alignments
between the projection units and projecting anno-
tations along these alignments; (c) reducing the
amount of noise in the projected annotations, often
due to errors and omissions in the word alignment.
The degree to which analyses are parallel across
languages is crucial for the success of projection
approaches. A number of recent studies rely on
this notion of parallelism and demonstrate that an-
notations can be adequately projected for parts of
speech (Yarowsky and Ngai, 2001; Hi and Hwa,
2005), chunks (Yarowsky and Ngai, 2001), and de-
pendencies (Hwa et al, 2002).
In previous work (Pad? and Lapata, 2005) we
considered the annotation projection of seman-
tic roles conveyed by sentential constituents such
as AGENT, PATIENT, or INSTRUMENT. Semantic
roles exhibit a high degree of parallelism across
languages (Boas, 2005) and thus appear amenable
to projection. Furthermore, corpora labelled with
semantic role information can be used to train
shallow semantic parsers (Gildea and Jurafsky,
2002), which could in turn benefit applications in
need of broad-coverage semantic analysis. Exam-
ples include question answering, information ex-
traction, and notably machine translation.
Our experiments concentrated primarily on the
first projection step, i.e., establishing the right
level of linguistic analysis for effecting projec-
tion. We showed that projection schemes based
on constituent alignments significantly outperform
schemes that rely exclusively on word alignments.
A local optimisation strategy was used to find con-
stituent alignments, while relying on a simple fil-
tering technique to handle noise.
The study described here generalises our earlier
semantic role projection framework in two impor-
tant ways. First, we formalise constituent projec-
tion as the search for aminimum weight edge cover
in a weighted bipartite graph. This formalisation
1161
efficiently yields constituent alignments that are
globally optimal. Second, we propose tree prun-
ing as a general noise reduction strategy, which ex-
ploits both structural and linguistic information to
enable projection. Furthermore, we quantitatively
assess the impact of noise on the task by evaluating
both on automatic and manual word alignments.
In Section 2, we describe the task of role-
semantic projection and the syntax-based frame-
work introduced in Pad? and Lapata (2005). Sec-
tion 3 explains how semantic role projection can
be modelled with minimum weight edge covers in
bipartite graphs. Section 4 presents our tree prun-
ing strategy. We present our evaluation framework
and results in Section 5. A discussion of related
and future work concludes the paper.
2 Cross-lingual Semantic Role projection
Semantic role projection is illustrated in Figure 1
using English and German as the source-target
language pair. We assume a FrameNet-style se-
mantic analysis (Fillmore et al, 2003). In this
paradigm, the semantics of predicates and their
arguments are described in terms of frames, con-
ceptual structures which model prototypical situ-
ations. The English sentence Kim promised to be
on time in Figure 1 is an instance of the COM-
MITMENT frame. In this particular example, the
frame introduces two roles, i.e., SPEAKER (Kim)
and MESSAGE (to be on time). Other possible,
though unrealised, roles are ADDRESSEE, MES-
SAGE, and TOPIC. The COMMITMENT frame can
be introduced by promise and several other verbs
and nouns such as consent or threat.
We also assume that frame-semantic annota-
tions can be obtained reliably through shallow
semantic parsing.1 Following the assignment of
semantic roles on the English side, (imperfect)
word alignments are used to infer semantic align-
ments between constituents (e.g., to be on time
is aligned with p?nktlich zu kommen), and the
role labels are transferred from one language to
the other. Note that role projection can only take
place if the source predicate (here promised ) is
word-aligned to a target predicate (here versprach )
evoking the same frame; if this is not the case
(e.g., in metaphors), projected roles will not be
generally appropriate.
We represent the source and target sentences
as sets of linguistic units, Us and Ut , respectively.
1See Carreras and M?rquez (2005) for an overview of re-
cent approaches to semantic parsing.
Kim versprach, p?nktlich zu kommen
Kim promised to be on time
S
S
NP
NP
Commitment
M
e
s
s
a
g
e
S
p
e
a
k
e
r
Commitment
S
p
e
a
k
e
r
M
e
s
s
a
g
e
Figure 1: Projection of semantic roles from En-
glish to German (word alignments as dotted lines)
The assignment of semantic roles on the source
side is a function roles : R ? 2Us from roles to
sets of source units. Constituent alignments are
obtained in two steps. First, a real-valued func-
tion sim : Us ?Ut ? R estimates pairwise simi-
larities between source and target units. To make
our model robust to alignment noise, we use only
content words to compute the similarity func-
tion. Next, a decision procedure uses the similar-
ity function to determine the set of semantically
equivalent, i.e., aligned units A?Us?Ut . Once A
is known, semantic projection reduces to transfer-
ring the semantic roles from the source units onto
their aligned target counterparts:
rolet(r) = {ut |?us ? roles(r) : (us,ut) ? A}
In Pad? and Lapata (2005), we evaluated two
main parameters within this framework: (a) the
choice of linguistic units and (b) methods for com-
puting semantic alignments. Our results revealed
that constituent-based models outperformed word-
based ones by a wide margin (0.65 Fscore
vs. 0.46), thus demonstrating the importance of
bracketing in amending errors and omissions in
the automatic word alignment. We also com-
pared two simplistic alignment schemes, back-
ward alignment and forward alignment. The
first scheme aligns each target constituent to its
most similar source constituent, whereas the sec-
ond (A f ) aligns each source constituent to its most
similar target constituent:
A f = {(us,ut) |ut = argmax
u?t?Ut
sim(us,u
?
t)}
1162
An example constituent alignment obtained from
the forward scheme is shown in Figure 2 (left
side). The nodes represent constituents in the
source and target language and the edges indicate
the resulting alignment. Forward alignment gener-
ally outperformed backward alignment (0.65 Fs-
core vs. 0.45). Both procedures have a time com-
plexity quadratic in the maximal number of sen-
tence nodes: O(|Us||Ut |) = O(max(|Us|, |Ut |)2).
A shortcoming common to both decision proce-
dures is that they are local, i.e., they optimise the
alignment for each node independently of all other
nodes. Consider again Figure 2. Here, the for-
ward procedure creates alignments for all source
nodes, but leaves constituents from the target set
unaligned (see target node (1)). Moreover, local
alignment methods constitute a rather weak model
of semantic equivalence since they allow one tar-
get node to correspond to any number of source
nodes (see target node (3) in Figure 2, which is
aligned to three source nodes). In fact, by allow-
ing any alignment between constituents, the lo-
cal models can disregard important linguistic in-
formation, thus potentially leading to suboptimal
results. We investigate this possibility by propos-
ing well-understood global optimisation models
which suitably constrain the resulting alignments.
Besides matching constituents reliably, poor
word alignments are a major stumbling block
for achieving accurate projections. Previous re-
search addresses this problem in a post-processing
step, by reestimating parameter values (Yarowsky
and Ngai, 2001), by applying transformation
rules (Hwa et al, 2002), by using manually la-
belled data (Hi and Hwa, 2005), or by relying on
linguistic criteria (Pad? and Lapata, 2005). In this
paper, we present a novel filtering technique based
on tree pruning which removes extraneous con-
stituents in a preprocessing stage, thereby disasso-
ciating filtering from the alignment computation.
In the remainder of this paper, we present the
details of our global optimisation and filtering
techniques. We only consider constituent-based
models, since these obtained the best performance
in our previous study (Pad? and Lapata, 2005).
3 Globally optimal constituent alignment
We model constituent alignment as a minimum
weight bipartite edge cover problem. A bipartite
graph is a graph G = (V,E) whose node set V is
partitioned into two nonempty sets V1 and V2 in
such a way that every edge E joins a node in V1
to a node in V2. In a weighted bipartite graph a
weight is assigned to each edge. An edge cover is
a subgraph of a bipartite graph so that each node is
linked to at least one node of the other partition. A
minimum weight edge cover is an edge cover with
the least possible sum of edge weights.
In our projection application, the two parti-
tions are the sets of source and target sentence
constituents, Us and Ut , respectively. Each source
node is connected to all target nodes and each tar-
get node to all source nodes; these edges can be
thought of as potential constituent alignments. The
edge weights, which represent the (dis)similarity
between nodes us and ut are set to 1? sim(us,ut).2
The minimum weight edge cover then represents
the alignment with the maximal similarity be-
tween source and target constituents. Below, we
present details on graph edge covers and a more
restricted kind, minimum weight perfect bipartite
matchings. We also discuss their computation.
Edge covers Given a bipartite graph G, a min-
imum weight edge cover Ae can be defined as:
Ae = argmin
Edge cover E
?
(us,ut)?E
1? sim(us,ut)
An example edge cover is illustrated in Figure 2
(middle). Edge covers are somewhat more con-
strained compared to the local model described
above: all source and target nodes have to take part
in some alignment. We argue that this is desirable
in modelling constituent alignment, since impor-
tant linguistic units will not be ignored. As can be
seen, edge covers allow one-to-many alignments
which are common when translating from one lan-
guage to another. For example, an English con-
stituent might be split into several German con-
stituents or alternatively two English constituents
might be merged into a single German constituent.
In Figure 2, the source nodes (3) and (4) corre-
spond to target node (4). Since each node of either
side has to participate in at least one alignment,
edge covers cannot account for insertions arising
when constituents in the source language have no
counterpart in their target language, or vice versa,
as is the case for deletions.
Weighted perfect bipartite matchings Per-
fect bipartite matchings are a more constrained
version of edge covers, in which each node has ex-
actly one adjacent edge. This restricts constituent
2The choice of similarity function is discussed in Sec-
tion 5.
1163
23
4
5
6
1
2
3
4
1
U
s
U
t
r
1
r
2
r
2
r
1
r
2
2
3
4
5
6
1
2
3
4
1
U
s
U
t
r
1
r
2
r
2
r
1
r
2
2
3
4
5
6
1
2
3
4
1
U
s
U
t
r
1
r
2
r
2
r
1
r
2
d
d
Figure 2: Constituent alignments and role projections resulting from different decision procedures
(Us,Ut : sets of source and target constituents; r1,r2: two semantic roles). Left: local forward alignment;
middle: edge cover; right: perfect matching with dummy nodes
alignment to a bijective function: each source
constituent is linked to exactly one target con-
stituent, and vice versa. Analogously, a minimum
weight perfect bipartite matching Am is a mini-
mum weight edge cover obeying the one-to-one
constraint:
Am = argmin
Matching M
?
(us,ut)?M
1? sim(us,ut)
An example of a perfect bipartite matching is
given in Figure 2 (right), where each node has ex-
actly one adjacent edge. Note that the target side
contains two nodes labelled (d), a shorthand for
?dummy? node. Since sentence pairs will often
differ in length, the resulting graph partitions will
have different sizes as well. In such cases, dummy
nodes are introduced in the smaller partition to
enable perfect matching. Dummy nodes are as-
signed a similarity of zero with all other nodes.
Alignments to dummy nodes (such as for source
nodes (3) and (6)) are ignored during projection.
Perfect matchings are more restrictive models
of constituent alignment than edge covers. Being
bijective, the resulting alignments cannot model
splitting or merging operations at all. Insertions
and deletions can be modelled only indirectly by
aligning nodes in the larger partition to dummy
nodes on the other side (see the source side in Fig-
ure 2 where nodes (3) and (6) are aligned to (d)).
Section 5 assesses if these modelling limitations
impact the quality of the resulting alignments.
Algorithms Minimum weight perfect match-
ings in bipartite graphs can be computed effi-
ciently in cubic time using algorithms for net-
work optimisation (Fredman and Tarjan, 1987;
timeO(|Us|2 log |Us|+ |Us|2|Ut |)) or algorithms for
the equivalent linear assignment problem (Jonker
and Volgenant, 1987; time O(max(|Us|, |Ut |)3)).
Their complexity is a linear factor slower than the
quadratic runtime of the local optimisation meth-
ods presented in Section 2.
The computation of (general) edge covers has
been investigated by Eiter and Mannila (1997) in
the context of distance metrics for point sets. They
show that edge covers can be reduced to minimum
weight perfect matchings of an auxiliary bipar-
tite graph with two partitions of size |Us|+ |Ut |.
This allows the computation of general minimum
weight edge covers in time O((|Us|+ |Ut |)3).
4 Filtering via Tree Pruning
We introduce two filtering techniques which effec-
tively remove constituents from source and target
trees before alignment takes place. Tree pruning as
a preprocessing step is more general and more effi-
cient than our original post-processing filter (Pad?
and Lapata, 2005) which was embedded into the
similarity function. Not only does tree pruning not
interfere with the similarity function but also re-
duces the size of the graph, thus speeding up the
algorithms discussed in the previous section.
We present two instantiations of tree pruning:
word-based filtering, which subsumes our earlier
method, and argument-based filtering, which elim-
inates unlikely argument candidates.
Word-based filtering This technique re-
moves terminal nodes from parse trees accord-
ing to certain linguistic or alignment-based crite-
ria. We apply two word-based filters in our ex-
periments. The first removes non-content words,
i.e., all words which are not adjectives, adverbs,
verbs, or nouns, from the source and target sen-
1164
Kim versprach, p?nktlich zu kommen.
VP
S
VP
S
Figure 3: Filtering of unlikely arguments (predi-
cate in boldface, potential arguments in boxes).
tences (Pad? and Lapata, 2005). We also use a
novel filter which removes all words which remain
unaligned in the automatic word alignment. Non-
terminal nodes whose terminals are removed by
these filters, are also pruned.
Argument filtering Previous work in shal-
low semantic parsing has demonstrated that not
all nodes in a tree are equally probable as seman-
tic roles for a given predicate (Xue and Palmer,
2004). In fact, assuming a perfect parse, there is
a ?set of likely arguments?, to which almost all
semantic roles roles should be assigned to. This
set of likely arguments consists of all constituents
which are a child of some ancestor of the pred-
icate, provided that (a) they do not dominate the
predicate themselves and (b) there is no sentence
boundary between a constituent and its predicate.
This definition covers long-distance dependencies
such as control constructions for verbs, or support
constructions for nouns and adjectives, and can be
extended slightly to accommodate coordination.
This argument-based filter reduces target trees
to a set of likely arguments. In the example in Fig-
ure 3, all tree nodes are removed except Kim and
p?nktlich zu kommen.
5 Evaluation Set-up
Data For evaluation, we used the parallel cor-
pus3 from our earlier work (Pad? and Lapata,
2005). It consists of 1,000 English-German sen-
tence pairs from the Europarl corpus (Koehn,
2005). The sentences were automatically parsed
(using Collin?s 1997 parser for English and
Dubey?s 2005 parser for German), and manually
annotated with FrameNet-like semantic roles (see
Pad? and Lapata 2005 for details.)
Word alignments were computed with the
GIZA++ toolkit (Och and Ney, 2003), using the
3The corpus can be downloaded from http://www.
coli.uni-saarland.de/~pado/projection/.
entire English-German Europarl bitext as training
data (20M words). We used the GIZA++ default
settings to induce alignments for both directions
(source-target, target-source). Following common
practise in MT (Koehn et al, 2003), we considered
only their intersection (bidirectional alignments
are known to exhibit high precision). We also pro-
duced manual word alignments for all sentences
in our corpus, using the GIZA++ alignments as a
starting point and following the Blinker annotation
guidelines (Melamed, 1998).
Method and parameter choice The con-
stituent alignment models we present are unsu-
pervised in that they do not require labelled data
for inferring correct alignments. Nevertheless, our
models have three parameters: (a) the similarity
measure for identifying semantically equivalent
constituents; (b) the filtering procedure for remov-
ing noise in the data (e.g., wrong alignments); and
(c) the decision procedure for projection.
We retained the similarity measure introduced
in Pad? and Lapata (2005) which computes the
overlap between a source constituent and its can-
didate projection, in both directions. Let y(cs) and
y(ct) denote the yield of a source and target con-
stituent, respectively, and al(T ) the union of all
word alignments for a token set T :
sim(cs,ct) =
|y(ct)?al(y(cs))|
|y(cs)|
|y(cs)?al(y(ct))|
|y(ct)|
We examined three filtering procedures (see Sec-
tion 4): removing non-aligned words (NA), re-
moving non-content words (NC), and removing
unlikely arguments (Arg). These were combined
with three decision procedures: local forward
alignment (Forward), perfect matching (Perf-
Match), and edge cover matching (EdgeCover)
(see Section 3). We used Jonker and Vol-
genant?s (1987) solver4 to compute weighted per-
fect matchings.
In order to find optimal parameter settings for
our models, we split our corpus randomly into a
development and test set (both 50% of the data)
and examined the parameter space exhaustively
on the development set. The performance of the
best models was then assessed on the test data.
The models had to predict semantic roles for Ger-
man, using English gold standard roles as input,
and were evaluated against German gold standard
4The software is available from http://www.
magiclogic.com/assignment.html.
1165
Model Prec Rec F-score
WordBL 45.6 44.8 45.1
Forward 66.0 56.5 60.9
PerfMatch 71.7 54.7 62.1
N
o
F
il
te
r
EdgeCover 65.6 57.3 61.2
UpperBnd 85.0 84.0 84.0
Model Prec Rec F-score
WordBL 45.6 44.8 45.1
Forward 74.1 56.1 63.9
PerfMatch 73.3 62.1 67.2
N
A
F
il
te
r
EdgeCover 70.5 62.9 66.5
UpperBnd 85.0 84.0 84.0
Model Prec Rec F-score
WordBL 45.6 44.8 45.1
Forward 64.3 47.8 54.8
PerfMatch 73.1 56.9 64.0
N
C
F
il
te
r
EdgeCover 67.5 57.0 61.8
UpperBnd 85.0 84.0 84.0
Model Prec Rec F-score
WordBL 45.6 44.8 45.1
Forward 69.9 60.7 65.0
PerfMatch 80.4 48.1 60.2
A
rg
F
il
te
r
EdgeCover 69.6 60.6 64.8
UpperBnd 85.0 84.0 84.0
Table 1: Model comparison using intersective alignments (development set)
roles. To gauge the extent to which alignment er-
rors are harmful, we present results both on inter-
sective and manual alignments.
Upper bound and baseline In Pad? and La-
pata (2005), we assessed the feasibility of seman-
tic role projection by measuring how well anno-
tators agreed on identifying roles and their spans.
We obtained an inter-annotator agreement of 0.84
(F-score), which can serve as an upper bound for
the projection task. As a baseline, we use a sim-
ple word-based model (WordBL) from the same
study. The units of this model are words, and the
span of a projected role is the union of all target
terminals aligned to a terminal of the source role.
6 Results
Development set Our results on the develop-
ment set are summarised in Table 1. We show how
performance varies for each model according to
different filtering procedures when automatically
produced word alignments are used. No filtering
is applied to the baseline model (WordBL).
Without filtering, local and global models yield
comparable performance. Models based on perfect
bipartite matchings (PerfMatch) and edge covers
(EdgeCover) obtain slight F-score improvements
over the forward alignment model (Forward). It
is worth noticing that PerfMatch yields a signifi-
cantly higher precision (using a ?2 test, p < 0.01)
than Forward and EdgeCover. This indicates that,
even without filtering, PerfMatch delivers rather
accurate projections, however with low recall.
Model performance seems to increase with tree
pruning. When non-aligned words are removed
(Table 1, NA Filter), PerfMatch and EdgeCover
reach an F-score of 67.2 and 66.5, respectively.
This is an increase of approximately 3% over the
local Forward model. Although the latter model
yields high precision (74.1%), its recall is sig-
nificantly lower than PerfMatch and EdgeCover
(p < 0.01). This demonstrates the usefulness of
filtering for the more constrained global models
which as discussed in Section 3 can only represent
a limited set of alignment possibilities.
The non-content words filter (NC filter) yields
smaller improvements. In fact, for the Forward
model, results are worse than applying no filter-
ing at all. We conjecture that NC is an overly
aggressive filter which removes projection-critical
words. This is supported by the relatively low re-
call values. In comparison to NA, recall drops
by 8.3% for Forward and by almost 6% for Perf-
Match and EdgeCover. Nevertheless, both Perf-
Match and EdgeCover outperform the local For-
ward model. PerfMatch is the best performing
model reaching an F-score of 64.0%.
We now consider how the models behave when
the argument-based filter is applied (Arg, Table 1,
bottom). As can be seen, the local model benefits
most from this filter, whereas PerfMatch is worst
affected; it obtains its highest precision (80.4%) as
well as its lowest recall (48.1%). This is somewhat
expected since the filter removes the majority of
nodes in the target partition causing a proliferation
of dummy nodes. The resulting edge covers are
relatively ?unnatural?, thus counterbalancing the
advantages of global optimisation.
To summarise, we find on the development set
that PerfMatch in the NA Filter condition obtains
the best performance (F-score 67.2%), followed
closely by EdgeCover (F-score 66.5%) in the same
1166
Model Prec Rec F-score
WordBL 45.7 45.0 43.3
Forward (Arg) 72.4 63.2 67.5
PerfMatch (NA) 75.7 63.7 69.2
EdgeCover (NA) 73.0 64.9 68.7
In
te
rs
ec
tiv
e
UpperBnd 85.0 84.0 84.0
Model Prec Rec F-score
WordBL 62.1 60.7 61.4
Forward (Arg) 72.2 68.6 70.4
PerfMatch (NA) 75.7 67.5 71.4
EdgeCover (NA) 71.9 69.3 70.6M
an
ua
l
UpperBnd 85.0 84.0 84.0
Table 2: Model comparison using intersective and
manual alignments (test set)
condition. In general, PerfMatch seems less sensi-
tive to the type of filtering used; it yields best re-
sults in three out of four filtering conditions (see
boldface figures in Table 1). Our results further in-
dicate that Arg boosts the performance of the local
model by guiding it towards linguistically appro-
priate alignments.5
A comparative analysis of the output of Perf-
Match and EdgeCover revealed that the two mod-
els make similar errors (85% overlap). Disagree-
ments, however, arise with regard to misparses.
Consider as an example the sentence pair:
The Charter is [NP an opportunity to
bring the EU closer to the people.]
Die Charta ist [NP eine Chance], [S die
EU den B?rgern n?herzubringen.]
An ideal algorithm would align the English NP
to both the German NP and S. EdgeCover, which
can model one-to-many-relationships, acts ?con-
fidently? and aligns the NP to the German S to
maximise the overlap similarity, incurring both a
precision and a recall error. PerfMatch, on the
other hand, cannot handle one-to-many relation-
ships, acts ?cautiously? and aligns the English NP
to a dummy node, leading to a recall error. Thus,
even though EdgeCover?s analysis is partly right,
it will come out worse than PerfMatch, given the
current dataset and evaluation method.
Test set We now examine whether our results
carry over to the test data. Table 2 shows the
5Experiments using different filter combinations did not
lead to performance gains over individual filters and are not
reported here due to lack of space.
performance of the best models (Forward (Arg),
PerfMatch (NA), and EdgeCover (NA)) on auto-
matic (Intersective) and manual (Manual) align-
ments.6 All models perform significantly better
than the baseline but significantly worse than the
upper bound (both in terms of precision and recall,
p < 0.01). PerfMatch and EdgeCover yield better
F-scores than the Forward model. In fact, Perf-
Match yields a significantly better precision than
Forward (p < 0.01).
Relatively small performance gains are ob-
served when manual alignments are used. The F-
score increases by 2.9% for Forward, 2.2% for
PerfMatch, and 1.9% for EdgeCover. Also note
that this better performance is primarily due to a
significant increase in recall (p < 0.01), but not
precision. This is an encouraging result indicating
that our filters and graph-based algorithms elim-
inate alignment noise to a large extent. Analysis
of the models? output revealed that the remain-
ing errors are mostly due to incorrect parses (none
of the parsers employed in this work were trained
on the Europarl corpus) but also to modelling de-
ficiencies. Recall from Section 3 that our global
models cannot currently capture one-to-zero cor-
respondences, i.e., deletions and insertions.
7 Related work
Previous work has primarily focused on the pro-
jection of grammatical (Yarowsky and Ngai, 2001)
and syntactic information (Hwa et al, 2002). An
exception is Fung and Chen (2004), who also
attempt to induce FrameNet-style annotations in
Chinese. Their method maps English FrameNet
entries to concepts listed in HowNet7, an on-line
ontology for Chinese, without using parallel texts.
The present work extends our earlier projection
framework (Pad? and Lapata, 2005) by proposing
global methods for automatic constituent align-
ment. Although our models are evaluated on the
semantic role projection task, we believe they also
show promise in the context of statistical ma-
chine translation. Especially for systems that use
syntactic information to enhance translation qual-
ity. For example, Xia and McCord (2004) exploit
constituent alignment for rearranging sentences in
the source language so as to make their word or-
6Our results on the test set are slightly higher in compar-
ison to the development set. The fluctuation reflects natural
randomness in the partitioning of our corpus.
7See http://www.keenage.com/zhiwang/e_
zhiwang.html.
1167
der similar to that of the target language. They
learn tree reordering rules by aligning constituents
heuristically using a naive local optimisation pro-
cedure analogous to forward alignment. A simi-
lar approach is described in Collins et al (2005);
however, the rules are manually specified and the
constituent alignment step reduces to inspection of
the source-target sentence pairs. The global opti-
misation models presented in this paper could be
easily employed for the reordering task common
to both approaches.
Other approaches treat rewrite rules not as a
preprocessing step (e.g., for reordering source
strings), but as a part of the translation model
itself (Gildea, 2003; Gildea, 2004). Constituent
alignments are learnt by estimating the probabil-
ity of tree transformations, such as node deletions,
insertions, and reorderings. These models have a
greater expressive power than our edge cover mod-
els; however, this implies that approximations are
often used to make computation feasible.
8 Conclusions
In this paper, we have proposed a novel method
for obtaining constituent alignments between par-
allel sentences and have shown that it is use-
ful for semantic role projection. A key aspect of
our approach is the formalisation of constituent
alignment as the search for a minimum weight
edge cover in a bipartite graph. This formalisation
provides efficient mechanisms for aligning con-
stituents and yields results superior to heuristic ap-
proaches. Furthermore, we have shown that tree-
based noise filtering techniques are essential for
good performance.
Our approach rests on the assumption that con-
stituent alignment can be determined solely from
the lexical similarity between constituents. Al-
though this allows us to model constituent align-
ments efficiently as edge covers, it falls short of
modelling translational divergences such as substi-
tutions or insertions/deletions. In future work, we
will investigate minimal tree edit distance (Bille,
2005) and related formalisms which are defined
on tree structures and can therefore model diver-
gences explicitly. However, it is an open ques-
tion whether cross-linguistic syntactic analyses are
similar enough to allow for structure-driven com-
putation of alignments.
Acknowledgments The authors acknowledge
the support of DFG (Pad?; grant Pi-154/9-2) and
EPSRC (Lapata; grant GR/T04540/01).
References
P. Bille. 2005. A survey on tree edit distance and related
problems. Theoretical Computer Science, 337(1-3):217?
239.
H. C. Boas. 2005. Semantic frames as interlingual represen-
tations for multilingual lexical databases. International
Journal of Lexicography, 18(4):445?478.
X. Carreras, L. M?rquez, eds. 2005. Proceedings of the
CoNLL shared task: Semantic role labelling, Boston, MA,
2005.
M. Collins, P. Koehn, I. Kuc?erov?. 2005. Clause restructur-
ing for statistical machine translation. In Proceedings of
the 43rd ACL, 531?540, Ann Arbor, MI.
M. Collins. 1997. Three generative, lexicalised models for
statistical parsing. In Proceedings of the ACL/EACL, 16?
23, Madrid, Spain.
A. Dubey. 2005. What to do when lexicalization fails: pars-
ing German with suffix analysis and smoothing. In Pro-
ceedings of the 43rd ACL, 314?321, Ann Arbor, MI.
T. Eiter, H. Mannila. 1997. Distance measures for point sets
and their computation. Acta Informatica, 34(2):109?133.
C. J. Fillmore, C. R. Johnson, M. R. Petruck. 2003. Back-
ground to FrameNet. International Journal of Lexicogra-
phy, 16:235?250.
M. L. Fredman, R. E. Tarjan. 1987. Fibonacci heaps and
their uses in improved network optimization algorithms.
Journal of the ACM, 34(3):596?615.
P. Fung, B. Chen. 2004. BiFrameNet: Bilingual frame se-
mantics resources construction by cross-lingual induction.
In Proceedings of the 20th COLING, 931?935, Geneva,
Switzerland.
D. Gildea, D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3):245?288.
D. Gildea. 2003. Loosely tree-based alignment for machine
translation. In Proceedings of the 41st ACL, 80?87, Sap-
poro, Japan.
D. Gildea. 2004. Dependencies vs. constituents for tree-
based alignment. In Proceedings of the EMNLP, 214?221,
Barcelona, Spain.
C. Hi, R. Hwa. 2005. A backoff model for bootstrapping
resources for non-english languages. In Proceedings of
the HLT/EMNLP, 851?858, Vancouver, BC.
R. Hwa, P. Resnik, A. Weinberg, O. Kolak. 2002. Evaluation
of translational correspondence using annotation projec-
tion. In Proceedings of the 40th ACL, 392?399, Philadel-
phia, PA.
R. Jonker, T. Volgenant. 1987. A shortest augmenting path
algorithm for dense and sparse linear assignment prob-
lems. Computing, 38:325?340.
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical phrase-based
translation. In Proceedings of the HLT/NAACL, 127?133,
Edmonton, AL.
P. Koehn. 2005. Europarl: A parallel corpus for statistical
machine translation. In Proceedings of the MT Summit X,
Phuket, Thailand.
I. D. Melamed. 1998. Manual annotation of translational
equivalence: The Blinker project. Technical Report IRCS
TR #98-07, IRCS, University of Pennsylvania, 1998.
F. J. Och, H. Ney. 2003. A systematic comparison of various
statistical alignment models. Computational Linguistics,
29(1):19?52.
S. Pad?, M. Lapata. 2005. Cross-lingual projection
of role-semantic information. In Proceedings of the
HLT/EMNLP, 859?866, Vancouver, BC.
F. Xia, M. McCord. 2004. Improving a statistical MT system
with automatically learned rewrite patterns. In Proceed-
ings of the 20th COLING, 508?514, Geneva, Switzerland.
N. Xue, M. Palmer. 2004. Calibrating features for seman-
tic role labeling. In Proceedings of the EMNLP, 88?94,
Barcelona, Spain.
D. Yarowsky, G. Ngai. 2001. Inducing multilingual text
analysis tools via robust projection across aligned corpora.
In Proceedings of the HLT, 161?168, San Diego, CA.
1168
Semantic Role Labelling with Similarity-Based Generalization Using
EM-based Clustering
Ulrike Baldewein, Katrin Erk, Sebastian Pad?
Saarland University
Saarbr?cken, Germany
{ulrike,erk,pado}@coli.uni-sb.de
Detlef Prescher
University of Amsterdam
Amsterdam, The Netherlands
prescher@science.uva.nl
Abstract
We describe a system for semantic role assignment
built as part of the Senseval III task, based on an
off-the-shelf parser and Maxent and Memory-Based
learners. We focus on generalisation using several
similarity measures to increase the amount of train-
ing data available and on the use of EM-based clus-
tering to improve role assignment. Our final score
is Precision=73.6%, Recall=59.4% (F=65.7).
1 Introduction
This paper describes a study in semantic role la-
belling in the context of the Senseval III task, for
which the training and test data were both drawn
from the current FrameNet release (Johnson et al,
2002). We concentrated on two questions: first,
whether role assignment can be improved by gener-
alisation over training instances using different sim-
ilarity measures; and second, the impact of EM-
based clustering, both in deriving more informative
selectional preference features and in the generali-
sations mentioned above. The basis of our experi-
ments was formed by off-the-shelf statistical tools
for data processing and modelling.
After listing our data preparation steps (Sec. 2)
and features (Sec. 3), we describe our classification
procedure and the learners we used (Sec. 4). Sec. 5
outlines our experiments in similarity-based gener-
alisations, and Section 6 discusses our results.
2 Data and Instances
Parsing. To tag and parse the data, we used
LoPar (Schmid, 2000), a probabilistic context-
free parser, which comes with a Head-Lexicalised
Grammar for English (Carroll and Rooth, 1998).
We considered only the most probable parse for
each sentence and simplified parse trees by elim-
inating unary nodes. The resulting nodes form
the instances of our classification. We used the
Stuttgart TreeTagger (Schmid, 1994) to lemmatise
constituent heads.
Projection of role labels. FrameNet provides se-
mantic roles as character offsets. We labelled
those instances (i.e. nodes in the parse tree) with
gold standard semantic roles which corresponded to
roles? maximal projections. 13.95% of roles in the
training corpus spanned more than one parse tree
node. Figure 1 shows an example sentence for the
AWARENESS frame. The nodes? respective seman-
tic role labels are given in small caps, and the target
predicate is marked in boldface.
S (NONE)
NP (COGNIZER)
Peter
VP (NONE)
V (NONE)
does not VP (NONE)
know NP (CONTENT)
the answer
Figure 1: Example parse tree with role labels
Semantic clustering. We used clustering to gen-
eralise over possible fillers of roles. In a first model,
we derived a probability distribution   for pairs
	

, where  is a target:role combination
and   is the head lemma of a role filler. The key
idea is that   and   are mutually independent, but
conditioned on an unobserved class  . In this
manner, we define the probability of 
     
Semantic Role Labelling With Chunk Sequences
Ulrike Baldewein, Katrin Erk, Sebastian Pad?
Saarland University
Saarbr?cken, Germany
{ulrike,erk,pado}@coli.uni-sb.de
Detlef Prescher
University of Amsterdam
Amsterdam, The Netherlands
prescher@science.uva.nl
Abstract
We describe a statistical approach to semantic
role labelling that employs only shallow infor-
mation. We use a Maximum Entropy learner,
augmented by EM-based clustering to model
the fit between a verb and its argument can-
didate. The instances to be classified are se-
quences of chunks that occur frequently as ar-
guments in the training corpus. Our best model
obtains an F score of 51.70 on the test set.
1 Introduction
This paper describes a statistical approach to semantic
role labelling addressing the CoNLL shared task 2004,
which is based on the the current release of the English
PropBank data (Kingsbury et al, 2002). For further de-
tails of the task, see (Carreras and M?rquez, 2004).
We address the main challenge of the task, the absence
of deep syntactic information, with three main ideas:
  Proper constituents being unavailable, we use chunk
sequences as instances for classification.
  The classification is performed by a maximum en-
tropy model, which can integrate features from het-
erogeneous data sources.
  We model the fit between verb and argument can-
didate by clusters induced with EM on the training
data, which we use as features during classification.
Sections 2 through 4 describe the systems? architec-
ture. First, we compute chunk sequences for all sentences
(Sec. 2). Then, we classify these sequences with max-
imum entropy models (Sec. 3). Finally, we determine
the most probable chain of sequences covering the whole
sentence (Sec. 4). Section 5 discusses the impact of dif-
ferent parameters and gives final results.
2 Chunk Sequences as Instances
All studies of semantic role labelling we are aware of
have used constituents as instances for classification.
However, constituents are not available in the shallow
syntactic information provided by this task. Two other
levels of granularity are available in the data: words and
chunks. In a pretest, we found that words are too fine
grained, such that learners find it very difficult to identify
argument boundaries on the word level. Chunks, too, are
problematic, since one third of the arguments span more
than one chunk, and for one tenth of the arguments the
boundaries do not coincide with any chunk boundaries.
We decided to use chunk sequences as instances for
classification. They can describe multi-chunk and part-
chunk arguments, and by approximating constituents,
they allow the use of linguistically informed features. In
the sentence in Figure 1, Britain?s manufacturing indus-
try forms a sequence of type NP_NP. To make sequences
more distinctive, we conflate whole clauses embedded
deeper than the target to S: For the target transform-
ing, we characterise the sequence for to boost exports
as S rather than VP_NP. An argument boundary inside
a chunk is indicated by the part of speech of the last in-
cluded word: For boost the sequence is VP(NN).
To determine ?good? sequences, we collected argu-
ment realisations from the training corpus, generalising
them by simple heuristics (e.g. removing anything en-
closed in brackets). The generalised argument sequences
exhibit a Zipfian distribution (see Fig. 2). NP is by
far the most frequent sequence, followed by S. An ex-
ample of a very infrequent argument chunk sequence
is NP_PP_NP_PP_NP_VP_PP_NP_NP (in words: a
bonus in the form of charitable donations made from an
employer ?s treasury).
The chunk sequence approach also allows us to con-
sider the divider chunk sequences that separate arguments
and targets. For example, A0s are usually divided from
the target by the empty divider, while A2 arguments are
Britain ?s manufacturing industry is transforming itself to boost exports
NNP POS VBG NN VBZ VBG PRP TO NN NNS
[NP ] [NP ] [VP ] [NP] [VP ] [NP ]
[S ]
Figure 1: Part of a sentence with part of speech, chunk and clause information
 0
 5000
 10000
 15000
 20000
 25000
Fr
eq
ue
nc
y 
in
 tr
ai
ni
ng
 d
at
a
Sequence frequencies
Divider frequencies
Figure 2: Frequency distribution for the 20 most frequent
sequences and dividers in the training data
separated from it by e.g. a typical A1 sequence. Gen-
eralised divider chunk sequences separating actual argu-
ments and targets in the training set show a Zipfian distri-
bution similar to the chunk sequences (see Fig. 2).
As instances to be classified, we consider all sequences
whose generalised sequence and divider each appear at
least 10 times for an argument in the training corpus, and
whose generalised sequence and divider appear together
at least 5 times. The first cutoff reduces the number of
sequences from 1089 to 87, and the number of dividers
from 999 to 120, giving us 581,813 sequences as training
data (about twice as many as words), of which 45,707
are actual argument labels. The additional filter for se-
quence/divider pairs reduces the training data to 354,916
sequences, of which 43,622 are actual arguments. We pay
for the filtering by retaining only 87.49% of arguments on
the training set (83.32% on the development set).
3 Classification
3.1 Maximum Entropy Modelling
We use a log-linear model as classifier, which defines the
probability of a class  given an feature vector

 as




	
ffThe Influence of Argument Structure on Semantic Role Assignment
Sebastian Pad?
SALSA
Dept. of Computational Linguistics
Saarland University
Saarbr?cken
pado@coli.uni-sb.de
Gemma Boleda
GLiCom
Dept. of Translation and Interpreting
Pompeu Fabra University
Barcelona
gemma.boleda@upf.edu
Abstract
We present a data and error analysis for semantic role
labelling. In a first experiment, we build a generic statis-
tical model for semantic role assignment in the FrameNet
paradigm and show that there is a high variance in perfor-
mance across frames. The main hypothesis of our paper
is that this variance is to a large extent a result of differ-
ences in the underlying argument structure of the pred-
icates in different frames. In a second experiment, we
show that frame uniformity, which measures argument
structure variation, correlates well with the performance
figures, effectively explaining the variance.
1 Introduction
Recent years have witnessed growing interest in
corpora with semantic annotation, especially on
the semantic role (or argument structure) level. A
number of projects are working on producing such
corpora through manual annotation, among which
are FrameNet (Baker et al, 1998), the Prague
Dependency Treebank (Hajic?ov?, 1998), Prop-
Bank (Kingsbury et al, 2002), and SALSA (Erk et
al., 2003).
For semantic role annotation to be widely useful
for NLP, however, robust and accurate methods for
automatic semantic role assignment are necessary.
Starting with Gildea and Jurafsky (2000), a num-
ber of studies have developed (almost exclusively
statistical) models of this task, e.g. Thompson et
al. (2003) and Fleischman et al (2003). This year
(2004), semantic role labelling served as the shared
task at two conferences, CoNLL1 and SENSEVAL2.
However, almost all studies have concentrated on
the technical aspects of the models ? identifying in-
formative feature sets and suitable statistical frame-
works ? with the goal of optimising the performance
of the models on the complete dataset. The only
study we are aware of with a more detailed evalu-
ation is Fleischman et al (2003), who nevertheless
come to the conclusion that either ?new features?,
1http://www.lsi.upc.es/~conll04st/
2http://www.clres.com/SensSemRoles.html
?more data?, or ?more sophisticated models? are
needed.
The present study is a first step in pursuing the
third alternative, presenting a data and error anal-
ysis for semantic role assignment in the FrameNet
paradigm. We first build two different, generic sta-
tistical models for semantic role assignment, which
are fairly representative for the span of models in-
vestigated in the literature. A frame-wise evalua-
tion shows that the models exhibit a large variance
in performance across frames.
Our hypothesis is that this variance is to a large
extent caused by differences in the underlying ar-
gument structure of the predicates: Frames which
are less uniform, i.e. whose predicates have a more
heterogeneous mapping between semantic roles and
syntactic functions, are more difficult to label auto-
matically. In order to put this hypothesis, which is
intuitively very plausible, on a a firm empirical foot-
ing, we investigate the relationship between frame
uniformity and the variance in the data and show
that the two variables correlate. Since argument
structure has been investigated mostly for verbs, we
restrict our study to verbal predicates.
Structure of the paper. In Section 2 we give a
brief introduction to FrameNet. Section 3 outlines
the first experiment and discusses the variance in
performance across frames. In Section 4, we define
two measures of frame uniformity based on argu-
ment structure, and show in our second experiment
(Section 5) that they correlate with the performance
figures. Finally, Section 6 discusses the implica-
tions of our results for semantic role assignment.
2 FrameNet
FrameNet is a lexical resource based on Fillmore?s
Frame Semantics (Fillmore, 1985). It is designed
as an ontology of frames, representations of pro-
totypical situations. Each frame provides a set of
predicates (nouns, verbs or adjectives) which can
introduce the frame. The semantic roles are frame-
specific, since they are defined as categories of enti-
ties or concepts pertaining to the particular situation
a predicate evokes.
The following sentences are examples for the se-
mantic annotation provided in the FrameNet cor-
pus for verbs in the IMPACT frame, which describes
a situation in which typically ?an Impactor makes
sudden, forcible contact with the Impactee, or two
Impactors both ... [make] forcible contact?3 .
(1) a. [Impactee His car] was struck [Impactor by
a third vehicle].
b. [Impactor The door] slammed
[Result shut].
c. [Impactors Their vehicles] collided
[Place at Pond Hill].
Note that the frame-specificity of semantic roles in
FrameNet has important consequences for semantic
role assignment, since there is no direct way to gen-
eralise across frames. Therefore, the learning for
automatic assignment of semantic roles has to pro-
ceed frame-wise. Thus, the data sparseness prob-
lem is especially acute, and automatic assignment
for frames with no training data is very difficult (see
Gildea and Jurafsky (2002)).
3 Experiment 1: Frame-Wise Evaluation
of Semantic Role Assignment
In our first experiment, we perform a detailed
(frame-wise) evaluation of semantic role assign-
ment to discover general patterns in the data. Our
aim is not to outperform existing models, but to
replicate the workings of existing models so that our
findings are representative for the task as it is cur-
rently addressed. To this end, we (a) use a standard
dataset, the FrameNet data, (b) model the task with
two different statistical frameworks, and (c) keep
our models as generic as possible.
3.1 Data and experimental setup
For this experiment, we use 57758 manually anno-
tated sentences from FrameNet (release 2), corre-
sponding to all the sentences with verbal predicates
(2228 lemmata from 196 frames). Gildea and Ju-
rafsky (2000) and Fleischman et al (2003) used a
previous release of the dataset with less annotated
instances, but covered all predicates (verbs, nouns
and adjectives).
Data preparation. After tagging the data with
TnT (Brants, 2000), we parse them using the Collins
parsing model 3 (Collins, 1997). We consider only
3From the definition of the frame at http://www.icsi.
berkeley.edu/~framenet/. Examples adapted from
the FrameNet data, release 2.
the most probable parse for each sentence and sim-
plify the resulting parse tree by removing all unary
nodes. We lemmatise the head of each constituent
with TreeTagger (Schmid, 1994).
Gold standard. We transform the FrameNet
character-offset annotations for semantic roles into
our constituent format by determining the maximal
projection for each semantic role, i.e. the set of con-
stituents that exactly covers the extent of the role. A
constituent is assigned a role iff it is in the maximum
projection of a role.
Classification procedure. The instances to be
classified are all parse tree constituents. Since di-
rect assignment of role labels to instances fails due
to the preponderance of unlabelled instances, which
make up 86.7% of all instances, we follow Gildea
and Jurafsky (2000) in splitting the task into two
sequential subtasks: first, argument recognition de-
cides for each instance whether it bears a semantic
role or not; then, argument labelling assigns a label
to instances recognised as role-bearers. For the sec-
ond step, we train frame-specific classifiers, since
the frame-specificity of roles does not allow to eas-
ily combine training data from different frames.
Statistical modelling. We perform the classifica-
tion twice, with two learners from different statisti-
cal frameworks, in order to make our results more
representative for the different statistical models
employed so far for the task. The first learner uses
the maximum entropy (Maxent) framework, which
has been applied e.g. by Fleischman et al (2003).
The model is trained with the estimate software,
which implements the LMVM algorithm (Malouf,
2002)4. The second learner is an instance of a
memory-based learning (MBL) algorithm, the   -
nearest neighbour algorithm. We use the implemen-
tation provided by TiMBL (Daelemans et al, 2003)
with the recommended parameters, namely
 
,
adopting modified value difference with gain ratio
feature weighting as similarity metric.
3.2 Features
In accordance with our goal of keeping our models
generic, we use a set of vary (syntactic and lexical)
features which more than one study in the literature
has found helpful, without optimising the features
for the individual learners.
Constituent features: The first type of feature
represents properties of the constituent in question.
We use the phrase type and head lemma of each
constituent; its preposition (if available); its position
4Software available for download at http://www-
rohan.sdsu.edu/ malouf/pubs.html
relative to the predicate (left, right or overlapping);
the phrase type of its mother constituent; whether it
is an argument of the target, according to the parser;
and the path between target and constituent as well
as its length.
Sentence level features: The second type of fea-
ture describes the context of the current instance.
The predicate is represented by its lemma, its part of
speech, its (heuristic) subcategorisation frame, and
its governing verb. We also compile a list of all the
prepositions in the sentence.
3.3 Results
All results in this section are averages over F scores
obtained using 10-fold cross validation. For each
frame, we perform two evaluations, one in exact
match and one in overlap mode. In exact match
mode, an assignment only counts as a true positive
if it coincides exactly with the gold standard, while
in overlap mode it suffices that they are not disjoint.
F scores are then computed in the usual manner.
Table 1 shows the performance of the different
configurations over the complete dataset, and the
standard deviation of these results over all frames.
To illustrate the results for individual frames, Ta-
ble 2 lists frame-specific performances for five ran-
domly selected frames and how they varied over
cross validation runs.
Maxent MBL
Exact Match 53.3   10.8 56.9   10.1
Overlap 70.0   11.0 74.2   10.0
Table 1: Overall F scores and standard deviation
across frames for Experiment 1.
3.4 Analysis and Discussion
In terms of overall results, the MBL model outper-
forms the Maxent model by 3 to 4 points F-score.
However, all our results lie broadly in the range
of existing systems with a similar architecture (i.e.
sequential argument identification and labelling):
Gildea and Jurafsky (2002) report      , and
Fleischman et al (2003)   
	 for exact match
evaluation. We assume that our feature formulation
is more suitable for the MBL model. Also, we do
not smooth the Maxent model, while we use the rec-
ommended optimised parameters for TiMBL.
Our most remarkable finding is the high amount
of variance presented by the numbers in Table 1.
Computed across frames, the standard deviation
amounts to 10% to 11%, consistently across eval-
uation measures and statistical frameworks. Since
these figures are results of a 10-fold cross valida-
tion run, it is improbable that the effect is solely
Exact match Maxent MBL
APPEARANCE 50.5   4.5 60.1   7.3
AVOIDING 47.9   5.0 51.3   6.9
JUDGM._COMM. 57.0   1.5 57.5   3.4
ROBBERY 38.4   19.1 37.9   16.2
WAKING_UP 60.5   11.4 64.4   11.8
Overlap Maxent MBL
APPEARANCE 68.3   4.0 75.0   5.6
AVOIDING 68.6   4.3 72.7   5.9
JUDGM._COMM. 76.9   1.6 77.6   1.8
ROBBERY 61.2   20.6 55.2   17.6
WAKING_UP 75.1   9.1 77.6   7.8
Total Exact Match 53.3   0.5 56.9   0.4
Total Overlap 70.0   0.4 74.2   0.5
Table 2: F scores and standard deviations over cross
validation runs for five random frames (Exp. 1).
due to chance splits into training and test data. This
assessment is supported by Table 2, which shows
that, while the performance on individual frames
can vary largely (especially for small frames like
ROBBERY), the average performance on all frames
varies less than 0.5% over the cross validation runs.
The reasons which lead to the across-frames vari-
ance warrant investigation, since they may lead to
new insights about the nature of the task in question,
answering Fleischman et al?s (2003) call for bet-
ter models. Some of the plausible variables which
might explain the variance are the number of seman-
tic roles per frame, the amount of training data, and
the number of verbs per frame.
However, we suggest that a fourth variable might
have a more decisive influence. Seen from a lin-
guistic perspective, semantic role assignment is just
an application of linking, i.e. learning the regulari-
ties of the relationship between semantic roles and
their possible syntactic realisation and applying this
knowledge. Therefore, our main hypothesis is: The
more varied the realisation possibilities of the verbs
in a frame, the more difficult it is for the learner to
learn the correct linking patterns, and therefore the
more error-prone semantic role assignment. Even
though this claim appears intuitively true, it has
never been explicitly made nor empirically tested,
and its consequences might be relevant for the de-
sign of future models of semantic role assignment.
As an example, compare the frame IMPACT,
as exemplified by the instances in (1), with the
frame INGESTION, which contains predicates such
as drink, consume or nibble. While every sentence
in (1) shows a different linking pattern, linking for
INGESTION is rather straightforward: the subject is
usually the Ingestor, and the direct object is an In-
gestible. This is reflected in the scores: 
  
for IMPACT and 
   
	
for INGESTION (exact
match scores for the MBL model).
The most straightforward strategy to test for the
different variables would be to perform multiple
correlation analyses. However, this approach has a
serious drawback: The results are hard to interpret
when more than one variable is significantly corre-
lated with the data, and this is increasingly prob-
able with higher amounts of data points. Instead,
we adopt a second strategy, namely to design a new
data set in which all variables but one are controlled
for and correlation can be tested unequivocally. The
new experiment is explained in Section 5. Section 4
describes the quantitative model of argument struc-
ture required for the experiment.
4 Argument Structure and Frame
Uniformity
In this section, we define the concepts we require to
test our hypothesis quantitatively. First, we define
argument structure for our data in a corpus-driven
way. Then, we define the uniformity of a frame ac-
cording to its variance in argument structure.
4.1 An Empirical Model of Argument
Structure
Work in theoretical linguistics since at least Gru-
ber (1965) and Jackendoff (1972) has attempted to
account for the regularities in the syntactic reali-
sation of semantic arguments. Models for role as-
signment also rely on these regularities, as can be
seen from the kind of features used for this task (see
Section 3.2), which are either syntactic or lexical.
Thus, current models for automatic role labelling
rely on the regularities at the syntax-semantics in-
terface. Unlike theoretical work, however, they do
not explicitly represent these regularities, but extract
statistical properties about them from data.
The model of argument structure we develop in
this section retains the central idea of linking theory,
namely to model argument structure symbolically,
but deviates in two ways from traditional work in
order to bridge the gap to statistical approaches: (1),
in order to emulate the situation of the learners, we
use only the data available from the FrameNet cor-
pus; this excludes e.g. the use of more detailed lexi-
cal information about the predicates. (2), to be able
to characterise not only the possibility, but also the
probability of linking patterns, we take frequency
information into account.
Our definition proceeds in three steps. First, we
define the concept of a pattern, then we define the
argument structure of a predicate, and finally the ar-
gument structure of a frame.
Patterns. A pattern encodes the argument struc-
ture information present in one annotated corpus
sentence. It is an unordered set of pairs of seman-
tic role and syntactic function, corresponding to all
roles occurring in the sentences and their realisa-
tions. The syntactic functions used in the FrameNet
corpus are as follows5: COMP (complement), EXT
(subject in a broad sense, which includes control-
ling subjects), OBJ (object), MOD (modifier), GEN
(genitive modifier, as ?John? in John?s hat). For ex-
ample, Sentence (1-a) gives rise to the pattern

Impactee  EXT 

Impactor  COMP 
	
which states that the Impactee is realised as subject
and the Impactor as complement.
Argument Structure for Predicates and Frames.
For each verb, we collect the set of all patterns in
the annotated sentences. The argument structure of
a verb is then a vector  , whose dimensionality is the
number of patterns found for the frame. Each cell 
is filled with the frequency with which pattern  oc-
curs for the predicate, so that the vector mirrors the
distribution of the occurrences of the verb over the
possible patterns. Finally, the set of all vectors for
the predicates in a frame is a model for the argument
structure of the frame.
The intuition behind this formalisation is that two
verbs which realise their arguments alike will show
a similar distribution of patterns, and conversely, if
they differ in their linking, these differences will be
mirrored in different pattern distributions.
Example. If we only had the three sentences in (1)
for the IMPACT corpus, the three occurring patterns
would be {(Impactee, EXT), (Impactor, COMP)},
{(Impactor, EXT), (Result, COMP)}, and {(Im-
pactors, EXT), (Place, MOD)}. The argument struc-
ture of the frame would be



 
 



 

 



 
 

Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 297?305,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Robust Machine Translation Evaluation with Entailment Features?
Sebastian Pado?
Stuttgart University
pado@ims.uni-stuttgart.de
Michel Galley, Dan Jurafsky, Chris Manning
Stanford University
{mgalley,jurafsky,manning}@stanford.edu
Abstract
Existing evaluation metrics for machine translation
lack crucial robustness: their correlations with hu-
man quality judgments vary considerably across lan-
guages and genres. We believe that the main reason
is their inability to properly capture meaning: A good
translation candidate means the same thing as the
reference translation, regardless of formulation. We
propose a metric that evaluates MT output based on
a rich set of features motivated by textual entailment,
such as lexical-semantic (in-)compatibility and ar-
gument structure overlap. We compare this metric
against a combination metric of four state-of-the-
art scores (BLEU, NIST, TER, and METEOR) in
two different settings. The combination metric out-
performs the individual scores, but is bested by the
entailment-based metric. Combining the entailment
and traditional features yields further improvements.
1 Introduction
Constant evaluation is vital to the progress of ma-
chine translation (MT). Since human evaluation is
costly and difficult to do reliably, a major focus of
research has been on automatic measures of MT
quality, pioneered by BLEU (Papineni et al, 2002)
and NIST (Doddington, 2002). BLEU and NIST
measure MT quality by using the strong correla-
tion between human judgments and the degree of
n-gram overlap between a system hypothesis trans-
lation and one or more reference translations. The
resulting scores are cheap and objective.
However, studies such as Callison-Burch et al
(2006) have identified a number of problems with
BLEU and related n-gram-based scores: (1) BLEU-
like metrics are unreliable at the level of individual
sentences due to data sparsity; (2) BLEU metrics
can be ?gamed? by permuting word order; (3) for
some corpora and languages, the correlation to hu-
man ratings is very low even at the system level;
(4) scores are biased towards statistical MT; (5) the
quality gap between MT and human translations is
not reflected in equally large BLEU differences.
?This paper is based on work funded by the Defense Ad-
vanced Research Projects Agency through IBM. The content
does not necessarily reflect the views of the U.S. Government,
and no official endorsement should be inferred.
This is problematic, but not surprising: The met-
rics treat any divergence from the reference as a
negative, while (computational) linguistics has long
dealt with linguistic variation that preserves the
meaning, usually called paraphrase, such as:
(1) HYP: However, this was declared terrorism
by observers and witnesses.
REF: Nevertheless, commentators as well as
eyewitnesses are terming it terrorism.
A number of metrics have been designed to account
for paraphrase, either by making the matching more
intelligent (TER, Snover et al (2006)), or by using
linguistic evidence, mostly lexical similarity (ME-
TEOR, Banerjee and Lavie (2005); MaxSim, Chan
and Ng (2008)), or syntactic overlap (Owczarzak et
al. (2008); Liu and Gildea (2005)). Unfortunately,
each metrics tend to concentrate on one particu-
lar type of linguistic information, none of which
always correlates well with human judgments.
Our paper proposes two strategies. We first ex-
plore the combination of traditional scores into a
more robust ensemble metric with linear regression.
Our second, more fundamental, strategy replaces
the use of loose surrogates of translation quality
with a model that attempts to comprehensively as-
sess meaning equivalence between references and
MT hypotheses. We operationalize meaning equiv-
alence by bidirectional textual entailment (RTE,
Dagan et al (2005)), and thus predict the qual-
ity of MT hypotheses with a rich RTE feature set.
The entailment-based model goes beyond existing
word-level ?semantic? metrics such as METEOR
by integrating phrasal and compositional aspects
of meaning equivalence, such as multiword para-
phrases, (in-)correct argument and modification
relations, and (dis-)allowed phrase reorderings. We
demonstrate that the resulting metric beats both in-
dividual and combined traditional MT metrics. The
complementary features of both metric types can
be combined into a joint, superior metric.
297
HYP: Three aid workers were kidnapped.
REF: Three aid workers were kidnapped by pirates.
no entailment entailment
HYP: The virus did not infect anybody.
REF: No one was infected by the virus.
entailment
entailment
Figure 1: Entailment status between an MT system
hypothesis and a reference translation for equiva-
lent (top) and non-equivalent (bottom) translations.
2 Regression-based MT Quality Prediction
Current MTmetrics tend to focus on a single dimen-
sion of linguistic information. Since the importance
of these dimensions tends not to be stable across
language pairs, genres, and systems, performance
of these metrics varies substantially. A simple strat-
egy to overcome this problem could be to combine
the judgments of different metrics. For example,
Paul et al (2007) train binary classifiers on a fea-
ture set formed by a number of MT metrics. We
follow a similar idea, but use a regularized linear
regression to directly predict human ratings.
Feature combination via regression is a super-
vised approach that requires labeled data. As we
show in Section 5, this data is available, and the
resulting model generalizes well from relatively
small amounts of training data.
3 Textual Entailment vs. MT Evaluation
Our novel approach to MT evaluation exploits the
similarity between MT evaluation and textual en-
tailment (TE). TE was introduced by Dagan et
al. (2005) as a concept that corresponds more
closely to ?common sense? reasoning patterns than
classical, strict logical entailment. Textual entail-
ment is defined informally as a relation between
two natural language sentences (a premise P and
a hypothesis H) that holds if ?a human reading P
would infer that H is most likely true?. Knowledge
about entailment is beneficial for NLP tasks such as
Question Answering (Harabagiu and Hickl, 2006).
The relation between textual entailment and MT
evaluation is shown in Figure 1. Perfect MT output
and the reference translation entail each other (top).
Translation problems that impact semantic equiv-
alence, e.g., deletion or addition of material, can
break entailment in one or both directions (bottom).
On the modelling level, there is common ground
between RTE and MT evaluation: Both have to
distinguish between valid and invalid variation to
determine whether two texts convey the same in-
formation or not. For example, to recognize the
bidirectional entailment in Ex. (1), RTE must ac-
count for the following reformulations: synonymy
(However/Nevertheless), more general semantic
relatedness (observers/commentators), phrasal re-
placements (and/as well as), and an active/passive
alternation that implies structural change (is de-
clared/are terming). This leads us to our main hy-
pothesis: RTE features are designed to distinguish
meaning-preserving variation from true divergence
and are thus also good predictors in MT evaluation.
However, while the original RTE task is asymmet-
ric, MT evaluation needs to determine meaning
equivalence, which is a symmetric relation. We do
this by checking for entailment in both directions
(see Figure 1). Operationally, this ensures we detect
translations which either delete or insert material.
Clearly, there are also differences between the
two tasks. An important one is that RTE assumes
the well-formedness of the two sentences. This is
not generally true in MT, and could lead to de-
graded linguistic analyses. However, entailment
relations are more sensitive to the contribution of
individual words (MacCartney andManning, 2008).
In Example 2, the modal modifiers break the entail-
ment between two otherwise identical sentences:
(2) HYP: Peter is certainly from Lincolnshire.
REF: Peter is possibly from Lincolnshire.
This means that the prediction of TE hinges on
correct semantic analysis and is sensitive to mis-
analyses. In contrast, human MT judgments behave
robustly. Translations that involve individual errors,
like (2), are judged lower than perfect ones, but
usually not crucially so, since most aspects are
still rendered correctly. We thus expect even noisy
RTE features to be predictive for translation quality.
This allows us to use an off-the-shelf RTE system
to obtain features, and to combine them using a
regression model as described in Section 2.
3.1 The Stanford Entailment Recognizer
The Stanford Entailment Recognizer (MacCartney
et al, 2006) is a stochastic model that computes
match and mismatch features for each premise-
hypothesis pair. The three stages of the system
are shown in Figure 2. The system first uses a
robust broad-coverage PCFG parser and a deter-
ministic constituent-dependency converter to con-
struct linguistic representations of the premise and
298
Stage 3: Feature computation (w/ numbers of features)
Premise: India buys 1,000 tanks.
Hypothesis: India acquires arms.
Stage 1: Linguistic analysis
India
buys
1,000 tanks
subj dobj
India
acquires
arms
subj dobj
Stage 2: Alignment
India
buys
1,000 tanks
subj dobj
India
acquires
arms
subj dobj
0.9
1.0
0.7
Alignment (8):
Semantic 
compatibility 
(34): 
Insertions and
deletions (20):
Preservation of 
reference (16):
Structural 
alignment (28):
Overall alignment quality
Modality, Factivity, Polarity, 
Quantification, Lexical-semantic 
relatedness, Tense
Felicity of appositions and adjuncts, 
Types of unaligned material 
Locations, Dates, Entities
Alignment of main verbs and 
syntactically prominent words, 
Argument structure (mis-)matches
Figure 2: The Stanford Entailment Recognizer
the hypothesis. The results are typed dependency
graphs that contain a node for each word and la-
beled edges representing the grammatical relations
between words. Named entities are identified, and
contiguous collocations grouped. Next, it identifies
the highest-scoring alignment from each node in
the hypothesis graph to a single node in the premise
graph, or to null. It uses a locally decomposable
scoring function: The score of an alignment is the
sum of the local word and edge alignment scores.
The computation of these scores make extensive
use of about ten lexical similarity resources, in-
cluding WordNet, InfoMap, and Dekang Lin?s the-
saurus. Since the search space is exponential in
the hypothesis length, the system uses stochastic
(rather than exhaustive) search based on Gibbs sam-
pling (see de Marneffe et al (2007)).
Entailment features. In the third stage, the sys-
tem produces roughly 100 features for each aligned
premise-hypothesis pair. A small number of them
are real-valued (mostly quality scores), but most
are binary implementations of small linguistic the-
ories whose activation indicates syntactic and se-
mantic (mis-)matches of different types. Figure 2
groups the features into five classes. Alignment
features measure the overall quality of the align-
ment as given by the lexical resources. Semantic
compatibility features check to what extent the
aligned material has the same meaning and pre-
serves semantic dimensions such as modality and
factivity, taking a limited amount of context into
account. Insertion/deletion features explicitly ad-
dress material that remains unaligned and assess its
felicity. Reference features ascertain that the two
sentences actually refer to the same events and par-
ticipants. Finally, structural features add structural
considerations by ensuring that argument structure
is preserved in the translation. See MacCartney et
al. (2006) for details on the features, and Sections
5 and 6 for examples of feature firings.
Efficiency considerations. The use of deep lin-
guistic analysis makes our entailment-based met-
ric considerably more heavyweight than traditional
MT metrics. The average total runtime per sentence
pair is 5 seconds on an AMD 2.6GHz Opteron core
? efficient enough to perform regular evaluations on
development and test sets. We are currently investi-
gating caching and optimizations that will enable
the use of our metric for MT parameter tuning in a
Minimum Error Rate Training setup (Och, 2003).
4 Experimental Evaluation
4.1 Experiments
Traditionally, human ratings for MT quality have
been collected in the form of absolute scores on a
five- or seven-point Likert scale, but low reliabil-
ity numbers for this type of annotation have raised
concerns (Callison-Burch et al, 2008). An alter-
native that has been adopted by the yearly WMT
evaluation shared tasks since 2008 is the collection
of pairwise preference judgments between pairs of
MT hypotheses which can be elicited (somewhat)
more reliably. We demonstrate that our approach
works well for both types of annotation and differ-
ent corpora. Experiment 1 models absolute scores
on Asian newswire, and Experiment 2 pairwise
preferences on European speech and news data.
4.2 Evaluation
We evaluate the output of our models both on the
sentence and on the system level. At the sentence
level, we can correlate predictions in Experiment 1
directly with human judgments with Spearman?s ? ,
299
a non-parametric rank correlation coefficient appro-
priate for non-normally distributed data. In Experi-
ment 2, the predictions cannot be pooled between
sentences. Instead of correlation, we compute ?con-
sistency? (i.e., accuracy) with human preferences.
System-level predictions are computed in both
experiments from sentence-level predictions, as the
ratio of sentences for which each system provided
the best translation (Callison-Burch et al, 2008).
We extend this procedure slightly because real-
valued predictions cannot predict ties, while human
raters decide for a significant portion of sentences
(as much as 80% in absolute score annotation) to
?tie? two systems for first place. To simulate this
behavior, we compute ?tie-aware? predictions as
the percentage of sentences where the system?s hy-
pothesis was assigned a score better or at most ?
worse than the best system. ? is set to match the
frequency of ties in the training data.
Finally, the predictions are again correlated with
human judgments using Spearman?s ? . ?Tie aware-
ness? makes a considerable practical difference,
improving correlation figures by 5?10 points.1
4.3 Baseline Metrics
We consider four baselines. They are small regres-
sion models as described in Section 2 over com-
ponent scores of four widely used MT metrics. To
alleviate possible nonlinearity, we add all features
in linear and log space. Each baselines carries the
name of the underlying metric plus the suffix -R.2
BLEUR includes the following 18 sentence-level
scores: BLEU-n and n-gram precision scores
(1? n? 4); BLEU brevity penalty (BP); BLEU
score divided by BP. To counteract BLEU?s brittle-
ness at the sentence level, we also smooth BLEU-n
and n-gram precision as in Lin and Och (2004).
NISTR consists of 16 features. NIST-n scores
(1? n? 10) and information-weighted n-gram
precision scores (1? n? 4); NIST brevity penalty
(BP); and NIST score divided by BP.
1Due to space constraints, we only show results for ?tie-
aware? predictions. See Pado? et al (2009) for a discussion.
2The regression models can simulate the behaviour of each
component by setting the weights appropriately, but are strictly
more powerful. A possible danger is that the parameters over-
fit on the training set. We therefore verified that the three
non-trivial ?baseline? regression models indeed confer a bene-
fit over the default component combination scores: BLEU-1
(which outperformed BLEU-4 in the MetricsMATR 2008 eval-
uation), NIST-4, and TER (with all costs set to 1). We found
higher robustness and improved correlations for the regression
models. An exception is BLEU-1 and NIST-4 on Expt. 1 (Ar,
Ch), which perform 0.5?1 point better at the sentence level.
TERR includes 50 features. We start with the
standard TER score and the number of each of the
four edit operations. Since the default uniform cost
does not always correlate well with human judg-
ment, we duplicate these features for 9 non-uniform
edit costs. We find it effective to set insertion cost
close to 0, as a way of enabling surface variation,
and indeed the new TERp metric uses a similarly
low default insertion cost (Snover et al, 2009).
METEORR consists of METEOR v0.7.
4.4 Combination Metrics
The following three regression models implement
the methods discussed in Sections 2 and 3.
MTR combines the 85 features of the four base-
line models. It uses no entailment features.
RTER uses the 70 entailment features described
in Section 3.1, but no MTR features.
MT+RTER uses all MTR and RTER features,
combining matching and entailment evidence.3
5 Expt. 1: Predicting Absolute Scores
Data. Our first experiment evaluates the models
we have proposed on a corpus with traditional an-
notation on a seven-point scale, namely the NIST
OpenMT 2008 corpus.4 The corpus contains trans-
lations of newswire text into English from three
source languages (Arabic (Ar), Chinese (Ch), Urdu
(Ur)). Each language consists of 1500?2800 sen-
tence pairs produced by 7?15 MT systems.
We use a ?round robin? scheme. We optimize
the weights of our regression models on two lan-
guages and then predict the human scores on the
third language. This gauges performance of our
models when training and test data come from the
same genre, but from different languages, which
we believe to be a setup of practical interest. For
each test set, we set the system-level tie parameter
? so that the relative frequency of ties was equal
to the training set (65?80%). Hypotheses generally
had to receive scores within 0.3?0.5 points to tie.
Results. Table 1 shows the results. We first con-
centrate on the upper half (sentence-level results).
The predictions of all models correlate highly sig-
nificantly with human judgments, but we still see
robustness issues for the individual MT metrics.
3Software for RTER and MT+RTER is available from
http://nlp.stanford.edu/software/mteval.shtml.
4Available from http://www.nist.gov.
300
Evaluation Data Metrics
train test BLEUR METEORR NISTR TERR MTR RTER MT+RTER
Sentence-level
Ar+Ch Ur 49.9 49.1 49.5 50.1 50.1 54.5 55.6
Ar+Ur Ch 53.9 61.1 53.1 50.3 57.3 58.0 62.7
Ch+Ur Ar 52.5 60.1 50.4 54.5 55.2 59.9 61.1
System-level
Ar+Ch Ur 73.9 68.4 50.0 90.0? 92.7? 77.4? 81.0?
Ar+Ur Ch 38.5 44.3 40.0 59.0? 51.8? 47.7 57.3?
Ch+Ur Ar 59.7? 86.3? 61.9? 42.1 48.1 59.7? 61.7?
Table 1: Expt. 1: Spearman?s ? for correlation between human absolute scores and model predictions on
NIST OpenMT 2008. Sentence level: All correlations are highly significant. System level: ?: p<0.05.
METEORR achieves the best correlation for Chi-
nese and Arabic, but fails for Urdu, apparently the
most difficult language. TERR shows the best result
for Urdu, but does worse than METEORR for Ara-
bic and even worse than BLEUR for Chinese. The
MTR combination metric alleviates this problem to
some extent by improving the ?worst-case? perfor-
mance on Urdu to the level of the best individual
metric. The entailment-based RTER system outper-
forms MTR on each language. It particularly im-
proves on MTR?s correlation on Urdu. Even though
METEORR still does somewhat better than MTR
and RTER, we consider this an important confirma-
tion for the usefulness of entailment features in MT
evaluation, and for their robustness.5
In addition, the combined model MT+RTER is
best for all three languages, outperforming METE-
ORR for each language pair. It performs consid-
erably better than either MTR or RTER. This is a
second result: the types of evidence provided by
MTR and RTER appear to be complementary and
can be combined into a superior model.
On the system level (bottom half of Table 1),
there is high variance due to the small number of
predictions per language, and many predictions are
not significantly correlated with human judgments.
BLEUR, METEORR, and NISTR significantly pre-
dict one language each (all Arabic); TERR, MTR,
and RTER predict two languages. MT+RTER is
the only model that shows significance for all three
languages. This result supports the conclusions we
have drawn from the sentence-level analysis.
Further analysis. We decided to conduct a thor-
ough analysis of the Urdu dataset, the most difficult
source language for all metrics. We start with a fea-
5These results are substantially better than the performance
our metric showed in the MetricsMATR 2008 challenge. Be-
yond general enhancement of our model, we attribute the less
good MetricsMATR 2008 results to an infelicitous choice
of training data for the submission, coupled with the large
amount of ASR output in the test data, whose disfluencies
represent an additional layer of problems for deep approaches.
20 40 60 80 1000.4
2
0.46
0.50
0.54
% Training data MT08 Ar+Ch
Spe
arm
an's
 rho
 on 
MT 
08 U
r
l
l
l
l
l l l l l
l l l
l l l l l l
l
l
MetricsMt?RteRRteRMtRMetR
Figure 3: Experiment 1: Learning curve (Urdu).
ture ablation study. Removing any feature group
from RTER results in drops in correlation of at least
three points. The largest drops occur for the struc-
tural (? = ?11) and insertion/deletion (? = ?8)
features. Thus, all feature groups appear to con-
tribute to the good correlation of RTER. However,
there are big differences in the generality of the
feature groups: in isolation, the insertion/deletion
features achieve almost no correlation, and need to
be complemented by more robust features.
Next, we analyze the role of training data. Fig-
ure 3 shows Urdu average correlations for models
trained on increasing subsets of the training data
(10% increments, 10 random draws per step; Ar
and Ch show similar patterns.) METEORR does not
improve, which is to be expected given the model
definition. RTER has a rather flat learning curve
that climbs to within 2 points of the final correla-
tion value for 20% of the training set (about 400
sentence pairs). Apparently, entailment features do
not require a large training set, presumably because
most features of RTER are binary. The remaining
two models, MTR and MT+RTER, show clearer
benefit from more data. With 20% of the total data,
they climb to within 5 points of their final perfor-
mance, but keep slowly improving further.
301
REF: I shall face that fact today.
HYP: Today I will face this reality.
[doc WL-34-174270-7483871, sent 4, system1]
Gold: 6
METEORR: 2.8
RTER: 6.1
? Only function words unaligned (will, this)
? Alignment fact/reality: hypernymy is ok
in upward monotone context
REF: What does BBC?s Haroon Rasheed say after a visit to Lal Masjid Jamia
Hafsa complex? There are no underground tunnels in Lal Masjid or Jamia
Hafsa. The presence of the foreigners could not be confirmed as well. What
became of the extremists like Abuzar?
HYP: BBC Haroon Rasheed Lal Masjid, Jamia Hafsa after his visit to Auob
Medical Complex says Lal Masjid and seminary in under a land mine, not
also been confirmed the presence of foreigners could not be, such as Abu by
the extremist? [doc WL-12-174261-7457007, sent 2, system2]
Gold: 1
METEORR: 4.5
RTER: 1.2
? Hypothesis root node unaligned
? Missing alignments for subjects
? Important entities in hypothesis cannot be
aligned
? Reference, hypothesis differ in polarity
Table 2: Expt. 1: Reference translations and MT output (Urdu). Scores are out of 7 (higher is better).
Finally, we provide a qualitative comparison of
RTER?s performance against the best baseline met-
ric, METEORR. Since the computation of RTER
takes considerably more resources than METEORR,
it is interesting to compare the predictions of RTER
against METEORR. Table 2 shows two classes of
examples with apparent improvements.
The first example (top) shows a good translation
that is erroneously assigned a low score by ME-
TEORR because (a) it cannot align fact and reality
(METEORR aligns only synonyms) and (b) it pun-
ishes the change of word order through its ?penalty?
term. RTER correctly assigns a high score. The
features show that this prediction results from two
semantic judgments. The first is that the lack of
alignments for two function words is unproblem-
atic; the second is that the alignment between fact
and reality, which is established on the basis of
WordNet similarity, is indeed licensed in the cur-
rent context. More generally, we find that RTER
is able to account for more valid variation in good
translations because (a) it judges the validity of
alignments dependent on context; (b) it incorpo-
rates more semantic similarities; and (c) it weighs
mismatches according to the word?s status.
The second example (bottom) shows a very bad
translation that is scored highly by METEORR,
since almost all of the reference words appear either
literally or as synonyms in the hypothesis (marked
in italics). In combination with METEORR?s con-
centration on recall, this is sufficient to yield a
moderately high score. In the case of RTER, a num-
ber of mismatch features have fired. They indicate
problems with the structural well-formedness of
the MT output as well as semantic incompatibil-
ity between hypothesis and reference (argument
structure and reference mismatches).
6 Expt. 2: Predicting Pairwise Preferences
In this experiment, we predict human pairwise pref-
erence judgments (cf. Section 4). We reuse the
linear regression framework from Section 2 and
predict pairwise preferences by predicting two ab-
solute scores (as before) and comparing them.6
Data. This experiment uses the 2006?2008 cor-
pora of the Workshop on Statistical Machine
Translation (WMT).7 It consists of data from EU-
ROPARL (Koehn, 2005) and various news com-
mentaries, with five source languages (French, Ger-
man, Spanish, Czech, and Hungarian). As training
set, we use the portions of WMT 2006 and 2007
that are annotated with absolute scores on a five-
point scale (around 14,000 sentences produced by
40 systems). The test set is formed by the WMT
2008 relative rank annotation task. As in Experi-
ment 1, we set ? so that the incidence of ties in the
training and test set is equal (60%).
Results. Table 4 shows the results. The left result
column shows consistency, i.e., the accuracy on
human pairwise preference judgments.8 The pat-
tern of results matches our observations in Expt. 1:
Among individual metrics, METEORR and TERR
do better than BLEUR and NISTR. MTR and RTER
outperform individual metrics. The best result by a
wide margin, 52.5%, is shown by MT+RTER.
6We also experimented with a logistic regression model
that predicts binary preferences directly. Its performance is
comparable; see Pado? et al (2009) for details.
7Available from http://www.statmt.org/.
8The random baseline is not 50%, but, according to our
experiments, 39.8%. This has two reasons: (1) the judgments
include contradictory and tie annotations that cannot be pre-
dicted correctly (raw inter-annotator agreement on WMT 2008
was 58%); (2) metrics have to submit a total order over the
translations for each sentence, which introduces transitivity
constraints. For details, see Callison-Burch et al (2008).
302
Segment MTR RTER MT+RTER Gold
REF: Scottish NHS boards need to improve criminal records checks for
employees outside Europe, a watchdog has said.
HYP: The Scottish health ministry should improve the controls on extra-
community employees to check whether they have criminal precedents,
said the monitoring committee. [1357, lium-systran]
Rank: 3 Rank: 1 Rank: 2 Rank: 1
REF: Arguments, bullying and fights between the pupils have extended
to the relations between their parents.
HYP: Disputes, chicane and fights between the pupils transposed in
relations between the parents. [686, rbmt4]
Rank: 5 Rank: 2 Rank: 4 Rank: 5
Table 3: Expt. 2: Reference translations and MT output (French). Ranks are out of five (smaller is better).
Feature set Consis-
tency (%)
System-level
correlation (?)
BLEUR 49.6 69.3
METEORR 51.1 72.6
NISTR 50.2 70.4
TERR 51.2 72.5
MTR 51.5 73.1
RTER 51.8 78.3
MT+RTER 52.5 75.8
WMT 08 (worst) 44 37
WMT 08 (best) 56 83
Table 4: Expt. 2: Prediction of pairwise preferences
on the WMT 2008 dataset.
The right column shows Spearman?s ? for the
correlation between human judgments and tie-
aware system-level predictions. All metrics predict
system scores highly significantly, partly due to the
larger number of systems compared (87 systems).
Again, we see better results for METEORR and
TERR than for BLEUR and NISTR, and the indi-
vidual metrics do worse than the combination mod-
els. Among the latter, the order is: MTR (worst),
MT+RTER, and RTER (best at 78.3).
WMT 2009. We submitted the Expt. 2 RTER
metric to the WMT 2009 shared MT evaluation
task (Pado? et al, 2009). The results provide fur-
ther validation for our results and our general ap-
proach. At the system level, RTER made third place
(avg. correlation ? = 0.79), trailing the two top met-
rics closely (? = 0.80, ? = 0.83) and making the
best predictions for Hungarian. It also obtained the
second-best consistency score (53%, best: 54%).
Metric comparison. The pairwise preference an-
notation of WMT 2008 gives us the opportunity to
compare the MTR and RTER models by comput-
ing consistency separately on the ?top? (highest-
ranked) and ?bottom? (lowest-ranked) hypotheses
for each reference. RTER performs about 1.5 per-
cent better on the top than on the bottom hypothe-
ses. The MTR model shows the inverse behavior,
performing 2 percent worse on the top hypothe-
ses. This matches well with our intuitions: We see
some noise-induced degradation for the entailment
features, but not much. In contrast, surface-based
features are better at detecting bad translations than
at discriminating among good ones.
Table 3 further illustrates the difference between
the top models on two example sentences. In the top
example, RTER makes a more accurate prediction
than MTR. The human rater?s favorite translation
deviates considerably from the reference in lexi-
cal choice, syntactic structure, and word order, for
which it is punished by MTR (rank 3/5). In contrast,
RTER determines correctly that the propositional
content of the reference is almost completely pre-
served (rank 1). In the bottom example, RTER?s
prediction is less accurate. This sentence was rated
as bad by the judge, presumably due to the inap-
propriate main verb translation. Together with the
subject mismatch, MTR correctly predicts a low
score (rank 5/5). RTER?s attention to semantic over-
lap leads to an incorrect high score (rank 2/5).
Feature Weights. Finally, we make two observa-
tions about feature weights in the RTER model.
First, the model has learned high weights not
only for the overall alignment score (which be-
haves most similarly to traditional metrics), but also
for a number of binary syntacto-semantic match
and mismatch features. This confirms that these
features systematically confer the benefit we have
shown anecdotally in Table 2. Features with a con-
sistently negative effect include dropping adjuncts,
unaligned or poorly aligned root nodes, incompat-
ible modality between the main clauses, person
and location mismatches (as opposed to general
mismatches) and wrongly handled passives. Con-
303
versely, higher scores result from factors such as
high alignment score, matching embeddings under
factive verbs, and matches between appositions.
Second, good MT evaluation feature weights are
not good weights for RTE. Some differences, par-
ticularly for structural features, are caused by the
low grammaticality of MT data. For example, the
feature that fires for mismatches between depen-
dents of predicates is unreliable on the WMT data.
Other differences do reflect more fundamental dif-
ferences between the two tasks (cf. Section 3). For
example, RTE puts high weights onto quantifier
and polarity features, both of which have the poten-
tial of influencing entailment decisions, but are (at
least currently) unimportant for MT evaluation.
7 Related Work
Researchers have exploited various resources to en-
able the matching between words or n-grams that
are semantically close but not identical. Banerjee
and Lavie (2005) and Chan and Ng (2008) use
WordNet, and Zhou et al (2006) and Kauchak
and Barzilay (2006) exploit large collections of
automatically-extracted paraphrases. These ap-
proaches reduce the risk that a good translation
is rated poorly due to lexical deviation, but do not
address the problem that a translation may contain
many long matches while lacking coherence and
grammaticality (cf. the bottom example in Table 2).
Thus, incorporation of syntactic knowledge has
been the focus of another line of research. Amigo?
et al (2006) use the degree of overlap between the
dependency trees of reference and hypothesis as a
predictor of translation quality. Similar ideas have
been applied by Owczarzak et al (2008) to LFG
parses, and by Liu and Gildea (2005) to features
derived from phrase-structure tress. This approach
has also been successful for the related task of
summarization evaluation (Hovy et al, 2006).
The most comparable work to ours is Gime?nez
and Ma?rquez (2008). Our results agree on the cru-
cial point that the use of a wide range of linguistic
knowledge in MT evaluation is desirable and im-
portant. However, Gime?nez and Ma?rquez advocate
the use of a bottom-up development process that
builds on a set of ?heterogeneous?, independent
metrics each of which measures overlap with re-
spect to one linguistic level. In contrast, our aim
is to provide a ?top-down?, integrated motivation
for the features we integrate through the textual
entailment recognition paradigm.
8 Conclusion and Outlook
In this paper, we have explored a strategy for the
evaluation of MT output that aims at comprehen-
sively assessing the meaning equivalence between
reference and hypothesis. To do so, we exploit the
common ground between MT evaluation and the
Recognition of Textual Entailment (RTE), both of
which have to distinguish valid from invalid lin-
guistic variation. Conceputalizing MT evaluation
as an entailment problem motivates the use of a
rich feature set that covers, unlike almost all earlier
metrics, a wide range of linguistic levels, including
lexical, syntactic, and compositional phenomena.
We have used an off-the-shelf RTE system to
compute these features, and demonstrated that a
regression model over these features can outper-
form an ensemble of traditional MT metrics in two
experiments on different datasets. Even though the
features build on deep linguistic analysis, they are
robust enough to be used in a real-world setting, at
least on written text. A limited amount of training
data is sufficient, and the weights generalize well.
Our data analysis has confirmed that each of the
feature groups contributes to the overall success of
the RTE metric, and that its gains come from its
better success at abstracting away from valid vari-
ation (such as word order or lexical substitution),
while still detecting major semantic divergences.
We have also clarified the relationship between MT
evaluation and textual entailment: The majority of
phenomena (but not all) that are relevant for RTE
are also informative for MT evaluation.
The focus of this study was on the use of an ex-
isting RTE infrastructure for MT evaluation. Future
work will have to assess the effectiveness of individ-
ual features and investigate ways to customize RTE
systems for the MT evaluation task. An interesting
aspect that we could not follow up on in this paper
is that entailment features are linguistically inter-
pretable (cf. Fig. 2) and may find use in uncovering
systematic shortcomings of MT systems.
A limitation of our current metric is that it is
language-dependent and relies on NLP tools in
the target language that are still unavailable for
many languages, such as reliable parsers. To some
extent, of course, this problem holds as well for
state-of-the-art MT systems. Nevertheless, it must
be an important focus of future research to develop
robust meaning-based metrics for other languages
that can cash in the promise that we have shown
for evaluating translation into English.
304
References
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and
Llu??s Ma`rquez. 2006. MT Evaluation: Human-
like vs. human acceptable. In Proceedings of COL-
ING/ACL 2006, pages 17?24, Sydney, Australia.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures, pages 65?72, Ann Ar-
bor, MI.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU
in machine translation research. In Proceedings of
EACL, pages 249?256, Trento, Italy.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the ACL Workshop on Statistical Ma-
chine Translation, pages 70?106, Columbus, OH.
Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM: A
maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55?62, Columbus, Ohio, June.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entailment
challenge. In Proceedings of the PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Southampton, UK.
Marie-Catherine de Marneffe, Trond Grenager, Bill
MacCartney, Daniel Cer, Daniel Ramage, Chloe?
Kiddon, and Christopher D. Manning. 2007. Align-
ing semantic graphs for textual inference and ma-
chine reading. In Proceedings of the AAAI Spring
Symposium, Stanford, CA.
George Doddington. 2002. Automatic evaluation of
machine translation quality using n-gram cooccur-
rence statistics. In Proceedings of HLT, pages 128?
132, San Diego, CA.
Jesu?s Gime?nez and Llu??s Ma?rquez. 2008. Het-
erogeneous automatic MT evaluation through non-
parametric metric combinations. In Proceedings of
IJCNLP, pages 319?326, Hyderabad, India.
Sanda Harabagiu and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain ques-
tion answering. In Proceedings of ACL, pages 905?
912, Sydney, Australia.
Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Junichi
Fukumoto. 2006. Automated summarization evalu-
ation with basic elements. In Proceedings of LREC,
Genoa, Italy.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for automatic evaluation. In Proceedings of HLT-
NAACL, pages 455?462.
Phillip Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
MT Summit X, Phuket, Thailand.
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE:
a method for evaluating automatic evaluation met-
rics for machine translation. In Proceedings of COL-
ING, pages 501?507, Geneva, Switzerland.
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceed-
ings of the ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures, pages 25?32, Ann Arbor, MI.
Bill MacCartney and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in
natural language inference. In Proceedings of COL-
ING, pages 521?528, Manchester, UK.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of
valid textual entailments. In Proceedings of NAACL,
pages 41?48, New York City, NY.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167, Sapporo, Japan.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2008. Evaluating machine translation with
LFG dependencies. Machine Translation, 21(2):95?
119.
Sebastian Pado?, Michel Galley, Dan Jurafsky, and
Christopher D. Manning. 2009. Textual entailment
features for machine translation evaluation. In Pro-
ceedings of the EACL Workshop on Statistical Ma-
chine Translation, pages 37?41, Athens, Greece.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318, Philadelphia, PA.
Michael Paul, Andrew Finch, and Eiichiro Sumita.
2007. Reducing human assessment of machine
translation quality to binary classifiers. In Proceed-
ings of TMI, pages 154?162, Sko?vde, Sweden.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proceedings of AMTA, pages 223?231, Cam-
bridge, MA.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with
a tunable MT metric. In Proceedings of the EACL
Workshop on Statistical Machine Translation, pages
259?268, Athens, Greece.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006.
Re-evaluating machine translation results with para-
phrase support. In Proceedings of EMNLP, pages
77?84, Sydney, Australia.
305
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 1?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple Languages
Jan Hajic?? Massimiliano Ciaramita? Richard Johansson? Daisuke Kawahara?
Maria Anto`nia Mart???? Llu??s Ma`rquez?? Adam Meyers?? Joakim Nivre?? Sebastian Pado???
Jan ?Ste?pa?nek? Pavel Stran?a?k? Mihai Surdeanu?? Nianwen Xue?? Yi Zhang??
?: Charles University in Prague, {hajic,stepanek,stranak}@ufal.mff.cuni.cz
?: Google Inc., massi@google.com
?: University of Trento, johansson@disi.unitn.it
?: National Institute of Information and Communications Technology, dk@nict.go.jp
??: University of Barcelona, amarti@ub.edu
??: Technical University of Catalonia, Barcelona, lluism@lsi.upc.edu
??: New York University, meyers@cs.nyu.edu
??: Uppsala University and Va?xjo? University, joakim.nivre@lingfil.uu.se
??: Stuttgart University, pado@ims.uni-stuttgart.de
??: Stanford University, mihais@stanford.edu
??: Brandeis University, xuen@brandeis.edu
??: Saarland University, yzhang@coli.uni-sb.de
Abstract
For the 11th straight year, the Conference
on Computational Natural Language Learn-
ing has been accompanied by a shared task
whose purpose is to promote natural language
processing applications and evaluate them in
a standard setting. In 2009, the shared task
was dedicated to the joint parsing of syntac-
tic and semantic dependencies in multiple lan-
guages. This shared task combines the shared
tasks of the previous five years under a unique
dependency-based formalism similar to the
2008 task. In this paper, we define the shared
task, describe how the data sets were created
and show their quantitative properties, report
the results and summarize the approaches of
the participating systems.
1 Introduction
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
launches a competitive, open ?Shared Task?. A
common (?shared?) task is defined and datasets are
provided for its participants. In 2004 and 2005, the
shared tasks were dedicated to semantic role label-
ing (SRL) in a monolingual setting (English). In
2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using corpora
from up to 13 languages. In 2008, the shared task
(Surdeanu et al, 2008) used a unified dependency-
based formalism, which modeled both syntactic de-
pendencies and semantic roles for English. The
CoNLL-2009 Shared Task has built on the 2008 re-
sults by providing data for six more languages (Cata-
lan, Chinese, Czech, German, Japanese and Span-
ish) in addition to the original English1. It has thus
naturally extended the path taken by the five most
recent CoNLL shared tasks.
As in 2008, the CoNLL-2009 shared task com-
bined dependency parsing and the task of identify-
ing and labeling semantic arguments of verbs (and
other parts of speech whenever available). Partici-
pants had to choose from two tasks:
? Joint task (syntactic dependency parsing and
semantic role labeling), or
? SRL-only task (syntactic dependency parses
have been provided by the organizers, using
state-of-the art parsers for the individual lan-
guages).
1There are some format changes and deviations from the
2008 task data specification; see Sect. 2.3
1
In contrast to the previous year, the evaluation data
indicated which words were to be dealt with (for the
SRL task). In other words, (predicate) disambigua-
tion was still part of the task, whereas the identi-
fication of argument-bearing words was not. This
decision was made to compensate for the significant
differences between languages and between the an-
notation schemes used.
The ?closed? and ?open? challenges have been
kept from last year as well; participants could have
chosen one or both. In the closed challenge, systems
had to be trained strictly with information contained
in the given training corpus; in the open challenge,
systems could have been developed making use of
any kind of external tools and resources.
This paper is organized as follows. Section 2 de-
fines the task, including the format of the data, the
evaluation metrics, and the two challenges. A sub-
stantial portion of the paper (Section 3) is devoted
to the description of the conversion and develop-
ment of the data sets in the additional languages.
Section 4 shows the main results of the submitted
systems in the Joint and SRL-only tasks. Section 5
summarizes the approaches implemented by partic-
ipants. Section 6 concludes the paper. In all sec-
tions, we will mention some of the differences be-
tween last year?s and this year?s tasks while keeping
the text self-contained whenever possible; for details
and observations on the English data, please refer to
the overview paper of the CoNLL-2008 Shared Task
(Surdeanu et al, 2008) and to the references men-
tioned in the sections describing the other languages.
2 Task Definition
In this section we provide the definition of the shared
task; after introducing the two challenges and the
two tasks the participants were to choose, we con-
tinue with the format of the shared task data, fol-
lowed by a description of the evaluation metrics
used.
For three of the languages (Czech, English and
German), out-of-domain data (OOD) have also been
prepared for the final evaluation, following the same
guidelines and formats.
2.1 Closed and Open Challenges
Similarly to the CoNLL-2005 and CoNLL-2008
shared tasks, this shared task evaluation is separated
into two challenges:
Closed Challenge The aim of this challenge was to
compare performance of the participating systems in
a fair environment. Systems had to be built strictly
with information contained in the given training cor-
pus, and tuned with the development section. In
addition, the lexical frame files (such as the Prop-
Bank and NomBank for English, the valency dictio-
nary PDT-Vallex for Czech etc.) were provided and
may have been used. These restrictions mean that
outside parsers (not trained by the participants? sys-
tems) could not be used. However, we did provide
the output of a single, state-of-the-art dependency
parser for each language so that participants could
build a SRL-only system (using the provided parses
as inputs) within the closed challenge (as opposed to
the 2008 shared task).
Open Challenge Systems could have been devel-
oped making use of any kind of external tools and
resources. The only condition was that such tools or
resources must not have been developed with the an-
notations of the test set, both for the input and output
annotations of the data. In this challenge, we were
interested in learning methods which make use of
any tools or resources that might improve the per-
formance. The comparison of different systems in
this setting may not be fair, and thus ranking of sys-
tems is not necessarily important.
2.2 Joint and SRL-only tasks
In 2008, systems participating in the open challenge
could have used state-of-the-art parsers for the syn-
tactic dependency part of the task. This year, we
have provided the output of these parsers for all the
languages in an uniform way, thus allowing an or-
thogonal combination of the two tasks and the two
challenges. For the SRL-only task, participants in
the closed challenge simply had to use the provided
parses only.
Despite the provisions for the SRL-only task, we
are more interested in the approaches and results of
the Joint task. Therefore, primary system ranking is
provided for the Joint task while additional measures
2
are computed for various combinations of parsers
and SRL methods across the tasks and challenges.
2.3 Data Format
The data format used in this shared task has been
based on the CoNLL-2008 shared task, with some
differences. The data follows these general rules:
? The files contain sentences separated by a blank
line.
? A sentence consists of one or more tokens and
the information for each token is represented on
a separate line.
? A token consists of at least 14 fields. The fields
are separated by one or more whitespace char-
acters (spaces or tabs). Whitespace characters
are not allowed within fields.
The data is thus a large table with whitespace-
separated fields (columns). The fields provided in
the data are described in Table 1. They are identical
for all languages, but they may differ in contents;
for example, some fields might not be filled for all
the languages provided (such as the FEAT or PFEAT
fields).
For the SRL-only task, participants have been
provided will all the data but the PRED and
APREDs, which they were supposed to fill in with
their correct values. However, they did not have
to determine which tokens are predicates (or more
precisely, which are the argument-bearing tokens),
since they were marked by ?Y? in the FILLPRED
field.
For the Joint task, participants could not (in ad-
dition to the PRED and APREDs) see the gold-
standard nor the predicted syntactic dependencies
(HEAD, PHEAD) and their labels (DEPREL, PDE-
PREL). These syntactic dependencies were also to
be filled by participants? systems.
In both tasks, participants have been free to
use any other data (columns) provided, except the
LEMMA, POS and FEAT columns (to get more ?re-
alistic? results using only their automatically pre-
dicted variants PLEMMA, PPOS and PFEAT).
Besides the corpus proper, predicate dictionaries
have been provided to participants in order to be able
to properly match the predicates to the tokens in the
corpus; their contents could have been used e.g. as
features for the PRED/APREDs predictions (or even
for the syntactic dependencies, i.e., for filling in the
PHEAD and PDEPREL fields).
The system of filling-in the APREDs follows
the 2008 pattern; for each argument-bearing token
(predicate), a new APREDn column is created in the
order in which the predicate token is encountered
within the sentence (i.e., based on its ID seen as a
numerical value). Then, for each token in the sen-
tence, the value in the intersection of the APREDn
column and the token row is either left unfilled
(if the token is not an argument), or a predicate-
argument label(s) is(are) filled in.
The differences between the English-only 2008
task and this year?s multilingual task can be briefly
summarized as follows:
? only ?split?2 lemmas and forms have been pro-
vided in the English datasets (for the other lan-
guages, original tokenization from the respec-
tive treebanks has been used);
? rich morphological features have been added
wherever available;
? syntactic dependencies by state-of-the-art
parsers have been provided (for the SRL-only
task);
? multiple semantic labels for a single token have
been allowed (and properly evaluated) in the
APREDs columns;
? predicates have been pre-identified and marked
in both the training and test data;
? some of the fields (e.g. the APREDx) and val-
ues (ARG0? A0 etc.) have been renamed.
2.4 Evaluation Measures
It was required that participants submit results in all
seven languages in the chosen task and in any of (or
both) the challenges. Submission of out-of-domain
data files has been optional.
The main evaluation measure, according to which
systems are primarily compared, is the Joint task,
2Splitting of forms and lemmas in English has been intro-
duced in the 2008 shared task to match the tokenization con-
vention for the arguments in NomBank.
3
Field # Name Description
1 ID Token counter, starting at 1 for each new sentence
2 FORM Form or punctuation symbol (the token; ?split? for English)
3 LEMMA Gold-standard lemma of FORM
4 PLEMMA Automatically predicted lemma of FORM
5 POS Gold-standard POS (major POS only)
6 PPOS Automatically predicted major POS by a language-specific tagger
7 FEAT Gold-standard morphological features (if applicable)
8 PFEAT Automatically predicted morphological features (if applicable)
9 HEAD Gold-standard syntactic head of the current token (ID or 0 if root)
10 PHEAD Automatically predicted syntactic head
11 DEPREL Gold-standard syntactic dependency relation (to HEAD)
12 PDEPREL Automatically predicted dependency relation to PHEAD
13 FILLPRED Contains ?Y? for argument-bearing tokens
14 PRED (sense) identifier of a semantic ?predicate? coming from a current token
15... APREDn Columns with argument labels for each semantic predicate (in the ID order)
Table 1: Description of the fields (columns) in the data provided. The values of columns 9, 11 and 14 and above are
not provided in the evaluation data; for the Joint task, columns 9?12 are also empty in the evaluation data.
closed challenge, Macro F1 score. However, scores
can also be computed for a number of other condi-
tions:
? Task: Joint or SRL-only
? Challenge: open or closed
? Domain: in-domain data (IDD, separated from
training corpus) or out-of-domain data (OOD)
Joint task participants are also evaluated separately
on the syntactic dependency task (labeled attach-
ment score, LAS). Finally, systems competing in
both tasks are compared on semantic role labeling
alone, to assess the impact of the the joint pars-
ing/SRL task compared to an SRL-only task on pre-
parsed data.
Finally, as an explanatory measure, precision and
recall of the semantic labeling task have been com-
puted and tabulated.
We have decided to omit several evaluation fig-
ures that were reported in previous years, such as the
percentage of completely correct sentences (?Exact
Match?), unlabeled scores, etc. With seven lan-
guages, two tasks (plus two challenges, and the
IDD/OOD distinction), there are enough results to
get lost even as it is.
2.4.1 Syntactic Dependency Measures
The LAS score is defined similarly as in the pre-
vious shared tasks, as the percentage of tokens for
which a system has predicted the correct HEAD and
DEPREL columns. The unlabeled attachment score
(UAS), i.e., the percentage of tokens with correct
HEAD regardless if the DEPREL is correct, has not
been officially computed this year. No precision and
recall measures are applicable, since all systems are
supposed to output a single dependency with a single
label (see also below the footnote to the description
of the combined score).
2.4.2 Semantic Labeling Measures
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we cre-
ate n semantic dependencies from every predicate
to its n arguments. These dependencies are labeled
with the labels of the corresponding arguments. Ad-
ditionally, we create a semantic dependency from
each predicate to a virtual ROOT node. The latter
dependencies are labeled with the predicate senses.
This approach guarantees that the semantic depen-
dency structure conceptually forms a single-rooted,
connected (but not necessarily acyclic) graph. More
importantly, this scoring strategy implies that if a
system assigns the incorrect predicate sense, it still
receives some points for the arguments correctly as-
signed. For example, for the correct proposition:
verb.01: A0, A1, AM-TMP
the system that generates the following output for
the same argument tokens:
4
verb.02: A0, A1, AM-LOC
receives a labeled precision score of 2/4 because two
out of four semantic dependencies are incorrect: the
dependency to ROOT is labeled 02 instead of 01
and the dependency to the AM-TMP is incorrectly la-
beled AM-LOC. Using this strategy we compute pre-
cision, recall, and F1 scores for semantic dependen-
cies (labeled only).
For some languages (Czech, Japanese) there may
be more than one label in a given argument position;
for example, this happens in Czech in special cases
of reciprocity when the same token serves as two or
more arguments to the same predicate. The scorer
takes this into account and considers such cases to
be (as if) multiple predicate-argument relations for
the computation of the evaluation measures.
For example, for the correct proposition:
v1f1: ACT|EFF, ADDR
the system that generates the following output for
the same argument tokens:
v1f1: ACT, ADDR|PAT
receives a labeled precision score of 3/4 because
the PAT is incorrect and labeled recall 3/4 be-
cause the EFF is missing (should the ACT|EFF and
ADDR|PAT be taken as atomic values, the scores
would then be zero).
2.4.3 Combined Syntactic and Semantic Score
We combine the syntactic and semantic measures
into one global measure using macro averaging. We
compute macro precision and recall scores by aver-
aging the labeled precision and recall for semantic
dependencies with the LAS for syntactic dependen-
cies:3
LMP = Wsem ? LPsem + (1?Wsem) ? LAS (1)
LMR = Wsem ? LRsem + (1 ?Wsem) ? LAS (2)
where LMP is the labeled macro precision and
LPsem is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LRsem is the labeled recall for semantic
dependencies. Wsem is the weight assigned to the
3We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the predicted
number of dependencies is equal to the number of gold depen-
dencies.
semantic task.4 The macro labeled F1 score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
3 Data
The unification of the data formats for the various
languages appeared to be a challenge in itself. We
will briefly describe the processes of the conversion
of the existing treebanks in the seven languages of
the CoNLL-2009 shared task. In many instances,
the original treebanks had to be not only converted
format-wise, but also merged with other resources in
order to generate useful training and testing data that
fit the task description.
3.1 The Input Corpora
The data used as the input for the transformations
aimed at arriving at the data contents and format de-
scribed in Sect. 2.3 are described in (Taule? et al,
2008), (Xue and Palmer, 2009), (Hajic? et al, 2006),
(Surdeanu et al, 2008), (Burchardt et al, 2006) and
(Kawahara et al, 2002).
In the subsequent sections, the procedures for the
data conversion for the individual languages are de-
scribed. The data has been collected by the main
organization site and checked for format errors, and
repackaged for distribution.
There were three packages of the data distributed
to the participants: Trial, Training plus Develop-
ment, and Evaluation. The Trial data were rather
small, just to give the feeling of the format and
languages involved. A visual representation of the
Trial data was also created to make understanding
of the data easier. Any data in the same format
can be transformed and displayed in the Tree Editor
TrEd5 (Pajas and ?Ste?pa?nek, 2008) with the CoNLL
2009 Shared Task extension that can be installed
from within the editor. A sample visualization of an
English sentence after its conversion to the shared
task format (Sect. 2.3) is in Fig. 1.
Due to licensing requirements, every package of
the data had to be split into two portions. One
portion (Catalan, German, Japanese, and Spanish
data) was published on the task?s webpage for down-
4We assign equal weight to the two tasks, i.e., Wsem = 0.5.
5http://ufal.mff.cuni.cz/?pajas/tred
5
$QG
'(3 &&
VRPHWLPHV
703 5%
D
102' '7
UHSXWDEOH
102' --
FKDULW\
6%- 11
ZLWK
102' ,1
D
102' '7
KRXVHKROG
102' 11
QDPH QDPH
302' 11
JHWV JHW
5227 9%=
XVHG XVH
9& 9%1
DQG
&225' &&
GRHV
&21- 9%=
Q
W
$'9 5%
HYHQ
$'9 5%
NQRZ NQRZ
9& 9%
LW
2%- 353

3  
$0703$0703$0703
 

$$$$

 
 

$
 

 

$



$01(*

$0$'9
 


$

Figure 1: Visualisation of the English sentence ?And sometimes a reputable charity with a houshold name gets used
and doesn?t even know it.? (Penn Treebank, wsj 0559) showing jointly the labeled syntactic and semantic depen-
dencies. The basic tree shape comes from the syntactic dependencies; syntactic labels and POS tags are on the 2nd
line at each node. Semantic dependencies which do not follow the syntactic ones use dotted lines. Predicate senses
in parentheses (use:01, ...) follow the word label. SRLs (A0, AM-TMP, ...) are on the last line. Please note that
multiple semantic dependencies (e.g., there are four for charity: A0? know, A1? gets, A1? used, A1? name)
and self-dependencies (name) appear in this sentence.
load, the other portion (Czech, English, and Chinese
data) was invoiced and distributed by the Linguistic
Data Consortium under a special agreement free of
charge.
Distribution of the Evaluation package was a bit
more complicated, because there were two types of
the packages - one for the Joint task and one for the
SRL-only task. Every participant had to subscribe
to one of the two tasks; subsequently, they obtained
the appropriate data (again, from the webpage and
LDC).
Prior to release, each data file was checked to
eliminate errors. The following test were carried
out:
? For every sentence, number of PREDs rows
matches the number of APREDs columns.
? The first line of each file is never empty, while
the last line always is.
? The first character on a non-empty line is al-
ways a digit, the last one is never a whitespace.
? The number of empty lines (i.e. the number
of sentences) equals the number of lines begin-
ning with ?1?.
? The data contain no spaces nor double tabs.
Some statistics on the data can be seen in Ta-
bles 2, 3 and 4. Whereas the training sizes of the
data have not been that different as they were e.g.
for the 2007 shared task on multilingual dependency
parsing (Nivre et al, 2007)6, substantial differences
existed in the distribution of the predicates and ar-
guments, the input features, the out-of-vocabulary
rates, and other statistical characteristics of the data.
Data sizes have been relatively uniform in all the
datasets, with Japanese having the smallest dataset
6http://nextens.uvt.nl/depparse-wiki/
DataOverview
6
containing data for SRL annotation training. To
compensate at least for the dependency parsing part,
an additional, large Japanese corpus with syntactic
dependency annotation has been provided.
The average sentence length, the vocabulary sizes
for FORM and LEMMA fields and the OOV rates
characterize quite naturally the properties of the re-
spective languages (in the domain of the training and
evaluation data). It is no surprise that the FORM
OOV rate is the highest for Czech, a highly inflec-
tional language, and that the LEMMA OOV rate is
the highest for German (as a consequence of keeping
compounds as a single lemma). The other statistics
also reflect (to a large extent) the annotation speci-
fication and conventions used for the original tree-
banks and/or the result of the conversion process to
the unified CoNLL-2009 Shared Task format.
Starting with the POS and FEAT fields, it can be
seen that Catalan, Czech and Spanish use only the
12 major part-of-speech categories as values of the
POS field (with richly populated FEAT field); En-
glish and Chinese are the opposite extreme, disre-
garding the use of the FEAT field completely and
coding everything as a POS value. While for Chi-
nese this is quite understandable, English follows the
PTB tradition in this respect. German and Japanese
use relatively rich set of values in both the POS and
FEAT fields.
For the dependency relations (DEPREL), all
the languages use a similarly-sized set except for
Japanese, which only encodes the distinction be-
tween a root and a dependent node (and some in-
frequent special ones).
Evaluation data are over 10% of the size of the
training data for Catalan, Chinese, Czech, Japanese
and Spanish and roughly 5% for English and Ger-
man.
Table 3 shows the distribution of the five most fre-
quent dependency relations (determined as part of
the subtask of syntactic parsing). With the exception
of Japanese, which essentially does not label depen-
dency relations at this level, all the other languages
show little difference in this distribution. For exam-
ple, the unconditioned probability of ?subjects? is
almost the same for all the six other languages (be-
tween 6 and 8 percent). The probability mass cov-
ered by the first five most frequent DEPRELs is also
almost the same (again, except for Japanese), sug-
gesting that the labeling task might have similar dif-
ficulty7. The most skewed one is for Czech (after
Japanese).
Table 4 shows similar statistics for the argument
labels (PRED/APREDs); it also adds the average
number of arguments per ?predicate? token, since
this is part of the SRL task8. It is apparent from the
comparison of the ?Total? rows in this table and Ta-
ble 3 that the first five argument labels cover more
that their syntactic counterparts. For example, the
arguments A0-A4 account for all but 3% of all ar-
guments labels, whereas Spanish and Catalan have
much more rich set of argument labels, with a high
entropy of the most-frequent-label distribution.
3.2 Catalan and Spanish
The Catalan and Spanish datasets (Taule? et al, 2008)
were generated from the AnCora corpora9 through
an automatic conversion process from a constituent-
based formalism to dependencies (Civit et al, 2006).
AnCora corpora contain about half million words
for Catalan and Spanish annotated with syntactic
and semantic information. Text sources for the Cata-
lan corpus are EFE news agency (?75Kw), ACN
Catalan news agency (?225Kw), and ?El Perio?dico?
newspaper (?200Kw). The Spanish corpus comes
from the Lexesp Spanish balanced corpus (?75Kw),
the EFE Spanish news agency (?225Kw), and the
Spanish version of ?El Perio?dico? (?200Kw). The
subset from ?El Perio?dico? corresponds to the same
news in Catalan and Spanish, spanning from January
to December 2000.
Linguistic annotation is the same in both lan-
guages and includes: PoS tags with morphologi-
cal features (gender, number, person, etc.), lemma-
tization, syntactic dependencies (syntactic func-
tions), semantic dependencies (arguments and the-
matic roles), named entities and predicate semantic
classes (Lexical Semantic Structure, LSS). Tag sets
are shared by the two languages.
If we take into account the complete PoS tags,
7Yes, this is overgeneralization since this distribution does
not condition on the features, dependencies etc. But as a rough
measure, it often correlates well with the results.
8A number below 1 means there are some argument-bearing
words (often nouns) which have no arguments in the particular
sentence in which they appear.
9http://clic.ub.edu/ancora
7
Characteristic Catalan Chinese Czech English German Japanese Spanish
Training data size (sentences) 13200 22277 38727 39279 36020 4393a 14329
Training data size (tokens) 390302 609060 652544 958167 648677 112555a 427442
Avg. sentence length (tokens) 29.6 27.3 16.8 24.4 18.0 25.6 29.8
Tokens with argumentsb (%) 9.6 16.9 63.5 18.7 2.7 22.8 10.3
DEPREL types 50 41 49 69 46 5 49
POS types 12 41 12 48 56 40 12
FEAT types 237 1 1811 1 267 302 264
FORM vocabulary size 33890 40878 86332 39782 72084 36043 40964
LEMMA vocabulary size 24143 40878 37580 28376 51993 30402 26926
Evaluation data size (sent.) 1862 2556 4213 2399 2000 500 1725
Evaluation data size (tokens) 53355 73153 70348 57676 31622 13615 50630
Evaluation FORM OOVc 5.40 3.92 7.98/8.62d 1.58/3.76d 7.93/7.57d 6.07 5.63
Evaluation LEMMA OOVc 4.14 3.92 3.03/4.29d 1.08/2.30d 5.83/7.36d 5.21 3.69
Table 2: Elementary data statistics for the CoNLL-2009 Shared Task languages. The data themselves, the original
treebanks they were derived from and the conversion process are described in more detail in sections 3.2-3.7. All
evaluation data statistics are derived from the in-domain evaluation data.
aThere were additional 33257 sentences (839947 tokens) available for syntactic dependency parsing of Japanese; the type and
vocabulary statistics are computed using this larger dataset.
bPercentage of tokens with FILLPRED=?Y?.
cPercentage of FORM/LEMMA tokens not found in the respective vocabularies derived solely from the training data.
dOOV percentage for in-domain/out-of-domain data.
DEPREL Catalan Chinese Czech English German Japanese Spanish
sn 0.16 COMP 0.21 Atr 0.26 NMOD 0.27 NK 0.31 D 0.93 sn 0.16
spec 0.15 NMOD 0.14 AuxP 0.10 P 0.11 PUNC 0.14 ROOT 0.04 spec 0.15
Labels f 0.11 ADV 0.10 Adv 0.10 PMOD 0.10 MO 0.12 P 0.03 f 0.12
sp 0.09 UNK 0.09 Obj 0.07 SBJ 0.07 SB 0.07 A 0.00 sp 0.08
suj 0.07 SBJ 0.08 Sb 0.06 OBJ 0.06 ROOT 0.06 I 0.00 suj 0.08
Total 0.58 0.62 0.59 0.61 0.70 1.00 0.59
Table 3: Unigram probability for the five most frequent DEPREL labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five dependency labels shown.
APRED Catalan Chinese Czech English German Japanese Spanish
arg1-pat 0.22 A1 0.30 RSTR 0.30 A1 0.37 A0 0.40 GA 0.33 arg1-pat 0.20
arg0-agt 0.18 A0 0.27 PAT 0.18 A0 0.25 A1 0.39 WO 0.15 arg0-agt 0.19
Labels arg1-tem 0.15 ADV 0.20 ACT 0.17 A2 0.12 A2 0.12 NO 0.15 arg1-tem 0.15
argM-tmp 0.08 TMP 0.07 APP 0.06 AM-TMP 0.06 A3 0.06 NI 0.09 arg2-atr 0.08
arg2-atr 0.08 DIS 0.04 LOC 0.04 AM-MNR 0.03 A4 0.01 DE 0.06 argM-tmp 0.08
Total 0.71 0.91 0.75 0.83 0.97 0.78 0.70
Avg. 2.25 2.26 0.88 2.20 1.97 1.71 2.26
Table 4: Unigram probability for the five most frequent APRED labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five argument labels shown. The ?Avg.? line
shows the average number of arguments per predicate or other argument-bearing token (i.e. for those marked by
FILLPRED=?Y?).
8
AnCora has 280 different labels. Considering only
the main syntactic categories, the tag set is reduced
to 47 tags. The syntactic tag set consists of 50 dif-
ferent syntactic functions. Regarding semantic ar-
guments, we distinguish Arg0, Arg1, Arg2, Arg3,
Arg4, ArgM, and ArgL. The first five tags are num-
bered from less to more obliqueness with respect
to the verb, ArgM corresponds to adjuncts. The
list of thematic roles consists of 20 different labels:
AGT (Agent), AGI (Induced Agent), CAU (Cause),
EXP (Experiencer), SCR (Source), PAT (Patient),
TEM (Theme), ATR (Attribute), BEN (Beneficiary),
EXT (Extension), INS (Instrument), LOC (Loca-
tive), TMP (Time), MNR (Manner), ORI (Origin),
DES (Goal), FIN (Purpose), EIN (Initial State), EFI
(Final State), and ADV (Adverbial). Each argument
position can map onto specific thematic roles. By
way of example, Arg1 can be PAT, TEM or EXT. For
Named Entities, we distinguish six types: Organiza-
tion, Person, Location, Date, Number, and Others.
An incremental process guided the annotation of
AnCora, since semantics depends on morphosyntax,
and syntax relies on morphology. This procedure
made it possible to check, correct, and complete
the previous annotations, thus guaranteeing the final
quality of the corpora and minimizing the error rate.
The annotation process was carried out sequentially
from lower to upper layers of linguistic description.
All resulting layers are independent of each other,
thus making easier the data management. The ini-
tial annotation was performed manually for syntax,
semiautomatically in the case of arguments and the-
matic roles, and fully automatically for PoS (Mart??
et al, 2007; Ma`rquez et al, 2007).
The Catalan and Spanish AnCora corpora were
straightforwardly translated into the CoNLL-2009
shared task formatting (information about named
entities was skipped in this process). The resulting
Catalan corpus (including training, development and
test partitions) contains 16,786 sentences with an av-
erage length of 29.59 lexical tokens per sentence.
Long sentences abound in this corpus. For instance,
10.73% of the sentences are longer than 50 tokens,
and 4.42% are longer than 60. The corpus con-
tains 47,537 annotated predicates (2.83 predicates
per sentence, on average) with 107,171 arguments
(2.25 arguments per predicate, on average). From
the latter, 73.89% correspond to core arguments and
26.11% to adjuncts. Numbers for the Spanish cor-
pus are comparable in all aspects: 17,709 sentences
with 29.84 lexical tokens on average (11.58% of the
sentences longer than 50 tokens, 4.07% longer than
60); 54,075 predicates (3.05 per sentence, on aver-
age) and 122,478 arguments (2.26 per predicate, on
average); 73.34% core arguments and 26.66% ad-
juncts.
The following are important features of the Cata-
lan and Spanish corpora in the CoNLL-2009 shared
task setting: (1) all dependency trees are projective;
(2) no word can be the argument of more than one
predicate in a sentence; (3) semantic dependencies
completely match syntactic dependency structures
(i.e., no new edges are introduced by the semantic
structure); (4) only verbal predicates are annotated
(with exceptional cases referring to words that can
be adjectives and past participles); (5) the corpus is
segmented so multi-words, named entities, temporal
expressions, compounds, etc. are grouped together;
and (6) segmentation also accounts for elliptical pro-
nouns (there are marked as empty lexical tokens ?_?
with a pronoun POS tag).
Finally, the predicted columns (PLEMMA,
PPOS, and PFEAT) have been generated with the
FreeLing Open source suite of Language Analyz-
ers10. Accuracy in PLEMMA and PPOS columns
is above 95% for the two languages. PHEAD
and PDEPREL columns have been generated using
MaltParser11. Parsing accuracy (LAS) is above 86%
for the the two languages.
3.3 Chinese
The Chinese Corpus for the 2009 CoNLL Shared
Task was generated by merging the Chinese Tree-
bank (Xue et al, 2005) and the Chinese Proposition
Bank (Xue and Palmer, 2009) and then converting
the constituent structure to a dependency formalism
as specified in the CoNLL Shared Task. The Chi-
nese data used in the shared task is based on Chinese
Treebank 6.0 and the Chinese Proposition Bank 2.0,
both of which are publicly available via the Linguis-
tic Data Consortium.
The Chinese Treebank Project originated at Penn
and was later moved to University of Colorado at
10http://www.lsi.upc.es/?nlp/freeling
11http://w3.msi.vxu.se/?jha/maltparser
9
Boulder. Now it is the process of being to moved
to Brandeis University. The data sources of the Chi-
nese Treebank range from Xinhua newswire (main-
land China), Hong Kong news, and Sinorama Maga-
zine (Taiwan). More recently under DARPA GALE
funding it has been expanded to include broadcast
news, broadcast conversation, news groups and web
log data. It currently has over one million words
and is fully segmented, POS-tagged and annotated
with phrase structure. The version of the Chinese
Treebank used in this shared task, CTB 6.0, includes
newswire, magazine articles, and transcribed broad-
cast news 12. The training set has 609,060 tokens,
the development set has 49,620 tokens, and the test
set has 73,153 tokens.
The Chinese Proposition Bank adds a layer of se-
mantic annotation to the syntactic parses in the Chi-
nese Treebank. This layer of semantic annotation
mainly deals with the predicate-argument structure
of Chinese verbs and their nominalizations. Each
major sense (called frameset) of a predicate takes a
number of core arguments annotated with numeri-
cal labels Arg0 through Arg5 which are defined in
a predicate-specific manner. The Chinese Proposi-
tion Bank also annotates adjunctive arguments such
as locative, temporal and manner modifiers of the
predicate. The version of the Chinese Propbank used
in this CoNLL Shared Task is CPB 2.0, but nominal
predicates are excluded because the annotation is in-
complete.
Since the Chinese Treebank is annotated with
constituent structures, the conversion and merging
procedure converts the constituent structures to de-
pendencies by identifying the head for each con-
stituent in a parse tree and making its sisters its de-
pendents. The Chinese Propbank pointers are then
shifted from the entire constituent to the head of that
constituent. The conversion procedure identifies the
head by first exploiting the structural information
in the syntactic parse and detecting six broad cate-
gories of syntactic relations that hold between the
head and its dependents (predication, modification,
complementation, coordination, auxiliary, and flat)
and then designating the head based on these rela-
tions. In particular, the first conjunct of a coordina-
12A small number of files were taken out of the CoNLL
shared task data due to conversion problems and time con-
straints to fix them.
tion structure is designated as the head and the heads
of the other conjuncts are the conjunctions preced-
ing them. The conjunctions all ?modify? the first
conjunct.
3.4 Czech
For the training, development and evaluation data,
Prague Dependency Treebank 2.0 was used (Hajic?
et al, 2006). For the out-of-domain evaluation data,
part of the Czech side of the Prague Czech-English
Dependency Treebank (version 2, under construc-
tion) was used13, see also ( ?Cmejrek et al, 2004). For
the OOD data, no manual annotation of LEMMA,
POS, and FEAT existed, so the predicted values
were used. The same conversion procedure has been
applied to both sources.
The FORM column was created from the form
element of the morphological layer, not from the
?token? from the word-form layer. Therefore, most
typos, errors in word segmentation and tokenization
are corrected and numerals are normalized.
The LEMMA column was created from the
lemma element of the morphological layer. Only
the initial string of the element was used, so there is
no distinction between homonyms. However, some
components of the detailed lemma explanation were
incorporated into the FEAT column (see below).
The POS column was created form the morpho-
logical tag element, its first character more pre-
cisely.
The FEAT column was created from the remain-
ing characters of the tag element. In addition, the
special feature ?Sem? corresponds to a semantic fea-
ture of the lemma.
For the HEAD and DEPREL columns, the PDT
analytical layer was used. The DEPREL was taken
from the analytic function (the afun node at-
tribtue). There are 27 possible values for afun el-
ement: Pred, Pnom, AuxV, Sb, Obj, Atr, Adv,
Atv, AtvV, Coord, Apos, ExD, and a number
of auxiliary and ?double-function? labels. The first
nine of these are the ?most interesting? from the
point of view of the shared task, since they relate to
semantics more closely than the rest (at least from
the linguistic point of view). The HEAD is a pointer
to its parent, which means the PDT?s ord attribute
13http://ufal.mff.cuni.cz/pedt
10
(within-sentence ID / word position number) of the
parent. If a node is a member of a coordination
or apposition (is_member element), its DEPREL
obtains the _M suffix. The parenthesis annotation
(is_parenthesis_root element) was ignored.
The PRED and APREDs columns were created
from the tectogrammatical layer of PDT 2.0 and the
valency lexicon PDT-Vallex according to the follow-
ing rules:
? Every line corresponding to an analytical node
referenced by a lexical reference (a/lex.rf)
from the tectogrammatical layer has a PRED
value filled. If the referring non-generated
tectogrammatical node (is_generated not
equal to 1) has a valency frame assigned
(val_frame.rf), the value of PRED is the
identifier of the frame. Otherwise, it is set to
the same value as the LEMMA column.
? For every tectogrammatical node, a corre-
sponding analytical node is searched for:
1. If the tectogrammatical node is not
generated and has a lexical reference
(a/lex.rf), the referenced node is
taken.
2. Otherwise, if the tectogrammatical node
has a coreference (coref_text.rf or
coref_gram.rf) or complement refer-
ence (compl.rf) to a node that has an
analytical node assigned (by 1. or 2.), the
assigned node is taken.
APRED columns are filled with respect to the
following correspondence: for a tectogrammatical
node P and its effective child C with functor F, the
column for P?s corresponding analytical node at the
row for C?s corresponding analytical node is filled
with F. Some nodes can thus have several functors
in one APRED column, separated by a vertical bar
(see Sect. 2.4.2).
PLEMMA, PPOS and PFEAT were gener-
ated by the (cross-trained) morphological tagger
MORCE (Spoustova? et al, 2009), which gives full
combined accuracy (PLEMMA+PPOS+PFEAT)
slightly under 96%.
PHEAD and PDEPREL were generated by
the (cross-trained) MST parser for Czech (Chu?
Liu/Edmonds algorithm, (McDonald et al, 2005)),
which has typical dependency accuracy around
85%.
The valency lexicon, converted from (Hajic? et al,
2003), has four columns:
1. lemma (can occur several times in the lexicon,
with different frames)
2. frame identifier (as found in the PRED column)
3. list of space-separated actants and obligatory
members of the frame
4. example(s)
The source of the out-of-domain data uses an
extended valency lexicon (because of out-of-
vocabulary entries). For simplicity, the extended
lexicon was not provided; instead, such words were
not marked as predicates in the OOD data (their
FILLPRED was set to ?_?) and thus not evaluated.
3.5 English
The English corpus is almost identical to the cor-
pus used in the closed challenge in the CoNLL-2008
shared task evaluation (Surdeanu et al, 2008). This
corpus was generated through a process that merges
several input corpora and converts them from the
constituent-based formalism to dependencies. The
following corpora were used as input to the merging
procedure:
? Penn Treebank 3 ? The Penn Treebank 3 cor-
pus (Marcus et al, 1994) consists of hand-
coded parses of the Wall Street Journal (test,
development and training) and a small subset
of the Brown corpus (W. N. Francis and H.
Kucera, 1964) (test only).
? BBN Pronoun Coreference and Entity Type
Corpus ? BBN?s NE annotation of the Wall
Street Journal corpus (Weischedel and Brun-
stein, 2005) takes the form of SGML inline
markup of text, tokenized to be completely
compatible with the Penn Treebank annotation.
For the CoNLL-2008 shared task evaluation,
this corpus was extended by the task organizers
to cover the subset of the Brown corpus used as
a secondary testing dataset. From this corpus
we only used NE boundaries to derive NAME
11
dependencies between NE tokens, e.g., we cre-
ate a NAME dependency from Mary to Smith
given the NE mention Mary Smith.
? Proposition Bank I (PropBank) ? The Prop-
Bank annotation (Palmer et al, 2005) classifies
the arguments of all the main verbs in the Penn
Treebank corpus, other than be. Arguments are
numbered (Arg0, Arg1, . . .) based on lexical
entries or frame files. Different sets of argu-
ments are assumed for different rolesets. De-
pendent constituents that fall into categories in-
dependent of the lexical entries are classified as
various types of adjuncts (ArgM-TMP, -ADV,
etc.).
? NomBank ? NomBank annotation (Meyers et
al., 2004) uses essentially the same framework
as PropBank to annotate arguments of nouns.
Differences between PropBank and NomBank
stem from differences between noun and verb
argument structure; differences in treatment of
nouns and verbs in the Penn Treebank; and dif-
ferences in the sophistication of previous re-
search about noun and verb argument structure.
Only the subset of nouns that take arguments
are annotated in NomBank and only a subset of
the non-argument siblings of nouns are marked
as ArgM.
The complete merging process and the conversion
from the constituent representation to dependencies
is detailed in (Surdeanu et al, 2008).
The main difference between the 2008 and 2009
version of the corpora is the generation of word lem-
mas. In the 2008 version the only lemmas pro-
vided were predicted using the built-in lemmatizer
in WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and the predicted part-of-
speech tag. These lemmas are listed in the 2009
corpus under the PLEMMA column. The LEMMA
column in the 2009 version of the corpus contains
lemmas generated using the same algorithm but us-
ing the correct Treebank part-of-speech tags. Addi-
tionally, the PHEAD and PDEPREL columns were
generated using MaltParser14, similarly to the open
challenge corpus in the CoNLL 2008 shared task.
14http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
3.6 German
The German in-domain dataset is based on the an-
notated verb instances of the SALSA corpus (Bur-
chardt et al, 2006), a total of around 40k sen-
tences15. SALSA provides manual semantic role
annotation on top of the syntactically annotated
TIGER newspaper corpus, one of the standard Ger-
man treebanks. The original SALSA corpus uses se-
mantic roles in the FrameNet paradigm. We con-
structed mappings between FrameNet frame ele-
ments and PropBank argument positions at the level
of frame-predicate pairs semi-automatically. For the
frame elements of each frame-predicate pair, we first
identified the semantically defined PropBank Arg-
0 and Arg-1 positions. To do so, we annotated a
small number of very abstract frame elements with
these labels (Agent, Actor, Communicator as Arg-
0, and Theme, Effect, Message as Arg-1) and per-
colated these labels through the FrameNet hierar-
chy, adding further manual labels where necessary.
Then, we used frequency and grammatical realiza-
tion information to map the remaining roles onto
higher-numbered Arg roles. We considerably sim-
plified the annotations provided by SALSA, which
use a rather complex annotation scheme. In partic-
ular, we removed annotation for multi-word expres-
sions (which may be non-contiguous), annotations
involving multiple frames for the same predicate
(metaphors, underspecification), and inter-sentence
roles.
The out-of-domain dataset was taken from a study
on the multi-lingual projection of FrameNet annota-
tion (Pado and Lapata, 2005). It is sampled from
the EUROPARL corpus and was chosen to maxi-
mize the lexical coverage, i.e., it contains of a large
number of infrequent predicates. Both syntactic and
semantic structure were annotated manually, in the
TIGER and SALSA format, respectively. Since it
uses a simplified annotation schemes, we did not
have to discard any annotation.
For both datasets, we converted the syntactic
TIGER (Brants et al, 2002) representations into de-
pendencies with a similar set of head-finding rules
used for the preparation of the CoNLL-X shared task
German dataset. Minor modifications (for the con-
15Note, however, that typically not all predicates in each sen-
tence are annotated (cf. Table 2).
12
version of person names and coordinations) were
made to achieve better consistency with datasets
of other languages. Since the TIGER annotation
allows non-contiguous constituents, the resulting
dependencies can be non-projective. Secondary
edges were discarded in the conversion. As for the
automatically constructed features, we used Tree-
Tagger (Schmid, 1994) to produce the PLEMMA
and PPOS columns, and the Morphisto morphol-
ogy (Zielinski and Simon, 2008) for PFEAT.
3.7 Japanese
For Japanese, we used the Kyoto University Text
Corpus (Kawahara et al, 2002), which consists of
approximately 40k sentences taken from Mainichi
Newspapers. Out of them, approximately 5k sen-
tences are annotated with syntactic and semantic de-
pendencies, and are used the training, development
and test data of this year?s shared task. The remain-
ing sentences, which are annotated with only syntac-
tic dependencies, are provided for the training cor-
pus of syntactic dependency parsers.
This corpus adopts a dependency structure repre-
sentation, and thus the conversion to the CoNLL-
2009 format was relatively straightforward. How-
ever, since the original dependencies are annotated
on the basis of phrases (Japanese bunsetsu), we
needed to automatically convert the original annota-
tions to word-based ones using several criteria. We
used the following basic criteria: the words except
the last word in a phrase depend on the next (right)
word, and the last word in a phrase basically depends
on the head word of the governing phrase.
Semantic dependencies are annotated for both
verbal predicates and nominal predicates. The se-
mantic roles (APRED columns) consist of 41 sur-
face cases, many of which are case-marking post-
positions such as ga (nominative), wo (accusative)
and ni (dative). Semantic frame discrimination is not
annotated, and so the PRED column is the same as
the LEMMA column. The original corpus contains
coreference annotations and inter-sentential seman-
tic dependencies, such as inter-sentential zero pro-
nouns and bridging references, but we did not use
these annotations, which are not the target of this
year?s shared task.
To produce the PLEMMA, PPOS and PFEAT
columns, we used the morphological analyzer JU-
MAN 16 and the dependency and case structure an-
alyzer KNP 17. To produce the PHEAD and PDE-
PREL columns, we used the MSTParser 18.
4 Submissions and Results
Participants uploaded the results through the shared
task website, and the official evaluation was per-
formed centrally. Feedback was provided if any for-
mal problems were encountered (for a list of checks,
see the previous section). One submission had to
be rejected because only English results were pro-
vided. After the evaluation period had passed, the
results were anonymized and published on the web.
A total of 20 systems participated in the closed
challenge; 13 of them in the Joint task and seven in
the SRL-only task. Two systems participated in the
open challenge (Joint task). Moreover, 17 systems
provided output in the out-of-domain part of the task
(11 in the OOD Joint task and six in the OOD SRL-
only task).
The main results for the core task - the Joint task
(dependency syntax and semantic relations) in the
context of the closed challenge - are summarized and
ranked in Table 5.
The largest number of systems can be compared
in the SRL results table (Table 6), where all the sys-
tems have been evaluated solely on the SRL perfor-
mance regardless whether they participated in the
Joint or SRL-only task. However, since the results
might have been influenced by the supplied parser,
separate ranking is provided for both types of the
systems.
Additional breakdown of the results (open chal-
lenge, precision and recall tables for the semantic
labeling task, etc.) are available from the CoNLL-
2009 Shared Task website19.
5 Approaches
Table 7 summarizes the properties of the systems
that participated in the closed the open challenges.
16http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
17http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
18http://sourceforge.net/projects/
mstparser
19http://ufal.mff.cuni.cz/conll2009-st
13
Rank System Average Catalan Chinese Czech English German Japanese Spanish
1 Che 82.64 81.84 76.38 83.27 87.00 82.44 85.65 81.90
2 Chen 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
3 Merlo 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43
4 Bohnet 80.85 80.44 75.91 79.57 85.14 81.60 82.51 80.75
5 Asahara 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12
6 Brown 77.27 77.40 72.12 75.66 83.98 77.86 76.65 77.21
7 Zhang 76.49 75.00 73.42 76.93 82.88 73.76 78.17 75.25
8 Dai 73.98 72.09 72.72 67.14 81.89 75.00 80.89 68.14
9 Lu Li 73.97 71.32 65.53 75.85 81.92 70.93 80.49 71.72
10 Llu??s 71.49 56.64 66.18 75.95 81.69 72.31 81.76 65.91
11 Vallejo 70.81 73.75 67.16 60.50 78.19 67.51 77.75 70.78
12 Ren 67.81 59.42 75.90 60.18 77.83 65.77 77.63 57.96
13 Zeman 51.07 49.61 43.50 57.95 50.27 49.57 57.69 48.90
Table 5: Official results of the Joint task, closed challenge. Teams are denoted by the last name (first name added
only where needed) of the author who registered for the evaluation data. Results are sorted in descending order of the
language-averaged macro F1 score on the closed challenge Joint task. Bold numbers denote the best result for a given
language.
Rank Rank in task System Average Catalan Chinese Czech English German Japanese Spanish
1 1 (SRLonly) Zhao 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
2 2 (SRLonly) Nugues 80.31 80.01 78.60 85.41 85.63 79.71 76.30 76.52
3 1 (Joint) Chen 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
4 2 (Joint) Che 79.94 77.10 77.15 86.51 85.51 78.61 78.26 76.47
5 3 (Joint) Merlo 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19
6 3 (SRLonly) Meza-Ruiz 77.46 78.00 77.73 75.75 83.34 73.52 76.00 77.91
7 4 (Joint) Bohnet 76.00 74.53 75.29 79.02 80.39 75.72 72.76 74.31
8 5 (Joint) Asahara 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50
9 6 (Joint) Brown 72.85 72.18 72.43 78.02 80.43 73.40 61.57 71.95
10 7 (Joint) Dai 70.78 66.34 71.57 75.50 78.93 67.43 71.02 64.64
11 8 (Joint) Zhang 70.31 67.34 73.20 78.28 77.85 62.95 64.71 67.81
12 9 (Joint) Lu Li 69.72 66.95 67.06 79.08 77.17 61.98 69.58 66.23
13 4 (SRLonly) Baoli Li 69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54
14 10 (Joint) Vallejo 68.95 70.14 66.71 71.49 75.97 61.01 68.82 68.48
15 5 (SRLonly) Moreau 66.49 65.60 67.37 71.74 72.14 66.50 57.75 64.33
16 11 (Joint) Llu??s 63.06 46.79 59.72 76.90 75.86 62.66 71.60 47.88
17 6 (SRLonly) Ta?ckstro?m 61.27 57.11 63.41 71.05 67.64 53.42 54.74 61.51
18 7 (SRLonly) Lin 57.18 61.70 70.33 60.43 65.66 59.51 23.78 58.87
19 12 (Joint) Ren 56.69 41.00 72.58 62.82 67.56 54.31 58.73 39.80
20 13 (Joint) Zeman 32.14 24.19 34.71 58.13 36.05 16.44 30.13 25.36
Table 6: Official results of the semantic labeling, closed challenge, all systems. Teams are denoted by the last name
(first name added only where needed) of the author who registered for the evaluation data. Results are sorted in
descending order of the semantic labeled F1 score (closed challenge). Bold numbers denote the best result for a given
language. Separate ranking is provided for SRL-only systems.
The second column of the table highlights the over-
all architectures. We used + to indicate that the
components are sequentially connected. The lack of
a + sign indicates that the corresponding tasks are
performed jointly.
It is perhaps not surprising that most of the obser-
vations from the 2008 shared task still hold; namely,
the best systems overall do not use joint learning or
optimization (the best such system was placed third
in the Joint task, and there were only four systems
where the learning methodology can be considered
?joint?).
Therefore, most of the observations and conclu-
sions from 2008 shared task hold as well for the
current results. For details, we will leave it to the
reader to interpret the architectures and methods
14
O
v
er
a
ll
D
D
D
PA
PA
PA
Jo
in
t
M
L
Sy
st
em
a
A
rc
h.
b
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
c
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
Le
a
rn
in
g/
O
pt
.
M
et
ho
ds
Zh
ao
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
/g
lo
ba
l
se
ar
ch
(S
R
L-
o
n
ly
)
M
E
N
u
gu
es
(P
C+
A
I+
A
C)
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
be
am
se
ar
ch
+
re
ra
n
ki
n
g
(S
R
L-
o
n
ly
)
L2
-
re
gu
la
riz
ed
lin
.
re
gr
es
sio
n
Ch
en
P
+
PC
+
A
I+
A
C
gr
ap
h
pa
rt
ia
lly
M
ST
C
L
/E
cl
as
s
n
o
gr
ee
dy
(?)
n
o
M
E
Ch
e
D
+
PC
+
A
IC
gr
ap
h
n
o
M
ST
H
O
E
cl
as
s
n
o
IL
P
n
o
SV
M
,
M
E
M
er
lo
D
PA
IC
+
D
ge
n
er
at
iv
e,
tr
an
s
n
o
be
am
se
ar
ch
tr
an
s
n
o
be
am
se
ar
ch
sy
n
ch
ro
n
iz
ed
de
riv
at
io
n
IS
B
N
M
ez
a-
R
u
iz
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
M
ar
ko
v
LN
n
o
Cu
tti
n
g
Pl
an
e
(S
R
L-
o
n
ly
)
M
IR
A
B
o
hn
et
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
C
+
re
ar
ra
n
ge
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
IR
A
)
A
sa
ha
ra
D
+
PI
C
+
A
IC
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
n
-
be
st
re
la
x
.
n
o
pe
rc
ep
tr
o
n
D
ai
D
+
PC
+
A
C
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
pr
o
b
ite
ra
tiv
e
M
E
Zh
an
g
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
E
cl
as
s
n
o
cl
as
sifi
ca
tio
n
n
o
M
IR
A
,
M
E
Lu
Li
D
+
(P
C
||
A
IC
)
gr
ap
h
fo
r
ea
ch
la
n
g.
M
ST
C
L
/E
,
M
ST
E
cl
as
s
n
o
gr
ee
dy
n
o
M
E
B
ao
li
Li
PC
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
,
kN
N
,
M
E
Va
lle
jod
[D
+
P+
A
]C
+
D
I
cl
as
s
n
o
re
ra
n
ki
n
g
cl
as
s
n
o
re
ra
n
ki
n
g
u
n
ifi
ed
la
be
ls
M
B
L
M
o
re
au
D
+
PI
+
Cl
u
st
er
in
g
+
A
I+
A
C
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
CR
F
(S
R
L-
o
n
ly
)
CR
F
Ll
u
??s
D
+
D
A
IC
+
PC
gr
ap
h
n
o
M
ST
E
gr
ap
h
n
o
M
ST
E
ye
s,
M
ST
E
Av
g.
Pe
rc
ep
tr
o
n
Ta?
ck
st
ro?
m
D
+
PI
+
A
I
+
A
C
+
Co
n
st
ra
in
tS
at
isf
ac
tio
n
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
R
en
D
+
PC
+
A
IC
tr
an
s
n
o
gr
ee
dy
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
al
t),
M
E
Ze
m
an
D
I+
D
C+
PC
+
A
I+
A
C
tr
an
s
n
o
gr
ee
dy
w
ith
he
u
ris
tic
s
cl
as
s
n
o
gr
ee
dy
n
o
co
o
cc
u
rr
en
ce
Ta
bl
e
7:
Su
m
m
ar
y
o
fs
ys
te
m
ar
ch
ite
ct
u
re
s
fo
r
th
e
Co
N
LL
-
20
09
sh
ar
ed
ta
sk
;
al
ls
ys
te
m
s
ar
e
in
cl
u
de
d.
SR
L-
o
n
ly
sy
st
em
s
do
n
o
t
ha
v
e
th
e
D
co
lu
m
n
s
an
d
th
e
Jo
in
t
Le
ar
in
g/
O
pt
.
co
lu
m
n
s
fil
le
d
in
.
Th
e
sy
st
em
s
ar
e
so
rt
ed
by
th
e
se
m
an
tic
la
be
le
d
F 1
sc
o
re
av
er
ag
ed
o
v
er
al
lt
he
la
n
gu
ag
es
(sa
m
e
as
in
Ta
bl
e
6).
O
n
ly
th
e
sy
st
em
s
th
at
ha
v
e
a
co
rr
es
po
n
di
n
g
pa
pe
r
in
th
e
pr
o
ce
ed
in
gs
ar
e
in
cl
u
de
d.
A
cr
o
n
ym
s
u
se
d:
D
-
sy
n
ta
ct
ic
de
pe
n
de
n
ci
es
,
P
-
pr
ed
ic
at
e,
A
-
ar
gu
m
en
t,
I-
id
en
tifi
ca
tio
n
,
C
-
cl
as
sifi
ca
tio
n
.
O
v
er
a
ll
a
rc
h.
st
an
ds
fo
r
th
e
co
m
pl
et
e
sy
st
em
ar
ch
ite
ct
u
re
;D
A
rc
h.
st
an
ds
fo
r
th
e
ar
ch
ite
ct
u
re
o
ft
he
sy
n
ta
ct
ic
pa
rs
er
;D
C
o
m
b.
in
di
ca
te
s
if
th
e
fin
al
pa
rs
er
o
u
tp
u
tw
as
ge
n
er
at
ed
u
sin
g
pa
rs
er
co
m
bi
n
at
io
n
;D
In
fe
re
n
ce
st
an
ds
fo
r
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
sy
n
ta
ct
ic
pa
rs
in
g;
PA
A
rc
h.
st
an
ds
th
e
ty
pe
o
fa
rc
hi
te
ct
u
re
u
se
d
fo
r
PA
IC
;P
A
C
o
m
b.
in
di
ca
te
s
if
th
e
PA
o
u
tp
u
t
w
as
ge
n
er
at
ed
th
ro
u
gh
sy
st
em
co
m
bi
n
at
io
n
;P
A
In
fe
re
n
ce
st
an
ds
fo
r
th
e
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
PA
IC
;J
o
in
tL
ea
rn
in
g/
O
pt
.
in
di
ca
te
s
if
so
m
e
fo
rm
o
fjo
in
tl
ea
rn
in
g
o
r
o
pt
im
iz
at
io
n
w
as
im
pl
em
en
te
d
fo
r
th
e
sy
n
ta
ct
ic
+
se
m
an
tic
gl
o
ba
lt
as
k;
M
L
M
et
ho
ds
lis
ts
th
e
M
L
m
et
ho
ds
u
se
d
th
ro
u
gh
o
u
tt
he
co
m
pl
et
e
sy
st
em
.
a
A
u
th
o
rs
o
ft
w
o
sy
st
em
s:
?
B
ro
w
n
?
an
d
?
Li
n
?
di
dn
?
ts
u
bm
it
a
pa
pe
r,
so
th
ei
r
sy
st
em
s?
ar
ch
ite
ct
u
re
s
ar
e
u
n
kn
ow
n
.
b T
he
sy
m
bo
l+
in
di
ca
te
s
se
qu
en
tia
lp
ro
ce
ss
in
g
(ot
he
rw
ise
,
pa
ra
lle
l/jo
in
t).
Th
e
||
m
ea
n
s
th
at
se
v
er
al
di
ffe
re
n
ta
rc
hi
te
ct
u
re
s
sp
an
n
in
g
m
u
lti
pl
e
su
bt
as
ks
ra
n
in
pa
ra
lle
l.
c
M
ST
C
L
/E
as
u
se
d
by
M
cD
o
n
al
d
(20
05
),
M
ST
C
by
Ca
rr
er
as
(20
07
),M
ST
E
by
Ei
sn
er
(20
00
),
M
ST
H
O
E
=
M
ST
E
w
ith
hi
gh
er
-
o
rd
er
fe
at
u
re
s
(si
bl
in
gs
+
al
lg
ra
n
dc
hi
ld
re
n
).
d T
he
sy
st
em
u
n
ifi
es
th
e
sy
n
ta
ct
ic
an
d
se
m
an
tic
la
be
ls
in
to
o
n
e
la
be
l,
an
d
tr
ai
n
s
cl
as
sifi
er
s
o
v
er
th
em
.
It
is
th
u
s
di
ffi
cu
lt
to
sp
lit
th
e
sy
st
em
ch
ar
ac
te
ris
tic
in
to
a
?
D
?
/?
PA
?
pa
rt
.
15
when comparing Table 7 with the Tables 5 and 6).
6 Conclusion
This year?s task has been demanding in several re-
spects, but certainly the most difficulty came from
the fact that participants had to tackle all seven lan-
guages. It is encouraging that despite this added af-
fort the number of participating systems has been
almost the same as last year (20 vs. 22 in 2008).
There are several positive outcomes from this
year?s enterprise:
? we have prepared a unified format and data for
several very different lanaguages, as a basis
for possible extensions towards other languages
and unified treatment of syntactic depenndecies
and semantic role labeling across natural lan-
guages;
? 20 participants have produced SRL results for
all seven languages, using several different
methods, giving hope for a combined system
with even substantially better performance;
? initial results have been provided for three lan-
guages on out-of-domain data (being in fact
quite close to the in-domain results).
Only four systems tried to apply what can be de-
scribed as joint learning for the syntactic and seman-
tic parts of the task. (Morante et al, 2009) use a true
joint learning formulation that phrases syntactico-
semantic parsing as a series of classification where
the class labels are concatenations of syntactic and
semantic edge labels. They predict (a), the set of
syntactico-semantic edge labels for each pair of to-
kens; (b), the set of incoming syntactico-semantic
edge labels for each individual token; and (c), the
existence of an edge between each pair of tokens.
Subsequently, they combine the (possibly conflict-
ing) output of the three classifiers by a ranking ap-
proach to determine the most likely structure that
meets all well-formedness constraints. (Llu??s et al,
2009) present a joint approach based on an exten-
sion of Eisner?s parser to accommodate also seman-
tic dependency labels. This architecture is similar
to the one presented by the same authors in the past
edition, with the extension to a second-order syn-
tactic parsing and a particular setting for Catalan
and Spanish. (Gesmundo et al, 2009) use an in-
cremental parsing model with synchronous syntac-
tic and semantic derivations and a joint probability
model for syntactic and semantic dependency struc-
tures. The system uses a single input queue but two
separate stacks and synchronizes syntactic and se-
mantic derivations at every word. The synchronous
derivations are modeled with an Incremental Sig-
moid Belief Network that has latent variables for
both syntactic and semantic states and connections
from syntax to semantics and vice versa. (Dai et
al., 2009) designed an iterative system to exploit
the inter-connections between the different subtasks
of the CoNLL shared task. The idea is to decom-
pose the joint learning problem into four subtasks
? syntactic dependency identification, syntactic de-
pendency labeling, semantic dependency identifica-
tion and semantic dependency labeling. The initial
step is to use a pipeline approach to use the input of
one subtask as input to the next, in the order speci-
fied. The iterative steps then use additional features
that are not available in the initial step to improve the
accuracy of the overall system. For example, in the
iterative steps, semantic information becomes avail-
able as features to syntactic parsing, so on and so
forth.
Despite these results, it is still not clear whether
joint learning has a significant advantage over other
approaches (and if yes, then for what languages). It
is thus necessary to carefully plan the next shared
tasks; it might be advantageous to bring up a sim-
ilar task in the future once again, and/or couple it
with selected application(s). There, (we hope) the
benefits of the dependency representation combined
with semantic roles the way we have formulated it
in 2008 and 2009 will really show up.
Acknowledgments
We would like to thank the Linguistic Data Consor-
tium, mainly to Denise DiPersio, Tony Casteletto
and Christopher Cieri for their help and handling
of invoicing and distribution of the data for which
LDC has a license. For all of the trial, training and
evaluation data they had to act a very short notice.
All the data has been at the participants? disposal
(again) free of charge. We are grateful to all of them
for LDC?s continuing support of the CoNLL Shared
16
Tasks.
We would also like to thank organizers of the pre-
vious four shared tasks: Sabine Buchholz, Xavier
Carreras, Ryan McDonald, Amit Dubey, Johan Hall,
Yuval Krymolowski, Sandra Ku?bler, Erwin Marsi,
Jens Nilsson, Sebastian Riedel and Deniz Yuret.
This shared task would not have been possible with-
out their previous effort.
We also acknowledge the support of the M?SMT
of the Czech Republic, projects MSM0021620838
and LC536; the Grant Agency of the Academy of
sciences of the Czech Republic 1ET201120505 (for
Jan Hajic?, Jan ?Ste?pa?nek and Pavel Stran?a?k).
Llu??s Ma`rquez and M. Anto`nia Mart?? partici-
pation was supported by the Spanish Ministry of
Education and Science, through the OpenMT and
TextMess research projects (TIN2006-15307-C03-
02, TIN2006-15265-C06-06).
The following individuals directly contributed to
the Chinese Treebank (in alphabetic order): Meiyu
Chang, Fu-Dong Chiou, Shizhe Huang, Zixin Jiang,
Tony Kroch, Martha Palmer, Mitch Marcus, Fei
Xia, Nianwen Xue. The contributors to the Chi-
nese Proposition Bank include (in alphabetic order):
Meiyu Chang, Gang Chen, Helen Chen, Zixin Jiang,
Martha Palmer, Zhiyi Song, Nianwen Xue, Ping Yu,
Hua Zhong. The Chinese Treebank and the Chinese
Proposition Bank were funded by DOD, NSF and
DARPA.
Adam Meyers? work on the shared task has been
supported by the NSF Grant IIS-0534700 ?Structure
Alignment-based MT.?
We thank the Mainichi Newspapers for the per-
mission of distributing the sentences of the Kyoto
University Text Corpus for this shared task.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
EMNLP-CoNLL 2007, pages 957?961, June. Prague,
Czech Republic.
Montserrat Civit, M. Anto`nia Mart??, and Nu?ria Buf??.
2006. Cat3LB and Cast3LB: from constituents to
dependencies. In Proceedings of the 5th Interna-
tional Conference on Natural Language Processing,
FinTAL, pages 141?153, Turku, Finland. Springer Ver-
lag, LNAI 4139.
Qifeng Dai, Enhong Chen, and Liu Shi. 2009. An it-
erative approach for joint dependency parsing and se-
mantic role labeling. In Proceedings of the 13th Con-
ference on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
June 4-5.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Tehcnologies, pages 29?62. Kluwer Academic
Publishers.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cambridge.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5, Boulder, Colorado, USA. June 4-5.
Jan Hajic?, Jarmila Panevova?, Zden?ka Ures?ova?, Alevtina
Be?mova?, Veronika Kola?r?ova?- ?Rezn??c?kova??, and Petr
Pajas. 2003. PDT-VALLEX: Creating a Large-
coverage Valency Lexicon for Treebank Annotation.
In J. Nivre and E. Hinrichs, editors, Proceedings of The
Second Workshop on Treebanks and Linguistic Theo-
ries, pages 57?68, Vaxjo, Sweden. Vaxjo University
Press.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k ?Zabokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Xavier Llu??s, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA. June 4-5.
17
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
Llu??s Ma`rquez, Luis Villarejo, M. Anto`nia Mart??, and
Mariona Taule?. 2007. SemEval-2007 Task 09: Mul-
tilevel semantic annotation of catalan and spanish.
In Proceedings of the 4th International Workshop on
Semantic Evaluations (SemEval-2007), pages 42?47,
Prague, Czech Republic.
M. Anto`nia Mart??, Mariona Taule?, Llu??s Ma`rquez, and
Manu Bertran. 2007. Anotacio?n semiautoma?tica
con papeles tema?ticos de los corpus CESS-ECE.
Procesamiento del Lenguaje Natural, SEPLN Journal,
38:67?76.
Ryan McDonald, Fernando Pereira, Jan Hajic?, and Kiril
Ribarov. 2005. Non-projective dependency parsing
using spanning tree algortihms. In Proceedings of
NAACL-HLT?05, Vancouver, Canada, pages 523?530.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
Roser Morante, Vincent Van Asch, and Antal van den
Bosch. 2009. A simple generative pipeline approach
to dependency parsing and semantic role labeling. In
Proceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), Boulder,
Colorado, USA. June 4-5.
Joakim Nivre, Johann Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on depen-
dency parsing. In Proceedings of the EMNLP-CoNLL
2007 Conference, pages 915?932, Prague, Czech Re-
public.
Sebastian Pado and Mirella Lapata. 2005. Cross-lingual
projection of role-semantic information. In Proceed-
ings of the Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP-2005), pages 859?
866, Vancouver, BC.
Petr Pajas and Jan ?Ste?pa?nek. 2008. Recent advances in
a feature-rich framework for treebank annotation. In
The 22nd International Conference on Computational
Linguistics - Proceedings of the Conference (COL-
ING?08), pages 673?680, Manchester.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the European ACL Cenference EACL?09,
Athens, Greece.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008), pages 159?177.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Martin ?Cmejrek, Jan Cur???n, Jan Hajic?, Jir??? Havelka,
and Vladislav Kubon?. 2004. Prague Czech-English
Dependency Treebank: Syntactically Anntoated Re-
sources for Machine Translation. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC-2004), pages 1597?
1600, Lisbon, Portugal.
W. N. Francis and H. Kucera. 1964. Brown Corpus Man-
ual of Information to accompany A Standard Corpus
of Present-Day Edited American English, for use with
Digital Computers. Revised 1971, Revised and Am-
plified 1979, available at www.clarinet/brown.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Lin- guistic Data Consortium.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Andrea Zielinski and Christian Simon. 2008. Morphisto:
An open-source morphological analyzer for german.
In Proceedings of the Conference on Finite State Meth-
ods in Natural Language Processing.
18
Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 1?9,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
Multi-word expressions in textual inference: Much ado about nothing?
Marie-Catherine de Marneffe
Linguistics Department
Stanford University
Stanford, CA
mcdm@stanford.edu
Sebastian Pad
?
o
Institut f?ur Maschinelle
Sprachverarbeitung
Stuttgart University, Germany
pado@ims.uni-stuttgart.de
Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA
manning@stanford.edu
Abstract
Multi-word expressions (MWE) have seen much at-
tention from the NLP community. In this paper, we
investigate their impact on the recognition of tex-
tual entailment (RTE). Using the manual Microsoft
Research annotations, we first manually count and
classify MWEs in RTE data. We find few, most
of which are arguably unlikely to cause processing
problems. We then consider the impact of MWEs on
a current RTE system. We are unable to confirm that
entailment recognition suffers from wrongly aligned
MWEs. In addition, MWE alignment is difficult
to improve, since MWEs are poorly represented in
state-of-the-art paraphrase resources, the only avail-
able sources for multi-word similarities. We con-
clude that RTE should concentrate on other phe-
nomena impacting entailment, and that paraphrase
knowledge is best understood as capturing general
lexico-syntactic variation.
1 Introduction
Multi-word expressions (MWEs) can be defined as
?idiosyncratic interpretations that cross word bound-
aries?, such as traffic light or kick the bucket. Called
a ?pain in the neck for NLP?, they have received
considerable attention in recent years and it has
been suggested that proper treatment could make
a significant difference in various NLP tasks (Sag
et al, 2002). The importance attributed to them is
also reflected in a number of workshops (Bond et
al., 2003; Tanaka et al, 2004; Moir?on et al, 2006;
Gr?egoire et al, 2007). However, there are few de-
tailed breakdowns of the benefits that improved
MWE handling provides to applications.
This paper investigates the impact of MWEs
on the ?recognition of textual entailment? (RTE)
task (Dagan et al, 2006). Our analysis ties in with
the pivotal question of what types of knowledge
are beneficial for RTE. A number of papers have
suggested that paraphrase knowledge plays a very
important role (Bar-Haim et al, 2005; Marsi et al,
2007; Dinu and Wang, 2009). For example, Bar-
Haim et al (2005) conclude: ?Our analysis also
shows that paraphrases stand out as a dominant
contributor to the entailment task.?
The term ?paraphrase? is however often con-
strued broadly. In Bar-Haim et al (2005), it refers
to the ability of relating lexico-syntactic reformula-
tions such as diathesis alternations, passivizations,
or symmetrical predicates (X lent his BMW to Y/Y
borrowed X?s BMW). If ?paraphrase? simply refers
to the use of a language?s lexical and syntactic
possibilities to express equivalent meaning in dif-
ferent ways, then paraphrases are certainly impor-
tant to RTE. But such a claim means little more
than that RTE can profit from good understand-
ing of syntax and semantics. However, given the
abovementioned interest in MWEs, there is another
possibility: does success in RTE involve proper
handling of MWEs, such as knowing that take a
pass on is equivalent to aren?t purchasing, or kicked
the bucket to died? This seems not too far-fetched:
Knowledge about MWEs is under-represented in
existing semantic resources like WordNet or dis-
tributional thesauri, but should be present in para-
phrase resources, which provide similarity judg-
ments between phrase pairs, including MWEs.
The goal of our study is to investigate the merits
of this second, more precise, hypothesis, measur-
ing the impact of MWE processing on RTE. In
the absence of a universally accepted definition
of MWEs, we define MWEs in the RTE setting
as multi-word alignments, i.e., words that partici-
pate in more than one word alignment link between
premise and hypothesis:
(1)
PRE: He died.
HYP: He kicked the bucket.
The exclusion of MWEs that do not lead to multi-
word alignments (i.e., which can be aligned word
by word) is not a significant loss, since these cases
are unlikely to cause significant problems for RTE.
In addition, an alignment-based approach has the
advantage of generality: Almost all existing RTE
models align the linguistic material of the premise
1
and hypothesis and base at least part of their de-
cision on properties of this alignment (Burchardt
et al, 2007; Hickl and Bensley, 2007; Iftene and
Balahur-Dobrescu, 2007; Zanzotto et al, 2007).
We proceed in three steps. First, we analyze
the Microsoft Research (MSR) manual word align-
ments (Brockett, 2007) for the RTE2 dataset (Bar-
Haim et al, 2006), shedding light on the rela-
tionship between alignments and multi-word ex-
pressions. We provide frequency estimates and
a coarse-grained classification scheme for multi-
word expressions on textual entailment data. Next,
we analyze two widely used types of paraphrase
resources with respect to their modeling of MWEs.
Finally, we investigate the impact of MWEs and
their handling on practical entailment recognition.
2 Multi-Word Expressions in Alignment
Almost all textual entailment recognition models
incorporate an alignment procedure that establishes
correspondences between the premise and the hy-
pothesis. The computation of word alignments
is usually phrased as an optimization task. The
search space is based on lexical similarities, but
usually extended with structural biases in order to
obtain alignments with desirable properties, such
as the contiguous alignment of adjacent words, or
the mapping of different source words on to differ-
ent target words. One prominent constraint of the
IBM word alignment models (Brown et al, 1993)
is functional alignment, that is each target word
is mapped onto at most one source word. Other
models produce only one-to-one alignments, where
both alignment directions must be functional.
MWEs that involve many-to-many or one-to-
many alignments like Ex. (1) present a problem
for such constrained word alignment models. A
functional alignment model can still handle cases
like Ex. (1) correctly in one direction (from bottom
to top), but not in the other one. One-to-one align-
ments manage neither. Various workarounds have
been proposed in the MT literature, such as comput-
ing word alignments in both directions and forming
the union or intersection. Even if an alignment is
technically within the search space, accurate knowl-
edge about plausible phrasal matches is necessary
for it to be assigned a high score and thus identified.
3 MWEs in the RTE2 Dataset
In the first part of our study, we estimate the extent
to which the inability of aligners to model one-to-
CARDINALITY
M-to-M 1-to-M
DECOM- yes (1) (3)
POSABLE? no (2) (4)
OTHER (5), (6), (7)
Table 1: MWEs categories and definition criteria
(M-to-M: many-to-many; 1-to-M: one-to-many).
many and many-to-many correspondences is an
issue. To do so, we use the Microsoft Research
manual alignments for the RTE2 data. To date, the
MSR data constitutes the only gold standard align-
ment corpus publicly available. Since annotators
were not constrained to use one-to-one alignments,
we assume that the MSR alignments contain multi-
word alignments where appropriate.
From the MSR data, we extract all multi-word
alignments that fall outside the scope of ?func-
tional? alignments, i.e., alignments of the form
?many-to-many? or ?one-to-many? (in the direction
hypothesis-premise). We annotate them according
to the categories defined below. The MSR data
distinguishes between SURE and POSSIBLE align-
ments. We only take the SURE alignments into
account. While this might mean missing some
multi-word alignments, we found many ?possible?
links to be motivated by the desire to obtain a high-
coverage alignment, as Ex. 2 shows:
(2)
PRE: ECB spokeswoman, Regina Schueller, ...
HYP: Regina Schueller ...
Here, the hypothesis words ?Regina Schueller? are
individually ?sure?-aligned to the premise words
?Regina Schueller? (solid lines), but are also both
?possible?-linked to ?ECB spokeswoman? (dashed
lines). This ?possible? alignment can be motivated
on syntactic or referential grounds, but does not
indicate a correspondence in meaning (as opposed
to reference).
3.1 Analysis of Multi-Word Expressions
Table 1 shows the seven categories we define to
distinguish the different types of multi-word align-
ments. We use two main complementary criteria
for our annotation. The first one is the cardinality
of the alignment: does it involve phrases proper
on both sides (many-to-many), or just on one side
(one-to-many)? The second one is decomposabil-
ity: is it possible to create one or more one-to-one
alignments that capture the main semantic contribu-
tion of the multi-word alignment? Our motivation
2
for introducing this criterion is that even aligners
that are unable to recover the complete MWE have
a chance to identify the links crucial for entailment
if the MWE is decomposable (categories (1) and
(3)). This is not possible for the more difficult
non-decomposable categories (2) and (4). The re-
maining categories, (5) to (7), involve auxiliaries,
multiple mentions, and named entities, which are
not MWEs in the narrow sense. We will henceforth
use the term ?true MWEs? to refer to categories
(1)?(4), as opposed to (5)?(7).
The criteria we use for MWE categorization are
different from the ones adopted by Sag et al (2002).
Sag et al?s goal is to classify constructions by their
range of admissible variation, and thus relies heav-
ily on syntactic variability. Since we are more inter-
ested in semantic properties, we base our classes on
alignment patterns, complemented by semantic de-
composability judgments (which reflect the severity
of treating MWEs like compositional phrases). As
mentioned in Section 1, our method misses MWEs
aligned with one-to-one links; however, the use of
a one-to-one link by the annotation can be seen as
evidence for decomposability.
A. Multiple words on both sides
(1) Compositional phrases (CP):
Each word in the left phrase can be aligned to one
word in the right phrase, e.g., capital punishment
? death penalty for which capital can be aligned
to death and punishment to penalty.
(2) Non-compositional phrases (NCP):
There is no simple way to align words between the
two phrases, such as in poorly represented? very
few or illegally entered? broke into.
B. One word to multiple words
(3) Headed multi-word expressions (MWEH):
A single word can be aligned with one token of
an MWE: e.g., vote? cast ballots where ballots
carries enough of the semantics of vote.
(4) Non-headed MWEs (MWENH):
The MWE as a whole is necessary to capture the
meaning of the single word, which doesn?t align
well to any individual word of the MWE: e.g., ferry
? passenger vessel.
(5) Multiple mentions (MENTION):
These alignments link one word to multiple occur-
rences of the same or related word(s) in the text,
e.g., military? forces ... Marines, antibiotics?
Status Category RTE2 dev RTE2 test
decomp. CP 5 0
MWEH 40 31
non- NCP 6 0
decomp. MWENH 30 29
Subtotal: True MWEs 81 60
other MENTION 26 48
PART 82 54
AUX 0 2
Total: All MWEs 189 164
Table 2: Frequencies of sentences with different
multi-word alignment categories in MSR data.
antibiotics ... drug.
(6) Parts of named entities (PART):
Each element of a named entity is aligned to the
whole named entity: e.g., Shukla? Nidhi Shukla.
This includes the use of acronyms or abbreviations
on one side and their spelled-out forms on the other
side, such as U.S.? United States.
(7) Auxiliaries (AUX):
The last category involves the presence of an auxil-
iary: e.g., were? are being.
Initially, one of the authors used these categories
to analyze the complete RTE2 MSR data (dev and
test sets). The most difficult distinction to draw
was, not surprisingly, the decision between decom-
posable multi-word alignments (categories (1) and
(3)) and non-decomposable ones (categories (2)
and (4)). To ascertain that a reliable distinction
can be made, another author did an independent
second analysis of the instances from categories
(1) through (4). We found moderate inter-annotator
agreement (? = 0.60), indicating that not all, but
most annotation decisions are uncontroversial.
3.2 Distribution of Multi-Word Expressions
Table 2 shows the distribution in the MSR data
of all alignment categories. Our evaluation will
concentrate on the ?true MWE? categories (1) to
(4): CP, NCP, MWEH and MWENH.
1
1
The OTHER categories (5) to (7) can generally be dealt
with during pre- or post-processing: Auxiliary-verb combi-
nations (cat. 7) are usually ?headed? so that it is sufficient to
align the main verb; multiple occurrences of words referring
to the same entity (cat. 5) is an anaphor resolution problem;
and named-entity matches (cat. 6) are best solved by using a
named entity recognizer to collapse NEs into a single token.
3
In RTE2 dev and test, we find only 81 and 60
true MWEs, respectively. Out of the 1600 sentence
pairs in the two datasets, 8.2% involve true MWEs
(73 in RTE2 dev and 58 in RTE2 test). On the level
of word alignments, the ratio is even smaller: only
1.2% of all SURE alignments involve true MWEs.
Furthermore, more than half of them are decom-
posable (MWEH/CP). Some examples from this
category are (?heads? marked in boldface):
sue? file lawsuits against
diseases? liver cancer
Barbie? Barbie doll
got? was awarded with
works? executive director
military? naval forces
In particular when light verbs are involved (file
lawsuits) or when modification adds just minor
meaning aspects (executive director), we argue that
it is sufficient to align the left-hand expression to
the ?head? in order to decide entailment.
Consider, in contrast, these examples from the
non-decomposable categories (MWENH/NCP):
politician? presidential candidate
killed? lost their lives
shipwreck? sunken ship
ever? in its history
widow? late husband
sexes? men and women
These cases span a broad range of linguistic rela-
tions from pure associations (widow/late husband)
to collective expressions (sexes/men and women).
Arguably, in these cases aligning the left-hand word
to any single word on the right can seriously throw
off an entailment recognition system. However,
they are fairly rare, occurring only in 65 out of
1600 sentences.
3.3 Conclusions from the MSR Analysis
Our analysis has found that 8% of the sentences
in the MSR dataset involve true MWEs. At the
word level, the fraction of true MWEs of all SURE
alignment links is just over 1%.
Of course, if errors in the alignment of these
MWEs had a high probability to lead to entailment
recognition errors, MWEs would still constitute a
major factor in determining entailment. However,
we have argued that about half of the true MWEs
are decomposable, that is, the part of the alignment
that is crucial for entailment can be recovered with
a one-to-one alignment link that can be identified
even by very limited alignment models.
This leaves considerably less than 1% of all word
alignments (or ?4% of sentence pairs) where im-
perfect MWE alignments are able at all to exert a
negative influence on entailment. However, this is
just an upper bound ? their impact is by no means
guaranteed. Thus, our conclusion from the annota-
tion study is that we do not expect MWEs to play a
large role in actual entailment recognition.
4 MWEs in Paraphrase Resources
Before we come to actual experiments on the au-
tomatic recognition of MWEs in a practical RTE
system, we need to consider the prerequisites for
this task. As mentioned in Section 2, if an RTE
system is to establish multi-word alignments, it re-
quires a knowledge source that provides accurate
semantic similarity judgments for ?many-to-many?
alignments (capital punishment ? death penalty)
as well as for ?one-to-many? alignments (vote ?
cast ballots). Such similarities are not present in
standard lexical resources like WordNet or Dekang
Lin?s thesaurus (Lin, 1998).
The best class of candidate resources to provide
wide-coverage of multi-word similarities seems to
be paraphrase resources. In this section, we ex-
amine to what extent two of the most widely used
paraphrase resource types provide supporting ev-
idence for the true MWEs in the MSR data. We
deliberately use corpus-derived, noisy resources,
since we are interested in the real-world (rather
than idealized) prospects for accurate MWE align-
ment.
Dependency-based paraphrases. Lin and Pan-
tel (2002)?s DIRT model collects lexicalized de-
pendency paths with two slots at either end. Paths
with similar distributions over slot fillers count as
paraphrases, with the quality measured by a mutual
information-based similarity over the slot fillers.
The outcome of their study is the DIRT database
which lists paraphrases for around 230,000 depen-
dency paths, extracted from about 1 GB of mis-
cellaneous newswire text. We converted the DIRT
paraphrases
2
into a resource of semantic similari-
ties between raw text phrases. We used a heuristic
mapping from dependency relations to word or-
der, and obtained similarity ratings by rescaling the
DIRT paraphrase ratings, which are based on a mu-
tual information-based measure of filler similarity,
onto the range [0,1].
2
We thank Patrick Pantel for granting us access to DIRT.
4
Parallel corpora-based paraphrases. An alter-
native approach to paraphrase acquisition was pro-
posed by Bannard and Callison-Burch (2005). It
exploits the variance inherent in translation to ex-
tract paraphrases from bilingual parallel corpora.
Concretely, it observes translational relationships
between a source and a target language and pairs
up source language phrases with other source lan-
guage phrases that translate into the same target
language phrases. We applied this method to
the large Chinese-English GALE MT evaluation
P3/P3.5 corpus (?2 GB text per language, mostly
newswire). The large number of translations makes
it impractical to store all observed paraphrases. We
therefore filtered the list of paraphrases against the
raw text of the RTE corpora, acquiring the 10 best
paraphrases for around 100,000 two- and three-
word phrases. The MLE conditional probabilities
were scaled onto [0,1] for each target.
Analysis. We checked the two resources for the
presence of the true MWEs identified in the MSR
data. We found that overall 34% of the MWEs ap-
pear in these resources, with more decomposable
MWEs (MWEH/CP) than non-decomposable ones
(MWENH/NCP) (42.1% vs. 24.6%). However, we
find that almost all of the MWEs that are covered
by the paraphrase resources are assigned very low
scores, while erroneous paraphrases (expressions
with clearly different meanings) have higher scores.
This is illustrated in Table 3 for the case of poorly
represented, which is aligned to very few in one
RTE2 sentence. This paraphrase is on the list, but
with a lower similarity than unsuitable paraphrases
such as representatives or good. This problem is
widespread. Other examples of low-scoring para-
phrases are: another step? measures, quarantine
? in isolation, punitive measures ? sanctions,
held a position? served as, or inability? could
not.
The noise in the rankings means that any align-
ment algorithm faces a dilemma: either it uses a
high threshold and misses valid MWE alignments,
or it lowers its threshold and risks constructing
incorrect alignments.
5 Impact of MWEs on Practical
Entailment Recognition
This section provides the final step in our study: an
evaluation of the impact of MWEs on entailment
recognition in a current RTE system, and of the
benefits of explicit MWE alignment. While the
poorly represented
represented 0.42
poorly 0.07
rarely 0.06
good 0.05
representatives 0.04
very few 0.04
well 0.02
representative 0.01
Table 3: Paraphrases of ?poorly represented? with
scores (semantic similarities).
results of this experiment are not guaranteed to
transfer to other RTE system architectures, or to
future, improved paraphrase resources, it provides
a current snapshot of the practical impact of MWE
handling.
5.1 The Stanford RTE System
We base our experiments on the Stanford RTE sys-
tem which uses a staged architecture (MacCartney
et al, 2006). After the linguistic analysis which
produces dependency graphs for premise and hy-
pothesis, the alignment stage creates links between
the nodes of the two dependency trees. In the infer-
ence stage, the system produces roughly 70 features
for the aligned premise-hypothesis pair, almost all
of which are implementations of ?small linguistic
theories? whose activation indicates lexical, syn-
tactic and semantic matches and mismatches of
different types. The entailment decision is com-
puted using a logistic regression on these features.
The Stanford system supports the use of dif-
ferent aligners without touching the rest of the
pipeline. We compare two aligners: a one-to-one
aligner, which cannot construct MWE alignments
(UNIQ), and a many-to-many aligner (MANLI)
(MacCartney et al, 2008), which can. Both align-
ers use around 10 large-coverage lexical resources
of semantic similarities, both manually compiled
resources (such as WordNet and NomBank) and
automatically induced resources (such as Dekang
Lin?s distributional thesaurus or InfoMap).
UNIQ: A one-to-one aligner. UNIQ constructs
an alignment between dependency graphs as the
highest-scoring mapping from each word in the
hypothesis to one word in the premise, or to null.
Mappings are scored by summing the alignment
scores of all individual word pairs (provided by the
lexical resources), plus edge alignment scores that
5
use the syntactic structure of premise and hypoth-
esis to introduce a bias for syntactic parallelism.
The large number of possible alignments (expo-
nential in the number of hypothesis words) makes
exhaustive search intractable. Instead, UNIQ uses a
stochastic search based on Gibbs sampling, a well-
known Markov Chain Monte Carlo technique (see
de Marneffe et al (2007) for details).
Since it does not support many-to-many align-
ments, the UNIQ aligner cannot make use of the
multi-word information present in the paraphrase
resources. To be able to capture some common
MWEs, the Stanford RTE system was originally
designed with a facility to concatenate MWEs
present in WordNet into a single token (mostly
particle verbs and collocations, e.g., treat as or
foreign minister). However, we discovered that
WordNet collapsing always has a negative effect.
Inspection of the constructed alignments suggests
that the lexical resources that inform the alignment
process do not provide scores for most collapsed
tokens (such as wait for), and precision suffers.
MANLI: A phrase-to-phrase aligner. MANLI
aims at finding an optimal alignment between
phrases, defined as contiguous spans of one or mul-
tiple words. MANLI characterizes alignments as
edit scripts, sets of edits (substitutions, deletions,
and insertions) over phrases. The quality of an
edit script is the sum of the quality of the individ-
ual edit steps. Individual edits are scored using a
feature-based scoring function that takes edit type
and size into consideration.
3
The score for substi-
tution edits also includes a lexical similarity score
similar to UNIQ, plus potential knowledge about
the semantic relatedness of multi-word phrases not
expressible in UNIQ. Substitution edits also use
contextual features, including a distortion score
and a matching-neighbors feature.
4
Due to the
dependence between alignment and segmentation
decisions, MANLI uses a simulated annealing strat-
egy to traverse the resulting large search space.
Even though MANLI is our current best candi-
date at recovering MWE alignments, it currently
has an important architectural limitation: it works
on textual phrases rather than dependency tree frag-
ments, and therefore misses all MWEs that are not
contiguous (e.g., due to inserted articles or adver-
3
Positive weights for all operation types ensure that
MANLI prefers small over large edits where appropriate.
4
An adaptation of the averaged perceptron algorithm
(Collins, 2002) is used to tune the model parameters.
micro-avg
P R F
1
UNIQ w/o para 80.4 80.8 80.6
MANLI w/o para 77.0 85.5 81.0
w/ para 76.7 85.4 80.8
Table 4: Evaluation of aligners and resources
against the manual MSR RTE2 test annotations.
bials). This accounts for roughly 9% of the MWEs
in RTE2 data. Other work on RTE has targeted
specifically this observation and has described para-
phrases on a dependency level (Marsi et al, 2007;
Dinu and Wang, 2009).
Setup. To set the parameters of the two models
(i.e., the weights for different lexical resources for
UNIQ, and the weights for the edit operation for
MANLI), we use the RTE2 development data. Test-
ing takes place on the RTE2 test and RTE4 datasets.
For MANLI, we performed this procedure twice,
with the paraphrase resources described in Sec-
tion 4 once deactivated and once activated. We
evaluated the output of the Stanford RTE system
both on the word alignment level, and on the entail-
ment decision level.
5.2 Evaluation of Alignment Accuracy
The results for evaluating the MANLI and UNIQ
alignments against the manual alignment links in
the MSR RTE2 test set are given in Table 4. We
present micro-averaged numbers, where each align-
ment link counts equally (i.e., longer problems have
a larger impact). The overall difference is not large,
but MANLI produces a slightly better alignment.
The ability of MANLI to construct many-to-
many alignments is reflected in a different position
on the precision/recall curve: the MANLI aligner
is less precise than UNIQ, but has a higher recall.
Examples for UNIQ and MANLI alignments are
shown in Figures 1 and 2. A comparison of the
alignments shows the pattern to be expected from
Table 4: MANLI has a higher recall, but contains
occasional questionable links, such as at President
? President in Figure 1.
However, the many-to-many alignments that
MANLI produces do not correspond well to the
MWE alignments. The overall impact of the para-
phrase resources is very small, and their addition
actually hurts MANLI?s performance slightly. A
more detailed analysis revealed two contrary trends.
On the one hand, the paraphrase resources provide
6
Aligner w/o para w/ para
UNIQ 63.8 ?
MANLI 60.6 60.6
Table 5: Entailment recognition accuracy of the
Stanford system on RTE2 test (two-way task).
Aligner w/o para w/ para TAC system
UNIQ 63.3 ? 61.4
MANLI 59.0 57.9 57.0
Table 6: Entailment recognition accuracy of the
Stanford system on RTE4 (two-way task).
beneficial information, maybe surprisingly, in the
form of broad distributional similarities for single
words that were not available from the standard lex-
ical resources (e.g., the alignment ?the company?s
letter?? ?the company?s certificate?).
On the other hand, MANLI captures not one of
the true MWEs identified in the MSR data. It only
finds two many-to-many alignments which belong
to the CP category: aimed criticism ? has criti-
cised, European currency ? euro currency. We
see this as the practical consequences of our ob-
servation from Section 4: The scores in current
paraphrase resources are too noisy to support accu-
rate MWE recognition (cf. Table 3).
5.3 Evaluation of Entailment Recognition
We finally evaluated the performance of the Stan-
ford system using UNIQ and MANLI alignments
on the entailment task. We consider two datasets:
RTE2 test, the alignment evaluation dataset, and
the most recent RTE4 dataset, where current num-
bers for the Stanford system are available from last
year?s Text Analysis Conference (TAC).
A reasonable conjecture would be that better
alignments translate into better entailment recog-
nition. However, as the results in Tables 5 and 6
show, this is not the case. Overall, UNIQ outper-
forms MANLI by several percent accuracy despite
MANLI?s better alignments. This ?baseline? differ-
ence should not be overinterpreted, since it may be
setup-specific: the features computed in the infer-
ence stage of the Stanford system were developed
mainly with the UNIQ aligner in mind. A more sig-
nificant result is that the integration of paraphrase
knowledge in MANLI has no effect on RTE2 test,
and even decreases performance on RTE4.
The general picture that we observe is that
there is only a loose coupling between alignments
and the entailment decision: individual align-
ments seldom matter. This is shown, for exam-
ple, by the alignments in Figures 1 and 2. Even
though MANLI provides a better overall alignment,
UNIQ?s alignment is ?good enough? for entailment
purposes. In Figure 1, the two words UNIQ leaves
unaligned are a preposition (at) and a light verb
(aimed), both of which are not critical to determine
whether or not the premise entails the hypothesis.
This interpretation is supported by another analy-
sis, where we tested whether entailments involving
at least one true MWE are more difficult to rec-
ognize. We computed the entailment accuracy for
all applicable RTE2 test pairs (7%, 58 sentences).
The accuracy on this subset is 62% for the MANLI
model without paraphrases, 64% for the MANLI
model with paraphrases, and 74% for UNIQ. The
differences from the numbers in Table 5 are not
significant due to the small size of the MWE sam-
ple, but we observe that the accuracy on the MWE
subset tends to be higher than on the whole set
(rather than lower). Futhermore, even though we fi-
nally see a small beneficial effect of paraphrases on
the MANLI aligner, the UNIQ aligner, which com-
pletely ignores MWEs, still performs substantially
better.
Our conclusion is that wrong entailment deci-
sions rarely hinge on wrongly aligned MWEs, at
least with a probabilistic architecture like the Stan-
ford system. Consequently, it suffices to recover
the most crucial alignment links to predict entail-
ment, and the benefits associated with the use of
a more restricted alignment formulation, like the
one-to-one alignment formulation of UNIQ, out-
weighs those of more powerful alignment models,
like MANLI?s phrasal alignments.
6 Conclusions
We have investigated the influence of multi-word
expressions on the task of recognizing textual en-
tailment. In contrast to the widely held view that
proper treatment of MWEs could bring about a sub-
stantial improvement in NLP tasks, we found that
the importance of MWEs in RTE is rather small.
Among the MWEs that we identified in the align-
ments, more than half can be captured by one-to-
one alignments, and should not pose problems for
entailment recognition.
Furthermore, we found that the remaining
MWEs are rather difficult to model faithfully. The
MSR MWEs are poorly represented in state-of-the-
7
Former
South
African
President
aimed
criticismat
President
Bush
Form
er
SouthAfrica
n
Presid
ent
has critic
ised
Presid
ent
Geor
ge
Bush NULL Former
South
African
President
aimed
criticismat
President
Bush
Form
er
SouthAfrica
n
Presid
ent
has critic
ised
Presid
ent
Geor
ge
Bush NULL
Figure 1: UNIQ (left) and MANLI (right) alignments for problem 483 in RTE2 test. The rows represent
the hypothesis words, and the columns the premise words.
Forme
Sor
uetrheueA
furi
caum
inPosoahe
deuiaBeBs
GgBP
derdG
e
Sor mguh
nhe
caumiaN mgffe
udeuia
BeBs
GgBPAaiaP
e
UL??
AaiaPe
???
Forme
Sor
uetrheueA
furi
caum
inPosoahe
deuiaBeBs
GgBP
derdG
e
Sor mguh
nhe
caumiaN mgffe
udeuia
BeBs
GgBPAaiaP
e
UL??
AaiaPe
???
Figure 2: UNIQ (left) and MANLI (right) alignments for problem 1 in RTE2 test.
art lexical resources, and when they are present,
scoring issues arise. Consequently, at least in
the Stanford system, the integration of paraphrase
knowledge to enable MWE recognition has made
almost no difference either in terms of alignment
accuracy nor in entailment accuracy. Furthermore,
it is not the case that entailment recognition accu-
racy is worse for sentences with ?true? MWEs. In
sum, we find that even though capturing and repre-
senting MWEs is an interesting problem in itself,
MWEs do not seem to be such a pain in the neck ?
at least not for textual entailment.
Our results may seem to contradict the results
of many previous RTE studies such as (Bar-Haim
et al, 2005) which found paraphrases to make an
important contribution. However, the beneficial ef-
fect of paraphrases found in these studies refers not
to an alignment task, but to the ability of relating
lexico-syntactic reformulations such as diathesis
alternations or symmetrical predicates (buy/sell).
In the Stanford system, this kind of knowledge
is already present in the features of the inference
stage. Our results should therefore rather be seen
as a clarification of the complementary nature of
the paraphrase and MWE issues.
In our opinion, there is much more potential
for improvement from better estimates of semantic
similarity. This is true for phrasal similarity, as our
negative results for multi-word paraphrases show,
but also on the single-word level. The 2% gain
in accuracy for the Stanford system here over the
reported TAC RTE4 results stems merely from ef-
forts to clean up and rescale the lexical resources
used by the system, and outweighs the effect of
MWEs. One possible direction of research is con-
ditioning semantic similarity on context: Most cur-
rent lexical resources characterize similarity at the
lemma level, but true similarities of word or phrase
pairs are strongly context-dependent: obtain and
be awarded are much better matches in the context
of a degree than in the context of data.
Acknowledgments
We thank Bill MacCartney for his help with the
MANLI aligner, and Michel Galley for the parallel
corpus-based paraphrase resource. This paper is
based on work funded in part by DARPA through
IBM. The content does not necessarily reflect the
views of the U.S. Government, and no official en-
dorsement should be inferred.
8
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 597?604, Ann
Arbor, MI.
Roy Bar-Haim, Idan Szpecktor, and Oren Glickman.
2005. Definition and analysis of intermediate entail-
ment levels. In Proceedings of the ACL Workshop on
Empirical Modeling of Semantic Equivalence and
Entailment, pages 55?60, Ann Arbor, MI.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second PASCAL recognising
textual entailment challenge. In Proceedings of the
Second PASCAL Challenges Workshop on Recognis-
ing Textual Entailment, Venice, Italy.
Francis Bond, Anna Korhonen, Diana McCarthy, and
Aline Villavicencio, editors. 2003. Proceedings of
the ACL 2003 workshop on multiword expressions:
Analysis, acquisition and treatment.
Chris Brockett. 2007. Aligning the RTE 2006 corpus.
Technical Report MSR-TR-2007-77, Microsoft Re-
search.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matic of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Aljoscha Burchardt, Nils Reiter, Stefan Thater, and
Anette Frank. 2007. A semantic approach to tex-
tual entailment: System evaluation and task analy-
sis. In Proceedings of the ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing, pages 10?
15, Prague, Czech Republic.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entail-
ment challenge. In J. Quinonero-Candela, I. Da-
gan, B. Magnini, and F. d?Alch Buc, editors, Ma-
chine Learning Challenges. Lecture Notes in Com-
puter Science, Vol. 3944, pages 177?190. Springer.
Marie-Catherine de Marneffe, Trond Grenager, Bill
MacCartney, Daniel Cer, Daniel Ramage, Chlo?e
Kiddon, and Christopher D. Manning. 2007. Align-
ing semantic graphs for textual inference and ma-
chine reading. In Proceedings of the AAAI Spring
Symposium.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entail-
ment. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages
211?219, Athens, Greece.
Nicole Gr?egoire, Stefan Evert, and Su Nam Kim, edi-
tors. 2007. Proceedings of the ACL workshop: A
broader perspective on multiword expressions.
Andrew Hickl and Jeremy Bensley. 2007. A discourse
commitment-based framework for recognizing tex-
tual entailment. In Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 171?176, Prague, Czech Republic.
Adrian Iftene and Alexandra Balahur-Dobrescu. 2007.
Hypothesis transformation and semantic variability
rules used in recognizing textual entailment. In
Proceedings of the ACL-PASCAL Workshop on Tex-
tual Entailment and Paraphrasing, pages 125?130,
Prague, Czech Republic.
Dekang Lin and Patrick Pantel. 2002. Discovery of
inference rules for question answering. Journal of
Natural Language Engineering, 7(4):343?360.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the joint An-
nual Meeting of the Association for Computational
Linguistics and International Conference on Com-
putational Linguistics, pages 768?774, Montr?eal,
Canada.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of valid
textual entailments. In Proceedings of NAACL.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, Honolulu, Hawaii.
Erwin Marsi, Emiel Krahmer, and Wauter Bosma.
2007. Dependency-based paraphrasing for recogniz-
ing textual entailment. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 83?88, Prague, Czech Republic.
Begona Villada Moir?on, Aline Villavicencio, Diana
McCarthy, Stefan Evert, and Suzanne Stevenson, ed-
itors. 2006. Proceedings of the ACL Workshop on
Multiword Expressions: Identifying and Exploiting
Underlying Properties.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multi-word
expressions: a pain in the neck for NLP. In Proceed-
ings of CICLing.
Takaaki Tanaka, Aline Villavicencio, Francis Bond,
and Anna Korhonen, editors. 2004. Proceedings of
the second ACL workshop on multiword expressions:
Integrating processing.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Alessandro Moschitti. 2007. Shallow semantic in
fast textual entailment rule learners. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, pages 72?77, Prague,
Czech Republic.
9
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 665?672
Manchester, August 2008
Semantic role assignment for event nominalisations
by leveraging verbal data
Sebastian Pad
?
o
Department of Linguistics
Stanford University
450 Serra Mall
Stanford CA 94305, USA
pado@stanford.edu
Marco Pennacchiotti and Caroline Sporleder
Computational Linguistics
Saarland University
Postfach 15 11 50
66041 Saarbr?ucken, Germany
{pennacchiotti|csporled}@coli.uni-sb.de
Abstract
This paper presents a novel approach to
the task of semantic role labelling for event
nominalisations, which make up a consider-
able fraction of predicates in running text,
but are underrepresented in terms of train-
ing data and difficult to model. We propose
to address this situation by data expansion.
We construct a model for nominal role la-
belling solely from verbal training data. The
best quality results from salvaging gram-
matical features where applicable, and gen-
eralising over lexical heads otherwise.
1 Introduction
The last years have seen a large body of work on
modelling the semantic properties of individual
words, both in the form of hand-built resources
like WordNet and data-driven methods like seman-
tic space models. It is still much less clear how the
combined meaning of phrases can be described.
Semantic roles describe an important aspect of
phrasal meaning by characterising the relationship
between predicates and their arguments on a seman-
tic level (e.g., agent, patient). They generalise over
surface categories (such as subject, object) and vari-
ations (such as diathesis alternations). Two frame-
works for semantic roles have found wide use in
the community, PropBank (Palmer et al, 2005) and
FrameNet (Fillmore et al, 2003). Their corpora are
used to train supervised models for semantic role
labelling (SRL) of new text (Gildea and Jurafsky,
2002; Carreras and M`arquez, 2005). The resulting
analysis can benefit a number of applications, such
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported license
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some
rights reserved.
as Information Extraction (Moschitti et al, 2003)
or Question Answering (Frank et al, 2007).
A commonly encountered criticism of seman-
tic roles, and arguably a major obstacle to their
adoption in NLP, is their limited coverage. Since
manual semantic role tagging is costly, it is hardly
conceivable that gold standard annotation will ulti-
mately be available for every predicate of English.
In addition, the lexically specific nature of the map-
ping between surface syntax and semantic roles
makes it difficult to generalise from seen predicates
to unseen predicates for which no training data is
available. Techniques for extending the coverage of
SRL therefore address an important need.
Unfortunately, pioneering work in unsupervised
SRL (Swier and Stevenson, 2004; Grenager and
Manning, 2006) currently either relies on a small
number of semantic roles, or cannot identify equiva-
lent roles across predicates. A promising alternative
direction is automatic data expansion, i.e., lever-
aging existing annotations to classify unseen, but
similar, predicates. The feasibility of this approach
was demonstrated by Gordon and Swanson (2007)
for syntactically similar verbs. However, their ap-
proach requires at least one annotated instance of
each new predicate, limiting its practicability.
In this paper, we present a pilot study on the
application of automatic data expansion to event
nominalisations of verbs, such as agreement for
agree or destruction for destroy. While event nom-
inalisations often afford the same semantic roles
as verbs, and often replace them in written lan-
guage (Gurevich et al, 2006), they have played a
largely marginal role in annotation. PropBank has
only annotated verbs.1 FrameNet annotates nouns,
but covers far fewer nouns than verbs. The same
1A follow-up project, NomBank (Meyers et al, 2004), has
since provided annotations for nominal instances, too.
665
situation holds in other languages (Erk et al, 2003).
Our fundamental intuition is that it is possible to
increase the annotation coverage of event nominal-
isations by data expansion from verbal instances,
since the verbal and nominal predicates share a
large part of the underlying argument structure. We
assume that annotation is available for verbal in-
stances. Then, for a given instance of a nominal-
isation and its arguments, the aim is to assign se-
mantic role labels to these arguments. We solve this
task by constructing mappings between the argu-
ments of the noun and the semantic roles realised
by the verb?s arguments. Crucially, unlike previous
work (Liu and Ng, 2007), we do not employ a clas-
sical supervised approach, and thus do not require
any nominal annotations.
Structure of the paper. Sec. 2 provides back-
ground on nominalisations and SRL. Sec. 3 pro-
vides concrete details on our expansion-based ap-
proach to SRL for nominalisations. The second part
of the paper (Sec. 4?6) provides a first evaluation
of different mapping strategies based on syntactic,
semantic, and hybrid information. Sec. 8 concludes.
2 Nominalisations
Nominalisations (or deverbal nouns) are commonly
defined as nouns morphologically derived from
verbs, usually by suffixation (Quirk et al, 1985).
They have been classified into at least three cate-
gories in the linguistic literature, event, result, and
agent/patient nominalisations (Grimshaw, 1990).
Event and result nominalisations account for the
bulk of deverbal nouns. The first class refers to an
event/activity/process, with the nominal expressing
this action (e.g. killing, destruction). Nouns in the
second class describe the result or goal of an ac-
tion (e.g. agreement). Many nominals have both
an event and a result reading (e.g., selection can
mean the process of selecting or the selected ob-
ject). Choosing a single reading for an instance is
often difficult; see Nunes (1993); Grimshaw (1990).
A smaller class is agent/patient nominalisations.
Agent nominals are usually identified by suffixes
such as -er, -or, -ant (e.g. speaker, applicant), while
patient nominalisations end with -ee, -ed (e.g. em-
ployee). While these nominalisations can be anal-
ysed as events (the baker?s bread implies that bak-
ing has taken place), they more naturally refer to
participants. In consequence, agent/patient nomi-
nals tend to realise fewer arguments ? the average
in FrameNet is 1.46 arguments, compared to 1.74
PropBank
Verbs (Carreras and M`arquez, 2005) 80%
Nouns (Liu and Ng, 2007) 73%
FrameNet
Verbs (Mihalcea and Edmonds, 2005) 72%
Nouns (Pradhan et al, 2004) 64%
Table 1: F-Scores for supervised SRL (end-to-end)
for events/results. As our goal is nominal SRL, we
concentrate on the event/results class.
SRL for nominalisations. Compared to the wealth
of studies on verbal SRL (e.g., Gildea and Juraf-
sky (2002); Fleischman and Hovy (2003)), there
is relatively little work that specifically addresses
nominal SRL. Nouns are generally treated like
verbs: the task is split into two classification steps,
argument recognition (telling arguments from non-
arguments) and argument labelling (labelling recog-
nised arguments with a role). Nominal SRL also
typically draws on feature sets that are similar to
those for verbs, i.e., comprising mainly syntac-
tic and lexical-semantic information (Liu and Ng,
2007; Jiang and Ng, 2006).
On the other hand, there is converging evidence
that nominal SRL is somewhat more difficult than
verbal SRL. Table 1 shows some results for both
verbal and nominal SRL from the literature. For
both PropBank and for FrameNet, we find a differ-
ence of 7?8% F-Score. Note, however, that these
studies use different datasets and are thus not di-
rectly comparable.
In order to confirm the difference between nouns
and verbs, we modelled a controlled dataset (de-
scribed in detail in Sec. 4) of verbs and corre-
sponding event nominalisations. We used Shal-
maneser (Erk and Pad?o, 2006), to our knowledge
the only freely available SRL system that handles
nouns. SRL models were trained on verbs and
nouns separately, using the same settings and fea-
tures. Table 2 shows the results, averaged over 10
cross-validation (CV) folds. Accuracy was about
equal in the recognition step, and 5% higher for
verbs in the labelling step. We analysed these re-
sults by fitting a logit mixed model. These models
determine which fixed factors are responsible for
differences in a response variable (here: SRL per-
formance) while correcting for imbalances intro-
duced by random factors (see Jaeger (2008)). We
modelled the training and test set sizes and the pred-
icates? parts of speech as fixed effects, and frames
and CV folds as random factors.
For both argument recognition and labelling, the
666
Step Verbs Nouns
Arg recognition (F
1
, class FE) 0.59 0.60
Arg labelling (Accuracy) 0.70 0.65
Table 2: FrameNet SRL on verbs and nouns
amount of training data turned out to be a signifi-
cant factor, i.e., more data leads to higher results.
While the part of speech was not systematically
linked to performance for argument recognition,
it was a highly significant predictor of accuracy
in the labelling step: Even when training set size
was taken into account, verbal arguments were still
significantly easier to label (z=4.5, p<0.001).
In sum, these results lend empirical support to
claims that nominal arguments are less tightly cou-
pled to syntactic realisation than verbal ones (Carl-
son, 1984); their interpretation is harder to capture
with shallow cues.
3 Data Expansion for Nominal SRL
The previous section has established two observa-
tions. First, the argument structures of verbs and
their event nominalisations correspond largely. Sec-
ond, nominal SRL is a difficult task, even given
nominal training data, which is hard to obtain.
Our proposal in this paper is to take advan-
tage of the first observation to address the sec-
ond. We do so by modelling SRL for event nom-
inalisations as a data expansion task ? i.e., us-
ing existing verbal annotations to carry out SRL
for novel nominal instances. In this manner, we
do away completely with the need for manual
annotation of nominal instances that is required
for previous supervised approaches (cf. Sec. 2).
Consider the following examples, given in format
[constituent]
grammatical function/SEMANTIC ROLE
:
(1) a. [Peter]
Subj/COGNIZER
laughs
[about the joke]
PP-about/STIMULUS
b. [Peter]
Subj/COGNIZER
laughs
[at him]
PP-at/STIMULUS
(2) [Peter?s]
Prenom-Gen/?
[hearty]
Prenom-Mod/?
laughter [about the event]
PP-about/?
The sentences with the verbal predicate laugh in
(1) are labelled with semantic roles, while the NP
containing the event nominalisation laughter in (2)
is not. The question we face are what information
from (1) can be re-used to perform argument recog-
nition and labelling on (2), and how.
In this respect, there is a fundamental difference
between lexical-semantic and syntactic information.
Lexical-semantic features, such as the head word,
are basically independent of the predicate?s part of
speech. Thus, the information from (1) that Peter is
a COGNIZER can be used directly for the analysis of
the occurrence of Peter in (2). Unfortunately, pure
lexical features tend to be sparse: the head word
of the last role, event, is unseen in (1), and due to
its abstract nature, also difficult to classify through
semantic similarity. Therefore, it is necessary to
consider syntactic features as well. However, these
vary substantially between verbs and nouns. When
they are applicable to both parts of speech, some
mileage can be apparently gained: the phrase in (2)
headed by event can be classified as STIMULUS
because it is an about-PP like (1a). In contrast, no
direct inferences can be drawn about prenominal
genitives or modifiers which do not exist for verbs.
In the remainder of this paper, we will present
experiments on different ways of combining syn-
tactic and lexical-semantic information to balance
precision and recall in data expansion. We address
argument recognition and labelling separately, since
the two tasks require different kinds of information.
We assume that the frame has been determined be-
forehand with word sense disambiguation methods.
4 Data
The dataset for our study consists of the annotated
FrameNet 1.3 examples. We obtained pairs of verbs
and corresponding event/result nominalisations by
intersecting the FrameNet predicate list with a list
of nominalisations obtained from Celex (Baayen
et al, 1995) and Nomlex (Macleod et al, 1998).
We found 306 nominalisations with correspond-
ing verbs in the same frame, but excluded some
pairs where either the nominalisation was not of the
event/result type, or no annotated FrameNet exam-
ples were available for either verb or noun. The final
dataset, consisting of 265 pairs exemplifying 117
frames, served for both the analysis in Section 2 and
the evaluations in subsequent sections. For the eval-
uations, we used the 26,479 verbal role instances
(2,066 distinct role types) as training data and the
6,502 nominal role instances (993 distinct role
types) as test data. The specification of the dataset
can be downloaded from http://www.coli.
uni-sb.de/
?
pado/nom data.html.
5 Argument Recognition
Argument recognition is usually modelled as a su-
pervised machine learning task. Unfortunately, ar-
667
NP
PP
NP
NP
NP
Peter?s laughter about the joke
Figure 1: Parse tree for example sentence
gument recognition ? at least within predicates ? re-
lies heavily on syntactic features, with the grammat-
ical function (or alternatively syntactic path) feature
as the single most important predictor (Gildea and
Jurafsky, 2002). Since we are bootstrapping from
verbal instances to nominal ones, and since there is
typically considerable variation between nominal
and verbal subcategorisation patterns, we cannot
model argument recognition as a supervised task.
Instead, we follow up on an idea developed by Xue
and Palmer (2004) for verbal SRL, who charac-
terise the set of grammatical functions that could
fill a semantic role in the first place. In our apppli-
cation, we simply extract all syntactic arguments of
the nominalisation, including any premodifiers. We
make no attempt to distinguish between adjuncts
and compulsory arguments. Fig. 1 shows an exam-
ple: in the NP Peter?s laughter about the joke, the
noun laughter has two syntactic arguments: the PP
about the joke and the premodifying NP Peter?s.
Both are extracted as (potential) arguments.
This method cannot identify roles that are syntac-
tically non-local, i.e., those that are not in the max-
imal projection of the frame evoking noun. Such
roles are more common for nouns than for verbs.
Example 3 shows that an ?external? NP like Bill can
be analysed as filling the HELPER role of the noun
help. However, the overall proportion of non-local
roles is still fairly small in our data (around 10%).
(3) [Bill]
HELPER
offered help in case of need.
Table 3 gives the argument recognition results for
our rule-based system on all roles in the gold stan-
dard and on the local roles alone. This simple ap-
proach is surprisingly effective, achieving an over-
all F-Measure of 76.89% on all roles, while on local
roles the F-Measure increases to 82.83% due to the
higher recall. Precision is 82.01%, as not all syn-
tactic arguments do fill a role. For example, modal
modifiers such as hearty in (2) rarely fill a (core)
role in FrameNet. False-negative errors, which af-
fect recall, are partly due to parser errors and partly
Roles Precision Recall F-Measure
all roles 82.01 72.37 76.89
local roles 82.01 83.66 82.83
Table 3: Argument recognition (local / all roles).
to role fillers that do not correspond to constituents
or that are embedded in syntactic arguments. For in-
stance, in (4) the PP in this country, which fills the
PLACE role of cause, is embedded in the PP of suf-
fering in this country, which fills the role EFFECT.
We extract only the larger PP.
(4) the causes [of suffering
[in this country]
PP-in
]
PP-of
6 Argument Labelling
Argument labelling presents a different picture
from argument recognition. Here, both syntactic
and lexical-semantic information contribute to suc-
cess in the task. We present three model families
for nominal argument labelling that take different
stances with respect to this observation.
The first (naive-semantic) and the second (naive-
syntactic) model families represent extreme posi-
tions that attempt to re-use verbal information as
directly as possible. Models from the third fam-
ily, distributional models infer the role of a noun?s
arguments by computing the semantic similarity
between nominal arguments and semantic represen-
tations of the verb roles given by the role fillers?
semantic heads.2 In the lexical-level instantiation,
the mapping is established between individual noun
arguments and roles. In the function-level instantia-
tion, complete nominal grammatical functions are
mapped onto roles.
3
6.1 Naive semantic model
The naive semantic model (naive sem) implements
the assumption that lexical-semantic features pro-
vide the same predictive evidence for verbal and
nominal arguments (cf. Sec. 3). It can be thought of
as constructing the trivial identity mapping between
the values of nominal and verbal semantic features.
To test the usefulness of this model, we train the
Shalmaneser SRL system (Erk and Pad?o, 2006)
on the verbal instances of the dataset described
2Usually, the semantic head of a phrase is its syntactic head.
Exceptions occur e.g. for PPs, where the semantic head is the
syntactic head of the embedded NP.
3We compute grammatical functions as phrase types plus
relative position; for PPs, we add the preposition.
668
in Sec. 4, using only the lexical-semantic features
(head word, first word, last word). We then apply
the resulting models directly to the corresponding
nominal instances.
6.2 Naive syntactic model
The intuition of this model (naive syn) is that gram-
matical functions shared between verbs and nouns
are likely to express the same semantic roles. It
maps all grammatical functions of a verb g
v
onto
the identical functions of the corresponding noun
g
n
and then assigns the most frequent role realised
by g
v
to all arguments with grammatical function
g
n
. For example, if PPs headed by about for the
verb laugh typically realise the STIMULUS role, all
arguments of the noun laughter which are realised
as PP-about are also assigned the STIMULUS role.
We predict that this strategy has a ?high
precision?low recall? profile: It produces reliable
mappings for those grammatical functions that are
preserved across verb and noun, in particular prepo-
sitional phrases; however, it fails for grammatical
functions that only occur for one part of speech.
This problem becomes particular pertinent for
two prominent role types, namely AGENT-style
roles (deep subjects) and PATIENT-style roles (deep
objects). These roles are usually expressed via dif-
ferent and ambiguous noun and verb functions
(Gurevich et al, 2006). For verbs, the AGENT
is typically expressed by the Subject, while for
nouns it is expressed by a Pre-Modifier. The PA-
TIENT is commonly realised as the Object for
verbs, and either as a Pre-Modifier or as a PP-of
for nouns. As the noun?s Pre-Modifier is highly
ambiguous, it is also ineffective to apply a non-
identity mapping such as (subject
v
,Pre-Modifier
n
)
or (object
v
,Pre-Modifier
n
).
4
A final variation of this model is the generalised
naive syntactic model (naive sem-gen), where we
assign the role most frequently realised by a given
function across all verbs in the frame. This method
alleviates data sparseness stemming from functions
never seen with particular verbs and is fairly safe,
since mapping within frames tends to be uniform.
6.3 Distributional models
The distributional models construct mappings be-
tween verbal and nominal semantic heads. In con-
4Lapata (2002) has shown that the mapping can be dis-
ambiguated for individual nominalisations. Her model, using
lexical-semantic, contextual and pragmatic information, is out-
side the scope of the present paper.
trast to the naive semantic model, they make use of
some measure of semantic similarity to find map-
pings, and optionally use syntactic constraints to
guide generalisation. In this manner, distributional
models can deal with unseen feature values more
effectively. In sentences (1) and (2), for example,
an ideal distributional model would find the head
word event in (2) to be more semantically similar
to the head joke in (1a) than to head him in (1b).
The resulting mapping (joke, event) leads to the
assignment of the role STIMULUS to event.
Semantic Similarity. Semantic similarity mea-
sures are commonly used to compute similarity
between two lexemes. There are two main types
of similarity: Ontology-based, computed through
the closeness of two lexemes in a lexical database
(e.g., WordNet); and distributional, given by some
measure of the distance between the lexemes? vec-
tor representations in a semantic co-occurrence
space. We chose the latter approach because it tends
to have a higher coverage and it is knowledge-lean,
requiring just an unannotated corpus.
We compute distributional-similarity with a
semantic space model based on lexical co-
occurrences backed by syntactic relations (Pad?o
and Lapata, 2007).5 The model is constructed from
the British National Corpus (BNC), using the 2.000
most pairs of words and grammatical functions as
dimensions. As similarity measure, we use cosine
distance on log-likelihood transformed counts.
Lexical level model. The lexical level model
(dist-lex) assigns to each nominal argument the verb
role that it is semantically most similar to. Each role
is represented by the semantic heads of its fillers.
For example, suppose that the role STIMULUS of
the verb laugh has been realised by the heads story,
scene, joke, and tale. Then, in ?Peter?s laughter
about the event?, we analyse event as STIMULUS,
since event is similar to these heads.
Formally, each argument head l is represented
by a co-occurrence vector ~l. A verb role r
v
? R
v
is
modelled by the centroid ~r
v
of its instances? heads:
~r
v
=
1
|L
r
v
|
?
l?L
r
v
~
l
Roles are assigned to nominal argument heads l
n
?
L
n
by finding the semantically most similar role r
5We also experimented with bag-of-words based vector
spaces, which showed worse performance throughout.
669
while the grammatical function g
n
is ignored:
r(l
n
, g
n
) = argmax
r
v
?R
v
sim
cos
(
~
l
n
, ~r
v
)
Function level model. The syntactic level model
(dist-fun) generalises the lexical level model by ex-
ploiting the intuition that, within nouns, most se-
mantic roles tend to be consistently realised by one
specific grammatical function. This function can
be identified as the one most semantically similar
to the role?s representation. Following the exam-
ple above, suppose that the grammatical function
PP-about of laughter has as semantic heads the
lexemes: event, story, news. Then, it is likely to
express the role STIMULUS, as its heads are seman-
tically similar to those of the verbal fillers of this
role: story, scene, sentence, tale. For each nomi-
nalisation, this model constructs mappings (r
v
, g
n
)
between a verbal semantic role r
v
and a nominal
grammatical function g
n
. The representations for
roles are computed as described above. We com-
pute the semantic representations for grammatical
functions, in parallel to the roles? definition above,
as the centroid of their fillers? representations L
g
n
:
~g
n
=
1
|L
g
n
|
?
l?L
g
n
~
l
The assignment of a role to a nominal arguments
is now determined by the argument?s grammatical
function g
n
; its lemma l
n
only enters indirectly, via
the similarity computation:
r(l
n
, g
n
) = argmax
r
v
?R
v
sim
cos
(~r
v
, ~g
n
)
This strategy guarantees that each nominal gram-
matical function is mapped to exactly one role. In
the inverse direction, roles can be left unmapped or
mapped to more than one function.
6
6.4 Hybrid models
Our last class combines the naive and distributional
models with a back-off approach. We first attempt
to harness the reliable naive syntactic approach
whenever a mapping for the argument?s grammati-
cal function is available. If this fails, it backs off to a
distributional model. This strategy helps to recover
the frequent AGENT- and PATIENT-style roles that
cannot be recovered on syntactic grounds.
6We also experimented with a global optimisation strategy
where we maximised the overall similarity between roles and
functions subject to different constraints (e.g., perfect match-
ing). Unfortunately, this strategy did not improve results.
System Accuracy
baseline random 17.09
B
L
baseline most common 42.97
naive syn 15.29
naive syn-gen 21.56
N
a
i
v
e
naive sem 24.00
dist-lex 44.57
D
i
s
t
dist-fun 52.00
naive syn + dist-lex 48.22
naive syn-gen + dist-lex 50.54
naive syn + dist-fun 54.39
H
y
b
r
i
d
naive syn-gen + dist-fun 56.42
Table 4: Results for nominal argument labelling
In (2), a hybrid model would assign the
role STIMULUS to the argument headed by
event, using the naive syntactic mapping
(PP-about
v
,PP-about
n
) derived from (1a). For the
prenominal modifier, no syntactic mapping is avail-
able; thus, it backs off to lexical-semantic evidence
from (1a-b) to analyse Peter as COGNIZER.
We experiment with two hybrid models: naive
syntactic plus lexical level distributional (naive syn
+ dist-lex), and naive syntactic plus functional level
distributional (naive syn + dist-fun).
7 Experimental results
The results of our experiments are reported in Ta-
ble 4. The models are compared against two base-
lines: A random baseline which randomly chooses
one of the verb roles for each of the arguments of
the corresponding noun; a most common baseline
which assigns to each nominal argument the most
frequent role of the corresponding verb ? i.e. the
role which has most fillers. All models with the
exception of naive syn significantly outperform the
random baseline, but only dist-fun and all hybrid
models outperform the most common baseline.
In general, the best performing methods are the
hybrid ones, with best accuracy achieved by naive
syn-gen + dist-fun. Non-hybrid approaches always
have lower accuracy. This validates our main hy-
pothesis in this paper, namely that the combination
of syntactic information with distributional seman-
tics is a promising strategy.
Matching our predictions, the low accuracy of
the naive syntactic model is mainly due to a lack
of coverage. In fact, the model leaves 5,010 of the
6,502 gold standard noun fillers unassigned since
they realise syntactic roles that are unseen for the
verbs in question. A large part of these are Pre-
Modifier and PP-of functions, which are central
for nouns, but mostly ungrammatical for verbs. On
670
the 1,492 fillers for which a role was assigned, the
model obtains an accuracy of 67%, indicating a rea-
sonably high, but not perfect, accuracy for shared
grammatical functions. The remaining errors stem
from two sources. First, many grammatical func-
tions are ambiguous, causing wrong assignments
by a ?syntax-only? model. For example, PP-in can
indicate both TIME and PLACE for many nominal-
isations.Second, a certain number of grammatical
functions do not preserve their role between verb to
noun (Hull and Gomez, 1996). For example, PP-to
realises the MESSAGE role of the verb require but
the ADDRESSEE role of the noun request.
Distributional models show in general better per-
formance than the naive syntactic approach (ap-
prox. +25% accuracy). They do not suffer from the
coverage problem, since they assign a role to each
filler. Yet, the accuracy over assigned roles is lower
than for the syntactic approach (52% for dist-fun).
We conclude that in the limited cases where a
pure syntactic mapping is applicable, it is far more
reliable than methods which are mainly based on
lexical-semantic information. The major limitation
of the latter is that lexical-semantics tend to fail
when roles are semantically very similar. For ex-
ample, for the noun announcement, the syntactic-
level distributional model wrongly builds the map-
ping (ADDRESSEE, PP-by) instead of (SPEAKER,
PP-by), because the two roles are very similar se-
mantically (the computed similarities of the PP-by
arguments to ADDRESSEE and SPEAKER in the
semantic space are 0.94 and 0.92, respectively).
The syntactic-level distributional model outper-
forms the lexical-level, suggesting that generalising
the mapping at the argument level offers more sta-
ble statistical evidence to find the correct role, i.e. a
set of noun arguments better defines the seman-
tics of the mapping than a single argument. This
is mostly the case when the context vector of the
argument is not a good representation because the
semantic head is ambiguous, infrequent or atypical.
Consider, for example, the following sentence for
the noun violation:
(5) Sterne?s Tristram Shandy consists
of a series of violations [of literary
conventions]
PP-OF/NORM
The syntactic-level model builds the correct map-
ping (NORM, PP-of ), as the role fillers of the verb
violate (e.g. principle, right, treaty, law) are very
similar to the noun?s category fillers (e.g. conven-
tion, rule, agreement, treaty, norm), causing the cen-
troids of NORM and PP-of to be close in the space.
The lexical-level model, however, builds the incor-
rect mapping (PROTAGONIST, convention). This
happens because convention is ambiguous, and one
of its senses (?a large formal assembly?) is compat-
ible with the PROTAGONIST role, and happens to
have a large influence on the position of the vector
for convention. Unfortunately, this is not the sense
in which the word is used in this sentence.
8 Conclusions
We have presented a data expansion approach to
SRL for event nominalisations. Instead of relying
on manually annotated nominal training data, we
harness annotated data for verbs to bootstrap a se-
mantic role labeller for nouns. For argument recog-
nition, we use a simple rule-based approach. For
argument labelling, we profit from the fact that the
argument structures of event nominalisations and
the corresponding verbs are typically similar. This
allows us to learn a mapping between verbal roles
and nominal arguments, using syntactic features,
lexical-semantic similarity, or both.
We found that our rule-based approach for argu-
ment recognition works fairly well. For argument
labelling, our approach does not yet attain the per-
formance of supervised models, but has the crucial
advantage of not requiring any labelled data for
nominal predicates.
We achieved the highest accuracy with a hybrid
syntactic-semantic model, which indicates that both
types of information need to be combined for op-
timal results. A purely syntactic approach results
in a high precision, but low coverage because fre-
quent grammatical functions in particular cannot be
trivially mapped. Backing off to semantic similarity
provides additional coverage. However, semantic
similarity has to be considered on the level of com-
plete functions rather than individual instances to
promote ?uniformity? in the mappings.
In this paper, we have only considered nominal
SRL by data expansion, i.e. we only applied our
approach to those nominalisations for which we
have annotated data for the corresponding verbs.
However, even if no data is available for the corre-
sponding verb, it might still be possible to bootstrap
from other verbs in the same frame (assuming that
the frame is known for the nominalisation) and we
plan to pursue this idea in furture research. We also
intend to investigate whether a joint optimisation of
671
the mapping constrained by additional syntactic in-
formation such as subcategorisation frames leads to
better results. Finally, we will verify that our meth-
ods, which we have evaluated on English FrameNet
data, carry over to other corpora and languages.
Acknowledgments. Our work was partly funded
by the German Research Foundation DFG (grant
PI 154/9-3).
References
Baayen, R., R. Piepenbrock, and L. Gulikers, 1995. The
CELEX Lexical Database (Release 2). LDC.
Carlson, G. 1984. Thematic roles and their role in se-
mantic interpretation. Linguistics, 22:259?279.
Carreras, X. and L. M`arquez, editors. 2005. Proceed-
ings of the CoNLL shared task: Semantic role la-
belling, Ann Arbor, MI.
Erk, K. and S. Pad?o. 2006. Shalmaneser ? a flexible
toolbox for semantic role assignment. In Proceed-
ings of LREC, Genoa, Italy.
Erk, K., A. Kowalski, S. Pad?o, and M. Pinkal. 2003.
Towards a resource for lexical semantics: A large
German corpus with extensive semantic annotation.
In Proceedings of ACL, pages 537?544, Sapporo,
Japan.
Fillmore, C., C. Johnson, and M. Petruck. 2003. Back-
ground to FrameNet. International Journal of Lexi-
cography, 16:235?250.
Fleischman, M. and E. Hovy. 2003. Maximum entropy
models for FrameNet classification. In Proceedings
of EMNLP, pages 49?56, Sapporo, Japan.
Frank, A., H.-U. Krieger, F. Xu, H. Uszkoreit, B. Crys-
mann, B. J?org, and U. Sch?afer. 2007. Question an-
swering from structured knowledge sources. Journal
of Applied Logic, 5(1):20?48.
Gildea, D. and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
Gordon, A. and R. Swanson. 2007. Generalizing
semantic role annotations across syntactically simi-
lar verbs. In Proceedings of ACL, pages 192?199,
Prague, Czechia.
Grenager, T. and C. Manning. 2006. Unsupervised dis-
covery of a statistical verb lexicon. In Proceedings
of EMNLP, pages 1?8, Sydney, Australia.
Grimshaw, J. 1990. Argument Structure. MIT Press.
Gurevich, O., R. Crouch, T. King, and V. de Paiva.
2006. Deverbal nouns in knowledge representation.
In Proceedings of FLAIRS, pages 670?675, Mel-
bourne Beach, FL.
Hull, R. and F. Gomez. 1996. Semantic interpretation
of nominalizations. In Proceedings of AAAI, pages
1062?1068, Portland, OR.
Jaeger, T. 2008. Categorical data analysis: Away from
ANOVAs and toward Logit Mixed Models. Journal
of Memory and Language. To appear.
Jiang, Zheng Ping and Hwee Tou Ng. 2006. Semantic
role labeling of NomBank: A maximum entropy ap-
proach. In Proceedings of EMNLP, pages 138?145,
Sydney, Australia.
Lapata, M. 2002. The disambiguation of nominalisa-
tions. Computational Linguistics, 28(3):357?388.
Liu, C. and H. Ng. 2007. Learning predictive structures
for semantic role labeling of NomBank. In Proceed-
ings of ACL, pages 208?215, Prague, Czechia.
Macleod, C., R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998. Nomlex: A lexicon of nominaliza-
tions. In Proceedings of EURALEX, Li`ege, Belgium.
Meyers, A., R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004. An-
notating Noun Argument Structure for NomBank. In
Proceedings of LREC, Lisbon, Portugal.
Mihalcea, Rada and Phil Edmonds, editors. 2005.
Proceedings of Senseval-3: The Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, Barcelona, Spain.
Moschitti, A., P. Morarescu, and S. Harabagiu. 2003.
Open-domain information extraction via automatic
semantic labeling. In Proceedings of FLAIRS, pages
397?401, St. Augustine, FL.
Nunes, M. 1993. Argument linking in English de-
rived nominals. In Valin, Robert D. Van, editor, Ad-
vances in Role and Reference Grammar, pages 372?
432. John Benjamins.
Pad?o, S. and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
Palmer, M., D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Pradhan, S., H. Sun, W. Ward, J. Martin, and D. Ju-
rafsky. 2004. Parsing arguments of nominaliza-
tions in English and Chinese. In Proceedings of
HLT/NAACL, pages 141?144, Boston, MA.
Quirk, R., S. Greenbaum, G. Leech, and J. Svartvik.
1985. A Comprehensive Grammar of the English
Language. Longman.
Swier, R. and S. Stevenson. 2004. Unsupervised se-
mantic role labelling. In Proceedings of EMNLP,
pages 95?102.
Xue, N. and M. Palmer. 2004. Calibrating features for
semantic role labeling. In Proceedings of EMNLP,
pages 88?94, Barcelona, Spain.
672
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 897?906,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A Structured Vector Space Model for Word Meaning in Context
Katrin Erk
Department of Linguistics
University of Texas at Austin
katrin.erk@mail.utexas.edu
Sebastian Pado?
Department of Linguistics
Stanford University
pado@stanford.edu
Abstract
We address the task of computing vector space
representations for the meaning of word oc-
currences, which can vary widely according to
context. This task is a crucial step towards a
robust, vector-based compositional account of
sentence meaning. We argue that existing mod-
els for this task do not take syntactic structure
sufficiently into account.
We present a novel structured vector space
model that addresses these issues by incorpo-
rating the selectional preferences for words?
argument positions. This makes it possible to
integrate syntax into the computation of word
meaning in context. In addition, the model per-
forms at and above the state of the art for mod-
eling the contextual adequacy of paraphrases.
1 Introduction
Semantic spaces are a popular framework for the rep-
resentation of word meaning, encoding the meaning
of lemmas as high-dimensional vectors. In the de-
fault case, the components of these vectors measure
the co-occurrence of the lemma with context features
over a large corpus. These vectors are able to pro-
vide a robust model of semantic similarity that has
been used in NLP (Salton et al, 1975; McCarthy and
Carroll, 2003; Manning et al, 2008) and to model
experimental results in cognitive science (Landauer
and Dumais, 1997; McDonald and Ramscar, 2001).
Semantic spaces are attractive because they provide a
model of word meaning that is independent of dictio-
nary senses and their much-discussed problems (Kil-
garriff, 1997; McCarthy and Navigli, 2007).
In a default semantic space as described above,
each vector represents one lemma, averaging over
all its possible usages (Landauer and Dumais, 1997;
Lund and Burgess, 1996). Since the meaning of
words can vary substantially between occurrences
(e.g., for polysemous words), the next necessary step
is to characterize the meaning of individual words in
context.
There have been several approaches in the liter-
ature (Smolensky, 1990; Schu?tze, 1998; Kintsch,
2001; McDonald and Brew, 2004; Mitchell and La-
pata, 2008) that compute meaning in context from
lemma vectors. Most of these studies phrase the prob-
lem as one of vector composition: The meaning of a
target occurrence a in context b is a single new vector
c that is a function (for example, the centroid) of the
vectors: c = a b.
The context b can consist of as little as one word,
as shown in Example (1). In (1a), the meaning of
catch combined with ball is similar to grab, while in
(1b), combined with disease, it can be paraphrased
by contract. Conversely, verbs can influence the in-
terpretation of nouns: In (1a), ball is understood as a
spherical object, and in (1c) as a dancing event.
(1) a. catch a ball
b. catch a disease
c. attend a ball
In this paper, we argue that models of word mean-
ing relying on this procedure of vector composition
are limited both in their scope and scalability. The
underlying shortcoming is a failure to consider syntax
in two important ways.
The syntactic relation is ignored. The first problem
concerns the manner of vector composition, which
ignores the relation between the target a and its con-
text b. This relation can have a decisive influence on
their interpretation, as Example (2) shows:
897
(2) a. a horse draws
b. draw a horse
In (2a), the meaning of the verb draw can be para-
phrased as pull, while in (2b) it is similar to sketch.
This difference in meaning is due to the difference in
relation: in (2a), horse is the subject, while in (2b)
it is the object. On the modeling side, however, a
vector combination function that ignores the relation
will assign the same representation to (2a) and (2b).
Thus, existing models are systematically unable to
capture this class of phenomena.
Single vectors are too weak to represent phrases.
The second problem arises in the context of the im-
portant open question of how semantic spaces can
?scale up? to provide interesting meaning representa-
tions for entire sentences. We believe that the current
vector composition methods, which result in a single
vector c, are not informative enough for this purpose.
One proposal for ?scaling up? is to straightforwardly
interpret c = a  b as the meaning of the phrase
a + b (Kintsch, 2001; Mitchell and Lapata, 2008).
The problem is that the vector c can only encode a
fixed amount of structural information if its dimen-
sionality is fixed, but there is no upper limit on sen-
tence length, and hence on the amount of structure
to be encoded. It is difficult to conceive how c could
encode deeper semantic properties, like predicate-
argument structure (distinguishing ?dog bites man?
and ?man bites dog?), that are crucial for sentence-
level semantic tasks such as the recognition of textual
entailment (Dagan et al, 2006). An alternative ap-
proach to sentence meaning would be to use the vec-
tor space representation only for representing word
meaning, and to represent sentence structure sepa-
rately. Unfortunately, present models cannot provide
this grounding either, since they compute a single
vector c that provides the same representations for
both the meanings of a and b in context.
In this paper, we propose a new, structured vector
space model for word meaning (SVS) that addresses
these problems. A SVS representation of a lemma
comprises several vectors representing the word?s
lexical meaning as well as the selectional preferences
that it has for its argument positions. The meaning
of word a in context b is computed by combining a
with b?s selectional preference vector specific to the
relation between a and b, addressing the first problem
above. In an expression a + b, the meanings of a
and b in this context are computed as two separate
vectors a? and b?. These vectors can then be combined
with a representation of the structure?s expression
(e.g., a parse tree), to address the second problem
discussed above. We test the SVS model on the task
of recognizing contextually appropriate paraphrases,
finding that SVS performs at and above the state-of-
the-art.
Plan of the paper. Section 2 reviews related work.
Section 3 presents the SVS model for word meaning
in context. Sections 4 to 6 relate experiments on the
paraphrase appropriateness task.
2 Related Work
In this section we give a short overview over existing
vector space based approaches to computing word
meaning in context.
General context effects. The first category of
models aims at integrating the widest possible range
of context information without recourse to linguistic
structure. The best-known work in this category is
Schu?tze (1998). He first computes ?first-order? vec-
tor representations for word meaning by collecting
co-occurrence counts from the entire corpus. Then,
he determines ?second-order? vectors for individual
word instances in their context, which is taken to be a
simple surface window, by summing up all first-order
vectors of the words in this context. The resulting
vectors form sense clusters.
McDonald and Brew (2004) present a similar
model. They compute the expectation for a word
wi in a sequence by summing the first-order vectors
for the words w1 to wi?1 and showed that the dis-
tance between expectation and first-order vector for
wi correlates with human reading times.
Predicate-argument combination. The second
category of prior studies concentrates on contexts
consisting of a single word only, typically modeling
the combination of a predicate p and an argument a.
Kintsch (2001) uses vector representations of p and
a to identify the set of words that are similar to both
p and a. After this set has been narrowed down in a
self-inhibitory network, the meaning of the predicate-
argument combination is obtained by computing the
898
centroid of its members? vectors. The procedure does
not take the relation between p and a into account.
Mitchell and Lapata (2008) propose a framework
to represent the meaning of the combination p+ a as
a function f operating on four components:
c = f(p, a,R,K) (3)
R is the relation holding between p and a, and K
additional knowledge. This framework allows sen-
sitivity to the relation. However, the concrete in-
stantiations that Mitchell and Lapata consider disre-
gards K and R, thus sharing the other models? limi-
tations. They focus instead on methods for the direct
combination of p and a: In a comparison between
component-wise addition and multiplication of p and
a, they find far superior results for the multiplication
approach.
Tensor product-basedmodels. Smolensky (1990)
uses tensor product to combine two word vectors a
and b into a vector c representing the expression a+b.
The vector c is located in a very high-dimensional
space and is thus capable of encoding the structure
of the expression; however, this makes the model
infeasible in practice, as dimensionality rises with
every word added to the representation. Jones and
Mewhort (2007) represent lemma meaning by using
circular convolution to encode n-gram co-occurrence
information into vectors of fixed dimensionality. Sim-
ilar to Brew and McDonald (2004), they predict most
likely next words in a sequence, without taking syn-
tax into account.
Kernel methods. One of the main tests for the
quality of models of word meaning in context is the
ability to predict the appropriateness of paraphrases
in given a context. Typically, a paraphrase applies
only to some senses of a word, not all, as can be seen
in the paraphrases ?grab? and ?contract? of ?catch?.
Vector space models generally predict paraphrase ap-
propriateness based on the similarity between vectors.
This task can also be addressed with kernel methods,
which project items into an implicit feature space
for efficient similarity computation. Consequently,
vector space methods and kernel methods have both
been used for NLP tasks based on similarity, no-
tably Information Retrieval and Textual Entailment.
Nevertheless, they place their emphasis on different
types of information. Current kernels are mostly tree
kernels that compare syntactic structure, and use se-
mantic information mostly for smoothing syntactic
similarity (Moschitti and Quarteroni, 2008). In con-
trast, vector-space models focus on the interaction
between the lexical meaning of words in composi-
tion.
3 A structured vector space model for
word meaning in context
In this section, we define the structured vector space
(SVS) model of word meaning.
The main intuition behind our model is to view
the interpretation of a word in context as guided by
expectations about typical events. For example, in
(1a), we assume that upon hearing the phrase ?catch a
ball?, the hearer will interpret the meaning of ?catch?
to match typical actions that can be performed with a
ball. Similarly, the interpretation of ?ball? will reflect
the hearer?s expectations about typical things that can
be caught. This move to include typical arguments
and predicates into a model of word meaning can be
motivated both on cognitive and linguistic grounds.
In cognitive science, the central role of expecta-
tions about typical events for human language pro-
cessing is well-established. Expectations affect read-
ing times (McRae et al, 1998), the interpretation of
participles (Ferretti et al, 2003), and sentence pro-
cessing generally (Narayanan and Jurafsky, 2002;
Pado? et al, 2006). Expectations exist both for verbs
and nouns (McRae et al, 1998; McRae et al, 2005).
In linguistics, expectations, in the form of selec-
tional restrictions and selectional preferences, have
long been used in semantic theories (Katz and Fodor,
1964; Wilks, 1975), and more recently induced
from corpora (Resnik, 1996; Brockmann and Lapata,
2003). Attention has mostly been limited to selec-
tional preferences of verbs, which have been used
for example for syntactic disambiguation (Hindle
and Rooth, 1993), word sense disambiguation (Mc-
Carthy and Carroll, 2003) and semantic role label-
ing (Gildea and Jurafsky, 2002). Recently, a vector-
spaced model of selectional preferences has been
proposed that computes the typicality of an argument
simply through similarity to previously seen argu-
ments (Erk, 2007; Pado? et al, 2007).
We first present the SVS model of word meaning
899
catch
he
fielder
dog
cold
baseball
drift
objsubj
accuse
say
claim
comp
-1
ball
whirl
fly
provide
throw
catch
organise
obj
-1
subj
-1
mod
red
golf
elegant
Figure 1: Structured meaning representations for noun
ball and verb catch : lexical information plus expectations
that integrates lexical information with selectional
preferences. Then, we show how the SVS model pro-
vides a new way of computing meaning in context.
Representing lemma meaning. We abandon the
traditional choice of representing word meaning as
a single vector. Instead, we encode each word as
a combination of (a) one vector that models the
lexical meaning of the word, and (b) a set of vec-
tors, each of which represents the semantic expecta-
tions/selectional preferences for one particular rela-
tion that the word supports.1
The idea is illustrated in Fig. 1. In the representa-
tion of the verb catch, the central square stands for
the lexical vector of catch itself. The three arrows
link it to catch ?s preferences for its subjects (subj),
its objects (obj), and for verbs for which it appears
as a complement (comp?1). The figure shows the se-
lectional preferences as word lists for readability; in
practice, each selectional preference is a single vector
(cf. Section 4). Likewise, ball is represented by one
vector for ball itself, one for ball ?s preferences for its
modifiers (mod), one vector for the verbs of which it
is a subject (subj?1), and one for the verbs of which
is an object (obj?1).
This representation includes selectional prefer-
ences (like subj, obj, mod) exactly parallel to
inverse selectional preferences (subj?1, obj?1,
comp?1). To our knowledge, preferences of the lat-
ter kind have not been studied in computational lin-
guistics. However, their existence is supported in
psycholinguistics by priming effects from nouns to
typical verbs (McRae et al, 2005).
Formally, let D be a vector space (the set of possi-
1We do not commit to a particular set of relations; see the
discussion at the end of this section.
catch
...
cold
baseball
drift
obj
subj
...
comp
-1
ball
...
throw
catch
organise
obj
-1
subj
-1
mod
...
!
!
Figure 2: Combining predicate and argument via relation-
specific semantic expectations
ble vectors), and let R be some set of relation labels.
In the structured vector space (SVS) model, we rep-
resent the meaning of a lemma w as a triple
w = (v,R,R?1)
where v ? D is a lexical vector describing the word
w itself, R : R ? D maps each relation label onto
a vector that describes w?s selectional preferences,
and R?1 : R ? D maps from role labels to vec-
tors describing inverse selectional preferences of w.
Both R and R?1 are partial functions. For example,
the direct object preference would be undefined for
intransitive verbs.
Computing meaning in context. The SVS model
of lemma meaning permits us to compute the mean-
ing of a word a in the context of another word b
in a new way, via their selectional preferences. Let
(va, Ra, R?1a ) and (vb, Rb, R
?1
b ) be the representa-
tions of the two words, and let r ? R be the relation
linking a to b. Then, we define the meaning of a and
b in this context as a pair (a?, b?) of vectors, where
a? is the meaning of a in the context of b, and b? the
meaning of b in the context of a:
a? =
(
va R
?1
b (r), Ra ? {r}, R
?1
a
)
b? =
(
vb Ra(r), Rb, R
?1
b ? {r}
) (4)
where v1 v2 is a direct vector combination function
as in traditional models, e.g. addition or component-
wise multiplication. If either Ra(r) or R
?1
b (r) are
not defined, the combination fails. Afterwards, the ar-
gument position r is considered filled, and is deleted
from Ra and R
?1
b .
900
Figure 2 illustrates this procedure on the represen-
tations from Figure 1. The dotted lines indicate that
the lexical vector for catch is combined with the in-
verse object preference of ball. Likewise, the lexical
vector for ball is combined with the object preference
vector of catch.
Note that our procedure for computing meaning
in context can be expressed within the framework of
Mitchell and Lapata (Eq. (3)). We can encode the
expectations of a and b as additional knowledge K.
The combined representation c is the pair (a?, b?) that
is computed according to our model (Eq. (4)).
The SVS scheme we have proposed incorporates
syntactic information in a more general manner than
previous models, and thus addresses the issues we
have discussed in Section 1. Since the representation
retains individual selectional preferences for all rela-
tions, combining the same words through different
relations can (and will in general) result in different
adapted representations. For instance, in the case of
Example (2), we would expect the inverse subject
preference of horse (?things that a horse typically
does?) to push the lexical vector of draw into the di-
rection of pulling, while its inverse object preference
(?things that are done to horses?) suggest a different
interpretation.
Rather than yielding a single, joint vector for the
whole expression, our procedure for computing mean-
ing in context results in one context-adapted meaning
representation per word, similar to the output of a
WSD system. As a consequence, our model can
be combined with any formalism representing the
structure of an expression. (The formalism used then
determines the set R of relations.) For example, com-
bining SVS with a dependency tree would yield a tree
in which each node is labeled by a SVS tuple that
represents the word?s meaning in context.
4 Experimental setup
This section provides the background to the following
experimental evaluation of SVS, including parameters
used for computing the SVS representations that will
be used in the experiments.
4.1 Experimental rationale
In this paper, we evaluate the SVS model against the
task of predicting, given a predicate-argument pair,
how appropriate a paraphrase (of either the predicate
or the argument) is in that context. We perform two
experiments that both use the paraphrase task, but
differ in their emphasis. Experiment 1 replicates an
existing evaluation against human judgments. This
evaluation uses synthetic dataset, limited to one par-
ticular construction, and constructed to provide max-
imally distinct paraphrase candidates. Experiment 2
considers a broader class of constructions along with
annotator-generated paraphrase candidates that are
not screened for distinctness. In both experiments,
we compare the SVS model against the state-of-the-
art model by Mitchell and Lapata 2008 (henceforth
M&L; cf. Sec. 2 for model details).
4.2 Parameter choices
Vector space. In our parameterization of the vector
space, we largely follow M&L because their model
has been rigorously evaluated and found to outper-
form a range of other models.
Our first space is a traditional ?bag-of-words? vec-
tor space (BOW, (Lund and Burgess, 1996)). For
each pair of a target word and context word, the BOW
space records a function of their co-occurrence fre-
quency within a surface window of size 10. The
space is constructed from the British National Cor-
pus (BNC), and uses the 2,000 most frequent context
words as dimensions.
We also consider a ?dependency-based? vector
space (SYN, (Pado? and Lapata, 2007)). In this space,
target and context words have to be linked by a ?valid?
dependency path in a dependency graph to count as
co-occurring.2 This space was built from BNC de-
pendency parses obtained from Minipar (Lin, 1993).
For both spaces, we used pre-experiments to com-
pare two methods for the computation of vector com-
ponents, namely raw co-occurrence counts, the stan-
dard model, and the pointwise mutual information
(PMI) definition employed by M&L.
Selectional preferences. We use a simple,
knowledge-lean representation for selectional
preferences inspired by Erk (2007), who models
selectional preference through similarity to seen filler
vectors ~va: We compute the selectional preference
vector for word b and relation r as the weighted
2More specifically, we used the minimal context specification
and plain weight function. See Pado? and Lapata (2007).
901
centroid of seen filler vectors ~va. We collect seen
fillers from the Minipar-parse of the BNC.
Let f(a, r, b) denote the frequency of a occurring
in relation r to b in the parsed BNC, then
Rb(r)SELPREF =
?
a:f(a,r,b)>0
f(a, r, b) ? ~va (5)
We call this base model SELPREF. We will also
study two variants of SELPREF, based on two dif-
ferent hypotheses about what properties of the se-
lectional preferences are particularly important for
meaning adaption. The first model aims specifically
at alleviating noise introduced by infrequent fillers, a
common problem in data-driven approaches. It only
uses fillers seen more often than a threshold ?. We
call this model SELPREF-CUT:
Rb(r)SELPREF-CUT =
?
a:f(a,r,b)>?
f(a, r, b) ? ~va (6)
Our second variant again aims at alleviating noise,
but noise introduced by low-valued dimensions rather
than infrequent fillers. It achieves this by taking each
component of the selectional preference vector to
the nth power. In this manner, dimensions with high
counts are further inflated, while dimensions with low
counts are depressed.3 This model, SELPREF-POW, is
defined as follows: If Rb(r)SELPREF = ?v1, . . . , vm?,
Rb(r)SELPREF-POW = ?v
n
1 , . . . , v
n
m? (7)
The inverse selectional preferences R?1b are de-
fined analogously for all three model variants. We
instantiate the vector combination function  as
component-wise multiplication, following M&L.
Baselines and significance testing. All tasks that
we consider below involve judgments for the mean-
ing of a word a in the context of a word b. A first
baseline that every model must beat is simply using
the original vector for a. We call this baseline ?target
only?. Since we assume that the selectional prefer-
ences of b model the expectations for a, we use b?s
selectional preference vector for the given relation as
a second baseline, ?selpref only?.
3Since we focus on the size-invariant cosine similarity, the
use of this model does not require normalization.
verb subject landmark sim judgment
slump shoulder slouch high 7
slump shoulder decline low 2
slump value slouch low 3
slump value decline high 7
Figure 3: Experiment 1: Human similarity judgements for
subject-verb pair with high- and low-similarity landmarks
Differences between the performance of mod-
els were tested for significance using a stratified
shuffling-based randomization test (Yeh, 2000).4.
5 Exp. 1: Predicting similarity ratings
In our first experiment, we attempt to predict human
similarity judgments. This experiment is a replication
of the evaluation of M&L on their dataset5.
Dataset. The M&L dataset comprises a total of
3,600 human similarity judgements for 120 experi-
mental items. Each item, as shown in Figure 3, con-
sists of an intransitive verb and a subject noun that
are combined with a ?landmark?, a synonym of the
verb that is chosen to be either similar or dissimilar
to the verb in the context of the given subject.
The dataset was constructed by extracting pairs
of subjects and intransitive verbs from a parsed ver-
sion of the BNC. Each item was paired with two
landmarks, chosen to be as dissimilar as possible ac-
cording to a WordNet similarity measure. All nouns
and verbs were subjected to a pretest, where only
those with highly significant variations in human
judgments across landmarks were retained.
For each item of the final dataset, judgements on
a 7-point scale were elicited. For example, judges
considered the compatible landmark ?slouch? to be
much more similar to ?shoulder slumps? than the
incompatible landmark ?decline?. In Figure 3, the
column sim shows whether the experiment designers
considered the respective landmark to have high or
low similarity to the verb, and the column judgment
shows a participant?s judgments.
Experimental procedure. We used cosine to com-
pute similarity to the lexical vector of the landmark.
4The software is available at http://www.nlpado.de/
?sebastian/sigf.html.
5We thank J. Mitchell and M. Lapata for providing their data.
902
Model high low ?
BOW space
Target only 0.32 0.32 0.0
Selpref only 0.46 0.4 0.06**
M&L 0.25 0.15 0.20**
SELPREF 0.32 0.26 0.12**
SELPREF-CUT, ?=10 0.31 0.24 0.11**
SELPREF-POW, n=20 0.11 0.03 0.27**
Upper bound ? ? 0.4
SYN space
Target only 0.2 0.2 0.08**
Selpref only 0.27 0.21 0.16**
M&L 0.13 0.06 0.24**
SELPREF 0.22 0.16 0.13**
SELPREF-CUT, ?=10 0.2 0.13 0.13**
SELPREF-POW, n=30 0.08 0.04 0.22**
Upper bound ? ? 0.4
Table 1: Experiment 1: Mean cosine similarity for items
with high- and low-similarity landmarks; correlation with
human judgements (?). (**: p < 0.01)
?Target only? compares the landmark against the lexi-
cal vector of the verb, and ?selpref only? compares
it to the noun?s subj?1 preference. For the M&L
model, the comparison is to the combined lexical
vectors of verb and noun. For our models SELPREF,
SELPREF-CUT and SELPREF-POW, we combine the
verb?s lexical vector with the subj?1 preference of
the noun. We used a held-out dataset of 10% of the
data to optimize the parameters of ? of SELPREF-CUT
and n of SELPREF-POW. Vectors with PMI compo-
nents could model the data, while raw frequency
components could not; we report only the former.
We use the same two evaluation scores as M&L:
The first score is the average similarity to compatible
landmarks (high) and incompatible landmarks (low).
The second is Spearman?s ?, a nonparametric corre-
lation coefficient. We compute ? between individual
human similarity scores and our predictions. Based
on agreement between human judges, M&L estimate
an upper bound ? of 0.4 for the dataset.
Results and discussion. Table 1 shows the results
of Exp. 1 on the test set. In the upper half (BOW), we
replicate M&L?s main finding that simple component-
wise multiplication of the predicate and argument
vectors results in a highly significant correlation of
Model lex. vector obj?1 selpref
SELPREF 0.23 (0.09) 0.88 (0.07)
SELPREF-CUT (10) 0.20 (0.10) 0.72 (0.18)
SELPREF-POW (30) 0.03 (0.08) 0.52 (0.48)
Table 2: Experiment 1: Average similarity (and standard
deviation) between the inverse subject preferences of a
noun and (left) its lexical vector and (right) inverse object
preferences vector (cosine similarity in SYN space)
? = 0.2, significantly outperforming both baselines.
It is interesting, though, that the subj?1 preference
itself (?Selpref only?) is already highly significantly
correlated with the human judgments.
A comparison of the upper half (BOW) with the
lower half (SYN) shows that the dependency-based
space generally shows better correlation with human
judgements. This corresponds to a beneficial effect of
syntactic information found for other applications of
semantic spaces (Lin, 1998; Pado? and Lapata, 2007).
All instances of the SELPREF model show highly
significant correlations. SELPREF and SELPREF-CUT
show very similar performance. They do better than
both baselines in the BOW space; however, in the
cleaner SYN space, their performance is numerically
lower than using selectional preferences only (? =
0.13 vs. 0.16). SELPREF-POW is always significantly
better than SELPREF and SELPREF-CUT, and shows
the best result of all tested models (? = 0.27, BOW
space). The performance is somewhat lower in the
SYN space (? = 0.22). However, this difference, and
the difference to the best M&L model at ? = 0.24,
are not statistically significant.
The SVS model computes meaning in context by
combining a word?s lexical representation with the
preference vector of its context. In this, it differs from
previous models, including that by M&L, which used
what we have been calling ?direct combination?. So
it is important to ask to what extent this difference
in method translate to a difference in predictions.
We analyzed this by measuring the similarity by the
nouns? lexical vectors, used by direct combination
methods, and their inverse subject preferences, which
SVS uses. The result is shown in the first column
in Table 2, computed as mean cosine similarities
and standard deviations between noun vectors and
selectional preferences. The table shows that these
vectors have generally low similarity, which is further
903
reduced by applying cutoff and potentiation. Thus,
the predictions of SVS will differ from those of direct
combination models like M&L.
A related question is whether syntax-aware vec-
tor combination makes a difference: Does the model
encode different expectations for different syntactic
relations (cf. Example 2)? The second column of Ta-
ble 2 explores this question by comparing inverse se-
lectional preferences for the subject and object slots.
We observe that the similarity is very high for raw
preferences, but becomes lower when noise is elim-
inated. Since the SELPREF-POW model performed
best in our evaluation, we read this as evidence that
potentiation helps to suppress noise introduced by
mis-identified subject and object fillers.
In Experiment 1, all experimental items were
verbs, which means that all disambiguation was done
through inverse selectional preferences. As inverse
selectional preferences are currently largely unex-
plored, it is interesting to note that the evidence that
they provide for the paraphrase task is as strong as
that of the context nouns themselves.
6 Exp. 2: Ranking paraphrases
This section reports on a second, more NLP-oriented
experiment whose task is to distinguish between ap-
propriate and inappropriate paraphrases on a broader
range of constructions.
Dataset. For this experiment, we use the SemEval-
1 lexical substitution (lexsub) dataset (McCarthy and
Navigli, 2007), which contains 10 instances each of
200 target words in sentential contexts, drawn from
Sharoff?s (2006) English Internet Corpus. Contex-
tually appropriate paraphrases for each instance of
each target word were elicited from up to 6 partic-
ipants. Fig. 4 shows two instances for the verb to
work. The distribution over paraphrases can be seen
as a characterization of the target word?s meaning in
each context.
Experimental procedure. In this paper, we pre-
dict appropriate paraphrases solely on the basis of a
single context word that stands in a direct predicate-
argument relation to the target word. We extracted
all instances from the lexsub test data with such a
relation. After parsing all sentences with verbal and
nominal targets with Minipar, this resulted in three
Sentence Substitutes
By asking people who work
there, I have since determined
that he didn?t. (# 2002)
be employed 4;
labour 1
Remember how hard your an-
cestors worked. (# 2005)
toil 4; labour 3;
task 1
Figure 4: Lexical substitution example items for ?work?
sets of sentences: (a), target intransitive verbs with
noun subjects (V-SUBJ, 48 sentences); (b), target tran-
sitive verbs with noun objects (V-OBJ, 213 sent.); and
(c), target nouns occurring as objects of verbs (N-OBJ,
102 sent.).6 Note that since we use only part of the
lexical substitution dataset in this experiment, a di-
rect comparison with results from the SemEval task
is not possible.
As in the original SemEval task, we phrase the
task as a ranking problem. For each target word, the
paraphrases given for all 10 instances are pooled. The
task is to rank the list for each item so that appropriate
paraphrases (such as ?be employed? for # 2002) rank
higher than paraphrases not given (e.g., ?toil?).
Our model ranks paraphrases by their similarity
to the following combinations (Eq. (4)): for V-SUBJ,
verb plus the noun?s subj?1 preferences; for V-OBJ,
verb plus the noun?s obj?1 preferences; and for N-
OBJ, the noun plus the verb?s obj preferences. Our
comparison model, M&L, ranks all paraphrases by
their similarity to the direct noun-verb combination.
To avoid overfitting, we consider only the two mod-
els that performed optimally in in the SYN space in
Experiment 1 (SELPREF-POW with n=30 and M&L).
However, since we found that vectors with raw fre-
quency components could model the data, while PMI
components could not, we only report the former.
For evaluation, we adopt the SemEval ?out of
ten? precision metric POOT. It uses the model?s ten
top-ranked paraphrases as its guesses for appropri-
ate paraphrases. Let Gi be the gold paraphrases for
item i, Mi the model?s top ten paraphrases for i, and
f(s, i) the frequency of s as paraphrase for i:
POOT = 1/|I|
?
i
?
s?Mi?Gi
f(s, i)
?
s?Gi
f(s, i)
(8)
McCarthy and Navigli propose this metric for the
6The specification of this dataset will be made available.
904
Model V-SUBJ V-OBJ N-OBJ
Target only 47.9 47.4 49.6
Selpref only 54.8 51.4 55.0
M&L 50.3 52.0 53.4
SELPREF-POW, n=30 63.1 55.8 56.9
Table 3: Experiment 2: Mean ?out of ten? precision (POOT)
dataset for robustness. Due to the sparsity of para-
phrases, a metric that considers fewer guesses leads
to artificially low results when a ?good? paraphrase
was not mentioned by the annotators by chance but
is ranked highly by a model.
Results and discussion. Table 6 shows the mean
out-of-ten precision for all models. The behavior is
fairly uniform across all three datasets. Unsurpris-
ingly, ?target only?, which uses the same ranking for
all instances of a target, yields the worst results.7
M&L?s direct combination model outperforms ?tar-
get only? significantly (p < 0.05). However, on both
the V-SUBJ and the N-OBJ the ?selpref only? baseline
does better than direct combination. The best results
on all datasets are obtained by SELPREF-POW. The
difference between SELPREF-POW and the ?target
only? baseline is highly significant (p < 0.01). The
difference to M&L?s model is significant at p = 0.05.
We interpret these results as encouraging evidence
for the usefulness of selectional preferences for judg-
ing substitutability in context. Knowledge about the
selectional preferences of a single context word can
already lead to a significant improvement in precision.
We find this overall effect even though the word is
not informative in all cases. For instance, the subject
of item 2002 in Fig. 4, ?who?, presumably helps little
in determining the verb?s context-adapted meaning.
It is interesting that the improvement of SELPREF-
POW over ?selpref only? is smallest for the N-OBJ
dataset (1.9% POOT). N-OBJ uses selectional prefer-
ences for nouns that may fill the direct object position,
, while V-SUBJ and V-OBJ use inverse selectional
preferences for verbs (cf. the two graphs in Fig. 1).
7?Target only? still does very much better than a random
baseline, which performs at 22% POOT.
7 Conclusion
In this paper, we have considered semantic space
models that can account for the meaning of word
occurrences in context. Arguing that existing models
do not sufficiently take syntax into account, we have
introduced the new structured vector space (SVS)
model of word meaning. In addition to a vector rep-
resenting a word?s lexical meaning, it contains vec-
tors representing the word?s selectional preferences.
These selectional preferences play a central role in
the computation of meaning in context.
We have evaluated the SVS model on two datasets
on the task of predicting the felicitousness of para-
phrases in given contexts. On the M&L dataset,
SVS outperforms the state-of-the-art model of M&L,
though the difference is not significant. On the Lex-
ical Substitution dataset, SVS significantly outper-
forms the state-of-the-art. This is especially interest-
ing as the Lexical Substitution dataset, in contrast to
the M&L data, uses ?realistic? paraphrase candidates
that are not necessarily maximally distinct.
The most important limitation of the evaluation
that we have given in this paper is that we have only
considered single words as context. Our next step
will be to integrate information from multiple rela-
tions (such as both the subject and object positions
of a verb) into the computation of context-specific
meaning. Our eventual aim is a model that can give
a compositional account of a word?s meaning in con-
text, where all words in an expression disambiguate
one another according to the relations between them.
We will explore the usability of vector space mod-
els of word meaning in NLP applications, formulated
as the question of how to perform inferences on them
in the context of the Textual Entailment task (Dagan
et al, 2006). Paraphrase-based inference rules play
a large role in several recent approaches to Textual
Entailment (e.g. Szpektor et al(2008)); appropriate-
ness judgments of paraphrases in context, the task of
Experiments 1 and 2 above, can be viewed as testing
the applicability of these inferences rules.
Acknowledgments. Many thanks for helpful dis-
cussion to Jason Baldridge, David Beaver, Dedre
Gentner, James Hampton, Dan Jurafsky, Alexander
Koller, Brad Love, and Ray Mooney.
905
References
C. Brockmann, M. Lapata. 2003. Evaluating and combin-
ing approaches to selectional preference acquisition. In
Proceedings of EACL, 27?34.
I. Dagan, O. Glickman, B. Magnini. 2006. The PASCAL
Recognising Textual Entailment Challenge. In Ma-
chine Learning Challenges, Lecture Notes in Computer
Science, 177?190. Springer.
K. Erk. 2007. A simple, similarity-based model for selec-
tional preferences. In Proceedings of ACL, 216?223.
T. Ferretti, C. Gagne?, K. McRae. 2003. Thematic role fo-
cusing by participle inflections: evidence form concep-
tual combination. Journal of Experimental Psychology,
29(1):118?127.
D. Gildea, D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3):245?
288.
D. Hindle, M. Rooth. 1993. Structural ambiguity and
lexical relations. Computational Linguistics, 19(1):103?
120.
M. Jones, D. Mewhort. 2007. Representing word mean-
ing and order information in a composite holographic
lexicon. Psychological review, 114:1?37.
J. J. Katz, J. A. Fodor. 1964. The structure of a semantic
theory. In The Structure of Language. Prentice-Hall.
A. Kilgarriff. 1997. I don?t believe in word senses. Com-
puters and the Humanities, 31(2):91?113.
W. Kintsch. 2001. Predication. Cognitive Science,
25:173?202.
T. Landauer, S. Dumais. 1997. A solution to Platos prob-
lem: the latent semantic analysis theory of acquisition,
induction, and representation of knowledge. Psycho-
logical Review, 104(2):211?240.
D. Lin. 1993. Principle-based parsing without overgener-
ation. In Proceedings of ACL, 112?120.
D. Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING-ACL, 768?774.
K. Lund, C. Burgess. 1996. Producing high-dimensional
semantic spaces from lexical co-occurrence. Behav-
ior Research Methods, Instruments, and Computers,
28:203?208.
C. D. Manning, P. Raghavan, H. Schu?tze. 2008. Introduc-
tion to Information Retrieval. CUP.
D. McCarthy, J. Carroll. 2003. Disambiguating nouns,
verbs, and adjectives using automatically acquired
selectional preferences. Computational Linguistics,
29(4):639?654.
D. McCarthy, R. Navigli. 2007. SemEval-2007 Task 10:
English Lexical Substitution Task. In Proceedings of
SemEval, 48?53.
S. McDonald, C. Brew. 2004. A distributional model
of semantic context effects in lexical processing. In
Proceedings of ACL, 17?24.
S. McDonald, M. Ramscar. 2001. Testing the distribu-
tional hypothesis: The influence of context on judge-
ments of semantic similarity. In Proceedings of CogSci,
611?616.
K. McRae, M. Spivey-Knowlton, M. Tanenhaus. 1998.
Modeling the influence of thematic fit (and other con-
straints) in on-line sentence comprehension. Journal of
Memory and Language, 38:283?312.
K. McRae, M. Hare, J. Elman, T. Ferretti. 2005. A
basis for generating expectancies for verbs from nouns.
Memory and Cognition, 33(7):1174?1184.
J. Mitchell, M. Lapata. 2008. Vector-based models of
semantic composition. In Proceedings of ACL, 236?
244.
A. Moschitti, S. Quarteroni. 2008. Kernels on linguistic
structures for answer extraction. In Proceedings of
ACL, 113?116, Columbus, OH.
S. Narayanan, D. Jurafsky. 2002. A Bayesian model
predicts human parse preference and reading time in
sentence processing. In Proceedings of NIPS, 59?65.
S. Pado?, M. Lapata. 2007. Dependency-based construc-
tion of semantic space models. Computational Linguis-
tics, 33(2):161?199.
U. Pado?, F. Keller, M. W. Crocker. 2006. Combining syn-
tax and thematic fit in a probabilistic model of sentence
processing. In Proceedings of CogSci, 657?662.
S. Pado?, U. Pado?, K. Erk. 2007. Flexible, corpus-based
modelling of human plausibility judgements. In Pro-
ceedings of EMNLP/CoNLL, 400?409.
P. Resnik. 1996. Selectional constraints: An information-
theoretic model and its computational realization. Cog-
nition, 61:127?159.
G. Salton, A. Wang, C. Yang. 1975. A vector-space model
for information retrieval. Journal of the American So-
ciety for Information Science, 18:613?620.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
S. Sharoff. 2006. Open-source corpora: Using the net to
fish for linguistic data. International Journal of Corpus
Linguistics, 11(4):435?462.
P. Smolensky. 1990. Tensor product variable binding and
the representation of symbolic structures in connection-
ist systems. Artificial Intelligence, 46:159?216.
I. Szpektor, I. Dagan, R. Bar-Haim, J. Goldberger. 2008.
Contextual preferences. In Proceedings of ACL, 683?
691, Columbus, OH.
Y. Wilks. 1975. Preference semantics. In Formal Seman-
tics of Natural Language. CUP.
A. Yeh. 2000. More accurate tests for the statistical
significance of result differences. In Proceeedings of
COLING, 947?953.
906
Formalising Multi-layer Corpora in OWL DL ?
Lexicon Modelling, Querying and Consistency Control
Aljoscha Burchardt1, Sebastian Pad?2?, Dennis Spohr3?, Anette Frank4?and Ulrich Heid3
1Dept. of Comp. Ling. 2Dept. of Linguistics 3Inst. for NLP 4Dept. of Comp. Ling.
Saarland University Stanford University University of Stuttgart University of Heidelberg
Saarbr?cken, Germany Stanford, CA Stuttgart, Germany Heidelberg, Germany
albu@coli.uni-sb.de pado@stanford.edu spohrds,heid@ims.uni-stuttgart.de frank@cl.uni-heidelberg.de
Abstract
We present a general approach to formally
modelling corpora with multi-layered anno-
tation, thereby inducing a lexicon model in a
typed logical representation language, OWL
DL. This model can be interpreted as a graph
structure that offers flexible querying func-
tionality beyond current XML-based query
languages and powerful methods for consis-
tency control. We illustrate our approach by
applying it to the syntactically and semanti-
cally annotated SALSA/TIGER corpus.
1 Introduction
Over the years, much effort has gone into the creation
of large corpora with multiple layers of linguistic an-
notation, such as morphology, syntax, semantics, and
discourse structure. Such corpora offer the possibility
to empirically investigate the interactions between
different levels of linguistic analysis.
Currently, the most common use of such corpora
is the acquisition of statistical models that make use
of the ?more shallow? levels to predict the ?deeper?
levels of annotation (Gildea and Jurafsky, 2002; Milt-
sakaki et al, 2005). While these models fill an im-
portant need for practical applications, they fall short
of the general task of lexicon modelling, i.e., creat-
ing an abstracted and compact representation of the
corpus information that lends itself to ?linguistically
informed? usages such as human interpretation or
integration with other knowledge sources (e.g., deep
grammar resources or ontologies). In practice, this
task faces three major problems:
?At the time of writing, Sebastian Pad? and Dennis Spohr
were affiliated with Saarland University, and Anette Frank with
DFKI Saarbr?cken and Saarland University.
Ensuring consistency. Annotation reliability and
consistency are key prerequisites for the extraction of
generalised linguistic knowledge. However, with the
increasing complexity of annotations for ?deeper? (in
particular, semantic) linguistic analysis, it becomes
more difficult to ensure that all annotation instances
are consistent with the annotation scheme.
Querying multiple layers of linguistic annotation.
A recent survey (Lai and Bird, 2004) found that cur-
rently available XML-based corpus query tools sup-
port queries operating on multiple linguistic levels
only in very restricted ways. Particularly problematic
are intersecting hierarchies, i.e., tree-shaped analyses
on multiple linguistic levels.
Abstractions and application interfaces. A per-
vasive problem in annotation is granularity: The gran-
ularity offered by a given annotation layer may di-
verge considerably from the granularity that is needed
for the integration of corpus-derived data in large
symbolic processing architectures or general lexical
resources. This problem is multiplied when more
than one layer of annotation is considered, for exam-
ple in the characterisation of interface phenomena.
While it may be possible to obtain coarser-grained
representations procedurally by collapsing categories,
such procedures are not flexibly configurable.
Figure 1 illustrates these difficulties with a sentence
from the SALSA/TIGER corpus (Burchardt et al,
2006), a manually annotated German newspaper cor-
pus which contains role-semantic analyses in the
FrameNet paradigm (Fillmore et al, 2003) on top
of syntactic structure (Brants et al, 2002).1 The se-
1While FrameNet was originally developed for English, the
majority of frames has been found to generalise well to other
389
which the official Croatia but in significant international-law difficulties bring would
Figure 1: Multi-layer annotation of a German phrase with syntax and frame semantics (?which would bring
official Croatia into significant difficulties with international law?)
mantic structure consists of frames, semantic classes
assigned to predicating expressions, and the semantic
roles introduced by these classes. The verb bringen
(?to bring?) is used metaphorically and is thus analy-
sed as introducing one frame for the ?literal? reading
(PLACING) and one for the ?understood? reading
(CAUSATION), both with their own role sets.
The high complexity of the semantic structure even
on its own shows the necessity of a device for con-
sistency checking. In conjunction with syntax, it
presents exactly the case of intersecting hierarchies
which is difficult to query. With respect to the issue of
abstraction, note that semantic roles are realised vari-
ously as individual words (was (?which?) ) and con-
stituents (NPs, PPs), a well-known problem in deriv-
ing syntax-semantics mappings from corpora (Frank,
2004; Babko-Malaya et al, 2006).
Our proposal. We propose that the problems in-
troduced above can be addressed by formalising cor-
pora in an integrated, multi-layered corpus and lexi-
con model in a declarative logical framework, more
specifically, the description logics-based OWL DL
formalism. The major benefits of this approach are
that all relevant properties of the annotation and the
underlying model are captured in a uniform represen-
tation and, moreover, that the formal semantics of the
model makes it possible to use general and efficient
knowledge representation techniques for consistency
control. Finally, we can extract specific subsets from
a corpus by defining task-specific views on the graph.
After a short discussion of related approaches in
languages (Burchardt et al, 2006; Boas, 2005).
Section 2, Section 3 provides details on our method-
ology. Sections 4 and 5 demonstrate the benefits of
our strategy on a model of the SALSA/TIGER data.
Section 6 concludes.
2 Related Work
One recent approach to lexical resource modelling
is the Lexical Systems framework (Polgu?re, 2006),
which aims at providing a highly general represen-
tation for arbitrary kinds of lexica. While this is
desirable from a representational point of view, the
resulting models are arguably too generic to support
strong consistency checks on the encoded data.
A further proposal is the currently evolving Lex-
ical Markup Framework (LMF; Francopoulo et
al. (2006)), an ISO standard for lexical resource mod-
elling, and an LMF version of FrameNet exists. How-
ever, we believe that our usage of a typed formalism
takes advantage of a strong logical foundation and
the notions of inheritance and entailment (cf. Schef-
fczyk et al (2006)) and is a crucial step beyond the
representational means provided by LMF.
Finally, the closest neighbour to our proposal is
the ATLAS project (Laprun et al, 2002), which
combines annotations with a descriptive meta-model.
However, to our knowledge, ATLAS only models
basic consistency constraints, and does not capture
dependencies between different layers of annotation.
390
3 Modelling Multilevel Corpora in OWL DL
3.1 A formal graph-based Lexicon
This section demonstrates how OWL DL, a strongly
typed representation language, can serve to transpar-
ently formalise corpora with multi-level annotation.
OWL DL is a logical language that combines the
expressivity of OWL2 with the favourable computa-
tional properties of Description Logics (DL), notably
decidability and monotonicity (Baader et al, 2003).
The strongly typed, well-defined model-theoretic se-
mantics distinguishes OWL DL from recent alterna-
tive approaches to lexicon modelling.
Due to the fact that OWL DL has been defined
in the Resource Description Framework (RDF3), the
first central benefit of using OWL DL is the possibil-
ity to conceive of the lexicon as a graph ? a net-like
entity with a high degree of interaction between lay-
ers of linguistic description, with an associated class
hierarchy. Although OWL DL itself does not have a
graph model but a model-theoretic semantics based
on First Order Logic, we will illustrate our ideas with
reference to a graph-like representation, since this is
what we obtain by transforming our OWL DL files
into an RDFS database.
Each node in the graph instantiates one or more
classes that determine the properties of the node. In
a straightforward sense, properties correspond to la-
belled edges between nodes. They are, however, also
represented as nodes in the graph which instantiate
(meta-)classes themselves.
The model is kept compact by OWL?s support for
multiple instantiation, i.e., the ability of instances
to realise more than one class. For example, in a
syntactically and semantically annotated corpus, all
syntactic units (constituents, words, or even parts
of words) can instantiate ? in addition to a syntac-
tic class ? one or more semantic classes. Multiple
instantiation enables the representation of informa-
tion about several annotation layers within single
instances.
As we have argued in Section 2, we believe that
having one generic model that can represent all cor-
pora is problematic. Instead, we propose to construct
lexicon models for specific types of corpora. The
2http://www.w3.org/2004/OWL/
3http://www.w3.org/RDF/
design of such models faces two central design ques-
tions: (a) Which properties of the annotated instances
should be represented?; (b) How are different types
of these annotation properties modelled in the graph?
Implicit features in annotations. Linguistic anno-
tation guidelines often concentrate on specifying the
linguistic data categories to be annotated. However,
a lot of linguistically relevant information often re-
mains implicit in the annotation scheme. Examples
from the SALSA corpus include, e.g., the fact that
the annotation in Figure 1 is metaphorical. This in-
formation has to be inferred from the configuration
that one predicate evokes two frames. As such infor-
mation about different annotation types is useful in
final lexicon resources, e.g. to define clean generali-
sations over the data (singling out ?special cases?), to
extract information about special data categories, and
to define formally grounded consistency constraints,
we include it in the lexicon model.
Form of representation. All relevant information
has to be represented either as assertional statements
in the model graph (i.e., nodes connected by edges),
or as definitional axioms in the class hierarchy.4
This decision involves a fundamental trade-off be-
tween expressivity and flexibility. Modelling features
as axioms in the class hierarchy imposes definitional
constraints on all instances of these classes and is
arguably more attractive from a cognitive perspec-
tive. However, modelling features as entities in the
graph leads to a smaller class hierarchy, increased
querying flexibility, and more robustness in the face
of variation and noise in the data.
3.2 Modelling SALSA/TIGER Data
We now illustrate these decisions concretely by de-
signing a model for a corpus with syntactic and
frame-semantic annotation, more concretely the
SALSA/TIGER corpus. However, the general points
we make are valid beyond this particular setting.
As concerns implicit annotation features, we have
designed a hierarchy of annotation types which now
explicitly expresses different classes of annotation
phenomena and which allows for the definition of
annotation class-specific properties. For example,
frame targets are marked as a multi-word target if
4This choice corresponds to the DL distinction between TBox
(?intensional knowledge?) and ABox (?extensional knowledge?).
391
L
in
gu
is
ti
c
m
od
el
? Frames
w Intentionally_affect
w Placing
w Motion, . . .
? Roles
w Intentionally_affect.Act
w Placing.Means
? TIGER edge labels and POS
w SB, OA, PPER, ADJA, . . .
? Generalised functions and categories
w subj, obj, NounP, AdjP, . . .
A
nn
ot
at
io
n
ty
pe
s
? Frame Annotations
w Simple
w Metaphoric
w Underspecified
? Role Annotations
w Simple
w Underspecified
? Target Annotations
w Single-word targets
w Multi-word targets
? Sentences, syntactic units, . . .
Figure 2: Schema of the OWL DL model?s class hierarchy (?TBox?)
their span contains at least two terminal nodes. The
hierarchy is shown on the right of Figure 2, which
shows parts of the bipartite class hierarchy.
The left-hand side of Figure 2 illustrates the lin-
guistic model, in which frames and roles are organ-
ised according to FrameNet?s inheritance relation.
Although this design seems to be straightforward, it
is the result of careful considerations concerning the
second design decision. Since FrameNet is a hierar-
chically structured resource with built-in inheritance
relations, one important question is whether to model
individual frames, such as SELF_MOTION or LEAD-
ERSHIP, and their relations either as instances of a
general class Frame and as links between these in-
stances, or as hierarchically structured classes with
richer axiomatisation. In line with our focus on con-
sistency checking, we adopt the latter option, which
allows us to use built-in reasoning mechanisms of
OWL DL to ensure consistency.
Annotation instances from the corpus instantiate
multiple classes in both hierarchies (cf. Figure 2): On
the annotation side according to their types of phe-
nomena; on the linguistic side based on their frames,
roles, syntactic functions, and categories.
Flexible abstraction. Section 1 introduced granu-
larity as a pervasive problem in the use of multi-level
corpora. Figure 2 indicates that the class hierarchy
of the OWL DL model offers a very elegant way
of defining generalised data categories that provide
abstractions over model classes, both for linguistic
categories and annotation types. Moreover, proper-
ties can be added to each abstracting class and then
be used, e.g., for consistency checking. In our case,
Figure 2 shows (functional) edge labels and part-of-
speech tags provided by TIGER, as well as sets of
(largely theory-neutral) grammatical functions and
categories that subsume these fine-grained categories
and support the extraction of generalised valence in-
formation from the lexicon.
An annotated corpus sentence. To substantiate
the above discussion, Figure 3 shows a partial lexicon
representation of the example in Figure 1. The boxes
represent instance nodes, with classes listed above
the horizontal line, and datatype properties below
it.5 The links between these instances indicate OWL
object properties which have been defined for the
instantiated classes. For example, the metaphorical
PLACING frame is shown as a grey box in the middle.
Multiple inheritance is indicated by instances
carrying more than one class, such as the in-
stance in the left centre, which instantiates the
classes SyntacticUnit, NP, OA, NounP and
obj. Multi-class instances inherit the properties
of each of these classes, so that e.g., the meta-
phoric frame annotation of the PLACING frame
in the middle has both the properties defined for
frames (hasCoreRole) and for frame annotations
(hasTarget). The generalised syntactic categories
discussed above are given in italics (e.g., NounP).
The figure highlights the model?s graph-based
structure with a high degree of interrelation between
the lexicon entities. For example, the grey PLAC-
ING frame instance is directly related to its roles
(left, bottom), its lexical anchor (right), the surround-
ing sentence (top), and a flag (top left) indicating
metaphorical use.
5For the sake of simplicity, we excluded explicit ?is-a? links.
392
MetaphoricFrameAnnotationUspFrameAnnotationCausation
SyntacticUnitPRELSSB
hasTigerIDhasContent
NounPsubj
SyntacticUnitNENKhasTigerIDhasContent s2910_17"Kroatien"
Source
SyntacticUnitNPOA
hasTigerIDhasContent
SimpleRoleAnnotationPlacing.ThemehasContent
UspFrameAnnotationSupport LemmahasLemma
LexicalUnitrdf:ID bringen.Placing
SingleWordTargethasContent
SyntacticUnitVVINFHDhasTigerIDhasContent
SyntacticUnitNNNKhasTigerIDhasContent
NounPobj
s2910_15
"das"
s2910_502
"das offizielle Kroatien"
s2910_14
"was"SyntacticUnitARTNKhasTigerIDhasContent
"bringen"
"bringen"
SimpleRoleAnnotationPlacing.CausehasContent "was"
SentenceAnnotationhasSentenceIDhasContent s2910"Die Ausrufung des ..."
MetaphoricFrameAnnotationPlacing
"das offizielle Kroatien"
SimpleRoleAnnotationPlacing.GoalhasContent "in betr?chtliche v?lker..."
"bringen"s2910_23
"Schwierigkeiten"s2910_22
consistsOf
isAssignedTo hasFlag
hasFrameAnnotation hasFrameAnnotation
isUspWith
hasFrameAnnotation
hasCoreRole
hasCoreRoleisAssignedTo
hasCoreRole
hasTargetisTargetOf isAssignedTo hasHead
hasAnnotation?Instance
hasReadingisReadingOf
isAnnotationInstanceOf
isAssignedToSyntacticUnitADJANKhasTigerIDhasContent s2910_16"offizielle"
consistsOf consistsOf
...
Figure 3: Partial lexicon representation of an annotated corpus sentence
4 Querying the Model
We now address the second desideratum introduced
in Section 1, namely a flexible and powerful query
mechanism. For OWL DL models, such a mecha-
nism is available in the form of the Sesame (Broekstra
et al, 2002) SeRQL query language. Since SeRQL
makes it possible to extract and view arbitrary sub-
graphs of the model, querying of intersective hierar-
chies is possible in an intuitive manner.
An interesting application for this querying mecha-
nism is to extract genuine lexicon views on the corpus
annotations, e.g., to extract syntax-semantics map-
ping information for particular senses of lemmas, by
correlating role assignments with deep syntactic in-
formation. These can serve both for inspection and
for interfacing the annotation data with deep gram-
matical resources or general lexica. Applied to our
complete corpus, this ?lexicon? contains on average
8.5 role sets per lemma, and 5.6 role sets per frame.
The result of such a query is illustrated in Table 1 for
the lemma senken (?to lower?).
From such view, frame- or lemma-specific role
sets, i.e., patterns of role-category-function assign-
ments can easily be retrieved. A typical example is
given in Table 2, with additional frequency counts.
The first row indicates that the AGENT role has been
realised as a (deep) subject noun phrase and the ITEM
as (deep) object noun phrase.
We found that generalisations over corpus cate-
gories encoded in the class hierarchies are central
Role Cat Func Freq
Item NounP obj 26
Agent NounP subj 15
Difference PrepP mod-um 6
Cause NounP subj 4
Value_2 PrepP mod-auf 3
Value_2 PrepP pobj-auf 2
Value_1 PrepP mod-von 1
Table 1: Role-category-function assignments for
senken / CAUSE_CHANGE_OF_SCALAR_POSITION (CCSP)
Role set for senken / CCSP Freq
Agent Item 11
subj obj
NounP NounP
Cause Item 4
subj obj
NounP NounP
Item 4
obj
NounP
Agent Item Difference 2
subj obj mod-um
NounP NounP PrepP
Table 2: Sample of role sets for senken / CCSP
to the usefulness of the resulting patterns. For ex-
ample, the number of unique mappings between se-
mantic roles and syntactic categories in our corpus
is 5,065 for specific corpus categories, and 2,289 for
abstracted categories. Thus, the definition of an ab-
straction layer, in conjunction with a flexible query
mechanism, allows us to induce lexical characterisa-
tions of the syntax-semantics mapping ? aggregated
393
and generalised from disparate corpus annotations.
Incremental refinements. Querying, and the re-
sulting lexical views, can serve yet another purpose:
Such aggregates make it possible to conduct a data-
driven search for linguistic generalisations which
might not be obvious from a theoretical perspective,
and allow quick inspection of the data for counterex-
amples to plausible regularities.
In the case of semantic roles, for example, such
a regularity would be that semantic roles are not
assigned to conflicting grammatical functions (e.g.,
deep subject and object) within a given lemma. How-
ever, some of the role sets we extracted contained
exactly such configurations. Further inspection re-
vealed that these irregularities resulted from either
noise introduced by errors in the automatic assign-
ment of grammatical functions, or instances with
syntactically non-local role assignments.
Starting from such observations, our approach sup-
ported a semi-automatic, incremental refinement of
the linguistic and annotation models, in this case in-
troducing a distinction between local and non-local
role realisations.
Size of the lexicon. Using a series of SeRQL
queries, we have computed the size of the cor-
pus/lexicon model for the SALSA/TIGER data (see
Table 3). The lexicon model architecture as described
in Section 3 results in a total of more than 304,000
instances in the lexicon, instantiating 581 different
frame classes and 1,494 role classes.
5 Consistency Control
The first problem pointed out in Section 1 was the
need for efficient consistency control mechanisms.
Our OWL DL-based model in fact offers two mech-
anisms for consistency checking: axiom-based and
query-based checking.
Axiom-based checking. Once some constraint has
been determined to be universally applicable, it can
be formulated in Description Logics in the form of
axiomatic expressions on the respective class in the
model. Although the general interpretation of these
axioms in DL is that they allow for inference of new
statements, they can still be used as a kind of well-
formedness ?constraint?. For example, if an individ-
ual is asserted as an instance of a particular class, the
Type No. of instances
Lemmas 523
Lemma-frame pairs (LUs) 1,176
Sentences 13,353
Syntactic units 223,302
Single-word targets 16,268
Multi-word targets 258
Frame annotations 16,526
Simple 14,700
Underspecified 995
Metaphoric 785
Elliptic 107
Role annotations 31,704
Simple 31,112
Underspecified 592
Table 3: Instance count based on the first SALSA
release
reasoner will detect an inconsistency if this instance
does not adhere to the axiomatic class definition. For
semantic role annotations, axioms can e.g. define the
admissible relations between a particular frame and
its roles. This is illustrated in the DL statements be-
low, which express that an instance of PLACING may
at most have the roles GOAL, PATH, etc.
Placing v ?.hasRole (Placing.Goal unionsq Placing.Path unionsq . . .)
Placing v ?.hasRole (Placing.Goal unionsq Placing.Path unionsq . . .)
Relations between roles can be formalised in a
similar way. An example is the excludes relation in
FrameNet, which prohibits the co-occurrence of roles
like CAUSE and AGENT of the PLACING frame. This
can be expressed by the following statement.
Placing v ?((?.hasRole Placing.Cause)u
(?.hasRole Placing.Agent))
The restrictions are used in checking the consistency
of the semantic annotation; violations of these con-
straints lead to inconsistencies that can be identified
by theorem provers. Although current state-of-the-art
reasoners do not yet scale to the size of entire cor-
pora, axiom-based checking still works well for our
data due to SALSA?s policy of dividing the original
TIGER corpus into separate subcorpora, each deal-
ing with one particular lemma (cf. Scheffczyk et al
(2006)).
394
Query-based checking. Due to the nature of our
graph representation, constraints can combine dif-
ferent types of information to control adherence to
annotation guidelines. Examples are the assignment
of the SUPPORTED role of support verb constructions,
which ought to be assigned to the maximal syntactic
constituent projected by the supported noun, or the
exclusion of reflexive pronouns from the span of the
target verb. However, the consistency of multi-level
annotation is often difficult to check: Not only are
some types of classification (e.g. assignment of se-
mantic classes) inherently difficult; the annotations
also need to be considered in context. For such cases,
axiom-based checking is too strict. In practice, it is
important that manual effort can be reduced by auto-
matically extracting subsets of ?suspicious? data for
inspection. This can be done using SeRQL queries
which ? in contrast to the general remarks on the
scalability of reasoners ? are processed and evaluated
very quickly on the entire annotated corpus data.
Example queries that we formulated examine sus-
picious configurations of annotation types, such as
target words evoking two or more frame annota-
tions which are neither marked as underspecified nor
tagged as a pair of (non-)literal metaphorical frame
annotations. Here, we identified 8 cases of omitted
annotation markup, namely 4 missing metaphor flags
and 4 omitted underspecification links.
On the semantic level, we extracted annotation
instances (in context) for metaphorical vs. non-
metaphorical readings, or frames that are involved
in underspecification in certain sentences, but not in
others. While the result sets thus obtained still re-
quire manual inspection, they clearly illustrate how
the detection of inconsistencies can be enhanced by
a declarative formalisation of the annotation scheme.
Another strategy could be to concentrate on frames
or lemmas exhibiting proportionally high variation
in annotation (Dickinson and Meurers, 2003).
6 Conclusion
In this paper, we have constructed a Description
Logics-based lexicon model directly from multi-layer
linguistic corpus annotations. We have shown how
such a model allows for explicit data modelling, and
for flexible and fine-grained definition of various de-
grees of abstractions over corpus annotations.
Furthermore, we have demonstrated that a pow-
erful logical formalisation which integrates an un-
derlying annotation scheme can be used to directly
control consistency of the annotations using general
KR techniques. It can also overcome limitations
of current XML-based search tools by supporting
queries which are able to connect multiple levels of
linguistic analysis. These queries can be used vari-
ously as an additional means of consistency control,
to derive quantitative tendencies from the data, to
extract lexicon views tailored to specific purposes,
and finally as a general tool for linguistic research.
Acknowledgements
This work has been partly funded by the German
Research Foundation DFG (grant PI 154/9-2). We
also thank the two anonymous reviewers for their
valuable comments and suggestions.
References
Franz Baader, Diego Calvanese, Deborah L. McGuinness,
Daniele Nardi, and Peter F. Patel-Schneider. 2003.
The Description Logic Handbook: Theory, Implemen-
tation and Applications. CUP.
Olga Babko-Malaya, Ann Bies, Ann Taylor, Szuting Yi,
Martha Palmer, Mitch Marcus, Seth Kulick, and Li-
bin Shen. 2006. Issues in Synchronizing the English
Treebank and PropBank. In Proceedings of the COL-
ING/ACL Workshop on Frontiers in Linguistically An-
notated Corpora, Sydney.
Hans C. Boas. 2005. Semantic frames as interlingual
representations for multilingual lexical databases. In-
ternational Journal of Lexicography, 18(4):445?478.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Jeen Broekstra, Arjohn Kampman, and Frank van Herme-
len. 2002. Sesame: A generic architecture for storing
and querying RDF and RDF Schema. In Proceedings
of the 1st ISWC, Sardinia.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pad?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th LREC,
Genoa.
Markus Dickinson and W. Detmar Meurers. 2003. De-
tecting errors in part-of-speech annotation. In Pro-
ceedings of the 10th EACL, Budapest.
395
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to FrameNet.
International Journal of Lexicography, 16:235?250.
Gil Francopoulo, Monte George, Nicoletta Calzolari,
Monica Monachini, Nuria Bel, Mandy Pet, and Clau-
dia Soria. 2006. LMF for multilingual, specialized
lexicons. In Proceedings of the 5th LREC, Genoa.
Anette Frank. 2004. Generalisations over corpus-
induced frame assignment rules. In Proceedings of the
LREC Workshop on Building Lexical Resources From
Semantically Annotated Corpora, Lisbon.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Catherine Lai and Steven Bird. 2004. Querying and up-
dating treebanks: A critical survey and requirements
analysis. In Proceedings of the Australasian Language
Technology Workshop, Sydney.
Christophe Laprun, Jonathan Fiscus, John Garofolo, and
Sylvain Pajot. 2002. Recent Improvements to the AT-
LAS Architecture. In Proceedings of HLT 2002, San
Diego.
Eleni Miltsakaki, Nikhil Dinesh, Rashmi Prasad, Ar-
avind Joshi, and Bonnie Webber. 2005. Exper-
iments on sense annotations and sense disambigua-
tion of discourse connectives. In Proceedings of the
Fourth Workshop on Treebanks and Linguistic Theo-
ries, Barcelona, Spain.
Alain Polgu?re. 2006. Structural properties of lexi-
cal systems: Monolingual and multilingual perspec-
tives. In Proceedings of the COLING/ACL Workshop
on Multilingual Language Resources and Interoper-
ability, Sydney.
Jan Scheffczyk, Collin F. Baker, and Srini Narayanan.
2006. Ontology-based reasoning about lexical re-
sources. In Proceedings of the 5th OntoLex, Genoa.
396
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 57?65,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Paraphrase assessment in structured vector space:
Exploring parameters and datasets
Katrin Erk
Department of Linguistics
University of Texas at Austin
katrin.erk@mail.utexas.edu
Sebastian Pad?
Department of Linguistics
Stanford University
pado@stanford.edu
Abstract
The appropriateness of paraphrases for words de-
pends often on context: ?grab? can replace ?catch?
in ?catch a ball?, but not in ?catch a cold?. Struc-
tured Vector Space (SVS) (Erk and Pad?, 2008) is
a model that computes word meaning in context
in order to assess the appropriateness of such para-
phrases. This paper investigates ?best-practice? pa-
rameter settings for SVS, and it presents a method to
obtain large datasets for paraphrase assessment from
corpora with WSD annotation.
1 Introduction
The meaning of individual occurrences or tokens of
a word can change vastly according to its context. A
central challenge for computational lexical semantics
is describe these token meanings and how they can be
computed for new occurrences.
One prominent approach to this question is the
dictionary-based model of token meaning: The differ-
ent meanings of a word are a set of distinct, disjoint
senses enumerated in a lexicon or ontology, such as
WordNet. For each new occurrence, determining token
meaning means choosing one of the senses, a classifica-
tion task known as Word Sense Disambiguation (WSD).
Unfortunately, this task has turned out to be very hard
both for human annotators and for machines (Kilgarriff
and Rosenzweig, 2000), not at least due to granularity
problems with available resources (Palmer et al, 2007;
McCarthy, 2006). Some researchers have gone so far
as to suggest fundamental problems with the concept of
categorical word senses (Kilgarriff, 1997; Hanks, 2000).
An interesting alternative is offered by vector space
models of word meaning (Lund and Burgess, 1996; Mc-
Donald and Brew, 2004) which characterize the mean-
ing of a word entirely without reference to word senses.
Word meaning is described in terms of a vector in a high-
dimensional vector space that is constructed with dis-
tributional methods. Semantic similarity is then simply
distance to vectors of other words. Vector space models
have been most successful in modeling the meaning of
word types (i.e. in constructing type vectors). The char-
acterization of token meaning by corresponding token
vectors would represent a very interesting alternative to
dictionary-based methods by providing a direct, graded,
unsupervised measure of (dis-)similarity between words
in context that completely avoids reference to dictionary
senses. However, there are still considerable theoretical
and practical problems, even though there is a substan-
tial body of work (Landauer and Dumais, 1997; Sch?tze,
1998; Kintsch, 2001; Mitchell and Lapata, 2008).
In a recent paper (Erk and Pad?, 2008), we have intro-
duced the structured vector space ( SVS) model which
addresses this challenge. It yields one token vector per
input word. Token vectors are not computed by com-
bining the lexical meaning of the surrounding words ?
which risks resulting in a ?topicality? vector ? but by
modifying the type meaning of a word with the semantic
expectations of syntactically related words, which can
be thought of as selectional preferences. For example,
in catch a ball, the token vector for ball is computed by
combining the type vector of ball with a vector for the
selectional preferences of catch for its object. The to-
ken vector for catch, conversely, is constructed from the
type vector of catch and the inverse object preference
vector of ball. The resulting token vectors describe the
meaning of a word in a particular sentence not through a
sense label, but through the distance of the token vector
to other vectors.
A natural question that arises is how vector-based
models of token meaning can be evaluated. It is of
course possible to apply them to a traditional WSD
task. However, this strategy remains vulnerable to all
criticism concerning the annotation of categorical word
senses, and also does not take advantage of the vec-
tor models? central asset, namely gradedness. Thus,
paraphrase-based assessment for models of token mean-
ing was proposed as a representation-neutral disam-
biguation task that can replace WSD (McCarthy and
Navigli, 2007; Mitchell and Lapata, 2008). Given a
word token in context and a set of potential paraphrases,
the task consists of identifying the subset of valid para-
phrases. For example, in the following example, the
first paraphrase is appropriate, but the second is not:
(1) Google acquired YouTube ?
Google bought YouTube
(2) How children acquire skills 6?
How children buy skills
This task is graded in the sense that there is no dis-
joint set of labels from which exactly one is picked for
each token; rather, the paraphrases form a set of labels
of which a subset is appropriate for each word token,
57
and the appropriate sets for two tokens may overlap to
varying degrees. In an ideal vector-based model, valid
paraphrases such as (1) should possess similar vectors,
and invalid ones such as (2) dissimilar ones.
In Erk and Pad? (2008), we evaluated SVS on two
variants of the paraphrase assessment test: first, the pre-
diction of human judgments on a seven-point scale for
paraphrases for verb-subject pairs (Mitchell and Lap-
ata, 2008); and second, the original Lexical Substitution
task by McCarthy and Navigli (2007). To avoid overfit-
ting, we optimized our parameters on the first dataset
and evaluated only the best model on the second dataset.
However, given evidence for substantial inter-task differ-
ences, it is unclear to what extent these parameters are
optimal beyond the Mitchell and Lapata dataset. This
paper addresses this question with two experiments:
Impact of parameters. We re-examine three central
parameters of SVS. The first one is the choice of vector
combination function. Following Mitchell and Lap-
ata (2008), we previously used componentwise multi-
plication, whose interpretation in vector space is not
straightforward. The second one is reweighting. We
obtained the best performance when the context expec-
tations were reweighted by taking each component to
a (high) n-th power, which is counterintuitive. Finally,
we found subjects to be more informative in judging
the appropriateness of paraphrases than objects. This
appears to contradict work in theoretical syntax (Levin
and Rappaport Hovav, 2005).
To reassess the role of these parameters, we construct
a controlled dataset of transitive instances from the Lex-
ical Substitution corpus to reexamine and investigate
these issues, with the aim of providing ?best practice?
settings for SVS. This turns out to be more difficult than
expected, leading us to suspect that a globally optimal
parameter setting across tasks may simply not exist. We
also test a simple extension of SVS that uses a richer
context (both subject and object) to construct the token
vector, with first positive results.
Dataset creation. The Lexical Substitution dataset
used in Erk and Pad? (2008) was very small, which lim-
its the conclusions that can be drawn from it. This points
towards a more general problem of paraphrase-based
assessment for models of token meaning: Until now, all
datasets for this task were specifically created by hand.
It would provide a strong boost for paraphrase assess-
ment if the large annotated corpora that are available for
WSD could be reused.
We present an experiment on converting the WordNet-
annotated SemCor corpus into a set of ?pseudo-
paraphrases? for paraphrase-based assessment. We use
the synonyms and direct hypernyms of an annotated
synset as these ?pseudo-paraphrases?. While the syn-
onyms and hypernyms are not guaranteed to work as
direct replacements of the target word in the given con-
text, they are semantically similar to the target word.
The result is a dataset ten times larger than the Lex-
Sub dataset. As we describe in this paper, we find that
this method is nevertheless problematic: The resulting
dataset is considerably more difficult to model than the
existing hand-built paraphrase corpora, and its proper-
ties differ considerably from the manually constructed
Lexical Substitution dataset.
2 The structured vector space model
The main intuition behind the SVS model is to treat the
interpretation of a word in context as guided by expecta-
tions about typical events. This move to include typical
arguments and predicates into a model of word meaning
is motivated both on cognitive and linguistic grounds.
In cognitive science, the central role of expectations
about typical events on almost all aspects of human
language processing is well-established (McRae et al,
1998; Narayanan and Jurafsky, 2002). In linguistics, ex-
pectations have long been used in semantic theories in
the form of selectional restrictions and selectional pref-
erences (Wilks, 1975), and more recently induced from
corpora (Resnik, 1996). Attention has mostly been lim-
ited to selectional preferences of verbs, which have been
used for for a variety of tasks (Hindle and Rooth, 1993;
Gildea and Jurafsky, 2002). A recent result that the SVS
model builds on is that selectional preferences can be
represented as prototype vectors constructed from seen
arguments (Erk, 2007; Pad? et al, 2007).
Representing lemma meaning. To accommodate in-
formation about semantic expectations, the SVS model
extends the traditional representation of word meaning
as a single vector by a set of vectors, each of which
represents the word?s selectional preferences for each
relation that the word can assume in its linguistic con-
text. While we ultimately think of these relations as
?properly semantic? in the sense of semantic roles, the
instantiation of SVS we consider in this paper makes
use of dependency relations as a level of representation
that generalizes over a substantial amount of surface
variation but that can be obtained automatically with
high accuracy using current NLP tools.
The idea is illustrated in Figure 1. In the representa-
tion of the verb catch, the central square stands for the
lexical vector of catch itself. The three arrows link it to
catch ?s preferences for dependency relations it can par-
ticipate in, such as for its subjects, its objects, and for
verbs for which it appears as a complement (comp?1).
The figure shows the head words that enter into the com-
putation of the selectional preference vector. Likewise,
ball is represented by one vector for ball itself, one for
ball ?s preferences for its modifiers (mod), and two for
the verbs of which it can occur as a subject (subj?1)
and an object (obj?1), respectively.
This representation includes selectional preferences
(like subj, obj, mod) exactly parallel to inverse selec-
tional preferences (subj?1, obj?1, comp?1). The SVS
model is then formalized as follows. Let D be a vector
space, and let R be some set of relation labels. We then
58
catch
he
fielder
dog
cold
baseball
drift
objsubj
accuse
say
claim
comp
-1
ball
whirl
fly
provide
throw
catch
organise
obj
-1
subj
-1
mod
red
golf
elegant
Figure 1: Structured Vector Space representations for
noun ball and verb catch : Each box represents one
vector (lexical information or expectations)
represent the meaning of a lemma w as a triple
m(w) = (vw, R,R
?1)
where vw ? D is the type vector of the word w itself,
R : R ? D maps each relation label onto a vector
that describes w?s selectional preferences, and R?1 :
R ? D maps from role labels to vectors describing
inverse selectional preferences of w. Both R and R?1
are partial functions. For example, the direct object
preference is undefined for intransitive verbs.1
Computing meaning in context. SVS computes the
meaning of a word a in the context of another word
b via their selectional preferences as follows: Let
m(a) = (va, Ra, R?1a ) and m(b) = (vb, Rb, R
?1
b ) be
the representations of the two words, and let r ? R
be the relation linking a to b. Then, the meaning of a
and b in this context is defined as a pair of structured
vector triples: m(a
r
? b) is the meaning of a with b as
its r-argument, and m(b
r?1
? a) the meaning of b as the
r-argument of a:
m(a
r
? b) =
(
va R
?1
b (r), Ra ? {r}, R
?1
a
)
m(b
r?1
? a) =
(
vb Ra(r), Rb, R
?1
b ? {r}
)
(3)
where v1  v2 is a direct vector combination function
as in traditional models, e.g. addition or component-
wise multiplication. If either Ra(r) or R
?1
b (r) are not
defined, the combination fails. Afterward, the filled
argument position r is deleted from Ra and R
?1
b .
Figure 2 illustrates the procedure on the representa-
tions from Figure 1. The dotted lines indicate that the
lexical vector for catch is combined with the inverse
object preference of ball. Likewise, the lexical vector
for ball combines with the object preference vector of
catch.
Recursive application. In Erk and Pad? (2008), we
considered only one combination step; however, the
1We use separate functions R, R?1 rather than a joint
syntactic context preference function because (a) this sepa-
ration models the conceptual difference between predicates
and arguments, and (b) it allows for a simpler, more elegant
formulation of the computation of meaning in context in Eq. 3.
catch
...
cold
baseball
drift
obj
subj
...
comp
-1
ball
...
throw
catch
organise
obj
-1
subj
-1
mod
...
!
!
Figure 2: Combining predicate and argument via
relation-specific semantic expectations
syntactic context of a word in a dependency tree often
consists of more than one word. It seems intuitively
plausible that disambiguation should profit from more
context information. Thus, we extend SVS with recur-
sive application. Let a stand in relation r to b. As
defined above, the result of combining m(a) and m(b)
by relation r are two structured vector triples m(a
r
? b)
and m(b
r?1
? a). If a also stands in relation s 6= r to
a word c with m(c) = (va, Ra, R?1a ), we define the
meaning of a in the context of b and c canonically as
m(m(a
r
? b)
s
? c) =
(
(va R
?1
b (r)) R
?1
c (s),
Ra ? {r, s}, R?1a
)
(4)
If  is associative and commutative, then m(m(a
r
?
b)
s
? c) = m(m(a
s
? c)
r
? b). This will be the case
for all the combination functions we use in this paper.
Note that this is a simplistic model of the influence
of multiple context words: it computes only lexical
meaning recursively, but does not model the influence
of context on the selectional preferences. For example,
the subject selectional preferences of catch are identical
to those of catch the ball, even though one would ex-
pect that the outfielder corresponds much better to the
expectations of catch the ball than of just catch.
3 Experimental Setup
The task that we are considering is paraphrase assess-
ment in context. Given a predicate-argument pair and
a paraphrase candidate, the models have to decide how
appropriate the paraphrase is for the predicate-argument
combination. This is the main task against which token
vector models have been evaluated in the past (Mitchell
and Lapata, 2008; Erk and Pad?, 2008). In Experi-
ment 1, we use manually created paraphrases. In Exper-
iment 2, we replaces human-generated paraphrases with
?pseudo-paraphrases?, contextually similar words that
may not be completely appropriate as paraphrases in the
given context, but can be collected automatically. Our
parameter choices for SVS are as similar as possible to
the second experiment of our earlier paper.
Vector space. We use a dependency-based vector
space that counts a target word and a context word
59
as co-occurring in a sentence if they are connected by
an ?informative? path in the dependency graph for the
sentence.2 We build the space from a Minipar-parsed
version of the British National Corpus with dependency
parses obtained from Minipar (Lin, 1993). It uses raw
co-occurrence counts and 2000 dimensions.
Selectional preferences and reweighting. We use
a prototype-based selectional preference model (Erk,
2007). It models the selectional preferences of a predi-
cate for an argument position as the weighted centroid
of the vectors for all head words seen for this position
in a large corpus. Let f(a, r, b) denote the frequency of
a occurring in relation r to b in the parsed BNC. Then,
we compute the selectional preferences as:
R?b(r) =
1
N
?
a:f(a,r,b)>0
f(a, r, b) ? ~va (5)
where N is the number of fillers a with f(a, r, b) > 0.
In Erk and Pad? (2008), we found that applying a
reweighting step to the selectional preference vector by
taking each component of the centroid vector R?b(r) to
the n-th power lead to substantial improvements. The
motivation for this technique is to alleviate noise aris-
ing from the use of unfiltered head words for the con-
struction. The reweighted selectional preference vector
Rb(r) is defined as:
Rb(r) = ?v
n
1 , . . . , v
n
m? for R
?
b(r) = ?v1, . . . , vm? (6)
where we write ?v1, . . . , vm? for the sequence of values
that make up a vector R?b(r). Inverse selectional pref-
erences R?1b (r) of nouns are defined analogously, by
computing the centroid of the verbs seen as governors
of the noun in relation r.
In this paper, we test reweighting parameters of n be-
tween 0.5 and 30. Generally, small ns will decrease the
influence of the selectional preference vector. The result
can be thought of as a ?word type vector modified by
context expectations?, while large ns increase the role
of context, until we arrive at a ?contextual expectation
vector modified by the word type vector?. 3
Vector combination. We test three vector combina-
tion functions , which have different interpretations
in vector space. The simplest one is componentwise
addition, abbreviated as add, i.e., simple vector addi-
tion.4 With addition, context dimensions receive a high
count whenever either of the two vectors has a high
co-occurrence count for the context.
2We used the minimal context specification and plain
weight of the DependencyVectors software package.
3For the component-wise minimum combination (see be-
low), where we normalize the vectors before the combination,
the reweighting has a different effect. It shifts most of the mass
onto the largest-value dimensions and sets smaller dimensions
to values close to zero.
4Since we subsequently focus on cosine similarity, which
is length-invariant, vector addition can also be interpreted as
centroid computation.
Next, we test component-wise multiplication (mult).
This operation is more difficult to interpret in terms of
vector space, since it does not correspond to the standard
inner or outer vector products. The most straightforward
interpretation is to reinterpret the second vector as a di-
agonal matrix, i.e., as a linear transformation of the first
vector. Large entries in the second vector increase the
weight of the corresponding contexts; small entries de-
crease it. Mitchell and Lapata (2008) found this method
to yield the best results.
The third vector combination function we consider
is component-wise minimum (min). This combination
function results in a vector with high counts only for
contexts which co-occur frequently with both input vec-
tors and can thus be understood as an intersection be-
tween the two context sets. Since the entries of two
vectors need to be on the same order to magnitude for
this method to yield meaningful results, we normalize
vectors before the combination for min.
Assessing models of token meaning. Given a transi-
tive verb v with subject a and direct object b, we test
three variants of computing a token vector for v. The
first two involve only one combination step. In the subj
condition, v?s type vector is combined with the inverse
subject preference vector of a. In the obj condition, v?s
type vector is combined with the inverse object pref-
erence vector of b. The third variant is the recursive
application of the SVS combination procedure described
in Section 2 (condition both). Specifically, we combine
v?s type vector with both a?s inverse subject preference
and with b?s inverse object preference to obtain a ?richer?
token vector.
In all three cases, the resulting token vector is com-
pared to the type vector of the paraphrase (in Experi-
ment 1) or the semantically related word (in Experiment
2). We use Cosine Similarity, a standard choice as vector
space similarity measure.
4 Experiments
4.1 Experiment 1: The impact of parameters
In our 2008 paper, we tested the LexSub data only with
the parameters that showed best results on the Mitchell
and Lapata data: vector combination using component-
wise multiplication (mult), and the computation of (in-
verse) selectional preference vectors with high powers
of n = 20 or n = 30. However, there were indications
that the two datasets showed fundamental differences.
In particular, the Mitchell and Lapata data could only be
modeled using a PMI-transformed vector space, while
the LexSub data could only be modeled using raw co-
occurrence count vectors.
Another one of our findings that warrants further in-
quiry stems from our comparison of different context
choices (verb plus subject, verb plus object, noun plus
embedding verb). We found that subjects are better dis-
ambiguators than objects. This seems counterintuitive
both on theoretical and empirical grounds. Theoretically,
60
Sentence Substitutes
By asking people who work
there, I have since determined
that he didn?t. (# 2002)
be employed 4;
labour 1
Remember how hard your ances-
tors worked. (# 2005)
toil 4; labour 3;
task 1
Figure 3: Lexical substitution example items for ?work?
the notion of verb phrase has been motivated, among
other things, with the claim that direct objects contribute
more to a verb?s disambiguation than subjects (Levin
and Rappaport Hovav, 2005). Empirically, subjects
are known to be realized more often as pronouns than
objects, which makes their vector representations less
semantically specific. However, we used two different
datasets ? the subject results on a set of intransitive
verbs, and the object results on a set of transitive verbs,
so the results are not comparable.
In this experiment, we construct a new, more con-
trolled dataset from the Lexical Substitution corpus to
systematically assess the importance of the three main
parameters: the relation used for disambiguation, the
combination function, and the reweighting parameter.
Construction of the LEXSUB-PARA dataset. The
original Lexical Substitution corpus, constructed for the
SemEval-1 lexical substitution task (McCarthy and Nav-
igli, 2007), consists of 10 instances each of 200 target
words in sentential contexts, drawn from a large inter-
net corpus (Sharoff, 2006). Contextually appropriate
paraphrases for each instance of each target word were
elicited from up to 6 participants. Figure 3 shows two in-
stances for the verb to work. The frequency distribution
over paraphrases can be understood as a characterization
of the target word?s meaning in each context.
For the current paper, we constructed a new subset of
LexSub we call LEXSUB-PARA by parsing LexSub with
Minipar (Lin, 1993) and extracting all 177 sentences
with transitive verbs that had overtly realized subjects
and objects, regardless of voice. We did not manually
verify the correctness of the parses, but discarded 17
sentences where we were not able to compute inverse
selectional preferences for the subject or object head
word (these were mostly rare proper names). This left
160 transitive instances of 42 verbs.
Evaluation For evaluation, we use a variant of the Se-
mEval ?out of ten? (OOT) evaluation metrics defined by
McCarthy and Navigli (2007). They developed two met-
rics, OOT Precision and Recall, which compare where a
predicted set of appropriate paraphrases must be evalu-
ated against a gold standard set. Their metrics are called
?out of ten? because they are measure the accuracy of the
first ten paraphrases predicted by the system. Since they
allow systems to abstain from predictions for any num-
ber of tokens, their two variants average this accuracy
(a), over the tokens with a prediction (OOT Precision),
and (b), over all tokens (OOT Recall). Since our system
0.5 1 2 5 10 20
add obj 61.5 59.7 58.9 56.1 56.0 55.7
add subj 61.7 61.7 59.5 58.4 57.3 57.0
add both 61.3 60.0 60.2 57.7 57.1 56.7
mult obj 59.8 59.7 57.8 55.7 55.7 55.4
mult subj 60.3 59.7 59.3 57.3 57.7 56.7
mult both 59.9 58.8 57.1 55.8 55.3 <1Pr
min obj 60.2 60.0 59.5 57.3 55.7 55.8
min subj 62.2 60.5 59.1 58.5 57.8 57.0
min both 62.3 60.2 59.8 57.3 55.8 55.1
Table 1: OOT accuracy on the LEXSUB-PARA dataset
across models and reweighting values (best results for
each model boldfaced). Random baseline: 53.7. Target
type vector baseline: 57.1. Pr: Numerical problem.
produces predictions for all tokens, OOT Precision and
Recall become identical.
Formally, let Gi be the gold paraphrases for occur-
rence i, and let f(s, i) be the frequency with which s
has been named as paraphrase for i. Let Mi be the ten
paraphrase candidates top-ranked by the SVS model for
i. We write out-of-ten accuracy (OOT) as:
OOT = 1/|I|
?
i
?
s?Mi?Gi
f(s, i)
?
s?Gi
f(s, i)
(7)
We compute two baselines. The first one is random
baseline that guesses whether paraphrases are appropri-
ate. The second baseline uses the original type vector
of the target verb without any combination, i.e., its ?out
of context meaning?, as representation for the token.
Results. Table 1 shows the results on the LEXSUB-
PARA dataset. Recall that the task is to decide the ap-
propriateness of paraphrases for verb instances, disam-
biguated by the inverse selectional preferences of their
subjects (subj), their objects (obj), and both. The ran-
dom baseline attains an OOT accuracy of 53.7, and the
type vector of the target vector performs at 57.1.
SVS is able to outperform both baselines for all val-
ues of the reweighting parameter n <2, and we find the
best results for the lowest value, n = 0.5. As for the
influence of the vector combination function, the best
result is yielded by min (OOT=62.3), followed by add
(OOT=61.7), while mult shows generally worse results
(OOT=60.3). For both add andmult, using only the sub-
ject as context only is optimal. The overall best result,
using min, is seen for both; however, the improvement
over subj is very small.
In the model mult-both-20, where target vectors were
multiplied with two very large expectation vectors, al-
most all instances failed due to overflow errors.
Discussion. Our results indicate that our parameter
optimization strategy in Erk and Pad? (2008) was in fact
flawed. The parameters that were best for the Mitchell
and Lapata (2008) data (mult, n = 20) are suboptimal
for LEXSUB-PARA data.5 The good results for low val-
5We assume that our results hold for the Pad? & Erk (2008)
lexical substitution dataset as well, due to its similar nature.
61
ues of n indicate that good discrimination between valid
and invalid paraphrases can be obtained by relatively
small modifications of the target vector in the direction
indicated by the context. Surprisingly, we still find that
the results in the subj condition are almost always better
than those in the obj condition, even though the dataset
consists only of transitive verbs, where we would have
expected the inverse result. We have two partial ex-
planations. First, we find that pronouns, which occur
frequently in subject position (I, he), are still informa-
tive enough to distinguish ?animate? from ?inanimate?
paraphrases of verbs such as touch. Second, we see
a higher number of Minipar errors in for object posi-
tions than for subject positions, and consequently more
data both for object fillers and for object selectional
preferences.
The overall best result was yielded by a condition that
used both (subject plus object) for disambiguation, using
the recursive modification from Eq. (4). While we see
this as a promising result, the difference to the second-
best result is very small, in almost all other conditions
the performance of both is close to the average of obj
and subj and thus a suboptimal choice.
4.2 Experiment 2: Creating larger datasets with
pseudo-paraphrases
With a size of 2,000 sentences, even the complete
LexSub dataset is tiny in comparison to many other
resources in NLP. Limiting attention to successfully
parsed transitive instances results in an even smaller
dataset on which it is difficult to distinguish noise from
genuine differences between models. This is a large
problem for the use of paraphrase appropriateness as
evaluation task for models of word meaning in context.
In consequence, the automatic creation of larger
datasets is an important task. While unsupervised meth-
ods for paraphrase induction are becoming available
(e.g., Callison-Burch (2008)), they are still so noisy
that the created datasets cannot serve as gold standards.
However, there is an alternative strategy: there is a
considerable amount of data in different languages an-
notated with categorical word sense, created (e.g.) for
Word Sense Disambiguation exercises such as Senseval.
We suggest to convert these data for use in a task similar
to paraphrase assessment, interpreting available infor-
mation about the word sense as pseudo-paraphrases.
Of course, the caveat is that these pseudo-paraphrases
may behave differently than genuine paraphrases. To
investigate this issue, we repeat Experiment 1 on this
dataset.
Construction of the SEMCOR-PARA dataset The
SemCor corpus is a subset of the Brown corpus that
contains 23,346 lemmas annotated with senses accord-
ing to WordNet 1.6. Fortunately, WordNet provides a
rich characterization of word senses. This allows us
to use the WordNet synonyms of a given word sense
as pseudo-paraphrases. Since it can be the case that
the target word is the only word in a synset, we also
0.5 1 2 5 10 20
add obj 21.7 20.7 23.2 24.3 24.2 21.8
add subj 20.6 20.1 22.9 24.4 23.3 19.7
add both 21.1 20.3 23.2 24.4 23.3 18.9
mult obj 22.6 24.8 25.0 24.4 24.2 21.4
mult subj 21.1 23.9 24.4 24.4 23.5 19.8
mult both 24.5 24.5 25.6 24.3 20.0 17.4
min obj 20.9 19.5 23.6 24.4 24.3 21.9
min subj 20.1 19.6 22.5 24.2 23.9 19.6
min both 20.1 19.8 25.2 24.5 24.3 19.0
Table 2: OOT accuracy on the SEMCOR-PARA dataset
across models and reweighting values (best results for
each line boldfaced). Random baseline: 19.6. Target
type vector baseline: 20.8
need to add direct hypernyms. Direct hypernyms have
been used in annotation tasks to characterize WordNet
senses (Mihalcea and Chklovski, 2003), an indicator
that they are usually close enough in meaning to func-
tion as pseudo-paraphrases.
Again, we parsed the corpus with Minipar and iden-
tified all sense-tagged instances of the verbs from
LEXSUB-PARA, to keep the two corpora as compa-
rable as possible. For each instance wi of word w, we
collected all synonyms and direct hypernyms of the
synset as the set of appropriate paraphrases. The list
of synonyms and direct hypernyms of all other senses
of w, whether they occur in SemCor or not, were con-
sidered inappropriate paraphrases for the instance wi.
This method does not provide us with frequencies for
the pseudo-paraphrases; we thus assumed a uniform fre-
quency of 1. This does not do away with the gradedness
of the meaning representation, though, since each token
is still associated with a set of appropriate paraphrases.
Out of 2242 transitive verb instances, we further re-
moved 153 since we could not compute selectional pref-
erences for at least one of the fillers. 484 instances were
removed because WordNet did not list any verbal para-
phrases for the annotated synset or its direct hypernym.
This resulted in 1605 instances for 40 verbs, a dataset
an order of magnitude larger than LEXSUB-PARA. (See
Section 4.3 for an example verb with paraphrases.)
Results and Discussion. We again use the OOT ac-
curacy measure. The results for paraphrase assessment
on SEMCOR-PARA are shown in Table 2. The numbers
are substantially lower than for LEXSUB-PARA. This
is first and foremost a consequence of the higher ?poly-
semy? of the pseudo-paraphrases. In LEXSUB-PARA,
the average numbers of possible paraphrases per tar-
get word is 20; in SEMCOR-PARA, 54. This is to be
expected and also reflected in the much lower random
baseline (19.6% OOT). However, we also observe that
the reduction in error rate over the baseline is consider-
ably lower for SEMCOR-PARA than for LEXSUB-PARA
(10% vs. 20% reduction).
Among the parameters of the model, we find the
largest impact for the reweighting parameter. The best
results occur in the middle range(n = 2 and n = 5),
62
with both lower and higher weights yielding consid-
erably lower scores. Apparently, it is more difficult
to strike the right balance between the target and the
expectations on this dataset. This is also mirrored in
the smaller improvement of the target type vector base-
line over the random baseline. As for vector combi-
nation functions, we find the best results for the more
?intersection?-like mult and min combinations, with
somewhat lower results for add; however, the differ-
ences are rather small. Finally, combination with obj
works better than combination with subj. At least among
the best results, both is able to improve over the use of ei-
ther individual relation. The best result uses mult-both,
with an OOT accuracy of 25.6.
4.3 Further analysis
In our two experiments, we have found systematic rela-
tionships between the SVS model parameters and their
performance within the LEXSUB-PARA and SEMCOR-
PARA datasets. Unfortunately, few of the parameter set-
tings we found to work well appear to generalize across
the two datasets; neither do they correspond to the op-
timal parameter values we established for the Mitchell
and Lapata dataset in our 2008 paper. Variables that
vary particularly strikingly are the reweighting parame-
ter and the performance of different relations. To better
understand these differences, we perform a further vali-
dation analysis that attempts to link model performance
to a variable that (a) behaves consistently across the two
datasets used in this paper and (b) sheds light onto the
patterns we have observed for the parameters.
The quantity we will use for this purpose is the aver-
age discriminativity of the model. We define discrimina-
tivity as the degree to which the token vector computed
by the model is on average more similar to the valid than
to the invalid paraphrases. For a paraphrase ordering
task such as the one we are considering, we want this
quantity to be as large as possible; very small quantities
indicate that the model is basically ?guessing? an order.
Figure 4 plots disciminativity against model perfor-
mance. As can be expected, it is indeed a very strong
correlation between discriminativity and OOT accu-
racy across all models. A Pearson?s correlation test
confirms that the correlation is highly significant for
both datasets (LEXSUB-PARA: r=0.65, p < 0.0001;
SEMCOR-PARA: r=0.76, p < 0.0001).
Next, we considered the relationship between the
mean discriminativity for different combinations and
reweighting values n. Figure 5 shows the resulting plots,
which reveal two main differences between the datasets.
The first one is the influence of the reweighting parame-
ter. For LEXSUB-PARA, the highest discriminativity is
found for small values of n, with decreasing values for
higher parameter values. In contrast, SEMCOR-PARA
shows the highest discriminativity for middle values of
n (on the order of 5?10), with lowest values on either
side. The second difference is the relative discrimina-
tivity of obj and subj. On LEXSUB-PARA, the subj
predictions are more discriminative than obj predictions
for all values of n. On SEMCOR-PARA, this picture is
reversed, with more discriminative obj predictions for
the best (and thus relevant) values of n.
We interpret these patterns, which fit the observed
OOT accuracy numbers well, as additional evidence that
the variations we see between the datasets are not noise
or artifacts of the setup, but arise due to the different
makeup of the two datasets. This ties in with our intu-
itions about the differences between human-generated
paraphrases and WordNet ?pseudo-paraphrases?. Com-
pare the following paraphrase lists:
dismiss (LexSub): banish, deride, discard, discharge, dis-
patch, excuse, fire, ignore, reject, release, remove, sack
dismiss (SemCor/WordNet): alter, axe, brush, can, change,
discount, displace, disregard, dissolve, drop, farewell,
fire, force, ignore, modify, notice, packing, push, reject,
remove, sack, send, terminate, throw, usher
The SEMCOR-PARA list contains a larger number of
unspecific pseudo-paraphrases such as change, push,
send, which stem from direct WordNet hypernyms of
the more specific dismiss senses. Presumably, these
terms are assigned rather general vectors which the SVS
finds difficult to rule out as paraphrases. This lowers
the discriminativity of the models, in particular for subj,
and results in the smaller relative improvement over
the baseline we observe for SEMCOR-PARA. This sug-
gests that the usability of word sense-derived datasets
in evaluations could be improved by taking depth in the
WordNet hierarchy into account when including direct
hypernyms among the pseudo-paraphrases.
5 Conclusions
In this paper, we have explored the parameter space
for the computation of vector-based representations of
token meaning with the SVS model.
Our evaluation scenario was paraphrase assessment.
To systematically assess the impact of parameter choice,
we created two new controlled datasets. The first one,
the LEXSUB-PARA dataset, is a small subset of the Lex-
ical Substitution corpus (McCarthy and Navigli, 2007)
that was specifically created for this task. The second
dataset, SEMCOR-PARA, which is considerably larger,
consists in instances from the SemCor corpus whose
WordNet annotation was automatically converted into
?pseudo-paraphrase? annotation.6
We found a small number of regularities that hold for
both datasets: namely, that the reweighting parameter
is the most important choice for a SVS model, followed
by the relation used as context, while the influence of
the vector combination function is comparatively small.
Unfortunately, the actual settings of these parameters
appeared not to generalize well from one dataset to
the other. We have collected evidence that these diver-
gences are not due to noise, but to genuine differences
6Both datasets can be obtained from the authors.
63
ll l
l l
l
l
l
l
l
l
l
l
l
l
l l l
l
l
l
l
l
l
l
l
l
l
l
l l l
l
l
l
l
l
l
l
l
l
l
0.018 0.020 0.022 0.024 0.026 0.028 0.030
0.56
0.57
0.58
0.59
0.60
0.61
0.62
mean sim(val)?sim(inval)
out 
of te
n pre
cisio
n
l
l
l
l
l
l
l l
l
l
l
l
l l
l
l
l
l
l
l
l l
l
l
l
l
l l
l
l
ll
l
ll
ll l
l
l
l
0.005 0.010 0.015 0.020 0.025 0.030
0.18
0.20
0.22
0.24
mean sim(val)?sim(inval)
o
ut o
f ten
 prec
ision
Figure 4: Scatterplot of "out of ten" accuracy against model discriminativity between valid and invalid paraphrases.
Left: LEXSUB-PARA, right: SEMCOR-PARA.
0 5 10 15 20 25 30
0.02
0
0.02
2
0.02
4
0.02
6
0.02
8
exponent
me
an 
sim(v
al)?si
m(inv
al)
target + object selpreftarget + subject selpref
0 5 10 15 20 25 30
0.00
5
0.01
0
0.01
5
0.02
0
0.02
5
exponent
me
an 
sim(v
al)?si
m(inv
al)
target + object selpreftarget + subject selpref
Figure 5: Average amount to which predictions are more similar to valid than to invalid paraphrases, for different
reweighting values. Left: LEXSUB-PARA, right: SEMCOR-PARA.
in the datasets. We describe an auxiliary quantity, dis-
criminativity, that measures the ability of the model?s
predictions to distinguish between valid and invalid para-
phrases.
The consequence we draw from this study is that it
is surprisingly difficult to establish generalizable ?best
practice? parameter setting for SVS. Good parameter
values appear to be sensitive to the properties of datasets.
For example, we have attributed the observation that
subjects are more informative on LEXSUB-PARA, while
objects work better on SEMCOR-PARA, to differences
in the set of paraphrase competitors. In this regard,
the conversion of the WSD corpus can be considered a
partial success. We have constructed the largest existing
paraphrase assessment corpus. However, the use of
WordNet information to create paraphrases results in a
very difficult corpus. We will investigate methods that
exclude overly general hypernyms of the target words as
paraphrases to alleviate the problems we see currently.
Discriminativity further suggests that paraphrase as-
sessment can be improved by selectional preference
representations that are trained to maximize the dis-
tance between valid and invalid paraphrases. Such a
representation could be provided by discriminative for-
mulations (Bergsma et al, 2008), or by exemplar-based
models that are able to deal better with the ambiguity
present in the preferences of very general words.
Another important topic for further research is the
computation of token vectors that incorporate more than
one context word. The current results we obtain for
?both? are promising but limited; it appears that the suc-
cessful integration of multiple context words requires
strategies that go beyond simplistic addition or intersec-
tion of observed contexts.
References
S. Bergsma, D. Lin, and R. Goebel. 2008. Discrimina-
tive learning of selectional preference from unlabeled
text. In Proceedings of EMNLP, pages 59?68.
C. Callison-Burch. 2008. Syntactic constraints on para-
phrases extracted from parallel corpora. In Proceed-
ings of EMNLP, pages 196?205.
K. Erk and S. Pad?. 2008. A structured vector space
model for word meaning in context. In Proceedings
of EMNLP.
64
K. Erk. 2007. A simple, similarity-based model for se-
lectional preferences. In Proceedings of ACL, pages
216?223.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
P. Hanks. 2000. Do word meanings exist? Computers
and the Humanities, 34(1-2):205?215.
D. Hindle and M. Rooth. 1993. Structural ambigu-
ity and lexical relations. Computational Linguistics,
19(1):103?120.
A. Kilgarriff and J. Rosenzweig. 2000. Framework
and results for English Senseval. Computers and the
Humanities, 34(1-2).
A. Kilgarriff. 1997. I don?t believe in word senses.
Computers and the Humanities, 31(2):91?113.
W. Kintsch. 2001. Predication. Cognitive Science,
25:173?202.
T. Landauer and S. Dumais. 1997. A solution to Platos
problem: the latent semantic analysis theory of ac-
quisition, induction, and representation of knowledge.
Psychological Review, 104(2):211?240.
B. Levin and M. Rappaport Hovav. 2005. Argument
Realization. Research Surveys in Linguistics Series.
CUP.
D. Lin. 1993. Principle-based parsing without overgen-
eration. In Proceedings of ACL, pages 112?120.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28.
D. McCarthy and R. Navigli. 2007. SemEval-2007
Task 10: English Lexical Substitution Task. In Pro-
ceedings of SemEval, pages 48?53.
D. McCarthy. 2006. Relating WordNet senses for word
sense disambiguation. In Proceedings of the ACL
Workshop on Making Sense of Sense, pages 17?24.
S. McDonald and C. Brew. 2004. A distributional
model of semantic context effects in lexical process-
ing. In Proceedings of ACL, pages 17?24.
K. McRae, M. Spivey-Knowlton, and M. Tanenhaus.
1998. Modeling the influence of thematic fit (and
other constraints) in on-line sentence comprehension.
Journal of Memory and Language, 38:283?312.
R. Mihalcea and T. Chklovski. 2003. Open Mind
Word Expert: Creating large annotated data collec-
tions with web users? help. In Proceedings of the
EACL 2003 Workshop on Linguistically Annotated
Corpora (LINC 2003), Budapest, Hungary.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL,
pages 236?244.
S. Narayanan and D. Jurafsky. 2002. A Bayesian model
predicts human parse preference and reading time in
sentence processing. In Proceedings of NIPS, pages
59?65.
S. Pad?, U. Pad?, and K. Erk. 2007. Flexible, corpus-
based modelling of human plausibility judgements.
In Proceedings of EMNLP/CoNLL, pages 400?409.
M. Palmer, H. Dang, and C. Fellbaum. 2007. Making
fine-grained and coarse-grained sense distinctions,
both manually and automatically. Journal of Natural
Language Engineering. To appear.
P. Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61:127?159.
H. Sch?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?124.
Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. International Journal
of Corpus Linguistics, 11(4):435?462.
Y. Wilks. 1975. Preference semantics. In Formal
Semantics of Natural Language. CUP.
65
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 37?41,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Textual Entailment Features for Machine Translation Evaluation
Sebastian Pado?, Michel Galley, Dan Jurafsky, Christopher D. Manning?
Stanford University
{pado,mgalley,jurafsky,manning}@stanford.edu
Abstract
We present two regression models for the prediction
of pairwise preference judgments among MT hy-
potheses. Both models are based on feature sets that
are motivated by textual entailment and incorporate
lexical similarity as well as local syntactic features
and specific semantic phenomena. One model pre-
dicts absolute scores; the other one direct pairwise
judgments. We find that both models are compet-
itive with regression models built over the scores
of established MT evaluation metrics. Further data
analysis clarifies the complementary behavior of the
two feature sets.
1 Introduction
Automatic metrics to assess the quality of machine trans-
lations have been a major enabler in improving the per-
formance of MT systems, leading to many varied ap-
proaches to develop such metrics. Initially, most metrics
judged the quality of MT hypotheses by token sequence
match (cf. BLEU (Papineni et al, 2002), NIST (Dod-
dington, 2002). These measures rate systems hypothe-
ses by measuring the overlap in surface word sequences
shared between hypothesis and reference translation.
With improvements in the state-of-the-art in machine
translation, the effectiveness of purely surface-oriented
measures has been questioned (see e.g., Callison-Burch
et al (2006)). In response, metrics have been proposed
that attempt to integrate more linguistic information
into the matching process to distinguish linguistically li-
censed from unwanted variation (Gime?nez andMa`rquez,
2008). However, there is little agreement on what types
of knowledge are helpful: Some suggestions concen-
trate on lexical information, e.g., by the integration of
word similarity information as in Meteor (Banerjee and
Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other
proposals use structural information such as dependency
edges (Owczarzak et al, 2007).
In this paper, we investigate an MT evaluation metric
that is inspired by the similarity between this task and
the textual entailment task (Dagan et al, 2005), which
?This paper is based on work funded by the Defense Ad-
vanced Research Projects Agency through IBM. The content
does not necessarily reflect the views of the U.S. Government,
and no official endorsement should be inferred..
HYP: Virus was infected.
REF: No one was infected by the virus.
no entailment
no entailment
HYP: The virus did not infect anybody.
REF: No one was infected by the virus.
entailment
entailment
Figure 1: Entailment status between an MT system hy-
pothesis and a reference translation for good translations
(above) and bad translations (below).
suggests that the quality of an MT hypothesis should be
predictable by a combination of lexical and structural
features that model the matches and mismatches be-
tween system output and reference translation. We use
supervised regression models to combine these features
and analyze feature weights to obtain further insights
into the usefulness of different feature types.
2 Textual Entailment for MT Evaluation
2.1 Textual Entailment vs. MT Evaluation
Textual entailment (TE) was introduced by Dagan et
al. (2005) as a concept that corresponds more closely
to ?common sense? reasoning than classical, categorical
entailment. Textual entailment is defined as a relation
between two natural language sentences (a premise P
and a hypothesis H) that holds if a human reading P
would infer that H is most likely true.
Information about the presence or absence of entail-
ment between two sentences has been found to be ben-
eficial for a range of NLP tasks such as Word Sense
Disambiguation or Question Answering (Dagan et al,
2006; Harabagiu and Hickl, 2006). Our intuition is that
this idea can also be fruitful in MT Evaluation, as illus-
trated in Figure 1. Very good MT output should entail
the reference translation. In contrast, missing hypothesis
material breaks forward entailment; additional material
breaks backward entailment; and for bad translations,
entailment fails in both directions.
Work on the recognition of textual entailment (RTE)
has consistently found that the integration of more syn-
tactic and semantic knowledge can yield gains over
37
surface-based methods, provided that the linguistic anal-
ysis was sufficiently robust. Thus, for RTE, ?deep?
matching outperforms surface matching. The reason is
that linguistic representation makes it considerably eas-
ier to distinguish admissible variation (i.e., paraphrase)
from true, meaning-changing divergence. Admissible
variation may be lexical (synonymy), structural (word
and phrase placement), or both (diathesis alternations).
The working hypothesis of this paper is that the ben-
efits of deeper analysis carry over to MT evaluation.
More specifically, we test whether the features that al-
low good performance on the RTE task can also predict
human judgments for MT output. Analogously to RTE,
these features should help us to differentiate meaning
preserving translation variants from bad translations.
Nevertheless, there are also substantial differences
between TE and MT evaluation. Crucially, TE assumes
the premise and hypothesis to be well-formed sentences,
which is not true in MT evaluation. Thus, a possible crit-
icism to the use of TE methods is that the features could
become unreliable for ill-formed MT output. However,
there is a second difference between the tasks that works
to our advantage. Due to its strict compositional nature,
TE requires an accurate semantic analysis of all sentence
parts, since, for example, one misanalysed negation or
counterfactual embedding can invert the entailment sta-
tus (MacCartney and Manning, 2008). In contrast, hu-
man MT judgments behave more additively: failure of a
translation with respect to a single semantic dimension
(e.g., polarity or tense) degrades its quality, but usually
not crucially so. We therefore expect that even noisy
entailment features can be predictive in MT evaluation.
2.2 Entailment-based prediction of MT quality
Regression-based prediction. Experiences from the
annotation of MT quality judgments show that human
raters have difficulty in consistently assigning absolute
scores to MT system output, due to the number of ways
in which MT output can deviate. Thus, the human an-
notation for the WMT 2008 dataset was collected in
the form of binary pairwise preferences that are con-
siderably easier to make (Callison-Burch et al, 2008).
This section presents two models for the prediction of
pairwise preferences.
The first model (ABS) is a regularized linear regres-
sion model over entailment-motivated features (see be-
low) that predicts an absolute score for each reference-
hypothesis pair. Pairwise preferences are created simply
by comparing the absolute predicted scores. This model
is more general, since it can also be used where absolute
score predictions are desirable; furthermore, the model
is efficient with a runtime linear in the number of sys-
tems and corpus size. On the downside, this model is
not optimized for the prediction of pairwise judgments.
The second model we consider is a regularized logis-
tic regression model (PAIR) that is directly optimized to
predict a weighted binary preference for each hypothe-
sis pair. This model is less efficient since its runtime is
Alignment score(3) Unaligned material (10)
Adjuncts (7) Apposition (2)
Modality (5) Factives (8)
Polarity (5) Quantors (4)
Tense (2) Dates (6)
Root (2) Semantic Relations (4)
Semantic relatedness (7) Structural Match (5)
Compatibility of locations and entities (4)
Table 1: Entailment feature groups provided by the
Stanford RTE system, with number of features
quadratic in the number of systems. On the other hand,
it can be trained on more reliable pairwise preference
judgments. In a second step, we combine the individ-
ual decisions to compute the highest-likelihood total
ordering of hypotheses. The construction of an optimal
ordering from weighted pairwise preferences is an NP-
hard problem (via reduction of CYCLIC-ORDERING;
Barzilay and Elhadad, 2002), but a greedy search yields
a close approximation (Cohen et al, 1999).
Both models can be used to predict system-level
scores from sentence-level scores. Again, we have two
method for doing this. The basic method (BASIC) pre-
dicts the quality of each system directly as the percent-
age of sentences for which its output was rated best
among all systems. However, we noticed that the man-
ual rankings for the WMT 2007 dataset show a tie for
best system for almost 30% of sentences. BASIC is
systematically unable to account for these ties. We
therefore implemented a ?tie-aware? prediction method
(WITHTIES) that uses the same sentence-level output as
BASIC, but computes system-level quality differently,
as the percentage of sentences where the system?s hy-
pothesis was scored better or at most ? worse than the
best system, for some global ?tie interval? ? .
Features. We use the Stanford RTE system (MacCart-
ney et al, 2006) to generate a set of entailment features
(RTE) for each pair of MT hypothesis and reference
translation. Features are generated in both directions
to avoid biases towards short or long translations. The
Stanford RTE system uses a three-stage architecture.
It (a) constructs a robust, dependency-based linguistic
analysis of the two sentences; (b) identifies the best
alignment between the two dependency graphs given
similarity scores from a range of lexical resources, us-
ing a Markov Chain Monte Carlo sampling strategy;
and (c) computes roughly 75 features over the aligned
pair of dependency graphs. The different feature groups
are shown in Table 1. A small number features are
real-valued, measuring different quality aspects of the
alignment. The other features are binary, indicating
matches and mismatches of different types (e.g., align-
ment between predicates embedded under compatible
or incompatible modals, respectively).
To judge to what extent the entailment-based model
delivers improvements that cannot be obtained with es-
tablished methods, we also experiment with a feature set
38
formed from a set of established MT evaluation metrics
(TRADMT). We combine different parametrization of
(smoothed) BLEU (Papineni et al, 2002), NIST (Dod-
dington, 2002), and TER (Snover et al, 2006), to give
a total of roughly 100 features. Finally, we consider a
combination of both feature sets (COMB).
3 Experimental Evaluation
Setup. To assess and compare the performance of our
models, we use corpora that were created by past in-
stances of the WMT workshop. We optimize the feature
weights for the ABS models on the WMT 2006 and
2007 absolute score annotations, and correspondingly
for the PAIR models on the WMT 2007 absolute score
and ranking annotations. All models are evaluated on
WMT 2008 to compare against the published results.
Finally, we need to set the tie interval ? . Since we
did not want to optimize ? , we simply assumed that the
percentage of ties observed on WMT 2007 generalizes
to test sets such as the 2008 dataset. We set ? so that
there are ties for first place on 30% of the sentences,
with good practical success (see below).
Results. Table 2 shows our results. The first results
column (Cons) shows consistency, i.e., accuracy in pre-
dicting human pairwise preference judgments. Note that
the performance of a random baseline is not at 50%, but
substantially lower. This is due to (a) the presence of
contradictions and ties in the human judgments, which
cannot be predicted; and (b) WMT?s requirement to
compute a total ordering of all translations for a given
sentence (rather than independent binary judgments),
which introduces transitivity constraints. See Callison-
Burch et al (2008) for details. Among our models, PAIR
shows a somewhat better consistency than ABS, as can
be expected from a model directly optimized on pair-
wise judgments. Across feature sets, COMB works best
with a consistency of 0.53, competitive with published
WMT 2008 results.
The two final columns (BASIC and WITHTIES) show
Spearman?s ? for the correlation between human judg-
ments and the two types of system-level predictions.
For BASIC system-level predictions, we find that
PAIR performs considerably worse than ABS, by a mar-
gin of up to ? = 0.1. Recall that the system-level analy-
sis considers only the top-ranked hypotheses; apparently,
a model optimized on pairwise judgments has a harder
time choosing the best among the top-ranked hypothe-
ses. This interpretation is supported by the large benefit
that PAIR derives from explicit tie modeling. ABS gains
as well, although not as much, so that the correlation of
the tie-aware predictions is similar for ABS and PAIR.
Comparing different feature sets, BASIC show a simi-
lar pattern to the consistency figures. There is no clear
winner between RTE and TRADMT. The performance
of TRADMT is considerably better than the performance
of BLEU and TER in the WMT 2008 evaluation, where
? ? 0.55. RTE is able to match the performance of an
Model Feature set Cons
(Acc.)
BASIC
(?)
WITHTIES
(?)
ABS TRADMT 0.50 0.74 0.74
ABS RTE 0.51 0.72 0.78
ABS COMB 0.51 0.74 0.74
PAIR TRADMT 0.52 0.63 0.73
PAIR RTE 0.51 0.66 0.77
PAIR COMB 0.53 0.70 0.77
WMT 2008 (worst) 0.44 0.37
WMT 2008 (best) 0.56 0.83
Table 2: Evaluation on the WMT 2008 dataset for our
regression models, compared to results fromWMT 2008
ensemble of state-of-the-art metrics, which validates our
hope that linguistically motivated entailment features
are sufficiently robust to make a positive contribution
in MT evaluation. Furthermore, the two individual fea-
ture sets are outperformed by the combined feature set
COMB. We interpret this as support for our regression-
based combination approach.
Moving to WITHTIES, we see the best results from
the RTE model which improves by ?? = 0.06 for ABS
and ?? = 0.11 for PAIR. There is less improvement for
the other feature sets, in particular COMB. We submitted
the two overall best models, ABS-RTE and PAIR-RTE
with tie-aware prediction, to the WMT 2009 challenge.
Data Analysis. We analyzed at the models? predic-
tions to gain a better understanding of the differences in
the behavior of TRADMT-based and RTE-based mod-
els. As a first step, we computed consistency numbers
for the set of ?top? translations (hypotheses that were
ranked highest for a given reference) and for the set
of ?bottom? translations (hypotheses that were ranked
worst for a given reference). We found small but con-
sistent differences between the models: RTE performs
about 1.5 percent better on the top hypotheses than on
the bottom translations. We found the inverse effect for
the TRADMT model, which performs 2 points worse on
the top hypotheses than on the bottom hypotheses. Re-
visiting our initial concern that the entailment features
are too noisy for very bad translations, this finding indi-
cates some ungrammaticality-induced degradation for
the entailment features, but not much. Conversely, these
numbers also provide support for our initial hypothesis
that surface-based features are good at detecting very
deviant translations, but can have trouble dealing with
legitimate linguistic variation.
Next, we analyzed the average size of the score dif-
ferences between the best and second-best hypotheses
for correct and incorrect predictions. We found that the
RTE-based model predicted on average almost twice the
difference for correct predictions (? = 0.30) than for
incorrect predictions (? = 0.16), while the difference
was considerably smaller for the TRADMT-based model
(? = 0.17 for correct vs. ? = 0.13 for incorrect). We
believe it is this better discrimination on the top hypothe-
39
Segment TRADMT RTE COMB Gold
REF: Scottish NHS boards need to improve criminal records checks for
employees outside Europe, a watchdog has said.
HYP: The Scottish health ministry should improve the controls on extra-
community employees to check whether they have criminal precedents,
said the monitoring committee. [1357, lium-systran]
Rank: 3 Rank: 1 Rank: 2 Rank: 1
REF: Arguments, bullying and fights between the pupils have extended
to the relations between their parents.
HYP: Disputes, chicane and fights between the pupils transposed in
relations between the parents. [686, rbmt4]
Rank: 5 Rank: 2 Rank: 4 Rank: 5
Table 3: Examples of reference translations and MT output from the WMT 2008 French-English News dataset.
Rank judgments are out of five (smaller is better).
ses that explains the increased benefit the RTE-based
model obtains from tie-aware predictions: if the best
hypothesis is wrong, chances are much better than for
the TRADMT-based model that counting the second-
best hypothesis as ?best? is correct. Unfortunately, this
property is not shared by COMB to the same degree, and
it does not improve as much as RTE.
Table 3 illustrates the difference between RTE and
TRADMT. In the first example, RTE makes a more ac-
curate prediction than TRADMT. The human rater?s
favorite translation deviates considerably from the ref-
erence translation in lexical choice, syntactic structure,
and word order, for which it is punished by TRADMT.
In contrast, RTE determines correctly that the propo-
sitional content of the reference is almost completely
preserved. The prediction of COMB is between the two
extremes. The second example shows a sentence where
RTE provides a worse prediction. This sentence was
rated as bad by the judge, presumably due to the inap-
propriate translation of the main verb. This problem,
together with the reformulation of the subject, leads
TRADMT to correctly predict a low score (rank 5/5).
RTE?s deeper analysis comes up with a high score (rank
2/5), based on the existing semantic overlap. The com-
bined model is closer to the truth, predicting rank 4.
Feature Weights. Finally, we assessed the impor-
tance of the different entailment feature groups in the
RTE model.1 Since the presence of correlated features
makes the weights difficult to interpret, we restrict our-
selves to two general observations.
First, we find high weights not only for the score of
the alignment between hypothesis and reference, but
also for a number of syntacto-semantic match and mis-
match features. This means that we do get an additional
benefit from the presence of these features. For example,
features with a negative effect include dropping adjuncts,
unaligned root nodes, incompatible modality between
the main clauses, person and location mismatches (as
opposed to general mismatches) and wrongly handled
passives. Conversely, some factors that increase the
prediction are good alignment, matching embeddings
under factive verbs, and matches between appositions.
1The feature weights are similar for the COMB model.
Second, we find clear differences in the usefulness
of feature groups between MT evaluation and the RTE
task. Some of them, in particular structural features,
can be linked to the generally lower grammaticality of
MT hypotheses. A case in point is a feature that fires
for mismatches between dependents of predicates and
which is too unreliable on the SMT data. Other differ-
ences simply reflect that the two tasks have different
profiles, as sketched in Section 2.1. RTE exhibits high
feature weights for quantifier and polarity features, both
of which have the potential to influence entailment deci-
sions, but are relatively unimportant for MT evaluation,
at least at the current state of the art.
4 Conclusion
In this paper, we have investigated an approach to MT
evaluation that is inspired by the similarity between
this task and textual entailment. Our two models ? one
predicting absolute scores and one predicting pairwise
preference judgments ? use entailment features to pre-
dict the quality of MT hypotheses, thus replacing sur-
face matching with syntacto-semantic matching. Both
models perform similarly, showing sufficient robustness
and coverage to attain comparable performance to a
committee of established MT evaluation metrics.
We have described two refinements: (1) combining
the features into a superior joint model; and (2) adding a
confidence interval around the best hypothesis to model
ties for first place. Both strategies improve correlation;
however, unfortunately the benefits do not currently
combine. Our feature weight analysis indicates that
syntacto-semantic features do play an important role in
score prediction in the RTE model. We plan to assess
the additional benefit of the full entailment feature set
against the TRADMT feature set extended by a proper
lexical similarity metric, such as METEOR.
The computation of entailment features is more
heavyweight than traditional MT evaluation metrics.
We found the speed (about 6 s per hypothesis on a cur-
rent PC) to be sufficient for easily judging the quality of
datasets of the size conventionally used for MT evalua-
tion. However, this may still be too expensive as part of
an MT model that directly optimizes some performance
measure, e.g., minimum error rate training (Och, 2003).
40
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Translation
and Summarization, pages 65?72, Ann Arbor, MI.
R. Barzilay and N. Elhadad. 2002. Inferring strategies
for sentence ordering in multidocument news summa-
rization. Journal of Artificial Intelligence Research,
17:35?55.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU in ma-
chine translation research. In Proceedings of EACL,
pages 249?256, Trento, Italy.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the ACL Workshop on Statistical Machine
Translation, pages 70?106, Columbus, OH.
Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM: A
maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55?62, Columbus, Ohio.
William W. Cohen, Robert E. Schapire, and Yoram
Singer. 1999. Learning to order things. Journal
of Artificial Intelligence Research, 10:243?270.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entailment
challenge. In Proceedings of the PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Southampton, UK.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct word
sense matching for lexical substitution. In Proceed-
ings of ACL, Sydney, Australia.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram cooccurrence
statistics. In Proceedings of HLT, pages 128?132,
San Diego, CA.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. A smorgas-
bord of features for automatic MT evaluation. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 195?198, Columbus, Ohio.
Sanda Harabagiu and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain question
answering. In Proceedings of ACL, pages 905?912,
Sydney, Australia.
Bill MacCartney and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in nat-
ural language inference. In Proceedings of Coling,
pages 521?528, Manchester, UK.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of valid
textual entailments. In Proceedings of NAACL, pages
41?48, New York City, NY.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167, Sapporo, Japan.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007. Dependency-based automatic evalu-
ation for machine translation. In Proceedings of
the NAACL-HLT / AMTA Workshop on Syntax and
Structure in Statistical Translation, pages 80?87,
Rochester, NY.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
ACL, pages 311?318, Philadelphia, PA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA, pages 223?231, Cambridge,
MA.
41
Dependency-Based Construction of Semantic
Space Models
Sebastian Pad??
Saarland University
Mirella Lapata??
University of Edinburgh
Traditionally, vector-based semantic space models use word co-occurrence counts from large
corpora to represent lexical meaning. In this article we present a novel framework for construct-
ing semantic spaces that takes syntactic relations into account. We introduce a formalization for
this class of models, which allows linguistic knowledge to guide the construction process. We
evaluate our framework on a range of tasks relevant for cognitive science and natural language
processing: semantic priming, synonymy detection, and word sense disambiguation. In all cases,
our framework obtains results that are comparable or superior to the state of the art.
1. Introduction
Vector space models of word co-occurrence have proved a useful framework for repre-
senting lexical meaning in a variety of natural language processing (NLP) tasks, such
as word sense discrimination (Sch?tze 1998) and ranking (McCarthy et al 2004), text
segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correc-
tion (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin
1998a), and notably, information retrieval (Salton, Wang, and Yang 1975). These models
have also been popular in cognitive science and figure prominently in several studies
simulating human behavior. Examples include similarity judgments (McDonald 2000),
semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and
McDonald 2000; McDonald and Brew 2004), and text comprehension (Landauer and
Dumais 1997; Foltz, Kintsch, and Landauer 1998).
The popularity of vector-based models in both fields lies in their ability to repre-
sent word meaning simply by using distributional statistics. The central assumption
here is that the context surrounding a given word provides important information
about its meaning (Harris 1968). The semantic properties of words are captured in a
multi-dimensional space by vectors that are constructed from large bodies of text by
observing the distributional patterns of co-occurrence with their neighboring words.
Co-occurrence information is typically collected in a frequency matrix, where each row
corresponds to a unique word, commonly referred to as ?target word,? and each column
represents a given linguistic context. The semantic similarity between any two words
? Computational Linguistics, P.O. Box 15 11 50, 66041 Saarbr?cken, Germany. E-mail: pado@coli.uni-sb.de.
?? School of Informatics, University of Edinburgh, EH8 9LW, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk.
Submission received: 20 December 2004; revised submission received: 26 September 2006;
accepted for publication: 23 November 2006.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 2
can then be quantified directly using a distance measure such as cosine or Euclidean
distance.
Contexts are defined as a small number of words surrounding the target word
(Lund and Burgess 1996; Lowe and McDonald 2000) or as entire paragraphs?even
documents (Salton, Wang, and Yang 1975; Landauer and Dumais 1997). Latent Semantic
Analysis (LSA; Landauer and Dumais 1997) is an example of a document-based vector
space model that is commonly used in information retrieval and cognitive science. Each
target word t is represented by a k element vector of paragraphs p1...k and the value
of each vector element is a function of the number of times t occurs in pi. In contrast,
the Hyperspace Analogue to Language model (HAL; Lund and Burgess 1996) creates
a word-based semantic space: each target word t is represented by a k element vector,
whose dimensions correspond to context words c1...k. The value of each vector element
is a function of the number of times each ci occurs within a window of size n before or
after t in a large corpus.
In their simplest incarnation, semantic space models treat context as a set of un-
ordered words, without even taking parts of speech into account (e.g., to drink and
a drink are represented by a single vector). In fact, with the exception of function words
(e.g., the, down), which are often removed, it is often assumed that all context words
within a certain distance from the target word are semantically relevant. Because no
linguistic knowledge is taken into account, the construction of semantic space models is
straightforward and language-independent?all that is needed is a segmented corpus
of written or spoken text.
However, the assumption that contextual information contributes indiscriminately
to a word?s meaning is clearly a simplification. There is ample evidence demonstrating
that syntactic relations across and within sentences are crucial for sentence and dis-
course processing (West and Stanovich 1986; Neville et al 1991; Fodor 1995; Miltsakaki
2003) and modulate cognitive behavior in sentence priming tasks (Morris 1994). Fur-
thermore, much research in lexical semantics hypothesizes that the behavior of words,
particularly with respect to the expression and interpretation of their arguments, is to a
large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983;
Talmy 1985; Gropen et al 1989; Pinker 1989; Levin 1993; Goldberg 1995).
It is therefore not surprising that there have been efforts to enrich vector-based
models with morpho-syntactic information. Extensions range from part of speech tag-
ging (Kanejiya, Kumar, and Prasad 2003; Widdows 2003) to shallow syntactic analy-
sis (Grefenstette 1994; Lee 1999; Curran and Moens 2002) and full-blown parsing (Lin
1998a). In these semantic space models, contexts are defined over words bearing a
syntactic relationship to the target words of interest. This makes semantic spaces more
flexible; different types of contexts can be selected; words do not have to co-occur within
a small, fixed word window; and word order or argument structure differences can be
naturally mirrored in the semantic space.
This article proposes a general framework for semantic space models which concep-
tualizes context in terms of syntactic relations. We introduce an algorithm for construct-
ing semantic space models from texts annotated with syntactic information (specifi-
cally dependency relations) and illustrate how different model classes can be derived
from this linguistically rich representation. Our guiding hypothesis is that syntactic
structure in general and argument structure in particular is a close reflection of lexical
meaning (Levin 1993). We thus model meaning by quantifying the degree to which
words are attested in similar syntactic environments. The expressive power of our
framework stems from three novel parameters which guide model construction. The
first parameter determines which types of syntactic structures contribute towards the
162
Pad? and Lapata Dependency-Based Semantic Spaces
representation of lexical meaning. The second parameter allows us to weigh the relative
importance of different syntactic relations. Finally, the third parameter determines how
the semantic space is actually represented, for instance as co-occurrences of words with
other words, words with parts of speech, or words with argument relations (e.g., subject,
object).
We evaluate our framework on tasks relevant for cognitive science and NLP. We
start by simulating semantic priming, a phenomenon that has received much atten-
tion in computational psycholinguistics and is typically modeled using word-based
semantic spaces (Landauer and Dumais 1997; McDonald and Brew 2004). We next
consider the problem of recognizing synonyms by selecting an appropriate synonym
for a target word from a set of semantically related candidate words. Specifically, we
evaluate the performance of our model on synonym questions from the Test of English
as a Foreign Language (TOEFL). These are routinely used as a testbed for assessing
how well vector-based models capture lexical knowledge (Landauer and Dumais 1997;
Turney 2001; Sahlgren 2006). Our final experiment concentrates on unsupervised word
sense disambiguation (WSD), thereby exploring the potential of the proposed frame-
work for NLP applications requiring large scale semantic processing. We automatically
infer predominant senses in untagged text by incorporating our syntax-based semantic
spaces into the modeling paradigm proposed by McCarthy et al (2004). In all cases, we
show that our framework consistently outperforms word-based models yielding results
that are comparable or superior to state of the art.
Our contributions are threefold: a novel framework for semantic spaces that in-
corporates syntactic information in the form of dependency relations and generalizes
previous syntax-based vector-based models; an application of this framework to a wide
range of tasks relevant to cognitive modeling and NLP; and an empirical comparison of
our dependency-based models against state-of-the-art word-based models.
In Section 2, we give a brief overview of existing word-based and syntax-based
models. In Section 3, we present our modeling framework and relate it to previous work.
Section 4 discusses the parameter settings for our experiments. Section 5 details our
priming experiment, Section 6 presents our study on the TOEFL synonymy task, and
Section 7 describes our sense-ranking experiment. Discussion of our results and future
work concludes the article (Section 8).
2. Overview of Semantic Space Models
2.1 Word-Based and Syntax-Based Models
To facilitate comparisons with our framework, we begin with a brief overview of exist-
ing semantic space models. We describe traditional word-based co-occurrence models
as exemplified in Lowe (2001), Lowe and McDonald (2000), McDonald (2000), and Levy
and Bullinaria (2001), as well as syntax-based models as presented in Grefenstette (1994)
and Lin (1998a).
Lowe (2001) defines a semantic space model as a quadruple ?B, A, S, V?. B is the set
b1...D of basis elements, the dimensions of the space. B can be a set of words (Lund and
Burgess 1996) or lemmas (McDonald 2000), words with their parts of speech (Widdows
2003) or words with a syntactic relation such as subject or object (Lin 1998a). Usually,
the dimensionality of the matrix is restricted to a relatively small number. A popular
choice is the k most frequent words (minus the stop words) in a corpus, typically 100?
2,000 (McDonald 2000; Levy and Bullinaria 2001). A is a lexical association function
163
Computational Linguistics Volume 33, Number 2
applied to the co-occurrence frequency of target word t with basis element b so that each
word is represented by a vector v = ?A( f (t, b1)), A( f (t, b2)), . . . , A( f (t, bn))?. If A is the
identity function, the raw frequencies are used. Functions such as mutual information
or the log-likelihood ratio are often applied to factor out co-occurrences due to chance.
S is a similarity measure that maps pairs of vectors onto a continuous-valued scale of
contextual similarity. V is an optional transformation that reduces the dimensionality
of the semantic space. Singular value decomposition (SVD; Berry, Dumais, and O?Brien
1994; Golub and Loan 1989) is commonly used for this purpose. SVD can be thought
of as a means of inferring latent structure in distributional data, while making sparse
matrices more informative. For the rest of this article, we will ignore V and other
statistical transformations and concentrate primarily on ways of inducing structure
from grammatical and syntactic information.
To illustrate this definition, we construct a word-based semantic space for the target
words T = {lorry, carry, sweet, fruit}, using as our corpus the following sentence: A lorry
might carry sweet apples. For a word-based space, we might use the basis elements
B = {lorry, might, carry, sweet, apples}, a symmetric window of size 2, and identity as
the association function A. Each target word ti ? T will then be represented by a five-
dimensional row vector, and the value of each vector element will record the number
of times each basis element bi ? B occurs within a window of two words to the left and
two words to the right of the target word ti. The co-occurrence matrix that we obtain
according to these specifications is shown in Figure 1. A variety of distance measures
can be used to compute the similarity S between two target words (see Lee [1999] for an
overview), the cosine being the most popular:
simcos(x,y ) =
n
?
i=1
xiyi
?
n
?
i=1
x2i
?
n
?
i=1
y2i
(1)
Syntax-based semantic space models (Grefenstette 1994; Lin 1998a) go beyond mere
co-occurrence by capturing syntactic relationships between words such as subject?verb
or modifier?noun, irrespectively of whether they are physically adjacent or not. The
basis elements are generally assumed to be tuples (r, w) where w is a word occurring in
relation type r with a target word t. The relations typically reflect argument structure
(e.g., subject, object, indirect object) or modification (e.g., adjective?noun, noun?noun)
and can be obtained via shallow syntactic processing (Grefenstette 1994; Lee 1999;
Curran and Moens 2002) or full parsing (Lin 1998a; Curran and Moens 2002; Curran
2004). The basis elements (r, w) are treated as a single unit and are often called attributes
(Grefenstette 1994; Curran and Moens 2002) or features (Lin 1998a).
Figure 1
Word-based semantic space (symmetric window size 2).
164
Pad? and Lapata Dependency-Based Semantic Spaces
Figure 2 shows a syntax-based semantic space in the manner of Grefenstette (1994),
using the basis elements (subj,lorry), (aux,might), (mod,sweet), and (obj,apples). The
binary association function A records whether the target word possesses the feature
(denoted by x in Figure 2) or not. Because the cells of the matrix do not contain
numerical values, a similarity measure that is appropriate for categorical values must be
chosen. Grefenstette (1994) uses a weighted version of Jaccard?s coefficient, a measure
of association commonly employed in information retrieval (Salton and McGill 1983).
Assuming Attr(t) is the set of basis elements co-occurring with t, Jaccard?s coefficient is
defined as:
simJacc(t1, t2) =
Attr(t1) ? Attr(t2)
Attr(t1) ? Attr(t2)
(2)
Lin (1998a) constructs a semantic space similar to Grefenstette (1994) except that the
matrix cells represent the number of times a target word t co-occurs with basis element
(r, w), as shown in Figure 3. He proposes an information theoretic similarity measure
based on the distribution of target words and basis elements:
simlin(t1, t2) =
?
(r,w)?T(t1 )?T(t2)
I(t1, r, w) + I(t2, r, w)
?
(r,w)?T(t1 )
I(t1, r, w) +
?
(r,w)?T(t2 )
I(t2, r, w)
(3)
where I(t, r, w) is the mutual information between t and r, w and T(t) is the set of basis
elements (r, w) such that I(t, r, w) is positive and
I(t, r, w) = log
P(t, r, w)P(r)
P(w, r)P(t, r)
= log
P(w|r, t)
P(w|r) (4)
Figure 2
Grefenstette?s (1994) semantic space.
Figure 3
Lin?s (1988a) semantic space.
165
Computational Linguistics Volume 33, Number 2
2.2 Discussion
Because syntax-based models capture more linguistic structure than word-based mod-
els, they should at least in theory provide more informative representations of word
meaning. Unfortunately, comparisons between the two types of models have been few
and far between in the literature. Furthermore, the potential of syntax-based models
has not been fully realized since most previous approaches limit themselves to a specific
model class (Grefenstette 1994; Lin 1998a; Lin and Pantel 2001; Curran and Moens 2002).
This section discusses these issues in more detail and sketches how we plan to address
them.
Modeling of syntactic context. All existing syntax-based semantic space models we are
aware of incorporate syntactic information in a rather limited fashion. For example,
the construction of the space is either based on all relations (Grefenstette 1994; Lin
1998a) or a fixed subset (Lee 1999), but there is no qualitative distinction between
different relations. Even in cases where many relations are used (Lin 1998a; Lin and
Pantel 2001), only direct relations are taken into account, ignoring potentially important
co-occurrence patterns between, for instance, the subject and the object of a verb, or
between a verb and its non-local argument (e.g., in control structures).
Comparison between model classes. Syntax-based vector space models have been used in
NLP for a variety of lexicon acquisition tasks ranging from thesaurus extraction (Grefen-
stette 1994; Lin 1998a) to paraphrase identification (Lin and Pantel 2001) and collocation
discovery (Lin 1999; Bannard, Baldwin, and Lascarides 2003; McCarthy, Keller, and
Carroll 2003). Comparisons between word-based and syntax-based models on the same
task are rare, and the effect of syntactic knowledge has not been rigorously investigated
or quantified. The few studies on this topic reveal an inconclusive picture. On the one
hand, Grefenstette compared the performance of the two classes of models on the task
of automatic thesaurus extraction and found that a syntactically enhanced model gave
significantly better results over a simple word co-occurrence model. A replication of
Grefenstette?s study with a more sophisticated parser (Curran and Moens 2002) re-
vealed that additional syntactic information yields further improvements. On the other
hand, attempts to generate more meaningful indexing terms for information retrieval
(IR) using syntactic analysis (Salton and Smith 1989; Strzalkowski 1999; Henderson et
al. 2002) have been largely unsuccessful. Experimental results show minimal differences
in retrieval effectiveness at a substantially greater processing cost (see Voorhees [1999]
for details).
Impact on cognitive modeling. Despite their widespread use in NLP, syntax-based seman-
tic spaces have attracted little attention in cognitive science and computational psy-
cholinguistics. Wiemer-Hastings and Zipitria (2001) construct a semantic space similar
to LSA, but enhanced with part-of-speech tags with the aim of modeling human raters
in an intelligent tutoring context. Their results, however, show that the tagged LSA space
yields worse performance than a word-based model. Kanejiya, Kumar, and Prasad
(2003) attempt to capture syntactic context in a shallow manner by enhancing target
words with the parts-of-speech of their immediately preceding words. They argue that
this representation can provide useful information for the upcoming target words, as is
often the case in language modeling and left-to-right parsing. They employ a document-
based semantic space, which they submit to SVD and subsequently compare against an
LSA model that contains no syntactic information, again in the context of an intelligent
166
Pad? and Lapata Dependency-Based Semantic Spaces
tutoring system. Their results indicate that the syntactically enhanced model has better
coverage than the LSA model (i.e., it is able to evaluate more student answers), although
it displays a lower correlation with human raters than raw LSA.
In this article, we argue the case for investigating dependency-based semantic space
models in more depth. We provide a general definition for these models, which incor-
porates a wider range of syntactic relations than previously considered and subsumes
existing syntax-based and word-based models. In order to demonstrate the scope of
our framework, we evaluate our models on tasks popular in both cognitive science and
NLP. Furthermore, in all cases we report comparisons against state of the art word-
based models and show that the additional processing cost incurred by syntax-based
models is worthwhile.
3. A General Framework for Semantic Space Models
Once we move away from words as the basic context unit, the issue of representa-
tion of syntactic information becomes pertinent. An ideal syntactic formalism should
abstract over surface word order, mirror semantic relationships as closely as possible,
and incorporate word-based information in addition to syntactic analysis. It should be
also applicable to different languages. These requirements point towards dependency
grammar, which can be considered as an intermediate layer between surface syntax and
semantics. More formally, dependency relations are asymmetric binary relationships
between a head and a modifier (Tesni?re 1959). The structure of a sentence is analyzed
as a directed graph whose nodes correspond to words. The graph?s edges correspond
to dependency relationships and each edge is labeled with a specific relationship type
(e.g., subject, object).
The dependency analysis for the sentence A lorry might carry sweet apples is given
in Figure 4. On the left, the sentence is represented as a graph. The sentence head is the
main verb carry which is modified by its subject lorry, its object apples and the auxiliary
might. The subject and object are modified respectively by a determiner (a) and an ad-
jective (sweet). On the right side of Figure 4, an adjacency matrix notation is used. Edges
in the graph are represented as triples of a dependent word (e.g., lorry), a dependency
label (e.g., [N, subj, V]), and a head word (e.g., carry). The dependency label consists of
the part of speech of the modifier (capitalized, e.g., N) , the dependency relation itself
(in lower case, e.g., subj), and the part of speech of the head (also capitalized, e.g., V).
It is combinations of dependencies like the ones in Figure 4 that will form the
context over which the semantic space will be constructed. We base our discussion
Figure 4
A dependency analysis of the sentence A lorry might carry sweet apples as parse tree (left) and set
of head-relation-modifier triples (right).
167
Computational Linguistics Volume 33, Number 2
and experiments on the broad-coverage dependency parser MINIPAR, version 0.5 (Lin
1998a, 2001). However, there is nothing inherent in our formalization that restricts us to
this particular parser. Any other parser with broadly similar dependency output (e.g.,
Briscoe and Carroll 2002) could serve our purposes.
In the remainder of this section, we first give a non-technical description of our
algorithm for the construction of semantic spaces. Then, we proceed to discuss each con-
struction step (context selection, basis mapping, and quantification of co-occurrences)
in more detail. Finally, we show how our framework subsumes existing models. Table 1
lists the notation we use in the rest of the article.
3.1 The Construction Algorithm
Our algorithm for creating semantic space models is summarized in Figure 5. Central
to the construction process is the notion of paths, namely sequences of dependency
edges extracted from the dependency parse of a sentence (we define paths formally in
Section 3.2). Consider again the graph in Figure 4. Besides individual edges (i.e., paths
of length 1), it contains several longer paths, such as the path between lorry and
sweet (?lorry, carry, apples, sweet?), the path between a and carry (?a, lorry, carry?), the path
between lorry and carry (?lorry, carry?), and so forth. The usage of paths allows us to
represent direct and indirect relationships between words and gives rise to three novel
parameters:
1. The context selection function cont(t) determines which paths in the
graph contribute towards the representation of target word t. For example,
we may choose to consider only paths of length 1, or paths with
length ? 3. The function is effectively a syntax-based generalization of the
traditional ?window size? parameter.
2. The path value function v assigns weights to paths, thus allowing
linguistic knowledge to influence the construction of the space. For
Table 1
Summary of notation.
b ? B Basis element
t ? T Target word type
W(t) Set of tokens of target type t
M[t][b] ? R Cell of semantic space matrix for target word t and basis element b
? Dependency path (in a given dependency tree)
? Set of all undirected paths
?s Set of all undirected paths in sentence s
?t Set of all undirected paths in a sentence anchored at word t
start(?), end(?) First and last node of an undirected path
Cat Set of POS categories (for given parser)
R Set of dependency relations (for given parser)
l : ?? (Cat ? R ? Cat)? Edge (sequence) labeling function
cont : T ? 2?s Local context selection function (subset of paths in sentence s)
? : ?? B Basis element mapping function
v : ?? R Path value function
A : R4 ? R Lexical association function
168
Pad? and Lapata Dependency-Based Semantic Spaces
Figure 5
Algorithm for construction of semantic space.
instance, it can be used to discount longer paths, or give more weight to
paths containing subjects and objects as opposed to determiners or
modifiers.
3. The basis mapping function ? creates the dimensions of the semantic
space. Although paths themselves could serve as dimensions, the
resulting co-occurrence matrix would be overly sparse (this is especially
true for lexicalized paths whose number can become unwieldy when
parsing a large corpus). For this reason, the basis elements forming
the dimensions of the space are defined independently from the path
construction. The basis mapping function maps paths onto basis elements
by collapsing paths deemed functionally equivalent. For instance, we
may consider paths carrying the same dependency relations, or paths
ending in the same word, as equivalent. We thus disassociate the
definition of context entities (paths) from the dimensions of the final
space (basis elements).
As discussed in Section 2, the main difference among variants of semantic space models
lies in the specification of basis elements B. By treating the dependency paths as distinct
from the basis elements, we obtain a general framework for vector-based models that
can be parametrized for different tasks and allows for the construction of spaces with
basis elements consisting of words, syntactic entities, or combinations of both. This
flexibility, in conjunction with the context selection and path value functions, allows
our model to subsume both traditional word-based and syntax-based models (see
Section 3.6 for more details).
3.2 Step 1: Building the Context
The first step in constructing a semantic space from a large collection of dependency
relations is to define an appropriate syntactic context for the target words of interest.
We define contexts as anchored paths, that is, paths in a dependency graph that start
at a particular target word t. Our assumption is that the set of paths anchored at
t is a superset of the paths that can contribute relevant distributional information
about t.
169
Computational Linguistics Volume 33, Number 2
Definition 1. The dependency parse p of a sentence s is a directed graph ps = (Vs, Es),
where Es ? Vs ? Vs. The nodes v ? Vs are labeled with individual words wi. For
simplicity, we use nodes and their labels interchangeably, and the set of nodes
corresponds to the words of the sentence: Vs = {w1, . . . , wn}. Each edge e ? Es bears
a label l : Es ? Cat ? R ? Cat where Cat belongs to a set of POS tags and R to a set of
dependency relations. We assume that this set is finite and parser-specific.1 We write
edge labels in square brackets. [Det,det,N] and [N,subj,V] are examples for labels
provided by MINIPAR (see Figure 4, right-hand side).
We are now ready to define paths in our dependency graph, save one important
issue: Should we confine ourselves to directed paths or perhaps disregard the direction
of the edges? In a dependency graph, directed paths can only capture the relationship
between a head and its (potentially transitive) dependents (e.g., carry and sweet in
Figure 4). This excludes informative contexts representing, for instance, the relationship
between the subject and the object of a predicate (e.g., lorry and apples in Figure 4). Our
intuition is therefore that directed paths would limit the context too severely. In the
following, we assume undirected paths.
Definition 2. An (undirected) path ? is an ordered tuple of nodes ?v0, . . . , vn? ? V?s for
some sentence s that meets the following two constraints:
? i : (vi?1, vi) ? Es ? (vi, vi?1) ? Es (connectedness)
? i? j : i = j ? vi = vj (cycle-freeness)
In the rest of the article, we use the term path as a shorthand for undirected path.
Definition 3. A path ? is anchored at a word t iff start(?) = t. We write ?t ? ?s for the
set of all paths anchored at t in sentence s.
As an example, the set of paths anchored at lorry in Figure 4 is:
{?lorry, carry?, ?lorry, a?, (two paths of length 1)
?lorry, carry, apples?, ?lorry, carry, might?, (two paths of length 2)
?lorry, carry, apples, sweet?} (one path of length 3)
Definition 4. The context selection function cont : W ? 2?t assigns to a word t a subset
of the paths anchored at t. We call this subset the syntactic context of t.
The context selection function allows direct control over the type of linguistic informa-
tion represented in the semantic space. In traditional vector-based models, the context
1 For the sake of simplicity, we use R without a subscript to denote the set of dependency relations
provided by MINIPAR. We utilize subscripts to distinguish between general sets (e.g., E for the set of all
conceivable edges) and sentence-specific sets (e.g., Es for the set of edges in the parse tree of sentence s).
170
Pad? and Lapata Dependency-Based Semantic Spaces
selection function does not take any syntactic information into account: All paths ? are
selected for which the absolute difference (abs) between the positions (pos) of the anchor
start(?) and the end word end(?) does not exceed the window size k:
cont(t) = {? ? ?t | abs(pos(start(?)) ? pos(end(?))) ? k} (5)
The dependency-based models proposed by Grefenstette (1994) and Lin (1998a) con-
sider minimal syntactic contexts in the form of individual dependency relations,
namely, dependency paths of length 1:
cont(t) = {? ? ?t | ||?|| = 1} (6)
The context selection function as defined herein permits the elimination of paths from
the semantic space on the basis of linguistic or other information. For example, it can be
argued that subjects and objects convey more semantic information than determiners
or auxiliaries. We can thus limit our context to the set of all anchored paths consisting
exclusively of subject or object dependencies:
cont(t) = {? ? ?t | l(?) ? {[V, subj, N], [V, obj, N]}?} (7)
When this context specification function is applied to the dependency graph in Figure 4,
only the edges shown in boxes are retained. The context of lorry is thus reduced to
two paths: ?lorry, carry? (length 1) and ?lorry, carry, apples? (length 2). The paths ?lorry, a?,
?lorry, carry, might?, and ?lorry, carry, apples, sweet? are omitted because their label se-
quences (such as [N,det,Det] for ?lorry, a?) are disallowed by (7).
3.3 Step 2: Basis Mapping
The second step in the construction of our semantic space model is to specify its
dimensions, the basis elements, following Lowe?s (2001) terminology.
Definition 5. The basis mapping function ? : ?? B maps paths onto basis elements.
By dissociating dependency paths and basis elements in this way, we decouple the
observed syntactic context from its representation in the final semantic space. The
basis mapping allows us to exploit underlying relationships among different paths:
Two paths which are (in some sense) equivalent can be mapped onto the same basis
element. The function effectively introduces a partitioning of paths into equivalence
classes ?labeled? by basis elements, thus offering more flexibility in defining the basis
elements themselves.
Traditional co-occurrence models use a word-based basis mapping. This means that all
paths ending at word w are mapped onto the basis element w, resulting in a semantic
space with context words as basis elements (recall that all paths in the local context start
at the target word):
?(?) = end(?) (8)
171
Computational Linguistics Volume 33, Number 2
A word-based mapping is also possible when paths are defined over dependency
graphs. As an example consider the paths anchored at lorry in Figure 4. Using (8), these
paths are mapped to the following basis elements:
?lorry, carry? carry
?lorry, a? a
?lorry, carry, apples? apples
?lorry, carry, might? might
?lorry, carry, apples, sweet? sweet
A different mapping is used in Grefenstette (1994) and Lin (1998a), who consider only
paths of length 1. In their case, paths are mapped onto pairs representing a dependency
relation r and the end word w (see the discussion in Section 2):
?(?) = (r, end(?)) where ||?|| = 1 ? ?r? = l(?) (9)
Any plausible and computationally feasible function can be used as basis mapping.
However, in this article we restrict ourselves to models which use a word-based ba-
sis mapping. The resulting spaces are similar to traditional word-based spaces?both
use sets of context words?which allows for direct comparisons between our models
and word-based alternatives. Crucially, our models differ from traditional models in
the more general treatment of (syntactic) context: Only paths in the syntactic context,
and not surface co-occurrences, contribute towards counts in the matrix. The context
selection function supports inference over classes of basis elements (which in previous
models would have been considered distinct) as well as fine-grained control over the
types of relationships that enter into the space construction.
3.4 Step 3: Quantifying Syntactic Co-occurrence
The last step in the construction of the dependency-based semantic models is to specify
the relative importance (i.e., value) of different paths:
Definition 6. The path value function v assigns a real number to a path: v : ?? R.
Traditional models do not exploit this possibility, thus giving equal weight to all paths:
vplain(?) = 1 (10)
The path value function provides additional flexibility for incorporating linguistic in-
formation into our framework. Even if two paths are mapped onto the same basis
element (by the basis mapping), the path value function can weigh their respective
contributions differently. For instance, it could discount longer paths that express
indirect relationships between words. An example of such a length-based path value
172
Pad? and Lapata Dependency-Based Semantic Spaces
function is given in Equation (11). It assigns a value of 1 to paths of length 1 and fractions
to longer paths:
vlength(?) = 1||?|| (11)
A more linguistically informed path value function can be defined by taking into
account the obliqueness hierarchy of grammatical relations (Keenan and Comrie 1977).
According to this hierarchy subjects are more salient than objects, which in turn are
more salient than obliques (e.g., prepositional phrases). And obliques are more salient
than genitives. We thus define a linear relation-based weighting scheme that ranks paths
according to their most salient grammatical function, without considering their length:
vgram?rel(?) =
?
?
?
?
?
?
?
?
?
?
?
?
?
5 if subj ? l(?)
4 if obj ? l(?)
3 if obl ? l(?)
2 if gen ? l(?)
1 else
(12)
The path value function assigns a numerical value to each path forming the syntac-
tic context of a token t. We can next define the local co-occurrence frequency between t
and a basis element b as the sum of the path values v(?) for all paths ? ? cont(t) that are
mapped onto b. Because our semantic space construction algorithm operates over word
types, we sum the local co-occurrence frequencies for all instances of a target word type t
(written as W(t)) to obtain its global co-occurrence frequency. The latter is a measure
of the co-occurrence of t and b over the entire corpus.
Definition 7. The global co-occurrence frequency of a basis element b and a target t is
computed by the function f : B ? T ? R defined by
f (b, t) =
?
w?W(t)
?
??cont(w)??(?)=b
v(?)
The global co-occurrence frequency f (b, t) could be used directly as the matrix value
M[b][t]. However, as Lowe (2001) notes, raw counts are likely to give misleading results.
This is due to the non-uniform distribution of words in corpora which will introduce a
frequency bias so that words with similar frequency will be judged more similar than
they actually are. It is therefore advisable to use a lexical association function A to factor
out chance co-occurrences explicitly.
Our definition allows an arbitrary choice of lexical association function (see
Manning and Sch?tze [1999] for an overview). In our experiments, we follow Lowe
and McDonald (2000) in using the well-known log-likelihood ratio G2 (Dunning 1993).
We can visualize the computation using a two-by-two contingency table whose four
cells correspond to four events (Kilgarriff 2001):
t ? t
b k l
? b m n
173
Computational Linguistics Volume 33, Number 2
The top left cell records the frequency k with which t and b co-occur (i.e., k corresponds
to raw frequency counts). The top right cell l records how many times b is attested with
any word other than t, the bottom left cell m represents the frequency of any word other
than b with t, and the bottom right cell n records the frequency of pairs involving neither
b nor t. The function G2 : R4 ? R is defined as
G2(k, l, m, n) = 2(k log k + l log l + m log m + n log n
? (k + l) log(k + l) ? (k + m) log(k + m)
? (l + n) log(l + n) ? (m + n) log(m + n)
+ (k + l + m + n) log(k + l + m + n))
(13)
A naive implementation of the log-likelihood ratio would keep track of all four events
for each pair (t, b); this strategy would require updating the entire matrix for each
path and would render the construction of the space prohibitively expensive. This
can be avoided by computing only k = f (t, b), the global co-occurrence frequency,
and using the marginal frequencies of paths and targets to estimate l, m and n as
follows:
l =
?
t
f (t, b) ? k m =
?
b
f (t, b) ? k n =
?
b
?
t
f (t, b) ? (k + l + m) (14)
For example, l can be computed as the total value of all paths in the corpus that are
mapped onto b minus the value of those paths that are anchored at t.
3.5 Definition of Semantic Space
Our framework of semantic space models can now be formally specified by extending
Lowe?s (2001) definition from Section 2:
Definition 8. A semantic space is a tuple ?B, T, M, S, A, cont,?, v?. B is the set of
basis elements, T the set of target words, and M is the matrix M = B ? T. We write
M[tj][bi] ? R for the matrix cell (i, j). S : T ? T ? R is the similarity measure, and
A : R4 ? R is the lexical association function. Our additional parameters are the
content selection function cont : T ? 2?s, the basis mapping function ? : ?? B, and
the path value function v : ?? R.
Note that the set of target words T can contain either word types or word tokens. In
the preceding definitions, we have assumed that co-occurrence counts are constructed
over word types, however the framework can be also used to represent word tokens. In
this case, each set of target tokens contains exactly one word (W(t) = {t}), and the outer
summation step in Definition 7 trivially does not apply. We work with type-based spaces
in the rest of this article. The use of tokens may be appropriate for other applications
such as word sense discrimination (Sch?tze 1998).
We can now construct a semantic space that illustrates our framework. Consider
again the sentence A lorry might carry sweet apples. According to Definition 8, in
order to construct vectors for the target words T = {lorry, might, carry, sweet, fruit}, we
174
Pad? and Lapata Dependency-Based Semantic Spaces
must provide a context selection function, a basis mapping function, and a path value
function. The space resulting from a context selection function which considers exclu-
sively subject and object dependencies (see Equation (7)), a word-based basis mapping
function (see Equation (8)), and a length-based path value function (see Equation (11)),
is shown in Figure 6.
3.6 Discussion
We have proposed a general framework for semantic space models which operates
on dependency relations and allows linguistic knowledge to inform the construction
of the semantic space. The framework is highly flexible: Depending on the context
selection and basis mapping functions, semantic spaces can be constructed over words,
words and parts of speech, syntactic relations, or combinations of words and syntactic
relations. This flexibility unavoidably increases the parameter space of our models,
because there is a potentially large number of context selection or path value functions
for which semantic spaces can be constructed.
At the same time, this allows us to subsume existing semantic space models in
our framework, and facilitates comparisons across different kinds of spaces (compare
Figures 1, 3, and 6). Our space is sparser than the word-based space in Figure 1,
due to the choice of a more selective context specification function (see Equations (5)
and (7)). However, this is expected because our main motivation is to distinguish
between informative and uninformative syntactico-semantic relations. Using a minimal
context selection function results in a space that contains indisputably valid semantic
relations, excluding potentially noisy relations like the one between might and sweet. By
adding richer linguistic information to the context selection function, the space can be
expanded in a principled manner. In comparison with previous syntax-based models,
which only use direct dependency relations (see Equation (6)), our dependency-based
space additionally represents indirect semantic relations (e.g., between lorry and apples).
A smaller parameter space could have resulted from collapsing the context selection
and path value functions into one parameter, for example, by defining context selection
directly as a function from (anchored) paths to their path values, and thus assigning a
value of zero to all paths ? ? cont(t). However, we refrained from doing this for two
reasons, a methodological one and a technical one. On the methodological side, we
believe that it makes sense to keep the two concepts of context selection and context
weighting distinct. The separation allows us to experiment with different path value
functions while keeping the set of paths resulting from context selection constant.
On the technical side, the two functions are easier to specify declaratively when kept
separately. Also, a separate context selection function can be used to efficiently isolate
relevant context paths without having to compute the values for all anchored paths.
Figure 6
A dependency-based semantic space using context selection function (7), basis mapping
function (8), and path value function (11).
175
Computational Linguistics Volume 33, Number 2
The context selection function operates over a subset of dependency paths that are
anchored, cycle-free, and connected. These three preconditions on paths are meant to
reflect linguistic properties of reasonable syntactic contexts while at the same time they
guarantee the efficient construction of the semantic space. Anchoredness ensures that
all paths are semantically connected to the target; this also means that the search space
can be limited to paths starting at the target word. Cycle-freeness and connectedness
exclude linguistically meaningless paths such as paths of infinite length (cycles) or paths
consisting of several unconnected fragments. These properties guarantee that context
paths can be created incrementally, and that construction terminates.
3.7 Runtime and Implementation
Our implementation uses path templates to encode the context selection function (see
Appendix A for more details). The runtime of the semantic space construction algorithm
presented in Section 3 is O(maxg ? |cont| ? t), where maxg is the maximal degree of a node
in the grammar, |cont| the number of path templates used for context selection, and t the
number of target tokens in the corpus. This assumes that ?(?) and v(?) can be computed
in constant time, which is warranted in practice because most linguistically interesting
paths will be of limited length (in our study, all paths have a length of at most 4). The
linear runtime in the size of the corpus provides a theoretical guarantee that the method
is applicable to large corpora such as the British National Corpus (BNC).
A Java implementation of the framework presented in this article is available un-
der the GNU General Public License from http://www.coli.uni-saarland.de/?pado/
dv/dv.html. The system can create dependency spaces from the output of MINIPAR (Lin
1998b, 2001). We also provide an interface for integrating other parsers. The distribution
includes a set of prespecified parameter settings, namely the word-based basis mapping
function, and the path value and context selection functions used in our experiments.
4. Experimental Setup
In this section, we describe the corpus and parser chosen for our experiments. We also
discuss our parameter and model choice procedure, and introduce the baseline word-
based model which we use for comparison with our approach. Our experiments are
then presented in Sections 5?7.
4.1 Corpus and Parser
All our experiments were conducted on the British National Corpus (BNC), a 100-
million word collection of samples of written and spoken English (Burnard 1995). The
corpus represents a wide range of British English, including samples from newspapers,
magazines, books (both academic and fiction), letters, essays, as well as spontaneous
conversations, business or government meetings, radio shows, and phone-ins. The BNC
has been used extensively in building vector space models for many tasks relevant
for cognitive science (Patel, Bullinaria, and Levy 1998; McDonald 2000; McDonald and
Brew 2004) and NLP (McCarthy et al 2004; Weeds 2003; Widdows 2003).
In order to construct dependency spaces, the BNC was parsed with MINIPAR,
version 0.5 (Lin 1998b, 2001), a wide-coverage dependency parser. MINIPAR employs a
manually constructed grammar and a lexicon derived from WordNet with the addition
of proper names (130,000 entries in total). Lexicon entries contain part-of-speech and
176
Pad? and Lapata Dependency-Based Semantic Spaces
subcategorization information. The grammar is represented as a network of 35 nodes
(i.e., grammatical categories) and 59 edges (i.e., types of dependency relationships).
MINIPAR uses a distributed chart parsing algorithm. Grammar rules are implemented
as constraints associated with the nodes and edges. When evaluated on the SUSANNE
corpus (Sampson 1995), the parser achieved a precision of 89% and a recall of 79% in
identifying labeled dependencies (Lin 1998b).
4.2 Model Selection
The construction of semantic space models involves a large number of parameters: the
dimensions of the space, the size and type of the employed context, and the choice of
similarity function. A number of studies (Patel, Bullinaria, and Levy 1998; Levy and
Bullinaria 2001; McDonald 2000) have explored the parameter space for word-based
models in detail, using evaluation benchmarks such as human similarity judgments or
synonymy choice tests. The motivation behind such studies is to identify parameters
or parameter classes that yield consistently good performance across tasks. To avoid
overfitting, exploration of the parameter space is typically performed on a development
data set different from the test data (McDonald 2000).
The benchmark data set collected by Rubenstein and Goodenough (1965) is rou-
tinely used in NLP and cognitive science for development purposes?for example, for
evaluating automatic measures of semantic similarity (Resnik 1995; Budanitsky and
Hirst 2001; Banerjee and Pedersen 2003) or for exploring the parameter space of vec-
tor space models (McDonald 2000). It consists of 65 noun-pairs ranging from highly
synonymous (gem-jewel ) to semantically unrelated (noon-string). For each pair, a sim-
ilarity judgment (on a scale of 0 to 4) was elicited from human subjects. The average
rating for each pair represents an estimate of the perceived similarity of the two words.
Correlation analysis is often used to examine the degree of linear relationship between
the human ratings and the corresponding automatically derived similarity values.
Following previous work, we explored the parameter space of our dependency
models on the Rubenstein and Goodenough (1965) data set. The best performing model
was then used in all our subsequent experiments. We expect a dependency model
optimized on the semantic similarity task to perform well across other related lexical
tasks, which incorporate semantic similarity either directly or indirectly. This is true
for all tasks reported in this article, namely priming (Experiment 1), inferring whether
two words are synonyms (Experiment 2), and acquiring predominant word senses
(Experiment 3). Some performance gains could be expected, if parameter optimization
took place separately for each task. However, such a strategy would unavoidably lead
to overfitting, especially because our data sets are generally small (see Experiments 1
and 2).
We next detail how parameters were instantiated in our dependency models with
an emphasis on the influence of the context selection and path value functions.
Parameters. Dependency contexts were defined over a set of 14 dependency relations,
each of which occurred more than 500,000 times in the BNC and which in total ac-
counted for about 76 million of the 88 million dependency relations found in the corpus.
These relations are: amod (adjective modifier), comp1 (first complement), conj (coordina-
tion), fc (finite complement), gen (genitive noun modifier), i (the relationship between
a main clause and a complement clause), lex-mod (lexical modifier), mod (modifier), nn
(noun-noun modifier), obj (object of a verb), pcomp-n (nominal complement of preposi-
tions), rel (relative clause), s (surface subject), and subj (subject of a verb). From these,
177
Computational Linguistics Volume 33, Number 2
we constructed three context selection functions (fully described in Appendix A), which
we implemented as parser-specific templates (one template per non-lexical dependency
path):
 minimum contexts contain paths of length 1 (27 templates; in Figure 4 sweet
and carry are the minimum context for apples). This definition of syntactic
context considers only direct relations and corresponds to local verbal
predicate-argument structure.
 medium contexts add to minimum contexts dependency paths which model
the internal structure of noun phrases (length ? 3; 59 templates). In
particular, the medium context covers phenomena such as coordination,
genitive constructions, noun compounds, and different kinds of
modification.
 maximum contexts combine all templates defined over the 14 dependency
relations described above into a rich context representation (length ? 4;
123 templates).
The context specification functions were combined with the three path value functions
introduced in Section 3:
 plain (vplain, see Equation (10)) assigns the same value (namely 1) to every
path. It is the simplest path value function and assumes that all paths are
equally important.
 length (vlength, see Equation (11)) implements a length-based weighting
scheme: It assigns each path a value inversely proportional to its length,
thus giving more weight to shorter paths corresponding to more direct
relationships.
 gram-rel (vgram-rel, see Equation (12)) uses the obliqueness hierarchy
(Keenan and Comrie 1977) to rank paths according to the salience of their
grammatical relations. Specifically, each path is assigned the value of its
most salient grammatical relation (subjects are more salient than objects,
which are more salient than other noun phrases).
The combination of the three context selection and three path value functions yields
nine model instantiations.2 To facilitate comparisons with traditional semantic space
models, we used a word-based basis mapping function (see Equation (8)) and the log-
likelihood score (see Equation (13)) as our lexical association function. We also created
semantic spaces with different dimensions, using the 500, 1,000, and 2,000 most frequent
basis elements obtained from the BNC. Finally, we experimented with a variety of
similarity measures: cosine, Euclidean distance, L1 norm, Jaccard?s coefficient, Kullback-
Leibler divergence, skew divergence, and Lin?s (1998a) measure.3
2 Because the minimum context selection only considers paths of length 1, the combinations minimum-plain
and minimum-length are identical.
3 The original specification of Lin?s distance measure (Equation (3)) assumes relation?word pairs as basis
elements. Because we work with a word-based basis mapping, we use a simplified version, where
I(t, r, w) reduces to I(t, w) = log P(t,w)P(t)P(w) .
178
Pad? and Lapata Dependency-Based Semantic Spaces
Results. The effects of different parameters on modeling semantic similarity (using
Rubenstein and Goodenough?s [1965] data set) are illustrated in Tables 2 and 3. We
report the Pearson product moment correlation (?Pearson?s r?) between human ratings
of similarity and vector-based similarity. Rubenstein and Goodenough report an inter-
subject correlation of r = 0.85 on the rating task. The latter can be considered an upper
bound for what can be expected from computational models. For the sake of brevity, we
only report results with 2,000 basis elements, because we found that models with fewer
dimensions (e.g., 500 and 1,000) generally obtained worse performance. Lin?s (1998a)
similarity measure uniformly outperformed all other measures by a large margin. For
comparison, we also give the results we obtained with the cosine similarity measure
(see Table 2).
As can be seen, the gram-rel path value function performs generally worse than
length or plain. We suspect that this function is, at least in its present form, too selective,
giving a low weight to a large number of possibly informative paths without subjects
or objects. A similar result is reported in Henderson et al (2002), who find that using
the obliqueness hierarchy to isolate important index terms in an information retrieval
task degrades performance. The use of the less fine-grained length path value function
delivers better results for the medium and maximum context configurations (see Table 3).
Finally, we observe that the medium context yields the best overall performance. Within
the currently explored parameter space, medium appears to strike the best balance: It
includes some dependency paths beyond length one (corresponding to informative
indirect relations), but also avoids very long and infrequent contexts which could
potentially lead to overly sparse representations. In sum, the best dependency-based
model uses the medium content selection and length path value functions, 2,000 basis
elements, and Lin?s (1998a) similarity measure. This model will be used for our subse-
quent experiments without additional parameter tuning. We will refer to this model as
the optimal dependency-based model.
Table 2
Correlations (Pearson?s r) between elicited similarity and dependency models using the cosine
distance, 2,000 basis elements, and the log-likelihood association function.
?
?
?
?
?
?
?
?
?
Context
Path plain length gram-rel
minimum 0.45 0.45 0.43
medium 0.45 0.45 0.44
maximum 0.47 0.46 0.45
Table 3
Correlations (Pearson?s r) between elicited similarity and dependency models using Lin?s
(1998a) similarity measure, 2,000 basis elements and the log-likelihood association function.
?
?
?
?
?
?
?
?
?
Context
Path plain length gram-rel
minimum 0.58 0.58 0.58
medium 0.60 0.62 0.59
maximum 0.56 0.59 0.55
179
Computational Linguistics Volume 33, Number 2
4.3 Baseline Model
Our experiments will compare the optimal dependency model just described against a
state-of-the art word-based vector space model commonly used in the literature. The
latter employs a ?bag of words? definition of context (see Equation (5)), uses words
as basis elements, and assumes that all words are given equal weight. In order to
allow a fair comparison, we trained the word-based model on the same corpus as
the dependency-based model (the complete BNC) and selected parameters that have
been considered ?optimal? in the literature (Patel, Bullinaria, and Levy 1998; Lowe and
McDonald 2000; McDonald 2000). Specifically, we built a word-based model with a
symmetric 10 word window as context and the most frequent 500 content words from
the BNC as dimensions.4 We used log-likelihood as our lexical association function and
the cosine similarity measure5 as our distance measure.
5. Experiment 1: Single-Word Priming
A large number of modeling studies in psycholinguistics have focused on simulating
semantic priming phenomena (Lund and Burgess 1996; Lowe and McDonald 2000;
McDonald 2000; McDonald and Brew 2004). The semantic priming paradigm provides a
natural test bed for semantic space models, as it concentrates on the semantic similarity
or dissimilarity between words, and it is precisely this type of lexical relation that
vector-based models should capture. If dependency-based models indeed represent
more linguistic knowledge, they should be able to model semantic priming better than
traditional word-based models.
In this experiment, we focus on Hodgson?s (1991) single-word lexical priming study.
In single-word semantic priming, the transient presentation of a prime word like tiger
directly facilitates pronunciation or lexical decision on a target word like lion: responses
are usually faster and more accurate when the prime is semantically related to the
target than when it is unrelated. Hodgson set out to investigate which types of lexical
relations induce priming. He collected a set of 144 word pairs exemplifying six different
lexical relations: (a) synonymy (words with the same meaning, e.g., value and worth);
(b) superordination and subordination (one word is an instance of the kind expressed by
the other word, e.g., pain and sensation); (c) category coordination (words which express
two instances of a common superordinate concept, e.g., truck and train); (d) antonymy
(words with opposite meaning, e.g., friend and enemy); (e) conceptual association (the
first word subjects produce in free association given the other word, e.g., leash and dog);
and (f) phrasal association (words which co-occur in phrases, e.g., private and property).
The pairs covered the most prevalent parts of speech (adjectives, verbs, and nouns); they
were selected to be unambiguous examples of the relation type they instantiate and
were matched for frequency. Hogdson found equivalent priming effects (i.e., reduced
reading times) for all six types of lexical relation, indicating that priming was not
restricted to particular types of prime?target relation.
The priming effects reported in Hodgson (1991) have recently been modeled by
McDonald and Brew (2004), using an incremental vector-based model of contextual
4 Increasing the dimensions of the space to 1,000 and 2,000 degraded performance. Smaller context
windows did not yield performance gains either.
5 We repeated all experiments for the word-based model with Lin?s (1998a) distance measure, obtaining
consistently worse results.
180
Pad? and Lapata Dependency-Based Semantic Spaces
facilitation. Their ICE model (short for Incremental Construction of Semantic Expec-
tations) simulates the difference in effort between processing a target word preceded
by a related prime and processing the same target preceded by an unrelated prime.
This is achieved by quantifying the ability of the distributional characteristics of the
prime word to predict the distributional properties of the target. The prime word is
represented by a vector of probabilities which reflects the likely location in semantic
space of the upcoming word. When the target word is observed, the representation
is updated using a Bayesian inference mechanism to reflect the newly arrived infor-
mation. McDonald and Brew use a traditional semantic space that takes only word
co-occurrences into account and is defined over the 500 most frequent words of the
spoken portion of the BNC. They measure distance in semantic space using relative
entropy (also known as Kullback?Leibler divergence) and successfully model the data
by predicting that the distance should be lower for related prime-target pairs than for
unrelated prime?target pairs.
5.1 Method
In this experiment we follow McDonald and Brew?s (2004) methodology in simulating
semantic priming. However, because our primary focus is on the representation of the
semantic space, we do not adopt their incremental model of semantic processing. We
simply model reading time for prime?target pairs by distance in the semantic space,
without making explicit predictions about upcoming words.
From the 143 prime?target pairs listed in Hodgson (1991) (one synonymy pair is
missing in the original data set), seven pairs containing at least one low-frequency word
(less than 100 occurrences in the BNC) were removed to avoid creating vectors with
unreliable counts.6 We constructed a dependency-based model with the parameters that
yielded best performance on our development set (see Section 4.2) and a baseline word-
based model (see Section 4.3). Each prime?target pair was represented by two vectors
(one corresponding to the prime and one corresponding to the target).
These prime?target pairs form the items in this experiment. The independent vari-
ables (i.e., the variables directly manipulated by Hodgson [1991] in his original experi-
ment) are (1) the type of Lexical Relation (antonyms, synonyms, conceptual associates,
phrasal associates, category coordinates, superordinate-subordinates), and (2) the Prime
(related, unrelated). The dependent variable (i.e., the quantity being measured) is the
distance between the vector space representations of the prime and the target. The
priming effect is simulated by comparing the distances between Related and Unrelated
prime?target pairs. Because the original materials do not provide Unrelated primes, we
emulated the unrelated pairs as described in McDonald and Brew (2004), by using the
average distance of a target to all other primes of the same relation.
We test two hypotheses: first, that our dependency-based model can simulate se-
mantic priming. Failure to do so would indicate that our model is deficient because it
cannot capture basic semantic relatedness, a notion underlying many tasks in cognitive
science and NLP. Second, we predict that the dependency-based model will be better at
simulating priming than a traditional word-based one.
6 Low-frequency words are deemed to produce high variance vectors because the co-occurrence counts
needed to determine M[t][b] will be unreliable (see McDonald [2000] for further evidence). Variance can
be decreased by providing more data or by smoothing; however, we leave this to future work.
181
Computational Linguistics Volume 33, Number 2
5.2 Results
We carried out a two-way analysis of variance (ANOVA) on the simulated priming
data generated by the optimal dependency-based and the baseline word-based model.
The factors were the two independent variables introduced herein, namely Lexical
Relation (six levels) and Prime (two levels). A reliable Prime effect was observed for
the dependency-based model (F(1, 129) = 182.46, MSE = 0.93, p < 0.01): the distance
between a target and its Related prime was significantly smaller than between a tar-
get and an Unrelated prime. We also observed a reliable Prime effect for the tradi-
tional word-based model that did not use any syntactic information (F(1, 129) = 106.69,
MSE = 2.92, p < 0.01). There was no main effect of Lexical Relation for either model
(F(5, 129) < 1).
The fact that the analysis of variance has produced a significant F for the two models
only indicates that there are differences between the Related and Unrelated prime-target
means that cannot be attributed to error. Ideally, we would like to compare the two
models, for example, by quantifying the magnitude of the Prime effect. Eta-squared
(?2) is a statistic7 often used to measure the strength of an experimental effect (Howell
2002). It is analogous to r2 in correlation analysis and represents how much of the
overall variability in the dependent variable (in our case distance in semantic space)
can be explained or accounted for by the independent variable (i.e., Prime). The use of
?2 allowed us to perform comparisons between models (the higher the ?2, the better the
model). The Prime effect size was greater for the dependency model, which obtained an
?2 of 0.332 compared to the word-based model whose ?2 was 0.284. In other words, the
dependency model accounted for 33.2% of the variance, whereas the word-based model
accounted for 28.4%.
To establish whether the priming effect observed by the dependency model holds
across all relations, we next conducted separate ANOVAS for each type of Lexical Rela-
tion. The ANOVAS revealed reliable priming effects for all six relations. Table 4 shows the
mean distance values for each relation in the Related and Unrelated condition and the
Prime Effect size for the dependency model. The latter was estimated as the difference
in distance values between related and unrelated prime-target pairs (asterisks indicate
whether the difference is statistically significant, according to a two-tailed paired t-test).
For comparison, we also report the Prime Effect size that McDonald and Brew (2004)
obtained in their simulation.
To summarize, our results indicate that a semantic space model defined over depen-
dency relations simulates direct priming across a wide range of lexical relations. Fur-
thermore, our model obtained a priming effect that is not only reliable but also greater
in magnitude than the one obtained by a traditional word-based model. Although we
used a less sophisticated model than McDonald and Brew (2004), without an update
procedure and an explicit computation of expectations, we obtained priming effects
across all relations. In fact, we consider the two models complementary. McDonald and
Brew?s model could straightforwardly incorporate syntax-based semantic spaces like
the ones defined in this article.
We next examine synonymy, a single lexical relation, in more detail and assess
whether the proposed dependency model can reliably distinguish synonyms from non-
synonyms. This capability may be exploited to automatically generate corpus-based
7 Eta-squared is defined as ?2 =
SSeffect
SStotal
where SSeffect is the variance (sum of squares) created by one
particular effect (Prime in our case) and SStotal is the variance of all observations together.
182
Pad? and Lapata Dependency-Based Semantic Spaces
Table 4
Mean distance values for Related and Unrelated prime?target pairs; Prime Effect size
(= Related ? Unrelated) for the dependency model and ICE.
Lexical Relation N Related Unrelated Effect Effect
(dependency) (ICE)
Synonymy 23 0.267 0.102 0.165** 0.063
Superordination 21 0.227 0.121 0.106** 0.067
Category coordination 23 0.256 0.119 0.137** 0.074
Antonymy 24 0.292 0.127 0.165** 0.097
Conceptual association 23 0.204 0.121 0.083** 0.086
Phrasal association 22 0.146 0.103 0.043** 0.058
**p < 0.01 (2-tailed)
thesauri (Grefenstette 1994; Lin 1998a; Curran and Moens 2002) or used in applications
that utilize semantic similarity. Examples include contextual spelling correction (Jones
and Martin 1997), summarization (Barzilay 2003; Erkan and Radev 2004) and question
answering (Lin and Pantel 2001).
6. Experiment 2: Detecting Synonymy
The Test of English as a Foreign Language (TOEFL) is commonly used as a benchmark
for comparing the merits of different similarity models. The test is designed to assess
non-native speakers? knowledge of English. It consists of multiple-choice questions,
each involving a target word embedded in a sentence and four potential synonyms.
The task is to identify the real synonym. An example is shown below where crossroads
is the real synonym for intersection.
You will find the office at the main intersection.
(a) place (b) crossroads (c) roundabout (d) building
Landauer and Dumais (1997) were the first to propose the TOEFL items as a test for
lexical semantic similarity. Their LSA model achieved an accuracy of 64.4% on 80 items,
a performance comparable to the average score attained by non-native speakers taking
the test. Sahlgren (2006) uses Random Indexing, a method comparable to LSA, to
represent the meaning of words and reports a 75.0% accuracy on the same TOEFL items.
It should be noted that both Landauer and Dumais and Sahlgren report results on seen
data, that is, parameters are optimized on the entire data set until performance has
peaked.
Rather than assuming that similar words tend to occur in similar contexts, Turney
(2001) and Higgins (2004) propose models that capitalize on the collocational nature of
semantically related words. Two words are considered similar if they tend to occur near
each other. Turney uses pointwise mutual information (PMI) to measure the similarity
between a target word and each of its candidate synonyms. Co-occurrence frequencies
are retrieved from the Web using an information retrieval (IR) engine:
SimilarityPMI?IR(w1, w2) =
P(w1, w2)
P(w1)P(w2)
? hits(w1 NEAR w2)
hits(w1)hits(w2)
(15)
183
Computational Linguistics Volume 33, Number 2
where P(w1, w2) is estimated by the number of hits (i.e., number of documents) returned
by the IR engine (Turney used AltaVista) when submitting a query with the NEAR
operator.8 The PMI-IR model obtained an accuracy of 72.5% on the TOEFL data set.
Higgins (2004) proposes a modification to Equation (15): He dispenses with the
NEAR operator by concentrating on word pairs that are strictly adjacent:
SimilarityLC?IR(w1, w2) =
min(hits(w1, w2), hits(w2, w1))
hits(w1)hits(w2)
(16)
Note that Equation (16) takes the minimum number of hits for the two possible orders
w1, w2 and w2, w1 in an attempt to rule out the effects of collocations and part-of-speech
ambiguities. The LC-IR (local-context information retrieval) model outperformed PMI-
IR, achieving an accuracy of 81.3% on the TOEFL items.
6.1 Method
For this experiment, we used the TOEFL benchmark data set9 (80 items). We com-
pared our optimal dependency-based model against the baseline word-based model.
We would also like to compare the vector-based models against Turney?s (2001) and
Higgins? (2004) collocational models. Ideally, such a comparison should take place on
the same corpus. Unfortunately, downloading and parsing a snapshot of the whole Web
is outside the scope of the present article. Instead, we assessed the performance of these
models on the BNC, using a search engine which simulated AltaVista. Specifically, we
indexed the BNC using Glimpse (Manber and Wu 1994), a fast and flexible indexing and
query system.10 Glimpse supports approximate and exact matching, Boolean queries,
wild cards, regular expressions, and many other options.
For the PMI-IR model, we estimated hits(w1 NEAR w2) by retrieving and counting
the number of documents containing w1 and w2 or w2 and w1 in the same sentence.
The target w1 and its candidate synonym w2 did not have to be adjacent, but the
number of the intervening words was bounded by the length of the sentence. The
frequencies hits(w1) and hits(w2) were estimated similarly by counting the number of
documents in which w1 and w2 occurred. Ties were resolved by randomly selecting
one of the candidate synonyms. The BNC proved too small a corpus for the LC-
IR model, which relies on w1 and w2 occurring in directly adjacent positions. This
is not a problem when frequencies are obtained from Web-scale corpora, but in our
case most queries retrieved no documents at all (96.6% of hits(w1, w2) and 95% of
hits(w2, w1) were zero). We thus report only the performance of the PMI-IR model on
the BNC.
The models performed a decision task similar to TOEFL test takers: They had to
decide which one of the four alternatives was synonymous with the target word. For
the vector-based models, we computed the distance between the vector representing
the candidate word and each of the candidate synonyms, and selected the candidate
with the smallest distance. Analogously, the candidate with the largest PMI-IR value
was chosen for Turney?s (2001) model. Accuracy was measured as the percentage of
8 The NEAR operator constrains the search to documents that contain w1 and w2 within ten words of one
another, in either order.
9 The items were kindly provided to us by Thomas Landauer.
10 The software can be downloaded from http://webglimpse.net/download.php.
184
Pad? and Lapata Dependency-Based Semantic Spaces
Table 5
Comparison of different models on the TOEFL synonymy task.
Model Corpus Accuracy (%)
Random baseline ? 25.0
Word-based space BNC 61.3?
Dependency space BNC 73.0?*
PMI-IR BNC 61.3?
PMI-IR Web 72.5?*
LC-IR Web 81.3?*
?Significantly better than random guessing.
*Significantly better than word-based vector model.
right decisions the model made. We also report the accuracy of a naive baseline model
which guesses synonyms at random.
In this experiment, we aim to show that the superior performance of the depen-
dency model carries over to a different task and data set. We are further interested to see
whether linguistic information (represented in our case by dependency paths) makes
up for the vast amounts of data required by the collocational models. We therefore
compare directly previously proposed Web-based similarity models with BNC-based
vector space models.
6.2 Results
Our results11 are summarized in Table 5. We used a ?2 test to determine whether the
differences in accuracy are statistically significant. Not surprisingly, all models are sig-
nificantly better than random guessing (p < 0.01). The dependency model significantly
outperforms the word-based model and PMI-IR when the latter uses BNC frequencies
(p < 0.05). PMI-IR performs comparably to our model when using Web frequencies.
The Web-based LC-IR numerically outperforms the dependency model, however the
difference is not statistically significant on the TOEFL data set (p < 1). Expectedly, Web-
based PMI-IR and LC-IR are significantly better than the word-based vector model and
the BNC-based PMI-IR (p < 0.05).
Our results show that the dependency-based model retains its advantage over the
word-based model on the synonymy detection task. On the BNC, it also outperforms
the collocation-based PMI-IR. Our interpretation is that the conceptually simpler collo-
cation models suffer from data sparseness, whereas the dependency model can profit
from the additional distributional information it incorporates. It is a matter of future
work to examine whether dependency models can carry over their advantage to larger
corpora.
Our following experiment applies the dependency space introduced in this article
to word sense disambiguation (WSD), a task which has received much attention in NLP
and is ultimately important for document understanding.
11 We omit LSA (Landauer and Dumais 1997) and Random indexing (Sahlgren 2006) from our comparison,
because these models were not evaluated on unseen data.
185
Computational Linguistics Volume 33, Number 2
7. Experiment 3: Sense Ranking
The ability to identify the intended reading of a polysemous word (the word sense)
in context is crucial for accomplishing many NLP tasks. Examples include lexicon
acquisition, discourse parsing, or metonymy resolution. Applications such as question
answering or machine translation could also benefit from large scale word sense disam-
biguation (WSD).
Given the importance of WSD for basic NLP tasks and multilingual applications,
a variety of approaches have been proposed for disambiguating word senses. To date,
most accurate WSD systems are supervised and rely on the availability of training data
(see Yarowsky and Florian [2002], Mihalcea and Edmonds [2004], and the references
therein). Although supervised methods typically achieve better performance than their
unsupervised alternatives, their applicability is limited to those words for which sense-
labeled data exists, and their accuracy is strongly correlated with the amount of labeled
data available. Furthermore, if the distribution of senses is skewed, as is often the case,
the simple heuristic of choosing the most common or predominant sense in the training
data (henceforth ?the first sense heuristic?) delivers results competitive with supervised
approaches based on local context (Hoste et al 2002).
Obtaining the first sense heuristic via annotation is obviously costly and time
consuming. More importantly, one would expect that a word?s first sense varies across
domains and text genres (the word court in legal documents will most likely mean
?tribunal? rather than ?yard?). Therefore, manual annotation must be redone for most
new languages, domains, and sense inventories. McCarthy et al (2004) show that the
annotation bottleneck can be avoided by inferring the first sense heuristic automatically
from raw text. They argue that, even though the first sense heuristic is not a WSD
method in itself, it can be usefully combined with context-based disambiguation meth-
ods in order to alleviate the data requirements for WSD. Their method builds on the
observation that a word?s distributionally similar neighbors often provide cues about its
senses. In their model, sense ranking is equivalent to quantifying the degree of similarity
between each neighbor and each sense description of a polysemous word. The sense
most similar to the neighbors is the first sense.
McCarthy et al?s (2004) approach crucially relies on the quality of the set of neigh-
bors to acquire more or less accurate first senses. In this experiment, we examine
whether the dependency-based models discussed in this article can be used for the sense
ranking task, thereby assessing their potential for practical NLP tasks. The aims of our
experiment are twofold: (1) to investigate whether our dependency-based framework
can be used to acquire distributionally similar words that differ in quality from those
obtained with word-based models and (2) to observe their impact on WSD. We first de-
scribe McCarthy et al?s sense-ranking model, which forms the basis of our experiments,
and then detail our methodology and results.
7.1 The Sense-Ranking Model
Let w be a word, N(w) = {n1, n2, . . . , nk} the set of the k most similar words to w, and
S(w) = {ws1, ws2, . . .wsn} the set of senses for w. McCarthy et al?s (2004) model assigns
each sense wsi a ?predominant sense score? PS(wsi) as follows:
PS(wsi) =
?
nj?N(w)
simdistr(w, nj) ?
simsem(wsi, nj)
?
ws
i??S(w)
simsem(wsi? , nj)
(17)
186
Pad? and Lapata Dependency-Based Semantic Spaces
where
simsem(wsi, nj) = max
wsx?S(nj )
simWN(wsi, wsx) (18)
The predominant sense of w is simply the one with the largest PS(wsi), that is, the sense
that is maximally similar to its neighbors nj ? N(w) according to Equations (17) and (18).
This sense ranking model has four free parameters: (1) the semantic space over
which distributionally similar words are acquired, (2) the measure of distributional
similarity (simdistr), (3) the number of neighbors taken into account (k), and (4) the
measure of sense similarity (simWN). The PS score combines distributional similarity
and sense similarity, taking into account both lexical knowledge gathered from corpora
and the organization and structure of the lexical resource that provides the sense inven-
tory. A large number of sense similarity measures have been developed for WordNet
and WordNet-like taxonomies. These vary from simple edge-counting (Rada, Mili, and
Bicknell 1989) to attempts to factor in peculiarities of the network structure by consid-
ering link direction (Hirst and St-Onge 1998), relative depth (Leacock and Chodorow
1998), and density (Agirre and Rigau 1996). A number of hybrid approaches have also
been proposed that combine WordNet with corpus statistics (Resnik 1995; Jiang and
Conrath 1997).
McCarthy et al (2004) use their ranking model to automatically infer the first senses
of all nouns attested in SemCor, a subset of the Brown corpus containing 23,346 lemmas
annotated with senses according to WordNet 1.6. They acquire distributionally similar
words from a large collection of dependency relations obtained from the written part
of the BNC (90 million words) using Briscoe and Carroll?s (2002) parser. Their model
considers solely dependency paths of length one (see context selection function (5)),
and is restricted to a small set of dependency relations (verb?subject, verb?object,
noun?noun, and adjective?noun). They employ a basis mapping function that maps
paths to (r, w) tuples (see Equation (9)) and Lin?s information-theoretic similarity
measure (see Equation (3)). They obtained a type-level accuracy of 54% (a random
baseline achieved 32%) at recovering the most prevalent sense (using 50 neighbors
and either Lesk?s [1986] or Jiang and Conrath?s [1997] measures). They also used a
token disambiguator that always defaults to the automatically acquired first sense and
obtained a token-level disambiguation accuracy of 48% for Lesk (50 neighbors) and
46% for Jiang and Conrath (50 neighbors). Their baseline for this task was 24%.
7.2 Method
We replicated McCarthy et al?s (2004) study using our optimal dependency-based
model (medium context selection, length path value functions, 2,000 basis elements, Lin?s
[1998a] similarity measure, and the log-likelihood association function) and the baseline
word-based model. We used Equation (17) to find the first sense for all polysemous
nouns in SemCor (according to WordNet 1.6). Following McCarthy et al, we only
considered polysemous nouns attested in SemCor with a frequency > 2, and in our
parsed version of the BNC with a frequency ? 10. The total number of nouns after
applying the frequency cutoffs was 2,75012 and the average sense ambiguity was 4.55
12 McCarthy et al (2004) use 2,595 nouns. The slight variation is due to the different parsers employed in
the two studies. Recall that we obtain dependency relations using MINIPAR (Lin 1998b), whereas
McCarthy et al employ Briscoe and Carroll?s (2002) parser.
187
Computational Linguistics Volume 33, Number 2
(the most ambiguous word had 30 senses, and least ambiguous 2). For each one of the
2,750 nouns, we generated the set of its distributionally similar neighbors from the set
of the nouns in the intersection between the BNC and WordNet (15,656 in total).
We did not experiment in detail with WordNet-based similarity measures or with
the number of distributionally similar neighbors required for the computation of the
prevalence score. McCarthy et al (2004) undertook a thorough comparison and ob-
tained best results with 50 neighbors using Lesk?s (1986) and Jiang and Conrath?s (1997)
measures. They argue that the latter measure is more efficient for large scale WSD and
use it exclusively in all subsequent work (McCarthy et al 2004; Koeling, McCarthy, and
Carroll 2005). We thus adopted the parameters that McCarthy et al found to be optimal,
namely 50 neighbors and Jiang and Conrath?s similarity measure, which we will briefly
describe.
Jiang and Conrath?s (1997) measure estimates the similarity between two word
senses by combining taxonomic information with corpus data. It is based on the notion
of information content (IC) of a WordNet synset s. IC is defined as the negative log-
likelihood of s:
IC(s) = ? log p(s) (19)
Jiang and Conrath define a distance measure that combines IC with edge counting
by taking into account local density, node depth, and link type. They introduce two
parameters, ? and ?, that control the influence of node depth and density, respectively.
Setting ? to zero and ? to one, their measure simplifies to:
Djcn(s1, s2) = log p(s1) + log p(s2) ? 2 ? log p(lso(s1, s2)) (20)
where lso(s1, s2) is the lowest super-ordinate (most specific common subsumer) of
synsets (that is, senses) s1 and s2. We used the WordNet Similarity Package (Pedersen,
Patwardhan, and Michelizzi 2004), which provides an implementation of Jiang and
Conrath?s (1997) measure (version 0.06).13 We re-estimated the IC counts from the BNC
because those provided with the package are derived from the manually annotated
SemCor and would positively bias our results.
We replicated McCarthy et al?s (2004) procedure for evaluating the acquired pre-
dominant sense against the manually annotated SemCor. We use the following notation
to describe our evaluation measures: W is the set of all word types (|W| = 2, 570) and
Wps is the set of word types with a predominant sense, that is, with a sense that is more
frequent than the second sense in SemCor (|Wps| = 2, 338). S(w) is the set of WordNet
senses for word type w, and T(w) the set of all tokens of w. Finally, we use pssc(w) and
psr(w) to refer to the predominant sense of word w according to SemCor and the sense
ranking model, respectively, and sensesc(t) to denote the sense annotated in SemCor for
a particular token t.
We first evaluate our models performance on the sense ranking task (Accsr),
namely, on identifying the predominant sense for a word type, if one exists:
Accsr =
|{w ? Wps | pssc(w) = psr(w)}|
|Wps|
(21)
13 The package is publicly available from http://www.d.umn.edu/?tpederse/similarity.html.
188
Pad? and Lapata Dependency-Based Semantic Spaces
A baseline for the sense ranking task can be easily defined by selecting a sense at ran-
dom for each word type from its sense inventory and assuming that this is the first sense:
Randomsr = 1|Wps|
?
w ?Wps
1
|S(w)| (22)
Like McCarthy et al (2004), we also assessed the word sense disambiguation potential
(Accwsd) of the automatically acquired first senses for each word token. We assigned
the predominant sense (according to the ranking model) to every noun token, without
taking its context into account, and measured the ratio of tokens for which the first
sense given by the ranking model is identical to the SemCor gold standard sense:
Accwsd =
?
w?W
|{t ? T(w) | psr(w) = sensesc(t)}|
?
w?W
|T(w)| (23)
A baseline disambiguator can be defined by assigning a random sense to each token:
Randomwsd = 1?
w?W
|T(w)|
?
w?W
|T(w)| 1|S(w)| (24)
7.3 Results
Table 6 shows the results for the optimal dependency-based model, the random base-
line, the baseline word-based model, and McCarthy et al?s (2004) state of the art model.
As an upper bound, we report WSD accuracy when defaulting to the first (i.e., most
frequent) sense provided by SemCor. All models use 50 nearest neighbors and Jiang and
Conrath?s (1997) WordNet-based semantic similarity measure. As far as distributional
similarity is concerned, our dependency model employs Lin?s (1998a) measure and
so do McCarthy et al, whereas the traditional word co-occurrence model uses cosine.
Our model differs from McCarthy et al in the context selection, path value, and basis
mapping functions (see the subsequent discussion). We used a ?2 test to determine if
the differences in performance are statistically significant. Note that we have a slightly
different set of nouns from McCarthy et al (2004); this is due to the use of a different
Table 6
Results on sense ranking and WSD tasks, using 50 neighbors and the Jiang and Conrath (1995)
distance measure.
Models Accsr Accwsd
Random baseline 31.0 25.4
Word-based space 49.3? 49.9?$
Dependency space 54.3?* 54.3?* $
McCarthy et al 54.0?* 46.0?
Upper bound ? 67.0
?Significantly better than random baseline.
*Significantly better than word-based model.
$Significantly better than McCarthy et al
189
Computational Linguistics Volume 33, Number 2
parser and a larger corpus. We work on the assumption that this difference is negligible.
We use a set of diacritics to denote statistical significance, explanations for which are
provided in Table 6.
We first consider the predominant sense acquisition task (Accsr). Table 6 shows
that all models significantly outperform the random baseline (p < 0.01). Furthermore,
both the dependency-based model and McCarthy et al (2004) significantly outperform
the word-based model. The two dependency models yield comparable performances
(p < 1). For the WSD task, we also observe that all models significantly outperform
the random baseline (p < 0.01). Our dependency model significantly outperforms the
word-based model and McCarthy et al (p < 0.01). The word-based model performs
significantly better than McCarthy et al (p < 0.01). All models expectedly perform
worse than the upper bound (p < 0.01).
An interesting observation is that our dependency model outperforms McCarthy
et al (2004) by a large margin (8.3%) on the WSD task, whereas the two models yield
comparable performances on sense ranking. Also, the word-based model performs
significantly better than McCarthy et al on WSD, while it is significantly worse than
McCarthy et al in sense ranking. This indicates that the words for which each model
delivers the first sense correctly are different. Indeed, inspection of the first sense
assignments reveals that McCarthy et al and our dependency model have only 35.7%
nouns in common for which they predict the first sense correctly. McCarthy et al
has 34.8% nouns in common with the word-based model, which in turn has 40.3%
nouns in common with our dependency model.
To follow up on this observation, we investigated how ambiguity and word fre-
quency influence the performance of our ranking model. In theory, an automatically
acquired sense ranker should have a good accuracy on all ambiguous words in order
to do well on WSD. However, in practice the sense ranker?s performance depends
crucially on its ability to correctly predict the first sense for highly frequent and highly
ambiguous words. An additional complicating factor is the sense distribution of the
words in question. For words whose sense distributions are not particularly skewed,
getting the first sense wrong will not be entirely detrimental as long as the WSD method
misclassifies relatively frequent senses as predominant.
Take, for example, the word corner, which is attested 61 times in Semcor and has
11 senses according to WordNet 1.6. Among these, sense 1 is found seventeen times,
sense 2 fifteen, sense 3 ten, and sense 4 nine (all other senses have considerably smaller
frequencies). Now suppose that the sense ranking method wrongly identifies sense 2
as the predominant sense for corner. Using this sense, our WSD system will correctly
disambiguate 24.6% of the instances of corner in Semcor, despite the fact that it will not
receive any credit for identifying the first sense. Note that the right first sense would
yield only slightly better accuracy (i.e., 27.4%).
We grouped all ambiguous noun tokens in SemCor into five frequency bands (fre-
quencies were estimated from the BNC as it constitutes a larger sample of English than
Semcor). Table 7 illustrates our models? sense ranking and WSD accuracy according to
these bands; we also list the average sense ambiguity and number of word types for each
band. As can be seen, our dependency model obtains consistently good performance on
both tasks, even in the high ambiguity bands (Bands 1,000?5,000 and 5,000+, highlighted
in Table 7). The obtained accuracies are well above the baseline of choosing a sense at
random (for example, an average ambiguity of 8.3 in the 5000+ band corresponds to
a random baseline of 12% in the sense ranking task). This is not entirely surprising;
frequent words are represented by more reliable vectors. As a result, the acquired
neighbors are of higher quality, which counteracts the increased ambiguity.
190
Pad? and Lapata Dependency-Based Semantic Spaces
The results in Table 7 furthermore reveal that WSD performance exceeds sense
ranking accuracy in high-frequency bands (most notably in Band 1,000?5,000), which
seems counterintuitive. This effect can be explained by taking into account the observed
sense frequencies and the types of errors introduced by our model in these bands. The
distribution of senses in the high-frequency bands tends to be less skewed, at least ac-
cording to Semcor (82% of nouns in Band 1,000?5,000 and 65% in Band 5,000+ have a
first sense with frequency <50). Our model?s mistakes are often ?near misses?, that is,
the first and second sense ranks are flipped. Specifically, near misses are observed for
25% of the noun types in Band 1,000?5,000, and 15% in Band 5,000+. Now, for nouns
with non-skewed sense distributions, disambiguating with the second sense will boost
WSD accuracy even though this is not the case for sense ranking (see the previous
discussion).
Our results show that semantic space models defined according to the framework
presented in this article can be successfully used for the automatic acquisition of first
senses from raw text. We obtained results similar to McCarthy et al (2004) on the sense
ranking task and demonstrated that our model performs significantly better on WSD.
Furthermore, it outperformed a word-based semantic space on both tasks. Our model
differs from McCarthy et al in three important ways: (a) following our terminology,
they use a semantic space with the minimum context selection (paths of length one)
and plain path value (no path weighting) functions, whereas our model employs the
medium content selection and length path value functions; (b) their space is constructed
over a limited set of dependency paths, namely subject, object, and adjective-noun
modification relations, whereas our model uses a wider range of relations including
information about tense (for example, whether a complement is finite or not), relativi-
sation, etc. (see Section 4.2 for details); and (c) their basis mapping function maps paths
to tuples whereas we employ a word-based function and restrict the dimensions of the
space to the 2,000 most frequent elements (McCarthy et al do not employ any cutoffs).
Furthermore, they used a slightly smaller corpus (only the written part of the BNC,
amounting to 90% of the total corpus) and a different parser (Briscoe and Carroll 2002).
Although replicating our study with Briscoe and Carroll?s parser (2002) is outside of
the scope of this article, we should note that the two parsers yield comparable perform-
ances and employ a similar inventory of dependency relations (see Curran (2004) for
more discussion). We thus suspect that differences in WSD accuracy cannot be uniquely
attributed to parser performance. We can, however, assess whether the difference is due
to corpus size by examining its effect on the performance of our model. If it is indeed
sensitive to corpus size, we would expect a relatively large drop in performance when
Table 7
Sense ranking and WSD accuracy for the dependency-based model as word frequency and
average sense ambiguity are varied.
FBand AvgAmbig Types accsr accwsd
<50 3.29 174 0.53 0.46
50?200 3.60 489 0.54 0.49
200?1,000 4.29 1,014 0.57 0.54
1,000?5,000 5.65 583 0.51 0.57
5,000+ 8.32 78 0.50 0.51
FBand = frequency band; AvgAmbig = average WordNet sense ambiguity within frequency
band; Types = number of noun types within frequency band.
191
Computational Linguistics Volume 33, Number 2
our semantic space is built on smaller corpora. We randomized the order of sentences
in the BNC and constructed semantic spaces on data sets progressively increasing in
size: The first space was constructed from 5% of the BNC, the next from 10%, and so
on. We tested each model on the SemCor data (see Section 7.2). Figure 7 shows the
resulting learning curves. When the dependency model is constructed on 5% of the
BNC, it delivers a WSD accuracy of 51%, which eventually increases to 54.3% when
the entire corpus is used. This result indicates that the model performs well when
trained on a small corpus and that its good performance cannot be attributed solely
to corpus size. However, it also suggests that a large increase in corpus size is necessary
to obtain substantial improvements with the present sense ranking strategy, which
uses distributional similarity as a corrective for taxonomy-based similarity: Accuracy
increases by approximately 4% when our corpus size increases by a factor of 20.
We believe that the differences in performance between the two models are largely
due to differences in the basis mapping function. Because McCarthy et al (2004) use all
available basis elements, their semantic space grows linearly with vocabulary (i.e., cor-
pus) size. Each target word is represented by a set of ?features??relation?word pairs
with a non-zero occurrence frequency?which may vary widely between target words.
In contrast, our model defines a modest number of basis elements (2,000) which are
shared between all target words. The resulting representation is a vector space which is
less sparse and the resulting neighbors capture more succinctly the semantic properties
of words. Additional evidence comes from the performance of the word-based model,
which also uses a word basis mapping function and a fixed number of dimensions (500
words). Although this model does not incorporate syntactic information in any way,
it manages to outperform McCarthy et al on the WSD task. In sum, we attribute the
superior performance of the vector-based model to two key factors: low dimensionality
Figure 7
Learning curve for the dependency-based model on a randomized version for the BNC: accuracy
of predominant sense acquisition (solid) and WSD (dashed) with varying corpus size.
192
Pad? and Lapata Dependency-Based Semantic Spaces
(as seen by the comparison to McCarthy et al) and the incorporation of linguistic
knowledge (as seen by the comparison to the word-based model).
8. General Discussion
In this article, we presented a general framework for the construction of semantic space
models. The framework operates on paths of dependency relations, allowing linguistic
knowledge to guide the construction of semantic spaces. It extends previous work
on traditional word-based semantic space models as well as syntax-based models by
providing a principled way for defining the context and the dimensions of the semantic
space. More specifically, we isolated three important parameters of space construction:
the context selection function, the basis mapping function, and the path value function.
In combination, these three functions determine which paths (e.g., local or distant),
dimensions (e.g., words, parts of speech or word?relation tuples), and dependency
relations (e.g., subjects, objects) contribute towards the construction of a semantic space.
We evaluated our framework on tasks relevant for NLP and cognitive science and
compared it against state-of-the-art models. Experiment 1 revealed that semantic space
models defined over dependency relations adequately simulate semantic priming. Ex-
periments 2 and 3 examined the usefulness of our framework for NLP: we used our
model to detect synonymy relations and to automatically acquire prevalent senses for
polysemous words. In all cases, syntactically enriched models outperformed traditional
word-based models that did not take account of syntax.
Our strategy in the present study was to define a small number of generic parame-
terizations, evaluate the resulting models on a development set, and select a broadly
optimal model for testing on unseen data. Therefore, our models were not specifically
tuned for the tasks at hand and we have only explored a relatively small subset of the
parameter space. Our examination of different parameter combinations in Section 4.2
revealed that medium syntactic contents yield consistently better performance when
combined with a path value function that penalizes longer paths (length). An important
avenue for future work concerns defining more fine-grained path value functions. Our
results show that a path value function inspired by the obliqueness hierarchy delivers
worse results than the linguistically naive length function. Alternatively, we could define
a function that combines gram-rel with length, or more generally learn a weighting
scheme for paths by optimizing some objective function.
Our experiments concentrated on spaces that used solely a basis mapping function
that maps dependency paths to words. It should also be interesting to experiment with
different types of basis mapping functions. For example, we could experiment with
more coarse-grained functions based on parts-of-speech or more fine-grained ones such
as the relation?word pairs used by McCarthy et al (2004). We would also like to observe
the impact of singular value decomposition (SVD) on our semantic spaces along the
lines of Kanejiya, Kumar, and Prasad?s (2003) cognitive modeling work. They use SVD
to reduce the dimensionality of a semantic space that uses (word, part-of-speech) pairs
as basis elements, obtaining better coverage compared with an LSA space constructed
over word co-occurrences. Further studies must examine the effect of parser quality on
the obtained co-occurrences, and the influence of the chosen similarity measure.
We have just scratched the surface of the possibilities for the framework discussed
in this article. The potential applications are many and varied both for cognitive science
and NLP. Our syntactically enriched models retain the simplicity of word co-occurrence
models while allowing for the role of syntactic structure to influence the representa-
tion of the semantic space. The resulting vectors have a higher degree of linguistic
193
Computational Linguistics Volume 33, Number 2
plausibility?it is not mere lexical association that accounts for the meaning of words
but rather their lexical and syntactic dependencies. Arguably, this property holds great
promise for languages less configurational than English. A prediction that we intend to
test in the future is that syntax-based semantic space models should be able to represent
meaning more adequately than traditional word-based models for languages that allow
constituent scrambling (e.g., German) or have free word order (e.g., Czech).
It remains to be seen whether our models can capture the wide range of data that
traditional and LSA-based models have accounted for. Possible future experiments in-
clude mediated priming (Lowe and McDonald 2000) and multiple priming (McDonald
and Brew 2004), intelligent tutoring (Kanejiya, Kumar, and Prasad 2003), and coherence
rating (Foltz, Kintsch, and Landauer 1998). A number of NLP tasks could also benefit
from the framework presented in this article. Examples include word sense discrimi-
nation (Lin 1998a; Sch?tze 1998), automatic thesaurus construction (Grefenstette 1994;
Curran and Moens 2002), automatic clustering, lexicon acquisition, and in general
similarity-based approaches to NLP.
Appendix A. Context Selection Functions
In what follows we present the context selection functions we used in our experiments.
These are encoded as non-lexicalized path templates and are distributed as part of the
software package that implements our dependency-based semantic space framework
(see Section 3.7 for details). Each context selection function cont is represented by a
set of path templates, Temp(cont). Each path template directly corresponds to a path
label sequence. Path templates are denoted by a comma-separated sequence of one or
more edge labels; each edge label is a colon-separated triple POS1:relation:POS2 (see
Definition 1). The semantics of a set of path templates Temp(c) is as follows: For a target
word t and a context selection function c, the context of t consists of all paths ?t (i.e., all
paths anchored at t) so that there is a path template temp ? Temp(c) that matches the
label sequence l(?t).
Minimum:
A:amod:V
A:mod:A
A:mod:A
A:mod:N
A:mod:Prep
A:mod:V
A:subj:N
N:conj:N
N:gen:N
N:mod:A
N:mod:Prep
N:nn:N
N:obj:V
N:pcomp-n:Prep
N:subj:A
N:subj:N
N:subj:V
(null):lex-mod:V
Prep:mod:A
Prep:mod:N
Prep:mod:V
Prep:pcomp-n:N
V:amod:A
V:lex-mod:(null)
V:mod:A
V:mod:Prep
V:obj:N
V:subj:N
Medium contains all minimum
templates and:
A:mod:N,N:lex-mod:(null)
A:mod:N,N:nn:N
A:subj:N,N:lex-mod:(null)
A:subj:N,N:nn:N
N:conj:N,N:lex-mod:(null)
N:conj:N,N:nn:N
N:gen:N,N:lex-mod:(null)
N:gen:N,N:nn:N
N:nn:N,N:conj:N
N:nn:N,N:conj:N,N:nn:N
N:nn:N,N:gen:N
194
Pad? and Lapata Dependency-Based Semantic Spaces
N:nn:N,N:gen:N,N:nn:N
N:nn:N,N:mod:A
N:nn:N,N:mod:Pred
N:nn:N,N:obj:V
N:nn:N,N:subj:A
N:nn:N,N:subj:V
(null):lex-mod:N,N:conj:N
(null):lex-mod:N,N:conj:N,
N:lex-mod:(null)
(null):lex-mod:N,N:gen:N
(null):lex-mod:N,N:gen:N,
N:lex-mod:(null)
(null):lex-mod:N,N:mod:A
(null):lex-mod:N,N:mod:Pred
(null):lex-mod:N,N:obj:V
(null):lex-mod:N,N:subj:A
(null):lex-mod:N,N:subj:V
Prep:mod:N,N:lex-mod:(null)
Prep:mod:N,N:nn:N
V:obj:N,N:lex-mod:(null)
V:obj:N,N:nn:N
V:subj:N,N:lex-mod:(null)
V:subj:N,N:nn:N
Maximum contains all medium
templates and:
A:mod:A,A:mod:N,N:lex-mod:(null)
A:mod:A,A:mod:N,N:nn:N
A:mod:Prep,Prep:pcomp-n:N,
N:lex-mod:(null)
N:mod:Prep,Prep:pcomp-n:N,
N:lex-mod:(null)
N:mod:Prep,Prep:pcomp-n:N,
N:nn:N
N:nn:N,N:mod:A,A:mod:A
N:nn:N,N:mod:Prep,Prep:pcomp-n:N
N:nn:N,N:mod:Prep,Prep:pcomp-n:N,
N:nn:N
N:nn:N,N:obj:V,V:subj:N
N:nn:N,N:obj:V,V:subj:N,N:nn:N
N:nn:N,N:pcomp-n:Prep
N:nn:N,N:pcomp-n:Prep,Prep:mod:N
N:nn:N,N:pcomp-n:Prep,Prep:mod:N,
N:nn:N
N:nn:N,N:subj:V,V:obj:N
N:nn:N,N:subj:V,V:obj:N,N:nn:N
N:nn:N,V:s:C,C:fc:V
N:obj:V,V:subj:N,N:lex-mod:(null)
N:obj:V,V:subj:N,N:nn:N
N:pcomp-n:Prep,Prep:mod:N,
N:lex-mod:(null)
N:pcomp-n:Prep,Prep:mod:N,N:nn:N
N:subj:V,V:obj:N,N:lex-mod:(null)
N:subj:V,V:obj:N,N:nn:N
(null):lex-mod:N,N:mod:A,A:mod:A
(null):lex-mod:N,N:mod:Prep,
Prep:pcomp-n:N
(null):lex-mod:N,N:mod:Prep,
Prep:pcomp-n:N,N:lex-mod:(null)
(null):lex-mod:N,N:obj:V,V:subj:N
(null):lex-mod:N,N:obj:V,
V:subj:N,N:lex-mod:(null)
(null):lex-mod:N,N:pcomp-n:Pred,
Prep:mod:A
(null):lex-mod:N,N:pcomp-n:Prep
(null):lex-mod:N,N:pcomp-n:Prep,
Prep:mod:N
(null):lex-mod:N,N:pcomp-n:Prep,
Prep:mod:N,N:lex-mod:(null)
(null):lex-mod:N,N:pcomp-n:Prep,
Prep:mod:V
(null):lex-mod:N,N:rel:C,C:i:V
(null):lex-mod:N,N:subj:V,V:obj:N
(null):lex-mod:N,N:subj:V,V:obj:N,
N:lex-mod:(null)
(null):lex-mod:N,V:s:C,C:fc:V
Prep:pcomp-n:N,N:lex-mod:(null)
Prep:pcomp-n:N,N:nn:N
V:fc:C,C:s:N,N:lex-mod:(null)
V:fc:C,C:s:N,N:nn:N
V:i:C,C:rel:N,N:lex-mod:(null)
V:mod:Prep,Prep:pcomp-n:N,
N:lex-mod:(null)
Acknowledgments
We are grateful to Diana McCarthy for
providing us with the results of her system
on our data. We are also grateful to four
anonymous reviewers for Computational
Linguistics whose feedback helped to
substantially improve the present article. We
also thank Colin Bannard, Gemma Boleda,
Amit Dubey, Katrin Erk, Frank Keller, Ulrike
Pad?, and Caroline Sporleder for useful
comments and suggestions. A preliminary
version of this work was published in the
proceedings of ACL 2003; we thank the
anonymous reviewers of that paper for their
comments.
References
Agirre, Eneko and German Rigau. 1996.
Word sense disambiguation using
conceptual density. In Proceedings
of the 16th International Conference on
Computational Linguistics, pages 16?22,
Copenhagen, Denmark.
Banerjee, Satanjeev and Ted Pedersen. 2003.
Extended gloss overlaps as a measure
of semantic relatedness. In Proceedings
of the 18th International Joint Conference
on Artificial Intelligence, pages 805?810,
Acapulco, Mexico.
Bannard, Colin, Timothy Baldwin, and
Alex Lascarides. 2003. A statistical
195
Computational Linguistics Volume 33, Number 2
approach to the semantics of
verb-particles. In Proceedings of the ACL
Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment,
pages 65?72, Sapporo, Japan.
Barzilay, Regina. 2003. Information Fusion for
Multi-Document Summarization:
Paraphrasing and Generation.Ph.D. thesis,
Columbia University, New York.
Berry, Michael W., Susan T. Dumais, and
Gavin W. O?Brien. 1994. Using linear
algebra for intelligent information
retrieval. SIAM Review, 37(4):573?595.
Briscoe, Ted and John Carroll. 2002. Robust
accurate statistical annotation of general
text. In Proceedings of the 3rd International
Conference on Language Resources and
Evaluation, pages 1499?1504, Las Palmas,
Canary Islands.
Budanitsky, Alexander and Graeme Hirst.
2001. Semantic distance in WordNet:
An experimental, application-oriented
evaluation of five measures. In Proceedings
of ACL Workshop on WordNet and
Other Lexical Resources, pages 29?34,
Pittsburgh, PA.
Burnard, Lou, 1995. Users Guide for the British
National Corpus. British National Corpus
Consortium, Oxford University
Computing Service, Oxford, UK.
Choi, Freddy, Peter Wiemer-Hastings,
and Johanna Moore. 2001. Latent
Semantic Analysis for text segmentation.
In Proceedings of the 6th Conference on
Empirical Methods in Natural Language
Processing, pages 109?117, Seattle, WA.
Curran, James R. 2004. From Distributional
to Semantic Similarity. Ph.D. thesis,
University of Edinburgh.
Curran, James R. and Marc Moens. 2002.
Scaling context space. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics, pages 231?238,
Philadelphia, PA.
Dunning, Ted. 1993. Accurate methods for
the statistics of surprise and coincidence.
Computational Linguistics, 19(1):61?74.
Erkan, G?nes and Dragomir R. Radev.
2004. Lexrank: Graph-based centrality as
salience in text summarization. Journal of
Artificial Intelligence Research, 22:457?479.
Fillmore, Charles. 1965. Indirect Object
Constructions and the Ordering of
Transformations. Mouton, The Hague.
Fodor, Janet Dean. 1995. Comprehending
sentence structure. In Lila R. Gleitman and
Mark Liberman, editors, Invitation to
Cognitive Science, volume 1. MIT Press,
Cambridge, MA, pages 209?246.
Foltz, Peter W., Walter Kintsch, and
Thomas K. Landauer. 1998. The
measurement of textual coherence
with latent semantic analysis. Discourse
Process, 15:285?307.
Goldberg, Adele. 1995. Constructions.
Chicago University Press, Chicago.
Golub, Gene H. and Charles F. Van Loan.
1989. Matrix Computations. Johns Hopkins
Series in the Mathematical Sciences. Johns
Hopkins University Press, Baltimore,
3rd edition.
Green, Georgia. 1974. Semantics and Syntactic
Regularity. Indiana University Press,
Bloomington.
Grefenstette, Gregory. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer
Academic Publishers, Dordrecht.
Gropen, Jess, Steven Pinker, Michelle
Hollander, Richard Goldberg, and
Ronald Wilson. 1989. The learnability
and acquisition of the dative alternation.
Language, 65(2):203?257.
Harris, Zellig. 1968. Mathematical Structures
of Language. Wiley, New York.
Henderson, James, Paola Merlo, Ivan Petroff,
and Gerold Schneider. 2002. Using
syntactic analysis to increase efficiency in
visualizing text collections. In Proceedings
of the 19th International Conference on
Computational Linguistics, pages 335?341,
Taipei, Taiwan.
Higgins, Derrick. 2004. Which statistics
reflect semantics? Rethinking synonymy
and word similarity. In Proceedings of the
International Conference on Linguistic
Evidence, pages 265?284, T?bingen,
Germany.
Hirst, Graeme and David St-Onge. 1998.
Lexical chains as representations of
context for the detection and correction of
malapropisms. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, MA,
pages 305?332.
Hodgson, James M. 1991. Informational
constraints on pre-lexical priming.
Language and Cognitive Processes, 6:169?205.
Hoste, V?ronique, Iris Hendrickx, Walter
Daelemans, and Antal van den Bosch.
2002. Parameter optimization for
machine-learning of word sense
disambiguation. Language Engineering,
8(4):311?325.
Howell, David C. 2002. Statistical Methods
for Psychology. Duxbury, Pacific Grove,
CA, 5th edition.
Jackendoff, Ray. 1983. Semantic and Cognition.
The MIT Press, Cambridge, MA.
196
Pad? and Lapata Dependency-Based Semantic Spaces
Jiang, Jay J. and David W. Conrath. 1997.
Semantic similarity based on corpus
statistics and lexical taxonomy. In
Proceedings of the 10th International
Conference on Research in Computational
Linguistics, pages 19?33, Taipei, Taiwan.
Jones, Michael P. and James H. Martin.
1997. Contextual spelling correction using
Latent Semantic Analysis. In Proceedings
of the 5th Conference on Applied Natural
Language Processing, pages 166?173,
Washington, DC.
Kanejiya, Dharmendra, Arun Kumar,
and Surendra Prasad. 2003. Automatic
evaluation of students? answers using
syntactically enhanced LSA. In Proceedings
of the HLT-NAACL Workshop on Building
Educational Applications Using Natural
Language Processing, pages 53?60,
Edmonton, Canada.
Keenan, Edward and Bernard Comrie. 1977.
Noun phrase accessibility and universal
grammar. Linguistic Inquiry, 8:62?100.
Kilgarriff, Adam. 2001. Comparing corpora.
International Journal of Corpus Linguistics,
6(1):97?133.
Koeling, Rob, Diana McCarthy, and John
Carroll. 2005. Domain-specific sense
distributions and predominant sense
acquisition. In Proceedings of the Joint
Human Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pages 419?426,
Vancouver, Canada.
Landauer, Thomas and Susan T. Dumais.
1997. A solution to Plato?s problem:
The latent semantic analysis theory of
acquisition, induction, and representation
of knowledge. Psychological Review,
104(2):211?240.
Leacock, Claudia and Martin Chodorow.
1998. Combining local context and
WordNet similarity for word sense
identification. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, MA,
pages 265?283.
Lee, Lillian. 1999. Measures of distributional
similarity. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics, pages 25?32, College Park, MA.
Lesk, Michael. 1986. Automatic sense
disambiguation: How to tell a pine cone
from an ice cream cone. In Proceedings of
the 1986 Special Interest Group in
Documentation, pages 24?26, New York.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago.
Levy, Joseph P. and John A. Bullinaria. 2001.
Learning lexical properties from word
usage patterns. In Robert M. French and
Jacques P. Sougn?, editors, Connectionist
Models of Learning, Development, and
Evolution. Springer, pages 273?282.
Lin, Dekang. 1998a. Automatic retrieval
and clustering of similar words. In
Proceedings of the Joint Annual Meeting
of the Association for Computational
Linguistics and International Conference on
Computational Linguistics, pages 768?774,
Montr?al, Canada.
Lin, Dekang. 1998b. Dependency-based
evaluation of MINIPAR. In Proceedings
of the LREC Workshop on the Evaluation
of Parsing Systems, pages 234?241,
Granada, Spain.
Lin, Dekang. 1999. Automatic identification
of non-compositional phrases. In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics,
pages 317?324, College Park, MA.
Lin, Dekang. 2001. LaTaT: Language and text
analysis tools. In Proceedings of the 1st
Human Language Technology Conference,
pages 222?227, San Francisco, CA.
Lin, Dekang and Patrick Pantel. 2001.
Discovery of inference rules for question
answering. Natural Language Engineering,
7(4):342?360.
Lowe, Will. 2001. Towards a theory of
semantic space. In Proceedings of
the 23rd Annual Conference of the
Cognitive Science Society, pages 576?581,
Edinburgh, UK.
Lowe, Will and Scott McDonald. 2000. The
direct route: Mediated priming in semantic
space. In Proceedings of the 22nd Annual
Conference of the Cognitive Science Society,
pages 675?680, Philadelphia, PA.
Lund, Kevin and Curt Burgess. 1996.
Producing high-dimensional semantic
spaces from lexical co-occurrence.
Behavior Research Methods, Instruments,
and Computers, 28:203?208.
Manber, Udi and Sun Wu. 1994. GLIMPSE: a
tool to search through entire file systems.
In Proceedings of the USENIX Winter 1994
Technical Conference, pages 23?32, San
Francisco, CA.
Manning, Chris and Hinrich Sch?tze. 1999.
Foundations of Statistical Natural Language
Processing. MIT Press, Cambridge, MA.
McCarthy, Diana, Bill Keller, and John
Carroll. 2003. Detecting a continuum
of compositionality in phrasal verbs.
In Proceedings of the ACL Workshop
on Multiword Expressions: Analysis,
197
Computational Linguistics Volume 33, Number 2
Acquisition and Treatment, pages 73?80,
Sapporo, Japan.
McCarthy, Diana, Rob Koeling, Julie
Weeds, and John Carroll. 2004. Finding
predominant senses in untagged text. In
Proceedings of the 42th Annual Meeting of the
Association for Computational Linguistics,
pages 280?287, Barcelona, Spain.
McDonald, Scott. 2000. Environmental
Determinants of Lexical Processing Effort.
Ph.D. thesis, University of Edinburgh.
McDonald, Scott and Chris Brew. 2004. A
distributional model of semantic context
effects in lexical processing. In Proceedings
of the 42th Annual Meeting of the Association
for Computational Linguistics, pages 17?24,
Barcelona, Spain.
Mihalcea, Rada and Phil Edmonds, editors.
2004. Proceedings of Senseval-3: The Third
International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text,
Barcelona, Spain.
Miltsakaki, Eleni. 2003. The Syntax-Discourse
Interface: Effects of the Main-Subordinate
Distinction on Attention Structure. Ph.D.
thesis, University of Pennsylvania.
Morris, Robin K. 1994. Lexical and
message-level sentence context effects
on fixation times in reading. Journal of
Experimental Psychology: Learning,
Memory, and Cognition, (20):92?103.
Neville, Helen, Janet L. Nichol, Andrew
Barss, Kenneth I. Forster, and Merrill F.
Garrett. 1991. Syntactically based sentence
prosessing classes: Evidence form
event-related brain potentials. Journal of
Cognitive Neuroscience, 3:151?165.
Patel, Malti, John A. Bullinaria, and
Joseph P. Levy. 1998. Extracting
semantic representations from large
text corpora. In Proceedings of the
4th Neural Computation and Psychology
Workshop: Connectionist Representations,
pages 199?212, London.
Pedersen, Ted, Siddharth Patwardhan,
and Jason Michelizzi. 2004.
WordNet::Similarity?measuring the
relatedness of concepts. In Proceedings of
the Joint Human Language Technology
Conference and Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics, pages 38?41,
Boston, MA.
Pinker, Steven. 1989. Learnability and
Cognition: The Acquisition of Argument
Structure. The MIT Press, Cambridge, MA.
Rada, Roy, Hafedh Mili, and Ellen Bicknell.
1989. Development and application of a
metric on semantic nets. IEEE Transactions
on Systems, Man, and Cybernetics,
19(1):17?30.
Resnik, Philip. 1995. Using information
content to evaluate semantic similarity.
In Proceedings of 14th International Joint
Conference on Artificial Intelligence,
pages 448?453, Montr?al, Canada.
Rubenstein, Herbert and John B.
Goodenough. 1965. Contextual correlates
of synonymy. Communications of the ACM,
8(10):627?633.
Sahlgren, Magnus. 2006. The Word-Space
Model: Using Distributional Analysis to
Represent Syntagmatic and Paradigmatic
Relations Between Words in
High-Dimensional Vector Spaces. Ph.D.
thesis, Stockholm University.
Salton, Gerard and Michael J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill, New York.
Salton, Gerard and Maria Smith. 1989. On
the application of syntactic methodologies
in automatic text indexing. In Proceedings
of the 12th ACM SIGIR Conference,
pages 137?150, Cambridge, MA.
Salton, G., A. Wang, and C. Yang.
1975. A vector-space model for
information retrieval. Journal of the
American Society for Information Science,
18:613?620.
Sampson, Geoffrey R. 1995. English for the
Computer. Oxford University Press,
Oxford.
Sch?tze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?124.
Strzalkowski, Tomek, editor. 1999. Natural
Language Information Retrieval. Kluwer
Academic Publishers, Dordrecht.
Talmy, L. 1985. Lexicalisation patterns:
Semantic structure in lexical forms.
In T. Shopen, editor, Language
Typology and Syntactic Description III:
Grammatical Categories and the Lexicon.
Cambrige University Press, Cambridge,
pages 57?149.
Tesni?re, Lucien. 1959. Elements de syntaxe
structurale. Klincksieck, Paris.
Turney, Peter D. 2001. Mining the Web
for synonyms: PMI-IR versus LSA
on TOEFL. In Proceedings of the 12th
European Conference on Machine
Learning, pages 491?502, Freiburg,
Germany.
Voorhees, Ellen M. 1999. Natural language
processing and information retrieval.
In 2nd School on Information Extraction
(SCIE99), pages 32?48, Frascati (Rome),
Italy.
198
Pad? and Lapata Dependency-Based Semantic Spaces
Weeds, Julie. 2003. Measures and Applications
of Lexical Distributional Similarity. Ph.D.
thesis, University of Sussex, UK.
West, R. F. and K. E. Stanovich. 1986. Robust
effects of syntactic structure on visual
word processing. Journal of Memory and
Cognition, 14:104?112.
Widdows, Dominic. 2003. Unsupervised
methods for developing taxonomies by
combining syntactic and statistical
information. In Proceedings of the Joint
Human Language Technology Conference
and Annual Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 197?204, Edmonton,
Canada.
Wiemer-Hastings, Peter and Iraide
Zipitria. 2001. Rules for syntax, vectors
for semantics. In Proceedings of the
23rd Annual Conference of the Cognitive
Science Society, pages 1140?1145,
Edinburgh, UK.
Yarowsky, David and Radu Florian. 2002.
Evaluating sense disambiguation across
diverse parameter spaces. Natural
Language Engineering, 9(4):293?310.
199

A Flexible, Corpus-Driven Model of Regular
and Inverse Selectional Preferences
Katrin Erk?
University of Texas at Austin
Sebastian Pad???
Heidelberg University
Ulrike Pad??
Vico Research and Consulting GmbH
We present a vector space?based model for selectional preferences that predicts plausibility
scores for argument headwords. It does not require any lexical resources (such as WordNet). It
can be trained either on one corpus with syntactic annotation, or on a combination of a small
semantically annotated primary corpus and a large, syntactically analyzed generalization cor-
pus. Our model is able to predict inverse selectional preferences, that is, plausibility scores for
predicates given argument heads.
We evaluate our model on one NLP task (pseudo-disambiguation) and one cognitive task
(prediction of human plausibility judgments), gauging the influence of different parameters and
comparing our model against other model classes. We obtain consistent benefits from using the
disambiguation and semantic role information provided by a semantically tagged primary cor-
pus. As for parameters, we identify settings that yield good performance across a range of experi-
mental conditions. However, frequency remains a major influence of prediction quality, and
we also identify more robust parameter settings suitable for applications with many infrequent
items.
1. Introduction
Selectional preferences or selectional constraints describe knowledge about possible
and plausible fillers for a predicate?s argument positions. They model the fact that there
is often a semantically coherent set of concepts that can fill a given argument posi-
tion. Selectional preferences can help for many text analysis tasks which involve com-
paring different attachment decisions. Examples include syntactic disambiguation
(Hindle and Rooth 1993; Toutanova et al 2005), word sense disambiguation (WSD,
? Department of Linguistics, Calhoun Hall 512, 1 University Station B 5100, Austin, TX 78712.
E-mail: katrin.erk@mail.utexas.edu.
?? E-mail: pado@cl.uni-heidelberg.de.
? E-mail: ulrike.pado@vico-research.com.
Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication:
29 June 2010. Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P.
a visiting scholar at Stanford University.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 4
McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), and
characterizing the conditions under which entailment holds between two predicates
(Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al 2007). Furthermore, selec-
tional preferences are also helpful for determining linguistic properties of predicates
and predicate?argument combinations, for example in compositionality assessment
(McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations
(McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibil-
ity judgments for predicate?argument combinations (Resnik 1996) and effects in human
sentence reading times (Pad?, Crocker, and Keller 2009).
All these applications rely on the availability of broad-coverage, reliable selectional
preferences for predicates and their argument positions. Given the immense effort nec-
essary for manual semantic lexicon building and its associated reliability problems (see,
e.g., Briscoe and Boguraev 1989), all contemporary models of selectional preferences
acquire selectional preferences automatically from large corpora.
The simplest strategy is to extract triples (v, r, a) of a predicate, role, and argument
headword (or filler) from a corpus, and then to compute selectional preference as
relative frequencies. However, due to the Zipfian nature of word frequencies, the first
step on its own results in a very sparse list of headwords, in particular for less frequent
predicates. As an example, the verb anglicize only appears with nine direct objects in
the 100-million word British National Corpus (BNC, Burnard 1995). Only one of them,
name, appears more than once. Many highly plausible fillers are missing from the list,
such as word or spelling.
In order to make sensible predictions for triples that are unseen at training time,
it is crucial to add a generalization step that infers a degree of preference for new,
unseen headwords for a given predicate and role.1 The result is, in the ideal case, an
assignment to every possible headword of some degree of compatibility (or plausibil-
ity) with the predicate?s preferences. In the case of anglicize, the desired result would be
a high plausibility for words like the (previously seen) wordlist and surname as well as
the (unseen) word and spelling, and a low plausibility for (likewise unseen) words like
cow and machine.
The predominant approach to generalizing over headwords, first introduced by
Resnik (1996), is based on semantic hierarchies such as WordNet (Miller et al 1990). The
idea is to map all observed headwords onto synsets, and then generalize to a characteri-
zation of the selectional preference in terms of the WordNet noun hierarchy. This can be
achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson
2000; Clark and Weir 2001). The performance of these models relies on the coverage
of the lexical resources, which can be a problem even for English (Gildea and Jurafsky
2002). An alternative approach to generalization uses co-occurrence information, either
in the form of distributional models or through a clustering approach. These models,
which avoid dependence on lexical resources, use corpus data for generalization
(Dagan, Lee, and Pereira 1999; Rooth et al 1999; Bergsma, Lin, and Goebel 2008).
In this article, we present a lightweight model for the acquisition and representa-
tion of selectional preferences. Our model is fully distributional and does not require
any knowledge sources beyond a large corpus where subjects and objects can be iden-
tified with reasonable accuracy. Its key point is to use vector space similarity (Lund
and Burgess 1996; Laundauer and Dumais 1997) to generalize from seen to unseen
1 Some approaches also fix a role and headword list and generalize from seen predicates to other, similar
predicates.
724
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
headwords. The vector space representations which serve as a basis for computing
similarity can in principle be computed from any arbitrary corpus, given that it is large
enough. In particular, this need not be the same corpus as the one on which we observe
predicate?headword co-occurrences. Our model thus distinguishes between a primary
corpus, from which the predicate?role?headword triples are extracted, and a generali-
zation corpus for computing the vector space representations. This distinction makes
it possible to apply our model to primary corpora with rich information that are too
small for efficient generalization, such as domain-specific corpora or corpora with
deeper linguistic analysis, as long as a larger, even if potentially noisier, generalization
corpus is available. We empirically demonstrate the benefit of this distinction. We use
FrameNet (Fillmore, Johnson, and Petruck 2003) as primary corpus and the BNC as
generalization corpus, modeling selectional preferences for semantic roles with near-
perfect coverage and low error rate.2
We evaluate our model on two tasks. The first task is pseudo-disambiguation
(Yarowsky 1993), where the model decides which of two randomly chosen words is a
better filler for the given argument position. This task tests model properties that are
needed for concrete semantic analysis tasks, most notably word sense disambiguation,
but also for semantic role labeling. The second task is the prediction of human
plausibility ratings, which is a standard task-independent benchmark for the quality
of selectional preferences. We test our model across a range of parameter settings to
identify best-practice values and show that it robustly outperforms both WordNet-
based and other distributional models on both tasks.
Finally, we investigate inverse preferences, that is, preferences that arguments
have for their predicates. Although there is ample cognitive evidence for the existence
of such preferences (e.g., McRae et al 2005), to our knowledge, they have not been in-
vestigated systematically in linguistics. However, statistics about inverse preferences
have been used implicitly in computational linguistics (e.g., Hindle 1990; Rooth et al
1999). We investigate the properties of inverse selectional preferences in comparison to
regular selectional preferences, and show that it is possible to predict inverse prefer-
ences with our selectional preference model as well.
The model that we discuss in this article, EPP, was first introduced in Erk (2007)
(using a pseudo-disambiguation task for evaluation) and further studied by Pad?, Pad?,
and Erk (2007) (evaluating against human plausibility judgments). In the current text,
we perform a more extensive evaluation and analysis, including the new evaluation on
inverse preferences, and we introduce a new similarity measure, nGCM, which achieves
excellent performance in many settings.
2. Computational Models of Selectional Preferences
In this section, we provide an overview of corpus-based models of selectional prefer-
ences. See Table 1 for a summary of the notation that we use.
2 As descriptions of semantic classes of participants in events, selectional preferences are most naturally
applied to semantic argument positions, that is, semantic roles (such as agent or patient). In contrast,
syntactic argument positions (like subject and object) can comprise several semantic argument positions,
due to the presence of diathesis alternations, and thus show less consistent selectional preferences.
Nevertheless, work in computational linguistics also makes use of selectional preferences for syntactic
argument positions, considering them noisy approximations of semantic argument positions.
725
Computational Linguistics Volume 36, Number 4
Table 1
Notation used throughout the article.
w ? Lemmas Word. We assume lemmatization throughout.
v ? Preds Predicate. Preds may be a subset of Lemmas, or a set of
semantic classes.
r ? Roles Role/Argument slot. Roles may be a set of grammatical
functions, or of semantic roles.
a ? Args ? Lemmas (Potential) argument headword.
c ? C Semantic class on which selectional preferences are
conditioned, for example, WordNet sense, FrameNet frame,
or latent semantic class.
VS = (DTrans,
Basis, sim, STrans)
Vector space. Basis is a set of basis elements, sima similarity
measure, DTrans a transformation of raw counts, and STrans
a transformation of the space.
We write w = ?wb1 , . . . , wbn? for the representation of w ?
Lemmas in a vector space with Basis = {b1, . . . , bn}.
wtr,v(a) Weight of argument headword a for predicate v and role r.
2.1 Historical Models
In formal linguistics, selectional restrictions were employed as strict Boolean restrictions
by Katz and Postal (Katz and Fodor 1963; Katz and Postal 1964) as input to a mutual dis-
ambiguation process between predicates and their modifiers. Sentences are semantically
anomalous if there are no mutually consistent readings for the two words. Semantically
anomalous sentences would receive no reading, whereas ambiguous sentences would
receive several readings.
The strict dismissal as meaningless of sentences that violate selectional restrictions
was later criticized. A case in point is metaphors, which often combine predicates and
arguments from different domains (Lakoff and Johnson 1980). Wilks (1975:329) stated
that ?rejecting utterances is just what humans do not. They try to understand them.?
He proposes to reconceptualize selectional restrictions as preferences whose violation
is dispreferred, but not fatal. His proposal for a semantic interpretation mechanism still
uses semantic primitives, but always produces a single most plausible interpretation by
choosing the senses of each word that maximize the compatibility between selectional
preferences and semantic types. In this manner, he is able to compute semantic repre-
sentations for sentences that violate selectional restrictions, including metaphors such
as ?my car drinks gasoline.?
2.2 Semantic Hierarchy?Based Models
The first broad-coverage computational model of selectional preferences, and still one
of the best-known ones, namely that of Resnik (1996), belongs to the class of semantic
hierarchy?based models. These models generalize over observed headwords using a
semantic hierarchy or ontology for nouns. The two main advantages of such models are
that (a) they can make predictions for all words covered by the hierarchy, even for very
infrequent ones for which distributional representations tend to be unreliable; and (b)
the hierarchy robustly guides generalization even for few observed headwords.
Resnik?s model instantiates the set of relations Roles with grammatical functions
which can be observed in syntactically analyzed corpora. More specifically, it concen-
726
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
trates on selectional preferences for subjects and objects. For the generalization step,
Resnik?s model maps all headwords onto WordNet synsets (or classes) c. Resnik first
computes the overall selectional preference strength for each verb?relation pair (v, r),
that is, the degree to which the pair constrains possible fillers. To estimate this quantity,
the distribution of WordNet synsets for this particular verb?relation pair is compared
to the distribution of synsets over all verbs, given the relation r. Technically, this is
achieved using Kullback?Leibler divergence:
SelStr(v, r) = D(P(c|v, r)||P(c|r)) =
?
c?C
P(c|v, r)log(P(c|v, r)
P(c|r) ) (1)
The parameters P(c|v, r) and P(c|r) are estimated from the corpus frequencies of tuples
(v, r, a) and the membership of nouns a in WordNet classes c: The observed frequency
of (v, r, a) is split equally among all WordNet classes for a. This avoids word sense
disambiguration, but incurs a certain share of wrong attributions. The intuition of
SelStr(v, r) is that a verb?relation pair that only allows a limited range of argument heads
will have a posterior distribution over classes that strongly diverges from the prior.
Next, the selectional association of the triple, SelAssoc(v, r, c), is computed as the
ratio of the selectional preference strength for this particular class c to the overall selec-
tional preference strength of the verb?relation pair (v, r). This is shown in Equation (2).
SelAssoc(v, r, c) =
P(c|v, r)logP(c|v,r)P(c|r)
SelStr(v, r)
(2)
Finally, the selectional preference between a verb, a relation, and an argument head
is defined as the maximal selectional association of the verb, the relation, and any
WordNet class c that the argument can instantiate. We will refer to this model as
RESNIK herein.
In subsequent years, a number of WordNet-based models were developed that
differ from Resnik?s model in the details of how the generalization in the WordNet
hierarchy is performed. Abe and Li (1996) characterize selectional preferences by a
tree cut through the WordNet noun hierarchy that minimizes tree cut length while
maximizing accuracy of prediction. Clark and Weir (2001) perform generalization by
ascending the WordNet noun hierarchy as long as the degree of selectional preference
among siblings is not significantly different. Ciaramita and Johnson (2000) encode
WordNet in a Bayesian Network to take advantage of the Bayes nets? ability to ?ex-
plain away? ambiguity. Grishman and Sterling (1992) perform generalization on the
basis of a manually constructed semantic hierarchy specifically developed on the same
corpus.
2.3 Distributional Models
Distributional models do not make use of any lexicon resource for the generalization
step. Instead, they use word co-occurrence?typically obtained from the same corpus
as the observed headwords?for generalization. This independence from manually
constructed resources gives distributional models a good cost?benefit ratio and makes
them especially attractive for domain-specific applications. These models, like the
727
Computational Linguistics Volume 36, Number 4
semantic hierarchy?based models, usually use grammatical functions as the set Roles
for which selectional preferences are predicted.
Pereira, Tishby, and Lee (1993) and Rooth et al (1999) generalize by discovering
latent classes of noun?verb pairs with soft clustering. They model the probability of
a word a as the argument of a predicate v as the probability of generating v and a
independently from the latent classes c:
P(v, a) =
?
c?C
P(c, v, a) =
?
c?C
P(c)P(v|c)P(a|c) (3)
Pereira, Tishby, and Lee (1993) develop a task-specific procedure to optimize P(c),
P(v|c), and P(a|c). Their procedure supports hierarchical clustering and can optimize
the number of clusters. Rooth et al (1999) present a simpler Expectation Maximization?
based estimation procedure which takes the number of clusters as input parameter. We
refer to this model as ROOTH ET AL. herein.
Dagan, Lee, and Pereira (1999) introduce a general model for computing co-
occurrence probabilities with similarity-based smoothing. Although not intended as a
model of selectional preferences, it can also be interpreted as such. Given a similarity
measure sim defined on word pairs, they compute the smoothed occurrence probability
of a word w2 given w1 as
Psim(w2|w1) =
?
w?Simset(w1)
sim(w1, w)
Z(w1)
P(w2|w) (4)
where Simset(w) is the set of words most similar to w according to sim, and Z(w1) =
?
w?Simset(w1) sim(w1, w) is a normalizing factor. This model predicts w2 given w1 by
backing off from w1 to words w similar to w1. The contribution of each w in predicting
P(w2|w1) is weighted by sim(w1, w). The similarity sim(w1, w) is computed on vector
space representations.
Recently, Bergsma, Lin, and Goebel (2008) have adopted a discriminative ap-
proach to the prediction of selectional preferences. The features they use are mainly co-
occurrence statistics, enriched with morphological context features to alleviate sparse
data problems for low-frequency argument heads. They train one SVM per verb?
argument position pair, using unobserved verb?argument combinations as negative
examples, which makes their approach independent of manually annotated training
data. Schulte im Walde et al (2008) present a model that combines features of the
semantic hierarchy?based and the distributional approaches by integrating WordNet
into an EM-based clustering model; Schulte im Walde (2010) shows that integrating
noun?modifier relations improves the prediction of human plausibility judgments.
2.4 Semantic Role?Based Models
The third class of models takes advantage of semantic resources beyond simple seman-
tic hierarchies, notably of corpora with semantic role annotation. Such corpora allow the
prediction of selectional preferences for semantic roles rather than grammatical func-
tions. From a linguistic perspective, semantic roles represent a more appropriate level
for defining selectional preferences. For that reason, the role annotation provides cleaner
and more specific training data than even a manually syntactically annotated corpus
728
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
would. These advantages, however, come at the cost of considerably greater sparsity
issues.
Pad?, Crocker, and Keller (2009) present a model based on FrameNet (Fillmore,
Johnson, and Petruck 2003). This model estimates selectional preferences with a gen-
erative probability model that equates the plausibility of a (v, r, a) triple with the joint
probability of observing the thematic role r, the verb v, and the argument a, plus the
verb?s FrameNet sense c and the grammatical function gf of the argument. This joint
probability can be decomposed using the chain rule:
P(v, c, r, gf , a) = P(v)P(c|v)P(r|v, c)P(gf |r, v, c)P(a|gf , r, v, c) (5)
The model does not make any independence assumptions. To counteract sparse data
issues for the more complex terms, the model applies WordNet-based generalization
(for nouns), distributional clustering (for verbs), and Good?Turing smoothing. We refer
to this model as PADO ET AL. Another semantic role?based model was proposed by
Vandekerckhove, Sandra, and Daelemans (2009). It acquires selectional preferences for
PropBank roles from a PropBank-labeled corpus, generalizing to unseen headwords
with memory-based learning.
3. A Distributional Exemplar-Based Model of Selectional Preferences: EPP
We now present the EPP model of selectional preferences. It falls into the category of
distributional models. More specifically, it is an exemplar model that remembers all
seen headwords for a given argument position and computes the degree of plausibility
for a new headword candidate through its similarity to the stored exemplars. Exemplars
are modeled as vectors in a semantic space.
Exemplar models are a well-known modeling framework that is used in psychol-
ogy (Nosofsky 1986), in computational linguistics (under the name of memory-based
learning [Daelemans and van der Bosch 2005]), and in linguistics, particularly phonet-
ics (Hay, Nolan, and Drager 2006). The appeal of exemplar models is that they provide
a cognitively plausible process of learning as storing exemplars, and categorization as
similarity computation that is grounded in features of the exemplars (e.g., formants in
phonetics, and contexts in lexical semantics).
The representation of selectional preferences through feature vectors also fits in well
with work in psycholinguistics by McRae, Ferretti, and Amyote (1997), who studied the
characterization of verb selectional preferences through features elicited from human
subjects. They found high overlap between features used to characterize the selectional
preferences on the one hand, and features listed for typical role fillers on the other hand.
For example, features generated for the agent role of frighten include mean, scary, and
ugly, features that were also highly relevant for the typical filler noun monster.
As briefly mentioned in Section 1, we consider selectional preferences to be charac-
terizations of typical fillers for the semantic roles of a predicate. Still, we keep our model
modular to different notions of argumenthood, such that it is also applicable to the
computation of selectional preferences for syntactic dependents of a predicate, as this is
an important case for computational applications. When we compute selectional prefer-
ences for syntactic dependents rather than semantic roles, we view syntactic argument
positions as noisy approximations of semantic roles.
729
Computational Linguistics Volume 36, Number 4
3.1 The Model
As stated previously, we assume that we have two corpora which assume different func-
tions in the model: the primary corpus, which provides information about predicate?
argument co-occurrences but may be too sparse for generalization; and the large, but
potentially noisy, generalization corpus, from which we obtain reliable semantic simi-
larity estimates.
Thus, the first step is the extraction of triples (v, r, a) of a predicate v ? Preds, a
relation r ? Roles, and a headword a ? Args from the primary corpus. Let Seenargs(r, v)
be the set of argument headwords seen with an argument position r of a predicate v
in the primary corpus. Given these triples, we predict the plausibility for an arbitrary
noun a0 in position (v, r) through the semantic similarity of a0 to all the members
of Seenargs(r, v). We obtain these similarity ratings by first computing vector space
representations for both and the members of seen(r, v) from the generalization corpus,
and then using a standard vector space similarity measure. We compute the plausibility
for a0 as
SelprefEPPr,v(a0) =
?
a?Seenargs(r,v)
wtr,v(a)
Zr,v
? sim(a0, a) (6)
where sim(a0, a) is the similarity between the vector space representations of a0 and
a, wtr,v(a) a weight for the seen headword a, and Zr,v a normalization constant, Zr,v =
?
a?Seenargs(r,v) wtr,v(a), so that the number of observed exemplars for each (v, r) pair does
not matter. Because SelprefEPP is basically a weighted average over similarity values, the
range of SelprefEPP is identical to the range of the employed similarity function sim. For
example, the range is [?1, 1] for cosine similarity, or [0, 1] for the Jaccard coefficient (cf.
Section 3.3). We discuss possible choices of both the similarity sim and the weight wtr,v
in Section 3.3.
3.2 Vector Space Representations
We use vector space representations for generalization. In a vector space model, each
target word is represented as a vector, typically constructed from co-occurrence counts
with context words in a large corpus (the so-called basis elements). The underlying
assumption, which goes back to Firth (1957) and Harris (1968), is that words with similar
meanings occur in similar contexts and will be assigned similar vectors. Thus, the
distance between the vectors of two target words, as given by some distance measure
(e.g., Cosine or Jaccard), reflects their semantic similarity.
Vector space models are simple to construct, and the semantic similarity they pro-
vide has found a wide range of applications. Examples in NLP include information
retrieval (Salton, Wong, and Yang 1975), automatic thesaurus extraction (Grefenstette
1994), and predominant sense identification (McCarthy et al 2004). Lexical resources
based on distributional similarity (e.g., Lin [1998]?s thesaurus) are used in a wide range
of applications that profit from knowledge about word similarity. In cognitive science,
they have been used, for example, to account for the influence of context on human
lexical processing (McDonald and Brew 2004) and lexical priming (Lowe and McDonald
2000).
An idealized example for a semantic space representation of selectional preferences
is shown in Figure 1(a). The two ellipses represent the exemplar clouds formed by the
730
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Figure 1
An idealized vector space for the plausibilities of (shoot, agent, hunter) and (shoot, patient, hunter).
fillers of the agent and patient position of shoot, respectively. In order to judge whether a
hunter is a plausible agent of shoot, the vector space representation of hunter is compared
to the members of the exemplar cloud for the agent position?namely, poacher, policeman,
and director. Due to the high average similarity of the hunter vector to these vectors,
hunter will be judged a fairly good agent of shoot. Compare this with the result for the
patient role: hunter is rather distant from roe, deer, and buck, and is therefore predicted
to be a bad patient of shoot. However, note that hunter is still more plausible as a patient
of shoot than, for example, director.
3.3 Formalization and Parameter Choice
Vector space models have been formalized by Lowe (2001) as tuples VS = (DTrans,
Basis, sim, STrans), where Basis is a set of basis elements or dimensions, DTrans is a
transformation of raw co-occurrence counts, sim is a similarity measure, and STrans is
a transformation of the whole space, typically dimensionality reduction. An additional
parameter that becomes relevant for our use of vector spaces (cf. Equation [6]) is the
weighting function wt that determines the contribution of each exemplar to the overall
similarity. We discuss the parameters in turn and discuss our reasons for either explor-
ing them or fixing them.
Basis elements Basis. Traditionally, context words are used as basis elements, and co-
occurrence is defined in terms of a surface window. Such bag-of-words spaces tend to
group words by topics. They ignore the syntactic relation between context items and
the target, which is a problem for selectional preference modeling. The top table in
Figure 1(b) illustrates the problem: deer and hunter receive identical vectors, even though
they show complementary plausibility ratings. The reason is that deer and hunter often
co-occur in similar lexical bag-of-words contexts (namely, hunting-related activities).
The bottom table in Figure 1(b) indicates a way out of this problem, namely the use
of word-relation pairs as basis elements (Grefenstette 1994; Pad? and Lapata 2007).
This space splits the co-occurrences with context words such as shoot based on the
grammatical relation between target and context word, and this split looks different
for different words: whereas deer occurs exclusively as the object of shoot, hunter pre-
dominantly occurs as the subject. We find the reverse pattern for escape. In consequence,
731
Computational Linguistics Volume 36, Number 4
Table 2
Similarity measures explored in this article. Notation: We assume Basis = {b1, . . . , bn}. We write I
for mutual information, and BE(a) for the set of basis elements that co-occur at least once with a.
simLin(a, a?) =
?
(r,v)?BE(a)?BE(a? ) I(a,r,v)+I(a
?,r,v)
?
(r,v)?BE(a) I(a,r,v)
?
(r,v)?BE(a? ) I(a
? ,r,v) simcosine(a, a
?) =
?n
i=1 abi
?a?bi
||a||?||a?||
simDice(a, a?) =
2?|BE(a)?BE(a? )|
|BE(a)|+|BE(a? )| simJaccard(a, a
?) = |BE(a)?BE(a
? )|
|BE(a)?BE(a? )|
simnGCM(a, a?) = exp
(
?
?
?n
i=1 (
abi
||a|| ?
a?bi
||a?||
)2
)
where ||a|| =
?
?n
i=1 a
2
bi
simHindle(a, a?) =
?n
i=1 simHindle(a, a
?, i) where
simHindle(a, a?, i) =
{
min(I(a,bi ),I(a
?,bi )) if I(a, bi) > 0 and I(a?, bi) > 0
abs(max(I(a,bi ),I(a
? ,bi ))) if I(a, bi) < 0 and I(a?, bi) < 0
0 else
the resulting spaces gain the ability to distinguish between words like hunter and deer,
based on differences in typical occurrences in argument positions.
On the downside, dependency-based spaces are more expensive to compute than
word-based spaces because they require a corpus with syntactic analysis. Thus, we
explore both options. The word-based space records co-occurrences within a surface
window of 10 (lemmatized) words.3 We refer to it as WORDSPACE. The dependency-
based space, called DEPSPACE, has basis elements consisting of a grammatical function
concatenated with a word, as in the bottom example in Figure 1(b) (Pad? and Lapata
2007). Following earlier experiments on the representation of selectional preferences
in word-dependency-relation spaces (Pad?, Pad?, and Erk 2007), we use a subject?
object context specification that only considers co-occurrences between verbs and their
subjects and direct objects.4 In each case, we adopt the 2,000 most frequent context items
as basis elements.
Similarity measure sim. In principle, any similarity measure for vectors can be plugged
into our model. Previous studies that compared similarity measures came to various
conclusions about the usefulness of different measures. Cosine similarity is very popu-
lar in Information Retrieval. Lee (1999) obtains good results for the Jaccard coefficient
in pseudo-disambiguation. In the synonymy prediction task of Curran (2004), Dice
emerged in first place. Pad? and Lapata (2007) found good results with Lin?s measure
for predominant word sense identification.
Because it is unclear whether the findings about best similarity measures general-
ize to new tasks, we will investigate a range of similarity measures shown in Table 2:
Cosine, the Dice and Jaccard coefficients, Hindle?s (1990) and Lin?s (1998) mutual
information-based metrics, and an adaptation of Nosofsky?s (1986) Generalized Context
Model (GCM), a model for exemplar-based similarity from psychology. The original
GCM includes normalization by summed similarity over all classes of exemplars, which
introduces competition between categories. Our version, which we call nGCM, instead
normalizes by vector length to alleviate the influence of overall target frequency, but
3 We do not remove stop words for reasons of simplicity, as there is no unequivocal definition of this set,
and we do not wish to remove potentially informative contexts.
4 This context specification is available as soonly in the DependencyVectors software package
(http://www.nlpado.de/?sebastian/dv.html) starting from Release 2.5.
732
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
preserves the central idea that similarity decreases exponentially with distance (Shepard
1987).
All similarity measures from Table 2 are applicable to semantic spaces with arbitrary
basis elements, with the exception of the Lin measure, whose definition applies only
to dependency-based spaces. The reason is that it decomposes the basis elements into
relation?word pairs (r, v). For semantic spaces with words as basis elements, the Lin
measure can be adapted by omitting the random variable r (cf. Pad? and Lapata 2007).
Transformations DTrans and STrans. Next, we come to transformations on counts and vec-
tor spaces. Concerning the count transformations DTrans, all counts are log-likelihood
transformed (Dunning 1993), a standard procedure for word-based semantic space
models which alleviates the problematic effects of the Zipfian distribution of lexical
items, as proposed by Lowe (2001). As for transformations on the complete space STrans,
many studies do not perform dimensionality reduction at all. Others, like the LSA fam-
ily of vector spaces (Landauer and Dumais 1997), regard it as a crucial ingredient. To
gauge the impact of STrans, we compare unreduced spaces (2,000 dimensions) to 500-
dimensional spaces created using Principal Component Analysis (PCA), a standard
method for dimensionality reduction that identifies the directions of highest variance
in a high-dimensional space.
Weight functions wt. Exemplar-based models are usually applied in conjunction with a
function that can assign each exemplar an individual weight, which can be interpreted
cognitively as degree of activation (Nosofsky 1986). We assess a small number of
weight functions to investigate their importance within the EPP model. The first one,
UNI, assumes a uniform distribution, wtr,v(a) = 1. The second one, FREQ, uses the co-
occurrence frequency as weight, wtr,v(a) = freq(a, r, v), with the intuition that more fre-
quent exemplars should be both more activated and more reliable. Finally, we consider
a weight function that is an analogue of inverse document frequency in Information
Retrieval. It weights words higher that occur with a smaller number of verb?role pairs:
wtr,v(a) = log
|
?
a? Seenrv (a
? )|
|Seenrv (a)| , where we write Seenrv(a) for the set of verb?role pairs (r, v)
for which a occurs as a headword.5 We abbreviate this weight function by DISCR for
?discrimination?.
3.4 Discussion
Our EPP model can be seen as a straightforward implementation of the intuition to
model selectional preference by generalizing from seen headwords to other, similar,
words. We use vector space representations to judge the similarity of words, obtaining
a completely corpus-driven model that does not require any additional resources and is
very flexible. A complementary view on this model is as a generalization of traditional
vector space models that represent semantic similarities between pairs of words. The
EPP model goes beyond this by computing similarity between a vector and a set of other
vectors. By instantiating the set with the vectors for seen headwords of some relation r,
the similarity turns into a plausibility prediction that is specific to this relation.
Like other distributional models, the EPP model is applicable whenever corpus
data are available; no lexical resource is required. Additionally, it does not require the
headword observation step and the generalization step (cf. Section 1) to use the same
5 By keeping the constant |
?
a? Seenrv(a
? )|, we guarantee that the fraction remains larger than one, and
wtr,v(a) remains positive. This is to ensure that the weighted average in Equation (6) yields correct results.
733
Computational Linguistics Volume 36, Number 4
corpus.6 This allows us to work with a relatively small and deeply linguistically ana-
lyzed corpus of seen headwords, the FrameNet corpus, while using a much bigger data
set to generalize over seen headwords. It also allows us to make predictions for the
potentially deeper relations annotated in the primary corpus, for example, semantic
roles. We will investigate the potential of this setup in our Experiments 1 and 2.
As a distributional model, EPP avoids the two pitfalls of resource-based models.
One is a coverage problem due to the limited size of the resource (see the task-based
evaluation in Gildea and Jurafsky [2002]). For example, the semantic role?based PADO
ET AL. model resorts to class-based smoothing methods to improve coverage, which
EPP does not need. The other problem of resource-based models is that the shape of the
WordNet hierarchy determines the generalizations that the models make. These are not
always intuitive. For example, Resnik (1996) observes that (answer, obj, tragedy) receives
a high preference because tragedy in WordNet is a type of written communication, which
is a preferred argument class of answer.
The ROOTH ET AL. model (Rooth et al 1999) shares the resource independence of
EPP, but has complementary benefits and problems. Querying the probabilistic ROOTH
ET AL. model takes only constant time, whereas querying the exemplar-based EPP
model takes time linear in the number of seen arguments for the argument position.
However, the ROOTH ET AL. model requires a dedicated training phase with a space
complexity linear in the total number of verbs and nouns, which can lead to practical
problems for large corpora (cf. Section 5.1). The separation of similarity computation
and headword observation in EPP also gives the experimenter more fine-grained control
over the types and sources of information in the model.
The EPP model looks superficially similar to the model of Dagan, Lee, and Pereira
(1999). However, they differ in the role of the similarity measure: The Dagan, Lee, and
Pereira model computes a co-occurrence probability, and it uses similarity as a weight-
ing scheme. The EPP model computes similarity (of a word to the typical fillers of an
argument position), and its weighting schemes are separate from the similarity measure.
The two models also differ in the kinds of items they consider as a basis for generaliza-
tion (or smoothing): In computing the probability of seeing a word w2 after w1, the sum
in the Dagan, Lee, and Pereira model runs over all words that are similar to w1, whereas
the sum in the EPP model runs over all words that have been seen as headwords in the
argument position in question. Given that occurrence in an argument position is a form
of co-occurrence, and similarity (in both models) is computed on the basis of vectors
derived from co-occurrence counts, one could say that the sum in the EPP model runs
over words determined by first-order co-occurrence, whereas the sum in Dagan, Lee,
and Pereira runs over words chosen through second-order co-occurrence (where w1 and
w2 are second-order co-occurring if they both tend to occur with the same words w3).
4. Design of the Experimental Evaluation
In this section, we give a high-level overview over the experiments and experimental
settings we will use subsequently. Details will be provided in the following sections.
We evaluate the EPP model in three ways: We test the prediction of verbal
selectional preference models with a pseudo-disambiguation task (Experiment 1).
Then, we address the task of predicting human verb?argument plausibility ratings
(Experiment 2). Finally, we investigate inverse selectional preferences?preferences of
6 Dagan, Lee, and Pereira (1999) could in principle do the same, but do not explore this option.
734
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
nouns for the predicates that they co-occur with?again using pseudo-disambiguation
(Experiment 3).
We compare the EPP model to models from the three model categories presented in
Section 2: RESNIK as a hierarchical model; ROOTH ET AL. as a distributional model; and
PADO ET AL. as a semantic role?based model. As both Brockmann and Lapata (2003)
and Pad? (2007) have argued, no WordNet-based model systematically outperforms
the others, and the RESNIK model shows the most consistent behavior across different
scenarios. Among the distributional models, we choose ROOTH ET AL. as a model that
performs soft clustering and thus shows a marked difference to the EPP model. To our
knowledge, this is the first comparison of all three generalization paradigms: semantic
hierarchy?based, distributional, and semantic role?based.7
As mentioned earlier, we employ two tasks to evaluate the four models: pseudo-
disambiguation and the prediction of human plausibility ratings. The pseudo-
disambiguation task (Yarowsky 1993) has become a standard evaluation measure for
selectional preference models (Dagan, Lee, and Pereira 1999; Rooth et al 1999). Given
a choice of two potential headwords, the task of a selectional preference model is to
pick the more plausible one to fill a particular argument position of a given predicate.
Pseudo-disambiguation can be viewed as a word sense disambiguation task in which
the two potential headwords together form a ?pseudo-word,? for example herb/struggle
from the original words herb and struggle. The task is to ?disambiguate? the pseudo-
word to the word that fits better in the given context. It can also be viewed as an in vitro
version of semantic role labeling and dependency parsing (depending on whether the
relations are semantic roles or grammatical functions) (Zapirain, Agirre, and M?rquez
2009). In this case, the scenario is that of a sentence containing a predicate and two
words that could potentially fill an argument position of that predicate, for example, the
predicate recommend with the potential headwords herb and struggle for the grammatical
relation of direct object. The task is to decide which of the two potential headwords is
better suited to fill the argument position.
Human plausibility ratings, on the other hand, make considerably more fine-
grained distinctions than those occurring in pseudo-disambiguation tasks. Here, mod-
els predict the exact human ratings for verb?argument?role triples. Ratings are collected
to further control carefully selected experimental items for psycholinguistic studies
(Trueswell, Tanenhaus, and Garnsey 1994; McRae, Spivey-Knowlton, and Tanenhaus
1998), or are solicited for corpus-derived triples specifically to create evaluation data for
plausibility models (Brockmann and Lapata 2003; Pad? 2007).
We contrast two different levels of semantic analysis for the predicates and argu-
ment positions. In the SEM PRIMARY setting, the predicates are FrameNet frames, each
of them potentially instantiated by multiple different verbs. The argument positions in
these settings are frame-semantic roles. This setting most closely matches the notion
of selectional preferences as characterizations of semantic arguments of an event. In
addition, we study the SYN PRIMARY setting, where predicates are verbs, and argument
positions are grammatical functions (subject and direct object). Viewing grammatical
functions as shallow approximations of semantic roles, we can expect the selectional
preference models for this setting to yield noisier estimates than in the SEM PRIMARY
setting. The two settings will differ only in the choice of primary corpus, but will use
the same generalization corpus.
7 Erk (2007) has a comparison between hierarchy-based and distributional models, but does not include a
semantic role?based model.
735
Computational Linguistics Volume 36, Number 4
Table 3 illustrates the difference between the SEM PRIMARY setting and the SYN
PRIMARY setting on an example from a pseudo-disambiguation task: The SEM PRIMARY
setting has predicates like the FrameNet frame (predicate sense) ADORNING, with the
semantic role THEME as argument position. In contrast, the SYN PRIMARY setting has
predicates that are verb lemmas, such as cause, and argument positions that are gram-
matical functions (subj). In both settings, the two potential headwords (here called
headword and confounder, to be explained in more detail in the next section) to be
distinguished in the pseudo-disambiguation task are noun lemmas.
The verb?dependency?headword tuples of the SYN PRIMARY setting yield much
more coarse-grained and noisy characterizations of selectional preferences; however,
they can be extracted from corpora with only syntactic annotation. We are therefore
able to use the 100-million word BNC (Burnard 1995) as the primary corpus for this
setting by parsing it with the Minipar dependency parser (Lin 1993). Minipar could
parse almost all of the corpus, resulting in 6,005,130 parsed sentences.
For the SEM PRIMARY setting, we require a primary corpus with role-semantic
annotation. We use the much smaller FrameNet corpus (Fillmore, Johnson, and Petruck
2003). FrameNet is a semantic lexicon for English that groups words in semantic classes
called frames and lists fine-grained semantic argument roles for each frame. Ambiguity
is expressed by membership of a word in multiple frames. Each frame is exemplified
with annotated example sentences extracted from the BNC. The FrameNet release 1.2
comprises 131,582 annotated sentences (roughly three million words). To determine
headwords of the semantic roles, the corpus was parsed using the Collins (1997) parser.
As generalization corpus, we use the Minipar-parsed BNC in both settings. The ex-
perimentation with two different primary corpora allows us to directly study the influ-
ence of the disambiguation of predicates and the semantic characterization of argument
positions on the performance of selectional preference models. Note, however, that the
comparison is complicated by differences between the two corpora: The primary corpus
for the SYN PRIMARY setting is parsed automatically, which can introduce noise in the
determination of predicates, grammatical functions, and headwords. The primary cor-
pus for the SEM PRIMARY setting is manually annotated for semantics but is parsed
automatically to determine headwords. This can introduce noise in the headwords, but
not in the determination of predicates and semantic roles. Also, the primary corpus for
the SYN PRIMARY setting is much larger than the one used in the SEM PRIMARY setting.
5. Experiment 1: Pseudo-Disambiguation
The first experiment uses a pseudo-disambiguation task to evaluate the models? perfor-
mance on modeling the plausibility of nouns as headwords of argument positions of
verbal predicates.
Table 3
Pseudo-disambiguation items for the SYN PRIMARY setting and the SEM PRIMARY setting.
Setting Predicate (v) Arg. pos. (r) Headword (a) Confounder (a?)
SYN cause subj succession island
appear subj feasibility desire
SEM ADORNING THEME illustration axe
ROPE_MANIPULATION ROPE cord literature
736
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Require: Some corpus T: a list of triples (v, r, a) of seen predicates, roles, and arguments.
Require: Some corpus N: a list of noun lemmas, along with a function freqN : N ?  
that associates each noun n ? N with its corpus frequency.
1: Nmid = {n ? N | freqN(n) ? 30 and freqN(n) ? 3, 000}
2: We define a probability distribution pN over the n ? Nmid by pN(n) = freqN (n)?
m freqN (m)
3: conf = { } # set of headword/confounder mappings, starts empty
4: AT = {a | (v, r, a) ? T} # set of seen headwords
5: for every a in AT do
6: choose a confounder a? ? Nmid according to pN
7: conf = conf ? { a ? a? }
8: end for
9: Return: conf
Figure 2
Algorithm for choosing confounders.
5.1 Setup
Task and data. In a data set of tuples (v, r, a) of a predicate v, argument position r, and
headword a, each tuple is paired with a confounder a?. The task is to pick the original
headword by comparing the tuples (v, r, a) and (v, r, a?). Table 3 shows some examples.
We begin by collecting all triples (v, r, a) observed in the respective primary corpus.
In the SYN PRIMARY setting, this corresponds to all headwords observed in subject or
direct object position of a verbal predicate in the BNC, and in the SEM PRIMARY setting,
to all nouns observed as headword of some semantic role in a frame introduced by a
verb. From this set of triples (v, r, a) for a given primary corpus, we draw an evaluation
sample that is balanced by the corpus frequency of predicates and argument position.
As test set, we choose 100 (v, r) pairs at random, drawing 20 pairs each from five fre-
quency bands: 50?100 occurrences; 100?200 occurrences; 200?500; 500?1,000; and more
than 1,000 occurrences. For any chosen predicate?relation pair, we sample triples (v, r, a)
equally from six frequency bands of arguments a: 1?50 occurrences; 50?100; 100?200;
200?500; 500-1,000; and more than 1,000 occurrences. These evaluation samples contain
a total of 213,929 (SYN) and 65,902 (SEM) tuples.
Next, we pair each headword with a confounder sampled from the primary corpus
as described in Figure 2.8 In the literature, there have been two different approaches to
choosing confounders for pseudo-disambiguation tasks: The first approach, used by
Dagan, Lee, and Pereira (1999), chooses confounders to match the headword a in
frequency. The second approach, used in Rooth et al (1999), sets the probability that
a word is drawn as a confounder to its relative frequency. The advantage and dis-
advantage of the first approach is that it largely eliminates the frequency bias that is
a general problem of vector space-based approaches. This is an advantage in that it
allows the generalization achieved by the model to be evaluated without any distortion
from frequency bias. It is a disadvantage in that in any practical application making
use of selectional preferences, the data will not be frequency-balanced. For example,
selectional preferences could be used by a dependency parser to decide which word in
the sentence to link to a given verb via a subject edge, or selectional preferences could
8 The confounder is the same for all instances of the headword a in the evaluation sample, regardless of the
values for r and v. As confounder candidates, we only use words with between 30 and 3,000 occurrences
in the BNC, following Rooth et al (1999).
737
Computational Linguistics Volume 36, Number 4
be used by a semantic role labeler to decide which constituent is the overall best filler
for the AGENT role for a given predicate. In such cases, it does not appear warranted to
assume that the frequencies of different headword candidates are balanced. We choose
the second option for our experiments, using relative corpus frequency to approximate
the probability of encountering different headword candidates.
Training of models. As stated earlier, we evaluate all models in the SYN PRIMARY setting
and the SEM PRIMARY setting. In all experiments herein, we perform two 2-fold cross-
validations runs. In each run, we randomly split the respective (SYN or SEM) evaluation
sample into a training and a test set at the token level. Figure 3 describes the experimen-
tal procedure in pseudo-code.
The EPP, RESNIK, and PADO ET AL. models are trained on the training split of the
evaluation sample. The EPP model additionally uses the BNC as generalization corpus
in both the SYN PRIMARY setting and the SEM PRIMARY setting. This generalization
corpus is used to compute either a WORDSPACE or a DEPSPACE vector space, as
discussed in Section 3.3. For the ROOTH ET AL. model, we had to employ a frequency
Require: A set Formalisms of formalisms to test
Require: A primary corpus T: a list of triples (v, r, a) of seen predicates, argument
positions, and arguments, along with a function freqT : T ?   that associates each
triple (v, r, a) ? T with its corpus frequency
Require: A mapping conf : Lemmas ? Lemmas of headwords to confounders such that
{a | (v, r, a) ? T} ? Domain(conf )
1: eval_results = { }
2: for splitno in 1:2 do
3: # prepare two independent splits
4: half1 = { }, half2 = { } # mappings from headwords to counts
5: for each tuple t in T do
6: # decide how many occurrences of t to put in half1, half2 by drawing from the binomial
distribution
7: Sample k ? B( freqT(t), 0.5)
8: half1 = half1 ? { t ? k }, half2 = half2 ? { t ? freqT(t) ? k }
9: end for
10: splits = { (half1, half2), (half2, half1) }
11: for ( ftrain, ftest) in splits do
12: for each formalism F in Formalisms do
13: train a model mF according to formalism F using the training set defined by
the frequency function ftrain.
14: for each tuple (v, r, a) in T do
15: for i in 1:ftest(v, r, a) do
16: Evaluate the performance of mF on the tuple (v, r, a, conf (a)) and add the
result to eval_results
17: end for
18: end for
19: end for
20: end for
21: end for
22: Return: eval_results
Figure 3
Algorithm for running a pseudo-disambiguation experiment
738
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
cutoff of five in the SYN PRIMARY setting to reduce the amount of training data due to
memory limitations. The PADO ET AL. model is only used in the SEM PRIMARY setting:
FrameNet is an integral part of this model, and it cannot be used in a syntax-only
setting without major changes. For details on training, see Section 2.4. Note that no
verb classes had to be induced from the data, because the predicates v are already
instantiated by verb classes, namely, FrameNet frames (see Table 3).
Finally, we report three baselines. The first baseline, headword frequency (HW), is
very simple. It decides between the headword a and the confounder a? by comparing
the frequencies f (a) and f (a?). The second, more informed, baseline is triple frequency
(TRIPLE). It votes for a if f (v, r, a) > f (v, r, a?), and vice versa. The third baseline, a bigram
language model (LM), was constructed by training a 2-gram language model from the
large English ukWAC Web corpus (Baroni et al 2009) using the SRILM toolkit (Stolcke
2002) with default Good?Turing smoothing. We retained only verbs, nouns, adjectives,
and adverbs in order to maximize the proximity between verbs and their subjects and
objects. We defined the preference score for verb?subject triples as the probability of the
sequence av, that is, Pref (v, subj, a) = P(v|a). Conversely, the preference score for verb?
object triples was defined as the probability of the sequence va, that is, Pref (v, obj, a) =
P(a|v). Again, the model compares Pref (v, r, a) and Pref (v, r, a?) to make its decision.
Evaluation. For all models, we report two evaluation figures. One is coverage: A tuple
is covered if the model assigns some preference to both a and a?, and the preferences are
not equal. The second is error rate, which is the relative frequency, among all covered
tuples, of instances where the confounder was at least equally preferred. Both coverage
and error rate are averages over the 2 x 2 cross-validation runs in each setting.
We determine the statistical significance of differences between error rates using
bootstrap resampling (Efron and Tibshirani 1994). This procedure samples correspond-
ing model predictions with replacement from the set of predictions made by the models
to be compared and computes the difference in error rates. On the basis of n such
samples (n = 1,000), the empirical 95% confidence interval for the difference in strength
on the basis of all observed differences is computed. If the interval includes 0, the
difference is not statistically significant.
5.2 SYN PRIMARY Setting: Results
Table 4 shows the results for the SYN PRIMARY setting. The overall best error rate is
achieved by a variant of the EPP model, with the RESNIK model coming in second
(the performance difference is significant at the 0.05 level). The EPP variants also show
near-perfect coverage, whereas the RESNIK model delivers results only for 63% of the
data points. We found a very high error rate and a comparatively low coverage for
ROOTH ET AL., which most likely stems from the data pruning necessary to reduce the
training data (compare the subsequent results in the SEM PRIMARY setting). The PADO
ET AL. model was not tested in the SEM PRIMARY setting, because it requires semantic
role annotation. The HW baseline is somewhat below chance (50%), which is an effect of
our by-token sampling procedure, according to which confounders often have higher
corpus frequencies than the real arguments. The TRIPLE baseline has a better error rate
than the LM baseline, but has very low coverage. Both the RESNIK and the EPP models
outperform the baselines in terms of error rate. That they outperform the TRIPLE
baseline in terms of error rate indicates that we sometimes have confounders that have
actually been seen more often with the verb?argument pair than the headword, but that
739
Computational Linguistics Volume 36, Number 4
Table 4
SYN PRIMARY setting: Pseudo-disambiguation results for different weighting schemes.
Model Similarity Error rate (%) Coverage (%)
UNI FREQ DISCR
EPP:DEPSPACE
Cosine 32.8 30.3 31.2 98.5
Dice 49.4 48.2 47.5 97.1
nGCM 27.6 27.5 25.7 98.5
Hindle 53.7 52.3 52.8 96.6
Jaccard 49.5 48.2 47.6 97.1
Lin 35.5 34.3 33.2 98.8
EPP:DEPSPACE, PCA
Cosine 30.2 28.7 28.8 98.1
Dice 29.9 30.8 28.6 98.2
nGCM 26.4 26.4 25.6 98.1
Hindle 45.0 44.4 44.2 95.7
Jaccard 29.7 30.7 28.5 98.2
Lin 28.7 29.1 26.7 97.7
EPP:WORDSPACE
Cosine 35.3 35.8 34.0 97.4
Dice 51.0 50.7 50.3 96.0
nGCM 33.2 34.7 31.8 97.4
Hindle 52.7 52.8 52.4 96.0
Jaccard 51.8 52.0 51.3 96.0
Lin 32.0 31.8 31.4 98.2
EPP:WORDSPACE, PCA
Cosine 30.3 31.3 29.4 97.1
Dice 31.3 32.4 30.5 97.8
nGCM 30.0 30.9 29.0 97.1
Hindle 40.2 41.0 40.4 95.3
Jaccard 31.0 32.1 30.2 97.8
Lin 27.8 29.8 26.9 97.3
RESNIK 28.1 63.4
ROOTH ET AL. 58.1 61.5
PADO ET AL. ? ?
HW 60.0 100.0
TRIPLE 32.0 4.0
LM 37.0 86.0
are dissimilar from other seen headwords, which allows RESNIK and EPP to identify
them as confounders in spite of their higher co-occurrence frequency.
We now turn to a comparison of the EPP variants. The coverage of all EPP models is
very high (0.95 or higher), independent of space, similarity measure, and dimensionality
reduction. We generally observe that error rates are lower when word meaning is
represented in DEPSPACE, and when discrimination weighting is used. In DEPSPACE,
nGCM works best, yielding the overall best result with an error rate of 25.6?25.7%.
In WORDSPACE, the Lin measure shows the best error rates with an error rate of just
below 27%. These results hold both for the unreduced and the reduced spaces and are
highly significant (p ? 0.01). Hindle is clearly the worst measure at around random
performance.
740
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
The difference between UNI and DISCR is significant throughout; the difference
between FREQ and DISCR is less uniform. In DEPSPACE, the difference between the best
measure with and without PCA (nGCM in both cases) is not significant; in WORDSPACE,
the difference between the best measure with and without PCA (Lin in both cases) is
significant (p ? 0.01).
For both WORDSPACEs and DEPSPACEs without PCA, the similarity measures divide
into two distinct groups: Lin, nGCM, and Cosine on the one hand and Jaccard, Dice, and
Hindle on the other, with a significant difference in performance between the groups
(p ? 0.01). The use of dimensionality reduction through PCA improves performance
for all similarity measures, in WORDSPACE as well as DEPSPACE. The improvement is
especially marked for the Dice and Jaccard measures, which perform at the level of
a random baseline for unreduced spaces. We assume that these set intersection-based
measures benefit from the independent dimensions that PCA produces. For the simi-
larity measures with best performance, the improvement through PCA is less marked.
Thus, PCA-reduced spaces show more similar error rates across similarity measures.
After PCA, only nGCM and Lin still significantly (p ? 0.01) outperform the others
in DEPSPACE, and in WORDSPACE, Lin is the only measure that performs significantly
differently from the rest (p ? 0.01).
As arguments are sampled from six frequency bins, we can inspect the effect of
argument frequency on error rate. Figure 4 examines the performance of the EPP model
with different similarity measures and weighting schemes by argument frequency bins
(cf. the subsection Task and Data in Section 5). We find that the overall best weighting
scheme, DISCR, also works best for all except the highest argument frequency bin. In
the DEPSPACE setting (upper row), all similarity measures show a frequency bias in that
Figure 4
SYN PRIMARY setting: Error rate by argument frequency bin. Bins: 1 = 1?50; 2 = 50?100;
3 = 100?200; 4 = 200?500; 5 = 500?1,000; 6 > 1,000.
741
Computational Linguistics Volume 36, Number 4
Figure 5
SYN PRIMARY setting: Error rate by predicate frequency bin: DISCR weighting. Bins: 1 = 50?100;
2 = 100?200; 3 = 200?500; 4 = 500?1,000; 5 > 1,000.
error rate is lower for more frequent arguments, but this bias is much less pronounced
in Cosine and nGCM than in the other measures, with error rates varying between 45%
and 25% rather than 80% and 20%. (Dice and Hindle, not shown here, exhibit similar
behavior to Jaccard.) In PCA-transformed DEPSPACE (middle row), this frequency bias
largely disappears for all similarity measures. In WORDSPACE (bottom row), although
there is again a frequency bias in all similarity measures, Lin now joins Cosine and
nGCM in being much less biased than Jaccard, Dice, and Hindle. For WORDSPACE
with PCA-transformation, not shown here, the curves resemble those of DEPSPACE with
PCA-transformation.
Figure 5 examines the effect of (predicate, argument position) pair frequency
on error rate. Predicate?argument position pairs were sampled from five frequency
bins. The figure shows DISCR weighting only. In the spaces without dimensionality
reduction, there is a clear division between Cosine, nGCM, and Lin on the one hand,
and Jaccard, Dice, and Hindle on the other. In PCA spaces, all measures except for
Hindle are similar in their performance. In both DEPSPACE conditions, error rate
decreases towards the higher frequency predicate bins, although this is not so in
742
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
WORDSPACE. It seems that in the sparser DEPSPACE, models can still profit from the
additional seen headwords in the highest predicate frequency bins, whereas in the less
sparse but noisier WORDSPACE, the added noise is stronger than the added signal in
the highest predicate frequency bins. For the lowest predicate frequency bins, the best
results in WORDSPACE are better than those in DEPSPACE.
5.3 SEM PRIMARY Setting: Results
Table 5 shows the results for the SEM PRIMARY setting, where we predict head words for
pairs of a frame (predicate sense) and semantic role. In comparison to the SYN PRIMARY
setting (Table 4), error rates are lower across the board. The difference for the EPP models
is on average around 10%.
Table 5
SEM PRIMARY setting: Pseudo-disambiguation results for different weighting schemes.
Model Similarity Error rate (%) Coverage (%)
UNI FREQ DISCR
EPP:DEPSPACE
Cosine 19.8 16.9 19.0 97.1
Dice 42.3 32.4 39.5 96.3
nGCM 20.2 16.3 20.8 97.1
Hindle 48.3 46.8 47.8 93.4
Jaccard 41.5 31.5 38.5 96.3
Lin 31.1 20.2 29.0 97.7
EPP:DEPSPACE, PCA
Cosine 18.5 17.0 17.8 96.9
Dice 19.3 19.2 18.0 97.6
nGCM 16.9 15.7 16.4 96.9
Hindle 44.9 45.6 44.7 89.3
Jaccard 18.2 18.5 17.5 97.6
Lin 18.8 19.5 18.3 98.9
EPP:WORDSPACE
Cosine 24.1 20.4 23.4 93.1
Dice 23.7 24.5 22.5 89.6
nGCM 21.1 17.8 19.4 93.1
Hindle 31.8 33.1 31.8 83.1
Jaccard 24.8 26.5 24.2 89.6
Lin 22.3 18.4 21.9 92.8
EPP:WORDSPACE, PCA
Cosine 21.0 17.6 20.5 93.1
Dice 18.5 16.4 17.8 96.8
nGCM 19.7 16.4 19.3 93.1
Hindle 41.0 39.8 40.7 90.6
Jaccard 18.1 16.2 17.6 96.8
Lin 21.3 17.1 20.7 98.3
RESNIK 16.5 62.8
ROOTH ET AL. 24.9 100.0
PADO ET AL. 7.1 59.0
HW 65.0 100.0
TRIPLE 44.0 2.0
LM NA NA
743
Computational Linguistics Volume 36, Number 4
The error rate of the PADO ET AL. model, at 7%, is the best by a large margin. We
attribute this to the extensive generalization mechanisms that the model uses, which
draw on an array of lexical?semantic resources. However, with a coverage of 59%,
the model is still unable to make predictions for many of the test items. Error rates
for the RESNIK and the EPP models are comparable, at 16.5% for RESNIK and 15.7% for
the best EPP variant. The two models differ sharply in coverage, however: 62.8% for
RESNIK, consistent with the findings of Gildea and Jurafsky (2002), and between 90%
and 98% for EPP variants. The RESNIK model also profits from the presence of semantic
disambiguation in the SEM PRIMARY setting (in the SYN PRIMARY setting its error
rate was 28%), which underlines the substantial impact that properties of the training
data have on semantic hierarchy?based models of selectional preferences. ROOTH
ET AL. now has perfect coverage, affirming our assumption that the very bad results
of the ROOTH ET AL. model in the SYN PRIMARY setting were an artifact of the data
sampling necessary for that data set. Although its error rate of 24.9% is a substantial
improvement over all baselines, the EPP model achieves error rates that are up to
9 points lower at a comparable coverage. Among the baselines, HW shows that here, as
in the SYN PRIMARY setting, arguments have some tendency of having lower frequency
than the confounders. The TRIPLE baseline shows near-random performance, at very
low coverage, a result of the very small size of the corpus. Because there is no large
corpus with frame-semantic roles, nor is the annotation easily linearizable, we could
not compute a LM baseline in the SEM PRIMARY setting.
Among EPP models, the DEPSPACEs and WORDSPACEs perform comparably, with a
non-significant advantage for DEPSPACE among the best models. Overall error rates
show the same clear divide between the three high-performing similarity measures
(Cosine, nGCM, and Lin) and the three weaker ones (Dice, Jaccard, and Hindle). Di-
mensionality reduction again dramatically improves the weaker models, with Jaccard
yielding the best result for the PCA-reduced WORDSPACE.9 Whereas all best parame-
trizations in the SYN PRIMARY setting used DISCR weighting, it is now FREQ weighting
that yields the best results.
Figure 6 again analyzes the influence of argument frequency on performance by
showing the performance of different variants of the EPP model over six argument
frequency bins. The upper row shows DEPSPACE without dimensionality reduction.
Note that FREQ weighting now works especially well for the lowest argument frequency
bin, much better than DISCR and PLAIN. This is the opposite of what we saw for the
SYN PRIMARY setting in Figure 4. With DISCR and PLAIN weighting, Jaccard and Lin
again have noticeable problems with the lowest argument frequency bins?as in the SYN
PRIMARY setting?but not with FREQ weighting. With DEPSPACE and dimensionality
reduction (middle row), we get error rates of ? 26% for all settings and all frequency
bins. On the lowest frequency bin, we again see a large advantage of FREQ weighting
over the two other weighting schemes. The bottom row shows WORDSPACE without
dimensionality reduction. Note that there is much less variation in error rates across
frequency bins here than in unreduced DEPSPACE.
Figure 7 charts error rate by predicate frequency bin, showing FREQ weighting
only, as this showed the best results on this data set. The figure clearly illustrates the
divide between the top and the bottom three similarity measures in DEPSPACE, as well
as the disappearance of this divide for both PCA settings. In unreduced WORDSPACE,
9 The differences to other similarity metrics in the FREQ setting are insignificant, with the exception
of Hindle.
744
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
the divide is not as clearly visible. The figure also indicates a slight tendency for error
rates to rise for the lowest-frequency as well as the highest-frequency predicates, across
all spaces.
5.4 Discussion
The resource-based approaches that we tested, RESNIK and PADO ET AL., show superior
performance when they have coverage (which coincides with findings in other lexical
semantics tasks that supervised data, when available, always increases performance),
but showed low coverage, at most 63% (RESNIK, SYN PRIMARY setting). The EPP model
achieves near-perfect coverage at good error rates: In the SYN PRIMARY setting, the
RESNIK model achieved an error rate of 28%, and the best EPP variant was at 26%. In
the SEM PRIMARY setting, error rates were 7% for the PADO ET AL. model, 16.5% for
the RESNIK model, and 16% for the best EPP variant. Comparing the EPP and ROOTH
ET AL. models in the SEM PRIMARY setting, we find that the use of an additional gen-
eralization corpus in the EPP model seems to offset any advantages introduced by the
joint clustering of predicates and arguments.
The difference in model performance on the two primary corpora (SYN and SEM)
is striking. Even though the FrameNet corpus is smaller and a sparse data prob-
lem might be expected, models perform at considerably lower error rates in the SEM
PRIMARY setting than when the primary corpus is the larger BNC. This underscores
the point that selectional preferences belong to a predicate sense rather than a predicate
lemma, and that they describe the semantics of fillers of semantic roles rather than of
Figure 6
SEM PRIMARY setting: Error rate by argument frequency bin. Bins: 1 = 1?50; 2 = 50?100;
3 = 100?200; 4 = 200?500; 5 = 500?1,000; 6 > 1,000.
745
Computational Linguistics Volume 36, Number 4
Figure 7
SEM PRIMARY setting: Error rate by predicate frequency bin: FREQ weighting. Bins: 1 = 50?100;
2 = 100?200; 3 = 200?500; 4 = 500?1,000; 5 > 1,000.
syntactic dependents (recall that in this setting, we predict head words for pairs of a
predicate sense and semantic role). In the SEM PRIMARY setting, the data is cleaner, so
it is expected that seen headwords of an argument position will be more semantically
uniform. This has a strong influence on model performance. Another factor contributing
to the difference in performance between the two data sets may be that the primary
corpus in the SYN PRIMARY setting is parsed automatically, whereas manual annotation
is available in the FrameNet corpus. However, although this manual annotation iden-
tifies predicate senses, role headwords are still determined through automatic parsing.
The division of the training data into a primary and a secondary corpus allows us to
successfully use FrameNet as the basis for semantic space?based similarity estimates
despite the fact that this corpus alone would be too small to sustain the construction of a
robust space.
In terms of model parameters for EPP, the following patterns stand out. Cosine,
Lin, and nGCM show good performance across all spaces and parameter settings; Dice
and Jaccard work comparably only on spaces that use dimensionality reduction. The
Hindle measure is an underperformer in all conditions. With Lin, Jaccard, Dice, and
Hindle, error rates rise sharply for less frequent arguments in many spaces. Although
Cosine and nGCM also have some frequency bias, it is much less pronounced. nGCM
seems to work well with sparse data sets that are not too noisy, as evidenced by the
fact that it has the best performance among all EPP variants on both DEPSPACEs in
746
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Table 6
Verb?argument position?noun triples with plausibility judgments on a 7-point scale (McRae
et al, 1998).
Verb Argument position Noun Plausibility
shoot agent hunter 6.9
shoot patient hunter 2.8
shoot agent deer 1.0
shoot patient deer 6.4
the SYN PRIMARY setting, as well as in all SEM conditions except reduced WORDSPACE.
The Lin measure seems to work well with noisier data: It is the best EPP model when
using WORDSPACE in the SYN PRIMARY setting. Cosine, although never showing the
top performance, is among the best models in any setting. Although dimensionality
reduction only improves the overall error rates of the best models by a few points, it
has two important consequences: First, dimensionality reduction reduces dependence
of the results on the exact similarity measure chosen, as all measures except Hindle
show nearly indistinguishable error rates on reduced spaces (Figures 5 and 7). Second,
low-frequency arguments profit by a huge margin when PCA is used (Figures 4 and 6).
Among weighting schemes, DISCR weighting seems to be most useful when the data
is sparse but somewhat noisy (as is the case in the lower argument frequency bins in
the SYN PRIMARY setting). Frequency weighting seems to work best when the data is
either not sparse (as in the highest argument frequency bin in the SYN PRIMARY setting)
or very clean but sparse (as in the lowest argument frequency bin in the SEM PRIMARY
setting). A comparison of the two vector spaces, DEPSPACE and WORDSPACE, shows
no clear winner. When the collections of seen headwords are noisier, as they are in the
SYN PRIMARY setting, DEPSPACE, with its more aggressive filtering, yields the better
results. Sets of headwords collected by predicate sense, as in the SEM PRIMARY setting,
are sparser but cleaner, and WORDSPACE shows lower error rates.
6. Experiment 2: Human Plausibility Judgments
Experimental psycholinguistics affords a second perspective on selectional preferences:
The plausibility of verb?argument pairs has been shown to have an important effect
on human sentence processing (e.g., Trueswell, Tanenhaus, and Gransey 1994; Garnsey
et al 1997; McRae, Spivey-Knowlton, and Tanenhaus 1998). In these studies, plausibility
was operationalized as the thematic fit or selectional preference between a verb and its
argument in a specific argument position. Models of human sentence processing there-
fore need selectional preference models (Pad?, Crocker, and Keller 2009). Conversely,
psycholinguistic plausibility judgments can be used to evaluate computational models
of selectional preferences.
6.1 Experimental Materials
We present evaluations on two plausibility judgment data sets used in recent studies.
747
Computational Linguistics Volume 36, Number 4
The first data set consists of 100 data points10 from McRae, Spivey-Knowlton, and
Tanenhaus (1998). Our example in Table 6, which is taken from this data set, was
elicited by asking study participants to rate the plausibility of, for example, a hunter
shooting (AGENT) or being shot (PATIENT). The data point demonstrates the McRae set?s
balanced structure: 25 verbs are paired with two argument headwords in two argument
positions each, such that each argument is highly plausible in one argument position
but implausible in the other (hunters shoot, but are seldom shot, and vice versa for deer).
The resulting distribution of ratings is thus highly bimodal. Models can only reliably
predict the human ratings in this data set if they can capture the difference between
verb argument positions as well as between individual fillers. However, because the
verb?argument pairs were created by hand and with strict requirements, many of the
arguments are infrequent in standard corpora (e.g., wimp, bellboy, or knight). When
FrameNet is used to annotate senses for the verbs, no appropriate senses are available
for 28 of the 100 verb?argument pairs, reducing the test set to 72 data points.
The second, larger data set addresses this sparseness issue. Its triples are con-
structed on the basis of corpus co-occurrences (Pad? 2007). Eighteen verbs are combined
with their three most frequent subjects and objects found in the Penn Treebank and
FrameNet corpora, respectively, up to a total of 12 arguments. Each verb?argument pair
was rated both as an agent and as a patient (i.e., both in the observed and an unobserved
argument position), which leads to a total of 24 rated triples per verb. The data set
contains ratings for 414 triples. The resulting judgments show a more even distribution
of data. With FrameNet annotation for the verbs, appropriate senses are not attested for
six verb?argument pairs, reducing the test set to 408 data points.
6.2 Setup
We evaluate the same four models as in Experiment 1: EPP, the WordNet-based RESNIK
model, the distributional ROOTH ET AL. model, and the semantic role?based PADO
ET AL. model. We again compare a SYN PRIMARY setting, where the models make pre-
dictions for pairs of a verb and a grammatical function, with a SEM PRIMARY setting,
for which the two test data sets were annotated with verb sense and semantic roles in
the FrameNet paradigm (Pad? 2007) and where models make predictions for pairs of a
frame and a semantic role. As before, the PADO ET AL. model is only tested in the SEM
PRIMARY setting.11 For the EPP model, we focus on parsed, dimensionally unreduced
spaces and DISCR weighting, following earlier results (Pad?, Pad?, and Erk 2007). We
provide results for the best WORDSPACE models from Experiment 1 for comparison.
The primary corpora for training selectional preference models were prepared as in
Experiment 1 (cf. Section 5.1). The generalization corpus for EPP was again the BNC.
For the ROOTH ET AL. model in the SYN PRIMARY setting, we again used a frequency
cutoff. We found the RESNIK model to perform better when using just a subset of the
BNC (namely, all the triples for verbs present in the test set).
6.3 Evaluation Procedure
We evaluate our models by correlating the predicted plausibility values with the human
judgments, which range between 1 and 7. Because we do not assume a priori that there
10 The original data set has 60 data points more, which were used as the development set for the PADO
ET AL. model.
11 The PADO ET AL. model now uses automatically induced verb clusters instead of FrameNet frames.
748
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Table 7
Comparison of EPP DEPSPACE models on McRae data. Unreduced spaces, DISCR weighting.
***p < 0.001.
SEM SYN
Sim Coverage Spearman?s ? Coverage Spearman?s ?
Dice 100% 0.038 ns 98% 0.148 ns
Jaccard 100% 0.045 ns 98% 0.153 ns
Cosine 100% 0.162 ns 98% 0.197 ns
Hindle 100% 0.060 ns 98% 0.108 ns
Lin 100% 0.085 ns 98% 0.094 ns
nGCM 100% 0.154 ns 98% 0.325 ***
is a linear correlation between the two variables, we do not use Pearson?s product-
moment correlation, but instead Spearman?s ?, a non-parametric rank-order correlation
coefficient.12 Note that significance is harder to reach the smaller the number of data
points is.
In line with Experiment 1, we include a simple frequency baseline FREQ, which
predicts the plausibility of each item as its frequency in the BNC (SYN) and in FrameNet
(SEM), respectively. With regard to an upper bound, we assume that automatic models
of plausibility should not be expected to surpass the typical human agreement on the
plausibility judgment. This is roughly ? ? 0.7 for the Pado data set.
6.4 McRae Data Set: Results and Discussion
Table 7 focuses on EPP variants with unreduced DEPSPACE for the McRae data set. We
see that this data set is rather difficult to model. None of the models trained in the SEM
PRIMARY setting achieves a significant correlation.13 Apparently, the FrameNet corpus
is too small to acquire selectional preferences that generalize well to the infrequent
items that make up the McRae data set. In the SYN PRIMARY setting, the nGCM model?s
predictions reach significance.
Table 8 shows results on the McRae data set for all selectional preference models
that we are considering. For EPP, we only show nGCM as the best-performing similarity
measure from the pseudo-disambiguation task, and Cosine as a widely used vanilla
measure. The results for the SEM PRIMARY setting (left-hand side) mirror the results
for the SEM PRIMARY setting in Experiment 1: The deep PADO ET AL. model shows the
best correlation (it is the only model to predict human judgments significantly). It
overcomes the sparseness in the FrameNet corpus by using semantic verb classes that
are particularly geared towards grouping the existing verb occurrences in the way
that is most meaningful for this task. It covers about 80% of the test data. EPP has
full coverage, and although it does not make statistically significant predictions, it
shows substantially higher correlation coefficients than ROOTH ET AL. and RESNIK.
12 A second concern is the computation of significance values: The methods most widely used for the
Pearson coefficient (Student?s t-distribution, Fisher transformation) assume that the variables are
normally distributed, which is not the case in our data set. For Spearman?s ?, we use the algorithm by
Best and Roberts (1975), which does not make this assumption.
13 Significance here refers to significance of correlation with the human data, not significance of differences
between models.
749
Computational Linguistics Volume 36, Number 4
Table 8
Comparison across models on McRae data. **p < 0.01, ***p < 0.001.
SEM SYN
Model Coverage Spearman?s ? Coverage Spearman?s ?
EPP (DEPSPACE nGCM) 100% 0.154 ns 98% 0.325 ***
EPP (DEPSPACE Cosine) 100% 0.162 ns 98% 0.197 ns
RESNIK 100% ?0.041 ns 100% 0.123 ns
ROOTH ET AL. 67% 0.078 ns 48% 0.465 ***
PADO ET AL. 78% 0.415 ** ? ?
EPP (WORDSPACE Lin) 100% 0.138 ns 98% 0.062 ns
EPP (WORDSPACE nGCM) 100% 0.167 ns 98% 0.110 ns
FREQ 18% 0.087 ns 36% 0.103 ns
The DEPSPACE and WORDSPACE variants of EPP perform similarly here, and the simple
frequency baseline has very low coverage and correlation.
As the right-hand side of Table 8 shows, both ROOTH ET AL. and EPP achieve
better results in the SYN PRIMARY setting than in the SEM PRIMARY setting. The ROOTH
ET AL. model obtains a highly significant correlation. The combination of infrequent
headwords in the McRae data set and the large primary corpus brings out the benefits
that the ROOTH ET AL. model can derive from generalizing from verbs and nouns to
the latent classes via soft clustering. Unfortunately, its coverage is still quite low (48%),
and for this reason, the difference from the best EPP model is not significant.14 In the
SYN PRIMARY setting, the EPP DEPSPACE models clearly outperform the WORDSPACE
because of the DEPSPACE models? more aggressive filtering. Interestingly, RESNIK
still performs poorly in the SYN PRIMARY setting: WordNet does not make the right
generalizations to capture the selectional preferences at play in the McRae data, no
matter how much training data is available. This is underscored by an analysis of which
WordNet classes were most frequently determined as the strongest association with
the target verbs: The classes entity, person, and physical object are assigned in 60 out of
100 test cases for the McRae data (SYN PRIMARY setting), a data set where plausibility
is determined by factors much more fine-grained than animacy. (In the SEM PRIMARY
setting, the picture is similar with classes person, organism, and entity assigned in 48 out
of 72 test cases.) The frequency baseline again performs badly.
6.5 Pado Data Set: Results and Discussion
We now turn to the Pado data set. Again, we first focus on the performance of differ-
ent similarity measures in EPP using unreduced DEPSPACE (Table 9). Correlation with
human judgments is much better than for the McRae data set, and highly significant
for all SEM PRIMARY setting models and three of the SYN PRIMARY setting models. In
both settings, Cosine and Lin are the best measures (difference not significant), followed
by nGCM. Hindle comes out worst once more. The difference between the strong and
14 As in Experiment 1, we apply bootstrap resampling to determine the significance of differences between
models. This procedure also takes differences in coverage into account?specifically, a significant
difference becomes harder to achieve as the number of data points shared between the models shrinks.
750
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Table 9
Comparison of EPP DEPSPACE parametrizations on Pad? data. Unreduced spaces, DISCR
weighting. **p < 0.01; ***p < 0.001.
SEM SYN
Sim Coverage Spearman?s ? Coverage Spearman?s ?
Dice 100% 0.289 *** 100% 0.026 ns
Jaccard 100% 0.285 *** 100% 0.023 ns
Cosine 100% 0.508 *** 100% 0.403 ***
Hindle 100% 0.160 ** 100% ?0.004 ns
Lin 100% 0.498 *** 100% 0.229 ***
nGCM 100% 0.384 *** 100% 0.156 **
weak measures is more pronounced for the SYN PRIMARY setting, compared with the
SEM PRIMARY setting. Coverage is at or close to 100% throughout.
Table 10 shows results on the Pado data set for all selectional preference models that
we consider. In the SEM PRIMARY setting (where both the data and the primary corpus
have FrameNet annotation), EPP and the deep PADO ET AL. model predict the human
judgments similarly well (difference not significant). Because all verbs in this data set
are covered by FrameNet, the PADO ET AL. model also shows a nearly perfect cover-
age. EPP and PADO ET AL. do much better than ROOTH ET AL. (differences significant
at p ? 0.01). ROOTH ET AL. has the lowest coverage at 88%, but this is still higher than
its coverage of the McRae data. As with the McRae data, ROOTH ET AL. achieves better
correlation in the SYN PRIMARY setting than the SEM PRIMARY setting, indicating that
the frequency cutoff does not harm performance as much in Experiment 2 as it did in
Experiment 1. However, the coverage of ROOTH ET AL. is lower in the SYN PRIMARY
setting, perhaps because the SEM PRIMARY setting smoothes rare verbs by grouping
them in frames with other verbs. RESNIK also achieves better correlation in the SYN
PRIMARY setting, but recall that it was trained on a subset of the BNC only to reduce
noise in the training data?when trained on the whole BNC set, performance degrades
to ? = 0.060. The difference from the best EPP model remains numerically large. As for
Table 10
Comparison across models on Pad? data. ***p < 0.001.
SEM SYN
Model Coverage Spearman?s ? Coverage Spearman?s ?
EPP (DEPSPACE Cosine) 100% 0.489 *** 98% 0.470 ***
EPP (DEPSPACE nGCM) 100% 0.393 *** 98% 0.328 ***
RESNIK 98% 0.230 *** 97% 0.317 ***
ROOTH ET AL. 88% 0.060 ns 58% 0.200 ***
PADO ET AL. 97% 0.515 *** ? ?
EPP (WORDSPACE Lin) 100% 0.254 *** 100% 0.056 ns
EPP (WORDSPACE nGCM) 100% 0.192 *** 100% 0.078 ns
FREQ 32% ?0.041 ns 69% 0.090 ns
751
Computational Linguistics Volume 36, Number 4
the McRae data set, the EPP WORDSPACE models show much worse performance than
the DEPSPACE models, and do not significantly predict the human plausibility ratings.
The frequency baseline shows a considerably better coverage for this data set, but
its correlations hover around zero, which underlines our intuition that verb?argument
combinations can be plausible without being frequent in corpora. An example is the
combination (to) embarrass (an) official, which is rated as highly plausible, but occurs
only once each in the BNC and FrameNet.
6.6 Discussion
The McRae data set seems in general more difficult to account for than the Pado data
set, as noted by Pad?, Pad?, and Erk (2007). They explain it by a general frequency effect
in the BNC data (which are a superset of the FrameNet data): The median frequency of
the hand-selected McRae nouns in the BNC is 1,356, as opposed to 8,184 for the corpus-
derived Pado nouns.
Comparing all selectional preference models, we find that the RESNIK and the
ROOTH ET AL. models generally do worse than EPP both in terms of coverage and
quality of predictions. One notable exception is the excellent performance of the ROOTH
ET AL. model on the McRae data in the SYN PRIMARY setting, which comes, however,
with a low coverage of less than 50%. A closer inspection of the predictions showed
that ROOTH ET AL. makes many predictions for verb?object pairs but abstains from
subjects, thus reducing the complexity of the task. For only 20% of verbs, predictions
are made for subjects and objects. As noted in Pad?, Pad?, and Erk (2007), the relatively
poor performance of the RESNIK model may be explained by the fact that its ability to
generalize is limited to the structure of WordNet, where some semantic distinctions are
easier to make than others. For example, a fairly easy distinction to make for WordNet-
based models is animate vs. inanimate. Because the Pado set contains a portion of
inanimate arguments with animate counterparts, the RESNIK model does well on those.
In contrast, in the McRae test set, all arguments are animates, and thus similar to one
another in terms of WordNet.
The deep PADO ET AL. model achieves the best correlation with the human judg-
ments on both data sets, but it is limited to the SEM PRIMARY setting. Although the
best model is not always among the EPP DEPSPACE models, they consistently show a
coverage of close to 100%, and are generally statistically indistinguishable from the best
model. Unlike ROOTH ET AL. and RESNIK, whose performance varies widely between
the SEM PRIMARY setting and the SYN PRIMARY setting, the correlation coefficients for
the EPP models are generally similar across settings. We take this as evidence that EPP
models can extract relevant information from deeper annotation on small corpora as
well as from large, but noisy and shallow, training data.
Finally, we consider the different similarity measures for the EPP model evaluated
on unreduced DEPSPACE. The picture differs somewhat between the two data sets, but
the Cosine measure performs well overall, with Lin and nGCM generally in second
and third place. So, the group of the three best similarity measures is the same as in
Experiment 1, but Cosine shows better performance. One possible reason for this lies
in the verb frequency, which is relatively high in both data sets: 68% of the McRae
verbs and 83% of the Pado verbs have BNC frequencies of 1,000 and more, whereas
Experiment 1 used an equal number of predicates from five frequency bins, the highest
being 1,000 and more occurrences. In that highest predicate frequency bin, Cosine
consistently performed as well as Lin or better in Experiment 1 (Figures 5 and 7).
752
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
7. Experiment 3: Inverse Selectional Preferences
The term selectional preference is typically used to describe the semantic constraints
that predicates place on their arguments. In this section, we will investigate how nominal
arguments place semantic constraints or expectations on the predicates with which they
occur. Such expectations can be thought of as typical events that involve the given
object. For example, a noun like apple could be said to have preferences about its inverse
subject position, that is, the verbs that can take it as a plausible subject. Examples might
be verbs like grow or fall; for its inverse object position, apple probably prefers verbs
like eat, cut, or plant. We will use the term inverse selectional preference to refer to
preferences of nouns for their predicates, distinguishing them from regular selectional
preferences.
It is clear that not all verbs will be equally likely to occur with a given noun?
role pair. Still, inverse selectional preferences warrant a closer look: To what extent do
inverse selectional preferences differ from regular ones? And are the tasks of predicting
regular and inverse selectional preferences equally difficult? We start in Section 7.2 with
an exploratory data analysis of inverse selectional preferences, which shows that inverse
selectional preferences show semantically coherent patterns like regular selectional
preferences, but that, in contrast to most verbs, nouns tend to occur with multiple
semantic groups of verbs. In Sections 7.3?7.5, we test the EPP model on a pseudo-
disambiguation task for inverse selectional preferences.
7.1 Related Work
In computational linguistics, some approaches to characterizing selectional preferences
have used the symmetric nature of their models to characterize nouns in terms of the
verbs that they use (Hindle 1990; Rooth et al 1999). However, they do not explicitly
compare the two types of preferences. Also, there are approaches using selectional
preference information, in particular for word sense disambiguation and related tasks,
that could be characterized as using regular along with inverse selectional preferences
(Dligach and Palmer 2008; Erk and Pad? 2008; Nastase 2008). By comparing selectional
preference model performance on the tasks of predicting inverse and regular selectional
preferences in Sections 7.3?7.5, we hope to contribute to an understanding of what can
be achieved by using inverse preferences in word sense analysis tasks.
At the same time, inverse selectional preferences have been the object of fruitful
research in both psycholinguistics and theoretical linguistics. In psycholinguistics, a
particularly plausible argument for the existence of expectations of nouns for their
predicates in human language processing is head-final word order (as in Japanese or
in German subordinate clauses), where hearers may encounter all objects before the
head. It is likely that these objects are immediately integrated into a preliminary event
structure with an assumed predicate instead of being stored in short-term memory until
the predicate is encountered (Konieczny and D?ring 2003; Nakatani and Gibson 2009).
Another strand of work is McRae et al (2001, 2005), who have studied priming of verbs
from nouns. They found that a noun engenders priming of verbs for which it is a typical
agent, patient, instrument, or location.
In theoretical linguistics, the idea of event knowledge being encoded in the lex-
ical entries of nouns has been formulated in the context of Pustejovsky?s generative
lexicon (Pustejovsky 1995), where the qualia roles TELIC and AGENTIVE provide infor-
mation about the typical use of an object (book: read) and its construction (book: write),
753
Computational Linguistics Volume 36, Number 4
respectively. Pustejovsky uses this knowledge to account, for example, for the interpre-
tation of logical metonymy (begin a book). Although qualia roles are instantiated with
individual predicates rather than characterizations of all possible events, construction
and use are arguably two very salient events for an object. Through the data exploration
in Section 7.2, we hope to contribute to a linguistic characterization of inverse selectional
preferences.
7.2 Empirical Analysis of Inverse Selectional Preferences
The first question we ask concerns the selectional preference strength of regular and
inverse selectional preferences, using the measure introduced by Resnik (1996) to de-
termine the degree to which verbs select for nouns, and vice versa. As verb?role pairs,
we re-use the same 100 pairs that were used for the pseudo-disambiguation task in
Experiment 1. For the comparison, we randomly sample a total of 100 noun/inverse-
role pairs from the BNC, using the same five frequency bands as for the verbs (50?
100, 100?200, 200?500, 500?1,000, >1,000). The sample contains approximately the same
number of (inverse) subject and object roles.
We adapt the selectional preference strength measure from Equation (1) to our
case: Unlike Resnik, we compute KL divergence not on a distribution across WordNet
synsets, but on a distribution across lemmas.
SelStr(w1, r) = D(P(w2|w1, r)||P(w2|r)) (7)
For regular selectional preferences, w1 is a verb lemma, w2 a noun lemma, and r a role.
For inverse preferences, w1 is a noun lemma, w2 a verb lemma, and r an inverse role.
SelStr(w1, r) can be interpreted as a measure of the degree to which w1 has selectional
preferences concerning the role r. We induce the probability distributions through
maximum likelihood estimation on the BNC.
We can expect to see the same overall tendency in regular and inverse selec-
tional preference strength. It is not possible that inverse selectional preference strength
would be uniform throughout if regular selectional preference strength varied between
verbs. After all, if we fix the relation r for the time being, P(v|n) and P(n|v) are re-
lated through Bayes? formula. Instead, the questions we will ask are more specific.
Are regular and inverse preference strengths similar in size? Are regular and inverse
preference strengths similar by frequency band?that is, do frequent nouns behave
similarly to frequent verbs? And what effects do we see of the prior distributions P(n|r)
and P(v|r)?
Table 11 shows the range of selectional preference strengths found in each frequency
band for verbs and nouns. As expected, we see substantial strengths in both regular
and inverse preferences. Both parts of speech show the same pattern of decreasing KL
divergences for higher-frequency words, presumably because frequent words tend to be
polysemous, and can combine with many different words. However, the strengths for
inverse selectional preferences are in general lower than those for regular preferences.
One possible reason for this is that the number of nouns seen with a verb?role
pair might differ, in general, from the number of verbs seen with each noun?role
pair. However, we find that verbs and nouns occur with roughly the same number of
associates in the frequency bands up to the 200?500 band. In the band 500?1,000, verbs
appear with roughly one third more nouns than nouns appear with verbs, and in the
band of 1,000 occurrences or more, verbs appear with twice as many nouns (on average)
754
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Table 11
Minimal, median, and maximal selectional preference strength (measured in terms of KL
divergence) in a sample of 100 verbs and 100 nouns (20 lemmas each per frequency band).
Band Verbs Nouns
min median max min median max
50?100 4.5 7.4 8.8 3.7 4.8 6.3
100?200 3.9 5.8 7.6 2.7 3.8 5.0
200?500 3.3 5.2 6.9 2.4 3.3 4.7
500?1,000 2.4 4.4 5.9 1.9 2.9 4.1
1,000? 1.8 3.6 6.2 1.4 2.3 3.7
as nouns appear with verbs in this band (1,189 vs. 636). Incidentally, the fact that the
highest-frequency verbs (which also tend to be the most ambiguous) appear in a much
larger number of contexts than the highest-frequency nouns could be a contributing
factor to the well-known problem that verbs are harder to disambiguate than nouns.
For the lower frequency bands, number of associates is unlikely to be the reason for
the weaker inverse preferences. Instead, a more likely reason for the overall weaker
inverse preferences lies in the overall distributions of nouns and verbs in the BNC.
Both show a Zipfian distribution, but there are 15,570 verbs as opposed to 455,173
nouns. Recall that KL divergence will be high whenever the individual terms p(w2|w1,r)p(w2|r)
to be summed are large. This, in turn, is the case when p(w2|r) is small. And p(w2|r) may
be small when the distribution p(w2|r) ranges over a larger number of words w2. For
regular selectional preferences, the w2 are nouns, and for inverse preferences the w2 are
verbs. Because there are many more nouns than verbs, the denominator p(w2|r) tends to
be smaller for regular preferences.
To get a clearer understanding of how inverse selectional preferences compare to
regular selectional preferences, we next do a qualitative analysis, looking at association
strength SelAssoc for individual triples verb?role?noun and noun?inverse-role?verb.
We adapt Equation (2) to the lexicon-free case and obtain
SelAssoc(w1, r, w2) =
1
SelStr(w1, r)
P(w2|w1, r) log
P(w2|w1, r)
P(w2|r)
(8)
Table 12 shows the five strongest associates for one verb?role pair and one noun?role
pair from each frequency band. The associates on both sides of the table generally
are semantically coherent and make intuitive sense. However, there is an interesting
difference between the verbs and nouns: We find that the nouns? preferred verbs can
often be grouped loosely into several meaning clusters, whereas the verbs? associates
tend to group into one cluster per grammatical function. For example, predicates taking
wheat as objects fall into those describing production (grow, sow) and those describing
processing (shred, grind, mill). Similarly, the predicates found for pill either concern
ingestion (take, swallow, pop), prescription, or idiomatic usage. In contrast, the objects of
rebut describe different kinds of statements, and the objects of celebrate are anniversaries
and other special events. Another observation that we can make in Table 12 is that
the nouns? most preferred associates have a similarly large share in the nouns? overall
selectional preference strength as the verbs? most preferred associates have in the verbs?
selectional preference strength. This indicates that the distribution of selectional prefer-
ences is similarly skewed towards the most preferred associate for verbs and nouns.
755
Computational Linguistics Volume 36, Number 4
Table 12
Examples of regular and inverse selectional preferences from different frequency bands for
argument positions of nouns and verbs: overall selectional preference strength SelStr and most
highly associated fillers with association strengths SelAssoc.
Band Verbs Nouns
50?100
rebut?obj, SelStr(w)= 7.43 wreckage?obj?1, SelStr(w)= 5.91
presumption 0.283 survey 0.126
allegation 0.088 examine 0.089
charge 0.082 sift 0.075
criticism 0.049 clear 0.056
claim 0.041 sight 0.051
100?200
enunciate?obj, SelStr(w)= 6.89 wheat?obj?1, SelStr(w)= 5.00
principle 0.242 grow 0.184
word 0.085 shred 0.049
theory 0.034 grind 0.049
philosophy 0.034 mill 0.042
policy 0.029 sow 0.040
200?500
break_with?obj, SelStr(w)= 6.92 pill?obj?1, SelStr(w)= 4.15
tradition 0.237 take 0.290
past 0.054 swallow 0.165
precedent 0.035 sweeten 0.070
convention 0.035 prescribe 0.049
Rome 0.022 pop 0.028
500?1,000
commence?obj, SelStr(w)= 5.92 dividend?obj?1, SelStr(w)= 4.10
proceedings 0.185 pay 0.508
action 0.051 declare 0.064
work 0.043 receive 0.064
proceeding 0.041 recommend 0.054
operation 0.033 raise 0.023
1,000?
celebrate?obj, SelStr(w)= 6.23 requirement?obj?1, SelStr(w)= 3.24
anniversary 0.177 meet 0.332
birthday 0.170 satisfy 0.015
centenary 0.046 comply_with 0.093
victory 0.033 fulfill 0.061
mass 0.028 impose 0.028
In sum, we find that inverse selectional preferences have weaker overall selectional
preference strength than regular preferences, but that may be due more to specifics of
the formula used rather than the skewness towards preferred role fillers. Two differ-
ences do emerge, though. First, noun selectional preferences show more semantic filler
sets than verb preferences. Second, the highest frequency verbs appear with many more
different associates than the highest frequency nouns.
7.3 Modeling Inverse Selectional Preferences
In the rest of this section, we test selectional preference models on the task of pre-
dicting inverse selectional preferences in a pseudo-disambiguation task, and compare
the results to the performance on predicting regular preferences in Experiment 1. We
do not repeat Experiment 2 even though it would have been technically possible to
756
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
re-use the McRae and Pado data sets and predict plausibility judgments through inverse
preferences. However, the data sets combine each verb with both plausible and im-
plausible nouns, but they do not combine each noun with different verbs in a balanced
fashion, so a repetition of Experiment 2 with inverse preferences would not be very
informative.
For the pseudo-disambiguation experiment, we focus on the EPP model. Distri-
butional models can, in general, be used straightforwardly to model both regular
and inverse selectional preferences. This is different for models like RESNIK that use
the WordNet noun hierarchy to represent regular selectional preferences. To model
inverse preferences, it would be necessary to use the WordNet verb hierarchy. However,
WordNet organizes verbs in a comparatively flat, unconnected hierarchy with a high
branching factor formed by the hypernymy/troponymy (?type of?) relation. This makes
effective generalization difficult, in particular in conjunction with the marked variation
in the set of preferred predicates that we observed for inverse selectional preferences in
Section 7.2.
We adapt the formulation of the EPP model to the inverse selectional preference
case as follows. Let a stand for a noun, r for an inverse argument position of this
noun, and Seenpreds(r, a) for the set of predicates seen with noun a and role r. Then the
selectional preference SelprefEPP of (r, a) for a verb v0 is defined in parallel to Equation (6)
as weighted average similarity to seen verbs:
SelprefEPPr,a(v0) =
?
v?Seenpreds(r,a)
wtr,a(v)
Zr,a
sim(v, v0) (9)
with Zr,a =
?
v?Seenpreds(r,a) wtr,a(v) as the normalization constant.
7.4 Pseudo-Disambiguation: Experimental Setup
We evaluate inverse selectional preferences on a pseudo-disambiguation task that is
set up completely analogously to our experiments on regular preferences in Section 5:
given a noun, an inverse argument position, one verb observed in this position, and a
confounder verb, distinguish between the two verbs. We use the 100 nouns sampled
across five frequency bands that we already used in Section 7.2. We experiment with
both WORDSPACE and DEPSPACE models, but restrict our attention to DISCR weighting,
which showed good results in Experiment 1.
In Section 5, we experimented on two different primary corpora, the BNC (SYN
PRIMARY setting) and FrameNet (SEM PRIMARY setting). Subsequently, we will use the
SYN PRIMARY setting again, but not the SEM PRIMARY setting. In the SEM PRIMARY
setting, the roles are FrameNet frame elements (semantic roles). However, frame ele-
ments are specific to a single frame, for example, the frame element ROPE belongs to
the frame ROPE_MANIPULATION.15 It would thus be pointless to predict a verb frame
given a noun and a frame element name, as the frame element already gives away the
frame.
15 It is possible for multiple frame elements to share a name, for example there are multiple frames with a
frame element named THEME. However, conceptually, this is only a shared name, not a shared role across
frames.
757
Computational Linguistics Volume 36, Number 4
7.5 Pseudo-Disambiguation: Results and Discussion
Table 13 shows the results of testing the EPP model for inverse selectional preferences
on pseudo-disambiguation. Coverage is very good for all model variants, similarly to
Experiment 1. The error rates, as well, are close to those for the regular preferences in
the SYN PRIMARY setting (cf. Table 4). The best model there (DEPSPACE, PCA, nGCM
with DISCR weighting) achieved an error rate of 25.6%, and the best model for inverse
preferences (WORDSPACE, Lin with DISCR weighting) reaches an error rate of 27.2%
here. Lin shows the best error rates in all conditions, closely followed by nGCM (the
difference is significant in WORDSPACE and the reduced DEPSPACE, but not significant
in the unreduced DEPSPACE). The Hindle similarity measure again brings up the rear.
In PCA-transformed spaces, the error rates are similar across all similarity measures
except for Hindle, as in Experiment 1.
WORDSPACEs yield better results than DEPSPACEs here, in contrast to Experiment 1.
The best WORDSPACE model (Lin without PCA) reaches significantly better error rates
(p ? 0.01) than the best DEPSPACE model (Lin with PCA). We think that the reason for
this lies in the fact that for inverse selectional preferences, the true associate and the
confounder that need to be distinguished in the pseudo-disambiguation task are verbs
rather than nouns. A noun will probably have more other nouns in a bag-of-words
context window than a verb would other verbs, which will make it easier to distinguish
verbs in a WORDSPACE than to distinguish nouns. A DEPSPACE, in contrast, will bring
out differences in the immediate syntactic neighborhood of nouns even if they occur in
the same sentence.
8. Conclusion
In this article, we have presented a similarity-based model of selectional preferences,
EPP. It computes the selectional fit of a candidate role filler as a weighted sum of seman-
tic similarities to headwords observed in a corpus, in a straightforward implementation
Table 13
Pseudo-disambiguation results for inverse selectional preferences (BNC as primary and
secondary corpus, DISCR weighting). ER = Error rate; Cov = Coverage.
Dimensions Similarity DEPSPACE WORDSPACE
ER (%) Cov (%) ER (%) Cov (%)
Original
2,000 dimensions
Cosine 37.4 99.0 34.0 99.1
Dice 42.4 98.8 43.4 98.7
nGCM 33.7 99.3 31.5 99.3
Hindle 48.8 96.0 52.2 94.6
Jaccard 36.7 99.4 44.9 98.7
Lin 32.8 98.9 27.2 98.9
PCA
500 dimensions
Cosine 35.2 99.0 31.3 99.4
Dice 35.0 99.6 32.9 99.8
nGCM 32.4 99.2 30.3 99.6
Hindle 44.2 99.0 48.7 99.1
Jaccard 34.8 99.6 32.6 99.8
Lin 30.6 99.8 28.8 99.8
758
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
of the intuition that plausibility judgments should generalize to fillers with similar
meaning. Our model is simple and easy to compute. In common with other distri-
butional models like Rooth et al (1999), it does not depend on lexical resources. Our
model derives additional flexibility from distinguishing between a primary corpus (for
observing headwords) and a generalization corpus (for inducing semantic similarities).
This allows it to use primary corpora with deeper semantic annotation that are too small
as a basis for computing vector space representations.
We have evaluated the EPP model on two tasks, a pseudo-disambiguation task
that can be viewed as an abstraction of both word sense disambiguation and semantic
role labeling, as well as on the prediction of human plausibility judgments. The model
achieves similar error rates to the semantic hierarchy?based RESNIK model, at consid-
erably higher coverage, and it achieves lower error rates than the ROOTH ET AL. soft
clustering model. The semantic role?based PADO ET AL. model, although highly accu-
rate in its predictions, has much lower coverage and needs a semantically annotated
corpus as a basis. We have also demonstrated that our model is able to meaningfully
model inverse selectional preferences, that is, expectations of nouns about verbs for
which they appear as arguments.
With respect to parameter settings of the EPP model, we find consistent patterns
across the three tasks we have considered. nGCM, Lin, and Cosine are the best-
performing similarity measures throughout. The good performance of the nGCM mea-
sure, an exponential similarity measure, is particularly noteworthy. We found it to work
well on data sets that are sparse and not too noisy, whereas the Lin similarity measure
achieved better performance when the data was noisy (see Section 5.4 for details). Di-
mensionality reduction (PCA) on the vector space raises the performance of the Jaccard
and Dice similarity measures to a similar level as the best three. More importantly, PCA
neutralizes a strong frequency bias that otherwise leads to a large performance drop
on rare arguments. Concerning weighting schemes, we found that frequency-based
weighting works well when the data is either clean or not too sparse. In the face of sparse
noisy data, DISCR weighting (a variant of tf/idf) is helpful. Comparing bag-of-words?
based and dependency-based vector spaces, DEPSPACEs are sparser but cleaner than
WORDSPACEs. Accordingly, DEPSPACEs are at an advantage when many headwords are
available, making efficient use of this information, whereas WORDSPACEs work better
for predicates with few seen headwords because they are less affected by sparseness.
We conclude with two open questions. The first question concerns the appropriate
representation of selectional preferences for polysemous verbs such as address, whose
direct object can either be a person, or a problem. Polysemy leads to headwords with
lower similarity among them than for non-polysemous verbs, which in turn can lead
to artificially low plausibilities for all fillers. In the SEM PRIMARY setting, occurrences
of polysemous verbs are separated into different frames. In future work, we hope to
improve our SYN PRIMARY setting models by clustering the seen headwords, and then
computing plausibility of new headwords relative to the nearest cluster.
A second question is the usefulness of inverse selectional preferences for the ac-
quisition of fine-grained information about nouns. As we discussed in Section 7, the
preferred verbs for a noun can often be grouped into meaning clusters. In future work,
we plan to investigate whether there are groups of predicates that recur across similar
nouns, and how they can be characterized. We expect some groups to correspond to
Pustejovsky?s qualia (Pustejovsky 1995), which constitute particularly salient events
for an object, namely, their creation and typical use. However, we expect corpus data
to yield a more complex picture of the events connected to a noun, which manifest
themselves in the form of additional, more specific meaning clusters.
759
Computational Linguistics Volume 36, Number 4
Acknowledgments
We would like to thank Detlef Prescher
and Carsten Brockmann for their
implementations of the ROOTH ET AL.
and RESNIK models, respectively. We
are also grateful to Nate Chambers and
Yves Peirsman, as well as to the
anonymous reviewers, for their
comments and suggestions.
References
Abe, Naoki and Hang Li. 1996. Learning
word association norms using tree cut pair
models. In Proceedings of the 10th
International Conference on Machine
Learning, pages 3?11, Bari.
Baroni, Marco, Silvia Bernardini, Adriano
Ferraresi, and Eros Zanchetta. 2009. The
wacky wide Web: A collection of very
large linguistically processed Web-crawled
corpora. Language Resources and Evaluation,
43(3):209?222.
Bergsma, Shane, Dekang Lin, and Randy
Goebel. 2008. Discriminative learning of
selectional preference from unlabeled text.
In Proceedings of the 13th Conference on
Empirical Methods in Natural Language
Processing, pages 59?68, Honolulu, HI.
Best, John and D. E. Roberts. 1975. Algorithm
AS 89: The upper tail probabilities of
Spearman?s Rho. Applied Statistics,
24:377?379.
Briscoe, Ted and Bran Boguraev, editors.
1989. Computational Lexicography for
Natural Language Processing. Longman
Publishing Group, New York.
Brockmann, Carsten and Mirella Lapata.
2003. Evaluating and combining
approaches to selectional preference
acquisition. In Proceedings of the 16th
Meeting of the European Chapter of the
Association for Computational Linguistics,
pages 27?34, Budapest.
Burnard, Lou, 1995. User?s Guide for the
British National Corpus. British National
Corpus Consortium, Oxford University
Computing Services.
Ciaramita, Massimiliano and Mark Johnson.
2000. Explaining away ambiguity:
Learning verb selectional preference
with Bayesian networks. In Proceedings
of the 18th International Conference on
Computational Linguistics, pages 187?193,
Saarbr?cken.
Clark, Stephen and David Weir. 2001.
Class-based probability estimation using a
semantic hierarchy. In Proceedings of the
2nd Annual Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 95?102, Pittsburgh, PA.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics,
pages 16?23, Madrid.
Curran, James. 2004. From Distributional to
Semantic Similarity. Ph.D. thesis, University
of Edinburgh.
Daelemans, Walter and Antal van den
Bosch. 2005. Memory-Based Language
Processing. Cambridge University Press,
Cambridge, UK.
Dagan, Ido, Lillian Lee, and Fernando C. N.
Pereira. 1999. Similarity-based models
of word cooccurrence probabilities.
Machine Learning, 34(1):34?69.
Dligach, Dmitriy and Martha Palmer. 2008.
Novel semantic features for verb sense
disambiguation. In Proceedings of the
46th Annual Meeting of the Association for
Computational Linguistics:Human Language
Technologies, Short Papers, pages 29?32,
Columbus, OH.
Dunning, Ted. 1993. Accurate methods for
the statistics of surprise and coincidence.
Computational Linguistics, 19(1):61?74.
Efron, Bradley and Robert Tibshirani. 1994.
An Introduction to the Bootstrap.
Monographs on Statistics and Applied
Probability 57. Chapman & Hall, London.
Erk, Katrin. 2007. A simple, similarity-based
model for selectional preferences. In
Proceedings of the Annual Meeting of the
Association for Computational Linguistics,
pages 216?223, Prague.
Erk, Katrin and Sebastian Pad?. 2008.
A structured vector space model for
word meaning in context. In Proceedings
of the 13th Conference on Empirical
Methods in Natural Language Processing,
pages 897?906, Honolulu, HI.
Fillmore, Charles J., Christopher R. Johnson,
and Miriam R. L. Petruck. 2003.
Background to FrameNet. International
Journal of Lexicography, 16:235?250.
Firth, John Rupert. 1957. A synopsis of
linguistic theory, 1930?1955. In Philological
Society, editor, Studies in Linguistic
Analysis. Blackwell, Oxford, pages 1?32.
Garnsey, Susan, Neal Pearlmutter, Elizabeth
Myers, and Melanie Lotocky. 1997. The
contributions of verb bias and plausibility
to the comprehension of temporarily
ambiguous sentences. Journal of Memory
and Language, 37:58?93.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
760
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Grefenstette, Gregory. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer
Academic Publishers, Dordrecht.
Grishman, Ralph and John Sterling. 1992.
Acquisition of selectional patterns. In
Proceedings of the 14th International
Conference on Computational Linguistics,
pages 658?664, Nantes.
Harris, Zellig. 1968. Mathematical Structure of
Language. Wiley, New York.
Hay, Jennifer, Aaron Nolan, and Katie
Drager. 2006. From fush to feesh: Exemplar
priming in speech perception. The
Linguistic Review, 23(3):351?379.
Hindle, Donald. 1990. Noun classification
from predicate-argument structures. In
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics,
pages 268?275, Pittsburgh, PA.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical relations.
Computational Linguistics, 19(1):103?120.
Katz, Jerrold J. and Jerry A. Fodor. 1963. The
structure of a semantic theory. Language,
39(2):170?210.
Katz, Jerrold J. and Paul M. Postal. 1964. An
Integrated Theory of Linguistic Descriptions.
Research Monograph No. 26. MIT Press,
Cambridge, MA.
Konieczny, Lars and Philipp D?ring. 2003.
Anticipation of clause-final heads.
Evidence from eye-tracking and SRNs.
In Proceedings of the 4th International
Conference on Cognitive Science,
pages 330?335, Sydney.
Lakoff, George and Mark Johnson. 1980.
Metaphors We Live By. University of
Chicago Press, Chicago, IL.
Landauer, Thomas and Susan Dumais. 1997.
A solution to Plato?s problem: The latent
semantic analysis theory of acquisition,
induction, and representation of
knowledge. Psychological Review,
104(2):211?240.
Lee, Lillian. 1999. Measures of distributional
similarity. In Proceedings of the 31st Annual
Meeting of the Association for Computational
Linguistics, pages 25?32, College Park, MA.
Lin, Dekang. 1993. Principle-based parsing
without overgeneration. In Proceedings of
the 31st Annual Meeting of the Association for
Computational Linguistics, pages 112?120,
Columbus, OH.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the Joint Annual Meeting of the Association
for Computational Linguistics and
International Conference on Computational
Linguistics, pages 768?774, Montreal.
Lowe, Will. 2001. Towards a theory of
semantic space. In Proceedings of the 23rd
Annual Conference of the Cognitive Science
Society, pages 576?581, Edinburgh.
Lowe, Will and Scott McDonald. 2000. The
direct route: Mediated priming in semantic
space. In Proceedings of the 22nd Annual
Conference of the Cognitive Science Society,
pages 675?680, Philadelphia, PA.
Lund, Kevin and Curt Burgess. 1996.
Producing high-dimensional semantic
spaces from lexical co-occurrence. Behavior
Research Methods, Instruments, and
Computers, 28:203?208.
McCarthy, Diana. 2000. Using semantic
preferences to identify verbal participation
in role switching alternations. In
Proceedings of the 1st Annual Meeting of the
North American Chapter of the Association for
Computational Linguistics, pages 256?263,
Seattle, WA.
McCarthy, Diana and John Carroll. 2003.
Disambiguating nouns, verbs, and
adjectives using automatically acquired
selectional preferences. Computational
Linguistics, 29(4):639?654.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004. Finding
predominant word senses in untagged
text. In Proceedings of the 42th Annual
Meeting of the Association for Computational
Linguistics, pages 279?286, Barcelona.
McCarthy, Diana, Sriram Venkatapathy, and
Aravind K. Joshi. 2007. Detecting
compositionality of verb-object
combinations using selectional
preferences. In Proceedings of the 12th Joint
Conference on Empirical Methods in Natural
Language Processing and Conference on
Natural Language Learning, pages 369?379,
Prague.
McDonald, Scott and Chris Brew. 2004. A
distributional model of semantic context
effects in lexical processing. In Proceedings
of the 42th Annual Meeting of the Association
for Computational Linguistics, pages 17?24,
Barcelona.
McRae, Ken, Todd Ferretti, and Liane
Amyote. 1997. Thematic roles as
verb-specific concepts. Language and
Cognitive Processes, 12(2/3):137?176.
McRae, Ken, Mary Hare, Jeffrey Elman, and
Todd Ferretti. 2005. A basis for generating
expectancies for verbs from nouns.
Memory and Cognition, 33(7):1174?1184.
McRae, Ken, Mary Hare, Todd Ferretti, and
Jeffrey Elman. 2001. Activating verbs
typical agents, patients, instruments, and
locations via event schemas. In Proceedings
761
Computational Linguistics Volume 36, Number 4
of the Twenty-Third Annual Conference of the
Cognitive Science Society, pages 617?622,
Mahwah, NJ.
McRae, Ken, Michael Spivey-Knowlton, and
Michael Tanenhaus. 1998. Modeling the
influence of thematic fit (and other
constraints) in on-line sentence
comprehension. Journal of Memory and
Language, 38:283?312.
Miller, George, Richard Beckwith, Christiane
Fellbaum, Derek Gross, and Katherine
Miller. 1990. Five papers on WordNet.
International Journal of Lexicography,
3(4):235?312.
Nakatani, Kentaro and Edward Gibson. 2009.
An on-line study of Japanese nesting
complexity. Cognitive Science, 1(34):94?112.
Nastase, Vivi. 2008. Unsupervised all-words
word sense disambiguation with
grammatical dependencies. In Proceedings
of the 3rd International Joint Conference
on Natural Language Processing,
pages 757?762, Honolulu, HI.
Nosofsky, Robert. 1986. Attention, similarity,
and the identification-categorization
relationship. Journal of Experimental
Psychology: General, 115(1):39?57.
Pad?, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of
semantic space models. Computational
Linguistics, 33(2):161?199.
Pad?, Sebastian, Ulrike Pad?, and Katrin Erk.
2007. Flexible, corpus-based modelling of
human plausibility judgements. In
Proceedings of the 12th Joint Conference on
Empirical Methods in Natural Language
Processing and Conference on Natural
Language Learning, pages 400?409, Prague.
Pad?, Ulrike. 2007. The Integration of Syntax
and Semantic Plausibility in a Wide-Coverage
Model of Human Sentence Processing. Ph.D.
thesis, Saarland University, Saarbr?cken,
Germany.
Pad?, Ulrike, Matthew W. Crocker, and
Frank Keller. 2009. A probabilistic model
of semantic plausibility in sentence
processing. Cognitive Science, 33:794?838.
Pantel, Patrick, Rahul Bhagat, Bonaventura
Coppola, Timothy Chklovski, and
Eduard Hovy. 2007. ISP: Learning
inferential selectional preferences. In
Proceedings of the Joint Human Language
Technology Conference and Annual Meeting
of the North American Chapter of the
Association for Computational Linguistics,
pages 564?571, Rochester, NY.
Pereira, Fernando, Naftali Tishby, and Lillian
Lee. 1993. Distributional clustering of
English words. In Proceedings of the 31st
Annual Meeting of the Association for
Computational Linguistics, pages 183?190,
Columbus, OH.
Pustejovsky, James. 1995. The Generative
Lexicon. MIT Press, Cambridge, MA.
Resnik, Philip. 1996. Selectional constraints:
An information-theoretic model and its
computational realization. Cognition,
61:127?159.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1999.
Inducing a semantically annotated lexicon
via EM-based clustering. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, pages 104?111,
College Park, MA.
Salton, Gerard, Anita Wong, and Chung-Shu
Yang. 1975. A vector-space model for
information retrieval. Journal of the
American Society for Information Science,
18:613?620.
Schulte im Walde, Sabine. 2010. Comparing
computational approaches to selectional
preferences: Second-order co-occurrence
vs. latent semantic clusters. In Proceedings
of the 7th International Conference on
Language Resources and Evaluation,
pages 1381?1388, Valleta.
Schulte im Walde, Sabine, Christian Hying,
Christian Scheible, and Helmut Schmid.
2008. Combining EM training and the
MDL principle for an automatic verb
classification incorporating selectional
preferences. In Proceedings of the 46th
Annual Meeting of the Association for
Computational Linguistics, pages 496?504,
Columbus, OH.
Shepard, Roger. 1987. Towards a universal
law of generalization for psychological
science. Science, 237(4820):1317?1323.
Stolcke, Andreas. 2002. SRILM?an
extensible language modeling toolkit. In
Proceedings of the International Conference on
Spoken Language Processing, pages 901?904,
Denver, CO.
Toutanova, Kristina, Christoper D. Manning,
Dan Flickinger, and Stephan Oepen. 2005.
Stochastic HPSG parse selection using the
Redwoods corpus. Journal of Research on
Language and Computation, 3(1):83?105.
Trueswell, John, Michael Tanenhaus, and
Susan Garnsey. 1994. Semantic influences
on parsing: Use of thematic role
information in syntactic ambiguity
resolution. Journal of Memory and Language,
33:285?318.
Vandekerckhove, Bram, Dominiek Sandra,
and Walter Daelemans. 2009. A robust and
extensible exemplar-based model of
762
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
thematic fit. In Proceedings of the 12th
Meeting of the European Chapter of the
Association for Computational Linguistics,
pages 826?834, Athens.
Wilks, Yorick. 1975. Preference semantics.
In E. Keenan, editor, Formal Semantics of
Natural Language. Cambridge University
Press, Cambridge, UK, pages 329?350.
Yarowsky, David. 1993. One sense per
collocation. In Proceedings of the ARPA
Human Language Technology Workshop,
pages 266?271, Princeton, NJ.
Zanzotto, Fabio Massimo, Marco
Pennacchiotti, and Maria Teresa Pazienza.
2006. Discovering asymmetric entailment
relations between verbs using selectional
preferences. In Proceedings of the Joint
Annual Meeting of the Association for
Computational Linguistics and International
Conference on Computational Linguistics,
pages 849?856, Sydney.
Zapirain, Be?at, Eneko Agirre, and Llu?s
M?rquez. 2009. Generalizing over lexical
features: Selectional preferences for
semantic role classification. In Proceedings
of the 47th Annual Meeting of the Association
for Computational Linguistics, pages 73?76,
Singapore.
763

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 921?929,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Cross-lingual Induction of Selectional Preferences
with Bilingual Vector Spaces
Yves Peirsman
QLVL, University of Leuven
Research Foundation ? Flanders (FWO)
yves.peirsman@arts.kuleuven.be
Sebastian Pad?
IMS, University of Stuttgart
pado@ims.uni-stuttgart.de
Abstract
We describe a cross-lingual method for the in-
duction of selectional preferences for resource-
poor languages, where no accurate monolin-
gual models are available. The method uses
bilingual vector spaces to ?translate? foreign
language predicate-argument structures into
a resource-rich language like English. The
only prerequisite for constructing the bilin-
gual vector space is a large unparsed corpus
in the resource-poor language, although the
model can profit from (even noisy) syntactic
knowledge. Our experiments show that the
cross-lingual predictions correlate well with
human ratings, clearly outperforming monolin-
gual baseline models.
1 Introduction
Selectional preferences capture the empirical observa-
tion that not all words are equally good arguments to
a given verb in a particular argument position (Wilks,
1975; Resnik, 1996). For instance, the subjects of
the English verb to shoot are generally people, while
the direct objects can be people or animals. This is
reflected in speakers? intuitions. Table 1 shows that
the combination the hunter shot the deer is judged
more plausible than the deer shot the hunter. Selec-
tional preferences do not only play an important role
in human sentence processing (McRae et al, 1998),
but are also helpful for NLP tasks like word sense
disambiguation (McCarthy and Carroll, 2003) and
semantic role labeling (Gildea and Jurafsky, 2002).
Computational models of selectional preferences
predict such plausibilities for triples of a predicate p,
an argument position a, and a head word h, such as
Predicate Relation Noun Plausibility
shoot subject hunter 6.9
shoot object hunter 2.8
shoot subject deer 1.0
shoot object deer 6.4
Table 1: Predicate-relation-noun triples with human plau-
sibility judgments on a 7-point scale (McRae et al, 1998)
(shoot,object,hunter). All recent models take a two-
step approach: (1), they extract all triples (p, a, h)
from a large corpus; (2), they apply some type of
generalization to make predictions for unseen items.
Clearly, the accuracy of these models relies crucially
on the quality and coverage of the extracted triples,
and thus on the syntactic analysis of the corpus. Un-
fortunately, corpora that are both large enough and
have a very good syntactic analysis are only available
for a handful of Western and Asian languages, which
leaves all other languages without reliable selectional
preference models.
In this paper, we propose a cross-lingual knowl-
edge transfer approach to this problem: We automat-
ically translate triples (p, a, h) from resource-poor
languages into English, where large and high-quality
parsed corpora are available and we can compute a
reliable plausibility estimate. The translations are
extracted from a bilingual semantic space, which can
be constructed via bootstrapping from large unparsed
corpora in the two languages, without the need for
parallel corpora or bilingual lexical resources.
Structure of the paper. Section 2 reviews models
for selectional preferences. In Section 3, we describe
our approach. Section 4 introduces our experimental
setup, and Sections 5 and 6 present and discuss our
experiments. Section 7 wraps up.
921
2 Selectional Preferences
The first broad-coverage model of selectional prefer-
ences was developed by Resnik (1996). To estimate
the plausibility of a triple (p, a, h), Resnik first ex-
tracted all head words seen with predicate p in posi-
tion a, Seena(p), from a corpus. He then used the
WordNet hierarchy to generalize over the head words
and to create predictions for unseen ones. A number
of studies has followed the same approach, exploring
different ways of using the structure of WordNet (Abe
and Li, 1996; Clark and Weir, 2002). While these
approaches show good results, they can only make
predictions for argument heads that are covered by
WordNet. This is already a problem for English, and
much more so in other languages, where comparable
resources are often much smaller or entirely absent.
A promising alternative approach is to derive
the generalizations from distributional informa-
tion (Prescher et al, 2000; Pad? et al, 2007; Bergsma
et al, 2008). For example, the Pad? et al (2007)
model computes vector space representations for all
head words h and defines the plausibility of the triple
(p, a, h) as a weighted mean of the vector space simi-
larities between h and all h? in Seena(p):
Pl(p, a, h) =
?
h??Seena(p)
w(h?)? sim(h, h?)
?
h? w(h
?)
(1)
where w(h?) is a weight, typically frequency.
In this model, the generalization is provided by dis-
tributional similarity, which can be computed from a
large corpus, without the need for additional lexical
resources. Pad? et al found it to outperform Resnik?s
approach in an evaluation against human plausibility
judgments. However, note that competitive results
are only obtained by representing the head words in
?syntactic? vector spaces whose dimensions consist
of context words with their syntactic relation to the
target rather than just context words. This is not sur-
prising: Presumably, hunter and deer share a domain
and are likely to have similar word-based context
distributions, even though they differ with regard to
their plausibility for particular predicate-argument
positions. Only when the vector space can capture
their different syntactic co-occurrence patterns can
the model predict different plausibilities.
English triple
German triple
(schie?en,obj,Hirsch)
monolingual 
selectional
preference
model
monolingual 
selectional
preference
model
(shoot,obj,deer)
bilingual
vector space
deer
Hirsch
schie?en 
shoot
Figure 1: Predicting selectional preferences for a source
language (e.g. German) by translating into a target lan-
guage (e.g. English) with a bilingual vector space.
3 Cross-lingual selectional preferences
In order to compute reliable selectional preference
representations, distributional models need to see
at least some head words for each (p, a) combina-
tion. Manually annotated treebank corpora, which
are becoming available for an increasing number of
languages, are too small for this task. We therefore
explore the idea of predicting the selectional pref-
erences for such languages by taking advantage of
large corpora with high-quality syntactic analyses in
resource-rich languages like English. This idea falls
into the general approach of cross-lingual knowledge
transfer (see e.g. Hwa et al, 2005). The application
to selectional preferences was suggested by Agirre et
al. (2003), who demonstrated its feasibility by man-
ual translation between Basque and English. We
extend their experiments to an automatic model that
predicts plausibility judgments in a resource-poor
language (source language) by exploiting a model in
a resource-rich language (target language).
Figure 1 sketches our method. We assume that
there is not enough high-quality data to build a mono-
lingual selectional preference model for the source
language (shown by dotted lines). However, we can
use a bilingual vector space, that is, a semantic space
in which words of both the source and the target
language are represented, to translate each source
language word s into the target language by identify-
ing its nearest (most similar) target word tr(s):
tr(s) = argmaxt sim(s, t) (2)
Now we can use a target language selectional prefer-
ence model to obtain plausibilities for source triples:
Pls(p, a, h) = Plt(tr(p), a, tr(h)) (3)
where the superscript indicates the language.
922
Eq. (3) gives rise to three questions: (1), How can
we construct the bilingual space to model tr? (2), Is
translating actually the appropriate way of transfer-
ring selectional preferences? (3), Is it reasonable to
retain the source language argument positions like
subject or object? The following subsections discuss
(1) and (2); we will address (3) in Sections 5 and 6.
3.1 Bilingual Vector Spaces
Bilingual vector spaces are vector spaces in which
words from two languages are represented (cf. Fig. 2).
The dimensions of this space are labeled with bilin-
gual context word pairs (like secretly/heimlich and
rifle/Gewehr for German?English) that are mutual
translations. By treating such context word pairs as
single dimensions, the vector space can represent tar-
get words from both languages, counting the target
words? co-occurrences with the context words from
the respective language. In other words, a source-
target word pair (s, t) will be assigned similar vectors
in the semantic space if the context words of s are
translations of the context words of t. Cross-lingual
semantic similarity between words can be measured
using standard vector space similarity (Lee, 1999).
Importantly, bilingual vector spaces can be built
on the basis of co-occurrences drawn from two un-
related corpora for the source and target languages.
Their construction does not require resources such
as parallel corpora or bilingual translation lexicons,
which might not be available for resource-poor source
languages. Where parallel corpora exist, they often
cover specific domains (e.g., politics), while many
bilingual lexicons are prone to ambiguity problems.
The main challenge in constructing bilingual vec-
tor spaces is determining the set of dimensions,
i.e., bilingual word pairs, using as little knowledge as
possible. Most often, such pairs are extracted from
small bilingual lexicons (Fung and McKeown, 1997;
Rapp, 1999; Chiao and Zweigenbaum, 2002). As
mentioned above, such resources might not be avail-
able. We thus follow an alternative approach by using
frequent cognates, words that are shared between the
two languages (Mark? et al, 2005). Cognates can
be extracted by simple string matching between the
corpora, and mostly share their meaning (Koehn and
Knight, 2002). However, they account for (at most) a
small percentage of all interesting translation pairs.
To extend the set of dimensions available for the
shoot
hit
stalk
rifle/
Gewehr
secretly/
heimlich
schie?en
anschleichen
Figure 2: Sketch of a bilingual vector space for English
(solid dots) and German (empty circles).
bilingual space, we use these cognates merely as a
starting point for a bootstrapping process: We build
a bilingual vector space with the initial word pairs as
dimensions, and identify nearest neighbors between
the two languages in the space. These are added as
dimensions of the bilingual space, and the process
is repeated. Since the focus is on identifying reli-
able source-target word pairs rather than complete
coverage as in Eq. (2), we adopt a symmetrical defi-
nition of translation that pairs up only mutual nearest
neighbors, and allows words to remain untranslated:1
trsym(s) = t iff tr(s) = t and tr(t) = s (4)
From the second iteration onward, this process intro-
duces dimensions that are not identical graphemes,
such as Kind?child and Geschwindigkeit?speed, and
is iterated until convergence. Since each word of
either language can only participate in at most one
dimension, dimensions acquired in later steps can cor-
rect wrong pairs from previous steps, like the ?false
friend? German Kind ?child? ? English kind, which
is part of the initial set of cognates.
3.2 Translation and Selectional Preferences
As Figure 1 shows, the easiest way of exploiting a
bilingual semantic space is to identify for each source
word the target language word with the highest se-
mantic similarity. For example, in Figure 2, the best
translation of German schie?en is its English nearest
neighbor, shoot. However, it is risky to rely on the
single nearest neighbor ? it might simply be wrong.
Even if it is correct, data sparsity is an issue: The
translations may be infrequent in the target language,
or the two translations of p and h may form unlikely
collocates for target language-internal reasons (like
1To avoid unreliable vectors, we also adopt only the 50%
most frequent of the trsym pairs. Frequency is defined as the
geometric mean of the two words? monolingual frequencies.
923
difference in register) that do not reflect plausibility.
A third issue are monolingual semantic phenomena
like polysemy and idioms: The implausible German
triple (schie?en,obj,Brise) will be judged as very plau-
sible due to the English idiom to shoot the breeze.
A look at the broader neighborhood of schie?en
suggests that its second and third-best English neigh-
bors, hit, and stalk, can be used to smooth plausibility
estimates for schie?en. Instead of translating source
language words by their single nearest neighbor, we
will take its k nearest neighbors into account. This
is defensible also from a more fundamental point of
view, which suggests that the cross-lingual transfer of
selectional preferences does not require literal trans-
lation in order to work. First, ontological models
like Resnik?s assume that synonymous words behave
similarly with respect to selectional preferences. Sec-
ond, recent work by Chambers and Jurafsky (2009)
has induced ?narrative chains?, i.e., likely sequences
of events, by their use of similar head words. Thus,
we expect that all k nearest neighbors of a source
predicate s are informative for the selectional prefer-
ences of s (like schie?en) as long as they are either
synonyms of its literal translation (shoot/hit) or come
from the same narrative chain (stalk/kill/. . . ).
It is also clear that smoothing does not always
equate better predictions. Closeness in a word-based
vector space can also just reflect semantic association.
For example, Spanish tenista ?tennis player? is highly
associated with English tennis, but is a bad translation
in terms of selectional preferences. We assume that
this problem is more acute for nouns than for verbs:
The context of verbs is dominated by their arguments,
which is not true for nouns. Consequently, close
nouns in vector space can differ widely in ontological
type, while close verbs generally have one or more
similar argument slots. In our model, we will thus
consider several verb translations, but just the best
head word translation. For details, see Section 5.
4 Experimental Setup
Our evaluation uses English as the target language
and two source languages: German (as a very close
neighbor of English) and Spanish (as a more distant
one). Neither of these languages are really resource-
poor, but they allow us to compare our cross-lingual
model against monolingual models, to emulate dif-
ferent levels of ?resource poorness? and to examine
the model?s learning curve.
Plausibility Data. For German, we used the plau-
sibility judgments collected by Brockmann (2002).
The dataset contains human judgments for ninety
triples sampled from the manually annotated 1 mil-
lion word TiGer corpus (Brants et al, 2002): ten
verbs with three argument positions (subject [SUBJ],
direct object [DOBJ], and oblique (prepositional) ob-
ject [POBJ]) combined with three head words. Mod-
els are evaluated against such datasets by correlating
predicted plausibilities with the (not normally dis-
tributed) human judgments using Spearman?s ?, a
non-parametric rank-order correlation coefficient.
We constructed a similar 90-triple data set for
Spanish by sampling triples from two Spanish cor-
pora (see below) using Brockmann?s (2002) crite-
ria. Human judgments for the triples were collected
through the Amazon Mechanical Turk (AMT) crowd-
sourcing platform (Snow et al, 2008). We asked
native speakers of Spanish to rate the plausibility of
a simple sentence with the relevant verb-argument
combination on a five-point Likert scale, obtaining
between 12 and 17 judgments for each triple. For
each datapoint, we removed the single lowest and
highest judgments and computed the mean. We as-
sessed the reliability of our data by replicating Brock-
mann?s experiment for German with our AMT setup.
With a Spearman ? of almost .90, our own judgments
correlate very well with Brockmann?s original data.
Monolingual Prior Work and Baselines. For
German, Brockmann and Lapata (2003) evaluated
ontology-based models trained on TiGer triples and
the GermaNet ontology. The results in Table 2 show
that while both models are able to predict the data
significantly, neither of the models can predict all of
the data. We attribute this to the small size of TiGer.2
To gauge the limits of monolingual knowledge-
lean approaches, we constructed two monolingual
distributional models for German and Spanish ac-
cording to the Pad? et al (2007) model (Eq. (1)).
Recall that this model performs generalization in a
syntax-based vector space model. We computed vec-
tor spaces from dependency-parsed corpora for the
2For each of the three argument positions and ?all?, Brock-
mann and Lapata report the results for the best parametrization
of the models, which explains the apparently inconsistent results.
924
Resnik Clark & Weir
SUBJ .408* .268
DOBJ .430* .611***
POBJ .330 .597***
all .374*** .232*
Table 2: Monolingual baselines 1. Spearman correla-
tions for ontology-based models in German as reported by
Brockmann and Lapata (2003). *: p < .05; ***: p < .001
Lang. German Spanish
Corpus Schulte?s HGC AnCora Encarta
? Cov. ? Cov. ? Cov.
SUBJ .34? 90% .44* 80% .14 100%
DOBJ .51** 97% .29 83% -.05 100%
POBJ .41* 93% -.03 100% ? ?3
all .33** 93% .16 88% .11 67%
Table 3: Monolingual baselines 2. Spearman correlation
and coverage for distributional models. ? : p < .1; *: p <
.05; **: p < .01.
two languages, using the 2,000 most frequent lemma-
dependency relation pairs as dimensions and adopt-
ing the popular pointwise mutual information metric
as co-occurrence statistic. For German, we used
Schulte im Walde?s verb frame resource (Schulte im
Walde et al, 2001), which contains the frequency of
triples calculated from probabilistic parses of 30M
words from the Huge German Corpus (HGC) of
newswire. For Spanish, we consulted two syntac-
tically analyzed corpora: the AnCora (Taul? et al,
2008) and the Encarta corpus (Calvo et al, 2005). At
0.5M words, the AnCora corpus is small, but man-
ually annotated, whereas the larger, automatically
parsed Encarta corpus amounts to over 18M tokens.
Table 3 shows the results for the distributional
monolingual models. For German, we get significant
correlations for DOBJ and POBJ, an almost signif-
icant correlation for SUBJs, and high significance
for the complete dataset (p < 0.01). These figures
rival the performance of the ontological models (cf.
Table 2), without using ontological information. For
Spanish, the only significant correlation with human
judgments is obtained for subjects, the most frequent
argument position, with the clean AnCora data. An-
Cora is presumably too sparse for the other argument
positions. The large Encarta corpus, in turn, is very
noisy, supporting our concerns from Section 2.
3Since the Encarta data consists of individual dependency
n noun adj verb all
German 7340 .61 .57 .43 .56
Spanish 4143 .62 .67 .41 .58
Table 4: First-translation accuracy for German-English
and Spanish-English translation (n: size of gold standard).
Cross-lingual Selectional Preferences. Our archi-
tecture for the cross-lingual prediction of selectional
preferences shown in Figure 1 consists of two com-
ponents, namely the bilingual vector space and a
selectional preference model in the target language.
As our English selectional preference model, we
again use the Pad? et al (2007) model, trained on
a version of the BNC parsed with MINIPAR (Lin,
1993). The parameters of the syntactic vector space
were the same as for the monolingual baseline mod-
els. The bilingual vector spaces were constructed
from three large, unparsed, comparable monolin-
gual corpora. For German, we used the HGC de-
scribed above. For Spanish, we obtained a corpus
with around 100M words, consisting of 2.5 years of
crawled text from two major Spanish newspapers.
For English, we used the BNC.
We first constructed initial sets of bilingual labels.
For German?English, we identified 1064 graphem-
ically identical word pairs that occurred more than
4 times per million words. Due to the larger lex-
ical distance between Spanish and English, there
are fewer graphemically identical tokens for this lan-
guage pair. We therefore applied a Porter stemmer
and found 2104 identical stems, at a higher risk of
?false friends?. We then applied the bootstrapping
cycle from Section 3.1. The set of dimensions con-
verged after around five iterations.
We evaluated the (asymmetric) nearest neighbor
pairs from the final spaces, (s, tr(s)), against two
online dictionaries.4 Table 4 shows that 55% to 60%
of the pairs are listed in the dictionaries, with parallel
tendencies for both language pairs. The bilingual
space performs fairly well for nouns and adjectives,
but badly for verbs, which is a well-known weakness
of distributional models (Peirsman et al, 2008).
Even taking into account the incompleteness of
dictionaries, this looks like a negative result: more
relations rather than trees, we could not model the POBJ data.
4DE-EN: www.dict.cc; ES-EN: www.freelang.net.
Pairs (s, tr(s)) were only evaluated if the dictionary listed s.
925
than half of all verb translations are incorrect. How-
ever, following up on our intuitions from Section 3.2,
we performed an analysis of the ?incorrect? transla-
tions. It revealed that many of the errors in Table 4
are informative, semantically related words. Near-
est neighbor target language verbs in particular tend
to represent the same event type and take the same
kinds of arguments as the source verb. Examples
are German gef?hrden ?threaten? ? English affect,
and German Neugier ?curiosity? ? English enthusi-
asm. We concluded that literal translation quality is
a misleading figure of merit for our task.
Experimental rationale. Section 3 introduced one
major design decision of our model: the question of
how to treat the argument position, which cannot
be translated by the bilingual vector space, in the
cross-lingual transfer. We present two experiments
that investigate the model?s behavior in the absence
and presence of knowledge about argument positions.
Experiment 1 uses no syntactic knowledge about the
source language whatsoever. In this situation, the
best we can do is to assume that source language
argument positions like SUBJ will correspond to the
same argument position in the target language. Exper-
iment 2 attempts to identify, for each source language
argument position, the ?best fit? position in the target
language. This results in better plausibility estimates,
but also means that we need at least some syntac-
tic information about the source language. In both
experiments, we vary the number of translations we
consider for each verb.
5 Exp. 1: Induction without syntactic
knowledge in the source language
This experiment assumes that argument positions
simply carry over between languages. While this
assumption clearly simplifies linguistic reality, it has
the advantage of not needing any syntactic informa-
tion about the source language. We thus model Ger-
man and Spanish SUBJ relations by English SUBJ
relations and DOBJs by DOBJs. In the case of (lex-
icalized) POBJs, where we cannot assume identity,
we compute plausibility scores for all English POBJs
that account for at least 10% of the predicate?s ar-
gument tokens, and select the PP with the highest
plausibility estimate. The k best ?translations? of the
predicate p, trk(p), are turned into a single prediction
using maximization, yielding the final model:
Plsnosyn(p, a, h) = max
pt?trk(p)
Plt(pt, a, tr(h)) (5)
Note that this model does not use any source lan-
guage information, except the bilingual vector space.
The results of Experiment 1 are given in Table 5
(coverage always 100%). For German, all predictions
correlate significantly with human ratings, and most
even at p < 0.01, despite our naive assumption about
the cross-lingual argument position identity. The
results exceed both monolingual model types (onto-
logical, Tab. 2, and distributional, Tab. 3), notably
without the use of syntactic data. In particular, the
results for the POBJs, notoriously difficult to model
monolingually, are higher than for SUBJs or DOBJs.
We attribute this to the cross-lingual generalization
which takes all prepositional arguments into account.
The Spanish dataset is harder to model overall.
We obtain significantly high correlations for SUBJ,
but non-significant results for DOBJ and POBJ. This
corresponds well to the patterns for the monolingual
AnCora corpus (Table 3). However, we outperform
AnCora on the complete dataset, where it did not
achieve significance, while the cross-lingual model
does at p < 0.01 ? again, even without the use of
syntactic analyses. We attribute the overall lower
results compared to German to systematic syntactic
differences between English and Spanish. For exam-
ple, animate direct objects in Spanish are realized
as POBJs headed by the preposition a. Estimating
the plausibility of such objects by looking at English
POBJs is unlikely to yield good results. The use of
a larger number of verb translations yields a clear
increase in correlation for the German data, but in-
conclusive results for Spanish.
6 Exp. 2: Induction with syntactic
knowledge in the source language
As discussed in Section 3.2, verbs that are semanti-
cally similar in the bilingual vector space may very
well realize their (semantic) argument positions dif-
ferently in the surface syntax. For example, German
teilnehmen is correctly translated to English attend,
but the crucial event argument is realized differently,
namely as a POBJ headed by an in German and as
a DOBJ in English. To address this problem, we
926
DE 1-best 2-best 3-best 4-best 5-best
SUBJ .44* .47** .45* .47** .54**
DOBJ .39* .39* .52** .54** .55**
POBJ .58** .61** .61** .61** .62**
all .35** .37** .37** .38** .40**
ES 1-best 2-best 3-best 4-best 5-best
SUBJ .58** .64** .64** .58** .58**
DOBJ .13 .16 .11 .07 .07
POBJ .13 .13 .09 .14 .14
all .34** .36** .34** .32** .32**
Table 5: Exp.1: Spearman correlation between syntaxless
cross-lingual model and human judgments for k best verb
translations. Best k for each argument position marked in
boldface. Coverage of all models: 100%.
learn a mapping function m that identifies the argu-
ment position at of a target language predicate pt
that corresponds best to an argument position a of a
predicate p in the source language. Our simple model
is in the same spirit as the cross-lingual plausibility
model itself: It returns the argument position at of
pt for which the seen head words of (p, a) are most
plausible when translated into the target language:5
m(p, a, pt) = argmax
at
?
h?Seena(p)
Plt(pt, at, tr(h))
Parallel to Eq. (5), the cross-lingual model is now:
Plssyn(p, a, h) = max
pt?trk(p)
Plt(pt,m(p, a, pt), tr(h))
(6)
This model can recover English argument positions
that correspond better to the original ones than the
identity mapping. For example, on our data, it discov-
ers the mapping for teilnehmen an/attend discussed
above. A second example concerns the incorrect, but
informative translation of stagnieren ?stagnate? as
boost. Here the model recognizes that the SUBJ of
stagnieren (the stagnating entity) corresponds to the
DOBJ of boost.
Establishing m requires syntactic information in
the source language, in order to obtain the set of
seen head words Seenas(ps). For this reason, Exp. 2
uses the parsed subset of the HGC (German), and the
AnCora and Encarta corpora (Spanish). The results
are shown in Table 6. We generally improve over
5To alleviate sparse data, we ignore argument positions of
English verbs that represent less than 10% of its argument tokens.
DE 1-best 2-best 3-best 4-best 5-best
SUBJ .55** .59** .49** .52** .54**
DOBJ .52** .52** .66** .66** .68**
POBJ .61** .68** .70** .69** .70**
all .41** .44** .44* .46** .48**
ES-A 1-best 2-best 3-best 4-best 5-best
SUBJ .52** .47* .42* .41* .42*
DOBJ .52*c .64**c .54*c .42*c .42*c
POBJ .32? .18 .13 .13 .24
all .47** .41** .36** .33** .37**
ES-E 1-best 2-best 3-best 4-best 5-best
SUBJ .40* .42* .39* .39* .41*
DOBJ .21 .02 .06 .13 .20
Table 6: Exp.2: Spearman correlation between syntax-
aware cross-lingual model and human judgments for k
best verb translations. ES-A: AnCora corpus, ES-E: En-
carta corpus. Best k for each argument position in bold-
face. Coverage of all models: 100%, except c: 60%.
Exp. 1. For German, every single model now corre-
lates highly significantly with human judgments (p
< 0.01), and the correlation for the complete dataset
increases from .40 to .48. For Spanish, we see very
good results for the AnCora corpus. Compared to
Exp. 1, we see a slight degradation for the SUBJs;
however, the correlations remain significant for all
values of k. Conversely, all predictions for DOBJs
are now significant,6 and the POBJs have improved at
least numerically, which validates our analysis of the
problems in Exp. 1. The best correlation for the com-
plete dataset improves from .36 to .47. The results
for the Encarta corpus disappoint, though. SUBJs
are significant, but worse than for AnCora, and the
DOBJs remain non-significant throughout. With re-
gard to increasing the number of verb translations,
Exp. 2 shows an almost universal benefit for Ger-
man, but still mixed results for Spanish, which may
indicate that verb translations for Spanish are still
?looser? than the German ones.
In fact, most remaining poor judgments are the
result of problematic translations, which stem from
three main sources. The first one is sparse data. Infre-
quent German and Spanish words often receive unre-
liable vector representations. Some examples are the
6Note, however, that AnCora has an imperfect coverage for
DOBJs (60%). This is because our Spanish dataset contains
verbs sampled from Encarta that do not occur in AnCora.
927
German Tau (?dew?, frequency of 180 in the HGC),
translated as alley, and Reifepr?fung (German SAT,
frequency 120), translated as affiliation. Both of these
may also be due to the difference in genre between
the HGC and the BNC. A second problem is formed
by nearest neighbors that are ontologically dissimi-
lar, as in the tenista ?tennis player?/tennis example
from above. A final issue relates to limitations of the
Pad? et al (2007) model, whose architecture is sus-
ceptible to polysemy-related problems. For instance,
the Spanish combination (excavar, obj, terreno) was
judged by speakers as very plausible, but its English
equivalent (excavate, obj, land) is assigned a very
low score by the model. This might be due to the
fact that in the BNC, land occurs often in its political
meaning, and forms an outlier among the head words
for (excavate,obj).
How much syntactic information is necessary?
The syntax-aware model requires syntactic infor-
mation about the source language, which seems to
run counter to our original motivation of developing
methods for resource-poor languages. To address this
point, we analyzed the behavior of the syntax-aware
model for small syntactically analyzed corpora that
contained only at most m occurrences for each pred-
icate. We obtained the m occurrences by sampling
from the syntactically analyzed part of the HGC; if
fewer than m occurrences were present in the corpus,
we simply used these. Figure 3 shows the training
curve with 1 verb translation, averaged over n rounds
(n = 10 for 5 arguments, n = 5 for 10 arguments,
n = 4 for 20, 50 and 100 arguments). The general
picture is clear: most of the benefit of the syntactic
data is drawn form the first five occurrences for each
argument position. This shows that a small amount of
targeted syntactic annotation can improve the cross-
lingual model substantially.
7 Conclusions
In this article, we have presented a first unsuper-
vised cross-lingual model of selectional preferences.
Our model proceeds by automatically translating
(predicate, argument position, head word) triples for
resource-poor source languages into a resource-rich
target language, where accurate selectional prefer-
ence models are available. The translation is based on
a bilingual vector space, which can be bootstrapped
l
l l
l l l
number of observed heads
Spe
arm
an's
 rho
l SUBJDOBJPOBJ
all
0 5 10 20 50 100
0.3
0.4
0.5
0.6
0.7
Figure 3: Training curve for the bilingual German?English
model as a function of the number of observed head words
per argument position in the source language.
from large unparsed corpora in the two languages.
Our results indicate that bilingual methods can go
a long way towards the modeling of selectional pref-
erences in resource-poor languages, where bilingual
lexicons, parallel corpora, or ontologies might not be
available. Our experiments have looked at German
and Spanish, where the cross-lingual models rival
and even exceed monolingual methods that typically
have to rely on small, clean ?treebank?-style corpora
or large, very noisy, automatically parsed corpora.
We have also demonstrated that noisy syntactic data
from the source language can be integrated in our
model, where it helps improve the cross-lingual han-
dling of argument positions. The linguistic distance
between the languages can impact (1) the ability to
find accurate translations and (2) the degree of syntac-
tic overlap; nevertheless, as Agirre et al (2003) show,
the transfer is possible even for unrelated languages.
In this paper, we have instantiated the selectional
preference model in the target language (English)
with the distributional model by Pad? et al (2007).
However, our approach is modular and can be com-
bined with any other selectional preference model.
We see two main avenues for future work: (1), The
construction of properly bilingual models where
source language information can also help to fur-
ther improve the target language model (Diab and
Resnik, 2002); (2), The extension of our cross-lingual
mapping for the argument position to mappings that
hold across multiple predicates as well as argument-
dependent mappings like the Spanish direct objects,
whose realization depends on their animacy.
928
References
Naoki Abe and Hang Li. 1996. Learning word association
norms using tree cut pair models. In Proc. ICML, pages
3?11, Bari, Italy.
Eneko Agirre, Izaskun Aldezabal, and Eli Pociello. 2003.
A pilot study of English selectional preferences and
their cross-lingual compatibility with Basque. In Proc.
TSD, pages 12?19, Brno, Czech Republic.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative learning of selectional preference from
unlabeled text. In Proc. EMNLP, pages 59?68, Hon-
olulu, HI.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proc. Workshop on Treebanks and Linguistic
Theories, Sozopol, Bulgaria.
Carsten Brockmann and Mirella Lapata. 2003. Evaluating
and combining approaches to selectional preference
acquisition. In Proc. EACL, pages 27?34, Budapest,
Hungary.
Carsten Brockmann. 2002. Evaluating and combining ap-
proaches to selectional preference acquisition. Master?s
thesis, Universit?t des Saarlandes, Saarbr?cken.
Hiram Calvo, Alexander Gelbukh, and Adam Kilgarriff.
2005. Distributional thesaurus vs. wordnet: A compari-
son of backoff techniques for unsupervised PP attach-
ment. In Proc. CICLing, pages 177?188, Mexico City,
Mexico.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In Proc. ACL, pages 602?610, Singapore.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in spe-
cialized, comparable corpora. In Proc. COLING, pages
1?5, Taipei, Taiwan.
Stephen Clark and David Weir. 2002. Class-based proba-
bility estimation using a semantic hierarchy. Computa-
tional Linguistics, 28(2):187?206.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proc. ACL, pages 255?262, Philadelphia, PA.
Pascale Fung and Kathleen McKeown. 1997. Finding
terminology translations from non-parallel corpora. In
Proc. 3rd Annual Workshop on Very Large Corpora,
pages 192?202, Hong Kong.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguistics,
28(3):245?288.
Rebecca Hwa, Philipp Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(3):311?325.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Proc.
ACL-02 Workshop on Unsupervised Lexical Acquisi-
tion, pages 9?16, Philadelphia, PA.
Lillian Lee. 1999. Measures of distributional similarity.
In Proc. ACL, pages 25?32, College Park, MD.
Dekang Lin. 1993. Principle-based parsing without over-
generation. In Proc. ACL, pages 112?120.
Korn?l Mark?, Stefan Schulz, Olena Medelyan, and Udo
Hahn. 2005. Bootstrapping dictionaries for cross-
language information retrieval. In Proc. SIGIR, pages
528?535, Seattle, WA.
Diana McCarthy and John Carroll. 2003. Disambiguat-
ing nouns, verbs and adjectives using automatically
acquired selectional preferences. Computational Lin-
guistics, 29(4):639?654.
Ken McRae, Michael Spivey-Knowlton, and Michael
Tanenhaus. 1998. Modeling the influence of thematic
fit (and other constraints) in on-line sentence compre-
hension. Journal of Memory and Language, 38:283?
312.
Sebastian Pad?, Ulrike Pad?, and Katrin Erk. 2007. Flex-
ible, corpus-based modelling of human plausibility
judgements. In Proc. EMNLP-CoNLL, pages 400?409,
Prague, Czech Republic.
Yves Peirsman, Kris Heylen, and Dirk Geeraerts. 2008.
Size matters. Tight and loose context definitions in
English word space models. In Proc. ESSLLI Workshop
on Lexical Semantics, pages 9?16, Hamburg, Germany.
Detlef Prescher, Stefan Riezler, and Mats Rooth. 2000.
Using a probabilistic class-based lexicon for lexical
ambiguity resolution. In Proc. COLING, pages 649?
655, Saarbr?cken, Germany.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proc. ACL, pages 519?526, College Park, MD.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational real-
ization. Cognition, 61:127?159.
Sabine Schulte im Walde, Helmut Schmid, Mats Rooth,
Stefan Riezler, and Detlef Prescher. 2001. Statistical
Grammar Models and Lexicon Acquisition. In Linguis-
tic Form and its Computation, pages 389?440. CSLI
Publications, Stanford, CA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
Evaluating non-expert annotations for natural language
tasks. In Proc. EMNLP, pages 254?263, Honolulu, HI.
Mariona Taul?, M. Ant?nia Mart?, and Marta Recasens.
2008. Ancora: Multilevel annotated corpora for Cata-
lan and Spanish. In Proc. LREC, Marrakech, Morocco.
Yorick Wilks. 1975. Preference semantics. In E. Keenan,
editor, Formal Semantics of Natural Language. Cam-
bridge University Press.
929
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1209?1219,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Assessing the Role of Discourse References in Entailment Inference
Shachar Mirkin, Ido Dagan
Bar-Ilan University
Ramat-Gan, Israel
{mirkins,dagan}@cs.biu.ac.il
Sebastian Pado?
University of Stuttgart
Stuttgart, Germany
pado@ims.uni-stuttgart.de
Abstract
Discourse references, notably coreference
and bridging, play an important role in
many text understanding applications, but
their impact on textual entailment is yet to
be systematically understood. On the ba-
sis of an in-depth analysis of entailment
instances, we argue that discourse refer-
ences have the potential of substantially
improving textual entailment recognition,
and identify a number of research direc-
tions towards this goal.
1 Introduction
The detection and resolution of discourse refer-
ences such as coreference and bridging anaphora
play an important role in text understanding appli-
cations, like question answering and information
extraction. There, reference resolution is used for
the purpose of combining knowledge from multi-
ple sentences. Such knowledge is also important
for Textual Entailment (TE), a generic framework
for modeling semantic inference. TE reduces the
inference requirements of many text understand-
ing applications to the problem of determining
whether the meaning of a given textual assertion,
termed hypothesis (H), can be inferred from the
meaning of certain text (T ) (Dagan et al, 2006).
Consider the following example:
(1) T: ?Not only had he developed an aversion
to the President1 and politics in general,
Oswald2 was also a failure with Marina, his
wife. [...] Their relationship was supposedly
responsible for why he2 killed Kennedy1.?
H: ?Oswald killed President Kennedy.?
The understanding that the second sentence of the
text entails the hypothesis draws on two corefer-
ence relationships, namely that he is Oswald, and
that the Kennedy in question is President Kennedy.
However, the utilization of discourse information
for such inferences has been so far limited mainly
to the substitution of nominal coreferents, while
many aspects of the interface between discourse
and semantic inference needs remain unexplored.
The recently held Fifth Recognizing Textual
Entailment (RTE-5) challenge (Bentivogli et al,
2009a) has introduced a Search task, where the
text sentences are interpreted in the context of their
full discourse, as in Example 1 above. Accord-
ingly, TE constitutes an interesting framework ?
and the Search task an adequate dataset ? to study
the interrelation between discourse and inference.
The goal of this study is to analyze the roles
of discourse references for textual entailment in-
ference, to provide relevant findings and insights
to developers of both reference resolvers and en-
tailment systems and to highlight promising direc-
tions for the better incorporation of discourse phe-
nomena into inference. Our focus is on a manual,
in-depth assessment that results in a classification
and quantification of discourse reference phenom-
ena and their utilization for inference. On this ba-
sis, we develop an account of formal devices for
incorporating discourse references into the infer-
ence computation. An additional point of inter-
est is the interrelation between entailment knowl-
edge and coreference. E.g., in Example 1 above,
knowing that Kennedy was a president can alle-
viate the need for coreference resolution. Con-
versely, coreference resolution can often be used
to overcome gaps in entailment knowledge.
Structure of the paper. In Section 2, we pro-
vide background on the use of discourse refer-
ences in natural language processing (NLP) in
general and specifically in TE. Section 3 describes
the goals of this study, followed by our analy-
sis scheme (Section 4) and the required inference
1209
mechanisms (Section 5). Section 6 presents quan-
titative findings and further observations. Conclu-
sions are discussed in Section 7.
2 Background
2.1 Discourse in NLP
Discourse information plays a role in a range
of NLP tasks. It is obviously central to dis-
course processing tasks such as text segmenta-
tion (Hearst, 1997). Reference information pro-
vided by discourse is also useful for text under-
standing tasks such as question answering (QA),
information extraction (IE) and information re-
trieval (IR) (Vicedo and Ferrndez, 2006; Zelenko
et al, 2004; Na and Ng, 2009), as well as for the
acquisition of lexical-semantic ?narrative schema?
knowledge (Chambers and Jurafsky, 2009). Dis-
course references have been the subject of atten-
tion in both the Message Understanding Confer-
ence (Grishman and Sundheim, 1996) and the Au-
tomatic Content Extraction program (Strassel et
al., 2008).
The simplest form of information that discourse
provides is coreference, i.e., information that two
linguistic expressions refer to the same entity or
event. Coreference is particularly important for
processing pronouns and other anaphoric expres-
sions, such as he in Example 1. Ability to re-
solve this reference translates directly into, e.g., a
QA system?s ability to answer questions like Who
killed Kennedy?.
A second, more complex type of information
stems from bridging references, such as in the fol-
lowing discourse (Asher and Lascarides, 1998):
(2) ?I?ve just arrived. The camel is outside.?
While coreference indicates equivalence, bridging
points to the existence of a salient semantic rela-
tion between two distinct entities or events. Here,
it is (informally) ?means of transport?, which
would make the discourse (2) relevant for a ques-
tion like How did I arrive here?. Other types of
bridging relations include set-membership, roles
in events and consequence (Clark, 1975).
Note, however, that text understanding systems
are generally limited to the resolution of entity (or
even just pronoun) coreference, e.g. (Li et al,
2009; Dali et al, 2009). An important reason is the
unavailability of tools to resolve the more complex
(and difficult) forms of discourse reference such as
event coreference and bridging.1 Another reason
is uncertainty about their practical importance.
2.2 Discourse in Textual Entailment
Textual Entailment has been introduced in Sec-
tion 1 as a common-sense notion of inference.
It has spawned interest in the computational lin-
guistics community as a common denominator of
many NLP tasks including IE, summarization and
tutoring (Romano et al, 2006; Harabagiu et al,
2007; Nielsen et al, 2009).
Architectures for Textual Entailment. Over
the course of recent RTE challenges (Giampic-
colo et al, 2007; Giampiccolo et al, 2008), the
main benchmark for TE technology, two archi-
tectures for modeling TE have emerged as dom-
inant: transformations and alignment. The goal
of transformation-based TE models is to deter-
mine the entailment relation T ? H by find-
ing a ?proof?, i.e., a sequence of consequents,
(T, T1, . . . , Tn), such that Tn=H (Bar-Haim et al,
2008; Harmeling, 2009), and that in each trans-
formation, Ti? Ti+1, the consequent Ti+1 is en-
tailed by Ti. These transformations commonly in-
clude lexical modifications and the generation of
syntactic alternatives. The second major approach
constructs an alignment between the linguistic en-
tities of the trees (or graphs) of T and H , which
can represent syntactic structure, semantic struc-
ture, or non-hierarchical phrases (Zanzotto et al,
2009; Burchardt et al, 2009; MacCartney et al,
2008). H is assumed to be entailed by T if its en-
tities are aligned ?well? to corresponding entities
in T . Alignment quality is generally determined
based on features that assess the validity of the lo-
cal replacement of the T entity by the H entity.
While transformation- and alignment-based en-
tailment models look different at first glance, they
ultimately have the same goal, namely obtaining
a maximal coverage of H by T , i.e. to identify
matches of as many elements of H within T as
possible.2 To do so, both architectures typically
make use of inference rules such as ?Y was pur-
chased by X? X paid for Y?, either by directly ap-
plying them as transformations, or by using them
1Some studies, e.g. (Markert et al, 2003; Poesio et al,
2004), address the resolution of a few specific kinds of bridg-
ing relations; yet, wide-scope systems for bridging resolution
are unavailable.
2Clearly, the details of how the final entailment decision
is made based on the attained coverage differ substantially
among models.
1210
to score alignments. Rules are generally drawn
from external knowledge resources, such as Word-
Net (Fellbaum, 1998) or DIRT (Lin and Pantel,
2001), although knowledge gaps remain a key ob-
stacle (Bos, 2005; Balahur et al, 2008; Bar-Haim
et al, 2008).
Discourse in previous RTE challenges. The
first two rounds of the RTE challenge used ?self-
contained? texts and hypotheses, where discourse
considerations played virtually no role. A first step
towards a more comprehensive notion of entail-
ment was taken with RTE-3 (Giampiccolo et al,
2007), when paragraph-length texts were first in-
cluded and constituted 17% of the texts in the test
set. Chambers et al (2007) report that in a sample
of T ? H pairs drawn from the development set,
25% involved discourse references.
Using the concepts introduced above, the im-
pact of discourse references can be generally de-
scribed as a coverage problem, independent of the
system?s architecture. In Example 1, the hypoth-
esis word Oswald cannot be safely linked to the
text pronoun he without further knowledge about
he; the same is true for ?Kennedy ? President
Kennedy? which involves a specialization that is
only warranted in the specific discourse.
A number of systems have tried to address the
question of coreference in RTE as a preprocessing
step prior to inference proper, with most systems
using off-the-shelf coreference resolvers such as
JavaRap (Qiu et al, 2004) or OpenNLP3. Gen-
erally, anaphoric expressions were textually re-
placed by their antecedents. Results were in-
conclusive, however, with several reports about
errors introduced by automatic coreference res-
olution (Agichtein et al, 2008; Adams et al,
2007). Specific evaluations of the contribution
of coreference resolution yielded both small nega-
tive (Bar-Haim et al, 2008) and insignificant pos-
itive (Chambers et al, 2007) results.
3 Motivation and Goals
The results of recent studies, as reported in Sec-
tion 2.2, seem to show that current resolution of
discourse references in RTE systems hardly af-
fects performance. However, our intuition is that
these results can be attributed to four major lim-
itations shared by these studies: (1) the datasets,
where discourse phenomena were not well repre-
3http://opennlp.sourceforge.net
sented; (2) the off-the-shelf coreference resolution
systems which may have been not robust enough;
(3) the limitation to nominal coreference; and (4)
overly simple integration of reference information
into the inference engines.
The goal of this paper is to assess the impact of
discourse references on entailment with an anno-
tation study which removes these limitations. To
counteract (1), we use the recent RTE-5 Search
dataset (details below). To avoid (2), we perform
a manual analysis, assuming discourse references
as predicted by an oracle. With regards to (3), our
annotation scheme covers coreference and bridg-
ing relations of all syntactic categories and classi-
fies them. As for (4), we suggest several opera-
tions necessary to integrate the discourse informa-
tion into an entailment engine.
In contrast to the numerous existing datasets
annotated for discourse references (Hovy et al,
2006; Strassel et al, 2008), we do not annotate ex-
haustively. Rather, we are interested specifically in
those references instances that impact inference.
Furthermore, we analyze each instance from an
entailment perspective, characterizing the relevant
factors that have an impact on inference. To our
knowledge, this is the first such in-depth study.4
The results of our study are of twofold interest.
First, they provide guidance for the developers of
reference resolvers who might prioritize the scope
of their systems to make them more valuable for
inference. Second, they point out potential direc-
tions for the developers of inference systems by
specifying what additional inference mechanisms
are needed to utilize discourse information.
The RTE-5 Search dataset. We base our anno-
tation on the Search task dataset, a new addition
to the recent Fifth RTE challenge (Bentivogli et
al., 2009a) that is motivated by the needs of NLP
applications and drawn from the TAC summariza-
tion track. In the Search task, TE systems are re-
quired to find all individual sentences in a given
corpus which entail the hypothesis ? a setting that
is sensible not only for summarization, but also for
information access tasks like QA. Sentences are
judged individually, but ?are to be interpreted in
the context of the corpus as they rely on explicit
and implicit references to entities, events, dates,
places, etc., mentioned elsewhere in the corpus?
(Bentivogli et al, 2009b).
4The guidelines and the dataset are available at
http://www.cs.biu.ac.il/? nlp/downloads/
1211
Text Hypothesis
i T
? Once the reform becomes law, Spain will join the Netherlands
and Belgium in allowing homosexual marriages. Massachusetts allows homosexual
T Such unions are also legal in six Canadian provinces and the
northeastern US state of Massachusetts.
marriages
T ? The official name of 2003 UB313 has yet to be determined.
ii T Brown said he expected to find a moon orbiting Xena because
many Kuiper Belt objects are paired with moons.
2003 UB313 is in the Kuiper Belt
iii
T ?a
All seven aboard the AS-28 submarine appeared to be in satis-
factory condition, naval spokesman said.
T ?b
British crews were working with Russian naval authorities to ma-
neuver the unmanned robotic vehicle and untangle the AS-28.
The AS-28 mini submarine was trapped
underwater
T The Russian military was racing against time early Friday to res-
cue a mini submarine trapped on the seabed.
iv T
? China seeks solutions to its coal mine safety. A mining accident in China has killed
several minersT A recent accident has cost more than a dozen miners their lives.
v
T ??
A remote-controlled device was lowered to the stricken vessel to
cut the cables in which the AS-28 vehicle is caught.
T ?
The mini submarine was resting on the seabed at a depth of about
200 meters.
The AS-28 mini submarine was trapped
underwater
T Specialists said it could have become tangled up with a metal
cable or in sunken nets from a fishing trawler.
vi T . . . dried up lakes in Siberia, because the permafrost beneaththem has begun to thaw.
The ice is melting in the Arctic
Table 1: Examples for discourse-dependent entailment in the RTE-5 dataset, where the inference of H
depends on reference information from the discourse sentences T ? / T ??. Referring terms (in T ) and target
terms (in H) are shown in boldface.
4 Analysis Scheme
For annotating the RTE-5 data, we operationalize
reference relations that are relevant for entailment
as those that improve coverage. Recall from Sec-
tion 2.2 that the concept of coverage is applicable
to both transformation and alignment models, all
of which aim at maximizing coverage of H by T .
We represent T and H as syntactic trees, as
common in the RTE literature (Zanzotto et al,
2009; Agichtein et al, 2008). Specifically, we
assume MINIPAR-style (Lin, 1993) dependency
trees where nodes represent text expressions and
edges represent the syntactic relations between
them. We use ?term? to refer to text expressions,
and ?components? to refer to nodes, edges, and
subtrees. Dependency trees are a popular choice
in RTE since they offer a fairly semantics-oriented
account of the sentence structure that can still be
constructed robustly. In an ideal case of entail-
ment, all nodes and dependency edges of H are
covered by T .
For each T ? H pair, we annotate all relevant
discourse references in terms of three items: the
target component in H , the focus term in T , and
the reference term which stands in a reference re-
lation to the focus term. By resolving this ref-
erence, the target component can usually be in-
ferred; sometimes, however, more than one ref-
erence term needs to be found. We now define
and illustrate these concepts on examples from
Table 1.5
The target component is a tree component in
H that cannot be covered by the ?local? material
from T . An example for a tree component is Ex-
ample (v), where the target component AS-28 mini
submarine in H cannot be inferred from the pro-
noun it in T . Example (vi) demonstrates an edge
as target component. In this case, the edge in H
connecting melt with the modifier in the Arctic is
not found in T . Although each of the hypothesis?
nodes can be covered separately via knowledge-
based rules (e.g. ?Siberia ? Arctic?, ?permafrost
? ice?, ?thaw ? melt?), the resulting fragments
in T are unconnected without the (intra-sentential)
coreference between them and lakes in Siberia.
For each target component, we identify its focus
term as the expression in T that does not cover the
target component itself but participates in a refer-
ence relation that can help covering it.
We follow the focus term?s reference chain to
a reference term which can, either separately or
in combination with the focus term, help covering
the target component. In Example (ii), where the
5In our annotation, we assume throughout that some
knowledge about basic admissible transformations is avail-
able, such as passive to active or derivational transformations;
for brevity, we ignore articles in the examples and treat named
entities as single nodes.
1212
target component in H is 2003 UB313, Xena is the
focus term in T and the reference term is a men-
tion of 2003 UB313 in a previous sentence, T ?. In
this case, the reference term covers the entire tar-
get component on its own.
An additional attribute that we record for each
instance is whether resolving the discourse refer-
ence is mandatory for determining entailment, or
optional. In Example (v), it is mandatory: the in-
ference cannot be completed without the knowl-
edge provided by the discourse. In contrast, in
Example (ii), inferring 2003 UB313 from Xena
is optional. It can be done either by identify-
ing their coreference relation, or by using back-
ground knowledge in the form of an entailment
rule, ?Xena ? 2003 UB313?, that is applicable
in the context of astronomy. Optional discourse
references represent instances where discourse in-
formation and TE knowledge are interchange-
able. As mentioned, knowledge gaps constitute
a major obstacle for TE systems, and we can-
not rely on the availability of any ceratin piece of
knowledge to the inference process. Thus, in our
scheme, mandatory references provide a ?lower
bound? with regards to the necessity to resolve
discourse references, even in the presence of com-
plete knowledge; optional references, on the other
hand, set an ?upper bound? for the contribution of
discourse resolution to inference, when no knowl-
edge is available. At the same time, this scheme
allows investigating how much TE knowledge can
be replaced by (perfect) discourse processing.
When choosing a reference term, we search the
reference chain of the focus term for the nearest
expression that is identical to the target component
or a subcomponent of it. If we find such an expres-
sion, covering the identical part of the target com-
ponent requires no entailment knowledge. If no
identical reference term exists, we choose the se-
mantically ?closest? term from the reference chain,
i.e. the term which requires the least knowledge to
infer the target component. For instance, we may
pick permafrost as the semantically closet term to
the target ice if the latter is not found in the focus
term?s reference chain.
Finally, for each reference relation that we an-
notate, we record four additional attributes which
we assumed to be informative in an evaluation.
First, the reference type: Is the relation a coref-
erence or a bridging reference? Second, the syn-
tactic type of the focus and reference terms. Third,
the focus/reference terms entailment status ? does
some kind of entailment relation hold between the
two terms? Fourth, the operation that should be
performed on the focus and reference terms to ob-
tain coverage of the target component (as specified
in Section 5).
5 Integrating Discourse References into
Entailment Recognition
In initial analysis we found that the standard sub-
stitution operation applied by virtually all previous
studies for integrating coreference into entailment
is insufficient. We identified three distinct cases
for the integration of discourse reference knowl-
edge in entailment, which correspond to different
relations between the target component, the fo-
cus term and the reference term. This section de-
scribes the three cases and characterizes them in
terms of tree transformations. An initial version of
these transformations is described in (Abad et al,
2010). We assume a transformation-based entail-
ment architecture (cf. Section 2.2), although we
believe that the key points of our account are also
applicable to alignment-based architecture. Trans-
formations create revised trees that cover previ-
ously uncovered target components in H . The
output of each transformation, T1, is comprised
of copies of the components used to construct it,
and is appended to the discourse forest, which in-
cludes the dependency trees of all sentences and
their generated consequents.
We assume that we have access to a dependency
tree for H , a dependency forest for T and its dis-
course context, as well as the output of a perfect
discourse processor, i.e., a complete set of both
coreference and bridging relations, including the
type of bridging relation (e.g. part-of, cause).
We use the following notation. We use x, y
for tree nodes, and Sx to denote a (sub-)tree with
root x. lab(x) is the label of the incoming edge
of x (i.e., its grammatical function). We write
C(x, y) for a coreference relation between Sx and
Sy, the corresponding trees of the focus and refer-
ence terms, respectively. We write Br(x, y) for a
bridging relation, where r is its type.
(1) Substitution: This is the most intuitive and
widely-used transformation, corresponding to the
treatment of discourse information in existing sys-
tems. It applies to coreference relations, when an
expression found elsewhere in the text (the refer-
ence term) can cover all missing information (the
1213
be lega
l als
o
unio
n
suc
h
pred
mod
subj
be lega
l
also
mar
riage
s
hom
ose
xua
lpre
d mod
subj
mod
T
T 1
mar
riage
s
hom
ose
xua
l
modT?
pre
Figure 1: The Substitution transformation, demon-
strated on the relevant subtrees of Example (i).
The dashed line denotes a discourse reference.
target component) on its own. In such cases, the
reference term can replace the entire focus term.
Apparently (cf. Section 6), substitution applies
also to some types of bridging relations, such as
set-membership, when the member is sufficient for
representing the entire set for the necessary infer-
ence. For example, in ?I met two people yesterday.
The woman told me a story.? (Clark, 1975), sub-
stituting two people with woman results in a text
which is entailed from the discourse, and which
allows inferring ?I met a woman yesterday.?
In a parse tree representation, given a corefer-
ence relation C(x, y) (or Br(x, y)), the newly gen-
erated tree, T1, consists of a copy of T , where the
entire tree Sx is replaced by a copy of Sy . In Fig-
ure 1, which shows Example (i) from Table 1, such
unions is substituted by homosexual marriages.
Head-substitution. Occasionally, substituting
only the head of the focus term is sufficient. In
such cases, only the root nodes x and y are sub-
stituted. This is the case, for example, with syn-
onymous verbs with identical subcategorization
frames (like melt and thaw). As verbs typically
constitute tree roots in dependency parses, sub-
stituting or merging (see below) their entire trees
might be inappropriate or wasteful. In such cases,
the simpler head-substitution may be applied.
(2) Merge: In contrast to substitution, where a
match for the entire target component is found
elsewhere in the text, this transformation is re-
quired when parts of the missing information are
scattered among multiple locations in the text.
We distinguish between two types of merge trans-
formations: (a) dependent-merge, and (b) head-
merge, depending on the syntactic roles of the
merged components.
(a) Dependent-Merge. This operation is ap-
plicable when the head of either the focus or ref-
erence terms (of both) matches the head node of
subm
arine
mini
ontrapp
ed mod
T
T 1
subm
arine AS-2
8nnT? a
pcom
p-n
pnm
od
mod
sea
bed
subm
arine
mini
trapp
ed
mod
pnm
od
mod
AS-2
8nn
AS-2
8  
T? b
on
pcom
p-n sea
bed
Figure 2: The dependent-merge (T ?a) and head-
merge (T ?b) transformations (Example (iii)).
the target component, but modifiers from both of
them are required to cover the target component?s
dependents. The modifiers are therefore merged
as dependents of a single head node, to create
a tree that covers the entire target component.
Dependent-merge is illustrated in Figure 2, using
Example (iii). The component we wish to cover in
H is the noun phrase AS-28 mini submarine. Un-
fortunately, the focus term in T , ?mini submarine
trapped on the seabed?, covers only the modifier
mini, but not AS-28. This modifier can however be
provided by the coreferent term in T ?a (left upper
corner). Once merged, the inference engine can,
e.g., employ the rule ?on seabed ? underwater?
to cover H completely.
Formally, assume without loss of generality that
y, the reference term?s head, matches the root node
of the target component. Given C(x, y), we define
T1 as a copy of T , where (i) the subtree Sx is re-
placed by Sy, and (ii) for all children c of x, a copy
of Sc is placed under the copy of y in T1 with its
original edge label, lab(c).
(b) Head-merge. An alternative way to recover
the missing information in Example (iii) is to find
a reference term whose head word itself (rather
than one of its modifiers) matches the target com-
ponent?s missing dependent, as with AS-28 in Fig-
ure 2 in the bottom left corner (T ?b). In terms of
parse trees, we need to add one tree as a depen-
dent of the other. Formally, given C(x, y), simi-
larly to dependent-merge, T1 is created as a copy
of T where the subtree Sx is replaced by either Sx
or Sy, depending on whichever of x and y matches
the target component?s head. Assume it is x, for
example. Then, a copy of Sy is added as a new
child to x. In our sample, head-merge operations
correspond to internal coreferences within nomi-
nal target components (such as between AS-28 and
mini submarine in this case). The appropriate la-
bel, lab(y), in these cases is nn (nominal modi-
1214
in
T
T 1
T?
pcom
p-n Chin
acost have
thanmore comp1 pcomp
-nobj
have
doze
n
acc
identsubj recen
t
mod
cos
t have
thanmore co
mp1 pcom
p-n
obj
have
doze
n
acc
identsubj recen
tmod
mod
Solut
ionseek
Chin
a
tomod pco
mp-n
safet
y coa
lmin
e
nn
nn
itsgenobj
subj
Figure 3: The insertion transformation. Dotted
edges mark the newly inserted path (Ex. (iv)).
fier). Further analysis is required to specify what
other dependencies can hold between such core-
ferring heads.
(3) Insertion: The last transformation, insertion,
is used when a relation that is realized in H is
missing from T and is only implied via a bridg-
ing relation. In Example (iv), the location that is
explicitly mentioned in H can only be covered by
T by resolving a bridging reference with China
in T ?. To connect the bridging referents, a new
tree component representing the bridging relation
is inserted into the consequent tree T1. In this ex-
ample, the component connects China and recent
accident via the in preposition. Formally, given
a bridging relation Br(x, y), we introduce a new
subtree Srz into T1, where z is a child of x and
lab(z) = labr. Srz must contain a variable node
that is instantiated with a copy of S(y).
This transformation stands out from the others
in that it introduces new material. For each bridg-
ing relation, it adds a specific subtrees Sr via an
edge labeled with labr. These two items form the
dependency representation of the bridging relation
Br and must be provided by the interface between
the discourse and the inference systems. Clearly,
their exact form depends on the set of bridging re-
lations provided by the discourse resolver as well
as the details of the dependency parses.
As shown in Figure 3, the bridging relation
located-in (r) is represented by inserting a subtree
Srz headed by in (z) into T1 and connecting it to
accident (x) as a modifier (labr). The subtree Srz
consists of a variable node which is connected to
in with a pcomp-n dependency (a nominal head of
a prepositional phrase), and which is instantiated
with the node China (y) when the transformation
is applied. Note that the structure of Srz and the
way it is inserted into T1 are predefined by the
abovementioned interface; only the node to which
it is attached and the contents of the variable node
are determined at transformation-time.
As another example, consider the following
short text from (Clark, 1975): John was murdered
yesterday. The knife lay nearby. Here, the bridg-
ing relation between the murder event and the in-
strument, the knife (x), can be addressed by in-
serting under x a subtree for the clause with which
as Srz , with a variable which is instantiated by the
parse-tree (headed by murdered, y) of the entire
first sentence John was murdered yesterday.
Transformation chaining. Since our transfor-
mations are defined to be minimal, some cases re-
quire the application of multiple transformations
to achieve coverage. Consider Example (v), Ta-
ble 1. We wish to cover AS-28 mini submarine in
H from the coreferring it in T , mini submarine in
T ? and AS-28 vehicle in T ??. A substitution of it by
either coreference does not suffice, since none of
the antecedents contains all necessary modifiers. It
is therefore necessary to substitute it first by one of
the coreferences and then merge it with the other.
6 Results
We analyzed 120 sentence-hypothesis pairs of the
RTE-5 development set (21 different hypotheses,
111 distinct sentences, 53 different documents).
Below, we summarize our findings, focusing on
the relation between our findings and the assump-
tions of previous studies as discussed in Section 3.
General statistics. We found that 44% of the
pairs contained reference relations whose resolu-
tion was mandatory for inference. In another 28%,
references could optionally support the inference
of the hypothesis. In the remaining 28%, refer-
ences did not contribute towards inference. The
total number of relevant references was 137, and
37 pairs (27%) contained multiple relevant refer-
ences. These numbers support our assumption that
discourse references play an important role in in-
ference.
Reference types. 73% of the identified refer-
ences are coreferences and 27% are bridging re-
lations. The most common bridging relation was
the location of events (e.g. Arctic in ice melting
events), generally assumed to be known through-
out the document. Other bridging relations we en-
countered include cause (e.g. between injured and
attack), event participants and set membership.
1215
(%) Pronoun NE NP VP
Focus term 9 19 49 23
Reference term - 43 43 14
Table 2: Syntactic types of discourse references
(%) Sub. Merge Insertion
Coreference 62 38 -
Bridging 30 - 70
Total 54 28 18
Table 3: Distribution of transformation types
Syntactic types. Table 2 shows that 77% of all
focus terms and 86% of the reference terms were
nominal phrases, which justifies their prominent
position in work on anaphora and coreference res-
olution. However, almost a quarter of the focus
terms were verbal phrases. We found these focus
terms to be frequently crucial for entailment since
they included the main predicate of the hypothe-
sis.6 This calls for an increased focus on the reso-
lution of event references.
Transformations. Table 3 shows the relative
frequencies of all transformations. Again, we
found that the ?default? transformation, substitu-
tion, is the most frequent one, and is helpful for
both coreference and bridging relations. Substitu-
tion is particularly useful for handling pronouns
(14% of all substitution instances), the replace-
ment of named entities by synonymous names
(32%), the replacement of other NPs (38%), and
the substitution of verbal head nodes in event
coreference (16%). Yet, in nearly half the cases,
a different transformation had to be applied. In-
sertion accounts for the majority of bridging cases.
Head-merge is necessary to integrate proper nouns
as modifiers of other head nouns. Dependent-
merge, responsible for 85% of the merge transfor-
mations, can be used to complete nominal focus
terms with missing modifiers (e.g., adjectives), as
well as for merging other dependencies between
coreferring predicates. This result indicates the
importance of incorporating other transformations
into inference systems.
Distance of reference terms. The distance be-
tween the focus and the reference terms varied
considerably, ranging from intra-sentential refer-
ence relations and up to several dozen sentences.
For more than a quarter of the focus terms, we
6The lower proportion of VPs among reference terms
stems from bridging relations between VPs and nominal de-
pendents, such as the abovementioned ?location? relation.
had to go to other documents to find reference
terms that, possibly in conjunction with the focus
term, could cover the target components. Interest-
ingly, all such cases involved coreference (about
equally divided between the merge transforma-
tions and substitutions), while bridging was al-
ways ?document-local?. This result reaffirms the
usefulness of cross-document coreference resolu-
tion for inference (Huang et al, 2009).
Discourse resolution as preprocessing? In ex-
isting RTE systems, discourse references are typ-
ically resolved as a preprocessing step. While
our annotation was manual and cannot yield di-
rect results about processing considerations, we
observed that discourse relations often hold be-
tween complex, and deeply embedded, expres-
sions, which makes their automatic resolution dif-
ficult. Of course, many RTE systems attempt to
normalize and simplify H and T , e.g., by split-
ting conjunctions or removing irrelevant clauses,
but these operations are usually considered a part
of the inference rather the preprocessing phase (cf.
e.g., Bar-Haim et al (2007)). Since the resolu-
tion of discourse references is likely to profit from
these steps, it seems desirable to ?postpone? it un-
til after simplification. In transformation-based
systems, it might be natural to add discourse-based
transformations to the set of inference operations,
while in alignment-based systems, discourse ref-
erences can be integrated into the computation of
alignment scores.
Discourse references vs. entailment knowledge.
We have stated before that even if a discourse ref-
erence is not strictly necessary for entailment, it
may be interesting because it represents an alter-
native to the use of knowledge rules to cover the
hypothesis. Sometimes, these rules are generally
applicable (e.g., ?Alaska? Arctic?). However, of-
ten they are context-specific. Consider the follow-
ing sentence as T for the hypothesis H: ?The ice
is melting in the Arctic?:
(3) T : ?The scene at the receding edge of the Exit
Glacier was part festive gathering, part nature
tour with an apocalyptic edge.?
While it is possible to cover melting using a rule
?melting? receding?, this rule is only valid under
quite specific conditions (e.g., for the subject ice).
Instead of determining the applicability of the rule,
a discourse-aware system can take the next sen-
1216
tence into account, which contains a coreferring
event to receding that can cover melting in H:
(4) T ?: ?. . . people moved closer to the rope line
near the glacier as it shied away, practically
groaning and melting before their eyes.?
Discourse relations can in fact encode arbitrar-
ily complex world knowledge, as in the following
pair:
(5) H: ?The serial killer BTK was accused of at
least 7 killings starting in the 1970?s.?
T: ?Police say BTK may have killed as many
as 10 people between 1974 and 1991.?
Here, the H modifier serial, which does not occur
in T , can be covered either by world knowledge
(a person who killed 10 people is a serial killer),
or by resolving the coreference of BTK to the term
the serial killer BTK which occurs in the discourse
around T . Our conclusion is that not only can
discourse references often replace world knowl-
edge in principle, in practice it often seems easier
to resolve discourse references than to determine
whether a rule is applicable in a given context or
to formalize complex world knowledge as infer-
ence rules. Our annotation provides further em-
pirical support to this claim: An entailment rela-
tion exists between the focus and reference terms
in 60% of the focus-reference term pairs, and in
many of the remainder, entailment holds between
the terms? heads. Thus, discourse provides rela-
tions which are many times equivalent to entail-
ment knowledge rules and can therefore be uti-
lized in their stead.
7 Conclusions
This work has presented an analysis of the relation
between discourse references and textual entail-
ment. We have identified a set of limitations com-
mon to the handling of discourse relations in vir-
tually all entailment systems. They include the use
of off-the-shelf resolvers that concentrate on nom-
inal coreference, the integration of reference in-
formation through substitution, and the RTE eval-
uation schemes, which played down the role of
discourse. Since in practical settings, discourse
plays an important role, our goal was to develop
an agenda for improving the handling of discourse
references in entailment-based inference.
Our manual analysis of the RTE-5 dataset
shows that while the majority of discourse refer-
ences that affect inference are nominal coreference
relations, another substantial part is made up by
verbal terms and bridging relations. Furthermore,
we have demonstrated that substitution alone is in-
sufficient to extract all relevant information from
the wide range of discourse references that are
frequently relevant for inference. We identified
three general cases, and suggested matching op-
erations to obtain the relevant inferences, formu-
lated as tree transformations. Furthermore, our ev-
idence suggests that for practical reasons, the res-
olution of discourse references should be tightly
integrated into entailment systems instead of treat-
ing it as a preprocessing step.
A particularly interesting result concerns the
interplay between discourse references and en-
tailment knowledge. While semantic knowledge
(e.g., from WordNet or Wikipedia) has been used
beneficially for coreference resolution (Soon et al,
2001; Ponzetto and Strube, 2006), reference res-
olution has, to our knowledge, not yet been em-
ployed to validate entailment rules? applicability.
Our analyses suggest that in the context of de-
ciding textual entailment, reference resolution and
entailment knowledge can be seen as complemen-
tary ways of achieving the same goal, namely en-
riching T with additional knowledge to allow the
inference of H . Given that both of the technolo-
gies are still imperfect, we envisage the way for-
ward as a joint strategy, where reference resolution
and entailment rules mutually fill each other?s gaps
(cf. Example 3).
In sum, our study shows that textual entailment
can profit substantially from better discourse han-
dling. The next challenge is to translate the the-
oretical gain into practical benefit. Our analy-
sis demonstrates that improvements are necessary
both on the side of discourse reference resolution
systems, which need to cover more types of refer-
ences, as well as a better integration of discourse
information in entailment systems, even for those
relations which are within the scope of available
resolvers.
Acknowledgements
This work was partially supported by the
PASCAL-2 Network of Excellence of the Eu-
ropean Community FP7-ICT-2007-1-216886 and
the Israel Science Foundation grant 1112/08.
1217
References
Azad Abad, Luisa Bentivogli, Ido Dagan, Danilo Gi-
ampiccolo, Shachar Mirkin, Emanuele Pianta, and
Asher Stern. 2010. A resource for investigating the
impact of anaphora and coreference on inference. In
Proceedings of LREC.
Rod Adams, Gabriel Nicolae, Cristina Nicolae, and
Sanda Harabagiu. 2007. Textual entailment through
extended lexical overlap and lexico-semantic match-
ing. In Proceedings of the ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing.
E. Agichtein, W. Askew, and Y. Liu. 2008. Combining
lexical, syntactic, and semantic evidence for textual
entailment classification. In Proceedings of TAC.
Nicholas Asher and Alex Lascarides. 1998. Bridging.
Journal of Semantics, 15(1):83?113.
Alexandra Balahur, Elena Lloret, O?scar Ferra?ndez,
Andre?s Montoyo, Manuel Palomar, and Rafael
Mun?oz. 2008. The DLSIUAES team?s participation
in the TAC 2008 tracks. In Proceedings of TAC.
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI.
Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo
Greental, Shachar Mirkin, and Eyal Shnarch amd
Idan Szpektor. 2008. Efficient semantic deduc-
tion and approximate matching over compact parse
forests. In Proceedings of TAC.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009a. The
fifth pascal recognizing textual entailment chal-
lenge. In Proceedings of TAC.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, Medea Lo Leggio, and Bernardo
Magnini. 2009b. Considering discourse references
in textual entailment annotation. In Proceedings of
the 5th International Conference on Generative Ap-
proaches to the Lexicon (GL2009).
Johan Bos. 2005. Recognising textual entailment with
logical inference. In Proceedings of EMNLP.
Aljoscha Burchardt, Marco Pennacchiotti, Stefan
Thater, and Manfred Pinkal. 2009. Assessing
the impact of frame semantics on textual entail-
ment. Journal of Natural Language Engineering,
15(4):527?550.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings of ACL-IJCNLP.
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh,
and Christopher D. Manning. 2007. Learning align-
ments and leveraging natural logic. In Proceedings
of the ACL-PASCAL Workshop on Textual Entail-
ment and Paraphrasing.
Herbert H. Clark. 1975. Bridging. In R. C. Schank
and B. L. Nash-Webber, editors, Theoretical issues
in natural language processing, pages 169?174. As-
sociation of Computing Machinery.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Machine Learning Challenges, vol-
ume 3944 of Lecture Notes in Computer Science,
pages 177?190. Springer.
Lorand Dali, Delia Rusu, Blaz Fortuna, Dunja
Mladenic, and Marko Grobelnik. 2009. Ques-
tion answering based on semantic graphs. In Pro-
ceedings of the Workshop on Semantic Search (Sem-
Search 2009).
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, and Bill Dolan. 2008. The
fourth pascal recognizing textual entailment chal-
lenge. In Proceedings of TAC.
Ralph Grishman and Beth Sundheim. 1996. Mes-
sage Understanding Conference-6: a brief history.
In Proceedings of the 16th conference on Computa-
tional Linguistics.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2007. Satisfying information needs with multi-
document summaries. Information Processing &
Management, 43:1619?1642.
Stefan Harmeling. 2009. Inferring textual entailment
with a probabilistically sound calculus. Journal of
Natural Language Engineering, pages 459?477.
Marti A. Hearst. 1997. Segmenting text into multi-
paragraph subtopic passages. Computational Lin-
guistics, 23(1):33?64.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of HLT-NAACL.
Jian Huang, Sarah M. Taylor, Jonathan L. Smith, Kon-
stantinos A. Fotiadis, and C. Lee Giles. 2009. Pro-
file based cross-document coreference using kernel-
ized fuzzy relational clustering. In Proceedings of
ACL-IJCNLP.
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering opinion questions with
random walks on graphs. In Proceedings of ACL-
IJCNLP.
1218
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 4(7):343?360.
Dekang Lin. 1993. Principle-based parsing without
overgeneration. In Proceedings of ACL.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
EMNLP.
Katja Markert, Malvina Nissim, and Natalia N. Mod-
jeska. 2003. Using the web for nominal anaphora
resolution. In Proceedings of EACL Workshop on
the Computational Treatment of Anaphora.
Seung-Hoon Na and Hwee Tou Ng. 2009. A 2-poisson
model for probabilistic coreference of named enti-
ties for improved text retrieval. In Proceedings of
SIGIR.
Rodney D. Nielsen, Wayne Ward, and James H. Mar-
tin. 2009. Recognizing entailment in intelligent
tutoring systems. Natural Language Engineering,
15(4):479?501.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridging
references. In Proceedings of ACL.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceed-
ings of HLT.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2004. A
public reference implementation of the rap anaphora
resolution algorithm. In Proceedings of LREC.
Lorenza Romano, Milen Kouylekov, Idan Szpektor,
Ido Dagan, and Alberto Lavelli. 2006. Investigat-
ing a generic paraphrase-based approach for relation
extraction. In Proceedings of EACL.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Stephanie Strassel, Mark Przybocki, Kay Peterson,
Zhiyi Song, and Kazuaki Maeda. 2008. Linguistic
resources and evaluation techniques for evaluation
of cross-document automatic content extraction. In
Proceedings of LREC.
Jose L. Vicedo and Antonio Ferrndez. 2006. Coref-
erence in Q&A. In Tomek Strzalkowski and
Sanda M. Harabagiu, editors, Advances in Open Do-
main Question Answering, pages 71?96. Springer.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Alessandro Moschitti. 2009. A machine learning
approach to textual entailment recognition. Journal
of Natural Language Engineering, 15(4):551?582.
Dmitry Zelenko, Chinatsu Aone, and Jason Tibbetts.
2004. Coreference resolution for information ex-
traction. In Proceedings of the ACL Workshop on
Reference Resolution and its Applications.
1219
Proceedings of the ACL 2010 Conference Short Papers, pages 92?97,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Exemplar-Based Models for Word Meaning In Context
Katrin Erk
Department of Linguistics
University of Texas at Austin
katrin.erk@mail.utexas.edu
Sebastian Pado?
Institut fu?r maschinelle Sprachverarbeitung
Stuttgart University
pado@ims.uni-stuttgart.de
Abstract
This paper describes ongoing work on dis-
tributional models for word meaning in
context. We abandon the usual one-vector-
per-word paradigm in favor of an exemplar
model that activates only relevant occur-
rences. On a paraphrasing task, we find
that a simple exemplar model outperforms
more complex state-of-the-art models.
1 Introduction
Distributional models are a popular framework
for representing word meaning. They describe
a lemma through a high-dimensional vector that
records co-occurrence with context features over a
large corpus. Distributional models have been used
in many NLP analysis tasks (Salton et al, 1975;
McCarthy and Carroll, 2003; Salton et al, 1975), as
well as for cognitive modeling (Baroni and Lenci,
2009; Landauer and Dumais, 1997; McDonald and
Ramscar, 2001). Among their attractive properties
are their simplicity and versatility, as well as the
fact that they can be acquired from corpora in an
unsupervised manner.
Distributional models are also attractive as a
model of word meaning in context, since they do
not have to rely on fixed sets of dictionary sense
with their well-known problems (Kilgarriff, 1997;
McCarthy and Navigli, 2009). Also, they can
be used directly for testing paraphrase applicabil-
ity (Szpektor et al, 2008), a task that has recently
become prominent in the context of textual entail-
ment (Bar-Haim et al, 2007). However, polysemy
is a fundamental problem for distributional models.
Typically, distributional models compute a single
?type? vector for a target word, which contains co-
occurrence counts for all the occurrences of the
target in a large corpus. If the target is polyse-
mous, this vector mixes contextual features for all
the senses of the target. For example, among the
top 20 features for coach, we get match and team
(for the ?trainer? sense) as well as driver and car
(for the ?bus? sense). This problem has typically
been approached by modifying the type vector for
a target to better match a given context (Mitchell
and Lapata, 2008; Erk and Pado?, 2008; Thater et
al., 2009).
In the terms of research on human concept rep-
resentation, which often employs feature vector
representations, the use of type vectors can be un-
derstood as a prototype-based approach, which uses
a single vector per category. From this angle, com-
puting prototypes throws away much interesting
distributional information. A rival class of mod-
els is that of exemplar models, which memorize
each seen instance of a category and perform cat-
egorization by comparing a new stimulus to each
remembered exemplar vector.
We can address the polysemy issue through an
exemplar model by simply removing all exem-
plars that are ?not relevant? for the present con-
text, or conversely activating only the relevant
ones. For the coach example, in the context of
a text about motorways, presumably an instance
like ?The coach drove a steady 45 mph? would be
activated, while ?The team lost all games since the
new coach arrived? would not.
In this paper, we present an exemplar-based dis-
tributional model for modeling word meaning in
context, applying the model to the task of decid-
ing paraphrase applicability. With a very simple
vector representation and just using activation, we
outperform the state-of-the-art prototype models.
We perform an in-depth error analysis to identify
stable parameters for this class of models.
2 Related Work
Among distributional models of word, there are
some approaches that address polysemy, either
by inducing a fixed clustering of contexts into
senses (Schu?tze, 1998) or by dynamically modi-
92
fying a word?s type vector according to each given
sentence context (Landauer and Dumais, 1997;
Mitchell and Lapata, 2008; Erk and Pado?, 2008;
Thater et al, 2009). Polysemy-aware approaches
also differ in their notion of context. Some use a
bag-of-words representation of words in the cur-
rent sentence (Schu?tze, 1998; Landauer and Du-
mais, 1997), some make use of syntactic con-
text (Mitchell and Lapata, 2008; Erk and Pado?,
2008; Thater et al, 2009). The approach that we
present in the current paper computes a representa-
tion dynamically for each sentence context, using
a simple bag-of-words representation of context.
In cognitive science, prototype models predict
degree of category membership through similar-
ity to a single prototype, while exemplar theory
represents a concept as a collection of all previ-
ously seen exemplars (Murphy, 2002). Griffiths et
al. (2007) found that the benefit of exemplars over
prototypes grows with the number of available ex-
emplars. The problem of representing meaning in
context, which we consider in this paper, is closely
related to the problem of concept combination in
cognitive science, i.e., the derivation of representa-
tions for complex concepts (such as ?metal spoon?)
given the representations of base concepts (?metal?
and ?spoon?). While most approaches to concept
combination are based on prototype models, Voor-
spoels et al (2009) show superior results for an
exemplar model based on exemplar activation.
In NLP, exemplar-based (memory-based) mod-
els have been applied to many problems (Daele-
mans et al, 1999). In the current paper, we use an
exemplar model for computing distributional repre-
sentations for word meaning in context, using the
context to activate relevant exemplars. Comparing
representations of context, bag-of-words (BOW)
representations are more informative and noisier,
while syntax-based representations deliver sparser
and less noisy information. Following the hypothe-
sis that richer, topical information is more suitable
for exemplar activation, we use BOW representa-
tions of sentential context in the current paper.
3 Exemplar Activation Models
We now present an exemplar-based model for
meaning in context. It assumes that each target
lemma is represented by a set of exemplars, where
an exemplar is a sentence in which the target occurs,
represented as a vector. We use lowercase letters
for individual exemplars (vectors), and uppercase
Sentential context Paraphrase
After a fire extinguisher is used, it must
always be returned for recharging and
its use recorded.
bring back (3),
take back (2),
send back (1),
give back (1)
We return to the young woman who is
reading the Wrigley?s wrapping paper.
come back (3),
revert (1), revisit
(1), go (1)
Table 1: The Lexical Substitution (LexSub) dataset.
letters for sets of exemplars.
We model polysemy by activating relevant ex-
emplars of a lemma E in a given sentence context
s. (Note that we use E to refer to both a lemma
and its exemplar set, and that s can be viewed as
just another exemplar vector.) In general, we define
activation of a set E by exemplar s as
act(E, s) = {e ? E | sim(e, s) > ?(E, s)}
where E is an exemplar set, s is the ?point of com-
parison?, sim is some similarity measure such as
Cosine or Jaccard, and ?(E, s) is a threshold. Ex-
emplars belong to the activated set if their similarity
to s exceeds ?(E, s).1 We explore two variants of
activation. In kNN activation, the k most simi-
lar exemplars to s are activated by setting ? to the
similarity of the k-th most similar exemplar. In
q-percentage activation, we activate the top q%
of E by setting ? to the (100-q)-th percentile of the
sim(e, s) distribution. Note that, while in the kNN
activation scheme the number of activated exem-
plars is the same for every lemma, this is not the
case for percentage activation: There, a more fre-
quent lemma (i.e., a lemma with more exemplars)
will have more exemplars activated.
Exemplar activation for paraphrasing. A para-
phrases is typically only applicable to a particular
sense of a target word. Table 1 illustrates this on
two examples from the Lexical Substitution (Lex-
Sub) dataset (McCarthy and Navigli, 2009), both
featuring the target return. The right column lists
appropriate paraphrases of return in each context
(given by human annotators). 2 We apply the ex-
emplar activation model to the task of predicting
paraphrase felicity: Given a target lemma T in a
particular sentential context s, and given a list of
1In principle, activation could be treated not just as binary
inclusion/exclusion, but also as a graded weighting scheme.
However, weighting schemes introduce a large number of
parameters, which we wanted to avoid.
2Each annotator was allowed to give up to three para-
phrases per target in context. As a consequence, the number
of gold paraphrases per target sentence varies.
93
potential paraphrases of T , the task is to predict
which of the paraphrases are applicable in s.
Previous approaches (Mitchell and Lapata, 2008;
Erk and Pado?, 2008; Erk and Pado?, 2009; Thater
et al, 2009) have performed this task by modify-
ing the type vector for T to the context s and then
comparing the resulting vector T ? to the type vec-
tor of a paraphrase candidate P . In our exemplar
setting, we select a contextually adequate subset
of contexts in which T has been observed, using
T ? = act(T, s) as a generalized representation of
meaning of target T in the context of s.
Previous approaches used all of P as a repre-
sentation for a paraphrase candidate P . However,
P includes also irrelevant exemplars, while for a
paraphrase to be judged as good, it is sufficient that
one plausible reading exists. Therefore, we use
P ? = act(P, s) to represent the paraphrase.
4 Experimental Evaluation
Data. We evaluate our model on predicting para-
phrases from the Lexical Substitution (LexSub)
dataset (McCarthy and Navigli, 2009). This dataset
consists of 2000 instances of 200 target words in
sentential contexts, with paraphrases for each tar-
get word instance generated by up to 6 participants.
Paraphrases are ranked by the number of annota-
tors that chose them (cf. Table 1). Following Erk
and Pado? (2008), we take the list of paraphrase can-
didates for a target as given (computed by pooling
all paraphrases that LexSub annotators proposed
for the target) and use the models to rank them for
any given sentence context.
As exemplars, we create bag-of-words co-
occurrence vectors from the BNC. These vectors
represent instances of a target word by the other
words in the same sentence, lemmatized and POS-
tagged, minus stop words. E.g., if the lemma
gnurge occurs twice in the BNC, once in the sen-
tence ?The dog will gnurge the other dog?, and
once in ?The old windows gnurged?, the exemplar
set for gnurge contains the vectors [dog-n: 2, other-
a:1] and [old-a: 1, window-n: 1]. For exemplar
similarity, we use the standard Cosine similarity,
and for the similarity of two exemplar sets, the
Cosine of their centroids.
Evaluation. The model?s prediction for an item
is a list of paraphrases ranked by their predicted
goodness of fit. To evaluate them against a
weighted list of gold paraphrases, we follow Thater
et al (2009) in using Generalized Average Preci-
para- actT actP
meter kNN perc. kNN perc.
10 36.1 35.5 36.5 38.6
20 36.2 35.2 36.2 37.9
30 36.1 35.3 35.8 37.8
40 36.0 35.3 35.8 37.7
50 35.9 35.1 35.9 37.5
60 36.0 35.0 36.1 37.5
70 35.9 34.8 36.1 37.5
80 36.0 34.7 36.0 37.4
90 35.9 34.5 35.9 37.3
no act. 34.6 35.7
random BL 28.5
Table 2: Activation of T or P individually on the
full LexSub dataset (GAP evaluation)
sion (GAP), which interpolates the precision values
of top-n prediction lists for increasing n. Let G =
?q1, . . . , qm? be the list of gold paraphrases with
gold weights ?y1, . . . , ym?. Let P = ?p1, . . . , pn?
be the list of model predictions as ranked by the
model, and let ?x1, . . . , xn? be the gold weights
associated with them (assume xi = 0 if pi 6? G),
where G ? P . Let I(xi) = 1 if pi ? G, and zero
otherwise. We write xi = 1i
?i
k=1 xk for the av-
erage gold weight of the first i model predictions,
and analogously yi. Then
GAP (P,G) =
1
?m
j=1 I(yj)yj
n?
i=1
I(xi)xi
Since the model may rank multiple paraphrases the
same, we average over 10 random permutations of
equally ranked paraphrases. We report mean GAP
over all items in the dataset.
Results and Discussion. We first computed two
models that activate either the paraphrase or the
target, but not both. Model 1, actT , activates only
the target, using the complete P as paraphrase, and
ranking paraphrases by sim(P, act(T, s)). Model
2, actP, activates only the paraphrase, using s as
the target word, ranking by sim(act(P, s), s).
The results for these models are shown in Ta-
ble 2, with both kNN and percentage activation:
kNN activation with a parameter of 10 means that
the 10 closest neighbors were activated, while per-
centage with a parameter of 10 means that the clos-
est 10% of the exemplars were used. Note first
that we computed a random baseline (last row)
with a GAP of 28.5. The second-to-last row (?no
activation?) shows two more informed baselines.
94
The actT ?no act? result (34.6) corresponds to a
prototype-based model that ranks paraphrase can-
didates by the distance between their type vectors
and the target?s type vector. Virtually all exem-
plar models outperform this prototype model. Note
also that both actT and actP show the best results
for small values of the activation parameter. This
indicates paraphrases can be judged on the basis
of a rather small number of exemplars. Neverthe-
less, actT and actP differ with regard to the details
of their optimal activation. For actT , a small ab-
solute number of activated exemplars (here, 20)
works best , while actP yields the best results for
a small percentage of paraphrase exemplars. This
can be explained by the different functions played
by actT and actP (cf. Section 3): Activation of the
paraphrase must allow a guess about whether there
is reasonable interpretation of P in the context s.
This appears to require a reasonably-sized sample
from P . In contrast, target activation merely has to
counteract the sparsity of s, and activation of too
many exemplars from T leads to oversmoothing.
We obtained significances by computing 95%
and 99% confidence intervals with bootstrap re-
sampling. As a rule of thumb, we find that 0.4%
difference in GAP corresponds to a significant dif-
ference at the 95% level, and 0.7% difference in
GAP to significance at the 99% level. The four
activation methods (i.e., columns in Table 2) are
significantly different from each other, with the ex-
ception of the pair actT/kNN and actP/kNN (n.s.),
so that we get the following order:
actP/perc > actP/kNN ? actT/kNN > actT/perc
where > means ?significantly outperforms?. In par-
ticular, the best method (actT/kNN) outperforms
all other methods at p<0.01. Here, the best param-
eter setting (10% activation) is also significantly
better than the next-one one (20% activation). With
the exception of actT/perc, all activation methods
significantly outperform the best baseline (actP, no
activation).
Based on these observations, we computed a
third model, actTP, that activates both T (by kNN)
and P (by percentage), ranking paraphrases by
sim(act(P, s), act(T, s)). Table 3 shows the re-
sults. We find the overall best model at a similar
location in parameter space as for actT and actP
(cf. Table 2), namely by setting the activation pa-
rameters to small values. The sensitivity of the
parameters changes considerably, though. When
P activation (%) ? 10 20 30
T activation (kNN) ?
5 38.2 38.1 38.1
10 37.6 37.8 37.7
20 37.3 37.4 37.3
40 37.2 37.2 36.1
Table 3: Joint activation of P and T on the full
LexSub dataset (GAP evaluation)
we fix the actP activation level, we find compara-
tively large performance differences between the
T activation settings k=5 and k=10 (highly signif-
icant for 10% actP, and significant for 20% and
30% actP). On the other hand, when we fix the
actT activation level, changes in actP activation
generally have an insignificant impact.
Somewhat disappointingly, we are not able to
surpass the best result for actP alone. This indicates
that ? at least in the current vector space ? the
sparsity of s is less of a problem than the ?dilution?
of s that we face when we representing the target
word by exemplars of T close to s. Note, however,
that the numerically worse performance of the best
actTP model is still not significantly different from
the best actP model.
Influence of POS and frequency. An analysis
of the results by target part-of-speech showed that
the globally optimal parameters also yield the best
results for individual POS, even though there are
substantial differences among POS. For actT , the
best results emerge for all POS with kNN activation
with k between 10 and 30. For k=20, we obtain a
GAP of 35.3 (verbs), 38.2 (nouns), and 35.1 (adjec-
tives). For actP, the best parameter for all POS was
activation of 10%, with GAPs of 36.9 (verbs), 41.4
(nouns), and 37.5 (adjectives). Interestingly, the
results for actTP (verbs: 38.4, nouns: 40.6, adjec-
tives: 36.9) are better than actP for verbs, but worse
for nouns and adjectives, which indicates that the
sparsity problem might be more prominent than for
the other POS. In all three models, we found a clear
effect of target and paraphrase frequency, with de-
teriorating performance for the highest-frequency
targets as well as for the lemmas with the highest
average paraphrase frequency.
Comparison to other models. Many of the
other models are syntax-based and are therefore
only applicable to a subset of the LexSub data.
We have re-evaluated our exemplar models on the
subsets we used in Erk and Pado? (2008, EP08, 367
95
Models
EP08 EP09 TDP09
EP08 dataset 27.4 NA NA
EP09 dataset NA 32.2 36.5
actT actP actTP
EP08 dataset 36.5 38.0 39.9
EP09 dataset 39.1 39.9 39.6
Table 4: Comparison to other models on two sub-
sets of LexSub (GAP evaluation)
datapoints) and Erk and Pado? (2009, EP09, 100 dat-
apoints). The second set was also used by Thater et
al. (2009, TDP09). The results in Table 4 compare
these models against our best previous exemplar
models and show that our models outperform these
models across the board. 3 Due to the small sizes
of these datasets, statistical significance is more
difficult to attain. On EP09, the differences among
our models are not significant, but the difference
between them and the original EP09 model is.4 On
EP08, all differences are significant except for actP
vs. actTP.
We note that both the EP08 and the EP09
datasets appear to be simpler to model than the
complete Lexical Substitution dataset, at least by
our exemplar-based models. This underscores an
old insight: namely, that direct syntactic neighbors,
such as arguments and modifiers, provide strong
clues as to word sense.
5 Conclusions and Outlook
This paper reports on work in progress on an ex-
emplar activation model as an alternative to one-
vector-per-word approaches to word meaning in
context. Exemplar activation is very effective in
handling polysemy, even with a very simple (and
sparse) bag-of-words vector representation. On
both the EP08 and EP09 datasets, our models sur-
pass more complex prototype-based approaches
(Tab. 4). It is also noteworthy that the exemplar
activation models work best when few exemplars
are used, which bodes well for their efficiency.
We found that the best target representations re-
3Since our models had the advantage of being tuned on
the dataset, we also report the range of results across the
parameters we tested. On the EP08 dataset, we obtained 33.1?
36.5 for actT; 33.3?38.0 for actP; 37.7-39.9 for actTP. On the
EP09 dataset, the numbers were 35.8?39.1 for actT; 38.1?39.9
for actP; 37.2?39.8 for actTP.
4We did not have access to the TDP09 predictions to do
significance testing.
sult from activating a low absolute number of exem-
plars. Paraphrase representations are best activated
with a percentage-based threshold. Overall, we
found that paraphrase activation had a much larger
impact on performance than target activation, and
that drawing on target exemplars other than s to
represent the target meaning in context improved
over using s itself only for verbs (Tab. 3). This sug-
gests the possibility of considering T ?s activated
paraphrase candidates as the representation of T in
the context s, rather than some vector of T itself,
in the spirit of Kintsch (2001).
While it is encouraging that the best parameter
settings involved the activation of only few exem-
plars, computation with exemplar models still re-
quires the management of large numbers of vectors.
The computational overhead can be reduced by us-
ing data structures that cut down on the number
of vector comparisons, or by decreasing vector di-
mensionality (Gorman and Curran, 2006). We will
experiment with those methods to determine the
tradeoff of runtime and accuracy for this task.
Another area of future work is to move beyond
bag-of-words context: It is known from WSD
that syntactic and bag-of-words contexts provide
complementary information (Florian et al, 2002;
Szpektor et al, 2008), and we hope that they can be
integrated in a more sophisticated exemplar model.
Finally, we will to explore task-based evalua-
tions. Relation extraction and textual entailment
in particular are tasks where similar models have
been used before (Szpektor et al, 2008).
Acknowledgements. This work was supported
in part by National Science Foundation grant IIS-
0845925, and by a Morris Memorial Grant from
the New York Community Trust.
References
R. Bar-Haim, I. Dagan, I. Greental, and E. Shnarch.
2007. Semantic inference at the lexical-syntactic
level. In Proceedings of AAAI, pages 871?876, Van-
couver, BC.
M. Baroni and A. Lenci. 2009. One distributional
memory, many semantic spaces. In Proceedings of
the EACL Workshop on Geometrical Models of Nat-
ural Language Semantics, Athens, Greece.
W. Daelemans, A. van den Bosch, and J. Zavrel. 1999.
Forgetting exceptions is harmful in language learn-
ing. Machine Learning, 34(1/3):11?43. Special Is-
sue on Natural Language Learning.
K. Erk and S. Pado?. 2008. A structured vector space
96
model for word meaning in context. In Proceedings
of EMNLP, pages 897?906, Honolulu, HI.
K. Erk and S. Pado?. 2009. Paraphrase assessment in
structured vector space: Exploring parameters and
datasets. In Proceedings of the EACL Workshop on
Geometrical Models of Natural Language Seman-
tics, Athens, Greece.
R. Florian, S. Cucerzan, C. Schafer, and D. Yarowsky.
2002. Combining classifiers for word sense disam-
biguation. Journal of Natural Language Engineer-
ing, 8(4):327?341.
J. Gorman and J. R. Curran. 2006. Scaling distribu-
tional similarity to large corpora. In Proceedings of
ACL, pages 361?368, Sydney.
T. Griffiths, K. Canini, A. Sanborn, and D. J. Navarro.
2007. Unifying rational models of categorization
via the hierarchical Dirichlet process. In Proceed-
ings of CogSci, pages 323?328, Nashville, TN.
A. Kilgarriff. 1997. I don?t believe in word senses.
Computers and the Humanities, 31(2):91?113.
W. Kintsch. 2001. Predication. Cognitive Science,
25:173?202.
T. Landauer and S. Dumais. 1997. A solution to Platos
problem: the latent semantic analysis theory of ac-
quisition, induction, and representation of knowl-
edge. Psychological Review, 104(2):211?240.
D. McCarthy and J. Carroll. 2003. Disambiguating
nouns, verbs, and adjectives using automatically ac-
quired selectional preferences. Computational Lin-
guistics, 29(4):639?654.
D. McCarthy and R. Navigli. 2009. The English lexi-
cal substitution task. Language Resources and Eval-
uation, 43(2):139?159. Special Issue on Compu-
tational Semantic Analysis of Language: SemEval-
2007 and Beyond.
S. McDonald and M. Ramscar. 2001. Testing the dis-
tributional hypothesis: The influence of context on
judgements of semantic similarity. In Proceedings
of CogSci, pages 611?616.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL,
pages 236?244, Columbus, OH.
G. L. Murphy. 2002. The Big Book of Concepts. MIT
Press.
G Salton, A Wang, and C Yang. 1975. A vector-
space model for information retrieval. Journal of the
American Society for Information Science, 18:613?
620.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?124.
I. Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger.
2008. Contextual preferences. In Proceedings of
ACL, pages 683?691, Columbus, OH.
S. Thater, G. Dinu, and M. Pinkal. 2009. Ranking
paraphrases in context. In Proceedings of the ACL
Workshop on Applied Textual Inference, pages 44?
47, Singapore.
W. Voorspoels, W. Vanpaemel, and G. Storms. 2009.
The role of extensional information in conceptual
combination. In Proceedings of CogSci.
97
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467?472,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
?I Thou Thee, Thou Traitor?:
Predicting Formal vs. Informal Address in English Literature
Manaal Faruqui
Computer Science and Engineering
Indian Institute of Technology
Kharagpur, India
manaalfar@gmail.com
Sebastian Pad?
Computational Linguistics
Heidelberg University
Heidelberg, Germany
pado@cl.uni-heidelberg.de
Abstract
In contrast to many languages (like Russian or
French), modern English does not distinguish
formal and informal (?T/V?) address overtly,
for example by pronoun choice. We describe
an ongoing study which investigates to what
degree the T/V distinction is recoverable in
English text, and with what textual features it
correlates. Our findings are: (a) human raters
can label English utterances as T or V fairly
well, given sufficient context; (b), lexical cues
can predict T/V almost at human level.
1 Introduction
In many Indo-European languages, such as French,
German, or Hindi, there are two pronouns corre-
sponding to the English you. This distinction is
generally referred to as the T/V dichotomy, from
the Latin pronouns tu (informal, T) and vos (formal,
V) (Brown and Gilman, 1960). The V form can
express neutrality or polite distance and is used to
address socially superiors. The T form is employed
for friends or addressees of lower social standing,
and implies solidarity or lack of formality. Some
examples for V pronouns in different languages are
Sie (German), Vous (French), andaAp [Aap] (Hindi).
The corresponding T pronouns are du, tu, and t
 
m
[tum].
English used to have a T/V distinction until the
18th century, using you as V and thou as T pronoun.
However, in contemporary English, you has taken
over both uses, and the T/V distinction is not marked
morphosyntactically any more. This makes gener-
ation in English and translation into English easy.
Conversely, the extraction of social information from
texts, and translation from English into languages
with a T/V distinction is very difficult.
In this paper, we investigate the possibility to re-
cover the T/V distinction based on monolingual En-
glish text. We first demonstrate that annotators can
assign T/V labels to English utterances fairly well
(but not perfectly). To identify features that indicate
T and V, we create a parallel English?German corpus
of literary texts and preliminarily identify features
that correlate with formal address (like titles, and
formulaic language) as well as informal address. Our
results could be useful, for example, for MT from
English into languages that distinguish T and V, al-
though we did not test this prediction with the limits
of a short paper.
From a Natural Language Processing point of view,
the recovery of T/V information is an instance of a
more general issue in cross-lingual NLP and ma-
chine translation where for almost every language
pair, there are distinctions that are not expressed
overtly in the source language, but are in the target
language, and must therefore be recovered in some
way. Other examples from the literature include
morphology (Fraser, 2009) and tense (Schiehlen,
1998). The particular problem of T/V address has
been considered in the context of translation into
Japanese (Hobbs and Kameyama, 1990; Kanayama,
2003) and generation (Bateman, 1988), but only
on the context of knowledge-rich methods. As for
data-driven studies, we are only aware of Li and
Yarowsky?s (2008) work, who learn pairs of formal
and informal constructions in Chinese where T/V is
expressed mainly in construction choice.
467
Naturally, there is a large body of work on T/V
in (socio-)linguistics and translation science, cover-
ing in particular the conditions governing T/V use
in different languages (Kretzenbacher et al, 2006;
Sch?pbach et al, 2006) and on the difficulties in
translating them (Ardila, 2003; K?nzli, 2010). How-
ever, these studies are generally not computational in
nature, and most of their observations and predictions
are difficult to operationalize.
2 A Parallel Corpus of Literary Texts
2.1 Data Selection
We chose literary texts to build a parallel corpus for
the investigation of the T/V distinction. The main
reason is that commonly used non-literary collections
like EUROPARL (Koehn, 2005) consist almost ex-
clusively of formal interactions and are therefore of
no use to us. Fortunately, many 18th and 19th century
texts are freely available in several languages.
We identified 115 novels among the texts pro-
vided by Project Gutenberg (English) and Project
Gutenberg-DE (German) that were available in both
languages, with a total of 0.5M sentences per lan-
guage.1 Examples include Dickens? David Copper-
field or Tolstoy?s Anna Karenina. We decided to
exclude plays and poems as they often include partial
sentences and structures that are difficult to align.
2.2 Data Preparation
As the German and English novels come from two
different websites, they were not coherent in their
structure. They were first manually cleaned by delet-
ing the index, prologue, epilogue and Gutenberg li-
cense from the beginning and end of the files. To
some extent the chapter numbers and titles occurring
at the beginning of each chapter were cleared as well.
The files were then formatted to contain one sentence
per line and a blank line was inserted to preserve the
segmentation information.
The sentence splitter and tokenizer provided with
EUROPARL (Koehn, 2005) were used. We ob-
tained a comparable corpus of English and German
novels using the above pre-processing. The files
in the corpus were sentence-aligned using Gargan-
tuan (Braune and Fraser, 2010), an aligner that sup-
ports one-to-many alignments. After obtaining the
1http://www.gutenberg.org and http://gutenberg.spiegel.de/
ID Position Lemma Cap Category
(1) any du any T
(2) non-initial sie yes V
(3) non-initial ihr no T
(4) non-initial ihr yes V
Table 1: Rules for T/V determination for German personal
pronouns. (Cap: Capitalized)
sentence aligned corpus we computed word align-
ments in both English to German and German to En-
glish directions using Giza++ (Och and Ney, 2003).
The corpus was lemmatized and POS-tagged using
TreeTagger (Schmid, 1994). We did not apply a full
parser to keep processing as efficient as possible.
2.3 T/V Gold Labels for English Utterances
The goal of creating our corpus is to enable the in-
vestigation of contextual correlates of T/V in English.
In order to do this, we need to decide for as many
English utterances in our corpus as possible whether
they instantiate formal or informal address. Given
that we have a parallel corpus where the German side
overtly realizes T and V, this is a classical case of
annotation projection (Yarowsky and Ngai, 2001):
We transfer the German T/V information onto the
English side to create an annotated English corpus.
This allows us to train and evaluate a monolingual
English classifier for this phenomenon. However,
two problems arise on the way:
Identification of T/V in German pronouns. Ger-
man has three relevant personal pronouns: du, sie,
and ihr. These pronouns indicate T and V, but due to
their ambiguity, it is impossible to simply interpret
their presence or absense as T or V. We developed
four simple disambiguation rules based on position
on the sentence and capitalization, shown in Table 1.
The only unambiguous pronoun is du, which ex-
presses (singular) T (Rule 1). The V pronoun for
singular, sie, doubles as the pronoun for third person
(singular and plural), which is neutral with respect
to T/V. Since TreeTagger does not provide person
information, the only indicator that is available is
capitalization: Sie is 2nd person V. However, since
all words are capitalized in utterance-initial positions,
we only assign the label V in non-initial positions
468
(Rule 2).2
Finally, ihr is also ambiguous: non-capitalized, it
is used as T plural (Rule 3); capitalized, it is used as
an archaic alternative to Sie for V plural (Rule 4).
These rules leave a substantial number of instances
of German second person pronouns unlabeled; we
cover somewhat more than half of all pronouns. In
absolute numbers, from 0.5M German sentences we
obtained about 15% labeled sentences (45K for V
and 30K for T). However, this is not a fundamental
problem, since we subsequently used the English
data to train a classifier that is able to process any
English sentence.
Choice of English units to label. On the German
side, we assign the T/V labels to pronouns, and the
most straightforward way of setting up annotation
projection would be to label their word-aligned En-
glish pronouns as T/V. However, pronouns are not
necessarily translated into pronouns; additionally, we
found word alignment accuracy for pronouns, as a
function of word class, to be far from perfect. For
these reasons, we decided to treat complete sentences
as either T or V. This means that sentence alignment
is sufficient for projection, but English sentences can
receive conflicting labels, if a German sentence con-
tains both a T and a V label. However, this occurs
very rarely: of the 76K German sentences with T or
V pronouns, only 515, or less than 1%, contain both.
Our projection on the English side results in 53K V
and 35K T sentences, of which 731 are labeled as
both T and V.3
Finally, from the English labeled sentences we ex-
tracted a training set with 72 novels (63K sentences)
and a test set with 21 novels (15K sentences).4
3 Experiment 1: Human Annotation
The purpose of our first experiment is to investigate
how well the T/V distinction can be made in English
by human raters, and on the basis of what information.
We extracted 100 random sentences from the training
set. Two annotators with advanced knowledge of
2An initial position is defined as a position after a sentence
boundary (POS ?$.?) or after a bracket (POS ?$(?).
3Our sentence aligner supports one-to-many alignments and
often aligns single German to multiple English sentences.
4The corpus can be downloaded for research purposes from
http://www.nlpado.de/~sebastian/data.shtml.
Acc (Ann1) Acc (Ann2) IAA
No context 63 65 68
In context 70 69 81
Table 2: Manual annotation for T/V on a 100-sentence
sample (Acc: Accuracy, IAA: Inter-annotator agreement)
English were asked to label these sentences as T or V.
In a first round, the sentences were presented in isola-
tion. In a second round, the sentences were presented
with three sentences pre-context and three sentences
post-context. The results in Table 2 show that it is
fairly difficult to annotate the T/V distinction on indi-
vidual sentences since it is not expressed systemati-
cally. At the level of small discourses, the distinction
can be made much more confidently: In context, av-
erage agreement with the gold standard rises from
64% to 70%, and raw inter-annotator agreement goes
up from 68% to 81%.
Concerning the interpretation of these findings, we
note that the two taggers were both native speakers
of languages which make an overt T/V distinction.
Thus, our present findings cannot be construed as
firm evidence that English speakers make a distinc-
tion, even if implicitly. However, they demonstrate
at least that native speakers of such languages can
recover the distinction based solely on the clues in
English text.
An analysis of the annotation errors showed that
many individual sentences can be uttered in both T
and V situations, making it impossible to label them
in isolation:
(1) ?And perhaps sometime you may see her.?
This case (gold label: V) is however disambiguated
by looking at the previous sentence, which indicates
the social relation between speaker and addressee:
(2) ?And she is a sort of relation of your lord-
ship?s,? said Dawson.
Still, a three-sentence window is often not sufficient,
since the surrounding sentences may be just as unin-
formative. In these cases, global information about
the situation would be necessary.
A second problem is the age of the texts. They are
often difficult to label because they talk about social
situations that are unfamiliar to modern speakers (as
469
between aristocratic friends) or where the usage has
changed (as in married couples).
4 Experiment 2: Statistical Modeling
Task Setup. In this pilot modeling experiment, we
explore a (limited) set of cues which can be used to
predict the V vs. T dichotomy for English sentences.
Specifically, we use local words (i.e. information
present within the current sentence ? similar to the
information available to the human annotators in the
?No context? condition of Experiment 1). We ap-
proach the task by supervised classification, applying
a model acquired from the training set on the test
set. Note, however, that the labeled training data are
acquired automatically through the parallel corpus,
without the need for human annotation.
Statistical Model. We train a Naive Bayes classi-
fier, a simple but effective model for text categoriza-
tion (Domingos and Pazzani, 1997). It predicts the
class c for a sentence s by maximising the product
of the probabilities for the features f given the class,
multiplied by the class probability:
c? = argmax
c
P (c|s) = argmax
c
P (c)P (s|c) (3)
= argmax
c
P (c)
?
f?s
P (f |c) (4)
We experiment with three sets of features. The first
set consists of words, following the intuition that
some words should be correlated with formal ad-
dress (like titles), while others should indicate infor-
mal address (like first names). The second set con-
sists of part of speech bigrams, to explore whether
this more coarse-grained, but at the same time less
sparse, information can support the T/V decision.
The third set consists of one feature that represents a
semantic class, namely a set of 25 archaic verbs and
pronouns (like hadst or thyself ), which we expect
to correlate with old-fashioned T use. All features
are computed by MLE with add-one smoothing as
P (f |c) = freq(f,c)+1freq(c)+1 .
Results. Accuracies are shown in Table 3. A ran-
dom baseline is at 50%, and the majority class (V)
corresponds to 60%. The Naive Bayes models signif-
icantly outperform the frequency baselines at up to
67.0%; however, only the difference between the best
Model Accuracy
Random BL 50.0
Frequency BL 60.1
Words 66.1
Words + POS 65.0
Words + Archaic 67.0
Human (no context) 64
Human (in context) 70
Table 3: NB classifier results for the T/V distinction
(Words+Archaic) and the worst (Words+POS) model
is significant according to a ?2 test. Thus, POS fea-
tures tend to hurt, and the archaic feature helps, even
though it technically overcounts evidence.5
The Naive Bayes model notably performs at a
roughly human level, better than human annotators
on the same setup (no context sentences), but worse
than humans that have more context at their disposal.
Overall, however, the T/V distinction appears to be a
fairly difficult one. An important part of the problem
is the absence of strong indicators in many sentences,
in particular short ones (cf. Example 1). In contrast
to most text categorization tasks, there is no topi-
cal difference between the two categories: T and V
can both co-occur with words from practically any
domain.
Table 4, which lists the top ten words for T and
V (ranked by the ratio of probabilities for the two
classes), shows that among these indicators, many
are furthermore names of persons from particular
novels which are systematically addressed formally
(like Phileas Fogg from Jules Vernes? In eighty days
around the world) or informally (like Mowgli, Baloo,
and Bagheera from Rudyard Kipling?s Jungle Book).
Nevertheless, some features point towards more
general patterns. In particular, we observe ti-
tles among the V-indicators (gentlemen, madam,
ma+?am) as well as formulaic language (Permit
(me)). Indicators for T seem to be much more general,
with the expected exception of archaic thou forms.
5 Conclusions and Future Work
In this paper, we have reported on an ongoing study
of the formal/informal (T/V) address distinction in
5We experimented with logistic regression models, but were
unable to obtain better performance, probably because we intro-
duced a frequency threshold to limit the feature set size.
470
Top 10 words for V Top 10 words for T
Word w P (w|V )P (w|T ) Word w
P (w|T )
P (w|V )
Fogg 49.7 Thee 67.2
Oswald 32.5 Trot 46.8
Ma 31.8 Bagheera 37.7
Gentlemen 25.2 Khan 34.7
Madam 24.2 Mowgli 33.2
Parfenovitch 23.2 Baloo 30.2
Monsieur 22.6 Sahib 30.2
Fix 22.5 Clare 29.7
Permit 22.5 didst 27.7
?am 22.4 Reinhard 27.2
Table 4: Words that are indicative for T or V
modern English, where it is not determined through
pronoun choice or other overt means. We see this task
as an instance of the general problem of recovering
?hidden? information that is not expressed overtly.
We have created a parallel German-English cor-
pus and have used the information provided by the
German pronouns to induce T/V labels for English
sentences. In a manual annotation study for English,
annotators find the form of address very difficult to
determine for individual sentences, but can draw this
information from broader English discourse context.
Since our annotators are not native speakers of En-
glish, but of languages that make the T/V distinction,
we can conclude that English provides lexical cues
that can be interpreted as to the form of address, but
cannot speak to the question whether English speak-
ers in fact have a concept of this distinction.
In a first statistical analysis, we found that lexical
cues from the sentence can be used to predict the
form of address automatically, although not yet on a
very satisfactory level.
Our analyses suggest a number of directions for
future research. On the technical level, we would like
to apply a sequence model to account for the depen-
decies among sentences, and obtain more meaningful
features for formal and informal address. In order
to remove idiosyncratic features like names, we will
only consider features that occur in several novels;
furthermore, we will group words using distributional
clustering methods (Clark, 2003) and predict T/V
based on cluster probabilities.
The conceptually most promising direction, how-
ever, is the induction of social networks in such nov-
els (Elson et al, 2010): Information on the social re-
lationship between a speaker and an addressee should
provide global constraints on all instances of com-
munications between them, and predict the form of
address much more reliably than word features can.
Acknowledgments
Manaal Faruqui has been partially supported by a
Microsoft Research India Travel Grant.
References
John Ardila. 2003. (Non-Deictic, Socio-Expressive) T-
/V-Pronoun Distinction in Spanish/English Formal Lo-
cutionary Acts. Forum for Modern Language Studies,
39(1):74?86.
John A. Bateman. 1988. Aspects of clause politeness in
japanese: An extended inquiry semantics treatment. In
Proceedings of the 26th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 147?154,
Buffalo, New York.
Fabienne Braune and Alexander Fraser. 2010. Improved
unsupervised sentence alignment for symmetrical and
asymmetrical parallel corpora. In Coling 2010: Posters,
pages 81?89, Beijing, China.
Roger Brown and Albert Gilman. 1960. The pronouns
of power and solidarity. In Thomas A. Sebeok, editor,
Style in Language, pages 253?277. MIT Press, Cam-
bridge, MA.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 59?66, Budapest, Hungary.
Pedro Domingos and Michael J. Pazzani. 1997. On the
optimality of the simple Bayesian classifier under zero-
one loss. Machine Learning, 29(2?3):103?130.
David Elson, Nicholas Dames, and Kathleen McKeown.
2010. Extracting social networks from literary fiction.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 138?147,
Uppsala, Sweden.
Alexander Fraser. 2009. Experiments in morphosyntactic
processing for translating to and from German. In Pro-
ceedings of the Fourth Workshop on Statistical Machine
Translation, pages 115?119, Athens, Greece.
Jerry Hobbs and Megumi Kameyama. 1990. Translation
by abduction. In Proceedings of the 13th International
Conference on Computational Linguistics, Helsinki,
Finland.
471
Hiroshi Kanayama. 2003. Paraphrasing rules for auto-
matic evaluation of translation into japanese. In Pro-
ceedings of the Second International Workshop on Para-
phrasing, pages 88?93, Sapporo, Japan.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In Proceedings of the 10th
Machine Translation Summit, pages 79?86, Phuket,
Thailand.
Heinz L. Kretzenbacher, Michael Clyne, and Doris Sch?p-
bach. 2006. Pronominal Address in German: Rules,
Anarchy and Embarrassment Potential. Australian Re-
view of Applied Linguistics, 39(2):17.1?17.18.
Alexander K?nzli. 2010. Address pronouns as a problem
in French-Swedish translation and translation revision.
Babel, 55(4):364?380.
Zhifei Li and David Yarowsky. 2008. Mining and mod-
eling relations between formal and informal Chinese
phrases from web corpora. In Proceedings of the 2008
Conference on Empirical Methods in Natural Language
Processing, pages 1031?1040, Honolulu, Hawaii.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Michael Schiehlen. 1998. Learning tense translation
from bilingual corpora. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics and 17th International Conference on Com-
putational Linguistics, pages 1183?1187, Montreal,
Canada.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49.
Doris Sch?pbach, John Hajek, Jane Warren, Michael
Clyne, Heinz Kretzenbacher, and Catrin Norrby. 2006.
A cross-linguistic comparison of address pronoun use in
four European languages: Intralingual and interlingual
dimensions. In Proceedings of the Annual Meeting of
the Australian Linguistic Society, Brisbane, Australia.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
the 2nd Meeting of the North American Chapter of
the Association of Computational Linguistics, pages
200?207, Pittsburgh, PA.
472
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1201?1211,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
DERIVBASE: Inducing and Evaluating a
Derivational Morphology Resource for German
Britta Zeller? Jan ?najder? Sebastian Pad??
?Heidelberg University, Institut f?r Computerlinguistik
69120 Heidelberg, Germany
?University of Zagreb, Faculty of Electrical Engineering and Computing
Unska 3, 10000 Zagreb, Croatia
{zeller, pado}@cl.uni-heidelberg.de jan.snajder@fer.hr
Abstract
Derivational models are still an under-
researched area in computational morphol-
ogy. Even for German, a rather resource-
rich language, there is a lack of large-
coverage derivational knowledge. This pa-
per describes a rule-based framework for
inducing derivational families (i.e., clus-
ters of lemmas in derivational relation-
ships) and its application to create a high-
coverage German resource, DERIVBASE,
mapping over 280k lemmas into more than
17k non-singleton clusters. We focus on the
rule component and a qualitative and quan-
titative evaluation. Our approach achieves
up to 93% precision and 71% recall. We
attribute the high precision to the fact that
our rules are based on information from
grammar books.
1 Introduction
Morphological processing is generally recognized
as an important step for many NLP tasks. Morpho-
logical analyzers such as lemmatizers and part of
speech (POS) taggers are commonly the first NLP
tools developed for any language (Koskenniemi,
1983; Brill, 1992). They are also applied in NLP
applications where little other linguistic analysis is
performed, such as linguistic annotation of corpora
or terminology acquisition; see Daille et al (2002)
for an informative summary.
Most work on computational morphology has
focused on inflectional morphology, that is, the
handling of grammatically determined variation of
form (Bickel and Nichols, 2001), which can be
understood, overimplifying somewhat, as a normal-
ization step. Derivational morphology, which is
concerned with the formation of new words from
existing ones, has received less attention. Exam-
ples are nominalization (to understand? the un-
derstanding), verbalization (the shelf ? to shelve),
and adjectivization (the size ? sizable). Part of
the reason for the relative lack of attention lies in
the morphological properties of English, such as
the presence of many zero derivations (the fish?
to fish), the dominance of suffixation, and the rel-
ative absence of stem changes in derivation. For
these reasons, simple stemming algorithms (Porter,
1980) provide a cheap and accurate approximation
to English derivation.
Two major NLP resources deal with derivation.
WordNet lists so-called ?morphosemantic? rela-
tions (Fellbaum et al, 2009) for English, and a
number of proposals exist for extending WordNets
in other languages with derivational relations (Bil-
gin et al, 2004; Pala and Hlav?c?kov?, 2007). Cat-
Var, the ?Categorial Variation Database of English?
(Habash and Dorr, 2003), is a lexicon aimed specif-
ically at derivation. It groups English nouns, verbs,
adjectives, and adverbs into derivational equiva-
lence classes or derivational families such as
askV askerN askingN askingA
Derivational families are commonly understood as
groups of derivationally related lemmas (Daille et
al., 2002; Milin et al, 2009). The lemmas in CatVar
come from various open word classes, and multiple
words may be listed for the same POS. The above
family lists two nouns: an event noun (asking) and
an agentive noun (asker). However, CatVar does
not consider prefixation, which is why, e.g., the
adjective unasked is missing.
CatVar has found application in different areas
of English NLP. Examples are the acquisition of
paraphrases that cut across POS lines, applied, for
example, in textual entailment (Szpektor and Da-
gan, 2008; Berant et al, 2012). Then there is the
induction and extension of semantic roles resources
for predicates of various parts of speech (Meyers et
al., 2004; Green et al, 2004). Finally, CatVar has
1201
been used as a lexical resource to generate sentence
intersections (Thadani and McKeown, 2011).
In this paper, we describe the project of obtain-
ing derivational knowledge for German to enable
similar applications. Even though there are two
derivational resources for this language, IMSLEX
(Fitschen, 2004) and CELEX (Baayen et al, 1996),
both have shortcomings. The former does not ap-
pear to be publicly available, and the latter has a
limited coverage (50k lemmas) and does not ex-
plicitly represent derivational relationships within
families, which are necessary for fine-grained op-
timization of families. For this reason, we look
into building a novel derivational resource for Ger-
man. Unfortuantely, the approach used to build
CatVar cannot be adopted: it builds on a collection
of high-quality lexical-semantic resources such as
NOMLEX (Macleod et al, 1998), which are not
available for German.
Instead, we employ a rule-based framework to
define derivation rules that cover both suffixation
and prefixation and describes stem changes. Fol-
lowing the work of ?najder and Dalbelo Ba?ic?
(2010), we define the derivational processes using
derivational rules and higher-order string transfor-
mation functions. The derivational rules induce
a partition of the language?s lemmas into deriva-
tional families. Our method is applicable to many
languages if the following are available: (1) a com-
prehensive set of lemmas (optionally including gen-
der information); (2) knowledge about admissible
derivational patterns, which can be gathered, for
example, from linguistics textbooks.
The result is a freely available high-precision
high-coverage resource for German derivational
morphology that has a structure parallel to Cat-
Var, but was obtained without using manually con-
structed lexical-semantic resources. We conduct
a thorough evaluation of the induced derivational
families both regarding precision and recall.
Plan of the paper. Section 2 discusses prior
work. Section 3 defines our derivation model that
is applied to German in Section 4. Sections 5 and
6 present our evaluation setup and results. Section
7 concludes the paper and outlines future work.
2 Related Work
Computational models of morphology have a long
tradition. Koskenniemi (1983) was the first who
analyzed and generated morphological phenomena
computationally. His two-level theory has been
applied in finite state transducers (FST) for several
languages (Karttunen and Beesley, 2005).
Many recent approaches automatically induce
morphological information from corpora. They
are either based solely on corpus statistics (D?jean,
1998), measure semantic similarity between input
and output lemma (Schone and Jurafsky, 2000),
or bootstrap derivation rules starting from seed ex-
amples (Piasecki et al, 2012). Hammarstr?m and
Borin (2011) give an extensive overview of state-
of-the-art unsupervised learning of morphology.
Unsupervised approaches operate at the level of
word-forms and have complementary strengths and
weaknesses to rule-based approaches. On the up-
side, they do not require linguistic knowledge; on
the downside, they have a harder time distinguish-
ing between derivation and inflection, which may
result in lower precision, and are not guaranteed
to yield analyses that correspond to linguistic intu-
ition. An exception is the work by Gaussier (1999),
who applies an unsupervised model to construct
derivational families for French.
For German, several morphological tools exist.
Morphix is a classification-based analyzer and gen-
erator of German words on the inflectional level
(Finkler and Neumann, 1988). SMOR (Schmid
et al, 2004) employs a finite-state transducer to
analyze German words at the inflectional, deriva-
tional, and compositional level, and has been used
in other morphological analyzers, e.g., Morphisto
(Zielinski and Simon, 2008). The site canoonet1 of-
fers broad-coverage information about the German
language including derivational word formation.
3 Framework
In this section, we describe our rule-based model of
derivation, its operation to define derivational fam-
ilies, and the application of the model to German.
We note that the model is purely surface-based,
i.e., it does not model any semantic regularities be-
yond those implicit in string transformations. We
begin by outlining the characteristics of German
derivational morphology.
3.1 German Derivational Morphology
As German is a morphologically complex language,
we analyzed its derivation processes before imple-
menting our rule-based model. We relied on tradi-
tional grammar books and lexicons, e.g., Hoeppner
(1980) and Augst (1975), in order to linguistically
1http://canoo.net
1202
justify our assumptions as well as to achieve the
best possible precision and coverage.
We concentrate on German derivational pro-
cesses that involve nouns, verbs, and adjectives.2
Nouns are simple to recognize due to capitaliza-
tion: stauenV ? StauN (to jam ? jam), essenV ?
EssenN (to eat ? food). Verbs bear three typical
suffixes (-en, -eln, -ern). An example of a derived
verb is festA ? festigenV (tight ? to tighten), where
-ig is the derivational suffix. Adjectivization works
similarlty: TagN ? t?glichA (day ? daily).
This example shows that derivation can also in-
volve stem changes in the form of umlaut (e.g.,
a ? ?) and ablaut shift, e.g., siedenV ? SudN
(to boil ? infusion). Other frequent processes
in German derivation are circumfixation (HaftN
? inhaftierenV (arrest ? to arrest)) and prefixation
(hebenV ? behebenV (to raise ? to remedy)). Pre-
fixation often indicates a semantic shift, either in
terms of the general meaning (as above) or in terms
of the polarity ( klarA ? unklarA (clear ? unclear)).
Also note that affixes can be either Germanic, e.g.,
?len ? ?lung (to oil ? oiling), or Latin/Greek, e.g.,
generieren ? Generator (to generate ? generator).
As this analysis shows, derivation in German
involves transformation as well as affixation pro-
cesses, which has to be taken into account when
modeling a derivational resource.
3.2 A Rule-based Derivation Model
The purpose of a derivational model is to define
a set of transformations that correspond to valid
derivational word formation rules. Rule-based
frameworks offer convenient representations for
derivational morphology because they can take ad-
vantage of linguistic knowledge about derivation,
have interpretable representations, and can be fine-
tuned for high precision. The choice of the frame-
work is in principle arbitrary, as long as it can con-
veniently express the derivational phenomena of
a language. Typically used for this purpose are
two-level formalism rules (Karttunen and Beesley,
1992) or XFST replace rules (Beesley and Kart-
tunen, 2003).
In this paper, we adopt the modeling framework
proposed by ?najder and Dalbelo Ba?ic? (2010).
The framework corresponds closely to simple,
human-readable descriptions in traditional gram-
2We ignore adverb derivation; the German language dis-
tinguishes between adverbial adjectives and adverbs, the latter
being a rather unproductive class and thus of no interest for
derivation (Schiller et al, 1999).
mar books. The expressiveness of the formalism
is equivalent to the replacement rules commonly
used in finite state frameworks, thus the rules can
be compiled into FSTs for efficient processing.
The framework makes a clear distinction be-
tween inflectional and derivational morphology and
provides separate modeling components for these
two; we only make use of the derivation modeling
component. We use an implementation of the mod-
eling framework in Haskell. For details, see the
studies by ?najder and Dalbelo Ba?ic? (2008) and
?najder and Dalbelo Ba?ic? (2010).
The building blocks of the derivational compo-
nent are derivational rules (patterns) and transfor-
mation functions. A derivational rule describes the
derivation of a derived word from a basis word. A
derivational rule d is defined as a triple:
d = (t,P1,P2) (1)
where t is the transformation function that maps
the word?s stem (or lemma) into the derived word?s
stem (or lemma), while P1 and P2 are the sets of
inflectional paradigms of the basis word and the
derived word, respectively, which specify the mor-
phological properties of the rule?s input and output.
For German, our study assumes that inflectional
paradigms are combinations of part-of-speech and
gender information (for nouns).
A transformation function t : S ? ?(S) maps
strings to a set of strings, representing possible
transformations. At the lowest level, t is defined
in terms of atomic string replacement operations
(replacement of prefixes, suffixes, and infixes). The
framework then uses the notion of higher-order
functions ? functions that take other transforma-
tions as arguments and return new transformations
as results ? to succinctly define common deriva-
tional processes such as prefixation, suffixation,
and stem change. More complex word-formation
rules, such as those combining prefixation and suf-
fixation, can be obtained straightforwardly by func-
tional composition.
Table 1 summarizes the syntax we use for trans-
formation functions and shows two example deriva-
tional rules. Rule 1 defines an English adjectiviza-
tion rule. It uses the conditional try operator to
apply to nouns with and without the -ion suffix
(action ? active, instinct ? instinctive). Infix re-
placement is used to model stem alternation, as
shown in rule 2 for German nominalization, e.g.,
vermachtA ? Verm?chtnisN (bequethed ? bequest).
1203
Function Description
sfx (s) concatenate the suffix s
dsfx (s) delete the suffix s
aifx (s1, s2) alternate the infix s1 to s2
try(t) perform transformation t, if possible
opt(t) optionally perform transformation t
uml alternate infixes for an umlaut shift:
uml = aifx ({(a, ?), (o, ?), (u, ?)})
Examples
1 (EN) (sfx (ive) ? try(dsfx (ion)),N ,A)
?derive -ive adjectives from nouns poten-
tially ending in -ion?
2 (DE) (sfx (nis) ? try(uml),A,N )
?derive -nis nouns from adjectives with
optional umlaut creation?
Table 1: Transformation functions and exemplary
derivational rules in the framework by ?najder and
Dalbelo Ba?ic? (2010)
N and A denote the paradigms for nouns (without
gender restriction) and adjectives, respectively.
3.3 Induction of Derivational Families
Recall that our goal is to induce derivational fami-
lies, that is, classes of derivationally related words.
We define derivational families on the basis of
derivational rules as follows.
Given a lemma-paradigm pair (l, p) as input,
a single derivational rule d = (t,P1,P2) gen-
erates a set of possible derivations Ld(l, p) =
{(l1, p1), . . . , (ln, pn)}, where p ? P1 and pi ? P2
for all i. Given a set of derivational rules D, we de-
fine a binary derivation relation?D between two
lemma-paradigm pairs that holds if the second pair
can be derived from the first one as:
(l1, p1)?D (l2, p2) (2)
iff ?d ? D. (l2, p2) ? Ld(l1, p1)
Let L denote the set of lemma-paradigm pairs. The
set of derivational families defined by D on L is
given by the equivalence classes of the transitive,
symmetric, and reflexive closure of?D over L.
Note that in addition to the quality of the rules,
the properties ofL plays a central role in the quality
of the induced families. High coverage of L is im-
portant because the transitivity of?D ranges only
over lemmas in L, so low coverage of L may result
in fragmented derivational families. However, L
should also not contain erroneous lemma-paradigm
pairs. The reason is that the derivational rules only
define admissible derivations, which need not be
morphologically valid, and therefore routinely over-
generate; L plays an important role in filtering out
derivations that are not attested in the data.
4 Building the Resource
4.1 Derivational Rules
We implemented the derivational rules from Hoepp-
ner (1980) for verbs, nouns, and adjectives, cov-
ering all processes described in Section 3.1 (zero
derivation, prefixation, suffixation, circumfixation,
and stem changes). We found many derivational
patterns in German to be conceptually simple (e.g.,
verb-noun zero derivation) so that substantial cov-
erage can already be achieved with very simple
transformation functions. However, there are many
more complex patterns (e.g., suffixation combined
with optional stem changes) that in sum also af-
fect a considerable number of lemmas, which re-
quired us to either implement low-coverage rules
or generalize existing rules. In order to preserve
precision as much as possible, we restricted rule
application by using try instead of opt, and by using
gender information from the noun paradigms (for
example, some rules only apply to masculine nouns
and produce female nouns). As a result, we end
up with high-coverage rules, such as derivations
of person-denoting nouns (SchuleN ? Sch?lerN
(school ? pupil)) as well as high-accuracy rules
such as negation prefixes (PolN ? GegenpolN (pole
? antipole)).
Even though we did not focus on the explana-
tory relevance of rules, we found that the under-
lying modeling formalism, and the methodology
used to develop the model, offer substantial lin-
guistic plausibility in practice. We had to resort to
heuristics mostly for words with derivational trans-
formations that are motivated by Latin or Greek
morphology and do not occur regularly in German,
e.g., selegierenV ? SelektionN (select ? selection).
In the initial development phase, we imple-
mented 154 rules, which took about 22 person-
hours. We then revised the rules with the aim of
increasing both precision and recall. To this end,
we constructed a development set comprised of a
sample of 1,000 derivational families induced us-
ing our rules. On this set, we inspected the deriva-
tional families for false positives, identified the
problematic rules, and identified unused and redun-
dant rules. In order to identify the false negatives,
we additionally sampled a list of 1,000 lemmas and
used string distance measures (cf. Section 5.1) to re-
trieve the 10 most similar words for each lemma not
1204
Process N-N N-A N-V A-A A-V V-V
Zero derivation ? 1 5 ? ? ?
Prefixation 10 ? 5 5 2 9
+ Stem change ? ? 3 ? 1 ?
Suffixation 15 35 20 1 14 ?
+ Stem change 2 8 7 ? 3 1
Circumfixation ? ? 1 ? ? ?
+ Stem change ? ? 1 ? ? ?
Stem change ? ? 7 ? ? 2
Total 27 44 49 6 20 12
Table 2: Breakdown of derivation rules by category
of the basis and the derived word
already covered by the derivational families. The
refinement process took another 8 person-hours. It
revealed three redundant rules and seven missing
rules, leading us to a total of 158 rules.
Table 2 shows the distribution of rules with re-
spect to the derivational processes they implement
and the part of speech combinations for the ba-
sis and the derived words. All affixations occur
both with and without stem changes, mostly um-
laut shifts. Suffixation is by far the most frequently
used derivation process, and noun-verb derivation
is most diverse in terms of derivational processes.
We also estimated the reliability of derivational
rules by analyzing the accuracy of each rule on
the development set. We assigned each rule a con-
fidence rating on a three-level scale: L3 ? very
reliable (high-accuracy rules), L2 ? generally reli-
able, and L1 ? less reliable (low-accuracy rules).
We manually analyzed the correctness of rule ap-
plications for 100 derivational families of different
size (counting 2 up to 114 lemmas), and assigned
55, 79, and 24 rules to L3, L2 and L1, respectively.
4.2 Data and Preprocessing
For an accurate application of nominal derivation
rules, we need a lemma list with POS and gender
information. We POS-tag and lemmatize SDEWAC,
a large German-language web corpus from which
boilerplate paragraphs, ungrammatical sentences,
and duplicate pages were removed (Faa? et al,
2010). For POS tagging and lemmatization, we use
TreeTagger (Schmid, 1994) and determine gram-
matical gender with the morphological layer of
the MATE Tools (Bohnet, 2010). We treat proper
nouns like common nouns.
We apply three language-specific filtering steps
based on observations in Section 3.1. First, we dis-
card non-capitalized nominal lemmas. Second, we
deleted verbal lemmas not ending in verb suffixes.
Third, we removed frequently occurring erroneous
comparative forms of adjectives (usually formed
by adding -er, like neuer / newer) by checking for
the presence of lemmas without -er (neu / new).
An additional complication in German concerns
prefix verbs, because prefix is separated in tensed
instances. For example, the 3rd person male singu-
lar of aufh?ren (to stop) is er h?rt auf (he stops).
Since most prefixes double as prepositions, the cor-
rect lemmas can only be reconstructed by parsing.
We parse the corpus using the MST parser (Mc-
Donald et al, 2006) and recover prefix verbs by
searching for instances of the dependency relation
labeled PTKVZ.
Since SDEWAC, as a web corpus, still contains
errors, we only take into account lemmas that occur
three times or more in the corpus. Considering the
size of SDEWAC, we consider this as a conservative
filtering step that preserves high recall and provides
a comprehensive basis for evaluation. After prepro-
cessing and filtering, we run the induction of the
derivational families as explained in Section 3 to
obtain the DERIVBASE resource.
4.3 Statistics on DERIVBASE
The preparation of the SDEWAC corpus as ex-
plained in Section 4.2 yields 280,336 lemmas,
which we cover with our resource. We induced
a total of 239,680 derivational families from this
data, with 17,799 non-singletons and 221,881 sin-
gletons (most of them due to compound nouns).
11,039 of the families consist of two lemmas, while
the biggest contains 116 lemmas (an overgenerated
family). The biggest family with perfect precision
(i.e., it contains only morphologically related lem-
mas) contains 40 lemmas, e.g., haltenV , erhaltenV ,
Verh?ltnisN (to hold, to uphold, relation), etc. For
comparison, CatVar v2.1 contains only 82,676 lem-
mas in 13,368 non-singleton clusters and 38,604
singletons.
The following sample family has seven members
across all three POSes and includes prefixation,
suffixation, and infix umlaut shifts:
taubA (numbA), TaubheitNf (numbnessN ),
bet?ubenV (to anesthetizeV ), Bet?ubungNf
(anesthesiaN ), bet?ubtA (anesthetizedA),
bet?ubendA (anestheticA), Bet?ubenNn
(act of anesthetizingN )
1205
5 Evaluation
5.1 Baselines
We use two baselines against which we compare
the induced derivational families: (1) clusters ob-
tained with the German version of Porter?s stem-
mer (Porter, 1980)3 and (2) clusters obtained us-
ing string distance-based clustering. We have con-
sidered a number of string distance measures and
tested them on the development set (cf. Section
4.1). The measure proposed by Majumder et al
(2007) turned out to be the most effective in cap-
turing suffixal variation. For words X and Y , it is
defined as
D4(X,Y ) =
n?m+ 1
n+ 1
n?
i=m
1
2i?m (3)
where m is the position of left-most character mis-
match, and n + 1 is the length of the longer of
the two strings. To capture prefixal variation and
stem changes, we use the n-gram based measure
proposed by Adamson and Boreham (1974):
Dicen(X,Y ) = 1?
2c
x+ y (4)
where x and y are the total number of distinct n-
grams inX and Y , respectively, and c is the number
of distinct n-grams shared by both words. In our
experiments, the best performance was achieved
with n = 3.
We used hierarchical agglomerative clustering
with average linkage. To reduce the computational
complexity, we performed a preclustering step by
recursively partitioning the set of lemmas sharing
the same prefix into partitions of manageable size
(1000 lemmas). Initially, we set the number of clus-
ters to be roughly equal to the number of induced
derivational families. For the final evaluation, we
optimized the number of clusters based on F1 score
on calibration and validation sets (cf. Section 5.3).
5.2 Evaluation Methodology
The induction of derivational families could be eval-
uated globally as a clustering problem. Unfortu-
nately, cluster evaluation is a non-trivial task for
which there is no consensus on the best approach
(Amig? et al, 2009). We decided to perform our
evaluation at the level of pairs: we manually judge
for a set of pairs whether they are derivationally
related or not.
3http://snowball.tartarus.org
We obtain the gold standard for this evaluation
by sampling lemmas from the lemma list. With ran-
dom sampling, the evaluation would be unrealistic
because a vast majority of pairs would be deriva-
tionally unrelated and count as true negatives in our
analysis. Moreover, in order to reliably estimate the
overall precision of the obtained derivational fam-
ilies, we need to evaluate on pairs sampled from
these families. On the other hand, in order to assess
recall, we need to sample from pairs that are not
included in our derivational families.
To obtain reliable estimates of both precision
and recall, we decided to draw two different sam-
ples: (1) a sample of lemma pairs sampled from
the induced derivational families, on which we
estimate precision (P-sample) and (2) a sample
of lemma pairs sampled from the set of possibly
derivationally related lemma pairs, on which we
estimate recall (R-sample). In both cases, pairs
(l1, l2) are sampled in two steps: first a lemma l1
is drawn from a non-singleton family, then the sec-
ond lemma l2 is drawn from the derivational family
of l1 (P-sample) or the set of lemmas possibly re-
lated to l1 (R-sample). The set of possibly related
lemmas is a union of the derivational family of l1,
the clusters of l1 obtained with the baseline meth-
ods, and k lemmas most similar to l1 according to
the two string distance measures. We use k = 7
in our experiments. This is based on preliminary
experiments on the development set (cf. Section
4.1), which showed that k = 7 retrieves about 92%
of the related lemmas retrieved for k = 20 with
a much smaller number of true negatives. Thus,
the evaluation on the R-sample might overestimate
the recall, but only slightly so, while the P-sample
yields a reliable estimate of precision by reducing
the number of true negatives in the sample.
Both samples contain 2400 lemma pairs each.
Lemmas included in the development set (Sec-
tion 4.1) were excluded from sampling.
5.3 Gold Standard Annotation
Two German native speakers annotated the pairs
from the P-sample and R-samples. We defined five
categories into which all lemma pairs are classified
as shown in Table 3. We count R and M as positives
and N, C, L as negatives (cf. Section 3).4 Note
that this binary distinction would be sufficient to
compute recall and precision. However, the more
4Ambiguous lemmas are categorized as positive (R or M)
if there is a matching sense.
1206
Label Description Example
R l1 and l2 are morphologi-
cally and semantically re-
lated
kratzigA ? verkratztA(scratchy ? scuffed)
M l1 and l2 are morphologi-
cally but not semantically
related
bombenV ? bombigA(to bomb ? smashing)
N no morphological relation belebtA ? lobenV
(lively ? to praise)
C no derivational relation,
but the pair is composi-
tionally related
FilmendeN ? filmenV(end of film ? to film)
L not a valid lemma (mis-
lemmatization, wrong
gender, foreign words)
HaufeN ? H?ufungN(N/A ? accumulation)
Table 3: Categories for lemma pair classification
Agreement Cohen?s ?
R-sample 0.85 0.79
P-sample 0.86 0.70
Table 4: Inter-annotator agreement on validation
sample
fine-grained five-class annotation scheme provides
a more detailed picture. The separation between R
and M gives a deeper insight into the semantics of
the derivational families. Distinguishing between
C and N, in turn, allows us to identify the pairs that
are derivationally unrelated, but compositionally
related, e.g., EhemannN ? EhefrauN (husband ?
wife).
We first carried out a calibration phase in which
the annotators double-annotated 200 pairs from
each of the two samples and refined the annotation
guidelines. In a subsequent validation phase, we
computed inter-annotator agreements on the anno-
tations of another 200 pairs each from the P- and
the R-samples. Table 4 shows the proportion of
identical annotations by both annotators as well as
Cohen?s ? score (Cohen, 1968). We achieve sub-
stantial agreement for ? (Carletta, 1996). On the
P-sample, ? is a little lower because the distribu-
tion of the categories is skewed towards R, which
makes an agreement by chance more probable.
In our opinion, the IAA results were sufficiently
high to switch to single annotation for the produc-
tion phase. Here, each annotator annotated another
1000 pairs from the P-sample and R-sample so
that the final test set consists of 2000 pairs from
each sample. The P-sample contains 1663 positive
(R+M) and 337 negative (N+C+L) pairs, respec-
tively, the R-sample contains 575 positive and 1425
negative pairs. As expected, there are more positive
Precision Recall
Method P-sample R-sample
DERIVBASE (initial) 0.83 0.58
DERIVBASE-L123 0.83 0.71
DERIVBASE-L23 0.88 0.61
DERIVBASE-L3 0.93 0.35
R-sample
Stemming 0.66 0.07
String distance D4 0.36 0.20
String distance Dice3 0.23 0.23
Table 5: Precision and recall on test samples
pairs in the P-sample and more negative pairs in
the R-sample.
6 Results
6.1 Quantitative Evaluation
Table 5 presents the overall results. We eval-
uate four variants of the induced derivational
families: those obtained before rule refinement
(DERIVBASE initial), and three variants after rule
refinement: using all rules (DERIVBASE-L123),
excluding the least reliable rules (DERIVBASE-
L23), and using only highly reliable rules
(DERIVBASE-L3).
We measure the precision of our method on the
P-sample and recall on the R-sample. For the base-
lines, precision was also computed on the R-sample
(computing it on P-sample, which is obtained from
the induced derivational families, would severely
underestimate the number of false positives). We
omit the F1 score because its use for precision and
recall estimates from different samples is unclear.
DERIVBASE reaches 83% precision when us-
ing all rules and 93% precision when using only
highly reliable rules. DERIVBASE-L123 achieves
the highest recall, outperforming other methods
and variants by a large margin. Refinement of the
initial model has produced a significant improve-
ment in recall without losses in precision. The base-
lines perform worse than our method: the stemmer
we use is rather conservative, which fragments the
families and leads to a very low recall. The string
distance-based approaches achieve more balanced
precision and recall scores. Note that for these
methods, precision and recall can be traded off
against each other by varying the number of clus-
ters; we chose the number of clusters by optimizing
the F1 score on the calibration and validaton sets.
All subsequent analyses refer to DERIVBASE-
1207
Accuracy
Coverage High Low Total
High 18 ? 18
Low 53 21 74
Total 71 21 92
Table 6: Proportions of accuracy and coverage for
direct derivations (measured on P-sample)
P R P R
N-N 0.78 0.68 N-A 0.89 0.83
A-A 0.87 0.70 N-V 0.79 0.68
V-V 0.55 0.24 A-V 0.88 0.73
Table 7: Precision and recall across different part
of speech (first POS: basis; second POS: derived
word)
L123, which is the model with the highest recall.
If optimal precision is required, DERIVBASE-L3
should however be preferred.
Analysis by frequency. We cross-classified our
rules according to high/low accuracy and high/low
coverage based on the pairs in the P-sample.
We only considered directly derivationally related
(?D) pairs and defined ?high accuracy? and ?high
coverage? as all rules above the 25th percentile in
terms of accuracy and coverage, respectively. The
results are shown in Table 6: all high-coverage
rules are also highly accurate. Most rules are ac-
curate but infrequent. Only 21 rules have a low
accuracy, but all of them apply infrequently.
Analysis by parts of speech. Table 7 shows pre-
cision and recall values for different part of speech
combinations for the basis and derived words. High
precision and recall are achieved for N-A deriva-
tions. The recall is lowest for V-V derivations,
suggesting that the derivational phenomena for this
POS combination are not yet covered satisfactorily.
6.2 Error analysis
Table 8 shows the frequencies of true positives and
false positives on the P-sample and false negatives
on the R-sample for each annotated category. True
negatives are not reported, since their analysis gives
no deeper insight.
True positives. In our analysis we treated both R
and M pairs as related, but it is interesting to see
how many of the true positives are in fact semanti-
cally unrelated. Out of 1,663 pairs, 90% are seman-
tically as well as morphologically related (R), e.g.,
TPs FPs FNs
Label P-sample P-sample R-sample
R 1,492 ? 107
M 171 ? 60
N ? 216 ?
C ? 7 ?
L ? 114 ?
Total 1,663 337 167
Table 8: Predictions over annotated categories
alkoholisierenV ? antialkoholischA (to alcoholize
? nonalcoholic), BeschuldigungN ? unschuldigA
(accusation ? innocent). Most R pairs result from
high-accuracy rules, i.e., zero derivation, negation
prefixation and simple suffixation. The remaining
10% are only morphologically related (M), e.g.,
beschwingtA ? schwingenV (cheerful ? to swing),
StolzierenN ? stolzA (strut ? proud). In both pairs,
the two lemmas share a common semantic concept
? i.e., being in motion or being proud ? but nowa-
day?s meanings have grown apart from each other.
Among the M true positives, we observe prefixa-
tion derivations in 66% of the cases, often involv-
ing prefixation at both lemmas, e.g., ErdenklicheN
? bedenklichA (imaginable ? questionable).
False positives. We observe many errors in pairs
involving short lemmas, e.g., GenN ? genierenV
(gene ? to be embarrassed), where orthographic
context is unsufficient to reject the derivation.
About 64% of the 337 incorrect pairs are of class
N (unrelated lemmas). For example, the rule for
deriving nouns denoting a male person incorrectly
links MorseN ? M?rserN (Morse ? mortar). Tran-
sitively applied rules often produce incorrect pairs;
e.g., SpeicheN ? speicherbarA (spoke ? storable)
results from the rule chain SpeicheN ? SpeicherN
? speichernV ? speicherbarA (spoke? storage
? to store? storable). Chains that involve ablaut
shifts (cf. Section 3.1) can lead to surprising re-
sults, e.g., ErringungN ? rangiertA (achievement ?
shunted). Meanwhile, some pairs judged as un-
related by the annotators might conceivably be
weakly related, such as schl?rfenV and schlurfenV
(to sip ? to shuffle), both of which refer to specific
long drawn out sounds. About 20% out of these un-
related lemma pairs is due to derivations between
proper nouns (PNs) and common nouns. This hap-
pens especially for short PNs (cf. the above exam-
ple of Morse). However, since PNs also participate
in valid derivations (e.g., Chaplin ? chaplinesque),
1208
one could investigate their impact on derivations
rather than omitting them.
Errors of the category L ? 34% of the false posi-
tives ? are caused during preprocessing by the lem-
matizer. They cannot be blamed on our derivational
model, but of course form part of the output.
False negatives. Errors of this type are due to
missing derivation rules, erroneous rules that leave
some lemmas undiscovered, or the absence of lem-
mas in the corpus required for transitive closure.
About 64% of the 167 missed pairs are of category
R. About half of these pairs result from a lack of
prefixation rules ? mainly affecting verbs ? with a
wide variety of prefixes (zu-, um-, etc.), including
prepositional prefixes like herum- (around) or ?ber-
(over). We intentionally ignored these derivations,
since they frequently lead to semantically unrelated
pairs. In fact, merely five of the remaining 36%
false negative pairs (M) do not involve prefixation.
However, this analysis as well as the rather low cov-
erage for verb-involved rules (cf. Table 7) shows
that DERIVBASE might benefit from more prefix
rules. Apart from the lack of prefixation coverage
and a few other, rather infrequent rules, we did not
find any substantial deficits. Most of the remaining
errors are due to German idiosyncrasies and excep-
tional derivations, e.g., fahrenV ? FahrtN (drive ?
trip), where the regular zero derivation would result
in Fahr.
7 Conclusion and Future Work
In this paper, we present DERIVBASE, a deriva-
tional resource for German based on a rule-based
framework. A few work days were enough to build
the underlying rules with the aid of grammar text-
books. We collected derivational families for over
280,000 lemmas with high accuracy as well as solid
coverage. The resource is freely available.5
Our approach for compiling a derivational re-
source is not restricted to German. In addition
to the typologically most similar Germanic and
Romance languages, it is also applicable to agglu-
tinative languages like Finnish, or other fusional
languages like Russian. Its main requirements are
a large list of lemmas for the language (optionally
with further morphological features) and linguistic
literature on morphological patterns.
We have employed an evaluation method that
uses two separate samples to assess precision and
5http://goo.gl/7KG2U; license cc-by-sa 3.0
recall to deal with the high number of false neg-
atives. Our analyses indicate two interesting di-
rections for future work: (a) specific handling of
proper nouns, which partake in specific derivations;
and (b) the use of graph clustering instead of the
transitive closure to avoid errors resulting from
long transitive chains.
Finally, we plan to employ distributional seman-
tics methods (Turney and Pantel, 2010) to help re-
move semantically unrelated pairs as well as distin-
guish automatically between only morphologically
(M) or both morphologically and semantically (R)
related pairs. Last, but not least, this allows us to
group derivation rules according to their semantic
properties. For example, nouns with -er suffixes
often denote persons and are agentivizations of a
basis word (Bilgin et al, 2004).
Acknowledgments
The first and third authors were supported by
the EC project EXCITEMENT (FP7 ICT-287923).
The second author was supported by the Croatian
Science Foundation (project 02.03/162: ?Deriva-
tional Semantic Models for Information Retrieval?).
We thank the reviewers for their constructive com-
ments.
References
George W. Adamson and Jillian Boreham. 1974. The
use of an association measure based on character
structure to identify semantically related pairs of
words and document titles. Information Processing
and Management, 10(7/8):253?260.
Enrique Amig?, Julio Gonzalo, Javier Artiles, and Fe-
lisa Verdejo. 2009. A comparison of extrinsic
clustering evaluation metrics based on formal con-
straints. Information Retrieval, 12(4):461?486.
Gerhard Augst. 1975. Lexikon zur Wortbil-
dung. Forschungsberichte des Instituts f?r Deutsche
Sprache. Narr, T?bingen.
Harald R. Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1996. The CELEX Lexical Database. Re-
lease 2. LDC96L14. Linguistic Data Consortium,
University of Pennsylvania, Philadelphia, PA.
Kenneth R Beesley and Lauri Karttunen. 2003. Finite
state morphology, volume 18. CSLI publications
Stanford.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2012. Learning entailment relations by global graph
structure optimization. Computational Linguistics,
38(1):73?111.
1209
Balthazar Bickel and Johanna Nichols. 2001. Inflec-
tional morphology. In Timothy Shopen, editor, Lan-
guage Typology and Syntactic Description, Volume
III: Grammatical categories and the lexicon, pages
169?240. CUP, Cambridge.
Orhan Bilgin, ?zlem ?etinog?lu, and Kemal Oflazer.
2004. Morphosemantic relations in and across
Wordnets. In Proceedings of the Global WordNet
Conference, pages 60?66, Brno, Czech Republic.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 89?97, Beijing, China.
Eric Brill. 1992. A simple rule-based part of speech
tagger. In Proceedings of the Workshop on Speech
and Natural Language, pages 112?116, Harriman,
New York.
Jean C. Carletta. 1996. Assessing agreement on clas-
sification tasks: the kappa statistic. Computational
Linguistics, 22(2):249?254.
Jacob Cohen. 1968. Weighted kappa: Nominal scale
agreement with provision for scaled disagreement or
partial credit. Psychological Bulletin, 70:213?220.
B?atrice Daille, C?cile Fabre, and Pascale S?billot.
2002. Applications of computational morphology.
In Paul Boucher, editor, Many Morphologies, pages
210?234. Cascadilla Press.
Herv? D?jean. 1998. Morphemes as necessary concept
for structures discovery from untagged corpora. In
Proceedings of the Joint Conferences on New Meth-
ods in Language Processing and Computational Nat-
ural Language Learning, pages 295?298, Sydney,
Australia.
Gertrud Faa?, Ulrich Heid, and Helmut Schmid. 2010.
Design and application of a gold standard for mor-
phological analysis: SMOR in validation. In Pro-
ceedings of the Seventh International Conference
on Language Resources and Evaluation, pages 803?
810.
Christiane Fellbaum, Anne Osherson, and Peter Clark.
2009. Putting semantics into WordNet?s "morphose-
mantic" links. In Proceedings of the Third Language
and Technology Conference, pages 350?358, Poz-
nan?, Poland.
Wolfgang Finkler and G?nter Neumann. 1988. Mor-
phix - a fast realization of a classification-based ap-
proach to morphology. In Proceedings of 4th Aus-
trian Conference of Artificial Intelligence, pages 11?
19, Vienna, Austria.
Arne Fitschen. 2004. Ein computerlinguistisches
Lexikon als komplexes System. Ph.D. thesis, IMS,
Universit?t Stuttgart.
?ric Gaussier. 1999. Unsupervised learning of deriva-
tional morphology from inflectional lexicons. In
ACL?99 Workshop Proceedings on Unsupervised
Learning in Natural Language Processing, pages
24?30, College Park, Maryland, USA.
Rebecca Green, Bonnie J. Dorr, and Philip Resnik.
2004. Inducing frame semantic verb classes from
wordnet and ldoce. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, pages 375?382, Barcelona, Spain.
Nizar Habash and Bonnie Dorr. 2003. A categorial
variation database for English. In Proceedings of
the Anuual Meeting of the North American Associ-
ation for Computational Linguistics, pages 96?102,
Edmonton, Canada.
Harald Hammarstr?m and Lars Borin. 2011. Unsuper-
vised learning of morphology. Computational Lin-
guistics, 37(2):309?350.
Wolfgang Hoeppner. 1980. Derivative Wortbildung
der deutschen Gegenwartssprache und ihre algorith-
mische Analyse. Narr, T?bingen.
Lauri Karttunen and Kenneth R Beesley. 1992. Two-
level rule compiler. Xerox Corporation. Palo Alto
Research Center.
Lauri Karttunen and Kenneth R. Beesley. 2005.
Twenty-five years of finite-state morphology. In
Antti Arppe, Lauri Carlson, Krister Lind?n, Jussi Pi-
itulainen, Mickael Suominen, Martti Vainio, Hanna
Westerlund, and Anssi Yli-Jyr, editors, Inquiries
into Words, Constraints and Contexts. Festschrift for
Kimmo Koskenniemi on his 60th Birthday, pages 71?
83. CSLI Publications, Stanford, California.
Kimmo Koskenniemi. 1983. Two-level Morphology:
A General Computational Model for Word-Form
Recognition and Production. Ph.D. thesis, Univer-
sity of Helsinki.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A lexicon of nominalizations. In In Proceedings of
Euralex98, pages 187?193.
Prasenjit Majumder, Mandar Mitra, Swapan K. Parui,
Gobinda Kole, Pabitra Mitra, and Kalyankumar
Datta. 2007. YASS: Yet another suffix strip-
per. ACM Transactions on Information Systems,
25(4):18:1?18:20.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a
two-stage discriminative parser. In In Proceedings
of the Conference on Computational Natural Lan-
guage Learning, pages 216?220, New York, NY.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. Annotating noun ar-
gument structure for NomBank. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation, Lisbon, Portugal.
1210
Petar Milin, Victor Kuperman, Aleksandar Kostic, and
R Harald Baayen. 2009. Paradigms bit by bit: An
information theoretic approach to the processing of
paradigmatic structure in inflection and derivation.
Analogy in grammar: Form and acquisition, pages
214?252.
Karel Pala and Dana Hlav?c?kov?. 2007. Derivational
relations in Czech WordNet. In Proceedings of the
ACL Workshop on Balto-Slavonic Natural Language
Processing: Information Extraction and Enabling
Technologies, pages 75?81.
Maciej Piasecki, Radoslaw Ramocki, and Marek
Maziarz. 2012. Recognition of Polish derivational
relations based on supervised learning scheme. In
Proceedings of the Eighth International Conference
on Language Resources and Evaluation, pages 916?
922, Istanbul, Turkey.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Anne Schiller, Simone Teufel, Christine St?ckert, and
Christine Thielen. 1999. Guidelines f?r das Tag-
ging deutscher Textcorpora mit STTS. Technical
report, Institut fur maschinelle Sprachverarbeitung,
Stuttgart.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
Smor: A German computational morphology cover-
ing derivation, composition and inflection. In Pro-
ceedings of the Fourth International Conference on
Language Resources and Evaluation, Lisbon, Portu-
gal.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49, Manchester, UK.
Patrick Schone and Daniel Jurafsky. 2000.
Knowledge-free induction of morphology us-
ing latent semantic analysis. In Proceedings of the
Conference on Natural Language Learning, pages
67?72, Lisbon, Portugal.
Jan ?najder and Bojana Dalbelo Ba?ic?. 2008. Higher-
order functional representation of Croatian inflec-
tional morphology. In Proceedings of the 6th In-
ternational Conference on Formal Approaches to
South Slavic and Balkan Languages, pages 121?130,
Dubrovnik, Croatia.
Jan ?najder and Bojana Dalbelo Ba?ic?. 2010. A
computational model of Croatian derivational mor-
phology. In Proceedings of the 7th International
Conference on Formal Approaches to South Slavic
and Balkan Languages, pages 109?118, Dubrovnik,
Croatia.
Idan Szpektor and Ido Dagan. 2008. Learning en-
tailment rules for unary templates. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 849?856, Manchester, UK.
Kapil Thadani and Kathleen McKeown. 2011. To-
wards strict sentence intersection: Decoding and
evaluation strategies. In Proceedings of the ACL
Workshop on Monolingual Text-To-Text Generation,
pages 43?53, Portland, Oregon.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188.
Andrea Zielinski and Christian Simon. 2008. Mor-
phisto - an open source morphological analyzer for
German. In Proceedings of the 7th International
Workshop on Finite-State Methods and Natural Lan-
guage Processing, pages 224?231, Ispra, Italy.
1211
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 731?735,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Derivational Smoothing for Syntactic Distributional Semantics
Sebastian Pado?? Jan S?najder? Britta Zeller?
?Heidelberg University, Institut fu?r Computerlinguistik
69120 Heidelberg, Germany
?University of Zagreb, Faculty of Electrical Engineering and Computing
Unska 3, 10000 Zagreb, Croatia
{pado, zeller}@cl.uni-heidelberg.de jan.snajder@fer.hr
Abstract
Syntax-based vector spaces are used widely
in lexical semantics and are more versatile
than word-based spaces (Baroni and Lenci,
2010). However, they are also sparse, with
resulting reliability and coverage problems.
We address this problem by derivational
smoothing, which uses knowledge about
derivationally related words (oldish? old)
to improve semantic similarity estimates.
We develop a set of derivational smoothing
methods and evaluate them on two lexical
semantics tasks in German. Even for mod-
els built from very large corpora, simple
derivational smoothing can improve cover-
age considerably.
1 Introduction
Distributional semantics (Turney and Pantel, 2010)
builds on the assumption that the semantic similar-
ity of words is strongly correlated to the overlap
between their linguistic contexts. This hypothesis
can be used to construct context vectors for words
directly from large text corpora in an unsupervised
manner. Such vector spaces have been applied suc-
cessfully to many problems in NLP (see Turney and
Pantel (2010) or Erk (2012) for current overviews).
Most distributional models in computational lex-
ical semantics are either (a) bag-of-words models,
where the context features are words within a sur-
face window around the target word, or (b) syn-
tactic models, where context features are typically
pairs of dependency relations and context words.
The advantage of syntactic models is that they
incorporate a richer, structured notion of context.
This makes them more versatile; the Distributional
Memory framework by Baroni and Lenci (2010) is
applicable to a wide range of tasks. It is also able ?
at least in principle ? to capture more fine-grained
types of semantic similarity such as predicate-
argument plausibility (Erk et al, 2010). At the
same time, syntactic spaces are much more prone
to sparsity problems, as their contexts are sparser.
This leads to reliability and coverage problems.
In this paper, we propose a novel strategy
for combating sparsity in syntactic vector spaces,
derivational smoothing. It follows the intuition that
derivationally related words (feed ? feeder, blocked
? blockage) are, as a rule, semantically highly simi-
lar. Consequently, knowledge about derivationally
related words can be used as a ?back off? for sparse
vectors in syntactic spaces. For example, the pair
oldish ? ancient should receive a high semantic
similarity, but in practice, the vector for oldish will
be very sparse, which makes this result uncertain.
Knowing that oldish is derivationally related to old
allows us to use the much less sparse vector for old
as a proxy for oldish.
We present a set of general methods for smooth-
ing vector similarity computations given a resource
that groups words into derivational families (equiv-
alence classes) and evaluate these methods on Ger-
man for two distributional tasks (similarity predic-
tion and synonym choice). We find that even for
syntactic models built from very large corpora, a
simple derivational resource that groups words on
morphological grounds can improve the results.
2 Related Work
Smoothing techniques ? either statistical, distribu-
tional, or knowledge-based ? are widely applied in
all areas of NLP. Many of the methods were first
applied in Language Modeling to deal with unseen
n-grams (Chen and Goodman, 1999; Dagan et al,
1999). Query expansion methods in Information
Retrieval are also prominent cases of smoothing
that addresses the lexical mismatch between query
and document (Voorhees, 1994; Gonzalo et al,
1998; Navigli and Velardi, 2003). In lexical se-
mantics, smoothing is often achieved by backing
731
off from words to semantic classes, either adopted
from a resource such as WordNet (Resnik, 1996) or
induced from data (Pantel and Lin, 2002; Wang et
al., 2005; Erk et al, 2010). Similarly, distributional
features support generalization in Named Entity
Recognition (Finkel et al, 2005).
Although distributional information is often used
for smoothing, to our knowledge there is little
work on smoothing distributional models them-
selves. We see two main precursor studies for our
work. Bergsma et al (2008) build models of se-
lectional preferences that include morphological
features such as capitalization and the presence of
digits. However, their approach is task-specific and
requires a (semi-)supervised setting. Allan and Ku-
maran (2003) make use of morphology by building
language models for stemming-based equivalence
classes. Our approach also uses morphological
processing, albeit more precise than stemming.
3 A Resource for German Derivation
Derivational morphology describes the process of
building new (derived) words from other (basis)
words. Derived words can, but do not have to, share
the part-of-speech (POS) with their basis (oldA?
oldishA vs. warmA? warmV , warmthN ). Words
can be grouped into derivational families by form-
ing the transitive closure over individual derivation
relations. The words in these families are typically
semantically similar, although the exact degree de-
pends on the type of relation and idiosyncratic fac-
tors (bookN ? bookishA, Lieber (2009)).
For German, there are several resources with
derivational information. We use version 1.3
of DERIVBASE (Zeller et al, 2013),1 a freely
available resource that groups over 280,000 verbs,
nouns, and adjectives into more than 17,000 non-
singleton derivational families. It has a precision of
84% and a recall of 71%. Its higher coverage com-
pared to CELEX (Baayen et al, 1996) and IMSLEX
(Fitschen, 2004) makes it particularly suitable for
the use in smoothing, where the resource should
include low-frequency lemmas.
The following example illustrates a family that
covers three POSes as well as a word with a pre-
dominant metaphorical reading (to kneel? to beg):
knieenV (to kneelV ), beknieenV (to
begV ), KniendeN (kneeling personN ),
kniendA (kneelingA), KnieNn (kneeN )
1Downloadable from: http://goo.gl/7KG2U
Using derivational knowledge for smoothing raises
the question of how semantically similar the lem-
mas within a family really are. Fortunately, DE-
RIVBASE provides information that can be used in
this manner. It was constructed with hand-written
derivation rules, employing string transformation
functions that map basis lemmas onto derived lem-
mas. For example, a suffixation rule using the affix
?heit? generates the derivation dunkel ? Dunkel-
heit (darkA ? darknessN ). Since derivational fam-
ilies are defined as transitive closures, each pair
of words in a family is connected by a derivation
path. Because the rules do not have a perfect pre-
cision, our confidence in pairs of words decreases
the longer the derivation path between them. To re-
flect this, we assign each pair a confidence of 1/n,
where n is the length of the shortest path between
the lemmas. For example, bekleiden (enrobeV ) is
connected to Verkleidung (disguiseN ) through three
steps via the lemmas kleiden (dressV ) and verklei-
den (disguiseV ) and is assigned the confidence 1/3.
4 Models for Derivational Smoothing
Derivational smoothing exploits the fact that deriva-
tionally related words are also semantically related,
by combining and/or comparing distributional rep-
resentations of derivationally related words. The
definition of a derivational smoothing algorithm
consists of two parts: a trigger and a scheme.
Notation. Given a word w, we use w to denote
its distributional vector and D(w) to denote the set
of vectors for the derivational family of w. We
assume that w ? D(w). For words that have no
derivations in DERIVBASE, D(w) is a singleton
set, D(w) = {w}. Let ?(w,w?) denote the confi-
dence of the pair (w,w?), as explained in Section 3.
Smoothing trigger. As discussed above, there is
no guarantee for high semantic similarity within a
derivational family. For this reason, smoothing may
also drown out information. In this paper, we report
on two triggers: smooth always always performs
smoothing; smooth if sim=0 smooths only when
the unsmoothed similarity sim(w1,w2) is zero or
unknown (due to w1 or w2 not being in the model).
Smoothing scheme. We present three smoothing
schemes, all of which apply to the level of complete
families. The first two schemes are exemplar-based
schemes, which define the smoothed similarity for
a word pair as a function of the pairwise similarities
between all words of the two derivational families.
732
The first one, maxSim, checks for particularly simi-
lar words in the families:
maxSim(w1, w2) = max
w?1?D(w1)
w?2?D(w2)
sim(w?1,w?2)
The second one, avgSim, computes the average
pairwise similarity (N is the number of pairs):
avgSim(w1, w2) =
1
N
?
w?1?D(w1)
w?2?D(w2)
sim(w?1,w?2)
The third scheme, centSim, is prototype-based. It
computes a centroid vector for each derivational
family, which can be thought of as a representation
for the concept(s) that it expresses:
centSim(w1, w2) = sim
(
c(D(w1)), c(D(w2))
)
where c(D(w)) =?w??D(w) ?(w,w?) ?w? is the
confidence-weighted centroid vector. centSim is
similar to avgSim. It is more efficient to calculate
and effectively introduces a kind or regularization,
where outliers in either family have less impact on
the overall result.
These models only represents a sample of possi-
ble derivational smoothing methods. We performed
a number of additional experiments (POS-restricted
smoothing, word-based, and pair-based smoothing
triggers), but they did not yield any improvements
over the simpler models we present here.
5 Experimental Evaluation
Syntactic Distributional Model. The syntactic
distributional model that we use represents target
words by pairs of dependency relations and context
words. More specifically, we use the W ? LW
matricization of DM.DE, the German version (Pado?
and Utt, 2012) of Distributional Memory (Baroni
and Lenci, 2010). DM.DE was created on the basis
of the 884M-token SDEWAC web corpus (Faa? et
al., 2010), lemmatized, tagged, and parsed with the
German MATE toolkit (Bohnet, 2010).
Experiments. We evaluate the impact of smooth-
ing on two standard tasks from lexical semantics.
The first task is predicting semantic similarity. We
lemmatized and POS-tagged the German GUR350
dataset (Zesch et al, 2007), a set of 350 word pairs
with human similarity judgments, created analo-
gously to the well-known Rubenstein and Good-
enough (1965) dataset for English.2 We predict
2Downloadable from: http://goo.gl/bFokI
semantic similarity as cosine similarity. We make
a prediction for a word pair if both words are repre-
sented in the semantic space and their vectors have
a non-zero similarity.
The second task is synonym choice on the Ger-
man version of the Reader?s Digest WordPower
dataset (Wallace and Wallace, 2005).2 This dataset,
which we also lemmatized and POS-tagged, con-
sists of 984 target words with four synonym can-
didates each (including phrases), one of which is
correct. Again, we compute semantic similarity as
the cosine between target and a candidate vector
and pick the highest-similarity candidate as syn-
onym. For phrases, we compute the maximum
pairwise word similarity. We make a prediction for
an item if the target as well as at least one candi-
date are represented in the semantic space and their
vectors have a non-zero similarity.
We expect differences between the two tasks
with regard to derivational smoothing, since the
words within derivational families are generally re-
lated but often not synonymous (cf. the example
in Section 3). Thus, semantic similarity judgments
should profit more easily from derivational smooth-
ing than synonym choice.
Baseline. Our baseline is a standard bag-of-
words vector space (BOW), which represents target
words by the words occurring in their context. We
use standard parameters (?10 word window, 8.000
most frequent verb, noun, and adjective lemmas).
The model was created from the same corpus as
DM.DE. We also applied derivational smoothing
to this model, but did not obtain improvements.
Evaluation. To analyze the impact of smoothing,
we evaluate the coverage of models and the quality
of their predictions separately. In both tasks, cover-
age is the percentage of items for which we make
a prediction. We measure quality of the semantic
similarity task as the Pearson correlation between
the model predictions and the human judgments
for covered items (Zesch et al, 2007). For syn-
onym choice, we follow the method established by
Mohammad et al (2007), measuring accuracy over
covered items, with partial credit for ties.
Results for Semantic Similarity. Table 1 shows
the results for the first task. The unsmoothed
DM.DE model attains a correlation of r = 0.44
and a coverage of 58.9%. Smoothing increases the
coverage substantially to 88%. Additionally, con-
servative, prototype-based smoothing (if sim = 0)
733
Smoothing
trigger
Smoothing
scheme
r Cov
%
DM.DE, unsmoothed .44 58.9
DM.DE,
smooth always
avgSim .30 88.0
maxSim .43 88.0
centSim .44 88.0
DM.DE,
smooth if sim = 0
avgSim .43 88.0
maxSim .42 88.0
centSim .47 88.0
BOW baseline .36 94.9
Table 1: Results on the semantic similarity task
(r: Pearson correlation, Cov: Coverage)
increases correlation somewhat to r = 0.47. The
difference to the unsmoothed model is not signif-
icant at p = 0.05 according to Fisher?s (1925)
method of comparing correlation coefficients.
The bag-of-words baseline (BOW) has a greater
coverage than DM.DE models, but at the cost
of lower correlation across the board. The only
DM.DE model that performs worse than the BOW
baseline is the non-conservative avgSim (average
similarity) scheme. We attribute this weak per-
formance to the presence of many pairwise zero
similarities in the data, which makes the avgSim
predictions unreliable.
To our knowledge, there are no previous pub-
lished papers on distributional approaches to mod-
eling this dataset. The best previous result is a
GermaNet/Wikipedia-based model by Zesch et al
(2007). It reports a higher correlation (r = 0.59)
but a very low coverage at 33.1%.
Results for Synonym Choice. The results for
the second task are shown in Table 2. The un-
smoothed model achieves an accuracy of 53.7%
and a coverage of 80.8%, as reported by Pado?
and Utt (2012). Smoothing increases the cover-
age by almost 6% to 86.6% (for example, a ques-
tion item for inferior becomes covered after back-
ing off from the target to Inferiorita?t (inferiority)).
All smoothed models show a loss in accuracy, al-
beit small. The best model is again a conservative
smoothing model (sim = 0) with a loss of 1.1% ac-
curacy. Using bootstrap resampling (Efron and Tib-
shirani, 1993), we established that the difference
to the unsmoothed DM.DE model is not signifi-
cant at p < 0.05. This time, the avgSim (average
similarity) smoothing scheme performs best, with
the prototype-based scheme in second place. Thus,
the results for synonym choice are less clear-cut:
derivational smoothing can trade accuracy against
Smoothing
trigger
Smoothing
scheme
Acc
%
Cov
%
DM.DE, unsmoothed (Pado? & Utt 2012) 53.7 80.8
DM.DE,
smooth always
avgSim 46.0 86.6
maxSim 50.3 86.6
centSim 49.1 86.6
DM.DE,
smooth if sim = 0
avgSim 52.6 86.6
maxSim 51.2 86.6
centSim 51.3 86.6
BOW ?baseline? 56.9 98.5
Table 2: Results on the synonym choice task
(Acc: Accuracy, Cov: Coverage)
coverage but does not lead to a clear improvement.
What is more, the BOW ?baseline? significantly
outperforms all syntactic models, smoothed and
unsmoothed, with an almost perfect coverage com-
bined with a higher accuracy.
6 Conclusions and Outlook
In this paper, we have introduced derivational
smoothing, a novel strategy to combating sparsity
in syntactic vector spaces by comparing and com-
bining the vectors of morphologically related lem-
mas. The only information strictly necessary for
the methods we propose is a grouping of lemmas
into derivationally related classes. We have demon-
strated that derivational smoothing improves two
tasks, increasing coverage substantially and also
leading to a numerically higher correlation in the
semantic similarity task, even for vectors created
from a very large corpus. We obtained the best re-
sults for a conservative approach, smoothing only
zero similarities. This also explains our failure
to improve less sparse word-based models, where
very few pairs are assigned a similarity of zero.
A comparison of prototype- and exemplar-based
schemes did not yield a clear winner. The estima-
tion of generic semantic similarity can profit more
from derivational smoothing than the induction of
specific lexical relations.
In future work, we plan to work on other eval-
uation tasks, application to other languages, and
more sophisticated smoothing schemes.
Acknowledgments. Authors 1 and 3 were sup-
ported by the EC project EXCITEMENT (FP7 ICT-
287923). Author 2 was supported by the Croatian
Science Foundation (project 02.03/162: ?Deriva-
tional Semantic Models for Information Retrieval?).
We thank Jason Utt for his support and expertise.
734
References
James Allan and Giridhar Kumaran. 2003. Stemming
in the Language Modeling Framework. In Proceed-
ings of SIGIR, pages 455?456.
Harald R. Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1996. The CELEX Lexical Database. Re-
lease 2. LDC96L14. Linguistic Data Consortium,
University of Pennsylvania, Philadelphia, Pennsyl-
vania.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A General Framework for
Corpus-based Semantics. Computational Linguis-
tics, 36(4):673?721.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative Learning of Selectional Preference
from Unlabeled Text. In Proceedings of EMNLP,
pages 59?68, Honolulu, Hawaii.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 89?97, Beijing, China.
Stanley F. Chen and Joshua Goodman. 1999. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Computer Speech and Language,
13(4):359?394.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-Based Models of Word Cooccur-
rence Probabilities. Machine Learning, 34(1?3):43?
69.
Bradley Efron and Robert J. Tibshirani. 1993. An
Introduction to the Bootstrap. Chapman and Hall,
New York.
Katrin Erk, Sebastian Pado?, and Ulrike Pado?. 2010.
A Flexible, Corpus-driven Model of Regular and In-
verse Selectional Preferences. Computational Lin-
guistics, 36(4):723?763.
Katrin Erk. 2012. Vector Space Models of Word Mean-
ing and Phrase Meaning: A Survey. Language and
Linguistics Compass, 6(10):635?653.
Gertrud Faa?, Ulrich Heid, and Helmut Schmid. 2010.
Design and Application of a Gold Standard for Mor-
phological Analysis: SMOR in Validation. In Pro-
ceedings of LREC-2010, pages 803?810.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the 43rd Annual Meet-
ing of the ACL, pages 363?370.
Ronald Aylmer Fisher. 1925. Statistical methods for
research workers. Oliver and Boyd, Edinburgh.
Arne Fitschen. 2004. Ein computerlinguistisches
Lexikon als komplexes System. Ph.D. thesis, IMS,
Universita?t Stuttgart.
Julio Gonzalo, Felisa Verdejo, Irina Chugur, and
Juan M. Cigarra?n. 1998. Indexing with WordNet
Synsets Can Improve Text Retrieval. In Proceed-
ings of the COLING/ACL Workshop on Usage of
WordNet in Natural Language Processing Systems,
Montre?al, Canada.
Rochelle Lieber. 2009. Morphology and Lexical Se-
mantics. Cambridge University Press.
Saif Mohammad, Iryna Gurevych, Graeme Hirst, and
Torsten Zesch. 2007. Cross-Lingual Distributional
Profiles of Concepts for Measuring Semantic Dis-
tance. In Proceedings of the 2007 Joint Conference
on EMNLP and CoNLL, pages 571?580, Prague,
Czech Republic.
Roberto Navigli and Paola Velardi. 2003. An Analysis
of Ontology-based Query Expansion Strategies. In
Workshop on Adaptive Text Extraction and Mining,
Dubrovnik, Croatia.
Sebastian Pado? and Jason Utt. 2012. A Distributional
Memory for German. In Proceedings of KONVENS
2012 workshop on lexical-semantic resources and
applications, pages 462?470, Vienna, Austria.
Patrick Pantel and Dekang Lin. 2002. Discovering
Word Senses from Text. In In Proceedings of ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining, pages 613?619.
Philip Resnik. 1996. Selectional Constraints: An
Information-theoretic Model and its Computational
Realization. Cognition, 61(1-2):127?159.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual Correlates of Synonymy. Communica-
tions of the ACM, 8(10):627?633.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188.
Ellen M. Voorhees. 1994. Query Expansion Using
Lexical-semantic Relations. In Proceedings of SI-
GIR, pages 61?69.
DeWitt Wallace and Lila Acheson Wallace. 2005.
Reader?s Digest, das Beste fu?r Deutschland. Verlag
Das Beste, Stuttgart.
Qin Iris Wang, Dale Schuurmans, and Dekang Lin.
2005. Strictly Lexical Dependency Parsing. In Pro-
ceedings of IWPT, pages 152?159.
Britta Zeller, Jan S?najder, and Sebastian Pado?. 2013.
DErivBase: Inducing and Evaluating a Derivational
Morphology Resource for German. In Proceedings
of ACL, Sofia, Bulgaria.
Torsten Zesch, Iryna Gurevych, and Max Mu?hlha?user.
2007. Comparing Wikipedia and German Word-
net by Evaluating Semantic Relatedness on Multi-
ple Datasets. In Proceedings of NAACL/HLT, pages
205?208, Rochester, NY.
735
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 784?789,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Building and Evaluating a Distributional Memory for Croatian
Jan S?najder? Sebastian Pado?? Z?eljko Agic??
?University of Zagreb, Faculty of Electrical Engineering and Computing
Unska 3, 10000 Zagreb, Croatia
?Heidelberg University, Institut fu?r Computerlinguistik
69120 Heidelberg, Germany
?University of Zagreb, Faculty of Humanities and Social Sciences
Ivana Luc?ic?a 3, 10000 Zagreb, Croatia
jan.snajder@fer.hr pado@cl.uni-heidelberg.de zagic@ffzg.hr
Abstract
We report on the first structured dis-
tributional semantic model for Croatian,
DM.HR. It is constructed after the model
of the English Distributional Memory (Ba-
roni and Lenci, 2010), from a dependency-
parsed Croatian web corpus, and covers
about 2M lemmas. We give details on the
linguistic processing and the design prin-
ciples. An evaluation shows state-of-the-
art performance on a semantic similarity
task with particularly good performance on
nouns. The resource is freely available.
1 Introduction
Most current work in lexical semantics is based
on the Distributional Hypothesis (Harris, 1954),
which posits a correlation between the degree of
words? semantic similarity and the similarity of
the contexts in which they occur. Using this hy-
pothesis, word meaning representations can be ex-
tracted from large corpora. Words are typically rep-
resented as vectors whose dimensions correspond
to context features. The vector similarities, which
are interpreted as semantic similarities, are used in
numerous applications (Turney and Pantel, 2010).
Most vector spaces in current use are either word-
based (co-occurrence defined by surface window,
context words as dimensions) or syntax-based (co-
occurrence defined syntactically, syntactic objects
as dimensions). Syntax-based models have sev-
eral desirable properties. First, they are model to
fine-grained types of semantic similarity such as
predicate-argument plausibility (Erk et al, 2010).
Second, they are more versatile ? Baroni and Lenci
(2010) have presented a generic framework, the
Distributional Memory (DM), which is applicable
to a wide range of tasks beyond word similarity.
Third, they avoid the ?syntactic assumption? in-
herent in word-based models, namely that context
words are relevant iff they are in an n-word window
around the target. This property is particularly rele-
vant for free word order languages with many long
distance dependencies and non-projective structure
(Ku?bler et al, 2009). Their obvious problem, of
course, is that they require a large parsed corpus.
In this paper, we describe the construction of
a Distributional Memory for Croatian (DM.HR),
a free word order language. To do so, we parse
hrWaC (Ljubes?ic? and Erjavec, 2011), a 1.2B-token
Croatian web corpus. We evaluate DM.HR on a
synonym choice task, where it outperforms the
standard bag-of-word model for nouns and verbs.
2 Related Work
Vector space semantic models have been applied
to a number of Slavic languages, including Bul-
garian (Nakov, 2001a), Czech (Smrz? and Rychly?,
2001), Polish (Piasecki, 2009; Broda et al, 2008;
Broda and Piasecki, 2008), and Russian (Nakov,
2001b; Mitrofanova et al, 2007). Previous work
on distributional semantic models for Croatian
dealt with similarity prediction (Ljubes?ic? et al,
2008; Jankovic? et al, 2011) and synonym detec-
tion (Karan et al, 2012), however using only word-
based and not syntactic-based models.
So far the only DM for a language other than
English is the German DM.DE by Pado? and Utt
(2012), who describe the process of building
DM.DE and the evaluation on a synonym choice
task. Our work is similar, though each language
has its own challenges. Croatian, like other Slavic
languages, has rich inflectional morphology and
free word order, which lead to errors in linguistic
processing and affect the quality of the DM.
784
3 Distributional Memory
DM represents co-occurrence information in a gen-
eral, non-task-specific manner, as a tensor, i.e., a
three-dimensional matrix, of weighted word-link-
word tuples. Each tuple is mapped onto a number
by scoring function ? : W ? L ?W ? R+, that
reflects the strength of the association. When a par-
ticular task is selected, a vector space for this task
can be generated from the tensor by matricization.
Regarding the examples from Section 1, synonym
discovery would use a word by link-word space
(W ? LW ), which contains vectors for words w
represented by pairs ?l, w? of a link and a context
word. Analogy discovery would use a word-word
by link space (WW ? L), which represents word
pairs ?w1, w2? by vectors over links l.
The links can be chosen to model any relation
of interest between words. However, as noted by
Pado? and Utt (2012), dependency relations are the
most obvious choice. Baroni and Lenci (2010) in-
troduce three dependency-based DM variants: De-
pDM, LexDM, and TypeDM. DepDM uses links
that correspond to dependency relations, with sub-
categorization for subject (subj tr and subj intr)
and object (obj and iobj). Furthermore, all prepo-
sitions are lexicalized into links (e.g., ?sun, on,
Sunday?). Finally, the tensor is symmetrized: for
each tuple ?w1, l, w2?, its inverse ?w2, l?1, w1? is
included. The other two variants are more complex:
LexDM uses more lexicalized links, encoding, e.g.,
lexical material between the words, while TypeDM
extends LexDM with a scoring function based on
lexical variability.
Following the work of Pado? and Utt (2012), we
build a DepDM variant for DM.HR. Although Ba-
roni and Lenci (2010) show that TypeDM can out-
perform the other two variants, DepDM often per-
forms at a comparable level, while being much
simpler to build and more efficient to compute.
4 Building DM.HR
To build DM.HR, we need to collect co-occurrence
counts from a corpus. Since no sufficiently large
suitable corpus exists for Croatian, we first explain
how we preprocessed, tagged, and parsed the data.
Corpus and preprocessing. We adopted hrWaC,
the 1.2B-token Croatian web corpus (Ljubes?ic? and
Erjavec, 2011), as starting point. hrWaC was built
with the aim of obtaining a cleaner-than-usual web
corpus. To this end, a conservative boilerplate re-
moval procedure was used; Ljubes?ic? and Erjavec
(2011) report a precision of 97.9% and a recall of
70.7%. Nonetheless, our inspection revealed that,
apart from the unavoidable spelling and grammati-
cal errors, hrWaC still contains non-textual content
(e.g., code snippets and formatting structure), en-
coding errors, and foreign-language content. As
this severely affects linguistic processing, we addi-
tionally filtered the corpus.
First, we removed from hrWaC the content
crawled from main discussion forum and blog web-
sites. This content is highly ungrammatical and
contains a lot of non-diacriticized text, typical for
user-generated content. This step alone removed
one third of the data. We processed the remaining
content with a tokenizer and a sentence segmenter
based on regular expressions, obtaining 66M sen-
tences. Next, we applied a series of heuristic filters
at the document- and sentence-level. At the doc-
ument level, we discard all documents (1) whose
length is below a specified threshold, (2) contain
no diacritics, (3) contain no words from a list of fre-
quent Croatian words, or (4) contain a single word
from lists of distinctive foreign-language words
(for Serbian). The last two steps serve to eliminate
foreign-language content. In particular, the last
step serves to filter out the text in Serbian, which at
the sentence-level is difficult to automatically dis-
criminate from Croatian. At the sentence-level, we
discard sentences that are (1) shorter than a speci-
fied threshold, (2) contain non-standard symbols,
(3) contain non-diacriticized Croatian words, or
(4) contain too many foreign words from a list of
foreign-language words (for English and Slovene).
The last step filters out specifically the sentences
in English and Slovene, as we found that these of-
ten occur mixed with text in Croatian. The final
filtered version of hrWaC contains 51M sentences
and 1.2B tokens. The corpus is freely available for
download, along with a more detailed description
of the preprocessing steps.1
Tagging, lemmatization, and parsing. For mor-
phosyntactic (MSD) tagging, lemmatization, and
dependency parsing of hrWaC, we use freely avail-
able tools with models trained on the new SETimes
Corpus of Croatian (SETIMES.HR), based on the
Croatian part of the SETimes parallel corpus.2 SE-
TIMES.HR and the derived tools are prototypes
1http://takelab.fer.hr/data
2http://www.nljubesic.net/resources/
corpora/setimes/
785
SETIMES.HR Wikipedia
HunPos (POS only) 97.1 94.1
HunPos (full MSD) 87.7 81.5
CST lemmatizer 97.7 96.5
MSTParser 77.5 68.8
Table 1: Tagging, lemmatization, and parsing accu-
racy
that are about to be released as parts of another
work. Here we give a general description and a
re-evaluation that we consider relevant for building
DM.HR.
SETIMES.HR consists of 90K tokens and 4K
sentences, manually lemmatized and MSD-tagged
according to Multext East v4 tagset (Erjavec, 2012),
with the help of the Croatian Lemmatization Server
(Tadic?, 2005). It is used also as a basis for a novel
formalism for syntactic annotation and dependency
parsing of Croatian (Agic? and Merkler, 2013).
On the basis of previous evaluation for Croa-
tian (Agic? et al, 2008; Agic? et al, 2009; Agic?,
2012) and availability and licensing considerations,
we chose HunPos tagger (Hala?csy et al, 2007),
CST lemmatizer (Ingason et al, 2008), and MST-
Parser (McDonald et al, 2006) to process hrWaC.
We evaluated the tools on 100-sentence test sets
from SETIMES.HR and Wikipedia; performance
on Wikipedia should be indicative of the perfor-
mance on a cross-domain dataset, such as hrWaC.
In Table 1 we show lemmatization and tagging ac-
curacy, as well as dependency parsing accuracy
in terms of labeled attachment score (LAS). The
results show that lemmatization, tagging and pars-
ing accuracy improves on the state of the art for
Croatian. The SETIMES.HR dependency parsing
models are publicly available.3
Syntactic patterns. We collect the co-occur-
rence counts of tuples using a set of syntactic pat-
terns. The patterns effectively define the link types,
and hence the dimensions of the semantic space.
Similar to previous work, we use two sorts of links:
unlexicalized and lexicalized.
For unlexicalized links, we use ten syntactic pat-
terns. These correspond to the main dependency re-
lations produced by our parser: Pred for predicates,
Atr for attributes, Adv for adverbs, Atv for verbal
complements, Obj for objects, Prep for preposi-
tions, and Pnom for nominal predicates. We sub-
categorized the subject relation into Sub tr (sub-
3http://zeljko.agic.me/resources/
Link P (%) R (%) F1 (%)
Unlexicalized
Adv 57.3 52.7 54.9
Atr 85.0 89.3 87.1
Atv 75.3 70.9 73.1
Obj 71.4 71.7 71.5
Pnom 55.7 50.8 53.1
Pred 81.8 70.6 75.8
Prep 50.0 28.6 36.4
Sb tr 67.8 73.8 70.7
Sb intr 64.5 64.8 64.7
Verb 61.6 73.6 67.1
Lexicalized
Prepositions 67.2 67.9 67.5
Verbs 61.6 73.6 67.1
All links 73.7 75.5 74.6
Table 2: Tuple extraction performance on SE-
TIMES.HR
jects of transitive verbs) and Sub intr (subject of
intransitive verbs). The motivation for this is better
modeling of verb semantics by capturing diathe-
sis alternations. In particular, for many Croatian
verbs reflexivization introduces a meaning shift,
e.g., predati (to hand in/out) vs. predati se (to
surrender). With subject subcategorization, re-
flexive and irreflexive readings will have differ-
ent tensor representations; e.g., ?student, Subj tr,
zadac?a? (?student, Subj tr, homework?) vs. ?trupe,
Subj intr, napadac?? (?troops, Subj intr, invadors?).
Finally, similar to Pado? and Utt (2012), we use
Verb as an underspecified link between subjects
and objects linked by non-auxiliary verbs.
For lexicalized links, we use two more extraction
patterns for prepositions and verbs. Prepositions
are directly lexicalized as links; e.g., ?mjesto, na,
sunce? (?place, on, sun?). The same holds for non-
auxiliary verbs linking subjects to objects; e.g.,
?drz?ava, kupiti, kolic?ina? (?state, buy, amount?).
Tuple extraction and scoring. The overall qual-
ity of the DM.HR depends on the accuracy of ex-
tracted tuples, which is affected by all preprocess-
ing steps. We computed the performance of tu-
ple extraction by evaluating a sample of tuples
extracted from a parsed version of SETIMES.HR
against the tuples extracted from the SETIMES.HR
gold annotations (we use the same sample as for
tagging and parsing performance evaluation). Ta-
ble 2 shows Precision, Recall, and F1 score. Over-
all, we achieve the best performance on the Atr
links, followed by Pred links. The performance is
generally higher on unlexicalized links than on lex-
icalized links (note that performance on unlexical-
786
Link Word LMI Link Word LMI
Atv moc?i 225107 Adv moguc?e 9669
Atv z?eljeti 22049 Atv namjeravati 9095
Obj stan 19997 Obj karta 8936
po cijena 18534 prije godina 8584
Pred kada 14408 Adv nedavno 7842
Obj dionica 13720 Atv odluc?iti 7578
Atv morati 12097 Adv godina 7496
Obj ulaznica 11126 Obj zemljis?te 7180
Table 3: Top 16 LMI-scored tuples for the verb
kupiti (to buy)
ized Verb links is identical to overall performance
on lexicalized verb links). The overall F1 score of
tuple extraction is 74.6%.
Following DM and DM.DE, we score each
extracted tuple using Local Mutual Information
(LMI) (Evert, 2005):
LMI(i, j, k) = f(i, j, k) log P (i, j, k)P (i)P (j)P (k)
For a tuple (w1, l, w2), LMI scores the association
strength between word w1 and word w2 via link l
by comparing their joint distribution against the dis-
tribution under the independence assumption, mul-
tiplied with the observed frequency f(w1, l, w2) to
discount infrequent tuples. The probabilities are
computed from tuple counts as maximum likeli-
hood estimates. We exclude from the tensor all
tuples with a negative LMI score. Finally, we sym-
metrize the tensor by introducing inverse links.
Model statistics. The resulting DM.HR tensor
consists of 2.3M lemmas, 121M links and 165K
link types (including inverse links). On average,
each lemma has 53 links. This makes DM.HR
more sparse than English DM (796 link types), but
less sparse than German DM (220K link types; 22
links per lemma). Table 3 shows an example of
the extracted tuples for the verb kupiti (to buy).
DM.HR tensor is freely available for download.4
5 Evaluating DM.HR
Task. We present a pilot evaluation DM.HR on a
standard task from distributional semantics, namely
synonym choice. In contrast to tasks like predict-
ing word similarity We use the dataset created by
Karan et al (2012), with more than 11,000 syn-
onym choice questions. Each question consists of
one target word (nouns, verbs, and adjectives) with
4http://takelab.fer.hr/dmhr
Accuracy (%) Coverage (%)
Model N A V N A V
DM.HR 70.0 66.3 63.2 99.9 99.1 100
BOW-LSA 67.2 68.9 61.0 100 100 100
BOW baseline 59.9 65.7 55.9 99.9 99.7 100
Table 4: Results on synonym choice task
four synonym candidates (one is correct). The ques-
tions were extracted automatically from a machine-
readable dictionary of Croatian. An example item
is tez?ak (farmer): poljoprivrednik (farmer), um-
jetnost (art), radijacija (radiation), bod (point).
We sampled from the dataset questions for nouns,
verbs, and adjectives, with 1000 questions each.5
Additionally, we manually corrected some errors
in the dataset, introduced by the automatic extrac-
tion procedure. To make predictions, we compute
pairwise cosine similarities of the target word vec-
tors with the four candidates and predict the can-
didate(s) with maximal similarity (note that there
may be ties).
Evaluation. Our evaluation follows the scheme
developed by Mohammad et al (2007), who define
accuracy as the average number of correct predic-
tions per covered question. Each correct prediction
with a single most similar candidate receives a full
credit (A), while ties for maximal similarity are
discounted (B: two-way tie, C: three-way tie, D:
four-way tie): A+ 12B+ 13C+ 14D. We consider aquestion item to be covered if the target and at least
one answer word are modeled. In our experiments,
ties occur when vector similarities are zero for all
word pairs (due to vector sparsity). Note that a
random baseline would perform at 0.25 accuracy.
As baseline to compare against the DM.HR, we
build a standard bag-of-word model from the same
corpus. It uses a ?5-word within-sentence con-
text window, and the 10,000 most frequent context
words (nouns, adjectives, and verbs) as dimensions.
We also compare against BOW-LSA, a state-of-
the-art synonym detection model from Karan et
al. (2012), which uses 500 latent dimensions and
paragraphs as contexts. We determine the signifi-
cance of differences between the models by com-
puting 95% confidence intervals with bootstrap re-
sampling (Efron and Tibshirani, 1993).
Results. Table 4 shows the results for the three
considered models on nouns (N), adjectives (A),
5Available at: http://takelab.fer.hr/crosyn
787
and verbs (V). The performance of BOW-LSA
differs slightly from that reported by Karan et al
(2012), because we evaluate on a sample of their
dataset. DM.HR outperforms the baseline BOW
model for nouns and verbs (differences are sig-
nificant at p < 0.05). Moreover, on these cate-
gories DM.HR performs slightly better than BOW-
LSA, but the differences are not statistically sig-
nificant. Conversely, on adjectives BOW-LSA per-
forms slightly better than DM.HR, but the differ-
ence is again not statistically significant. All mod-
els achieve comparable and almost perfect cov-
erage on this dataset (BOW-LSA achieves com-
plete coverage because of the way how the original
dataset was filtered).
Overall, the biggest improvement over the base-
line is achieved for nouns. Nouns occur as heads
and dependents of many link types (unlexicalized
and lexicalized), and are thus well represented in
the semantic space. On the other hand, adjectives
seem to be less well modeled. Although the major-
ity of adjectives occur as heads or dependents of
the Atr relation, for which extraction accuracy is
the highest (cf. Table 2), it is likely that a single link
type is not sufficient. As noted by a reviewer, more
insight could perhaps be gained by comparing the
predictions of BOW-LSA and DM.HR models. The
generally low performance on verbs suggests that
their semantic is not fully covered in word- and
syntax-based spaces.
6 Conclusion
We have described the construction of DM.HR, a
syntax-based distributional memory for Croatian
built from a dependency-parsed web corpus. To the
best of our knowledge, DM.HR is the first freely
available distributional memory for a Slavic lan-
guage. We have conducted a preliminary evalua-
tion of DM.HR on a synonym choice task, where
DM.HR outperformed the bag-of-word model and
performed comparable to an LSA model.
This work provides a starting point for a sys-
tematic study of dependency-based distributional
semantics for Croatian and similar languages. Our
first priority will be to analyze how corpus prepro-
cessing and the choice of link types relates to model
performance on different semantic tasks. Better
modeling of adjectives and verbs is also an impor-
tant topic for future research.
Acknowledgments
The first author was supported by the Croatian
Science Foundation (project 02.03/162: ?Deriva-
tional Semantic Models for Information Retrieval?).
We thank the reviewers for their constructive com-
ments. Special thanks to Hiko Schamoni, Tae-Gil
Noh, and Mladen Karan for their assistance.
References
Z?eljko Agic? and Danijela Merkler. 2013. Three syn-
tactic formalisms for data-driven dependency pars-
ing of Croatian. Proceedings of TSD 2013, Lecture
Notes in Artificial Intelligence.
Z?eljko Agic?, Marko Tadic?, and Zdravko Dovedan.
2008. Improving part-of-speech tagging accuracy
for Croatian by morphological analysis. Informat-
ica, 32(4):445?451.
Z?eljko Agic?, Marko Tadic?, and Zdravko Dovedan.
2009. Evaluating full lemmatization of Croatian
texts. In Recent Advances in Intelligent Information
Systems, pages 175?184. EXIT Warsaw.
Z?eljko Agic?. 2012. K-best spanning tree dependency
parsing with verb valency lexicon reranking. In Pro-
ceedings of COLING 2012: Posters, pages 1?12,
Bombay, India.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguistics,
36(4):673?721.
Bartosz Broda and Maciej Piasecki. 2008. Superma-
trix: a general tool for lexical semantic knowledge
acquisition. In Speech and Language Technology,
volume 11, pages 239?254. Polish Phonetics Asso-
cation.
Bartosz Broda, Magdalena Derwojedowa, Maciej Pi-
asecki, and Stanis?aw Szpakowicz. 2008. Corpus-
based semantic relatedness for the construction of
Polish WordNet. In Proceedings of LREC, Mar-
rakech, Morocco.
Bradley Efron and Robert J. Tibshirani. 1993. An
Introduction to the Bootstrap. Chapman and Hall,
New York.
Tomaz? Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic resources for Central and Eastern European
languages. Language Resources and Evaluation,
46(1):131?142.
Katrin Erk, Sebastian Pado?, and Ulrike Pado?. 2010.
A Flexible, Corpus-driven Model of Regular and In-
verse Selectional Preferences. Computational Lin-
guistics, 36(4):723?763.
788
Stefan Evert. 2005. The statistics of word cooccur-
rences. Ph.D. thesis, PhD Dissertation, Stuttgart
University.
Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz.
2007. HunPos: An open source trigram tagger. In
Proceedings of ACL 2007, pages 209?212, Prague,
Czech Republic.
Zelig S. Harris. 1954. Distributional structure. Word,
10(23):146?162.
Anton Karl Ingason, Sigru?n Helgado?ttir, Hrafn Lofts-
son, and Eir??kur Ro?gnvaldsson. 2008. A mixed
method lemmatization algorithm using a hierarchy
of linguistic identities (HOLI). In Proceedings of
GoTAL, pages 205?216.
Vedrana Jankovic?, Jan S?najder, and Bojana Dalbelo
Bas?ic?. 2011. Random indexing distributional se-
mantic models for Croatian language. In Proceed-
ings of Text, Speech and Dialogue, pages 411?418,
Plzen?, Czech Republic.
Mladen Karan, Jan S?najder, and Bojana Dalbelo Bas?ic?.
2012. Distributional semantics approach to detect-
ing synonyms in Croatian language. In Proceedings
of the Language Technologies Conference, Informa-
tion Society, Ljubljana, Slovenia.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures
on Human Language Technologies. Morgan & Clay-
pool.
Nikola Ljubes?ic? and Tomaz? Erjavec. 2011. hrWaC
and slWac: Compiling web corpora for Croatian and
Slovene. In Proceedings of Text, Speech and Dia-
logue, pages 395?402, Plzen?, Czech Republic.
Nikola Ljubes?ic?, Damir Boras, Nikola Bakaric?, and Jas-
mina Njavro. 2008. Comparing measures of seman-
tic similarity. In Proceedings of the ITI 2008 30th
International Conference of Information Technology
Interfaces, Cavtat, Croatia.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a
two-stage discriminative parser. In Proceedings of
CoNLL-X, pages 216?220, New York, NY.
Olga Mitrofanova, Anton Mukhin, Polina Panicheva,
and Vyacheslav Savitsky. 2007. Automatic word
clustering in Russian texts. In Proceedings of Text,
Speech and Dialogue, pages 85?91, Plzen?, Czech
Republic.
Saif Mohammad, Iryna Gurevych, Graeme Hirst, and
Torsten Zesch. 2007. Cross-lingual distributional
profiles of concepts for measuring semantic distance.
In Proceedings of EMNLP/CoNLL, pages 571?580,
Prague, Czech Republic.
Preslav Nakov. 2001a. Latent semantic analysis
for Bulgarian literature. In Proceedings of Spring
Conference of Bulgarian Mathematicians Union,
Borovets, Bulgaria.
Preslav Nakov. 2001b. Latent semantic analysis for
Russian literature investigation. In Proceedings of
the 120 years Bulgarian Naval Academy Confer-
ence.
Sebastian Pado? and Jason Utt. 2012. A distributional
memory for German. In Proceedings of the KON-
VENS 2012 workshop on lexical-semantic resources
and applications, pages 462?470, Vienna, Austria.
Maciej Piasecki. 2009. Automated extraction of lexi-
cal meanings from corpus: A case study of potential-
ities and limitations. In Representing Semantics in
Digital Lexicography. Innovative Solutions for Lexi-
cal Entry Content in Slavic Lexicography, pages 32?
43. Institute of Slavic Studies, Polish Academy of
Sciences.
Pavel Smrz? and Pavel Rychly?. 2001. Finding semanti-
cally related words in large corpora. In Text, Speech
and Dialogue, pages 108?115. Springer.
Marko Tadic?. 2005. The Croatian Lemmatization
Server. Southern Journal of Linguistics, 29(1):206?
217.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
789
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 33?38,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 8: Multi-Way Classification
of Semantic Relations Between Pairs of Nominals
Iris Hendrickx
?
, Su Nam Kim
?
, Zornitsa Kozareva
?
, Preslav Nakov
?
,
Diarmuid
?
O S
?
eaghdha
?
, Sebastian Pad
?
o
?
, Marco Pennacchiotti
??
,
Lorenza Romano
??
, Stan Szpakowicz
??
Abstract
SemEval-2 Task 8 focuses on Multi-way
classification of semantic relations between
pairs of nominals. The task was designed
to compare different approaches to seman-
tic relation classification and to provide a
standard testbed for future research. This
paper defines the task, describes the train-
ing and test data and the process of their
creation, lists the participating systems (10
teams, 28 runs), and discusses their results.
1 Introduction
SemEval-2010 Task 8 focused on semantic rela-
tions between pairs of nominals. For example, tea
and ginseng are in an ENTITY-ORIGIN relation in
?The cup contained tea from dried ginseng.?. The
automatic recognition of semantic relations has
many applications, such as information extraction,
document summarization, machine translation, or
construction of thesauri and semantic networks.
It can also facilitate auxiliary tasks such as word
sense disambiguation, language modeling, para-
phrasing, and recognizing textual entailment.
Our goal was to create a testbed for automatic
classification of semantic relations. In developing
the task we met several challenges: selecting a
suitable set of relations, specifying the annotation
procedure, and deciding on the details of the task
itself. They are discussed briefly in Section 2; see
also Hendrickx et al (2009), which includes a sur-
vey of related work. The direct predecessor of Task
8 was Classification of semantic relations between
nominals, Task 4 at SemEval-1 (Girju et al, 2009),
?
University of Lisbon, iris@clul.ul.pt
?
University of Melbourne, snkim@csse.unimelb.edu.au
?
Information Sciences Institute/University of Southern
California, kozareva@isi.edu
?
National University of Singapore, nakov@comp.nus.edu.sg
?
University of Cambridge, do242@cl.cam.ac.uk
?
University of Stuttgart, pado@ims.uni-stuttgart.de
??
Yahoo! Inc., pennacc@yahoo-inc.com
??
Fondazione Bruno Kessler, romano@fbk.eu
??
University of Ottawa and Polish Academy of Sciences,
szpak@site.uottawa.ca
which had a separate binary-labeled dataset for
each of seven relations. We have defined SemEval-
2010 Task 8 as a multi-way classification task in
which the label for each example must be chosen
from the complete set of ten relations and the map-
ping from nouns to argument slots is not provided
in advance. We also provide more data: 10,717 an-
notated examples, compared to 1,529 in SemEval-1
Task 4.
2 Dataset Creation
2.1 The Inventory of Semantic Relations
We first decided on an inventory of semantic rela-
tions. Ideally, it should be exhaustive (enable the
description of relations between any pair of nomi-
nals) and mutually exclusive (each pair of nominals
in context should map onto only one relation). The
literature, however, suggests that no relation inven-
tory satisfies both needs, and, in practice, some
trade-off between them must be accepted.
As a pragmatic compromise, we selected nine
relations with coverage sufficiently broad to be of
general and practical interest. We aimed at avoid-
ing semantic overlap as much as possible. We
included, however, two groups of strongly related
relations (ENTITY-ORIGIN / ENTITY-DESTINA-
TION and CONTENT-CONTAINER / COMPONENT-
WHOLE / MEMBER-COLLECTION) to assess mod-
els? ability to make such fine-grained distinctions.
Our inventory is given below. The first four were
also used in SemEval-1 Task 4, but the annotation
guidelines have been revised, and thus no complete
continuity should be assumed.
Cause-Effect (CE). An event or object leads to an
effect. Example: those cancers were caused
by radiation exposures
Instrument-Agency (IA). An agent uses an in-
strument. Example: phone operator
Product-Producer (PP). A producer causes a
product to exist. Example: a factory manu-
factures suits
33
Content-Container (CC). An object is physically
stored in a delineated area of space. Example:
a bottle full of honey was weighed
Entity-Origin (EO). An entity is coming or is de-
rived from an origin (e.g., position or mate-
rial). Example: letters from foreign countries
Entity-Destination (ED). An entity is moving to-
wards a destination. Example: the boy went
to bed
Component-Whole (CW). An object is a com-
ponent of a larger whole. Example: my
apartment has a large kitchen
Member-Collection (MC). A member forms a
nonfunctional part of a collection. Example:
there are many trees in the forest
Message-Topic (MT). A message, written or spo-
ken, is about a topic. Example: the lecture
was about semantics
2.2 Annotation Guidelines
We defined a set of general annotation guidelines
as well as detailed guidelines for each semantic
relation. Here, we describe the general guidelines,
which delineate the scope of the data to be col-
lected and state general principles relevant to the
annotation of all relations.
1
Our objective is to annotate instances of seman-
tic relations which are true in the sense of hold-
ing in the most plausible truth-conditional inter-
pretation of the sentence. This is in the tradition
of the Textual Entailment or Information Valida-
tion paradigm (Dagan et al, 2009), and in con-
trast to ?aboutness? annotation such as semantic
roles (Carreras and M`arquez, 2004) or the BioNLP
2009 task (Kim et al, 2009) where negated rela-
tions are also labelled as positive. Similarly, we
exclude instances of semantic relations which hold
only in speculative or counterfactural scenarios. In
practice, this means disallowing annotations within
the scope of modals or negations, e.g., ?Smoking
may/may not have caused cancer in this case.?
We accept as relation arguments only noun
phrases with common-noun heads. This distin-
guishes our task from much work in Information
Extraction, which tends to focus on specific classes
of named entities and on considerably more fine-
grained relations than we do. Named entities are a
specific category of nominal expressions best dealt
1
The full task guidelines are available at http://docs.
google.com/View?id=dfhkmm46_0f63mfvf7
with using techniques which do not apply to com-
mon nouns. We only mark up the semantic heads of
nominals, which usually span a single word, except
for lexicalized terms such as science fiction.
We also impose a syntactic locality requirement
on example candidates, thus excluding instances
where the relation arguments occur in separate sen-
tential clauses. Permissible syntactic patterns in-
clude simple and relative clauses, compounds, and
pre- and post-nominal modification. In addition,
we did not annotate examples whose interpretation
relied on discourse knowledge, which led to the
exclusion of pronouns as arguments. Please see
the guidelines for details on other issues, includ-
ing noun compounds, aspectual phenomena and
temporal relations.
2.3 The Annotation Process
The annotation took place in three rounds. First,
we manually collected around 1,200 sentences for
each relation through pattern-based Web search. In
order to ensure a wide variety of example sentences,
we used a substantial number of patterns for each
relation, typically between one hundred and several
hundred. Importantly, in the first round, the relation
itself was not annotated: the goal was merely to
collect positive and near-miss candidate instances.
A rough aim was to have 90% of candidates which
instantiate the target relation (?positive instances?).
In the second round, the collected candidates for
each relation went to two independent annotators
for labeling. Since we have a multi-way classifi-
cation task, the annotators used the full inventory
of nine relations plus OTHER. The annotation was
made easier by the fact that the cases of overlap
were largely systematic, arising from general phe-
nomena like metaphorical use and situations where
more than one relation holds. For example, there is
a systematic potential overlap between CONTENT-
CONTAINER and ENTITY-DESTINATION depend-
ing on whether the situation described in the sen-
tence is static or dynamic, e.g., ?When I came,
the <e1>apples</e1> were already put in the
<e2>basket</e2>.? is CC(e1, e2), while ?Then,
the <e1>apples</e1> were quickly put in the
<e2>basket</e2>.? is ED(e1, e2).
In the third round, the remaining disagreements
were resolved, and, if no consensus could be
achieved, the examples were removed. Finally, we
merged all nine datasets to create a set of 10,717
instances. We released 8,000 for training and kept
34
the rest for testing.
2
Table 1 shows some statistics about the dataset.
The first column (Freq) shows the absolute and rel-
ative frequencies of each relation. The second col-
umn (Pos) shows that the average share of positive
instances was closer to 75% than to 90%, indicating
that the patterns catch a substantial amount of ?near-
miss? cases. However, this effect varies a lot across
relations, causing the non-uniform relation distribu-
tion in the dataset (first column).
3
After the second
round, we also computed inter-annotator agreement
(third column, IAA). Inter-annotator agreement
was computed on the sentence level, as the per-
centage of sentences for which the two annotations
were identical. That is, these figures can be inter-
preted as exact-match accuracies. We do not report
Kappa, since chance agreement on preselected can-
didates is difficult to estimate.
4
IAA is between
60% and 95%, again with large relation-dependent
variation. Some of the relations were particularly
easy to annotate, notably CONTENT-CONTAINER,
which can be resolved through relatively clear cri-
teria, despite the systematic ambiguity mentioned
above. ENTITY-ORIGIN was the hardest relation to
annotate. We encountered ontological difficulties
in defining both Entity (e.g., in contrast to Effect)
and Origin (as opposed to Cause). Our numbers
are on average around 10% higher than those re-
ported by Girju et al (2009). This may be a side
effect of our data collection method. To gather
1,200 examples in realistic time, we had to seek
productive search query patterns, which invited
certain homogeneity. For example, many queries
for CONTENT-CONTAINER centered on ?usual sus-
pect? such as box or suitcase. Many instances of
MEMBER-COLLECTION were collected on the ba-
sis of from available lists of collective names.
3 The Task
The participating systems had to solve the follow-
ing task: given a sentence and two tagged nominals,
predict the relation between those nominals and the
direction of the relation.
We released a detailed scorer which outputs (1) a
confusion matrix, (2) accuracy and coverage, (3)
2
This set includes 891 examples from SemEval-1 Task 4.
We re-annotated them and assigned them as the last examples
of our training dataset to ensure that the test set was unseen.
3
To what extent our candidate selection produces a biased
sample is a question that we cannot address within this paper.
4
We do not report Pos or IAA for OTHER, since OTHER is
a pseudo-relation that was not annotated in its own right. The
numbers would therefore not be comparable to other relations.
Relation Freq Pos IAA
Cause-Effect 1331 (12.4%) 91.2% 79.0%
Component-Whole 1253 (11.7%) 84.3% 70.0%
Entity-Destination 1137 (10.6%) 80.1% 75.2%
Entity-Origin 974 (9.1%) 69.2% 58.2%
Product-Producer 948 (8.8%) 66.3% 84.8%
Member-Collection 923 (8.6%) 74.7% 68.2%
Message-Topic 895 (8.4%) 74.4% 72.4%
Content-Container 732 (6.8%) 59.3% 95.8%
Instrument-Agency 660 (6.2%) 60.8% 65.0%
Other 1864 (17.4%) N/A
4
N/A
4
Total 10717 (100%)
Table 1: Annotation Statistics. Freq: Absolute and
relative frequency in the dataset; Pos: percentage
of ?positive? relation instances in the candidate set;
IAA: inter-annotator agreement
precision (P), recall (R), and F
1
-Score for each
relation, (4) micro-averaged P, R, F
1
, (5) macro-
averaged P, R, F
1
. For (4) and (5), the calculations
ignored the OTHER relation. Our official scoring
metric is macro-averaged F
1
-Score for (9+1)-way
classification, taking directionality into account.
The teams were asked to submit test data pre-
dictions for varying fractions of the training data.
Specifically, we requested results for the first 1000,
2000, 4000, and 8000 training instances, called
TD1 through TD4. TD4 was the full training set.
4 Participants and Results
Table 2 lists the participants and provides a rough
overview of the system features. Table 3 shows the
results. Unless noted otherwise, all quoted numbers
are F
1
-Scores.
Overall Ranking and Training Data. We rank
the teams by the performance of their best system
on TD4, since a per-system ranking would favor
teams with many submitted runs. UTD submit-
ted the best system, with a performance of over
82%, more than 4% better than the second-best
system. FBK IRST places second, with 77.62%,
a tiny margin ahead of ISI (77.57%). Notably, the
ISI system outperforms the FBK IRST system for
TD1 to TD3, where it was second-best. The accu-
racy numbers for TD4 (Acc TD4) lead to the same
overall ranking: micro- versus macro-averaging
does not appear to make much difference either.
A random baseline gives an uninteresting score of
6%. Our competitive baseline system is a simple
Naive Bayes classifier which relies on words in the
sentential context only; two systems scored below
this baseline.
35
System Institution Team Description Res. Class.
Baseline Task organizers local context of 2 words only BN
ECNU-SR-1 East China Normal
University
Man Lan, Yuan
Chen, Zhimin
Zhou, Yu Xu
stem, POS, syntactic patterns S SVM
(multi)
ECNU-SR-2,3 features like ECNU-SR-1, dif-
ferent prob. thresholds
SVM
(binary)
ECNU-SR-4 stem, POS, syntactic patterns,
hyponymy and meronymy rela-
tions
WN,
S
SVM
(multi)
ECNU-SR-5,6 features like ECNU-SR-4, dif-
ferent prob. thresholds
SVM
(binary)
ECNU-SR-7 majority vote of ECNU-1,2,4,5
FBK IRST-6C32 Fondazione Bruno
Kessler
Claudio Giu-
liano, Kateryna
Tymoshenko
3-word window context features
(word form, part of speech, or-
thography) + Cyc; parameter
estimation by optimization on
training set
Cyc SVM
FBK IRST-12C32 FBK IRST-6C32 + distance fea-
tures
FBK IRST-12VBC32 FBK IRST-12C32 + verbs
FBK IRST-6CA,
-12CA, -12VBCA
features as above, parameter es-
timation by cross-validation
FBK NK-RES1 Fondazione Bruno
Kessler
Matteo Negri,
Milen Kouylekov
collocations, glosses, semantic
relations of nominals + context
features
WN BN
FBK NK-RES 2,3,4 like FBK NK-RES1 with differ-
ent context windows and collo-
cation cutoffs
ISI Information Sci-
ences Institute,
University of
Southern Califor-
nia
Stephen Tratz features from different re-
sources, a noun compound
relation system, and various
feature related to capitalization,
affixes, closed-class words
WN,
RT, G
ME
ISTI-1,2 Istituto di sci-
enca e tecnologie
dell?informazione
?A. Faedo?
Andrea Esuli,
Diego Marcheg-
giani, Fabrizio
Sebastiani
Boosting-based classification.
Runs differ in their initializa-
tion.
WN 2S
JU Jadavpur Univer-
sity
Santanu Pal, Partha
Pakray, Dipankar
Das, Sivaji Bandy-
opadhyay
Verbs, nouns, and prepositions;
seed lists for semantic relations;
parse features and NEs
WN,
S
CRF
SEKA Hungarian
Academy of
Sciences
Eszter Simon, An-
dras Kornai
Levin and Roget classes, n-
grams; other grammatical and
formal features
RT,
LC
ME
TUD-base Technische Univer-
sit?at Darmstadt
Gy?orgy Szarvas,
Iryna Gurevych
word, POS n-grams, depen-
dency path, distance
S ME
TUD-wp TUD-base + ESA semantic re-
latedness scores
+WP
TUD-comb TUD-base + own semantic relat-
edness scores
+WP,WN
TUD-comb-threshold TUD-comb with higher thresh-
old for OTHER
UNITN University of
Trento
Fabio Celli punctuation, context words,
prepositional patterns, estima-
tion of semantic relation
? DR
UTD University of Texas
at Dallas
Bryan Rink, Sanda
Harabagiu
context wods, hypernyms, POS,
dependencies, distance, seman-
tic roles, Levin classes, para-
phrases
WN,
S, G,
PB/NB,
LC
SVM,
2S
Table 2: Participants of SemEval-2010 Task 8. Res: Resources used (WN: WordNet data; WP:
Wikipedia data; S: syntax; LC: Levin classes; G: Google n-grams, RT: Roget?s Thesaurus, PB/NB:
PropBank/NomBank). Class: Classification style (ME: Maximum Entropy; BN: Bayes Net; DR: Decision
Rules/Trees; CRF: Conditional Random Fields; 2S: two-step classification)
36
System TD1 TD2 TD3 TD4 Acc TD4 Rank Best Cat Worst Cat-9
Baseline 33.04 42.41 50.89 57.52 50.0 - MC (75.1) IA (28.0)
ECNU-SR-1 52.13 56.58 58.16 60.08 57.1
4
CE (79.7) IA (32.2)
ECNU-SR-2 46.24 47.99 69.83 72.59 67.1 CE (84.4) IA (52.2)
ECNU-SR-3 39.89 42.29 65.47 68.50 62.0 CE (83.4) IA (46.5)
ECNU-SR-4 67.95 70.58 72.99 74.82 70.5 CE (84.6) IA (61.4)
ECNU-SR-5 49.32 50.70 72.63 75.43 70.2 CE (85.1) IA (60.7)
ECNU-SR-6 42.88 45.54 68.87 72.19 65.8 CE (85.2) IA (56.7)
ECNU-SR-7 58.67 58.87 72.79 75.21 70.2 CE (86.1) IA (61.8)
FBK IRST-6C32 60.19 67.31 71.78 76.81 72.4
2
ED (82.6) IA (69.4)
FBK IRST-12C32 60.66 67.91 72.04 76.91 72.4 MC (84.2) IA (68.8)
FBK IRST-12VBC32 62.64 69.86 73.19 77.11 72.3 ED (85.9) PP (68.1)
FBK IRST-6CA 60.58 67.14 71.63 76.28 71.4 CE (82.3) IA (67.7)
FBK IRST-12CA 61.33 67.80 71.65 76.39 71.4 ED (81.8) IA (67.5)
FBK IRST-12VBCA 63.61 70.20 73.40 77.62 72.8 ED (86.5) IA (67.3)
FBK NK-RES1 55.71
?
64.06
?
67.80
?
68.02 62.1
7
ED (77.6) IA (52.9)
FBK NK-RES2 54.27
?
63.68
?
67.08
?
67.48 61.4 ED (77.4) PP (55.2)
FBK NK-RES3 54.25
?
62.73
?
66.11
?
66.90 60.5 MC (76.7) IA (56.3)
FBK NK-RES4 44.11
?
58.85
?
63.06
?
65.84 59.4 MC (76.1) IA/PP (58.0)
ISI 66.68 71.01 75.51 77.57 72.7 3 CE (87.6) IA (61.5)
ISTI-1 50.49
?
55.80
?
61.14
?
68.42 63.2
6
ED (80.7) PP (53.8)
ISTI-2 50.69
?
54.29
?
59.77
?
66.65 61.5 ED (80.2) IA (48.9)
JU 41.62
?
44.98
?
47.81
?
52.16 50.2 9 CE (75.6) IA (27.8)
SEKA 51.81 56.34 61.10 66.33 61.9 8 CE (84.0) PP (43.7)
TUD-base 50.81 54.61 56.98 60.50 56.1
5
CE (80.7) IA (31.1)
TUD-wp 55.34 60.90 63.78 68.00 63.5 ED (82.9) IA (44.1)
TUD-comb 57.84 62.52 66.41 68.88 64.6 CE (83.8) IA (46.8)
TUD-comb-? 58.35 62.45 66.86 69.23 65.4 CE (83.4) IA (46.9)
UNITN 16.57
?
18.56
?
22.45
?
26.67 27.4 10 ED (46.4) PP (0)
UTD 73.08 77.02 79.93 82.19 77.9 1 CE (89.6) IA (68.5)
Table 3: F
1
-Score of all submitted systems on the test dataset as a function of training data: TD1=1000,
TD2=2000, TD3=4000, TD4=8000 training examples. Official results are calculated on TD4. The results
marked with
?
were submitted after the deadline. The best-performing run for each participant is italicized.
As for the amount of training data, we see a sub-
stantial improvement for all systems between TD1
and TD4, with diminishing returns for the transi-
tion between TD3 and TD4 for many, but not all,
systems. Overall, the differences between systems
are smaller for TD4 than they are for TD1. The
spread between the top three systems is around 10%
at TD1, but below 5% at TD4. Still, there are clear
differences in the influence of training data size
even among systems with the same overall archi-
tecture. Notably, ECNU-SR-4 is the second-best
system at TD1 (67.95%), but gains only 7% from
the eightfold increase of the size of the training data.
At the same time, ECNU-SR-3 improves from less
than 40% to almost 69%. The difference between
the systems is that ECNU-SR-4 uses a multi-way
classifier including the class OTHER, while ECNU-
SR-3 uses binary classifiers and assigns OTHER
if no other relation was assigned with p>0.5. It
appears that these probability estimates for classes
are only reliable enough for TD3 and TD4.
The Influence of System Architecture. Almost
all systems used either MaxEnt or SVM classifiers,
with no clear advantage for either. Similarly, two
systems, UTD and ISTI (rank 1 and 6) split the task
into two classification steps (relation and direction),
but the 2nd- and 3rd-ranked systems do not. The
use of a sequence model such as a CRF did not
show a benefit either.
The systems use a variety of resources. Gener-
ally, richer feature sets lead to better performance
(although the differences are often small ? compare
the different FBK IRST systems). This improve-
ment can be explained by the need for semantic
generalization from training to test data. This need
can be addressed using WordNet (contrast ECNU-1
to -3 with ECNU-4 to -6), the Google n-gram col-
lection (see ISI and UTD), or a ?deep? semantic
resource (FBK IRST uses Cyc). Yet, most of these
resources are also included in the less successful
systems, so beneficial integration of knowledge
sources into semantic relation classification seems
to be difficult.
System Combination. The differences between
the systems suggest that it might be possible to
achieve improvements by building an ensemble
37
system. When we combine the top three systems
(UTD, FBK IRST-12VBCA, and ISI) by predict-
ing their majority vote, or OTHER if there was none,
we obtain a small improvement over the UTD sys-
tem with an F
1
-Score of 82.79%. A combination of
the top five systems using the same method shows
a worse performance, however (80.42%). This sug-
gests that the best system outperforms the rest by
a margin that cannot be compensated with system
combination, at least not with a crude majority vote.
We see a similar pattern among the ECNU systems,
where the ECNU-SR-7 combination system is out-
performed by ECNU-SR-5, presumably since it
incorporates the inferior ECNU-SR-1 system.
Relation-specific Analysis. We also analyze the
performance on individual relations, especially the
extremes. There are very stable patterns across all
systems. The best relation (presumably the eas-
iest to classify) is CE, far ahead of ED and MC.
Notably, the performance for the best relation is
75% or above for almost all systems, with compar-
atively small differences between the systems. The
hardest relation is generally IA, followed by PP.
5
Here, the spread among the systems is much larger:
the highest-ranking systems outperform others on
the difficult relations. Recall was the main prob-
lem for both IA and PP: many examples of these
two relations are misclassified, most frequently as
OTHER. Even at TD4, these datasets seem to be
less homogeneous than the others. Intriguingly, PP
shows a very high inter-annotator agreement (Ta-
ble 1). Its difficulty may therefore be due not to
questionable annotation, but to genuine variability,
or at least the selection of difficult patterns by the
dataset creator. Conversely, MC, among the easiest
relations to model, shows only a modest IAA.
Difficult Instances. There were 152 examples
that are classified incorrectly by all systems. We
analyze them, looking for sources of errors. In ad-
dition to a handful of annotation errors and some
borderline cases, they are made up of instances
which illustrate the limits of current shallow mod-
eling approaches in that they require more lexical
knowledge and complex reasoning. A case in point:
The bottle carrier converts your <e1>bottle</e1>
into a <e2>canteen</e2>. This instance of
OTHER is misclassified either as CC (due to the
5
The relation OTHER, which we ignore in the overall F
1
-
score, does even worse, often below 40%. This is to be ex-
pected, since the OTHER examples in our datasets are near
misses for other relations, thus making a very incoherent class.
nominals) or as ED (because of the preposition
into). Another example: [...] <e1>Rudders</e1>
are used by <e2>towboats</e2> and other ves-
sels that require a high degree of manoeuvrability.
This is an instance of CW misclassified as IA, prob-
ably on account of the verb use which is a frequent
indicator of an agentive relation.
5 Discussion and Conclusion
There is little doubt that 19-way classification is a
non-trivial challenge. It is even harder when the
domain is lexical semantics, with its idiosyncrasies,
and when the classes are not necessarily disjoint,
despite our best intentions. It speaks to the success
of the exercise that the participating systems? per-
formance was generally high, well over an order
of magnitude above random guessing. This may
be due to the impressive array of tools and lexical-
semantic resources deployed by the participants.
Section 4 suggests a few ways of interpreting
and analyzing the results. Long-term lessons will
undoubtedly emerge from the workshop discussion.
One optimistic-pessimistic conclusion concerns the
size of the training data. The notable gain TD3?
TD4 suggests that even more data would be helpful,
but that is so much easier said than done: it took
the organizers well in excess of 1000 person-hours
to pin down the problem, hone the guidelines and
relation definitions, construct sufficient amounts of
trustworthy training data, and run the task.
References
X. Carreras and L. M`arquez. 2004. Introduction to
the CoNLL-2004 shared task: Semantic role label-
ing. In Proc. CoNLL-04, Boston, MA.
I. Dagan, B. Dolan, B. Magnini, and D. Roth. 2009.
Recognizing textual entailment: Rational, evalua-
tion and approaches. Natural Language Engineer-
ing, 15(4):i?xvii.
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-
ney, and D. Yuret. 2009. Classification of semantic
relations between nominals. Language Resources
and Evaluation, 43(2):105?121.
I. Hendrickx, S. Kim, Z. Kozareva, P. Nakov, D.
?
O
S?eaghdha, S. Pad?o, M. Pennacchiotti, L. Romano,
and S. Szpakowicz. 2009. SemEval-2010 Task
8: Multi-way classification of semantic relations be-
tween pairs of nominals. In Proc. NAACL Workshop
on Semantic Evaluations, Boulder, CO.
J. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.
2009. Overview of BioNLP?09 shared task on event
extraction. In Proc. BioNLP-09, Boulder, CO.
38
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 151?160,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Regular polysemy: A distributional model
Gemma Boleda
Dept. of Linguistics
University of Texas at Austin
gemma.boleda@upf.edu
Sebastian Pado?
ICL
University of Heidelberg
pado@cl.uni-heidelberg.de
Jason Utt
IMS
University of Stuttgart
uttjn@ims.uni-stuttgart.de
Abstract
Many types of polysemy are not word specific,
but are instances of general sense alternations
such as ANIMAL-FOOD. Despite their perva-
siveness, regular alternations have been mostly
ignored in empirical computational semantics.
This paper presents (a) a general framework
which grounds sense alternations in corpus
data, generalizes them above individual words,
and allows the prediction of alternations for
new words; and (b) a concrete unsupervised
implementation of the framework, the Cen-
troid Attribute Model. We evaluate this model
against a set of 2,400 ambiguous words and
demonstrate that it outperforms two baselines.
1 Introduction
One of the biggest challenges in computational se-
mantics is the fact that many words are polysemous.
For instance, lamb can refer to an animal (as in The
lamb squeezed through the gap) or to a food item (as
in Sue had lamb for lunch). Polysemy is pervasive
in human language and is a problem in almost all
applications of NLP, ranging from Machine Trans-
lation (as word senses can translate differently) to
Textual Entailment (as most lexical entailments are
sense-specific).
The field has thus devoted a large amount of effort
to the representation and modeling of word senses.
The arguably most prominent effort is Word Sense
Disambiguation, WSD (Navigli, 2009), an in-vitro
task whose goal is to identify which, of a set of pre-
defined senses, is the one used in a given context.
In work on WSD and other tasks related to pol-
ysemy, such as word sense induction, sense alter-
nations are treated as word-specific. As a result, a
model for the meaning of lamb that accounts for the
relation between the animal and food senses cannot
predict that the same relation holds between instances
of chicken or salmon in the same type of contexts.
A large number of studies in linguistics and cog-
nitive science show evidence that there are regulari-
ties in the way words vary in their meaning (Apres-
jan, 1974; Lakoff and Johnson, 1980; Copestake
and Briscoe, 1995; Pustejovsky, 1995; Gentner et
al., 2001; Murphy, 2002), due to general analogical
processes such as regular polysemy, metonymy and
metaphor. Most work in theoretical linguistics has
focused on regular, systematic, or logical polysemy,
which accounts for alternations like ANIMAL-FOOD.
Sense alternations also arise from metaphorical use
of words, as dark in dark glass-dark mood, and also
from metonymy when, for instance, using the name
of a place for a representative (as in Germany signed
the treatise). Disregarding this evidence is empiri-
cally inadequate and leads to the well-known lexical
bottleneck of current word sense models, which have
serious problems in achieving high coverage (Navigli,
2009).
We believe that empirical computational semantics
could profit from a model of polysemy1 which (a) is
applicable across individual words, and thus capable
of capturing general patterns and generalizing to new
1Our work is mostly inspired in research on regular polysemy.
However, given the fuzzy nature of ?regularity? in meaning
variation, we extend the focus of our attention to include other
types of analogical sense construction processes.
151
words, and (b) is induced in an unsupervised fashion
from corpus data. This is a long-term goal with many
unsolved subproblems.
The current paper presents two contributions to-
wards this goal. First, since we are working on a
relatively unexplored area, we introduce a formal
framework that can encompass different approaches
(Section 2). Second, we implement a concrete instan-
tiation of this framework, the unsupervised Centroid
Attribute Model (Section 3), and evaluate it on a new
task, namely, to detect which of a set of words in-
stantiate a given type of polysemy (Sections 4 and 5).
We finish with some conclusions and future work
(Section 7).
2 Formal framework
In addition to introducing formal definitions for terms
commonly found in the literature, our framework pro-
vides novel terminology to deal with regular poly-
semy in a general fashion (cf. Table 1; capital letters
designate sets and small letters elements of sets).2
For a lemma l like lamb, we want to know
how well a meta alternation (such as ANIMAL-
FOOD) explains a pair of its senses (such as the
animal and food senses of lamb).3 This is for-
malized through the function score, which maps
a meta alternation and two senses onto a score.
As an example, let lambanm denote the ANIMAL
sense of lamb, lambfod the FOOD sense, and
lambhum the PERSON sense. Then, an appropri-
ate model of meta alternations should predict that
score(animal,food, lambanm, lambfod) is greater
than score(animal,food, lambanm, lambhum).
Meta alternations are defined as unordered pairs
of meta senses, or cross-word senses like ANIMAL.
The meta sensesM can be defined a priori or induced
from data. They are equivalence classes of senses to
which they are linked through the function meta. A
sense s instantiates a meta sense m iff meta(s) =
m. Functions inst and sns allow us to define meta
senses and lemma-specific senses in terms of actual
instances, or occurrences of words in context.
2We re-use inst as a function that returns the set of instances
for a sense: SL ? ?(IL) and assume that senses partition
lemmas? instances: ?l : inst(l) =
?
s?sns(l) inst(s).
3Consistent with the theoretical literature, this paper focuses
on two-way polysemy. See Section 7 for further discussion.
L set of lemmas
IL set of (lemma-wise) instances
SL set of (lemma-wise) senses
inst : L? ?(IL) mapping lemma? instances
sns : L? ?(SL) mapping lemma? senses
M set of meta senses
meta: SL ?M mapping senses?meta senses
A ?M ?M set of meta alternations (MAs)
A set of MA representations
score : A? S2L ? R scoring function for MAs
repA : A? A MA representation function
comp: A?S2L ? R compatibility function
Table 1: Notation and signatures for our framework.
We decompose the score function into two parts:
a representation function repA that maps a meta al-
ternation into some suitable representation for meta
alternations, A, and a compatibility function comp
that compares the relation between the senses of a
word to the meta alternation?s representation. Thus,
comp ? repA = score.
3 The Centroid Attribute Model
The Centroid Attribute Model (CAM) is a simple
instantiation of the framework defined in Section 2,
designed with two primary goals in mind. First, it is
a data-driven model. Second, it does not require any
manual sense disambiguation, a notorious bottleneck.
To achieve the first goal, CAM uses a distribu-
tional approach. It represents the relevant entities as
co-occurrence vectors that can be acquired from a
large corpus (Turney and Pantel, 2010). To achieve
the second goal, CAM represents meta senses using
monosemous words only, that is, words whose senses
all correspond to one meta sense. 4 Examples are
cattle and robin for the meta sense ANIMAL. We
define the vector for a meta sense as the centroid (av-
erage vector) of the monosemous words instantiating
it. In turn, meta alternations are represented by the
centroids of their meta senses? vectors.
This strategy is not applicable to test lemmas,
which instantiate some meta alternation and are by
definition ambiguous. To deal with these without
410.8% of noun types in the corpus we use are monosemous
and 2.3% are disemous, while, on a token level, 23.3% are
monosemous and 20.2% disemous.
152
vecI : IL ? Rk instance vector computation
C : Rk?m ? Rk centroid computation
vecL : L? Rk lemma (type) vector computation
repM : M ? Rk meta sense representation
Table 3: Additional notation and signatures for CAM
explicit sense disambiguation, CAM represents lem-
mas by their type vectors, i.e., the centroid of their
instances, and compares their vectors (attributes) to
those of the meta alternation ? hence the name.
CoreLex: A Semantic Inventory. CAM uses
CoreLex (Buitelaar, 1998) as its meta sense inven-
tory. CoreLex is a lexical resource that was designed
specifically for the study of polysemy. It builds on
WordNet (Fellbaum, 1998), whose sense distinctions
are too fine-grained to describe general sense al-
ternations. CoreLex defines a layer of abstraction
above WordNet consisting of 39 basic types, coarse-
grained ontological classes (Table 2). These classes
are linked to one or more Wordnet anchor nodes,
which define a mapping from WordNet synsets onto
basic types: A synset s maps onto a basic type b if b
has an anchor node that dominates s and there is no
other anchor node on the path from b and s.5
We adopt the WordNet synsets as S, the set of
senses, and the CoreLex basic types as our set of
meta senses M . The meta function (mapping word
senses onto meta senses) is given directly by the an-
chor mapping defined in the previous paragraph. This
means that the set of meta alternations is given by the
set of pairs of basic types. Although basic types do
not perfectly model meta senses, they constitute an
approximation that allows us to model many promi-
nent alternations such as ANIMAL-FOOD.
Vectors for Meta Senses and Alternations. All
representations used by CAM are co-occurrence vec-
tors in Rk (i.e., A := Rk). Table 3 lists new concepts
that CAM introduces to manipulate vector represen-
tations. vecI returns a vector for a lemma instance,
vecL a (type) vector for a lemma, and C the centroid
of a set of vectors.
We leave vecI and C unspecified: we will experi-
ment with these functions in Section 4. CAM does fix
5This is necessary because some classes have non-disjoint
anchor nodes: e.g., ANIMALs are a subset of LIVING BEINGs.
the definitions for vecL and repA. First, vecL defines
a lemma?s vector as the centroid of its instances:
vecL(l) = C{vecI(i) | i ? inst(l)} (1)
Before defining repA, we specify a function repM
that computes vector representations for meta senses
m. In CAM, this vector is defined as the centroid
of the vectors for all monosemous lemmas whose
WordNet sense maps onto m:
repM(m) = C{vecL(l) | meta(sns(l)) = {m}} (2)
Now, repA can be defined simply as the centroid of
the meta senses instantiating a:
repA(m1,m2) = C{repM(m1), repM(m2)} (3)
Predicting Meta Alternations. The final compo-
nent of CAM is an instantiation of comp (cf. Table 1),
i.e., the degree to which a sense pair (s1, s2) matches
a meta alternation a. Since CAM does not represent
these senses separately, we define comp as
comp(a, s1, s2) = sim(a, vecL(l))
so that {s1, s2} = sns(l)
(4)
The complete model, score, can now be stated as:
score(m,m?, s, s?) = sim(repA(m,m
?), vecL(l))
so that {s, s?} = sns(l) (5)
CAM thus assesses how well a meta alternation
a = (m,m?) explains a lemma l by comparing the
centroid of the meta senses m,m? to l?s centroid.
Discussion. The central feature of CAM is that
it avoids word sense disambiguation, although it
still relies on a predefined sense inventory (Word-
Net, through CoreLex). Our use of monosemous
words to represent meta senses and meta alternations
goes beyond previous work which uses monosemous
words to disambiguate polysemous words in context
(Izquierdo et al, 2009; Navigli and Velardi, 2005).
Because of its focus on avoiding disambiguation,
CAM simplifies the representation of meta alterna-
tions and polysemous words to single centroid vec-
tors. In the future, we plan to induce word senses
(Schu?tze, 1998; Pantel and Lin, 2002; Reisinger and
Mooney, 2010), which will allow for more flexible
and realistic models.
153
abs ABSTRACTION ent ENTITY loc LOCATION prt PART
act ACT evt EVENT log GEO. LOCATION psy PSYCHOL. FEATURE
agt AGENT fod FOOD mea MEASURE qud DEFINITE QUANTITY
anm ANIMAL frm FORM mic MICROORGANISM qui INDEFINITE QUANTITY
art ARTIFACT grb BIOLOG. GROUP nat NATURAL BODY rel RELATION
atr ATTRIBUTE grp GROUPING phm PHENOMENON spc SPACE
cel CELL grs SOCIAL GROUP pho PHYSICAL OBJECT sta STATE
chm CHEMICAL hum HUMAN plt PLANT sub SUBSTANCE
com COMMUNICATION lfr LIVING BEING pos POSSESSION tme TIME
con CONSEQUENCE lme LINEAR MEASURE pro PROCESS pro PROCESS
Table 2: CoreLex?s basic types with their corresponding WordNet anchors. CAM adopts these as meta senses.
4 Evaluation
We test CAM on the task of identifying which lem-
mas of a given set instantiate a specific meta alterna-
tion. We let the model rank the lemmas through the
score function (cf. Table (1) and Eq. (5)) and evaluate
the ranked list using Average Precision. While an
alternative would be to rank meta alternations for a
given polysemous lemma, the method chosen here
has the benefit of providing data on the performance
of individual meta senses and meta alternations.
4.1 Data
All modeling and data extraction was carried out on
the written part of the British National Corpus (BNC;
Burnage and Dunlop (1992)) parsed with the C&C
tools (Clark and Curran, 2007). 6
For the evaluation, we focus on disemous words,
words which instantiate exactly two meta senses
according to WordNet. For each meta alternation
(m,m?), we evaluate CAM on a set of disemous tar-
gets (lemmas that instantiate (m,m?)) and disemous
distractors (lemmas that do not). We define three
types of distractors: (1) distractors sharing m with
the targets (but not m?), (2) distractors sharing m?
with the targets (but not m), and (3) distractors shar-
ing neither. In this way, we ensure that CAM cannot
obtain good results by merely modeling the similarity
of targets to either m or m?, which would rather be a
coarse-grained word sense modeling task.
To ensure that we have enough data, we evaluate
CAM on all meta alternations with at least ten targets
that occur at least 50 times in the corpus, discarding
nouns that have fewer than 3 characters or contain
non-alphabetical characters. The distractors are cho-
6The C&C tools were able to reliably parse about 40M words.
sen so that they match targets in frequency. This
leaves us with 60 meta alternations, shown in Ta-
ble 5. For each meta alternation, we randomly select
40 lemmas as experimental items (10 targets and 10
distractors of each type) so that a total of 2,400 lem-
mas is used in the evaluation.7 Table 4 shows four
targets and their distractors for the meta alternation
ANIMAL-FOOD.8
4.2 Evaluation Measure and Baselines
To measure success on this task, we use Average
Precision (AP), an evaluation measure from IR that
reaches its maximum value of 1 when all correct
items are ranked at the top (Manning et al, 2008).
It interpolates the precision values of the top-n pre-
diction lists for all positions n in the list that con-
tain a target. Let T = ?q1, . . . , qm? be the list of
targets, and let P = ?p1, . . . , pn? be the list of pre-
dictions as ranked by the model. Let I(xi) = 1 if
pi ? T , and zero otherwise. Then AP (P, T ) =
1
m
?m
i=1 I(xi)
?i
j=1 I(xi)
i . AP measures the quality
of the ranked list for a single meta alternation. The
overall quality of a model is given by Mean Average
Precision (MAP), the mean of the AP values for all
meta alternations.
We consider two baselines: (1) A random baseline
that ranks all lemmas in random order. This baseline
is the same for all meta alternations, since the distri-
bution is identical. We estimate it by sampling. (2)
A meta alternation-specific frequency baseline which
orders the lemmas by their corpus frequencies. This
7Dataset available at http://www.nlpado.de/
?sebastian/data.shtml.
8Note that this experimental design avoids any overlap be-
tween the words used to construct sense vectors (one meta sense)
and the words used in the evaluation (two meta senses).
154
Targets Distractors with meta sense anm Distractors with meta sense fod Random distractors
carp amphibian (anm-art) mousse (art-fod) appropriation (act-mea)
duckling ape (anm-hum) parsley (fod-plt) scissors (act-art)
eel leopard (anm-sub) pickle (fod-sta) showman (agt-hum)
hare lizard (anm-hum) pork (fod-mea) upholstery (act-art)
Table 4: Sample of experimental items for the meta alternation anm-fod. (Abbreviations are listed in Table 2.)
baseline uses the intuition that frequent words will
tend to exhibit more typical alternations.
4.3 Model Parameters
There are four more parameters to set.
Definition of vector space. We instantiate the vecI
function in three ways. All three are based on
dependency-parsed spaces, following our intuition
that topical similarity as provided by window-based
spaces is insufficient for this task. The functions dif-
fer in the definition of the space?s dimensions, incor-
porating different assumptions about distributional
differences among meta alternations.
The first option, gram, uses grammatical paths
of lengths 1 to 3 as dimensions and thus character-
izes lemmas and meta senses in terms of their gram-
matical context (Schulte im Walde, 2006), with a
total of 2,528 paths. The second option, lex, uses
words as dimensions, treating the dependency parse
as a co-occurrence filter (Pado? and Lapata, 2007),
and captures topical distinctions. The third option,
gramlex, uses lexicalized dependency paths like
obj?see to mirror more fine-grained semantic proper-
ties (Grefenstette, 1994). Both lex and gramlex
use the 10,000 most frequent items in the corpus.
Vector elements. We use ?raw? corpus co-
occurrence frequencies as well as log-likelihood-
transformed counts (Lowe, 2001) as elements of the
co-occurrence vectors.
Definition of centroid computation. There are
three centroid computations in CAM: to combine
instances into lemma (type) vectors (function vecL
in Eq. (1)); to combine lemma vectors into meta
sense vectors (function repM in Eq. (2)); and to com-
bine meta sense vectors into meta alternation vectors
(function repA in Eq. (3)).
For vecL, the obvious definition of the centroid
function is as a micro-average, that is, a simple av-
erage over all instances. For repM and repA, there
is a design choice: The centroid can be computed
by micro-averaging as well, which assigns a larger
weight to more frequent lemmas (repM) or meta
senses (repA). Alternatively, it can be computed
by macro-averaging, that is, by normalizing the in-
dividual vectors before averaging. This gives equal
weight to the each lemma or meta sense, respectively.
Macro-averaging in repA thus assumes that senses
are equally distributed, which is an oversimplifica-
tion, as word senses are known to present skewed
distributions (McCarthy et al, 2004) and vectors for
words with a predominant sense will be similar to the
dominant meta sense vector. Micro-averaging par-
tially models sense skewedness under the assumption
that word frequency correlates with sense frequency.
Similarity measure. As the vector similarity mea-
sure in Eq. (5), we use the standard cosine similar-
ity (Lee, 1999). It ranges between ?1 and 1, with 1
denoting maximum similarity. In the current model
where the vectors do not contain negative counts, the
range is [0; 1].
5 Results
Effect of Parameters The four parameters of Sec-
tion 4.3 (three space types, macro-/micro-averaging
for repM and repA, and log-likelihood transforma-
tion) correspond to 24 instantiations of CAM.
Figure 1 shows the influence of the four parame-
ters. The only significant difference is tied to the use
of lexicalized vector spaces (gramlex / lex are
better than gram). The statistical significance of this
difference was verified by a t-test (p < 0.01). This
indicates that meta alternations can be characterized
better through fine-grained semantic distinctions than
by syntactic ones.
The choice of micro- vs. macro-average does not
have a clear effect, and the large variation observed
in Figure 1 suggests that the best setup is dependent
on the specific meta sense or meta alternation being
155
MACRO MICRO0
.3
5
0.
37
0.
39
repM
MACRO MICRO0
.3
5
0.
37
0.
39
repA
gram gramlex lex0
.3
5
0.
37
0.
39
space type
?
False True0
.3
5
0.
37
0.
39
LL transformation
Figure 1: Effect of model parameters on performance. A
data point is the mean AP (MAP) across all meta alterna-
tions for a specific setting.
modeled. Focusing on meta alternations, whether the
two intervening meta senses should be balanced or
not can be expected to depend on the frequencies of
the concepts denoted by each meta sense, which vary
for each case. Indeed, for AGENT-HUMAN, the alter-
nation which most benefits from the micro-averaging
setting, the targets are much more similar to the HU-
MAN meta sense (which is approximately 8 times as
frequent as AGENT) than to the AGENT meta sense.
The latter contains anything that can have an effect on
something, e.g. emulsifier, force, valium. The targets
for AGENT-HUMAN, in contrast, contain words such
as engineer, manipulator, operative, which alternate
between an agentive role played by a person and the
person herself.
While lacking in clear improvement, log-
likelihood transformation tends to reduce variance,
consistent with the effect previously found in selec-
tional preference modeling (Erk et al, 2010).
Overall Performance Although the performance
of the CAM models is still far from perfect, all 24
models obtain MAP scores of 0.35 or above, while
the random baseline is at 0.313, and the overall fre-
quency baseline at 0.291. Thus, all models con-
sistently outperform both baselines. A bootstrap
resampling test (Efron and Tibshirani, 1994) con-
firmed that the difference to the frequency baseline
is significant at p < 0.01 for all 24 models. The
difference to the random baseline is significant at
p < 0.01 for 23 models and at p < 0.05 for the
remaining model. This shows that the models cap-
ture the meta alternations to some extent. The best
model uses macro-averaging for repM and repA in
a log-likelihood transformed gramlex space and
achieves a MAP of 0.399.
Table 5 breaks down the performance of the best
CAM model by meta alternation. It shows an en-
couraging picture: CAM outperforms the frequency
baseline for 49 of the 60 meta alternations and both
baselines for 44 (73.3%) of all alternations. The per-
formance shows a high degree of variance, however,
ranging from 0.22 to 0.71.
Analysis by Meta Alternation Coherence Meta
alternations vary greatly in their difficulty. Since
CAM is an attribute similarity-based approach, we
expect it to perform better on the alternations whose
meta senses are ontologically more similar. We next
test this hypothesis.
Let Dmi = {dij} be the set of distractors for
the targets T = {tj} that share the meta sense mi,
and DR = {d3j} the set of random distractors. We
define the coherence ? of an alternation a of meta
senses m1,m2 as the mean (?) difference between
the similarity of each target vector to a and the simi-
larity of the corresponding distractors to a, or for-
mally ?(a) = ? sim(repA(m1,m2), vecL(tj)) ?
sim(repA(m1,m2), vecL(dij)), for 1 ? i ? 3 and
1 ? j ? 10. That is, ? measures how much more
similar, on average, the meta alternation vector is to
the target vectors than to the distractor vectors. For a
meta alternation with a higher ?, the targets should
be easier to distinguish from the distractors.
Figure 2 plots AP by ? for all meta alternations.
As we expect from the definition of ?, AP is strongly
correlated with ?. However, there is a marked Y
shape, i.e., a divergence in behavior between high-
? and mid-AP alternations (upper right corner) and
mid-? and high-AP alternations (upper left corner).
In the first case, meta alternations perform worse
than expected, and we find that this typically points
to missing senses, that is, problems in the underlying
lexical resource (WordNet, via CoreLex). For in-
stance, the FOOD-PLANT distractor almond is given
156
grs-psy 0.709 com-evt 0.501 art-com 0.400 atr-com 0.361 art-frm 0.286
pro-sta 0.678 art-grs 0.498 act-pos 0.396 atr-sta 0.361 act-hum 0.281
fod-plt 0.645 hum-psy 0.486 phm-sta 0.388 act-phm 0.339 art-fod 0.280
psy-sta 0.630 hum-nat 0.456 atr-psy 0.384 anm-art 0.335 grs-hum 0.272
hum-prt 0.602 anm-hum 0.448 fod-hum 0.383 art-atr 0.333 act-art 0.267
grp-psy 0.574 com-psy 0.443 plt-sub 0.383 act-psy 0.333 art-grp 0.258
grs-log 0.573 act-grs 0.441 act-com 0.382 agt-hum 0.319 art-nat 0.248
act-evt 0.539 atr-rel 0.440 grp-grs 0.379 art-evt 0.314 act-atr 0.246
evt-psy 0.526 art-qui 0.433 art-psy 0.373 atr-evt 0.312 art-hum 0.240
act-tme 0.523 act-sta 0.413 art-prt 0.364 art-sta 0.302 art-loc 0.238
art-pho 0.520 art-sub 0.412 evt-sta 0.364 act-grp 0.296 art-pos 0.228
act-pro 0.513 art-log 0.407 anm-fod 0.361 com-hum 0.292 com-sta 0.219
Table 5: Meta alternations and their average precision values for the task. The random baseline performs at 0.313 while
the frequency baseline ranges from 0.255 to 0.369 with a mean of 0.291. Alternations for which the model outperforms
the frequency baseline are in boldface (mean AP: 0.399, standard deviation: 0.119).
grs-psy democracy, faculty, humanism, regime,
pro-sta bondage, dehydration, erosion,urbanization
psy-sta anaemia,delight, pathology, sensibility
hum-prt bum, contractor, peter, subordinate
grp-psy category, collectivism, socialism, underworld
Table 6: Sample targets for meta alternations with high
AP and mid-coherence values.
a PLANT sense by WordNet, but no FOOD sense. In
the case of SOCIAL GROUP-GEOGRAPHICAL LOCA-
TION, distractors laboratory and province are miss-
ing SOCIAL GROUP senses, which they clearly pos-
sess (cf. The whole laboratory celebrated Christmas).
This suggests that our approach can help in Word
Sense Induction and thesaurus construction.
In the second case, meta alternations perform bet-
ter than expected: They have a low ?, but a high
AP. These include grs-psy, pro-sta, psy-sta,
hum-prt and grp-psy. These meta alternations
involve fairly abstract meta senses such as PSYCHO-
LOGICAL FEATURE and STATE.9 Table 6 lists a
sample of targets for the five meta alternations in-
volved. The targets are clearly similar to each other
on the level of their meta senses. However, they can
occur in very different semantic contexts. Thus, here
it is the underlying model (the gramlex space) that
can explain the lower than average coherence. It is
striking that CAM can account for abstract words and
meta alternations between these, given that it uses
first-order co-occurrence information only.
9An exception is hum-prt. It has a low coherence because
many WordNet lemmas with a PART sense are body parts.
0.00 0.05 0.10 0.15 0.20 0.250.
2
0.3
0.4
0.5
0.6
0.7
coherence
AP
act?artact?atr
act?com
act?evt
act?grp
act?grs
act?hum
act?phm
act?pos
act?pro
sy
act?sta
act?tme
agt?humanm?art
anm?fod
anm?hum
art?at
art?com
art?evt
art?fodart?frmart?grp
art?grs
art?humart?loc
art?log
r nat
art?pho
art?pos
art?prtart?psy
art?qui
r sta
art?sub
atr?com
atr?evt
a r?psy
atr?rel
sta
com?evt
com?hum
com psy
com?sta
evt?psy
ev ?stafod hum
fod?plt
grp?grs
grp?psy
grs?hum
grs?log
grs?psy
hum?nat
hum?prt
hum?psy
phm?staplt?sub
pro?sta
psy?sta
Figure 2: Average Precision and Coherence (?) for each
meta alternation. Correlation: r = 0.743 (p < 0.001)
6 Related work
As noted in Section 1, there is little work in empiri-
cal computational semantics on explicitly modeling
sense alternations, although the notions that we have
formalized here affect several tasks across NLP sub-
fields.
Most work on regular sense alternations has fo-
cused on regular polysemy. A pioneering study is
Buitelaar (1998), who accounts for regular polysemy
through the CoreLex resource (cf. Section 3). A
similar effort is carried out by Tomuro (2001), but
he represents regular polysemy at the level of senses.
Recently, Utt and Pado? (2011) explore the differences
between between idiosyncratic and regular polysemy
patterns building on CoreLex. Lapata (2000) focuses
157
on the default meaning arising from word combina-
tions, as opposed to the polysemy of single words as
in this study.
Meta alternations other than regular polysemy,
such as metonymy, play a crucial role in Informa-
tion Extraction. For instance, the meta alternation
SOCIAL GROUP-GEOGRAPHICAL LOCATION cor-
responds to an ambiguity between the LOCATION-
ORGANIZATION Named Entity classes which is
known to be a hard problem in Named Entity Recog-
nition and Classification (Markert and Nissim, 2009).
Metaphorical meta alternations have also received
attention recently (Turney et al, 2011)
On a structural level, the prediction of meta al-
ternations shows a clear correspondence to analogy
prediction as approached in Turney (2006) (carpen-
ter:wood is analogous to mason:stone, but not to
photograph:camera). The framework defined in Sec-
tion 2 conceptualizes our task in a way parallel to that
of analogical reasoning, modeling not ?first-order?
semantic similarity, but ?second-order? semantic re-
lations. However, the two tasks cannot be approached
with the same methods, as Turney?s model relies on
contexts linking two nouns in corpus sentences (what
does A do to B?). In contrast, we are interested in
relations within words, namely between word senses.
We cannot expect two different senses of the same
noun to co-occur in the same sentence, as this is dis-
couraged for pragmatic reasons (Gale et al, 1992).
A concept analogous to our notion of meta sense
(i.e., senses beyond single words) has been used in
previous work on class-based WSD (Yarowsky, 1992;
Curran, 2005; Izquierdo et al, 2009), and indeed,
the CAM might be used for class-based WSD as
well. However, our emphasis lies rather on modeling
polysemy across words (meta alternations), some-
thing that is absent in WSD, class-based or not. The
only exception, to our knowledge, is Ando (2006),
who pools the labeled examples for all words from a
dataset for learning, implicitly exploiting regularities
in sense alternations.
Meta senses also bear a close resemblance to the
notion of semantic class as used in lexical acqui-
sition (Hindle, 1990; Merlo and Stevenson, 2001;
Schulte im Walde, 2006; Joanis et al, 2008). How-
ever, in most of this research polysemy is ignored.
A few exceptions use soft clustering for multiple as-
signment of verbs to semantic classes (Pereira et al,
1993; Rooth et al, 1999; Korhonen et al, 2003),
and Boleda et al (to appear) explicitly model regular
polysemy for adjectives.
7 Conclusions and Future Work
We have argued that modeling regular polysemy and
other analogical processes will help improve current
models of word meaning in empirical computational
semantics. We have presented a formal framework
to represent and operate with regular sense alterna-
tions, as well as a first simple instantiation of the
framework. We have conducted an evaluation of dif-
ferent implementations of this model in the new task
of determining whether words match a given sense
alternation. All models significantly outperform the
baselines when considered as a whole, and the best
implementation outperforms the baselines for 73.3%
of the tested alternations.
We have two next steps in mind. The first is to
become independent of WordNet by unsupervised
induction of (meta) senses and alternations from the
data. This will allow for models that, unlike CAM,
can go beyond ?disemous? words. Other improve-
ments on the model and evaluation will be to develop
more informed baselines that capture semantic shifts,
as well as to test alternate weighting schemes for the
co-occurrence vectors (e.g. PMI) and to use larger
corpora than the BNC.
The second step is to go beyond the limited in-vitro
evaluation we have presented here by integrating al-
ternation prediction into larger NLP tasks. Knowl-
edge about alternations can play an important role in
counteracting sparseness in many tasks that involve
semantic compatibility, e.g., testing the applicability
of lexical inference rules (Szpektor et al, 2008).
Acknowledgements
This research is partially funded by the Spanish Min-
istry of Science and Innovation (FFI2010-15006,
TIN2009-14715-C04-04), the AGAUR (2010 BP-
A00070), the German Research Foundation (SFB
732), and the EU (PASCAL2; FP7-ICT-216886). It
is largely inspired on a course by Ann Copestake at
U. Pompeu Fabra (2008). We thank Marco Baroni,
Katrin Erk, and the reviewers of this and four other
conferences for valuable feedback.
158
References
Rie Kubota Ando. 2006. Applying alternating structure
optimization to word sense disambiguation. In Proceed-
ings of the 10th Conference on Computational Natural
Language Learning, pages 77?84, New York City, NY.
Iurii Derenikovich Apresjan. 1974. Regular polysemy.
Linguistics, 142:5?32.
Gemma Boleda, Sabine Schulte im Walde, and Toni Badia.
to appear. Modeling regular polysemy: A study of the
semantic classification of Catalan adjectives. Computa-
tional Linguistics.
Paul Buitelaar. 1998. CoreLex: An ontology of sys-
tematic polysemous classes. In Proceedings of For-
mal Ontologies in Information Systems, pages 221?235,
Amsterdam, The Netherlands.
Gavin Burnage and Dominic Dunlop. 1992. Encoding
the British National Corpus. In Jan Aarts, Pieter de
Haan, and Nelleke Oostdijk, editors, English Language
Corpora: Design, Analysis and Exploitation, Papers
from the Thirteenth International Conference on En-
glish Language Research on Computerized Corpora.
Rodopi, Amsterdam.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and log-l
inear models. Computational Linguistics, 33(4).
Ann Copestake and Ted Briscoe. 1995. Semi-productive
Polysemy and Sense Extension. Journal of Semantics,
12(1):15?67.
James Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings of the
43rd Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 26?33, Ann Arbor,
Michigan.
Bradley Efron and Robert Tibshirani. 1994. An Introduc-
tion to the Bootstrap. Monographs on Statistics and
Applied Probability 57. Chapman & Hall.
Katrin Erk, Sebastian Pado?, and Ulrike Pado?. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36(4):723?763.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT, London.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Proceed-
ings of the 1992 ARPA Human Language Technologies
Workshop, pages 233?237, Harriman, NY.
Dedre Gentner, Brian F. Bowdle, Phillip Wolff, and Con-
suelo Boronat. 2001. Metaphor is like analogy. In
D. Gentner, K. J. Holyoak, and B. N. Kokinov, edi-
tors, The analogical mind: Perspectives from Cognitive
Science, pages 199?253. MIT Press, Cambridge, MA.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers.
Donald Hindle. 1990. Noun classification from predicate-
argument structures. In Proceedings of the 28th Meet-
ing of the Association for Computational Linguistics,
pages 268?275.
Rube?n Izquierdo, Armando Sua?rez, and German Rigau.
2009. An empirical study on class-based word sense
disambiguation. In Proceedings of the 12th Conference
of the European Chapter of the ACL (EACL 2009),
pages 389?397, Athens, Greece.
Eric Joanis, Suzanne Stevenson, and David James. 2008.
A general feature space for automatic verb classifica-
tion. Natural Language Engineering, 14(03):337?367.
Anna Korhonen, Yuval Krymolowski, and Zvika Marx.
2003. Clustering polysemic subcategorization frame
distributions semantically. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 64?71.
George Lakoff and Mark Johnson. 1980. Metaphors We
Live By. University of Chicago Press.
Mirella Lapata. 2000. The Acquisition and Modeling
of Lexical Knowledge: A Corpus-based Investigation
of Systematic Polysemy. Ph.D. thesis, University of
Edinburgh.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 25?32,
College Park, MA.
Will Lowe. 2001. Towards a theory of semantic space. In
Proceedings of the 23rd Annual Meeting of the Cogni-
tive Science Society, pages 576?581, Edinburgh, UK.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge, UK,
1st edition.
Katja Markert and Malvina Nissim. 2009. Data and
models for metonymy resolution. Language Resources
and Evaluation, 43(2):123?138.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Car-
roll. 2004. Using automatically acquired predominant
senses for word sense disambiguation. In Proceedings
of the ACL SENSEVAL-3 workshop, pages 151?154.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
verb classification based on statistical distributions
of argument structure. Computational Linguistics,
27(3):373?408.
Gregory L. Murphy. 2002. The Big Book of Concepts.
MIT Press, Cambridge, MA.
Roberto Navigli and Paola Velardi. 2005. Structural se-
mantic interconnections: a knowledge-based approach
to word sense disambiguation. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 27(7):1075?
1086, July.
159
Roberto Navigli. 2009. Word sense disambiguation:
A survey. ACM Computing Surveys, 41:10:1?10:69,
February.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of ACM SIGKDD
Conference on Knowledge Discovery and Data Mining
2002, pages 613?619, Edmonton.
Fernando C. N. Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words. In
Proceedings of the 31st Meeting of the Association for
Computational Linguistics, pages 183?190, Columbus,
OH.
James Pustejovsky. 1995. The Generative Lexicon. MIT
Press, Cambridge, MA.
Joseph Reisinger and Raymond J. Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Proceedings of the 11th Annual Conference of the North
American Chapter of the Association for Computational
Linguistics (NAACL-2010), pages 109?117.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics, College Park, MD.
Sabine Schulte im Walde. 2006. Experiments on the
automatic induction of German semantic verb classes.
Computational Linguistics, 32(2):159?194.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob Gold-
berger. 2008. Contextual preferences. In Proceed-
ings of the 46th Annual Meeting of the Association for
Computational Linguistics, pages 683?691, Columbus,
Ohio.
Noriko Tomuro. 2001. Tree-cut and a lexicon based on
systematic polysemy. In Proceedings of the second
meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics on Language
technologies, NAACL ?01, pages 1?8, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Peter D. Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
Peter Turney, Yair Neuman, Dan Assaf, and Yohai Cohen.
2011. Literal and metaphorical sense identification
through concrete and abstract context. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 680?690, Edinburgh, Scot-
land, UK.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32:379?416.
Jason Utt and Sebastian Pado?. 2011. Ontology-based
distinction between polysemy and homonymy. In Pro-
ceedings of the 9th International Conference on Com-
putational Semantics, Oxford, UK.
David Yarowsky. 1992. Word-sense disambiguation using
statistical models of Roget?s categories trained on large
corpora. In Proceedings of the 14th conference on
Computational linguistics - Volume 2, COLING ?92,
pages 454?460, Stroudsburg, PA, USA. Association for
Computational Linguistics.
160
Acquiring entailment pairs across languages and domains:
A data analysis
Manaal Faruqui
Dept. of Computer Science and Engineering
Indian Institute of Technology
Kharagpur, India
manaal.iitkgp@gmail.com
Sebastian Pad?
Seminar f?r Computerlinguistik
Universit?t Heidelberg
Heidelberg, Germany
pado@cl.uni-heidelberg.de
Abstract
Entailment pairs are sentence pairs of a premise and a hypothesis, where the premise textually
entails the hypothesis. Such sentence pairs are important for the development of Textual Entailment
systems. In this paper, we take a closer look at a prominent strategy for their automatic acquisition
from newspaper corpora, pairing first sentences of articles with their titles. We propose a simple
logistic regression model that incorporates and extends this heuristic and investigate its robustness
across three languages and three domains. We manage to identify two predictors which predict
entailment pairs with a fairly high accuracy across all languages. However, we find that robustness
across domains within a language is more difficult to achieve.
1 Introduction
Semantic processing has become a major focus of attention in NLP. However, different applications
such as Question Answering, Information Extraction and Machine Translation often adopt very different,
task-specific semantic processing strategies. Textual entailment (TE) was introduced by Dagan et al
(2006) as a ?meta-task? that can subsume a large part of the semantic processing requirements of such
applications by providing a generic concept of inference that corresponds to ?common sense? reasoning
patterns. Textual Entailment is defined as a relation between two natural language utterances (a Premise
P and a Hypothesis H) that holds if ?a human reading P would infer that H is most likely true?. See,
e.g., the ACL ?challenge paper? by Sammons et al (2010) for further details.
The successive TE workshops that have taken place yearly since 2005 have produced annotation for
English which amount to a total of several thousand entailing Premise-Hypothesis sentence pairs, which
we will call entailment pairs:
(1) P: Swedish bond yields end 21 basis points higher.
H: Swedish bond yields rose further.
From the machine learning perspective assumed by many approaches to TE, this is a very small number
of examples, given the complex nature of entailment. Given the problems of manual annotation, therefore,
Burger and Ferro (2005) proposed to take advantage of the structural properties of a particular type of
discourse ? namely newspaper articles ? to automatically harvest entailment pairs. They proposed to pair
the title of each article with its first sentence, interpreting the first sentence as Premise and the title as
Hypothesis. Their results were mixed, with an average of 50% actual entailment pairs among all pairs
constructed in this manner. SVMs which identified ?entailment-friendly? documents based on their bags
of words lead to an accuracy of 77%. Building on the same general idea, Hickl et al (2006) applied a
simple unsupervised filter which removes all entailment pair candidates that ?did not share an entity (or
an NP)?. They report an accuracy of 91.8% on a manually evaluated sample ? considerably better Burger
and Ferro. The article however does not mention the size of the original corpus, and whether ?entity? is to
95
be understood as named entity, so it is difficult to assess what its recall is, and whether it presupposes a
high-quality NER system.
In this paper, we model the task using a logistic regression model that allows us to synchronously
analyse the data and predict entailment pairs, and focus on the question of how well these results generalize
across domains and languages, for many of which no entailment pairs are available at all. We make three
main contributions: (a), we define an annotation scheme based on semantic and discourse phenomena that
can break entailment and annotate two datasets with it; (b), we idenfiy two robust properties of sentence
pairs that correlate strongly with entailment and which are robust enough to support high-precision
entailment pair extraction; (c), we find that cross-domain differences are actually larger than cross-lingual
differences, even for languages as different as German and Hindi.
Plan of the paper. Section 2 defines our annotation scheme. In Section 3, we sketch the logistic
regression framework we use for analysis, and motivate our choice of predictors. Sections 4 and 5 present
the two experiments on language and domain comparisons, respectively. We conclude in Section 6.
2 A fine-grained annotation scheme for entailment pairs
The motivation of our annotation scheme is to better understand why entailment breaks down between
titles and first sentences of newswire articles. We subdivide the general no entailment category of earlier
studies according to an inventory of reasons for non-entailment that we collected from an informal
inspection of some dozen articles from an English-language newspaper. Additionally, we separate out
sentences that are ill-formed in the sense of not forming one proposition.
2.1 Subtypes of non-entailment
No-par (Partial entailment). The Premise entails the Hypothesis almost, but not completely, in one of
two ways: (a), The Hypothesis is a conjunction and the Premise entails just one conjunct; or (b),
Premise and Hypothesis share the main event, but the Premise is missing an argument or adjunct
that forms part of the Hypothesis. Presumably, in our setting, such information is provided by the
other sentences in the article than the first one. In Ex. (1), if P and H were switched, this would be
the case for the size of the rise.
No-pre (Presupposition): The Premise uses a construction which can only be understood with informa-
tion from the Hypothesis, typically a definite description or an adjunct. This category arises because
the title stands before the first sentence and is available as context. In the following example, the
Premise NP ?des Verbandes? can only be resolved through the mention of ?VDA? (the German car
manufacturer?s association) in the Hypothesis.
(2) P: Herzog
Herzog
wird
will
in
in
dem
the
vierk?pfigen
four-head
F?hrungsgremium
management board
des
of the
Verbands
association
f?r
for
die
the
Teile-
parts
und
and
Zubeh?rindustrie
accessory business
zust?ndig
resposible
sein.
be.
H: Martin
Martin
Herzog
Herzog
wird
becomes
VDA-Gesch?ftsf?hrer.
VDA manager.
No-con (Contradiction): Direct contradiction of Premise and Hypothesis.
(3) P: Wie
How
die
the
innere
biological
Uhr
clock
[...]
[...]
funktioniert,
works,
ist
is
noch
still
weitgehend
mostly
unbekannt.
unknown.
H: Licht
Light
stellt
regulates
die
the
innere
biological
Uhr.
clock.
96
No-emb (Embedding): The Premise uses an embedding that breaks entailment (e.g., modal adverbials or
non-factural embedding verb). In the following pair, the proposition in the Hypothesis is embedded
under ?expect?.
(4) P: An Arkansas gambling amendment [...] is expected to be submitted to the state Supreme
Court Monday for a rehearing, a court official said.
H: Arkansas gaming petition goes before court again Monday
No-oth (Other): All other negative examples where Premise and Hypothesis are well-formed, and which
could not be assigned to a more specific category, are included under this tag. In this sense, ?Other?
is a catch-all category. Often, Premise and Hypothesis, taken in isolation, are simply unrelated:
(5) P: Victor the Parrot kept shrieking "Voda, Voda" ? "Water, Water".
H: Thirsty jaguar procures water for Bulgarian zoo.
2.2 Ill-formed sentence pairs
Err (Error): These cases arise due to errors in sentence boundary detection: Premise or Hypothesis may
be cut off in the middle of the sentence.
Ill (Ill-formed): Often, the titles are not single grammatical sentences and can therefore not be interpreted
sensibly as the Hypothesis of an entailment pair. They can be incomplete proposition such as NPs
or PPs (?Beautiful house situated in woods?), or, frequently, combinations of multiple sentences
(?RESEARCH ALERT - Mexico upped, Chile cut.?).
3 Modeling entailment with logistic regression
We will model the entailment annotation labels on candidate sentence pairs using a logistic regression
model. From a machine learning point of view, logistic regression models can be seen as a rather simple
statistical classifier which can be used to acquire new entailment pairs. From a linguistic point of view,
they can be used to explain the phenomena in the data, see e.g., Bresnan et al (2007).
Formally, logistic regression models assume that datapoints consist of a set of predictors x and a
binary response variable y. They have the form
p(y = 1) = 1
1 + e?z with z =
?
i
?ixi (1)
where p is the probability of a datapoint x, ?i is the coefficient assigned to the linguistically motivated
factor xi. Model estimation sets the parameters ? so that the likelihood of the observed data is maximized.
From the linguistics perspective, we are most interested in analysing the importance of the different
predictors: for each predictor xi, the comparison of the estimated value of its coefficient ?i can be
compared to its estimated standard error, and it is possible to test the hypothesis that ?i = 0, i.e., the
predictor does not significantly contribute to the model. Furthermore, the absolute value of ?i can be
interpreted as the log odds ? that is, as the change in the probability of the response variable being positive
depending on xi being positive.
e?i = P (y = 1|x = 1, . . . )/P (y = 0|x = 1, . . . )P (y = 1|x = 0, . . . )/P (y = 0|x = 0, . . . ) (2)
The fact that z is just a linear combination of predictor weights encodes the assumption that the log odds
combine linearly among factors.
From the natural language processing perspective, we would like to create predictions for new
observations. Note, however, that simply assessing the significance of predictors on some dataset, as
97
provided by the logistic regression model, corresponds to an evaluation of the model on the training set,
which is prone to the problem of overfitting. We will therefore in our experiments always apply the models
acquired from one dataset on another to see how well they generalize.
3.1 Choice of Predictors
Next, we need a set of plausible predictors that we can plug into the logistic regression framework. These
predictors should ideally be language-independent. We analyse the categories of our annotation, as an
inventory of phenomena that break entailment, to motivate a small set of robust predictors.
Following early work on textual entailment, we use word overlap as a strong indicator of entail-
ment (Monz and de Rijke, 2001). Our weighted overlap predictor uses the well-known tf/idf weighting
scheme to compute the overlap between P and H (Manning et al, 2008):
weightedOverlap(T,H,D) =
?
w?T?H tf-idf(w,D)
?
w?H tf-idf(w,D)
(3)
where we treat each article as a separate document and the whole corpus as document collection D. We
expect that No-oth pairs have generally the lowest weighted overlap, followed by No-par pairs, while Yes
pairs have the highest weighted overlap. We also use a categorical version of this observation in the form
of our strict noun match predictor. This predictor is similar in spirit to the proposal by Hickl et al (2006)
mentioned in Section 1. The boolean strict noun match predictor is true if all Hypothesis nouns are present
in the Premise, and is therefore a predictor that is geared at precision rather than recall. A third predictor
that was motivated by the No-par and No-oth categories was the number of words in the article: No-oth
sentence pairs often come from long articles, where the first sentence provides merely an introduction. For
this predictor, log num words, we count the total number of words in the article and logarithmize it.1 The
remaining subcategories of No were more difficult to model. No-pre pairs should be identifiable by testing
whether the Premise contains a definite description that cannot be accommodated, a difficult problem
that seems to require world knowledge. Similarly, the recognition of contradictions, as is required to find
No-con pairs, is very difficult in itself (de Marneffe et al, 2008). Finally, No-emb requires the detection
of a counterfactual context in the Premise. Since we do not currently see robust, language-independent
ways of modelling these phenomena, we do not include specific predictors to address them.
The situation is similar with regard to the Err category. While it might be possible to detect incomplete
sentences with the help of a parser, this again involves substantial knowledge about the language. The Ill
category, however, appears easier to target: at least cases of Hypotheses consisting of multiple phrases
case be detected easily by checking for sentence end markers in the middle of the Hypothesis (full stop,
colon, dash). We call this predictor punctuation.
4 Experiment 1: Analysis by Language
4.1 Data sources and preparation
This experiment performs a cross-lingual comparison of three newswire corpora. We use English, German,
and Hindi. All three belong to the Indo-European language family, but English and German are more
closely related.
For English and German, we used the Reuters RCV2 Multilingual Corpus2. RCV2 contains over
487,000 news stories in 13 different languages. Almost all news stories cover the business and politics
domains. The corpus marks the title of each article; we used the sentence splitter provided by Treetag-
ger (Schmid, 1995) to extract the first sentences. Our Hindi corpus is extracted from the text collection
of South Asian languages prepared by the EMILLE project (Xiao et al, 2004)3. We use the Hindi
1This makes the coefficiently easier to interpret. The predictive difference is minimal.
2http://trec.nist.gov/data/reuters/reuters.html
3http://www.elda.org/catalogue/en/text/W0037.html
98
No. of sentence pairs English German Hindi
Original 473,874 (100%) 112,259 (100%) 20,209 (100%)
Filtered 264.711 (55.8%) 50.039 (44.5%) 10.475 (51.8%)
Table 1: Pair extraction statistics
Corpus err ill no-con no-emb no-oth no-par no-pre yes
English Reuters 3.5 2.9 0 0.2 3.7 7.4 0 82.3
German Reuters 2.1 11.0 0.4 0.2 4.3 2.1 0.2 79.7
Hindi Emille 1.1 2.5 0 0.3 14.7 5.7 0 75.7
Table 2: Exp.1: Distribution of annotation categories (in percent)
monolingual data, which was crawled from Webdunia,4 an Indian daily online newspaper. The articles
are predominantly political, with a focus on Indo-Pakistani and Indo-US affairs. We identify sentence
boudaries with the Hindi sentence marker (?|?), which is used exclusively for this purpose.
We preprocessed the data by extracting the title and the first sentence, treating the first sentence as
Premise and the title as Hypothesis. We applied a filter to remove pairs where the chance of entailment
was impossible or very small. Specifically, our filter keeps only sentence pairs that (a) share at least one
noun and where (b) both sentences include at least one verb and are not questions. Table 1 shows the
corpus sizes before and after filtering. Note that the percentage of selected sentences across the languages
are all in the 45%-55% range. This filter could presumably be improved by requiring a shared named
entity, but since language-independent NER is still an open research issue, we did not follow up on this
avenue. We randomly sampled 1,000 of the remaining sentence pairs per language for manual annotation.
4.2 Distribution of annotation categories
First, we compared the frequencies of the annotation categories defined in Section 3.1. The results are
shown in Table 2. We find our simple preprocessing filter results in an accuracy of between 75 and 82%.
This is still considerably below the results of Hickl et al, who report 92% accuracy on their English data.5
Even though the overall percentage of ?yes? cases is quite similar among languages, the details of the
distribution differ. One fairly surprising observation was the fairly large number of ill-formed sentence
pairs. As described in Section 2, this category comprises cases where the Hypothesis (i.e., a title) is not a
grammatical sentence. Further analysis of the category shows that the common patterns are participle
constructions (Ex. (6)) and combinations of multiple statements (Ex. (7)). The participle construction is
particularly prominent in German.
(6) Glencoe Electric, Minn., rated single-A by Moody?s.
(7) Wieder
Again
K?mpfe
fights
in
in
S?dlibanon
Southern Lebanon
-
-
Israeli
Israeli
get?tet.
killed.
The ?no?-categories make up a total of 11.3% (English), 6.6% (German), and 20.7% (Hindi). The ?other?
and ?partial? categories clearly dominate. This is to be expected, in particular the high number of partial
entailments. The ?other? category mostly consists of cases where the title summarizes the whole article,
but the first sentence provides only a gentle introduction to the topic:
(8) P: One automotive industry analyst has dubbed it the ?Lincoln Town Truck?.
H: Ford hopes Navigator will lure young buyers to Lincoln.
As regards the high ratio of ?no-other? cases in the Hindi corpus, we found a high number of instances
where the title states the gist of the article too differently from the first sentence to preserve entailment:
4http://www.webdunia.com
5We attribute the difference to the filtering scheme which is difficult to reconstruct from Hickl et al (2006).
99
Predictor German sig English sig Hindi sig
weighted overlap -0.77 ** -2.30 *** -3.35 ***
log num words -0.05 ? -0.03 ? -0.17 ?
punctuation -1.04 *** -0.43 ** -0.35 **
strict noun match -0.12 ? -0.19 ? -0.38 **
Table 3: Exp. 1: Predictors in the logreg model (*: p<0.05; **: p<0.01; ***: p<0.001)
(9) P: aAj BF E?\ss XAynA kF lokE?ytA km nhF\ h
 
I h{ .
Even today, Princess Diana?s popularity has not decreased.
H: E?\ss XAynA k p/ aOr kAX nFlAm ho\g .
Bidding on Princess Diana?s letter and cards would take place.
The remaining error categories (embedding, presupposition, contradiction) were, disappointingly, almost
absent. Another sizable category is formed by errors, though. We find the highest percentage for English,
where our sentence splitter misinterpreted full stops in abbreviations as sentence boundaries.
4.3 Modelling the data
We estimated logistic regression models on each dataset, using the predictors from Section 3.1. Consider-
ing the eventual goal of extracting entailment pairs, we use the decision yes vs. everything else as our
response variable. The analysis was performed with R, using the rms6 and ROCR7 packages.
Analysis of predictors. The coefficients for the predictors and their significances are shown in Table 3.
There is considerable parallelism between the languages. In all three languages, weighted overlap between
H and P is a significant predictor: high overlap indicates entailment, and vice versa. Its effect size is large
as well: Perfect overlap increases the probability of entailment for German by a factor of e0.77 = 2.16, for
English by 10, and for Hindi even by 28. Similarly, the punctuation predictor comes out as a significant
negative effect for all three languages, presumably by identifying ill-formed sentence pairs. In contrast,
the length of the article (log num words) is not a significant predictor. This is a surprising result, given
our hypothesis that long articles often involve an ?introduction? which reduces the chance for entailment
between the title and the first sentence. The explanation is that the two predictors, log num words and
weighted overlap, are highly significantly correlated in all three corpora. Since weighted overlap is the
predictive of the two, the model discards article length.
Finally, strict noun match, which requires that all nouns match between H and P, is assigned a
positive coefficient for each language, but only reaches significance for Hindi. This is the only genuine
cross-lingual difference: In our Hindi corpus, the titles are copied more verbatim from the text than for
English and German (median weighted overlap: Hindi 0.76, English 0.72, German 0.69). Consequently,
in English and German the filter discards too many entailment instances. For all three languages, though,
the coefficient is small ? for Hindi, where it is largest, it increases the odds by a factor of e0.39 ? 1.4.
Evaluation. We trained models on the three corpora, using only the two predictors that contributed
significantly in all languages (weighted overlap and punctuation), in order to avoid overfitting on the
individual datasets.8 We applied each model to each dataset. How such models should be evaluated
depends on the intended purpose of the classification. We assume that it is fairly easy to obtain large
corpora of newspaper text, which makes precision an issue rather than recall. The logistic regression
classifier assigns a probability to each datapoint, so we can trade off recall and precision. We fix recall at
a reasonable value (30%) and compare precision values.
6http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/Design
7http://rocr.bioinf.mpi-sb.mpg.de/
8Subsequent analysis of ?full? models (with all features) showed that they did not generally improve over two-feature models.
100
PPPPPPPP
Data
Models German model English model Hindi model
German data 91.6 88.8 88.8
English data 93.2 94.3 94.6
Hindi data 98.7 98.7 99.1
Table 4: Exp. 1: Precision for the class ?yes? (entailment) at 30% Recall
Our expectation is that each model will perform best on its own corpus (since this is basically the
training data), and worse on the other languages. The size of the drop for the other languages reflects the
differences between the corpora as well as the degree of overfitting models show to their training data.
The actual results are shown in Table 4.3. The precision is fairly high, generally over 90%, and well
above the baseline percentage of entailment pairs. The German data is modelled best by the German
model, with the two other models performing 3 percent worse. The situation is similar, although less
pronounced, on Hindi data, where the Hindi-trained model is 0.4% better than the two other models. For
English, the Hindi model even outperforms the English model by 0.3%9, which in turn works about 1%
better than the German model. In sum, the logistic regression models can be applied very well across
languages, with little loss in precision. The German data with its high ratio of ill-formed headlines (cf.
Table 2) is most difficult to model. Hindi is simplest, due to the tendency of title and first sentence to be
almost identical (cf. the large weight for the overlap predictor).
5 Experiment 2: Analysis by Domain of German corpora
5.1 Data
This experiment compares three German corpora from different newspapers to study the impact of domain
differences: Reuters, ?Stuttgarter Zeitung?, and ?Die Zeit?. These corpora differ in domain and in style.
The Reuters corpus was already described in Section 4.1. ?Stuttgarter Zeitung? (StuttZ) is a daily regional
newspaper which covers international business and politics like Reuters, but does not draw its material
completely from large news agencies and gives more importance to regional and local events. Its style is
therefore less consistent. Our corpus covers some 80,000 sentences of text from StuttZ. The third corpus
comprises over 4 million sentences of text from ?Die Zeit?, a major German national weekly. The text is
predominantly from the 2000s, plus selected articles from the 1940s through 1990s. ?Die Zeit? focuses on
op-ed pieces and general discussions of political and social issues. It also covers arts and science, which
the two other newspapers rarely do.
5.2 Distribution of annotation categories
We extracted and annotated entailment pair candidates in the same manner as before (cf. Section 4.1).
The new breakdown of annotation categories in Table (10) shows, in comparison to the cross-lingual
results in Table 2, a higher incidence of errors, which we attribute to formatting problems of these corpora.
Compared to the German Reuters corpus we considered in Exp. 1, StuttZ and Die Zeit contain considerably
fewer entailment pairs, most notably Die Zeit, where the percentage of entailment pairs is just 21.6% in
our sample, compared to 82.3% for Reuters. Notably, there are almost no cases where the first sentence
represents a partial entailment; in contrast, for more than one third of the examples (33.9%), there is no
entailment relation between the title and the first sentence. This seems to be a domain-dependent, or even
stylistic, effect: in ?Die Zeit?, titles are often designed solely as ?bait? to interest readers in the article:
(10) P: Sat.1
Sat.1
sah
watched
[...]
[...]
Doris
Doris
dabei zu ,
,
wie
how
sie
she
[...]
[...]
Auto fahren
to drive
lernte.
learned.
9The English model outperforms the Hindi model at higher recall levels, though.
101
Corpus err ill no-con no-emb no-oth no-par no-pre yes
Reuters 3.5 2.9 0 0.2 3.7 7.4 0 82.3
StuttZ 6.2 3.6 0.5 2.8 12.4 3.0 0.6 70.7
Die Zeit 2.3 39.0 0.5 1.8 33.9 0.9 0.0 21.6
Table 5: Exp. 2: Distribution of annotation categories on German corpora (in percent)
Predictor Reuters sig StuttZ sig Die Zeit sig
weighted overlap -0.77 ** -1.82 *** -2.60 ***
log num words -0.05 ? -0.24 ? -0.20 ?
punctuation -1.04 *** -0.01 ? -1.21 ***
strict noun match -0.12 ? -0.20 ? -0.01 ?
Table 6: Exp. 2: Predictors in the logreg model (*: p<0.05; **: p<0.01; ***: p<0.001)
PPPPPPPP
Data
Models Reuters StuttZ Die Zeit
Reuters 91.6 85.4 91.6
StuttZ 83.0 83.0 82.6
Die Zeit 45.2 45.2 46.7
Table 7: Exp. 2: Precision for the class ?yes? at 30% recall
H: Doris,
Doris,
es
it
ist
is
gr?n!
green!
Other titles are just noun or verb phrases, which accounts for the large number (39%) of ill-formed pairs.
5.3 Modelling the data
Predictors and evaluation. The predictors of the logistic regression models for the three German
corpora are shown in Table 6. The picture is strikingly similar to the results of Exp. 1 (Table 3): weighted
overlap and punctuation are highly significant predictors for all three corpora (except punctuation, which
is insignificant for StuttZ); even the effect sizes are roughly similar. Again, neither sentence length
nor strict noun match are significant. This indicates that the predictors we have identified work fairly
robustly. Unfortunately, this does not imply that they always work well. Table 6 shows the precision of
the predictors in Exp. 2, again at 30% Recall. Here, the difference to Exp. 1 (Table 4.3) is striking. First,
overfitting of the predictors is worse across domains, with losses of 5% on Reuters and Die Zeit when they
are classified with models trained on other corpora even though use just two generic features. Second, and
more seriously, it is much more difficult to extract entailment pairs from the Stuttgarter Zeitung corpus
and, especially, the Die Zeit corpus. For the latter, we can obtain a precision of at most 46.7%, compared
to >90% in Exp. 1.
We interpret this result as evidence that domain adaptation may be an even greater challenge than
multilinguality in the acquisition of entailment pairs. More specifically, our impression is that the heuristic
of pairing title and first sentence works fairly well for a particular segment of newswire text, but not
otherwise. This segment consists of factual, ?no-nonsense? articles provided by large news agencies such
as Reuters, which tend to be simple in their discourse structure and have an informative title. In domains
where articles become longer, and the intent to entertain becomes more pertinent (as for Die Zeit), the
heuristic fails very frequently. Note that the weighted overlap predictor cannot recover all negative cases.
Example (10) is a case in point: one of the two informative words in H, ?Doris? and ?gr?n?, is in fact in P.
Domain specificity. The fact that it is difficult to extract entailment pairs from some corpora is serious
exactly because, according to our intuition, the ?easier? news agency corpora (like Reuters) are domain-
102
Corpus D( ? | deWac) words w with highest P (w)/Q(w)
Reuters 0.98 H?ndler (trader), B?rse (exchange), Prozent (per cent), erkl?rte (stated)
StuttZ 0.93 DM (German Mark), Prozent (per cent), Millionen (millions), Gesch?fts-
jahr (fiscal year), Milliarden (billions)
Die Zeit 0.64 hei?t (means), wei? (knows), l??t (leaves/lets)
Table 8: Exp. 2: Domain specificity (KL distance from deWac); typical content words
specific. We quantify this intuition with an approach by Ciaramita and Baroni (2006), who propose
to model the representativeness of web-crawled corpora as the KL divergence between their Laplace-
smoothed unigram distribution P and that of a reference corpus, Q (w ? W are vocabulary words):
D(P,Q) =
?
w?W
P (w) log P (w)Q(w) (4)
We use the deWac German web corpus (Baroni et al, 2009) as reference, making the idealizing assumption
that it is representative for the German language. We interpret a large distance from deWac as domain
specificity. The results in Table 8 bear out our hypothesis: Die Zeit is less domain specific than StuttZ,
which in turn is less specific than Reuters. The table also lists the content words (nouns/verbs) that are
most typical for each corpus, i.e., which have the highest value of P (w)/Q(w). The lists bolster the
interpretation that Reuters and StuttZ concentrate on the economical domain, while the typical terms of
Die Zeit show an argumentative style, but no obvious domain bias. In sum, domain specificity is inversely
correlated with the difficulty of extracting entailment pairs: from a representativity standpoint, we should
draw entailment pairs from Die Zeit.
6 Conclusion
In this paper, we have discussed the robustness of extracting entailment pairs from the title and first
sentence of newspaper articles. We have proposed a logistic regression model and have analysed its
performance on two datasets that we have created: a cross-lingual one a cross-domain one. Our cross-
lingual experiment shows a positive result: despite differences in the distribution of annotation categories
across domains and languages, the predictors of all logistic regression models look remarkably similar. In
particular, we have found two predictors which are correlated significantly with entailment across (almost)
all languages and domains. These are (a), a tf/idf measure of word overlap between the title and the first
sentence; and (b), the presence of punctuation indicating that the title is not a single grammatical sentence.
These predictors extract entailment pairs from newswire text at a precision of > 90%, at a recall of 30%,
and represent a simple, cross-lingually robust method for entailment pair acquisition.
The cross-domain experiment, however, forces us to qualify this positive result. On two other German
corpora from different newspapers, we see a substantial degradation of the model?s performance. It may
seem surprising that cross-domain robustness is a larger problem than cross-lingual robustness. Our
interpretation is that the limiting factor is the degree to which the underlying assumption, namely that
first sentence entails the title, is true. If the assumption is true only for a minority of sentences, our
predictors cannot save the day. This assumption holds well in the Reuters corpora, but less so for the
other newspapers. Unfortunately, we also found that the Reuters corpora are at the same time thematically
constrained, and therefore only of limited use for extracting a representative corpus of entailment pairs. A
second problem is that the addition of features we considered beyond the two mentioned above threatens
to degrade the classifier due to overfitting, at least across domains.
Given these limitation of the present headline-based approach, other approaches that are more
generally applicable may need to be explored. Entailment pairs have for example been extracted from
Wikipedia (Bos et al, 2009). Another direction is to build on methods to extract paraphrases from
comparable corpora (Barzilay and Lee, 2003), and extend them to capture asymmetrical pairs, where
entailment holds in one, but not the other, direction.
103
Acknowledgments. The first author would like to acknowledge the support of a WISE scholarship
granted by DAAD (German Academic Exchange Service).
References
Baroni, M., S. Bernardini, A. Ferraresi, and E. Zanchetta (2009). The wacky wide web: A collection
of very large linguistically processed web-crawled corpora. Journal of Language Resources and
Evaluation 43(3), 209?226.
Barzilay, R. and L. Lee (2003). Learning to paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL, Edmonton, AL, pp. 16?23.
Bos, J., M. Pennacchiotti, and F. M. Zanzotto (2009). Textual entailment at EVALITA 2009. In
Proceedings of IAAI, Reggio Emilia.
Bresnan, J., A. Cueni, T. Nikitina, and H. Baayen (2007). Predicting the dative alternation. In G. Bouma,
I. Kraemer, and J. Zwarts (Eds.), Cognitive Foundations of Interpretation, pp. 69?94. Royal Netherlands
Academy of Science.
Burger, J. and L. Ferro (2005). Generating an entailment corpus from news headlines. In Proceedings of
the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pp. 49?54.
Ciaramita, M. and M. Baroni (2006). A figure of merit for the evaluation of web-corpus randomness. In
Proceedings of EACL, Trento, Italy, pp. 217?224.
Dagan, I., O. Glickman, and B. Magnini (2006). The PASCAL recognising textual entailment challenge.
In Machine Learning Challenges, Volume 3944 of Lecture Notes in Computer Science, pp. 177?190.
Springer.
de Marneffe, M.-C., A. N. Rafferty, and C. D. Manning (2008). Finding contradictions in text. In
Proceedings of the ACL, Columbus, Ohio, pp. 1039?1047.
Hickl, A., J. Williams, J. Bensley, K. Roberts, B. Rink, and Y. Shi (2006). Recognizing textual entailment
with LCC?s Groundhog system. In Proceedings of the Second PASCAL Challenges Workshop.
Manning, C. D., P. Raghavan, and H. Sch?tze (2008). Introduction to Information Retrieval (1st ed.).
Cambridge University Press.
Monz, C. and M. de Rijke (2001). Light-weight entailment checking for computational semantics. In
Proceedings of ICoS, Siena, Italy, pp. 59?72.
Sammons, M., V. Vydiswaran, and D. Roth (2010). ?Ask Not What Textual Entailment Can Do for You...?.
In Proceedings of ACL, Uppsala, Sweden, pp. 1199?1208.
Schmid, H. (1995). Improvements in part-of-speech tagging with an application to german. In Proceedings
of the SIGDAT Workshop at ACL, Cambridge, MA.
Xiao, Z., T. McEnery, P. Baker, and A. Hardie (2004). Developing Asian language corpora: Standards
and practice. In In Proceedings of the Fourth Workshop on Asian Language Resources, Sanya, China,
pp. 1?8.
104
Ontology-based Distinction between Polysemy and Homonymy
Jason Utt
Institut fu?r Maschinelle Sprachverarbeitung
Universita?t Stuttgart
uttjn@ims.uni-stuttgart.de
Sebastian Pado?
Seminar fu?r Computerlinguistik
Universita?t Heidelberg
pado@cl.uni-heidelberg.de
Abstract
We consider the problem of distinguishing polysemous from homonymous nouns. This distinction
is often taken for granted, but is seldom operationalized in the shape of an empirical model. We
present a first step towards such a model, based on WordNet augmented with ontological classes
provided by CoreLex. This model provides a polysemy index for each noun which (a), accurately
distinguishes between polysemy and homonymy; (b), supports the analysis that polysemy can be
grounded in the frequency of the meaning shifts shown by nouns; and (c), improves a regression
model that predicts when the ?one-sense-per-discourse? hypothesis fails.
1 Introduction
Linguistic studies of word meaning generally divide ambiguity into homonymy and polysemy. Homony-
mous words exhibit idiosyncratic variation, with essentially unrelated senses, e.g. bank as FINANCIAL
INSTITUTION versus as NATURAL OBJECT. In polysemy, meanwhile, sense variation is systematic,
i.e., appears for whole sets of words. E.g., lamb, chicken and salmon have ANIMAL and FOOD senses.
It is exactly this systematicity that represents a challenge for lexical semantics. While homonymy is
assumed to be encoded in the lexicon for each lemma, there is a substantial body of work on dealing with
general polysemy patterns (cf. Nunberg and Zaenen (1992); Copestake and Briscoe (1995); Pustejovsky
(1995); Nunberg (1995)). This work is predominantly theoretical in nature. Examples of questions
addressed are the conditions under which polysemy arises, the representation of polysemy in the semantic
lexicon, disambiguation mechanisms in the syntax-semantics interface, and subcategories of polysemy.
The distinction between polysemy and homonymy also has important potential ramifications for
computational linguistics, in particular for Word Sense Disambiguation (WSD). Notably, Ide and Wilks
(2006) argue that WSD should focus on modeling homonymous sense distinctions, which are easy to
make and provide most benefit. Another case in point is the one-sense-per-discourse hypothesis (Gale
et al, 1992), which claims that within a discourse, instances of a word will strongly tend towards realizing
the same sense. This hypothesis seems to apply primarily to homonyms, as pointed out by Krovetz (1998).
Unfortunately, the distinction between polysemy and homonymy is still very much an unsolved
question. The discussion in the theoretical literature focuses mostly on clear-cut examples and avoids
the broader issue. Work on WSD, and in computational linguistics more generally, almost exclusively
builds on the WordNet (Fellbaum, 1998) word sense inventory, which lists an unstructured set of senses
for each word and does not indicate in which way these senses are semantically related. Diachronic
linguistics proposes etymological criteria; however, these are neither undisputed nor easy to operationalize.
Consequently, there are currently no broad-coverage lexicons that indicate the polysemy status of words,
nor even, to our knowledge, precise, automatizable criteria.
Our goal in this paper is to take a first step towards an automatic polysemy classification. Our approach
is based on the aforementioned intuition that meaning variation is systematic in polysemy, but not in
homonymy. This approach is described in Section 2. We assess systematicity by mapping WordNet senses
onto basic types, a set of 39 ontological categories defined by the CoreLex resource (Buitelaar, 1998),
and looking at the prevalence of pairs of basic types (such as {FINANCIAL INSTITUTION, NATURAL
265
OBJECT} above) across the lexicon. We evaluate this model on two tasks. In Section 3, we apply the
measure to the classification of a set of typical polysemy and homonymy lemmas, mostly drawn from the
literature. In Section 4, we apply it to the one-sense-per-discourse hypothesis and show that polysemous
words tend to violate this hypothesis more than homonyms. Section 5 concludes.
2 Modeling Polysemy
Our goal is to take the first steps towards an empirical model of polysemy, that is, a computational model
which makes predictions for ? in principle ? arbitrary words on the basis of their semantic behavior.
The basis of our approach mirrors the focus of much linguistic work on polysemy, namely the fact
that polysemy is systematic: There is a whole set of words which show the same variation between two
(or more) ontological categories, cf. the ?universal grinder? (Copestake and Briscoe, 1995). There are
different ways of grounding this notion of systematicity empirically. An obvious choice would be to use a
corpus. However, this would introduce a number of problems. First, while corpora provide frequency
information, the role of frequency with respect to systematicity is unclear: should acceptable but rare
senses play a role, or not? We side with the theoretical literature in assuming that they do. Another
problem with corpora is the actual observation of sense variation. Few sense-tagged corpora exist, and
those that do are typically small. Interpreting context variation in untagged corpora, on the other hand,
corresponds to unsupervised WSD, a serious research problem in itself ? see, e.g., Navigli (2009).
We therefore decided to adopt a knowledge-based approach that uses the structure of the WordNet
ontology to calculate how systematically the senses of a word vary. The resulting model sets all senses of
a word on equal footing. It is thus vulnerable to shortcomings in the architecture of WordNet, but this
danger is alleviated in practice by our use of a ?coarsened? version of WordNet (see below).
2.1 WordNet, CoreLex and Basic Types
WordNet provides only a flat list of senses for each word. This list does not indicate the nature of the
sense variation among the senses. However, building on the generative lexicon theory by Pustejovsky
(1995), Buitelaar (1998) has developed the ?CoreLex? resource. It defines a set of 39 so-called basic
types which correspond to coarse-grained ontological categories. Each basic type is linked to one or more
WordNet anchor nodes, which define a complete mapping between WordNet synsets and basic types by
dominance.1 Table 1 shows the set of basic types and their main anchors; Table 2 shows example lemmas
for some basic types.
Ambiguous lemmas are often associated with two or more basic types. CoreLex therefore further
assigns each lemma to what Buitelaar calls a polysemy class, the set of all basic types its synsets belong to;
a class with multiple representatives is considered systematic. These classes subsume both idiosyncratic
and systematic patterns, and thus, despite their name, provide no clue about the nature of the ambiguity.
CoreLex makes it possible to represent the meaning of a lemma not through a set of synsets, but instead
in terms of a set of basic types. This constitutes an important step forward. Our working hypothesis is that
these basic types approximate the ontological categories that are used in the literature on polysemy to
define polysemy patterns. That is, we can define a meaning shift to mean that a lemma possesses one sense
in one basic type, while another sense belongs to another basic type. Naturally, this correspondence is not
perfect: systematic polysemy did not play a role in the design of the WordNet ontology. Nevertheless,
there is a fairly good approximation that allows us to recover many prominent polysemy patterns. Table 3
shows three polysemy patterns characterized in terms of basic types. The first class was already mentioned
before. The second class contains a subset of ?transparent nouns? which can denote a container or a
quantity. The last class contains words which describe a place or a group of people.
1Note that not all of CoreLex anchor nodes are disjoint; therefore a given WordNet synset may be dominated by two CoreLex
anchor nodes. We assign each synset to the basic type corresponding to the most specific dominating anchor node.
266
BT WordNet anchor BT WordNet anchor BT WordNet anchor
abs ABSTRACTION loc LOCATION pho PHYSICAL OBJECT
act ACTION log GEOGRAPHICAL AREA plt PLANT
agt AGENT mea MEASURE pos POSSESSION
anm ANIMAL mic MICROORGANISM pro PROCESS
art ARTIFACT nat NATURAL OBJECT prt PART
atr ATTRIBUTE phm PHENOMENON psy PSYCHOLOGICAL FEATURE
cel CELL frm FORM qud DEFINITE QUANTITY
chm CHEMICAL ELEMENT grb BIOLOGICAL GROUP qui INDEFINITE QUANTITY
com COMMUNICATION grp GROUP rel RELATION
con CONSEQUENCE grs SOCIAL GROUP spc SPACE
ent ENTITY hum PERSON sta STATE
evt EVENT lfr LIVING THING sub SUBSTANCE
fod FOOD lme LINEAR MEASURE tme TIME
Table 1: The 39 CoreLex basic types (BTs) and their WordNet anchor nodes
Basic type WordNet anchor Examples
agt AGENT driver, menace, power, proxy, . . .
grs SOCIAL GROUP city, government, people, state, . . .
pho PHENOMENON life, pressure, trade, work, . . .
pos POSSESSION figure, land, money, right, . . .
qui INDEFINITE QUANTITY bit, glass, lot, step, . . .
rel RELATION function, part, position, series, . . .
Table 2: Basic types with example words
Pattern (Basic types) Examples
ANIMAL, FOOD fowl, hare, lobster, octopus, snail, . . .
ARTIFACT, INDEFINITE QUANTITY bottle, jug, keg, spoon, tub, . . .
ARTIFACT, SOCIAL GROUP academy, embassy, headquarters, . . .
Table 3: Examples of polysemous meaning variation patterns
2.2 Polysemy as Systematicity
Given the intuitions developed in the previous section, we define a basic ambiguity as a pair of basic
types, both of which are associated with a given lemma. The variation spectrum of a word is then the set
of all its basic ambiguities. For example, bottle would have the variation spectrum {{art qui} } (cf.
Table 3); the word course with the three basic types act, art, grs would have the variation spectrum
{{act art}; {act grs}; {art grs} }.
There are 39 basic types and thus 39 ? 38/2 = 741 possible basic ambiguities. In practice, only 663
basic ambiguities are attested in WordNet. We can quantify each basic ambiguity by the number of words
that exhibit it. For the moment, we simply interpret frequency as systematicity.2 Thus, we interpret the
high-frequency (systematic) basic ambiguities as polysemous, and low-frequency (idiosyncratic) basic
ambiguities as homonymous. Table 4 shows the most frequent basic ambiguities, all of which apply to
several hundred lemmas and can safely be interpreted as polysemous. At the other end, 56 of the 663
basic ambiguities are singletons, i.e. are attested by only a single lemma.
In a second step, we extend this classification from basic ambiguities to lemmas. The intuition is again
fairly straightforward: A word whose basic ambiguities are systematic will be perceived as polysemous,
and as homonymous otherwise. This is clearly an oversimplification, both practically, since we depend
on WordNet/CoreLex having made the correct design decisions in defining the ontology and the basic
types; as well as conceptually, since not all polysemy patterns will presumably show the same degree of
systematicity. Nevertheless, we believe that basic types provide an informative level of abstraction, and
that our model is in principle even able to account for conventionalized metaphor, to the extent that the
corresponding senses are encoded in WordNet.
2Note that this is strictly a type-based notion of frequency: corpus (token) frequencies do not enter into our model.
267
Basic ambiguity Examples
{act com} construction, consultation, draft, estimation, refusal, . . .
{act art} press, review, staging, tackle, . . .
{com hum} egyptian, esquimau, kazakh, mojave, thai, . . .
{act sta} domination, excitement, failure, marriage, matrimony, . . .
{art hum} dip, driver, mouth, pawn, watch, wing, . . .
Table 4: Top five basic ambiguities with example lemmas
Noun Basic types Noun Basic types
chicken anm fod evt hum lamb anm fod hum
salmon anm fod atr nat duck anm fod art qud
Table 5: Words exhibiting the ?grinding? (animal ? food) pattern
The exact manner in which the systematicity of the individual basic ambiguities of one lemma are
combined is not a priori clear. We have chosen the following method. Let P be a basic ambiguity, P(w)
the variation spectrum of a lemma w, and freq(P ) the number of WordNet lemmas with basic ambiguity P .
We define the set of polysemous basic ambiguities PN as the N -most frequent bins of basic ambiguities:
PN = {[P1], ..., [PN ]}, where [Pi] = {Pj | freq(Pi) = freq(Pj)} and freq(Pk) > freq(Pl) for k < l.
We call non-polysemous basic ambiguities idiosyncratic. The polysemy index of a lemma w, piN (w), is:
piN (w) =
| PN ?P(w)|
| P(w)| (1)
piN simply measures the ratio of w?s basic ambiguities which are polysemous, i.e., high-frequency basic
ambiguities. piN ranges between 0 and 1, and can be interpreted analogously to the intuition that we
have developed on the level of basic ambiguities: high values of pi (close to 1) mean that the majority
of a lemma?s basic ambiguities are polysemous, and therefore the lemma is perceived as polysemous.
In contrast, low values of pi (close to 0) mean that the lemma?s basic ambiguities are predominantly
idiosyncratic, and thus the lemma counts as homonymous. Again, note that we consider basic ambiguities
at the type level, and that corpus frequency does not enter into the model.
This model of polysemy relies crucially on the distinction between systematic and idiosyncratic basic
ambiguities, and therefore in turn on the parameter N . N corresponds to the sharp cutoff that our model
assumes. At the N -th most frequent basic ambiguity, polysemy turns into homonymy. Since frequency
is our only criterion, we have to lump together all basic ambiguities with the same frequency into 135
bins. If we set N = 0, none of the bins count as polysemous, so pi0(w) = 0 for all w ? all lemmas are
homonymous. In the other extreme, we can set N to 135, the total number of frequency bins, which
makes all basic ambiguities polysemous, and thus all lemmas: pi135(w) = 1 for all w. The optimization
of N will be discussed in Section 3.
2.3 Gradience between Homonymy and Polysemy
We assign each lemma a polysemy index between 0 and 1. We thus abandon the dichotomy that is usually
made in the literature between two distinct categories of polysemy and homonymy. Instead, we consider
polysemy and homonymy the two end points on a gradient, where words in the middle show elements of
both. This type of behavior can be seen even for prototypical examples of either category, such as the
homonym bank, which shows a variation between SOCIAL GROUP and ARTIFACT:
(1) a. The bill would force banks [...] to report such property. (grs)
b. The coin bank was empty. (art)
Note that this is the same basic ambiguity that is often cited as a typical example of polysemous sense
variation, for example for words like newspaper.
On the other hand, many lemmas which are presumably polysemous show rather unsystematic basic
ambiguities. Table 5 shows four lemmas which are instances of the meaning variation between ANIMAL
268
Homonymous nouns ball, bank, board, chapter, china, degree, fall, fame, plane, plant, pole, post, present, rest,
score, sentence, spring, staff, stage, table, term, tie, tip, tongue
Polysemous nouns bottle, chicken, church, classification, construction, cup, development, fish, glass, improve-
ment, increase, instruction, judgment, lamb, management, newspaper, painting, paper, picture,
pool, school, state, story, university
Table 6: Experimental items for the two classes hom and poly
(anm) and FOOD (fod), a popular example of a regular and productive sense extension. Yet each of the
nouns exhibits additional basic types. The noun chicken also has the highly idiosyncratic meaning of a
person who lacks confidence. A lamb can mean a gullible person, salmon is the name of a color and a
river, and a duck a score in the game of cricket. There is thus an obvious unsystematic variety in the words?
sense variations ? a single word can show both homonymic as well as polysemous sense alternation.
3 Evaluating the Polysemy Model
To identify an optimal cutoff value N for our polysemy index, we use a simple supervised approach: we
optimize the quality with which our polysemy index models a small, manually created dataset. More
specifically, we created a two-class, 48-word dataset with 24 homonymous nouns (class hom) and 24
polysemous nouns (class poly) drawn from the literature. The dataset is shown in Table 6.
We now rank these items according to piN for different values of N and observe the ability of piN
to distinguish the two classes. We measure this ability with the Mann-Whitney U test, a nonparametric
counterpart of the t-test.3 In our case, the U statistic is defined as
U(N) =
m
?
i=1
n
?
j=1
1(piN (homi) < piN (polyi))
where 1 is the function function that returns the truth value of its argument (1 for ?true?). Informally,
U(N) counts the number of correctly ranked pairs of a homonymous and a polysemous noun.
The maximum for U is the number of item pairs from the classes (24 ?24 = 576). A score of U = 576
would mean that every piN -value of a homonym is smaller than every polysemous value. U = 0 means
that there are no homonyms with smaller pi-scores. So U can be directly interpreted as the quality of
separation between the two classes. The null hypothesis of this test is that the ranking is essentially
random, i.e., half the rankings are correct4. We can reject the null hypothesis if U is significantly larger.
Figure 1(a) shows the U -statistic for all values of N (between 0 and 135). The left end shows the
quality of separation (i.e. U ) for few basic ambiguities (i.e. small N ) which is very small. As soon as we
start considering the most frequent basic ambiguities as systematic and thus as evidence for polysemy,
hom and poly become much more distinct. We see a clear global maximum of U for N = 81 (U = 436.5).
This U value is highly significant at p < 0.005, which means that even on our fairly small dataset, we can
reject the null hypothesis that the ranking is random. pi81 indeed separates the classes with high confidence:
436.5 of 576 or roughly 75% of all pairwise rankings in the dataset are correct. For N > 81, performance
degrades again: apparently these settings include too many basic ambiguities in the ?systematic? category,
and homonymous words start to be misclassified as polysemous.
The separation between the two classes is visualized in the box-and-whiskers plot in Figure 1(b). We
find that more than 75% of the polysemous words have pi81 > .6. The median value for poly is 1, thus
for more than half of the class pi81 = 1, which can be seen in Figure 2(b) as well. This is a very positive
result, since our hope is that highly polysemous words get high scores. Figure 2(a) shows that homonyms
are concentrated in the mid-range while exhibiting a small number of pi81-values at both extremes.
We take the fact that there is indeed an N which clearly maximizes U as a very positive result that
validates our choice of introducing a sharp cutoff between polysemous and idiosyncratic basic ambiguities.
3The advantage of U over t is that t assumes comparable variance in the two samples, which we cannot guarantee.
4Provided that, like in this case, the classes are of equal size.
269
0 20 40 60 80 100 120
30
0
35
0
40
0
N
U
(a) The U statistic for different values of the cutoff N
l
l
l
l
hom poly
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
(b) Distribution of pi81 values by class
Figure 1: Separation of the hom and poly classes in our dataset
These 81 frequency bins contain roughly 20% of the most frequent basic ambiguities. This corresponds to
the assumption that basic ambiguities are polysemous if they occur with a minimum of about 50 lemmas.
If we look more closely at those polysemous words that obtain low scores (school, glass and cup),
we observe that they also show idiosyncratic variation as discussed in Section 2.3. In the case of school,
we have the senses schooltime of type tme and group of fish of type grb which one would not expect to
alternate regularly with grs and art, the rest of its variation spectrum. The word glass has the unusual
type agt due to its use as a slang term for crystal methamphetamine. Finally, cup is unique in that means
both an indefinite quantity as well as the definite measurement equal to half a pint. Only 10 other words
have this variation in WordNet, including such words as million and billion, which are often used to
describe an indefinite but large number.
On the other hand, those homonyms that have a high score (e.g. tie, staff and china) have somewhat
unexpected regularities due to obscure senses. Both tie and staff are terms used in musical notation. This
leads to basic ambiguities with the com type, something that is very common. Finally, the obviously
unrelated senses for china, China and porcelain, are less idiosyncratic when abstracted to their types, log
and art, respectively. There are 117 words that can mean a location as well as an artifact, (e.g. fireguard,
bath, resort, front, . . . ) which are clearly polysemous in that the location is where the artifact is located.
In conclusion, those examples which are most grossly miscategorized by pi81 contain unexpected
sense variations, a number of which have been ignored in previous studies.
ball
bank
board
chapter
china
degree
fall
game
plane
plant
pole
post
present rest
score
sentence
spring
staffstage
table
term
tie
tip
tongue
0 1
(a) Class hom
classification
chicken bottle constructioncup
development
fish
glass
improvement
increase
instruction
judgment
lamb management
newspaper
painting
paper
picturepool
school
state
story
university
church
0 1
(b) Class poly
Figure 2: Words and their pi81-scores
270
4 The One-Sense-Per-Discourse Hypothesis
The second evaluation that we propose for our polysemy index concerns a broader question on word
sense, namely the so-called one-sense-per-discourse (1spd) hypothesis. This hypothesis was introduced
by Gale et al (1992) and claims that ?[...] if a word such as sentence appears two or more times in
a well-written discourse, it is extremely likely that they will all share the same sense?. The authors
verified their hypothesis on a small experiment with encouraging results (only 4% of discourses broke
the hypothesis). Indeed, if this hypothesis were unreservedly true, then it would represent a very strong
global constraint that could serve to improve word sense disambiguation ? and in fact, a follow-up paper
by Yarowsky (1995) exploited the hypothesis for this benefit.
Unfortunately, it seems that 1spd does not apply universally. At the time (1992), WordNet had
not yet emerged as a widely used sense inventory, and the sense labels used by Gale et al were fairly
coarse-grained ones, motivated by translation pairs (e.g., English duty translated as French droit (tax)
vs. devoir (obligation)), which correspond mostly to homonymous sense distinctions.5 Current WSD, in
contrast, uses the much more fine-grained WordNet sense inventory which conflates homonymous and
polysemous sense distinctions. Now, 1spd seems intuitively plausible for homonyms, where the senses
describe different entities that are unlikely to occur in the same discourse (or if they do, different words
will be used). However, the situation is different for polysemous words: In a discourse about a party, bottle
might felicitously occur both as an object and a measure word. A study by Krovetz (1998) confirmed this
intuition on two sense-tagged corpora, where he found 33% of discourses to break 1spd. He suggests that
knowledge about polysemy classes can be useful as global biases for WSD.
In this section, we analyze the sense-tagged SemCor corpus in terms of the basic type-based framework
of polysemy that we have developed in Section 2 both qualitatively and quantitatively to demonstrate that
basic types, and our polysemy index pi, help us better understand the 1spd hypothesis.
4.1 Analysis by Basic Types and One-Basic-Type-Per-Discourse
The first step in our analysis looks specifically at the basic types and basic ambiguities we observe in
discourses that break 1spd. Our study reanalyses SemCor, a subset of the Brown corpus annotated ex-
haustively with WordNet senses (Fellbaum, 1998). SemCor contains a total of 186 discourses, paragraphs
of between 645 and 1023 words. These 186 discourses, in combination with 1088 nouns, give rise to
7520 lemma-discourse pairs, that is, cases where a sense-tagged lemma occurs more than once within a
discourse.6 These 7520 lemma-discourse pairs form the basis of our analysis. We started by looking at
the relative frequency of 1spd. We found that the hypothesis holds for 69% of the lemma-discourse pairs,
but not for the remaining 31%. This is a good match with Krovetz? findings, and indicates that there are
many discourses where there lemmas are used in different senses.
In accordance with our approach to modeling meaning variation at the level of basic types, we
implemented a ?coarsened? version of 1spd, namely one-basic-type-per-discourse (1btpd). This hypothesis
is parallel to the original, claiming that it is extremely likely that all words in a discourse share the
same basic type. As we have argued before, the basic-type level is a fairly good approximation to the
most important ontological categories, while smoothing over some of the most fine-grained (and most
troublesome) sense distinctions in WordNet. In this vein, 1btpd should get rid of ?spurious? ambiguity,
but preserve meaningful ambiguity, be it homonymous or polysemous. In fact, the basic type with most
of these ?within-basic-type? ambiguities is PSYCHOLOGICAL FEATURE, which contains many subtle
distinctions such as the following senses of perception:
a. a way of conceiving something b. the process of perceiving
c. knowledge gained by perceiving d. becoming aware of something via the senses
Such distinctions are collapsed in 1btpd. In consequence, we expect a noticeable, but limited, reduction in
5Note that Gale et al use the term ?polysemy? synonymously with ?ambiguous?.
6We exclude cases where a lemma occurs once in a discourse, since 1spd holds trivially.
271
Basic ambiguity most common breaking words freq(P breaks 1btpd) freq(P ) N
{com psy} evidence, sense, literature, meaning, style, . . . 89 365 13
{act psy} study, education, pattern, attention, process, . . . 88 588 7
{psy sta} need, feeling, difficulty, hope, fact, . . . 79 338 14
{act atr} role, look, influence, assistance, interest, . . . 79 491 9
{act art} church, way, case, thing, design, . . . 67 753 2
{act sta} operation, interest, trouble, employment, absence, . . . 60 615 4
{act com} thing, art, production, music, literature, . . . 59 755 1
{atr sta} life, level, desire, area, unity, . . . 58 594 6
Table 7: Most frequent basic ambiguities that break the 1btpd hypothesis in SemCor
the cases that break the hypothesis. Indeed, 1btpd holds for 76% of all lemma-discourse pairs, i.e., for 7%
more than 1spd. For the remainder of this analysis, we will test the 1btpd hypothesis instead of 1spd.
The basic type level also provides a good basis to analyze the lemma-discourse pairs where the
hypothesis breaks down. Table 7 shows the basic ambiguities that break the hypothesis in SemCor most
often. The WordNet frequencies are high throughout, which means that these basic ambiguities are poly-
semous according to our framework. It is noticeable that the two basic types PSYCHOLOGICAL FEATURE
and ACTION participate in almost all of these basic ambiguities. This observation can be explained
straightforwardly through polysemous sense extension as sketched above: Actions are associated, among
other things, with attributes, states, and communications, and discussion of an action in a discourse can
fairly effortlessly switch to these other basic types. A very similar situation applies to psychological
features, which are also associated with many of the other categories. In sum, we find that the data bears
out our hypothesis: almost all of the most frequent cases of several-basic-types-per-discourse clearly
correspond to basic ambiguities that we have classified as polysemous rather than homonymous.
4.2 Analysis by Regression Modeling
This section complements the qualitative analysis of the previous section with a quantitative analysis
which predicts specifically for which lemma-discourse pairs 1btpd breaks down. To do so, we fit a logit
mixed effects model (Breslow and Clayton, 1993) to the SemCor data. Logit mixed effects models can
be seen as a generalization of logistic regression models. They explain a binary response variable y in
terms of a set of fixed effects x, but also include a set of random effects x?. Fixed effects correspond to
?ordinary? predictors as in traditional logistic regression, while random effects account for correlations in
the data introduced by groups (such as items or subjects) without ascribing these random effects the same
causal power as fixed effects ? see, e.g., Jaeger (2008) for details.
The contribution of each factor is modelled by a coefficient ?, and their sum is interpreted as the
logit-transformed probability of a positive outcome for the response variable:
p(y = 1) = 11 + e?z with z =
?
?ixi +
?
??jx?j (2)
Model estimation is usually performed using numeric approximations. The coefficients ?? of the random
effects are drawn from a multivariate normal distribution, centered around 0, which ensures that the
majority of random effects are ascribed very small coefficients.
From a linguistic perspective, a desirable property of regression models is that they describe the
importance of the different effects. First of all, each coefficient can be tested for significant difference
to zero, which indicates whether the corresponding effect contributes significantly to modeling the data.
Furthermore, the absolute value of each ?i can be interpreted as the log odds ? that is, as the (logarithmized)
change in the probability of the response variable being positive depending on xi being positive.
In our experiment, each datapoint corresponds to one of the 7520 lemma-discourse pair from SemCor
(cf. Section 4.1). The response variable is binary: whether 1btpd holds for the lemma-discourse pair or
not. We include in the model five predictors which we expect to affect the response variable: three fixed
effects and two random ones. The first fixed effect is the ambiguity of the lemma as measured by the
272
Predictor Coefficient Odds (95% confidence interval) Significance
Number of basic types -0.50 0.61 (0.59?0.63) ***
Log length of discourse (words) -0.60 1.83 (1.14?2.93) ?
Polysemy index (pi81) -0.91 0.40 (0.35?0.46) ***
Table 8: Logit mixed effects model for the response variable ?one-basic-type-per-discourse (1btpd) holds?
(SemCor; random effects: discourse and lemma; significances: ?: p > 0.05; ***: p < 0.001)
number of its basic types, i.e. the size of its variation spectrum. We expect that the more ambiguous a
noun, the smaller the chance for 1btpd. We expect the same effect for the (logarithmized) length of the
discourse in words: longer discourses run a higher risk for violating the hypothesis. Our third fixed effect
is the polysemy index pi81, for which we also expect a negative effect. The two random effects are the
identity of the discourse and the noun. Both of these can influence the outcome, but should not be used as
full explanatory variables.
We build the model in the R statistical environment, using the lme47 package. The main results are
shown in Table 8. We find that the number of basic types has a highly significant negative effect on the
1btpd hypothesis (p < 0.001) . Each additional basic type lowers the odds for the hypothesis by a factor
of e?0.50 ? 0.61. The confidence interval is small; the effect is very consistent. This was to be expected ?
it would have been highly suspicious if we had not found this basic frequency effect. Our expectations are
not met for the discourse length predictor, though. We expected a negative coefficient, but find a positive
one. The size of the confidence interval shows the effect to be insignificant. Thus, we have to assume that
there is no significant relationship between the length of the discourse and the 1btpd hypothesis. Note
that this outcome might result from the limited variation of discourse lengths in SemCor: recall that no
discourse contains less than 645 or more than 1023 words.
However, we find a second highly significant negative effect (p < 0.001) in our polysemy index pi81.
With a coefficient of -0.91, this means that a word with a polysemy index of 1 is only 40% as likely
to preserve 1btpd than a word with a polysemy index of 0. The confidence interval is larger than for
the number of basic types, but still fairly small. To bolster this finding, we estimated a second mixed
effects model which was identical to the first one but did not contain pi81 as predictor. We tested the
difference between the models with a likelihood ratio test and found that the model that includes pi81 is
highly preferred (p < 0.0001;D = ?2?LL = 40; df = 1).
These findings establish that our polysemy index pi can indeed serve a purpose beyond the direct
modeling of polysemy vs. homonymy, namely to explain the distribution of word senses in discourse
better than obvious predictors like the overall ambiguity of the word and the length of the discourse can.
This further validates the polysemy index as a contribution to the study of the behavior of word senses.
5 Conclusion
In this paper, we have approached the problem of distinguishing empirically two different kinds of
word sense ambiguity, namely homonymy and polysemy. To avoid sparse data problems inherent in
corpus work on sense distributions, our framework is based on WordNet, augmented with the ontological
categories provided by the CoreLex lexicon. We first classify the basic ambiguities (i.e., the pairs of
ontological categories) shown by a lemma as either polysemous or homonymous, and then assign the ratio
of polysemous basic ambiguities to each word as its polysemy index.
We have evaluated this framework on two tasks. The first was distinguishing polysemous from
homonymous lemmas on the basis of their polysemy index, where it gets 76% of all pairwise rankings
correct. We also used this task to identify an optimal value for the threshold between polysemous and
homonymous basic ambiguities. We located it at around 20% of all basic ambiguities (113 of 663 in
the top 81 frequency bins), which apparently corresponds to human intuitions. The second task was
an analysis of the one-sense-per-discourse heuristic, which showed that this hypothesis breaks down
7http://cran.r-project.org/web/packages/lme4/index.html
273
frequently in the face of polysemy, and that the polysemy index can be used within a regression model to
predict the instances within a discourse where this happens.
It may seem strange that our continuous index assumes a gradient between homonymy and polysemy.
Our analyses indicate that on the level of actual examples, the two classes are indeed not separated by a
clear boundary: many words contain basic ambiguities of either type. Nevertheless, even in the linguistic
literature, words are often considered as either polysemous or homonymous. Our interpretation of this
contradiction is that some basic types (or some basic ambiguities) are more prominent than others. The
present study has ignored this level, modeling the polysemy index simply on the ratio of polysemous
patterns without any weighting. In future work, we will investigate human judgments of polysemy vs.
homonymy more closely, and assess other correlates of these judgments (e.g., corpus counts).
A second area of future work is more practical. The logistic regression incorporating our polysemous
index predicts, for each lemma-discourse pair, the probability that the one-sense-per-discourse hypothesis
is violated. We will use this information as a global prior on an ?all-words? WSD task, where all
occurrences of a word in a discourse need to be disambiguated. Finally, Stokoe (2005) demonstrates
the chances for improvement in information retrieval systems if we can reliably distinguish between
homonymous and polysemous senses of a word.
References
Breslow, N. and D. Clayton (1993). Approximate inference in generalized linear mixed models. Journal
of the American Statistical Society 88(421), 9?25.
Buitelaar, P. (1998). CoreLex: An ontology of systematic polysemous classes. In Proceedings of FOIS,
Amsterdam, Netherlands, pp. 221?235.
Copestake, A. and T. Briscoe (1995). Semi-productive polysemy and sense extension. Journal of
Semantics 12, 15?67.
Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. MIT Press.
Gale, W. A., K. W. Church, and D. Yarowsky (1992). One sense per discourse. In Proceedings of HLT,
Harriman, NY, pp. 233?237.
Ide, N. and Y. Wilks (2006). Making sense about sense. In E. Agirre and P. Edmonds (Eds.), Word Sense
Disambiguation: Algorithms and Applications, pp. 47?74. Springer.
Jaeger, T. (2008). Categorical data analysis: Away from ANOVAs and toward Logit Mixed Models.
Journal of Memory and Language 59(4), 434?446.
Krovetz, R. (1998). More than one sense per discourse. In Proceedings of SENSEVAL, Herstmonceux
Castle, England.
Navigli, R. (2009). Word Sense Disambiguation: a survey. ACM Computing Surveys 41(2), 1?69.
Nunberg, G. (1995). Transfers of meaning. Journal of Semantics 12(2), 109?132.
Nunberg, G. and A. Zaenen (1992). Systematic polysemy in lexicology and lexicography. In Proceedings
of Euralex II, Tampere, Finland, pp. 387?395.
Pustejovsky, J. (1995). The Generative Lexicon. Cambridge MA: MIT Press.
Stokoe, C. (2005). Differentiating homonymy and polysemy in information retrieval. In Proceedings of
the conference on Human Language Technology and Empirical Methods in NLP, Morristown, NJ, pp.
403?410.
Yarowsky, D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. In Proceed-
ings of ACL, Cambridge, MA, pp. 189?196.
274
In: R. Levy & D. Reitter (Eds.), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 70?79,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Modeling covert event retrieval in logical metonymy:
probabilistic and distributional accounts
Alessandra Zarcone, Jason Utt
Institut f?r Maschinelle Sprachverarbeitung
Universit?t Stuttgart
{zarconaa,uttjn}@ims.uni-stuttgart.de
Sebastian Pad?
Institut f?r Computerlinguistik
Universit?t Heidelberg
pado@cl.uni-heidelberg.de
Abstract
Logical metonymies (The student finished
the beer) represent a challenge to composi-
tionality since they involve semantic content
not overtly realized in the sentence (covert
events ? drinking the beer). We present a
contrastive study of two classes of computa-
tional models for logical metonymy in German,
namely a probabilistic and a distributional,
similarity-based model. These are built using
the SDEWAC corpus and evaluated against a
dataset from a self-paced reading and a probe
recognition study for their sensitivity to the-
matic fit effects via their accuracy in predicting
the correct covert event in a metonymical con-
text. The similarity-based models allow for
better coverage while maintaining the accuracy
of the probabilistic models.
1 Introduction
Logical metonymies (The student finished the beer)
require the interpretation of a covert event which
is not overtly realized in the sentence (? drinking
the beer). Logical metonymy has received much
attention as it raises issues that are relevant to both
theoretical as well as cognitive accounts of language.
On the theoretical side, logical metonymies consti-
tute a challenge for theories of compositionality (Par-
tee et al, 1993; Baggio et al, in press) since their in-
terpretation requires additional, inferred information.
There are two main accounts of logical metonymy:
According to the lexical account, a type clash be-
tween an event-subcategorizing verb (finish) and an
entity-denoting object (beer) triggers the recovery of
a covert event from complex lexical entries, such as
qualia structures (Pustejovsky, 1995). The pragmatic
account of logical metonymy suggests that covert
events are retrieved through post-lexical inferences
triggered by our world knowledge and communica-
tion principles (Fodor and Lepore, 1998; Cartson,
2002; De Almeida and Dwivedi, 2008).
On the experimental side, logical metonymy leads
to higher processing costs (Pylkk?nen and McEl-
ree, 2006; Baggio et al, 2010). As to covert
event retrieval, it has been found that verbs cue
fillers with a high thematic fit for their argument
positions (e.g. arrest
agent
???? cop, (Ferretti et al,
2001)) and that verbs and arguments combined cue
fillers with a high thematic fit for the remaining
argument slots (e.g. ?journalist , check?
patient
?????
spelling but ?mechanic, check?
patient
????? car (Bick-
nell et al, 2010). The interpretation of logical
metonymy is also highly sensitive to context (e.g.
?confectioner , begin, icing?
covertevent
???????? spread
but ?child , begin, icing?
covertevent
???????? eat (Zarcone
and Pad?, 2011; Zarcone et al, 2012). It thus pro-
vides an excellent test bed for cognitively plausible
computational models of language processing.
We evaluate two classes of computational mod-
els for logical metonymy. The classes represent the
two main current approaches in lexical semantics:
probabilistic and distributional models. Probabilistic
models view the interpretation as the assignment of
values to random variables. Their advantage is that
they provide a straightforward way to include con-
text, by simply including additional random variables.
However, practical estimation of complex models
typically involves independence assumptions, which
70
may or may not be appropriate, and such models
only take first-order co-occurrence into account1. In
contrast, distributional models represent linguistic
entities as co-occurrence vectors and phrase interpre-
tation as a vector similarity maximization problem.
Distributional models typically do not require any
independence assumptions, and include second-order
co-occurrences. At the same time, how to integrate
context into the vector computation is essentially an
open research question (Mitchell and Lapata, 2010).
In this paper, we provide the first (to our knowl-
edge) distributional model of logical metonymy
by extending the context update of Lenci?s ECU
model (Lenci, 2011). We compare this model to
a previous probabilistic approach (Lapata and Las-
carides, 2003a; Lapata et al, 2003b). In contrast
to most experimental studies on logical metonymy,
which deal with English data (with the exception of
Lapata et al (2003b)), we focus on German. We
estimate our models on a large web corpus and eval-
uate them on a psycholinguistic dataset (Zarcone
and Pad?, 2011; Zarcone et al, 2012). The task
we use to evaluate our models is to distinguish
covert events with a high typicality / thematic fit
(e.g. The student finished the beer ?? drinking)
from low typicality / thematic fit covert events
(?? brewing).
2 Probabilistic models of logical metonymy
Lapata et al (2003b; 2003a) model the interpretation
of a logical metonymy (e.g. The student finished the
beer) as the joint distribution P (s, v, o, e) of the vari-
ables s (the subject, e.g. student), v (the metonymic
verb, e.g. finish), o (the object, e.g. beer), e (the
covert event, drinking).
This model requires independence assumptions
for estimation. We present two models with different
independence assumptions.
1This statement refers to the simple probabilistic models we
consider, which are estimated directly from corpus co-occurrence
frequencies. The situation is different for more complex prob-
abilistic models, for example generative models that introduce
latent variables, which can amount to clustering based on higher-
order co-occurrences, as in, e.g., Prescher et al (2000).
2.1 The SOVp model
Lapata et al develop a model which we will refer
to as the SOVp model.2 It assumes a generative pro-
cess which first generates the covert event e and then
generates all other variables based on the choice of e:
P (s, v, o, e) ? P (e) P (o|e) P (v|e) P (s|e)
They predict that the selected covert event e? for
a given context is the event which maximizes
P (s, v, o, e):
e? = argmax
e
P (e) P (o|e) P (v|e) P (s|e)
These distributions are estimated as follows:
P? (e) =
f(e)
N
, P? (o|e) =
f(e
o
?? o)
f(e
o
?? ?)
,
P? (v|e) =
f(v
c
?? e)
f(?
c
?? e)
, P? (s|e) =
f(e
s
?? s)
f(e
s
?? ?)
,
where N is the number of occurrences of full verbs
in the corpus; f(e) is the frequency of the verb e;
f(e
o
?? ?) and f(e
s
?? ?) are the frequencies of e
with a direct object and subject, respectively; and
f(e
c
?? ?) is number of times e is the complement of
another full verb.
2.2 The SOp model
In Lapata et al?s covert event model, v, the
metonymic verb, was used to prime different choices
of e for the same object (begin book?? writing;
enjoy book?? reading). In our dataset (Sec. 4), we
keep v constant and consider e only as a function of
s and o. Thus, the second model we consider is the
SOp model which does not consider v:
P (s, v, o, e) ? P (s, o, e) ? P (e) P (o|e) P (s|e)
Again, the preferred interpretation e? is the one that
maximizes P (s, v, o, e):
e? = argmax
e
P (e) P (o|e) P (s|e)
2In Lapata et al (2003b; 2003a), this model is called the
simplified model to distinguish it from a full model. Since the full
model performs worse, we do not include it into consideration
and use a more neutral name for the simplified model.
71
3 Similarity-based models
3.1 Distributional semantics
Distributional or vector space semantics (Turney and
Pantel, 2010) is a framework for representing word
meaning. It builds on the Distributional Hypothe-
sis (Harris, 1954; Miller and Charles, 1991) which
states that words occurring in similar contexts are
semantically similar. In distributional models, the
meaning of a word is represented as a vector whose
dimensions represent features of its linguistic con-
text. These features can be chosen in different ways;
popular choices are simple words (Sch?tze, 1992) or
lexicalized dependency relations (Lin, 1998; Pad?
and Lapata, 2007). Semantic similarity can then be
approximated by vector similarity using a wide range
of similarity metrics (Lee, 1999).
3.1.1 Distributional Memory
A recent multi-purpose framework in distribu-
tional semantics is Distributional Memory (DM, Ba-
roni and Lenci (2010)). DM does not immedi-
ately construct vectors for words. Instead, it ex-
tracts a three-dimensional tensor of weighted word-
link-word tuples each of which is mapped onto a
score by a function ? : ?w1 l w2? ? R+. For ex-
ample, ?pencil obj use? has a higher weight than
?elephant obj use?. The set of links can be defined in
different ways, yielding various DM instances. Ba-
roni and Lenci present DepDM (mainly syntactic
links such as subj_tr ), LexDM (strongly lexicalized
links, e.g., such_as), or TypeDM (syntactic and lexi-
calized links).3
The benefit of the tensor-based representation is
that it is general, being applicable to many tasks.
Once a task is selected, a dedicated semantic space
for this task can be generated efficiently from the
tensor. For example, the word by link-word space
(W1 ? LW2) contains vectors for the words w1
whose dimensions are labeled with ?l, w2? pairs. The
word-word by link space (W1W2 ? L) contains co-
occurrence vectors for word pairs ?w1, w2? whose
dimensions are labeled with l.
3.2 Compositional Distributional Semantics
Probabilistic models can account for composition-
ality by estimating conditional probabilities. Com-
3l?1 is used to denote the inverse link of l (i.e., exchanging
the positions of w1 and w2).
positionality is less straightforward in a similarity-
based distributional model, because similarity-based
distributional models traditionally model meaning
at word level. Nevertheless, the last years have
seen a wave of distributional models which make
progress at building compositional representations
of higher-level structures such as noun-adjective or
verb-argument combinations (Mitchell and Lapata,
2010; Guevara, 2011; Reddy et al, 2011).
3.2.1 Expectation Composition and Update
Lenci (2011) presents a model to predict the degree
of thematic fit for verb-argument combinations: the
Expectation Composition and Update (ECU) model.
More specifically, the goal of ECU is explain how the
choice of a specific subject for a given verb impacts
the semantic expectation for possible objects. For
example, the verb draw alone might have fair, but not
very high, expectations for the two possible objects
landscape and card. When it is combined with the
subject painter, the resulting phrase painter draw the
expectation for the object landscape should increase,
while it should drop for card.
The idea behind ECU is to first compute the verb?s
own expectations for the object from a TypeDM
W1 ? LW2 matrix and then update it with the sub-
ject?s expectations for the object, as mediated by the
TypeDM verb link type.4 More formally, the verb?s
expectations for the object are defined as
EXV (v) = ?o. ?(
?
v obj?1 o
?
)
The subject?s expectations for the object are
EXS(s) = ?o. ?(?s verb o?)
And the updated expectation is
EXSV (s, v) = ?o.EXV (v)(o) ? EXS(s)(o)
where ? is a composition operation which Lenci in-
stantiates as sum and product, following common
practice in compositional distributional semantics
(Mitchell and Lapata, 2010). The product composi-
tion approximates a conjunction, promoting objects
that are strongly preferred by both verb and subject.
It is, however, also prone to sparsity problems as well
4In DM, verb directly connects the subject and the object of
transitive verb instances, e.g ?marine verb gun?.
72
shortcomings of the scoring function ?. The sum
composition is more akin to a disjunction where it
suffices that an object is strongly preferred by either
the verb or the subject.
It would be possible to use these scores as direct
estimates of expectations, however, sinceEXSV con-
tains three lexical variables, sparsity is a major issue.
ECU thus introduces a distributional generalization
step. It only uses the updated expectations to identify
the 20 most expected nouns for the object position.
It then determines the prototype of the updated ex-
pectations as the centroid of their W1?LW2 vectors.
Now, the thematic fit for any noun can be computed
as the similarity of its vector to the prototype.
Lenci evaluates ECU against a dataset from Bick-
nell et al (2010), where objects (e.g. spelling) are
matched with a high-typicality subject-verb combi-
nations (e.g. ?journalist, check? - high thematic fit)
and with a low-typicality subject-verb combination
(e.g. ?mechanic, check? - low thematic fit). ECU is
in fact able to correctly distinguish between the two
contexts differing in thematic fit with the object.
3.3 Cognitive relevance
Similarity-based models build upon the Distribu-
tional Hypothesis, which, in its strong version, is
a cognitive hypothesis about the form of semantic
representations (Lenci, 2008): the distributional be-
havior of a word reflects its semantic behavior but
is also a direct correlate of its semantic content at
the cognitive level. Also, similarity-based models
are highly compatible with known features of hu-
man cognition, such as graded category member-
ship (Rosch, 1975) or multiple sense activation (Erk,
2010). Their cognitive relevance for language has
been supported by studies of child lexical devel-
opment (Li et al, 2004), category-related deficits
(Vigliocco et al, 2004), selectional preferences (Erk,
2007), event types (Zarcone and Lenci, 2008) and
more (see Landauer et al (2007) and Baroni and
Lenci (2010) for a review).
3.4 Modeling Logical Metonymy with ECU
3.4.1 Logical Metonymy as Thematic Fit
The hypothesis that we follow in this paper is that
the ECU model can also be used, with modifications,
to predict the interpretation of logical metonymy.
The underlying assumption is that the interpretation
of logical metonymy is essentially the recovery of
a covert event with a maximal thematic fit (high-
typicality) and can thus make use of ECU?s mech-
anisms to treat verb-argument composition. Strong
evidence for this assumption has been found in psy-
cholinguistic studies, which have established that
thematic fit dynamically affects processing, with on-
line updates of expectations for typical fillers during
the incremental processing of linguistic input (see
McRae and Matsuki (2009) for a review). Thus, we
can hope to transfer the benefits of similarity-based
models (notably, high coverage) to the interpretation
of logical metonymy.
3.4.2 Extending ECU
The ECU model nevertheless requires some modi-
fications to be applicable to logical metonymy. Both
the entity of interest and the knowledge sources
change. The entity of interest used to be the ob-
ject of the sentence; now it is the covert event, which
we will denote with e. As for knowledge sources,
there are three sources in logical metonymy. These
are (a), the subject (compare the author began the
beer and the reader began the book)); (b), the object
the reader began the book vs. the reader began the
sandwich); and (c), the metonymic verb (compare
Peter began the report vs. Peter enjoyed the report).
The basic equations of ECU can be applied to this
new scenario as follows. We first formulate three
basic equations that express the expectations of the
covert event given the subject, object, and metonymic
verb individually. They are all derived from direct de-
pendency relations in the DM tensor (e.g., the novel
metonymic verb?covert event relation from the ver-
bal complement relation):
EXS(s) = ?e. ?(?s subj e?)
EXO(o) = ?e. ?(?o obj e?)
EXV (v) = ?e. ?(
?
v comp?1 e
?
)
To combine (or update) these basic expectations into
a final expectation, we propose two variants:
ECU SOV In this model, we compose all three
expectations:
EXSOV (s, v, o) = ?e.EXS(s)(e) ?
EXO(o)(e) ? EXV (v)(e)
73
CE
high thematic fit low thematic fit
Der
The
Konditor
baker
begann,
started
die
the
Glasur
icing
aufzutragen.
to spread.
zu essen.
to eat.
Das
The
Kind
child
begann,
started
die
the
Glasur
icing
zu essen.
to eat.
aufzutragen.
to spread.
Table 1: Example materials for the self-paced reading and probe recognition studies
We will refer to this model as SOV? when the
composition function is sum, and as the SOV? model
when the composition function is product.
ECU SO Analogous to the SO probabilistic model,
this model abstracts away from the metonymic verb.
We assume most information about an event to be
determined by the subject and object:
EXSO(n, n
?) = ?e.EXS(n)(e) ? EXO(n
?)(e)
After the update, the prototype computation proceeds
as defined in the original ECU.
We will refer to this model as SO? when the com-
position function is sum, and as the SO? model when
the composition function is product.
4 Experimental Setup
We evaluate the probabilistic models (Sec. 2) and the
similarity-based models (Sec. 3) on a dataset con-
structed from two German psycholinguistic studies
on logical metonymy. One study used self-paced
reading and the second one probe recognition.
Dataset The dataset we use is composed of 96 sen-
tences. There are 24 sets of four ?s, v, o, e? tuples,
where s is the object, v the metonymic verb, o the
object and e the covert event. The materials are illus-
trated in Table 1. As can be seen, all tuples within
a set share the same metonymic verb and the same
object. Each of the two subject e is matched once
with a high-typicality covert event and once with a
low-typicality covert event. This results in 2 high-
typicality tuples and 2 low-typicality tuples in each
set. Typical events (e) were elicited by 20 partici-
pants given the corresponding object o, subjects were
elicited by 10 participants as the prototypical agents
subjects for each e, o combination.
The experiments yielded a main effect of typicality
on self-paced reading times (Zarcone and Pad?, 2011)
and on probe recognition latencies (Zarcone et al,
2012): typical events involved in logical metonymy
interpretation are read faster and take longer to be
rejected as probe words after sentences which evoke
them. The effect is seen early on (after the patient
position in the self-paced reading and at short ISI for
the probe recognition), suggesting that knowledge
of typical events is quickly integrated in processing
and that participants access a broader pool of knowl-
edge than what has traditionally been argued to be
in the lexical entries of nouns (Pustejovsky, 1995).
The finding is in agreement with results of psycholin-
guistic studies which challenge the very distinction
between world knowledge and linguistic knowledge
(Hagoort et al, 2004; McRae and Matsuki, 2009).
DM for German Since DM exists only for English,
we constructed a German analog using the 884M
word SDEWAC web corpus (Faa? et al, 2010) parsed
with the MATE German dependency parser (Bohnet,
2010).
From this corpus, we extract 55M instances of
simple syntactic relations (subj_tr, subj_intr, obj,
iobj, comp, nmod) and 104M instances of lexicalized
patterns such as noun?prep?noun e.g. ?Recht auf
Auskunft? (?right to information?), or adj?noun-(of)-
noun such as ?strittig Entscheidung Schiedsrichter?
(?contested decision referee?). These lexicalized pat-
terns make our model roughly similar to the English
TypeDM model (Sec. 3.1.1).
As for ?, we used local mutual information (LMI)
as proposed by Baroni and Lenci (2010). The LMI
of a triple is defined as Ow1lw2 log(Ow1lw2/Ew1lw2),
where Ow1lw2 is the observed co-occurrence fre-
quency of the triple and Ew1lw2 its expected co-
occurrence frequency (under the assumption of inde-
pendence). Like standard MI, LMI measures the
informativity or surprisal of a co-occurrence, but
74
weighs it by the observed frequency to avoid the
overestimation for low-probability events.
4.1 Task
We evaluate the models using a binary selection
task, similar to Lenci (2011). Given a triple ?s, v, o?
and a pair of covert events e, e? (cf. rows in
Tab. 1), the task is to pick the high-typicality covert
event for the given triple: ?Chauffeur, vermeiden,
Auto? ? fahren/reparieren (?driver, avoid, car? ?
drive/repair). Since our dataset consists of 96 sen-
tences, we have 48 such contexts.
With the probabilistic models, we compare the
probabilities P (s, v, o, e) and P (s, v, o, e?) (ignoring
v in the SO model). Analogously, for the similarity-
based models, we compute the similarities of the
vectors for e and e? to the prototype vectors for the ex-
pectations EXSOV (s, v, o) and predict the one with
higher similarity. For the simplified ECU SO model,
we use EXSO(s, o) as the point of comparison.
4.2 Baseline
Following the baseline choice in Lapata et al
(2003b), we evaluated the probabilistic models
against a baseline (Bp) which, given a ?s, v, o?
triplet (e.g. ?Chauffeur, vermeiden, Auto?), scores
a ?hit? if the P? (e|o) for the high-typicality e is
higher than the P? (e|o) for the low-typicality e. The
similarity-based models were evaluated against a
baseline (Bs) which, given an ?s, v, o? triplet (e.g.
?Chauffeur, vermeiden, Auto?), makes a correct pre-
diction if the prototypical event vector for o has a
higher thematic fit (i.e. similarity) with the high-
typicality e than with the low-typicality e.
Since our dataset is counterbalanced ? that is, each
covert event appears once as the high-typicality event
for a given object (with a congruent subject) and once
as the low-typicality event ? the baseline predicts
the correct covert event in exactly 50% of the cases.
Note, however, that this is not a random baseline: the
choice of the covert event is made deterministically
on the basis of the input parameters.
4.3 Evaluation measures
We evaluate the output of the model with the stan-
dard measures coverage and accuracy. Coverage is
defined as the percentage of datapoints for which
a model can make a prediction. Lack of coverage
arises primarily from sparsity, that is, zero counts for
co-occurrences that are necessary in the estimation
of a model. Accuracy is computed on the covered
contexts only, as the ratio of correct predictions to
the number of predictions of the model. This allows
us to judge the quality of the model?s predictions
independent of its coverage.
We also consider a measure that combines cov-
erage and accuracy, Backoff Accuracy, defined as:
coverage?accuracy+((1?coverage)?0.5). Back-
off Accuracy emulates a backoff procedure: the
model?s predictions are adopted where they are avail-
able; for the remaining datapoints, it assumes base-
line performance (in the current setup, 50%). The
Backoff Accuracy of low-coverage models tends to
degrade towards baseline performance.
We determine the significance of differences be-
tween models with a ?2 test, applied to a 2?2 contin-
gency matrix containing the number of correct and
incorrect answers. Datapoints outside a model?s cov-
erage count half for each category, which corresponds
exactly to the definition of Backoff Accuracy.
5 Results
The results are shown in Table 2. Looking at the
probabilistic models, we find SOp yields better cov-
erage and better accuracy than SOVp (Lapata?s sim-
plified model). It is worth noting the large differ-
ence in coverage, namely .75 as opposed to .44: The
SOVp model is unable to make a prediction for more
than half of all contexts. This is due to the fact that
many ?o, v? combinations are unattested in the cor-
pus. Even on those contexts for which the proba-
bilistic SOVp model can make a prediction, it is less
reliable than the more general SOp model (0.62 ver-
sus 0.75 accuracy). This indicates that, at least on our
dataset, the metonymic verb does not systematically
help to predict the covert event; it rather harms perfor-
mance by introducing noisy estimates. As the lower
half of the Table shows, the SOVp model does not
significantly outperform any other model (including
both baselines Bp and Bs).
The distributional models do not have such cover-
age issues. The main problematic combination for
the similarity model is ?Pizzabote hassen Pizza? (i.e.
?Pizza delivery man hate pizza?) which is paired
with the covert events liefern (deliver) and backen
(bake). The computation of ECU predictions for
75
Probabilistic Models Similarity-based Models
Bp SOVp SOp Bs SOV? SOV? SO? SO?
Accuracy 0.50 0.62 0.75 0.50 0.68 0.56 0.68 0.70
Coverage 1.00 0.44 0.75 1.00 0.98 0.94 0.98 0.98
Backoff Accuracy 0.50 0.55 0.69 0.50 0.68 0.56 0.68 0.70
Probabilistic Models Similarity-based Models
Bp SOVp SOp Bs SOV? SOV? SO? SO?
Bp
P
ro
b. SOVp -
SOp * -
Bs - - *
SOV? * - - *
S
im
il
ar
it
y
SOV? - - - - -
SO? * - - * - -
SO? ** ?? - ** - ?? -
Table 2: Results (above) and significance levels for difference in backoff accuracy determined by ?2-test (below)
for all probabilistic and similarity-based models (**: p<0.01, *: p?0.05, -: p>0.05). For ?? (SO? ? SOVp and
SO? ? SOV?) p was just above 0.05 (p=0.053).
this combination requires corpus transitive corpus
constructions for Pizzabote, in the corpus it is only
attested once as the subject of the intransitive verb
kommen (come).
Among distributional models, the difference be-
tween SO and SOV is not as clear-cut as on the
probabilistic side. We observe an interaction with the
composition operation. Sum is less sensitive to com-
plexity of updating: for sum models, the inclusion
of the metonymic verb (SOV? vs. SOV?) does not
make any difference. On the side of the product mod-
els, there is a major difference similar to the one for
the probabilistic models: SOV? is the worst model
at near-baseline performance, and SO? is the best
one. This supports our interpretation from above that
the metonymic model introduces noisy expectations
which, in the product model, have the potential of
disrupting the update process.
Comparing the best models from the probabilistic
and similarity-based classes (SOp and SO?), we find
that both significantly outperform the baselines. This
shows that the subject contributes to the models with
a significant improvement over the baseline models,
which are only informed by the object. Their back-
off accuracies do not significantly differ from one
another, which is not surprising given the small size
of our dataset, however, the similarity-based model
outperforms the probabilistic model by 1% Backoff
Accuracy. The two models have substantially differ-
ent profiles: the accuracy of the probabilistic model
is 5% higher (0.70 vs. 0.75); at the same time, its
coverage is much lower. It covers only 75% of the
contexts, while the distributional model SO? covers
all but one (98%).
6 Discussion
As mentioned above, the main issue with the proba-
bilistic models is coverage. This is due to the reliance
of these models on first-order co-occurrence.
For example, probabilistic models cannot
assign a probability to any of the triples
?Dieb/Juwelier schmuggeln/schleifen Diamant?
(?thief/jeweler smuggle/cut diamond?), since the
subjects do not occur with either of the verbs in
corpus, even though Diamant does occur as the
object of both.
In contrast, the similarity-based models are able to
compute expectations for these triples from second-
order co-occurrences by taking into account other
verbs that co-occur with Diamant. The ECU model
is not punished by the extra context, as both Dieb and
Diamant are associated with the verbs: stehlen (steal),
76
EXSO(?Chauffeur,Auto?) EXSO(?Mechaniker,Auto?)
fahren (drive) bauen (build)
parken (park) lassen (let/leave)
lassen (let/leave) besitzen (own)
geben (give) reparieren (repair)
sehen (see) brauchen (need)
bringen (bring) sehen (see)
steuern (steer) benutzen (use)
halten (keep/hold) stellen (put)
Table 3: Updated expectations in SO? for Chauffeur
(chauffeur), Mechaniker (mechanic) and Auto (car).
rauben (thieve), holen (get), entwenden (purloin),
erbeuten (snatch), verkaufen (sell), nehmen (take),
klauen (swipe). We also note that these are typical
events for a thief, which fits the intuition that Dieb is
more predictive of the event than Diamant.
For both ?Chauffeur Auto? and ?Mechaniker Auto?
the probabilistic model predicts fahren due to the
high overall frequency of fahren.5 The distributional
model, however, takes the mutual information into
account and is thus able to determine events that
are more strongly associated with Mechaniker (e.g.
bauen, reparieren, etc.) while at the same time dis-
counting the uninformative verb fahren.
There are, however, items that all models have dif-
ficulty with. Three such cases are due to a frequency
disparity between the high and low-typicality event.
E.g. for ?Lehrerin Klausur benoten/schreiben?
(?teacher exam grade/take?), schreiben occurs much
more frequently than benoten. In the case
of ?Sch?ler Geschichte lernen/schreiben? (?student
story learn/write?), none of the models or baselines
correctly assigned lernen. The probabilistic mod-
els are influenced by the very frequent Geschichte
schreiben which is part of an idiomatic expression (to
write history). On the other hand, the distributional
models judge the story and history sense of the word
to have the most informative events, e.g. erz?hlen
(tell), lesen (read), h?ren (hear), erfinden (invent),
and studieren (study), lehren (teach).
The baselines were able to correctly choose
auspacken (unwrap) over einpacken (wrap) for
?Geburtstagskind Geschenk? (?birthday-boy/girl
present?) while the models were not. The prob-
5The combination Mechaniker fahren was seen once more
often than Mechaniker reparieren.
abilistic models lacked coverage and were not
able to make a prediction. For the distributional
models, while both auspacken and verpacken (wrap)
are highly associated with Geschenk, the most
strongly associated actions of Geburtstagskind are
extraordinarily diverse, e.g.: bekommen (receive),
sagen (say), auffuttern (eat up), herumkommandieren
(boss around), ausblasen (blow out). Neither of the
events of interest though were highly associated.
7 Future Work
We see a possible improvement in the choice of the
number of fillers, with which we construct the pro-
totype vectors. A smaller number might lead to less
noisy prototypes.
It has been shown (Bergsma et al, 2010) that the
meaning of the prefix verb can be accurately pre-
dicted using the stem?s vector, when compositional-
ity applies. We suspect covert events that are prefix
verbs to suffer from sparser representations than the
vectors of their stem. E.g., absaugen (vacuum off )
is much less frequent than the semantically nearly
identical saugen (vacuum). Thus, by leveraging the
richer representation of the stem, our distributional
models could more likely assign the correct event.
8 Conclusions
We have presented a contrastive study of two classes
of computational models, probabilistic and distribu-
tional similarity-based ones, for the prediction of
covert events for German logical metonymies.
We found that while both model classes models
outperform baselines which only take into account
information coming from the object, similarity-based
models rival and even outperform probabilistic mod-
els. The reason is that probabilistic models have to
rely on first-order co-occurrence information which
suffers from sparsity issues even in large web corpora.
This is particularly true for languages like German
that have a complex morphology, which tends to ag-
gravate sparsity (e.g., through compound nouns).
In contrast, similarity-based models can take ad-
vantage of higher-order co-occurrences. Provided
that some care is taken to identify reasonable vec-
tor composition strategies, they can maintain the ac-
curacy of probabilistic models while guaranteeing
higher coverage.
77
Acknowledgments
We would like to thank Alessandro Lenci, Siva Reddy
and Sabine Schulte im Walde for useful feedback
and discussion. The research for this paper has
been funded by the German Research Foundation
(Deutsche Forschungsgemeinschaft) as part of the
SFB 732 ?Incremental specification in context? /
project D6 ?Lexical-semantic factors in event inter-
pretation? at the University of Stuttgart.
References
Giosu? Baggio, Travis Chroma, Michiel van Lambalgen,
and Peter Hagoort. 2010. Coercion and composition-
ality. Journal of Cognitive Neuroscience, 22(9):2131?
2140.
Giosu? Baggio, Michiel van Lambalgen, and Peter Ha-
goort. in press. The processing consequences of com-
positionality. In The Oxford Handbook of Composition-
ality. Oxford University Press.
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):1?49.
Shane Bergsma, Aditya Bhargava, Hua He, and Grzegorz
Kondrak. 2010. Predicting the semantic composition-
ality of prefix verbs. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 293?303, Cambridge, MA, October.
Association for Computational Linguistics.
Klinton Bicknell, Jeffrey L. Elman, Mary Hare, Ken
McRae, and Marta Kutas. 2010. Effects of event
knowledge in processing verbal arguments. Journal of
Memory and Language, 63(4):489?505.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics, pages 89?97, Beijing, China.
Robyn Cartson. 2002. Thoughts and utterances. Black-
well.
Roberto G. De Almeida and Veena D. Dwivedi. 2008. Co-
ercion without lexical decomposition: Type-shifting
effects revisited. Canadian Journal of Linguistics,
53(2/3):301?326.
Katrin Erk. 2007. A simple, similarity-based model for
selectional preferences. In Proceedings of ACL, Prague,
Czech Republic.
Katrin Erk. 2010. What is word meaning, really? (and
how can distributional models help us describe it?).
In Proceedings of the workshop on Geometrical Mod-
els of Natural Language Semantics (GEMS), Uppsala,
Sweden.
Gertrud Faa?, Ulrich Heid, and Helmut Schmid. 2010.
Design and Application of a Gold Standard for Mor-
phological Analysis: SMOR as an Example of Mor-
phological Evaluation. In Proceedings of the Seventh
International Conference on Language Resources and
Evaluation (LREC?10), Valletta, Malta.
T. R. Ferretti, K. McRae, and A. Hatherell. 2001. Integrat-
ing verbs, situation schemas and thematic role concept.
Journal of Memory and Language, 44:516?547.
Jerry A. Fodor and Ernie Lepore. 1998. The emptiness
of the lexicon: Reflections on James Pustejovsky?s The
Generative Lexicon. Linguistic Inquiry, 29(2):269?
288.
Emiliano Raul Guevara. 2011. Computing semantic com-
positionality in distributional semantics. In Proceed-
ings of IWCS-2011, Oxford, UK.
Peter Hagoort, Lea Hald, Marcel Bastiaansen, and
Karl Magnus Petersson. 2004. Integration of word
meaning and world knowledge in language comprehen-
sion. Science, 304:438?441.
Zelig S. Harris. 1954. Distributional structure. Word,
10(23):146?162.
Thomas K. Landauer, Danielle S. McNamara, Simon Den-
nis, and Walter Kintsch, editors. 2007. Handbook of
Latent Semantic Analysis. Lawrence Erlbaum Asso-
ciates Publishers, Mahwah, NJ, US.
Mirella Lapata and Alex Lascarides. 2003a. A proba-
bilistic account of logical metonymy. Computational
Linguistics, 29(2):263?317.
Mirella Lapata, Frank Keller, and Christoph Scheepers.
2003b. Intra-sentential context effects on the inter-
pretation of logical metonymy. Cognitive Science,
27(4):649?668.
Lillian Lee. 1999. Measures of Distributional Similarity.
In Proceedings of the 37th annual meeting of the Associ-
ation for Computational Linguistics on Computational
Linguistics, College Park, MA.
Alessandro Lenci. 2008. Distributional semantics in lin-
guistic and cognitive research. From context to mean-
ing: Distributional models of the lexicon in linguistics
and cognitive science. Special issue of the Italian Jour-
nal of Linguistics, 20(1):1?31.
Alessandro Lenci. 2011. Composing and updating
verb argument expectations: A distributional semantic
model. In Proceedings of the 2nd Workshop on Cog-
nitive Modeling and Computational Linguistics, pages
58?66, Portland, Oregon.
Ping Li, Igor Farkas, and Brian MacWhinney. 2004. Early
lexical development in a self-organizing neural network.
Neural Networks, 17:1345?1362.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING/ACL, pages
768?774, Montreal, QC.
78
Ken McRae and Kazunaga Matsuki. 2009. People use
their knowledge of common events to understand lan-
guage, and do so as quickly as possible. Language and
Linguistics Compass, 3/6:1417?1429.
George A. Miller and Walter G. Charles. 1991. Contex-
tual correlates of semantic similarity. Language and
Cognitive Processes, 6(1):1?28.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Sebastian Pad? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33:161?199, June.
Barbara H. Partee, Alice ter Meulen, and Robert E. Wall.
1993. Mathematical Methods in Linguistics. Kluwer.
Detlef Prescher, Stefan Riezler, and Mats Rooth. 2000.
Using a Probabilistic Class-Based Lexicon for Lexical
Ambiguity Resolution. In Proceedings of COLING
2000, Saarbr?cken, Germany.
James Pustejovsky. 1995. The Generative Lexicon. MIT
Press.
Liina Pylkk?nen and Brian McElree. 2006. The syntax-
semantic interface: On-line composition of sentence
meaning. In Handbook of Psycholinguistics, pages
537?577. Elsevier.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in com-
pound nouns. In Proceedings of IJCNLP 2011, Chiang
Mai, Thailand.
Eleanor Rosch. 1975. Cognitive representations of seman-
tic categories. Journal of Experimental Psychology:
General, 104:192?233.
Hinrich Sch?tze. 1992. Dimensions of meaning. In
Proceedings of Supercomputing ?92, pages 787 ?796.
Peter D. Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
Gabriella Vigliocco, David P. Vinson, William Lewis,
and Merrill F. Garrett. 2004. Representing the mean-
ings of object and action words: The featural and uni-
tary semantic space hypothesis. Cognitive Psychology,
48(4):422?488.
Alessandra Zarcone and Alessandro Lenci. 2008. Compu-
tational models for event type classification in context.
In Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC?08),
Marrakech, Morocco. ELRA.
Alessandra Zarcone and Sebastian Pad?. 2011. General-
ized event knowledge in logical metonymy resolution.
In Proceedings of the 33rd Annual Conference of the
Cognitive Science Society, pages 944?949, Austin, TX.
Alessandra Zarcone, Sebastian Pad?, and Alessandro
Lenci. 2012. Inferring covert events in logical
metonymies: a probe recognition experiment. In Pro-
ceedings of the 34th Annual Conference of the Cogni-
tive Science Society, Austin, TX.
79
