Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 137?140,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Sub-Sentence Division for Tree-Based Machine Translation 
 
Hao Xiong*, Wenwen Xu+, Haitao Mi*, Yang Liu* and Qun Liu* 
*Key Lab. of Intelligent Information Processing 
+Key Lab. of Computer System and Architecture 
Institute of Computing Technology 
Chinese Academy of Sciences 
P.O. Box 2704, Beijing 100190, China 
{xionghao,xuwenwen,htmi,yliu,liuqun}@ict.ac.cn
 
Abstract 
Tree-based statistical machine translation 
models have made significant progress in re-
cent years, especially when replacing 1-best 
trees with packed forests. However, as the 
parsing accuracy usually goes down dramati-
cally with the increase of sentence length, 
translating long sentences often takes long 
time and only produces degenerate transla-
tions. We propose a new method named sub-
sentence division that reduces the decoding 
time and improves the translation quality for 
tree-based translation. Our approach divides 
long sentences into several sub-sentences by 
exploiting tree structures. Large-scale ex-
periments on the NIST 2008 Chinese-to-
English test set show that our approach 
achieves an absolute improvement of 1.1 
BLEU points over the baseline system in 
50% less time. 
1 Introduction 
Tree-based statistical machine translation 
models in days have witness promising progress 
in recent years, such as tree-to-string models (Liu 
et al, 2006; Huang et al, 2006), tree-to-tree 
models (Quirk et al,2005;Zhang et al, 2008). 
Especially, when incorporated with forest, the 
correspondent forest-based tree-to-string models 
(Mi et al, 2008; Zhang et al, 2009), tree-to-tree 
models (Liu et al, 2009) have achieved a prom-
ising improvements over correspondent tree-
based systems. However, when we translate long 
sentences, we argue that two major issues will be 
raised. On one hand, parsing accuracy will be 
lower as the length of sentence grows. It will in-
evitably hurt the translation quality (Quirk and 
Corston-Oliver, 2006; Mi and Huang, 2008). On 
the other hand, decoding on long sentences will 
be time consuming, especially for forest ap-
proaches. So splitting long sentences into sub- 
 
Figure 1. Main framework of our method 
 
sentences becomes a natural way in MT litera-
ture.  
A simple way is to split long sentences by 
punctuations. However, without concerning 
about the original whole tree structures, this ap-
proach will result in ill-formed sub-trees which 
don?t respect to original structures. In this paper, 
we present a new approach, which pays more 
attention to parse trees on the long sentences. We 
firstly parse the long sentences into trees, and 
then divide them accordingly into sub-sentences, 
which will be translated independently (Section 
3). Finally, we combine sub translations into a 
full translation (Section 4). Large-scale experi-
ments (Section 5) show that the BLEU score 
achieved by our approach is 1.1 higher than di-
rect decoding and 0.3 higher than always split-
ting on commas on the 2008 NIST MT Chinese-
English test set. Moreover, our approach has re-
duced decoding time significantly. 
2 Framework  
Our approach works in following steps. 
(1) Split a long sentence into sub-sentences.  
(2) Translate all the sub-sentences respectively. 
(3) Combine the sub-translations.   
Figure 1 illustrates the main idea of our ap-
proach. The crucial issues of our method are how 
to divide long sentences and how to combine the 
sub-translations.  
3 Sub Sentence Division  
Long sentences could be very complicated in 
grammar and sentence structure, thereby creating 
an obstacle for translation. Consequently, we 
need to break them into shorter and easier 
clauses. To divide sentences by punctuation is 
137
 
 
Figure 2. An undividable parse tree 
 
 
Figure 3. A dividable parse tree 
 
one of the most commonly used methods. How-
ever, simply applying this method might damage 
the accuracy of parsing. As a result, the strategy 
we proposed is to operate division while con-
cerning the structure of parse tree. 
As sentence division should not influence the 
accuracy of parsing, we have to be very cautious 
about sentences whose division might decrease 
the accuracy of parsing. Figure 2(a) shows an 
example of the parse tree of an undividable sen-
tence. 
As can be seen in Figure 2, when we divide 
the sentence by comma, it would break the struc-
ture of ?VP? sub-tree and result in a ill-formed 
sub-tree ?VP? (right sub-tree), which don?t have 
a subject and don?t respect to original tree struc-
tures. 
Consequently, the key issue of sentence divi-
sion is finding the sentences that can be divided 
without loosing parsing accuracy. Figure 2(b) 
shows the parse tree of a sentence that can be 
divided by punctuation, as sub-sentences divided 
by comma are independent. The reference trans-
lation of the sentence in figure 3 is 
 
Less than two hours earlier, a Palestinian took 
on a shooting spree on passengers in the town of 
Kfar Saba in northern Israel. 
Pseudocode 1 Check Sub Sentence Divi-
sion Algorithm 
1: procedure CheckSubSentence(sent) 
2: for each word i in sent 
3:    if(i is a comma) 
4:       left={words in left side of i}; 
          //words between last comma and cur-
rent comma i 
5:       right={words in right side of i}; 
         //words between i and next comma or
 semicolon, period, question mark 
6:       isDividePunct[i]=true; 
7:       for each j in left 
8:          if(( LCA(j, i)!=parent[i]) 
9:             isDividePunct[i]=false; 
10:           break; 
11:     for each j in right 
12:        if(( LCA(j, i)!=parent[i]) 
13:           isDividePunct[i]=false; 
14:           break; 
15: function LCA(i, j) 
16:    return lowest common ancestor(i, j);
 
It demonstrates that this long sentence can be 
divided into two sub-sentences, providing a good 
support to our division. 
In addition to dividable sentences and non-
dividable sentences, there are sentences contain-
ing more than one comma, some of which are 
dividable and some are not. However, this does 
not prove to be a problem, as we process each 
comma independently. In other words, we only 
split the dividable part of this kind of sentences, 
leaving the non-dividable part unchanged.  
To find the sentences that can be divided, we 
present a new method and provide its pseudo 
code. Firstly, we divide a sentence by its commas. 
For each word in the sub-sentence on the left 
side of a comma, we compute its lowest common 
ancestor (LCA) with the comma. And we process 
the words in the sub-sentence on the right side of 
the comma in the same way. Finally, we check if 
all the LCA we have computed are comma?s par-
ent node.  If all the LCA are the comma?s parent 
node, the sub-sentences are independent.  
As shown in figure 3, the LCA (AD ?? , 
PU ?),  is ?IP? ,which is the parent node of 
?PU ??; and the LCA (NR ??? , PU ?) is 
also ?IP?.  Till we have checked all the LCA of 
each word and comma, we finally find that all 
the LCA are ?IP?. As a result, this sentence can 
be divided without loosing parsing accuracy. 
LCA can be computed by using union-set (Tar-
jan, 1971) in lineal time. Concerning the  
138
sub-sentence 1: ???? 
Translation 1: Johndroe said                   A1
Translation 2: Johndroe pointed out       A2
Translation 3: Qiang Zhuo said              A3
comma 1: , 
Translation: punctuation translation (white 
space, that ? ) 
sub-sentence 2: ???????????
??????????????? 
Translation 1: the two presidents also wel-
comed the US-South Korea free trade 
agreement that was signed yesterday       B1
Translation 2: the two presidents also ex-
pressed welcome to the US ? South Korea 
free trade agreement signed yesterday     B2
comma 2: , 
Translation: punctuation translation (white 
space, that ? ) 
sub-sentence 3:???????????
?????? 
Translation 1: and would work to ensure 
that the congresses of both countries ap-
prove this agreement.                               C1
Translation 2: and will make efforts to en-
sure the Congress to approve this agreement 
of the two countries.                                C2
 
Table 1. Sub translation example 
 
implementation complexity, we have reduced the 
problem to range minimum query problem 
(Bender et al, 2005) with a time complexity of  
(1)?  for querying.  
Above all, our approach for sub sentence 
works as follows: 
(1)Split a sentence by semi-colon if there is 
one. 
(2)Parse a sentence if it contains a comma, 
generating k-best parses (Huang Chiang, 2005) 
with k=10.  
 (3)Use the algorithm in pseudocode 1 to 
check the sentence and divide it if there are 
more than 5 parse trees indicates that the sen-
tence is dividable.  
4 Sub Translation Combining  
For sub translation combining, we mainly use the 
best-first expansion idea from cube pruning 
(Huang and Chiang, 2007) to combine sub- 
translations and generate the whole k-best trans-
lations. We first select the best translation from 
sub translation sets, and then use an interpolation 
 
Test Set 02 05 08 
No Sent Division 34.56 31.26 24.53 
Split by Comma 34.59 31.23 25.39 
Our Approach 34.86 31.23 25.69 
 
Table 2. BLEU results (case sensitive) 
 
Test Set 02 05 08 
No Sent Division 28 h 36 h 52 h 
Split by Comma 18h 23h 29h 
Our Approach 18 h 22 h 26 h 
 
Table 3. Decoding time of our experiments 
(h means hours) 
 
language model for rescoring (Huang and Chiang, 
2007).  
For example, we split the following sentence ??
???,??????????????????
????????,?????????????
????? into three sub-sentences and generate 
some translations, and the results are displayed in 
Table 1.  
As seen in Table 1, for each sub-sentence, 
there are one or more versions of translation. For 
convenience, we label the three translation ver-
sions of sub-sentence 1 as A1, A2, and A3, re-
spectively. Similarly, B1, B2, C1, C2 are also 
labels of translation. We push the A1, white 
space, B1, white space, C1 into the cube, and 
then generate the final translation. 
According to cube pruning algorithm, we will 
generate other translations until we get the best 
list we need. Finally, we rescore the k-best list 
using interpolation language model and find the 
best translation which is A1 that B1 white space 
C1. 
5 Experiments  
5.1 Data preparation 
We conduct our experiments on Chinese-English 
translation, and use the Chinese parser of Xiong 
et al (2005) to parse the source sentences. And 
our decoder is based on forest-based tree-to-
string translation model (Mi et al 2008). 
Our training corpus consists of 2.56 million 
sentence pairs. Forest-based rule extractor (Mi 
and Huang 2008) is used with a pruning thresh-
old p=3. And we use SRI Language Modeling 
Toolkit (Stolcke, 2002) to train two 5-gram lan-
guage models with Kneser-Ney smoothing on the 
English side of the training corpus and the Xin-
hua portion of Gigaword corpora respectively. 
139
We use 2006 NIST MT Evaluation test set as 
development set, and 2002, 2005 and 2008 NIST 
MT Evaluation test sets as test sets. We also use 
minimum error-rate training (Och, 2003) to tune 
our feature weights. We evaluate our results with 
case-sensitive BLEU-4 metric (Papineni et al, 
2002). The pruning threshold p for parse forest in 
decoding time is 12. 
5.2 Results 
The final BLEU results are shown in Table 2, our 
approach has achieved a BLEU score that is 1.1 
higher than direct decoding and 0.3 higher than 
always splitting on commas. 
The decoding time results are presented in Ta-
ble 3. The search space of our experiment is ex-
tremely large due to the large pruning threshold 
(p=12), thus resulting in a long decoding time. 
However, our approach has reduced the decoding 
time by 50% over direct decoding, and 10% over 
always splitting on commas. 
6 Conclusion & Future Work  
We have presented a new sub-sentence division 
method and achieved some good results. In the 
future, we will extend our work from decoding to 
training time, where we divide the bilingual sen-
tences accordingly.  
Acknowledgement 
The authors were supported by National Natural 
Science Foundation of China, Contracts 0873167 
and 60736014, and 863 State Key Project 
No.2006AA010108. We thank Liang Huang for 
his insightful suggestions.  
References  
Bender, Farach-Colton, Pemmasani, Skiena, Sumazin, 
Lowest common ancestors in trees and di- 
rected acyclic graphs. J. Algorithms 57(2), 75?
94 (2005) 
Liang Huang and David Chiang. 2005. Better kbest 
Parsing. In Proceedings of IWPT-2005. 
Liang Huang and David Chiang. 2007. Forest res-
coring: Fast decoding with integrated lan-
guage models. In Proceedings of ACL. 
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. 
Statistical syntax-directed translation with ex-
tended domain of locality. In Proceedings of 
AMTA 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 127-133. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String alignments template for statistical ma-
chine translation. In Proceedings of ACL. 
Yang Liu, Yajuan Lv and Qun Liu.2009. Improving 
Tree-to-Tree Translation with Packed Forests.To 
appear in Proceedings of ACL/IJCNLP.. 
Daniel Marcu, Wei Wang, AbdessamadEchihabi, and 
Kevin Knight. 2006. Statistical Machine Trans-
lation with syntactifiedtarget language 
phrases. In Proceedings of EMNLP. 
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT. 
Haitao Mi and Liang Huang. 2008. Forest-based 
translation rule extraction. In Proceedings of 
EMNLP. 
Franz J. Och. 2003. Minimum error rate training 
in statistical machine translation. In Proceed-
ings of ACL, pages 160?167. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In 
Proceedings of ACL, pages 311?318,. 
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically 
informed phrasal SMT. In Proceedings of ACL. 
Chris Quirk and Simon Corston-Oliver. 2006. The 
impact of parse quality on syntactically-
informed statistical machine translation. In 
Proceedings of EMNLP. 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of 
ICSLP, volume 30, pages 901?904. 
Georgianna Tarjan, Depth First Search and Linear 
Graph Algorithms. SIAM J. Comp. 1:2, pp. 146?
160, 1972. 
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun 
Lin.2005. Parsing the Penn Chinese Treebank 
with semantic knowledge. In Proceedings of 
IJCNLP. 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, 
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree transla-
tion model. In Proceedings of ACL. 
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and 
Chew Lim Tan. 2009. Forest-based Tree Sequence 
to String Translation Model. To appear in Proceed-
ings of ACL/IJCNLP 
140
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 514?518,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
ICT:A System Combination for Chinese Semantic Dependency Parsing
Hao Xiong and Qun Liu
Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{xionghao, liuqun}@ict.ac.cn
Abstract
The goal of semantic dependency parsing is to
build dependency structure and label seman-
tic relation between a head and its modifier.
To attain this goal, we concentrate on obtain-
ing better dependency structure to predict bet-
ter semantic relations, and propose a method
to combine the results of three state-of-the-art
dependency parsers. Unfortunately, we made
a mistake when we generate the final output
that results in a lower score of 56.31% in term
of Labeled Attachment Score (LAS), reported
by organizers. After giving golden testing set,
we fix the bug and rerun the evaluation script,
this time we obtain the score of 62.8% which
is consistent with the results on developing set.
We will report detailed experimental results
with correct program as a comparison stan-
dard for further research.
1 Introduction
In this year?s Semantic Evaluation Task, the organiz-
ers hold a task for Chinese Semantic Dependency
Parsing. The semantic dependency parsing (SDP)
is a kind of dependency parsing. It builds a depen-
dency structure for a sentence and labels the seman-
tic relation between a head and its modifier. The
semantic relations are different from syntactic rela-
tions. They are position independent, e.g., the pa-
tient can be before or behind a predicate. On the
other hand, their grains are finer than syntactic re-
lations, e.g., the syntactic subject can be agent or
experiencer. Readers can refer to (Wanxiang Che,
2012) for detailed introduction.
Figure 1: The pipeline of our system, where we com-
bine the results of three dependency parsers and use max-
entropy classifier to predict the semantic relations.
Different from most methods proposed in
CoNLL-2008 1 and 2009 2, in which some re-
searchers build a joint model to simultaneously gen-
erate dependency structure and its syntactic relations
(Surdeanu et al, 2008; Hajic? et al, 2009), here,
we first employ several parsers to generate depen-
dency structure and then propose a method to com-
bine their outputs. After that, we label relation be-
tween each head and its modifier via the traversal
of this refined parse tree. The reason why we use
a pipeline model while not a joint model is that
the number of semantic relations annotated by or-
ganizers is more than 120 types, while in the for-
mer task is only 21 types. Compared to the former
task, the large number of types will obviously drop
the performance of classifier. On the other hand, the
performance of syntactic dependency parsing is ap-
proaching to perfect, intuitively, that better depen-
dency structure does help to semantic parsing, thus
we can concentrate on improving the accuracy of de-
pendency structure construction.
The overall framework of our system is illustrated
1http://www.yr-bcn.es/conll2008/
2http://ufal.mff.cuni.cz/conll2009-st/
514
in figure 1, where three dependency parsers are em-
ployed to generate the dependency structure, and a
maximum entropy classifier is used to predict rela-
tion for head and its modifier over combined parse
tree. Final experimental results show that our sys-
tem achieves 80.45% in term of unlabeled attach-
ment score (UAS), and 62.8 % in term of LAS. Both
of them are higher than the baseline without using
system combinational techniques.
In the following of this paper, we will demonstrate
the detailed information of our system, and report
several experimental results.
2 System Description
As mentioned, we employ three single dependency
parsers to generate respect dependency structure. To
further improve the accuracy of dependency struc-
ture construction, we blend the syntactic outputs and
find a better dependency structure. In the followings,
we will first introduce the details of our strategy for
dependency structure construction.
2.1 Parsers
We implement three transition-based dependency
parsers with three different parsing algorithms:
Nivre?s arc standard, Nivre?s arc eager (see Nivre
(2004) for a comparison between the two Nivre al-
gorithms), and Liang?s dynamic algorithm(Huang
and Sagae, 2010). We use these algorithms for
several reasons: first, they are easy to implement
and their reported performance are approaching to
state-of-the-art. Second, their outputs are projective,
which is consistent with given corpus.
2.2 Parser Combination
We use the similar method presented in Hall et al
(2011) to advance the accuracy of parses. The parses
of each sentence are combined into a weighted di-
rected graph. The left procedure is similar to tradi-
tional graph-based dependency parsing except that
the number of edges in our system is smaller since
we reserve best edges predicted by three single
parsers. We use the popular Chu-Liu-Edmonds al-
gorithm (Chu and Liu, 1965; Edmonds et al, 1968)
to find the maximum spanning tree (MST) of the
new constructed graph, which is considered as the
final parse of the sentence. Specifically, we use the
parsing accuracy on developing set to represent the
weight of graph edge. Formally, the weight of graph
edge is computed as follows,
we =
?
p?P
Accuracy(p) ? I(e, p) (1)
where the Accuracy(p) is the parsing score of
parse tree p whose value is the score of parsing accu-
racy on developing set, and I(e, p) is an indicator, if
there is such dependency in parse tree p, it returns 1,
otherwise returns 0. Since the value of Accuracy(p)
ranges from 0 to 1, we doesn?t need to normalize its
value.
Thus, the detailed procedure for dependency
structure construction is,
? Parsing each sentence using Nivre?s arc stan-
dard, Nivre?s arc eager and Liang?s dynamic al-
gorithm, respectively.
? Combining parses outputted by three parsers
into weighted directed graph, and representing
its weight using equation 1.
? Using Chu-Liu-Edmonds algorithm to search
final parse for each sentence.
2.3 Features for Labeling
After given dependency structure, for each relation
between head and its modifier, we extract 31 types
of features, which are typically exploited in syntac-
tic dependency parsing, as our basic features. Based
on these basic features, we also add a additional dis-
tance metric for each features and obtain 31 types of
distance incorporated features. Besides that, we use
greedy hill climbing approach to select additional 29
features to obtain better performance. Table 1 shows
the basic features used in our system,
And the table 2 gives the additional features. It
is worth mentioning, that the distance is calculated
as the difference between the head and its modifier,
which is different from the calculation reported by
most literatures.
2.4 Classifier
We use the classifier from Le Zhang?s Maximum
Entropy Modeling Toolkit3 and use the L-BFGS
3http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit
.html
515
Features
Basic
mw:modifier?s word
mp:modifier?s POS tag
hw:head?s word
hp:head?s POS tag
Combination
hw|hp,mw|mp,hw|mw
hp|mp,hw|mp,hp|mw
hw|hp|mw
hw|hp|mp
hw|mw|mp
hp|mw|mp
hp|mp|mp-1
hp|mp|mp+1
hp|hp-1|mp
hp|hp+1|mp
hp|hp-1|mp-1
hp|hp-1|mp+1
hp|hp+1|mp-1
hp|hp+1|mp+1
hp-1|mp|mp-1
hp-1|mp|mp+1
hp+1|mp|mp-1
hp+1|mp|mp+1
hw|hp|mw|mp
hp|hp-1|mp|mp-1
hp|hp+1|mp|mp+1
hp|hp+1|mp|mp-1
hp|hp-1|mp|mp+1
Table 1: The basic features used in our system. -1 and
+1 indicate the one on the left and right of given word.
parameter estimation algorithm with gaussian prior
smoothing(Chen and Rosenfeld, 1999). We set the
gaussian prior to 2 and train the model in 1000 iter-
ations according to the previous experience.
3 Experiments
The given corpus consists of 8301 sentences
for training(TR), and 569 sentences for develop-
ing(DE). For tuning parameters, we just use TR por-
tion, while for testing, we combine two parts and
retrain the parser to obtain better results. Surely, we
also give results of testing set trained on TR portion
for comparison. In the following of this section, we
will report the detailed experimental results both on
Features
Distance dist:basic features with distance
Additional
lmw:leftmost word of modifier
rnw:rightnearest word of modifier
gfw:grandfather of modifier
lmp,rnp,gfp
lmw|lmp,rnw|rnp,lmw|rnw
lmp|rnp,lmw|mw,lmp|mp
rnw|mw,rnp|mp,gfw|mw
gfp|mp,gfw|hw,gfp|hp
gfw|mw|gfp|mp
lmw|lmp|mw|mp
rnw|rnp|mw|mp
lmw|rnw|mw,lmp|rnp|mp
gfw|hw|gfp|hp
gfw|mw|hw,gfp|mp|hp
gfw|mw|hw|gfp|mp|hp
lmw|rnw|lmp|rnp|mw|mp
lmw|rnw|lmp|rnp
Table 2: The additional features used in our system.
developing and testing set.
3.1 Results on Developing Set
We first report the accuracy of dependency construc-
tion on developing set using different parsing al-
gorithms in table 3. Note that, the features used
in our system are similar to that used in their pub-
lished papers(Nivre, 2003; Nivre, 2004; Huang and
Sagae, 2010). From table 3 we find that although
Precision (%)
Nivre?s arc standard 78.86
Nivre?s arc eager 79.11
Liang?s dynamic 79.78
System Combination 80.85
Table 3: Syntactic precision of different parsers on devel-
oping set.
using simple method for combination over three sin-
gle parsers, the system combination technique still
achieves 1.1 points improvement over the highest
single system. Since the Liang?s algorithm is a dy-
namic algorithm, which enlarges the searching space
in decoding, while the former two Nivre?s arc al-
516
gorithms actually still are simple beam search al-
gorithm, thus the Liang?s algorithm achieves better
performance than Nivre?s two algorithm, which is
consistent with the experiments in Liang?s paper.
To acknowledge that the better dependency struc-
ture does help to semantic relation labeling, we fur-
ther predict semantic relations on different depen-
dency structures. For comparison, we also report the
performance on golden structure. Since our combi-
Precision (%)
Nivre?s arc standard 60.84
Nivre?s arc eager 60.76
Liang?s dynamic 61.43
System Combination 62.92
Golden Tree 76.63
Table 4: LAS of semantic relations over different parses
on developing set.
national algorithm requires weight for each edges,
we use the developing parsing accuracy 0.7886,
0.7911, and 0.7978 as corresponding weights for
each single system. Table 4 shows, that the pre-
diction of semantic relation could benefit from the
improvement of dependency structure. We also no-
tice that even given the golden parse tree, the per-
formance of relation labeling is still far from per-
fect. Two reasons could be explained for that: first
is the small size of supplied corpus, second is that
the relation between head and its modifier is too
fine-grained to distinguish for a classifier. More-
over, here we use golden segmentation for parsing,
imagining that an automatic segmenter would fur-
ther drop the accuracy both on syntactic and seman-
tic parsing.
3.2 Results on Testing Set
Since there is a bug4 in our final results submitted
to organizers, here, in order to confirm the improve-
ment of our method and supply comparison standard
for further research, we reevaluate the correct output
and report its performance on different training set.
Table 5 and table 6 give the results trained on dif-
ferent corpus. We can see that when increasing the
4The bug is come from that when we converting the CoNLL-
styled outputs generated by our combination system into plain
text. While in developing stage, we directly used CoNLL-styled
outputs as our input, thus we didn?t realize this mistake.
training size, the performance is slightly improved.
Also, we find the results on testing set is consistent
with that on developing set, where best dependency
structure achieves the best performance.
LAS (%) UAS(%)
Nivre?s arc standard 60.38 78.19
Nivre?s arc eager 60.78 78.62
Liang?s dynamic 60.85 79.09
System Combination 62.76 80.23
Submitted Error Results 55.26 71.85
Table 5: LAS and UAS on testing set trained on TR.
LAS (%) UAS(%)
Nivre?s arc standard 60.49 78.25
Nivre?s arc eager 60.99 78.78
Liang?s dynamic 61.29 79.59
System Combination 62.80 80.45
Submitted Error Results 56.31 73.20
Table 6: LAS and UAS on testing set trained on TR and
DE.
4 Conclusion
In this paper, we demonstrate our system framework
for Chinese Semantic Dependency Parsing, and re-
port the experiments with different configurations.
We propose to use system combination to better the
dependency structure construction, and then label
semantic relations over refined parse tree. Final ex-
periments show that better syntactic parsing do help
to improve the accuracy of semantic relation predic-
tion.
Acknowledgments
The authors were supported by National Science
Foundation of China, Contracts 90920004, and
High-Technology R&D Program (863) Project No
2011AA01A207 and 2012BAH39B03. We thank
Heng Yu for generating parse tree using Liang?s al-
gorithm. We thank organizers for their generous
supplied resources and arduous preparation. We also
thank anonymous reviewers for their thoughtful sug-
gestions.
517
References
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical report, CMU-CS-99-108.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14(1396-
1400):270.
J. Edmonds, J. Edmonds, and J. Edmonds. 1968. Opti-
mum branchings. National Bureau of standards.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara, M.A.
Mart??, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,
J. ?Ste?pa?nek, et al 2009. The conll-2009 shared
task: Syntactic and semantic dependencies in multiple
languages. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning:
Shared Task, pages 1?18. Association for Computa-
tional Linguistics.
J. Hall, J. Nilsson, and J. Nivre. 2011. Single malt or
blended? a study in multilingual parser optimization.
Trends in Parsing Technology, pages 19?33.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1077?1086. Association
for Computational Linguistics.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies (IWPT. Cite-
seer.
J. Nivre. 2004. Incrementality in deterministic depen-
dency parsing. In Proceedings of the Workshop on In-
cremental Parsing: Bringing Engineering and Cogni-
tion Together, pages 50?57. Association for Computa-
tional Linguistics.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, and
J. Nivre. 2008. The conll-2008 shared task on joint
parsing of syntactic and semantic dependencies. In
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 159?177.
Association for Computational Linguistics.
Ting Liu Wanxiang Che. 2012. Semeval-2012 Task 5:
Chinese Semantic Dependency Parsing. In Proceed-
ings of the 6th International Workshop on Semantic
Evaluation (SemEval 2012).
518
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 715?720,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
ICT: A Translation based Method for Cross-lingual Textual Entailment 
 
 
Fandong Meng, Hao Xiong and Qun Liu 
Key Lab. of Intelligent Information Processing 
Institute of Computing Technology 
Chinese Academy of Sciences 
P.O. Box 2704, Beijing 100190, China 
{mengfandong,xionghao,liuqun}@ict.ac.cn 
 
 
 
 
 
 
Abstract 
In this paper, we present our system descrip-
tion in task of Cross-lingual Textual Entail-
ment. The goal of this task is to detect 
entailment relations between two sentences 
written in different languages. To accomplish 
this goal, we first translate sentences written 
in foreign languages into English. Then, we 
use EDITS1, an open source package, to rec-
ognize entailment relations. Since EDITS only 
draws monodirectional relations while the task 
requires bidirectional prediction, thus we ex-
change the hypothesis and test to detect en-
tailment in another direction. Experimental 
results show that our method achieves promis-
ing results but not perfect results compared to 
other participants. 
1 Introduction 
In Cross-Lingual Textual Entailment task (CLTE) 
of 2012, the organizers hold a task for Cross-
Lingual Textual Entailment. The Cross-Lingual 
Textual Entailment task addresses textual entail-
ment (TE) recognition under a new dimension 
(cross-linguality), and within a new challenging 
application scenario (content synchronization) 
Readers can refer to M. Negri et al 2012.s., for 
more detailed introduction. 1 
Textual entailment, on the other hand, recog-
nize, generate, or extract pairs of natural language 
expressions, and infer that if one element is true, 
whether the other element is also true. Several 
methods are proposed by previous researchers. 
There have been some workshops on textual en-
tailment in recent years. The recognizing textual 
entailment challenges (Bar-Haim et al 2006; 
Giampiccolo, Magnini, Dagan, & Dolan, 2007; 
Giampiccolo, Dang, Magnini, Dagan, & Dolan, 
2008), currently in the 7th year, provide additional 
significant thrust. Consequently, there are a large 
number of published articles, proposed methods, 
and resources related to textual entailment. A spe-
cial issue on textual entailment was also recently 
published, and its editorial provides a brief over-
view of textual entailment methods (Dagan, Dolan, 
Magnini, & Roth, 2009).  
Textual entailment recognizers judge whether 
or not two given language expressions constitute a 
correct textual entailment pair. Different methods 
may operate at different levels of representation of 
the input expressions. For example, they may treat 
the input expressions simply as surface strings, 
they may operate on syntactic or semantic repre-
sentations of the input expressions, or on represen-
tations combining information from different 
                                                          
1http://edits.fbk.eu/ 
715
levels. Logic-based approach is to map the lan-
guage expressions to logical meaning representa-
tions, and then rely on logical entailment checks, 
possibly by invoking theorem provers (Rinaldi et 
al., 2003; Bos & Markert, 2005; Tatu & Moldovan, 
2005, 2007). An alternative to use logical meaning 
representations is to start by mapping each word of 
the input language expressions to a vector that 
shows how strongly the word co-occurs with par-
ticular other words in corpora (Lin, 1998b), possi-
bly also taking into account syntactic information, 
for example requiring that the co-occurring words 
participate in particular syntactic dependencies 
(Pad?o & Lapata, 2007). Several textual entailment 
recognizing methods operate directly on the input 
surface strings. For example, they compute the 
string edit distance (Levenshtein, 1966) of the two 
input strings, the number of their common words, 
or combinations of several string similarity 
measures (Malakasiotis & Androutsopoulos, 2007). 
Dependency grammar parsers (Melcuk, 1987; Ku-
bler, McDonald, & Nivre, 2009) are popular in 
textual entailment research. However, cross-lingual 
textual entailment brings some problems on past 
algorithms. On the other hand, many methods can?t 
be applied to it directly.  
In this paper, we propose a translation based 
method for cross-lingual textual entailment, which 
has been described in Mehdad et al 2010. First, we 
translate one part of the text, which termed as ?t1? 
and written in one language, into English, which 
termed as ?t2?. Then, we use EDITS, an open 
source package, to recognize entailment relations 
between two parts. Large-scale experiments are 
conducted on four language pairs, French-English, 
Spanish-English, Italian-English and German-
English. Although our method achieves promising 
results reported by organizers, it is still far from 
perfect compared to other participants. 
The remainder of this paper is organized as 
follows. We describe our system framework in 
section 2. We report experimental results in section 
3 and draw our conclusions in the last section. 
2 System Description 
Figure 1 illustrates the overall framework of our 
system, where a machine translation model is em-
ployed to translate foreign language into English, 
since original EDITS could only deal with the text 
in the same language pairs.  
   In the following of this section, we will de-
scribe the translation module and configuration of 
EDITS in details. 
 
Figure 1:  The framework of our system. 
 
2.1 Machine Translation 
Recently, machine translation has attracted inten-
sive attention and has been well studied in natural 
language community. Effective models, such as 
Phrase-Based model (Koehn et al, 2003), Hierar-
chical Phrase-Based model (HPB) (Chiang, 2005), 
and Syntax-Based (Liu et al, 2006) model have 
been proposed to improve the translation quality. 
However, since current translation models require 
parallel corpus to extract translation rules, while 
parallel corpus on some language pairs such as 
Italian-English and Spanish-English are hard to 
obtain, therefore, we could use Google Translation 
Toolkit (GTT) to generate translation. 
Specifically, WMT 2 released some bilingual 
corpus for training, thus we use some portion to 
train a French-English translation engine using 
hierarchical phrase-based model. We also exploit 
system combination technique (A Rosti et al, 2007) 
to improve translation quality via blending the 
translation of our models and GTT?s. It is worth 
noting that GTT only gives 1-best translation, thus 
we duplicate 50 times to generate 50-best for sys-
tem combination.  
                                                          
2  http://www.statmt.org/wmt12/ 
716
2.2 Textual Entailment 
Many methods have been proposed to recognize 
textual entailment relations between two expres-
sions written in the same language. Since edit dis-
tance algorithms are effective on this task, we 
choose this method. And we use popular toolkit, 
EDITS, to accomplish the textual entailment task. 
EDITS is an open source software, which is 
used for recognizing entailment relations between 
two parts of text, termed as ?T? and ?H?. The sys-
tem is based on the edit distance algorithms, and 
computes the ?T?-?H? distance as the cost of the 
edit operations (i.e. insertion, deletion and substitu-
tion) that are necessary to transform ?T? into ?H?. 
EDITS requires that three modules are defined: an 
edit distance algorithm, a cost scheme for the three 
edit operations, and a set of rules expressing either 
entailment or contradiction. Each module can be 
easily configured by the user as well as the system 
parameters. EDITS can work at different levels of 
complexity, depending on the linguistic analysis 
carried on over ?T? and ?H?. Both linguistic pro-
cessors and semantic resources that are available to 
the user can be integrated within EDITS, resulting 
in a flexible, modular and extensible approach to 
textual entailment. 
 
 
Figure 2: An Example of two expressions 
EDITS can recognize.  
 
Figure 2 shows an example of two expressions 
that EDITS can recognize. EDITS will give an an-
swer that whether expression ?H? is true given that 
expression ?T? is true. The result is a Boolean val-
ue. If ?H? is true given ?T? is true, then the result 
is ?YES?, otherwise ?NO?. 
EDITS implements a distance-based frame-
work which assumes that the probability of an en-
tailment relation between a given ?T?-?H? pair is 
inversely proportional to the distance between ?T? 
and ?H? (i.e. the higher the distance, the lower is 
the probability of entailment). Within this frame-
work the system implements and harmonizes dif-
ferent approaches to distance computation, 
providing both edit distance algorithms, and simi-
larity algorithms. Each algorithm returns a normal-
ized distance score (a number between 0 and 1). At 
a training stage, distance scores calculated over 
annotated ?T?-?H? pairs are used to estimate a 
threshold that best separates positive from negative 
examples. The threshold, which is stored in a 
Model, is used at a test stage to assign an entail-
ment judgment and a confidence score to each test 
pair. 
 
 
Figure 3: Our configured file for training 
 
Figure 3 shows our configuration file for train-
ing models, we choose ?distance? algorithm in 
EDITS, and ?default_matcher?, and ?ignore_case? , 
and some other default but effective configured 
parameters. 
 
 
Figure 4: The overall training and decoding 
procedure in our system. 
 
Figure 4 shows our training and decoding 
procedure. As EDITS can only recognize textual 
entailment from one part to the other, we manually 
change the tag ?H? with ?T?, and generate the re-
sults again, and then compute two parts? entailment 
relations. For example, if ?T?-?H? is ?YES?, and 
?H?-?T? is ?NO?, then the entailment result be-
tween them is ?forward?; if ?T?-?H? is ?NO?, and 
?H?-?T? is ?YES?, then the entailment result be-
tween them is ?backward?; if both  ?T?-?H? and 
?H?-?T? are ?YES?, the result is ?bidirectional?; 
717
otherwise ?no_entailment?. 
3 Experiments and Results 
Since organizers of SemEval 2012 task 8 supply a 
piece of data for training, we thus exploit it to op-
timize parameters for EDITS. Table 1 shows the F-
measure score of training set analyzed by EDITS, 
where ?FE? represents French-English, ?SE? rep-
resents Spanish-English, ?IE? represents Italian-
English and ?GE? represents Italian-English.  
 
Judgment  FE SE IE GE 
forward 
backward 
no_entailment 
bidirectional 
Overall 
0.339 
0.611 
0.533 
0.515 
0.516 
0.373 
0.574 
0.535 
0.502 
0.506 
0.440 
0.493 
0.494 
0.506 
0.488 
0.327 
0.552 
0.494 
0.495 
0.482 
Table 1:  Results on training set. 
 
From Table 1, we can see that the perfor-
mance of ?forward? prediction is lower than others. 
One explanation is that the ?T? is translated from 
foreign language, which is error unavoidable. Thus 
some rules used for checking ?T?, such as stop-
word list will be disabled. Then it is possible to 
induce a ?NO? relation between ?T? and ?H? that 
results in lower recall of ?forward?. 
Since for French-English, we build a system 
combination for improving the quality of transla-
tion. Table 2 shows the results of BLEU score of 
translation quality, and F-score of entailment 
judgment.  
 
System  BLEU4 F-score 
HPB 
GTT 
COMB 
28.74 
30.08 
30.57 
0.496 
0.508 
0.516 
Table 2:  Performance of different translation 
model, where COMB represents system com-
bination. 
 
From table 2, we find that the translation qual-
ity slightly affect the correctness of entailment 
judgment. However, the difference of performance 
in entailment judgment is smaller than that in 
translation quality. We explain that the translation 
models exploit phrase-based rules to direct the 
translation, and the translation errors mainly come 
from the disorder between each phrases.  While a 
distance based entailment model generally consid-
ers the similarity of phrases between test and hy-
pothesis, thus the disorder of phrases influences the 
judgment slightly.   
Using the given training data for tuning pa-
rameters, table 3 to table 6 shows the detailed ex-
perimental results on testing data, where P 
represents precision and R indicates recall, and 
both of them are calculated by given evaluation 
script. 
  
French -- English 
Judgment P R F-measure 
forward 
backward 
no_entailment 
bidirectional 
Overall 
Best System 
0.750 
0.517 
0.385 
0.444 
0.192 
0.496 
0.656 
0.480 
0.306 
0.506 
0.485 
0.462 
0.456 
0.570 
 Table 3: Test results on French-English 
 
Spanish -- English 
Judgment  P R F-measure 
forward 
backward 
no_entailment 
bidirectional 
Overall 
Best System 
0.750 
0.440 
0.395 
0.436 
0.240 
0.472 
0.560 
0.520 
0.364 
0.456 
0.464 
0.474 
0.448 
0.632 
Table 4: Test results on Spanish-English 
  
Italian ? English 
Judgment  P R F-measure 
forward 
backward 
no_entailment 
bidirectional 
Overall 
Best System 
0.661 
0.554 
0.427 
0.383 
0.296 
0.368 
0.448 
0.704 
0.409 
0.442 
0.438 
0.496 
0.454 
0.566 
 Table 5: Test results on Italian-English 
 
German ? English 
Judgment  P R F-measure 
forward 
backward 
no_entailment 
bidirectional 
Overall 
Best System 
0.718 
0.493 
0.390 
0.439 
0.224 
0.552 
0.512 
0.552 
0.341 
0.521 
0.443 
0.489 
0.460 
0.558 
Table 6: Test results on German-English 
718
 
After given golden testing reference, we also 
investigate the effect of training set to testing set. 
We choose testing set from RTE1 and RTE2, both 
are English text, as our training set for optimiza-
tion of EDITS, and the overall results are shown in 
table 7 to table 10, where CLTE is training set giv-
en by this year?s organizers. 
 
French -- English 
Judgment  CLTE RTE1 RTE2 
forward 
backward 
no_entailment 
bidirectional 
Overall 
0.306 
0.506 
0.485 
0.462 
0.456 
0.248 
0.425 
0.481 
0.472 
0.430 
0.289 
0.440 
0.485 
0.485 
0.444 
Table 7: Test results on French-English 
given different training set. 
 
Spanish ? English 
Judgment  CLTE RTE1 RTE2 
forward 
backward 
no_entailment 
bidirectional 
Overall 
0.364 
0.456 
0.464 
0.474 
0.448 
0.293 
0.332 
0.386 
0.484 
0.400 
0.297 
0.372 
0.427 
0.503 
0.424 
Table 8: Test results on Spanish-English 
given different training set. 
 
Italian -- English 
Judgment  CLTE RTE1 RTE2 
forward 
backward 
no_entailment 
bidirectional 
Overall 
0.409 
0.442 
0.438 
0.496 
0.454 
0.333 
0.394 
0.410 
0.474 
0.420 
0.335 
0.436 
0.421 
0.480 
0.432 
Table 9: Test results on Italian-English 
given different training set. 
 
German ? English 
Judgment  CLTE RTE1 RTE2 
forward 
backward 
no_entailment 
bidirectional 
Overall 
0.341 
0.521 
0.443 
0.489 
0.460 
0.377 
0.372 
0.437 
0.487 
0.434 
0.425 
0.460 
0.457 
0.508 
0.470 
Table 10: Test results on German-English 
given different training set. 
 
Results in table 7 and table 8 shows that mod-
els trained on ?CLTE? have better performance 
than those trained on RTE1 and RTE2, except ?bi-
directional? judgment type. In Table 9, all results 
decoding by models trained on ?CLTE? are the 
best. And in Table 10, only a few results decoding 
by models trained on ?RTE1? and ?RTE2? have 
higher score. The reason may be that, the test cor-
pora are bilingual, there are some errors in the ma-
chine translation procedure when translate one part 
of the test from its language into the other. When 
training on these bilingual text and decoding these 
bilingual text, these two procedure have error con-
sistency. Some errors may be counteracted. If we 
train on RTE, a standard monolingual text, and 
decode a bilingual text, more errors may exist be-
tween the two procedures. So we believe that, if 
we use translation based strategy (machine transla-
tion and monolingual textual entailment) to gener-
ate cross-lingual textual entailment, we should use 
translation based strategy to train models, rather 
than use standard monolingual texts. 
4 Conclusion 
In this paper, we demonstrate our system frame-
work for this year?s cross-lingual textual entail-
ment task. We propose a translation based model 
to address cross-lingual entailment. We first trans-
late all foreign languages into English, and then 
employ EDITS to induce entailment relations. Ex-
periments show that our method achieves promis-
ing results but not perfect results compared to other 
participants. 
Acknowledgments 
The authors were supported by National Science 
Foundation of China, Contracts 90920004, and 
High-Technology R&D Program (863) Project No 
2011AA01A207 and 2012BAH39B03.  We thank 
organizers for their generous supplied resources 
and arduous preparation. We also thank anony-
mous reviewers for their thoughtful suggestions. 
References  
Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampic-
colo, D., Magnini, B., & Szpektor, I. 2006.The 2nd 
PASCAL recognising textual entailment challenge. In 
Proc. of the 2nd PASCAL ChallengesWorkshop on 
Recognising Textual Entailment, Venice, Italy. 
719
Bos, J., & Markert, K. 2005. Recognising textual en-
tailment with logical inference. In Proc. Of the Conf. 
on HLT and EMNLP, pp. 628?635, Vancouver, BC, 
Canada. 
Dagan, I., Dolan, B., Magnini, B., & Roth, D. 2009. 
Recognizing textual entailment: Rational,evaluation 
and approaches. Nat. Lang. Engineering, 15(4), i?
xvii. Editorial of the special issue on Textual Entail-
ment. 
David Chiang. 2005. A hierarchical phrase-based model 
for statistical machine translation. In Proceedings of 
ACL 2005, pages 263?270. 
Giampiccolo, D., Dang, H., Magnini, B., Dagan, I., & 
Dolan, B. 2008. The fourth PASCAL recognizing tex-
tual entailment challenge. In Proc. of the Text Anal-
ysis Conference, pp. 1?9, Gaithersburg, MD. 
Giampiccolo, D., Magnini, B., Dagan, I., & Dolan, B. 
2007. The third PASCAL recognizing textual entail-
ment challenge. In Proc. of the ACL-Pascal Work-
shop on Textual Entailment and Paraphrasing, pp. 1?
9, Prague, Czech Republic. 
I. Dagan and O. Glickman.2004. Probabilistic Textual 
Entailment: Generic Applied Modeling of Language 
Variability. Proceedings of the PASCAL Workshop 
of Learning Methods for Text Understanding and 
Mining. 
Ion Androutsopoulos and Prodromos Malakasiotis. 
2010.A Survey of Paraphrasing and Textual Entail-
ment Methids. Journal of Artificial Intelligence Re-
search, 32, 135-187. 
Kouylekov, M. and Negri, M. 2010. An open-source 
package for recognizing textual entailment. Proceed-
ings of the ACL 2010 System Demonstrations, 42-47. 
Kubler, S., McDonald, R., & Nivre, J. 2009. Dependen-
cy Parsing. Synthesis Lectures on HLT. Morgan and 
Claypool Publishers. 
Levenshtein, V. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet 
Physice-Doklady, 10, 707?710. 
Lin, D. 1998b. An information-theoretic definition of 
similarity. In Proc. of the 15th Int. Conf. on Machine 
Learning, pp. 296?304, Madison, WI. Morgan 
Kaufmann, San Francisco, CA. 
Malakasiotis, P., & Androutsopoulos, I. 2007. Learning 
textual entailment using SVMs and string similarity 
measures. In Proc. of the ACL-PASCAL Workshop 
on Textual Entailment and Paraphrasing, pp. 42?47, 
Prague. ACL. 
Mehdad, Y. and Negri, M. and Federico, M.2010. To-
wards Cross-Lingual Textual Entailment. Human 
Language Technologies.The 2010 Annual Confer-
ence of the NAACL. 321-324. 
Mehdad, Y. and Negri, M. and Federico, M.2011. Using 
bilingual parallel corpora for cross-lingual textual 
entailment. Proceedings of ACL-HLT 
Melcuk, I. 1987. Dependency Syntax: Theory and Prac-
tice. State University of New York Press. 
M. Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, and 
D. Giampiccolo.2012. Semeval-2012 Task 8: Cross-
ligual Textual Entailment for Content Synchronizatio
n. In Proceedings of the 6th International Workshop 
on Semantic Evaluation (SemEval 2012).  
Negri, M. and Bentivogli, L. and Mehdad, Y. and 
Giampiccolo, D. and Marchetti, A.2011. Divide and 
conquer: crowdsourcing the creation of cross-lingual 
textual entailment corpora. Proceedings of the Con-
ference on Empirical Methods in Natural Language 
Processing. 
Pad?o, S., & Lapata, M. 2007. Dependency-based con-
struction of semantic space models. Comp. Ling., 
33(2), 161?199. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical phrase-based translation. In Proceedings 
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Edmonton, 
Canada, July. 
Rinaldi, F., Dowdall, J., Kaljurand, K., Hess, M., & 
Molla, D. 2003. Exploiting paraphrases in a question 
answering system. In Proc. of the 2nd Int. Workshop 
in Paraphrasing, pp. 25?32, Saporo, Japan. 
Rosti, A. and Matsoukas, S. and Schwartz, R. Improved 
word-level system combination for machine transla-
tion, ANNUAL MEETING-ASSOCIATION FOR 
COMPUTATIONAL LINGUISTICS,2007 
Tatu, M., & Moldovan, D. 2005. A semantic approach 
to recognizing textual entailment. In Proc. of the 
Conf. on HLT and EMNLP, pp. 371?378, Vancouver, 
Canada. 
Tatu, M., & Moldovan, D. 2007. COGEX at RTE 3. In 
Proc. of the ACL-PASCAL Workshop on Textual 
Entailment and Paraphrasing, pp. 22?27, Prague, 
Czech Republic. 
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree?to 
string alignment template for statistical machine 
translation. In Proceedings of ACL 2006, pages 609?
616, Sydney, Australia, July. 
720
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 76?80,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
ETS: An Error Tolerable System for Coreference Resolution
Hao Xiong , Linfeng Song , Fandong Meng , Yang Liu , Qun Liu and Yajuan Lu?
Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{xionghao,songlinfeng,mengfandong,yliu,liuqun,lvyajuan}@ict.ac.cn
Abstract
This paper presents our error tolerable sys-
tem for coreference resolution in CoNLL-
2011(Pradhan et al, 2011) shared task (closed
track). Different from most previous reported
work, we detect mention candidates based on
packed forest instead of single parse tree, and
we use beam search algorithm based on the
Bell Tree to create entities. Experimental re-
sults show that our methods achieve promising
results on the development set.
1 Introduction
Over last decades, there has been increasing inter-
est on coreference resolution within NLP commu-
nity. The task of coreference resolution is to iden-
tify expressions in a text that refer to the same dis-
course entity. This year, CoNLL1 holds a shared
task aiming to model unrestricted coreference in
OntoNotes.2 The OntoNotes project has created a
large-scale, accurate corpus for general anaphoric
coreference that covers entities and events not lim-
ited to noun phrases or a limited set of entity types.
And Pradhan et al (2007) have ever used this corpus
for similar unrestricted coreference task.
Our approach to this year?s task could be divided
into two steps: mention identification and creation
of entities. The first stage is conducted on the anal-
ysis of parse trees produced by input data. The of-
ficial data have provided gold and automatic parse
trees for each sentences in training and development
1http://conll.bbn.com/
2http://www.bbn.com/ontonotes/
set. However, according to statistics, almost 3%
mentions have no corresponding constituents in au-
tomatic parse trees. Since only automatic parse trees
will be provided in the final test set, the effect of
parsing errors are inevitable. To alleviate this issue,
based on given automatic parse trees, we modify a
state-of-the-art parser (Charniak and Johnson, 2005)
to generate packed forest, and determine mention
candidates among all constituents from both given
parse tree and packed forest. The packed forest is a
compact representation of all parse trees for a given
sentence. Readers can refer to (Mi et al, 2008) for
detailed definitions.
Once the mentions are identified, the left step is
to group mentions referring to same object into sim-
ilar entity. This problem can be viewed as binary
classification problem of determining whether each
mention pairs corefer. We use a Maximum Entropy
classifier to predict the possibility that two mentions
refer to the similar entity. And mainly following the
work of Luo et al (2004), we use a beam search
algorithm based on Bell Tree to obtain the global
optimal classification.
As this is the first time we participate competi-
tion of coreference resolution, we mainly concen-
trate on developing fault tolerant capability of our
system while omitting feature engineering and other
helpful technologies.
2 Mention Detection
The first step of the coreference resolution tries to
recognize occurrences of mentions in documents.
Note that we recognize mention boundaries only on
development and test set while generating training
76
Figure 1: Left side is parse tree extracted from develop-
ment set, and right side is a forest. ?my daughter? is a
mention in this discourse, however it has no correspond-
ing constituent in parse tree, but it has a corresponding
constituent NP0 in forest.
instances using gold boundaries provided by official
data.
The first stage of our system consists of following
three successive steps:
? Extracting constituents annotated with NP,
NNP, PRP, PRP$ and VBD POS tags from sin-
gle parse tree.
? Extracting constituents with the same tags as
the last step from packed forest.
? Extracting Named Entity recognized by given
data.
It is worth mentioning that above three steps will
produce duplicated mentions, we hence collect all
mentions into a list and discard duplicated candi-
dates. The contribution of using packed forest is that
it extends the searching space of mention candidates.
Figure 1 presents an example to explain the advan-
tage of employing packed forest to enhance the men-
tion detection process. The left side of Figure 1 is
the automatic parse tree extracted from development
set, in which mention ?my daughter? has no corre-
sponding constituent in its parse tree. Under nor-
mal strategy, such mention will not be recognized
and be absent in the clustering stage. However, we
find that mention has its constituent NP0 in packed
forest. According to statistics, when using packed
forest, only 0.5% mentions could not be recognized
while the traditional method is 3%, that means the
theoretical upper bound of our system reaches 99%
compared to baseline?s 97%.
Since the requirement of this year?s task is
to model unrestricted coreference, intuitively, we
should not constraint in recognizing only noun
phrases but also adjective phrase, verb and so on.
However, we find that most mentions appeared in
corpus are noun phrases, and our experimental re-
sults indicate that considering constituents annotated
with above proposed POS tags achieve the best per-
formance.
3 Determining Coreference
This stage is to determine which mentions belong to
the same entity. We train a Maximum Entropy clas-
sifier (Le, 2004) to decide whether two mentions are
coreferent. We use the method proposed by Soon, et
al.?s to generate the training instances, where a posi-
tive instance is formed between current mention Mj
and its closest preceding antecedent Mi, and a neg-
ative instance is created by paring Mj with each of
the intervening mentions, Mi+1, Mi+2,...,Mj?1.
We use the following features to train our classi-
fier.
Features in Soon et al?s work (Soon et al, 2001)
Lexical features
IS PREFIX: whether the string of one mention is
prefix of the other;
IS SUFFIX: whether the string of one mention is
suffix of the other;
ACRONYM: whether one mention is the acronym
of the other;
Distance features
SENT DIST: distance between the sentences con-
taining the two mentions;
MEN DIST: number of mentions between two
mentions;
Grammatical features
IJ PRONOUN: whether both mentions are pro-
noun;
I NESTED: whether mention i is nested in an-
other mention;
J NESTED: whether mention j is nested in an-
other mention;
Syntax features
HEAD: whether the heads of two mentions have
the same string;
HEAD POS: whether the heads of two mentions
have the same POS;
HEA POS PAIRS: pairs of POS of the two men-
tions? heads;
77
Semantic features
WNDIST: distance between two mentions in
WordNet;
I ARG0: whether mention i has the semantic role
of Arg0;
J ARG0: whether mention j has the semantic role
of Arg0;
IJ ARGS: whether two mentions have the seman-
tic roles for similar predicate;
In the submitted results, we use the L-BFGS pa-
rameter estimation algorithm with gaussian prior
smoothing (Chen and Rosenfeld, 1999). We set the
gaussian prior to 2 and train the model in 100 itera-
tions.
3.1 Creation of Entities
This stage aims to create the mentions detected in
the first stage into entities, according to the predic-
tion of classifier. One simple method is to use a
greedy algorithm, by comparing each mention to its
previous mentions and refer to the one that has the
highest probability. In principle, this algorithm is
too greedy and sometimes results in unreasonable
partition (Ng, 2010). To address this problem, we
follow the literature (Luo et al, 2004) and propose
to use beam search to find global optimal partition.
Intuitively, creation of entities can be casted as
partition problem. And the number of partitions
equals the Bell Number (Bell, 1934), which has a
?closed? formula B(n) = 1e
??
k=0
kn
k! . Clearly, this
number is very huge when n is large, enumeration of
all partitions is impossible, so we instead designing
a beam search algorithm to find the best partition.
Formally, the task is to optimize the following ob-
jective,
y? = argmax
??P
?
e??
Prob(e) (1)
where P is all partitions, Prob(e) is the cost of
entity e. And we can use the following formula to
calculate the Prob(e),
Prob(e) =
?
i?e,j?e
pos(mi,mj)
+
?
i?e,j /?e
neg(mi,mj)
(2)
where pos(mi,mj) is the score predicted by clas-
sifier that the possibility two mentions mi and mj
group into one entity, and neg(mi,mj) is the score
that two mentions are not coreferent.
Theoretically, we can design a dynamic algorithm
to obtain the best partition schema. Providing there
are four mentions from A to D, and we have ob-
tained the partitions of A, B and C. To incorporate
D, we should consider assigning D to each entity of
every partition, and generate the partitions of four
mentions. For detailed explanation, the partitions
of three mentions are [A][B][C], [AB][C], [A][BC]
and [ABC], when considering the forth mention D,
we generate the following partitions:
? [A][B][C][D], [AD][B][C], [A][BD][C],
[A][B][CD]
? [AB][C][D], [ABD][C],[AB][CD]
? [A][BC][D], [AD][BC], [A][BCD]
? [ABC][D], [ABCD]
The score of partition [AD][B][C] can be
calculated by score([A][B][C]) + pos(A,D) +
neg(B,D) + neg(C,D). Since we can computer
pos and neg score between any two mentions in
advance, this problem can be efficiently solved by
dynamic algorithm. However, in practice, enumer-
ating the whole partitions is intractable, we instead
exploiting a beam with size k to store the top k parti-
tions of current mention size, according to the score
the partition obtain. Due to the scope limitation, we
omit the detailed algorithm, readers can refer to Luo
et al (2004) for detailed description, since our ap-
proach is almost similar to theirs.
4 Experiments
4.1 Data Preparation
The shared task provided data includes information
of lemma, POS, parse tree, word sense, predicate
arguments, named entity and so on. In addition to
those information, we use a modified in house parser
to generate packed forest for each sentence in devel-
opment set, and prune the packed forest with thresh-
old p=3 (Huang, 2008). Since the OntoNotes in-
volves multiple genre data, we merge all files and
78
Mention MUC BCUBED CEAFM CEAFE BLANC
baseline 58.97% 44.17% 63.24% 45.08% 37.13% 62.44%
baseline gold 59.18% 44.48% 63.46% 45.37% 37.47% 62.36%
sys forest 59.07% 44.4% 63.39% 45.29% 37.41% 62.41%
sys btree 59.44% 44.66% 63.77% 45.62% 37.82% 62.47%
sys forest btree 59.71% 44.97% 63.95% 45.91% 37.96% 62.52%
Table 1: Experimental results on development set (F score).
Mention MUC BCUBED CEAFM CEAFE BLANC
sys1 54.5% 39.15% 63.91% 45.32% 37.16% 63.18%
sys2 53.06% 35.55% 59.68% 38.24% 32.03% 50.13%
Table 2: Experimental results on development set with different training division (F score).
take it as our training corpus. We use the sup-
plied score toolkit 3 to compute MUC, BCUBED,
CEAFM, CEAFE and BLANC metrics.
4.2 Experimental Results
We first implement a baseline system (baseline)
that use single parse tree for mention detection
and greedy algorithm for creation of entities. We
also run the baseline system using gold parse tree,
namely baseline gold. To investigate the contribu-
tion of packed forest, we design a reinforced sys-
tem, namely sys forest. And another system, named
as sys btree, is used to see the contribution of beam
search with beam size k=10. Lastly, we combine
two technologies and obtain system sys forest btree.
Table 1 shows the experimental results on devel-
opment data. We find that the system using beam
search achieve promising improvement over base-
line. The reason for that has been discussed in last
section. We also find that compared to baseline,
sys forest and baseline gold both achieve improve-
ment in term of some metrics. And we are glad to
find that using forest, the performance of our sys-
tem is approaching the system based on gold parse
tree. But even using the gold parse tree, the im-
provement is slight. 4 One reason is that we used
some lexical and grammar features which are dom-
3http://conll.bbn.com/download/scorer.v4.tar.gz
4Since under task requirement, singleton mentions are fil-
tered out, it is hard to recognize the contribution of packed for-
est to mention detection, while we may incorrectly resolve some
mentions into singletons that affects the score of mention detec-
tion.
inant during prediction, and another explanation is
that packed forest enlarges the size of mentions but
brings difficulty to resolve them.
To investigate the effect of different genres to de-
velop set, we also perform following compared ex-
periments:
? sys1: all training corpus + WSJ development
corpus
? sys2: WSJ training corpus + WSJ development
corpus
Table 2 indicates that knowledge from other genres
can help coreference resolution. Perhaps the reason
is the same as last experiments, where syntax diver-
sity affects the task not very seriously.
5 Conclusion
In this paper, we describe our system for CoNLL-
2011 shared task. We propose to use packed for-
est and beam search to improve the performance of
coreference resolution. Multiple experiments prove
that such improvements do help the task.
6 Acknowledgement
The authors were supported by National Natural
Science Foundation of China, Contracts 90920004.
We would like to thank the anonymous reviewers
for suggestions, and SHUGUANG COMPUTING
PLATFORM for supporting experimental platform.
79
References
E.T. Bell. 1934. Exponential numbers. The American
Mathematical Monthly, 41(7):411?419.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 173?180.
Association for Computational Linguistics.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical report, CMU-CS-99-108.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594, Columbus, Ohio, June.
Z. Le. 2004. Maximum entropy modeling toolkit for
Python and C++.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, pages 135?es. As-
sociation for Computational Linguistics.
H. Mi, L. Huang, and Q. Liu. 2008. Forestbased transla-
tion. In Proceedings of ACL-08: HLT, pages 192?199.
Citeseer.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1396?1411, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted Coreference: Identifying Entities and Events
in OntoNotes. In in Proceedings of the IEEE Inter-
national Conference on Semantic Computing (ICSC),
September 17-19.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
80
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 71?75,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
ICT: System Description for CoNLL-2012
Hao Xiong and Qun Liu
Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{xionghao, liuqun}@ict.ac.cn
Abstract
In this paper, we present our system de-
scription for the CoNLL-2012 coreference
resolution task on English, Chinese and
Arabic. We investigate a projection-based
model in which we first translate Chinese
and Arabic into English, run a publicly
available coreference system, and then use
a new projection algorithm to map the
coreferring entities back from English in-
to mention candidates detected in the Chi-
nese and Arabic source. We compare to
a baseline that just runs the English coref-
erence system on the supplied parses for
Chinese and Arabic. Because our method
does not beat the baseline system on the
development set, we submit outputs gen-
erated by the baseline system as our final
submission.
1 Introduction
Modeling multilingual unrestricted coreference in
the OntoNotes data is the shared task for CoNLL-
2012. This is an extension of the CoNLL-
2011 shared task and would involve automatic
anaphoric mention detection and coreference res-
olution across three languages ? English, Chinese
and Arabic ? using OntoNotes v5.0 corpus, giv-
en predicted information on the syntax, proposi-
tion, word sense and named entity layers. Au-
tomatic identification of coreferring entities and
events in text has been an uphill battle for sev-
eral decades, partly because it can require world
knowledge which is not well-defined and partly
owing to the lack of substantial annotated data.
Figure 1: The overall process of our system, where
we use Google Translator to translate Chinese and
Arabic into English.
For more details, readers can refer to (Pradhan et
al., 2012).
Before this year?s task, researchers proposed t-
wo typical novel methods to address the prob-
lem of natural language processing across multiple
languages: projection and joint learning (Rahman
and Ng, 2012). Specific to this year?s coreference
resolution task, for projection based method, we
could first develop a strong resolver or utilize a
publicly available system on English, and trans-
late other languages into English, eventually, we
could project the coreferring entities resolved on
English back into other language sides. General-
ly, a projection method is easier to develop since
it doesn?t need sentence alignment across multiple
languages. Thus, in this year?s task, we investigate
a translation based model to resolve coreference
on English, Chinese and Arabic. The whole pro-
cess is illustrated in figure 1, in which we first use
Google Translator to translate Chinese and Ara-
bic into English, and we then employ a strong En-
glish coreference resolver to generate coreferring
entities, after mapping entities from English into
71
Chinese and Arabic mention candidates, we could
obtain coreferring entities for these languages.
Intuitively, the performance of coreference re-
solver on English should perform better than that
on Chinese and Arabic since we have substantial
corpus for English and coreference resolution on
English is well studied compared to another two
languages. Thus we could imagine that projecting
the results from English into Chinese and Arabic
should still beats the baseline system using mono-
lingual resolution method. However, in our exper-
iments, we obtain negative results on developing
set that means our projection based model perfor-
m worse than the baseline system. According to
our experimental results on developing set, final-
ly, we submit results of baseline system in order to
obtain better ranking.
The rest of this paper is organized as follows, in
section 2, we will introduce our method in details,
and section 3 is our experimental results, we draw
conclusion in section 4.
2 Projection based Model
As the last section mentioned, we propose to use
a projection based model to resolve coreference
on multiple languages. The primary procedures
of our method could be divided into three steps:
first step is translation, where Google Translator is
employed to translate Chinese and Arabic into En-
glish, second is coreference resolution for English,
last is the projection of coreferring entities. Since
the first step is clear that we extract sentences from
Chinese and Arabic documents and translate them
into English using Google Translator, hence in this
section we will mainly describe the configuration
of our English resolver and details of projection
method.
2.1 English Resolver
In last year?s evaluation task, the Standford
Natural Language Processing Group ranked the
first position and they also open their toolkit for
research community, namely Standford CoreNLP
(Lee et al, 2011) 1, better yet, their toolkit is op-
timized for CoNLL task. Thus we could use their
toolkit as our English resolver and concentrate
on bettering the projection of coreferring entities.
1http://nlp.stanford.edu/software/
corenlp.shtml
Figure 2: A minimum cost and maximum flow
structure is used to solve the problem that map-
ping coreferring entities into each mention candi-
dates with highest probability.
We use the basic running script that is ?java -cp
joda-time.jar:stanford-corenlp.jar:stanford-
corenlp-models.jar:xom.jar -Xmx3g e-
du.stanford.nlp.pipeline.StanfordCoreNLP
-filelist filelist.txt? to resolve the resolution,
where ?filelist? involves all documents need to be
performed coreference resolution.
2.2 Projection of Coreferring Entities
After generating coreferring entities on English,
the key step of our system is how to map them into
mention candidates detected on Chinese and Ara-
bic. For instance, assuming we translate Chinese
documents into English and obtain coreferring en-
tities e1, e2, ei,.., eE on translated English doc-
uments through aforementioned step, meanwhile,
we consider all noun phrases(NP) in original Chi-
nese documents and generate mention candidates
m1, m2, mj ,.., mM . Therefore, our task is to map
each ei into one mention candidate mj with high-
est probability, and it can be obtained by the max-
72
Algorithm 1 Algorithm for computing similarity
between two phrases in different languages.
1: Input: we1 , .., wen , wc1 , .., wcm , Phrase Table
PT
2: s[n] = [0,? inf, ..,? inf]
3: for i? 1..n do
4: for j ? 0..10 do
5: s[i + j] = max(s[i + j], s[i ? 1] +
p(i, i + j))
6: Output: s[n]V
imization of the following formula,
P? =
?
ei?E,mj?M
{a(i, j)b(j, i)p(i, j)} (1)
with constrains
?
i,j{a(i, j)} = 1 and
?
i,j{b(j, i)} = 1, where p(i, j) is the prob-
ability of ei mapping into mj and a(i, j) as
well as b(i, j) are integers guaranteeing each
coreferring entity map into one mention and each
mention has only one entity to be mapped into.
To solve this problem, we reduce it as a Cost
Flow problem since it is easier to understand
and implement compared to other methods such
as integer linear programming. Note that the
number of mention candidates is theoretically
larger than that of coreferring entities, thus this
problem couldn?t be reduced as the bipartite graph
matching problem since it needs equal number of
nodes in two parts.
Figure 2 shows the graph structure designed to
solve this problem, where the symbols labeled on
each edge is a two tuples(Cost,Flow), indicating
the cost and flow for each edge. Since object of
Cost Flow problem is to minimize the cost while
maximizing the flows, thus we compute the c(i, j)
as 1 ? p(i, j) in order to be consistent with the
equation 1. To satisfy two constraints aforemen-
tioned, we set up two dummy nodes ?Start? and
?End?, and connect ?Start? to each entity ei with
cost 0 and flow 1 ensuring each entity is available
to map one mention. We also link each mention
candidate mj to node ?End? with the same val-
ue ensuring each mention could be mapped into
by only one entity. Clearly, there is an edge with
tuple (1?p(i, j), 1) between each entity end men-
tion indicating that each entity could map into any
mention while with different probabilities. Thus,
solving this Cost-Flow problem is equal to maxi-
mizing the equation 1 with two constraints. Since
Cost-Flow problem is well studied, thus some al-
gorithm can solve this problem in polynomial time
(Ahuja et al, 1993). One may argue that we can
modify translation decoder to output alignments
between Chinese and translated English sentence,
unfortunately, Google Translator API doesn?t sup-
ply these information while its translation quality
is obviously better than others for translating doc-
uments in OntoNotes, moreover, it is impossible to
output alignment for each word since some trans-
lation rules used for directing translation include
some unaligned words, thus an algorithm to map
each entity into each mention is more applicable.
Clearly, another problem is how to compute
p(i, j) for each edge between entity and mention
candidate. This problem could be casted as how
to compute similarity of phrases across multiple
languages. Formally, given an English phrases
we1 , .., wen and a Chinese phrase wc1 , .., wcm , the
problem is how to compute the similar score S be-
tween them. Although we could compute lexical,
syntactic or semantic similar score to obtain ac-
curate similarity, here for simplicity, we just com-
pute the lexical similarity using the phrase table
extracted by a phrased-based machine translation
decoder (Koehn et al, 2003). Phrase table is a rich
resource that contains probability score for phrase
in one language translated into another language,
thus we could design a dynamic algorithm shown
in Algorithm 1 to compute the similar score. E-
quation in line 5 is used to reserve highest simi-
lar score for its sub-phrases, and p(i, i + j) is the
similar score between sub-phrases wi, .., wi+j and
its translation. When we compute the score of the
sub-phrases wi, .., wi+j , we literately pick one pti
from PT and check whether wc1 , .., wcm involves
pti?s target side, if that we record its score un-
til we obtain a higher score obtained by another
ptj and then update it. For instance, assuming the
Chinese input sentence is ?????????
?? ?? ? ?? ? ?? ?? ??, and the
Google translation of this sentence is ?The world
?s fifth Disneyland will soon open to the public .
?. Following the aforementioned steps, we utilize
English resolver to find a coreferring entity: ?The
world ?s fifth Disneyland?, and find two translation
rules involving the former English phrase from the
73
bilingual phrase table: ?The world ?s fifth Disney-
land => ??????????? (probabili-
ty=0.6) ? and ?The world ?s fifth Disneyland =>
?????????? (probability=0.4)?. S-
ince the Chinese translation of both rules all con-
tain the noun phrase ??? ?? ? ??? ?
?? in the original Chinese input, we thus add this
noun phrase into the coreferring entities as the En-
glish resolve finding with the probability 0.6.
3 Experiments
3.1 English Results
In this section, we will report our experimental re-
sults in details. We use Standford CoreNLP toolkit
to generate results for English. Table 1 lists the F-
score obtained on developing set.
3.2 Chinese and Arabic Results
As last section mentioned, we first translate
Chinese and Arabic into English and then use
CoreNLP to resolve coreference on English. To
obtain high translation quality, we use Google
Translator Toolkit 2. And to compute similarity
score, we run Giza++(Och and Ney, 2003) 3, an
open source toolkit for word alignment, to perfor-
m word alignment. For Chinese, we use 1 million
bilingual corpus provided by NIST MT evaluation
task to extract phrase table, and for Arabic its size
is 2 million. Note that, we extract phrase table
from English to Chinese and Arabic with maxi-
mum phrase length 10. The reason is that our al-
gorithm check English phrase whose length is less
than 10 tokens. To compare our results, we al-
so use CoreNLP to generate results for Chinese
and Arabic. Since CoreNLP use some syntac-
tic knowledge to resolving coreference, it can al-
so output coreferring entities for other languages.
From table 2 we find that although CoreNLP is not
designed for other languages, it still obtain accept-
able scores and beat our projection based mod-
el. The main reason is that our method is coarse
and obtain lower precision for mention detection,
while CoreNLP use some manually written rules
to detect mention candidates. Another explana-
tion is that projection based model is hard to map
2http://www.google.cn/url?source=
transpromo&rs=rsmf&q=http://translate.
google.com/toolkit
3http://code.google.com/p/giza-pp/
some phrases back into original languages, such
as ?that, it, this?. Moreover, translation quality for
some corpus like web corpus is far from perfect,
translation errors will surely affect the precision of
coreference resolution. Thus, for the final testing
set, we run the CoreNLP to generate the results.
3.3 Testing Results
Since CoreNLP beats our system in Chinese and
Arabic, thus we run CoreNLP for all three lan-
guages. Table 3 lists the final results, and we also
give results using golden parse tree for prediction
in table 4. From these two tables, we find that for
any language, the system using golden parse tree
show better performance than the one using pre-
dicted system in term of each metric. The reason
is that the CoreNLP resolve coreference on parse
tree and employ some parse features to corefer. On
the other hand, we could also see that the improve-
ment is slight, because parsing errors affect lit-
tle on finding mention candidates benefiting from
high precision on noun phrase prediction. Final-
ly, since we use an open source toolkit to generate
results, unfortunately, we have no ranking in this
task.
4 Conclusion
In this paper, we present a projection based mod-
el for coreference resolution. We first translate
Chinese and Arabic into English, and then em-
ploy a strong English resolver to generate core-
ferring entities, after that a projection algorithm is
designed to map coreferring entities into mention
candidates detected in Chinese and Arabic. How-
ever, since our approach is coarse and due to limit
time preparing for this task, the output generate
by CoreNLP beats our results in three languages,
thus we submit results generated by CoreNLP as
our final submission.
Acknowledgments
The authors were supported by National Science
Foundation of China, Contracts 90920004, and
High-Technology R&D Program (863) Project No
2011AA01A207 and 2012BAH39B03. We thank
organizers for their generous supplied resources
and arduous preparation. We also thank anony-
mous reviewers for their thoughtful suggestions.
74
Mention MUC BCUB CEAFE
CoreNLP 73.68% 64.58% 70.60% 46.64
Table 1: Experimental results on developing set(F-score) for English.
Mention MUC BCUB CEAFE
CoreNLP-Chinese 52.15% 38.16% 60.38% 34.58
Projection-Chinese 48.51% 32.31% 63.77% 24.72
CoreNLP-Arabic 52.97% 27.88% 60.75% 40.52
Projection-Arabic 42.68% 22.39% 62.18% 32.83
Table 2: Experimental results on developing set(F-score) for Chinese and Arabic using CoreNLP and
our system.
Mention MUC BCUB CEAFE
CoreNLP-Chinese 49.82% 37.83% 60.30% 34.93
CoreNLP-Arabic 53.89% 28.31% 61.83% 42.97
CoreNLP-English 73.69% 63.82% 68.52% 45.36
Table 3: Experimental results on testing set(F-score) using predicted parse tree.
Mention MUC BCUB CEAFE
CoreNLP-Chinese 53.42% 40.60% 60.37% 35.75
CoreNLP-Arabic 55.17% 30.54% 62.36% 43.03
CoreNLP-English 75.58% 66.14% 69.55% 46.54
Table 4: Experimental results on testing set(F-score) using golden parse tree.
References
R.K. Ahuja, T.L. Magnanti, and J.B. Orlin. 1993. Net-
work flows: theory, algorithms, and applications.
1993.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology-Volume 1, pages 48?54.
Association for Computational Linguistics.
H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Sur-
deanu, and D. Jurafsky. 2011. Stanford?s multi-
pass sieve coreference resolution system at the conll-
2011 shared task. In Proceedings of the Fifteenth
Conference on Computational Natural Language
Learning: Shared Task, pages 28?34. Association
for Computational Linguistics.
F.J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional linguistics, 29(1):19?51.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unrestrict-
ed coreference in OntoNotes. In Proceedings of
the Sixteenth Conference on Computational Natural
Language Learning (CoNLL 2012), Jeju, Korea.
Altaf Rahman and Vincent Ng. 2012. Translation-
based projection for multilingual coreference reso-
lution. In NAACL 2012.
75
