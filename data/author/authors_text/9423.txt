Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 9?16
Manchester, August 2008
A Supervised Algorithm for Verb Disambiguation into VerbNet Classes
Omri Abend1 Roi Reichart2 Ari Rappoport1
1Institute of Computer Science , 2ICNC
Hebrew University of Jerusalem
{omria01|roiri|arir}@cs.huji.ac.il
Abstract
VerbNet (VN) is a major large-scale En-
glish verb lexicon. Mapping verb instances
to their VN classes has been proven use-
ful for several NLP tasks. However, verbs
are polysemous with respect to their VN
classes. We introduce a novel supervised
learning model for mapping verb instances
to VN classes, using rich syntactic features
and class membership constraints. We
evaluate the algorithm in both in-domain
and corpus adaptation scenarios. In both
cases, we use the manually tagged Sem-
link WSJ corpus as training data. For in-
domain (testing on Semlink WSJ data), we
achieve 95.9% accuracy, 35.1% error re-
duction (ER) over a strong baseline. For
adaptation, we test on the GENIA corpus
and achieve 72.4% accuracy with 10.7%
ER. This is the first large-scale experimen-
tation with automatic algorithms for this
task.
1 Introduction
The organization of verbs into classes whose mem-
bers exhibit similar syntactic and semantic behav-
ior has been discussed extensively in the linguistics
literature (see e.g. (Levin and Rappaport Hovav,
2005; Levin, 1993)). Such an organization helps
in avoiding lexicon representation redundancy and
enables generalizations across similar verbs. It
can also be of great practical use, e.g. in com-
pensating NLP statistical models for data sparse-
ness. Indeed, Levin?s seminal work had motivated
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
much research aimed at automatic discovery of
verb classes (see Section 2).
VerbNet (VN) (Kipper et al, 2000; Kipper-
Schuler, 2005) is a large scale, publicly available
domain independent verb lexicon that builds on
Levin classes and extends them with new verbs,
new classes, and additional information such as
semantic roles and selectional restrictions. VN
classes were proven beneficial for Semantic Role
Labeling (SRL) (Swier and Stevenson, 2005), Se-
mantic Parsing (Shi and Mihalcea, 2005) and
building conceptual graphs (Hensman and Dun-
nion, 2004). Levin-inspired classes have been
used in several NLP tasks, such as Machine Trans-
lation (Dorr, 1997) and Document Classification
(Klavans and Kan, 1998).
Many applications that use VN need to map verb
instances onto their VN classes. However, verbs
are polysemous with respect to VN classes. Sem-
link (Loper et al, 2007) is a dataset that maps each
verb instance in the WSJ Penn Treebank to its VN
class. The mapping has been created using a com-
bination of automatic and manual methods. Yi et
al. (2007) have used Semlink to improve SRL.
In this paper we present the first large-scale ex-
perimentation with a supervised machine learning
classification algorithm for disambiguating verb
instances to their VN classes. We use rich syntactic
features extracted from a treebank-style parse tree,
and utilize a learning algorithm capable of impos-
ing class membership constraints, thus taking ad-
vantage of the nature of our task. We use Semlink
as the training set.
We evaluate our algorithm in both in-domain
and corpus adaptation scenarios. In the former,
we test on the WSJ (using Semlink again), ob-
taining 95.9% accuracy with 35.1% error reduc-
tion (ER) over a strong baseline (most frequent
9
class) when using a modern statistical parser. In
the corpus adaptation scenario, we disambiguate
verbs in sentences taken from outside the train-
ing domain. Since the manual annotation of new
corpora is costly, and since VN is designed to be
a domain independent resource, adaptation results
are important to the usability in NLP in practice.
We manually annotated 400 sentences from GE-
NIA (Kim et al, 2003), a medical domain cor-
pus1. Testing on these, we achieved 72.4% ac-
curacy with 10.7% ER. Our adaptation scenario
is complete in the sense that the parser we use
was also trained on a different corpus (WSJ). We
also report experiments done using gold-standard
(manually created) parses.
The most relevant previous works addressing
verb instance class classification are (Lapata and
Brew, 2004; Li and Brew, 2007; Girju et al, 2005).
The former two do not use VerbNet and their ex-
periments were narrower than ours, so we can-
not compare to their results. The latter mapped to
VN, but used a preliminary highly restricted setup
where most instances were monosemous. For
completeness, we compared our method to theirs2,
achieving similar results.
We review related work in Section 2, and dis-
cuss the task in Section 3. Section 4 introduces the
model, Section 5 describes the experimental setup,
and Section 6 presents our results.
2 Related Work
VerbNet. VN is a major electronic English verb
lexicon. It is organized in a hierarchical struc-
ture of classes and sub-classes, each sub-class in-
heriting the full characterization of its super-class.
VN is built on a refinement of the Levin classes,
the intersective Levin classes (Dang et al, 1998),
aimed at achieving more coherent classes both se-
mantically and syntactically. VN was also sub-
stantially extended (Kipper et al, 2006) using the
Levin classes extension proposed in (Korhonen
and Briscoe, 2004). VN today contains 3626 verb
lemmas (forms), organized in 237 main classes
having 4991 verb types (we refer to a lemma with
an ascribed class as a type). Of the 3626 lem-
mas, 912 are polysemous (i.e., appear in more
than a single class). VN?s significant coverage of
the English verb lexicon is demonstrated by the
1Our annotations will be made available to the community.
2Using the same sentences and instances, obtained from
the authors.
75.5% coverage of VN classes over PropBank3
instances (Loper et al, 2007). Each class con-
tains rich semantic information, including seman-
tic roles of the arguments augmented with se-
lectional restrictions, and possible subcategoriza-
tion frames consisting of a syntactic description
and semantic predicates with temporal informa-
tion. VN thematic roles are relatively coarse, vs.
the situation-specific FrameNet role system or the
verb-specific PropBank role system, enabling gen-
eralizations across a wide semantic scope. Swier
and Stevenson (2005) and Yi et al (2007) used VN
for SRL.
Verb type classification. Quite a few works
have addressed the issue of verb type classification
and in particular classification to ?Levin inspired?
classes (e.g., (Schulte im Walde, 2000; Merlo and
Stevenson, 2001)). Such work is not comparable
to ours, as it deals with verb type (sense) rather
than verb token (instance) classification.
Verb token classification. Lapata and Brew
(2004) dealt with classification to Levin classes of
polysemous verbs. They established a prior from
the BNC in an unsupervised manner. They also
showed that this prior helps in the training of a
naive Bayes classifier employed to distinguish be-
tween possible verb classes of a given verb in a
given frame (when the ambiguity is not solved by
knowing the frame alone). Li and Brew (2007) ex-
tended this model by proposing a method to train
the class disambiguator without using hand-tagged
data. While these papers have good results, their
experimental setup was rather narrow and used
only at most 67 polysemous verbs (in 4 frames).
VN includes 912 polysemous verbs, of which 695
appeared in our in-domain experiments.
Girju et al (2005) performed the only previous
work we are aware of that addresses the problem of
token level verb disambiguation into VN classes.
They treated the task as a supervised learning prob-
lem, proposing features based on a POS tagger, a
Chunker and a named entity classifier. In order
to create the data4, they used a mapping between
Propbank rolesets and VN classes, and took the in-
stances in WSJ sections 15-18,20,21 that were an-
notated by Propbank and for which the roleset de-
termines the VN class uniquely. This resulted in
most instances being in fact monosemous. Their
3Propbank (Palmer et al, 2005) is a corpus annotation of
the WSJ sections of the Penn Treebank with semantic roles of
each verbal proposition.
4Semlink was not available then.
10
experiment was conducted in a WSJ in-domain
scenario, and in a much narrower scope than in
this paper. They had 870 (39 polysemous) unique
verb lemmas, compared to 2091 (695 polysemous)
in our in-domain scenario. They did not test their
model in an adaptation scenario. The scope and
difficulty contrast between our setup and theirs are
demonstrated by the large differences in the num-
ber of instances and in the percentage of polyse-
mous instances: 972/12431 (7.8%) in theirs, com-
pared to 49571/84749 (58.5%) in our in-domain
scenario (training+test). We compared our method
to theirs for completeness and achieved similar re-
sults.
Semlink. The Semlink project (Yi et al, 2007;
Loper et al, 2007) aims to create a mapping of
PropBank, FrameNet (Baker et al, 1998), Word-
Net (henceforth WN) and VN to one another, thus
allowing these resources to synergize. In addition,
the project includes the most extensive token map-
ping of verbs to their VN classes available today.
It covers all verbs in the WSJ sections of the Penn
Treebank within VN coverage (out of 113K verb
instances, 97K have lemmas present in VN).
3 Nature of the Task
Polysemy is a major issue in NLP. Verbs are not an
exception, resulting in a single verb form (lemma)
appearing in more than a single class. This pol-
ysemy is also present in the original Levin clas-
sification, where polysemous classes account for
more than 48% of the BNC verb instances (Lapata
and Brew, 2004).
Given a verb instance whose lemma is within
the coverage of VN, given the sentence in which
it appears, given a parse tree of this sentence (see
below), and given the VN resource, our task is to
classify the verb instance to its correct VN class.
There are currently 237 possible classes5. Each
verb has only a few possible classes (no more than
10, but only about 2.5 on the average over the poly-
semous verbs). Depending on the application, the
parse tree for the sentence may be either a gold
standard parse or a parse tree generated by a parser.
We have experimented with both options.
The task can be viewed in two complemen-
tary ways: per-class and per-verb type. The per-
class perspective takes into consideration the small
5We ignore sub-class distinctions. This is justified since in
98.2% of the in-coverage instances in Semlink, knowing the
verb and its class suffices for knowing its exact sub-class.
number of classes relative to the number of types6.
A classifier may gather valuable information for all
members of a certain VN class, without seeing all
of its members in the training data. From this per-
spective the task resembles POS tagging. In both
tasks there are many dozens (or more) of possible
labels, while each word has only a small subset of
possible labels. Different words may receive the
same label.
The per-verb perspective takes into consider-
ation the special properties of every verb type.
Even the best lexicons necessarily ignore certain
idiosyncratic characteristics of the verb when as-
signing it to a certain class. If a verb appears
many times in the corpus, it is possible to estimate
its parameters to a reasonable reliability, and thus
to use its specific distributional properties for dis-
ambiguation. Viewed in this manner, the task re-
sembles a word sense disambiguation (WSD) task:
each verb has a small distinct set of senses (types),
and no two different verbs have the same sense.
The similarity to WSD suggests that our task
might be solved by WN sense disambiguation fol-
lowed by a mapping from WN to VN. However,
good results are not to be expected, due to the
medium quality of today?s WSD algorithms and
because the mapping between WN and VN is both
incomplete and many-to-many7. Even for a perfect
WN WSD algorithm, the resulting WN synset may
not be mapped to VN at all or may be mapped onto
multiple VN classes. We experimented with this
method and obtained results below the MF base-
line we used8.
The above discussion does not rule out the pos-
sibility of obtaining reasonable results through ap-
plying a high quality WSD engine followed by a
WN to VN mapping. However, there are much
fewer VN classes than WN classes per verb. This
may result in the WSD engine learning many dis-
tinctions that are not useful in this context, which
may in turn jeopardize its performance with re-
spect to our task. Moreover, a word sense may
belong to a single verb only while a VN class con-
tains many verbs. Consequently, the performance
6237 classes vs. 4991 types.
7In the WN to VN mapping built into VN, 14.69% of the
covered WN synsets were mapped to more than a single VN
class.
8We used the publicly available SenseLearner 2.0, the VB-
Collocations model. We chose VN classes containing the
lemma in random when a single mapping is not specified. We
obtained 67.74% accuracy on section 00 of the WSJ, which is
less than the MF baseline. See Sections 5 and 7.
11
on a certain lemma may benefit from training in-
stances of other lemmas.
Note that our task is not reducible to VN frame
identification (a non-trivial task given the rich-
ness of the information used to define a frame
in VN). Although the categorizing criterion for
Levin?s classification is the subset of frames the
verb may appear in (equivalently, the diathesis al-
ternations the verbal proposition may perform),
knowing only the frame in which an instance ap-
pears does not suffice, as frames are shared among
classes.
4 The Learning Model
As common in supervised learning models, we en-
code the verb instances into feature vectors and
then apply a learning algorithm to induce a clas-
sifier. We first discuss the feature set and then the
learning algorithm.
Features. Our feature set heavily relies on syn-
tactic annotation. Dorr and Jones (1996) showed
that perfect knowledge of the allowable syntactic
frames for a verb allows 98% accuracy in type as-
signment to Levin classes. This motivates the en-
coding of the syntactic structure of the sentence
as features, since we have no access to all frames,
only to the one appearing in the sentence.
Since some verbs may appear in the same syn-
tactic frame in different VN classes, a model rely-
ing on the syntactic frame alone would not be able
to disambiguate instances of these verbs when ap-
pearing in those frames. Hence our features in-
clude lexical context words. The parse tree en-
ables us to use words that appear in specific syn-
tactic slots rather than in a linear window around
the verb. To this end, we use the head words of
the neighboring constituents. The definition of the
head of a constituent is given in (Collins, 1999).
Our feature set is comprised of two parallel sets
of features. The first contains features extracted
from the parse tree and the verb?s lemma as a stan-
dalone feature. In the second set, each feature is a
conjunction of a feature from the first set with the
verb?s lemma. By doing so we created a general
feature space shared by all verbs, and replications
of it for each and every verb. This feature selection
strategy was chosen in view of the two perspec-
tives on the task (per-class and per-verb) discussed
in Section 3.
Our first set of features encodes the verb?s con-
text as inferred from the sentence?s parse tree (Fig-
First Feature Set
The stemmed head words, POS, parse tree labels,
function tags, and ordinals of the verb?s right k
r
siblings (k
r
is the maximum number of right sib-
lings in the corpus. These are at most 5k
r
differ-
ent features).
The stemmed head words, POS, labels, function
tags and ordinals of the verb?s left k
l
siblings, as
above.
The stemmed head word & POS of the ?second
head word? nodes on the left and right (see text
for precise definition).
All of the above features employed on the sib-
lings of the parent of the verb (only if the verb?s
parent is the head constituent of its grandparent)
The number of right/left siblings of the verb.
The number of right/left siblings of the verb?s
parent.
The parse tree label of the verb?s parent.
The verb?s voice (active or passive).
The verb?s lemma.
Figure 1: The first set of features in our model. All
of them are binary. The final feature set includes
two sets: the set here, and a set obtained by its
conjunction with the verb?s lemma.
ure 1). We attempt to encode both the syntactic
frame, by encoding the tree structure, and the ar-
gument preferences, by encoding the head words
of the arguments and their POS. The restriction on
the verb?s parent being the head constituent of its
grandparent is done in order to focus on the correct
verb in verb series such as ?intend to run?.
The 3rd cell in the table makes use of a ?sec-
ond head word? node, defined as follows. Consider
a left sibling (right siblings are addressed analo-
gously) M of the verb?s node. Take the node H
in the subtree of M where M ?s head appears. H
is a descendent of a node J which is a child of
M . The ?second head word? node is J?s sibling on
the right. For example, in the sentence We went to
school (see Figure 2) the head word of the PP ?to
school? is ?to?, and the ?second head word? node is
?school?. The rationale is that ?school? could be a
useful feature for ?went?, in addition to ?to?, which
is highly polysemous (note that it is also a feature
for ?went?, in the 1st and 2nd cells of the table).
The voice feature was computed using a simple
heuristic based on the verb?s POS tag (past partici-
ple) and presence of auxiliary verbs to its left.
12
SNP
PRP
We
VP
VBD
went
PP
TO
to
NP
NN
school
Figure 2: An example parse tree for the ?second
head word? feature.
The current set of features does not detect verb
particle constructions. We leave this for future re-
search.
Learning Algorithm. Our learning task can be
formulated as follows. Let x
i
denote the feature
vector of an instance i, and let X denote the space
of all such feature vectors. The subset of possi-
ble labels for x
i
is denoted by C
i
, and the correct
label by c
i
? C
i
. We denote the label space by
S. Let T be the training set of instances T = {<
x
1
, C
1
, c
1
>,< x
2
, C
2
, c
2
>, ..., < x
n
, C
n
, c
n
>
} ? (X ? 2
S
? S)
n
, where n is the size of the
training set. Let < x
n+1
, C
n+1
>? (X ? 2
S
) be
a new instance. Our task is to select which of the
labels in C
n+1
is its correct label c
n+1
(x
n+1
does
not have to be a previously observed lemma, but
its lemma must appear in a VN class).
The structure of the task lets us apply a learn-
ing algorithm that is especially appropriate for it.
What we need is an algorithm that allows us to re-
strict the possible labels of each instance, both in
training and in testing. The sequential model algo-
rithm presented by Even-Zohar and Roth (2001)
directly supports this requirement. We use the
SNOW learning architecture for multi-class clas-
sification (Roth, 1998), which contains an imple-
mentation of that algorithm 9.
5 Experimental Setup
We used SemLink VN annotations and parse trees
on sections 02-21 of the WSJ Penn Treebank for
training, and section 00 as a development set, as
is common in the parsing community. We per-
formed two parallel sets of experiments, one us-
ing manually created gold standard parse trees and
one using parse trees created by a state-of-the-art
9Experiments on development data revealed that for verbs
for which almost all of the training instances are mapped to
the same VN class, it is most beneficial to select that class.
Thus, where more than 90% of the training instances of a verb
are mapped to the same class, our algorithm mapped the in-
stances of the verb to that class regardless of the context.
parser (Charniak and Johnson, 2005) (Note that
this parser does not output function tags). The
parser was also trained on sections 02-21 and tuned
on section 0010. Consequently, our adaptation sce-
nario is a full adaptation situation in which both the
parser and the VerbNet training data are not in the
test domain. Note that generative parser adaptation
results are known to be of much lower quality than
in-domain results (Lease and Charniak, 2005). The
quality of the discriminative parser we used did
indeed decrease in our adaptation scenario (Sec-
tion 7).
The training data included 71209 VN in-scope
instances (of them 41753 polysemous) and the de-
velopment 3624 instances (2203 polysemous). An
?in-scope? instance is one that appears in VN and
is tagged with a verb POS. The same trained model
was used in both the in-domain and adaptation sce-
narios, which only differ in their test sets.
In-Domain. Tests were held on sections
01,22,23,24 of WSJ PTB. Test data includes all in-
scope instances for which there is a SemLink anno-
tation, yielding 13540 instances, 7798 (i.e., 57.6%)
of them polysemous.
Adaptation. For the testing we annotated sen-
tences from GENIA (Kim et al, 2003) (version
3.0.2). The GENIA corpus is composed of MED-
LINE abstracts related to transcription factors in
human blood cells. We annotated 400 sentences
from the corpus, each including at least one in-
scope verb instance. We took the first 400 sen-
tences from the corpus that met that criterion11 .
After cleaning some GENIA POS inconsistencies,
this amounts to 690 in-scope instances (380 of
them polysemous). The tagging was done by two
annotators with an inter-annotator agreement rate
of 80.35% and Kappa 67.66%.
Baselines. We used two baselines, random and
most frequent (MF). The random baseline selects
uniformly and independently one of the possible
classes of the verb. The most frequent (MF) base-
line selects the most frequent class of the verb in
the training data for verbs seen while training, and
selects in random for the unseen ones. Conse-
quently, it obtains a perfect score over the monose-
mous verbs. This baseline is a strong one and is
common in disambiguation tasks.
We repeated all of the setup above in two sce-
10For the very few sentences out of coverage for the parser,
we used the MF baseline (see below).
11Discarding the first 120 sentences, which were used to
design the annotator guidelines.
13
narios. In the first (main) scenario, in-scope in-
stances were always mapped to VN classes, while
in the second (?other is possible? (OIP)) scenario,
in-scope instances were allowed to be tagged (dur-
ing training) and classified (during test) as not be-
longing to any existing VN class12. In all cases,
out-of-scope instances (verbs whose lemmas do
not appear in VN) were ignored. For the OIP sce-
nario, we used a different ?other? label for each of
the lemmas, not a single label shared by them all.
6 Results
Table 1 shows our results. In addition to the over-
all results, we also show results for the polysemous
ones alone, since the task is trivial for the monose-
mous ones. The results using gold standard parses
effectively set an upper bound on our model?s per-
formance, while those using statistical parser out-
put demonstrate its current usability.
In-Domain. Results are shown in the WSJ ?
WSJ columns of Table 1. Using gold standard
parses (top), we achieve 96.42% accuracy over-
all. Over the polysemous verbs, the accuracy is
93.68%. This translates to an error reduction over
the MF baseline of 43.35% overall and 43.22% for
the polysemous verbs. In the ?other is possible?
scenario (right), we obtained 36.67% error reduc-
tion. Using a state-of-the-art parser (Charniak and
Johnson, 2005) (bottom), we experienced some
degradation of the results (as expected), but they
remained significantly above baseline. We achieve
95.9% accuracy overall and 92.77% for the polyse-
mous verbs, which translates to about 35.13% and
35.04% error reduction respectively. In the OIP
scenario, we obtained 28.95% error reduction.
The results of the random baseline for the in-
domain scenario are substantially worse than the
MF baseline. On the WSJ the random baseline
scored 66.97% (37.51%) accuracy in the main
(OIP) scenarios.
Adaptation. Here we test our model?s ability
to generalize across domains. Since VN is sup-
posed to be a domain independent resource, we
hope to acquire statistics that are relevant across
domains as well and so to enable us to automati-
cally map verbs in domains of various genres. The
results are shown in the WSJ ? GENIA columns
of Table 1. When using gold standard parses, our
model scored 73.16%. This translates to about
13.17% ER on GENIA. We interestingly experi-
12i.e., including instances tagged by SemLink as ?none?.
enced very little degradation in the results when
moving to parser output, achieving 72.4% accu-
racy which translates to 10.71% error reduction
over the MF baseline. The random baseline on GE-
NIA was again worse than MF, obtaining 66.04%
accuracy as compared to 69.09% of MF (in the OIP
scenario, 39.12% compared to 46.41%).
Run-time performance. Given a parsed cor-
pus, our main model trains and runs in no more
than a few minutes for a training set of ?60K in-
stances and a test set of ?11K instances, using a
Pentium 4 CPU 2.40GHz with 1GB main mem-
ory. The bottleneck in tagging large corpora using
our model is thus most likely the running time of
current parsers.
7 Discussion
In this paper we introduced a new statistical model
for automatically mapping verb instances into
VerbNet classes, and presented the first large-scale
experiments for this task, for both in-domain and
corpus adaptation scenarios.
Using gold standard parse trees, we achieved
96.42% accuracy on WSJ test data, showing
43.35% error reduction over a strong baseline.
For adaptation to the GENIA corpus, we showed
13.1% error reduction over the baseline. A sur-
prising result in the context of adaptation is the lit-
tle influence of using gold standard parses versus
using parser output, especially given the relatively
low performance of today?s parsers in the adapta-
tion task (91.4% F-score for the WSJ in-domain
scenario compared to 81.24% F-score when pars-
ing our GENIA test set). This is an interesting di-
rection for future work.
In addition, we conducted some additional pre-
liminary experiments in order to shed light on
some aspects of the task. The experiments reported
below were conducted on the development data,
given gold standard parse trees.
First, motivated by the close connection be-
tween WSD and our task (see Section 3), we con-
ducted an experiment to test the applicability of
using a WSD engine. In addition to the experi-
ments listed above, we also attempted to encode
the output of a modern WSD engine (the VBCollo-
cations Model of SenseLearner 2.0 (Mihalcea and
Csomai, 2005)), both by encoding the synset (if
exists) of the verb instance as a feature, and by en-
coding each possible mapped class of the WSD
engine output synset as a feature. There are k
14
Main Scenario ?Other is Possible? (OIP) Scenario
WSJ?WSJ WSJ?GENIA WSJ?WSJ WSJ?GENIA
MF Model MF Model MF Model MF Model
Gold Std Total 93.68 96.42 69.09 73.16 88.6 92.78 46.41 52.46
ER 43.35 13.17 36.67 11.29
Poly. 88.87 93.68 48.58 55.35 ? ? ? ?
ER 43.22 13.17 ? ?
Parser Total 93.68 95.9 69.09 72.4 88.6 91.9 46.41 52.46
ER 35.13 10.71 28.95 11.29
Poly. 88.87 92.77 48.58 55.35 ? ? ? ?
ER 35.04 10.72 ? ?
Table 1: Accuracy and error reduction (ER) results (in percents) for our model and the MF baseline.
Error reduction is computed as MODEL?MF
100?MF
. Results are given for the WSJ and GENIA corpora test
sets. The top table is for a model receiving gold standard parses of the test data. The bottom is for a
model using (Charniak and Johnson, 2005) state-of-the-art parses of the test data. In the main scenario
(left), instances were always mapped to VN classes, while in the OIP one (right) it was possible (during
both training and test) to map instances as not belonging to any existing class. For the latter, no results
are displayed for polysemous verbs, since each verb can be mapped both to ?other? and to at least one
class.
features if there are k possible classes13. There
was no improvement over the previous model. A
possible reason for this is the performance of the
WSD engine (e.g. 56.1% precision on the verbs in
Senseval-3 all-words task data). Naturally, more
research is needed to establish better methods of
incorporating WSD information to assist in this
task.
Second, we studied the relative usability of class
information as opposed to verb idiosyncratic infor-
mation in the VN disambiguation task. By mea-
suring the accuracy of our model, first given the
per-class features (the first set of features exclud-
ing the verb?s lemma feature) and second given the
per-verb features (the conjunction of the first set
with the verb?s lemma), we tried to address this
question. We obtained 94.82% accuracy for the
per-class experiment, and 95.51% for the per-verb
experiment, compared to 95.95% when using both
in the in-domain gold standard scenario. The MF
baseline scored 92.45% on this development set.
These results, which are close in the per-class ex-
periment to those of the MF baseline, indicate that
combining both approaches in the construction of
the classifier is justified.
Third, we studied the importance of having a
learning algorithm utilizing the task?s structure
(mapping into a large label space where each in-
13The mapping is many-to-many and partial. To overcome
the first issue, given a WN sense of the verb, we encoded all
possible VN classes that correspond to it. To overcome the
second, we treated a verb in a certain VN class, for which the
mapping to WN was available, as one that can be mapped to
all WN senses of the verb.
stance can be mapped to only a small subspace).
Our choice of the algorithm in (Even-Zohar and
Roth, 2001) was done in light of this requirement.
We conducted an experiment in which we omitted
these per-instance restrictions on the label space,
effectively allowing each verb to take every label
in the label space. We obtained 94.54% accuracy,
which translates to 27.68% error reduction, com-
pared to 95.95% accuracy (46.36% error reduc-
tion) when using the restrictions. These results in-
dicate that although our feature set keeps us sub-
stantially above baseline even without the above
algorithm, using it boosts our results even further.
This result is different from the results obtained
in (Girju et al, 2005), where the results of the un-
constrained (flat) model were significantly below
baseline.
As noted earlier, the field of instance level
verb classification into Levin-inspired classes is far
from being exhaustively explored. We intend to
make our implementation of the model available
to the community, to enable others to engage in
further research on this task.
Acknowledgements. We would like to thank Dan
Roth, Mark Sammons and Ran Luria for their help.
References
Collin F. Baker, Charles J. Fillmore and John B. Lowe,
1998. The Berkeley FrameNet Project. Proc. of the
36th Meeting of the ACL and the 17th COLING.
Eugene Charniak and Mark Johnson, 2005. Coarse-
15
to-fine n-best parsing and maxent discriminative
reranking. Proc. of the 43rd Meeting of the ACL.
Michael Collins, 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Hoa Trang Dang, Karin Kipper, Martha Palmer and
Joseph Rosenzweig, 1998. Investigating regular
sense extensions based on intersective Levin classes.
Proc. of the 36th Meeting of the ACL and the 17th
COLING.
Bonnie J. Dorr, 1997. Large-Scale Dictionary Con-
struction for Foreign Language Tutoring and Inter-
lingual Machine Translation. Machine Translation,
12:1-55.
Bonnie J. Dorr and Douglas Jones, 1996. Role of Word
Sense Disambiguation in Lexical Acquisition: Pre-
dicting Semantics from Syntactic Cues. Proc. of the
16th COLING.
Yair Even-Zohar and Dan Roth, 2001. A Sequential
Model for Multi-Class Classification. Proc. of the
2001 Conference on Empirical Methods in Natural
Language Processing.
Roxana Girju, Dan Roth and Mark Sammons, 2005.
Token-level Disambiguation of VerbNet classes. The
Interdisciplinary Workshop on Verb Features and
Verb Classes.
Svetlana Hensman and John Dunnion, 2004. Automat-
ically building conceptual graphs using VerbNet and
WordNet. International Symposium on Information
and Communication Technologies (ISICT).
Jin?Dong Kim, Tomoko Ohta, Yuka Teteisi and
Jun?ichi Tsujii, 2003. GENIA corpus ? a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19:i180?i182, Oxford U. Press 2003.
Karin Kipper, Hoa Trang Dang and Martha Palmer,
2000. Class-Based Construction of a Verb Lexicon.
Proc. of the 17th National Conference on Artificial
Intelligence.
Karin Kipper-Schuler, 2005. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph. D. the-
sis, University of Pennsylvania.
Karin Kipper, Anna Korhonen, Neville Ryant and
Martha Palmer, 2006. Extending VerbNet with
Novel Verb Classes. Proc. of the 5th International
Conference on Language Resources and Evaluation.
Judith Klavans and Min-Yen Kan, 1998. Role of verbs
in document analysis. Proc. of the 36th Meeting of
the ACL and the 17th International Conference on
Computational Linguistics.
Anna Korhonen and Ted Briscoe, 2004. Extended
Lexical-Semantic Classification of English Verbs.
Proc. of the 42nd Meeting of the ACL, Workshop on
Computational Lexical Semantics.
Mirella Lapata and Chris Brew, 2004. Verb Class
Disambiguation using Informative Priors. Compu-
tational Linguistics, 30(1):45-73
Matthew Lease and Eugene Charniak, 2005. Towards
a Syntactic Account of Punctuation. Proc. of the 2nd
International Joint Conference on Natural Language
Processing.
Beth Levin, 1993. English Verb Classes And Alterna-
tions: A Preliminary Investigation. The University
of Chicago Press.
Beth Levin and Malka Rappaport Hovav, 2005. Argu-
ment Realization. Cambridge University Press.
Juanguo Li and Chris Brew, 2007. Disambiguating
Levin Verbs Using Untagged Data. Proc. of the
2007 International Conference on Recent Advances
in Natural Language Processing.
Edward Loper, Szu-ting Yi and Martha Palmer, 2007.
Combining Lexical Resources: Mapping Between
PropBank and VerbNet. Proc. of the 7th Inter-
national Workshop on Computational Linguistics,
Tilburg, the Netherlands.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
Verb-Classification Based On Statistical Distribu-
tion Of Argument Structure. Computational Linguis-
tics, 27(3):373?408.
Rada Mihalcea and Andras Csomai 2005. Sense-
Learner: word sense disambiguation for all words
in unrestricted text. Proc. of the 43rd Meeting of the
ACL , Poster Session.
Martha Palmer, Daniel Gildea and Paul Kingsbury,
2005. The proposition bank: A corpus annotated
with semantic roles. Computational Linguistics,
31(1).
Dan Roth, 1998. Learning to resolve natural language
ambiguities: A unified approach. Proc. of the 15th
National Conference on Artificial Intelligence
Sabine Schulte im Walde, 2000. Clustering verbs se-
mantically according to their alternation behavior.
Proc. of the 18th COLING.
Lei Shi and Rada Mihalcea, 2005. Putting pieces to-
gether: Combining FrameNet, VerbNet and WordNet
for robust semantic parsing. Proc. of the Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics.
Robert S. Swier and Suzanne Stevenson, 2005. Ex-
ploiting a Verb Lexicon in Automatic Semantic Role
Labelling. Proc. of the 2005 conference on empirical
methods in natural language processing.
Szu-ting Yi, Edward Loper and Martha Palmer, 2007.
Can Semantic Roles Generalize Across Genres?
Proc. of the 2007 conference of the north american
chapter of the association for computational linguis-
tics.
16
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 28?36,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Unsupervised Argument Identification for Semantic Role Labeling
Omri Abend1 Roi Reichart2 Ari Rappoport1
1Institute of Computer Science , 2ICNC
Hebrew University of Jerusalem
{omria01|roiri|arir}@cs.huji.ac.il
Abstract
The task of Semantic Role Labeling
(SRL) is often divided into two sub-tasks:
verb argument identification, and argu-
ment classification. Current SRL algo-
rithms show lower results on the identifi-
cation sub-task. Moreover, most SRL al-
gorithms are supervised, relying on large
amounts of manually created data. In
this paper we present an unsupervised al-
gorithm for identifying verb arguments,
where the only type of annotation required
is POS tagging. The algorithm makes use
of a fully unsupervised syntactic parser,
using its output in order to detect clauses
and gather candidate argument colloca-
tion statistics. We evaluate our algorithm
on PropBank10, achieving a precision of
56%, as opposed to 47% of a strong base-
line. We also obtain an 8% increase in
precision for a Spanish corpus. This is
the first paper that tackles unsupervised
verb argument identification without using
manually encoded rules or extensive lexi-
cal or syntactic resources.
1 Introduction
Semantic Role Labeling (SRL) is a major NLP
task, providing a shallow sentence-level semantic
analysis. SRL aims at identifying the relations be-
tween the predicates (usually, verbs) in the sen-
tence and their associated arguments.
The SRL task is often viewed as consisting of
two parts: argument identification (ARGID) and ar-
gument classification. The former aims at identi-
fying the arguments of a given predicate present
in the sentence, while the latter determines the
type of relation that holds between the identi-
fied arguments and their corresponding predicates.
The division into two sub-tasks is justified by
the fact that they are best addressed using differ-
ent feature sets (Pradhan et al, 2005). Perfor-
mance in the ARGID stage is a serious bottleneck
for general SRL performance, since only about
81% of the arguments are identified, while about
95% of the identified arguments are labeled cor-
rectly (Ma`rquez et al, 2008).
SRL is a complex task, which is reflected by the
algorithms used to address it. A standard SRL al-
gorithm requires thousands to dozens of thousands
sentences annotated with POS tags, syntactic an-
notation and SRL annotation. Current algorithms
show impressive results but only for languages and
domains where plenty of annotated data is avail-
able, e.g., English newspaper texts (see Section 2).
Results are markedly lower when testing is on a
domain wider than the training one, even in En-
glish (see the WSJ-Brown results in (Pradhan et
al., 2008)).
Only a small number of works that do not re-
quire manually labeled SRL training data have
been done (Swier and Stevenson, 2004; Swier and
Stevenson, 2005; Grenager and Manning, 2006).
These papers have replaced this data with the
VerbNet (Kipper et al, 2000) lexical resource or
a set of manually written rules and supervised
parsers.
A potential answer to the SRL training data bot-
tleneck are unsupervised SRL models that require
little to no manual effort for their training. Their
output can be used either by itself, or as training
material for modern supervised SRL algorithms.
In this paper we present an algorithm for unsu-
pervised argument identification. The only type of
annotation required by our algorithm is POS tag-
28
ging, which needs relatively little manual effort.
The algorithm consists of two stages. As pre-
processing, we use a fully unsupervised parser to
parse each sentence. Initially, the set of possi-
ble arguments for a given verb consists of all the
constituents in the parse tree that do not contain
that predicate. The first stage of the algorithm
attempts to detect the minimal clause in the sen-
tence that contains the predicate in question. Us-
ing this information, it further reduces the possible
arguments only to those contained in the minimal
clause, and further prunes them according to their
position in the parse tree. In the second stage we
use pointwise mutual information to estimate the
collocation strength between the arguments and
the predicate, and use it to filter out instances of
weakly collocating predicate argument pairs.
We use two measures to evaluate the perfor-
mance of our algorithm, precision and F-score.
Precision reflects the algorithm?s applicability for
creating training data to be used by supervised
SRL models, while the standard SRL F-score mea-
sures the model?s performance when used by it-
self. The first stage of our algorithm is shown to
outperform a strong baseline both in terms of F-
score and of precision. The second stage is shown
to increase precision while maintaining a reason-
able recall.
We evaluated our model on sections 2-21 of
Propbank. As is customary in unsupervised pars-
ing work (e.g. (Seginer, 2007)), we bounded sen-
tence length by 10 (excluding punctuation). Our
first stage obtained a precision of 52.8%, which is
more than 6% improvement over the baseline. Our
second stage improved precision to nearly 56%, a
9.3% improvement over the baseline. In addition,
we carried out experiments on Spanish (on sen-
tences of length bounded by 15, excluding punctu-
ation), achieving an increase of over 7.5% in pre-
cision over the baseline. Our algorithm increases
F?score as well, showing an 1.8% improvement
over the baseline in English and a 2.2% improve-
ment in Spanish.
Section 2 reviews related work. In Section 3 we
detail our algorithm. Sections 4 and 5 describe the
experimental setup and results.
2 Related Work
The advance of machine learning based ap-
proaches in this field owes to the usage of large
scale annotated corpora. English is the most stud-
ied language, using the FrameNet (FN) (Baker et
al., 1998) and PropBank (PB) (Palmer et al, 2005)
resources. PB is a corpus well suited for evalu-
ation, since it annotates every non-auxiliary verb
in a real corpus (the WSJ sections of the Penn
Treebank). PB is a standard corpus for SRL eval-
uation and was used in the CoNLL SRL shared
tasks of 2004 (Carreras and Ma`rquez, 2004) and
2005 (Carreras and Ma`rquez, 2005).
Most work on SRL has been supervised, requir-
ing dozens of thousands of SRL annotated train-
ing sentences. In addition, most models assume
that a syntactic representation of the sentence is
given, commonly in the form of a parse tree, a de-
pendency structure or a shallow parse. Obtaining
these is quite costly in terms of required human
annotation.
The first work to tackle SRL as an indepen-
dent task is (Gildea and Jurafsky, 2002), which
presented a supervised model trained and evalu-
ated on FrameNet. The CoNLL shared tasks of
2004 and 2005 were devoted to SRL, and stud-
ied the influence of different syntactic annotations
and domain changes on SRL results. Computa-
tional Linguistics has recently published a special
issue on the task (Ma`rquez et al, 2008), which
presents state-of-the-art results and surveys the lat-
est achievements and challenges in the field.
Most approaches to the task use a multi-level
approach, separating the task to an ARGID and an
argument classification sub-tasks. They then use
the unlabeled argument structure (without the se-
mantic roles) as training data for the ARGID stage
and the entire data (perhaps with other features)
for the classification stage. Better performance
is achieved on the classification, where state-
of-the-art supervised approaches achieve about
81% F-score on the in-domain identification task,
of which about 95% are later labeled correctly
(Ma`rquez et al, 2008).
There have been several exceptions to the stan-
dard architecture described in the last paragraph.
One suggestion poses the problem of SRL as a se-
quential tagging of words, training an SVM clas-
sifier to determine for each word whether it is in-
side, outside or in the beginning of an argument
(Hacioglu and Ward, 2003). Other works have in-
tegrated argument classification and identification
into one step (Collobert and Weston, 2007), while
others went further and combined the former two
along with parsing into a single model (Musillo
29
and Merlo, 2006).
Work on less supervised methods has been
scarce. Swier and Stevenson (2004) and Swier
and Stevenson (2005) presented the first model
that does not use an SRL annotated corpus. How-
ever, they utilize the extensive verb lexicon Verb-
Net, which lists the possible argument structures
allowable for each verb, and supervised syntac-
tic tools. Using VerbNet alng with the output of
a rule-based chunker (in 2004) and a supervised
syntactic parser (in 2005), they spot instances in
the corpus that are very similar to the syntactic
patterns listed in VerbNet. They then use these as
seed for a bootstrapping algorithm, which conse-
quently identifies the verb arguments in the corpus
and assigns their semantic roles.
Another less supervised work is that
of (Grenager and Manning, 2006), which presents
a Bayesian network model for the argument
structure of a sentence. They use EM to learn
the model?s parameters from unannotated data,
and use this model to tag a test corpus. However,
ARGID was not the task of that work, which dealt
solely with argument classification. ARGID was
performed by manually-created rules, requiring a
supervised or manual syntactic annotation of the
corpus to be annotated.
The three works above are relevant but incom-
parable to our work, due to the extensive amount
of supervision (namely, VerbNet and a rule-based
or supervised syntactic system) they used, both in
detecting the syntactic structure and in detecting
the arguments.
Work has been carried out in a few other lan-
guages besides English. Chinese has been studied
in (Xue, 2008). Experiments on Catalan and Span-
ish were done in SemEval 2007 (Ma`rquez et al,
2007) with two participating systems. Attempts
to compile corpora for German (Burdchardt et al,
2006) and Arabic (Diab et al, 2008) are also un-
derway. The small number of languages for which
extensive SRL annotated data exists reflects the
considerable human effort required for such en-
deavors.
Some SRL works have tried to use unannotated
data to improve the performance of a base su-
pervised model. Methods used include bootstrap-
ping approaches (Gildea and Jurafsky, 2002; Kate
and Mooney, 2007), where large unannotated cor-
pora were tagged with SRL annotation, later to
be used to retrain the SRL model. Another ap-
proach used similarity measures either between
verbs (Gordon and Swanson, 2007) or between
nouns (Gildea and Jurafsky, 2002) to overcome
lexical sparsity. These measures were estimated
using statistics gathered from corpora augmenting
the model?s training data, and were then utilized
to generalize across similar verbs or similar argu-
ments.
Attempts to substitute full constituency pars-
ing by other sources of syntactic information have
been carried out in the SRL community. Sugges-
tions include posing SRL as a sequence labeling
problem (Ma`rquez et al, 2005) or as an edge tag-
ging problem in a dependency representation (Ha-
cioglu, 2004). Punyakanok et al (2008) provide
a detailed comparison between the impact of us-
ing shallow vs. full constituency syntactic infor-
mation in an English SRL system. Their results
clearly demonstrate the advantage of using full an-
notation.
The identification of arguments has also been
carried out in the context of automatic subcatego-
rization frame acquisition. Notable examples in-
clude (Manning, 1993; Briscoe and Carroll, 1997;
Korhonen, 2002) who all used statistical hypothe-
sis testing to filter a parser?s output for arguments,
with the goal of compiling verb subcategorization
lexicons. However, these works differ from ours
as they attempt to characterize the behavior of a
verb type, by collecting statistics from various in-
stances of that verb, and not to determine which
are the arguments of specific verb instances.
The algorithm presented in this paper performs
unsupervised clause detection as an intermedi-
ate step towards argument identification. Super-
vised clause detection was also tackled as a sepa-
rate task, notably in the CoNLL 2001 shared task
(Tjong Kim Sang and De`jean, 2001). Clause in-
formation has been applied to accelerating a syn-
tactic parser (Glaysher and Moldovan, 2006).
3 Algorithm
In this section we describe our algorithm. It con-
sists of two stages, each of which reduces the set
of argument candidates, which a-priori contains all
consecutive sequences of words that do not con-
tain the predicate in question.
3.1 Algorithm overview
As pre-processing, we use an unsupervised parser
that generates an unlabeled parse tree for each sen-
30
tence (Seginer, 2007). This parser is unique in that
it is able to induce a bracketing (unlabeled pars-
ing) from raw text (without even using POS tags)
achieving state-of-the-art results. Since our algo-
rithm uses millions to tens of millions sentences,
we must use very fast tools. The parser?s high
speed (thousands of words per second) enables us
to process these large amounts of data.
The only type of supervised annotation we
use is POS tagging. We use the taggers MX-
POST (Ratnaparkhi, 1996) for English and Tree-
Tagger (Schmid, 1994) for Spanish, to obtain POS
tags for our model.
The first stage of our algorithm uses linguisti-
cally motivated considerations to reduce the set of
possible arguments. It does so by confining the set
of argument candidates only to those constituents
which obey the following two restrictions. First,
they should be contained in the minimal clause
containing the predicate. Second, they should be
k-th degree cousins of the predicate in the parse
tree. We propose a novel algorithm for clause de-
tection and use its output to determine which of
the constituents obey these two restrictions.
The second stage of the algorithm uses point-
wise mutual information to rule out constituents
that appear to be weakly collocating with the pred-
icate in question. Since a predicate greatly re-
stricts the type of arguments with which it may
appear (this is often referred to as ?selectional re-
strictions?), we expect it to have certain character-
istic arguments with which it is likely to collocate.
3.2 Clause detection stage
The main idea behind this stage is the observation
that most of the arguments of a predicate are con-
tained within the minimal clause that contains the
predicate. We tested this on our development data
? section 24 of the WSJ PTB, where we saw that
86% of the arguments that are also constituents
(in the gold standard parse) were indeed contained
in that minimal clause (as defined by the tree la-
bel types in the gold standard parse that denote
a clause, e.g., S, SBAR). Since we are not pro-
vided with clause annotation (or any label), we at-
tempted to detect them in an unsupervised manner.
Our algorithm attempts to find sub-trees within the
parse tree, whose structure resembles the structure
of a full sentence. This approximates the notion of
a clause.
L
L
DT
The
NNS
materials
L
L
IN
in
L
DT
each
NN
set
L
VBP
reach
L
L
IN
about
CD
90
NNS
students
L
L L
L L
VBP L
L
VBP L
Figure 1: An example of an unlabeled POS tagged
parse tree. The middle tree is the ST of ?reach?
with the root as the encoded ancestor. The bot-
tom one is the ST with its parent as the encoded
ancestor.
Statistics gathering. In order to detect which
of the verb?s ancestors is the minimal clause, we
score each of the ancestors and select the one that
maximizes the score. We represent each ancestor
using its Spinal Tree (ST ). The ST of a given
verb?s ancestor is obtained by replacing all the
constituents that do not contain the verb by a leaf
having a label. This effectively encodes all the k-
th degree cousins of the verb (for every k). The
leaf labels are either the word?s POS in case the
constituent is a leaf, or the generic label ?L? de-
noting a non-leaf. See Figure 1 for an example.
In this stage we collect statistics of the occur-
rences of ST s in a large corpus. For every ST in
the corpus, we count the number of times it oc-
curs in a form we consider to be a clause (positive
examples), and the number of times it appears in
other forms (negative examples).
Positive examples are divided into two main
types. First, when the ST encodes the root an-
cestor (as in the middle tree of Figure 1); second,
when the ancestor complies to a clause lexico-
syntactic pattern. In many languages there is a
small set of lexico-syntactic patterns that mark a
clause, e.g. the English ?that?, the German ?dass?
and the Spanish ?que?. The patterns which were
used in our experiments are shown in Figure 2.
For each verb instance, we traverse over its an-
31
English
TO + VB. The constituent starts with ?to? followed by
a verb in infinitive form.
WP. The constituent is preceded by a Wh-pronoun.
That. The constituent is preceded by a ?that? marked
by an ?IN? POS tag indicating that it is a subordinating
conjunction.
Spanish
CQUE. The constituent is preceded by a word with the
POS ?CQUE? which denotes the word ?que? as a con-
junction.
INT. The constituent is preceded by a word with the
POS ?INT? which denotes an interrogative pronoun.
CSUB. The constituent is preceded by a word with one
of the POSs ?CSUBF?, ?CSUBI? or ?CSUBX?, which
denote a subordinating conjunction.
Figure 2: The set of lexico-syntactic patterns that
mark clauses which were used by our model.
cestors from top to bottom. For each of them we
update the following counters: sentence(ST ) for
the root ancestor?s ST , patterni(ST ) for the ones
complying to the i-th lexico-syntactic pattern and
negative(ST ) for the other ancestors1.
Clause detection. At test time, when detecting
the minimal clause of a verb instance, we use
the statistics collected in the previous stage. De-
note the ancestors of the verb with A1 . . . Am.
For each of them, we calculate clause(STAj )
and total(STAj ). clause(STAj ) is the sum
of sentence(STAj ) and patterni(STAj ) if this
ancestor complies to the i-th pattern (if there
is no such pattern, clause(STAj ) is equal to
sentence(STAj )). total(STAj ) is the sum of
clause(STAj ) and negative(STAj ).
The selected ancestor is given by:
(1) Amax = argmaxAj
clause(STAj )
total(STAj )
An ST whose total(ST ) is less than a small
threshold2 is not considered a candidate to be the
minimal clause, since its statistics may be un-
reliable. In case of a tie, we choose the low-
est constituent that obtained the maximal score.
1If while traversing the tree, we encounter an ancestor
whose first word is preceded by a coordinating conjunction
(marked by the POS tag ?CC?), we refrain from performing
any additional counter updates. Structures containing coor-
dinating conjunctions tend not to obey our lexico-syntactic
rules.
2We used 4 per million sentences, derived from develop-
ment data.
If there is only one verb in the sentence3 or if
clause(STAj ) = 0 for every 1 ? j ? m, we
choose the top level constituent by default to be
the minimal clause containing the verb. Other-
wise, the minimal clause is defined to be the yield
of the selected ancestor.
Argument identification. For each predicate in
the corpus, its argument candidates are now de-
fined to be the constituents contained in the min-
imal clause containing the predicate. However,
these constituents may be (and are) nested within
each other, violating a major restriction on SRL
arguments. Hence we now prune our set, by keep-
ing only the siblings of all of the verb?s ancestors,
as is common in supervised SRL (Xue and Palmer,
2004).
3.3 Using collocations
We use the following observation to filter out some
superfluous argument candidates: since the argu-
ments of a predicate many times bear a semantic
connection with that predicate, they consequently
tend to collocate with it.
We collect collocation statistics from a large
corpus, which we annotate with parse trees and
POS tags. We mark arguments using the argu-
ment detection algorithm described in the previous
two sections, and extract all (predicate, argument)
pairs appearing in the corpus. Recall that for each
sentence, the arguments are a subset of the con-
stituents in the parse tree.
We use two representations of an argument: one
is the POS tag sequence of the terminals contained
in the argument, the other is its head word4. The
predicate is represented as the conjunction of its
lemma with its POS tag.
Denote the number of times a predicate x
appeared with an argument y by nxy. Denote
the total number of (predicate, argument) pairs
by N . Using these notations, we define the
following quantities: nx = ?ynxy, ny = ?xnxy,
p(x) = nxN , p(y) =
ny
N and p(x, y) =
nxy
N . The
pointwise mutual information of x and y is then
given by:
3In this case, every argument in the sentence must be re-
lated to that verb.
4Since we do not have syntactic labels, we use an approx-
imate notion. For English we use the Bikel parser default
head word rules (Bikel, 2004). For Spanish, we use the left-
most word.
32
(2) PMI(x, y) = log p(x,y)p(x)?p(y) = log
nxy
(nx?ny)/N
PMI effectively measures the ratio between
the number of times x and y appeared together and
the number of times they were expected to appear,
had they been independent.
At test time, when an (x, y) pair is observed, we
check if PMI(x, y), computed on the large cor-
pus, is lower than a threshold ? for either of x?s
representations. If this holds, for at least one rep-
resentation, we prune all instances of that (x, y)
pair. The parameter ? may be selected differently
for each of the argument representations.
In order to avoid using unreliable statistics,
we apply this for a given pair only if nx?nyN >
r, for some parameter r. That is, we consider
PMI(x, y) to be reliable, only if the denomina-
tor in equation (2) is sufficiently large.
4 Experimental Setup
Corpora. We used the PropBank corpus for de-
velopment and for evaluation on English. Section
24 was used for the development of our model,
and sections 2 to 21 were used as our test data.
The free parameters of the collocation extraction
phase were tuned on the development data. Fol-
lowing the unsupervised parsing literature, multi-
ple brackets and brackets covering a single word
are omitted. We exclude punctuation according
to the scheme of (Klein, 2005). As is customary
in unsupervised parsing (e.g. (Seginer, 2007)), we
bounded the lengths of the sentences in the cor-
pus to be at most 10 (excluding punctuation). This
results in 207 sentences in the development data,
containing a total of 132 different verbs and 173
verb instances (of the non-auxiliary verbs in the
SRL task, see ?evaluation? below) having 403 ar-
guments. The test data has 6007 sentences con-
taining 1008 different verbs and 5130 verb in-
stances (as above) having 12436 arguments.
Our algorithm requires large amounts of data
to gather argument structure and collocation pat-
terns. For the statistics gathering phase of the
clause detection algorithm, we used 4.5M sen-
tences of the NANC (Graff, 1995) corpus, bound-
ing their length in the same manner. In order
to extract collocations, we used 2M sentences
from the British National Corpus (Burnard, 2000)
and about 29M sentences from the Dmoz cor-
pus (Gabrilovich and Markovitch, 2005). Dmoz
is a web corpus obtained by crawling and clean-
ing the URLs in the Open Directory Project
(dmoz.org). All of the above corpora were parsed
using Seginer?s parser and POS-tagged by MX-
POST (Ratnaparkhi, 1996).
For our experiments on Spanish, we used 3.3M
sentences of length at most 15 (excluding punctua-
tion) extracted from the Spanish Wikipedia. Here
we chose to bound the length by 15 due to the
smaller size of the available test corpus. The
same data was used both for the first and the sec-
ond stages. Our development and test data were
taken from the training data released for the Se-
mEval 2007 task on semantic annotation of Span-
ish (Ma`rquez et al, 2007). This data consisted
of 1048 sentences of length up to 15, from which
200 were randomly selected as our development
data and 848 as our test data. The development
data included 313 verb instances while the test
data included 1279. All corpora were parsed us-
ing the Seginer parser and tagged by the ?Tree-
Tagger? (Schmid, 1994).
Baselines. Since this is the first paper, to our
knowledge, which addresses the problem of unsu-
pervised argument identification, we do not have
any previous results to compare to. We instead
compare to a baseline which marks all k-th degree
cousins of the predicate (for every k) as arguments
(this is the second pruning we use in the clause
detection stage). We name this baseline the ALL
COUSINS baseline. We note that a random base-
line would score very poorly since any sequence of
terminals which does not contain the predicate is
a possible candidate. Therefore, beating this ran-
dom baseline is trivial.
Evaluation. Evaluation is carried out using
standard SRL evaluation software5. The algorithm
is provided with a list of predicates, whose argu-
ments it needs to annotate. For the task addressed
in this paper, non-consecutive parts of arguments
are treated as full arguments. A match is consid-
ered each time an argument in the gold standard
data matches a marked argument in our model?s
output. An unmatched argument is an argument
which appears in the gold standard data, and fails
to appear in our model?s output, and an exces-
sive argument is an argument which appears in
our model?s output but does not appear in the gold
standard. Precision and recall are defined accord-
ingly. We report an F-score as well (the harmonic
mean of precision and recall). We do not attempt
5http://www.lsi.upc.edu/?srlconll/soft.html#software.
33
to identify multi-word verbs, and therefore do not
report the model?s performance in identifying verb
boundaries.
Since our model detects clauses as an interme-
diate product, we provide a separate evaluation
of this task for the English corpus. We show re-
sults on our development data. We use the stan-
dard parsing F-score evaluation measure. As a
gold standard in this evaluation, we mark for each
of the verbs in our development data the minimal
clause containing it. A minimal clause is the low-
est ancestor of the verb in the parse tree that has
a syntactic label of a clause according to the gold
standard parse of the PTB. A verb is any terminal
marked by one of the POS tags of type verb ac-
cording to the gold standard POS tags of the PTB.
5 Results
Our results are shown in Table 1. The left section
presents results on English and the right section
presents results on Spanish. The top line lists re-
sults of the clause detection stage alone. The next
two lines list results of the full algorithm (clause
detection + collocations) in two different settings
of the collocation stage. The bottom line presents
the performance of the ALL COUSINS baseline.
In the ?Collocation Maximum Precision? set-
ting the parameters of the collocation stage (? and
r) were generally tuned such that maximal preci-
sion is achieved while preserving a minimal recall
level (40% for English, 20% for Spanish on the de-
velopment data). In the ?Collocation Maximum F-
score? the collocation parameters were generally
tuned such that the maximum possible F-score for
the collocation algorithm is achieved.
The best or close to best F-score is achieved
when using the clause detection algorithm alone
(59.14% for English, 23.34% for Spanish). Note
that for both English and Spanish F-score im-
provements are achieved via a precision improve-
ment that is more significant than the recall degra-
dation. F-score maximization would be the aim of
a system that uses the output of our unsupervised
ARGID by itself.
The ?Collocation Maximum Precision?
achieves the best precision level (55.97% for
English, 21.8% for Spanish) but at the expense
of the largest recall loss. Still, it maintains a
reasonable level of recall. The ?Collocation
Maximum F-score? is an example of a model that
provides a precision improvement (over both the
baseline and the clause detection stage) with a
relatively small recall degradation. In the Spanish
experiments its F-score (23.87%) is even a bit
higher than that of the clause detection stage
(23.34%).
The full two?stage algorithm (clause detection
+ collocations) should thus be used when we in-
tend to use the model?s output as training data for
supervised SRL engines or supervised ARGID al-
gorithms.
In our algorithm, the initial set of potential ar-
guments consists of constituents in the Seginer
parser?s parse tree. Consequently the fraction
of arguments that are also constituents (81.87%
for English and 51.83% for Spanish) poses an
upper bound on our algorithm?s recall. Note
that the recall of the ALL COUSINS baseline is
74.27% (45.75%) for English (Spanish). This
score emphasizes the baseline?s strength, and jus-
tifies the restriction that the arguments should be
k-th cousins of the predicate. The difference be-
tween these bounds for the two languages provides
a partial explanation for the corresponding gap in
the algorithm?s performance.
Figure 3 shows the precision of the collocation
model (on development data) as a function of the
amount of data it was given. We can see that
the algorithm reaches saturation at about 5M sen-
tences. It achieves this precision while maintain-
ing a reasonable recall (an average recall of 43.1%
after saturation). The parameters of the colloca-
tion model were separately tuned for each corpus
size, and the graph displays the maximum which
was obtained for each of the corpus sizes.
To better understand our model?s performance,
we performed experiments on the English cor-
pus to test how well its first stage detects clauses.
Clause detection is used by our algorithm as a step
towards argument identification, but it can be of
potential benefit for other purposes as well (see
Section 2). The results are 23.88% recall and 40%
precision. As in the ARGID task, a random se-
lection of arguments would have yielded an ex-
tremely poor result.
6 Conclusion
In this work we presented the first algorithm for ar-
gument identification that uses neither supervised
syntactic annotation nor SRL tagged data. We
have experimented on two languages: English and
Spanish. The straightforward adaptability of un-
34
English (Test Data) Spanish (Test Data)
Precision Recall F1 Precision Recall F1
Clause Detection 52.84 67.14 59.14 18.00 33.19 23.34
Collocation Maximum F?score 54.11 63.53 58.44 20.22 29.13 23.87
Collocation Maximum Precision 55.97 40.02 46.67 21.80 18.47 20.00
ALL COUSINS baseline 46.71 74.27 57.35 14.16 45.75 21.62
Table 1: Precision, Recall and F1 score for the different stages of our algorithm. Results are given for English (PTB, sentences
length bounded by 10, left part of the table) and Spanish (SemEval 2007 Spanish SRL task, right part of the table). The results
of the collocation (second) stage are given in two configurations, Collocation Maximum F-score and Collocation Maximum
Precision (see text). The upper bounds on Recall, obtained by taking all arguments output by our unsupervised parser, are
81.87% for English and 51.83% for Spanish.
0 2 4 6 8 10
42
44
46
48
50
52
Number of Sentences (Millions)
Pr
ec
isi
on
 
 
Second Stage
First Stage
Baseline
Figure 3: The performance of the second stage on English
(squares) vs. corpus size. The precision of the baseline (trian-
gles) and of the first stage (circles) is displayed for reference.
The graph indicates the maximum precision obtained for each
corpus size. The graph reaches saturation at about 5M sen-
tences. The average recall of the sampled points from there
on is 43.1%. Experiments were performed on the English
development data.
supervised models to different languages is one
of their most appealing characteristics. The re-
cent availability of unsupervised syntactic parsers
has offered an opportunity to conduct research on
SRL, without reliance on supervised syntactic an-
notation. This work is the first to address the ap-
plication of unsupervised parses to an SRL related
task.
Our model displayed an increase in precision of
9% in English and 8% in Spanish over a strong
baseline. Precision is of particular interest in this
context, as instances tagged by high quality an-
notation could be later used as training data for
supervised SRL algorithms. In terms of F?score,
our model showed an increase of 1.8% in English
and of 2.2% in Spanish over the baseline.
Although the quality of unsupervised parses is
currently low (compared to that of supervised ap-
proaches), using great amounts of data in identi-
fying recurring structures may reduce noise and
in addition address sparsity. The techniques pre-
sented in this paper are based on this observation,
using around 35M sentences in total for English
and 3.3M sentences for Spanish.
As this is the first work which addressed un-
supervised ARGID, many questions remain to be
explored. Interesting issues to address include as-
sessing the utility of the proposed methods when
supervised parses are given, comparing our model
to systems with no access to unsupervised parses
and conducting evaluation using more relaxed
measures.
Unsupervised methods for syntactic tasks have
matured substantially in the last few years. No-
table examples are (Clark, 2003) for unsupervised
POS tagging and (Smith and Eisner, 2006) for un-
supervised dependency parsing. Adapting our al-
gorithm to use the output of these models, either to
reduce the little supervision our algorithm requires
(POS tagging) or to provide complementary syn-
tactic information, is an interesting challenge for
future work.
References
Collin F. Baker, Charles J. Fillmore and John B. Lowe,
1998. The Berkeley FrameNet Project. ACL-
COLING ?98.
Daniel M. Bikel, 2004. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4):479?511.
Ted Briscoe, John Carroll, 1997. Automatic Extraction
of Subcategorization from Corpora. Applied NLP
1997.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pad and Manfred Pinkal, 2006
The SALSA Corpus: a German Corpus Resource for
Lexical Semantics. LREC ?06.
Lou Burnard, 2000. User Reference Guide for the
British National Corpus. Technical report, Oxford
University.
Xavier Carreras and Llu?`s Ma`rquez, 2004. Intro-
duction to the CoNLL?2004 Shared Task: Semantic
Role Labeling. CoNLL ?04.
35
Xavier Carreras and Llu?`s Ma`rquez, 2005. Intro-
duction to the CoNLL?2005 Shared Task: Semantic
Role Labeling. CoNLL ?05.
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech In-
duction. EACL ?03.
Ronan Collobert and Jason Weston, 2007. Fast Se-
mantic Extraction Using a Novel Neural Network
Architecture. ACL ?07.
Mona Diab, Aous Mansouri, Martha Palmer, Olga
Babko-Malaya, Wajdi Zaghouani, Ann Bies and
Mohammed Maamouri, 2008. A pilot Arabic Prop-
Bank. LREC ?08.
Evgeniy Gabrilovich and Shaul Markovitch, 2005.
Feature Generation for Text Categorization using
World Knowledge. IJCAI ?05.
Daniel Gildea and Daniel Jurafsky, 2002. Automatic
Labeling of Semantic Roles. Computational Lin-
guistics, 28(3):245?288.
Elliot Glaysher and Dan Moldovan, 2006. Speed-
ing Up Full Syntactic Parsing by Leveraging Partial
Parsing Decisions. COLING/ACL ?06 poster ses-
sion.
Andrew Gordon and Reid Swanson, 2007. Generaliz-
ing Semantic Role Annotations across Syntactically
Similar Verbs. ACL ?07.
David Graff, 1995. North American News Text Cor-
pus. Linguistic Data Consortium. LDC95T21.
Trond Grenager and Christopher D. Manning, 2006.
Unsupervised Discovery of a Statistical Verb Lexi-
con. EMNLP ?06.
Kadri Hacioglu, 2004. Semantic Role Labeling using
Dependency Trees. COLING ?04.
Kadri Hacioglu and Wayne Ward, 2003. Target Word
Detection and Semantic Role Chunking using Sup-
port Vector Machines. HLT-NAACL ?03.
Rohit J. Kate and Raymond J. Mooney, 2007. Semi-
Supervised Learning for Semantic Parsing using
Support Vector Machines. HLT?NAACL ?07.
Karin Kipper, Hoa Trang Dang and Martha Palmer,
2000. Class-Based Construction of a Verb Lexicon.
AAAI ?00.
Dan Klein, 2005. The Unsupervised Learning of Natu-
ral Language Structure. Ph.D. thesis, Stanford Uni-
versity.
Anna Korhonen, 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge.
Christopher D. Manning, 1993. Automatic Acquisition
of a Large Subcategorization Dictionary. ACL ?93.
Llu?`s Ma`rquez, Xavier Carreras, Kenneth C. Lit-
tkowski and Suzanne Stevenson, 2008. Semantic
Role Labeling: An introdution to the Special Issue.
Computational Linguistics, 34(2):145?159
Llu?`s Ma`rquez, Jesus Gime`nez Pere Comas and Neus
Catala`, 2005. Semantic Role Labeling as Sequential
Tagging. CoNLL ?05.
Llu?`s Ma`rquez, Lluis Villarejo, M. A. Mart?` and Mar-
iona Taule`, 2007. SemEval?2007 Task 09: Multi-
level Semantic Annotation of Catalan and Spanish.
The 4th international workshop on Semantic Evalu-
ations (SemEval ?07).
Gabriele Musillo and Paula Merlo, 2006. Accurate
Parsing of the proposition bank. HLT-NAACL ?06.
Martha Palmer, Daniel Gildea and Paul Kingsbury,
2005. The Proposition Bank: A Corpus Annotated
with Semantic Roles. Computational Linguistics,
31(1):71?106.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin and Daniel Jurafsky,
2005. Support Vector Learning for Semantic Argu-
ment Classification. Machine Learning, 60(1):11?
39.
Sameer Pradhan, Wayne Ward, James H. Martin, 2008.
Towards Robust Semantic Role Labeling. Computa-
tional Linguistics, 34(2):289?310.
Adwait Ratnaparkhi, 1996. Maximum Entropy Part-
Of-Speech Tagger. EMNLP ?96.
Helmut Schmid, 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees International Confer-
ence on New Methods in Language Processing.
Yoav Seginer, 2007. Fast Unsupervised Incremental
Parsing. ACL ?07.
Noah A. Smith and Jason Eisner, 2006. Annealing
Structural Bias in Multilingual Weighted Grammar
Induction. ACL ?06.
Robert S. Swier and Suzanne Stevenson, 2004. Unsu-
pervised Semantic Role Labeling. EMNLP ?04.
Robert S. Swier and Suzanne Stevenson, 2005. Ex-
ploiting a Verb Lexicon in Automatic Semantic Role
Labelling. EMNLP ?05.
Erik F. Tjong Kim Sang and Herve? De?jean, 2001. In-
troduction to the CoNLL-2001 Shared Task: Clause
Identification. CoNLL ?01.
Nianwen Xue and Martha Palmer, 2004. Calibrating
Features for Semantic Role Labeling. EMNLP ?04.
Nianwen Xue, 2008. Labeling Chinese Predicates
with Semantic Roles. Computational Linguistics,
34(2):225?255.
36
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 226?236,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Fully Unsupervised Core-Adjunct Argument Classification
Omri Abend?
Institute of Computer Science
The Hebrew University
omria01@cs.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University
arir@cs.huji.ac.il
Abstract
The core-adjunct argument distinction is a
basic one in the theory of argument struc-
ture. The task of distinguishing between
the two has strong relations to various ba-
sic NLP tasks such as syntactic parsing,
semantic role labeling and subcategoriza-
tion acquisition. This paper presents a
novel unsupervised algorithm for the task
that uses no supervised models, utilizing
instead state-of-the-art syntactic induction
algorithms. This is the first work to tackle
this task in a fully unsupervised scenario.
1 Introduction
The distinction between core arguments (hence-
forth, cores) and adjuncts is included in most the-
ories on argument structure (Dowty, 2000). The
distinction can be viewed syntactically, as one
between obligatory and optional arguments, or
semantically, as one between arguments whose
meanings are predicate dependent and indepen-
dent. The latter (cores) are those whose function in
the described event is to a large extent determined
by the predicate, and are obligatory. Adjuncts are
optional arguments which, like adverbs, modify
the meaning of the described event in a predictable
or predicate-independent manner.
Consider the following examples:
1. The surgeon operated [on his colleague].
2. Ron will drop by [after lunch].
3. Yuri played football [in the park].
The marked argument is a core in 1 and an ad-
junct in 2 and 3. Adjuncts form an independent
semantic unit and their semantic role can often be
inferred independently of the predicate (e.g., [af-
ter lunch] is usually a temporal modifier). Core
? Omri Abend is grateful to the Azrieli Foundation for
the award of an Azrieli Fellowship.
roles are more predicate-specific, e.g., [on his col-
league] has a different meaning with the verbs ?op-
erate? and ?count?.
Sometimes the same argument plays a different
role in different sentences. In (3), [in the park]
places a well-defined situation (Yuri playing foot-
ball) in a certain location. However, in ?The troops
are based [in the park]?, the same argument is
obligatory, since being based requires a place to
be based in.
Distinguishing between the two argument types
has been discussed extensively in various formu-
lations in the NLP literature, notably in PP attach-
ment, semantic role labeling (SRL) and subcatego-
rization acquisition. However, no work has tack-
led it yet in a fully unsupervised scenario. Unsu-
pervised models reduce reliance on the costly and
error prone manual multi-layer annotation (POS
tagging, parsing, core-adjunct tagging) commonly
used for this task. They also allow to examine the
nature of the distinction and to what extent it is
accounted for in real data in a theory-independent
manner.
In this paper we present a fully unsupervised al-
gorithm for core-adjunct classification. We utilize
leading fully unsupervised grammar induction and
POS induction algorithms. We focus on preposi-
tional arguments, since non-prepositional ones are
generally cores. The algorithm uses three mea-
sures based on different characterizations of the
core-adjunct distinction, and combines them us-
ing an ensemble method followed by self-training.
The measures used are based on selectional prefer-
ence, predicate-slot collocation and argument-slot
collocation.
We evaluate against PropBank (Palmer et al,
2005), obtaining roughly 70% accuracy when
evaluated on the prepositional arguments and
more than 80% for the entire argument set. These
results are substantially better than those obtained
by a non-trivial baseline.
226
Section 2 discusses the core-adjunct distinction.
Section 3 describes the algorithm. Sections 4 and
5 present our experimental setup and results.
2 Core-Adjunct in Previous Work
PropBank. PropBank (PB) (Palmer et al, 2005)
is a widely used corpus, providing SRL annotation
for the entire WSJ Penn Treebank. Its core labels
are predicate specific, while adjunct (or modifiers
under their terminology) labels are shared across
predicates. The adjuncts are subcategorized into
several classes, the most frequent of which are
locative, temporal and manner1.
The organization of PropBank is based on
the notion of diathesis alternations, which are
(roughly) defined to be alternations between two
subcategorization frames that preserve meaning or
change it systematically. The frames in which
each verb appears were collected and sets of al-
ternating frames were defined. Each such set was
assumed to have a unique set of roles, named ?role-
set?. These roles include all roles appearing in any
of the frames, except of those defined as adjuncts.
Adjuncts are defined to be optional arguments
appearing with a wide variety of verbs and frames.
They can be viewed as fixed points with respect to
alternations, i.e., as arguments that do not change
their place or slot when the frame undergoes an
alternation. This follows the notions of optionality
and compositionality that define adjuncts.
Detecting diathesis alternations automatically
is difficult (McCarthy, 2001), requiring an initial
acquisition of a subcategorization lexicon. This
alone is a challenging task tackled in the past us-
ing supervised parsers (see below).
FrameNet. FrameNet (FN) (Baker et al, 1998)
is a large-scale lexicon based on frame semantics.
It takes a different approach from PB to semantic
roles. Like PB, it distinguishes between core and
non-core arguments, but it does so for each and
every frame separately. It does not commit that a
semantic role is consistently tagged as a core or
a non-core across frames. For example, the se-
mantic role ?path? is considered core in the ?Self
Motion? frame, but as non-core in the ?Placing?
frame. Another difference is that FN does not al-
low any type of non-core argument to attach to
a given frame. For instance, while the ?Getting?
1PropBank annotates modals and negation words as mod-
ifiers. Since these are not arguments in the common usage of
the term, we exclude them from the discussion in this paper.
frame allows a ?Duration? non-core argument, the
?Active Perception? frame does not.
PB and FN tend to agree in clear (prototypical)
cases, but to differ in others. For instance, both
schemes would tag ?Yuri played football [in the
park]? as an adjunct and ?The commander placed
a guard [in the park]? as a core. However, in ?He
walked [into his office]?, the marked argument is
tagged as a directional adjunct in PB but as a ?Di-
rection? core in FN.
Under both schemes, non-cores are usually con-
fined to a few specific semantic domains, no-
tably time, place and manner, in contrast to cores
that are not restricted in their scope of applica-
bility. This approach is quite common, e.g., the
COBUILD English grammar (Willis, 2004) cate-
gorizes adjuncts to be of manner, aspect, opinion,
place, time, frequency, duration, degree, extent,
emphasis, focus and probability.
Semantic Role Labeling. Work in SRL does
not tackle the core-adjunct task separately but as
part of general argument classification. Super-
vised approaches obtain an almost perfect score
in distinguishing between the two in an in-domain
scenario. For instance, the confusion matrix in
(Toutanova et al, 2008) indicates that their model
scores 99.5% accuracy on this task. However,
adaptation results are lower, with the best two
models in the CoNLL 2005 shared task (Carreras
and Ma`rquez, 2005) achieving 95.3% (Pradhan et
al., 2008) and 95.6% (Punyakanok et al, 2008) ac-
curacy in an adaptation between the relatively sim-
ilar corpora WSJ and Brown.
Despite the high performance in supervised sce-
narios, tackling the task in an unsupervised man-
ner is not easy. The success of supervised methods
stems from the fact that the predicate-slot com-
bination (slot is represented in this paper by its
preposition) strongly determines whether a given
argument is an adjunct or a core (see Section 3.4).
Supervised models are provided with an anno-
tated corpus from which they can easily learn the
mapping between predicate-slot pairs and their
core/adjunct label. However, induction of the
mapping in an unsupervised manner must be based
on inherent core-adjunct properties. In addition,
supervised models utilize supervised parsers and
POS taggers, while the current state-of-the-art in
unsupervised parsing and POS tagging is consid-
erably worse than their supervised counterparts.
This challenge has some resemblance to un-
227
supervised detection of multiword expressions
(MWEs). An important MWE sub-class is that
of phrasal verbs, which are also characterized by
verb-preposition pairs (Li et al, 2003; Sporleder
and Li, 2009) (see also (Boukobza and Rappoport,
2009)). Both tasks aim to determine semantic
compositionality, which is a highly challenging
task.
Few works addressed unsupervised SRL-related
tasks. The setup of (Grenager and Manning,
2006), who presented a Bayesian Network model
for argument classification, is perhaps closest to
ours. Their work relied on a supervised parser
and a rule-based argument identification (both dur-
ing training and testing). Swier and Stevenson
(2004, 2005), while addressing an unsupervised
SRL task, greatly differ from us as their algorithm
uses the VerbNet (Kipper et al, 2000) verb lex-
icon, in addition to supervised parses. Finally,
Abend et al (2009) tackled the argument identi-
fication task alone and did not perform argument
classification of any sort.
PP attachment. PP attachment is the task of de-
termining whether a prepositional phrase which
immediately follows a noun phrase attaches to the
latter or to the preceding verb. This task?s relation
to the core-adjunct distinction was addressed in
several works. For instance, the results of (Hindle
and Rooth, 1993) indicate that their PP attachment
system works better for cores than for adjuncts.
Merlo and Esteve Ferrer (2006) suggest a sys-
tem that jointly tackles the PP attachment and the
core-adjunct distinction tasks. Unlike in this work,
their classifier requires extensive supervision in-
cluding WordNet, language-specific features and
a supervised parser. Their features are generally
motivated by common linguistic considerations.
Features found adaptable to a completely unsuper-
vised scenario are used in this work as well.
Syntactic Parsing. The core-adjunct distinction
is included in many syntactic annotation schemes.
Although the Penn Treebank does not explicitly
annotate adjuncts and cores, a few works sug-
gested mapping its annotation (including func-
tion tags) to core-adjunct labels. Such a mapping
was presented in (Collins, 1999). In his Model
2, Collins modifies his parser to provide a core-
adjunct prediction, thereby improving its perfor-
mance.
The Combinatory Categorial Grammar (CCG)
formulation models the core-adjunct distinction
explicitly. Therefore, any CCG parser can be used
as a core-adjunct classifier (Hockenmaier, 2003).
Subcategorization Acquisition. This task spec-
ifies for each predicate the number, type and order
of obligatory arguments. Determining the allow-
able subcategorization frames for a given predi-
cate necessarily involves separating its cores from
its allowable adjuncts (which are not framed). No-
table works in the field include (Briscoe and Car-
roll, 1997; Sarkar and Zeman, 2000; Korhonen,
2002). All these works used a parsed corpus in
order to collect, for each predicate, a set of hy-
pothesized subcategorization frames, to be filtered
by hypothesis testing methods.
This line of work differs from ours in a few
aspects. First, all works use manual or super-
vised syntactic annotations, usually including a
POS tagger. Second, the common approach to the
task focuses on syntax and tries to identify the en-
tire frame, rather than to tag each argument sep-
arately. Finally, most works address the task at
the verb type level, trying to detect the allowable
frames for each type. Consequently, the common
evaluation focuses on the quality of the allowable
frames acquired for each verb type, and not on the
classification of specific arguments in a given cor-
pus. Such a token level evaluation was conducted
in a few works (Briscoe and Carroll, 1997; Sarkar
and Zeman, 2000), but often with a small num-
ber of verbs or a small number of frames. A dis-
cussion of the differences between type and token
level evaluation can be found in (Reichart et al,
2010).
The core-adjunct distinction task was tackled in
the context of child language acquisition. Villav-
icencio (2002) developed a classifier based on
preposition selection and frequency information
for modeling the distinction for locative preposi-
tional phrases. Her approach is not entirely corpus
based, as it assumes the input sentences are given
in a basic logical form.
The study of prepositions is a vibrant research
area in NLP. A special issue of Computational Lin-
guistics, which includes an extensive survey of re-
lated work, was recently devoted to the field (Bald-
win et al, 2009).
228
3 Algorithm
We are given a (predicate, argument) pair in a test
sentence, and we need to determine whether the
argument is a core or an adjunct. Test arguments
are assumed to be correctly bracketed. We are al-
lowed to utilize a training corpus of raw text.
3.1 Overview
Our algorithm utilizes statistics based on the
(predicate, slot, argument head) (PSH) joint dis-
tribution (a slot is represented by its preposition).
To estimate this joint distribution, PSH samples
are extracted from the training corpus using unsu-
pervised POS taggers (Clark, 2003; Abend et al,
2010) and an unsupervised parser (Seginer, 2007).
As current performance of unsupervised parsers
for long sentences is low, we use only short sen-
tences (up to 10 words, excluding punctuation).
The length of test sentences is not bounded. Our
results will show that the training data accounts
well for the argument realization phenomena in
the test set, despite the length bound on its sen-
tences. The sample extraction process is detailed
in Section 3.2.
Our approach makes use of both aspects of the
distinction ? obligatoriness and compositionality.
We define three measures, one quantifying the
obligatoriness of the slot, another quantifying the
selectional preference of the verb to the argument
and a third that quantifies the association between
the head word and the slot irrespective of the pred-
icate (Section 3.3).
The measures? predictions are expected to coin-
cide in clear cases, but may be less successful in
others. Therefore, an ensemble-based method is
used to combine the three measures into a single
classifier. This results in a high accuracy classifier
with relatively low coverage. A self-training step
is now performed to increase coverage with only a
minor deterioration in accuracy (Section 3.4).
We focus on prepositional arguments. Non-
prepositional arguments in English tend to be
cores (e.g., in more than 85% of the cases in
PB sections 2?21), while prepositional arguments
tend to be equally divided between cores and ad-
juncts. The difficulty of the task thus lies in the
classification of prepositional arguments.
3.2 Data Collection
The statistical measures used by our classifier
are based on the (predicate, slot, argument head)
(PSH) joint distribution. This section details the
process of extracting samples from this joint dis-
tribution given a raw text corpus.
We start by parsing the corpus using the Seginer
parser (Seginer, 2007). This parser is unique in its
ability to induce a bracketing (unlabeled parsing)
from raw text (without even using POS tags) with
strong results. Its high speed (thousands of words
per second) allows us to use millions of sentences,
a prohibitive number for other parsers.
We continue by tagging the corpus using
Clark?s unsupervised POS tagger (Clark, 2003)
and the unsupervised Prototype Tagger (Abend et
al., 2010)2. The classes corresponding to preposi-
tions and to verbs are manually selected from the
induced clusters3. A preposition is defined to be
any word which is the first word of an argument
and belongs to a prepositions cluster. A verb is
any word belonging to a verb cluster. This manual
selection requires only a minute, since the number
of classes is very small (34 in our experiments).
In addition, knowing what is considered a prepo-
sition is part of the task definition itself.
Argument identification is hard even for super-
vised models and is considerably more so for un-
supervised ones (Abend et al, 2009). We there-
fore confine ourselves to sentences of length not
greater than 10 (excluding punctuation) which
contain a single verb. A sequence of words will
be marked as an argument of the verb if it is a con-
stituent that does not contain the verb (according
to the unsupervised parse tree), whose parent is
an ancestor of the verb. This follows the pruning
heuristic of (Xue and Palmer, 2004) often used by
SRL algorithms.
The corpus is now tagged using an unsupervised
POS tagger. Since the sentences in question are
short, we consider every word which does not be-
long to a closed class cluster as a head word (an
argument can have several head words). A closed
class is a class of function words with relatively
few word types, each of which is very frequent.
Typical examples include determiners, preposi-
tions and conjunctions. A class which is not closed
is open. In this paper, we define closed classes to
be clusters in which the ratio between the number
of word tokens and the number of word types ex-
2Clark?s tagger was replaced by the Prototype Tagger
where the latter gave a significant improvement. See Sec-
tion 4.
3We also explore a scenario in which they are identified
by a supervised tagger. See Section 4.
229
ceeds a threshold T 4.
Using these annotation layers, we traverse the
corpus and extract every (predicate, slot, argument
head) triplet. In case an argument has several head
words, each of them is considered as an inde-
pendent sample. We denote the number of times
that a triplet occurred in the training corpus by
N(p, s, h).
3.3 Collocation Measures
In this section we present the three types of mea-
sures used by the algorithm and the rationale be-
hind each of them. These measures are all based
on the PSH joint distribution.
Given a (predicate, prepositional argument) pair
from the test set, we first tag and parse the argu-
ment using the unsupervised tools above5. Each
word in the argument is now represented by its
word form (without lemmatization), its unsuper-
vised POS tag and its depth in the parse tree of the
argument. The last two will be used to determine
which are the head words of the argument (see be-
low). The head words themselves, once chosen,
are represented by the lemma. We now compute
the following measures.
Selectional Preference (SP). Since the seman-
tics of cores is more predicate dependent than the
semantics of adjuncts, we expect arguments for
which the predicate has a strong preference (in a
specific slot) to be cores.
Selectional preference induction is a well-
established task in NLP. It aims to quantify the
likelihood that a certain argument appears in a
certain slot of a predicate. Several methods have
been suggested (Resnik, 1996; Li and Abe, 1998;
Schulte im Walde et al, 2008).
We use the paradigm of (Erk, 2007). For a given
predicate slot pair (p, s), we define its preference
to the argument head h to be:
SP (p, s, h) =
?
h??Heads
Pr(h?|p, s) ? sim(h, h?)
Pr(h|p, s) = N(p, s, h)?h?N(p, s, h?)
sim(h, h?) is a similarity measure between argu-
ment heads. Heads is the set of all head words.
4We use sections 2?21 of the PTB WSJ for these counts,
containing 0.95M words. Our T was set to 50.
5Note that while current unsupervised parsers have low
performance on long sentences, arguments, even in long sen-
tences, are usually still short enough for them to operate well.
Their average length in the test set is 5.1 words.
This is a natural extension of the naive (and sparse)
maximum likelihood estimator Pr(h|p, s), which
is obtained by taking sim(h, h?) to be 1 if h = h?
and 0 otherwise.
The similarity measure we use is based on the
slot distributions of the arguments. That is, two
arguments are considered similar if they tend to
appear in the same slots. Each head word h is as-
signed a vector where each coordinate corresponds
to a slot s. The value of the coordinate is the num-
ber of times h appeared in s, i.e. ?p?N(p?, s, h)
(p? is summed over all predicates). The similarity
measure between two head words is then defined
as the cosine measure of their vectors.
Since arguments in the test set can be quite long,
not every open class word in the argument is taken
to be a head word. Instead, only those appearing in
the top level (depth = 1) of the argument under its
unsupervised parse tree are taken. In case there are
no such open class words, we take those appearing
in depth 2. The selectional preference of the whole
argument is then defined to be the arithmetic mean
of this measure over all of its head words. If the ar-
gument has no head words under this definition or
if none of the head words appeared in the training
corpus, the selectional preference is undefined.
Predicate-Slot Collocation. Since cores are
obligatory, when a predicate persistently appears
with an argument in a certain slot, the arguments
in this slot tends to be cores. This notion can be
captured by the (predicate, slot) joint distribu-
tion. We use the Pointwise Mutual Information
measure (PMI) to capture the slot and the predi-
cate?s collocation tendency. Let p be a predicate
and s a slot, then:
PS(p, s) = PMI(p, s) = log Pr(p, s)
Pr(s) ? Pr(p) =
= log N(p, s)?p?,s?N(p
?, s?)
?s?N(p, s?)?p?N(p?, s)
Since there is only a meager number of possi-
ble slots (that is, of prepositions), estimating the
(predicate, slot) distribution can be made by the
maximum likelihood estimator with manageable
sparsity.
In order not to bias the counts towards predi-
cates which tend to take more arguments, we de-
fine here N(p, s) to be the number of times the
(p, s) pair occurred in the training corpus, irre-
spective of the number of head words the argu-
ment had (and not e.g., ?hN(p, s, h)). Argu-
230
ments with no prepositions are included in these
counts as well (with s = NULL), so not to bias
against predicates which tend to have less non-
prepositional arguments.
Argument-Slot Collocation. Adjuncts tend to
belong to one of a few specific semantic domains
(see Section 2). Therefore, if an argument tends to
appear in a certain slot in many of its instances, it
is an indication that this argument tends to have a
consistent semantic flavor in most of its instances.
In this case, the argument and the preposition can
be viewed as forming a unit on their own, indepen-
dent of the predicate with which they appear. We
therefore expect such arguments to be adjuncts.
We formalize this notion using the following
measure. Let p, s, h be a predicate, a slot and a
head word respectively. We then use6:
AS(s, h) = 1?Pr(s|h) = 1? ?p?N(p
?, s, h)
?p?,s?N(p?, s?, h)
We select the head words of the argument as
we did with the selectional preference measure.
Again, the AS of the whole argument is defined
to be the arithmetic mean of the measure over all
of its head words.
Thresholding. In order to turn these measures
into classifiers, we set a threshold below which ar-
guments are marked as adjuncts and above which
as cores. In order to avoid tuning a parameter for
each of the measures, we set the threshold as the
median value of this measure in the test set. That
is, we find the threshold which tags half of the ar-
guments as cores and half as adjuncts. This relies
on the prior knowledge that prepositional argu-
ments are roughly equally divided between cores
and adjuncts7.
3.4 Combination Model
The algorithm proceeds to integrate the predic-
tions of the weak classifiers into a single classi-
fier. We use an ensemble method (Breiman, 1996).
Each of the classifiers may either classify an argu-
ment as an adjunct, classify it as a core, or ab-
stain. In order to obtain a high accuracy classifier,
to be used for self-training below, the ensemble
classifier only tags arguments for which none of
6The conditional probability is subtracted from 1 so that
higher values correspond to cores, as with the other measures.
7In case the test data is small, we can use the median value
on the training data instead.
the classifiers abstained, i.e., when sufficient infor-
mation was available to make all three predictions.
The prediction is determined by the majority vote.
The ensemble classifier has high precision but
low coverage. In order to increase its coverage, a
self-training step is performed. We observe that a
predicate and a slot generally determine whether
the argument is a core or an adjunct. For instance,
in our development data, a classifier which assigns
all arguments that share a predicate and a slot their
most common label, yields 94.3% accuracy on the
pairs appearing at least 5 times. This property of
the core-adjunct distinction greatly simplifies the
task for supervised algorithms (see Section 2).
We therefore apply the following procedure: (1)
tag the training data with the ensemble classifier;
(2) for each test sample x, if more than a ratio of ?
of the training samples sharing the same predicate
and slot with x are labeled as cores, tag x as core.
Otherwise, tag x as adjunct.
Test samples which do not share a predicate and
a slot with any training sample are considered out
of coverage. The parameter ? is chosen so half
of the arguments are tagged as cores and half as
adjuncts. In our experiments ? was about 0.25.
4 Experimental Setup
Experiments were conducted in two scenarios. In
the ?SID? (supervised identification of prepositions
and verbs) scenario, a gold standard list of prepo-
sitions was provided. The list was generated by
taking every word tagged by the preposition tag
(?IN?) in at least one of its instances under the
gold standard annotation of the WSJ sections 2?
21. Verbs were identified using MXPOST (Ratna-
parkhi, 1996). Words tagged with any of the verb
tags, except of the auxiliary verbs (?have?, ?be? and
?do?) were considered predicates. This scenario
decouples the accuracy of the algorithm from the
quality of the unsupervised POS tagging.
In the ?Fully Unsupervised? scenario, preposi-
tions and verbs were identified using Clark?s tag-
ger (Clark, 2003). It was asked to produce a tag-
ging into 34 classes. The classes corresponding
to prepositions and to verbs were manually identi-
fied. Prepositions in the test set were detected with
84.2% precision and 91.6% recall.
The prediction of whether a word belongs to an
open class or a closed was based on the output of
the Prototype tagger (Abend et al, 2010). The
Prototype tagger provided significantly more ac-
231
curate predictions in this context than Clark?s.
The 39832 sentences of PropBank?s sections 2?
21 were used as a test set without bounding their
lengths8. Cores were defined to be any argument
bearing the labels ?A0? ? ?A5?, ?C-A0? ? ?C-A5?
or ?R-A0? ? ?R-A5?. Adjuncts were defined to
be arguments bearing the labels ?AM?, ?C-AM? or
?R-AM?. Modals (?AM-MOD?) and negation mod-
ifiers (?AM-NEG?) were omitted since they do not
represent adjuncts.
The test set includes 213473 arguments, 45939
(21.5%) are prepositional. Of the latter, 22442
(48.9%) are cores and 23497 (51.1%) are adjuncts.
The non-prepositional arguments include 145767
(87%) cores and 21767 (13%) adjuncts. The aver-
age number of words per argument is 5.1.
The NANC (Graff, 1995) corpus was used as a
training set. Only sentences of length not greater
than 10 excluding punctuation were used (see Sec-
tion 3.2), totaling 4955181 sentences. 7673878
(5635810) arguments were identified in the ?SID?
(?Fully Unsupervised?) scenario. The average
number of words per argument is 1.6 (1.7).
Since this is the first work to tackle this task
using neither manual nor supervised syntactic an-
notation, there is no previous work to compare
to. However, we do compare against a non-trivial
baseline, which closely follows the rationale of
cores as obligatory arguments.
Our Window Baseline tags a corpus using MX-
POST and computes, for each predicate and
preposition, the ratio between the number of times
that the preposition appeared in a window of W
words after the verb and the total number of
times that the verb appeared. If this number ex-
ceeds a certain threshold ?, all arguments hav-
ing that predicate and preposition are tagged as
cores. Otherwise, they are tagged as adjuncts. We
used 18.7M sentences from NANC of unbounded
length for this baseline. W and ? were fine-tuned
against the test set9.
We also report results for partial versions of
the algorithm, starting with the three measures
used (selectional preference, predicate-slot col-
location and argument-slot collocation). Results
for the ensemble classifier (prior to the bootstrap-
ping stage) are presented in two variants: one
8The first 15K arguments were used for the algorithm?s
development and therefore excluded from the evaluation.
9Their optimal value was found to be W=2, ?=0.03. The
low optimal value of ? is an indication of the noisiness of this
technique.
in which the ensemble is used to tag arguments
for which all three measures give a prediction
(the ?Ensemble(Intersection)? classifier) and one
in which the ensemble tags all arguments for
which at least one classifier gives a prediction (the
?Ensemble(Union)? classifier). For the latter, a tie
is broken in favor of the core label. The ?Ensem-
ble(Union)? classifier is not a part of our model
and is evaluated only as a reference.
In order to provide a broader perspective on the
task, we compare the measures in the basis of our
algorithm to simplified or alternative measures.
We experiment with the following measures:
1. Simple SP ? a selectional preference measure
defined to be Pr(head|slot, predicate).
2. Vast Corpus SP ? similar to ?Simple SP?
but with a much larger corpus. It uses roughly
100M arguments which were extracted from the
web-crawling based corpus of (Gabrilovich and
Markovitch, 2005) and the British National Cor-
pus (Burnard, 2000).
3. Thesaurus SP ? a selectional preference mea-
sure which follows the paradigm of (Erk, 2007)
(Section 3.3) and defines the similarity between
two heads to be the Jaccard affinity between their
two entries in Lin?s automatically compiled the-
saurus (Lin, 1998)10.
4. Pr(slot|predicate) ? an alternative to the used
predicate-slot collocation measure.
5. PMI(slot, head) ? an alternative to the used
argument-slot collocation measure.
6. Head Dependence ? the entropy of the pred-
icate distribution given the slot and the head (fol-
lowing (Merlo and Esteve Ferrer, 2006)):
HD(s, h) = ??pPr(p|s, h) ? log(Pr(p|s, h))
Low entropy implies a core.
For each of the scenarios and the algorithms,
we report accuracy, coverage and effective accu-
racy. Effective accuracy is defined to be the ac-
curacy obtained when all out of coverage argu-
ments are tagged as adjuncts. This procedure al-
ways yields a classifier with 100% coverage and
therefore provides an even ground for comparing
the algorithms? performance.
We see accuracy as important on its own right
since increasing coverage is often straightforward
given easily obtainable larger training corpora.
10Since we aim for a minimally supervised scenario,
we used the proximity-based version of his thesaurus
which does not require parsing as pre-processing.
http://webdocs.cs.ualberta.ca/?lindek/Downloads/sims.lsp.gz
232
Collocation Measures Ensemble + Cov.
Sel. Preference Pred-Slot Arg-Slot Ensemble(I) Ensemble(U) E(I) + ST
SID Scenario Accuracy 65.6 64.5 72.4 74.1 68.7 70.6
Coverage 35.6 77.8 44.7 33.2 88.1 74.2
Eff. Acc. 56.7 64.8 58.8 58.8 67.8 68.4
Fully Unsupervised Accuracy 62.6 61.1 69.4 70.6 64.8 68.8
Scenario Coverage 24.8 59.0 38.7 22.8 74.2 56.9
Eff. Acc. 52.6 57.5 55.8 53.8 61.0 61.4
Table 1: Results for the various models. Accuracy, coverage and effective accuracy are presented in percents. Effective
accuracy is defined to be the accuracy resulting from labeling each out of coverage argument with an adjunct label. The
rows represent the following models (left to right): selectional preference, predicate-slot collocation, argument-slot collocation,
?Ensemble(Intersection)?, ?Ensemble(Union)? and the ?Ensemble(Intersection)? followed by self-training (see Section 3.4). ?En-
semble(Intersection)? obtains the highest accuracy. The ensemble + self-training obtains the highest effective accuracy.
Selectional Preference Measures Pred-Slot Measures Arg-Slot Measures
SP? S. SP V.C. SP Lin SP PS? Pr(s|p) Window AS? PMI(s, h) HD
Acc. 65.6 41.6 44.8 49.9 64.5 58.9 64.1 72.4 67.5 67.4
Cov. 35.6 36.9 45.3 36.7 77.8 77.8 92.6 44.7 44.7 44.7
Eff. Acc. 56.7 48.2 47.7 51.3 64.8 60.5 65.0 58.8 56.6 56.6
Table 2: Comparison of the measures used by our model to alternative measures in the ?SID? scenario. Results are in percents.
The sections of the table are (from left to right): selectional preference measures, predicate-slot measures, argument-slot mea-
sures and head dependence. The measures are (left to right): SP?, Simple SP, Vast Corpus SP, Lin SP, PS?, Pr(slot|predicate),
Window Baseline, AS?, PMI(slot, head) and Head Dependence. The measures marked with ? are the ones used by our model.
See Section 4.
Another reason is that a high accuracy classifier
may provide training data to be used by subse-
quent supervised algorithms.
For completeness, we also provide results for
the entire set of arguments. The great majority of
non-prepositional arguments are cores (87% in the
test set). We therefore tag all non-prepositional as
cores and tag prepositional arguments using our
model. In order to minimize supervision, we dis-
tinguish between the prepositional and the non-
prepositional arguments using Clark?s tagger.
Finally, we experiment on a scenario where
even argument identification on the test set is
not provided, but performed by the algorithm of
(Abend et al, 2009), which uses neither syntactic
nor SRL annotation but does utilize a supervised
POS tagger. We therefore run it in the ?SID? sce-
nario. We apply it to the sentences of length at
most 10 contained in sections 2?21 of PB (11586
arguments in 6007 sentences). Non-prepositional
arguments are invariably tagged as cores and out
of coverage prepositional arguments as adjuncts.
We report labeled and unlabeled recall, preci-
sion and F-scores for this experiment. An un-
labeled match is defined to be an argument that
agrees in its boundaries with a gold standard ar-
gument and a labeled match requires in addition
that the arguments agree in their core/adjunct la-
bel. We also report labeling accuracy which is the
ratio between the number of labeled matches and
the number of unlabeled matches11.
5 Results
Table 1 presents the results of our main experi-
ments. In both scenarios, the most accurate of the
three basic classifiers was the argument-slot col-
location classifier. This is an indication that the
collocation between the argument and the prepo-
sition is more indicative of the core/adjunct label
than the obligatoriness of the slot (as expressed by
the predicate-slot collocation).
Indeed, we can find examples where adjuncts,
although optional, appear very often with a certain
verb. An example is ?meet?, which often takes a
temporal adjunct, as in ?Let?s meet [in July]?. This
is a semantic property of ?meet?, whose syntactic
expression is not obligatory.
All measures suffered from a comparable dete-
rioration of accuracy when moving from the ?SID?
to the ?Fully Unsupervised? scenario. The dete-
rioration in coverage, however, was considerably
lower for the argument-slot collocation.
The ?Ensemble(Intersection)? model in both
cases is more accurate than each of the basic clas-
sifiers alone. This is to be expected as it combines
the predictions of all three. The self-training step
significantly increases the ensemble model?s cov-
11Note that the reported unlabeled scores are slightly lower
than those reported in the 2009 paper, due to the exclusion of
the modals and negation modifiers.
233
Precision Recall F-score lAcc.
Unlabeled 50.7 66.3 57.5 ?
Labeled 42.4 55.4 48.0 83.6
Table 3: Unlabeled and labeled scores for the experi-
ments using the unsupervised argument identification system
of (Abend et al, 2009). Precision, recall, F-score and label-
ing accuracy are given in percents.
erage (with some loss in accuracy), thus obtaining
the highest effective accuracy. It is also more accu-
rate than the simpler classifier ?Ensemble(Union)?
(although the latter?s coverage is higher).
Table 2 presents results for the comparison to
simpler or alternative measures. Results indicate
that the three measures used by our algorithm
(leftmost column in each section) obtain superior
results. The only case in which performance is
comparable is the window baseline compared to
the Pred-Slot measure. However, the baseline?s
score was obtained by using a much larger corpus
and a careful hand-tuning of the parameters12.
The poor performance of Simple SP can be as-
cribed to sparsity. This is demonstrated by the
median value of 0, which this measure obtained
on the test set. Accuracy is only somewhat better
with a much larger corpus (Vast Corpus SP). The
Thesaurus SP most probably failed due to insuffi-
cient coverage, despite its applicability in a similar
supervised task (Zapirain et al, 2009).
The Head Dependence measure achieves a rel-
atively high accuracy of 67.4%. We therefore at-
tempted to incorporate it into our model, but failed
to achieve a significant improvement to the overall
result. We expect a further study of the relations
between the measures will suggest better ways of
combining their predictions.
The obtained effective accuracy for the entire
set of arguments, where the prepositional argu-
ments are automatically identified, was 81.6%.
Table 3 presents results of our experiments with
the unsupervised argument identification model
of (Abend et al, 2009). The unlabeled scores
reflect performance on argument identification
alone, while the labeled scores reflect the joint per-
formance of both the 2009 and our algorithms.
These results, albeit low, are potentially benefi-
cial for unsupervised subcategorization acquisi-
tion. The accuracy of our model on the entire
set (prepositional argument subset) of correctly
identified arguments was 83.6% (71.7%). This is
12We tried about 150 parameter pairs for the baseline. The
average of the five best effective accuracies was 64.3%.
somewhat higher than the score on the entire test
set (?SID? scenario), which was 83.0% (68.4%),
probably due to the bounded length of the test sen-
tences in this case.
6 Conclusion
We presented a fully unsupervised algorithm for
the classification of arguments into cores and ad-
juncts. Since most non-prepositional arguments
are cores, we focused on prepositional arguments,
which are roughly equally divided between cores
and adjuncts. The algorithm computes three sta-
tistical measures and utilizes ensemble-based and
self-training methods to combine their predictions.
The algorithm applies state-of-the-art unsuper-
vised parser and POS tagger to collect statistics
from a large raw text corpus. It obtains an accu-
racy of roughly 70%. We also show that (some-
what surprisingly) an argument-slot collocation
measure gives more accurate predictions than a
predicate-slot collocation measure on this task.
We speculate the reason is that the head word dis-
ambiguates the preposition and that this disam-
biguation generally determines whether a preposi-
tional argument is a core or an adjunct (somewhat
independently of the predicate). This calls for
a future study into the semantics of prepositions
and their relation to the core-adjunct distinction.
In this context two recent projects, The Preposi-
tion Project (Litkowski and Hargraves, 2005) and
PrepNet (Saint-Dizier, 2006), which attempt to
characterize and categorize the complex syntactic
and semantic behavior of prepositions, may be of
relevance.
It is our hope that this work will provide a better
understanding of core-adjunct phenomena. Cur-
rent supervised SRL models tend to perform worse
on adjuncts than on cores (Pradhan et al, 2008;
Toutanova et al, 2008). We believe a better under-
standing of the differences between cores and ad-
juncts may contribute to the development of better
SRL techniques, in both its supervised and unsu-
pervised variants.
References
Omri Abend, Roi Reichart and Ari Rappoport, 2009.
Unsupervised Argument Identification for Semantic
Role Labeling. ACL ?09.
Omri Abend, Roi Reichart and Ari Rappoport, 2010.
Improved Unsupervised POS Induction through Pro-
totype Discovery. ACL ?10.
234
Collin F. Baker, Charles J. Fillmore and John B. Lowe,
1998. The Berkeley FrameNet Project. ACL-
COLING ?98.
Timothy Baldwin, Valia Kordoni and Aline Villavicen-
cio, 2009. Prepositions in Applications: A Sur-
vey and Introduction to the Special Issue. Computa-
tional Linguistics, 35(2):119?147.
Ram Boukobza and Ari Rappoport, 2009. Multi-
Word Expression Identification Using Sentence Sur-
face Features. EMNLP ?09.
Leo Breiman, 1996. Bagging Predictors. Machine
Learning, 24(2):123?140.
Ted Briscoe and John Carroll, 1997. Automatic Ex-
traction of Subcategorization from Corpora. Ap-
plied NLP ?97.
Lou Burnard, 2000. User Reference Guide for the
British National Corpus. Technical report, Oxford
University.
Xavier Carreras and Llu?`s Ma`rquez, 2005. Intro-
duction to the CoNLL?2005 Shared Task: Semantic
Role Labeling. CoNLL ?05.
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech In-
duction. EACL ?03.
Michael Collins, 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
David Dowty, 2000. The Dual Analysis of Adjuncts
and Complements in Categorial Grammar. Modify-
ing Adjuncts, ed. Lang, Maienborn and Fabricius?
Hansen, de Gruyter, 2003.
Katrin Erk, 2007. A Simple, Similarity-based Model
for Selectional Preferences. ACL ?07.
Evgeniy Gabrilovich and Shaul Markovitch, 2005.
Feature Generation for Text Categorization using
World Knowledge. IJCAI ?05.
David Graff, 1995. North American News Text Cor-
pus. Linguistic Data Consortium. LDC95T21.
Trond Grenager and Christopher D. Manning, 2006.
Unsupervised Discovery of a Statistical Verb Lexi-
con. EMNLP ?06.
Donald Hindle and Mats Rooth, 1993. Structural Am-
biguity and Lexical Relations. Computational Lin-
guistics, 19(1):103?120.
Julia Hockenmaier, 2003. Data and Models for Sta-
tistical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Karin Kipper, Hoa Trang Dang and Martha Palmer,
2000. Class-Based Construction of a Verb Lexicon.
AAAI ?00.
Anna Korhonen, 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge.
Hang Li and Naoki Abe, 1998. Generalizing Case
Frames using a Thesaurus and the MDL Principle.
Computational Linguistics, 24(2):217?244.
Wei Li, Xiuhong Zhang, Cheng Niu, Yuankai Jiang and
Rohini Srihari, 2003. An Expert Lexicon Approach
to Identifying English Phrasal Verbs. ACL ?03.
Dekang Lin, 1998. Automatic Retrieval and Cluster-
ing of Similar Words. COLING?ACL ?98.
Ken Litkowski and Orin Hargraves, 2005. The Prepo-
sition Project. ACL-SIGSEM Workshop on ?The
Linguistic Dimensions of Prepositions and Their
Use in Computational Linguistic Formalisms and
Applications?.
Diana McCarthy, 2001. Lexical Acquisition at the
Syntax-Semantics Interface: Diathesis Alternations,
Subcategorization Frames and Selectional Prefer-
ences. Ph.D. thesis, University of Sussex.
Paula Merlo and Eva Esteve Ferrer, 2006. The No-
tion of Argument in Prepositional Phrase Attach-
ment. Computational Linguistics, 32(3):341?377.
Martha Palmer, Daniel Gildea and Paul Kingsbury,
2005. The Proposition Bank: A Corpus Annotated
with Semantic Roles. Computational Linguistics,
31(1):71?106.
Sameer Pradhan, Wayne Ward and James H. Martin,
2008. Towards Robust Semantic Role Labeling.
Computational Linguistics, 34(2):289?310.
Vasin Punyakanok, Dan Roth and Wen-tau Yih, 2008.
The Importance of Syntactic Parsing and Inference
in Semantic Role Labeling. Computational Linguis-
tics, 34(2):257?287.
Adwait Ratnaparkhi, 1996. Maximum Entropy Part-
Of-Speech Tagger. EMNLP ?96.
Roi Reichart, Omri Abend and Ari Rappoport, 2010.
Type Level Clustering Evaluation: New Measures
and a POS Induction Case Study. CoNLL ?10.
Philip Resnik, 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61:127?159.
Patrick Saint-Dizier, 2006. PrepNet: A Multilingual
Lexical Description of Prepositions. LREC ?06.
Anoop Sarkar and Daniel Zeman, 2000. Automatic
Extraction of Subcategorization Frames for Czech.
COLING ?00.
Sabine Schulte im Walde, Christian Hying, Christian
Scheible and Helmut Schmid, 2008. Combining
EM Training and the MDL Principle for an Auto-
matic Verb Classification Incorporating Selectional
Preferences. ACL ?08.
235
Yoav Seginer, 2007. Fast Unsupervised Incremental
Parsing. ACL ?07.
Caroline Sporleder and Linlin Li, 2009. Unsupervised
Recognition of Literal and Non-Literal Use of Id-
iomatic Expressions. EACL ?09.
Robert S. Swier and Suzanne Stevenson, 2004. Unsu-
pervised Semantic Role Labeling. EMNLP ?04.
Robert S. Swier and Suzanne Stevenson, 2005. Ex-
ploiting a Verb Lexicon in Automatic Semantic Role
Labelling. EMNLP ?05.
Kristina Toutanova, Aria Haghighi and Christopher D.
Manning, 2008. A Global Joint Model for Se-
mantic Role Labeling. Computational Linguistics,
34(2):161?191.
Aline Villavicencio, 2002. Learning to Distinguish PP
Arguments from Adjuncts. CoNLL ?02.
Dave Willis, 2004. Collins Cobuild Intermedia En-
glish Grammar, Second Edition. HarperCollins Pub-
lishers.
Nianwen Xue and Martha Palmer, 2004. Calibrating
Features for Semantic Role Labeling. EMNLP ?04.
Ben?at Zapirain, Eneko Agirre and Llu??s Ma`rquez,
2009. Generalizing over Lexical Features: Selec-
tional Preferences for Semantic Role Classification.
ACL ?09, short paper.
236
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298?1307,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Improved Unsupervised POS Induction through Prototype Discovery
Omri Abend1? Roi Reichart2 Ari Rappoport1
1Institute of Computer Science, 2ICNC
Hebrew University of Jerusalem
{omria01|roiri|arir}@cs.huji.ac.il
Abstract
We present a novel fully unsupervised al-
gorithm for POS induction from plain text,
motivated by the cognitive notion of proto-
types. The algorithm first identifies land-
mark clusters of words, serving as the
cores of the induced POS categories. The
rest of the words are subsequently mapped
to these clusters. We utilize morpho-
logical and distributional representations
computed in a fully unsupervised manner.
We evaluate our algorithm on English and
German, achieving the best reported re-
sults for this task.
1 Introduction
Part-of-speech (POS) tagging is a fundamental
NLP task, used by a wide variety of applications.
However, there is no single standard POS tag-
ging scheme, even for English. Schemes vary
significantly across corpora and even more so
across languages, creating difficulties in using
POS tags across domains and for multi-lingual
systems (Jiang et al, 2009). Automatic induction
of POS tags from plain text can greatly alleviate
this problem, as well as eliminate the efforts in-
curred by manual annotations. It is also a problem
of great theoretical interest. Consequently, POS
induction is a vibrant research area (see Section 2).
In this paper we present an algorithm based
on the theory of prototypes (Taylor, 2003), which
posits that some members in cognitive categories
are more central than others. These practically de-
fine the category, while the membership of other
elements is based on their association with the
? Omri Abend is grateful to the Azrieli Foundation for
the award of an Azrieli Fellowship.
central members. Our algorithm first clusters
words based on a fine morphological representa-
tion. It then clusters the most frequent words,
defining landmark clusters which constitute the
cores of the categories. Finally, it maps the rest
of the words to these categories. The last two
stages utilize a distributional representation that
has been shown to be effective for unsupervised
parsing (Seginer, 2007).
We evaluated the algorithm in both English and
German, using four different mapping-based and
information theoretic clustering evaluation mea-
sures. The results obtained are generally better
than all existing POS induction algorithms.
Section 2 reviews related work. Sections 3 and
4 detail the algorithm. Sections 5, 6 and 7 describe
the evaluation, experimental setup and results.
2 Related Work
Unsupervised and semi-supervised POS tagging
have been tackled using a variety of methods.
Schu?tze (1995) applied latent semantic analysis.
The best reported results (when taking into ac-
count all evaluation measures, see Section 5) are
given by (Clark, 2003), which combines dis-
tributional and morphological information with
the likelihood function of the Brown algorithm
(Brown et al, 1992). Clark?s tagger is very sen-
sitive to its initialization. Reichart et al (2010b)
propose a method to identify the high quality runs
of this algorithm. In this paper, we show that
our algorithm outperforms not only Clark?s mean
performance, but often its best among 100 runs.
Most research views the task as a sequential la-
beling problem, using HMMs (Merialdo, 1994;
Banko and Moore, 2004; Wang and Schuurmans,
2005) and discriminative models (Smith and Eis-
ner, 2005; Haghighi and Klein, 2006). Several
1298
techniques were proposed to improve the HMM
model. A Bayesian approach was employed by
(Goldwater and Griffiths, 2007; Johnson, 2007;
Gao and Johnson, 2008). Van Gael et al (2009)
used the infinite HMM with non-parametric pri-
ors. Grac?a et al (2009) biased the model to induce
a small number of possible tags for each word.
The idea of utilizing seeds and expanding them
to less reliable data has been used in several pa-
pers. Haghighi and Klein (2006) use POS ?pro-
totypes? that are manually provided and tailored
to a particular POS tag set of a corpus. Fre-
itag (2004) and Biemann (2006) induce an ini-
tial clustering and use it to train an HMM model.
Dasgupta and Ng (2007) generate morphological
clusters and use them to bootstrap a distributional
model. Goldberg et al (2008) use linguistic con-
siderations for choosing a good starting point for
the EM algorithm. Zhao and Marcus (2009) ex-
pand a partial dictionary and use it to learn dis-
ambiguation rules. Their evaluation is only at the
type level and only for half of the words. Ravi
and Knight (2009) use a dictionary and an MDL-
inspired modification to the EM algorithm.
Many of these works use a dictionary provid-
ing allowable tags for each or some of the words.
While this scenario might reduce human annota-
tion efforts, it does not induce a tagging scheme
but remains tied to an existing one. It is further
criticized in (Goldwater and Griffiths, 2007).
Morphological representation. Many POS in-
duction models utilize morphology to some ex-
tent. Some use simplistic representations of termi-
nal letter sequences (e.g., (Smith and Eisner, 2005;
Haghighi and Klein, 2006)). Clark (2003) models
the entire letter sequence as an HMM and uses it
to define a morphological prior. Dasgupta and Ng
(2007) use the output of the Morfessor segmenta-
tion algorithm for their morphological representa-
tion. Morfessor (Creutz and Lagus, 2005), which
we use here as well, is an unsupervised algorithm
that segments words and classifies each segment
as being a stem or an affix. It has been tested on
several languages with strong results.
Our work has several unique aspects. First,
our clustering method discovers prototypes in a
fully unsupervised manner, mapping the rest of
the words according to their association with the
prototypes. Second, we use a distributional repre-
sentation which has been shown to be effective for
unsupervised parsing (Seginer, 2007). Third, we
use a morphological representation based on sig-
natures, which are sets of affixes that represent a
family of words sharing an inflectional or deriva-
tional morphology (Goldsmith, 2001).
3 Distributional Algorithm
Our algorithm is given a plain text corpus and op-
tionally a desired number of clusters k. Its output
is a partitioning of words into clusters. The al-
gorithm utilizes two representations, distributional
and morphological. Although eventually the latter
is used before the former, for clarity of presenta-
tion we begin by detailing the base distributional
algorithm. In the next section we describe the mor-
phological representation and its integration into
the base algorithm.
Overview. The algorithm consists of two main
stages: landmark clusters discovery, and word
mapping. For the former, we first compute a dis-
tributional representation for each word. We then
cluster the coordinates corresponding to high fre-
quency words. Finally, we define landmark clus-
ters. In the word mapping stage we map each word
to the most similar landmark cluster.
The rationale behind using only the high fre-
quency words in the first stage is twofold. First,
prototypical members of a category are frequent
(Taylor, 2003), and therefore we can expect the
salient POS tags to be represented in this small
subset. Second, higher frequency implies more re-
liable statistics. Since this stage determines the
cores of all resulting clusters, it should be as accu-
rate as possible.
Distributional representation. We use a sim-
plified form of the elegant representation of lexi-
cal entries used by the Seginer unsupervised parser
(Seginer, 2007). Since a POS tag reflects the
grammatical role of the word and since this rep-
resentation is effective to parsing, we were moti-
vated to apply it to the present task.
Let W be the set of word types in the corpus.
The right context entry of a word x ? W is a pair
of mappings r intx : W ? [0, 1] and r adjx :
W ? [0, 1]. For each w ? W , r adjx(w) is an
adjacency score of w to x, reflecting w?s tendency
to appear on the right hand side of x.
For each w ? W , r intx(w) is an interchange-
ability score of x with w, reflecting the tendency
of w to appear to the left of words that tend to ap-
pear to the right of x. This can be viewed as a
1299
similarity measure between words with respect to
their right context. The higher the scores the more
the words tend to be adjacent/interchangeable.
Left context parameters l intx and l adjx are
defined analogously.
There are important subtleties in these defini-
tions. First, for two words x,w ? W , r adjx(w)
is generally different from l adjw(x). For exam-
ple, if w is a high frequency word and x is a low
frequency word, it is likely that w appears many
times to the right of x, yielding a high r adjx(w),
but that x appears only a few times to the left of w
yielding a low l adjw(x). Second, from the defi-
nition of r intx(w) and r intw(x), it is clear that
they need not be equal.
These functions are computed incrementally by
a bootstrapping process. We initialize all map-
pings to be identically 0. We iterate over the words
in the training corpus. For every word instance x,
we take the word immediately to its right y and
update x?s right context using y?s left context:
?w ? W : r intx(w) +=
l adjy(w)
N(y)
?w ? W : r adjx(w) +=
{
1 w = y
l inty(w)
N(y) w 6= y
The division by N(y) (the number of times y
appears in the corpus before the update) is done in
order not to give a disproportional weight to high
frequency words. Also, r intx(w) and r adjx(w)
might become larger than 1. We therefore nor-
malize them after all updates are performed by the
number of occurrences of x in the corpus.
We update l intx and l adjx analogously using
the word z immediately to the left of x. The up-
dates of the left and right functions are done in
parallel.
We define the distributional representation of a
word type x to be a 4|W | + 2 dimensional vector
vx. Each word w yields four coordinates, one for
each direction (left/right) and one for each map-
ping type (int/adj). Two additional coordinates
represent the frequency in which the word appears
to the left and to the right of a stopping punc-
tuation. Of the 4|W | coordinates corresponding
to words, we allow only 2n to be non-zero: the
n top scoring among the right side coordinates
(those of r intx and r adjx), and the n top scoring
among the left side coordinates (those of l intx
and l adjx). We used n = 50.
The distance between two words is defined to
be one minus the cosine of the angle between their
representation vectors.
Coordinate clustering. Each of our landmark
clusters will correspond to a set of high frequency
words (HFWs). The number of HFWs is much
larger than the number of expected POS tags.
Hence we should cluster HFWs. Our algorithm
does that by unifying some of the non-zero coordi-
nates corresponding to HFWs in the distributional
representation defined above.
We extract the words that appear more than N
times per million1 and apply the following proce-
dure I times (5 in our experiments).
We run average link clustering with a threshold
? (AVGLINK?, (Jain et al, 1999)) on these words,
in each iteration initializing every HFW to have
its own cluster. AVGLINK? means running the av-
erage link algorithm until the two closest clusters
have a distance larger than ?. We then use the in-
duced clustering to update the distributional rep-
resentation, by collapsing all coordinates corre-
sponding to words appearing in the same cluster
into a single coordinate whose value is the sum
of the collapsed coordinates? values. In order to
produce a conservative (fine) clustering, we used a
relatively low ? value of 0.25.
Note that the AVGLINK? initialization in each
of the I iterations assigns each HFW to a sepa-
rate cluster. The iterations differ in the distribu-
tional representation of the HFWs, resulting from
the previous iterations.
In our English experiments, this process re-
duced the dimension of the HFWs set (the num-
ber of coordinates that are non-zero in at least one
of the HFWs) from 14365 to 10722. The aver-
age number of non-zero coordinates per word de-
creased from 102 to 55.
Since all eventual POS categories correspond to
clusters produced at this stage, to reduce noise we
delete clusters of less than five elements.
Landmark detection. We define landmark clus-
ters using the clustering obtained in the final iter-
ation of the coordinate clustering stage. However,
the number of clusters might be greater than the
desired number k, which is an optional parame-
ter of the algorithm. In this case we select a sub-
set of k clusters that best covers the HFW space.
We use the following heuristic. We start from the
most frequent cluster, and greedily select the clus-
1We used N = 100, yielding 1242 words for English and
613 words for German.
1300
ter farthest from the clusters already selected. The
distance between two clusters is defined to be the
average distance between their members. A clus-
ter?s distance from a set of clusters is defined to
be its minimal distance from the clusters in the
set. The final set of clusters {L1, ..., Lk} and their
members are referred to as landmark clusters and
prototypes, respectively.
Mapping all words. Each word w ? W is as-
signed the cluster Li that contains its nearest pro-
totype:
d(w,Li) = minx?Li{1 ? cos(vw, vx)}
Map(w) = argminLi{d(w,Li)}
Words that appear less than 5 times are consid-
ered as unknown words. We consider two schemes
for handling unknown words. One randomly maps
each such word to a cluster, using a probabil-
ity proportional to the number of unique known
words already assigned to that cluster. However,
when the number k of landmark clusters is rela-
tively large, it is beneficial to assign all unknown
words to a separate new cluster (after running the
algorithm with k? 1). In our experiments, we use
the first option when k is below some threshold
(we used 15), otherwise we use the second.
4 Morphological Model
The morphological model generates another word
clustering, based on the notion of a signature.
This clustering is integrated with the distributional
model as described below.
4.1 Morphological Representation
We use the Morfessor (Creutz and Lagus, 2005)
word segmentation algorithm. First, all words in
the corpus are segmented. Then, for each stem,
the set of all affixes with which it appears (its sig-
nature, (Goldsmith, 2001)) is collected. The mor-
phological representation of a word type is then
defined to be its stem?s signature in conjunction
with its specific affixes2 (See Figure 1).
We now collect all words having the same rep-
resentation. For instance, if the words joined and
painted are found to have the same signature, they
would share the same cluster since both have the
affix ? ed?. The word joins does not share the same
cluster with them since it has a different affix, ? s?.
This results in coarse-grained clusters exclusively
defined according to morphology.
2A word may contain more than a single affix.
Types join joins joined joining
Stem join join join join
Affixes ? s ed ing
Signature {?, ed, s, ing}
Figure 1: An example for a morphological representation,
defined to be the conjunction of its affix(es) with the stem?s
signature.
In addition, we incorporate capitalization infor-
mation into the model, by constraining all words
that appear capitalized in more than half of their
instances to belong to a separate cluster, regard-
less of their morphological representation. The
motivation for doing so is practical: capitalization
is used in many languages to mark grammatical
categories. For instance, in English capitalization
marks the category of proper names and in Ger-
man it marks the noun category . We report En-
glish results both with and without this modifica-
tion.
Words that contain non-alphanumeric charac-
ters are represented as the sequence of the non-
alphanumeric characters they include, e.g., ?vis-a`-
vis? is represented as (?-?, ?-?). We do not as-
sign a morphological representation to words in-
cluding more than one stem (like weatherman), to
words that have a null affix (i.e., where the word
is identical to its stem) and to words whose stem
is not shared by any other word (signature of size
1). Words that were not assigned a morphologi-
cal representation are included as singletons in the
morphological clustering.
4.2 Distributional-Morphological Algorithm
We detail the modifications made to our base
distributional algorithm given the morphological
clustering defined above.
Coordinate clustering and landmarks. We
constrain AVGLINK? to begin by forming links be-
tween words appearing in the same morphologi-
cal cluster. Only when the distance between the
two closest clusters gets above ? we remove this
constraint and proceed as before. This is equiv-
alent to performing AVGLINK? separately within
each morphological cluster and then using the re-
sult as an initial condition for an AVGLINK? coor-
dinate clustering. The modified algorithm in this
stage is otherwise identical to the distributional al-
gorithm.
Word mapping. In this stage words that are not
prototypes are mapped to one of the landmark
1301
clusters. A reasonable strategy would be to map
all words sharing a morphological cluster as a sin-
gle unit. However, these clusters are too coarse-
grained. We therefore begin by partitioning the
morphological clusters into sub-clusters according
to their distributional behavior. We do so by apply-
ing AVGLINK? (the same as AVGLINK? but with a
different parameter) to each morphological clus-
ter. Since our goal is cluster refinement, we use a
? that is considerably higher than ? (0.9).
We then find the closest prototype to each such
sub-cluster (averaging the distance across all of
the latter?s members) and map it as a single unit
to the cluster containing that prototype.
5 Clustering Evaluation
We evaluate the clustering produced by our algo-
rithm using an external quality measure: we take
a corpus tagged by gold standard tags, tag it using
the induced tags, and compare the two taggings.
There is no single accepted measure quantifying
the similarity between two taggings. In order to
be as thorough as possible, we report results using
four known measures, two mapping-based mea-
sures and two information theoretic ones.
Mapping-based measures. The induced clus-
ters have arbitrary names. We define two map-
ping schemes between them and the gold clus-
ters. After the induced clusters are mapped, we
can compute a derived accuracy. The Many-to-1
measure finds the mapping between the gold stan-
dard clusters and the induced clusters which max-
imizes accuracy, allowing several induced clusters
to be mapped to the same gold standard cluster.
The 1-to-1 measure finds the mapping between
the induced and gold standard clusters which max-
imizes accuracy such that no two induced clus-
ters are mapped to the same gold cluster. Com-
puting this mapping is equivalent to finding the
maximal weighted matching in a bipartite graph,
whose weights are given by the intersection sizes
between matched classes/clusters. As in (Reichart
and Rappoport, 2008), we use the Kuhn-Munkres
algorithm (Kuhn, 1955; Munkres, 1957) to solve
this problem.
Information theoretic measures. These are
based on the observation that a good clustering re-
duces the uncertainty of the gold tag given the in-
duced cluster, and vice-versa. Several such mea-
sures exist; we use V (Rosenberg and Hirschberg,
2007) and NVI (Reichart and Rappoport, 2009),
VI?s (Meila, 2007) normalized version.
6 Experimental Setup
Since a goal of unsupervised POS tagging is in-
ducing an annotation scheme, comparison to an
existing scheme is problematic. To address this
problem we compare to three different schemes
in two languages. In addition, the two English
schemes we compare with were designed to tag
corpora contained in our training set, and have
been widely and successfully used with these cor-
pora by a large number of applications.
Our algorithm was run with the exact same pa-
rameters on both languages: N = 100 (high fre-
quency threshold), n = 50 (the parameter that
determines the effective number of coordinates),
? = 0.25 (cluster separation during landmark
cluster generation), ? = 0.9 (cluster separation
during refinement of morphological clusters).
The algorithm we compare with in most detail
is (Clark, 2003), which reports the best current
results for this problem (see Section 7). Since
Clark?s algorithm is sensitive to its initialization,
we ran it a 100 times and report its average and
standard deviation in each of the four measures.
In addition, we report the percentile in which our
result falls with respect to these 100 runs.
Punctuation marks are very frequent in corpora
and are easy to cluster. As a result, including them
in the evaluation greatly inflates the scores. For
this reason we do not assign a cluster to punctua-
tion marks and we report results using this policy,
which we recommend for future work. However,
to be able to directly compare with previous work,
we also report results for the full POS tag set.
We do so by assigning a singleton cluster to each
punctuation mark (in addition to the k required
clusters). This simple heuristic yields very high
performance on punctuation, scoring (when all
other words are assumed perfect tagging) 99.6%
(99.1%) 1-to-1 accuracy when evaluated against
the English fine (coarse) POS tag sets, and 97.2%
when evaluated against the German POS tag set.
For English, we trained our model on the
39832 sentences which constitute sections 2-21 of
the PTB-WSJ and on the 500K sentences from
the NYT section of the NANC newswire corpus
(Graff, 1995). We report results on the WSJ part
of our data, which includes 950028 words tokens
in 44389 types. Of the tokens, 832629 (87.6%)
1302
English Fine k=13 Coarse k=13 Fine k=34
Prototype Clark Prototype Clark Prototype Clark
Tagger ? ? % Tagger ? ? % Tagger ? ? %
Many?to?1 61.0 55.1 1.6 100 70.0 66.9 2.1 94 71.6 69.8 1.5 90
55.5 48.8 1.8 100 66.1 62.6 2.3 94 67.5 65.5 1.7 90
1?to?1 60.0 52.2 1.9 100 58.1 49.4 2.9 100 63.5 54.5 1.6 100
54.9 46.0 2.2 100 53.7 43.8 3.3 100 58.8 48.5 1.8 100
NVI 0.652 0.773 0.027 100 0.841 0.972 0.036 100 0.663 0.725 0.018 100
0.795 0.943 0.033 100 1.052 1.221 0.046 100 0.809 0.885 0.022 100
V 0.636 0.581 0.015 100 0.590 0.543 0.018 100 0.677 0.659 0.008 100
0.542 0.478 0.019 100 0.484 0.429 0.023 100 0.608 0.588 0.010 98
German k=17 k=26
Prototype Clark Prototype Clark
Tagger ? ? % Tagger ? ? %
Many?to-1 64.6 64.7 1.2 41 68.2 67.8 1.0 60
58.9 59.1 1.4 40 63.2 62.8 1.2 60
1?to?1 53.7 52.0 1.8 77 56.0 52.0 2.1 99
48.0 46.0 2.3 78 50.7 45.9 2.6 99
NVI 0.667 0.675 0.019 66 0.640 0.682 0.019 100
0.819 0.829 0.025 66 0.785 0.839 0.025 100
V 0.646 0.645 0.010 50 0.675 0.657 0.008 100
0.552 0.553 0.013 48 0.596 0.574 0.010 100
Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark?s average score (?),
Clark?s standard deviation (?) and the fraction of Clark?s results that scored worse than our model (%). For the mapping based
measures, results are accuracy percentage. For V ? [0, 1], higher is better. For high quality output, NV I ? [0, 1] as well, and
lower is better. In each entry, the top number indicates the score when including punctuation and the bottom number the score
when excluding it. In English, our results are always better than Clark?s. In German, they are almost always better.
are not punctuation. The percentage of unknown
words (those appearing less than five times) is
1.6%. There are 45 clusters in this annotation
scheme, 34 of which are not punctuation.
We ran each algorithm both with k=13 and
k=34 (the number of desired clusters). We com-
pare the output to two annotation schemes: the fine
grained PTB WSJ scheme, and the coarse grained
tags defined in (Smith and Eisner, 2005). The
output of the k=13 run is evaluated both against
the coarse POS tag annotation (the ?Coarse k=13?
scenario) and against the full PTB-WSJ annotation
scheme (the ?Fine k=13? scenario). The k=34 run
is evaluated against the full PTB-WSJ annotation
scheme (the ?Fine k=34? scenario).
The POS cluster frequency distribution tends to
be skewed: each of the 13 most frequent clusters
in the PTB-WSJ cover more than 2.5% of the to-
kens (excluding punctuation) and together 86.3%
of them. We therefore chose k=13, since it is both
the number of coarse POS tags (excluding punctu-
ation) as well as the number of frequent POS tags
in the PTB-WSJ annotation scheme. We chose
k=34 in order to evaluate against the full 34 tags
PTB-WSJ annotation scheme (excluding punctua-
tion) using the same number of clusters.
For German, we trained our model on the 20296
sentences of the NEGRA corpus (Brants, 1997)
and on the first 450K sentences of the DeWAC
corpus (Baroni et al, 2009). DeWAC is a cor-
pus extracted by web crawling and is therefore
out of domain. We report results on the NEGRA
part, which includes 346320 word tokens of 49402
types. Of the tokens, 289268 (83.5%) are not
punctuation. The percentage of unknown words
(those appearing less than five times) is 8.1%.
There are 62 clusters in this annotation scheme,
51 of which are not punctuation.
We ran the algorithms with k=17 and k=26.
k=26 was chosen since it is the number of clus-
ters that cover each more than 0.5% of the NE-
GRA tokens, and in total cover 96% of the (non-
punctuation) tokens. In order to test our algo-
rithm in another scenario, we conducted experi-
ments with k=17 as well, which covers 89.9% of
the tokens. All outputs are compared against NE-
GRA?s gold standard scheme.
We do not report results for k=51 (where the
number of gold clusters is the same as the number
of induced clusters), since our algorithm produced
only 42 clusters in the landmark detection stage.
We could of course have modified the parame-
ters to allow our algorithm to produce 51 clusters.
However, we wanted to use the exact same param-
eters as those used for the English experiments to
minimize the issue of parameter tuning.
In addition to the comparisons described above,
we present results of experiments (in the ?Fine
1303
B B+M B+C F(I=1) F
M-to-1 53.3 54.8 58.2 57.3 61.0
1-to-1 50.2 51.7 55.1 54.8 60.0
NVI 0.782 0.720 0.710 0.742 0.652
V 0.569 0.598 0.615 0.597 0.636
Table 2: A comparison of partial versions of the model in
the ?Fine k=13? WSJ scenario. M-to-1 and 1-to-1 results are
reported in accuracy percentage. Lower NVI is better. B is the
strictly distributional algorithm, B+M adds the morphologi-
cal model, B+C adds capitalization to B, F(I=1) consists of
all components, where only one iteration of coordinate clus-
tering is performed, and F is the full model.
M-to-1 1-to-1 V VI
Prototype 71.6 63.5 0.677 2.00
Clark 69.8 54.5 0.659 2.18
HK ? 41.3 ? ?
J 43?62 37?47 ? 4.23?5.74
GG ? ? ? 2.8
GJ ? 40?49.9 ? 4.03?4.47
VG ? ? 0.54-0.59 2.5?2.9
GGTP-45 65.4 44.5 ? ?
GGTP-17 70.2 49.5 ? ?
Table 4: Comparison of our algorithms with the recent fully
unsupervised POS taggers for which results are reported. The
models differ in the annotation scheme, the corpus size and
the number of induced clusters (k) that they used. HK:
(Haghighi and Klein, 2006), 193K tokens, fine tags, k=45.
GG: (Goldwater and Griffiths, 2007), 24K tokens, coarse
tags, k=17. J : (Johnson, 2007), 1.17M tokens, fine tags,
k=25?50. GJ: (Gao and Johnson, 2008), 1.17M tokens, fine
tags, k=50. VG: (Van Gael et al, 2009), 1.17M tokens, fine
tags, k=47?192. GGTP-45: (Grac?a et al, 2009), 1.17M to-
kens, fine tags, k=45. GGTP-17: (Grac?a et al, 2009), 1.17M
tokens, coarse tags, k=17. Lower VI values indicate better
clustering. VI is computed using e as the base of the loga-
rithm. Our algorithm gives the best results.
k=13? scenario) that quantify the contribution of
each component of the algorithm. We ran the base
distributional algorithm, a variant which uses only
capitalization information (i.e., has only one non-
singleton morphological class, that of words ap-
pearing capitalized in most of their instances) and
a variant which uses no capitalization information,
defining the morphological clusters according to
the morphological representation alone.
7 Results
Table 1 presents results for the English and Ger-
man experiments. For English, our algorithm ob-
tains better results than Clark?s in all measures and
scenarios. It is without exception better than the
average score of Clark?s and in most cases better
than the maximal Clark score obtained in 100 runs.
A significant difference between our algorithm
and Clark?s is that the latter, like most algorithms
which addressed the task, induces the clustering
0 5 10 15 20 25 30 35 40 45
0
0.2
0.4
0.6
0.8
1
 
 
Gold Standard
Induced
Figure 2: POS class frequency distribution for our model
and the gold standard, in the ?Fine k=34? scenario. The dis-
tributions are similar.
by maximizing a non-convex function. These
functions have many local maxima and the specific
solution to which algorithms that maximize them
converge strongly depends on their (random) ini-
tialization. Therefore, their output?s quality often
significantly diverges from the average. This issue
is discussed in depth in (Reichart et al, 2010b).
Our algorithm is deterministic3.
For German, in the k=26 scenario our algorithm
outperforms Clark?s, often outperforming even its
maximum in 100 runs. In the k=17 scenario, our
algorithm obtains a higher score than Clark with
probability 0.4 to 0.78, depending on the measure
and scenario. Clark?s average score is slightly bet-
ter in the Many-to-1 measure, while our algorithm
performs somewhat better than Clark?s average in
the 1-to-1 and NVI measures.
The DeWAC corpus from which we extracted
statistics for the German experiments is out of do-
main with respect to NEGRA. The correspond-
ing corpus in English, NANC, is a newswire cor-
pus and therefore clearly in-domain with respect
to WSJ. This is reflected by the percentage of un-
known words, which was much higher in German
than in English (8.1% and 1.6%), lowering results.
Table 2 shows the effect of each of our algo-
rithm?s components. Each component provides
an improvement over the base distributional algo-
rithm. The full coordinate clustering stage (sev-
eral iterations, F) considerably improves the score
over a single iteration (F(I=1)). Capitalization in-
formation increases the score more than the mor-
phological information, which might stem from
the granularity of the POS tag set with respect to
names. This analysis is supported by similar ex-
periments we made in the ?Coarse k=13? scenario
(not shown in tables here). There, the decrease in
performance was only of 1%?2% in the mapping
3The fluctuations inflicted on our algorithm by the random
mapping of unknown words are of less than 0.1% .
1304
Excluding Punctuation Including Punctuation Perfect Punctuation
M-to-1 1-to-1 NVI V M-to-1 1-to-1 NVI V M-to-1 1-to-1 NVI V
Van Gael 59.1 48.4 0.999 0.530 62.3 51.3 0.861 0.591 64.0 54.6 0.820 0.610
Prototype 67.5 58.8 0.809 0.608 71.6 63.5 0.663 0.677 71.6 63.9 0.659 0.679
Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al, 2009) and ours with various punctuation assign-
ment schemes. Left section: punctuation tokens are excluded. Middle section: punctuation tokens are included. Right section:
perfect assignment of punctuation is assumed.
based measures and 3.5% in the V measure.
Finally, Table 4 presents reported results for all
recent algorithms we are aware of that tackled the
task of unsupervised POS induction from plain
text. Results for our algorithm?s and Clark?s are
reported for the ?Fine, k=34? scenario. The set-
tings of the various experiments vary in terms of
the exact annotation scheme used (coarse or fine
grained) and the size of the test set. However, the
score differences are sufficiently large to justify
the claim that our algorithm is currently the best
performing algorithm on the PTB-WSJ corpus for
POS induction from plain text4.
Since previous works provided results only for
the scenario in which punctuation is included, the
reported results are not directly comparable. In
order to quantify the effect various punctuation
schemes have on the results, we evaluated the
?iHMM: PY-fixed? model (Van Gael et al, 2009)
and ours when punctuation is excluded, included
or perfectly tagged5. The results (Table 3) indi-
cate that most probably even after an appropriate
correction for punctuation, our model remains the
best performing one.
8 Discussion
In this work we presented a novel unsupervised al-
gorithm for POS induction from plain text. The al-
gorithm first generates relatively accurate clusters
of high frequency words, which are subsequently
used to bootstrap the entire clustering. The dis-
tributional and morphological representations that
we use are novel for this task.
We experimented on two languages with map-
ping and information theoretic clustering evalua-
tion measures. Our algorithm obtains the best re-
ported results on the English PTB-WSJ corpus. In
addition, our results are almost always better than
Clark?s on the German NEGRA corpus.
4Grac?a et al (2009) report very good results for 17 tags in
the M-1 measure. However, their 1-1 results are quite poor,
and results for the common IT measures were not reported.
Their results for 45 tags are considerably lower.
5We thank the authors for sending us their data.
We have also performed a manual error anal-
ysis, which showed that our algorithm performs
much better on closed classes than on open
classes. In order to asses this quantitatively, let
us define a random variable for each of the gold
clusters, which receives a value corresponding to
each induced cluster with probability proportional
to their intersection size. For each gold cluster,
we compute the entropy of this variable. In ad-
dition, we greedily map each induced cluster to a
gold cluster and compute the ratio between their
intersection size and the size of the gold cluster
(mapping accuracy).
We experimented in the ?Fine k=34? scenario.
The clusters that obtained the best scores were
(brackets indicate mapping accuracy and entropy
for each of these clusters) coordinating conjunc-
tions (95%, 0.32), prepositions (94%, 0.32), de-
terminers (94%, 0.44) and modals (93%, 0.45).
These are all closed classes.
The classes on which our algorithm performed
worst consist of open classes, mostly verb types:
past tense verbs (47%, 2.2), past participle verbs
(44%, 2.32) and the morphologically unmarked
non-3rd person singular present verbs (32%, 2.86).
Another class with low performance is the proper
nouns (37%, 2.9). The errors there are mostly
of three types: confusions between common and
proper nouns (sometimes due to ambiguity), un-
known words which were put in the unknown
words cluster, and abbreviations which were given
a separate class by our algorithm. Finally, the al-
gorithm?s performance on the heterogeneous ad-
verbs class (19%, 3.73) is the lowest.
Clark?s algorithm exhibits6 a similar pattern
with respect to open and closed classes. While
his algorithm performs considerably better on ad-
verbs (15% mapping accuracy difference and 0.71
entropy difference), our algorithm scores consid-
erably better on prepositions (17%, 0.77), su-
perlative adjectives (38%, 1.37) and plural proper
names (45%, 1.26).
6Using average mapping accuracy and entropy over the
100 runs.
1305
Naturally, this analysis might reflect the arbi-
trary nature of a manually design POS tag set
rather than deficiencies in automatic POS induc-
tion algorithms. In future work we intend to ana-
lyze the output of such algorithms in order to im-
prove POS tag sets.
Our algorithm and Clark?s are monosemous
(i.e., they assign each word exactly one tag), while
most other algorithms are polysemous. In order to
assess the performance loss caused by the monose-
mous nature of our algorithm, we took the M-1
greedy mapping computed for the entire dataset
and used it to compute accuracy over the monose-
mous and polysemous words separately. Results
are reported for the English ?Fine k=34? scenario
(without punctuation). We define a word to be
monosemous if more than 95% of its tokens are
assigned the same gold standard tag. For English,
there are approximately 255K polysemous tokens
and 578K monosemous ones. As expected, our
algorithm is much more accurate on the monose-
mous tokens, achieving 76.6% accuracy, com-
pared to 47.1% on the polysemous tokens.
The evaluation in this paper is done at the token
level. Type level evaluation, reflecting the algo-
rithm?s ability to detect the set of possible POS
tags for each word type, is important as well. It
could be expected that a monosemous algorithm
such as ours would perform poorly in a type level
evaluation. In (Reichart et al, 2010a) we discuss
type level evaluation at depth and propose type
level evaluation measures applicable to the POS
induction problem. In that paper we compare the
performance of our Prototype Tagger with lead-
ing unsupervised POS tagging algorithms (Clark,
2003; Goldwater and Griffiths, 2007; Gao and
Johnson, 2008; Van Gael et al, 2009). Our al-
gorithm obtained the best results in 4 of the 6
measures in a margin of 4?6%, and was second
best in the other two measures. Our results were
better than Clark?s (the only other monosemous
algorithm evaluated there) on all measures in a
margin of 5?21%. The fact that our monose-
mous algorithm was better than good polysemous
algorithms in a type level evaluation can be ex-
plained by the prototypical nature of the POS phe-
nomenon (a longer discussion is given in (Reichart
et al, 2010a)). However, the quality upper bound
for monosemous algorithms is obviously much
lower than that for polysemous algorithms, and
we expect polysemous algorithms to outperform
monosemous algorithms in the future in both type
level and token level evaluations.
The skewed (Zipfian) distribution of POS class
frequencies in corpora is a problem for many POS
induction algorithms, which by default tend to in-
duce a clustering having a balanced distribution.
Explicit modifications to these algorithms were in-
troduced in order to bias their model to produce
such a distribution (see (Clark, 2003; Johnson,
2007; Reichart et al, 2010b)). An appealing prop-
erty of our model is its ability to induce a skewed
distribution without being explicitly tuned to do
so, as seen in Figure 2.
Acknowledgements. We would like to thank
Yoav Seginer for his help with his parser.
References
Michele Banko and Robert C. Moore, 2004. Part of
Speech Tagging in Context. COLING ?04.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi and
Eros Zanchetta, 2009. The WaCky Wide Web: A
Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation.
Chris Biemann, 2006. Unsupervised Part-of-
Speech Tagging Employing Efficient Graph Cluster-
ing. COLING-ACL ?06 Student Research Work-
shop.
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de
Souze, Jenifer C. Lai and Robert Mercer, 1992.
Class-Based N-Gram Models of Natural Language.
Computational Linguistics, 18(4):467?479.
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech In-
duction. EACL ?03.
Mathias Creutz and Krista Lagus, 2005. Inducing the
Morphological Lexicon of a Natural Language from
Unannotated Text. AKRR ?05.
Sajib Dasgupta and Vincent Ng, 2007. Unsu-
pervised Part-of-Speech Acquisition for Resource-
Scarce Languages. EMNLP-CoNLL ?07.
Dayne Freitag, 2004. Toward Unsupervised Whole-
Corpus Tagging. COLING ?04.
Jianfeng Gao and Mark Johnson, 2008. A Compar-
ison of Bayesian Estimators for Unsupervised Hid-
den Markov Model POS Taggers. EMNLP ?08.
Yoav Goldberg, Meni Adler and Michael Elhadad,
2008. EM Can Find Pretty Good HMM POS-
Taggers (When Given a Good Start). ACL ?08.
1306
John Goldsmith, 2001. Unsupervised Learning of the
Morphology of a Natural Language. Computational
Linguistics, 27(2):153?198.
Sharon Goldwater and Tom Griffiths, 2007. Fully
Bayesian Approach to Unsupervised Part-of-Speech
Tagging. ACL ?07.
Joa?o Grac?a, Kuzman Ganchev, Ben Taskar and Fre-
nando Pereira, 2009. Posterior vs. Parameter Spar-
sity in Latent Variable Models. NIPS ?09.
David Graff, 1995. North American News Text Cor-
pus. Linguistic Data Consortium. LDC95T21.
Aria Haghighi and Dan Klein, 2006. Prototype-driven
Learning for Sequence Labeling. HLT?NAACL ?06.
Anil K. Jain, Narasimha M. Murty and Patrick J. Flynn,
1999. Data Clustering: A Review. ACM Computing
Surveys 31(3):264?323.
Wenbin Jiang, Liang Huang and Qun Liu, 2009. Au-
tomatic Adaptation of Annotation Standards: Chi-
nese Word Segmentation and POS Tagging ? A Case
Study. ACL ?09.
Mark Johnson, 2007. Why Doesnt EM Find Good
HMM POS-Taggers? EMNLP-CoNLL ?07.
Harold W. Kuhn, 1955. The Hungarian method for
the Assignment Problem. Naval Research Logistics
Quarterly, 2:83-97.
Marina Meila, 2007. Comparing Clustering ? an In-
formation Based Distance. Journal of Multivariate
Analysis, 98:873?895.
Bernard Merialdo, 1994. Tagging English Text with
a Probabilistic Model. Computational Linguistics,
20(2):155?172.
James Munkres, 1957. Algorithms for the Assignment
and Transportation Problems. Journal of the SIAM,
5(1):32?38.
Sujith Ravi and Kevin Knight, 2009. Minimized Mod-
els for Unsupervised Part-of-Speech Tagging. ACL
?09.
Roi Reichart and Ari Rappoport, 2008. Unsupervised
Induction of Labeled Parse Trees by Clustering with
Syntactic Features. COLING ?08.
Roi Reichart and Ari Rappoport, 2009. The NVI Clus-
tering Evaluation Measure. CoNLL ?09.
Roi Reichart, Omri Abend and Ari Rappoport, 2010a.
Type Level Clustering Evaluation: New Measures
and a POS Induction Case Study. CoNLL ?10.
Roi Reichart, Raanan Fattal and Ari Rappoport, 2010b.
Improved Unsupervised POS Induction Using In-
trinsic Clustering Quality and a Zipfian Constraint.
CoNLL ?10.
Andrew Rosenberg and Julia Hirschberg, 2007. V-
Measure: A Conditional Entropy-Based External
Cluster Evaluation Measure. EMNLP ?07.
Hinrich Schu?tze, 1995. Distributional part-of-speech
tagging. EACL ?95.
Yoav Seginer, 2007. Fast Unsupervised Incremental
Parsing. ACL ?07.
Noah A. Smith and Jason Eisner, 2005. Contrastive
Estimation: Training Log-Linear Models on Unla-
beled Data. ACL ?05.
John R. Taylor, 2003. Linguistic Categorization: Pro-
totypes in Linguistic Theory, Third Edition. Oxford
University Press.
Jurgen Van Gael, Andreas Vlachos and Zoubin Ghahra-
mani, 2009. The Infinite HMM for Unsupervised
POS Tagging. EMNLP ?09.
Qin Iris Wang and Dale Schuurmans, 2005. Im-
proved Estimation for Unsupervised Part-of-Speech
Tagging. IEEE NLP?KE ?05.
Qiuye Zhao and Mitch Marcus, 2009. A Simple Un-
supervised Learner for POS Disambiguation Rules
Given Only a Minimal Lexicon. EMNLP ?09.
1307
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 663?672,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Neutralizing Linguistically Problematic Annotations
in Unsupervised Dependency Parsing Evaluation
Roy Schwartz1 Omri Abend1? Roi Reichart2 Ari Rappoport1
1Institute of Computer Science
Hebrew University of Jerusalem
{roys02|omria01|arir}@cs.huji.ac.il
2Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
roiri@csail.mit.edu
Abstract
Dependency parsing is a central NLP task. In
this paper we show that the common eval-
uation for unsupervised dependency parsing
is highly sensitive to problematic annotations.
We show that for three leading unsupervised
parsers (Klein and Manning, 2004; Cohen and
Smith, 2009; Spitkovsky et al, 2010a), a small
set of parameters can be found whose mod-
ification yields a significant improvement in
standard evaluation measures. These param-
eters correspond to local cases where no lin-
guistic consensus exists as to the proper gold
annotation. Therefore, the standard evaluation
does not provide a true indication of algorithm
quality. We present a new measure, Neutral
Edge Direction (NED), and show that it greatly
reduces this undesired phenomenon.
1 Introduction
Unsupervised induction of dependency parsers is a
major NLP task that attracts a substantial amount
of research (Klein and Manning, 2004; Cohen et
al., 2008; Headden et al, 2009; Spitkovsky et al,
2010a; Gillenwater et al, 2010; Berg-Kirkpatrick
et al, 2010; Blunsom and Cohn, 2010, inter alia).
Parser quality is usually evaluated by comparing its
output to a gold standard whose annotations are lin-
guistically motivated. However, there are cases in
which there is no linguistic consensus as to what the
correct annotation is (Ku?bler et al, 2009). Examples
include which verb is the head in a verb group struc-
ture (e.g., ?can? or ?eat? in ?can eat?), and which
? Omri Abend is grateful to the Azrieli Foundation for the
award of an Azrieli Fellowship.
noun is the head in a sequence of proper nouns (e.g.,
?John? or ?Doe? in ?John Doe?). We refer to such
annotations as (linguistically) problematic. For such
cases, evaluation measures should not punish the al-
gorithm for deviating from the gold standard.
In this paper we show that the evaluation mea-
sures reported in current works are highly sensitive
to the annotation in problematic cases, and propose
a simple new measure that greatly neutralizes the
problem.
We start from the following observation: for three
leading algorithms (Klein and Manning, 2004; Co-
hen and Smith, 2009; Spitkovsky et al, 2010a), a
small set (at most 18 out of a few thousands) of pa-
rameters can be found whose modification dramati-
cally improves the standard evaluation measures (the
attachment score measure by 9.3-15.1%, and the
undirected measure by a smaller but still significant
1.3-7.7%). The phenomenon is implementation in-
dependent, occurring with several algorithms based
on a fundamental probabilistic dependency model1.
We show that these parameter changes can be
mapped to edge direction changes in local structures
in the dependency graph, and that these correspond
to problematic annotations. Thus, the standard eval-
uation measures do not reflect the true quality of the
evaluated algorithm.
We explain why the standard undirected evalua-
tion measure is in fact sensitive to such edge direc-
1It is also language-independent; we have produced it in five
different languages: English, Czech, Japanese, Portuguese, and
Turkish. Due to space considerations, in this paper we focus
on English, because it is the most studied language for this task
and the most practically useful one at present.
663
tion changes, and present a new evaluation measure,
Neutral Edge Direction (NED), which greatly allevi-
ates the problem by ignoring the edge direction in lo-
cal structures. Using NED, manual modifications of
model parameters always yields small performance
differences. Moreover, NED sometimes punishes
such manual parameter tweaking by yielding worse
results. We explain this behavior using an exper-
iment revealing that NED always prefers the struc-
tures that are more consistent with the modeling as-
sumptions lying in the basis of the algorithm. When
manual parameter modification is done against this
preference, the NED results decrease.
The contributions of this paper are as follows.
First, we show the impact of a small number of an-
notation decisions on the performance of unsuper-
vised dependency parsers. Second, we observe that
often these decisions are linguistically controversial
and therefore this impact is misleading. This reveals
a problem in the common evaluation of unsuper-
vised dependency parsing. This is further demon-
strated by noting that recent papers evaluate the task
using three gold standards which differ in such deci-
sions and which yield substantially different results.
Third, we present the NED measure, which is agnos-
tic to errors arising from choosing the non-gold di-
rection in such cases.
Section 2 reviews related work. Section 3 de-
scribes the performed parameter modifications. Sec-
tion 4 discusses the linguistic controversies in anno-
tating problematic dependency structures. Section 5
presents NED. Section 6 describes experiments with
it. A discussion is given in Section 7.
2 Related Work
Grammar induction received considerable attention
over the years (see (Clark, 2001; Klein, 2005) for
reviews). For unsupervised dependency parsing, the
Dependency Model with Valence (DMV) (Klein and
Manning, 2004) was the first to beat the simple
right-branching baseline. A technical description of
DMV is given at the end of this section.
The great majority of recent works, including
those experimented with in this paper, are elabora-
tions of DMV. Smith and Eisner (2005) improved
the DMV results by generalizing the function maxi-
mized by DMV?s EM training algorithm. Smith and
Eisner (2006) used a structural locality bias, experi-
menting on five languages. Cohen et al (2008) ex-
tended DMV by using a variational EM training al-
gorithm and adding logistic normal priors. Cohen
and Smith (2009, 2010) further extended it by us-
ing a shared logistic normal prior which provided a
new way to encode the knowledge that some POS
tags are more similar than others. A bilingual joint
learning further improved their performance.
Headden et al (2009) obtained the best reported
results on WSJ10 by using a lexical extension of
DMV. Gillenwater et al (2010) used posterior reg-
ularization to bias the training towards a small num-
ber of parent-child combinations. Berg-Kirkpatrick
et al (2010) added new features to the M step of the
DMV EM procedure. Berg-Kirkpatrick and Klein
(2010) used a phylogenetic tree to model parame-
ter drift between different languages. Spitkovsky
et al (2010a) explored several training protocols
for DMV. Spitkovsky et al (2010c) showed the
benefits of Viterbi (?hard?) EM to DMV training.
Spitkovsky et al (2010b) presented a novel lightly-
supervised approach that used hyper-text mark-up
annotation of web-pages to train DMV.
A few non-DMV-based works were recently pre-
sented. Daume? III (2009) used shift-reduce tech-
niques. Blunsom and Cohn (2010) used tree sub-
stitution grammar to achieve best results on WSJ?.
Druck et al (2009) took a semi-supervised ap-
proach, using a set of rules such as ?A noun is usu-
ally the parent of a determiner which is to its left?,
experimenting on several languages. Naseem et al
(2010) further extended this idea by using a single
set of rules which globally applies to six different
languages. The latter used a model similar to DMV.
The controversial nature of some dependency
structures was discussed in (Nivre, 2006; Ku?bler
et al, 2009). Klein (2005) discussed controversial
constituency structures and the evaluation problems
stemming from them, stressing the importance of a
consistent standard of evaluation.
A few works explored the effects of annotation
conventions on parsing performance. Nilsson et
al. (2006) transformed the dependency annotations
of coordinations and verb groups in the Prague
TreeBank. They trained the supervised MaltParser
(Nivre et al, 2006) on the transformed data, parsed
the test data and re-transformed the resulting parse,
664
w3 w2 w1
(a)
w3 w2 w1
(b)
Figure 1: A dependency structure on the words
w1, w2, w3 before (Figure 1(a)) and after (Figure 1(b))
an edge-flip of w2?w1.
thus improving performance. Klein and Manning
(2004) observed that a large portion of their errors is
caused by predicting the wrong direction of the edge
between a noun and its determiner. Ku?bler (2005)
compared two different conversion schemes in Ger-
man supervised constituency parsing and found one
to have positive influence on parsing quality.
Dependency Model with Valence (DMV). DMV
(Klein and Manning, 2004) defines a probabilistic
grammar for unlabeled dependency structures. It is
defined as follows: the root of the sentence is first
generated, and then each head recursively generates
its right and left dependents. The parameters of the
model are of two types: PSTOP and PATTACH .
PSTOP (dir, h, adj) determines the probability to
stop generating arguments, and is conditioned on 3
arguments: the head h, the direction dir ((L)eft
or (R)ight) and adjacency adj (whether the head
already has dependents ((Y )es) in direction dir or
not ((N)o)). PATTACH(arg|h, dir) determines the
probability to generate arg as head h?s dependent in
direction dir.
3 Significant Effects of Edge Flipping
In this section we present recurring error patterns
in some of the leading unsupervised dependency
parsers. These patterns are all local, confined to a
sequence of up to three words (but mainly of just
two consecutive words). They can often be mended
by changing the directions of a few types of edges.
The modified parameters described in this section
were handpicked to improve performance: we ex-
amined the local parser errors occurring the largest
number of times, and found the corresponding pa-
rameters. Note that this is a valid methodology,
since our goal is not to design a new algorithm but
to demonstrate that modifying a small set of param-
eters can yield a major performance boost and even-
tually discover problems with evaluation methods or
algorithms.
I
PRP
want
VBP
to
TO
eat
VB
.
ROOT
Figure 2: A parse of the sentence ?I want to eat?, before
(straight line) and after (dashed line) an edge-flip of the
edge ?to???eat?.
We start with a few definitions. Consider Fig-
ure 1(a) that shows a dependency structure on the
words w1, w2, w3. Edge flipping (henceforth, edge-
flip) the edge w2?w1 is the following modification
of a parse tree: (1) setting w2?s parent as w1 (instead
of the other way around), and (2) setting w1?s par-
ent as w3 (instead of the edge w3?w2). Figure 1(b)
shows the dependency structure after the edge-flip.
Note that (1) imposes setting a new parent to w2,
as otherwise it would have had no parent. Setting
this parent to be w3 is the minimal modification of
the original parse, since it does not change the at-
tachment of the structure [w2, w1] to the rest of the
sentence, but only the direction of the internal edge.
Figure 2 presents a parse of the sentence ?I want
to eat?, before and after an edge-flip of the edge
?to???eat?.
Since unsupervised dependency parsers are gen-
erally structure prediction models, the predictions
of the parse edges are not independent. Therefore,
there is no single parameter which completely con-
trols the edge direction, and hence there is no direct
way to perform an edge-flip by parameter modifica-
tion. However, setting extreme values for the param-
eters controlling the direction of a certain edge type
creates a strong preference towards one of the direc-
tions, and effectively determines the edge direction.
This procedure is henceforth termed parameter-flip.
We show that by performing a few parameter-
flips, a substantial improvement in the attachment
score can be obtained. Results are reported for three
algorithms.
Parameter Changes. All the works experimented
with in this paper are not lexical and use sequences
of POS tags as their input. In addition, they all use
the DMV parameter set (PSTOP and PATTACH) for
parsing. We will henceforth refer to this set, condi-
tioned on POS tags, as the model parameter set.
We show how an edge in the dependency graph
is encoded using the DMV parameters. Say the
665
model prefers setting ?to? (POS tag: TO) as a de-
pendent of the infinitive verb (POS tag: V B) to its
right (e.g., ?to eat?). This is reflected by a high
value of PATTACH(TO|V B,L), a low value of
PATTACH(V B|TO,R), since ?to? tends to be a left
dependent of the verb and not the other way around,
and a low value of PSTOP (V B,L,N), as the verb
usually has at least one left argument (i.e., ?to?).
A parameter-flip of w1?w2 is hence performed
by setting PATTACH(w2|w1, R) to a very low
value and PATTACH(w1|w2, L) to a very high
value. When the modifications to PATTACH
are insufficient to modify the edge direction,
PSTOP (w2, L,N) is set to a very low value and
PSTOP (w1, R,N) to a very high value2.
Table 1 describes the changes made for the three
algorithms. The ?+? signs in the table correspond to
edges in which the algorithm disagreed with the gold
standard, and were thus modified. Similarly, the ???
signs in the table correspond to edges in which the
algorithm agreed with the gold standard, and were
thus not modified. The number of modified param-
eters does not exceed 18 (out of a few thousands).
The Freq. column in the table shows the percent-
age of the tokens in sections 2-21 of PTB WSJ that
participate in each structure. Equivalently, the per-
centage of edges in the corpus which are of either
of the types appearing in the Orig. Edge column.
As the table shows, the modified structures cover a
significant portion of the tokens. Indeed, 42.9% of
the tokens in the corpus participate in at least one of
them3.
Experimenting with Edge Flipping. We experi-
mented with three DMV-based algorithms: a repli-
cation of (Klein and Manning, 2004), as appears in
(Cohen et al, 2008) (henceforth, km04), Cohen and
Smith (2009) (henceforth, cs09), and Spitkovsky et
al. (2010a) (henceforth, saj10a). Decoding is done
using the Viterbi algorithm4. For each of these algo-
rithms we present the performance gain when com-
pared to the original parameters.
The training set is sections 2-21 of the Wall Street
2Note that this yields unnormalized models. Again, this is
justified since the resulting model is only used as a basis for
discussion and is not a fully fledged algorithm.
3Some tokens participate in more than one structure.
4http://www.cs.cmu.edu/?scohen/parser.html.
Structure Freq. Orig. Edge km04 cs09 saj10a
Coordination
(?John & Mary?) 2.9% CC?NNP ? + ?
Prepositional
Phrase (?in
the house?)
32.7%
DT?NN + + +
DT?NNP ? + +
DT?NNS ? ? +
IN?DT + + ?
IN?NN + + ?
IN?NNP + ? ?
IN?NNS ? + ?
PRP$?NN ? ? +
Modal Verb
(?can eat?) 2.4% MD?V B ? + ?
Infinitive Verb
(?to eat?) 4.5% TO?V B ? + +
Proper Name
Sequence
(?John Doe?)
18.5% NNP?NNP + ? ?
Table 1: Parameter changes for the three algorithms. The
Freq. column shows what percentage of the tokens in sec-
tions 2-21 of PTB WSJ participate in each structure. The
Orig. column indicates the original edge. The modified
edge is of the opposite direction. The other columns show
the different algorithms: km04: basic DMV model (repli-
cation of (Klein and Manning, 2004)); cs09; (Cohen and
Smith, 2009); saj10a: (Spitkovsky et al, 2010a).
Journal Penn TreeBank (Marcus et al, 1993). Test-
ing is done on section 23. The constituency annota-
tion was converted to dependencies using the rules
of (Yamada and Matsumoto, 2003)5.
Following standard practice, we present the at-
tachment score (i.e., percentage of words that have a
correct head) of each algorithm, with both the origi-
nal parameters and the modified ones. We present
results both on all sentences and on sentences of
length ? 10, excluding punctuation.
Table 2 shows results for all algorithms6. The
performance difference between the original and the
modified parameter set is considerable for all data
sets, where differences exceed 9.3%, and go up to
15.1%. These are enormous differences from the
perspective of current algorithm evaluation results.
4 Linguistically Problematic Annotations
In this section, we discuss the controversial nature
of the annotation in the modified structures (Ku?bler
5http://www.jaist.ac.jp/?h-yamada/
6Results are slightly worse than the ones published in the
original papers due to the different decoding algorithms (cs09
use MBR while we used Viterbi) and a different conversion pro-
cedure (saj10a used (Collins, 1999) and not (Yamada and Mat-
sumoto, 2003)) ; see Section 5.
666
Algo. ? 10 ? ?
Orig. Mod. ? Orig. Mod. ?
km04 45.8 59.8 14 34.6 43.9 9.3
cs09 60.9 72.9 12 39.9 54.6 14.7
saj10a 54.7 69.8 15.1 41.6 54.3 12.7
Table 2: Results of the original (Orig. columns), the
modified (Mod. columns) parameter sets and their dif-
ference (? columns) for the three algorithms.
et al, 2009). We remind the reader that structures
for which no linguistic consensus exists as to their
correct annotation are referred to as (linguistically)
problematic.
We begin by showing that all the structures mod-
ified are indeed linguistically problematic. We then
note that these controversies are reflected in the eval-
uation of this task, resulting in three, significantly
different, gold standards currently in use.
Coordination Structures are composed of two
proper nouns, separated by a conjunctor (e.g., ?John
and Mary?). It is not clear which token should be the
head of this structure, if any (Nilsson et al, 2006).
Prepositional Phrases (e.g., ?in the house? or ?in
Rome?), where every word is a reasonable candidate
to head this structure. For example, in the annotation
scheme used by (Collins, 1999) the preposition is the
head, in the scheme used by (Johansson and Nugues,
2007) the noun is the head, while TUT annotation,
presented in (Bosco and Lombardo, 2004), takes the
determiner to be the noun?s head.
Verb Groups are composed of a verb and an aux-
iliary or a modal verb (e.g., ?can eat?). Some
schemes choose the modal as the head (Collins,
1999), others choose the verb (Rambow et al, 2002).
Infinitive Verbs (e.g., ?to eat?) are also in contro-
versy, as in (Yamada and Matsumoto, 2003) the verb
is the head while in (Collins, 1999; Bosco and Lom-
bardo, 2004) the ?to? token is the head.
Sequences of Proper Nouns (e.g., ?John Doe?)
are also subject to debate, as PTB?s scheme takes the
last proper noun as the head, and BIO?s scheme de-
fines a more complex scheme (Dredze et al, 2007).
Evaluation Inconsistency Across Papers. A fact
that may not be recognized by some readers is that
comparing the results of unsupervised dependency
parsers across different papers is not directly pos-
sible, since different papers use different gold stan-
dard annotations even when they are all derived from
the Penn Treebank constituency annotation. This
happens because they use different rules for con-
verting constituency annotation to dependency an-
notation. A probable explanation for this fact is that
people have tried to correct linguistically problem-
atic annotations in different ways, which is why we
note this issue here7.
There are three different annotation schemes
in current use: (1) Collins head rules (Collins,
1999), used in e.g., (Berg-Kirkpatrick et al, 2010;
Spitkovsky et al, 2010a); (2) Conversion rules of
(Yamada and Matsumoto, 2003), used in e.g., (Co-
hen and Smith, 2009; Gillenwater et al, 2010); (3)
Conversion rules of (Johansson and Nugues, 2007)
used, e.g., in the CoNLL shared task 2007 (Nivre et
al., 2007) and in (Blunsom and Cohn, 2010).
The differences between the schemes are substan-
tial. For instance, 14.4% of section 23 is tagged dif-
ferently by (1) and (2)8.
5 The Neutral Edge Direction (NED)
Measure
As shown in the previous sections, the annotation
of problematic edges can substantially affect perfor-
mance. This was briefly discussed in (Klein and
Manning, 2004), which used undirected evaluation
as a measure which is less sensitive to alternative
annotations. Undirected accuracy was commonly
used since to assess the performance of unsuper-
vised parsers (e.g., (Smith and Eisner, 2006; Head-
den et al, 2008; Spitkovsky et al, 2010a)) but also
of supervised ones (Wang et al, 2005; Wang et al,
2006). In this section we discuss why this measure
is in fact not indifferent to edge-flips and propose a
new measure, Neutral Edge Direction (NED).
7Indeed, half a dozen flags in the LTH Constituent-to-
Dependency Conversion Tool (Johansson and Nugues, 2007)
are used to control the conversion in problematic cases.
8In our experiments we used the scheme of (Yamada and
Matsumoto, 2003), see Section 3. The significant effects of
edge flipping were observed with the other two schemes as well.
667
w1
w2
w3
(a)
w1
w3
w2
(b)
w4
w3
w2
(c)
Figure 3: A dependency structure on the words
w1, w2, w3 before (Figure 3(a)) and after (Figure 3(b)) an
edge-flip of w2?w3, and when the direction of the edge
between w2 and w3 is switched and the new parent of w3
is set to be some other word, w4 (Figure 3(c)).
Undirected Evaluation. The measure is defined
as follows: traverse over the tokens and mark a cor-
rect attachment if the token?s induced parent is either
(1) its gold parent or (2) its gold child. The score is
the ratio of correct attachments and the number of
tokens.
We show that this measure does not ignore edge-
flips. Consider Figure 3 that shows a depen-
dency structure on the words w1, w2, w3 before (Fig-
ure 3(a)) and after (Figure 3(b)) an edge-flip of
w2?w3. Assume that 3(a) is the gold standard and
that 3(b) is the induced parse. Consider w2. Its
induced parent (w3) is its gold child, and thus undi-
rected evaluation does not consider it an error. On
the other hand, w3 is assigned w2?s gold parent, w1.
This is considered an error, since w1 is neither w3?s
gold parent (as it is w2), nor its gold child9. There-
fore, one of the two tokens involved in the edge-flip
is penalized by the measure.
Recall the example ?I want to eat? and the edge-
flip of the edge ?to???eat? (Figure 2). As ?to??s
parent in the induced graph (?want?) is neither its
gold parent nor its gold child, the undirected evalu-
ation measure marks it as an error. This is an exam-
ple where an edge-flip in a problematic edge, which
should not be considered an error, was in fact con-
sidered an error by undirected evaluation.
Neutral Edge Direction (NED). The NED measure
is a simple extension of the undirected evaluation
measure10. Unlike undirected evaluation, NED ig-
nores all errors directly resulting from an edge-flip.
9Otherwise, the gold parse would have contained a
w1?w2?w3?w1 cycle.
10An implementation of NED is available at
http://www.cs.huji.ac.il/?roys02/software/ned.html
NED is defined as follows: traverse over the to-
kens and mark a correct attachment if the token?s in-
duced parent is either (1) its gold parent (2) its gold
child or (3) its gold grandparent. The score is the ra-
tio of correct attachments and the number of tokens.
NED, by its definition, ignores edge-flips. Con-
sider again Figure 3, where we assume that 3(a) is
the gold standard and that 3(b) is the induced parse.
Much like undirected evaluation, NED will mark the
attachment of w2 as correct, since its induced parent
is its gold child. However, unlike undirected evalua-
tion, w3?s induced attachment will also be marked as
correct, as its induced parent is its gold grandparent.
Now consider another induced parse in which the
direction of the edge between w2 and w3 is switched
and the w3?s parent is set to be some other word,
w4 (Figure 3(c)). This should be marked as an er-
ror, even if the direction of the edge between w2 and
w3 is controversial, since the structure [w2, w3] is no
longer a dependent of w1. It is indeed a NED error.
Note that undirected evaluation gives the parses in
Figure 3(b) and Figure 3(c) the same score, while if
the structure [w2, w3] is problematic, there is a major
difference in their correctness.
Discussion. Problematic structures are ubiquitous,
with more than 40% of the tokens in PTB WSJ
appearing in at least one of them (see Section 3).
Therefore, even a substantial difference in the at-
tachment between two parsers is not necessarily in-
dicative of a true quality difference. However, an at-
tachment score difference that persists under NED is
an indication of a true quality difference, since gen-
erally problematic structures are local (i.e., obtained
by an edge-flip) and NED ignores such errors.
Reporting NED alone is insufficient, as obviously
the edge direction does matter in some cases. For
example, in adjective?noun structures (e.g., ?big
house?), the correct edge direction is widely agreed
upon (?big???house?) (Ku?bler et al, 2009), and
thus choosing the wrong direction should be con-
sidered an error. Therefore, we suggest evaluating
using both NED and attachment score in order to get
a full picture of the parser?s performance.
A possible criticism on NED is that it is only in-
different to alternative annotations in structures of
size 2 (e.g., ?to eat?) and does not necessarily handle
larger problematic structures, such as coordinations
668
ROOT
John
and Mary
(a)
ROOT
John
and
Mary
(b)
ROOT
in
house
the
(c)
ROOT
in
the
house
(d)
ROOT
house
in
the
(e)
Figure 4: Alternative parses of ?John and Mary? and ?in
the house?. Figure 4(a) follows (Collins, 1999), Fig-
ure 4(b) follows (Johansson and Nugues, 2007). Fig-
ure 4(c) follows (Collins, 1999; Yamada and Matsumoto,
2003). Figure 4(d) and Figure 4(e) show induced parses
made by (km04,saj10a) and cs09, respectively.
(see Section 4). For example, Figure 4(a) and Fig-
ure 4(b) present two alternative annotations of the
sentence ?John and Mary?. Assume the parse in Fig-
ure 4(a) is the gold parse and that in Figure 4(b) is
the induced parse. The word ?Mary? is a NED error,
since its induced parent (?and?) is neither its gold
child nor its gold grandparent. Thus, NED does not
accept all possible annotations of structures of size
3. On the other hand, using a method which accepts
all possible annotations of structures of size 3 seems
too permissive. A better solution may be to modify
the gold standard annotation, so to explicitly anno-
tate problematic structures as such. We defer this
line of research to future work.
NED is therefore an evaluation measure which is
indifferent to edge-flips, and is consequently less
sensitive to alternative annotations. We now show
that NED is indifferent to the differences between the
structures originally learned by the algorithms men-
tioned in Section 3 and the gold standard annotation
in all the problematic cases we consider.
Most of the modifications made are edge-flips,
and are therefore ignored by NED. The exceptions
are coordinations and prepositional phrases which
are structures of size 3. In the former, the alter-
native annotations differ only in a single edge-flip
(i.e., CC?NNP ), and are thus not NED errors. Re-
garding prepositional phrases, Figure 4(c) presents
the gold standard of ?in the house?, Figure 4(d) the
parse induced by km04 and saj10a and Figure 4(e)
the parse induced by cs09. As the reader can verify,
both induced parses receive a perfect NED score.
In order to further demonstrate NED?s insensitiv-
ity to alternative annotations, we took two of the
three common gold standard annotations (see Sec-
tion 4) and evaluated them one against the other. We
considered section 23 of WSJ following the scheme
of (Yamada and Matsumoto, 2003) as the gold stan-
dard and of (Collins, 1999) as the evaluated set. Re-
sults show that the attachment score is only 85.6%,
the undirected accuracy is improved to 90.3%, while
the NED score is 95.3%. This shows that NED is sig-
nificantly less sensitive to the differences between
the different annotation schemes, compared to the
other evaluation measures.
6 Experimenting with NED
In this section we show that NED indeed reduces
the performance difference between the original and
the modified parameter sets, thus providing empiri-
cal evidence for its validity. For brevity, we present
results only for the entire WSJ corpus. Results on
WSJ10 are similar. The datasets and decoding algo-
rithms are the same as those used in Section 3.
Table 3 shows the score differences between the
parameter sets using attachment score, undirected
evaluation and NED. A substantial difference per-
sists under undirected evaluation: a gap of 7.7% in
cs09, of 3.5% in saj10a and of 1.3% in km04.
The differences are further reduced using NED.
This is consistent with our discussion in Section 5,
and shows that undirected evaluation only ignores
some of the errors inflicted by edge-flips.
For cs09, the difference is substantially reduced,
but a 4.2% performance gap remains. For km04 and
saj10a, the original parameters outperform the new
ones by 3.6% and 1% respectively.
We can see that even when ignoring edge-flips,
some difference remains, albeit not necessarily in
the favor of the modified models. This is because
we did not directly perform edge-flips, but rather
parameter-flips. The difference is thus a result of
second-order effects stemming from the parameter-
flips. In the next section, we explain why the remain-
ing difference is positive for some algorithms (cs09)
and negative for others (km04, saj10a).
For completeness, Table 4 shows a comparison of
some of the current state-of-the-art algorithms, using
attachment score, undirected evaluation and NED.
The training and test sets are those used in Section 3.
The table shows that the relative orderings of the al-
gorithms under NED is different than under the other
669
Algo. Mod. ? Orig.Attach. Undir. NED
km04 9.3 (43.9?34.6) 1.3 (54.2?52.9) ?3.6 (63?66.6)
cs09 14.7 (54.6?39.9) 7.7 (56.9?49.2) 4.2 (66.8?62.6)
saj10a 12.7 (54.3?41.6) 3.5 (59.4?55.9) ?1 (66.8?67.8)
Table 3: Differences between the modified and original
parameter sets when evaluated using attachment score
(Attach.), undirected evaluation (Undir.), and NED.
measures. This is an indication that NED provides a
different perspective on algorithm quality11 .
Algo. Att10 Att? Un10 Un? NED10 NED?
bbdk10 66.1 49.6 70.1 56.0 75.5 61.8
bc10 67.2 53.6 73 61.7 81.6 70.2
cs09 61.5 42 66.9 50.4 81.5 62.9
gggtp10 57.1 45 62.5 53.2 80.4 65.1
km04 45.8 34.6 60.3 52.9 78.4 66.6
saj10a 54.7 41.6 66.5 55.9 78.9 67.8
saj10c 63.8 46.1 72.6 58.8 84.2 70.8
saj10b? 67.9 48.2 74.0 57.7 86.0 70.7
Table 4: A comparison of recent works, using Att (at-
tachment score) Un (undirected evaluation) and NED, on
sentences of length ? 10 (excluding punctuation) and
on all sentences. The gold standard is obtained using
the rules of (Yamada and Matsumoto, 2003). bbdk10:
(Berg-Kirkpatrick et al, 2010), bc10: (Blunsom and
Cohn, 2010), cs09: (Cohen and Smith, 2009), gggtp10:
(Gillenwater et al, 2010), km04: A replication of (Klein
and Manning, 2004), saj10a: (Spitkovsky et al, 2010a),
saj10c: (Spitkovsky et al, 2010c), saj10b?: A lightly-
supervised algorithm (Spitkovsky et al, 2010b).
7 Discussion
In this paper we explored two ways of dealing with
cases in which there is no clear theoretical justifi-
cation to prefer one dependency structure over an-
other. Our experiments suggest that it is crucial to
deal with such structures if we would like to have
a proper evaluation of unsupervised parsing algo-
rithms against a gold standard.
The first way was to modify the parameters of the
parsing algorithms so that in cases where such prob-
lematic decisions are to be made they follow the gold
standard annotation. Indeed, this modification leads
to a substantial improvement in the attachment score
of the algorithms.
11Results may be different than the ones published in the
original papers due to the different conversion procedures used
in each work. See Section 4 for discussion.
The second way was to change the evaluation.
The NED measure we proposed does not punish for
differences between gold and induced structures in
the problematic cases. Indeed, in Section 6 (Table 3)
we show that the differences between the original
and modified models are much smaller when eval-
uating with NED compared to when evaluating with
the traditional attachment score.
As Table 3 reveals, however, even when evaluat-
ing with NED, there is still some difference between
the original and the modified model, for each of the
algorithms we consider. Moreover, for two of the al-
gorithms (km04 and saj10a) NED prefers the original
model while for one (cs09) it prefers the modified
version. In this section we explain these patterns and
show that they are both consistent and predictable.
Our hypothesis, for which we provide empirical
justification, is that in cases where there is no theo-
retically preferred annotation, NED prefers the struc-
tures that are more learnable by DMV. That is, NED
gives higher scores to the annotations that better fit
the assumptions and modeling decisions of DMV,
the model that lies in the basis of the parsing algo-
rithms.
To support our hypothesis we perform an experi-
ment requiring two preparatory steps for each algo-
rithm. First, we construct a supervised version of
the algorithm. This supervised version consists of
the same statistical model as the original unsuper-
vised algorithm, but the parameters are estimated to
maximize the likelihood of a syntactically annotated
training corpus, rather than of a plain text corpus.
Second, we construct two corpora for the algo-
rithm, both consist of the same text and differ only
in their syntactic annotation. The first is annotated
with the gold standard annotation. The second is
similarly annotated except in the linguistically prob-
lematic structures. We replace these structures with
the ones that would have been created with the un-
supervised version of the algorithm (see Table 1 for
the relevant structures for each algorithm)12. Each
12In cases the structures are comprised of a single edge, the
second corpus is obtained from the gold standard by an edge-
flip. The only exceptions are the cases of the prepositional
phrases. Their gold standard and the learned structures for each
of the algorithms are shown in Figure 4. In this case, the sec-
ond corpus is obtained from the gold standard by replacing each
prepositional phrase in the gold standard with the corresponding
670
corpus is divided into a training and a test set.
We then train the supervised version of the algo-
rithms on each of the training sets. We parse the test
data twice, once with each of the resulting models.
We evaluate both parsed corpora against the corpus
annotation from which they originated.
The training set of each corpus consists of sec-
tions 2?21 of WSJ20 (i.e., WSJ sentences of length
?20, excluding punctuation)13 and the test set is sec-
tion 23 of WSJ?. Evaluation is performed using
both NED and attachment score. The patterns we
observed are very similar for both. For brevity, we
report only attachment score results.
km04 cs09 saj10a
Orig. Gold Orig. Gold Orig. Gold
NED,
Unsup. 66.6 63 62.6 66.8 67.8 66.8
Sup. 71.3 69.9 63.3 69.9 71.8 69.9
Table 5: The first line shows the NED results from
Section 6, when using the original parameters (Orig.
columns) and the modified parameters (Gold columns).
The second line shows the results of the supervised ver-
sions of the algorithms using the corpus which agrees
with the unsupervised model in the problematic cases
(Orig.) and the gold standard (Gold).
The results of our experiment are presented in Ta-
ble 5 along with a comparison to the NED scores
from Section 6. The table clearly demonstrates that a
set of parameters (original or modified) is preferred
by NED in the unsupervised experiments reported in
Section 6 (top line) if and only if the structures pro-
duced by this set are better learned by the supervised
version of the algorithm (bottom line).
This observation supports our hypothesis that in
cases where there is no theoretical preference for
one structure over the other, NED (unlike the other
measures) prefers the structures that are more con-
sistent with the modeling assumptions lying in the
basis of the algorithm. We consider this to be a de-
sired property of a measure since a more consistent
model should be preferred where no theoretical pref-
erence exists.
learned structure.
13In using WSJ20, we follow (Spitkovsky et al, 2010a),
which showed that training the DMV on sentences of bounded
length yields a higher score than using the entire corpus. We
use it as we aim to use an optimal setting.
8 Conclusion
In this paper we showed that the standard evalua-
tion of unsupervised dependency parsers is highly
sensitive to problematic annotations. We modified a
small set of parameters that controls the annotation
in such problematic cases in three leading parsers.
This resulted in a major performance boost, which
is unindicative of a true difference in quality.
We presented Neutral Edge Direction (NED), a
measure that is less sensitive to the annotation of
local structures. As the problematic structures are
generally local, NED is less sensitive to their alterna-
tive annotations. In the future, we suggest reporting
NED along with the current measures.
Acknowledgements. We would like to thank Shay
Cohen for his assistance with his implementation of
the DMV parser and Taylor Berg-Kirkpatrick, Phil
Blunsom and Jennifer Gillenwater for providing us
with their data sets. We would also like to thank
Valentin I. Spitkovsky for his comments and for pro-
viding us with his data sets.
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero and Dan Klein, 2010. Painless unsu-
pervised learning with features. In Proc. of NAACL.
Taylor Berg-Kirkpatrick and Dan Klein, 2010. Phyloge-
netic Grammar Induction. In Proc. of ACL.
Cristina Bosco and Vincenzo Lombardo, 2004. Depen-
dency and relational structure in treebank annotation.
In Proc. of the Workshop on Recent Advances in De-
pendency Grammar at COLING?04.
Phil Blunsom and Trevor Cohn, 2010. Unsupervised
Induction of Tree Substitution Grammars for Depen-
dency Parsing. In Proc. of EMNLP.
Shay B. Cohen, Kevin Gimpel and Noah A. Smith, 2008.
Logistic Normal Priors for Unsupervised Probabilistic
Grammar Induction. In Proc. of NIPS.
Shay B. Cohen and Noah A. Smith, 2009. Shared Logis-
tic Normal Distributions for Soft Parameter Tying. In
Proc. of HLT-NAACL.
Michael J. Collins, 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania, Philadelphia.
Alexander Clark, 2001. Unsupervised language acquisi-
tion: theory and practice. Ph.D. thesis, University of
Sussex.
Hal Daume? III, 2009. Unsupervised search-based struc-
tured prediction. In Proc. of ICML.
671
Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuz-
man Ganchev, Joa?o V. Grac?a and Fernando Pereira,
2007. Frustratingly Hard Domain Adaptation for De-
pendency Parsing. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
Gregory Druck, Gideon Mann and Andrew McCal-
lum, 2009. Semi-supervised learning of dependency
parsers using generalized expectation criteria. In
Proc. of ACL.
Jennifer Gillenwater, Kuzman Ganchev, Joa?o V. Grac?a,
Ben Taskar and Fernando Preira, 2010. Sparsity in
dependency grammar induction. In Proc. of ACL.
William P. Headden III, David McClosky, and Eugene
Charniak, 2008. Evaluating Unsupervised Part-of-
Speech Tagging for Grammar Induction. In Proc. of
COLING.
William P. Headden III, Mark Johnson and David Mc-
Closky, 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Proc.
of HLT-NAACL.
Richard Johansson and Pierre Nugues, 2007. Ex-
tended Constituent-to-Dependency Conversion for En-
glish. In Proc. of NODALIDA.
Dan Klein, 2005. The unsupervised learning of natural
language structure. Ph.D. thesis, Stanford University.
Dan Klein and Christopher Manning, 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proc. of ACL.
Sandra Ku?bler, 2005. How Do Treebank Annotation
Schemes Influence Parsing Results? Or How Not to
Compare Apples And Oranges. In Proc. of RANLP.
Sandra Ku?bler, R. McDonald and Joakim Nivre, 2009.
Dependency Parsing. Morgan And Claypool Publish-
ers.
Mitchell Marcus, Beatrice Santorini and Mary Ann
Marcinkiewicz, 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics 19:313-330.
Tahira Naseem, Harr Chen, Regina Barzilay and Mark
Johnson, 2010. Using universal linguistic knowledge
to guide grammar induction. In Proc. of EMNLP.
Joakim Nivre, 2006. Inductive Dependency Parsing.
Springer.
Joakim Nivre, Johan Hall and Jens Nilsson, 2006. Malt-
Parser: A data-driven parser-generator for depen-
dency parsing. In Proc. of LREC-2006.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel and Deniz Yuret,
2007. The CoNLL 2007 shared task on dependency
parsing. In Proc. of the CoNLL Shared Task, EMNLP-
CoNLL, 2007.
Jens Nilsson, Joakim Nivre and Johan Hall, 2006. Graph
transformations in data-driven dependency parsing.
In Proc. of ACL.
Owen Rambow, Cassandre Creswell, Rachel Szekely,
Harriet Tauber and Marilyn Walker, 2002. A depen-
dency treebank for English. In Proc. of LREC.
Noah A. Smith and Jason Eisner, 2005. Guiding unsu-
pervised grammar induction using contrastive estima-
tion. In Proc. of IJCAI.
Noah A. Smith and Jason Eisner, 2006. Annealing struc-
tural bias in multilingual weighted grammar induc-
tion. In Proc. of ACL.
Valentin I. Spitkovsky, Hiyan Alshawi and Daniel Juraf-
sky, 2010a. From Baby Steps to Leapfrog: How ?Less
is More? in Unsupervised Dependency Parsing. In
Proc. of NAACL-HLT.
Valentin I. Spitkovsky, Hiyan Alshawi and Daniel Juraf-
sky, 2010b. Profiting from Mark-Up: Hyper-Text An-
notations for Guided Parsing. In Proc. of ACL.
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky
and Christopher D. Manning, 2010c. Viterbi training
improves unsupervised dependency parsing. In Proc.
of CoNLL.
Qin Iris Wang, Dale Schuurmans and Dekang Lin, 2005.
Strictly Lexical Dependency Parsing. In IWPT.
Qin Iris Wang, Colin Cherry, Dan Lizotte and Dale Schu-
urmans, 2006. Improved Large Margin Dependency
Parsing via Local Constraints and Laplacian Regular-
ization. In Proc. of CoNLL.
Hiroyasu Yamada and Yuji Matsumoto, 2003. Statistical
dependency analysis with support vector machines. In
Proc. of the International Workshop on Parsing Tech-
nologies.
672
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 228?238,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Universal Conceptual Cognitive Annotation (UCCA)
Omri Abend?
Institute of Computer Science
The Hebrew University
omria01@cs.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University
arir@cs.huji.ac.il
Abstract
Syntactic structures, by their nature, re-
flect first and foremost the formal con-
structions used for expressing meanings.
This renders them sensitive to formal vari-
ation both within and across languages,
and limits their value to semantic ap-
plications. We present UCCA, a novel
multi-layered framework for semantic rep-
resentation that aims to accommodate the
semantic distinctions expressed through
linguistic utterances. We demonstrate
UCCA?s portability across domains and
languages, and its relative insensitivity
to meaning-preserving syntactic variation.
We also show that UCCA can be ef-
fectively and quickly learned by annota-
tors with no linguistic background, and
describe the compilation of a UCCA-
annotated corpus.
1 Introduction
Syntactic structures are mainly committed to rep-
resenting the formal patterns of a language, and
only indirectly reflect semantic distinctions. For
instance, while virtually all syntactic annotation
schemes are sensitive to the structural difference
between (a) ?John took a shower? and (b) ?John
showered?, they seldom distinguish between (a)
and the markedly different (c) ?John took my
book?. In fact, the annotations of (a) and (c) are
identical under the most widely-used schemes for
English, the Penn Treebank (PTB) (Marcus et al,
1993) and CoNLL-style dependencies (Surdeanu
et al, 2008) (see Figure 1).
? Omri Abend is grateful to the Azrieli Foundation for
the award of an Azrieli Fellowship.
Underscoring the semantic similarity between
(a) and (b) can assist semantic applications. One
example is machine translation to target languages
that do not express this structural distinction (e.g.,
both (a) and (b) would be translated to the same
German sentence ?John duschte?). Question An-
swering applications can also benefit from dis-
tinguishing between (a) and (c), as this knowl-
edge would help them recognize ?my book? as a
much more plausible answer than ?a shower? to
the question ?what did John take??.
This paper presents a novel approach to gram-
matical representation that annotates semantic dis-
tinctions and aims to abstract away from specific
syntactic constructions. We call our approach Uni-
versal Conceptual Cognitive Annotation (UCCA).
The word ?cognitive? refers to the type of cate-
gories UCCA uses and its theoretical underpin-
nings, and ?conceptual? stands in contrast to ?syn-
tactic?. The word ?universal? refers to UCCA?s
capability to accommodate a highly rich set of se-
mantic distinctions, and its aim to ultimately pro-
vide all the necessary semantic information for
learning grammar. In order to accommodate this
rich set of distinctions, UCCA is built as a multi-
layered structure, which allows for its open-ended
extension. This paper focuses on the foundational
layer of UCCA, a coarse-grained layer that rep-
resents some of the most important relations ex-
pressed through linguistic utterances, including ar-
gument structure of verbs, nouns and adjectives,
and the inter-relations between them (Section 2).
UCCA is supported by extensive typologi-
cal cross-linguistic evidence and accords with
the leading Cognitive Linguistics theories. We
build primarily on Basic Linguistic Theory (BLT)
(Dixon, 2005; 2010a; 2010b; 2012), a typological
approach to grammar successfully used for the de-
228
scription of a wide variety of languages. BLT uses
semantic similarity as its main criterion for cate-
gorizing constructions both within and across lan-
guages. UCCA takes a similar approach, thereby
creating a set of distinctions that is motivated
cross-linguistically. We demonstrate UCCA?s rel-
ative insensitivity to paraphrasing and to cross-
linguistic variation in Section 4.
UCCA is exceptional in (1) being a semantic
scheme that abstracts away from specific syntactic
forms and is not defined relative to a specific do-
main or language, (2) providing a coarse-grained
representation which allows for open-ended ex-
tension, and (3) using cognitively-motivated cat-
egories. An extensive comparison of UCCA to ex-
isting approaches to syntactic and semantic repre-
sentation, focusing on the major resources avail-
able for English, is found in Section 5.
This paper also describes the compilation of a
UCCA-annotated corpus. We provide a quanti-
tative assessment of the annotation quality. Our
results show a quick learning curve and no sub-
stantial difference in the performance of annota-
tors with and without background in linguistics.
This is an advantage of UCCA over its syntactic
counterparts that usually need annotators with ex-
tensive background in linguistics (see Section 3).
We note that UCCA?s approach that advocates
automatic learning of syntax from semantic super-
vision stands in contrast to the traditional view of
generative grammar (Clark and Lappin, 2010).
2 The UCCA Scheme
2.1 The Formalism
UCCA uses directed acyclic graphs (DAGs) to
represent its semantic structures. The atomic
meaning-bearing units are placed at the leaves of
the DAG and are called terminals. In the founda-
tional layer, terminals are words and multi-word
chunks, although this definition can be extended
to include arbitrary morphemes.
The nodes of the graph are called units. A unit
may be either (i) a terminal or (ii) several ele-
ments jointly viewed as a single entity according
to some semantic or cognitive consideration. In
many cases, a non-terminal unit is comprised of a
single relation and the units it applies to (its argu-
ments), although in some cases it may also contain
secondary relations. Hierarchy is formed by using
units as arguments or relations in other units.
Categories are annotated over the graph?s edges,
and represent the descendant unit?s role in forming
the semantics of the parent unit. Therefore, the in-
ternal structure of a unit is represented by its out-
bound edges and their categories, while the roles
a unit plays in the relations it participates in are
represented by its inbound edges.
We note that UCCA?s structures reflect a single
interpretation of the text. Several discretely dif-
ferent interpretations (e.g., high vs. low PP at-
tachments) may therefore yield several different
UCCA annotations.
UCCA is a multi-layered formalism, where
each layer specifies the relations it encodes. The
question of which relations will be annotated
(equivalently, which units will be formed) is de-
termined by the layer in question. For example,
consider ?John kicked his ball?, and assume our
current layer encodes the relations expressed by
?kicked? and by ?his?. In that case, the unit ?his?
has a single argument1 (?ball?), while ?kicked?
has two (?John? and ?his ball?). Therefore, the
units of the sentence are the terminals (which are
always units), ?his ball? and ?John kicked his
ball?. The latter two are units by virtue of express-
ing a relation along with its arguments. See Fig-
ure 2(a) for a graph representation of this example.
For a brief comparison of the UCCA formalism
with other dependency annotations see Section 5.
2.2 The UCCA Foundational Layer
The foundational layer is designed to cover the
entire text, so that each word participates in at
least one unit. It focuses on argument structures
of verbal, nominal and adjectival predicates and
the inter-relations between them. Argument struc-
ture phenomena are considered basic by many ap-
proaches to semantic and grammatical representa-
tion, and have a high applicative value, as demon-
strated by their extensive use in NLP.
The foundational layer views the text as a col-
lection of Scenes. A Scene can describe some
movement or action, or a temporally persistent
state. It generally has a temporal and a spatial di-
mension, which can be specific to a particular time
and place, but can also describe a schematized
event which refers to many events by highlight-
ing a common meaning component. For example,
the Scene ?John loves bananas? is a schematized
event, which refers to John?s disposition towards
bananas without making any temporal or spatial
1The anaphoric aspects of ?his? are not considered part of
the current layer (see Section 2.3).
229
John took a shower -ROOT-
ROOT
SBJ
OBJ
NMOD
(a)
John showered -ROOT-
ROOTSBJ
(b)
John took my book -ROOT-
ROOT
SBJ
OBJ
NMOD
(c)
Figure 1: CoNLL-style dependency annotations. Note that (a) and (c), which have different semantics but superficially similar
syntax, have the same annotation.
Abb. Category Short Definition
Scene Elements
P Process The main relation of a Scene that evolves in time (usually an action or movement).
S State The main relation of a Scene that does not evolve in time.
A Participant A participant in a Scene in a broad sense (including locations, abstract entities and Scenes serving
as arguments).
D Adverbial A secondary relation in a Scene (including temporal relations).
Elements of Non-Scene Units
C Center Necessary for the conceptualization of the parent unit.
E Elaborator A non-Scene relation which applies to a single Center.
N Connector A non-Scene relation which applies to two or more Centers, highlighting a common feature.
R Relator All other types of non-Scene relations. Two varieties: (1) Rs that relate a C to some super-ordinate
relation, and (2) Rs that relate two Cs pertaining to different aspects of the parent unit.
Inter-Scene Relations
H Parallel
Scene
A Scene linked to other Scenes by regular linkage (e.g., temporal, logical, purposive).
L Linker A relation between two or more Hs (e.g., ?when?, ?if?, ?in order to?).
G Ground A relation between the speech event and the uttered Scene (e.g., ?surprisingly?, ?in my opinion?).
Other
F Function Does not introduce a relation or participant. Required by the structural pattern it appears in.
Table 1: The complete set of categories in UCCA?s foundational layer.
specifications. The definition of a Scene is moti-
vated cross-linguistically and is similar to the se-
mantic aspect of the definition of a ?clause? in Ba-
sic Linguistic Theory2.
Table 1 provides a concise description of the
categories used by the foundational layer3. We
turn to a brief description of them.
Simple Scenes. Every Scene contains one main
relation, which is the anchor of the Scene, the most
important relation it describes (similar to frame-
evoking lexical units in FrameNet (Baker et al,
1998)). We distinguish between static Scenes, that
describe a temporally persistent state, and proces-
sual Scenes that describe a temporally evolving
event, usually a movement or an action. The main
relation receives the category State (S) in static and
Process (P) in processual Scenes. We note that
the S-P distinction is introduced here mostly for
practical purposes, and that both categories can be
viewed as sub-categories of the more abstract cat-
egory Main Relation.
A Scene contains one or more Participants (A).
2As UCCA annotates categories on its edges, Scene nodes
bear no special indication. They can be identified by examin-
ing the labels on their outgoing edges (see below).
3Repeated here with minor changes from (Abend and
Rappoport, 2013), which focuses on the categories them-
selves.
This category subsumes concrete and abstract par-
ticipants as well as embedded Scenes (see be-
low). Scenes may also contain secondary rela-
tions, which are marked as Adverbials (D).
The above categories are indifferent to the syn-
tactic category of the Scene-evoking unit, be it a
verb, a noun, an adjective or a preposition. For in-
stance, in the Scene ?The book is in the garden?,
?is in? is the S, while ?the book? and ?the garden?
are As. In ?Tomatoes are red?, the main static re-
lation is ?are red?, while ?Tomatoes? is an A.
The foundational layer designates a separate set
of categories to units that do not evoke a Scene.
Centers (C) are the sub-units of a non-Scene unit
that are necessary for the unit to be conceptualized
and determine its semantic type. There can be one
or more Cs in a non-Scene unit4.
Other sub-units of non-Scene units are catego-
rized into three types. First, units that apply to a
single C are annotated as Elaborators (E). For in-
stance, ?big? in ?big dogs? is an E, while ?dogs? is
a C. We also mark determiners as Es in this coarse-
grained layer5. Second, relations that relate two or
4By allowing several Cs we avoid the difficulties incurred
by the common single head assumption. In some cases the
Cs are inferred from context and can be implicit.
5Several Es that apply to a single C are often placed in
230
more Cs, highlighting a common feature or role
(usually coordination), are called Connectors (N).
See an example in Figure 2(b).
Relators (R) cover all other types of relations
between two or more Cs. Rs appear in two main
varieties. In one, Rs relate a single entity to a
super-ordinate relation. For instance, in ?I heard
noise in the kitchen?, ?in? relates ?the kitchen?
to the Scene it is situated in. In the other, Rs re-
late two units pertaining to different aspects of the
same entity. For instance, in ?bottom of the sea?,
?of? relates ?bottom? and ?the sea?, two units that
refer to different aspects of the same entity.
Some units do not introduce a new relation or
entity into the Scene, and are only part of the for-
mal pattern in which they are situated. Such units
are marked as Functions (F). For example, in the
sentence ?it is customary for John to come late?,
the ?it? does not refer to any specific entity or re-
lation and is therefore an F.
Two example annotations of simple Scenes are
given in Figure 2(a) and Figure 2(b).
More complex cases. UCCA allows units to
participate in more than one relation. This is a nat-
ural requirement given the wealth of distinctions
UCCA is designed to accommodate. Already in
the foundational layer of UCCA, the need arises
for multiple parents. For instance, in ?John asked
Mary to join him?, ?Mary? is a Participant of both
the ?asking? and the ?joining? Scenes.
In some cases, an entity or relation is prominent
in the interpretation of the Scene, but is not men-
tioned explicitly anywhere in the text. We mark
such entities as Implicit Units. Implicit units are
identical to terminals, except that they do not cor-
respond to a stretch of text. For example, ?playing
games is fun? has an implicit A which corresponds
to the people playing the game.
UCCA annotates inter-Scene relations (linkage)
and, following Basic Linguistic Theory, distin-
guishes between three major types of linkage.
First, a Scene can be an A in another Scene. For
instance, in ?John said he must leave?, ?he must
leave? is an A inside the Scene evoked by ?said?.
Second, a Scene may be an E of an entity in an-
other Scene. For instance, in ?the film we saw yes-
terday was wonderful?, ?film we saw yesterday? is
a Scene that serves as an E of ?film?, which is both
an A in the Scene and the Center of an A in the
a flat structure. In general, the coarse-grained foundational
layer does not try to resolve fine scope issues.
John
A
kicked
P
his
E
ball
C
A
(a)
John
C
and
N
Mary
C
A
bought
P
a
E
sofa
C
A
together
D
(b)
the film
A
we
A
saw
P
yesterday
D
E
A
was
F
wonderful
C
S
E C
(c)
Figure 2: Examples of UCCA annotation graphs.
Scene evoked by ?wonderful? (see Figure 2(c)).
A third type of linkage covers all other cases,
e.g., temporal, causal and conditional inter-Scene
relations. The linked Scenes in such cases are
marked as Parallel Scenes (H). The units speci-
fying the relation between Hs are marked as Link-
ers (L)6. As with other relations in UCCA, Linkers
and the Scenes they link are bound by a unit.
Unlike common practice in grammatical anno-
tation, linkage relations in UCCA can cross sen-
tence boundaries, as can relations represented in
other layers (e.g., coreference). UCCA therefore
annotates texts comprised of several paragraphs
and not individual sentences (see Section 3).
Example sentences. Following are complete
annotations of two abbreviated example sentences
from our corpus (see Section 3).
?Golf became a passion for his oldest daughter:
she took daily lessons and became very good,
reaching the Connecticut Golf Championship.?
This sentence contains four Scenes, evoked by
?became a passion?, ?took daily lessons?, ?be-
came very good? and ?reaching?. The individual
Scenes are annotated as follows:
1. ?GolfA [becameE aE passionC]P [forR hisE
oldestE daughterC]A?
6It is equally plausible to include Linkers for the other two
linkage types. This is not included in the current layer.
231
2. ?sheA [tookF [dailyE lessonsC]C]P ?
3. ?sheA ... [becameE [veryE goodC]C]S?
4. ?sheA ... reachingP [theE ConnecticutE
GolfE ChampionshipC ]A?
There is only one explicit Linker in this sen-
tence (?and?), which links Scenes (2) and (3).
None of the Scenes is an A or an E in the other, and
they are therefore all marked as Parallel Scenes.
We also note that in the case of the light verb
construction ?took lessons? and the copula clauses
?became good? and ?became a passion?, the verb
is not the Center of the main relation, but rather
the following noun or adjective. We also note that
the unit ?she? is an A in Scenes (2), (3) and (4).
We turn to our second example:
?Cukor encouraged the studio to
accept her demands.?
This sentence contains three Scenes, evoked by
?encouraged?, ?accept? and ?demands?:
1. CukorA encouragedP [theE studioC]A [toR
[accept her demands]C ]A
2. [the studio]A ... acceptP [her demands]A
3. herA demandsP IMPA
Scenes (2) and (3) act as Participants in Scenes
(1) and (2) respectively. In Scene (2), there is
an implicit Participant which corresponds to what-
ever was demanded. Note that ?her demands? is a
Scene, despite being a noun phrase.
2.3 UCCA?s Multi-layered Structure
Additional layers may refine existing relations or
otherwise annotate a complementary set of dis-
tinctions. For instance, a refinement layer can
categorize linkage relations according to their se-
mantic types (e.g., temporal, purposive, causal) or
provide tense distinctions for verbs. Another im-
mediate extension to UCCA?s foundational layer
can be the annotation of coreference relations. Re-
call the example ?John kicked his ball?. A coref-
erence layer would annotate a relation between
?John? and ?his? by introducing a new node whose
descendants are these two units. The fact that
this node represents a coreference relation would
be represented by a label on the edge connecting
them to the coreference node.
There are three common ways to extend an an-
notation graph. First, by adding a relation that re-
lates previously established units. This is done by
introducing a new node whose descendants are the
related units. Second, by adding an intermediate
Passage #
1 2 3 4 5 6
# Sents. 8 20 23 14 13 15
# Tokens 259 360 343 322 316 393
ITA 67.3 74.1 71.2 73.5 77.8 81.1
Vs. Gold 72.4 76.7 75.5 75.7 79.5 84.2
Correction 93.7
Table 2: The upper part of the table presents the number of
sentences and the number of tokens in the first passages used
for the annotator training. The middle part presents the av-
erage F-scores obtained by the annotators throughout these
passages. The first row presents the average F-score when
comparing the annotations of the different annotators among
themselves. The second row presents the average F-score
when comparing them to a ?gold standard?. The bottom row
shows the average F-score between an annotated passage of
a trained annotator and its manual correction by an expert. It
is higher due to conforming analyses (see text). All F-scores
are in percents.
unit between a parent unit and some of its sub-
units. For instance, consider ?he replied foolishly?
and ?he foolishly replied?. A layer focusing on
Adverbial scope may refine the flat Scene structure
assigned by the foundational layer, expressing the
scope of ?foolishly? over the relation ?replied? in
the first case, and over the entire Scene in the sec-
ond. Third, by adding sub-units to a terminal. For
instance, consider ?gave up?, an expression which
the foundational layer considers atomic. A layer
that annotates tense can break the expression into
?gave? and ?up?, in order to annotate ?gave? as the
tense-bearing unit.
Although a more complete discussion of the for-
malism is beyond the scope of this paper, we note
that the formalism is designed to allow different
annotation layers to be defined and annotated in-
dependently of one another, in order to facilitate
UCCA?s construction through a community effort.
3 A UCCA-Annotated Corpus
The annotated text is mostly based on English
Wikipedia articles for celebrities. We have chosen
this genre as it is an inclusive and diverse domain,
which is still accessible to annotators from varied
backgrounds.
For the annotation process, we designed and im-
plemented a web application tailored for UCCA?s
annotation. A sample of the corpus containing
roughly 5K tokens, as well as the annotation ap-
plication can be found in our website7.
UCCA?s annotations are not confined to a sin-
gle sentence. The annotation is therefore carried
out in passages of 300-400 tokens. After its an-
7www.cs.huji.ac.il/?omria01
232
notation, a passage is manually corrected before
being inserted into the repository.
The section of the corpus annotated thus far
contains 56890 tokens in 148 annotated passages
(average length of 385 tokens). Each passage con-
tains 450 units on average and 42.2 Scenes. Each
Scene contains an average of 2 Participants and 0.3
Adverbials. 15% of the Scenes are static (contain
an S as the main relation) and the rest are dynamic
(containing a P). The average number of tokens in
a Scene (excluding punctuation) is 10.7. 18.3%
of the Scenes are Participants in another Scene,
11.4% are Elaborator Scenes and the remaining
are Parallel Scenes. A passage contains an aver-
age of 11.2 Linkers.
Inter-annotator agreement. We employ 4 an-
notators with varying levels of background in lin-
guistics. Two of the annotators have no back-
ground in linguistics, one took an introductory
course and one holds a Bachelor?s degree in lin-
guistics. The training process of the annotators
lasted 30?40 hours, which includes the time re-
quired for them to get acquainted with the web
application. As this was the first large-scale trial
with the UCCA scheme, some modifications to the
scheme were made during the annotator?s training.
We therefore expect the training process to be even
faster in later distributions.
There is no standard evaluation measure for
comparing two grammatical annotations in the
form of labeled DAGs. We therefore converted
UCCA to constituency trees8 and, following stan-
dard practice, computed the number of brackets in
both trees that match in both span and label. We
derive an F-score from these counts.
Table 2 presents the inter-annotator agreement
in the training phase. The four annotators were
given the same passage in each of these cases. In
addition, a ?gold standard? was annotated by the
authors of this paper. The table presents the av-
erage F-score between the annotators, as well as
the average F-score when comparing to the gold
standard. Results show that although it repre-
sents complex hierarchical structures, the UCCA
scheme is learned quickly and effectively.
We also examined the influence of prior linguis-
tic background on the results. In the first passage
there was a substantial advantage to the annotators
8In cases a unit had multiple parents, we discarded all but
one of its incoming edges. This resulted in discarding 1.9%
of the edges. We applied a simple normalization procedure to
the resulting trees.
who had prior training in linguistics. The obtained
F-scores when comparing to a gold standard, or-
dered decreasingly according to the annotator?s
acquaintance with linguistics, were 78%, 74.4%,
69.5% and 67.8%. However, this performance gap
quickly vanished. Indeed, the obtained F-scores,
again compared to a gold standard and averaged
over the next five training passages, were (by the
same order) 78.6%, 77.3%, 79.2% and 78%.
This is an advantage of UCCA over other syn-
tactic annotation schemes that normally require
highly proficient annotators. For instance, both
the PTB and the Prague Dependency Treebank
(Bo?hmova? et al, 2003) employed annotators with
extensive linguistic background. Similar findings
to ours were reported in the PropBank project,
which successfully employed annotators with var-
ious levels of linguistic background. We view
this as a major advantage of semantic annotation
schemes over their syntactic counterparts, espe-
cially given the huge amount of manual labor re-
quired for large syntactic annotation projects.
The UCCA interface allows for multiple non-
contradictory (?conforming?) analyses of a stretch
of text. It assumes that in some cases there is
more than one acceptable option, each highlight-
ing a different aspect of meaning of the analyzed
utterance (see below). This makes the computa-
tion of inter-annotator agreement fairly difficult.
It also suggests that the above evaluation is exces-
sively strict, as it does not take into account such
conforming analyses. To address this issue, we
conducted another experiment where an expert an-
notator corrected the produced annotations. Com-
paring the corrected versions to the originals, we
found that F-scores are typically in the range of
90%?95%. An average taken over a sample of
passages annotated by all four annotators yielded
an F-score of 93.7%.
It is difficult to compare the above results to the
inter-annotator agreement of other projects for two
reasons. First, many existing schemes are based
on other annotation schemes or heavily rely on
automatic tools for providing partial annotations.
Second, some of the most prominent annotation
projects do not provide reliable inter-annotator
agreement scores (Artstein and Poesio, 2008).
A recent work that did report inter-annotator
agreement in terms of bracketing F-score is an an-
notation project of the PTB?s noun phrases with
more elaborate syntactic structure (Vadas and Cur-
233
ran, 2011). They report an agreement of 88.3% in
a scenario where their two annotators worked sep-
arately. Note that this task is much more limited
in scope than UCCA (annotates noun phrases in-
stead of complete passages in UCCA; uses 2 cat-
egories instead of 12 in UCCA). Nevertheless, the
obtained inter-annotator agreement is comparable.
Disagreement examples. Here we discuss two
major types of disagreements that recurred in the
training process. The first is the distinction be-
tween Elaborators and Centers. In most cases this
distinction is straightforward, particularly where
one sub-unit determines the semantic type of the
parent unit, while its siblings add more informa-
tion to it (e.g., ?truckE companyC? is a type of a
company and not of a truck). Some structures do
not nicely fall into this pattern. One such case is
with apposition. In the example ?the Fox drama
Glory days?, both ?the Fox drama? and ?Glory
days? are reasonable candidates for being a Cen-
ter, which results in disagreements.
Another case is the distinction between Scenes
and non-Scene relations. Consider the example
?[John?s portrayal of the character] has been de-
scribed as ...?. The sentence obviously contains
two scenes, one in which John portrays a charac-
ter and another where someone describes John?s
doings. Its internal structure is therefore ?John?sA
portrayalP [of the character]A?. However, the
syntactic structure of this unit leads annotators at
times into analyzing the subject as a non-Scene re-
lation whose C is ?portrayal?.
Static relations tend to be more ambiguous be-
tween a Scene and a non-Scene interpretation.
Consider ?Jane Smith (ne?e Ross)?. It is not at all
clear whether ?ne?e Ross? should be annotated as a
Scene or not. Even if we do assume it is a Scene,
it is not clear whether the Scene it evokes is her
Scene of birth, which is dynamic, or a static Scene
which can be paraphrased as ?originally named
Ross?. This leads to several conforming analyses,
each expressing a somewhat different conceptual-
ization of the Scene. This central notion will be
more elaborately addressed in future work.
We note that all of these disagreements can be
easily resolved by introducing an additional layer
focusing on the construction in question.
4 UCCA?s Benefits to Semantic Tasks
UCCA?s relative insensitivity to syntactic forms
has potential benefits for a wide variety of seman-
tic tasks. This section briefly demonstrates these
benefits through a number of examples.
Recall the example ?John took a shower? (Sec-
tion 1). UCCA annotates the sentence as a sin-
gle Scene, with a single Participant and a proces-
sual main relation: ?JohnA [tookF [aE showerC]C
]P ?. The paraphrase ?John showered? is anno-
tated similarly: ?JohnA showeredP ?. The struc-
ture is also preserved under translation to other
languages, such as German (?JohnA duschteP ?,
where ?duschte? is a verb), or Portuguese ?JohnA
[tomouF banhoC]P ? (literally, John took shower).
In all of these cases, UCCA annotates the example
as a Scene with an A and a P, whose Center is a
word expressing the notion of showering.
Another example is the sentence ?John does
not have any money?. The foundational layer
of UCCA annotates negation units as Ds, which
yields the annotation ?JohnA [doesF ]S- notD
[haveC]-S [anyE moneyC]A? (where ?does ...
have? is a discontiguous unit)9. This sentence can
be paraphrased as ?JohnA hasP noD moneyA?.
UCCA reflects the similarity of these two sen-
tences, as it annotates both cases as a single Scene
which has two Participants and a negation. A syn-
tactic scheme would normally annotate ?no? in the
second sentence as a modifier of ?money?, and
?not? as a negation of ?have?.
The value of UCCA?s annotation can again be
seen in translation to languages that have only one
of these forms. For instance, the German transla-
tion of this sentence, ?JohnA hatS keinD GeldA?,
is a literal translation of ?John has no money?. The
Hebrew translation of this sentence is ?eyn le john
kesef? (literally, ?there-is-no to John money?).
The main relation here is therefore ?eyn? (there-
is-no) which will be annotated as S. This yields
the annotation ?eynS [leR JohnC]A kesefA?.
The UCCA annotation in all of these cases is
composed of two Participants and a State. In En-
glish and German, the negative polarity unit is rep-
resented as a D. The negative polarity of the He-
brew ?eyn? is represented in a more detailed layer.
As a third example, consider the two sentences
?There are children playing in the park? and ?Chil-
dren are playing in the park?. The two sentences
have a similar meaning but substantially different
syntactic structures. The first contains two clauses,
an existential main clause (headed by ?there are?)
9The foundational layer places ?not? in the Scene level to
avoid resolving fine scope issues (see Section 2)
234
and a subordinate clause (?playing in the park?).
The second contains a simple clause headed by
?playing?. While the parse trees of these sentences
are very different, their UCCA annotation in the
foundational layer differ only in terms of Function
units: ?ChildrenA [areF playingC]P [inR theE
parkC]A? and ?ThereF areF childrenA [playing]P
[inR theE parkC]A?10.
Aside from machine translation, a great vari-
ety of semantic tasks can benefit from a scheme
that is relatively insensitive to syntactic variation.
Examples include text simplification (e.g., for sec-
ond language teaching) (Siddharthan, 2006), para-
phrase detection (Dolan et al, 2004), summariza-
tion (Knight and Marcu, 2000), and question an-
swering (Wang et al, 2007).
5 Related Work
In this section we compare UCCA to some of the
major approaches to grammatical representation in
NLP. We focus on English, which is the most stud-
ied language and the focus of this paper.
Syntactic annotation schemes come in many
forms, from lexical categories such as POS tags
to intricate hierarchical structures. Some for-
malisms focus particularly on syntactic distinc-
tions, while others model the syntax-semantics in-
terface as well (Kaplan and Bresnan, 1981; Pollard
and Sag, 1994; Joshi and Schabes, 1997; Steed-
man, 2001; Sag, 2010, inter alia). UCCA diverges
from these approaches in aiming to abstract away
from specific syntactic forms and to only represent
semantic distinctions. Put differently, UCCA ad-
vocates an approach that treats syntax as a hidden
layer when learning the mapping between form
and meaning, while existing syntactic approaches
aim to model it manually and explicitly.
UCCA does not build on any other annotation
layers and therefore implicitly assumes that se-
mantic annotation can be learned directly. Recent
work suggests that indeed structured prediction
methods have reached sufficient maturity to allow
direct learning of semantic distinctions. Examples
include Naradowsky et al (2012) for semantic role
labeling and Kwiatkowski et al (2010) for seman-
tic parsing to logical forms. While structured pre-
diction for the task of predicting tree structures
is already well established (e.g., (Suzuki et al,
10The two sentences are somewhat different in terms of
their information structure (Van Valin Jr., 2005), which is rep-
resented in a more detailed UCCA layer.
2009)), recent work has also successfully tackled
the task of predicting semantic structures in the
form of DAGs (Jones et al, 2012).
The most prominent annotation scheme in NLP
for English syntax is the Penn Treebank. Many
syntactic schemes are built or derived from it. An
increasingly popular alternative to the PTB are
dependency structures, which are usually repre-
sented as trees whose nodes are the words of the
sentence (Ivanova et al, 2012). Such represen-
tations are limited due to their inability to natu-
rally represent constructions that have more than
one head, or in which the identity of the head
is not clear. They also face difficulties in repre-
senting units that participate in multiple relations.
UCCA proposes a different formalism that ad-
dresses these problems by introducing a new node
for every relation (cf. (Sangati and Mazza, 2009)).
Several annotated corpora offer a joint syntac-
tic and semantic representation. Examples in-
clude the Groningen Meaning bank (Basile et al,
2012), Treebank Semantics (Butler and Yoshi-
moto, 2012) and the Lingo Redwoods treebank
(Oepen et al, 2004). UCCA diverges from these
projects in aiming to abstract away from syntac-
tic variation, and is therefore less coupled with a
specific syntactic theory.
A different strand of work addresses the con-
struction of an interlingual representation, often
with a motivation of applying it to machine trans-
lation. Examples include the UNL project (Uchida
and Zhu, 2001), the IAMTC project (Dorr et al,
2010) and the AMR project (Banarescu et al,
2012). These projects share with UCCA their
emphasis on cross-linguistically valid annotations,
but diverge from UCCA in three important re-
spects. First, UCCA emphasizes the notion of
a multi-layer structure where the basic layers are
maximally coarse-grained, in contrast to the above
works that use far more elaborate representations.
Second, from a theoretical point of view, UCCA
differs from these works in aiming to represent
conceptual semantics, building on works in Cog-
nitive Linguistics (e.g., (Langacker, 2008)). Third,
unlike interlingua that generally define abstract
representations that may correspond to several dif-
ferent texts, UCCA incorporates the text into its
structure, thereby facilitating learning.
Semantic role labeling (SRL) schemes bear
similarity to the foundational layer, due to their
focus on argument structure. The leading SRL ap-
235
proaches are PropBank (Palmer et al, 2005) and
NomBank (Meyers et al, 2004) on the one hand,
and FrameNet (Baker et al, 1998) on the other. At
this point, all these schemes provide a more fine-
grained set of categories than UCCA.
PropBank and NomBank are built on top of the
PTB annotation, and provide for each verb (Prop-
Bank) and noun (NomBank), a delineation of their
arguments and their categorization into semantic
roles. Their structures therefore follow the syn-
tax of English quite closely. UCCA is generally
less tailored to the syntax of English (e.g., see sec-
ondary verbs (Dixon, 2005)).
Furthermore, PropBank and NomBank do not
annotate the internal structure of their arguments.
Indeed, the construction of the commonly used se-
mantic dependencies derived from these schemes
(Surdeanu et al, 2008) required a set of syntactic
head percolation rules to be used. These rules are
somewhat arbitrary (Schwartz et al, 2011), do not
support multiple heads, and often reflect syntac-
tic rather than semantic considerations (e.g., ?mil-
lions? is the head of ?millions of dollars?, while
?dollars? is the head of ?five million dollars?).
Another difference is that PropBank and Nom-
Bank each annotate only a subset of predicates,
while UCCA is more inclusive. This difference
is most apparent in cases where a single complex
predicate contains both nominal and verbal com-
ponents (e.g., ?limit access?, ?take a shower?). In
addition, neither PropBank nor Nomabnk address
copula clauses, despite their frequency. Finally,
unlike PropBank and NomBank, UCCA?s founda-
tional layer annotates linkage relations.
In order to quantify the similarity between
UCCA and PropBank, we annotated 30 sentences
from the PropBank corpus with their UCCA anno-
tations and converted the outcome to PropBank-
style annotations11. We obtained an unlabeled
F-score of 89.4% when comparing to PropBank,
which indicates that PropBank-style annotations
are generally derivable from UCCA?s. The dis-
agreement between the schemes reflects both an-
notation conventions and principle differences,
some of which were discussed above.
The FrameNet project (Baker et al, 1998)
11The experiment was conducted on the first 30 sentences
of section 02. The identity of the predicates was determined
according to the PropBank annotation. We applied a simple
conversion procedure that uses half a dozen rules that are not
conditioned on any lexical item. We used a strict evaluation
that requires an exact match in the argument?s boundaries.
proposes a comprehensive approach to semantic
roles. It defines a lexical database of Frames, each
containing a set of possible frame elements and
their semantic roles. It bears similarity to UCCA
both in its use of Frames, which are a context-
independent abstraction of UCCA?s Scenes, and
in its emphasis on semantic rather than distribu-
tional considerations. However, despite these sim-
ilarities, FrameNet focuses on constructing a lex-
ical resource covering specific cases of interest,
and does not provide a fully annotated corpus of
naturally occurring text. UCCA?s foundational
layer can be seen as a complementary effort to
FrameNet, as it focuses on high-coverage, coarse-
grained annotation, while FrameNet is more fine-
grained at the expense of coverage.
6 Conclusion
This paper presented Universal Conceptual Cog-
nitive Annotation (UCCA), a novel framework
for semantic representation. We described the
foundational layer of UCCA and the compilation
of a UCCA-annotated corpus. We demonstrated
UCCA?s relative insensitivity to paraphrases and
cross-linguistic syntactic variation. We also dis-
cussed UCCA?s accessibility to annotators with no
background in linguistics, which can alleviate the
almost prohibitive annotation costs of large syn-
tactic annotation projects.
UCCA?s representation is guided by conceptual
notions and has its roots in the Cognitive Linguis-
tics tradition and specifically in Cognitive Gram-
mar (Langacker, 2008). These theories represent
the meaning of an utterance according to the men-
tal representations it evokes and not according to
its reference in the world. Future work will ex-
plore options to further reduce manual annotation,
possibly by combining texts with visual inputs
during training.
We are currently attempting to construct a
parser for UCCA and to apply it to several seman-
tic tasks, notably English-French machine trans-
lation. Future work will also discuss UCCA?s
portability across domains. We intend to show
that UCCA, which is less sensitive to the idiosyn-
crasies of a specific domain, can be easily adapted
to highly dynamic domains such as social media.
Acknowledgements. We would like to thank
Tomer Eshet for partnering in the development of
the web application and to Amit Beka for his help
with UCCA?s software and development set.
236
References
Omri Abend and Ari Rappoport. 2013. UCCA: A
semantics-based grammatical annotation scheme. In
IWCS ?13, pages 1?12.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley Framenet project. In ACL-
COLING ?98, pages 86?90.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and
Nathan Schneider. 2012. Abstract mean-
ing representation (AMR) 1.0 specification.
http://www.isi.edu/natural-language/people/amr-
guidelines-10-31-12.pdf.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In LREC ?12, pages 3196?3200.
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2003. The Prague Dependency Treebank.
Treebanks, pages 103?127.
Alistair Butler and Kei Yoshimoto. 2012. Banking
meaning representations from treebanks. Linguistic
Issues in Language Technology, 7(1).
Alexander Clark and Shalom Lappin. 2010. Linguistic
Nativism and the Poverty of the Stimulus. Wiley-
Blackwell.
Robert M. W. Dixon. 2005. A Semantic Approach To
English Grammar. Oxford University Press.
Robert M. W. Dixon. 2010a. Basic Linguistic Theory:
Methodology, volume 1. Oxford University Press.
Robert M. W. Dixon. 2010b. Basic Linguistic Theory:
Grammatical Topics, volume 2. Oxford University
Press.
Robert M. W. Dixon. 2012. Basic Linguistic The-
ory: Further Grammatical Topics, volume 3. Ox-
ford University Press.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
COLING ?04, pages 350?356.
Bonnie Dorr, Rebecca Passonneau, David Farwell, Re-
becca Green, Nizar Habash, Stephen Helmreich, Ed-
ward Hovy, Lori Levin, Keith Miller, Teruko Mi-
tamura, Owen Rambow, and Advaith Siddharthan.
2010. Interlingual annotation of parallel text cor-
pora: A new framework for annotation and evalu-
ation. Natural Language Engineering, 16(3):197?
243.
Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, and
Dan Flickinger. 2012. Who did what to whom?:
A contrastive study of syntacto-semantic dependen-
cies. In LAW ?12, pages 2?11.
Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hyper-
edge replacement grammars. In COLING ?12,
pages 1359?1376.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. Handbook Of Formal Lan-
guages, 3:69?123.
Ronald M. Kaplan and Joan Bresnan. 1981. Lexical-
Functional Grammar: A Formal System For Gram-
matical Representation. Massachusetts Institute Of
Technology, Center For Cognitive Science.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization ? step one: Sentence compres-
sion. In AAAI ?00, pages 703?710.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In EMNLP ?10, pages 1223?1233.
R.W. Langacker. 2008. Cognitive Grammar: A Basic
Introduction. Oxford University Press, USA.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics, 19(2):313?330.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. Annotating noun argu-
ment structure for Nombank. In LREC ?04, pages
803?806.
Jason Naradowsky, Sebastian Riedel, and David Smith.
2012. Improving NLP through marginalization of
hidden syntactic structure. In EMNLP ?12, pages
810?820.
Stephan Oepen, Dan Flickinger, Kristina Toutanova,
and Christopher D Manning. 2004. Lingo red-
woods. Research on Language and Computation,
2(4):575?596.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):145?159.
Carl Pollard and Ivan A. Sag. 1994. Head-driven
Phrase Structure Grammar. University Of Chicago
Press.
Ivan A Sag. 2010. Sign-based construction gram-
mar: An informal synopsis. Sign-based Construc-
tion Grammar. CSLI Publications, Stanford, pages
39?170.
237
Federico Sangati and Chiara Mazza. 2009. An En-
glish dependency treebank a` la Tesnie`re. In TLT ?09,
pages 173?184.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency
parsing evaluation. In ACL-HLT ?11, pages 663?
672.
Advaith Siddharthan. 2006. Syntactic simplification
and text cohesion. Research on Language & Com-
putation, 4(1):77?109.
Mark Steedman. 2001. The Syntactic Process. MIT
Press.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In CoNLL ?08,
pages 159?177.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of semi-
supervised structured conditional models for depen-
dency parsing. In EMNLP ?09, pages 551?560.
Hiroshi Uchida and Meiying Zhu. 2001. The uni-
versal networking language beyond machine trans-
lation. In International Symposium on Language in
Cyberspace, pages 26?27.
David Vadas and James R Curran. 2011. Parsing noun
phrases in the Penn Treebank. Computational Lin-
guistics, 37(4):753?809.
Robert D. Van Valin Jr. 2005. Exploring The Syntax-
semantics Interface. Cambridge University Press.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? A quasi-
synchronous grammar for QA. In EMNLP-CoNLL
?07, pages 22?32.
238
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644?654,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Lexical Inference over Multi-Word Predicates: A Distributional Approach
Omri Abend Shay B. Cohen Mark Steedman
School of Informatics, University of Edinburgh,
Edinburgh EH8 9AB, United Kingdom
{oabend,scohen,steedman}@inf.ed.ac.uk
Abstract
Representing predicates in terms of their
argument distribution is common practice
in NLP. Multi-word predicates (MWPs) in
this context are often either disregarded or
considered as fixed expressions. The lat-
ter treatment is unsatisfactory in two ways:
(1) identifying MWPs is notoriously diffi-
cult, (2) MWPs show varying degrees of
compositionality and could benefit from
taking into account the identity of their
component parts. We propose a novel
approach that integrates the distributional
representation of multiple sub-sets of the
MWP?s words. We assume a latent distri-
bution over sub-sets of the MWP, and esti-
mate it relative to a downstream prediction
task. Focusing on the supervised identi-
fication of lexical inference relations, we
compare against state-of-the-art baselines
that consider a single sub-set of an MWP,
obtaining substantial improvements. To
our knowledge, this is the first work to
address lexical relations between MWPs
of varying degrees of compositionality
within distributional semantics.
1 Introduction
Multi-word expressions (MWEs) constitute a
large part of the lexicon and account for much
of its growth (Jackendoff, 2002; Seaton and
Macaulay, 2002). However, despite their impor-
tance, MWEs remain difficult to define and model,
and consequently pose serious difficulties for NLP
applications (Sag et al, 2001). Multi-word Predi-
cates (MWPs; sometimes termed Complex Predi-
cates) form an important and much addressed sub-
class of MWEs and are the focus of this paper.
MWPs are informally defined as multiple words
that constitute a single predicate (Alsina et al,
1997). MWPs encompass a wide range of phe-
nomena, including causatives, light verbs, phrasal
verbs, serial verb constructions and many others,
and pose considerable challenges to both linguistic
theory and NLP applications (see Section 2). Part
of the difficulty in treating them stems from their
position on the borderline between syntax and the
lexicon. It is therefore often unclear whether they
should be treated as fixed expressions, as compo-
sitional phrases that reflect the properties of their
component parts or as both.
This work addresses the modelling of MWPs
within the context of distributional semantics (Tur-
ney and Pantel, 2010), in which predicates are
represented through the distribution of arguments
they may take. In order to collect meaningful
statistics, the predicate?s lexical unit should be suf-
ficiently frequent and semantically unambiguous.
MWPs pose a challenge to such models, as
na??vely collecting statistics over all instances of
highly ambiguous verbs is likely to result in noisy
representations. For instance, the verb ?take? may
appear in MWPs as varied as ?take time?, ?take
effect? and ?take to the hills?. This heterogene-
ity of ?take? is likely to have a negative effect on
downstream systems that use its distributional rep-
resentation. For instance, while ?take? and ?ac-
cept? are often considered lexically similar, the
high frequency in which ?take? participates in
non-compositional MWPs is likely to push the two
verbs? distributional representations apart.
A straightforward approach to this problem is
to represent the predicate as a conjunction of mul-
tiple words, thereby trading ambiguity for spar-
sity. For instance, the verb ?take? could be con-
joined with its object (e.g., ?take care?, ?take a
bus?). This approach, however, raises the chal-
lenge of identifying the sub-set of the predicate?s
words that should be taken to represent it (hence-
forth, its lexical components or LCs).
We propose a novel approach that addresses this
644
challenge in the context of identifying lexical in-
ference relations between predicates (Lin and Pan-
tel, 2001; Schoenmackers et al, 2010; Melamud et
al., 2013a, inter alia). A (lexical) inference rela-
tion p
L
? p
R
is said to hold if the relation denoted
by p
R
generally holds between a set of arguments
whenever the relation p
L
does. For instance, an in-
ference relation holds between ?annex? and ?con-
trol? since if a country annexes another, it gener-
ally controls it. Most works to this task use dis-
tributional similarity, either as their main compo-
nent (Szpektor and Dagan, 2008; Melamud et al,
2013b), or as part of a more comprehensive system
(Berant et al, 2011; Lewis and Steedman, 2013).
For example, consider the verb ?take?. While
the inference relation ?have? take? does not gen-
erally hold, it does hold in the case of some light
verbs, such as ?have a look? take a look?, under-
scoring the importance of taking more inclusive
LCs into account. On the other hand, the pred-
icate ?likely to give a green light? is unlikely to
appear often even within a very large corpus, and
could benefit from taking its lexical sub-units (e.g.,
?likely? or ?give a green light?) into account.
We present a novel approach to the task that
models the selection and relative weighting of the
predicate?s LCs using latent variables. This ap-
proach allows the classifier that uses the distri-
butional representations to take into account the
most relevant LCs in order to make the predic-
tion. By doing so, we avoid the notoriously dif-
ficult problem of defining and identifying MWPs
and account for predicates of various sizes and de-
grees of compositionality. To our knowledge, this
is the first work to address lexical relations be-
tween MWPs of varying degrees of composition-
ality within distributional semantics.
We conduct experiments on the dataset of Ze-
ichner et al (2012) and compare our methods with
analogous ones that select a fixed LC, using state-
of-the-art feature sets. Our method obtains sub-
stantial performance gains across all scenarios.
Finally, we note that our approach is cognitively
appealing. Significant cognitive findings support
the claim that a speaker?s lexicon consists of par-
tially overlapping lexical units of various sizes, of
which several can be evoked in the interpretation
of an utterance (Jackendoff, 2002; Wray, 2008).
2 Background and Related Work
Inference Relations. The detection of inference
relations between predicates has become a central
task over the past few years (Sekine, 2005; Zan-
zotto et al, 2006; Schoenmackers et al, 2010;
Berant et al, 2011; Melamud et al, 2013a, in-
ter alia). Inference rules are used in a wide va-
riety of applications including Question Answer-
ing (Ravichandran and Hovy, 2002), Information
Extraction (Shinyama and Sekine, 2006), and as
a main component in Textual Entailment systems
(Dinu and Wang, 2009; Dagan et al, 2013).
Most approaches to the task used distributional
similarity as a major component within their sys-
tem. Lin and Pantel (2001) introduced DIRT, an
unsupervised distributional system for detecting
inference relations. The system is still considered
a state-of-the-art baseline (Melamud et al, 2013a),
and is often used as a component within larger sys-
tems. Schoenmackers et al (2010) presented an
unsupervised system for learning inference rules
directly from open-domain web data. Melamud
et al (2013a) used topic models to combine type-
level predicate inference rules with token-level in-
formation from their arguments in a specific con-
text. Melamud et al (2013b) used lexical expan-
sion to improve the representation of infrequent
predicates. Lewis and Steedman (2013) combined
distributional and symbolic representations, eval-
uating on a Question Answering task, as well as
on a quantification-focused entailment dataset.
Several studies tackled the task using super-
vised systems. Weisman et al (2012) used a set
of linguistically motivated features, but evaluated
their system on a corpus that consists almost en-
tirely of single-word predicates. Mirkin et al
(2006) presented a system for learning inference
rules between nouns, using distributional similar-
ity and pattern-based features. Hagiwara et al
(2009) identified synonyms using a supervised ap-
proach relying on distributional and syntactic fea-
tures. Berant et al (2011) used distributional simi-
larity between predicates to weight the edges of an
entailment graph. By imposing global constraints
on the structure of the graph, they obtained a more
accurate set of inference rules.
Previous work used simple methods to select
the predicate?s LC. Some filtered out frequent
highly ambiguous verbs (Lewis and Steedman,
2013), others selected a single representative word
(Melamud et al, 2013a), while yet others used
multi-word LCs but treated them as fixed expres-
sions (Lin and Pantel, 2001; Berant et al, 2011).
The goals of the above studies are largely com-
645
plementary to ours. While previous work focused
either on improving the quality of the distribu-
tional representations themselves or on their incor-
poration into more elaborate systems, we focus on
the integration of the distributional representation
of multiple LCs to improve the identification of
inference relations between MWPs.
MWP Extraction and Identification. MWPs
have received considerable attention over the years
in both theoretical and applicative contexts. Their
position on the crossroads of syntax and the lexi-
con, their varying degrees of compositionality, as
well as the wealth of linguistic phenomena they
exhibit, made them the object of ongoing linguis-
tic discussion (Alsina et al, 1997; Butt, 2010).
In NLP, the discovery and identification of
MWEs in general and MWPs in particular has
been the focus of much work over the years
(Lin, 1999; Baldwin et al, 2003; Biemann and
Giesbrecht, 2011). Despite wide interest, the
field has yet to converge to a general and widely
agreed-upon method for identifying MWPs. See
(Ramisch et al, 2013) for an overview.
Most work on MWEs emphasized idiosyncratic
or non-compositional expressions. Other lines of
work focused on specific MWP classes such as
light verbs (Tu and Roth, 2011; Vincze et al,
2013) and phrasal verbs (McCarthy et al, 2003;
Pichotta and DeNero, 2013). Our work proposes a
uniform treatment to MWPs of varying degrees of
compositionality, and avoids defining MWPs ex-
plicitly by modelling their LCs as latent variables.
Compositional Distributional Semantics.
Much work in recent years has concentrated on
the relation between the distributional representa-
tions of composite phrases and the representations
of their component sub-parts (Widdows, 2008;
Mitchell and Lapata, 2010; Baroni and Zampar-
elli, 2010; Coecke et al, 2010). Several works
have used compositional distributional semantics
(CDS) representations to assess the composition-
ality of MWEs, such as noun compounds (Reddy
et al, 2011) or verb-noun combinations (Kiela
and Clark, 2013). Despite significant advances,
previous work has mostly been concerned with
highly compositional cases and does not address
the distributional representation of predicates of
varying degrees of compositionality.
3 Our Proposal: A Latent LC Approach
This section details our approach for distribu-
tionally representing MWPs by leveraging their
component LCs. Section 3.1 describes our gen-
eral approach, Section 3.2 presents our model and
Section 3.3 details the feature set.
3.1 General Approach and Notation
We propose a method for addressing MWPs of
varying degrees of compositionality through the
integration of the distributional representation of
multiple sub-sets of the predicate?s words (LCs).
We use it to tackle a supervised prediction task that
represents predicates distributionally. Our model
assumes a latent distribution over the LCs, and es-
timates its parameters so to best conform to the
goals of the target prediction task.
Formally, given a predicate p, we denote the set
of words comprising it as W (p). The set of al-
lowable LCs for p is denoted with H
p
? 2
W (p)
.
H
p
contains all sub-sets of p that we consider as
apriori possible to represent p. For instance, if p is
?likely to give a green light?, H
p
may include LCs
such as ?likely? or ?give light?. As our method is
aimed at discovering the most relevant LCs, we do
not attempt to analyze the MWPs in advance, but
rather take an inclusive H
p
, allowing the model to
estimate the relative weights of the LCs.
The task we use as a testbed for our approach
is the lexical inference identification task between
predicates. Given a pair of predicates p =
(p
L
, p
R
), the task is to predict whether an infer-
ence relation holds between them. For instance, if
p
L
is ?devour? and p
R
is ?eat greedily?, the clas-
sifier should use the similarity between ?devour?
and ?eat? in order to correctly predict an infer-
ence relation in this case. Selecting the wider LC
?eat greedily? might result in sparser statistics. In
other examples, however, taking a wider LC is po-
tentially beneficial. For instance, the dissimilar-
ity between ?take? and ?make? should not prevent
the classifier from identifying the inference rela-
tion between ?take a step? and ?make a step?.
Our statistical model aims at predicting the cor-
rect label by making use of partially overlapping
LCs of various sizes, both for the premise left-
hand side (LHS) predicate p
L
and the hypothesis
right-hand side (RHS) predicate p
R
. More for-
mally, we take the space of values for our latent
LC variables to be H
p
L
,p
R
= H
p
L
?H
p
R
.
Our evaluation dataset consists of pairs p
(i)
=
(p
(i)
L
, p
(i)
R
) for i ? {1, . . . ,M}, where M is the
number of examples available, coupled with their
gold-standard labels y
(i)
? {1,?1}. For brevity,
we denote H
(i)
= H
p
(i)
= H
p
(i)
L
,p
(i)
R
. We also as-
646
sume the existence of a feature function ?(p, y, h)
which maps a triplet of a predicate pair p, an infer-
ence label y, and a latent state h ? H
p
to R
d
for
some integer d. We denote the training set by D.
3.2 The Model
We address the task with a latent variable log-
linear model, representing the LCs of the predi-
cates. We choose this model for its generality, con-
ceptual simplicity, and because it allows to easily
incorporate various feature sets and sets of latent
variables. We introduce L
2
regularization to avoid
over-fitting. We use maximum likelihood estima-
tion, and arrive at the following objective function:
L(w|D) =
1
M
M
X
i=1
logP (y
(i)
|p
(i)
, w)?
?
2
?w?
2
=
=
1
n
n
X
i=1
0
@
log
X
h?H
(i)
exp
?
w
>
?(p
(i)
, y
(i)
, h)
?
? logZ(w, i)
?
?
?
2
?w?
2
where:
Z(w, i) =
X
y?{?1,1}
X
h?H
i
exp(w
>
?(p
i
, y, h)).
We maximizeL using the BFGS algorithm (No-
cedal and Wright, 1999). The gradient (with re-
spect to w) is the following:
?L = E
h
[?(p
i
, y
i
, h)]? E
h,y
[?(p
i
, y, h)]? ? ? w
H
p
can be defined to be any sub-set of 2
W (p)
given that taking an expectation over H can be
done efficiently. It is therefore possible to use prior
linguistic knowledge to consider only sub-sets of p
that are likely to be non-compositional (e.g., verb-
preposition or verb-noun pairs).
In our experiments we attempt to keep the ap-
proach maximally general, and defineH
p
to be the
set of all subsets of size 1 or 2 of content words in
W
p
1
. We bound the size of h ? H
p
in order to re-
tain computational efficiency and a sufficient fre-
quency of the LCs in H
p
. MWPs of length greater
than 2 are effectively approximated by their set of
subsets of sizes 1 and 2.
Each h can therefore be written as a 4-tuple
(h
A
L
, h
B
L
, h
A
R
, h
B
R
), where h
A
L
(h
A
R
) denotes the first
word of the LHS (RHS) predicate?s LC. h
B
L
(h
B
R
)
denotes the (possibly empty) second word of the
predicate. Inference is carried out by maximizing
P (y|p
(i)
) over y. As |H
p
| = O(k
4
), where k is the
1
We use a POS tagger to identify content words. Preposi-
tions are considered content words under this definition.
number of content words in p, and as the number
of content words is usually small
2
, inference can
be carried out by directly summing over H
(i)
.
Initialization. The introduction of latent vari-
ables into the log-linear model leads to a non-
convex objective function. Consequently, BFGS
is not guaranteed to converge to the global opti-
mum, but rather to a stationary point. The result
may therefore depend on the parameter initializa-
tion. Indeed, preliminary experiments showed that
both initializing w to be zero and using a random
initializer results in lower performance.
Instead, we initialize our model with a simpli-
fied convex model that fixes the LCs to be the
pair of left-most content words comprising each
of the predicates. This is a common method for
selecting the predicate?s LC (e.g., Melamud et al,
2013a). Once h has been fixed, the model col-
lapses to a convex log-linear model. The optimal
w is then taken as an initialization point for the la-
tent variable model. While this method may still
not converge to the global maximum, our experi-
ments show that this initialization technique yields
high quality values for w (see Section 6).
3.3 Feature Set
This section lists the features used for our exper-
iments. We intentionally select a feature set that
relies on either completely unsupervised or shal-
low processing tools that are available for a wide
variety of languages and domains.
Given a predicate pair p
(i)
, a label y ? {1,?1}
and a latent state h ? H
(i)
, we define their feature
vector as ?(p
(i)
, y, h) = y ? ?(p
(i)
, h). The com-
putation of ?(p
(i)
, h) requires a reference corpus
R that contains triplets of the type (p, x, y) where
p is a binary predicate and x and y are its argu-
ments. We use the Reverb corpus as R in our ex-
periments (Fader et al, 2011; see Section 4). We
refrain from encoding features that directly reflect
the vocabulary of the training set. Such features
are not applicable beyond that set?s vocabulary,
and as available datasets contain no more than a
few thousand examples, these features are unlikely
to generalize well.
Table 1 presents the set of features we use in our
experiments. The features can be divided into two
main categories: similarity features between the
LHS and the RHS predicates (table?s top), and fea-
tures that reflect the individual properties of each
2
|H
p
| is about 15 on average in our dataset, where less
than 5% of the H
(i)
are of size greater than 50.
647
C
a
t
e
g
o
r
y
Name Description
S
i
m
i
l
a
r
i
t
y
COSINE DIRT cosine similarity between the vectors of h
L
and h
R
COSINE
A
DIRT cosine similarity between the vectors of h
A
L
and h
A
R
BInc DIRT BInc similarity between the vectors of h
L
and h
R
BInc
A
DIRT BInc similarity between the vectors of h
A
L
and h
A
R
W
o
r
d
A
L
H
S
POS
A
L
The most frequent POS tag for the lemma of h
A
L
POS2
A
L
The second most frequent POS tag for the word lemma of h
A
L
FREQ
A
L
The number of occurrences of h
A
L
in the reference corpus
COMMON
A
L
A binary feature indicating whether h
A
L
appears in both predicates
ORDINAL
A
L
The ordinal number of h
A
L
among the content words of the LHS predicate
P
a
i
r
L
H
S
POS
AB
L
The conjunction of POS
A
L
and POS
B
L
FREQ
AB
L
The frequency of h
A
L
and h
B
L
in the reference corpus
PREFAB
L
P (h
A
L
|h
A
L
) as estimated from the reference corpus
PREFBA
L
P (h
B
L
|h
A
L
) as estimated from the reference corpus
PMIAB
L
The point-wise mutual information of h
A
L
and h
B
L
L
D
A
TOPICS
L
P (topic|h
L
) for each of the induced topics.
TOPICENT
L
The entropy of the topic distribution P (topic|h
L
)
Table 1: The feature set used in our experiments. The top part presents the similarity measures based on the DIRT approach.
The rest of the listed features apply to the LHS predicate (h
L
), and to the first word in it (h
A
L
). Analogous features are
introduced for the second word, h
B
L
, and for the RHS predicate. The upper-middle part presents the word features for h
A
L
. The
lower-middle part presents features that apply where h
L
is of size 2. The bottom part lists the LDA-based features.
of them. Within the LHS feature set, we distin-
guish between two sub-types of features: word
features that encode the individual properties of
h
A
L
and h
B
L
(table?s upper middle part), and pair
features that only apply to LCs of size 2 and re-
flect the relation between h
A
L
and h
B
L
(table?s lower
middle part). We further incorporate LDA-based
features that reflect the selectional preferences of
the predicates (table?s bottom).
Distributional Similarity Features. The distri-
butional similarity features are based on the DIRT
system (Lin and Pantel, 2001). The score defines
for each predicate p and for each argument slot
s ? {L,R} (corresponding to the arguments to the
right and left of that predicate) a vector v
p
s
which
represents the distribution of arguments appearing
in that slot. We take v
p
s
(x) to be the number of
times that the argument x appeared in the slot s of
the predicate p. Given these vectors, the similarity
between the predicates p
1
and p
2
is defined as:
score(p
1
, p
2
) =
q
sim(v
p
1
L
, v
p
2
L
) ? sim(v
p
1
R
, v
p
2
R
)
where sim is some vector similarity measure.
We use two common similarity measures: the
vector cosine metric, and the BInc (Szpektor and
Dagan, 2008) similarity measure. These measures
give complementary perspectives on the similar-
ity between the predicates, as the cosine similar-
ity is symmetric between the LHS and RHS predi-
cates, while BInc takes into account the direction-
ality of the inference relation. Preliminary exper-
iments with other measures, such as those of Lin
(1998) and Weeds and Weir (2003) did not yield
additional improvements.
We encode the similarity of all measures for the
pair h
L
and h
R
as well as the pair h
A
L
and h
A
R
. The
latter feature is an approximation to the similar-
ity between the heads of the predicates, as heads
in English tend to be to the left of the predicates.
These two features coincide for h values of size 1.
Word and Pair Features. These features en-
code the basic properties of the LC. The motiva-
tion behind them is to allow a more accurate lever-
aging of the similarity features, as well as to better
determine the relative weights of h ? H
(i)
.
The feature set is composed of four analogous
sets corresponding to h
A
L
,h
B
L
,h
A
R
and h
B
R
, as well
as two sets of features that capture relations be-
tween h
A
L
, h
B
L
and h
A
R
, h
B
R
(in cases h is of size 2).
The features include the ordinal index of the word
within the predicate, the lemma?s frequency ac-
cording to R, and a feature that indicates whether
that word?s lemma also appears in both predicates
of the pair. For instance, when considering the
predicates ?likely to come? and ?likely to leave?,
?likely? appears in both predicates, while ?come?
and ?leave? appear only in one of them.
In addition, we use POS-based features that
encode the most frequent POS tag for the word
lemma and the second most frequent POS tag (ac-
cording toR). Information about the second most
frequent POS tag can be important in identifying
light verb constructions, such as ?take a swim? or
?give a smile?, where the object is derived from a
verb. It can thus be interpreted as a generalization
648
of the feature that indicates whether the object is
a deverbal noun, which is used by some light verb
identification algorithms (Tu and Roth, 2011).
In cases where h
L
is of size 2, we additionally
encode features that apply to the conjunction of
h
A
L
and h
B
L
. We encode the conjunction of their
POS and the number of times the two lemmas oc-
curred together in R. We also introduce features
that capture the statistical correlation between the
words of h
L
. To do so, we use point-wise mu-
tual information, and the conditional probabili-
ties P (h
A
L
|h
B
L
) and P (h
B
L
|h
A
L
). Similar measures
have often been used for the unsupervised detec-
tion of MWEs (Villavicencio et al, 2007; Fazly
and Stevenson, 2006). We also include the analo-
gous set of features for h
R
.
LDA-based Features. We further incorporate
features based on a Latent Dirichlet Allocation
(LDA) topic model (Blei et al, 2003). Several
recent works have underscored the usefulness of
using topic models to model a predicate?s selec-
tional preferences (Ritter et al, 2010; Dinu and
Lapata, 2010; S?eaghdha, 2010; Lewis and Steed-
man, 2013; Melamud et al, 2013a). We adopt the
approach of Lewis and Steedman (2013), and de-
fine a pseudo-document for each LC in the evalu-
ation corpus. We populate the pseudo-documents
of an LC with its arguments according to R. We
then train an LDA model with 25 topics over these
documents. This yields a probability distribution
P (topic|h) for each LC h, reflecting the types of
arguments h may take.
We further include a feature for the entropy of
the topic distribution of the predicate, which re-
flects its heterogeneity. This feature is motivated
by the assumption that a heterogeneous predicate
is more likely to benefit from selecting a more in-
clusive LC than a homogeneous one.
Technical Issues. All features used, except the
similarity ones and the topic distribution features
are binary. Frequency features are binned into 4
bins of equal frequency. We conjoin some of the
feature sets by multiplying their values. Specifi-
cally, we add the cross product of the features of
the category ?Similarity? (see Table 1) with the
rest of the features. In addition, we conjoin all
LHS (RHS) features with an indicator feature that
indicates whether h
L
(h
R
) is of size two. This re-
sults in 1605 non-constant features.
We further note that some LCs that appear in the
evaluation corpus do not appear at all inR. In our
experiments they amounted to 0.2% of the LCs in
our evaluation dataset. While previous work of-
ten discarded predicates below a certain frequency
from the evaluation, we include them in order to
facilitate comparison to future work. We assign
the similarity features of such examples a 0 value,
and assign their other numerical features the mean
value of those features.
4 Experimental Setup
Corpora and Preprocessing. As a reference
corpus R, we use Reverb (Fader et al, 2011), a
web-based corpus consisting of 15M web extrac-
tions of binary relations. Each relation is a triplet
of a predicate and two arguments, one preceding it
and one following it. Relations were extracted us-
ing regular expressions over the output of a POS
tagger and an NP chunker. Each predicate may
consist of a single verb, a verb and a preposi-
tion or a sequence of words starting in a verb and
ending in a preposition, between which there may
nouns, adjectives, adverbs, pronouns, determiners
and verbs. The verb may also be a copula. Exam-
ples of predicates are ?make the most of?, ?could
be exchanged for? and ?is happy with?.
Reverb is an appealing reference corpus for this
task for several reasons. First, it uses fairly shal-
low preprocessing technology which is available
for many domains and languages. Second, Reverb
applies considerable noise filtering, which results
in extractions of fair quality. Third, our evaluation
dataset is based on Reverb extractions.
We evaluate our algorithm on the dataset of
Zeichner et al (2012). This publicly available
corpus
3
provides pairs of Reverb binary relations
and an indication of whether an inference rela-
tion holds between them within the context of
a specific pair of argument fillers. The corpus
was compiled using distributional methods to de-
tect pairs of relations in Reverb that are likely
to have an inference relation between. Annota-
tors, employed through Amazon Mechanical Turk,
were then asked to determine whether each pair
is meaningful, and if so, to determine whether an
inference relation holds. Further measures were
taken to monitor the accuracy of the annotation.
For example, the pair of predicates ?make the
most of? and ?take advantage of? appears in the
corpus as a pair between which an inference rela-
tion holds. The arguments in this case are ?stu-
dents? and ?their university experience?. An ex-
3
http://tinyurl.com/krx2acd
649
ample of a pair between which an inference rela-
tion does not hold is ?tend to neglect? and ?under-
estimate the importance of?, where the arguments
are ?Robert? and ?his family?.
The dataset contains 6,565 instances in total.
We use 5,411 pairs of them, discarding instances
that were deemed as meaningless by the annota-
tors. We also discard cases where the set of ar-
guments is reversed between the LHS and RHS
predicates. In these examples, p
R
(x, y) is infer-
able from p
L
(y, x), rather than from p
L
(x, y). As
there are less than 150 reversed instances in the
corpus, experimenting on this sub-set is unlikely
to be informative.
The average length of a predicate in the cor-
pus is 2.7 words (including function words). In
87.3% of the predicate pairs, there was more than
one LC (i.e., |H
p
| > 1), underscoring the im-
portance of correctly leveraging the different LCs.
We randomly partition the corpus into a training
set which contains 4,343 instances (?80%), and a
test set that contains 1,068 instances, maintaining
the same positive to negative label ratio in both
datasets
4
. Development was carried out using
cross-validation on the training data (see below).
We use a Maximum Entropy POS Tagger,
trained on the Penn Treebank, and the WordNet
lemmatizer, both implemented within the NLTK
package (Loper and Bird, 2002). To obtain a
coarse-grained set of POS tags, we collapse the
tag set to 7 categories: nouns, verbs, adjectives,
adverbs, prepositions, the word ?to? and a cate-
gory that includes all other words. A Reverb argu-
ment is represented as the conjunction of its con-
tent words that appear more than 10 times in the
corpus. Function words are defined according to
their POS tags and include determiners, possessive
pronouns, existential ?there?, numbers and coordi-
nating conjunctions. Auxiliary verbs and copulas
are also considered function words.
To compute the LDA features, we use the on-
line variational Bayes algorithm of (Hoffman et
al., 2010) as implemented in the Gensim software
package (Rehurek and Sojka, 2010).
Evaluated Algorithms. The only two previous
works on this dataset (Melamud et al, 2013a;
Melamud et al, 2013b) are not directly compara-
ble, as they used unsupervised systems and evalu-
4
A script that replicates our train-test partition of the cor-
pus can be found here: http://homepages.inf.ed.
ac.uk/oabend/mwpreds.html
ated on sub-sets of the evaluation dataset. Instead,
we use several baselines to demonstrate the use-
fulness of integrating multiple LCs, as well as the
relative usefulness of our feature sets.
The simplest baseline is ALLNEG, which pre-
dicts the most frequent label in the dataset (in our
case: ?no inference?). The other evaluated sys-
tems are formed by taking various subsets of our
feature set. We experiment with 4 feature sets. The
smallest set, SIM, includes only the similarity fea-
tures. This feature set is related to the composi-
tional distributional model of Mitchell and Lap-
ata (2010) (see Section 6). We note that despite
recent advances in identifying predicate inference
relations, the DIRT system (Lin and Pantel, 2001)
remains a strong baseline, and is often used as a
component in state-of-the-art systems (Berant et
al., 2011), and specifically in the two aforemen-
tioned works that used the same evaluation corpus.
The next feature set BASIC includes the features
found to be most useful during the development
of the model: the most frequent POS tag, the fre-
quency features and the feature Common. More
inclusive is the feature set NO-LDA, which in-
cludes all features except the LDA features. Ex-
periments with this set were performed in order
to isolate the effect of the LDA features. Finally,
ALL includes our complete set of features.
The more direct comparison is against partial
implementations of our system where the LC h is
deterministically selected. Determining h for each
predicate yields a regular log-linear binary classi-
fication model. We use two variants of this base-
line. The first, LEFTMOST, selects the left-most
content word for each predicate. Similar selec-
tion strategy was carried out by Melamud et al
(2013a). The second, VPREP, selects h to be the
verb along with its following preposition. In cases
the predicate contains multiple verbs, the one pre-
ceding the preposition is selected, and where the
predicate does not contain any non-copula verbs,
it regresses to LEFTMOST. This LC selection
method approximates a baseline that includes sub-
categorized prepositions. Such cases are highly
frequent and account for a large portion of the
MWPs in English. Including a verb?s preposition
in its LC was commonly done in previous work
(e.g., Lewis and Steedman, 2013).
We also attempted to identify verb-preposition
constructions using a dependency parser. Unfor-
tunately, our evaluation dataset is only available in
650
a lemmatized version, which posed a difficulty for
the parser. Due to the low quality of the resulting
parses, we implemented VPREP using POS-based
regular expressions as defined above.
The full model is denoted with LATENTLC. For
each system and feature set, we report results us-
ing 10-fold cross-validation on the training set, as
well as results on the test set. Both cases use
the same set of parameters determined by cross-
validation on the training set. As the task at hand
is a binary classification problem, we use accuracy
scores to rate the performance of our systems.
5 Results
Table 2 presents the results of our experi-
ments. Rows correspond to the evaluated algo-
rithms, while columns correspond to the feature
sets used and the evaluation scenarios (i.e., train-
ing set cross-validation or test set evaluation). Our
experiments make first use of this dataset in its
fullest form for the problem of supervised learning
of inference relations, and may serve as a starting
point for further exploration of this dataset.
For all feature sets and settings, LATENTLC
scored highest, often with a considerable margin
of up to 3.0% in the cross-validation and up to
4.6% on the test set relative to the LEFTMOST
baseline, and 5.1% (cross-validation) and 6.8%
(test) margins relative to VPREP.
The best scoring result of our LATENTLC
model in the cross-validation scenario is 65.72%,
obtained by the feature set All. The best scoring
result by any of the baseline models in this sce-
nario is 62.7%, obtained by the same feature set.
For the test set scenario, LATENTLC obtained its
highest accuracy, 65.73%, when using the feature
set Basic. This is a substantial improvement over
the highest scoring baseline model in this scenario
that obtained 61.6% accuracy, using the feature set
All. This performance gap is substantial when tak-
ing into consideration that the improvements ob-
tained by the highly competitive DIRT similarity
features using the stronger LEFTMOST baseline,
result in an improvement of 3.1% and 5.3% over
the trivial ALLNEG baseline in the test set and
cross-validation scenarios respectively.
Comparing the different feature sets on our pro-
posed model, we find that the Basic feature set
gives a consistent and substantial increase over the
Sim feature set. Improvements are of 2.8% (test)
and 2.2% (cross-validation). Introducing more
elaborate features (i.e., the feature sets NoLDA
and All) yields some improvements in the cross-
validation, but these improvements are not repli-
cated on the test set. This may be due to idiosyn-
crasies in the test set that are averaged out in the
cross-validation scenario.
For a qualitative analysis, we took the best per-
forming model of the data set (i.e., with the Basic
feature set), and extracted the set of instances
where it made a correct prediction while both
baselines made an error. This set contains many
verb-preposition pairs, such as ?list as ? report
as? or ?submit via? deliver by?, underscoring the
utility of leveraging multiple LCs rather than con-
sidering only a head word (as with LEFTMOST)
or the entire phrase (as with VPREP). Other ex-
amples in this set contain more complex patterns.
These include the positive pairs ?talk much about
? have much to say about? and ?increase with
? go up with?, and the negative ?make predic-
tion about ? meet the challenge of? and ?enjoy
watching? love to play?.
6 Discussion
Relation to CDS. Much recent work subsumed
under the title Compositional Distributional Se-
mantics addressed the distributional representa-
tion of multi-word phrases (see Section 2). This
line of work focuses on compositional predicates,
such as ?kick the ball? and not on idiosyncratic
predicates such as ?kick the bucket?.
A variant of the CDS approach can be framed
within ours. Assume we wish to compute the
similarity of the predicates p
L
= (w
1
, ..., w
n
)
and p
R
= (w
?
1
, ..., w
?
m
). Let us denote the vec-
tor space representations of the individual words
as v
1
, ..., v
n
and v
?
1
, ..., v
?
m
respectively. A stan-
dard approach in CDS is to compose distributional
representations by taking their vector sum v
L
=
v
1
+ v
2
...+ v
n
and v
R
= v
?
1
+ ...+ v
?
m
(Mitchell
and Lapata, 2010). One of the most effective sim-
ilarity measures is the cosine similarity, which is a
normalized dot product. The distributional sim-
ilarity between p
L
and p
R
under this model is
sim(p
L
, p
R
) =
?
n
i=1
?
m
j=1
sim(w
i
, w
?
j
), where
sim(w
i
, w
?
j
) is the dot product between v
i
and v
?
j
.
This similarity score is similar in spirit to a
simplified version of our statistical model that
restricts the set of allowable LCs H
p
to be
{({w
i
}, {w
?
j
})|i ? n, j ? m}, i.e., only LCs of
size 1. Indeed, taking H
p
as above, and cosine
similarity as the only feature (i.e., w ? R), yields
the distribution
651
Test Set Cross Validation
Algorithm Sim Basic NoLDA All Sim Basic NoLDA All
LATENTLC 62.9 65.7 64.4 64.6 62.7 ? 1.9 64.9 ? 1.9 65.0 ? 1.7 65.7 ?1.9
LEFTMOST 59.0 61.1 60.0 60.4 61.2 ? 2.1 62.5 ? 2.4 62.4 ?2.2 62.7 ? 2.0
VPREP 56.1 60.9 60.7 61.6
?
58.1 ? 1.7 60.8 ? 2.2 60.4 ? 2.6 60.6 ? 2.2
ALLNEG 55.9 55.9
Table 2: Results for the various evaluated systems. Accuracy results are presented in percents, followed in the cross vali-
dation scenario by the standard deviation over the folds. The rows correspond to the various systems as defined in Section 4.
LATENTLC is our proposed model. The columns correspond to the various feature sets, from the least to the most inclusive.
SIM includes only similarity features. BASIC additionally includes POS-based and frequency features. NOLDA includes all
features except LDA-based features. ALL is the full feature set. ALLNEG is the classifier that invariably predicts the label ?no
inference?. Bold marks best overall accuracy per column, and
?
marks figures that are not significantly worse (McNemar?s test,
p < 0.05). The same positive to negative label ratio was maintained in both the cross validation and test set scenarios. In all
cases, LATENTLC obtains substantial improvements over the baseline systems.
P (y|p) ?
X
(w
i
,w
?
j
)?H
p
exp
`
w ? y ? sim(w
i
, w
?
j
)
?
.
This derivation highlights the relation of a sim-
plified version of our approach to the additive
CDS model, as both approaches effectively aver-
age over the similarities of all pairs of words in p
L
and p
R
. The derivation also highlights a few ad-
vantages of our approach. First, our approach al-
lows to straightforwardly introduce additional fea-
tures and to weight them in a way most consistent
with the task at hand. Second, it allows much more
flexibility in defining the set of allowable LCs,H
p
.
Specifically, H
p
may contain LCs of sizes greater
than 1. Third, our approach uses standard proba-
bilistic modelling, and therefore has a natural sta-
tistical interpretation.
In order to appreciate the effect of these advan-
tages, we perform an experiment that takes H to
be the set of all LCs of size 1, and uses a sin-
gle similarity measure. We run a 10-fold cross-
validation on our training data, obtaining 61.3%
accuracy using COSINE and 62.2% accuracy us-
ing BInc. The performance gap between these re-
sults and the accuracy obtained by our full model
(65.7%) underscores the latter?s effectiveness in
integrating multiple features and LCs.
Effectiveness of Optimization Method. Our
maximization of the log-likelihood function is
not guaranteed to converge to a global optimum.
Therefore, the quality of the learned parameters
may be sensitive to the initialization point. We
hereby describe an experiment that tests the sen-
sitivity of our approach to such variance.
Selecting the highest scoring feature set on our
test set (i.e., BASIC), we ran the model with mul-
tiple initializers, by randomly perturbing our stan-
dard convex initializer (see Section 3). Concretely,
given a convex initializer w, we select the starting
point to be w + ?, where ?
i
? N (0, ?|w
i
|). We
ran this experiment 400 times with ? = 0.8.
To combine the resulting weight vectors into a
single classifier, we apply two types of standard
approaches: a Product of Experts (Hinton, 2002),
as well as a voting approach that selects the most
frequently predicted label. Neither of these exper-
iments yielded any significant performance gain.
This demonstrates the robustness of our optimiza-
tion method to the initialization point.
7 Conclusion
We have presented a novel approach to the
distributional representation of multi-word pred-
icates. Since MWPs demonstrate varying levels
of compositionality, a uniform treatment of MWPs
either as fixed expressions or through head words
is lacking. Instead, our approach integrates mul-
tiple lexical units contained in the predicate. The
approach takes into account both multi-word LCs
that address low compositionality cases, as well as
single-word LCs that address compositional cases
and are more frequent. It assumes a latent distribu-
tion over the LCs of the predicates, and estimates
it relative to a target application task.
We addressed the supervised inference identi-
fication task, obtaining substantial improvement
over state-of-the-art baseline systems. In future
work we intend to assess the benefit of this ap-
proach in MWP classes that are well-known from
the literature. We believe that a permissive ap-
proach that integrates multiple analyses would
perform better than standard single-analysis meth-
ods in a wide range of applications.
Acknowledgements. We would like to thank
Mike Lewis, Reshef Meir, Oren Melamud,
Michael Roth and Nathan Schneider for their help-
ful comments. This work was supported by ERC
Advanced Fellowship 249520 GRAMPLUS.
652
References
Alex Alsina, Joan Wanda Bresnan, and Peter Sells.
1997. Complex predicates. Center for the Study of
Language and Information.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL 2003 workshop on Multiword
expressions: analysis, acquisition and treatment-
Volume 18, pages 89?96.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
EMNLP, pages 1183?1193.
Jonathan Berant, Jacob Goldberger, and Ido Dagan.
2011. Global learning of typed entailment rules. In
ACL, pages 610?619.
Chris Biemann and Eugenie Giesbrecht. 2011. Dis-
tributional semantics and compositionality 2011:
Shared task description and results. In Workshop
on Distributional Semantics and Compositionality,
pages 21?28.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet alocation. the Journal of
machine Learning research, 3:993?1022.
Miriam Butt. 2010. The light verb jungle: still hack-
ing away. In Complex predicates: cross-linguistic
perspectives on event structure, pages 48?78. Cam-
bridge University Press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. In J. van
Bentham, M. Moortgat, and W. Buszkowski, editors,
Linguistic Analysis, volume 36, pages 435?384.
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzotto. 2013. Recognizing textual entail-
ment: Models and applications. Synthesis Lectures
on Human Language Technologies, 6(4):1?220.
Georgiana Dinu and Mirella Lapata. 2010. Topic mod-
els for meaning similarity in context. In COLING:
Posters, pages 250?258.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entail-
ment. In EACL, pages 211?219.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In EMNLP, pages 1535?1545.
Afsaneh Fazly and Suzanne Stevenson. 2006. Auto-
matically constructing a lexicon of verb phrase id-
iomatic combinations. In EACL, pages 337?344.
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko
Toyama. 2009. Supervised synonym acquisition us-
ing distributional features and syntactic patterns. In-
formation and Media Technologies, 4(2):558?582.
Geoffrey E Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural com-
putation, 14(8):1771?1800.
Matthew Hoffman, Francis R Bach, and David M Blei.
2010. Online learning for latent Dirichlet alocation.
In NIPS, pages 856?864.
Ray Jackendoff. 2002. Foundations of language:
Brain, meaning, grammar, evolution. Oxford Uni-
versity Press.
Douwe Kiela and Stephen Clark. 2013. Detect-
ing compositionality of multi-word expressions us-
ing nearest neighbours in vector space models. In
EMNLP, pages 1427?1432.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. TACL, 1:179?
192.
Dekang Lin and Patrick Pantel. 2001. DIRT ? discov-
ery of inference rules from text. In SIGKDD 2001,
pages 323?328.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In COLING-ACL, pages 768?774.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In ACL, pages 317?324.
Edward Loper and Steven Bird. 2002. NLTK: The
natural language toolkit. In ACL Workshop on Ef-
fective tools and methodologies for teaching natural
language processing and computational linguistics,
pages 63?70.
Diana McCarthy, Bill Keller, and John Carroll.
2003. Detecting a continuum of compositionality
in phrasal verbs. In ACL workshop on Multiword
expressions: analysis, acquisition and treatment,
pages 73?80.
Oren Melamud, Jonathan Berant, Ido Dagan, Jacob
Goldberger, and Idan Szpektor. 2013a. A two level
model for context sensitive inference rules. In ACL
2013, pages 1331?1340.
Oren Melamud, Ido Dagan, Jacob Goldberger, and Idan
Szpektor. 2013b. Using lexical expansion to learn
inference rules from sparse data. In ACL: Short Pa-
pers, pages 283?288.
Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006.
Integrating pattern-based and distributional similar-
ity methods for lexical entailment acquisition. In
COLING-ACL: Poster Session, pages 579?586.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Jorge. Nocedal and Stephen J Wright. 1999. Numeri-
cal optimization, volume 2. Springer New York.
653
Karl Pichotta and John DeNero. 2013. Identify-
ing phrasal verbs using many bilingual corpora. In
EMNLP, pages 636?646.
Carlos Ramisch, Aline Villavicencio, and Valia Kor-
doni. 2013. Introduction to the special issue on
multiword expressions: From theory to practice and
use. ACM Transactions on Speech and Language
Processing (TSLP), 10(2):3.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In ACL, pages 41?47.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in
compound nouns. In IJCNLP, pages 210?218.
Radim Rehurek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of LREC 2010 workshop New Challenges
for NLP Frameworks, pages 46?50.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent Dirichlet alocation method for selectional pref-
erences. In ACL, pages 424?434.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2001. Multiword
expressions: A pain in the neck for NLP. In CI-
CLing, pages 1?15.
Stefan Schoenmackers, Oren Etzioni, Daniel S Weld,
and Jesse Davis. 2010. Learning first-order Horn
clauses from web text. In EMNLP, pages 1088?
1098.
Diarmuid
?
O. S?eaghdha. 2010. Latent variable models
of selectional preference. In ACL 2010, pages 435?
444.
Maggie Seaton and Alison Macaulay, editors. 2002.
Collins COBUILD Idioms Dictionary. Harper-
Collins Publishers, 2nd edition.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between NE pairs.
In IWP, pages 4?6.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In HLT-NAACL, pages 304?311.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In COLING, pages
849?856.
Yuancheng Tu and Dan Roth. 2011. Learning English
light verb constructions: contextual or statistical. In
ACL HLT 2011, page 31.
Peter D Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141?188.
Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
evaluation of automatically acquired multiword ex-
pressions for grammar engineering. In EMNLP-
CoNLL, pages 1034?1043.
Veronika Vincze, Istv?an Nagy T., and Rich?ard Farkas.
2013. Identifying English and Hungarian light verb
constructions: A contrastive approach. In ACL:
Short Papers, pages 255?261.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In EMNLP, pages
81?88.
Hila Weisman, Jonathan Berant, Idan Szpektor, and
Ido Dagan. 2012. Learning verb inference rules
from linguistically-motivated evidence. In EMNLP-
CoNLL, pages 194?204.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Second AAAI Sym-
posium on Quantum Interaction, volume 26, pages
28?35.
Alison Wray. 2008. Formulaic language: Pushing the
boundaries. Oxford University Press.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In ACL-COLING, pages 849?
856.
Naomi Zeichner, Jonathan Berant, and Ido Dagan.
2012. Crowdsourcing inference-rule evaluation. In
ACL: Short Papers, pages 156?160.
654
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 77?87,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Type Level Clustering Evaluation: New Measures
and a POS Induction Case Study
Roi Reichart1? Omri Abend2?? Ari Rappoport2
1ICNC 2Institute of Computer Science
Hebrew University of Jerusalem
{roiri|omria01|arir}@cs.huji.ac.il
Abstract
Clustering is a central technique in NLP.
Consequently, clustering evaluation is of
great importance. Many clustering algo-
rithms are evaluated by their success in
tagging corpus tokens. In this paper we
discuss type level evaluation, which re-
flects class membership only and is inde-
pendent of the token statistics of a partic-
ular reference corpus. Type level evalua-
tion casts light on the merits of algorithms,
and for some applications is a more natural
measure of the algorithm?s quality.
We propose new type level evaluation
measures that, contrary to existing mea-
sures, are applicable when items are pol-
ysemous, the common case in NLP. We
demonstrate the benefits of our measures
using a detailed case study, POS induc-
tion. We experiment with seven leading
algorithms, obtaining useful insights and
showing that token and type level mea-
sures can weakly or even negatively corre-
late, which underscores the fact that these
two approaches reveal different aspects of
clustering quality.
1 Introduction
Clustering is a central machine learning technique.
In NLP, clustering has been used for virtually ev-
ery semi- and unsupervised task, including POS
tagging (Clark, 2003), labeled parse tree induction
(Reichart and Rappoport, 2008), verb-type clas-
sification (Schulte im Walde, 2006), lexical ac-
quisition (Davidov and Rappoport, 2006; Davi-
dov and Rappoport, 2008), multilingual document
?
* Both authors equally contributed to this paper.
? Omri Abend is grateful to the Azrieli Foundation for
the award of an Azrieli Fellowship.
clustering (Montavlo et al, 2006), coreference res-
olution (Nicolae and Nicolae, 2006) and named
entity recognition (Elsner et al, 2009). Conse-
quently, the methodology of clustering evaluation
is of great importance. In this paper we focus
on external clustering evaluation, i.e., evaluation
against manually annotated gold standards, which
exist for almost all such NLP tasks. External eval-
uation is the dominant form of clustering evalu-
ation in NLP, although other methods have been
proposed (see e.g. (Frank et al, 2009)).
In this paper we discuss type level evaluation,
which evaluates the set membership structure cre-
ated by the clustering, independently of the token
statistics of the gold standard corpus. Many clus-
tering algorithms are evaluated by their success
in tagging corpus tokens (Clark, 2003; Nicolae
and Nicolae, 2006; Goldwater and Griffiths, 2007;
Gao and Johnson, 2008; Elsner et al, 2009). How-
ever, in many cases a type level evaluation is the
natural one. This is the case, for example, when
a POS induction algorithm is used to compute a
tag dictionary (the set of tags that each word can
take), or when a lexical acquisition algorithm is
used for constructing a lexicon containing the set
of frames that a verb can participate in, or when a
sense induction algorithm computes the set of pos-
sible senses of each word. In addition, even when
the goal is corpus tagging, a type level evaluation
is highly valuable, since it may cast light on the
relative or absolute merits of different algorithms
(as we show in this paper).
Clustering evaluation has been extensively in-
vestigated (Section 3). However, the discussion
centers around the monosemous case, where each
item belongs to exactly one cluster, although pol-
ysemy is the common case in NLP.
The contribution of the present paper is as fol-
lows. First, we discuss the issue of type level eval-
uation and explain why even in the monosemous
case a token level evaluation presents a skewed
77
picture (Section 2). Second, we show for the
common polysemous case why adapting existing
information-theoretic measures to type level eval-
uation is not natural (Section 3). Third, we pro-
pose new mapping-based measures and algorithms
to compute them (Section 4). Finally, we perform
a detailed case study with part-of-speech (POS)
induction (Section 5). We compare seven lead-
ing algorithms, showing that token and type level
measures can weakly or even negatively correlate.
This shows that type level evaluation indeed re-
veals aspects of a clustering solution that are not
revealed by the common tagging-based evaluation.
Clustering is a vast research area. As far as we
know, this is the first NLP paper to propose type
level measures for the polysemous case.
2 Type Level Clustering Evaluation
This section motivates why both type and token
level external evaluations should be done, even in
the monosemous case.
Clustering algorithms compute a set of induced
clusters (a clustering). Some algorithms directly
compute a clustering, while some others produce
a tagging of corpus tokens from which a clustering
can be easily derived. A clustering is monosemous
if each item is allowed to belong to a single cluster
only, and polysemous otherwise. An external eval-
uation is one which is based on a comparison of an
algorithm?s result to a gold standard. In this paper
we focus solely on external evaluation, which is
the most common evaluation approach in NLP.
Token and type level evaluations reflect differ-
ent aspects of a clustering. External token level
evaluation assesses clustering quality according to
the clustering?s accuracy on a given manually an-
notated corpus. This is certainly a useful evalua-
tion measure, e.g. when the purpose of the cluster-
ing algorithm is to annotate a corpus to serve as
input to another application.
External type level evaluation views the com-
puted clustering as a set membership structure and
evalutes it independently of the token statistics in
the gold standard corpus. There are two main
cases in which this is useful. First, a type level
evaluation can be the natural one in light of the
problem itself. For example, if the purpose of
the clustering algorithm is to automatically build
a lexicon (e.g., VerbNet (Kipper et al, 2000)),
then the lexicon structure itself should be evalu-
ated. Second, it may be valuable to decouple cor-
pus statistics from the induced clustering when the
latter is to be used for annotating corpora that ex-
hibit different statistics. In other words, if we eval-
uate an algorithm that will be invoked on a diverse
set of corpora having different token statistics, a
type level evaluation might provide a better picture
(or at least a complementary one) on the quality of
the clustering algorithm.
To motivate type level evaluation, consider POS
induction, which exemplifies both cases above.
Clearly, a word form may belong to several parts
of speech (e.g., ?contrast? is both a noun and a
verb, ?fast? is both an adjective and an adverb,
?that? can be a determiner, conjunction and adverb,
etc.). As an evaluation of a POS induction algo-
rithm, it is natural to evaluate the lexicon it gener-
ates, even if the main goal is to annotate a corpus.
The lexicon lists the possible POS tags for each
word, and thus its evaluation is a polysemous type
level one.
Even if we ignore polysemy, type level evalua-
tion is useful for a POS induction algorithm used
to tag a corpus. There are POS classes whose
members are very frequent, e.g., determiners and
prepositions. Here, a very small number of word
types usually accounts for a large portion of corpus
tokens. For example, in the WSJ Penn Treebank
(Marcus et al, 1993), there are 43,740 word types
and over 1M word tokens. Of the types, 88 are
tagged as prepositions. These types account for
only 0.2% of the types, but for as many as 11.9%
of the tokens. An algorithm which is accurate only
on prepositions would do much better in a token
level evaluation than in a type level one.
This phenomenon is not restricted to preposi-
tions or English. In the WSJ corpus, determiners
account for 0.05% of the types but for 9.8% of the
tokens. In the German NEGRA corpus (Brants,
1997), the article class (both definite and indefi-
nite) accounts for 0.04% of the word types and for
12.5% of the word tokens, and the coordinating
conjunctions class accounts for 0.05% of the word
types but for 3% of the tokens.
The type and token behavior differences result
from the Zipfian distribution of word tokens to
word types (Mitzenmacher, 2004). Since the word
frequency distribution is Zipfian, any clustering al-
gorithm that is accurate only on a small number of
frequent words (not necessarily members of a par-
ticular class) would perform well in a token level
evaluation but not in a type one. For example,
78
the most frequent 100 word types (regardless of
POS class) in WSJ (NEGRA) account for 43.9%
(41.3%) of the tokens in the corpus. These words
appear in 32 out of the 34 non-punctuation POS
classes in WSJ and in 38 out of the 51 classes in
NEGRA.
Other natural language entities also demonstrate
Zipfian distribution of tokens to types. For exam-
ple, the distribution of syntactic categories in parse
tree constituents is Zipfian, as shown in (Reichart
and Rappoport, 2008) for English, German and
Chinese corpora. Thus, the distinction between to-
ken and type level evaluation is important also for
grammar induction algorithms.
It may be argued that a token level evaluation
is sufficient since it already reflects type informa-
tion. In this paper we demonstrate that this is not
the case, by showing that they correlate weakly or
even negatively in an important NLP task.
3 Existing Clustering Evaluation
Measures
Clustering evaluation is challenging. Many mea-
sures have been proposed in the past decades
(Pfitzner et al, 2008). In this section, we briefly
survey the three main types: mapping based,
counting pairs, and information theoretic mea-
sures, and motivate our decision to focus on the
first in this paper.
Mapping based measures are based on a post-
processing step in which each induced cluster is
mapped to a gold class (or vice versa). The stan-
dard mappings are greedy many-to-one (M-1) and
greedy one-to-one (1-1). Several measures which
rely on these mappings were proposed. The most
common and perhaps the simplest one is accu-
racy, which computes the fraction of items cor-
rectly clustered under the mapping. Other mea-
sures include: L (Larsen, 1999), D (Van Dongen,
2000), misclassification index (MI) (Zeng et al,
2002), H (Meila and Heckerman, 2001), clustering
F-measure (Fung et al, 2003) and micro-averaged
precision and recall (Dhillon et al, 2003). In Sec-
tion 4 we show why existing mapping-based mea-
sures cannot be applied to the polysemous type
case and present new mapping-based measures for
this case.
Counting pairs measures are based on a com-
binatorial approach which examines the number
of data element pairs that are clustered similarly
in the reference and proposed clustering. Among
these are Rand Index (Rand, 1971), Adjusted Rand
Index (Hubert and Arabie, 1985), ? statistic (Hu-
bert and Schultz, 1976), Jaccard (Milligan et al,
1983), Fowlkes-Mallows (Fowlkes and Mallows,
1983) and Mirkin (Mirkin, 1996). Schulte im
Walde (2006) used such a measure for type level
evaluation of monosemous verb type clustering.
Meila (2007) described a few problems with
such measures. A serious one is that their values
are unbounded, making it hard to interpret their
results. This can be solved by adjusting their val-
ues to lie in [0, 1], but even adjusted measures suf-
fer from severe distributional problems, limiting
their usability in practice. We thus do not address
counting pairs measures in this paper.
Information-theoretic (IT) measures. IT
measures assume that the items in the dataset are
taken from a known distribution (usually the uni-
form distribution), and thus the gold and induced
clusters can be treated as random variables. These
measures utilize a co-occurrence matrix I between
the gold and induced clusters. We denote the in-
duced clustering by K and the gold clustering by
C. Iij contains the number of items in the in-
tersection of the i-th gold class and the j-th in-
duced cluster. When assuming the uniform dis-
tribution, the probability of an event (a gold class
c or an induced cluster k) is its relative size, so
p(c) = ?|K|k=1
Ick
N and p(k) =
?|C|
c=1
Ick
N (N is the
total number of clustered items).
Under this assumption we define the entropies
and the conditional entropies:
H(C) = ? P|C|c=1
P|K|
k=1 Ick
N log
P|K|
k=1 Ick
N
H(C|K) = ? P|K|k=1
P|C|
c=1
Ick
N log
Ick
P|C|
c=1 Ick
H(K) and H(K|C) are defined similarly.
In Section 5 we use two IT measures for token
level evaluation, V (Rosenberg and Hirschberg,
2007) and NVI (Reichart and Rappoport, 2009)
(a normalized version of VI (Meila, 2007)). The
appealing properties of these measures have been
extensively discussed in these references; see also
(Pfitzner et al, 2008). V and NVI are defined as
follows:
h =
(
1 H(C) = 0
1 ? H(C|K)H(C) H(C) 6= 0
c =
(
1 H(K) = 0
1 ? H(K|C)H(K) H(K) 6= 0
V = 2hc
h + c
79
NV I(C, K) =
(
H(C|K)+H(K|C)
H(C) H(C) 6= 0
H(K) H(C) = 0
In the monosemous case (type or token), the ap-
plication of the measures described in this section
to type level evaluation is straightforward. In the
polysemous case, however, they suffer from seri-
ous shortcomings.
Consider a case in which each item is assigned
exactly r gold clusters and each gold cluster has
the exact same number of items (i.e., each has a
size of l?r|C| , where l is the number of items). Now,
consider an induced clustering where there are |C|
induced clusters (|K| = |C|) and each item is as-
signed to all induced clusters. The co-occurrence
matrix in this case should have identical values in
all its entries. Even if we allow the weight each
item contributes to the matrix to depend on its gold
and induced entry sizes, the situation will remain
the same. This is because all items have the exact
same entry size and both gold and induced cluster-
ings have uniform cluster sizes.
In this case, the random variables defined by the
induced and gold clustering assignments are in-
dependent (this easily follows from the definition
of independent events, since the joint probability
is the multiplication of the marginals). Hence,
H(K|C) = H(K) and H(C|K) = H(C), and
both V and NVI obtain their worst possible val-
ues1. However, the score should surely depend on
r (the size of each word?s gold entry). Specifi-
cally, when r = |C| we get that the induced and
gold clusterings are identical. This case should not
get the worst score, and it should definitely score
higher than the case in which r = 1, where K is
dramatically different from C.
The problem can in theory be solved by pro-
viding the number of clusters per item as an input
to the algorithm. However, in NLP this is unre-
alistic (even if the total number of clusters can be
provided) and the number should be determined
by the algorithm. We therefore do not consider
IT-based measures in this paper, deferring them to
future work.
4 Mapping Based Measures for
Polysemous Type Evaluation
In this section we present new type level evalu-
ation measures for the polysemous case. As we
1V values are in [0, 1], 0 being the worst. NVI obtains its
highest and worst possible value, 1 + log(|K|)H(C) .
show below, these measures do not suffer from the
problems discussed for IT measures in Section 3.
All measures are mapping-based: first, a map-
ping between the induced and gold clusters is per-
formed, and then a measure E is computed. As
is common in the clustering evaluation literature
(Section 3), we use M-1 and 1-1 greedy mappings,
defined to be those that maximize the correspond-
ing measure E.
Let C = {c1, ..., cn} be the set of gold classes
and K = {k1, ..., km} be the set of induced clus-
ters. Denote the number of words types by l. Let
Ai ? C, Bi ? K, i = 1...l be the set of gold
classes and set of induced clusters for each word.
The polysemous nature of task is reflected by the
fact that Ai and Bi are subsets, rather than mem-
bers, of C and K respectively.
Our measures address quality from two persec-
tives, that of the individual items clustered (Sec-
tion 4.1) and that of the clusters (Section 4.2).
Item-based measures especially suit evaluation of
clustering quality for the purpose of lexicon induc-
tion, and have no counterpart in the monosemous
case. Cluster-based measures are a direct general-
ization of existing mapping based measures to the
polysemous case.
The difficulty in designing item-based and
cluster-based measures is that the number of clus-
ters assigned to each item is determined by the
clustering algorithm. Below we show how to over-
come this.
4.1 Item-Based Evaluation
For a given mapping h : K ? C, denote
h(Bi) = {h(x) : x ? Bi}. A fundamental quan-
tity for item-based evaluation is the number of cor-
rect clusters for each item (word type) under this
mapping, denoted by IMi (IM stands for ?item
match?):
IMi = |Ai ? h(Bi)|
The total item match IM is defined to be:
IM = ?li=1 IMi =
?l
i=1 |Ai ? h(Bi)|
In the monosemous case, IM is normalized by
the number of items, yielding an accuracy score.
Applying a similar definition in the polysemous
case, normalizing instead by the total number of
gold clusters assigned to the items, can be easily
manipulated. Even a clustering which has the cor-
rect number of induced clusters (equal to the num-
ber of gold classes) but which assigns each item to
80
all induced clusters, receives a perfect score under
both greedy M-1 and 1-1 mappings. This holds for
any induced clustering for which ?i, Ai ? h(Bi).
Note that using a mapping from C to K (or a
combination of both directions) would exhibit the
same problem.
To overcome the problem, we use the harmonic
average of two normalized terms (F-score). We
use two average variants, micro and macro. Macro
average computes the total number of matches
over all words and normalizes in the end. Recall
(R), Precision (P) and their harmonic average (F-
score) are accordingly defined:
R = IMPl
i=1 |Ai|
P = IMPl
i=1 |h(Bi)|
MacroI = 2RP
R + P =
= 2IM
Pl
i=1 |Ai| +
Pl
i=1 |h(Bi)|
= F (h) ?
l
X
i=1
IMi
F (h) is a constant depending on h. As all items
are equally weighted, those with larger gold and
induced entries have more impact on the measure.
The micro average, aiming to give all items an
equal status, first computes an F-score for each
item and then averages over them. Hence, each
item contributes at most 1 to the measure. This
MicroI measure is given by:
Ri = IMi|Ai| Pi =
IMi
|h(Bi)| Fi =
2RiPi
Ri+Pi =
2IMi
|Ai|+|h(Bi)|
MicroI = 1
l
l
X
i=1
Fi =
1
l
l
X
i=1
2IMi
|Ai| + |h(Bi)|
=
= 1
l
l
X
i=1
wi(h) ? IMi
Where wi(h) is a weight depending on h but
also on i.
For both measures, the maximum score is 1. It
is obtained if and only if Ai = h(Bi) for every i.
In 1-1 mapping, when the number of induced
clusters is larger than the number of gold clus-
ters, some of the induced clusters are not mapped.
To preserve the nature of 1-1 mapping that pun-
ishes for excessive clusters2, we define |h(Bi)| to
be equal to |Bi| even for these unmapped clusters.
Recall that any induced clustering in which
?i, Ai ? h(Bi) gets the best score under a greedy
mapping with the accuracy measure. In MacroI
and MicroI the obtained recalls are perfect, but the
precision terms reflect deviation from the correct
solution.
2And to allow us to compute it accurately, see below.
In the example in Section 3 showing an unrea-
sonable behavior of IT-based measures, the score
depends on r for both MacroI and MicroI. With
our new measures, recall is always 1, but precision
is rn . This is true both for 1-1 and M-1 mappings.
Hence, the new measures show reasonable behav-
ior in this example for all r values.
MicroI was used in (Dasgupta and Ng, 2007)
with a manually compiled mapping. Their map-
ping was not based on a well-defined scheme but
on a heuristic. Moreover, providing a manual
mapping might be impractical when the number of
clusters is large, and can be inaccurate, especially
when the clustering is not of very high quality.
In the following we discuss how to compute the
1-1 and M-1 greedy mappings for each measure.
1-1 Mapping. We compute h by finding the
maximal weighted matching in a bipartite graph.
In this graph one side represents the induced clus-
ters, the other represents the gold classes and
the matchings correspond to 1-1 mappings. The
problem can be efficiently solved by the Kuhn-
Munkres algorithm (Kuhn, 1955; Munkres, 1957).
To be able to use this technique, edge weights
must not depend upon h. In 1-1 mapping,
|h(Bi)| = |Bi|, and therefore F (h) = F and
wi(h) = wi. That is, both quantities are inde-
pendent of h3. For MacroI, the weight on the edge
between the s-th gold class and the j-th induced
cluster is: W (esj) =
?l
i=1 F ? Is?AiIj?Bi . For
MicroI it is: W (esj) =
?l
i=1 wi ? Is?AiIj?Bi .
Is?Ai is 1 if s ? Ai and 0 otherwise.
M-1 Mapping. There are two problems in ap-
plying the bipartite graph technique to finding an
M-1 mapping. First, under such mapping wi(h)
and F (h) do depend on h. The problem may
be solved by selecting some constant weighting
scheme. However, a more serious problem also
arises.
Consider a case in which an item x has a gold
entry {C1} and an induced entry {K1, K2}. Say
the chosen mapping mapped both K1 and K2 to
C1. By summing over the graph?s edges selected
by the mapping, we add weight (F (h) for MacroI
and wi(h) for MicroI) both to the edge between
K1 and C1 and to the edge between K2 and C1.
However, the item?s IMi is only 1. This prohibits
3Consequently, the increase in MacroI and MicroI follow-
ing an increase of 1 in an item?s gold/induced intersection size
(IMi) is independent of h.
81
the use of the bipartite graph method for the M-1
case.
Since we are not aware of any exact method for
solving this problem, we use a hill-climbing al-
gorithm. We start with a random mapping and a
random order on the induced clusters. Then we
iterate over the induced clusters and map each of
them to the gold class which maximizes the mea-
sure given that the rest of the mapping remains
constant. We repeat the process until no improve-
ment to the measure can be obtained by changing
the assignment of a single induced cluster. Since
the score depends on the initial random mapping
and random order, we repeat this process several
times and choose the maximum between the ob-
tained scores.
4.2 Cluster-Based Evaluation
The cluster-based evaluation measures we propose
are a direct generalization of existing monose-
mous mapping based measures to the polysemous
type case.
For a given mapping h : K ? C, we define h? :
Kh ? C. Kh is defined to be a clustering which
is obtained by performing set union between every
two clusters in K that are mapped to the same gold
cluster. The resulting h? is always 1-1. We denote
|Kh| = mh.
Our motivation for using h? in the definition of
the measures instead of h is to stay as close as
possible to accuracy, the most common mapping-
based measure in the monosemous case. M-1
(monosemous) accuracy does not punish for split-
ing classes. For instance, in a case where there is
a gold cluster ci and two induced clusters k1 and
k2 such that ci = k1 ? k2, the M-1 accuracy is the
same as in the case where there is one cluster k1
such that ci = k1. M-1 accuracy, despite its in-
difference to splitting, was shown to reflect better
than 1-1 accuracy the clustering?s applicability for
subsequent applications (at least in some contexts)
(Headden III et al, 2008).
Recall that in item-based evaluation, IMi mea-
sures the intersection between the induced and
gold entries of each item. Therefore, the set union
operation is not needed for that case, since when
an item appears in two induced clusters that are
mapped to the same gold cluster, its IMi is in-
creased only by 1.
A fundamental quantity for cluster-based eval-
uation is the intersection between each induced
cluster and the gold class to which it is mapped.
We denote this value by CMj (CM stands for
?cluster match?):
CMj = |kj ? h?(kj)|
The total intersection (CM ) is accordingly de-
fined to be:
CM = ?mhj=1 CMj =
?mh
j=1 |kj ? h?(kj)|
As with the item-based evaluation (Section 4.1),
using CM or a derived accuracy as a measure is
problematic. A clustering that assigns n induced
classes to each word (n is the number of gold
classes) will get the highest possible score under
every greedy mapping (1-1 or M-1), as will any
clustering in which ?i, Ai ? h(Bi).
As in the item-based evaluation, a possible so-
lution is based on defining recall, precision and F-
score measures, computed either in the micro or in
the macro level. The macro cluster-based measure
turns out to be identical to the macro item-based
measure MacroI4.
The following derivation shows the equivalence
for the 1-1 case. The M-1 case is similar. We note
that h = h? in the 1-1 case and we therefore ex-
change them in the definition of CM . It is enough
to show that CM = IM , since the denominator is
the same in both cases:
CM = Pmj=1 |kj ? h(kj)| =
= Pmj=1
Pl
i=1 Ii?kj Ii?h(kj) =
= Pli=1
Pm
j=1 Ii?kj Ii?h(kj) =
= Pli=1 |Ai ? h(Bi)| = IM
The micro cluster-based measures are defined:
Rj = CMj|h?(kj)| Pj =
CMj
|kj | Fj =
2RjPj
Rj+Pj
The micro cluster measure MicroC is obtained
by taking a weighted average over the Fj?s:
MicroC = ?k?Kh
|k|
N? Fk
Where N? = ?z?Kh |z| is the number of clus-
tered items after performing the set union and
including repetitions. If, in the 1-1 case where
m > n, an induced cluster is not mapped, we de-
fine Fk = 0. A definition of the measure using
a reverse mapping (i.e., from C to K) would have
used a weighted average with weights proportional
to the gold classes? sizes.
4Hence, we have six type level measures: MacroI (which
is equal to MacroC), MicroI, and MicroC, each of which in
two versions, M-1 and 1-1.
82
The definition of h? causes a similar computa-
tional difficulty as in the M-1 item-based mea-
sures. Consequently, we apply a hill climbing
algorithm similar to the one described in Sec-
tion 4.1.
The 1-1 mapping is computed using the same
bipartite graph method described in Section 4.1.
The graph?s vertices correspond to gold and in-
duced clusters and an edge?s weight is the F-score
between the class and cluster corresponding to its
vertices times the cluster?s weight (|k|/N?).
5 Evaluation of POS Induction Models
As a detailed case study for the ideas presented
in this paper, we apply the various measures for
the POS induction task, using seven leading POS
induction algorithms.
5.1 Experimental Setup
POS Induction Algorithms. We experimented
with the following models: ARR10 (Abend et al,
2010), Clark03 (Clark, 2003), GG07 (Goldwa-
ter and Griffiths, 2007), GJ08 (Gao and Johnson,
2008), and GVG09 (Van Gael et al, 2009) (three
models). Additional recent good results for vari-
ous variants of the POS induction problem are de-
scribed in e.g., (Smith and Eisner, 2004; Grac?a et
al., 2009).
Clark03 and ARR10 are monosemous algo-
rithms, allowing a single cluster for each word
type. The other algorithms are polysemous. They
perform sequence labeling where each token is
tagged in its context, and different tokens (in-
stances) of the same type (word form) may receive
different tags.
Data Set. All models were tested on sections
2-21 of the PTB-WSJ, which consists of 39832
sentences, 950028 tokens and 39546 unique types.
Of the tokens, 832629 (87.6%) are not punctuation
marks.
Evaluation Measures. Type level evaluation
used the measures MacroI (which is equal to
MacroC), MicroI and MicroC both with greedy
1-1 and M-1 mappings as described in Section 4.
The type level gold (induced) entry is defined to
be the set of all gold (induced) clusters with which
it appears.
For the token level evaluation, six measures are
used (see Section 3): accuracy with M-1 and 1-1
mappings, NVI, V, H(C|K) and H(K|C), using e
as the logarithm?s base. We use the full WSJ POS
tags set excluding punctuation5.
Punctuation. Punctuation marks occupy a
large volume of the corpus tokens (12.4% in our
experimental corpus), and are easy to cluster.
Clustering punctuation marks thus greatly inflates
token level results. To study the relationship be-
tween type and token level evaluations in a fo-
cused manner, we excluded punctuation from the
evaluation (they are still used during training, so
algorithms that rely on them are not harmed).
Number of Induced Clusters. The number
of gold POS tags in WSJ is 45, of which 11 are
punctuation marks. Therefore, for the ARR10 and
Clark03 models, 34 clusters were induced. For
GJ08 we received the output with 45 clusters. The
iHMM models of GVG09 determine the number
of clusters automatically (resulting in 47, 91 and
192 clusters, see below). For GG07, our com-
puting resources did not enable us to induce 45
clusters and we therefore used 176. Our focus in
this paper is to study the type vs. token distinction
rather than to provide a full scope comparison be-
tween algorithms, for which more clustering sizes
would need to be examined.
Configurations. We ran the ARR10 tagger
with the configuration detailed in (Abend et al,
2010). For Clark03, we ran his neyessenmorph
model7 10 times (using an unknown words thresh-
old of 5) and report the average score for each
measure. The models of GVG09 were run in the
three configurations reported in their paper: one
with a Dirichlet process prior and fixed parame-
ters, another with a Pittman-Yore prior with fixed
parameters, and a third with a Dirichlet process
prior with parameters learnt from the data. All five
models were run in an optimal configuration.
We obtained the code of Goldwater and Grif-
fiths? BHMM model and ran it for 10K iterations
with an annealing technique for parameter estima-
tion. That was the best parameter estimation tech-
nique available to us. This is the first time that this
model is evaluated on such a large experimental
corpus, and it performed well under these condi-
tions.
The output of the model of GJ08 was sent to
us by the authors. The model was run on sec-
5We use all WSJ tokens in the training stage, but omit
punctuation marks during evaluation.
6The 17 most frequent tags cover 94% of the word in-
stances and more than 99% of the word types in the WSJ
gold standard tagging.
7www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html
83
tions 2-21 of the WSJ-PTB using significantly
inferior computing resources compared to those
used for producing the results reported in their
paper. While this model cannot be compared to
the aforementioned six models due to the subopti-
mal configuration, we evaluate its output using our
measures to get a broader variety of experimental
results8.
5.2 Results and Discussion
Table 1 presents the scores of the compared mod-
els under all evaluation measures (six token level,
six type level). What is important here to note
are the differences between type and token level
evaluations for the algorithms. We are mainly
interested in two things: (1) seeing how relative
rankings change in the two evaluation types, thus
showing that the two types are not highly corre-
lated and are both useful; and (2) insights gained
by using a type level evaluation in addition to the
usual token level one.
Note that the table should not be used to deduce
which algorithm is the ?best? for the task, even ac-
cording to a single evaluation type. This is be-
cause, as explained above, the algorithms do not
induce the same number of clusters and this affects
their results.
Results indicate that type level evaluation re-
veals aspects of the clustering quality that are not
expressed in the token level. For the Clark03
model the disparity is most apparent. While in
the token level it performs very well (better than
the polysemous algorithms for the 1-1, V and NVI
token level measures), in the type level it is the
second worst in the item-based 1-1 scores and the
worst in the M-1 scores.
Here we have a clear demonstration of the value
of type level evaluation. The Clark03 algorithm
is assessed as excellent using token level evalua-
tion (second only to ARR10 in M-1, 1-1, V and
NVI), and only a type level one shows its rela-
tively poor type performance. Although readers
may think that this is natural due to the algorithm?s
monosemous nature, this is not the case, since the
monosemous ARR10 generally ranked first in the
type level measures (more on this below).
The disparity is also observed for polysemous
algorithms. The GG07 model?s token level scores
are mediocre, while in the type level MicroC 1-1
8We would like to thank all authors for sending us the
data.
measure this model is the best and in the type level
MicroI and MacroI 1-1 measures it is the second
best.
Monosemous vs. polysemous algorithms. The
table shows that the ARR10 model achieves the
best results in most type and token level evalua-
tion measures. The fact that this monosemous al-
gorithm outperforms the polysemous ones, even
in a type level evaluation, may seem strange at
first sight but can be explained as follows. Pol-
ysemous tokens account for almost 60% of the
corpus (565K out of 950K), so we could expect
that a monosemous algorithm should do badly in
a token-level evaluation. However, for most of the
polysemous tokens the polysemy is only weakly
present in the corpus9, so it is hard to detect even
for polysemous algorithms. Regarding types, pol-
ysemous types constitute only 16.6% of the cor-
pus types, so a monosemous algorithm which is
quite good in assigning types to clusters has a good
chance of beating polysemous algorithms in a type
level evaluation.
Hence, monosemous POS induction algorithms
are not at such a great disadvantage relative to pol-
ysemous ones. This observation, which was fully
motivated by our type level case study, might be
used to guide future work on POS induction, and
it thus serves as another demonstration for the util-
ity of type level evaluation.
Hill climbing algorithm. For the type level
measures with greedy M-1 mapping, we used the
hill-climbing algorithm described in Section 4.
Recall that the mapping to which our algorithm
converges depends on its random initialization.
We therefore ran the algorithm with 10 differ-
ent random initializations and report the obtained
maximum for MacroI, MicroI and MicroC in Ta-
ble 1. The different initializations caused very lit-
tle fluctuation: not more than 1% in the 9 (7) best
runs for the item-based (MicroC) measures. We
take this result as an indication that the obtained
maximum is a good approximation of the global
maximum.
We tried to improve the algorithm by selecting
an intelligent initialization heuristic. We used the
M-1 mapping obtained by mapping each induced
cluster to the gold class with which it has the high-
9Only about 27% of the tokens are instances of words that
are polysemous but not weakly polysemous (we call a word
weakly polysemous if more than 95% of its instances (tokens)
are tagged by the same tag).
84
Token Level Evaluation Type Level Evaluation
MacroI MicroI MicroC
M-1 1-1 NVI V H(C|K) H(K|C) M-1 1-1 M-1 1-1 M-1 1-1
ARR10 0.675 0.588 0.809 0.608 1.041 1.22 0.579 0.444 0.596 0.455 0.624 0.403
Clark03 0.65 0.484 0.887 0.586 1.04 1.441 0.396 0.301 0.384 0.288 0.463 0.347
GG07 0.5 0.415 0.989 0.479 1.523 1.241 0.497 0.405 0.461 0.398 0.563 0.445
GVG09(1) 0.51 0.444 1.033 0.477 1.471 1.409 0.513 0.354 0.436 0.352 0.486 0.33
GVG09(2) 0.591 0.484 0.998 0.529 1.221 1.564 0.637 0.369 0.52 0.373 0.548 0.32
GVG09(3) 0.668 0.368 1.132 0.534 0.978 2.18 0.736 0.280 0.558 0.276 0.565 0.199
GJ08* 0.605 0.383 1.09 0.506 1.231 1.818 0.467 0.298 0.446 0.311 0.561 0.291
Table 1: Token level (left columns) and type level (right columns) results for seven POS induction
algorithms (rows) (see text for details). Token and type level performance are weakly correlated and
complement each other as evaluation measures. ARR10, a monosemous algorithm, yields the best results
in most measures. (GJ08* results are different from those reported in the original paper because it was
run with weaker computing resources than those used there.)
est weight edge in the bipartite graph. Recall from
Section 4.1 that this is a reasonable approximation
of the greedy M-1 mapping. Again, we ran it for
the three type level measures for 10 times with a
random update order on the induced clusters. This
had only a minor effect on the final scores.
Number of clusters. Previous work (Reichart
and Rappoport, 2009) demonstrated that in data
sets where a relatively small fraction of the gold
classes covers most of the items, it is reasonable
to choose this number to be the number of induced
clusters. In our experimental data set, this number
(the ?prominent cluster number?) is around 17 (see
Section 5.1). Up to this number, increasing the
number of clusters is likely to have a positive ef-
fect on token level M-1, 1-1, H(C|K), and H(K|C)
scores. Inducing a larger number of clusters, how-
ever, is likely to positively affect M-1 and H(C|K)
but to have a negative effect on 1-1 and H(K|C).
This tendency is reflected in Table 1. For the
GG07 model the number of induced clusters, 17,
approximates the number of prominent clusters
and is lower than the number of induced clus-
ters of the other models. This is reflected by
its low token level M-1 and H(C|K) performance
and its high quality H(K|C) and NVI token level
scores. The GVG (1)-(3) models induced 47, 91
and 192 clusters respectively. This might explain
the high token level M-1 and H(C|K) performance
of GVG(3), as well as its high M-1 type level
performance, compared to its mediocre scores in
other measures.
The item based measures. The table indicates
that there is no substantial difference between the
two item based type level scores with 1-1 map-
ping. The definitions of MacroI and MicroI imply
that if |Ai|+ |h(Bi)| (which equals |Ai|+ |Bi| un-
der a 1-1 mapping) is constant for all word types,
then a clustering will score equally on both 1-1
type measures. Indeed, in our experimental cor-
pus 83.4% of the word types have one POS tag,
12.5% have 2, 3.1% have 3 and only 1% of the
words have more. Therefore, |Ai| is roughly con-
stant. The ARR10 and Clark03 models assign a
word type to a single cluster. For the other models,
the number of clusters per word type is generally
similar to that of the gold standard. Consequently,
|Bi| is roughly constant as well, which explains
the similar behavior of the two measures.
Note that for other clustering tasks |Ai| may not
necessarily be constant, so the MacroI and MicroI
scores are not likely to be as similar under the 1-1
mapping.
6 Summary
We discussed type level evaluation for polysemous
clustering, presented new mapping-based evalu-
ation measures, and applied them to the evalua-
tion of POS induction algorithms, demonstrating
that type level measures provide value beyond the
common token level ones.
We hope that type level evaluation in general
and the proposed measures in particular will be
used in the future for evaluating clustering perfor-
mance in NLP tasks.
References
Omri Abend, Roi Reichart and Ari Rappoport, 2010.
Improved Unsupervised POS Induction through Pro-
totype Discovery. ACL ?10.
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
85
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech In-
duction. EACL ?03.
Sajib Dasgupta and Vincent Ng, 2007. Unsu-
pervised Part-of-Speech Acquisition for Resource-
Scarce Languages. EMNLP-CoNLL ?07.
Dmitry Davidov, Ari Rappoport, 2006. Efficient
Unsupervised Discovery of Word Categories us-
ing Symmetric Patterns and High Frequency Words.
COLING-ACL ?06.
Dmitry Davidov, Ari Rappoport. 2008. Unsupervised
Discovery of Generic Relationships Using Pattern
Clusters and its Evaluation by Automatically Gen-
erated SAT Analogy Questions. ACL ?08
I. S. Dhillon, S. Mallela, and D. S. Modha, 2003. In-
formation Theoretic Co-clustering. KDD ?03
Micha Elsner, Eugene Charniak, and Mark Johnson,
2009. Structured Generative Models for Unsuper-
vised Named-Entity Clustering. NAACL ?09.
Stella Frank, Sharon Goldwater, and Frank Keller,
2009. Evaluating Models of Syntactic Category
Acquisition without Using a Gold Standard. Proc.
31st Annual Conf. of the Cognitive Science Society,
2576?2581.
E.B Fowlkes and C.L. Mallows, 1983. A Method for
Comparing Two Hierarchical Clusterings. Journal
of American statistical Association,78:553-569.
Benjamin C. M. Fung, Ke Wang, and Martin Ester,
2003. Hierarchical Document Clustering using Fre-
quent Itemsets. SIAM International Conference on
Data Mining ?03.
Jianfeng Gao and Mark Johnson, 2008. A Compar-
ison of Bayesian Estimators for Unsupervised Hid-
den Markov Model POS Taggers. EMNLP ?08.
Sharon Goldwater and Tom Griffiths, 2007. Fully
Bayesian Approach to Unsupervised Part-of-Speech
Tagging. ACL ?07.
Joa?o Grac?a, Kuzman Ganchev, Ben Taskar and Fre-
nando Pereira, 2009. Posterior vs. Parameter Spar-
sity in Latent Variable Models. NIPS ?09.
William P. Headden III, David McClosky and Eugene
Charniak, 2008. Evaluating Unsupervised Part-of-
Speech Tagging for Grammar Induction. COLING
?08.
L. Hubert and J. Schultz, 1976. Quadratic Assignment
as a General Data Analysis Strategy. British Journal
of Mathematical and Statistical Psychology, 29:190-
241.
L. Hubert and P. Arabie, 1985. Comparing Partitions.
Journal of Classification, 2:193-218.
Maurice Kandall and Jean Dickinson, 1990. Rank
Correlation Methods. Oxford University Press, New
York.
Karin Kipper, Hoa Trang Dang and Martha Palmer,
2000. Class-Based Construction of a Verb Lexicon.
AAAI ?00.
Harold W. Kuhn, 1955. The Hungarian Method for
the Assignment Problem. Naval Research Logistics
Quarterly, 2:83-97.
Bjornar Larsen and Chinatsu Aone, 1999. Fast and ef-
fective text mining using linear-time document clus-
tering. KDD ?99.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313-330.
Marina Meila and David Heckerman, 2001. An Ex-
perimental Comparison of Model-based Clustering
Methods. Machine Learning, 42(1/2):9-29.
Marina Meila, 2007. Comparing Clustering ? an In-
formation Based Distance. Journal of Multivariate
Analysis, 98:873-895.
C.W Milligan, S.C Soon and L.M Sokol, 1983. The
Effect of Cluster Size, Dimensionality and the Num-
ber of Clusters on Recovery of True Cluster Struc-
ture. IEEE transactions on Pattern Analysis and
Machine Intelligence, 5:40-47.
Boris G. Mirkin, 1996. Mathematical Classification
and Clustering. Kluwer Academic Press.
Michael Mitzenmacher , 2004. A Brief History of
Generative Models for Power Law and Lognormal
Distributions . Internet Mathematics, 1(2):226-251.
Soto Montalvo, Raquel Martnez, Arantza Casillas, and
Vctor Fresno, 2006. Multilingual Document Clus-
tering: an Heuristic Approach Based on Cognate
Named Entities. ACL ?06.
James Munkres, 1957. Algorithms for the Assignment
and Transportation Problems. Journal of the SIAM,
5(1):32-38.
Cristina Nicolae and Gabriel Nicolae, 2006. BEST-
CUT: A Graph Algorithm for Coreference Resolu-
tion. EMNLP ?06.
Darius M. Pfitzner, Richard E. Leibbrandt and David
M.W Powers, 2008. Characterization and Evalua-
tion of Similarity Measures for Pairs of Clusterings.
Knowledge and Information Systems: An Interna-
tional Journal, DOI 10.1007/s10115-008-0150-6.
William Rand, 1971. Objective Criteria for the Evalu-
ation of Clustering Methods. Journal of the Ameri-
can Statstical Association, 66(336):846-850.
86
Roi Reichart and Ari Rappoport, 2008. Unsupervised
Induction of Labeled Parse Trees by Clustering with
Syntactic Features. COLING ?08.
Roi Reichart and Ari Rappoport, 2009. The NVI Clus-
tering Evaluation Measure. CoNLL ?09.
Andrew Rosenberg and Julia Hirschberg, 2007. V-
Measure: A Conditional Entropy-based External
Cluster Evaluation Measure. EMNLP ?07.
Sabine Schulte im Walde, 2006. Experiments on
the Automatic Induction of German Semantic Verb
Classes. Computational Linguistics, 32(2):159-194.
Noah A. Smith and Jason Eisner, 2004. Annealing
Techniques for Unsupervised Statistical Language
Learning. ACL ?04.
Stijn Van Dongen, 2000. Performance Criteria for
Graph Clustering and Markov Cluster Experiments.
Technical report CWI, Amsterdam
Jurgen Van Gael, Andreas Vlachos and Zoubin Ghahra-
mani, 2009. The Infinite HMM for Unsupervised
POS Tagging. EMNLP ?09.
Yujing Zeng, Jianshan Tang, Javier Garcia-Frias, and
Guang R. Gao, 2002. An Adaptive Meta-clustering
Approach: Combining the Information from Differ-
ent Clustering Results . CSB 00:276
87
