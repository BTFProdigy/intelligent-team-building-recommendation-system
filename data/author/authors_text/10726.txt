Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 80?88,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Retrieval of Reading Materials for Vocabulary and Reading Practice
Michael Heilman, Le Zhao, Juan Pino and Maxine Eskenazi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{mheilman,lezhao,jmpino,max}@cs.cmu.edu
Abstract
Finding appropriate, authentic reading mate-
rials is a challenge for language instructors.
The Web is a vast resource of texts, but most
pages are not suitable for reading practice, and
commercial search engines are not well suited
to finding texts that satisfy pedagogical con-
straints such as reading level, length, text qual-
ity, and presence of target vocabulary. We
present a system that uses various language
technologies to facilitate the retrieval and pre-
sentation of authentic reading materials gath-
ered from the Web. It is currently deployed in
two English as a Second Language courses at
the University of Pittsburgh.
1 Introduction
Reading practice is an important component of first
and second language learning, especially with re-
gards to vocabulary learning (Hafiz and Tudor,
1989). Appropriating suitable reading material for
the needs of a particular curriculum or particular stu-
dent, however, is a challenging process. Manually
authoring or editing readings is time-consuming and
raises issues of authenticity, which are particularly
significant in second language learning (Peacock,
1997). On the other hand, the Web is a vast resource
of authentic reading material, but commercial search
engines which are designed for a wide variety of in-
formation needs may not effectively facilitate the re-
trieval of appropriate readings for language learners.
In order to demonstrate the problem of finding ap-
propriate reading materials, here is a typical exam-
ple of an information need from a teacher of an En-
glish as a Second Language (ESL) course focused
on reading skills. This example was encountered
during the development of the system. It should
be noted that while we describe the system in the
context of ESL, we claim that the approach is gen-
eral enough to be applied to first language reading
practice and to languages other than English. To
fit within his existing curriculum, the ESL teacher
wanted to find texts on the specific topic of ?interna-
tional travel.? He sought texts that contained at least
a few words from the list of target vocabulary that
his student were learning that week. In addition, he
needed the texts to be within a particular range of
reading difficulty, fifth to eighth grade in an Ameri-
can school, and shorter than a thousand words.
Sending the query ?international travel? to a pop-
ular search engine did not produce a useful list of re-
sults1. The first result was a travel warning from the
Department of State2, which was at a high reading
level (grade 10 according to the approach described
by (Heilman et al, 2008)) and not likely to be of
interest to ESL students because of legal and techni-
cal details. Most of the subsequent results were for
commercial web sites and travel agencies. A query
for a subset of the target vocabulary words for the
course also produced poor results. Since the search
engine used strict boolean retrieval methods, the top
results for the query ?deduce deviate hierarchy im-
plicit undertake? were all long lists of ESL vocabu-
lary words3.
We describe a search system, called REAP
Search, that is tailored to the needs of language
1www.google.com, March 5, 2008
2http://travel.state.gov/travel/cis pa tw/cis pa tw 1168.html
3e.g., www.espindle.org/university word list uwl.html
80
teachers and learners. The system facilitates the re-
trieval of texts satisfying particular pedagogical con-
straints such as reading level and text length, and al-
lows the user to constrain results so that they con-
tain at least some, but not necessarily all, of the
words from a user-specified target vocabulary list.
It also filters out inappropriate material as well as
pages that do not contain significant amounts of text
in well-formed sentences. The system provides sup-
port for learners including an interface for reading
texts, easy access to dictionary definitions, and vo-
cabulary exercises for practice and review.
The educational application employs multiple
language technologies to achieve its various goals.
Information retrieval and web search technologies
provide the core components. Automated text clas-
sifiers organize potential readings by general topic
area and reading difficulty. We are also developing
an approach to measuring reading difficulty that uses
a parser to extract grammatical structures. Part of
Speech (POS) tagging is used to filter web pages to
maintain text quality.
2 Path of a Reading
In the REAP Search system, reading materials take a
path from the Web to students through various inter-
mediate steps as depicted in Figure 1. First, a crawl-
ing program issues queries to large-scale commer-
cial search engines to retrieve candidate documents.
These documents are annotated, filtered, and stored
in a digital library, or corpus. This digital library cre-
ation process is done offline. A customized search
interface facilitates the retrieval of useful reading
materials by teachers, who have particular curricu-
lar goals and constraints as part of their information
needs. The teachers organize their selected readings
through a curriculum manager. The reading inter-
face for students accesses the curriculum manager?s
database and provides the texts along with support
in the form of dictionary definitions and practice ex-
ercises.
3 Creating a Digital Library of Readings
The foundation of the system is a digital library of
potential reading material. The customized search
component does not search the Web directly, but
rather accesses this filtered and annotated database
of Web pages. The current library consists of ap-
proximately five million documents. Construction
of the digital library begins with a set of target vo-
cabulary words that might be covered by a course or
set of courses (typically 100-1,500 words), and a set
of constraints on text characteristics. The constraints
can be divided into three sets: those that can be ex-
pressed in a search engine query (e.g., target words,
number of target words per text, date, Web domain),
those that can be applied using just information in
the Web search result list (e.g., document size), and
those that require local annotation and filtering (e.g.,
reading level, text quality, profanity).
The system obtains candidate documents by
query-based crawling, as opposed to following
chains of links. The query-based document crawl-
ing approach is designed to download documents
for particular target words. Queries are submitted
to a commercial Web search engine4, result links are
downloaded, and then the corresponding documents
are downloaded. A commercial web search engine
is used to avoid the cost of maintaining a massive,
overly general web corpus.
Queries consist of combinations of multiple tar-
get words. The system generates 30 queries for each
target word (30 is a manageable and sufficient num-
ber in practice). These are spread across 2-, 3-,
and 4-word combinations with other target words.
Queries to search engines can often specify a date
range. We employ ranges to find more recent mate-
rial, which students prefer. The tasks of submitting
queries, downloading the result pages, and extract-
ing document links are distributed among a dozen
or so clients running on desktop machines, to run as
background tasks. The clients periodically upload
their results to a server, and request a new batch of
queries.
Once the server has a list of candidate pages, it
downloads them and applies various filters. The fi-
nal yield of texts is typically approximately one per-
cent of the originally downloaded results. Many web
pages are too long, contain too little well-formed
text, or are far above the appropriate reading level
for language learners. After downloading docu-
ments, the system annotates them as described in
the next section. It then stores the pages in a full-
4www.altavista.com
81
Figure 1: Path of Reading Materials from the Web to a Student.
text search engine called Indri, which is part of
the Lemur Toolkit5. This index provides a consis-
tent and efficient interface to the documents. Using
Lemur and the Indri Query Language allows for the
retrieval of annotated documents according to user-
specified constraints.
4 Annotations and Filters
Annotators automatically tag the documents in the
corpus to enable the filtering and retrieval of read-
ing material that matches user-specified pedagogical
constraints. Annotations include reading difficulty,
general topic area, text quality, and text length. Text
length is simply the number of word tokens appear-
ing in the document.
4.1 Reading Level
The system employs a language modeling ap-
proach developed by Collins-Thompson and Callan
(Collins-Thompson and Callan, 2005) that creates a
model of the lexicon for each grade level and pre-
dicts reading level, or readability, of given docu-
ments according to those models. The readabil-
ity predictor is a specialized Naive Bayes classi-
fier with lexical unigram features. For web docu-
ments in particular, Collins-Thompson and Callan
report that this language modeling-based prediction
has a stronger correlation with human-assigned lev-
els than other commonly used readability measures.
This automatic readability measure allows the sys-
tem to satisfy user-specified constraints on reading
difficulty.
We are also experimenting with using syntac-
tic features to predict reading difficulty. Heilman,
Collins-Thompson, and Eskenazi (Heilman et al,
2008) describe an approach that combines predic-
tions based on lexical and grammatical features. The
5www.lemurproject.org
grammatical features are frequencies of occurrence
of grammatical constructions, which are computed
from automatic parses of input texts. Using multiple
measures of reading difficulty that focus on different
aspects of language may allow users more freedom
to find texts that match their needs. For example,
a teacher may want to find grammatically simpler
texts for use in a lesson focused on introducing dif-
ficult vocabulary.
4.2 General Topic Area
A set of binary topic classifiers automatically clas-
sifies each potential reading by its general topic, as
described by Heilman, Juffs, and Eskenazi (2007).
This component allows users to search for readings
on their general interests without specifying a par-
ticular query (e.g., ?international travel?) that might
unnecessarily constrain the results to a very narrow
topic.
A Linear Support Vector Machine text classifier
(Joachims, 1999) was trained on Web pages from
the Open Directory Project (ODP)6. These pages ef-
fectively have human-assigned topic labels because
they are organized into a multi-level hierarchy of
topics. The following general topics were manually
selected from categories in the ODP: Movies and
Theater; Music; Visual Arts; Computers and Tech-
nology; Business; Math, Physics and Chemistry; Bi-
ology and Environment; Social Sciences; Health and
Medicine; Fitness and Nutrition; Religion; Politics;
Law and Crime; History; American Sports; and Out-
door Recreation.
Web pages from the ODP were used as gold-
standard labels in the training data for the classi-
fiers. SVM-Light (Joachims, 1999) was used as an
implementation of the Support Vector Machines. In
preliminary tests, the linear kernel produced slightly
6dmoz.org
82
better performance than a radial basis function ker-
nel. The values of the decision functions of the clas-
sifiers for each topic are used to annotate readings
with their likely topics.
The binary classifiers for each topic category were
evaluated according to the F1 measure, the harmonic
mean of precision and recall, using leave-one-out
cross-validation. Values for the F1 statistic range
from .68 to .86, with a mean value of .76 across
topics. For comparison, random guessing would be
expected to correctly choose the gold-standard label
only ten percent of the time. During an error analy-
sis, we observed that many of the erroneous classifi-
cations were, in fact, plausible for a human to make
as well. Many readings span multiple topics. For
example, a document on a hospital merger might be
classified as ?Health and Medicine? when the cor-
rect label is ?Business.? In the evaluation, the gold
standard included only the single topic specified by
the ODP. The final system, however, assigns multi-
ple topic labels when appropriate.
4.3 Text Quality
A major challenge of using Web documents for ed-
ucational applications is that many web pages con-
tain little or no text in well-formed sentences and
paragraphs. We refer to this problem as ?Text Qual-
ity.? Many pages consist of lists of links, navigation
menus, multimedia, tables of numerical data, etc. A
special annotation tool filters out such pages so that
they do not clutter up search results and make it dif-
ficult for users to find suitable reading materials.
The text quality filter estimates the proportion of
the word tokens in a page that are contained in well-
formed sentences. To do this it parses the Document
Object Model structure of the web page, and orga-
nizes it into text units delineated by the markup tags
in the document. Each new paragraph, table ele-
ment, span, or divider markup tag corresponds to the
beginning of a new text unit. The system then runs
a POS tagger7 over each text unit. We have found
that a simple check for whether the text unit con-
tains both a noun and a verb can effectively distin-
guish between content text units and those text units
that are just part of links, menus, etc. The proportion
7The OpenNLP toolkit?s tagger was used
(opennlp.sourceforge.net).
of the total tokens that are part of content text units
serves as a useful measure of text quality. We have
found that a threshold of about 85% content text is
appropriate, since most web pages contain at least
some non-content text in links, menus, etc. This ap-
proach to content extraction is related to previous
work on increasing the accessibility of web pages
(Gupta et al, 2003).
5 Constructing Queries
Users search for readings in the annotated corpus
through a simple interface that appears similar to,
but extends the functionality of, the interfaces for
commercial web search engines. Figure 2 shows
a screenshot of the interface. Users have the op-
tion to specify ad hoc queries in a text field. They
can also use drop down menus to specify optional
minimum and/or maximum reading levels and text
lengths. Another optional drop-down menu allows
users to constrain the general topic area of results. A
separate screen allows users to specify a list of tar-
get vocabulary words, some but not all of which are
required to appear in the search results. For ease of
use, the target word list is stored for an entire session
(i.e., until the web browser application is closed)
rather than specified with each query. After the user
submits a query, the system displays multiple results
per screen with titles and snippets.
5.1 Ranked versus Boolean Retrieval
In a standard boolean retrieval model, with AND as
the default operator, the results list consists of doc-
uments that contain all query terms. In conjunc-
tion with relevance ranking techniques, commercial
search engines typically use this model, a great ad-
vantage of which is speed. Boolean retrieval can en-
counter problems when queries have many terms be-
cause every one of the terms must appear in a doc-
ument for it to be selected. In such cases, few or
no satisfactory results may be retrieved. This issue
is relevant because a teacher might want to search
for texts that contain some, but not necessarily all,
of a list of target vocabulary words. For example,
a teacher might have a list of ten words, and any
text with five of those words would be useful to give
as vocabulary and reading practice. In such cases,
ranked retrieval models are more appropriate be-
83
Figure 2: Screenshot of Search Interface for Finding Appropriate Readings.
cause they do not require that all of the query terms
appear. Instead, these models prefer multiple occur-
rences of different word types as opposed to multiple
occurrences of the same word tokens, allowing them
to rank documents with more distinct query terms
higher than those with distinct query terms. Docu-
ments that contain only some of the query terms are
thus assigned nonzero weights, allowing the user to
find useful texts that contain only some of the target
vocabulary. The REAP search system uses the Indri
Query Language?s ?combine? and ?weight? opera-
tors to implement a ranked retrieval model for target
vocabulary. For more information on text retrieval
models, see (Manning et al, 2008).
5.2 Example Query
Figure 3 shows an example of a structured query
produced by the system from a teacher?s original
query and constraints. This example was slightly
altered from its original form for clarity of presen-
tation. The first line with the filrej operator filters
and rejects any documents that contain any of a long
list of words considered to be profanity, which are
omitted in the illustration for brevity and posterity.
The filreq operator in line 2 requires that all of the
constraints on reading level, text length and quality
in lines 2-4 are met. The weight operator at the start
of line 5 balances between the ad hoc query terms in
line 5 and the user-specific target vocabulary terms
in lines 6-8. The uw10 operator on line 5 tells the
system to prefer texts where the query terms appear
together in an unordered window of size 10. Such
proximity operators cause search engines to prefer
documents in which query terms appear near each
other. The implicit assumption is that the terms in
queries such as ?coal miners safety? are more likely
to appear in the same sentence or paragraph in rele-
vant documents than irrelevant ones, even if they do
not appear consecutively. Importantly, query terms
are separated from target words because there are
usually a much greater number of target words, and
thus combining the two sets would often result in
the query terms being ignored. The higher weight
assigned to the set of target words ensures they are
not ignored.
6 Learner and Teacher Support
In addition to search facilities, the system provides
extensive support for students to read and learn from
texts as well as support for teachers to track stu-
dents? progress. All interfaces are web-based for
easy access and portability. Teachers use the search
system to find readings, which are stored in a cur-
riculum manager that allows them to organize their
selected texts. The manager interface allows teach-
ers to perform tasks such as specifying the order
of presentation of their selected readings, choosing
target words to be highlighted in the texts to focus
learner attention, and specifying time limits for each
text.
The list of available readings are shown to stu-
dents when they log in during class time or for
homework. Students select a text to read and move
on to the reading interface, which is illustrated in
Figure 4. The chosen web page is displayed in its
original format except that the original hyperlinks
and pop-ups are disabled. Target words that were
84
Figure 3: Example Structured Query. The line numbers on the left are for reference only.
chosen by the teacher are highlighted and linked to
definitions. Students may also click on any other
unknown words to access definitions. The dictio-
nary definitions are provided from the Cambridge
Advanced Learner?s Dictionary8, which is authored
specifically for ESL learners. All dictionary access
is logged, and teachers can easily see which words
students look up.
The system also provides vocabulary exercises af-
ter each reading for additional practice and review
of target words. Currently, students complete cloze,
or fill-in-the-blank, exercises for each target word in
the readings. Other types of exercises are certainly
possible. For extra review, students also complete
exercises for target words from previous readings.
Students receive immediate feedback on the prac-
tice and review exercises. Currently, sets of the ex-
ercises are manually authored for each target word
and stored in a database, but we are exploring auto-
mated question generation techniques (Brown et al,
2005; Liu et al, 2005). At runtime, the system se-
lects practice and review exercises from this reposi-
tory.
7 Related Work
A number of recent projects have taken similar ap-
proaches to providing authentic texts for language
learners. WERTi (Amaral et al, 2006) is an in-
telligent automatic workbook that uses texts from
the Web to increase knowledge of English gram-
matical forms and functions. READ-X (Miltsakaki
and Troutt, 2007) is a tool for finding texts at spec-
ified reading levels. SourceFinder (Sheehan et al,
2007) is an authoring tool for finding suitable texts
for standardized test items on verbal reasoning and
8dictionary.cambridge.org
reading comprehension.
The REAP Tutor (Brown and Eskenazi, 2004;
Heilman et al, 2006) for ESL vocabulary takes a
slightly different approach. Rather than teachers
choosing texts as in the REAP Search system, the
REAP Tutor itself selects individualized practice
readings from a digital library. The readings contain
target vocabulary words that a given student needs
to learn based on a student model. While the in-
dividualized REAP Tutor has the potential to better
match the needs of each student since each student
can work with different texts, a drawback of its ap-
proach is that instructors may have difficulty coor-
dinating group discussion about readings and inte-
grating the Tutor into their curriculum. In the REAP
Search system, however, teachers can find texts that
match the needs and interests of the class as a whole.
While some degree of individualization is lost, the
advantages of better coordinated support from teach-
ers and classroom integration are gained.
8 Pilot Study
8.1 Description
Two teachers and over fifty students in two ESL
courses at the University of Pittsburgh used the sys-
tem as part of a pilot study in the Spring of 2008.
The courses focus on developing the reading skills
of high-intermediate ESL learners. The target vo-
cabulary words covered in the courses come from
the Academic Word List (Coxhead, 2000), a list
of broad-coverage, general purpose English words
that frequently appear in academic writing. Students
used the system once per week in a fifty-minute class
for eight weeks. For approximately half of a ses-
sion, students read the teacher-selected readings and
worked through individualized practice exercises.
85
Figure 4: Screenshot of Student Interface Displaying a Reading and Dictionary Definition.
For the other half of each session, the teacher pro-
vided direct instruction on and facilitated discussion
about the texts and target words, making connec-
tions to the rest of the curriculum when possible.
For each session, the teachers found three to five
readings. Students read through at least two of the
readings, which were discussed in class. The extra
readings allowed faster readers to progress at their
own pace if they complete the first two. Teachers
learned to use the system in a training session that
lasted about 30 minutes.
8.2 Usage Analysis
To better understand the two teachers? interactions
with the search system, we analyzed query log data
from a four week period. In total, the teachers used
the system to select 23 readings for their students.
In the process, they issued 47 unique queries to the
system. Thus, on average they issued 2.04 queries
per chosen text. Ideally, a user would only have to
issue a single query to find useful texts, but from
the teachers? comments it appears that the system?s
usability is sufficiently good in general. Most of
the time, they specified 20 target words, only some
of which appeared in their selected readings. The
teachers included ad hoc queries only some of the
time. These were informational in nature and ad-
dressed a variety of topics. Example queries in-
clude the following: ?surviving winter?, ?coal min-
ers safety?, ?gender roles?, and ?unidentified flying
objects?. The teachers chose these topics because
they matched up with topics discussed in other parts
of their courses? curricula. In other cases, it was
more important for them to search for texts with tar-
get vocabulary rather than those on specific topics,
so they only specified target words and pedagogical
constraints.
8.3 Post-test and Survey Results
At the end of the semester, students took an exit sur-
vey followed by a post-test consisting of cloze vo-
cabulary questions for the target words they prac-
ticed with the system. In previous semesters, the
REAP Tutor has been used in one of the two courses
that were part of the pilot study. For comparison
with those results, we focus our analysis on the sub-
set of data for the 20 students in that course. The
exit survey results, shown in 5, indicate that stu-
dents felt it was easy-to-use and should be used in
future classes. These survey results are actually very
similar to previous results from a Spring 2006 study
with the REAP Tutor (Heilman et al, 2006). How-
ever, responses to the prompt ?My teacher helped
me to learn by discussing the readings after I read
86
Figure 5: The results from the pilot study exit survey, which used a Likert response format from 1-5 with 1=Strongly
Disagree, 3=Neither Agree nor Disagree, and 5=Strongly Agree. Error bars indicate standard deviations.
them? suggest that the tight integration of an edu-
cational system with other classroom activities, in-
cluding teacher-led discussions, can be beneficial.
Learning of target words was directly measured
by the post-test. On average, students answered
89% of cloze exercises correctly, compared to less
than 50% in previous studies with the REAP Tutor.
A direct comparison to those studies is challenging
since the system in this study provided instruction
on words that students were also studying as part of
their regular coursework, whereas systems in previ-
ous studies did not.
9 Discussion and Future Work
We have described a system that enables teachers
to find appropriate, authentic texts from the Web
for vocabulary and reading practice. A variety of
language technologies ranging from text retrieval to
POS tagging perform essential functions in the sys-
tem. The system has been used in two courses by
over fifty ESL students.
A number of questions remain. Can language
learners effectively and efficiently use such a system
to search for reading materials directly, rather than
reading what a teacher selects? Students could use
the system, but a more polished user interface and
further progress on filtering out readings of low text
quality is necessary. Is such an approach adaptable
to other languages, especially less commonly taught
languages for which there are fewer available Web
pages? Certainly there are sufficient resources avail-
able on the Web in commonly taught languages such
as French or Japanese, but extending to other lan-
guages with fewer resources might be significantly
more challenging. How effective would such a tool
be in a first language classroom? Such an approach
should be suitable for use in first language class-
rooms, especially by teachers who need to find sup-
plemental materials for struggling readers. Are there
enough high-quality, low-reading level texts for very
young readers? From observations made while de-
veloping REAP, the proportion of Web pages below
fourth grade reading level is small. Finding appro-
priate materials for beginning readers is a challenge
that the REAP developers are actively addressing.
Issues of speed and scale are also important to
consider. Complex queries such as the one shown
in Figure 3 are not as efficient as boolean queries.
The current system takes a few seconds to return re-
sults from its database of several million readings.
Scaling up to a much larger digital library may re-
quire sophisticated distributed processing of queries
across multiple disks or multiple servers. However,
we maintain that this is an effective approach for
providing texts within a particular grade level range
or known target word list.
Acknowledgments
This research was supported in part by the Insti-
tute of Education Sciences, U.S. Department of Ed-
ucation, through Grant R305B040063 to Carnegie
Mellon University; Dept. of Education grant
R305G03123; the Pittsburgh Science of Learning
Center which is funded by the National Science
Foundation, award number SBE-0354420; and a Na-
tional Science Foundation Graduate Research Fel-
lowship awarded to the first author. Any opin-
ions, findings, conclusions, or recommendations ex-
pressed in this material are the authors, and do not
necessarily reflect those of the sponsors.
References
Luiz Amaral, Vanessa Metcalf and Detmar Meurers.
87
2006. Language Awareness through Re-use of NLP
Technology. Pre-conference Workshop on NLP in
CALL ? Computational and Linguistic Challenges.
CALICO 2006.
Jon Brown and Maxine Eskenazi. 2004. Retrieval of
authentic documents for reader-specific lexical prac-
tice. Proceedings of InSTIL/ICALL Symposium 2004.
Venice, Italy.
Jon Brown, Gwen Frishkoff, and Maxine Eskenazi.
2005. Automatic question generation for vocabulary
assessment. Proceedings of HLT/EMNLP 2005. Van-
couver, B.C.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical language
models. Journal of the American Society for Informa-
tion Science and Technology, 56(13). pp. 1448-1462.
Averil Coxhead. 2000. A New Academic Word List.
TESOL Quarterly, 34(2). pp. 213-238.
S. Gupta, G. Kaiser, D. Neistadt, and P. Grimm. 2003.
DOM-based content extraction of HTML documents.
ACM Press, New York.
F. M. Hafiz and Ian Tudor. 1989. Extensive reading
and the development of language skills. ELT Journal
43(1):4-13. Oxford University Press.
Michael Heilman, Kevyn Collins-Thompson, Maxine Es-
kenazi. 2008. An Analysis of Statistical Models and
Features for Reading Difficulty Prediction. The 3rd
Workshop on Innovative Use of NLP for Building Edu-
cational Applications. Association for Computational
Linguistics.
Michael Heilman, Alan Juffs, Maxine Eskenazi. 2007.
Choosing Reading Passages for Vocabulary Learning
by Topic to Increase Intrinsic Motivation. Proceedings
of the 13th International Conferenced on Artificial In-
telligence in Education. Marina del Rey, CA.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2006. Classroom suc-
cess of an Intelligent Tutoring System for lexical prac-
tice and reading comprehension. Proceedings of the
Ninth International Conference on Spoken Language
Processing. Pittsburgh, PA.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. Advances in Kernel Methods -
Support Vector Learning, B. Schlkopf and C. Burges
and A. Smola (ed.) MIT-Press.
Chao-Lin Liu, Chun-Hung Wang, Zhao-Ming Gao, and
Shang-Ming Huang. 2005. Applications of Lexical
Information for Algorithmically Composing Multiple-
Choice Cloze Items Proceedings of the Second
Workshop on Building Educational Applications Us-
ing NLP. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press. Draft available
at http://www-csli.stanford.edu/?hinrich/information-
retrieval-book.html.
Eleni Miltsakaki and Audrey Troutt. 2007. Read-X: Au-
tomatic Evaluation of Reading Difficulty of Web Text.
Proceedings of E-Learn 2007, sponsored by the Asso-
ciation for the Advancement of Computing in Educa-
tion. Quebec, Canada.
Matthew Peacock. 1997. The effect of authentic mate-
rials on the motivation of EFL learners. ELT Journal
51(2):144-156. Oxford University Press.
Kathleen M. Sheehan, Irene Kostin, Yoko Futagi. 2007.
SourceFinder: A Construct-Driven Approach for Lo-
cating Appropriately Targeted Reading Comprehen-
sion Source Texts. Proceedings of the SLaTE Work-
shop on Speech and Language Technology in Educa-
tion. Carnegie Mellon University and International
Speech Communication Association (ISCA).
88
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 43?46,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
An Application of Latent Semantic Analysis to Word Sense Discrimination
for Words with Related and Unrelated Meanings
Juan Pino and Maxine Eskenazi
(jmpino, max)@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
Abstract
We present an application of Latent Semantic
Analysis to word sense discrimination within
a tutor for English vocabulary learning. We
attempt to match the meaning of a word in a
document with the meaning of the same word
in a fill-in-the-blank question. We compare
the performance of the Lesk algorithm to La-
tent Semantic Analysis. We also compare the
performance of Latent Semantic Analysis on a
set of words with several unrelated meanings
and on a set of words having both related and
unrelated meanings.
1 Introduction
In this paper, we present an application of Latent
Semantic Analysis (LSA) to word sense discrimi-
nation (WSD) within a tutor for English vocabu-
lary learning for non-native speakers. This tutor re-
trieves documents from the Web that contain tar-
get words a student needs to learn and that are at
an appropriate reading level (Collins-Thompson and
Callan, 2005). It presents a document to the student
and then follows the document reading with practice
questions that measure how the student?s knowledge
has evolved. It is important that the fill-in-the-blank
questions (also known as cloze questions) that we
ask to the students allow us to determine their vocab-
ulary knowledge accurately. An example of cloze
question is shown in Figure 1.
Some words have more than one meaning and so
the cloze question we give could be about a different
meaning than the one that the student learned in the
document. This is something that can lead to confu-
sion and must be avoided. To do this, we need to use
some automatic measure of semantic similarity.
Figure 1: Example of cloze question.
To define the problem formally, given a target
word w, a string r (the reading) containing w and
n strings q1, ..., qn (the sentences used for the ques-
tions) each containing w, find the strings qi where
the meaning of w is closest to its meaning in r.
We make the problem simpler by selecting only one
question.
This problem is challenging because the context
defined by cloze questions is short. Furthermore,
a word can have only slight variations in meaning
that even humans find sometimes difficult to distin-
guish. LSA was originally applied to Information
Retrieval (Dumais et al, 1988). It was shown to be
able to match short queries to relevant documents
even when there were no exact matches between the
words. Therefore LSA would seem to be an appro-
priate technique for matching a short context, such
as a question, with a whole document.
So we are looking to first discriminate between
the meanings of words, such as ?compound?, that
have several very different meanings (a chemical
compound or a set of buildings) and then to dis-
ambiguate words that have senses that are closely
related such as ?comprise? (?be composed of? or
?compose?). In the following sections, we present
43
LSA and some of its applications, then we present
some experimental results that compare a baseline to
the use of LSA for both tasks we have just described.
We expect the task to be easier on words with unre-
lated meanings. In addition, we expect that LSA will
perform better when we use context selection on the
documents.
2 Related Work
LSA was originally applied to Information Retrieval
(Dumais et al, 1988) and called Latent Semantic In-
dexing (LSI). It is based on the singular value de-
composition (SVD) theorem. A m ? n matrix X
with m ? n can be written as X = U ?S ?V T where
U is a m ? n matrix such that UT ? U = Im; S is
a n?n diagonal matrix whose diagonal coefficients
are in decreasing order; and V is a n?n matrix such
that V T ? V = In.
X is typically a term-document matrix that repre-
sents the occurrences of vocabulary words in a set of
documents. LSI uses truncated SVD, that is it con-
siders the first r columns of U (written Ur), the r
highest coefficients in S (Sr) and the first r columns
of V (Vr). Similarity between a query and a docu-
ment represented by vectors d and q is performed by
computing the cosine similarity between S?1r ?UTr ?d
and S?1r ?UTr ?q. The motivation for computing sim-
ilarity in a different space is to cope with the sparsity
of the vectors in the original space. The motivation
for truncating SVD is that only the most meaning-
ful semantic components of the document and the
query are represented after this transformation and
that noise is discarded.
LSA was subsequently applied to number of prob-
lems, such as synonym detection (Landauer et al,
1998), document clustering (Song and Park, 2007),
vocabulary acquisition simulation (Landauer and
Dumais, 1997), etc.
Levin and colleagues (2006) applied LSA to word
sense discrimination. They clustered documents
containing ambiguous words and for a test instance
of a document, they assigned the document to its
closest cluster. Our approach is to assign to a doc-
ument the question that is closest. In addition, we
examine the cases where a word has several unre-
lated meanings and where a word has several closely
related meanings.
3 Experimental Setup
We used a database of 62 manually generated cloze
questions covering 16 target words1. We manually
annotated the senses of the target words in these
questions using WordNet senses (Fellbaum, 1998).
For each word and for each sense, we manually gath-
ered documents from the Web containing the target
word with the corresponding sense. There were 84
documents in total. We added 97 documents ex-
tracted from the tutor database of documents that
contained at least one target word but we did not an-
notate their meaning.
We wanted to evaluate the performances of LSA
for WSD for words with unrelated meanings and for
words with both related and unrelated meanings. For
the first type of evaluation, we retained four target
words. For the second type of evaluation, all 16
words were included. We also wanted to evaluate
the influence of the size of the context of the tar-
get words. We therefore considered two matrices:
a term-document matrix and a term-context matrix
where context designates five sentences around the
target word in the document. In both cases each
cell of the matrix had a tf-idf weight. Finally, we
wanted to investigate the influence of the dimension
reduction on performance. In our experiments, we
explored these three directions.
4 Results
4.1 Baseline
We first used a variant of the Lesk algorithm (Lesk,
1986), which is based on word exact match. This al-
gorithm seems well suited for the unsupervised ap-
proach we took here since we were dealing with
discrimination rather than disambiguation. Given
a document d and a question q, we computed the
number of word tokens that were shared between d
and q, excluding the target word. The words were
lower cased and stemmed using the Porter stem-
mer. Stop words and punctuation were discarded;
we used the standard English stopword list. Finally,
we selected a window of nw words around the tar-
get word in the question q and a window of ns
sentences around the target word in the document
d. In order to detect sentence boundaries, we used
1available at: www.cs.cmu.edu/ jmpino/questions.xls
44
the OpenNLP toolkit (Baldridge et al, 2002). With
nw = 10 and ns = 2, we obtained an accuracy of
61% for the Lesk algorithm. This can be compared
to a random baseline of 44% accuracy.
4.2 LSA
We indexed the document database using the Lemur
toolkit (Allan et al, 2003). The database contained
both the manually annotated documents and the doc-
uments used by the tutor and containing the target
words. The Colt package (Binko et al, ) was used
to perform singular value decomposition and matrix
operations because it supports sparse matrix oper-
ations. We explored three directions in our analy-
sis. We investigated how LSA performs for words
with related meanings and for words with unrelated
meanings. We also explored the influence of the
truncation parameter r. Finally, we examined if re-
ducing the document to a selected context of the tar-
get word improved performance.
Figures 2 and 3 plot accuracy versus dimension
reduction in different cases. In all cases, LSA out-
performs the baseline for certain values of the trun-
cation parameter and when context selection was
used. This shows that LSA is well suited for measur-
ing semantic similarity between two contexts when
at least one of them is short. In general, using the full
dimension in SVD hurts the performances. Dimen-
sion reduction indeed helps discarding noise and
noise is certainly present in our experiments since
we do not perform stemming and do not use a stop-
word list. One could argue that filling the matrix
cells with tf-idf weights already gives less impor-
tance to noisy words.
Figure 2 shows that selecting context in docu-
ments does not give much improvement in accuracy.
It might be that the amount of context selected de-
pends on each document. Here we had a fixed size
context of five sentences around the target word.
In Figure 3, selecting context gives some im-
provement, although not statistically significant,
over the case with the whole document as context.
The best performance obtained for words with un-
related meanings and context selection is also better
than the performance for words with related and un-
related meanings.
 0
 0.2
 0.4
 0.6
 0.8
 1
 60  80  100  120  140  160  180
Ac
cu
ra
cy
Truncation Parameter
Accuracy vs. Dimension Reduction for Related Meanings with Different Contexts
whole document
selected context
Lesk baseline
Figure 2: Accuracy vs. r, the truncation parameter,
for words with related and unrelated meanings and with
whole document or selected context (95% confidence for
whole document: [0.59; 0.65], 95% confidence for se-
lected context: [0.52; 0.67])
 0
 0.2
 0.4
 0.6
 0.8
 1
 5  10  15  20  25  30  35
Ac
cu
ra
cy
Truncation Parameter
Accuracy vs. Dimension Reduction for Unrelated Meanings with Different Contexts
whole document
selected context
Lesk baseline
Figure 3: Accuracy vs. r, the truncation parameter, for
words with unrelated meanings only and with whole doc-
uments or selected context ((95% confidence for whole
document: [0.50; 0.59], 95% confidence for selected con-
text: [0.52; 0.71]))
45
5 Discussion
LSA helps overcome sparsity of short contexts such
as questions and gives an improvement over the ex-
act match baseline. However, reducing the context
of the documents to five sentences around the tar-
get word does not seem to give significant improve-
ment. This might be due to the fact that capturing
the right context for a meaning is a difficult task
and that a fixed size context does not always rep-
resent a relevant context. It is yet unclear how to set
the truncation parameter. Although dimension re-
duction seems to help, better results are sometimes
obtained when the truncation parameter is close to
full dimension or when the truncation parameter is
farther from the full dimension.
6 Conclusion and Future Work
We have shown that LSA, which can be considered
as a second-order representation of the documents
and question vectors, is better suited than the Lesk
algorithm, which is a first-order representation of
vectors, for measuring semantic similarity between
a short context such as a question and a longer con-
text such as a document. Dimension reduction was
shown to play an important role in the performances.
However, LSA is relatively difficult to apply to large
amounts of data because SVD is computationally in-
tensive when the vocabulary size is not limited. In
the context of tutoring systems, LSA could not be
applied on the fly, the documents would need to be
preprocessed and annotated beforehand.
We would like to further apply this promising
technique for WSD. Our tutor is able to provide def-
initions when a student is reading a document. We
currently provide all available definitions. It would
be more beneficial to present only the definitions
that are relevant to the meaning of the word in the
document or at least to order them according to their
semantic similarity with the context. We would also
like to investigate how the size of the selected con-
text in a document can affect performance. Finally,
we would like to compare LSA performance to other
second-order vector representations such as vectors
induced from co-occurrence statistics.
Acknowledgments
Thanks Mehrbod Sharifi, David Klahr and Ken
Koedinger for fruitful discussions. This research is
supported by NSF grant SBE-0354420. Any conclu-
sions expressed here are those of the authors.
References
James Allan, Jamie Callan, Kevin Collins-Thompson,
Bruce Croft, Fangfang Feng, David Fisher, John Laf-
ferty, Leah Larkey, Thi N. Truong, Paul Ogilvie, et al
2003. The lemur toolkit for language modeling and
information retrieval.
Jason Baldridge, Thomas Morton, and Gann Bierner.
2002. The opennlp maximum entropy package. Tech-
nical report, Technical report, SourceForge.
Pavel Binko, Dino Ferrero Merlino, Wolfgang Hoschek,
Tony Johnson, Andreas Pfeiffer, et al Open source
libraries for high performance scientific and technical
computing in java.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical language
models. Journal of the American Society for Informa-
tion Science and Technology, 56(13):1448?1462.
Susane T. Dumais, George W. Furnas, Thomas K.
Landauer, Scott Deerwester, and Richard Harshman.
1988. Using latent semantic analysis to improve ac-
cess to textual information. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 281?285.
Christiane Fellbaum. 1998. WordNet: An electronic lex-
ical database. MIT press.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to plato?s problem: The latent semantic analysis
theory of acquisition, induction and representation of
knowledge. Psychological review, 104:211?240.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse processes, 25:259?284.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
5th annual international conference on Systems Docu-
mentation, pages 24?26.
Esther Levin, Mehrbod Sharifi, and Jerry Ball. 2006.
Evaluation of utility of lsa for word sense discrimina-
tion. In Proceedings of HLT/NAACL, pages 77?80.
Wei Song and Soon Cheol Park. 2007. A novel docu-
ment clustering model based on latent semantic analy-
sis. In Proceedings of the Third International Confer-
ence on Semantics, Knowledge and Grid, pages 539?
542.
46
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 545?554,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Hierarchical Phrase-based Translation Grammars Extracted from
Alignment Posterior Probabilities
Adria` de Gispert, Juan Pino, William Byrne
Machine Intelligence Laboratory
Department of Engineering, University of Cambridge
Trumpington Street, CB2 1PZ, U.K.
{ad465|jmp84|wjb31}@eng.cam.ac.uk
Abstract
We report on investigations into hierarchi-
cal phrase-based translation grammars based
on rules extracted from posterior distributions
over alignments of the parallel text. Rather
than restrict rule extraction to a single align-
ment, such as Viterbi, we instead extract rules
based on posterior distributions provided by
the HMM word-to-word alignment model. We
define translation grammars progressively by
adding classes of rules to a basic phrase-based
system. We assess these grammars in terms
of their expressive power, measured by their
ability to align the parallel text from which
their rules are extracted, and the quality of the
translations they yield. In Chinese-to-English
translation, we find that rule extraction from
posteriors gives translation improvements. We
also find that grammars with rules with only
one nonterminal, when extracted from posteri-
ors, can outperform more complex grammars
extracted from Viterbi alignments. Finally, we
show that the best way to exploit source-to-
target and target-to-source alignment models
is to build two separate systems and combine
their output translation lattices.
1 Introduction
Current practice in hierarchical phrase-based trans-
lation extracts regular phrases and hierarchical rules
from word-aligned parallel text. Alignment models
estimated over the parallel text are used to generate
these alignments, but these models are then typically
used no further in rule extraction. This is less than
ideal because these alignment models, even if they
are not suitable for direct use in translation, can still
provide a great deal of useful information beyond a
single best estimate of the alignment of the parallel
text. Our aim is to use alignment models to generate
the statistics needed to build translation grammars.
The challenge in doing so is to extend the current
procedures, which are geared towards the use of a
single alignment, to make more of what can be pro-
vided by alignment models. The goal is to extract a
richer and more robust set of translation rules.
There are two aspects to hierarchical phrase-based
translation grammars which concern us. The first
is expressive power, which we take as the ability
to generate known reference translations from sen-
tences in the source language. This is determined
by the degree of phrase movements and the trans-
lations allowed by the rules of the grammar. For a
grammar with given types of rules, larger rule sets
will yield greater expressive power. This motivates
studies of grammars based on the rules which are ex-
tracted and the movement the grammar allows. The
second aspect is of course translation accuracy. If
the expressive power is adequate, then the desire is
that the grammar assigns a high score to a correct
translation.
We use posterior probabilities over parallel data to
address both of these aspects. These posteriors allow
us to build larger rule sets with improved transla-
tion accuracy. Ideally, for a sentence pair we wish to
consider all possible alignments between all possi-
ble source and target phrases within these sentences.
Given a grammar allowing certain types of move-
ment, we would then extract all possible parses that
are consistent with any alignments of these phrases.
545
To make this approach feasible, we consider only
phrase-to-phrase alignments with a high posterior
probability under the alignment models. In this way,
the alignment model probabilities guide rule extrac-
tion.
The paper is organized as follows. Section 2 re-
views related work on using posteriors to extract
phrases, as well as other approaches that tightly in-
tegrate word alignment and rule extraction. Sec-
tion 3 describes rule extraction based on word and
phrase posterior distributions provided by the HMM
word-to-word alignment model. In Section 4 we de-
fine translation grammars progressively by adding
classes of rules to a basic phrase-based system, mo-
tivating each rule type by the phrase movement it is
intended to achieve. In Section 5 we assess these
grammars in terms of their expressive power and the
quality of the translations they yield in Chinese-to-
English, showing that rule extraction from posteriors
gives translation improvements. We also find that
the best way to exploit source-to-target and target-
to-source alignment models is to build two sepa-
rate systems and combine their output translation
lattices. Section 6 presents the main conclusions of
this work.
2 Related Work
Some authors have previously addressed the limita-
tion caused by decoupling word alignment models
from grammar extraction. For instance Venugopal
et al (2008) extract rules from n-best lists of align-
ments for a syntax-augmented hierarchical system.
Alignment n-best lists are also used in Liu et al
(2009) to create a structure called weighted align-
ment matrices that approximates word-to-word link
posterior probabilities, from which phrases are ex-
tracted for a phrase-based system. Alignment pos-
teriors have been used before for extracting phrases
in non-hierarchical phrase-based translation (Venu-
gopal et al, 2003; Kumar et al, 2007; Deng and
Byrne, 2008).
In order to simplify hierarchical phrase-based
grammars and make translation feasible with rela-
tively large parallel corpora, some authors discuss
the need for various filters during rule extraction
(Chiang, 2007). In particular Lopez (2008) enforces
a minimum span of two words per nonterminal,
Zollmann et al (2008) use a minimum count thresh-
old for all rules, and Iglesias et al (2009) propose
a finer-grained filtering strategy based on rule pat-
terns. Other approaches include insisting that target-
side rules are well-formed dependency trees (Shen et
al., 2008).
We also note approaches to tighter coupling be-
tween translation grammars and alignments. Marcu
and Wong (2002) describe a joint-probability
phrase-based model for alignment, but the approach
is limited due to excessive complexity as Viterbi
inference becomes NP-hard (DeNero and Klein,
2008). More recently, Saers et al (2009) report
improvement on a phrase-based system where word
alignment has been trained with an inversion trans-
duction grammar (ITG) rather than IBM models.
Pauls et al (2010) also use an ITG to directly align
phrases to nodes in a string-to-tree model. Bayesian
methods have been recently developed to induce a
grammar directly from an unaligned parallel corpus
(Blunsom et al, 2008; Blunsom et al, 2009). Fi-
nally, Cmejrek et al (2009) extract rules directly
from bilingual chart parses of the parallel corpus
without using word alignments. We take a differ-
ent approach in that we aim to start with very strong
word alignment models and use them to guide gram-
mar extraction.
3 Rule Extraction from Alignment
Posteriors
The goal of rule extraction is to generate a set of
good-quality translation rules from a parallel cor-
pus. Rules are of the form X???,?,?? , where
?, ? ? {X ? T}+ are the source and target sides of
the rule, T denotes the set of terminals (words) and
? is a bijective function1 relating source and target
nonterminals X of each rule (Chiang, 2007). For
each ?, the probability over translations ? is set by
relative frequency over the extracted examples from
the corpus.
We take a general approach to rule extraction, as
described by the following procedure. For simplic-
ity we discuss the extraction of regular phrases, that
is, rules of the form X??w,w?, where w ? {T}+.
Section 3.3 extends this procedure to rules with non-
1This function is defined if there are at least two nontermi-
nals, and for clarity of presentation will be omitted in this paper
546
terminal symbols.
Given a sentence pair (fJ1 , eI1), the extraction al-
gorithm traverses the source sentence and, for each
sequence of terminals f j2j1 , it considers all possible
target-side sequences ei2i1 as translation candidates.
Each target-side sequence that satisfies the align-
ment constraints CA is ranked by the function fR.
For practical reasons, a set of selection criteria CS is
then applied to these ranked candidates and defines
the set of translations of the source sequence that are
extracted as rules. Each extracted rule is assigned a
count fC .
In this section we will explore variations of this
rule extraction procedure involving alternative def-
initions of the ranking and counting functions, fR
and fC , based on probabilities over alignment mod-
els.
Common practice (Koehn et al, 2003) takes a set
of word alignment links L and defines the alignment
constraints CA so that there is a consistency between
the links in the (f j2j1 , e
i2
i1) phrase pair. This is ex-
pressed by ?(j, i) ? L : (j ? [j1, j2]? i ? [i1, i2])?
(j 6? [j1, j2] ? i 6? [i1, i2]). If these constraints
are met, then alignment probabilities are ignored and
fR = fC = 1. We call this extraction Viterbi-based,
as the set of alignment links is generally obtained
after applying a symmetrization heuristic to source-
to-target and target-to-source Viterbi alignments.
In the following section we depart from this ap-
proach and apply novel functions to rank and count
target-side translations according to their quality in
the context of each parallel sentence, as defined by
the word alignment models. We also depart from
common practice in that we do not use a set of links
as alignment constraints. We thus find an increase
in the number of extracted rules, and consequently
better relative frequency estimates over translations.
3.1 Ranking and Counting Functions
We describe two alternative approaches to modify
the functions fR and fC so that they incorporate the
probabilities provided by the alignment models.
3.1.1 Word-to-word Alignment Posterior
Probabilities
Word-to-word alignment posterior probabilities
p(lji|fJ1 , eI1) express how likely it is that the words
in source position j and target position i are aligned
given a sentence pair. These posteriors can be effi-
ciently computed for Model 1, Model 2 and HMM,
as described in (Brown et al, 1993; Venugopal et al,
2003; Deng and Byrne, 2008).
We will use these posteriors in functions to
score phrase pairs. For a simple non-disjoint case
(f j2j1 , e
i2
i1) we use:
fR(f j2j1 , e
i2
i1) =
j2
?
j=j1
i2
?
i=i1
p(lji|fJ1 , eI1)
i2 ? i1 + 1
(1)
which is very similar to the score used for lexical
features in many systems (Koehn, 2010), with the
link posteriors for the sentence pair playing the role
of the Model 1 translation table.
For a particular source phrase, Equation 1 is not
a proper conditional probability distribution over all
phrases in the target sentence. Therefore it cannot be
used as such without further normalization. Indeed
we find that this distribution is too sharp and over-
emphasises short phrases, so we use fC = 1. How-
ever, it does allow us to rank target phrases as pos-
sible translations. In contrast to the common extrac-
tion procedure described in the previous section, the
ranking approach described here can lead to a much
more exhaustive extraction unless selection criteria
are applied. These we describe in Section 3.2.
We note that Equation 1 can be computed us-
ing link posteriors provided by alignment models
trained on either source-to-target or target-to-source
translation directions.
3.1.2 Phrase-to-phrase Alignment Posterior
Probabilities
Rather than limit ourselves to word-to-word
link posteriors we can define alignment proba-
bility distributions over phrase alignments. We
do this by defining the set of alignments A as
A(j1, j2; i1, i2) = {aJ1 : aj ? [i1, i2] iff j ?
[j1, j2]}, where aj is the random process that de-
scribes word-to-word alignments. These are the
alignments from which the phrase pair (f j2j1 , e
i2
i1)
would be extracted.
The posterior probability of these alignments
given the sentence pair is defined as follows:
p(A|eI1, fJ1 ) =
?
aJ1?A
p(fJ1 , aJ1 |eI1)
?
aJ1
p(fJ1 , aJ1 |eI1)
(2)
547
G0 G1 G2 G3
S??X,X? X??w X,X w? X??w X,X w? X??w X,X w?
S??S X,S X? X??X w,w X? X??X w,w X? X??X w,w X?
X??w,w? X??w X,w X? X??w X,w X?
X??w X w,w X w?
Table 1: Hierarchical phrase-based grammars containing different types of rules. The grammar expressivity is greater
as more types of rules are included. In addition to the rules shown in the respective columns, G1, G2 and G3 also
contain the rules of G0.
With IBM models 1 and 2, the numerator and de-
nominator in Equation 2 can be computed in terms
of posterior link probabilities (Deng, 2005). With
the HMM model, the denominator is computed us-
ing the forward algorithm while the numerator can
be computed using a modified forward algorithm
(Deng, 2005).
These phrase posteriors directly define a proba-
bility distribution over the alignments of translation
candidates, so we use them both for ranking and
scoring extracted rules, that is fR = fC = p. This
approach assigns a fractional count to each extracted
rule, which allows finer estimation of the forward
and backward translation probability distributions.
3.2 Alignment Constraints and Selection
Criteria
In order to keep this process computationally
tractable, some extraction constraints are needed. In
order to extract a phrase pair (f j2j1 , e
i2
i1), we define
the following:
? CA requires at least one pair of positions (j, i) :
(j ? [j1, j2] ? i ? [i1, i2]) with word-to-word
link posterior probability p(lji|fJ1 , eI1) > 0.5,
and that there is no pair of positions (j, i) : (j ?
[j1, j2]?i 6? [i1, i2])?(j 6? [j1, j2]?i ? [i1, i2])
with p(lji|fJ1 , eI1) > 0.5
? CS allows only the k best translation candidates
to be extracted. We use k = 3 for regular
phrases, and k = 2 for hierarchical rules.
Note that we do not discard rules according to
their scores fC at this point (unlike Liu et al
(2009)), since we prefer to add all phrases from
all sentence pairs before carrying out such filtering
steps.
Once all rules over the entire collection of paral-
lel sentences have been extracted, we require each
rule to occur at least nobs times and with a forward
translation probability p(?|?) > 0.01 to be used for
translation.
3.3 Extraction of Rules with Nonterminals
Extending the procedure previously described to
the case of more complex hierarchical rules includ-
ing one or even two nonterminals is conceptually
straightforward. It merely requires that we traverse
the source and target sentences and consider possi-
bly disjoint phrase pairs. Optionally, the alignment
constraints can also be extended to apply on the non-
terminal X.
Equation 1 is then only modified in the limits
of the product and summation, whereas Equation
2 remains unchanged, as long as the set of valid
alignments A is redefined. For example, for a rule
of the form X??w X w,w X w?, we use A ?
A(j1, j2; j3, j4; i1, i2; i3, i4).
4 Hierarchical Translation Grammar
Definition
In this section we define the hierarchical phrase-
based synchronous grammars we use for translation
experiments. Each grammar is defined by the type of
hierarchical rules it contains. The rule type can be
obtained by replacing every sequence of terminals
by a single symbol ?w?, thus ignoring the identity of
the words, but capturing its generalized structure and
the kind of reordering it encodes (this was defined as
rule pattern in Iglesias et al (2009)).
A monotonic phrase-based translation grammar
G0 can be defined as shown in the left-most col-
umn of Table 1; it includes all regular phrases, repre-
sented by the rule type X??w,w?, and the two glue
548
(G0) R1: S??X,X?
(G0) R2: X??s2 s3,t2?
(G1) R3: X??s1 X,X t3?
(G1) R4: X??X s4,t1 X?
(G2) R5: X??s1 X,t7 X?
(G3) R6: X??s1 X s4,t5 X t6?
Figure 1: Example of a hierarchical translation grammar and two parsing trees following alternative rule derivations
for the input sentence s1s2s3s4.
rules that allow concatenation. Our approach is now
simple: we extend this grammar by successively in-
corporating sets of hierarchical rules. The goal is to
obtain a grammar with few rule types but which is
capable of generating a rich set of translation candi-
dates for a given input sentence.
With this in mind, we define the following three
grammars, also summarized in Table 1:
? G1 := G0
?
{ X??w X,X w? , X??X w,w X? }. This
incorporates reordering capabilities with two
rule types that place the unique nonterminal
in an opposite position in each language; we
call these ?phrase swap rules?. Since all non-
terminals are of the same category X, nested
reordering is possible. However, this needs to
happen consecutively, i.e. a swap must apply
after a swap, or the rule is concatenated with
the glue rule.
? G2 := G1
?{ X??w X,w X? }. This
adds monotonic concatenation capabilities to
the previous translation grammar. The glue rule
already allows rule concatenation. However, it
does so at the S category, that is, it concate-
nates phrases and rules after they have been re-
ordered, in order to complete a sentence. With
this new rule type, G2 allows phrase/rule con-
catenation before reordering with another hier-
archical rule. Therefore, nested reordering does
not require successive swaps anymore.
? G3 := G2
?{ X??w X w,w X w? }. This
adds single nonterminal rules with disjoint ter-
minal sequences, which can encode a mono-
tonic or reordered relationship between them,
depending on what their alignment was in the
parallel corpus. Although one could expect the
movement captured by this phrase-disjoint rule
type to be also present in G2 (via two swaps or
one concatenation plus one swap), the terminal
sequences w may differ.
Figure 1 shows an example set of rules indicat-
ing to which of the previous grammars each rule be-
longs, and shows three translation candidates as gen-
erated by grammars G1 (left-most tree), G2 (mid-
dle tree) and G3 (right-most tree). Note that the
middle tree cannot be generated with G1 as it re-
quires monotonic concatenation before reordering
with rule R4.
The more rule types a hierarchical grammar con-
tains, the more different rule derivations and the
greater the search space of alternative translation
candidates. This is also connected to how many
rules are extracted per rule type. Ideally we would
like the grammar to be able to generate the correct
translation of a given input sentence, without over-
generating too many other candidates, as that makes
the translation task more difficult.
We will make use of the parallel data in measuring
the ability of a grammar to generate correct transla-
tions. By extracting rules from a parallel sentence,
we translate them and observe whether the transla-
tion grammar is able to produce the parallel target
translation. In Section 5.1 we evaluate this for a
Chinese-to-English task.
549
4.1 Reducing Grammar Redundancy
Let us discuss grammar G2 in more detail. As de-
scribed in the previous section, the motivation for in-
cluding rule type X??w X,w X? is that the gram-
mar be able to carry out monotonic concatenation
before applying another hierarchical rule with re-
ordering. This movement is permitted by this rule
type, but the use of a single nonterminal category X
also allows the grammar to apply the concatenation
after reordering, that is, immediately before the glue
rule is applied. This creates significant redundancy
in rule derivations, as this rule type is allowed to act
as a glue rule. For example, given an input sentence
s1s2 and the following simple grammar:
R0: S??X,X?
R1: S??S X,S X?
R2: X??s1,t1?
R3: X??s2,t2?
R4: X??s1 X,t1 X?
two derivations are possible: R2,R0,R3,R1 and
R3,R4,R0, and the translation result is identical.
To avoid this situation we introduce a nonterminal
M in the left-hand side of monotonic concatenation
rules of G2. All rules are allowed to use nontermi-
nals X and M in their right-hand side, except the
glue rules, which can only take X. In the context of
our example, R4 is substituted by:
R4a: M??s1 X,t1 X?
R4b: M??s1 M ,t1 M?
so that only the first derivation is possible:
R2,R0,R3,R1, because applying R3,R4a yields a non-
terminal M that cannot be taken by the glue rule R0.
5 Experiments
We report experiments in Chinese-to-English trans-
lation. Our system is trained on a subset of the
GALE 2008 evaluation parallel text;2 this is approx-
imately 50M words per language. We report trans-
lation results on a development set tune-nw and a
test set test-nw1. These contain translations pro-
duced by the GALE program and portions of the
newswire sections of MT02 through MT06. They
contain 1,755 sentences and 1,671 sentences respec-
tively. Results are also reported on a smaller held-
2See http://projects.ldc.upenn.edu/gale/data/catalog.html.
We excluded the UN material and the LDC2002E18,
LDC2004T08, LDC2007E08 and CUDonga collections.
 30
 40
 50
 60
 70
 80
V-st
V-ts
V-union
V-gdf
V-merge
WP-st
WP-ts
WP-merge
G0
G1
G2
G3
Figure 2: Percentage of parallel sentences successfully
aligned for various extraction methods and grammars.
out test set test-nw2, containing 60% of the NIST
newswire portion of MT06, that is, 369 sentences.
The parallel texts for both language pairs are
aligned using MTTK (Deng and Byrne, 2008). For
decoding we use HiFST, a lattice-based decoder im-
plemented with Weighted Finite State Transducers
(de Gispert et al, 2010). Likelihood-based search
pruning is applied if the number of states in the
lattice associated with each CYK grid cell exceeds
10,000, otherwise the entire search space is ex-
plored. The language model is a 4-gram language
model estimated over the English side of the paral-
lel text and the AFP and Xinhua portions of the En-
glish Gigaword Fourth Edition (LDC2009T13), in-
terpolated with a zero-cutoff stupid-backoff (Brants
et al, 2007) 5-gram estimated using 6.6B words of
English newswire text. In tuning the systems, stan-
dard MERT (Och, 2003) iterative parameter estima-
tion under IBM BLEU3 is performed on the devel-
opment sets.
5.1 Measuring Expressive Power
We measure the expressive power of the grammars
described in the previous section by running the
translation system in alignment mode (de Gispert
et al, 2010) over the parallel corpus. Conceptually,
this is equivalent to replacing the language model by
the target sentence and seeing if the system is able to
find any candidate. Here the weights assigned to the
3See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl
550
Grammar Extraction # Rules tune-nw test-nw1 test-nw2
time prune BLEU BLEU BLEU
GH V-union 979149 3.7 0.3 35.1 35.6 37.6
V-union 613962 0.4 0.0 33.6 34.6 36.4
G1 WP-st 920183 0.9 0.0 34.3 34.8 37.5
PP-st 893542 1.4 0.0 34.4 35.1 37.7
V-union 734994 1.0 0.0 34.5 35.4 37.2
G2 WP-st 1132386 5.8 0.5 35.1 36.0 37.7
PP-st 1238235 7.8 0.7 35.5 36.4 38.2
V-union 966828 1.2 0.0 34.9 35.3 37.0
G3 WP-st 2680712 8.3 1.1 35.1 36.2 37.9
PP-st 5002168 10.7 2.6 35.5 36.4 38.5
Table 2: Chinese-to-English translation results with alternative grammars and extraction methods (lower-cased BLEU
shown). Time (secs/word) and prune (times/word) measurements done on tune-nw set.
rules are irrelevant, as only the ability of the gram-
mar to create a desired hypothesis is important.
We compare the percentage of target sentences
that can be successfully produced by grammars G0,
G1, G2 and G3 for the following extraction meth-
ods:
? Viterbi (V). This is the standard extraction
method based on a set of alignment links. We
distinguish four cases, depending on the model
used to obtain the set of links: source-to-
target (V-st), target-to-source (V-ts), and two
common symmetrization strategies: union (V-
union) and grow-diag-final (V-gdf), described
in (Koehn et al, 2003).
? Word Posteriors (WP). The extraction method
is based on word alignment posteriors de-
scribed in Section 3.1.1. These rules can be ob-
tained either from the posteriors of the source-
to-target (WP-st) or the target-to-source (WP-
ts) alignment models. We apply the alignment
constraints and selection criteria described in
Section 3.2. We do not report alignment per-
centages when using phrase posteriors (as de-
scribed in Section 3.1.2) as they are roughly
identical to the WP case.
? Finally, in both cases, we also report results
when merging the extracted rules in both direc-
tions into a single rule set (V-merge and WP-
merge).
Figure 2 shows the results obtained for a random
selection of 10,000 parallel corpus sentences. As ex-
pected, we can see that for any extraction method,
the percentage of aligned sentences increases when
switching from G0 to G1, G2 and G3. Posterior-
based extraction is shown to outperform standard
methods based on a Viterbi set of alignments for
nearly all grammars. The highest alignment percent-
ages are obtained when merging rules obtained un-
der models trained in each direction (WP-merge),
approximately reaching 80% for grammar G3.
The maximum rule span in alignment was al-
lowed to be 15 words, so as to be similar to transla-
tion, where the maximum rule span is 10 words. Re-
laxing this in alignment to 30 words yields approxi-
mately 90% coverage for WP-merge under G3.
We note that if alignment constraints CA and se-
lection criteria CS were not applied, that is k = ?,
then alignment percentages would be 100% even
for G0, but the extracted grammar would include
many noisy rules with poor generalization power
and would suffer from overgeneration.
5.2 Translation Results
In this section we investigate the translation perfor-
mance of each hierarchical grammar, as defined by
rules obtained from three rule extraction methods:
? Viterbi union (V-union). Standard rule extrac-
tion from the union of the source-to-target and
target-to-source alignment link sets.
551
? Word Posteriors (WP-st). Extraction based
on word posteriors as described in Section
3.1.1. The posteriors are provided by the
source-to-target algnment model. Alignment
constraints and selection criteria of Section 3.2
are applied, with nobs = 2.
? Phrase Posteriors (PP-st). Extraction based
on phrase alignment posteriors, as described
in Section 3.1.2, with fractional counts pro-
portional to the phrase probability under the
source-to-target algnment model. Alignment
constraints and selection criteria of Section 3.2
are applied, with nobs = 0.2.
Table 2 reports the translation results, as well as
the number of extracted rules in each case. It also
shows the following decoding statistics as measured
on the tune-nw set: decoding time in seconds per in-
put word, and number of instances of search pruning
(described in Section 5) per input word.
As a contrast, we extract rules according to the
heuristics introduced in (Chiang, 2007) and apply
the filters described in (Iglesias et al, 2009) to gen-
erate a standard hierarchical phrase-based grammar
GH . This uses rules with up to two nonadjacent non-
terminals, but excludes identical rule types such as
X??w X,w X? or X??w X1 w X2,w X1 w X2?,
which were reported to cause computational difficul-
ties without a clear improvement in translation (Igle-
sias et al, 2009).
Grammar expressivity. As expected, for the stan-
dard extraction method (see rows entitled V-union),
grammar G1 is shown to underperform all other
grammars due to its structural limitations. On the
other hand, grammar G2 obtains much better scores,
nearly generating the same translation quality as
the baseline grammar GH . Finally, G3 does not
prove able to outperform G2, which suggests that
the phrase-disjoint rules with one nonterminal are
redundant for the translation grammar.
Rule extraction method. For all grammars, we
find that the proposed extraction methods based on
alignment posteriors outperform standard Viterbi-
based extraction, with improvements ranging from
0.5 to 1.1 BLEU points for test-nw1 (depending on
the grammar) and from 1.0 to 1.5 for test-nw2. In
all cases, the use of phrase posteriors PP is the best
option. Interestingly, we find that G2 extracted with
WP and PP methods outperforms the more complex
GH grammar as obtained from Viterbi alignments.
Rule set statistics. For grammar G2 evaluated
on the tune-nw set, standard Viterbi-based extrac-
tion produces 0.7M rules, whereas the WP and PP
extraction methods yield 1.1M and 1.2M rules re-
spectively. We further analyse the sets of rules
X???,?,?? in terms of the number of distinct
source and target sequences ? and ? which are ex-
tracted. Viterbi extraction yields 82k distinct source
sequences whereas the WP and PP methods yield
116k and 146k sequences, respectively. In terms
of the average number of target sequences for each
source sequence, Viterbi extraction yields an aver-
age of 8.7 while WP and PP yield 9.7 and 8.4 rules
on average. This shows that method PP yields wider
coverage but with sharper forward rule translation
probability distributions than method WP, as the av-
erage number of translations per rule is determined
by the p(?|?) > 0.01 threshold mentioned in Sec-
tion 3.2.
Decoding time and pruning in search. In connec-
tion to the previous comments, we find an increased
need for search pruning, and subsequently slower
decoding speed, as the search space grows larger
with methods WP and PP. A larger search space is
created by the larger rule sets, which allows the sys-
tem to generate new hypotheses of better quality.
5.3 Rule Concatenation in Grammar G2
In Section 4.1 we described a strategy to reduce
grammar redundancy by introducing an additional
nonterminal M for monotonic concatenation rules.
We find that without this distinction among nonter-
minals, search pruning and decoding time are in-
creased by a factor of 1.5, and there is a slight degra-
dation in BLEU (?0.2) as more search errors are in-
troduced.
Another relevant aspect of this grammar is the ac-
tual rule type selected for monotonic concatenation.
We described using type X??w X,w X? (con-
catenation on the right), but one could also include
X??X w,X w? (concatenation on the left), or both,
for the same purpose. We evaluated the three alter-
natives and found that scores are identical when ei-
ther including right or left concatenation types, but
including both is harmful for performance, as the
need to prune and decoding time increase by a fac-
552
tor of 5 and 4, respectively, and we observe again a
slight degradation in performance.
Rule Extraction tune-nw test-nw1 test-nw2
V-st 34.7 35.6 37.5
V-ts 34.0 34.8 36.6
V-union 34.5 35.4 37.2
V-gdf 34.4 35.3 37.1
WP-st 35.1 36.0 37.7
WP-ts 34.5 35.0 37.0
PP-st 35.5 36.4 38.2
PP-ts 34.8 35.3 37.2
PP-merge 35.5 36.4 38.4
PP-merge-MERT 35.5 36.4 38.3
LMBR(V-st) 35.0 35.8 38.4
LMBR(V-st,V-ts) 35.5 36.3 38.9
LMBR(PP-st) 36.1 36.8 38.8
LMBR(PP-st,PP-ts) 36.4 36.9 39.3
Table 3: Translation results under grammar G2 with indi-
vidual rule sets, merged rule sets, and rescoring and sys-
tem combination with lattice-based MBR (lower-cased
BLEU shown)
5.4 Symmetrizing Alignments of Parallel Text
In this section we investigate extraction from align-
ments (and posterior distributions) over parallel text
which are generated using alignment models trained
in the source-to-target (st) and target-to-source (ts)
directions. Our motivation is that symmetrization
strategies have been reported to be beneficial for
Viterbi extraction methods (Och and Ney, 2003;
Koehn et al, 2003).
Results are shown in Table 3 for grammar G2. We
find that rules extracted under the source-to-target
alignment models (V-st, WP-st and PP-st) consis-
tently perform better than the V-ts, WP-ts and PP-
ts cases. Also, for Viterbi extraction we find that the
source-to-target V-st case performs better than any
of the symmetrization strategies, which contradicts
previous findings for non-hierarchical phrase-based
systems(Koehn et al, 2003).
We use the PP rule extraction method to extract
two sets of rules, under the st and ts alignment mod-
els respectively. We now investigate two ways of
merging these sets into a single grammar for trans-
lation. The first strategy is PP-merge and merges
both rule sets by assigning to each rule the maximum
count assigned by either alignment model. We then
extend the previous strategy by adding three binary
feature functions to the system, indicating whether
the rule was extracted under the ?st? model, the ?ts?
model or both. The motivation is that MERT can
weight rules differently according to the alignment
model they were extracted from. However, we do
not find any improvement with either strategy.
Finally, we use linearised lattice minimum Bayes-
risk decoding (Tromble et al, 2008; Blackwood et
al., 2010) to combine translation lattices (de Gis-
pert et al, 2010) as produced by rules extracted
under each alignment direction (see rows named
LMBR(V-st,V-ts) and LMBR(PP-st,PP-ts)). Gains
are consistent when comparing this to applying
LMBR to each of the best individual systems (rows
named LMBR(V-st) and LMBR(PP-st)). Overall,
the best-performing strategy is to extract two sets of
translation rules under the phrase pair posteriors in
each translation direction, and then to perform trans-
lation twice and merge the results.
6 Conclusion
Rule extraction based on alignment posterior proba-
bilities can generate larger rule sets. This results in
grammars with more expressive power, as measured
by the ability to align parallel sentences. Assign-
ing counts equal to phrase posteriors produces bet-
ter estimation of rule translation probabilities. This
results in improved translation scores as the search
space grows.
This more exhaustive rule extraction method per-
mits a grammar simplification, as expressed by the
phrase movement allowed by its rules. In particular
a simple grammar with rules of only one nontermi-
nal is shown to outperform a more complex gram-
mar built on rules extracted from Viterbi alignments.
Finally, we find that the best way to exploit align-
ment models trained in each translation direction is
to extract two rule sets based on alignment posteri-
ors, translate the input independently with each rule
set and combine translation output lattices.
Acknowledgments
This work was supported in part by the GALE pro-
gram of the Defense Advanced Research Projects
553
Agency, Contract No. HR0011- 06-C-0022.
References
Graeme Blackwood, Adria` de Gispert, and William
Byrne. 2010. Efficient Path Counting Transducers
for Minimum Bayes-Risk Decoding of Statistical Ma-
chine Translation Lattices. In Proceedings of ACL,
Short Papers, pages 27?32.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian Synchronous Grammar Induction. In Ad-
vances in Neural Information Processing Systems, vol-
ume 21, pages 161?168.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proceedings of the ACL, pages
782?790.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of EMNLP-ACL,
pages 858?867.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguis-
tics, 19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Martin Cmejrek, Bowen Zhou, and Bing Xiang. 2009.
Enriching SCFG Rules Directly From Efficient Bilin-
gual Chart Parsing. In Proceedings of IWSLT, pages
136?143.
Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hier-
archical phrase-based translation with weighted finite
state transducers and shallow-n grammars. Computa-
tional Linguistics, 36(3).
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL-
HLT, Short Papers, pages 25?28.
Yonggang Deng and William Byrne. 2008. HMM word
and phrase alignment for statistical machine transla-
tion. IEEE Transactions on Audio, Speech, and Lan-
guage Processing, 16(3):494?507.
Yonggang Deng. 2005. Bitext Alignment for Statisti-
cal Machine Translation. Ph.D. thesis, Johns Hopkins
University.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
the EACL, pages 380?388.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT/NAACL, pages 48?54.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Shankar Kumar, Franz J. Och, and Wolfgang Macherey.
2007. Improving word alignment with bridge lan-
guages. In Proceedings of EMNLP-CoNLL, pages 42?
50.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of EMNLP, pages 1017?
1026.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of COLING, pages
505?512.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP, pages 133?139.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160?167.
Adam Pauls, Dan Klein, David Chiang, and Kevin
Knight. 2010. Unsupervised syntactic alignment with
inversion transduction grammars. In Proceedings of
the HLT-NAACL, pages 118?126.
Markus Saers and Dekai Wu. 2009. Improving phrase-
based translation via word alignments from stochastic
inversion transduction grammars. In Proceedings of
the HLT-NAACL Workshop on Syntax and Structure in
Statistical Translation, pages 28?36.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-HLT, pages 577?585.
Roy Tromble, Shankar Kumar, Franz J. Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-Risk
decoding for statistical machine translation. In Pro-
ceedings of EMNLP, pages 620?629.
Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective phrase translation extraction from
alignment models. In Proceedings of ACL, pages 319?
326.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2008. Wider pipelines: N-best
alignments and parses in mt training. In Proceedings
of AMTA, pages 192?201.
Andreas Zollmann, Ashish Venugopal, Franz J. Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of COLING, pages 1145?1152.
554
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 155?160,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The CUED HiFST System for the WMT10 Translation Shared Task
Juan Pino Gonzalo Iglesias?1 Adria` de Gispert
Graeme Blackwood Jamie Brunning William Byrne
Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, U.K.
{jmp84,gi212,ad465,gwb24,jjjb2,wjb31}@eng.cam.ac.uk
? Department of Signal Processing and Communications, University of Vigo, Vigo, Spain
Abstract
This paper describes the Cambridge Uni-
versity Engineering Department submis-
sion to the Fifth Workshop on Statistical
Machine Translation. We report results for
the French-English and Spanish-English
shared translation tasks in both directions.
The CUED system is based on HiFST, a
hierarchical phrase-based decoder imple-
mented using weighted finite-state trans-
ducers. In the French-English task, we
investigate the use of context-dependent
alignment models. We also show that
lattice minimum Bayes-risk decoding is
an effective framework for multi-source
translation, leading to large gains in BLEU
score.
1 Introduction
This paper describes the Cambridge University
Engineering Department (CUED) system submis-
sion to the ACL 2010 Fifth Workshop on Statis-
tical Machine Translation (WMT10). Our trans-
lation system is HiFST (Iglesias et al, 2009a), a
hierarchical phrase-based decoder that generates
translation lattices directly. Decoding is guided
by a CYK parser based on a synchronous context-
free grammar induced from automatic word align-
ments (Chiang, 2007). The decoder is imple-
mented with Weighted Finite State Transducers
(WFSTs) using standard operations available in
the OpenFst libraries (Allauzen et al, 2007). The
use of WFSTs allows fast and efficient exploration
of a vast translation search space, avoiding search
errors in decoding. It also allows better integration
with other steps in our translation pipeline such as
5-gram language model (LM) rescoring and lattice
minimum Bayes-risk (LMBR) decoding.
1Now a member of the Department of Engineering, Uni-
versity of Cambridge, Cambridge, CB2 1PZ, U.K.
# Sentences # Tokens # Types
(A)Europarl+News-Commentary
FR 1.7 M 52.4M 139.7kEN 47.6M 121.6k
(B)Europarl+News-Commentary+UN
FR 8.7 M 277.9M 421.0kEN 241.4M 482.1k
(C)Europarl+News-Commentary+UN+Giga
FR 30.2 M 962.4M 2.4MEN 815.3M 2.7M
Table 1: Parallel data sets used for French-to-
English experiments.
We participated in the French-English and
Spanish-English translation shared tasks in each
translation direction. This paper describes the de-
velopment of these systems. Additionally, we re-
port multi-source translation experiments that lead
to very large gains in BLEU score.
The paper is organised as follows. Section 2
describes each step in the development of our sys-
tem for submission, from pre-processing to post-
processing. Section 3 presents and discusses re-
sults and Section 4 describes an additional experi-
ment on multi-source translation.
2 System Development
We built three French-English and two Spanish-
English systems, trained on different portions of
the parallel data sets available for this shared task.
Statistics for the different parallel sets are sum-
marised in Tables 1 and 2. No additional parallel
data was used. As will be shown, the largest paral-
lel corpus gave the best results in French, but this
was not the case in Spanish.
2.1 Pre-processing
The data was minimally cleaned by replacing
HTML-related metatags by their corresponding
155
# Sentences # Tokens # Types
(A) Europarl + News-Commentary
SP 1.7M 49.4M 167.2kEN 47.0M 122.7k
(B) Europarl + News-Commentary + UN
SP 6.5M 205.6M 420.8kEN 192.0M 402.8k
Table 2: Parallel data sets used for Spanish-to-
English experiments.
UTF8 token (e.g., replacing ?&amp? by ?&?) as
this interacts with tokenization. Data was then to-
kenized and lowercased, so mixed case is added as
post-processing.
2.2 Alignments
Parallel data was aligned using the MTTK toolkit
(Deng and Byrne, 2005). In the English-to-French
and English-to-Spanish directions, we trained
a word-to-phrase HMM model with maximum
phrase length of 2. In the French to English and
Spanish to English directions, we trained a word-
to-phrase HMM Model with a bigram translation
table and maximum phrase length of 4.
We also trained context-dependent alignment
models (Brunning et al, 2009) for the French-
English medium-size (B) dataset. The context of
a word is based on its part-of-speech and the part-
of-speech tags of the surrounding words. These
tags were obtained by applying the TnT Tagger
(Brants, 2000) for English and the TreeTagger
(Schmid, 1994) for French. Decision tree clus-
tering based on optimisation of the EM auxiliary
function was used to group contexts that trans-
late similarly. Unfortunately, time constraints pre-
vented us from training context-dependent models
for the larger (C) dataset.
2.3 Language Model
For each target language, we used the SRILM
Toolkit (Stolcke, 2002) to estimate separate 4-
gram LMs with Kneser-Ney smoothing (Kneser
and Ney, 1995), for each of the corpora listed in
Tables 3, 4 and 5. The LM vocabulary was ad-
justed to the parallel data set used. The compo-
nent models of each language pair were then in-
terpolated to form a single LM for use in first-pass
translation decoding. For French-to-English trans-
lation, the interpolation weights were optimised
for perplexity on a development set.
Corpus # Sentences # Tokens
EU + NC + UN 9.0M 246.4M
CNA 1.3M 34.8M
LTW 12.9M 298.7M
XIN 16.0M 352.5M
AFP 30.4M 710.6M
APW 62.1M 1268.6M
NYT 73.6M 1622.5M
Giga 21.4M 573.8M
News 48.7M 1128.4M
Total 275.4M 6236.4M
Table 3: English monolingual training corpora.
Corpus # Sentences # Tokens
EU + NC + UN 9.0M 282.8
AFP 25.2M 696.0M
APW 12.7M 300.6M
News 15.2M 373.5M
Giga 21.4M 684.4M
Total 83.5 M 2337.3M
Table 4: French monolingual training corpora.
Corpus # Sentences # Tokens
NC + News 4.0M 110.8M
EU + Gigaword (5g) 249.4M 1351.5M
Total 253.4 M 1462.3M
Table 5: Spanish monolingual training corpora.
The Spanish-English first pass LM was trained
directly on the NC+News portion of monolingual
data, as we did not find improvements by using
Europarl. The second pass rescoring LM used all
available data.
2.4 Grammar Extraction and Decoding
After unioning the Viterbi alignments, phrase-
based rules of up to five source words in length
were extracted, hierarchical rules with up to two
non-contiguous non-terminals in the source side
were then extracted applying the restrictions de-
scribed in (Chiang, 2007). For Spanish-English
and French-English tasks, we used a shallow-1
grammar where hierarchical rules are allowed to
be applied only once on top of phrase-based rules.
This has been shown to perform as well as a
fully hierarchical grammar for a Europarl Spanish-
English task (Iglesias et al, 2009b).
For translation, we used the HiFST de-
156
coder (Iglesias et al, 2009a). HiFST is a hierarchi-
cal decoder that builds target word lattices guided
by a probabilistic synchronous context-free gram-
mar. Assuming N to be the set of non-terminals
and T the set of terminals or words, then we can
define the grammar as a set R = {Rr} of rules
Rr : N ? ??r,?r? / pr, where N ? N; and
?, ? ? {N ? T}+.
HiFST translates in three steps. The first step
is a variant of the CYK algorithm (Chappelier and
Rajman, 1998), in which we apply hypothesis re-
combination without pruning. Only the source
language sentence is parsed using the correspond-
ing source-side context-free grammar with rules
N ? ?. Each cell in the CYK grid is specified
by a non-terminal symbol and position: (N,x, y),
spanning sx+y?1x on the source sentence s1...sJ .
For the second step, we use a recursive algo-
rithm to construct word lattices with all possi-
ble translations produced by the hierarchical rules.
Construction proceeds by traversing the CYK grid
along the back-pointers established in parsing. In
each cell (N,x, y) of the CYK grid, we build a
target language word lattice L(N,x, y) containing
every translation of sx+y?1x from every derivation
headed by N . For efficiency, this lattice can use
pointers to lattices on other cells of the grid.
In the third step, we apply the word-based LM
via standard WFST composition with failure tran-
sitions, and perform likelihood-based pruning (Al-
lauzen et al, 2007) based on the combined trans-
lation and LM scores.
As explained before, we are using shallow-1 hi-
erarchical grammars (de Gispert et al, 2010) in
our experiments for WMT2010. One very inter-
esting aspect is that HiFST is able to build ex-
act search spaces with this model, i.e. there is no
pruning in search that may lead to spurious under-
generation errors.
2.5 Parameter Optimisation
Minimum error rate training (MERT) (Och, 2003)
under the BLEU score (Papineni et al, 2001) opti-
mises the weights of the following decoder fea-
tures with respect to the newstest2008 develop-
ment set: target LM, number of usages of the
glue rule, word and rule insertion penalties, word
deletion scale factor, source-to-target and target-
to-source translation models, source-to-target and
target-to-source lexical models, and three binary
rule count features inspired by Bender et al (2007)
indicating whether a rule occurs once, twice, or
more than twice in the parallel training data.
2.6 Lattice Rescoring
One of the advantages of HiFST is direct gener-
ation of large translation lattices encoding many
alternative translation hypotheses. These first-pass
lattices are rescored with second-pass higher-order
LMs prior to LMBR.
2.6.1 5-gram LM Lattice Rescoring
We build sentence-specific, zero-cutoff stupid-
backoff (Brants et al, 2007) 5-gram LMs esti-
mated over approximately 6.2 billion words for
English, 2.3 billion words for French, and 1.4 bil-
lion words for Spanish. For the English-French
task, the second-pass LM training data is the same
monolingual data used for the first-pass LMs (as
summarised in Tables 3, 4). The Spanish second-
pass 5-gram LM includes an additional 1.4 billion
words of monolingual data from the Spanish Giga-
Word Second Edition (Mendonca et al, 2009) and
Europarl, which were not included in the first-pass
LM (see Table 5).
2.6.2 LMBR Decoding
Minimum Bayes-risk (MBR) decoding (Kumar
and Byrne, 2004) over the full evidence space
of the 5-gram rescored lattices was applied to
select the translation hypothesis that maximises
the conditional expected gain under the linearised
sentence-level BLEU score (Tromble et al, 2008;
Blackwood and Byrne, 2010). The unigram preci-
sion p and average recall ratio r were set as de-
scribed in Tromble et al (2008) using the new-
stest2008 development set.
2.7 Hypothesis Combination
Linearised lattice minimum Bayes-risk decoding
(Tromble et al, 2008) can also be used as an ef-
fective framework for multiple lattice combination
(de Gispert et al, 2010). For the French-English
language pair, we used LMBR to combine transla-
tion lattices produced by systems trained on alter-
native data sets.
2.8 Post-processing
For both Spanish-English and French-English sys-
tems, the recasing procedure was performed with
the SRILM toolkit. For the Spanish-English sys-
tem, we created models from the GigaWord set
corresponding to each system output language.
157
Task Configuration newstest2008 newstest2009 newstest2010
FR ? EN
HiFST (A) 23.4 26.4 ?
HiFST (B) 24.0 27.3 ?
HiFST (B)CD 24.2 27.6 28.0
+5g+LMBR 24.6 28.4 28.9
HiFST (C) 24.7 28.4 28.5
+5g+LMBR 25.3 29.1 29.3
LMBR (B)CD+(C) 25.6 29.3 29.6
EN ? FR
HiFST (A) 22.5 24.2 ?
HiFST (B) 23.4 24.8 ?
HiFST (B)CD 23.3 24.8 26.7
+5g+LMBR 23.7 25.3 27.1
HiFST (C) 23.6 25.6 27.4
+5g+LMBR 23.9 25.8 27.8
LMBR (B)CD+(C) 24.2 26.1 28.2
Table 6: Translation Results for the French-English (FR-EN) language pair, shown in single-reference
lowercase IBM BLEU. Bold results correspond to submitted systems.
For the French-English system, the English model
was trained using the monolingual News corpus
and the target side of the News-Commentary cor-
pus, whereas the French model was trained using
all available constrained French data.
English, Spanish and French outputs were also
detokenized before submission. In French, words
separated by apostrophes were joined.
3 Results and Discussion
French?English Language Pair
Results are reported in Table 6. We can see
that using more parallel data consistently improves
performance. In the French-to-English direction,
the system HiFST (B) improves over HiFST (A)
by +0.9 BLEU and HiFST (C) improves over
HiFST (B) by +1.1 BLEU on the newstest2009
development set prior to any rescoring. The
same trend can be observed in the English-to-
French direction (+0.6 BLEU and +0.8 BLEU im-
provement). The use of context dependent align-
ment models gives a small improvement in the
French-to-English direction: system (B)CD im-
proves by +0.3 BLEU over system (B) on new-
stest2009. In the English-to-French direction,
there is no improvement nor degradation in per-
formance. 5-gram and LMBR rescoring also give
consistent improvement throughout the datasets.
Finally, combination between the medium-size
system (B)CD and the full-size system (C) gives
further small gains in BLEU over LMBR on each
individual system.
Spanish?English Language Pair
Results are reported in Table 7. We report experi-
mental results on two systems. The HiFST(A) sys-
tem is built on the Europarl + News-Commentary
training set. Systems HiFST (B),(B2) and (B3)
use UN data in different ways. System (B) simply
uses all the data for the standard rule extraction
procedure. System HiFST (B2) includes UN data
to build alignment models and therefore reinforce
alignments obtained from smaller dataset (A), but
extracts rules only from dataset (A). HiFST (B3)
combines hierarchical phrases extracted for sys-
tem (A) with phrases extracted from system (B).
Unfortunately, these three larger data strategies
lead to degradation over using only the smaller
dataset (A). For this reason, our best systems only
use the Euparl + News-Commentary parallel data.
This is surprising given that additional data was
helpful for the French-English task. Solving this
issue is left for future work.
4 Multi-Source Translation Experiments
Multi-source translation (Och and Ney, 2001;
Schroeder et al, 2009) is possible whenever mul-
tiple translations of the source language input sen-
tence are available. The motivation for multi-
source translation is that some of the ambiguity
that must be resolved in translating between one
pair of languages may not be present in a differ-
ent pair. In the following experiments, multiple
LMBR is applied for the first time to the task of
multi-source translation.
158
Task Configuration newstest2008 newstest2009 newstest2010
SP ? EN
HiFST (A) 24.6 26.0 29.1
+5g+LMBR 25.4 27.0 30.5
HiFST (B) 23.7 25.4 ?
HiFST (B2) 24.3 25.7 ?
HiFST (B3) 24.2 25.6 ?
EN ? SP HiFST (A) 23.9 24.5 28.0
+5g+LMBR 24.7 25.5 29.1
Table 7: Translation Results for the Spanish-English (SP-EN) language pair, shown in lowercase IBM
BLEU. Bold results correspond to submitted systems.
Configuration newstest2008 newstest2009 newstest2010
FR?EN HiFST+5g 24.8 28.5 28.8
+LMBR 25.3 29.0 29.2
ES?EN HiFST+5g 25.2 26.8 30.1
+LMBR 25.4 26.9 30.3
FR?EN + ES?EN LMBR 27.2 30.4 32.0
Table 8: Lowercase IBM BLEU for single-system LMBR and multiple LMBR multi-source translation
of French (FR) and Spanish (ES) into English (EN).
Separate second-pass 5-gram rescored lattices
EFR and EES are generated for each test set sen-
tence using the French-to-English and Spanish-to-
English HiFST translation systems. The MBR hy-
pothesis space is formed as the union of these lat-
tices. In a similar manner to MBR decoding over
multiple k-best lists in de Gispert et al (2009),
the path posterior probability of each n-gram u re-
quired for linearised LMBR is computed as a lin-
ear interpolation of the posterior probabilities ac-
cording to each individual lattice so that p(u|E) =
?FR p(u|EFR) + ?ES p(u|EES), where p(u|E) is the
sum of the posterior probabilities of all paths con-
taining the n-gram u. The interpolation weights
?FR + ?ES = 1 are optimised for BLEU score on
the development set newstest2008.
The results of single-system and multi-source
LMBR decoding are shown in Table 8. The opti-
mised interpolation weights were ?FR = 0.55 and
?ES = 0.45. Single-system LMBR gives relatively
small gains on these test sets. Much larger gains
are obtained through multi-source MBR combina-
tion. Compared to the best of the single-system 5-
gram rescored lattices, the BLEU score improves
by +2.0 for newstest2008, +1.9 for newstest2009,
and +1.9 for newstest2010. For scoring with re-
spect to a single reference, these are very large
gains indeed.
5 Summary
We have described the CUED submission to
WMT10 using HiFST, a hierarchical phrase-based
translation system. Results are very competitive in
terms of automatic metric for both English-French
and English-Spanish tasks in both directions. In
the French-English task, we have seen that the UN
and Giga additional parallel data are helpful. It
is surprising that UN data did not help for the
Spanish-English language pair.
Future work includes investigating this issue,
developing detokenization tailored to each output
language and applying context dependent align-
ment models to larger parallel datasets.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7-ICT-2009-4) under grant
agreement number 247762, and was supported in
part by the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-C-0022. Gonzalo Iglesias was sup-
ported by the Spanish Government research grant
BES-2007-15956 (projects TEC2006-13694-C03-
03 and TEC2009-14094-C04-04).
159
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11?23.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language
translation system. In Proceedings of ASRU, pages
396?401.
Graeme Blackwood and William Byrne. 2010. Ef-
ficient Path Counting Transducers for Minimum
Bayes-Risk Decoding of Statistical Machine Trans-
lation Lattices (to appear). In Proceedings of the
ACL.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP-ACL, pages 858?867.
Thorsten Brants. 2000. Tnt ? a statistical part-of-
speech tagger. In Proceedings of ANLP, pages 224?
231, April.
Jamie Brunning, Adria` de Gispert, and William Byrne.
2009. Context-dependent alignment models for
statistical machine translation. In Proceedings of
HLT/NAACL, pages 110?118.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133?137.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Adria` de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes Risk Com-
bination of Translation Hypotheses from Alterna-
tive Morphological Decompositions. In Proceed-
ings of HLT/NAACL, Companion Volume: Short Pa-
pers, pages 73?76.
Adria` de Gispert, Gonzalo Iglesias, Graeme Black-
wood, Eduardo R. Banga, and William Byrne. 2010.
Hierarchical phrase-based translation with weighted
finite state transducers and shallow-n grammars (to
appear). In Computational Linguistics.
Yonggang Deng and William Byrne. 2005. HMM
Word and Phrase Alignment for Statistical Machine
Translation. In Proceedings of HLT/EMNLP, pages
169?176.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009a. Hierarchical phrase-
based translation with weighted finite state transduc-
ers. In Proceedings of NAACL, pages 433?441.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009b. The HiFST System for
the EuroParl Spanish-to-English Task. In Proceed-
ings of SEPLN, pages 207?214.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of ICASSP, volume 1, pages 181?184.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Angelo Mendonca, David Graff, and Denise DiPersio.
2009. Spanish Gigaword Second Edition, Linguistic
Data Consortium.
Franz Josef Och and Hermann Ney. 2001. Statisti-
cal multi-source translation. In Machine Translation
Summit 2001, pages 253?258.
Franz J. Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, volume 12. Manchester, UK.
Josh Schroeder, Trevor Cohn, and Philipp Koehn.
2009. Word Lattices for Multi-Source Translation.
In Proceedings of EACL, pages 719?727.
Andreas Stolcke. 2002. SRILM?An Extensible Lan-
guage Modeling Toolkit. In Proceedings of ICSLP,
volume 3, pages 901?904.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice Minimum
Bayes-Risk decoding for statistical machine trans-
lation. In Proceedings of EMNLP, pages 620?629.
160
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 200?205,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The University of Cambridge Russian-English System at WMT13
Juan Pino Aurelien Waite Tong Xiao
Adria` de Gispert Federico Flego William Byrne
Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, UK
{jmp84,aaw35,tx212,ad465,ff257,wjb31}@eng.cam.ac.uk
Abstract
This paper describes the University of
Cambridge submission to the Eighth
Workshop on Statistical Machine Transla-
tion. We report results for the Russian-
English translation task. We use mul-
tiple segmentations for the Russian in-
put language. We employ the Hadoop
framework to extract rules. The decoder
is HiFST, a hierarchical phrase-based de-
coder implemented using weighted finite-
state transducers. Lattices are rescored
with a higher order language model and
minimum Bayes-risk objective.
1 Introduction
This paper describes the University of Cam-
bridge system submission to the ACL 2013
Eighth Workshop on Statistical Machine Transla-
tion (WMT13). Our translation system is HiFST
(Iglesias et al, 2009), a hierarchical phrase-based
decoder that generates translation lattices directly.
Decoding is guided by a CYK parser based on a
synchronous context-free grammar induced from
automatic word alignments (Chiang, 2007). The
decoder is implemented with Weighted Finite
State Transducers (WFSTs) using standard op-
erations available in the OpenFst libraries (Al-
lauzen et al, 2007). The use of WFSTs allows
fast and efficient exploration of a vast translation
search space, avoiding search errors in decoding.
It also allows better integration with other steps
in our translation pipeline such as 5-gram lan-
guage model (LM) rescoring and lattice minimum
Bayes-risk (LMBR) decoding (Blackwood, 2010).
We participate in the Russian-English transla-
tion shared task in the Russian-English direction.
This is the first time we train and evaluate a sys-
tem on this language pair. This paper describes the
development of the system.
The paper is organised as follows. Section 2
describes each step in the development of our sys-
tem for submission, from pre-processing to post-
processing and Section 3 presents and discusses
results.
2 System Development
2.1 Pre-processing
We use all the Russian-English parallel data avail-
able in the constraint track. We filter out non
Russian-English sentence pairs with the language-
detection library.2 A sentence pair is filtered out if
the language detector detects a different language
with probability more than 0.999995 in either the
source or the target. This discards 78543 sen-
tence pairs. In addition, sentence pairs where the
source sentence has no Russian character, defined
by the Perl regular expression [\x0400-\x04ff],
are discarded. This further discards 19000 sen-
tence pairs.
The Russian side of the parallel corpus is to-
kenised with the Stanford CoreNLP toolkit.3 The
Stanford CoreNLP tokenised text is additionally
segmented with Morfessor (Creutz and Lagus,
2007) and with the TreeTagger (Schmid, 1995).
In the latter case, we replace each token by its
stem followed by its part-of-speech. This of-
fers various segmentations that can be taken ad-
vantage of in hypothesis combination: CoreNLP,
CoreNLP+Morfessor and CoreNLP+TreeTagger.
The English side of the parallel corpus is tokenised
with a standard in-house tokeniser. Both sides of
the parallel corpus are then lowercased, so mixed
case is restored in post-processing.
Corpus statistics after filtering and for various
segmentations are summarised in Table 1.
2http://code.google.com/p/language-detection/
3http://nlp.stanford.edu/software/corenlp.shtml
200
Lang Segmentation # Tokens # Types
RU CoreNLP 47.4M 1.2M
RU Morfessor 50.0M 0.4M
RU TreeTagger 47.4M 1.5M
EN Cambridge 50.4M 0.7M
Table 1: Russian-English parallel corpus statistics
for various segmentations.
2.2 Alignments
Parallel data is aligned using the MTTK toolkit
(Deng and Byrne, 2008). We train a word-
to-phrase HMM model with a maximum phrase
length of 4 in both source-to-target and target-to-
source directions. The final alignments are ob-
tained by taking the union of alignments obtained
in both directions.
2.3 Rule Extraction and Retrieval
A synchronous context-free grammar (Chiang,
2007) is extracted from the alignments. The con-
straints are set as in the original publication with
the following exceptions:
? phrase-based rule maximum number of
source words: 9
? maximum number of source element (termi-
nal or nonterminal): 5
? maximum span for nonterminals: 10
Maximum likelihood estimates for the transla-
tion probabilities are computed using MapReduce.
We use a custom Hadoop-based toolkit which im-
plements method 3 of Dyer et al (2008). Once
computed, the model parameters are stored on disk
in the HFile format (Pino et al, 2012) for fast
querying. Rule extraction and feature computa-
tion takes about 2h30. The HFile format requires
data to be stored in a key-value structure. For the
key, we use shared source side of many rules. The
value is a list of tuples containing the possible tar-
gets for the source key and the associated param-
eters of the full rule. The query set of keys for
the test set is all possible source phrases (includ-
ing nonterminals) found in the test set.
During HFile querying we add other features.
These include IBM Model 1 (Brown et al, 1993)
lexical probabilities. Loading these models in
memory doesn?t fit well with the MapReduce
model so lexical features are computed for each
test set rather than for the entire parallel corpus.
The model parameters are stored in a client-server
based architecture. The client process computes
the probability of the rule by querying the server
process for the Model 1 parameters. The server
process stores the model parameters completely
in memory so that parameters are served quickly.
This architecture allows for many low-memory
client processes across many machines.
2.4 Language Model
We used the KenLM toolkit (Heafield et al, 2013)
to estimate separate 4-gram LMs with Kneser-Ney
smoothing (Kneser and Ney, 1995), for each of the
corpora listed in Tables 2 (self-explanatory abbre-
viations). The component models were then in-
terpolated with the SRILM toolkit (Stolcke, 2002)
to form a single LM for use in first-pass trans-
lation decoding. The interpolation weights were
optimised for perplexity on the news-test2008,
newstest2009 and newssyscomb2009 development
sets. The weights reflect both the size of the com-
ponent models and the genre of the corpus the
component models are trained on, e.g. weights are
larger for larger corpora in the news genre.
Corpus # Tokens
EU + NC + UN + CzEng + Yx 652.5M
Giga + CC + Wiki 654.1M
News Crawl 1594.3M
afp 874.1M
apw 1429.3M
cna + wpb 66.4M
ltw 326.5M
nyt 1744.3M
xin 425.3M
Total 7766.9M
Table 2: Statistics for English monolingual cor-
pora.
2.5 Decoding
For translation, we use the HiFST decoder (Igle-
sias et al, 2009). HiFST is a hierarchical decoder
that builds target word lattices guided by a prob-
abilistic synchronous context-free grammar. As-
suming N to be the set of non-terminals and T the
set of terminals or words, then we can define the
grammar as a set R = {R} of rules R : N ?
??,?? / p, where N ? N, ?, ? ? {N ?T}+ and p
the rule score.
201
HiFST translates in three steps. The first step
is a variant of the CYK algorithm (Chappelier and
Rajman, 1998), in which we apply hypothesis re-
combination without pruning. Only the source
language sentence is parsed using the correspond-
ing source-side context-free grammar with rules
N ? ?. Each cell in the CYK grid is specified
by a non-terminal symbol and position: (N, x, y),
spanning sx+y?1x on the source sentence s1...sJ .
For the second step, we use a recursive algo-
rithm to construct word lattices with all possi-
ble translations produced by the hierarchical rules.
Construction proceeds by traversing the CYK grid
along the back-pointers established in parsing. In
each cell (N, x, y) of the CYK grid, we build a
target language word lattice L(N, x, y) containing
every translation of sx+y?1x from every derivation
headed by N . For efficiency, this lattice can use
pointers to lattices on other cells of the grid.
In the third step, we apply the word-based LM
via standard WFST composition with failure tran-
sitions, and perform likelihood-based pruning (Al-
lauzen et al, 2007) based on the combined trans-
lation and LM scores.
We are using shallow-1 hierarchical gram-
mars (de Gispert et al, 2010) in our experiments.
This model is constrained enough that the decoder
can build exact search spaces, i.e. there is no prun-
ing in search that may lead to spurious undergen-
eration errors.
2.6 Features and Parameter Optimisation
We use the following standard features:
? language model
? source-to-target and target-to-source transla-
tion scores
? source-to-target and target-to-source lexical
scores
? target word count
? rule count
? glue rule count
? deletion rule count (each source unigram, ex-
cept for OOVs, is allowed to be deleted)
? binary feature indicating whether a rule is ex-
tracted once, twice or more than twice (Ben-
der et al, 2007)
No alignment information is used when com-
puting lexical scores as done in Equation (4) in
(Koehn et al, 2005). Instead, the source-to-target
lexical score is computed in Equation 1:
s(ru, en) = 1(E + 1)R
R?
r=1
E?
e=0
pM1(ene|rur)
(1)
where ru are the terminals in the Russian side of
a rule, en are the terminals in the English side of
a rule, including the null word, R is the number
of Russian terminals, E is the number of English
terminals and pM1 is the IBM Model 1 probability.
In addition to these standard features, we also
use provenance features (Chiang et al, 2011). The
parallel data is divided into four subcorpora: the
Common Crawl (CC) corpus, the News Commen-
tary (NC) corpus, the Yandex (Yx) corpus and the
Wiki Headlines (Wiki) corpus. For each of these
subcorpora, source-to-target and target-to-source
translation and lexical scores are computed. This
requires computing IBM Model 1 for each sub-
corpus. In total, there are 28 features, 12 standard
features and 16 provenance features.
When retrieving relevant rules for a particular
test set, various thresholds are applied, such as
number of targets per source or translation prob-
ability cutoffs. Thresholds involving source-to-
target translation scores are applied separately for
each provenance and the union of all surviving
rules for each provenance is kept. This strategy
gives slight gains over using thresholds only for
the general translation table.
We use an implementation of lattice minimum
error rate training (Macherey et al, 2008) to op-
timise under the BLEU score (Papineni et al,
2001) the feature weights with respect to the odd
sentences of the newstest2012 development set
(newstest2012.tune). The weights obtained match
our expectation, for example, the source-to-target
translation feature weight is higher for the NC cor-
pus than for other corpora since we are translating
news.
2.7 Lattice Rescoring
The HiFST decoder is set to directly generate
large translation lattices encoding many alterna-
tive translation hypotheses. These first-pass lat-
tices are rescored with second-pass higher-order
LMs prior to LMBR.
202
2.7.1 5-gram LM Lattice Rescoring
We build a sentence-specific, zero-cutoff stupid-
backoff (Brants et al, 2007) 5-gram LMs esti-
mated over the data described in section 2.4. Lat-
tices obtained by first-pass decoding are rescored
with this 5-gram LM (Blackwood, 2010).
2.7.2 LMBR Decoding
Minimum Bayes-risk decoding (Kumar and
Byrne, 2004) over the full evidence space of the 5-
gram rescored lattices is applied to select the trans-
lation hypothesis that maximises the conditional
expected gain under the linearised sentence-level
BLEU score (Tromble et al, 2008; Blackwood,
2010). The unigram precision p and average re-
call ratio r are set as described in Tromble et al
(2008) using the newstest2012.tune development
set.
2.8 Hypothesis Combination
LMBR decoding (Tromble et al, 2008) can also be
used as an effective framework for multiple lattice
combination (Blackwood, 2010). We used LMBR
to combine translation lattices produced by sys-
tems trained on alternative segmentations.
2.9 Post-processing
Training data is lowercased, so we apply true-
casing as post-processing. We used the disam-
big tool provided by the SRILM toolkit (Stolcke,
2002). The word mapping model which contains
the probability of mapping a lower-cased word
to its mixed-cased form is trained on all avail-
able data. A Kneser-Ney smoothed 4-gram lan-
guage model is also trained on the following cor-
pora: NC, News Crawl, Wiki, afp, apw, cna, ltw,
nyt, wpb, xin, giga. In addition, several rules are
manually designed to improve upon the output of
the disambig tool. First, casing information from
pass-through translation rules (for OOV source
words) is used to modify the casing of the output.
For example, this allows us to get the correct cas-
ing for the word Bundesrechnungshof. Other rules
are post-editing rules which force some words
to their upper-case forms, such as euro ? Euro.
Post-editing rules are developed based on high-
frequency errors on the newstest2012.tune devel-
opment set. These rules give an improvement of
0.2 mixed-cased NIST BLEU on the development
set.
Finally, the output is detokenised before sub-
mission and Cyrillic characters are transliterated.
We assume for human judgment purposes that it
is better to have a non English word in Latin al-
phabet than in Cyrillic (e.g. uprazdnyayushchie);
sometimes, transliteration can also give a correct
output (e.g. Movember), especially in the case of
proper nouns.
3 Results and Discussion
Results are reported in Table 3. We use the inter-
nationalisation switch for the NIST BLEU scor-
ing script in order to properly lowercase the hy-
pothesis and the reference. This introduces a
slight discrepancy with official results going into
the English language. The newstest2012.test de-
velopment set consists of even sentences from
newstest2012. We observe that the CoreNLP
system (A) outperforms the other two systems.
The CoreNLP+Morfessor system (B) has a much
smaller vocabulary but the model size is compa-
rable to the system A?s model size. Translation
did not benefit from source side morphological de-
composition. We also observe that the gain from
LMBR hypothesis combination (A+B+C) is mini-
mal. Unlike other language pairs, such as Arabic-
English (de Gispert et al, 2009), we have not yet
found any great advantage in multiple morpho-
logical decomposition or preprocessing analyses
of the source text. 5-gram and LMBR rescoring
give consistent improvements. 5-gram rescoring
improvements are very modest, probably because
the first pass 4-gram model is trained on the same
data. As noted, hypothesis combination using the
various segmentations gives consistent but modest
gains over each individual system.
Two systems were submitted to the evalua-
tion. System A+B+C achieved a mixed-cased
NIST BLEU score of 24.6, which was the top
score achieved under this measure. System A sys-
tem achieved a mixed-cased NIST BLEU score of
24.5, which was the second highest score.
4 Summary
We have successfully trained a Russian-English
system for the first time. Lessons learned include
that simple tokenisation is enough to process the
Russian side, very modest gains come from com-
bining alternative segmentations (it could also be
that the Morfessor segmentation should not be per-
formed after CoreNLP but directly on untokenised
data), and reordering between Russian and En-
glish is such that a shallow-1 grammar performs
203
Configuration newstest2012.tune newstest2012.test newstest2013
CoreNLP(A) 33.65 32.36 25.55
+5g 33.67 32.58 25.63
+5g+LMBR 33.98 32.89 25.89
CoreNLP+Morfessor(B) 33.21 31.91 25.33
+5g 33.28 32.12 25.44
+5g+LMBR 33.58 32.43 25.78
CoreNLP+TreeTagger(C) 32.92 31.54 24.78
+5g 32.94 31.85 24.97
+5g+LMBR 33.12 32.12 25.05
A+B+C 34.32 33.13 26.00
Table 3: Translation results, shown in lowercase NIST BLEU. Bold results correspond to submitted
systems.
competitively.
Future work could include exploring alterna-
tive grammars, applying a 5-gram Kneser-Ney
smoothed language model directly in first-pass de-
coding, and combining alternative segmentations
that are more diverse from each other.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7-ICT-2009-4) under grant
agreement number 247762. Tong Xiao was sup-
ported in part by the National Natural Science
Foundation of China (Grant 61073140 and Grant
61272376) and the China Postdoctoral Science
Foundation (Grant 2013M530131).
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11?23.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language
translation system. In Proceedings of ASRU, pages
396?401.
Graeme Blackwood. 2010. Lattice rescoring meth-
ods for statistical machine translation. Ph.D. thesis,
Cambridge University Engineering Department and
Clare College.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP-ACL, pages 858?867.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational linguistics,
19(2):263?311.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133?137.
David Chiang, Steve DeNeefe, and Michael Pust.
2011. Two easy improvements to lexical weighting.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 455?460, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing (TSLP), 4(1):3.
Adria` de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes Risk Com-
bination of Translation Hypotheses from Alterna-
tive Morphological Decompositions. In Proceed-
ings of HLT/NAACL, Companion Volume: Short Pa-
pers, pages 73?76.
Adria` de Gispert, Gonzalo Iglesias, Graeme Black-
wood, Eduardo R. Banga, and William Byrne. 2010.
Hierarchical phrase-based translation with weighted
finite state transducers and shallow-n grammars. In
Computational Linguistics.
Yonggang Deng and William Byrne. 2008. Hmm word
and phrase alignment for statistical machine trans-
lation. IEEE Transactions on Audio, Speech, and
Language Processing, 16(3):494?507.
Chris Dyer, Aaron Cordova, Alex Mont, and Jimmy
Lin. 2008. Fast, easy, and cheap: Construc-
tion of statistical machine translation models with
204
MapReduce. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 199?207,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, Sofia, Bulgaria,
August.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Hierarchical phrase-
based translation with weighted finite state transduc-
ers. In Proceedings of NAACL, pages 433?441.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of ICASSP, volume 1, pages 181?184.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 iwslt speech translation evaluation. In
International Workshop on Spoken Language Trans-
lation, volume 8.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
725?734, Honolulu, Hawaii, October. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Juan Pino, Aurelien Waite, and William Byrne. 2012.
Simple and efficient model filtering in statistical ma-
chine translation. The Prague Bulletin of Mathemat-
ical Linguistics, 98(1):5?24.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German. In
Proceedings of the ACL SIGDAT-Workshop, pages
47?50.
Andreas Stolcke. 2002. SRILM?An Extensible Lan-
guage Modeling Toolkit. In Proceedings of ICSLP,
volume 3, pages 901?904.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice Minimum
Bayes-Risk decoding for statistical machine trans-
lation. In Proceedings of EMNLP, pages 620?629.
205
