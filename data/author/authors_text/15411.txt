76
77
78
79
80
81
82
83
84
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 866?873,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Finding Synonyms Using Automatic Word Alignment and Measures of
Distributional Similarity
Lonneke van der Plas & Jo?rg Tiedemann
Alfa-Informatica
University of Groningen
P.O. Box 716
9700 AS Groningen
The Netherlands
{vdplas,tiedeman}@let.rug.nl
Abstract
There have been many proposals to ex-
tract semantically related words using
measures of distributional similarity, but
these typically are not able to distin-
guish between synonyms and other types
of semantically related words such as
antonyms, (co)hyponyms and hypernyms.
We present a method based on automatic
word alignment of parallel corpora con-
sisting of documents translated into mul-
tiple languages and compare our method
with a monolingual syntax-based method.
The approach that uses aligned multilin-
gual data to extract synonyms shows much
higher precision and recall scores for the
task of synonym extraction than the mono-
lingual syntax-based approach.
1 Introduction
People use multiple ways to express the same idea.
These alternative ways of conveying the same in-
formation in different ways are referred to by the
term paraphrase and in the case of single words
sharing the same meaning we speak of synonyms.
Identification of synonyms is critical for many
NLP tasks. In information retrieval the informa-
tion that people ask for with a set of words may be
found in in a text snippet that comprises a com-
pletely different set of words. In this paper we
report on our findings trying to automatically ac-
quire synonyms for Dutch using two different re-
sources, a large monolingual corpus and a multi-
lingual parallel corpus including 11 languages.
A common approach to the automatic extrac-
tion of semantically related words is to use dis-
tributional similarity. The basic idea behind this is
that similar words share similar contexts. Systems
based on distributional similarity provide ranked
lists of semantically related words according to
the similarity of their contexts. Synonyms are ex-
pected to be among the highest ranks followed by
(co)hyponyms and hypernyms, since the highest
degree of semantic relatedness next to identity is
synonymy.
However, this is not always the case. Sev-
eral researchers (Curran and Moens (2002), Lin
(1998), van der Plas and Bouma (2005)) have used
large monolingual corpora to extract distribution-
ally similar words. They use grammatical rela-
tions1 to determine the context of a target word.
We will refer to such systems as monolingual
syntax-based systems. These systems have proven
to be quite successful at finding semantically re-
lated words. However, they do not make a clear
distinction between synonyms on the one hand and
related words such as antonyms, (co)hyponyms,
hypernyms etc. on the other hand.
In this paper we have defined context in a mul-
tilingual setting. In particular, translations of a
word into other languages found in parallel cor-
pora are seen as the (translational) context of that
word. We assume that words that share transla-
tional contexts are semantically related. Hence,
relatedness of words is measured using distribu-
tional similarity in the same way as in the mono-
lingual case but with a different type of context.
Finding translations in parallel data can be approx-
1One can define the context of a word in a non-syntactic
monolingual way, that is as the document in which it occurs
or the n words surrounding it. From experiments we have
done and also building on the observations made by other
researchers (Kilgarriff and Yallop, 2000) we can state that
this approach generates a type of semantic similarity that is
of a looser kind, an associative kind,for example doctor and
disease. These words are typically not good candidates for
synonymy.
866
imated by automatic word alignment. We will
refer to this approach as multilingual alignment-
based approaches. We expect that these transla-
tions will give us synonyms and less semantically
related words, because translations typically do
not expand to hypernyms, nor (co)hyponyms, nor
antonyms. The word apple is typically not trans-
lated with a word for fruit nor pear, and neither is
good translated with a word for bad.
In this paper we use both monolingual syntax-
based approaches and multilingual alignment-
based approaches and compare their performance
when using the same similarity measures and eval-
uation set.
2 Related Work
Monolingual syntax-based distributional similar-
ity is used in many proposals to find semanti-
cally related words (Curran and Moens (2002),
Lin (1998), van der Plas and Bouma (2005)).
Several authors have used a monolingual par-
allel corpus to find paraphrases (Ibrahim et al
(2003), Barzilay and McKeown (2001)). How-
ever, bilingual parallel corpora have mostly been
used for tasks related to word sense disambigua-
tion such as target word selection (Dagan et al,
1991) and separation of senses (Dyvik, 1998). The
latter work derives relations such as synonymy and
hyponymy from the separated senses by applying
the method of semantic mirrors.
Turney (2001) reports on an PMI and IR driven
approach that acquires data by querying a Web
search engine. He evaluates on the TOEFL test in
which the system has to select the synonym among
4 candidates.
Lin et al (2003) try to tackle the problem of
identifying synonyms among distributionally re-
lated words in two ways: Firstly, by looking at
the overlap in translations of semantically similar
words in multiple bilingual dictionaries. Secondly,
by looking at patterns specifically designed to fil-
ter out antonyms. They evaluate on a set of 80
synonyms and 80 antonyms from a thesaurus.
Wu and Zhou?s (2003) paper is most closely re-
lated to our study. They report an experiment on
synonym extraction using bilingual resources (an
English-Chinese dictionary and corpus) as well
as monolingual resources (an English dictionary
and corpus). Their monolingual corpus-based ap-
proach is very similar to our monolingual corpus-
based approach. The bilingual approach is dif-
ferent from ours in several aspects. Firstly, they
do not take the corpus as the starting point to re-
trieve word alignments, they use the bilingual dic-
tionary to retrieve multiple translations for each
target word. The corpus is only employed to as-
sign probabilities to the translations found in the
dictionary. Secondly, the authors use a parallel
corpus that is bilingual whereas we use a multi-
lingual corpus containing 11 languages in total.
The authors show that the bilingual method out-
performs the monolingual methods. However a
combination of different methods leads to the best
performance.
3 Methodology
3.1 Measuring Distributional Similarity
An increasingly popular method for acquiring se-
mantically similar words is to extract distribution-
ally similar words from large corpora. The under-
lying assumption of this approach is that seman-
tically similar words are used in similar contexts.
The contexts a given word is found in, be it a syn-
tactic context or an alignment context, are used as
the features in the vector for the given word, the
so-called context vector. The vector contains fre-
quency counts for each feature, i.e., the multiple
contexts the word is found in.
Context vectors are compared with each other
in order to calculate the distributional similarity
between words. Several measures have been pro-
posed. Curran and Moens (2002) report on a large-
scale evaluation experiment, where they evaluated
the performance of various commonly used meth-
ods. Van der Plas and Bouma (2005) present a
similar experiment for Dutch, in which they tested
most of the best performing measures according
to Curran and Moens (2002). Pointwise Mutual
Information (I) and Dice? performed best in their
experiments. Dice is a well-known combinatorial
measure that computes the ratio between the size
of the intersection of two feature sets and the sum
of the sizes of the individual feature sets. Dice?
is a measure that incorporates weighted frequency
counts.
Dice? =
2
?
f min(I(W1, f), I(W2, f))
?
f I(W1, f) + I(W2, f)
,where f is the feature
W1 and W2 are the two words that are being compared,
and I is a weight assigned to the frequency counts.
867
3.2 Weighting
We will now explain why we use weighted fre-
quencies and which formula we use for weighting.
The information value of a cell in a word vec-
tor (which lists how often a word occurred in a
specific context) is not equal for all cells. We
will explain this using an example from mono-
lingual syntax-based distributional similarity. A
large number of nouns can occur as the subject of
the verb have, for instance, whereas only a few
nouns may occur as the object of squeeze. Intu-
itively, the fact that two nouns both occur as sub-
ject of have tells us less about their semantic sim-
ilarity than the fact that two nouns both occur as
object of squeeze. To account for this intuition,
the frequency of occurrence in a vector can be re-
placed by a weighted score. The weighted score
is an indication of the amount of information car-
ried by that particular combination of a noun and
its feature.
We believe that this type of weighting is benefi-
cial for calculating similarity between word align-
ment vectors as well. Word alignments that are
shared by many different words are most probably
mismatches.For this experiment we used Pointwise MutualInformation (I) (Church and Hanks, 1989).
I(W,f) = log P (W,f)P (W )P (f)
,where W is the target word
P(W) is the probability of seeing the word
P(f) is the probability of seeing the feature
P(W,f) is the probability of seeing the word and the feature
together.
3.3 Word Alignment
The multilingual approach we are proposing relies
on automatic word alignment of parallel corpora
from Dutch to one or more target languages. This
alignment is the basic input for the extraction of
the alignment context as described in section 5.2.2.
The alignment context is then used for measuring
distributional similarity as introduced above.
For the word alignment, we apply standard tech-
niques derived from statistical machine transla-
tion using the well-known IBM alignment mod-
els (Brown et al, 1993) implemented in the open-
source tool GIZA++ (Och, 2003). These mod-
els can be used to find links between words in a
source language and a target language given sen-
tence aligned parallel corpora. We applied stan-
dard settings of the GIZA++ system without any
optimisation for our particular input. We also used
plain text only, i.e. we did not apply further pre-
processing except tokenisation and sentence split-
ting. Additional linguistic processing such as lem-
matisation and multi-word unit detection might
help to improve the alignment but this is not part
of the present study.
The alignment models produced are asymmet-
ric and several heuristics exist to combine direc-
tional word alignments to improve alignment ac-
curacy. We believe, that precision is more cru-
cial than recall in our approach and, therefore, we
apply a very strict heuristics namely we compute
the intersection of word-to-word links retrieved by
GIZA++. As a result we obtain partially word-
aligned parallel corpora from which translational
context vectors are built (see section 5.2.2). Note,
that the intersection heuristics allows one-to-one
word links only. This is reasonable for the Dutch
part as we are only interested in single words and
their synonyms. However, the distributional con-
text of these words defined by their alignments is
strongly influenced by this heuristics. Problems
caused by this procedure will be discussed in de-
tail in section 7 of our experiments.
4 Evaluation Framework
In the following, we describe the data used and
measures applied.
The evaluation method that is most suitable
for testing with multiple settings is one that uses
an available resource for synonyms as a gold
standard. In our experiments we apply auto-
matic evaluation using an existing hand-crafted
synonym database, Dutch EuroWordnet (EWN,
Vossen (1998)).
In EWN, one synset consists of several syn-
onyms which represent a single sense. Polyse-
mous words occur in several synsets. We have
combined for each target word the EWN synsets
in which it occurs. Hence, our gold standard con-
sists of a list of all nouns found in EWN and their
corresponding synonyms extracted by taking the
union of all synsets for each word. Precision is
then calculated as the percentage of candidate syn-
onyms that are truly synonyms according to our
gold standard. Recall is the percentage of the syn-
onyms according to EWN that are indeed found
by the system. We have extracted randomly from
all synsets in EWN 1000 words with a frequency
868
above 4 for which the systems under comparison
produce output.
The drawback of using such a resource is that
coverage is often a problem. Not all words that
our system proposes as synonyms can be found in
Dutch EWN. Words that are not found in EWN
are discarded.2 . Moreover, EWN?s synsets are not
exhaustive. After looking at the output of our best
performing system we were under the impression
that many correct synonyms selected by our sys-
tem were classified as incorrect by EWN. For this
reason we decided to run a human evaluation over
a sample of 100 candidate synonyms classified as
incorrect by EWN.
5 Experimental Setup
In this section we will describe results from the
two synonym extraction approaches based on dis-
tributional similarity: one using syntactic context
and one using translational context based on word
alignment and the combination of both. For both
approaches, we used a cutoff n for each row in our
word-by-context matrix. A word is discarded if
the row marginal is less than n. This means that
each word should be found in any context at least
n times else it will be discarded. We refer to this
by the term minimum row frequency. The cutoff is
used to make the feature space manageable and to
reduce noise in the data. 3
5.1 Distributional Similarity Based on
Syntactic Relations
This section contains the description of the syn-
onym extraction approach based on distributional
similarity and syntactic relations. Feature vectors
for this approach are constructed from syntacti-
cally parsed monolingual corpora. Below we de-
scribe the data and resources used, the nature of
the context applied and the results of the synonym
extraction task.
5.1.1 Data and Resources
As our data we used the Dutch CLEF QA cor-
pus, which consists of 78 million words of Dutch
2Note that we use the part of EWN that contains only
nouns
3We have determined the optimum in F-score for the
alignment-based method, the syntax-based method and the
combination independently by using a development set of
1000 words that has no overlap with the test set used in eval-
uation. The minimum row frequency was set to 2 for all
alignment-based methods. It was set to 46 for the syntax-
based method and the combination of the two methods.
subject-verb cat eat
verb-object feed cat
adjective-noun black cat
coordination cat dog
apposition cat Garfield
prep. complement go+to work
Table 1: Types of dependency relations extracted
grammatical relation # pairs
subject 507K
object 240K
adjective 289K
coordination 400 K
apposition 109K
prep. complement 84K
total 1629K
Table 2: Number of word-syntactic-relation pairs
(types) per dependency relation with frequency >
1.
newspaper text (Algemeen Dagblad and NRC
Handelsblad 1994/1995). The corpus was parsed
automatically using the Alpino parser (van der
Beek et al, 2002; Malouf and van Noord, 2004).
The result of parsing a sentence is a dependency
graph according to the guidelines of the Corpus of
Spoken Dutch (Moortgat et al, 2000).
5.1.2 Syntactic Context
We have used several grammatical relations:
subect, object, adjective, coordination, apposi-
tion and prepositional complement. Examples are
given in table 1. Details on the extraction can be
found in van der Plas and Bouma (2005). The
number of pairs (types) consisting of a word and
a syntactic relation found are given in table 2. We
have discarded pairs that occur less than 2 times.
5.2 Distributional Similarity Based on Word
Alignment
The alignment approach to synonym extraction is
based on automatic word alignment. Context vec-
tors are built from the alignments found in a paral-
lel corpus. Each aligned word type is a feature in
the vector of the target word under consideration.
The alignment frequencies are used for weighting
the features and for applying the frequency cutoff.
In the following section we describe the data and
resources used in our experiments and finally the
results of this approach.
869
5.2.1 Data and Resources
Measures of distributional similarity usually re-
quire large amounts of data. For the alignment
method we need a parallel corpus of reasonable
size with Dutch either as source or as target lan-
guage. Furthermore, we would like to experiment
with various languages aligned to Dutch. The
freely available Europarl corpus (Koehn, 2003)
includes 11 languages in parallel, it is sentence
aligned, and it is of reasonable size. Thus, for
acquiring Dutch synonyms we have 10 language
pairs with Dutch as the source language. The
Dutch part includes about 29 million tokens in
about 1.2 million sentences. The entire corpus is
sentence aligned (Tiedemann and Nygaard, 2004)
which is a requirement for the automatic word
alignment described below.
5.2.2 Alignment Context
Context vectors are populated with the links to
words in other languages extracted from automatic
word alignment. We applied GIZA++ and the in-
tersection heuristics as explained in section . From
the word aligned corpora we extracted word type
links, pairs of source and target words with their
alignment frequency attached. Each aligned target
word type is a feature in the (translational) context
of the source word under consideration.
Note that we rely entirely on automatic process-
ing of our data. Thus, results from the automatic
word alignments include errors and their precision
and recall is very different for the various language
pairs. However, we did not assess the quality of
the alignment itself which would be beyond the
scope of this paper.
As mentioned earlier, we did not include any
linguistic pre-processing prior to the word align-
ment. However, we post-processed the alignment
results in various ways. We applied a simple lem-
matizer to the list of bilingual word type links
in order to 1) reduce data sparseness, and 2) to
facilitate our evaluation based on comparing our
results to existing synonym databases. For this
we used two resources: CELEX ? a linguistically
annotated dictionary of English, Dutch and Ger-
man (Baayen et al, 1993), and the Dutch snow-
ball stemmer implementing a suffix stripping al-
gorithm based on the Porter stemmer. Note that
lemmatization is only done for Dutch. Further-
more, we removed word type links that include
non-alphabetic characters to focus our investiga-
tions on ?real words?. In order to reduce alignment
noise, we also applied a frequency threshold to re-
move alignments that occur only once. Finally, we
restricted our study to Dutch nouns. Hence, we
extracted word type links for all words tagged as
noun in CELEX. We also included words which
are not found at all in CELEX assuming that most
of them will be productive noun constructions.
From the remaining word type links we popu-
lated the context vectors as described earlier. Ta-
ble 3 shows the number of context elements ex-
tracted in this manner for each language pair con-
sidered from the Europarl corpus4
#word-transl. pairs #word-transl. pairs
DA 104K FR 90K
DE 133K IT 96K
EL 60K PT 86K
EN 119K SV 97K
ES 119K ALL 994K
FI 89K
Table 3: Number of word-translation pairs for dif-
ferent languages with alignment frequency > 1
6 Results and Discussion
Table 4 shows the precision recall en F-score for
the different methods. The first 10 rows refer
to the results for all language pairs individually.
The 11th row corresponds to the setting in which
all alignments for all languages are combined.
The penultimate row shows results for the syntax-
based method and the last row the combination of
the syntax-based and alignment-based method.
Judging from the precision, recall and F-score
in table 4 Swedish is the best performing lan-
guage for Dutch synonym extraction from parallel
corpora. It seems that languages that are similar
to the target language, for example in word or-
der, are good candidates for finding synonyms at
high precision rates. Also the fact that Dutch and
Swedish both have one-word compounds avoids
mistakes that are often found with the other lan-
guages. However, judging from recall (and F-
score) French is not a bad candidate either. It is
possible that languages that are lexically different
from the target language provide more synonyms.
The fact that Finnish and Greek do not gain high
scores might be due to the fact that there are only
a limited amount of translational contexts (with a
frequency > 1) available for these language (as
is shown in table 3). The reasons are twofold.
4abbreviations taken from the ISO-639 2-letter codes
870
# candidate synonyms
1 2 3
Prec Rec F-sc Prec Rec F-sc Prec Rec F-sc
DA 19.8 5.1 8.1 15.5 7.6 10.2 13.3 9.4 11.0
DE 21.2 5.4 8.6 16.1 7.9 10.6 13.1 9.3 10.9
EL 18.2 4.5 7.2 14.0 6.5 8.9 11.8 7.9 9.4
EN 19.5 5.3 8.3 14.7 7.8 10.2 12.4 9.7 10.9
ES 18.4 5.0 7.9 14.7 7.8 10.2 12.1 9.4 10.6
FI 18.0 3.9 6.5 14.3 5.6 8.1 12.1 6.5 8.5
FR 20.3 5.5 8.7 15.8 8.3 10.9 13.0 10.1 11.4
IT 18.7 4.9 7.8 14.7 7.5 9.9 12.3 9.2 10.5
PT 17.7 4.8 7.6 14.0 7.4 9.7 11.6 8.9 10.1
SV 22.3 5.6 9.0 16.4 7.9 10.7 13.3 9.3 10.9
ALL 22.5 6.4 10.0 16.6 9.4 12.0 13.7 11.5 12.5
SYN 8.8 2.5 3.9 6.9 4.0 5.09 5.9 5.1 5.5
COMBI 19.9 5.8 8.9 14.5 8.4 10.6 11.7 10.1 10.9
Table 4: Precision, recall and F-score (%) at increasing number of candidate synonyms
Firstly, for Greek and Finnish the Europarl corpus
contains less data. Secondly, the fact that Finnish
is a language that has a lot of cases for nouns,
might lead to data sparseness and worse accuracy
in word alignment.
The results in table 4 also show the difference in
performance between the multilingual alignment-
method and the syntax-based method. The mono-
lingual alignment-based method outperforms the
syntax-based method by far. The syntax-based
method that does not rely on scarce multilingual
resources is more portable and also in this exper-
iment it makes use of more data. However, the
low precision scores of this method are not con-
vincing. Combining both methods does not result
in better performance for finding synonyms. This
is in contrast with the results reported by Wu and
Zhou (2003). This might well be due to the more
sophisticated method they use for combining dif-
ferent methods, which is a weighted combination.
The precision scores are in line with the scores
reported by Wu and Zhou (2003) in a similar ex-
periment discussed under related work. The re-
call we attain however is more than three times
higher. These differences can be due to differences
between their approach such as starting from a
bilingual dictionary for acquiring the translational
context versus using automatic word alignments
from a large multilingual corpus directly. Further-
more, the different evaluation methods used make
comparison between the two approaches difficult.
They use a combination of the English Word-
Net (Fellbaum, 1998) and Roget thesaurus (Ro-
get, 1911) as a gold standard in their evaluations.
It is obvious that a combination of these resources
leads to larger sets of synonyms. This could ex-
plain the relatively low recall scores. It does how-
ever not explain the similar precision scores.
We conducted a human evaluation on a sample
of 100 candidate synonyms proposed by our best
performing system that were classified as incor-
rect by EWN. Ten evaluators (authors excluded)
were asked to classify the pairs of words as syn-
onyms or non-synonyms using a web form of the
format yes/no/don?t know. For 10 out of the 100
pairs all ten evaluators agreed that these were syn-
onyms. For 37 of the 100 pairs more than half of
the evaluators agreed that these were synonyms.
We can conclude from this that the scores provided
in our evaluations based on EWN (table 4) are too
pessimistic. We believe that the actual precision
scores lie 10 to 37 % higher than the 22.5 % re-
ported in table 4. Over and above, this indicates
that we are able to extract automatically synonyms
that are not yet covered by available resources.
7 Error Analysis
In table 5 some example output is given for the
method combining word alignments of all 10 for-
eign languages as opposed to the monolingual
syntax-based method. These examples illustrate
the general patterns that we discovered by looking
into the results for the different methods.
The first two examples show that the syntax-
871
ALIGN(ALL) SYNTAX
consensus eensgezindheid evenwicht
consensus consensus equilibrium
herfst najaar winter
autumn autumn winter
eind einde begin
end end beginning
armoede armoedebestrijding werkloosheid
poverty poverty reduction unemployment
alcohol alcoholgebruik drank
alcohol alcohol consumption liquor
bes charme perzik
berry charm peach
definitie definie criterium
definition define+incor.stemm. criterion
verlamming lam verstoring
paralysis paralysed disturbance
Table 5: Example candidate synonyms at 1st rank
and their translations in italics
based method often finds semantically related
words whereas the alignment-based method finds
synonyms. The reasons for this are quite obvious.
Synonyms are likely to receive identical transla-
tions, words that are only semantically related are
not. A translator would not often translate auto
(car) with vrachtwagen (truck). However, the two
words are likely to show up in identical syntactic
relations, such as being the object of drive or ap-
pearing in coordination with motorcycle.
Another observation that we made is that the
syntax-based method often finds antonyms such as
begin (beginning) for the word einde (end). Expla-
nations for this are in line with what we said about
the semantically related words: Synonyms are
likely to receive identical translations, antonyms
are not but they do appear in similar syntactic con-
texts.
Compounds pose a problem for the alignment-
method. We have chosen intersection as align-
ment method. It is well-known that this method
cannot cope very well with the alignment of com-
pounds because it only allows one-to-one word
links. Dutch uses many one-word compounds that
should be linked to multi-word counterparts in
other languages. However, using intersection we
obtain only partially correct alignments and this
causes many mistakes in the distributional simi-
larity algorithm. We have given some examples in
rows 4 and 5 of table 5.
We have used the distributional similarity score
only for ranking the candidate synonyms. In some
cases it seems that we should have used it to set a
threshold such as in the case of berry and charm.
These two words share one translational context :
the article el in Spanish. The distributional sim-
ilarity score in such cases is often very low. We
could have filtered some of these mistakes by set-
ting a threshold.
One last observation is that the alignment-based
method suffers from incorrect stemming and the
lack of sufficient part-of-speech information. We
have removed all context vectors that were built
for a word that was registered in CELEX with a
PoS-tag different from ?noun?. But some words
are not found in CELEX and although they are
not of the word type ?noun? their context vec-
tors remain in our data. They are stemmed using
the snowball stemmer. The candidate synonym
denie is a corrupted verbform that is not found
in CELEX. Lam is ambiguous between the noun
reading that can be translated in English with lamb
and the adjective lam which can be translated with
paralysed. This adjective is related to the word
verlamming (paralysis), but would have been re-
moved if the word was correctly PoS-tagged.
8 Conclusions
Parallel corpora are mostly used for tasks related
to WSD. This paper shows that multilingual word
alignments can be applied to acquire synonyms
automatically without the need for resources such
as bilingual dictionaries. A comparison with a
monolingual syntax-based method shows that the
alignment-based method is able to extract syn-
onyms with much greater precision and recall. A
human evaluation shows that the synonyms the
alignment-based method finds are often missing in
EWN. This leads us to believe that the precision
scores attained by using EWN as a gold standard
are too pessimistic. Furthermore it is good news
that we seem to be able to find synonyms that are
not yet covered by existing resources.
The precision scores are still not satisfactory
and we see plenty of future directions. We would
like to use linguistic processing such as PoS-
tagging for word alignment to increase the accu-
racy of the alignment itself, to deal with com-
pounds more effectively and to be able to filter
out proposed synonyms that are of a different word
class than the target word. Furthermore we would
like to make use of the distributional similarity
score to set a threshold that will remove a lot of
errors. The last thing that remains for future work
is to find a more adequate way to combine the
872
syntax-based and the alignment-based methods.
Acknowledgements
This research was carried out in the project
Question Answering using Dependency Relations,
which is part of the research program for Inter-
act ive Multimedia Information Extraction, IMIX,
financed by NWO, the Dutch Organisation for Sci-
entific Research.
References
R.H. Baayen, R. Piepenbrock, and H. van Rijn. 1993.
The CELEX lexical database (CD-ROM). Lin-
guistic Data Consortium, University of Pennsylva-
nia,Philadelphia.
Regina Barzilay and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Meet-
ing of the Association for Computational Linguis-
tics, pages 50?57.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?296.
K.W. Church and P. Hanks. 1989. Word association
norms, mutual information and lexicography. Pro-
ceedings of the 27th annual conference of the Asso-
ciation of Computational Linguistics, pages 76?82.
J.R. Curran and M. Moens. 2002. Improvements in
automatic thesaurus extraction. In Proceedings of
the Workshop on Unsupervised Lexical Acquisition,
pages 59?67.
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two
languages are more informative than one. In Meet-
ing of the Association for Computational Linguis-
tics, pages 130?137.
Helge Dyvik. 1998. Translations as semantic mirrors.
In Proceedings of Workshop Multilinguality in the
Lexicon II, ECAI 98, Brighton, UK, pages 24?44.
C. Fellbaum. 1998. Wordnet, an electronic lexical
database. MIT Press.
A. Ibrahim, B. Katz, and J. Lin. 2003. Extract-
ing structural paraphrases from aligned monolingual
corpora.
A. Kilgarriff and C. Yallop. 2000. What?s in a the-
saurus? In Proceedings of the Second Conference
on Language Resource an Evaluation, pages 1371?
1379.
Philipp Koehn. 2003. Europarl: A multilin-
gual corpus for evaluation of machine trans-
lation. unpublished draft, available from
http://people.csail.mit.edu/koehn/publications/europarl/.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among distribu-
tionally similar words. In IJCAI, pages 1492?1493.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In COLING-ACL, pages 768?774.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value
grammars. In IJCNLP-04 Workshop Beyond Shal-
low Analyses - Formalisms and stati stical modeling
for deep analyses, Hainan.
Michael Moortgat, Ineke Schuurman, and Ton van der
Wouden. 2000. CGN syntactische annotatie. In-
ternal Project Report Corpus Gesproken Nederlands,
see http://lands. let.kun.nl/cgn.
Franz Josef Och. 2003. GIZA++: Training of
statistical translation models. Available from
http://www.isi.edu/?och/GIZA++.html.
P. Roget. 1911. Thesaurus of English words and
phrases.
Jo?rg Tiedemann and Lars Nygaard. 2004. The OPUS
corpus - parallel & free. In Proceedings of the
Fourth International Conference on Language Re-
sources and Evaluation (LREC?04), Lisbon, Portu-
gal.
Peter D. Turney. 2001. Mining the Web for synonyms:
PMI?IR versus LSA on TOEFL. Lecture Notes in
Computer Science, 2167:491?502.
Leonoor van der Beek, Gosse Bouma, and Gertjan van
Noord. 2002. Een brede computationele grammat-
ica voor het Nederlands. Nederlandse Taalkunde,
7(4):353?374.
Lonneke van der Plas and Gosse Bouma. 2005.
Syntactic contexts for finding semantically similar
words. Proceedings of the Meeting of Computa-
tional Linguistics in the Netherlands (CLIN).
P. Vossen. 1998. Eurowordnet a multilingual database
with lexical semantic networks.
Hua Wu and Ming Zhou. 2003. Optimizing syn-
onym extraction using monolingual and bilingual re-
sources. In Proceedings of the Second International
Workshop on Paraphrasing: Paraphrase Acquisition
and Applications (IWP2003), Sapporo, Japan.
873
Proceedings of NAACL HLT 2009: Short Papers, pages 125?128,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Domain Adaptation with Artificial Data for Semantic Parsing of Speech
Lonneke van der Plas
Department of Linguistics
University of Geneva
Geneva, Switzerland
James Henderson
Department of Computer Science
University of Geneva
Geneva, Switzerland
{Lonneke.vanderPlas,James.Henderson,Paola.Merlo}@unige.ch
Paola Merlo
Department of Linguistics
University of Geneva
Geneva, Switzerland
Abstract
We adapt a semantic role parser to the do-
main of goal-directed speech by creating an
artificial treebank from an existing text tree-
bank. We use a three-component model that
includes distributional models from both tar-
get and source domains. We show that we im-
prove the parser?s performance on utterances
collected from human-machine dialogues by
training on the artificially created data without
loss of performance on the text treebank.
1 Introduction
As the quality of natural language parsing improves
and the sophistication of natural language under-
standing applications increases, there are several do-
mains where parsing, and especially semantic pars-
ing, could be useful. This is particularly true in
adaptive systems for spoken language understand-
ing, where complex utterances need to be translated
into shallow semantic representation, such as dia-
logue acts.
The domain on which we are working is goal-
directed system-driven dialogues, where a system
helps the user to fulfil a certain goal, e.g. booking a
hotel room. Typically, users respond with short an-
swers to questions posed by the system. For exam-
ple In the South is an answer to the question Where
would you like the hotel to be? Parsing helps iden-
tifying the components (In the South is a PP) and
semantic roles identify the PP as a locative, yield-
ing the following slot-value pair for the dialogue act:
area=South. A PP such as in time is not identified as
a locative, whereas keyword-spotting techniques as
those currently used in dialogue systems may pro-
duce area=South and area=time indifferently.
Statistical syntactic and semantic parsers need
treebanks. Current available data is lacking in one or
more respects: Syntactic/semantic treebanks are de-
veloped on text, while treebanks of speech corpora
are not semantically annotated (e.g. Switchboard).
Moreover, the available human-human speech tree-
banks do not exhibit the same properties as the
system-driven speech on which we are focusing, in
particular in their proportion of non-sentential utter-
ances (NSUs), utterances that are not full sentences.
In a corpus study of a subset of the human-human
dialogues in the BNC, Ferna?ndez (2006) found that
only 9% of the total utterances are NSUs, whereas
we find 44% in our system-driven data.
We illustrate a technique to adapt an exist-
ing semantic parser trained on merged Penn Tree-
bank/PropBank data to goal-directed system-driven
dialogue by artificial data generation. Our main con-
tribution lies in the framework used to generate ar-
tificial data for domain adaptation. We mimic the
distributions over parse structures in the target do-
main by combining the text treebank data and the
artificially created NSUs, using a three-component
model. The first component is a hand-crafted model
of NSUs. The second component describes the dis-
tribution over full sentences and types of NSUs as
found in a minimally annotated subset of the target
domain. The third component describes the distribu-
tion over the internal parse structure of the generated
data and is taken from the source domain.
Our approach differs from most approaches to do-
main adaptation, which require some training on
fully annotated target data (Nivre et al, 2007),
whereas we use minimally annotated target data
only to help determine the distributions in the ar-
tificially created data. It also differs from previ-
125
ous work in domain adaptation by Foster (2007),
where similar proportions of ungrammatical and
grammatical data are combined to train a parser
on ungrammatical written text, and by Weilhammer
et al (2006), who use interpolation between two
separately trained models, one on an artificial cor-
pus of user utterances generated by a hand-coded
domain-specific grammar and one on available cor-
pora. Whereas much previous work on parsing
speech has focused on speech repairs, e.g. Charniak
and Johnson (2001), we focus on parsing NSUs.
2 The first component: a model of NSUs
To construct a model of NSUs we studied a subset of
the data under consideration: TownInfo. This small
corpus of transcribed spoken human-machine dia-
logues in the domain of hotel/restaurant/bar search
is gathered using the TownInfo tourist information
system (Lemon et al, 2006).
The NSUs we find in our data are mainly of the
type answers, according to the classification given
in Ferna?ndez (2006). More specifically, we find
short answers, plain and repeated affirmative an-
swers, plain and helpful rejections, but also greet-
ings.
Current linguistic theory provides several ap-
proaches to dealing with NSUs (Merchant, 2004;
Progovac et al, 2006; Ferna?ndez, 2006). Follow-
ing the linguistic analysis of NSUs as non-sentential
small clauses (Progovac et al, 2006) that do not have
tense or agreement functional nodes, we make the
assumption that they are phrasal projections. There-
fore, we reason, we can create an artificial data set
of NSUs by extracting phrasal projections from an
annotated treebank.
In the example given in the introduction, we saw
a PP fragment, but fragments can be NPs, APs, etc.
We define different types of NSUs based on the root
label of the phrasal projection and define rules that
allow us to extract NSUs (partial parse trees) from
the source corpus.1 Because the target corpus also
contains full sentences, we allow full sentences to
be taken without modification from the source tree-
bank.
1Not all of these rules are simple extractions of phrasal pro-
jections, as described in section 4.
3 The two distributional components
The distributional model consists of two compo-
nents. By applying the extraction rules to the source
corpus we build a large collection of both full sen-
tences and NSUs. The distributions in this collec-
tion follow the distributions of trees in the source do-
main (first distributional component). We then sam-
ple from this collection to generate our artificial cor-
pus following distributions from the target domain
(second distributional component).
The probability of an artificial tree P (fi(cj)) gen-
erated with an extraction rule fi applied to a con-
stituent from the source corpus cj is defined as
P (fi(cj)) = P (fi)P (cj |fi) ? Pt(fi)Ps(cj |fi)
The first distributional component originates from
the source domain. It is responsible for the internal
structure of the NSUs and full sentences extracted.
Ps(cj |fi) is the probability of the constituent taken
from the source treebank (cj), given that the rule fi
is applicable to that constituent.
Sampling is done according to distributions of
NSUs and full sentences found in the target corpus
(Pt(fi)). As explained in section 2, there are several
types of NSUs found in the target domain. This sec-
ond component describes the distributions of types
of NSUs (or full sentences) found in the target do-
main. It determines, for example, the proportion of
NP NSUs that will be added to the artificial corpus.
To determine the target distribution we classified
171 (approximately 5%) randomly selected utter-
ances from the TownInfo data, that were used as a
development set.2 In Table 1 we can see that 15.2 %
of the trees in the artificial corpus will be NP NSUs.3
4 Data generation
We constructed our artificial corpus from sections
2 to 21 of the Wall Street Journal (WSJ) section
of the Penn Treebank corpus (Marcus et al, 1993)
2We discarded very short utterances (yes, no, and greetings)
since they don?t need parsing. We also do not consider incom-
plete NSUs resulting from interruptions or recording problems.
3Because NSUs can be interpreted only in context, the same
NSU can correspond to several syntactic categories: South for
example, can be an noun, an adverb, or an adjective. In case of
ambiguity, we divided the score up for the several possible tags.
This accounts for the fractional counts.
126
Category # Occ. Perc. Category # Occ. Perc.
NP 19.0 15.2 RB 1.7 1.3
JJ 12.7 10.1 DT 1.0 0.8
PP 12.0 9.6 CD 1.0 0.8
NN 11.7 9.3 Total frag. 70.0 56.0
VP 11.0 8.8 Full sents 55.0 44.0
Table 1: Distribution of types of NSUs and full sentences
in the TownInfo development set.
merged with PropBank labels (Palmer et al, 2005).
We included all the sentences from this dataset in
our artificial corpus, giving us 39,832 full sentences.
In accordance with the target distribution we added
50,699 NSUs extracted from the same dataset. We
sampled NSUs according to the distribution given in
Table 1. After the extraction we added a root FRAG
node to the extracted NSUs4 and we capitalised the
first letter of each NSU to form an utterance.
There are two additional pre-processing steps.
First, for some types of NSUs maximal projections
are added. For example, in the subset from the tar-
get source we saw many occurrences of nouns with-
out determiners, such as Hotel or Bar. These types
of NSUs would be missed if we just extracted NPs
from the source data, since we assume that NSUs are
maximal projections. Therefore, we extracted single
nouns as well and we added the NP phrasal projec-
tions to these nouns in the constructed trees. Sec-
ond, not all extracted NSUs can keep their semantic
roles. Extracting part of the sentence often severs
the semantic role from the predicate of which it was
originally an argument. An exception to this are VP
NSUs and prepositional phrases that are modifiers,
such as locative PPs, which are not dependent on the
verb. Hence, we removed the semantic roles from
the generated NSUs except for VPs and modifiers.
5 Experiments
We trained three parsing models on both the original
non-augmented merged Penn Treebank/Propbank
corpus and the artificially generated augmented tree-
bank including NSUs. We ran a contrastive ex-
periment to examine the usefulness of the three-
component model by training two versions of the
4The node FRAG exists in the Penn Treebank. Our annota-
tion does not introduce new labels, but only changes their dis-
tribution.
augmented model: One with and one without the
target component.5
These models were tested on two test sets: a small
corpus of 150 transcribed utterances taken from the
TownInfo corpus, annotated with gold syntactic and
semantic annotation by two of the authors6: the
TownInfo test set. The second test set is used to
compare the performance of the parser on WSJ-style
sentences and consists of section 23 of the merged
Penn Treebank/Propbank corpus. We will refer to
this test set as the non-augmented test set.
5.1 The statistical parser
The parsing model is the one proposed in Merlo
and Musillo (2008), which extends the syntactic
parser of Henderson (2003) and Titov and Hender-
son (2007) with annotations which identify seman-
tic role labels, and has competitive performance.
The parser uses a generative history-based proba-
bility model for a binarised left-corner derivation.
The probabilities of derivation decisions are mod-
elled using the neural network approximation (Hen-
derson, 2003) to a type of dynamic Bayesian Net-
work called an Incremental Sigmoid Belief Network
(ISBN) (Titov and Henderson, 2007).
The ISBN models the derivation history with a
vector of binary latent variables. These latent vari-
ables learn to represent features of the parse history
which are useful for making the current and subse-
quent derivation decisions. Induction of these fea-
tures is biased towards features which are local in
the parse tree, but can find features which are passed
arbitrarily far through the tree. This flexible mecha-
nism for feature induction allows the model to adapt
to the parsing of NSUs without requiring any design
changes or feature engineering.
5.2 Results
In Table 2, we report labelled constituent recall, pre-
cision, and F-measure for the three trained parsers
(rows) on the two test sets (columns).7 These mea-
5The model without the target distribution has a uniform dis-
tribution over full sentences and NSUs and within NSUs a uni-
form distribution over the 8 types.
6This test set was constructed separately and is completely
different from the development set used to determine the distri-
butions in the target data.
7Statistical significance is determined using a stratified shuf-
fling method, using software available at http://www.cis.
127
Training Testing
TownInfo PTB nonaug
Rec Prec F Rec Prec F
PTB nonaug 69.4 76.7 72.9 81.4 82.1 81.7
PTB aug(+t) 81.4 77.8 79.5 81.3 82.0 81.7
PTB aug(?t) 62.6 64.3 63.4 81.2 81.9 81.6
Table 2: Recall, precision, and F-measure for the two test
sets, trained on non-augmented data and data augmented
with and without the target distribution component.
sures include both syntactic labels and semantic role
labels.
The results in the first two lines of the columns
headed TownInfo indicate the performance on the
real data to which we are trying to adapt our parser:
spoken data from human-machine dialogues. The
parser does much better when trained on the aug-
mented data. The differences between training on
newspaper text and newspaper texts augmented with
artificially created data are statistically significant
(p < 0.001) and particularly large for recall: almost
12%.
The columns headed PTB nonaug show that the
performance on parsing WSJ texts is not hurt by
training on data augmented with artificially cre-
ated NSUs (first vs. second line). The difference
in performance compared to training on the non-
augmented data is not statistically significant.
The last two rows of the TownInfo data show the
results of our contrastive experiment. It is clear
that the three-component model and in particular our
careful characterisation of the target distribution is
indispensable. The F-measure drops from 79.5% to
63.4% when we disregard the target distribution.
6 Conclusions
We have shown how a three-component model that
consists of a model of the phenomenon being stud-
ied and two distributional components, one from the
source data and one from the target data, allows
one to create data artificially for training a seman-
tic parser. Specifically, analysis and minimal anno-
tation of only a small subset of utterances from the
target domain of spoken dialogue systems suffices
to determine a model of NSUs as well as the nec-
essary target distribution. Following this framework
upenn.edu/?dbikel/software.html.
we were able to improve the performance of a statis-
tical parser on goal-directed spoken data extracted
from human-machine dialogues without degrading
the performance on full sentences.
Acknowledgements
The research leading to these results has received
funding from the EU FP7 programme (FP7/2007-
2013) under grant agreement nr 216594 (CLASSIC
project: www.classic-project.org).
References
E. Charniak and M. Johnson. 2001. Edit detection and
parsing for transcribed speech. In Procs. NAACL.
R. Ferna?ndez. 2006. Non-sentential utterances in dia-
logue: classification resolution and use. Ph.D. thesis,
University of London.
J. Foster. 2007. Treebanks gone bad: Parser evaluation
and retraining using a treebank of ungrammatical sen-
tences. International Journal of Document Analysis
and Recognition, 10:1?16.
J. Henderson. 2003. Inducing history representations for
broad-coverage statistical parsing. In Procs. NAACL-
HLT.
O. Lemon, K. Georgila, J. Henderson, and M. Stuttle.
2006. An ISU dialogue system exhibiting reinforce-
ment learning of dialogue policies: generic slot-filling
in the TALK in-car system. In Procs. EACL.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
the Penn Treebank. Comp. Ling., 19:313?330.
J. Merchant. 2004. Fragments and ellipsis. Linguistics
and Philosophy, 27:661?738.
P. Merlo and G. Musillo. 2008. Semantic parsing
for high-precision semantic role labelling. In Procs.
CONLL.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Procs. EMNLP-
CoNLL.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Comp. Ling., 31:71?105.
L. Progovac, K. Paesani, E. Casielles, and E. Barton.
2006. The Syntax of Nonsententials:Multidisciplinary
Perspectives. John Benjamins.
I Titov and J Henderson. 2007. Constituent parsing with
Incremental Sigmoid Belief Networks. In Procs. ACL.
K. Weilhammer, M. Stuttle, and S. Young. 2006. Boot-
strapping language models for dialogue systems. In
Procs. Conf. on Spoken Language Processing.
128
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 288?296,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Abstraction and Generalisation in Semantic Role Labels:
PropBank, VerbNet or both?
Paola Merlo
Linguistics Department
University of Geneva
5 Rue de Candolle, 1204 Geneva
Switzerland
Paola.Merlo@unige.ch
Lonneke Van Der Plas
Linguistics Department
University of Geneva
5 Rue de Candolle, 1204 Geneva
Switzerland
Lonneke.VanDerPlas@unige.ch
Abstract
Semantic role labels are the representa-
tion of the grammatically relevant aspects
of a sentence meaning. Capturing the
nature and the number of semantic roles
in a sentence is therefore fundamental to
correctly describing the interface between
grammar and meaning. In this paper, we
compare two annotation schemes, Prop-
Bank and VerbNet, in a task-independent,
general way, analysing how well they fare
in capturing the linguistic generalisations
that are known to hold for semantic role
labels, and consequently how well they
grammaticalise aspects of meaning. We
show that VerbNet is more verb-specific
and better able to generalise to new seman-
tic role instances, while PropBank better
captures some of the structural constraints
among roles. We conclude that these two
resources should be used together, as they
are complementary.
1 Introduction
Most current approaches to language analysis as-
sume that the structure of a sentence depends on
the lexical semantics of the verb and of other pred-
icates in the sentence. It is also assumed that only
certain aspects of a sentence meaning are gram-
maticalised. Semantic role labels are the represen-
tation of the grammatically relevant aspects of a
sentence meaning.
Capturing the nature and the number of seman-
tic roles in a sentence is therefore fundamental
to correctly describe the interface between gram-
mar and meaning, and it is of paramount impor-
tance for all natural language processing (NLP)
applications that attempt to extract meaning rep-
resentations from analysed text, such as question-
answering systems or even machine translation.
The role of theories of semantic role lists is to
obtain a set of semantic roles that can apply to
any argument of any verb, to provide an unam-
biguous identifier of the grammatical roles of the
participants in the event described by the sentence
(Dowty, 1991). Starting from the first proposals
(Gruber, 1965; Fillmore, 1968; Jackendoff, 1972),
several approaches have been put forth, ranging
from a combination of very few roles to lists of
very fine-grained specificity. (See Levin and Rap-
paport Hovav (2005) for an exhaustive review).
In NLP, several proposals have been put forth in
recent years and adopted in the annotation of large
samples of text (Baker et al, 1998; Palmer et al,
2005; Kipper, 2005; Loper et al, 2007). The an-
notated PropBank corpus, and therefore implicitly
its role labels inventory, has been largely adopted
in NLP because of its exhaustiveness and because
it is coupled with syntactic annotation, properties
that make it very attractive for the automatic learn-
ing of these roles and their further applications to
NLP tasks. However, the labelling choices made
by PropBank have recently come under scrutiny
(Zapirain et al, 2008; Loper et al, 2007; Yi et al,
2007).
The annotation of PropBank labels has been
conceived in a two-tiered fashion. A first tier
assigns abstract labels such as ARG0 or ARG1,
while a separate annotation records the second-
tier, verb-sense specific meaning of these labels.
Labels ARG0 or ARG1 are assigned to the most
prominent argument in the sentence (ARG1 for
unaccusative verbs and ARG0 for all other verbs).
The other labels are assigned in the order of promi-
nence. So, while the same high-level labels are
used across verbs, they could have different mean-
ings for different verb senses. Researchers have
usually concentrated on the high-level annotation,
but as indicated in Yi et al (2007), there is rea-
son to think that these labels do not generalise
across verbs, nor to unseen verbs or to novel verb
288
senses. Because the meaning of the role annota-
tion is verb-specific, there is also reason to think
that it fragments the data and creates data sparse-
ness, making automatic learning from examples
more difficult. These short-comings are more ap-
parent in the annotation of less prominent and less
frequent roles, marked by the ARG2 to ARG5 la-
bels.
Zapirain et al (2008), Loper et al (2007) and
Yi et al (2007) investigated the ability of the Prop-
Bank role inventory to generalise compared to the
annotation in another semantic role list, proposed
in the electronic dictionary VerbNet. VerbNet la-
bels are assigned in a verb-class specific way and
have been devised to be more similar to the inven-
tories of thematic role lists usually proposed by
linguists. The results in these papers are conflict-
ing.
While Loper et al (2007) and Yi et al (2007)
show that augmenting PropBank labels with Verb-
Net labels increases generalisation of the less fre-
quent labels, such as ARG2, to new verbs and new
domains, they also show that PropBank labels per-
form better overall, in a semantic role labelling
task. Confirming this latter result, Zapirain et al
(2008) find that PropBank role labels are more ro-
bust than VerbNet labels in predicting new verb
usages, unseen verbs, and they port better to new
domains.
The apparent contradiction of these results can
be due to several confounding factors in the exper-
iments. First, the argument labels for which the
VerbNet improvement was found are infrequent,
and might therefore not have influenced the over-
all results enough to counterbalance new errors in-
troduced by the finer-grained annotation scheme;
second, the learning methods in both these exper-
imental settings are largely based on syntactic in-
formation, thereby confounding learning and gen-
eralisation due to syntax ? which would favour
the more syntactically-driven PropBank annota-
tion ? with learning due to greater generality of
the semantic role annotation; finally, task-specific
learning-based experiments do not guarantee that
the learners be sufficiently powerful to make use
of the full generality of the semantic role labels.
In this paper, we compare the two annotation
schemes, analysing how well they fare in captur-
ing the linguistic generalisations that are known
to hold for semantic role labels, and consequently
how well they grammaticalise aspects of mean-
ing. Because the well-attested strong correlation
between syntactic structure and semantic role la-
bels (Levin and Rappaport Hovav, 2005; Merlo
and Stevenson, 2001) could intervene as a con-
founding factor in this analysis, we expressly limit
our investigation to data analyses and statistical
measures that do not exploit syntactic properties or
parsing techniques. The conclusions reached this
way are not task-specific and are therefore widely
applicable.
To preview, based on results in section 3, we
conclude that PropBank is easier to learn, but
VerbNet is more informative in general, it gener-
alises better to new role instances and its labels are
more strongly correlated to specific verbs. In sec-
tion 4, we show that VerbNet labels provide finer-
grained specificity. PropBank labels are more con-
centrated on a few VerbNet labels at higher fre-
quency. This is not true at low frequency, where
VerbNet provides disambiguations to overloaded
PropBank variables. Practically, these two sets
of results indicate that both annotation schemes
could be useful in different circumstances, and at
different frequency bands. In section 5, we report
results indicating that PropBank role sets are high-
level abstractions of VerbNet role sets and that
VerbNet role sets are more verb and class-specific.
In section 6, we show that PropBank more closely
captures the thematic hierarchy and is more corre-
lated to grammatical functions, hence potentially
more useful for semantic role labelling, for learn-
ers whose features are based on the syntactic tree.
Finally, in section 7, we summarise some previ-
ous results, and we provide new statistical evi-
dence to argue that VerbNet labels are more gen-
eral across verbs. These conclusions are reached
by task-independent statistical analyses. The data
and the measures used to reach these conclusions
are discussed in the next section.
2 Materials and Method
In data analysis and inferential statistics, careful
preparation of the data and choice of the appropri-
ate statistical measures are key. We illustrate the
data and the measures used here.
2.1 Data and Semantic Role Annotation
Proposition Bank (Palmer et al, 2005) adds
Levin?s style predicate-argument annotation and
indication of verbs? alternations to the syntactic
structures of the Penn Treebank (Marcus et al,
289
1993).
It defines a limited role typology. Roles are
specified for each verb individually. Verbal pred-
icates in the Penn Treebank (PTB) receive a label
REL and their arguments are annotated with ab-
stract semantic role labels A0-A5 or AA for those
complements of the predicative verb that are con-
sidered arguments, while those complements of
the verb labelled with a semantic functional label
in the original PTB receive the composite seman-
tic role label AM-X , where X stands for labels
such as LOC, TMP or ADV, for locative, tem-
poral and adverbial modifiers respectively. Prop-
Bank uses two levels of granularity in its annota-
tion, at least conceptually. Arguments receiving
labels A0-A5 or AA do not express consistent se-
mantic roles and are specific to a verb, while argu-
ments receiving an AM-X label are supposed to
be adjuncts and the respective roles they express
are consistent across all verbs. However, among
argument labels, A0 and A1 are assigned attempt-
ing to capture Proto-Agent and Proto-Patient prop-
erties (Dowty, 1991). They are, therefore, more
valid across verbs and verb instances than the A2-
A5 labels. Numerical results in Yi et al (2007)
show that 85% of A0 occurrences translate into
Agent roles and more than 45% instances of A1
map into Patient and Patient-like roles, using a
VerbNet labelling scheme. This is also confirmed
by our counts, as illustrated in Tables 3 and 4 and
discussed in Section 4 below.
VerbNet is a lexical resource for English verbs,
yielding argumental and thematic information
(Kipper, 2005). VerbNet resembles WordNet in
spirit, it provides a verbal lexicon tying verbal se-
mantics (theta-roles and selectional restrictions) to
verbal distributional syntax. VerbNet defines 23
thematic roles that are valid across verbs. The list
of thematic roles can be seen in the first column of
Table 4.
For some of our comparisons below to be valid,
we will need to reduce the inventory of labels of
VerbNet to the same number of labels in Prop-
Bank. Following previous work (Loper et al,
2007), we define equivalence classes of VerbNet
labels. We will refer to these classes as VerbNet
groups. The groups we define are illustrated in
Figure 1. Notice also that all our comparisons,
like previous work, will be limited to the obliga-
tory arguments in PropBank, the A0 to A5, AA
arguments, to be comparable to VerbNet. VerbNet
is a lexicon and by definition it does not list op-
tional modifiers (the arguments labelled AM-X in
PropBank).
In order to support the joint use of both these re-
sources and their comparison, SemLink has been
developed (Loper et al, 2007). SemLink1 pro-
vides mappings from PropBank to VerbNet for the
WSJ portion of the Penn Treebank. The mapping
have been annotated automatically by a two-stage
process: a lexical mapping and an instance classi-
fier (Loper et al, 2007). The results were hand-
corrected. In addition to semantic roles for both
PropBank and VerbNet, SemLink contains infor-
mation about verbs, their senses and their VerbNet
classes which are extensions of Levin?s classes.
The annotations in SemLink 1.1. are not com-
plete. In the analyses presented here, we have
only considered occurrences of semantic roles for
which both a PropBank and a VerbNet label is
available in the data (roughly 45% of the Prop-
Bank semantic roles have a VerbNet semantic
role).2 Furthermore, we perform our analyses on
training and development data only. This means
that we left section 23 of the Wall Street Journal
out. The analyses are done on the basis of 106,459
semantic role pairs.
For the analysis concerning the correlation be-
tween semantic roles and syntactic dependencies
in Section 6, we merged the SemLink data with the
non-projectivised gold data of the CoNNL 2008
shared task on syntactic and semantic dependency
parsing (Surdeanu et al, 2008). Only those depen-
dencies that bear both a syntactic and a semantic
label have been counted for test and development
set. We have discarded discontinous arguments.
Analyses are based on 68,268 dependencies in to-
tal.
2.2 Measures
In the following sections, we will use simple pro-
portions, entropy, joint entropy, conditional en-
tropy, mutual information, and a normalised form
of mutual information which measures correlation
between nominal attributes called symmetric un-
certainty (Witten and Frank, 2005, 291). These are
all widely used measures (Manning and Schuetze,
1999), excepted perhaps the last one. We briefly
describe it here.
1(http://verbs.colorado.edu/semlink/)
2In some cases SemLink allows for multiple annotations.
In those cases we selected the first annotation.
290
AGENT: Agent, Agent1
PATIENT: Patient
GOAL: Recipient, Destination, Location, Source,
Material, Beneficiary, Goal
EXTENT: Extent, Asset, Value
PREDATTR: Predicate, Attribute, Theme,
Theme1, Theme2, Topic, Stimulus, Proposition
PRODUCT: Patient2, Product, Patient1
INSTRCAUSE: Instrument, Cause, Experiencer,
Actor2, Actor, Actor1
Figure 1: VerbNet Groups
Given a random variable X, the entropy H(X)
describes our uncertainty about the value of X, and
hence it quantifies the information contained in a
message trasmitted by this variable. Given two
random variables X,Y, the joint entropy H(X,Y)
describes our uncertainty about the value of the
pair (X,Y). Symmetric uncertainty is a normalised
measure of the information redundancy between
the distributions of two random variables. It cal-
culates the ratio between the joint entropy of the
two random variables if they are not independent
and the joint entropy if the two random variables
were independent (which is the sum of their indi-
vidual entropies). This measure is calculated as
follows.
U(A,B) = 2
H(A) + H(B)?H(A,B)
H(A) + H(B)
where H(X) = ??x?X p(x)logp(x) and
H(X,Y ) = ??x?X,y?Y p(x, y)logp(x, y).
Symmetric uncertainty lies between 0 and 1. A
higher value for symmetric uncertainty indicates
that the two random variables are more highly as-
sociated (more redundant), while lower values in-
dicate that the two random variables approach in-
dependence.
We use these measures to evaluate how well two
semantic role inventories capture well-known dis-
tributional generalisations. We discuss several of
these generalisations in the following sections.
3 Amount of Information in Semantic
Roles Inventory
Most proposals of semantic role inventories agree
on the fact that the number of roles should be small
to be valid generally. 3
3With the notable exception of FrameNet, which is devel-
oping a large number of labels organised hierarchically and
Task PropBank ERR VerbNet ERR
Role generalisation 62 (82?52/48) 66 (77?33/67)
No verbal features 48 (76?52/48) 43 (58?33/67)
Unseen predicates 50 (75?52/48) 37 (62?33/67)
Table 2: Percent Error rate reduction (ERR) across
role labelling sets in three tasks in Zapirain et al
(2008). ERR= (result ? baseline / 100% ? base-
line )
PropBank and VerbNet clearly differ in the level
of granularity of the semantic roles that have been
assigned to the arguments. PropBank makes fewer
distinctions than VerbNet, with 7 core argument
labels compared to VerbNet?s 23. More important
than the size of the inventory, however, is the fact
that PropBank has a much more skewed distribu-
tion than VerbNet, illustrated in Table 1. Conse-
quently, the distribution of PropBank labels has
an entropy of 1.37 bits, and even when the Verb-
Net labels are reduced to 7 equivalence classes
the distribution has an entropy of 2.06 bits. Verb-
Net therefore conveys more information, but it is
also more difficult to learn, as it is more uncertain.
An uninformed PropBank learner that simply as-
signed the most frequent label would be correct
52% of the times by always assigning an A1 label,
while for VerbNet would be correct only 33% of
the times assigning Agent.
This simple fact might cast new light on some
of the comparative conclusions of previous work.
In some interesting experiments, Zapirain et al
(2008) test generalising abilities of VerbNet and
PropBank comparatively to new role instances in
general (their Table 1, line CoNLL setting, col-
umn F1 core), and also on unknown verbs and in
the absence of verbal features. They find that a
learner based on VerbNet has worse learning per-
formance. They interpret this result as indicating
that VerbNet labels are less general and more de-
pendent on knowledge of specific verbs. However,
a comparison that takes into consideration the dif-
ferential baseline is able to factor the difficulty of
the task out of the results for the overall perfor-
mance. A simple baseline for a classifier is based
on a majority class assignment (see our Table 1).
We use the performance results reported in Zapi-
rain et al (2008) and calculate the reduction in er-
ror rate based on this differential baseline for the
two annotation schemes. We compare only the
results for the core labels in PropBank as those
interpreted frame-specifically (Ruppenhofer et al, 2006).
291
PropBank VerbNet
A0 38.8 Agent 32.8 Cause 1.9 Source 0.9 Asset 0.3 Goal 0.00
A1 51.7 Theme 26.3 Product 1.6 Actor1 0.8 Material 0.2 Agent1 0.00
A2 9.0 Topic 11.5 Extent 1.3 Theme2 0.8 Beneficiary 0.2
A3 0.5 Patient 5.8 Destination 1.2 Theme1 0.8 Proposition 0.1
A4 0.0 Experiencer 4.2 Patient1 1.2 Attribute 0.7 Value 0.1
A5 0.0 Predicate 2.3 Location 1.0 Patient2 0.5 Instrument 0.1
AA 0.0 Recipient 2.2 Stimulus 0.9 Actor2 0.3 Actor 0.0
Table 1: Distribution of PropBank core labels and VerbNet labels.
are the ones that correspond to VerbNet.4 We
find more mixed results than previously reported.
VerbNet has better role generalising ability overall
as its reduction in error rate is greater than Prop-
Bank (first line of Table 2), but it is more degraded
by lack of verb information (second and third lines
of Table 2). The importance of verb information
for VerbNet is confirmed by information-theoretic
measures. While the entropy of VerbNet labels
is higher than that of PropBank labels (2.06 bits
vs. 1.37 bits), as seen before, the conditional en-
tropy of respective PropBank and VerbNet distri-
butions given the verb is very similar, but higher
for PropBank (1.11 vs 1.03 bits), thereby indicat-
ing that the verb provides much more information
in association with VerbNet labels. The mutual in-
formation of the PropBank labels and the verbs
is only 0.26 bits, while it is 1.03 bits for Verb-
Net. These results are expected if we recall the
two-tiered logic that inspired PropBank annota-
tion, where the abstract labels are less related to
verbs than labels in VerbNet.
These results lead us to our first conclusion:
while PropBank is easier to learn, VerbNet is more
informative in general, it generalises better to new
role instances, and its labels are more strongly cor-
related to specific verbs. It is therefore advisable
to use both annotations: VerbNet labels if the verb
is available, reverting to PropBank labels if no lex-
4We assume that our majority class can roughly corre-
spond to Zapirain et al (2008)?s data. Notice however that
both sampling methods used to collect the counts are likely
to slightly overestimate frequent labels. Zapirain et al (2008)
sample only complete propositions. It is reasonable to as-
sume that higher numbered PropBank roles (A3, A4, A5) are
more difficult to define. It would therefore more often happen
that these labels are not annotated than it happens that A0,
A1, A2, the frequent labels, are not annotated. This reason-
ing is confirmed by counts on our corpus, which indicate that
incomplete propositions include a higher proportion of low
frequency labels and a lower proportion of high frequency
labels that the overall distribution. However, our method is
also likely to overestimate frequent labels, since we count all
labels, even those in incomplete propositions. By the same
reasoning, we will find more frequent labels than the under-
lying real distribution of a complete annotation.
ical information is known.
4 Equivalence Classes of Semantic Roles
An observation that holds for all semantic role la-
belling schemes is that certain labels seem to be
more similar than others, based on their ability to
occur in the same syntactic environment and to
be expressed by the same function words. For
example, Agent and Instrumental Cause are of-
ten subjects (of verbs selecting animate and inan-
imate subjects respectively); Patients/Themes can
be direct objects of transitive verbs and subjects
of change of state verbs; Goal and Beneficiary can
be passivised and undergo the dative alternation;
Instrument and Comitative are expressed by the
same preposition in many languages (see Levin
and Rappaport Hovav (2005).) However, most an-
notation schemes in NLP and linguistics assume
that semantic role labels are atomic. It is there-
fore hard to explain why labels do not appear to be
equidistant in meaning, but rather to form equiva-
lence classes in certain contexts. 5
While both role inventories under scrutiny here
use atomic labels, their joint distribution shows
interesting relations. The proportion counts are
shown in Table 3 and 4.
If we read these tables column-wise, thereby
taking the more linguistically-inspired labels in
VerbNet to be the reference labels, we observe
that the labels in PropBank are especially con-
centrated on those labels that linguistically would
be considered similar. Specifically, in Table 3
A0 mostly groups together Agents and Instrumen-
tal Causes; A1 mostly refers to Themes and Pa-
tients; while A2 refers to Goals and Themes. If we
5Clearly, VerbNet annotators recognise the need to ex-
press these similarities since they use variants of the same
label in many cases. Because the labels are atomic however,
the distance between Agent and Patient is the same as Patient
and Patient1 and the intended greater similarity of certain la-
bels is lost to a learning device. As discussed at length in the
linguistic literature, features bundles instead of atomic labels
would be the mechanism to capture the differential distance
of labels in the inventory (Levin and Rappaport Hovav, 2005).
292
A0 A1 A2 A3 A4 A5 AA
Agent 32.6 0.2 - - - - -
Patient 0.0 5.8 - - - - -
Goal 0.0 1.5 4.0 0.2 0.0 0.0 -
Extent - 0.2 1.3 0.2 - - -
PredAttr 1.2 39.3 2.9 0.0 - - 0.0
Product 0.1 2.7 0.6 - 0.0 - -
InstrCause 4.8 2.2 0.3 0.1 - - -
Table 3: Distribution of PropBank by VerbNet
group labels according to SemLink. Counts indi-
cated as 0.0 approximate zero by rounding, while
a - sign indicates that no occurrences were found.
read these tables row-wise, thereby concentrating
on the grouping of PropBank labels provided by
VerbNet labels, we see that low frequency Prop-
Bank labels are more evenly spread across Verb-
Net labels than the frequent labels, and it is more
difficult to identify a dominant label than for high-
frequency labels. Because PropBank groups to-
gether VerbNet labels at high frequency, while
VerbNet labels make different distinctions at lower
frequencies, the distribution of PropBank is much
more skewed than VerbNet, yielding the differ-
ences in distributions and entropy discussed in the
previous section.
We can draw, then, a second conclusion: while
VerbNet is finer-grained than PropBank, the two
classifications are not in contradiction with each
other. VerbNet greater specificity can be used in
different ways depending on the frequency of the
label. Practically, PropBank labels could provide
a strong generalisation to a VerbNet annotation at
high-frequency. VerbNet labels, on the other hand,
can act as disambiguators of overloaded variables
in PropBank. This conclusion was also reached
by Loper et al (2007). Thus, both annotation
schemes could be useful in different circumstances
and at different frequency bands.
5 The Combinatorics of Semantic Roles
Semantic roles exhibit paradigmatic generalisa-
tions ? generalisations across similar semantic
roles in the inventory ? (which we saw in section
4.) They also show syntagmatic generalisations,
generalisations that concern the context. One kind
of context is provided by what other roles they can
occur with. It has often been observed that cer-
tain semantic roles sets are possible, while oth-
ers are not; among the possible sets, certain are
much more frequent than others (Levin and Rap-
paport Hovav, 2005). Some linguistically-inspired
A0 A1 A2 A3 A4 A5 AA
Actor 0.0 - - - - - -
Actor1 0.8 - - - - - -
Actor2 - 0.3 0.1 - - - -
Agent1 0.0 - - - - - -
Agent 32.6 0.2 - - - - -
Asset - 0.1 0.0 0.2 - - -
Attribute - 0.1 0.7 - - - -
Beneficiary - 0.0 0.1 0.1 0.0 - -
Cause 0.7 1.1 0.1 0.1 - - -
Destination - 0.4 0.8 0.0 - - -
Experiencer 3.3 0.9 0.1 - - - -
Extent - - 1.3 - - - -
Goal - - - - 0.0 - -
Instrument - - 0.1 0.0 - - -
Location 0.0 0.4 0.6 0.0 - 0.0 -
Material - 0.1 0.1 0.0 - - -
Patient 0.0 5.8 - - - - -
Patient1 0.1 1.1 - - - - -
Patient2 - 0.1 0.5 - - - -
Predicate - 1.2 1.1 0.0 - - -
Product 0.0 1.5 0.1 - 0.0 - -
Proposition - 0.0 0.1 - - - -
Recipient - 0.3 2.0 - 0.0 - -
Source - 0.3 0.5 0.1 - - -
Stimulus - 1.0 - - - - -
Theme 0.8 25.1 0.5 0.0 - - 0.0
Theme1 0.4 0.4 0.0 0.0 - - -
Theme2 0.1 0.4 0.3 - - - -
Topic - 11.2 0.3 - - - -
Value - 0.1 - - - - -
Table 4: Distribution of PropBank by original
VerbNet labels according to SemLink. Counts
indicated as 0.0 approximate zero by rounding,
while a - sign indicates that no occurrences were
found.
semantic role labelling techniques do attempt to
model these dependencies directly (Toutanova et
al., 2008; Merlo and Musillo, 2008).
Both annotation schemes impose tight con-
straints on co-occurrence of roles, independently
of any verb information, with 62 role sets for
PropBank and 116 role combinations for VerbNet,
fewer than possible. Among the observed role
sets, some are more frequent than expected un-
der an assumption of independence between roles.
For example, in PropBank, propositions compris-
ing A0, A1 roles are observed 85% of the time,
while they would be expected to occur only in 20%
of the cases. In VerbNet the difference is also great
between the 62% observed Agent, PredAttr propo-
sitions and the 14% expected.
Constraints on possible role sets are the expres-
sion of structural constraints among roles inherited
from syntax, which we discuss in the next section,
but also of the underlying event structure of the
verb. Because of this relation, we expect a strong
correlation between role sets and their associated
293
A0,A1 A0,A2 A1,A2
Agent, Theme 11650 109 4
Agent, Topic 8572 14 0
Agent, Patient 1873 0 0
Experiencer, Theme 1591 0 15
Agent, Product 993 1 0
Agent, Predicate 960 64 0
Experiencer, Stimulus 843 0 0
Experiencer, Cause 756 0 2
Table 5: Sample of role sets correspondences
verb, as well as role sets and verb classes for both
annotation schemes. However, PropBank roles are
associated based on the meaning of the verb, but
also based on their positional prominence in the
tree, and so we can expect their relation to the ac-
tual verb entry to be weaker.
We measure here simply the correlation as in-
dicated by the symmetric uncertainty of the joint
distribution of role sets by verbs and of role sets
by verb classes, for each of the two annotation
schemes. We find that the correlation between
PropBank role sets and verb classes is weaker
than the correlation between VerbNet role sets and
verb classes, as expected (PropBank: U=0.21 vs
VerbNet: U=0.46). We also find that correlation
between PropBank role sets and verbs is weaker
than the correlation between VerbNet role sets and
verbs (PropBank: U=0.23 vs VerbNet U=0.43).
Notice that this result holds for VerbNet role label
groups, and is therefore not a side-effect of a dif-
ferent size in role inventory. This result confirms
our findings reported in Table 2, which showed
a larger degradation of VerbNet labels in the ab-
sence of verb information.
If we analyse the data, we see that many role
sets that form one single set in PropBank are split
into several sets in VerbNet, with those roles that
are different being roles that in PropBank form a
group. So, for example, a role list (A0, A1) in
PropBank will corresponds to 14 different lists in
VerbNet (when using the groups). The three most
frequent VerbNet role sets describe 86% of the
cases: (Agent, Predattr) 71%, (InstrCause, Pre-
dAttr) 9%, and (Agent, Patient) 6% . Using the
original VerbNet labels ? a very small sample of
the most frequent ones is reported in Table 5 ?
we find 39 different sets. Conversely, we see that
VerbNet sets corresponds to few PropBank sets,
even for high frequency.
The third conclusion we can draw then is two-
fold. First, while VerbNet labels have been as-
signed to be valid across verbs, as confirmed by
their ability to enter in many combinations, these
combinations are more verb and class-specific
than combinations in PropBank. Second, the fine-
grained, coarse-grained correspondence of anno-
tations between VerbNet and PropBank that was
illustrated by the results in Section 4 is also borne
out when we look at role sets: PropBank role sets
appear to be high-level abstractions of VerbNet
role sets.
6 Semantic Roles and Grammatical
Functions: the Thematic Hierarchy
A different kind of context-dependence is pro-
vided by thematic hierarchies. It is a well-attested
fact that lexical semantic properties described by
semantic roles and grammatical functions appear
to be distributed according to prominence scales
(Levin and Rappaport Hovav, 2005). Seman-
tic roles are organized according to the thematic
hierarchy (one proposal among many is Agent
> Experiencer> Goal/Source/Location> Patient
(Grimshaw, 1990)). This hierarchy captures the
fact that the options for the structural realisation
of a particular argument do not depend only on
its role, but also on the roles of other arguments.
For example in psychological verbs, the position
of the Experiencer as a syntactic subject or ob-
ject depends on whether the other role in the sen-
tence is a Stimulus, hence lower in the hierar-
chy, as in the psychological verbs of the fear class
or an Agent/Cause as in the frighten class. Two
prominence scales can combine by matching ele-
ments harmonically, higher elements with higher
elements and lower with lower (Aissen, 2003).
Grammatical functions are also distributed accord-
ing to a prominence scale. Thus, we find that most
subjects are Agents, most objects are Patients or
Themes, and most indirect objects are Goals, for
example.
The semantic role inventory, thus, should show
a certain correlation with the inventory of gram-
matical functions. However, perfect correlation is
clearly not expected as in this case the two levels
of representation would be linguistically and com-
putationally redundant. Because PropBank was
annotated according to argument prominence, we
expect to see that PropBank reflects relationships
between syntax and semantic role labels more
strongly than VerbNet. Comparing syntactic de-
pendency labels to their corresponding PropBank
or VerbNet groups labels (groups are used to elim-
294
inate the confound of different inventory sizes), we
find that the joint entropy of PropBank and depen-
dency labels is 2.61 bits while the joint entropy of
VerbNet and dependency labels is 3.32 bits. The
symmetric uncertainty of PropBank and depen-
dency labels is 0.49, while the symmetric uncer-
tainty of VerbNet and dependency labels is 0.39.
On the basis of these correlations, we can con-
firm previous findings: PropBank more closely
captures the thematic hierarchy and is more corre-
lated to grammatical functions, hence potentially
more useful for semantic role labelling, for learn-
ers whose features are based on the syntactic tree.
VerbNet, however, provides a level of annotation
that is more independent of syntactic information,
a property that might be useful in several applica-
tions, such as machine translation, where syntactic
information might be too language-specific.
7 Generality of Semantic Roles
Semantic roles are not meant to be domain-
specific, but rather to encode aspects of our con-
ceptualisation of the world. A semantic role in-
ventory that wants to be linguistically perspicuous
and also practically useful in several tasks needs to
reflect our grammatical representation of events.
VerbNet is believed to be superior in this respect
to PropBank, as it attempts to be less verb-specific
and to be portable across classes. Previous results
(Loper et al, 2007; Zapirain et al, 2008) appear to
indicate that this is not the case because a labeller
has better performance with PropBank labels than
with VerbNet labels. But these results are task-
specific, and they were obtained in the context of
parsing. Since we know that PropBank is more
closely related to grammatical function and syn-
tactic annotation than VerbNet, as indicated above
in Section 6, then these results could simply indi-
cate that parsing predicts PropBank labels better
because they are more closely related to syntactic
labels, and not because the semantic roles inven-
tory is more general.
Several of the findings in the previous sections
shed light on the generality of the semantic roles in
the two inventories. Results in Section 3 show that
previous results can be reinterpreted as indicating
that VerbNet labels generalise better to new roles.
We attempt here to determine the generality of
the ?meaning? of a role label without recourse
to a task-specific experiment. It is often claimed
in the literature that semantic roles are better de-
scribed by feature bundles. In particular, the fea-
tures sentience and volition have been shown to be
useful in distinguishing Proto-Agents from Proto-
Patients (Dowty, 1991). These features can be as-
sumed to be correlated to animacy. Animacy has
indeed been shown to be a reliable indicator of
semantic role differences (Merlo and Stevenson,
2001). Personal pronouns in English grammati-
calise animacy. We extract all the occurrences of
the unambiguously animate pronouns (I, you, he,
she, us, we, me, us, him) and the unambiguously
inanimate pronoun it, for each semantic role label,
in PropBank and VerbNet. We find occurrences
for three semantic role labels in PropBank and six
in VerbNet. We reduce the VerbNet groups to five
by merging Patient roles with PredAttr roles to
avoid artificial variation among very similar roles.
An analysis of variance of the distributions of the
pronous yields a significant effect of animacy for
VerbNet (F(4)=5.62, p< 0.05), but no significant
effect for PropBank (F(2)=4.94, p=0.11). This re-
sult is a preliminary indication that VerbNet labels
might capture basic components of meaning more
clearly than PropBank labels, and that they might
therefore be more general.
8 Conclusions
In this paper, we have proposed a task-
independent, general method to analyse anno-
tation schemes. The method is based on
information-theoretic measures and comparison
with attested linguistic generalisations, to evalu-
ate how well semantic role inventories and anno-
tations capture grammaticalised aspects of mean-
ing. We show that VerbNet is more verb-specific
and better able to generalise to new semantic roles,
while PropBank, because of its relation to syntax,
better captures some of the structural constraints
among roles. Future work will investigate another
basic property of semantic role labelling schemes:
cross-linguistic validity.
Acknowledgements
We thank James Henderson and Ivan Titov for
useful comments. The research leading to these
results has received partial funding from the EU
FP7 programme (FP7/2007-2013) under grant
agreement number 216594 (CLASSIC project:
www.classic-project.org).
295
References
Judith Aissen. 2003. Differential object marking:
Iconicity vs. economy. Natural Language and Lin-
guistic Theory, 21:435?483.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the Thirty-Sixth Annual Meeting of the As-
sociation for Computational Linguistics and Seven-
teenth International Conference on Computational
Linguistics (ACL-COLING?98), pages 86?90, Mon-
treal, Canada.
David Dowty. 1991. Thematic proto-roles and argu-
ment selection. Language, 67(3):547?619.
Charles Fillmore. 1968. The case for case. In Emmon
Bach and Harms, editors, Universals in Linguistic
Theory, pages 1?88. Holt, Rinehart, and Winston.
Jane Grimshaw. 1990. Argument Structure. MIT
Press.
Jeffrey Gruber. 1965. Studies in Lexical Relation.
MIT Press, Cambridge, MA.
Ray Jackendoff. 1972. Semantic Interpretation in
Generative Grammar. MIT Press, Cambridge, MA.
Karin Kipper. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Ph.D. thesis, University of
Pennsylvania.
Beth Levin and Malka Rappaport Hovav. 2005. Ar-
gument Realization. Cambridge University Press,
Cambridge, UK.
Edward Loper, Szu ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between
PropBank and VerbNet. In Proceedings of the
IWCS.
Christopher Manning and Hinrich Schuetze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press.
Mitch Marcus, Beatrice Santorini, and M.A.
Marcinkiewicz. 1993. Building a large anno-
tated corpus of English: the Penn Treebank.
Computational Linguistics, 19:313?330.
Paola Merlo and Gabriele Musillo. 2008. Semantic
parsing for high-precision semantic role labelling.
In Proceedings of the Twelfth Conference on Com-
putational Natural Language Learning (CONLL-
08), pages 1?8, Manchester, UK.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
verb classification based on statistical distributions
of argument structure. Computational Linguistics,
27(3):373?408.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31:71?105.
Josef Ruppenhofer, Michael Ellsworth, Miriam
Petruck, Christopher Johnson, and Jan Scheffczyk.
2006. Framenet ii: Theory and practice. Technical
report, Berkeley,CA.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In Proceedings
of the 12th Conference on Computational Natural
Language Learning (CoNLL-2008), pages 159?177.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2).
Ian Witten and Eibe Frank. 2005. Data Mining. Else-
vier.
Szu-ting Yi, Edward Loper, and Martha Palmer. 2007.
Can semantic roles generalize across genres? In
Proceedings of the Human Language Technologies
2007 (NAACL-HLT?07), pages 548?555, Rochester,
New York, April.
Ben?at Zapirain, Eneko Agirre, and Llu??s Ma`rquez.
2008. Robustness and generalization of role sets:
PropBank vs. VerbNet. In Proceedings of ACL-08:
HLT, pages 550?558, Columbus, Ohio, June.
296
Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 45?53,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Combining Syntactic Co-occurrences and Nearest Neighbours in
Distributional Methods to Remedy Data Sparseness.
Lonneke van der Plas
Department of Linguistics
University of Geneva
Geneva, Switzerland
Abstract
The task of automatically acquiring semanti-
cally related words have led people to study
distributional similarity. The distributional
hypothesis states that words that are simi-
lar share similar contexts. In this paper we
present a technique that aims at improving
the performance of a syntax-based distribu-
tional method by augmenting the original in-
put of the system (syntactic co-occurrences)
with the output of the system (nearest neigh-
bours). This technique is based on the idea of
the transitivity of similarity.
1 Introduction
The approach described in this paper builds on the
DISTRIBUTIONAL HYPOTHESIS, the idea that se-
mantically related words are distributed similarly
over contexts. Harris (1968) claims that, ?the mean-
ing of entities and the meaning of grammatical re-
lations among them, is related to the restriction of
combinations of these entities relative to other enti-
ties.? In other words, you can grasp the meaning of
a word by looking at its context.
Context can be defined in many ways. In this pa-
per we look at the syntactic contexts a word is found
in. For example, the verbs that are in a object rela-
tion with a particular noun form a part of its context.
In accordance with the Firthian tradition these con-
texts can be used to determine the semantic related-
ness of words. For instance, words that occur in a
object relation with the verb drink have something
in common: they are liquid. We will refer to words
linked by a syntactic relation, such as drink -OBJ-
beer, as SYNTACTIC CO-OCCURRENCES. Syntac-
tic co-occurrences have often been used in work on
lexical acquisition (Lin, 1998b; Dagan et al, 1999;
Curran and Moens, 2002; Alfonseca and Manand-
har, 2002).
Distributional methods for automatic acquisition
of semantically related words suffer from data
sparseness. They generally perform less well
on low-frequency words (Weeds and Weir, 2005;
van der Plas, 2008). This is a pity because the avail-
able resources for semantically related words usu-
ally cover the frequent words rather well. It is for the
low-frequency words that automatic methods would
be most welcome.
This paper tries to find a way to improve the per-
formance on the words that are most wanted: the
middle to very-low-frequency words. At the basis of
the proposed technique lies the intuition that seman-
tic similarity between concepts is transitive: if A is
like B and B is like C ? A is like C. As explained
in the second paragraph of this section, the fact that
both milk and water are found in object relation with
the verb to drink tells us that they might be similar.
However, even if we had never seen lemonade in the
same syntactic contexts as water, we could still in-
fer that lemonade and water are similar because we
have found evidence that both water and lemonade
are similar to milk.
In an ideal world we would be able to infer that
milk and water are related from the syntactic co-
occurrences alone, however, because of data sparse-
ness we might not always encounter this evidence
directly. We hope that nearest neighbours are able
to account for the missing information. Nearest
neighbours such as milk and water, and water and
lemonade are the output of our system. We used the
nearest neighbours (the output of our system) as in-
put to our system that normally takes syntactic co-
45
occurrences as input. Thus it uses the output of the
system as input in a second round to smooth the syn-
tactic co-occurrences.
Grefenstette (1994) discusses the difference be-
tween FIRST- AND SECOND-ORDER AFFINITIES.
There exists a first-order affinity between words if
they often appear in the same context, i.e., if they are
often found in the vicinity of each other. Words that
co-occur frequently such as orange and squeezed
have a first-order affinity. There exists a second-
order affinity between words if they share many first-
order affinities. These words need not appear to-
gether themselves, but their contexts are similar. Or-
ange and lemon appear often in similar contexts such
as being the object of squeezed, or being modified by
juicy.
In this paper we will use second-order affinities as
input to the distributional system. We are thus com-
puting THIRD-ORDER AFFINITIES.1 There exists
a third-order affinity between words, if they share
many second-order affinities. If pear and water-
melon are similar and orange and watermelon are
similar, then pear and orange have a third-order
affinity.
We will refer to traditional approaches that com-
pute second-order affinities as second-order tech-
niques. In this paper we will compare a second-
order technique with a third-order technique, a tech-
nique that computes third-order affinities. In ad-
dition we use a combined technique that combines
both second-order and third-order techniques.
2 Previous work
In Edmonds (1997) the term third-order is used to
refer to a different concept. Firstly, we have to
mention that the author is working in a proximity-
based framework, that is, he is concerned with co-
occurrences of words in text, not relations between
words in syntactic dependencies. Secondly, the no-
tion of higher-order co-occurrences refers to con-
nectivity paths in networks, i.e. the network of re-
lations between words co-occurring is augmented
by connecting words that are connected by a path
of length 2 (second-order co-occurrences) and paths
1Grefenstette (1994) uses the term third-order affinities for
a different concept, i.e. for the subgroupings that can be found
in list of second-order nearest neighbours.
of length 3 (third-order co-occurrences) and so on.
In the above example water and lemonade would
be connected by a second-order relation implied by
the network in which water and lemonade both co-
occur with for example to pour. A third-order rela-
tion would be implied between lemonade and drink
if drink should co-occur with water. We define
third-order affinity as an iterative process of calcu-
lating similarity. The output of the system is fed
into the system again. There exists a third-order
affinity between words if they share many nearest
neighbours with another word, not if a word shares
a context that in turn shares a context with the other
word. The same perspective on higher-order co-
occurrence, that of connectivity paths in networks,
is taken in literature of computational modelling of
the acquisition of word meaning (Lemaire and Den-
hire, 2006).
Although Biemann et al (2004) work in the same
proximity-based tradition as the previous authors
their notion of third-order is closer to our definition.
It is defined as an iterative process in which words
are linked when their co-occurrence score trespasses
a certain threshold. These nth-order co-occurrences
are then used to construct an artificial corpus con-
sisting of the co-occurrence sets retrieved from the
original corpus.
Schu?tze and Walsh (2008) present a graph-
theoretic model of lexical-syntactic representation in
which higher-order syntactic relations, those that re-
quire some generalisation, are defined recursively.
The problem they are trying to solve, lexical syn-
tactic acquisition, is different form ours and so
is the evaluation method: discriminating sentences
that exhibit local coherence from those that do not.
Again the method is proximity-based, but since the
context are defined very locally (left and right neigh-
bours) the results are likely to be more compara-
ble to a syntax-based method than proximity-based
methods that use larger contexts.
3 Limits of the transitivity of similarity
The validity of the third-order affinities is depen-
dent on the transitivity of the similarity between con-
cepts. Unfortunately, it is not always the case that
the similarity between A and B and B and C implies
the similarity between A and C.
46
When two concepts are identical, the transitivity
of similarity holds. If A=B AND B=C ? A=C.
Does the same reasoning hold for similarity of a
lesser degree? For (near-)synonyms the transitivity
holds and it is symmetric. If felicity is like gladness,
and gladness is like joy? felicity is like joy. Also,
the near-synonymy relation is symmetric. We can
infer that gladness is like felicity.
Tversky and Gati (1978) give an example of co-
hyponymy where transitivity does not hold. Ja-
maica is similar to Cuba (with respect to geograph-
ical proximity); Cuba is similar to Russia (with re-
spect to their political affinity), but Jamaica and Rus-
sia are not similar at all. Geographical proximity and
political affinity are SEPARABLE FEATURES. Cuba
and Jamaica are co-hyponyms if we imagine a hy-
pernym Caribbean islands of which both concepts
are daughters. Cuba and Russia are co-hyponyms
too, but being daughters of another mother, i.e. the
concept communist countries. The concept Jamaica
thus inherits features from multiple mothers. What
can we say about the transitivity of meaning in this
case? The transitivity between two co-hyponyms
holds when restricted to single inheritance.
When words are ambiguous, we come to a sim-
ilar situation. Widdows (2004) gives the following
example: Apple is similar to IBM in the domain of
computer companies; Apple is similar to pear, when
we are thinking of fruit. Pear and IBM are not sim-
ilar at all. Again, there is the problem of multiple
inheritance. Apple is a daughter both of the con-
cept computer manufacturers and of fruits. For co-
hyponyms similarity is only transitive in case of sin-
gle inheritance. The same holds for synonyms. If a
word has multiple senses we get into trouble when
applying the transitivity of meaning.
Although we have seen many examples of cases
where the transitivity of meaning does not hold, we
hope to find improvements for finding semantically
related words, when using third-order affinity tech-
niques.
4 Methodology
We will now describe the methodology used to com-
pute nearest neighbours (subsection 4.1). In subsec-
tion 4.2 we will describe how we have used these
nearest neighbours as input to the third-order and
combined technique.
4.1 Syntax-based distributional similarity
In this section we will describe the syntactic con-
texts selected, the data we used, and the measures
and weights applied to retrieve nearest neighbours.
4.1.1 Syntactic context
Most research has been done using a limited num-
ber of syntactic relations (Lee, 1999; Weeds, 2003).
We use several syntactic relations: subject, ob-
ject, adjective, coordination, apposition, and prepo-
sitional complement. In Figure 1 examples are given
for these types of syntactic relations.2
Subj: De kat eet.
?The cat eats.?
Obj: Ik voer de kat.
?I feed the cat.?
Adj: De langharige kat loopt.
?The long-haired cat walks.?
Coord: Jip and Janneke spelen.
?Jip and Janneke are playing.?
Appo: De clown Bassie lacht.
?The clown Bassie is laughing.?
Prep: Ik begin met mijn werk.
?I start with my work.?
Figure 1: Types of syntactic relations extracted
4.1.2 Data collection
Because we believe that the method will remedy
data sparseness we applied the method to a medium-
sized corpus. Approximately 80 million words of
Dutch newspaper text.3 All data is parsed automat-
ically using the Alpino parser (van Noord, 2006).
The result of parsing a sentence is a dependency
graph according to the guidelines of the Corpus of
Spoken Dutch (Moortgat et al, 2000).
4.1.3 Syntactic co-occurrences
For each noun we find its syntactic contexts in the
data. This results in CO-OCCURRENCE VECTORS,
such as the vector given in Table 1 for the headword
kat. These are used to find distributionally similar
2We are working on Dutch and we are thus dealing with
Dutch data.
3This is the so-called CLEF corpus as it was used in the
Cross Language Evaluation Forum (CLEF). The corpus is a
subset of the TwNC corpus (Ordelman, 2002).
47
heb OBJ voer OBJ harig ADJ
?have OBJ? ?feed OBJ? ?furry? ADJ?
kat ?cat? 50 10 25
Table 1: Syntactic co-occurrence vector for kat
words. Every cell in the vector refers to a particular
SYNTACTIC CO-OCCURRENCE TYPE, for example,
kat ?cat? in object relation with voer ?feed?. The val-
ues of these cells indicate the number of times the
co-occurrence type under consideration is found in
the corpus. In the example, kat ?cat? is found in
object relation with voer ?feed? 10 times. In other
words, the CELL FREQUENCY for this co-occurrence
type is 10.
The first column of this table shows the HEAD-
WORD, i.e. the word for which we determine the
contexts it is found in. Here, we only find kat ?cat?.
The first row shows the contexts that are found, i.e.
the syntactic relation plus the accompanying word.
These contexts are referred to by the terms FEA-
TURES or ATTRIBUTES.
Each co-occurrence type has a cell frequency.
Likewise each headword has a ROW FREQUENCY.
The row frequency of a certain headword is the sum
of all its cell frequencies. In our example the row
frequency for the word kat ?cat? is 85. Cut-offs for
cell and row frequency can be applied to discard cer-
tain infrequent co-occurrence types or headwords,
respectively. We use cutoffs because we have too
little confidence in our characterisations of words
with low frequency. We have set a row cut-off of
10. So only headwords that appear in 10 or more
co-occurrence tokens in total are taken into account.
We have not set a cutoff for the cell frequency.
4.1.4 Measures and feature weights
Some syntactic contexts are more informative
than others. Large frequency counts do not always
indicate an important syntactic co-occurrence. A
large number of nouns can occur as the subject of the
verb hebben ?have?. The verb hebben is selectionally
weak (Resnik, 1993) or a LIGHT verb. A verb such
as voer ?feed? on the other hand occurs much less
frequently, and only with a restricted set of nouns
as direct object. Intuitively, the fact that two nouns
both occur as subject of hebben tells us less about
their semantic similarity than the fact that two nouns
both occur as the direct object of feed. The results
of vector-based methods can be improved if we take
into account the fact that not all combinations of
a word and syntactic relation have the same infor-
mation value. We have used POINTWISE MUTUAL
INFORMATION (PMI, Church and Hanks (1989)) to
account for the differences in information value be-
tween the several headwords and attributes.
The more similar the co-occurrence vectors of any
two headwords are, the more distributionally similar
the headwords are. In order to compare the vectors
of any two headwords, we need a similarity measure.
In these experiments we have used a variant of Dice:
Dice?, proposed by Curran and Moens (2002). It is
defined as:
Dice? = 2
?
min(wgt(W1, ?r, ?w?), wgt(W2, ?r, ?w?))?
wgt(W1, ?r, ?w?) + wgt(W2, ?r, ?w?)
We describe the function using an extension of the
notation used by Lin (1998a), adapted by Curran
(2003). Co-occurrence data is described as relation
tuples: ?word, relation, word??, for example, ?cat,
obj, have?.
Asterisks indicate a set of values ranging over all
existing values of that component of the relation tu-
ple. For example, (w, ?, ?) denotes for a given word
w all relations with any other word it has been found
in. W1 and W2 are the two words we are compar-
ing, and wgt is the weight given by PMI.
Whereas Dice does not take feature weights into
account, Dice? does. For each feature two words
share, the minimum is taken. If W1 occurred 15
times with relation r and word w? and W2 occurred
10 times with relation r and word w?, it selects 10
as the minimum (if weighting is set to 1). Note
that Dice? gives the same ranking as the well-known
Jaccard measure, i.e. there is a monotonic trans-
formation between their scores. Dice? is easier to
compute and therefore the preferred measure (Cur-
ran and Moens, 2002). Choices for measures and
weights are based on previous work (van der Plas
and Bouma, 2005).
4.2 Syntactic co-occurrences and nearest
neighbours
The syntactic co-occurrence vectors have co-
occurrence frequencies as values. An example is
given in Figure 2.
48
GRACHT ?canal?
97 Amsterdams ADJ ?Amsterdam ADJ?
26 ben SUBJ ?am SUBJ?
12 word SUBJ ?become SUBJ?
9 straat CONJ ?street CONJ?
9 gedempt ADJ ?closed ADJ?
8 Utrechts ADJ Utrecht ADJ
5 wal CONJ ?shore CONJ?
5 muur CONJ ?wall CONJ?
5 moet SUBJ ?has to SUBJ?
5 graaf OBJ ?ditch OBJ?
Figure 2: Syntactic co-occurrences for the word gracht
?canal?
To retrieve nearest neighbours, needed for the
third-order technique, we computed for each noun a
ranked list of most similar words using the method-
ology described in the two previous sections, i.e. by
comparing the weighted feature vector of the head-
word with all other words in the corpus. We col-
lected the 3 most similar nouns to all nouns. These
are the nearest neighbours that will be input to our
third-order system.
Now, how do we construct a second-order vec-
tor from these nearest neighbours? The cells of
the second-order vectors that we want to construct
should reflect the similarity between pairs of words.
The scores given to the pairs of words by the sys-
tem do not usually reflect the similarity very well
across different headwords and discriminates too lit-
tle between different nearest neighbours for a given
headword.
Instead we used the ranks or rather reversed ranks
for a given candidate word. However, the decrease
in similarity between the first candidate and the sec-
ond is not linear. It decreases more rapidly. After in-
specting the average decrease in similarity for near-
est neighbours, when going down the ranked list, we
decided to use a scoring method that is in line with
Zipf?s law (Zipf, 1949). We decided to attribute sim-
ilarity scores that are decreasing very rapidly for the
first ranks and less as we go down the ranked list of
nearest neighbours.
Apart from deciding on the slope of the similar-
ity score we needed to set a start value. We de-
cided to choose a start value according to the high-
est co-occurrence frequency (in the syntactic co-
occurrences) for that headword. So if a headword?s
GRACHT ?canal?
97 gracht ?canal?
48 laan ?avenue?
32 sloot ?ditch?
Figure 3: Nearest neighbours for the word gracht ?canal?
highest co-occurrence frequency was 100, a simi-
larity score of 100 is given to the word at the first
rank (that is itself) and a score of 50 to the candi-
date word at the second rank and so on. The in-
tuition between this is that we want to balance the
importance given to nearest neighbours and syntac-
tic co-occurrences. The importance of the nearest
neighbours will not tresspass the importance of the
syntactic co-occurrences.
The highest score will be given to the second-
order affinity between a headword and itself. This
seems an unnecessary addition, but it is not, because
we want canal to be similar to words that have canal
as a second-order affinity as well.
The second-order similarity score (SOSS) for a
given headword (h) and a given nearest neighbour
(nn) is defined as follows:
SOSS(h,nn) = max.freq.of.coocc(h)rank(nn)
We have given an example of the second-order
feature vector of the word gracht ?canal? in Figure 3.
As we see the highest score is given to second-order
affinity between the headword and the headword it-
self : gracht-gracht. This score is taken from the
highest co-occurrence frequency found for the word
gracht as can be seen in Figure 2. Second-order
feature vectors such as given in Figure 3 are con-
structed for all headwords to be used as input to the
third-order technique. For the combined technique
we concatenated both types of data. So the input to
the combined technique for the word canal would be
all its syntactic co-occurrences of which a subset is
given in Figure 2 plus the three nearest neighbours
given in Figure 3.
5 Evaluation
In the following subsections we will first explain
how we determined the semantic similarity of the re-
trieved nearest neighbours (subsection 5.1) and then
we will describe the test sets used (subsection 5.2).
49
5.1 EWN similarity measure and synonyms
Like most researchers in the field of distributional
methods we have little choice but to evaluate our
work on the resource that we want to enrich. We
want to be able to enrich Dutch EuroWordNet
(EWN, Vossen (1998)), but at the same time we use
it to evaluate on. Especially for Dutch there are
not many resources to evaluate semantically related
words available.
For each word we collected its k nearest neigh-
bours according to the system. For each pair of
words4 (target word plus one of the nearest neigh-
bours) we calculated the semantic similarity accord-
ing to EWN. We used the Wu and Palmer mea-
sure (Wu and Palmer, 1994) applied to Dutch EWN
for computing the semantic similarity between two
words.5 The EWN similarity of a set of word pairs
is defined as the average of the similarity between
the pairs.
The Wu and Palmer measure for computing the
semantic similarity between two words (W1 and
W2) in a word net, whose most specific common
subsumer (lowest super-ordinate) is W3, is defined
as follows:
Sim(W1,W2) = 2(D3)D1 + D2 + 2(D3)
We computed, D1 (D2) as the distance from W1
(W2) to the lowest common ancestor of W1 and W2,
W3. D3 is the distance of that ancestor to the root
node.
Some words returned by the system as near-
est neighbours cannot be found in EWN. Because
counting the words not found in EWN as errors
would be too harsh6 we select the next nearest neigh-
bour that is found in EWN, when encountering a not-
found word.
The Wu and Palmer measure gives an indication
of the degree of semantic similarity among the re-
4If a word is ambiguous according to EWN, i.e. is a member
of several synsets, the highest similarity score is used.
5This measure correlates well with human judgements (Lin,
1998b) without the need for sense-tagged frequency informa-
tion, which we believe is not available for Dutch.
6Dutch EWN is incomplete. It is about half the size of
Princeton WordNet (Fellbaum, 1998). Nearest neighbours that
are not found in EWN might be valuable additions that we do
not want to penalise the system too much for.
EWN similarity
k=1 k=3 k=5 k=10
VLF 2 0.391 0.378 0.364 0.350
2-3 0.395 0.392 0.376 0.359
3 0.413 0.412 0.411 0.410
LF 2 0.433 0.408 0.392 0.371
2-3 0.434 0.417 0.401 0.381
3 0.437 0.426 0.426 0.428
MF 2 0.644 0.605 0.586 0.555
2-3 0.646 0.608 0.589 0.561
3 0.643 0.608 0.589 0.575
HF 2 0.719 0.672 0.645 0.610
2-3 0.718 0.674 0.645 0.612
3 0.720 0.670 0.639 0.615
Table 2: EWN similarity several values of k for the four
test sets
trieved neighbours. The fact that it combines sev-
eral lexical relations, such as synonymy, hyponymy,
an co-hyponymy is an advantage on the one hand,
but it is coupled with the disadvantage that it is a
rather opaque measure. We have therefore decided
to look at one lexical relation in particular: We cal-
culated the percentage of synonyms according to
EWN. Note that it is a very strict evaluation and the
numbers will therefore be relatively low. Because
Dutch EWN is much smaller than Princeton Word-
Net many synonyms are missing.
5.2 Test sets
To evaluate on EWN, we have used four test sets of
each 1000 words ranging over four frequency bands:
high-frequency, middle frequency, low-frequency,
and very-low frequency. For every noun appearing
in EWN we have determined its frequency in the
80 million-word corpus of newspaper text. For the
high-frequency test set the frequency ranges from
258,253 (jaar, ?year?) to 2,278 (sce`ne, ?scene?). The
middle frequency test set has frequencies ranging
between 541 (celstraf, ?jail sentence?) and 364 (vre-
desverdrag, ?peace treaty?). The low-frequency test
set has frequencies ranging between 28 (ro?ntgenon-
derzoek, ?x-ray research?) and 23 (vriendenprijs,
?paltry amount?). For the very low frequency test
set the frequency goes from 9 (slaginstrument ?per-
cussion instrument?) to 8 (cederhout ?cedar wood?).
50
6 Results and discussion
In Table 2 the results of using second-order (2), com-
bined (2+3), and third-order (3) techniques is pre-
sented. The average EWN similarity is shown at sev-
eral values of k. At k=1 the average EWN similarity
between the test word and the nearest neighbour at
the first rank is calculated. For k=3 we average over
the top-three nearest neighbours returned by the sys-
tem and so on. Results are given for each of the four
test sets, the very-low-frequency set (VLF), the low-
frequency test set (LF), the middle-frequency test set
(MF), and high-frequency test set (HF).
We can easily compare the scores from the
second-order technique and the combined tech-
nique. The scores for the third-order technique is a
little more difficult to compare because, since there
is very little data, it is often not possible for all
test words to find the number of nearest neighbours
given under k. The coverage of the third-order tech-
nique is low, especially for the very-low to low-
frequency test set. Already at k=1 the number of test
word is about 60% and 70% (resp.) of the number
of nearest neighbours found when using the second-
order technique. For the middle and high-frequency
test set the number of nearest neighbours found is
comparable, but less for high values of k.
Let us compare the second-order and combined
techniques since coverage of these techniques is
more comparable.7 We see that the combined
method outperforms the second-order method for
almost all test sets. For the high frequency test
set there is no difference in performance and for
the middle-frequency testset the differences are very
small too. The largest improvements are for the
very-low-frequency and low-frequency test set. This
is expected, since the method was introduced to rem-
edy data sparseness and for these words data sparse-
ness is most severe. We can conclude that exploiting
the transitivity of meaning by augmenting the input
to the system with nearest neighbours from a previ-
ous round results in a higher degree of semantic sim-
ilarity among very-low and low-frequency words.
The differences in performance are small, but we
7In fact, the coverage of the combined method is a bit higher,
because it combines two types of data, but the differences are
not as big as between the third-order and the second-order tech-
nique.
Synonyms
k=1 k=3 k=5 k=10
HF
2 143(14.39) 276(9.26) 357(7.18) 461(4.64)
2+3 148(14.89) 275(9.22) 356(7.16) 465(4.68)
3 154(15.54) 259(8.84) 315(6.73) 382(5.26)
MF
2 105(10.56) 194(6.51) 245(4.93) 312(3.14)
2+3 109(10.97) 200(6.71) 250(5.03) 318(3.20)
3 107(11.38) 173(6.60) 198(5.07) 214(3.95)
LF
2 33(3.75) 65(2.47) 87(2.00) 108(1.28)
2+3 34(3.86) 73(2.77) 88(2.01) 113(1.32)
3 25(4.01) 41(3.18) 48(3.10) 54(3.20)
VLF
2 2(0.54) 4(0.36) 8(0.44) 10(0.30)
2+3 2(0.54) 4(0.36) 9(0.49) 10(0.29)
3 2(0.91) 2(0.50) 2(0.44) 2(0.42)
Table 3: Number of synonyms at several values of k for
the four test sets
should keep in mind that that EWN similarity does
not go from 0 to 1. The random baseline reported in
van der Plas (2008), i.e. the score obtained by pick-
ing random words from EWN as nearest neighbours
of a given target word, is 0.26 at k=5 and a score of
1 is impossible unless all words in the testset have k
synonyms.
To get a better idea of what is going on we in-
spected the nearest neighbours that are the output of
the system. There seemed to be many more syn-
onyms in the output of the combined method than
in the output of the second-order method. Because
synonymy is the lexical relation that is at the far end
of semantic similarity, it is important to find many
synonyms. To quantify our findings we determined
the number of synonyms among the nearest neigh-
bours according to EWN.
In Table 3 the number of synonyms as well as the
percentage of synonyms found at several values of k
is shown.8
Our initial findings proved quantifiable. The
combined technique (2+3) results in more syn-
onyms. Most surprising are the results for the high-
frequency testset. Whereas, based on evaluations
with the EWN similarity scores, we believed the
method did not do much good for the high-frequency
8At k=n we do not always find n nearest neighbour for all
words in the test set. That is the reason for showing both counts
and percentages in the table.
51
Second-order Combined
cassette videoband bandje CDi cassette
cassette videoband bandje CDi cassette
videoband cassette cassette DCC videoband
CDi videofilm videoband CD bandje
Figure 4: Nearest neighbours for videoband ?video tape?,
cassette ?cassette? bandje ?tape? and CDi ?CDi?
method, we now see that the number of synonyms
found is higher when using the combined technique,
especially at k=1. This holds for all but one test
set. Only for the very low frequency test set there
is hardly any difference.
We explained before that coverage of the third-
order technique is low. However, we see that the
technique results in higher numbers of synonyms
found at k=1 for the high-frequency (+11) and the
middle-frequency test set (+2). At higher values of
k the absolute numbers are smaller for the third-
order technique and also for the low and very-low-
frequency test set. This is to be expected because
the number of nearest neighbours found dramati-
cally decreases, when using a third-order technique
on its own. But it is surprising that we are able to ex-
tract more synonyms, when using only the two near-
est neighbours (plus the headword itself) computed
by the system before as input.
Manual inspection showed that what happens is
that nearest neighbours that have each other as near-
est neighbour are promoted. As can be seen in Fig-
ure 4, cassette ?cassette? has videoband ?video tape?,
and CDi as nearest neighbour. Because CDi has no
nearest neighbours in common with cassette, except
itself, it is demoted in the output of the combined
method. The word bandje ?tape? has two neighbours
in common with cassette. Bandje is promoted in the
output of the combined method.
This finding bring us to work by Lin (1998a),
where the author shows that, when selecting only
respective nearest neighbours (words that have each
other as the one most nearest neighbour), the results
are rather good. Our technique incorporates that no-
tion, but is less restricted, especially in the combined
technique.
7 Conclusion and future work
Guided by the idea of the transitivity of mean-
ing we have shown that by augmenting syntactic
co-occurrences (that are usually input to distribu-
tional methods) with nearest neighbours (the output
of the system from a previous round) we are able
to improve the performance on low- and middle-
frequency words with respect to semantic related-
ness in general. This result is encouraging, because
distributional methods usually perform rather poorly
on low- and middle-frequency words. In addition,
these are the words that are most sought after, be-
cause they are the ones that are missing in existing
resources. There is something to be gained for the
high-frequency to low-frequency words in addition.
The percentage of synonyms found is larger when
using combined techniques.
In future work we are planning to implement
a more principled way of combining syntactic-co-
occurrences and nearest neighbours. The method
and results presented here sufficed to support our in-
tuitions, but we believe that more convincing num-
bers could be attained when fully exploiting the prin-
ciple. Since the method uses a combination of la-
belled and unlabelled data (although in our case
the labelling is the result of the same unsupervised
method and not of manual annotation), we plan
to consult the literature on co-training (Blum and
Mitchell, 1998). Also, instead of expanding the
syntactic co-occurrences of words with their nearest
neighbours we could expand them with the syntactic
co-occurrences of their nearest neighbours to arrive
at more uniform data. Lastly, the technique allows
for iteration. We could measure the performance at
several iterations.
Acknowledgements
The research leading to these results has received
funding from the EU FP7 programme (FP7/2007-
2013) under grant agreement nr 216594 (CLASSIC
project: www.classic-project.org) and from NWO,
the Dutch Organisation for Scientific Research in the
framework of the research program for Interactive
Multimedia Information eXtraction, IMIX.
52
References
E. Alfonseca and S. Manandhar. 2002. Extending a lexi-
cal ontology by a combination of distributional seman-
tics signatures. In Proceedings of EKAW.
C. Biemann, S. Bordag, and U. Quasthoff. 2004. Auto-
matic acquisition of paradigmatic relations using iter-
ated co-occurrences. In Proceedings of LREC.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In Proceedings of the
1998 conference on computational learning theory.
K.W. Church and P. Hanks. 1989. Word association
norms, mutual information and lexicography. Pro-
ceedings of the Annual Conference of the Association
of Computational Linguistics (ACL).
J.R. Curran and M. Moens. 2002. Improvements in auto-
matic thesaurus extraction. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP, pages 222?229.
J.R. Curran. 2003. From Distributional to Semantic Sim-
ilarity. Ph.D. thesis, University of Edinburgh.
I. Dagan, L. Lee, and F. Pereira. 1999. Similarity-based
models of word cooccurrence probabilities. Machine
Learning, 34(1-3):43?69.
P. Edmonds. 1997. Choosing the word most typical in
context using a lexical co-occurrence network. In Pro-
ceedings of the European chapter of the Association
for Computational Linguistics, pages 507?509.
C. Fellbaum. 1998. WordNet, an electronic lexical
database. MIT Press.
G. Grefenstette. 1994. Corpus-derived first-, second-,
and third-order word affinities. In Proceedings of Eu-
ralex.
Z.S. Harris. 1968. Mathematical structures of language.
Wiley.
L. Lee. 1999. Measures of distributional similarity. In
37th Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
B. Lemaire and G. Denhire. 2006. Effects of high-order
co-occurrences on word semantic similarities. Current
Psychology Letters - Behaviour, Brain and Cognition,
18(1).
D. Lin. 1998a. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of COLING/ACL.
D. Lin. 1998b. An information-theoretic definition of
similarity. In Proceedings of the 15th International
Conference on Machine Learning.
M. Moortgat, I. Schuurman, and T. van der Wouden.
2000. CGN syntactische annotatie. Internal Project
Report Corpus Gesproken Nederlands, available from
http://lands.let.kun.nl/cgn.
R.J.F. Ordelman. 2002. Twente nieuws corpus (TwNC).
Parlevink Language Techonology Group. University
of Twente.
P. Resnik. 1993. Selection and information. Unpub-
lished doctoral thesis, University of Pennsylvania.
H. Schu?tze and M. Walsh. 2008. A graph-theoretic
model of lexical syntactic acquisition. In Proceedings
of EMNLP.
A. Tversky and I. Gati, 1978. Cognition and Categorisa-
tion, chapter Studies of similarity, pages 81?98. Erl-
baum.
L. van der Plas and G. Bouma. 2005. Syntactic contexts
for finding semantically similar words. In Proceed-
ings of Computational Linguistics in the Netherlands
(CLIN).
L. van der Plas. 2008. Automatic lexico-semantic acqui-
sition for question answering. Ph.D. thesis, University
of Groningen.
G. van Noord. 2006. At last parsing is now operational.
In Actes de la 13eme Conference sur le Traitement Au-
tomatique des Langues Naturelles.
P. Vossen. 1998. EuroWordNet a multilingual database
with lexical semantic networks.
J. Weeds and W. Weir. 2005. Co-occurrence retrieval: A
flexible framework for lexical distributional similarity.
Computational Linguistics, 31(4):439?475.
J. Weeds. 2003. Measures and Applications of Lexical
Distributional Similarity. Ph.D. thesis, University of
Sussex.
D. Widdows. 2004. Geometry and Meaning. Center for
the Study of Language and Information/SRI.
Z. Wu and M. Palmer. 1994. Verb semantics and lexical
selection. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL).
G.K. Zipf. 1949. Human behavior and the principle of
the least effort. Addison-Wesley.
53
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1047?1058, Dublin, Ireland, August 23-29 2014.
What good are ?Nominalkomposita? for ?noun compounds?:
Multilingual Extraction and Structure Analysis
of Nominal Compositions using Linguistic Restrictors
Patrick Ziering Lonneke van der Plas
Institute for Natural Language Processing
University of Stuttgart, Germany
{Patrick.Ziering, Lonneke.vanderPlas}@ims.uni-stuttgart.de
Abstract
Finding a definition of compoundhood that is cross-lingually valid is a non-trivial task as shown
by linguistic literature. We present an iterative method for defining and extracting English noun
compounds in a multilingual setting. We show how linguistic criteria can be used to extract
compounds automatically and vice versa how the results of this extraction can shed new lights
on linguistic theories about compounding. The extracted compound nouns and their multilingual
contexts are a rich source that serves several purposes. In an additional case study we show how
the database serves to predict the internal structure of tripartite noun compounds using spelling
variations across languages, which leads to a precision of over 91%.
1 Introduction
Compounding is a phenomenon that is studied extensively in linguistic literature. Also in computational
linguistics, compounds are enjoying more and more attention (
?
O S?eaghdha, 2008; Hendrickx et al.,
2013). Compounding is a very productive word formation. Already 2-year-olds are able to form new
words by using compounds consisting of two morphemes (Clark, 1981). As a consequence, compounds
are a very common word type but many occur with a very low token count. In an analysis of the German
APA corpus, Baroni et al. (2002) found that almost half (47%) of the word types were compounds. At
the same time, the compounds accounted for a small portion of the overall token count (7%), which
suggests that many of them are rare (83% of the compounds had a corpus frequency of 5 or lower). For
English, more than half of the two-noun compounds (e.g., car park) in the BNC occur exactly once (Kim
and Baldwin, 2006). The high productivity of compounds makes compositional approaches to automatic
processing indispensable: listing all possible compounds in a dictionary would be as infeasible as listing
all possible adjective-noun combinations. Even for compound nouns that occur 10 times or more in the
BNC, static English dictionaries provide only 27% coverage (Tanaka and Baldwin, 2003).
Being abundant as a phenomenon but scarce in terms of individual examples (the combination of
high type frequency and low token frequency) makes the analysis of these compound nouns particularly
problematic for statistical techniques that need high token frequencies to make accurate predictions. Data
sparsity is expected to lead to low performance. However, the correct analysis of compound nouns is
important for a number of NLP tasks, for example in machine translation (Bouillon et al., 1992; Rackow
et al., 1992; Johnston and Busa, 1999; Navigli et al., 2003). The accurate translation of compounds is
non-trivial, because we find a large amount of variation in the way languages deal with compounding.
Some languages such as German use closed compounding (i.e., they create one-word compounds, e.g.,
Todesstrafe (death penalty)) whereas others do not. In Romance languages, such as French, compounds
are not as productive, instead postmodifying prepositional phrases (e.g., peine de mort) and adjectives
(peine capitale) are used to construct complex nominals.
Another challenge in compound translation is due to the fact that the amount of underspecification in
compound surface structure varies between languages. For example, whereas English leaves the com-
pound relation (i.e., the semantic relation between two components, e.g., N
2
made of N
1
as in iron door)
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1047
covert, in French we find prepositions that correlate with the relation type (Girju, 2007; Celli and Nissim,
2009). Chocolate cake, cake made of chocolate, is translated with gateau au chocolat, whereas wedding
cake, cake made for a wedding, is gateau de marriage.
The first aim of this study is to extract a large database of compounds and their translations in context
from a parallel corpus. This database will serve multiple purposes. For example, it will be used to study
compounding across different languages, and we will exploit the cross-lingual variation for compound
processing. In the second part of this paper, we will show a case study of how the extracted database
can be used for analysing the structure of noun phrases, more specifically, we exploit spelling variations
across languages for bracketing three-noun compounds (3NCs) such as air traffic control, which could be
indicated as LEFT bracketing using the German phrase Kontrolle des Luftverkehrs (control of air traffic).
Compounding is an important subject of study in theoretical linguistics, because it constitutes a con-
tinuum from fully compositional to idiosyncratic word formation and is found at the boundary between
words and phrases. However, there is virtually no reliable and universally accepted criterion for distin-
guishing compounds from phrases or other types of word formations, as stated by Lieber and Stekauer
(2009). They discuss reasons for the complexity that arises when defining noun compounds, that we will
review in the next section. They do, however, also describe a number of linguistic tests, each with their
own advantages and drawbacks.
This brings us to the second aim of this paper. We propose an iterative method that, in absence of a
clear definition, validates several linguistic tests on corpus data and continuously refines the definition.
We show how we use linguistic tests to extract compounds automatically and vice versa how the results
of this extraction can shed new lights on linguistic theory about compoundhood. The multilingual nature
of our data (we work on parallel corpora) has the additional advantage that a cross-lingual definition can
be sought by studying compounds in context and their translation across several languages.
In Section 2, we discuss the problem of defining noun compounds (NCs) as described by Lieber and
Stekauer (2009) and present an iterative method for defining and extracting English NCs starting with
an initial definition based on some linguistic tests. In Section 3, we present our method for extracting
English NCs and their translations to several languages from a parallel corpus using a set of extraction
rules. An experimental setup and results are presented in Section 4. In Section 5 we show in a case
study of bracketing three-noun compounds how our database serves for exploiting multilingual spelling
variations. Section 6 describes related work and finally Section 7 concludes.
2 Iterative method for the definition and extraction of noun compounds
In this section, we outline the controversy of defining compoundhood as described in linguistic literature.
We present several linguistic tests for distinguishing compounds and show how we implement some
linguistic criteria that can be used for identification and extraction of noun compounds and how these
constitute the initial definition.
2.1 Definition of compounds and linguistic criteria
When we seek to find a working definition of noun compounds (NCs), we have to keep in mind that not
only the definition but also the existence of such an NC is controversial. Lieber and Stekauer (2009)
present a discussion about this controversy sketched below. While Bauer (2003) defines a compound as
a ?formation of a new lexeme by adjoining two or more lexemes?, Marchand (1967) argues that there
is no compounding word formation at all. Instead, he uses the word formation EXPANSION, which
combines prefixed words like reheat with such as steamboat using the criterion of a free head. Lieber
and Stekauer (2009) highlight two reasons for the complexity that arises when defining noun compounds.
Firstly, in some languages, constituents are not free but stems or roots. For example, the Slovak term
r?ychlovlak (express train) starts with the stem of the adjective r?ychly (as in the phrase r?ychly vlak (fast
train)). The lack of inflection in English makes compositional and phrasal structures (i.e., fast train as
phrase or as compound (express train)) collapse. Secondly, sometimes phrases and derivations cannot be
distinguished from compounds. While blackboard (in opposition to a black board) can be classified as
compound without dissent, a tomato bowl that just happens to hold tomatoes might not be regarded as a
1048
single lexeme (conforming Bauer?s (2003) definition).
So, the only way for getting a suitable definition is to find solid criterions. Although Lieber and
Stekauer (2009) come to the conclusion that there is almost no reliable and universally accepted criterion,
they mention several plausible tests. Compounds can be identified by prosody. While in the phrase black
bird, the head (bird) is stressed, in the compound blackbird the stress is on the first syllable (black).
A syntactic test mentioned by Lieber and Stekauer (2009) is inseparability, i.e., there must not be any
element intervening a compound?s components. While black bird can be understood as compound, black
ugly bird is a phrase. Another promising syntactic criterion is the inability to modify the first element
(i.e., the modifier) of a compound. In a phrase like social person, the first element (social) can be
modified (i.e., very social person). This is not possible for compounds (e.g., very social policy). A last
syntactic criterion, the inability to replace the second noun of a nominal compound with a proform such
as one (e.g., black bird vs. black one), would need human support. A morphological criterion states that
in compounds only the head is inflected. Although this assumption does not always hold (as shown in
examples like overseas investor or girls club), this seems to be a promising criterion when investigating
inflectional behaviour in aligned languages that show strong morphology, e.g., French. Conversely,
determining compoundhood on the basis of spelling is discarded by Lieber and Stekauer (2009). English
orthography is highly inconsistent: some compounds usually occur as a closed compound (e.g., football),
some occur hyphenated and some occur as an open compound (e.g., waiting room or rule of law). For
some compounds, several spellings are possible (e.g., flowerpot, flower-pot, flower pot or pot of flowers).
In our study, we focus on written language as given in a parallel corpus. Since we do not have any
speech data, we cannot use any phonological features such as stress for the extraction of noun com-
pounds. For the inability to replace the second noun of a nominal compound with a proform, we cannot
assess if the meaning of a sentence would have changed (e.g., We see blackbirds vs. We see black ones).
In this paper, we focus on criteria that are most suitable with the current data. Although Lieber and
Stekauer (2009) exclude spelling as a reliable criterion of compoundhood, we take it as starting point.
The parallel corpus we use for the extraction includes several languages. Spelling variations between
languages can be exploited to find compounds (e.g., social policy can be written as one word in German
(Sozialpolitik)). We account for the English spelling variations by defining part-of-speech (PoS) patterns
that cover most plausible spellings. These PoS patterns treat each noun or adjective as a compound?s
component and thus, this way of extraction inherently implements the criterion of inseparability. We
exploit multilingual evidence in terms of cross-lingual differences in spelling to extract compounds.
Diverse language families have different declinations of forming a closed compound. While languages
like Danish and German prefer closed compounding, English and Romance languages like Spanish use
open compounds. It is this spelling variation that we base our first set of extraction rules on with the aim
of having a set of English NCs and their translations in up to 9 European languages. We will show that
cross-lingual closed compounding is a promising feature for extracting English NCs.
The inability to modify the first element of a compound seems to be a promising test. Since there
are many linguistic factors that have to be taken into account (e.g., morphological agreement in gender,
number or case), we plan to include this criterion for several languages and any combination of contextual
modifier and potential noun compound. We will implement this and further morphological criteria in
future work.
2.2 Initial definition for compound extraction
With a focus on multilingual validity, we adapt the definition of Bauer (2003) to our multilingual setting.
Inspired by Behagel?s (1909) First Law (?Elements that belong close together intellectually will also be
placed close together?), we associate a closed compounding language realising an English word sequence
as a closed NC with an indicator for compoundhood:
Initial definition: A noun compound is a nominal composition of several lexemes that are represented
as a one-word expression in some of the languages studied.
This definition covers both target single words (e.g., blackbird translates to German as Amsel) or target
1049
closed compounds (e.g., football match translates to Dutch as voetbalwedstrijd).
We are aware of the fact that this definition leads to some controversial cases for English word se-
quences including pre-nominal adjectives. While some of them are commonly accepted such as social
policy (German: Sozialpolitik), others are less accepted such as strong wind (German: Starkwind) or
small car (German: Kleinwagen). This is not an unwanted side-effect. On the contrary, these contro-
versial cases are an essential part of the iterative process we described, as they will foster linguistic
discussions. Although the German Starkwind can be regarded as partly compositional, it is frequently
used with a concrete definition (in contrast to the phrase starker Wind) and cannot occur in a context
violating this definition, as shown in the table below.
1 a) Als Starkwind wird meist eine Windst?arke zwischen 6 und 7 Beaufort bezeichnet.
1 b) A {strong wind} usually refers to a wind force of 6-7 Beaufort.
2 a) Am Samstag weht ein starker Wind mit Windst?arke 8 von Westen.
2 b) On Saturday, a strong wind with wind force 8 will blow from the west.
3 a) *Am Samstag weht ein Starkwind mit Windst?arke 8 von Westen.
3 b) On Saturday, a *{strong wind} with wind force 8 will blow from the west.
3 Multilingual extraction of NCs
This method is based on the initial definition for compound extraction described in Section 2.2 and can be
adapted in succeeding iterations. English NCs are extracted from a parallel corpus that includes English
and some closed compounding languages (e.g., German).
3.1 Preprocessing the parallel data
In Section 4.1, we describe the tokenization, sentence alignment, word alignment and PoS tagging we
apply to the parallel data in more detail. In addition, we perform a binary compound splitter on each
word that is tagged as a noun by following a variant of the methods of Stymne et al., (2013). This un-
supervised splitter checks each noun for all possible segmentations into at most two components with at
least two characters. All possible segmentations are scored with the geometric mean of the components?
frequencies in the parallel corpus. The highest-scored segmentation (possibly with no split point) is used.
3.2 Preselection of English noun compounds using PoS patterns
As a basis for the extraction of English NCs, we use a set of possible English PoS sequences that can
constitute an NC. These PoS patterns account for the various ways of composing English NCs and for the
inseparability property as described in Section 2.1. Table 1 lists all plausible PoS patterns for bipartite and
tripartite NCs with some examples (cf. the Penn Treebank tag set (Marcus et al., 1993)). For all examples
in Table 1, we found translations to a closed compound in German, which satisfies our initial definition
described in Section 2.2, e.g., overall recovery rate has been translated to Gesamtr?uckforderungsquote.
Although the larger the number of components, the sparser the number of (correct) extractions, we create
a regular expression for PoS patterns that cover English NCs with n components (where 2 ? n ? 10).
This regular expression combines all possible combinations of observed NC types. In the next step, we
will filter noise, that occurs mostly in longer word sequences.
3.3 Noise filters
The selection of English NCs and their translations is based on automatic preprocessing, which leads to
some noise due to false PoS tags or flaws in word alignment. With increasing word sequence length, the
amount of noise increases. We apply several filters on each preselected NC and on their alignments to
all other languages in the corpus and keep only those that pass all filters.
3.3.1 PoS filters
1. Two filters are applied to all languages: we disqualify word sequences including nouns or adjectives
that (1) consist of only one character or (2) are contained in a stop list
1
.
1
ranks.nl/stopwords
1050
PoS pattern Example
Bipartite noun compounds
NN marketplace
NN NN death penalty
JJ NN structural policy
NN POS NN children?s development
NN IN NN fall in population
NN IN DT NN concussion of the brain
Tripartite noun compounds
NN NN NN energy security goal
JJ NN NN overall recovery rate
NN IN NN IN NN income per head of population
Regular expression for 2?10 components
NN ((IN (DT)?|POS))? NN){1,9} greenhouse gas emission allowance trading scheme
JJ NN ((IN (DT)?|POS))? (JJ)? NN){1,8} internal energy market package
Table 1: English PoS sequences for noun compounds
2. Then, to account for PoS tagging errors in English, we collect all words and their PoS tags in the
parallel corpus. For each word, we compute the probability of being tagged as a noun or adjective
as given in (1).
P (noun/adj | word) =
f((noun ? adj) ? word)
f(word)
(1)
We disqualify English word sequences, if they contain a noun or adjective w with
P (noun/adj | w) < ?. After testing several values for ?, we have decided to choose ? = 0.15
because it has turned out to be a promising trade-off between coverage and precision (e.g., accept-
ing words like human but rejecting words like anywhere).
3.3.2 Word alignment filter
Shortcomings in word alignment quality are remedied with three word alignment filters.
1. We truncate extraneous words (i.e., determiners, prepositions and (ad)verbs) from the border of the
word sequence (adjectives are removed from the right border for Germanic languages and from the
left border for Romance languages).
2. We disqualify the word sequence as being phrasal if it contains two consecutive nouns with verbs or
adjectives in between or if the nouns are more than ? tokens apart from each other. When analysing
many instances of Romance phrases aligned to an English noun compound, we observed that ? = 3
is the maximum token distance two nominal components can be apart (usually separated by prepo-
sition or preposition+determiner). If the word sequence is qualified as phrasal, we add determiners
and prepositions that occur in the context between the nouns, otherwise the word sequence remains
unchanged.
3. We remove the word sequence if it does not contain at least one noun.
The resulting set of English word sequences that conform to the regular expression in Table 1 and their
aligned and filtered word sequences are stored as a set of m-tuples of word sequences. Subsequently,
we will refer to this set as the basic set. The basic set still contains English word sequences that do not
comply with our initial definition for compound extraction (Section 2.2), i.e., that are not aligned to a
closed compound. In the next step, we apply a restrictor to all NCs in the basic set and keep only those
instances that pass the restrictor.
1051
3.4 Closed compound restrictor
An English word sequence is considered to be an NC if it is represented as a one-word expression in some
of the closed compounding languages (e.g., Dutch, German, Swedish, . . . ). Given a parallel corpus with
n > 1 closed compounding languages, this definition leaves space for investigating the degree of cross-
lingual closed compounding (deg
closed
) which is necessary for optimal extraction quality (i.e., optimal
precision and recall). Because the rules described in Section 3.3.2 still leave some word alignment errors
(i.e., English word sequences that are aligned to only a part of the true translation), a single compounding
language realising the English word sequence as one word (i.e., deg
closed
= 1) might not be restrictive
enough.
The closed compound restrictor with deg
closed
? i retains only English word sequences that are
aligned to at least i one-word expressions in the aligned closed compounding languages. We will refer
to this restrictor as CCR(i) and to the resulting data set as closed compound(i).
4 Experiments for NC Extraction
4.1 Setup
Data and preprocessing. We use the 7th release of the Europarl corpus
2
. Although the Europarl corpus
comprises 21 European languages, the amount of common data they cover is rather small. This means,
the more languages we use, the smaller the amount of common data. In order to get a good trade-off
between cross-lingual coverage and language variation exploitation, we decided on a set of 10 languages:
English, the closed compounding languages Danish, Dutch, German and Swedish, as well as Greek and
the Romance languages French, Italian, Portuguese and Spanish. Instead of preprocessing the parallel
corpus on our own, we exploit the already preprocessed Europarl resource of OPUS
3
(Tiedemann, 2012).
This preprocessed resource is PoS tagged using TreeTagger (Schmid, 1995) for English, Dutch, German,
French, Italian and Spanish and the Hunpos
4
tagger for Danish, Portuguese and Swedish. We additionally
tagged the Greek data using the MATE
5
tagger. The sentence alignment provided by OPUS is restricted
to language pairs. As we need a sentence representation that is parallel in all 10 languages, we apply
the OPUS sentence aligner (with English as pivot) on our language set and extract a total of 884,164
parallel sentence representations. The word alignment information provided by OPUS was also based on
language pairs. This means, the sentence-wise token indices has to be adapted to our updated sentence
representation (which is different due to a larger language set). In OPUS, the word alignment tool GIZA++
(Och and Ney, 2003) has been used with the symmetrisation heuristics (grow-diag-final-and (Koehn et
al., 2007)).
4.2 Evaluation procedure and scoring
In order to compare the added value in terms of recall and precision of each closed compound restrictor
(i.e., CCR(1) to CCR(4)), we randomly select 50 accepted and 50 rejected English word sequences for
each restrictor. We rate the correctness of acceptance and rejection and compute precision and recall as
given in (2) and (3). F-Score is defined as harmonic mean of precision and recall.
Precision =
accepted ? correct
accepted
(2)
Recall =
accepted ? correct
(accepted ? correct) ? (rejected ? incorrect)
(3)
The precision of the basic set is measured as the accuracy of a 50 sample subset. We do not compute
recall and F-Score for the basic set.
2
statmt.org/europarl
3
opus.lingfil.uu.se
4
code.google.com/p/hunpos/downloads/list
5
code.google.com/p/mate-tools
1052
We measure the amount of closed NCs in a given closed compounding language (ccl) and for a given
set of NCs (Set) by using the frequency of closed NCs relative to the number of all word sequences
(N
Set,cll
) (word sequences removed in Section 3.3 are excluded). Since the alignment to single words is
still somewhat noisy (i.e., our compound splitter does not work error-free and there are still deficiencies
in the word alignment), we select a set of 50 closed noun compound samples and rate the accuracy. The
final amount of closed NCs is the product of relative frequency and accuracy, as given in (4).
p
ccl
(Set) =
f
Set
(closed NC)
N
Set,cll
? acc
ccl
(Set) (4)
4.3 Results
Set Size Precision Recall F-Score p
en
Basic set 3,178,661 38.0% ? ? 1.5%
closed compound (1) 795,518 84.0% 71.2% 77.1% 4.7%
closed compound (2) 495,837 92.0% 74.2% 82.1% 6.6%
closed compound (3) 316,330 98.0% 65.3% 78.4% 9.2%
closed compound (4) 143,121 98.0% 63.6% 77.2% 10.4%
Table 2: Extraction quality of the basic set after restrictor application
Table 2 shows the results when applying the four different degrees of the closed compound restrictor
to the basic set. The first result is that using only a PoS-based method leads to a very poor extraction
accuracy (38%). For the applications of the closed compound restrictors, the result is that increasing
deg
closed
means increasing precision but decreasing recall in NC extraction. The reason for this is that an
aligned closed NC is generally a sufficient condition for an English NC (except for controversial cases
such as strong wind) but not a necessary condition (i.e., a true English NC may be aligned to only pe-
riphrastic constructions). The highest F-Score (82.1%) is achieved using CCR(2). We can conclude that
the closed compound restrictor is a reliable method for extracting English NCs. In future work, we will
use a large set of human annotators with different backgrounds in order to get a widely distributed sense
of compoundhood. Moreover, instead of a binary rating, we will consider compoundhood as a continuum
and compare rating scores with the amount of aligned closed compounding languages realising a closed
compound in a larger parallel corpus.
The last column in Table 2 shows the amount of closed English NCs in each respective set. Since
deg
closed
correlates with the amount of closed English NCs, we can conclude that, despite the cross-
lingual differences in spelling conventions attested in linguistic literature, there is a bias for a universal
consensus in closed compounding.
Language p
ccl
German 71.2%
Danish 63.3%
Swedish 62.2%
Dutch 58.7%
Table 3: The amounts of closed noun compounds
Table 3 shows the amounts of closed noun compounds in the closed compounding languages Danish,
Dutch, German and Swedish, extracted from the closed compound (1) set. Our result shows that German
is the most productive language in closed compounding (71.2%), while the other languages have a similar
productivity (58-63%).
The result of our extraction method is a database of English NCs and their translations in up to 9
European languages. As described in the introduction, this database will serve several purposes. One is to
study cross-lingual variation. Table 4 shows some examples of multilingual noun compound extractions
from closed compound (2).
1053
English German Dutch French Italian
automotive sector Automobilmarkt automobielsector secteur automobile mercato dell? automobile
fishing techniques Fischfangtechniken visserijmethoden techniques de p?eche tecniche di pesca
timetable Zeitplan tijdschema calendrier calendario
highways Autobahnen snelwegen autoroutes autostrade
trading system Handelssystem handelsbestel syst`eme commercial sistema di scambi
Table 4: Examples of multilingual noun compounds
The examples show that English noun compounds have various realisations in European languages.
Although French and Italian are open compounding languages, we do find closed compounding (e.g.,
autoroutes). Compounds such as timetable can also be aligned to single nouns such as calendrier (cal-
endar). We found three common word formation types in Romance languages for bipartite noun com-
pounds: (1) two nouns and a preposition in between, (2) one noun and a post-nominal adjective and (3)
a single (possibly compounding) noun. Although Romance languages usually agree with respect to the
word formation type, they may disagree as is the case for French and Italian for the example concern-
ing trading system. One interesting observation is that while the head of highways (ways) is translated
fairly literally, the modifier (high) is replaced by alternative aspects. On highways, cars (Autobahnen
(car-ways)) usually drive fast (snelwegen (fast-ways)). In future work, we will use this database for re-
searching the nature of compoundhood in a cross-lingual perspective. The resource is publicly available
for future research
6
.
5 Bracketing three-noun compounds
In this section, we show a case study of how our extracted database can be used to predict the structure
of NPs, more specifically to bracket tripartite noun compounds (3NCs), i.e., a composition of three bare
nouns that function as one unit. Given a 3NC, we can either have RIGHT bracketing, as in baby [bicycle
seat], or LEFT bracketing, as in [human rights] abuses.
5.1 The cross-lingual bracketing method
We first start with six phrase patterns that correspond to foreign phrases that are aligned to an English
3NC, as shown in Table 5, where SN refers to a single (non-compounding) noun, FC refers to a func-
tional context (i.e., a sequence of functional words), ADJ refers to an adjective and CNC refers to a
closed (bipartite) NC (based on the splitter described in Section 3.1). Each phrase pattern contains a
complex unit that is separated from the rest, e.g., a closed NC or a combination of adjective and single
noun. For each pattern, we know what is the head and what is the modifier: the first phrase pattern
contains only one nominal component, that can be identified as head. For the other patterns, the order
is: head, FC, modifier. Based on the assumption that the aligned head corresponds to the English head,
we can infer the English bracketing from the complexity of the aligned head. If the aligned head is the
complex unit, the English bracketing label is RIGHT, otherwise LEFT. The third column in Table 5 shows
the inferred labels for the English 3NC based on the foreign phrase pattern. For an English 3NC, we
check all aligned languages for a matching phrase pattern and collect, in the case of a match, the inferred
label. The majority label determines the final bracketing label.
The examples below illustrate instances for each phrase pattern, where the indices correspond to those
in Table 5.
6
www.ims.uni-stuttgart.de
1054
Phrase pattern in foreign language Label for English 3NC
(1) ADJ CNC RIGHT
(2) CNC FC SN RIGHT
(3) SN FC CNC LEFT
(4) SN FC ADJ SN LEFT
(5) ADJ SN FC SN RIGHT
(6) SN ADJ FC SN RIGHT
Table 5: Phrase pattern and inferred label
(1) de: staatliche
state
Steueraufsichtsbeh?orden
{tax inspectorates}
?state tax inspectorates?
(2) de: Absatzmarkt
{sales market}
f?ur
for
Fahrzeuge
vehicles
?car sales market?
(3) nl: methode
method
voor
for
geboortebeperking
{birth control}
?birth control method?
(4) sv: brottet
abuses
mot
of
m?anskliga
{human
r?attigheterna
rights}
?human rights abuses?
(5) da: gennemsnitlige
{average
overf?rsel
transfer}
af
of
data
data
?data transfer rate?
(6) es: consumo
{consumption
final
final}
de
of
energ??a
energy
?energy end consumption?
We observed that the initial assumption (saying that the aligned head corresponds to the English head)
is not always true. Sometimes the English head and modifier are swapped in aligned languages, as
illustrated in example (7).
(7) nl: stabiele
stable
wisselkoersen
{exchange rate}
?exchange rate stability?
To solve this problem, we inspect the word alignment from the phrase pattern of language l
j
to the
English nouns N
1
, N
2
and N
3
in a 3NC. If the complex unit is aligned to {N
2
, N
3
} or to {N
1
, N
3
}, l
j
provides the label RIGHT. If the complex unit is aligned to {N
1
, N
2
}, l
j
votes for LEFT. If the complex
unit is aligned to all three nouns, this is an indicator for a word alignment error. In this case, l
j
will not
perform any prediction. In all other cases, the inferred label from the phrase pattern is used.
5.2 Evaluation for cross-lingual bracketing
As there are only two possible structures for 3NCs, namely LEFT or RIGHT branching, we regard this
task as a binary classification and score the accuracy of class agreement. As basis, we use the basic set
created in Section 3, because alignments to closed compounds are not of interest for the bracketing task.
Two trained human annotators (of which one is one of the authors) individually bracket a sample of 100
randomly selected 3NCs in context. Contextual cues can help the annotator to disambiguate the structure
of the English NC, so the accompanying sentences are shown to the annotator. The annotators are no
domain experts and since terms in Europarl can be quite domain specific, they are allowed to look up the
meaning of the constituents in a dictionary or check Google. Annotators are asked to label 3NCs as LEFT
or RIGHT, or UNDECIDED if they are unclear. Furthermore, the annotators are asked to mark extraction
errors. When inspecting the inter-annotator agreement for the bracketing classes (LEFT/RIGHT; i.e., 76
of 100 samples), we achieved an agreement rate of 89% and ? = 0.693 (Cohen, 1960), which means
substantial agreement (Landis and Koch, 1977). Afterwards, the annotators discuss disagreements and
revise their annotations. This has led to a perfect agreement in our setting. The 8 UNDECIDED labellings
show that in some cases the bracketing remains ambiguous even in context. In future work, we would like
to investigate if larger contexts or domain knowledge is necessary for the disambiguation process or if
the NCs are inherently flat (i.e., if LEFT or RIGHT bracketing does not make any difference in meaning).
We evaluate our cross-lingual bracketing system for (1) inferred label of a phrase pattern and (2) word
1055
alignment information for phrase pattern with inferred label as back-off. We compare the bracketing
performance against the LEFT class baseline.
5.3 Results
Method Accuracy
LEFT baseline 71.1 %
Inferred phrase pattern labels 89.0
?
%
Word alignment for phrase patterns 91.6
?
%
Table 6: Bracketing performance; ? indicates significantly higher than the LEFT baseline
Table 6 shows the results of our system compared to the LEFT class baseline. The first result is that both
inferred label and word alignment information for phrase pattern outperform the LEFT class baseline
significantly
7
. Bracketing with word alignment information for phrase pattern outperforms bracketing
based on the inferred labels.
6 Related Work
Our methods for extracting and structuring English NCs rely on the spelling of various aligned languages.
Previous work on multilingual extraction include Morin and Daille (2010) and Weller and Heid (2012).
These type-based approaches focus on bilingual terminology extraction using comparable corpora. Our
token-based extraction method includes 10 languages and we extract both the NCs and their context.
While the aforementioned work serves as resource for improving machine translation (MT) systems, we
focus on NC research and how multilingual evidence can help analysing and interpreting English NCs.
This multilingual perspective on a considerable number of languages has been adopted as well by
Macherey et al., (2011), who present a multilingual language-independent approach to compound split-
ting. Moreover, they learned morphological operations on compounding automatically. Here, Macherey
et al., (2011) extract training instances using a method related to Garera and Yarowsky (2008): select a
single word f in a language l translated to several English words e
i
. If there is a translation for each e
i
to a word g
i
that shows a (partial) substring match with f , (f ; e
1
, . . . , e
n
; g
1
, . . . , g
n
) is extracted. While
Macherey et al., (2011) extract training instances type-based in a bilingual setting, we directly extract
NC instances with a set of four closed compounding languages. This token-based perspective has the
advantage that we can process English NCs for which there is no literal translation to the target language
(e.g., health insurance aligned to Krankenversicherung (lit. invalid insurance)).
In cross-lingual annotation transfer (Yarowsky and Ngai, 2001; Pad?o, 2007; Van der Plas et al., 2011)
human annotations are transferred from one language to the other in parallel data. In this paper, we use
the structural differences between languages as found in parallel corpora to generate annotations on the
target language and do not rely on annotations on the source language.
Bracketing methods for both three-noun compounds and complete base NPs have been designed both
supervised and unsupervised. Vadas and Curran (2007) used a supervised bracketing method on man-
ually annotated data. Pitler et al. (2010) used the data from Vadas and Curran (2007) for a parser
applicable on base NPs of any length including coordinations. Their supervised classifier exploited web-
scale N-grams. Although supervised methods outperform unsupervised methods by far, the need for
annotated data is a drawback of supervised approaches. Bergsma et al. (2011) used crosslingual data as
additional supervision to make the need for manual annotations less pressing. Unsupervised methods use
N-gram statistics (Marcus, 1980; Lauer, 1995; Nakov and Hearst, 2005) or semantic information (Kim
and Baldwin, 2013).
7 Conclusion
In this paper, we discussed the complexity related to the definition of compoundhood and presented
an iterative method that tries to refine existing definitions by tentatively demonstrating the efficacy of
7
Approximate randomization test (Yeh, 2000), p < 5%
1056
linguistic criteria on corpus data. The initial implementation of two linguistic criteria, based on cross-
lingual spelling conventions and the inseparability of a compound?s components, achieved an F-Score of
82.1% on the task of extracting English compounds.
The extracted multilingual database of compounds in contexts serves multiple purposes. For example,
it can be used to study cross-lingual variations in compounding. We showed in an additional experiment
how the cross-lingual evidence found in the multilingual database can be used to bracket English three-
noun compounds using cross-lingual spelling variation with a set of six phrase patterns. We achieved a
bracketing accuracy of 91.6% that is very close to human performance.
In future work, we plan to continue refining the definition of compoundhood in a cross-lingual setting.
We will experiment with additional linguistic criteria defined over multiple languages. This way, we hope
to improve the quality of the multilingual database that we will further explore for compound analysis
and translation.
Acknowledgements
This research was funded and supported by the German Research Foundation (Deutsche Forschungsge-
meinschaft, DFG) as part of the SFB 732. We thank the anonymous reviewers for their comments. We
also thank Gianina Iordachioaia for her helpful input and interesting discussion.
References
Marco Baroni, Johannes Matiasek, and Harald Trost. 2002. Predicting the components of German nominal
compounds. In Proceedings of ECAI, pages 470?474, Lyon. IOS Press.
L. Bauer. 2003. Introducing Linguistic Morphology. Introducing Linguistic Morphology. Edinburgh University
Press.
Otto Behaghel. 1909. Beziehungen zwischen umfang und reihenfolge von satzgliedern. Indogermanische
Forschungen, page 110142.
Shane Bergsma, David Yarowsky, and Kenneth Church. 2011. Using large monolingual and bilingual corpora to
improve coordination disambiguation. In ACL-HLT 2011, pages 1346?1355.
Pierrette Bouillon, Katharina Boesefeldt, and Graham Russell. 1992. Compound Nouns in a Unification-Based
MT System. In ANLP 1992, pages 209?215, Trento.
Fabio Celli and Malvina Nissim. 2009. Automatic identification of semantic relations in Italian complex nominals.
In IWCS 2009, pages 45?60.
Eve V. Clark. 1981. Lexical innovations: How children learn to create new words. In Werner Deutsch, editor, The
Child?s Construction of Language, pages 299?328. Academic Press, New York.
J. Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement,
20(1).
Nikesh Garera and David Yarowsky. 2008. Translating compounds by learning component gloss translation
models via multiple languages. In IJCNLP, pages 403?410.
Roxana Girju. 2007. Improving the Interpretation of Noun Phrases with Cross-linguistic Information. In ACL
2007, pages 568?575.
Iris Hendrickx, Zornitsa Kozareva, Preslav Nakov, Diarmuid
?
O S?eaghdha, Stan Szpakowicz, and Tony Veale. 2013.
Semeval-2013 task 4: Free paraphrases of noun compounds. In Workshop on Semantic Evaluation (SemEval
2013), pages 138?143.
Michael Johnston and Frederica Busa. 1999. Qualia structure and the compositional interpretation of compounds.
In E. Viegas (ed.), Breadth and depth of semantics lexicons, pages 167?187. Dordrecht: Kluwer Academic.
Su Nam Kim and Timothy Baldwin. 2006. Interpreting semantic relations in noun compounds via verb semantics.
In ACL 2006, pages 491?498.
Su Nam Kim and Timothy Baldwin. 2013. A lexical semantic approach to interpreting and bracketing english
noun compounds. Natural Language Engineering, 19(3):385?407.
1057
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL - Interactive Poster and
Demonstration Sessions 2007, pages 177?180.
J. R. Landis and G. G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics,
33:159?174.
Mark Lauer. 1995. Designing Statistical Language Learners: Experiments on Noun Compounds. Ph.D. thesis,
Macquarie University.
R. Lieber and P. Stekauer. 2009. The Oxford Handbook of Compounding. Oxford Handbooks in Linguistics. OUP
Oxford.
Klaus Macherey, Andrew M. Dai, David Talbot, Ashok C. Popat, and Franz Och. 2011. Language-independent
compound splitting with morphological operations. In ACL-HLT 2011.
Hans Marchand. 1967. Expansion, transposition, and derivation. La Linguistique, pages 13?26.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of
english: The penn treebank. Comput. Linguist., 19(2):313?330, June.
Mitchell Marcus. 1980. A Theory of Syntactic Recognition for Natural Language. MIT Press.
Emmanuel Morin and Batrice Daille. 2010. Compositionality and lexical alignment of multi-word terms. Lan-
guage Resources and Evaluation, 44:79?95.
Preslav Nakov and Marti Hearst. 2005. Search engine statistics beyond the n-gram: Application to noun compound
bracketing. In Proceedings of the Ninth Conference on Computational Natural Language Learning, CoNLL
2005, pages 17?24.
Roberto Navigli, Paola Velardi, and Aldo Gangemi. 2003. Ontology learning and its application to automated
terminology translation. IEEE Intelligent Systems, 18(1):22?31.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Comput. Linguist.
Diarmuid
?
O S?eaghdha. 2008. Learning compound noun semantics. Ph.D. thesis, Computer Laboratory, University
of Cambridge.
S. Pad?o. 2007. Cross-lingual Annotation Projection Models for Role-Semantic Information. Ph.D. thesis, Saarland
University.
Emily Pitler, Shane Bergsma, Dekang Lin, and Kenneth Ward Church. 2010. Using web-scale n-grams to improve
base np parsing performance. In COLING 2010, pages 886?894.
Ulrike Rackow, Ido Dagan, and Ulrike Schwall. 1992. Automatic translation of noun compounds. In COLING
1992, pages 1249?1253.
Helmut Schmid. 1995. Improvements in part-of-speech tagging with an application to german. In ACL SIGDAT-
Workshop 1995, pages 47?50.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-noun compound machine translation: A feasibility study on
shallow processing. In ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment,
page 1724.
J?org Tiedemann. 2012. Parallel data, tools and interfaces in opus. In LREC 2012.
David Vadas and James R. Curran. 2007. Large-scale supervised models for noun phrase bracketing. In PACLING
2007, pages 104?112.
L. Van der Plas, P. Merlo, and J. Henderson. 2011. Scaling up cross-lingual semantic annotation transfer. In
ACL-HLT 2011.
Marion Weller and Ulrich Heid. 2012. Analyzing and aligning german compound nouns. In LREC 2012, Istanbul,
Turkey.
D. Yarowsky and G. Ngai. 2001. Inducing multilingual pos taggers and np bracketers via robust projection across
aligned corpora. In NAACL 2001, pages 1?8.
A. Yeh. 2000. More accurate tests for the statistical significance of result differences. In COLING 2000.
1058
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1279?1290, Dublin, Ireland, August 23-29 2014.
Global Methods for Cross-lingual Semantic Role and Predicate Labelling
Lonneke van der Plas
Institute for NLP
Pfaffenwaldring 5B
70569 Stuttgart, Germany
vdplasme@ims.uni-stuttgart.de
Marianna Apidianaki
LIMSI-CNRS
Rue John von Neumann
91405 Orsay Cedex, France
marianna@limsi.fr
Chenhua Chen
Institute for NLP
Pfaffenwaldring 5B
70569 Stuttgart, Germany
cch.chenhua@googlemail.com
Abstract
We address the problem of transferring semantic annotations to new languages using parallel
corpora. Previous work has transferred these annotations on a token-to-token basis, an approach
that is sensitive to alignment errors and translation shifts. We present a global approach to transfer
that aggregates information across the whole parallel corpus and leads to more robust labellers.
We build two global models, one for predicate labelling and one for role labelling, each tailored
to the task at hand. We show that the combination of direct and global methods outperforms
previous results.
1 Introduction
With the proliferation of the Internet in non-English speaking countries, the need for multilingual
processing becomes more and more pressing. Various efforts have focused on developing language-
independent NLP tools and extending to other languages tools that had been exclusive to English. Fur-
thermore, several annotation efforts have been devoted to developing resources for different languages,
needed for supervised learning (Haji?c et al., 2009). However, there is still a large number of languages
for which corpora with semantic annotations do not exist. Since manual annotation is a costly and time-
consuming approach to resource development, cross-lingual annotation transfer offers an alternative.
Semantic parsing or semantic role labelling (SRL) is the task of automatically labelling predicates
and arguments with predicate-argument structure. This level of analysis provides a more stable semantic
representation across syntactically different sentences. The example sentences (1a) and (1b) illustrate
how the semantic annotation remains stable across the locative alternation of the verb load.
(1) a. [AGENT Jessica] [REL-LOAD.01 loaded] [THEME boxes] [DESTINATION into the wagon].
b. [AGENT Jessica] [REL-LOAD.01 loaded] [DESTINATION the wagon] [THEME with boxes].
Also in the cross-lingual setting, the predicate-argument structure of a sentence is considered to be
more stable than its syntactic form as the English sentence in (2a) and its French translation in (2b)
show:
(2) a. [EXPERIENCER Mary] [REL-LIKE.01 liked] [CONTENT the idea]. (English)
b. [CONTENT L?id?ee] a [REL-LIKE.01 plu] [EXPERIENCER `a Marie]. (French)
This is why several pieces of work have transferred semantic annotations from a source language, for
which semantic annotations exist, to a target language using parallel corpora (Pad?o, 2007; Basili et al.,
2009; Annesi and Basili, 2010). These transfer methods rely on the assumption of semantic equivalence
of the original and the translated sentences, but also on correct and complete alignments between words
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1279
or constituents in those sentences. We will refer to these traditional methods as direct transfer because
the semantic annotations are transferred directly from token to token. Although direct transfer methods
are straightforward and easy to implement, they are vulnerable to missing or incorrect alignments which
lead to missing and erroneous annotations in the target language. Consequently, non-literal translations
and translation shifts present major problems for these methods.
In this paper we propose a global approach to the cross-lingual transfer of PropBank (Palmer et al.,
2005) semantic annotations that aggregates information at the corpus level and, as a consequence, is
more robust to non-literal translations and alignment errors. Our global approach involves two steps:
in the learning step, two global models are learned on the basis of role and predicate annotations in the
source language (English). In the labelling step, these models assign labels to verbs and their arguments
in the target language (French) without consulting any parallel data. Contrary to previous work, we
build separate models for the transfer of semantic role and predicate annotation because predictors for
the two models are different in nature. We model cross-lingual transfer of predicate labels as a cross-
lingual word sense disambiguation (WSD) task because this fits well with the lexical nature of the task:
annotating French verbs with English predicate labels. Our approach to predicate labelling needs word
alignments but instead of relying on local (token-to-token) correspondences like the direct method, it
exploits alignment information gathered from the whole corpus thus avoiding transfer errors caused by
local misalignments. Our model for cross-lingual semantic role labelling
1
is based on syntactic-semantic
mappings learned from a gold annotated monolingual corpus. The SRL method does not need aligned
data. Our methods are knowledge-lean as our predicate labelling method only needs a part of speech
(PoS) tagger in the two languages and no syntactic information on either side, in contrast to previous
work. For SRL, a syntactic parser for the target language is needed, but no joint semantic-syntactic
parsing framework as was the case in previous work (van der Plas et al., 2011). The requirements of the
global annotation transfer methods in terms of data and annotations, and their differences from direct
transfer, are illustrated in Figure 1.
Our contribution is three-fold. First, we present a global approach to semantic annotation transfer that
corrects token-level mistakes found in traditional direct transfer methods. We show the strengths and
limitations of global vs. direct transfer and explain how the two can be combined. Second, in contrast to
previous work, we address the two tasks of cross-lingual predicate labelling and cross-lingual semantic
role labelling by building two separate models tailored to the task at hand. We show how the predicate
labels produced by our high-coverage and knowledge-lean model for cross-lingual predicate labelling
are successfully used as predictors for semantic role labelling. Third, due to its knowledge-lean and
flexible character, our method adapts relatively easily to other language pairs without requiring semantic
lexicons in the target language.
In the next section, we present related work on cross-lingual annotation transfer. In Section 3 we
present the data used in our experiments and in Section 4 we briefly discuss direct transfer. The two
global methods proposed in this paper are presented in Section 5. We report and discuss our results in
Section 6, before concluding.
2 Related work
Transferring annotation from one language to another in order to train monolingual tools for new lan-
guages was first introduced by Yarowsky and Ngai (2001). In their approach, token-level part-of-speech
(PoS) and noun phrase bracketing information was projected across word-aligned bitext and this partial
annotation served to estimate the parameters of a model that generalized from the noisy projection in a
robust way. In more recent work, Das and Petrov (2011) propose a graph-based framework for projecting
syntactic information across languages. They create type-level tag dictionaries by aggregating over pro-
jected token-level information extracted from bi-text and use label propagation on a similarity graph to
smooth and expand the label distributions. A different approach to cross-lingual PoS tagging is proposed
1
Most unsupervised approaches consider argument identification as a separate task that is omitted (Lang and Lapata, 2010)
or performed heuristically (Lang and Lapata, 2011). We focus on semantic role labelling in this paper and consider argument
identification as given.
1280
semantic annotations(predicates + roles) PoS tags
semantic annotations(predicates + roles)
semantic annotations(predicates)  
PoS tags semantic annotations(predicates) 
modelfor predicate labelling
semantic annotations(roles)syntactic annotations
syntactic annotations
semantic annotations(roles) 
modelfor role labelling
modelfor role labelling
syntactic annotations
FR FR
FR FR
FR
FRFREN EN
EN
EN
EN
FR
 learning 
 learning  labelling 
 labelling 
modeltransferhand-built cross-lingual syntactic mappings
OR
annotationtransfer
direct transfer                   global predicate labelling                global semantic role labelling
Parallel corpus
Parallel corpus
Parallel corpus
EN: EnglishFR: French
Figure 1: Direct vs. global cross-lingual transfer of semantic annotations.
by T?ackstr?om et al. (2013) who couple token and type constraints to guide learning. Our approach to
cross-lingual semantic role labelling follows this vein. Instead of solely relying on token-level informa-
tion acquired from word-alignments, we combine this with type-level information captured by our global
methods which are trained on the entire corpus. We however are concerned with semantic annotations
and not PoS.
Transfer of semantic annotation has started off with direct transfer of FrameNet semantic annotations
(Pad?o, 2007; Basili et al., 2009; Annesi and Basili, 2010). With the addition of a learning step and the
use of PropBank data, Van der Plas et al. (2011) have scaled up previous efforts. They show that a joint
semantic-syntactic parser trained on the output of direct transfer produces better parses than the input it
received by aggregating information across multiple examples. In their work, transfer of predicate labels
and semantic role labels is done in one step. The model needs an aggressive filter to compensate for
missing annotations on the predicate level after direct transfer. This filter successively leads to drops in
performance for the role labellings. Here, we build two separate global models that complement direct
transfer instead of relying on it.
The same emphasis on learning is found in cross-lingual model transfer where source language models
are adapted to work on the target language directly. For semantic role labelling, Kozhevnikov and Titov
(2013) use shared feature representations (syntactic and lexical) to adapt a source model to a target-
language model. The ideas behind their cross-lingual model adaptation resemble the ideas behind our
global method for semantic role labelling. However, in contrast to their work we do not consider the
predicate labelling as given because, as manual annotations show (van der Plas et al., 2010), this task is
not trivial. We first build a tailored global model for cross-lingual predicate labelling and then use the
predicted predicate labels for semantic role labelling.
3 Data
In our experiments, we use the English-French part of the Europarl corpus (Koehn, 2005). The dataset
is tokenised and lowercased and only sentence pairs corresponding to a one-to-one sentence alignment
with lengths ranging from one to 40 tokens on both French and English sides are considered. Further-
more, because translation shifts are known to pose problems for the automatic projection of semantic
roles across languages (Pad?o, 2007), we select only those parallel sentences in Europarl that are direct
1281
translations from English to French or vice versa. In the end, we have a parallel corpus of 276-thousand
sentence pairs.
The English part of the parallel corpus is annotated by a freely-available syntactic-semantic parser
(Henderson et al., 2008; Titov et al., 2009) trained on the CoNLL 2009 training set (the Penn Treebank
corpus (Marcus et al., 1993) merged with PropBank labels (Palmer et al., 2005) and NomBank labels
(Meyers, 2007)). The probabilistic model is a joint generative model of syntactic and semantic depen-
dencies that maximises the joint probability of the dependencies while building two separate structures.
The WSD classifier used for predicate labelling is trained on the parallel training corpus tagged with
semantic roles on the English side. The candidate predicate labels that are considered by the classifier
for each French verb are the labels of its English translations in the training corpus. Verbs on the English
side are replaced by the corresponding predicate label where available. Then both parts of the corpus are
lemmatized and tagged by part of speech (Schmid, 1994) and the parallel files are rebuilt (one sentence
per line) by replacing words on both sides by the corresponding ?lemma PoS tag? pair. The corpus is
then word aligned in both directions using GIZA++ (Och and Ney, 2003) and a lexicon is built from
intersecting alignments. Lexicon entries for French verbs contain the English predicate labels to which
they were aligned in the training corpus. The entry for the verb encourager, for instance, contains seven
predicate labels: {urge.01, foster.01, stimulate.01, promote.02, encourage.01, encourage.02, renew.01},
two of which correspond to the same English verb (encourage). We keep labels with an alignment
confidence score above 0.01 according to GIZA++.
Contrary to our predicate labelling model, the role labelling model needs syntactic information in
the target language. For parsing French, we use the dependency parser described in Titov and Hender-
son (2007). We train the parser on the dependency version of the French Paris 7 treebank (Candito et al.,
2009), achieving 87.2% labelled accuracy on this data set. The French Treebank (Abeill?e et al., 2003) is a
treebank of 21,564 sentences annotated with constituency annotation. We use the automatic dependency
conversion of the French Treebank into dependency format (Candito et al., 2009) to train the French
syntactic parser that is used to annotate the French part of the parallel corpus.
For testing, we use the hand-annotated data described in Van der Plas et al. (2010). We randomly
split those 1000 sentences into test and development set containing 500 sentences each. We use the
development set for the current experiments, which contains 1,917 core roles in total. We limit our
experiments to verbal predicates because the semantic annotations on French test sentences are limited
to verbal predicates.
4 Direct cross-lingual transfer
Before explaining the global methods, we present the direct semantic transfer (DST) method proposed
by Van der Plas et al. (2011) that we use for comparisons and combinations throughout this paper. The
method is based on the Direct Correspondence Assumption for syntactic dependency trees proposed by
Hwa et al. (2005). The transfer proceeds as follows: For any pair of sentences E and F that are translations
of each other in the parallel corpus, we transfer the semantic relationship R(x
E
, y
E
) to R(x
F
, y
F
) if and
only if there exists a word-alignment between x
E
and x
F
and between y
E
and y
F
, and we transfer the
semantic property P (x
E
) to P (x
F
) if and only if there exists a word-alignment between x
E
and x
F
.
The relationships that are transferred are semantic role dependencies and the properties are predicate
senses. These are transferred from the English part of the parallel training corpus that is automatically
annotated with syntactic-semantic analyses, as explained in the previous section.
5 Global cross-lingual transfer of semantic annotations
In contrast to direct transfer where annotations are transferred on a token-to-token basis in word-aligned
sentences, we propose two global methods for cross-lingual transfer, one for predicates and one for
semantic roles, that both consist of a learning and a labelling step. Our methods are globally defined
and as a consequence rely less on local translation correspondences than previous methods, which makes
them less vulnerable to missing and incorrect alignment links.
1282
5.1 Global cross-lingual predicate labelling
In cross-lingual predicate labelling, our aim is to put predicate labels that originate from the English
side of the parallel corpus on the French verbs in the other side of the corpus. The predicate labels
contain the English verb and its sense. For example, ?give.01? stands for the first sense of the verb
give. As the predicate label contains a lot of lexical information, putting the correct English predicate
label on a French verb is very close to Word Sense Disambiguation (WSD), the task of automatically
identifying the meaning of words in context (Navigli, 2009). In the cross-lingual variant of this task, the
candidate senses are the words? translations in other languages and WSD aims at predicting semantically
correct translations for words in context (Resnik and Yarowsky, 2000; Ng et al., 2003; Carpuat and
Wu, 2007; Apidianaki, 2009). The main difference between cross-lingual WSD and our cross-lingual
transfer of predicate labels is that we do not search for correct translations of French words but for the
most appropriate predicate labels in context (i.e. verbs disambiguated with a predicate sense).
The global predicate labelling method consists of a learning step and a labelling step. During learning,
we compute estimates for annotation transfer on the basis of the word alignments between English and
French predicates over the entire parallel training corpus. At labelling time, we label French verbs
with English predicate labels without the need for parallel data or alignments. The method is language-
independent and only requires minimal linguistic resources (PoS information).
In terms of coverage, a predicate label is provided for all French verbs in the test set for which in-
formation was retained during training and not only for aligned ones, in contrast to direct transfer. We
expect to augment the recall when using global estimates and hope that the effect on precision is not too
negative.
Learning
For each French verb (v) in the lexicon built as described in Section 3, we want to be able to identify its
correct predicate label in a new context by choosing one among its candidate labels (L) retained from
the training corpus. A feature vector is built for each candidate label L
i
(1 ? i ? |L|) found for the verb
v in the lexicon, following the procedure described in Apidianaki et al. (2012). For each candidate label,
we extract the content word co-occurrences of the verb v in the French sentences where it translates
an English verb tagged with this label in the training corpus. The retained French words constitute the
features of the vector built for that label. Let N be the number of features retained for each label L
i
of
the verb v from the corresponding French contexts. Each feature F
j
(1 ? j ? N ) receives a total weight
with the label (tw(F
j
, L
i
)) which is learned from the data and defined as the product of the feature?s
global weight (gw(F
j
)) and its local weight with that label (lw(F
j
, L
i
)). The global weight of a feature
F
j
is a function of the number n of candidate labels of v to which F
j
is related, and of the probabilities
(p
ij
) that F
j
co-occurs with instances of the verb v corresponding to each of the labels:
gw(F
j
) = 1?
?
n
i=1
p
ij
log(p
ij
)
n
(1)
Each p
ij
is computed as the ratio of the co-occurrence counts of F
j
with v when it is aligned to a label
L
i
to the total number of features (N ) seen with this candidate label:
p
ij
=
cooc count(F
j
, L
i
)
N
(2)
The local weight between feature F
j
and label L
i
(lw(F
j
, L
i
)) directly depends on the number of times
they occur together:
lw(F
j
, L
i
) = log(cooc count(F
j
, L
i
)) (3)
The intuition underlying this weighting scheme is that if an interesting semantic relation exists between a
feature F
j
and a specific predicate label L
i
of a verb v, then we expect the probability (p
ij
) of the feature
F
j
occurring in the contexts where v is translated by this label to be larger than if they were independent.
In other words, a feature gets a high total weight (tw) with a label when it appears frequently in the
corresponding French contexts and rarely in the contexts of the other labels.
1283
Labelling
Predicate identification on the French side is done by selecting verbs based on the PoS labels provided by
the tagger and subsequently filtering out modals and instances of the verb ?etre (be).
2
The most suitable
predicate labels are then assigned to the retained French verbs by the disambiguation classifier. The
context of a new instance of a French verb is compared to the weighted feature vectors (V
i
?s) built for
its candidate labels as described above, and an association score is assigned to each label. To facilitate
comparison with the vectors, the new contexts (sentences) are lemmatised and PoS tagged on the fly
(with TreeTagger) and the content word co-occurrences of the French verb are gathered in a bag of
words. If common features (CF s) are found between the new context and the vector of a label (L
i
), their
association score corresponds to the mean of the weights of their shared features with L
i
found in the
corresponding vector. In Equation 4, (CF
j
)
|CF |
j=1
is the set of common features between a label vector V
i
and the new context C and tw is the total weight of a CF with label L
i
, computed as explained in the
previous section.
assoc score(V
i
, C) =
?
|CF |
j=1
tw(CF
j
, L
i
)
|CF |
(4)
The label that receives the highest association score with the new context is returned and serves to
annotate the corresponding French verb.
5.2 Global cross-lingual role labelling
For role labelling, we adopt a different strategy. Whereas predicate labels include a lot of lexical infor-
mation, role labels do not. However, for role labels there is another source of information that helps to
define global estimates: the correlation between syntax and semantics.
Previous work in monolingual unsupervised semantic role induction (Grenager and Manning, 2006;
Lang and Lapata, 2010; Lang and Lapata, 2011) showed that mapping rules that assign semantic roles to
arguments of a verb based on the syntactic functions of these arguments, represent a baseline that is very
hard to beat. This strong correlation between syntactic labels and semantic role labels in the PropBank
annotation has been shown in detail by Merlo and Van der Plas (2009). In contrast to previous work on
monolingual unsupervised semantic role induction, we add the predicate label as a predictor. The core
arguments of the verb, that are the numbered labels in PropBank, are known to be verb-specific. We
have access to predicate labels assigned by the cross-lingual predicate labelling method described in the
previous section and exploit them for role labelling.
For a given predicate, diathesis alternations are the major source of variation in propositions. They
give rise to different syntactic structures, while the semantic roles remain stable. For example, the sen-
tence ?I gave the book to Jean? is syntactically different from ?I gave Jean the book?, but semantic roles
on the three arguments stay the same. We will show in a feasibility study that the effect of diathesis alter-
nations on the correlation between syntax and semantics is limited. In a cross-lingual setting, structural
divergences (Dorr, 1994) are expected to reduce the correlation between syntax and semantics. An ex-
ample is the difference in syntactic structure between the sentences ?Tu me manques? vs. ?I miss you?,
which are translations of each other, however the semantic roles are the same across languages.
As our global method is not restricted to alignments at labelling time, we are able to classify all given
arguments
3
and not just those that are aligned in a parallel corpus. In this way, we believe that the
negative effect of structural divergences and diathesis alternations is limited. Moreover, we show how
mild supervision from the partial annotations that result from the direct transfer can potentially remedy
these difficulties.
Learning syntactic-semantic mappings
The syntactic-semantic mapping rules that are exploited by our model for role labelling are extracted
from gold-annotated monolingual data. As a consequence, the extracted rules are of high quality which
2
We exclude the verb ?etre because its English counterpart (be) is not annotated in the CoNLL-2009 data used in our experi-
ments.
3
We focus on the classification of core semantic roles because diathesis alternations and cross-lingual divergences mainly
involve these roles.
1284
would not be the case if parallel data was used. Manually annotated parallel corpora are very sparse and
automatic parsing introduces errors which might be propagated by the direct transfer methods and result
in noisy annotations. Using gold-standard monolingual data thus ensures the quality of the mappings
exploited by our global model.
We build a model that determines the most suitable semantic role label r for a given argument of
a given predicate p, based on its syntactic dependency label d.
4
We simply compute the maximum
likelihood estimates (MLE) and count occurrences of the following triples < p, d, r > in a large body of
English gold semantically and syntactically annotated data.
P
MLE
(r|p, d) =
count(p, d, r)
count(p, d)
(5)
In the cross-lingual setting, the mapping rules extracted from the English training data are applied to
French. We learn the correspondences between English and French syntactic labels in a data-driven way
by syntactically annotating both sides of our parallel training corpus. We base the cross-lingual syntactic
mapping on alignment counts between syntactic labels in the parallel corpus parsed syntactically both on
the English and the French side (cf. Section 3). An alternative that needs no parallel data is to study the
annotation guidelines for the two languages and determine the cross-lingual correspondences between
syntactic labels by hand.
The syntactic label set used for French (Candito et al., 2009) is less fine-grained than the English labels
(20 versus 36). As a consequence, the mapping from English syntactic labels to French treebank labels
is for the most part a many-to-one mapping, which leads to information loss but suffices for our purpose
as will be shown in the next section.
Once the correspondences between the syntactic labels of the two syntactic annotation frameworks are
discovered, the cross-lingual transfer of syntactic-semantic mappings consists in substituting the English
syntactic labels with their French counterparts to adapt the model described above.
Labelling
For role labelling, we use estimates derived from the training data (see Equation 5) to determine the most
suitable role of a given argument. Because a particular triple in the test set might not have been seen
during training, we backoff to 2-tuples that discard the predicate label, and backoff to A1 if neither the
dependency label nor the predicate has been seen in training.
To treat the R-suffix, which takes care of anaphoric arguments, we use the following simple rule: for
the monolingual setting all arguments with PoS-tags ?WDT?, ?WP?, and ?WRB? receive the R-suffix.
In the cross-lingual setting, we translate the PoS tags to the single French PoS tag ?PROREL?. We do not
treat the C-prefix, which takes care of discontinuous arguments, because there were only a few examples.
We do not accept duplicate semantic roles, a constraint that leads to valid role configurations in general
(Punyakanok et al., 2008). We expect the more prominent semantic roles, such as A0 and A1, to appear
earlier in the sentence than semantic roles with higher numbers. We therefore attribute semantic roles of
a predicate from left to right.
5.3 Combining direct and global cross-lingual transfer
Direct transfer methods generally have low recall, we however expect them to be more precise than the
global methods. In our combined method, we use the annotations assigned by direct transfer as the
backbone and fill missing labels by the global methods. The annotations from direct transfer restrict
the possible roles the global method adds. We expect, as an additional benefit of this combination,
that the partial annotations from direct transfer together with the no-duplicate-role constraint described
above will remedy problems related to diathesis alternations. Although the probabilities computed will
favour the canonical alternation in general, the partial annotations may prevent a canonical analysis in a
particular proposition. Consider the following alternation example: Mary presented the flowers to John
vs. the less canonical alternation Mary presented John with the flowers. Although the most probable role
4
We chose not to include the complete dependency path from predicate to argument because of data sparseness. We select
the dependency label on the arc that points to the argument under discussion.
1285
Predicate identification and labelling
Labelled Unlabelled
Prec Rec F Prec Rec F
1 Direct 51 29 37 93 57 71
2 Global 45 39 42 95 83 89
3 Combined 45 45 45 92 91 91
4 Plas11 68 25 37 98 36 53
5 Plas11(f) 56 46 51 97 80 87
6 Manual 61 57 59 97 89 93
Table 1: Percent recall, precision and F-measure for predicate
identification and labelling.
Cross-lingual semantic role labelling
1 Direct 35
2 Global 68
3 Combined 73
4 Most frequent semantic role 48
Table 2: Percent accuracy for se-
mantic role labelling
for the prep relation would be A2, based on the canonical alternation, partial annotations on Mary (A0)
and John (A2) in combination with the no-duplicate-role constraint would rule that out and the next most
probable label would be put on with: A1.
6 Results and discussion
We ran experiments using the two global methods described in Section 5 separately and combined with
direct transfer. In this section, we present the results and compare to several baselines and upper bounds
from manual annotations and previous work.
6.1 Cross-lingual predicate labelling
Table 1 shows the results of cross-lingual predicate labelling (Labelled) and identification (Unlabelled).
The first row shows the results from using the traditional direct transfer method. The second row presents
results from the global method where we use cross-lingual WSD to label predicates. The third row
combines direct and global transfer, as explained in Section 5.3. For comparison, we present results
when using the parser from Van der Plas et al. (2011) on our test data: the fourth row contains results
when using all (unfiltered) data, the fifth row when using data filtered for incomplete predicate labellings.
We show an upper bound in the last row which corresponds to the inter-annotator agreement for manual
annotation on a random set of 100 sentences (van der Plas et al., 2010).
Overall the figures, including the upper bound from manual annotations, are not very high. Annotating
French verbs with English predicate labels is a hard task. When we look at the differences between the
three automatic methods, we see that recall is very low (29%) for the direct method. From the recall
figures for unlabelled predicates, we see that the direct method leaves many predicates without a label.
The global method has a much better recall, 39%, and a slightly lower precision. The best results
are however attained when the two methods are combined, that is, when global transfer is used to fill
in missing predicates from direct transfer. We get an F-measure of 45% which is a big improvement
over the baseline of direct transfer, which attained 37%. These results show that the global method for
predicate labelling improves recall without sacrificing precision too much.
We compare these results also to the results obtained by Van der Plas et al. (2011)?s three step model,
where a parser trained on transferred annotations annotates in turn the test sentences. We see that the
current method gives better results (recall and F-measure) when the parser is trained on unfiltered data.
An aggressive filter, that removes more than half of the data and leads to a big drop in performance for
argument labelling (recall that argument and predicate labelling is done in parallel in this model) finally
leads to a result that outperforms ours. This result is not surprising because the parser has access to much
more expressive syntax. Note that our global method only needs a PoS tagger in the source language
and no syntactic information nor joint semantic-syntactic parsing frameworks. It is thus knowledge-
lean and easier to apply to languages without a parser, a difference that should be taken into account
in the interpretation of the results. However, we can learn from these results that structural information
is beneficial. In future work, we plan to include word position information in our cross-lingual WSD
method. This will give the method access to structural information while keeping it knowledge-lean.
In Figure 2, we give an example that illustrates the contribution of the global method. In this example,
1286
English (automatic): There is in particular one amendment, let [let.01] me point [point.02] out, concerning [con-
cern.01] the energy sector, which, in my capacity as rapporteur, I see [see.01] as particularly important.
Transfer: Il y a notamment un amendement, je le souligne, concernant [concern.01] le secteur de l??energie, qui me
para??t en tant que rapporteur particuli`erement important.
CLWSD: Il y a notamment un amendement, je le souligne [stress.01], concernant [concern.01] le secteur de l??energie,
qui me para??t [seem.01] en tant que rapporteur particuli`erement important.
Figure 2: Predicate label addition and correction using CLWSD.
the cross-lingual WSD method annotates more verbs than the direct transfer approach: labels [stress.01]
and [seem.01] assigned during disambiguation, are missing from the first sentence after transfer. Even
with a high quality word alignment, it would not be possible to get these labels from the English source
sentence through direct transfer because they are simply not there, due to the non-literal translation. This
example shows the limitations of token-to token direct transfer and how the global method compensates
for that by using information aggregated across the whole parallel corpus.
6.2 Global cross-lingual role labelling
Though already supported by previous work (Grenager and Manning, 2006; Lang and Lapata, 2010;
Lang and Lapata, 2011), we tested the hypothesis that syntactic-semantic mappings provide good ap-
proximations for semantic role labelling, especially when adding predicate information. We therefore
first ran a monolingual feasibility study by collecting counts from the CoNLL 2009 training set and test-
ing on the CoNLL 2009 test set. The accuracy attained with this simple method is 79%. This shows that
in a monolingual setting, the predicate label combined with the syntactic label of the argument are good
predictors for the semantic role of the argument. This number can serve as a baseline for semantic role
labelling given the correct predicate label.
In previous sections, we discussed diathesis alternations as problematic for using syntactic-semantic
mapping rules. To measure the importance of diathesis alternations we need to measure the variation in
a large corpus. By applying the mapping rules learned from the training data on the same data we get
an idea of the amount of variation. We get an accuracy of 86%. Although the 14% probably contains
the most interesting examples from a linguistic point of view, these results on monolingual data show
that predicate-centered syntactic-semantic mapping rules are a promising direction for improving recall
in direct transfer methods.
Table 2 shows the results
5
for semantic role labelling for the three cross-lingual transfer methods
and the baseline of applying the most frequent semantic role label ?A1?. For the global and the com-
bined methods we use the predicate labels provided by the cross-lingual WSD method. The numbers
in Table 2 provide performance numbers given the predicate from the cross-lingual WSD method. We
discussed in Subsection 5.2 that, when applying syntactic-semantic mapping rules cross-lingually, dif-
ferences in annotation framework and cross-lingual divergences are at play. We see indeed that when
applying syntactic-semantic mapping rules cross-lingually the accuracy drops from 79 to 68%. This
drop in performance when applying to French the syntactic-semantic mappings that were learned on En-
glish data is not too important. This accuracy number is, in any case, much better than the results from
direct transfer. This is mainly due to the low recall of direct transfer which results in very few but rather
precise (87%) semantic roles. It is therefore very useful to use the direct transfer method as a backbone
that restricts the labels we get from global transfer by imposing consistency with the available annotation
(no-duplicate-argument-constraint). By combining the two methods, we get 73% accuracy that is not far
from the 79% in the monolingual setting. In future work, we would like to investigate whether the drop
in performance between the monolingual and cross-lingual setting is larger for languages that are less
related.
We also compare our results to previous work on cross-lingual transfer of semantic roles. Kozhevnikov
and Titov (2013) evaluate on the full test set described in Subsection 3 (1000 sentences), they use gold
predicates instead of predicted predicates and evaluate on both core roles and adjuncts. The authors
shared with us their results for core roles only: 74% and 77%, when using original and transferred
5
As we focus on argument labelling (and not identification) we provide accuracy scores.
1287
syntax, respectively. We use original syntax and should therefore compare to the 74%. When we use
gold predicate annotations as Kozhevnikov and Titov (2013) did, instead of the predicate labels obtained
through cross-lingual WSD, and test on all 1000 sentences, we attain 75% for the combined method and
71% for the global method. These results compare favourably with their results. This is encouraging
because their model uses a larger feature set that includes (cross-lingual) lexical features, the unlabelled
dependency graph and PoS information. Interestingly, they attain better scores when they use a trans-
ferred syntactic model instead of the original syntax. This result seems in line with our discussion on the
loss of information when trying to map the English syntactic label inventory to the French inventory. We
keep syntactic model transfer in mind for future work.
Because we consider the arguments as given, while Van der Plas et al. (2011) do both argument
identification and labelling for all core roles and adjuncts, and provide precision and recall given the
predicate only, we cannot directly compare to their results. We however include their results for the sake
of completeness. Their parser results in 65% F-score.
Applying A1 (the most frequent semantic role) to the entire data set gives us 48% accuracy. That is
much higher than results from transfer, again due to the low recall of the direct transfer method, but much
lower than the results of the combined and global methods.
7 Conclusion
We have introduced a global approach to transfer that aggregates information at the corpus level thereby
correcting and complementing the annotations from traditional direct transfer methods that suffer from
token-level mistakes. We show that the combination of direct transfer (a high-precision method) and
global methods (high in recall) outperforms previous results.
In contrast to previous work, we transfer predicate annotations and semantic role annotations by build-
ing two separate models tailored to the task at hand. We show how the predicate labels produced by our
high-coverage model for cross-lingual predicate labelling are successfully used as predictors for semantic
role labelling.
In future work, we would like to feed structural information to the cross-lingual WSD method such
as information about word position, which would preserve its knowledge-lean character without need
for syntactic parsing. Furthermore, we intend to use cross-lingual WSD for labelling adjuncts (non-
core semantic roles) since this task is also rather lexical in nature. Last but not least, we want to add
argument identification which will allow to propose a complete SRL annotation framework based on
global information.
Acknowledgements
This research was funded and supported by the German Research Foundation (Deutsche Forschungsge-
meinschaft, DFG) as part of the SFB 732.
References
A. Abeill?e, L. Cl?ement, and F. Toussenel. 2003. Building a treebank for French. In Treebanks: Building and
Using Parsed Corpora. Kluwer Academic Publishers.
P. Annesi and R. Basili. 2010. Cross-lingual alignment of FrameNet annotations through Hidden Markov Models.
In Proceedings of CICLing.
M. Apidianaki, G. Wisniewski, A. Sokolov, A. Max, and F. Yvon. 2012. WSD for n-best reranking and local
language modeling in SMT. In Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in
Statistical Translation, pages 1?9, Jeju, Republic of Korea, July. Association for Computational Linguistics.
M. Apidianaki. 2009. Data-driven Semantic Analysis for Multilingual WSD and Lexical Selection in Translation.
In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics
(EACL-09), pages 77?85, Athens, Greece.
1288
R. Basili, D. De Cao, D. Croce, B. Coppola, and A. Moschitti, 2009. Computational Linguistics and Intelligent Text
Processing, chapter Cross-Language Frame Semantics Transfer in Bilingual Corpora, pages 332?345. Springer
Berlin / Heidelberg.
M.-H. Candito, B. Crabb?e, P. Denis, and F. Gu?erin. 2009. Analyse syntaxique du franc?ais : des constituants
aux d?ependances. In Proceedings of la Conf?erence sur le Traitement Automatique des Langues Naturelles
(TALN?09), Senlis, France.
M. Carpuat and D. Wu. 2007. Improving Statistical Machine Translation using Word Sense Disambiguation. In
Proceedings of the Joint EMNLP-CoNLL Conference, pages 61?72, Prague, Czech Republic.
D. Das and S. Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections. In
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, pages 600?609, Portland, Oregon, USA, June. Association for Computational Linguistics.
B. Dorr. 1994. Machine translation divergences: A formal description and proposed solution. Computational
Linguistics, 20(4):597?633.
T. Grenager and C. Manning. 2006. Unsupervised discovery of a statistical verb lexicon. In Proceedings of
EMNLP.
J. Haji?c, M. Ciaramita, R. Johansson, D. Kawahara, M. A. Mart??, L. M`arquez, A. Meyers, J. Nivre, S. Pad?o,
J.
?
Step?anek, P. Stra?n?ak, M. Surdeanu, N. Xue, and Y. Zhang. 2009. The CoNLL-2009 shared task: Syntactic and
semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational
Natural Language Learning (CoNLL 2009).
J. Henderson, P. Merlo, G. Musillo, and I. Titov. 2008. A latent variable model of synchronous parsing for
syntactic and semantic dependencies. In Proceedings of CONLL 2008, pages 178?182.
R. Hwa, P. Resnik, A.Weinberg, C. Cabezas, and O. Kolak. 2005. Bootstrapping parsers via syntactic projection
accross parallel texts. Natural language engineering, 11:311?325.
P. Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Proceedings of MT Summit
X, pages 79?86, Phuket, Thailand.
M. Kozhevnikov and I. Titov. 2013. Crosslingual transfer of semantic role models. In In Proceedings of the
51th Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, August. Association for
Computational Linguistics.
J. Lang and M. Lapata. 2010. Unsupervised induction of semantic roles. In Human Language Technologies:
The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,
pages 939?947, Los Angeles, California, June. Association for Computational Linguistics.
J. Lang and M. Lapata. 2011. Unsupervised semantic role induction via split-merge clustering. In Proceedings of
the 49th Annual Meeting of the Association for Computational Linguistics.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn
Treebank. Comp. Ling., 19:313?330.
P. Merlo and L. van der Plas. 2009. Abstraction and generalisation in semantic role labels: PropBank, VerbNet or
both? In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of the AFNLP, pages 288?296, Suntec, Singapore.
A. Meyers. 2007. Annotation guidelines for NomBank - noun argument structure for PropBank. Technical report,
New York University.
R. Navigli. 2009. Word Sense Disambiguation: a Survey. ACM Computing Surveys, 41(2):1?69.
H. T. Ng, B. Wang, and Y. S. Chan. 2003. Exploiting Parallel Texts for Word Sense Disambiguation: An
Empirical Study. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,
pages 455?462, Sapporo, Japan.
F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational
Linguistics, 29:19?51.
S. Pad?o. 2007. Cross-lingual Annotation Projection Models for Role-Semantic Information. Ph.D. thesis, Saarland
University.
1289
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles.
Computational Linguistics, 31:71?105.
V. Punyakanok, D. Roth, and W. Yih. 2008. The importance of syntactic parsing and inference in semantic role
labeling. Computational Linguistics, 34(2):257?287.
P. Resnik and D. Yarowsky. 2000. Distinguishing Systems and Distinguishing Senses: New Evaluation Methods
for Word Sense Disambiguation. Natural Language Engineering, 5(3):113?133.
H. Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Processing, pages 44?49, Manchester, UK, September.
http://www.ims.uni-stuttgart.de/?schmid/.
O. T?ackstr?om, D. Das, S. Petrov, R. McDonald, and J. Nivre. 2013. Token and type constraints for cross-lingual
part-of-speech tagging. In Transactions of the ACL. Association for Computational Linguistics, March.
I. Titov and J. Henderson. 2007. A latent variable model for generative dependency parsing. In Proceedings of
the International Conference on Parsing Technologies (IWPT-07), pages 144?155, Prague, Czech Republic.
I. Titov, J. Henderson, P. Merlo, and G. Musillo. 2009. Online graph planarisation for synchronous parsing
of semantic and syntactic dependencies. In Proceedings of the twenty-first international joint conference on
artificial intelligence (IJCAI-09), Pasadena, California, July.
L. van der Plas, T. Samard?zi?c, and P. Merlo. 2010. Cross-lingual validity of PropBank in the manual annotation of
French. In In Proceedings of the 4th Linguistic Annotation Workshop (The LAW IV), Uppsala, Sweden.
L. van der Plas, P. Merlo, and J. Henderson. 2011. Scaling up cross-lingual semantic annotation transfer. In Pro-
ceedings of the 49th Annual Meeting of the Association for Computational Linguistics and the Human Language
Technologies conference.
D. Yarowsky and G. Ngai. 2001. Inducing multilingual pos taggers and np bracketers via robust projection across
aligned corpora. In Proceedings of the second meeting of the North American Chapter of the Association
for Computational Linguistics on Language technologies, NAACL ?01, pages 1?8, Stroudsburg, PA, USA.
Association for Computational Linguistics.
1290
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 299?304,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Scaling up Automatic Cross-Lingual Semantic Role Annotation
Lonneke van der Plas
Department of Linguistics
University of Geneva
Geneva, Switzerland
Paola Merlo
Department of Linguistics
University of Geneva
Geneva, Switzerland
{Lonneke.vanderPlas,Paola.Merlo,James.Henderson}@unige.ch
James Henderson
Department of Computer Science
University of Geneva
Geneva, Switzerland
Abstract
Broad-coverage semantic annotations for
training statistical learners are only available
for a handful of languages. Previous ap-
proaches to cross-lingual transfer of seman-
tic annotations have addressed this problem
with encouraging results on a small scale. In
this paper, we scale up previous efforts by us-
ing an automatic approach to semantic anno-
tation that does not rely on a semantic on-
tology for the target language. Moreover,
we improve the quality of the transferred se-
mantic annotations by using a joint syntactic-
semantic parser that learns the correlations be-
tween syntax and semantics of the target lan-
guage and smooths out the errors from auto-
matic transfer. We reach a labelled F-measure
for predicates and arguments of only 4% and
9% points, respectively, lower than the upper
bound from manual annotations.
1 Introduction
As data-driven techniques tackle more and more
complex natural language processing tasks, it be-
comes increasingly unfeasible to use complete, ac-
curate, hand-annotated data on a large scale for
training models in all languages. One approach to
addressing this problem is to develop methods that
automatically generate annotated data by transfer-
ring annotations in parallel corpora from languages
for which this information is available to languages
for which these data are not available (Yarowsky et
al., 2001; Fung et al, 2007; Pado? and Lapata, 2009).
Previous work on the cross-lingual transfer of se-
mantic annotations (Pado?, 2007; Basili et al, 2009)
has produced annotations of good quality for test
sets that were carefully selected based on seman-
tic ontologies on the source and target side. It has
been suggested that these annotations could be used
to train semantic role labellers (Basili et al, 2009).
In this paper, we generate high-quality broad-
coverage semantic annotations using an automatic
approach that does not rely on a semantic ontol-
ogy for the target language. Furthermore, to our
knowledge, we report the first results on using joint
syntactic-semantic learning to improve the quality
of the semantic annotations from automatic cross-
lingual transfer. Results on correlations between
syntax and semantics found in previous work (Merlo
and van der Plas, 2009; Lang and Lapata, 2010) have
led us to make use of the available syntactic anno-
tations on the target language. We use the seman-
tic annotations resulting from cross-lingual transfer
combined with syntactic annotations to train a joint
syntactic-semantic parser for the target language,
which, in turn, re-annotates the corpus (See Fig-
ure 1). We show that the semantic annotations pro-
duced by this parser are of higher quality than the
data on which it was trained.
Given our goal of producing broad-coverage an-
notations in a setting based on an aligned corpus,
our choices of formal representation and of labelling
scheme differ from previous work (Pado?, 2007;
Basili et al, 2009). We choose a dependency repre-
sentation both for the syntax and semantics because
relations are expressed as direct arcs between words.
This representation allows cross-lingual transfer to
use word-based alignments directly, eschewing the
need for complex constituent-alignment algorithms.
299
Train?a?French?syntacticparser
?Transfer?semantic?annotationsfrom?EN?to?FR?using?wordalignments
EN?syntactic?semanticannotations
EN?FR?word?aligneddata
FR?syntacticannotations
FR?semanticannotations evaluatio
n
Train?Frenchjoint?syntactic?semantic?parser
evaluatio
n
FR?syntacticannotations
FR?semanticannotations
Figure 1: System overview
We choose the semantic annotation scheme defined
by Propbank, because it has broad coverage and in-
cludes an annotated corpus, contrary to other avail-
able resources such as FrameNet (Fillmore et al,
2003) and is the preferred annotation scheme for a
joint syntactic-semantic setting (Merlo and van der
Plas, 2009). Furthermore, Monachesi et al (2007)
showed that the PropBank annotation scheme can be
used for languages other than English directly.
2 Cross-lingual semantic transfer
Data-driven induction of semantic annotation based
on parallel corpora is a well-defined and feasible
task, and it has been argued to be particularly suit-
able to semantic role label annotation because cross-
lingual parallelism improves as one moves to more
abstract linguistic levels of representation. While
Hwa et al (2002; 2005) find that direct syntactic de-
pendency parallelism between English and Spanish
concerns 37% of dependency links, Pado? (2007) re-
ports an upper-bound mapping correspondence cal-
culated on gold data of 88% F-measure for in-
dividual semantic roles, and 69% F-measure for
whole scenario-like semantic frames. Recently, Wu
and Fung (2009a; 2009b) also show that semantic
roles help in statistical machine translation, capi-
talising on a study of the correspondence between
English and Chinese which indicates that 84% of
roles transfer directly, for PropBank-style annota-
tions. These results indicate high correspondence
across languages at a shallow semantic level.
Based on these results, our transfer of semantic
annotations from English sentences to their French
translations is based on a very strong mapping hy-
pothesis, adapted from the Direct Correspondence
Assumption for syntactic dependency trees by Hwa
et al (2005).
Direct Semantic Transfer (DST) For any
pair of sentences E and F that are transla-
tions of each other, we transfer the seman-
tic relationship R(xE , yE) to R(xF , yF ) if
and only if there exists a word-alignment
between xE and xF and between yE and
yF , and we transfer the semantic property
P (xE) to P (xF ) if and only if there exists
a word-alignment between xE and xF .
The relationships which we transfer are semantic
role dependencies and the properties are predicate
senses. We introduce one constraint to the direct se-
mantic transfer. Because the semantic annotations in
the target language are limited to verbal predicates,
we only transfer predicates to words the syntactic
parser has tagged as a verb.
As reported by Hwa et al (2005), the direct cor-
respondence assumption is a strong hypothesis that
is useful to trigger a projection process, but will not
work correctly for several cases.
We used a filter to remove obviously incomplete
annotations. We know from the annotation guide-
lines used to annotate the French gold sentences that
all verbs, except modals and realisations of the verb
e?tre, should receive a predicate label. We define a
filter that removes sentences with missing predicate
labels based on PoS-information in the French sen-
tence.
2.1 Learning joint syntactic-semantic
structures
We know from previous work that there is a strong
correlation between syntax and semantics (Merlo
and van der Plas, 2009), and that this correla-
tion has been successfully applied for the unsuper-
vised induction of semantic roles (Lang and Lap-
ata, 2010). However, previous work in machine
translation leads us to believe that transferring the
correlations between syntax and semantics across
languages would be problematic due to argument-
structure divergences (Dorr, 1994). For example,
the English verb like and the French verb plaire do
not share correlations between syntax and seman-
tics. The verb like takes an A0 subject and an A1
300
direct object, whereas the verb plaire licences an A1
subject and an A0 indirect object.
We therefore transfer semantic roles cross-
lingually based only on lexical alignments and add
syntactic information after transfer. In Figure 1, we
see that cross-lingual transfer takes place at the se-
mantic level, a level that is more abstract and known
to port relatively well across languages, while the
correlations with syntax, that are known to diverge
cross-lingually, are learnt on the target language
only. We train a joint syntactic-semantic parser
on the combination of the two linguistic levels that
learns the correlations between these structures in
the target language and is able to smooth out errors
from automatic transfer.
3 Experiments
We used two statistical parsers in our transfer of
semantic annotations from English to French, one
for syntactic parsing and one for joint syntactic-
semantic parsing. In addition, we used several cor-
pora.
3.1 The statistical parsers
For our syntactic-semantic parsing model, we use
a freely-available parser (Henderson et al, 2008;
Titov et al, 2009). The probabilistic model is a joint
generative model of syntactic and semantic depen-
dencies that maximises the joint probability of the
syntactic and semantic dependencies, while building
two separate structures.
For the French syntactic parser, we used the de-
pendency parser described in Titov and Hender-
son (2007). We train the parser on the dependency
version of the French Paris treebank (Candito et al,
2009), achieving 87.2% labelled accuracy on this
data set.
3.2 Data
To transfer semantic annotation from English to
French, we used the Europarl corpus (Koehn,
2003)1. We word-align the English sentences to the
French sentences automatically using GIZA++ (Och
1As is usual practice in preprocessing for automatic align-
ment, the datasets were tokenised and lowercased and only sen-
tence pairs corresponding to a one-to-one sentence alignment
with lengths ranging from one to 40 tokens on both French and
English sides were considered.
and Ney, 2003) and include only intersective align-
ments. Furthermore, because translation shifts are
known to pose problems for the automatic projection
of semantic roles across languages (Pado?, 2007), we
select only those parallel sentences in Europarl that
are direct translations from English to French, or
vice versa. In the end, we have a word-aligned par-
allel corpus of 276-thousand sentence pairs.
Syntactic annotation is available for French. The
French Treebank (Abeille? et al, 2003) is a treebank
of 21,564 sentences annotated with constituency an-
notation. We use the automatic dependency conver-
sion of the French Treebank into dependency format
provided to us by Candito and Crabbe? and described
in Candito et al (2009).
The Penn Treebank corpus (Marcus et al, 1993)
merged with PropBank labels (Palmer et al, 2005)
and NomBank labels (Meyers, 2007) is used to train
the syntactic-semantic parser described in Subsec-
tion 3.1 to annotate the English part of the parallel
corpus.
3.3 Test sets
For testing, we used the hand-annotated data de-
scribed in (van der Plas et al, 2010). One-thousand
French sentences are extracted randomly from our
parallel corpus without any constraints on the se-
mantic parallelism of the sentences, unlike much
previous work. We randomly split those 1000 sen-
tences into test and development set containing 500
sentences each.
4 Results
We evaluate our methods for automatic annotation
generation twice: once after the transfer step, and
once after joint syntactic-semantic learning. The
comparison of these two steps will tell us whether
the joint syntactic-semantic parser is able to improve
semantic annotations by learning from the syntactic
annotations available. We evaluate the models on
unrestricted test sets2 to determine if our methods
scale up.
Table 1 shows the results of automatically an-
notating French sentences with semantic role an-
notation. The first set of columns of results re-
2Due to filtering, the test set for the transfer (filter) model is
smaller and not directly comparable to the other three models.
301
Predicates Arguments (given predicate)
Labelled Unlabelled Labelled Unlabelled
Prec Rec F Prec Rec F Prec Rec F Prec Rec F
1 Transfer (no filter) 50 31 38 91 55 69 61 48 54 72 57 64
2 Transfer (filter) 51 46 49 92 84 88 65 51 57 76 59 67
3 Transfer+parsing (no filter) 71 29 42 97 40 57 77 57 65 87 64 74
4 Transfer+parsing (filter) 61 50 55 95 78 85 71 52 60 83 61 70
5 Inter-annotator agreement 61 57 59 97 89 93 73 75 74 88 91 89
Table 1: Percent recall, precision, and F-measure for predicates and for arguments given the predicate, for the four
automatic annotation models and the manual annotation.
ports labelling and identification of predicates and
the second set of columns reports labelling and iden-
tification of arguments, respectively, for the predi-
cates that are identified. The first two rows show
the results when applying direct semantic transfer.
Rows three and four show results when using the
joint syntactic-semantic parser to re-annotate the
sentences. For both annotation models we show re-
sults when using the filter described in Section 2 and
without the filter.
The most striking result that we can read from
Table 1 is that the joint syntactic-semantic learning
step results in large improvements, especially for
argument labelling, where the F-measure increases
from 54% to 65% for the unfiltered data. The parser
is able to outperform the quality of the semantic
data on which it was trained by using the infor-
mation contained in the syntax. This result is in
accordance with results reported in Merlo and Van
der Plas (2009) and Lang and Lapata (2010), where
the authors find a high correlation between syntactic
functions and PropBank semantic roles.
Filtering improves the quality of the transferred
annotations. However, when training a parser on the
annotations we see that filtering only results in better
recall scores for predicate labelling. This is not sur-
prising given that the filters apply to completeness in
predicate labelling specifically. The improvements
from joint syntactic-semantic learning for argument
labelling are largest for the unfiltered setting, be-
cause the parser has access to larger amounts of data.
The filter removes 61% of the data.
As an upper bound we take the inter-annotator
agreement for manual annotation on a random set
of 100 sentences (van der Plas et al, 2010), given
in the last row of Table 1. The parser reaches an
F-measure on predicate labelling of 55% when us-
ing filtered data, which is very close to the up-
per bound (59%). The upper bound for argument
inter-annotator agreement is an F-measure of 74%.
The parser trained on unfiltered data reaches an
F-measure of 65%. These results on unrestricted
test sets and their comparison to manual annotation
show that we are able to scale up cross-lingual se-
mantic role annotation.
5 Discussion and error analysis
A more detailed analysis of the distribution of im-
provements over the types of roles further strength-
ens the conclusion that the parser learns the corre-
lations between syntax and semantics. It is a well-
known fact that there exists a strong correlation be-
tween syntactic function and semantic role for the
A0 and A1 arguments: A0s are commonly mapped
onto subjects and A1s are often realised as direct ob-
jects (Lang and Lapata, 2010). It is therefore not
surprising that the F-measure on these types of ar-
guments increases by 12% and 15%, respectively,
after joint-syntactic semantic learning. Since these
arguments make up 65% of the roles, this introduces
a large improvement. In addition, we find improve-
ments of more than 10% on the following adjuncts:
AM-CAU, AM-LOC, AM-MNR, and AM-MOD that to-
gether comprise 9% of the data.
With respect to predicate labelling, comparison
of the output after transfer with the output after
parsing (on the development set) shows how the
parser smooths out transfer errors and how inter-
lingual divergences can be solved by making use
of the variations we find intra-lingually. An exam-
ple is given in Figure 2. The first line shows the
predicate-argument structure given by the English
302
EN (source) Postal [A1 services] [AM-MOD must] [CONTINUE.01 continue] [C-A1 to] be public services.
FR (transfer) Les [A1services] postaux [AM-MOD doivent] [CONTINUE.01rester] des services publics.
FR (parsed) Les [A1 services] postaux [AM-MOD doivent] [REMAIN.01rester] des [A3 services] publics.
Figure 2: Differences in predicate-argument labelling after transfer and after parsing
syntactic-semantic parser to the English sentence.
The second line shows the French translation and
the predicate-argument structure as it is transferred
cross-lingually following the method described in
Section 2. Transfer maps the English predicate la-
bel CONTINUE.01 onto the French verb rester, be-
cause these two verbs are aligned. The first oc-
currence of services is aligned to the first occur-
rence of services in the English sentence and gets
the A1 label. The second occurrence of services
gets no argument label, because there is no align-
ment between the C-A1 argument to, the head of
the infinitival clause, and the French word services.
The third line shows the analysis resulting from the
syntactic-semantic parser that has been trained on a
corpus of French sentences labelled with automat-
ically transferred annotations and syntactic annota-
tions. The parser has access to several labelled ex-
amples of the predicate-argument structure of rester,
which in many other cases is translated with remain
and has the same predicate-argument structure as
rester. Consequently, the parser re-labels the verb
with REMAIN.01 and labels the argument with A3.
Because the languages and annotation framework
adopted in previous work are not directly compara-
ble to ours, and their methods have been evaluated
on restricted test sets, results are not strictly com-
parable. But for completeness, recall that our best
result for predicate identification is an F-measure
of 55% accompanied with an F-measure of 60%
for argument labelling. Pado? (2007) reports a 56%
F-measure on transferring FrameNet roles, know-
ing the predicate, from an automatically parsed and
semantically annotated English corpus. Pado? and
Pitel (2007), transferring semantic annotation to
French, report a best result of 57% F-measure for
argument labelling given the predicate. Basili et
al. (2009), in an approach based on phrase-based
machine translation to transfer FrameNet-like anno-
tation from English to Italian, report 42% recall in
identifying predicates and an aggregated 73% recall
of identifying predicates and roles given these pred-
icates. They do not report an unaggregated number
that can be compared to our 60% argument labelling.
In a recent paper, Annesi and Basili (2010) improve
the results from Basili et al (2009) by 11% using
Hidden Markov Models to support the automatic
semantic transfer. Johansson and Nugues (2006)
trained a FrameNet-based semantic role labeller for
Swedish on annotations transferred cross-lingually
from English parallel data. They report 55% F-
measure for argument labelling given the frame on
150 translated example sentences.
6 Conclusions
In this paper, we have scaled up previous efforts of
annotation by using an automatic approach to se-
mantic annotation transfer in combination with a
joint syntactic-semantic parsing architecture. We
propose a direct transfer method that requires nei-
ther manual intervention nor a semantic ontology for
the target language. This method leads to semanti-
cally annotated data of sufficient quality to train a
syntactic-semantic parser that further improves the
quality of the semantic annotation by joint learning
of syntactic-semantic structures on the target lan-
guage. The labelled F-measure of the resulting an-
notations for predicates is only 4% point lower than
the upper bound and the resulting annotations for ar-
guments only 9%.
Acknowledgements
The research leading to these results has received
funding from the EU FP7 programme (FP7/2007-
2013) under grant agreement nr 216594 (CLAS-
SIC project: www.classic-project.org), and from the
Swiss NSF under grant 122643.
References
A. Abeille?, L. Cle?ment, and F. Toussenel. 2003. Building
a treebank for French. In Treebanks: Building and
Using Parsed Corpora. Kluwer Academic Publishers.
303
P. Annesi and R. Basili. 2010. Cross-lingual alignment
of FrameNet annotations through Hidden Markov
Models. In Proceedings of CICLing.
R. Basili, D. De Cao, D. Croce, B. Coppola, and A. Mos-
chitti, 2009. Computational Linguistics and Intelli-
gent Text Processing, chapter Cross-Language Frame
Semantics Transfer in Bilingual Corpora, pages 332?
345. Springer Berlin / Heidelberg.
M.-H. Candito, B. Crabbe?, P. Denis, and F. Gue?rin. 2009.
Analyse syntaxique du franc?ais : des constituants
aux de?pendances. In Proceedings of la Confe?rence
sur le Traitement Automatique des Langues Naturelles
(TALN?09), Senlis, France.
B. Dorr. 1994. Machine translation divergences: A for-
mal description and proposed solution. Computational
Linguistics, 20(4):597?633.
C. J. Fillmore, R. Johnson, and M.R.L. Petruck. 2003.
Background to FrameNet. International journal of
lexicography, 16.3:235?250.
P. Fung, Z. Wu, Y. Yang, and D. Wu. 2007. Learn-
ing bilingual semantic frames: Shallow semantic pars-
ing vs. semantic role projection. In 11th Conference
on Theoretical and Methodological Issues in Machine
Translation (TMI 2007).
J. Henderson, P. Merlo, G. Musillo, and I. Titov. 2008. A
latent variable model of synchronous parsing for syn-
tactic and semantic dependencies. In Proceedings of
CONLL 2008, pages 178?182.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using anno-
tation projection. In Proceedings of the 40th Annual
Meeting of the ACL.
R. Hwa, P. Resnik, A.Weinberg, C. Cabezas, and O. Ko-
lak. 2005. Bootstrapping parsers via syntactic projec-
tion accross parallel texts. Natural language engineer-
ing, 11:311?325.
R. Johansson and P. Nugues. 2006. A FrameNet-based
semantic role labeler for Swedish. In Proceedings of
the annual Meeting of the Association for Computa-
tional Linguistics (ACL).
P. Koehn. 2003. Europarl: A multilingual corpus for
evaluation of machine translation.
J. Lang and M. Lapata. 2010. Unsupervised induction
of semantic roles. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 939?947, Los Angeles, California, June.
Association for Computational Linguistics.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
the Penn Treebank. Comp. Ling., 19:313?330.
P. Merlo and L. van der Plas. 2009. Abstraction and gen-
eralisation in semantic role labels: PropBank, VerbNet
or both? In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 288?296, Suntec, Singapore.
A. Meyers. 2007. Annotation guidelines for NomBank
- noun argument structure for PropBank. Technical
report, New York University.
P. Monachesi, G. Stevens, and J. Trapman. 2007. Adding
semantic role annotation to a corpus of written Dutch.
In Proceedings of the Linguistic Annotation Workshop
(LAW), pages 77?84, Prague, Czech republic.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29:19?51.
Sebastian Pado? and Mirella Lapata. 2009. Cross-lingual
annotation projection of semantic roles. Journal of Ar-
tificial Intelligence Research, 36:307?340.
S. Pado? and G. Pitel. 2007. Annotation pre?cise du
franc?ais en se?mantique de ro?les par projection cross-
linguistique. In Proceedings of TALN.
S. Pado?. 2007. Cross-lingual Annotation Projection
Models for Role-Semantic Information. Ph.D. thesis,
Saarland University.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31:71?105.
I. Titov and J. Henderson. 2007. A latent variable model
for generative dependency parsing. In Proceedings of
the International Conference on Parsing Technologies
(IWPT-07), pages 144?155, Prague, Czech Republic.
I. Titov, J. Henderson, P. Merlo, and G. Musillo. 2009.
Online graph planarisation for synchronous parsing of
semantic and syntactic dependencies. In Proceedings
of the twenty-first international joint conference on ar-
tificial intelligence (IJCAI-09), Pasadena, California,
July.
L. van der Plas, T. Samardz?ic?, and P. Merlo. 2010. Cross-
lingual validity of PropBank in the manual annotation
of French. In In Proceedings of the 4th Linguistic An-
notation Workshop (The LAW IV), Uppsala, Sweden.
D. Wu and P. Fung. 2009a. Can semantic role labeling
improve SMT? In Proceedings of the Annual Confer-
ence of European Association of Machine Translation.
D. Wu and P. Fung. 2009b. Semantic roles for SMT:
A hybrid two-pass model. In Proceedings of the
Joint Conference of the North American Chapter of
ACL/Human Language Technology.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001. In-
ducing multilingual text analysis tools via robust pro-
jection across aligned corpora. In Proceedings of the
International Conference on Human Language Tech-
nology (HLT).
304
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 50?57
Manchester, UK. August 2008
Using lexico-semantic information for query expansion in passage
retrieval for question answering
Lonneke van der Plas
LATL
University of Geneva
Switzerland
lonneke.vanderplas@lettres.unige.ch
Jo?rg Tiedemann
Alfa-Informatica
University of Groningen
The Netherlands
j.tiedemann@rug.nl
Abstract
In this paper we investigate the use of sev-
eral types of lexico-semantic information
for query expansion in the passage retrieval
component of our QA system. We have
used four corpus-based methods to acquire
semantically related words, and we have
used one hand-built resource. We eval-
uate our techniques on the Dutch CLEF
QA track.1 In our experiments expansions
that try to bridge the terminological gap
between question and document collection
do not result in any improvements. How-
ever, expansions bridging the knowledge
gap show modest improvements.
1 Introduction
Information retrieval (IR) is used in most QA sys-
tems to filter out relevant passages from large doc-
ument collections to narrow down the search for
answer extraction modules in a QA system. Accu-
rate IR is crucial for the success of this approach.
Answers in paragraphs that have been missed by
IR are lost for the entire QA system. Hence, high
performance of IR especially in terms of recall is
essential. Furthermore, high precision is desirable
as IR scores are used for answer extraction heuris-
tics and also to reduce the chance of subsequent
extraction errors.
Because the user?s formulation of the question
is only one of the many possible ways to state the
information need that the user might have, there is
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1The Cross-Language Evaluation Forum (http://clef-
qa.itc.it/)
often a discrepancy between the terminology used
by the user and the terminology used in the doc-
ument collection to describe the same concept. A
document might hold the answer to the user?s ques-
tion, but it will not be found due to the TERMI-
NOLOGICAL GAP. Moldovan et al (2002) show
that their system fails to answer many questions
(25.7%), because of the terminological gap, i.e.
keyword expansion would be desirable but is miss-
ing. Query expansion techniques have been devel-
oped to bridge this gap.
However, we believe that there is more than just
a terminological gap. There is also a KNOWLEDGE
GAP. Documents are missed or do not end up high
in the ranks, because additional world knowledge
is missing. We are not speaking of synonyms here,
but words belonging to the same subject field. For
example, when a user is looking for information
about the explosion of the first atomic bomb, in
his/her head a subject field is active that could in-
clude: war, disaster, World War II.
We have used three corpus-based methods
to acquire semantically related words: the
SYNTAX-BASED METHOD, the ALIGNMENT-
BASED METHOD, and the PROXIMITY-BASED
METHOD. The nature of the relations between
words found by the three methods is very differ-
ent. Ranging from free associations to synonyms.
Apart from these resources we have used cate-
gorised named entities, such as Van Gogh IS-A
painter and synsets from EWN as candidate ex-
pansion terms.
In this paper we have applied several types of
lexico-semantic information to the task of query
expansion for QA. We hope that the synonyms
retrieved automatically, and in particular the syn-
onyms retrieved by the alignment-based method,
as these are most precise, will help to overcome the
50
terminological gap. With respect to the knowledge
gap, we expect that the proximity-based method
would be most helpful as well as the list of cate-
gorised named entities. For example, knowing that
Monica Seles is a tennis player helps to find rele-
vant passages regarding this tennis star.
2 Related work
There are many ways to expand queries and ex-
pansions can be acquired from several sources.
For example, one can make use of collection-
independent resources, such as EWN. In contrast,
collection-dependent knowledge structures are of-
ten constructed automatically based on data from
the collection.
The results from using collection-independent,
hand-built sources are varied. Moldovan et al
(2003) show that using a lexico-semantic feed-
back loop that feeds lexico-semantic alternations
from WordNet as keyword expansions to the re-
trieval component of their QA system increments
the scores by 15%. Also, Pasc?a and Harabagiu
(2001) show substantial improvements when us-
ing lexico-semantic information from WordNet for
keyword alternation on the morphological, lexical
and semantic level. They evaluated their system on
question sets of TREC-8 and TREC-9. For TREC-
8 they reach a precision score of 55.3% with-
out including any alternations for question key-
words, 67.6% if lexical alternations are allowed
and 73.7% if both lexical and semantic alternations
are allowed.
However, Yang and Chua (2003) report that
adding additional terms from WordNet?s synsets
and glosses adds more noise than information to
the query. Also, Voorhees (1993) concludes that
expanding by automatically generated synonym
sets from EWN can degrade results.
In Yang et al (2003) the authors use external
knowledge extracted from WordNet and the Web
to expand queries for QA. Minor improvements
are attained when the Web is used to retrieve a
list of nearby (one sentence or snippet) non-trivial
terms. When WordNet is used to rank the retrieved
terms, the improvement is reduced. The best re-
sults are reached when structure analysis is added
to knowledge from the Web and WordNet. Struc-
ture analysis determines the relations that hold be-
tween the candidate expansion terms to identify
semantic groups. Semantic groups are then con-
nected by conjunction in the Boolean query.
Monz (2003) ran experiments using pseudo rel-
evance feedback for IR in a QA system. The author
reports dramatic decreases in performance. He ar-
gues that this might be due to the fact that there
are usually only a small number of relevant doc-
uments. Another reason he gives is the fact that
he used the full document to fetch expansion terms
and the information that allows one to answer the
question is expressed very locally.
A global technique that is most similar to ours
uses syntactic context to find suitable terms for
query expansion (Grefenstette, 1992; Grefenstette,
1994). The author reports that the gain is mod-
est: 2% when expanded with nearest neighbours
found by his system and 5 to 6%, when apply-
ing stemming and a second loop of expansions
of words that are in the family of the augmented
query terms.2 Although the gain is greater than
when using document co-occurrence as context,
the results are mixed, with expansions improving
some query results and degrading others.
Also, the approach by Qiu and Frei (1993) is
a global technique. They automatically construct
a similarity thesaurus, based on what documents
terms appear in. They use word-by-document ma-
trices, where the features are document IDs, to de-
termine the similarity between words. Expansions
are selected based on the similarity to the query
concept, i.e. all words in the query together, and
not based on the single words in the query inde-
pendently. The results they get are promising.
Pantel and Ravichandran (2004) have used a
method that is not related to query expansion,
but yet very related to our work. They have se-
mantically indexed the TREC-2002 IR collection
with the ISA-relations found by their system for
179 questions that had an explicit semantic answer
type, such as What band was Jerry Garcia with?
They show small gains in performance of the IR
output using the semantically indexed collection.
Recent work (Shen and Lapata, 2007; Kaisser
and Webber, 2007) that falls outside the scope of
this paper, but that is worth mentioning success-
fully applies semantic roles to question answering.
3 Lexico-semantic information
We have used several types of lexico-semantic
information as sources for candidate expansion
terms. The first three are automatically acquired
2i.e. words that appear in the same documents and that
share the first three, four or five letters.
51
from corpora by means of distributional methods.
? Nearest neighbours from proximity-based
distributional similarity
? Nearest neighbours from syntax-based distri-
butional similarity
? Nearest neighbours from alignment-based
distributional similarity
The idea behind distributional methods is rooted
in the DISTRIBUTIONAL HYPOTHESIS (Harris,
1968). Similar words appear in similar context.
The way words are distributed over contexts tells
us something about their meaning. Context can
be defined in several ways. The way the context
is defined determines the type of lexico-semantic
knowledge we will retrieve.
For example, one can define the context of a
word as the n words surrounding it. In that case
proximity to the head word is the determining
factor. We refer to these methods that use un-
structured context as PROXIMITY-BASED METH-
ODS. The nearest neighbours resulting from such
methods are rather unstructured as well. They are
merely associations between words, such as baby
and cry. We have used the 80 million-word corpus
of Dutch newspaper text (the CLEF corpus) that is
also part of the document collection in the QA task
to retrieve co-occurrences within sentences.
Another approach is one in which the context
of a word is determined by syntactic relations. In
this case, the head word is in a syntactic relation
to a second word and this second word accom-
panied by the syntactic relation form the context
of the head word. We refer to these methods as
SYNTAX-BASED METHODS. We have used several
syntactic relations to acquire syntax-based context
for our headwords. This method results in nearest
neighbours that at least belong to the same seman-
tic and syntactic class, for example baby and son.
We have used 500 million words of newspaper text
(the TwNC corpus parsed by Alpino (van Noord,
2006)) of which the CLEF corpus is a subset.
A third method we have used is the
ALIGNMENT-BASED METHOD. Here, trans-
lations of word, retrieved from the automatic
word alignment of parallel corpora are used to
determine the similarity between words. This
method results in even more tightly related data,
as it mainly finds synonyms, such as infant and
baby. We have used the Europarl corpus (Koehn,
2003) to extract word alignments from.3
By calculating the similarity between the con-
texts words are found in, we can retrieve a
ranked list of nearest neighbours for any head-
word. We gathered nearest neighbours for a
frequency-controlled list of words, that was still
manageable to retrieve. We included all words
(nouns, verbs, adjectives and proper names) with
a frequency of 150 and higher in the CLEF cor-
pus. This resulted in a ranked list of nearest neigh-
bours for the 2,387 most frequent adjectives, the
5,437 most frequent nouns, the 1,898 most fre-
quent verbs, and the 1,399 most frequent proper
names. For all words we retrieved a ranked list
of its 100 nearest neighbours with accompanying
similarity score.
In addition to the lexico-semantic information
resulting from the three distributional methods we
used:
? Dutch EuroWordNet (Vossen, 1998)
? Categorised named entities
With respect to the first resource we can be
short. We selected the synsets of this hand-built
lexico-semantic resource for nouns, verbs, adjec-
tives and proper names.
The categorised named entities are a by-product
of the syntax-based distributional method. From
the example in (1) we extract the apposition rela-
tion between Van Gogh and schilder ?painter? to
determine that the named entity Van Gogh belongs
to the category of painters.
(1) Van Gogh, de beroemde schilder huurde
een atelier, Het Gele huis, in Arles.
?Van Gogh, the famous painter, rented a
studio, The Yellow House, in Arles.?
We used the data of the TwNC corpus (500M
words) and Dutch Wikipedia (50M words) to ex-
tract apposition relations. The data is skewed. The
Netherlands appears with 1,251 different labels.
To filter out incorrect and highly unlikely labels
(often the result of parsing errors) we determined
the relative frequency of the combination of the
named entity and a category with regard to the fre-
quency of the named entity overall. All categorised
named entities with relative frequencies under 0.05
3In van der Plas and Tiedemann (2006) there is more in-
formation on the syntax-based and alignment-based distribu-
tional methods.
52
Lex. info Nouns Adj Verbs Proper
Proximity 5.3K 2.4K 1.9K 1.2K
Syntax 5.4K 2.3K 1.9K 1.4K
Align 4.0K 1.2K 1.6K
Cat. NEs 218K
EWN 44.9K 1.5K 9.0K 1.4K
Table 1: Number of words for which lexico-
semantic information is available
were discarded. This cutoff made the number of
unwanted labels considerably lower.
In Table 1 we see the amount of information
that is contained in individual lexico-semantic re-
sources. It is clear from the numbers that the
alignment-based method does not provide near-
est neighbours for all head words selected. Only
4.0K nouns from the 5.4K retrieve nearest neigh-
bours. The data is sparse. Also, the alignment-
based method does not have any nearest neigh-
bours for proper names, due to decisions we made
earlier regarding preprocessing: All words were
transformed to lowercase.
The proximity-based method also misses a num-
ber of words, but the number is far less impor-
tant. The amount of information the lists of cate-
gorised named entities provide is much larger than
the amount of information comprised in the list
provided by distributional methods. EWN also
provides more information than the distributional
methods, except for adjectives.4
4 Methodology
In order to test the performance of the var-
ious lexico-semantic resources we ran several
tests. The baseline is running a standard full-text
retrieval engine using Apache Lucene (Jakarta,
2004). Documents have been lemmatised and stop
words have been removed.
We applied the nearest neighbours resulting
from the three distributional methods as described
in section 3. For all methods we selected the top-
5 nearest neighbours that had a similarity score of
more than 0.2 as expansions.
For EWN all words in the same synset (for all
senses) were added as expansions. Since all syn-
onyms are equally similar, we do not have similar-
ity scores for them to be used in a threshold.
The categorised named entities were not only
used to expand named entities with the corre-
4Note that the number of nouns from EWN is the result of
subtracting the proper names.
sponding label, but also to expand nouns with
named entities. In the first case all labels were
selected. The maximum is not more than 18 la-
bels. In the second case some nouns get many
expansions. For example, a noun, such as vrouw
?woman?, gets 1,751 named entities as expansions.
We discarded nouns with more than 50 expansions,
as these were deemed too general and hence not
very useful.
The last two settings are the same for the expan-
sions resulting from distributional methods and the
last two types of lexico-semantic information.
? Expansions were added as root forms
? Expansions were given a weight such that all
expansions for one original keyword add up
to 0.5.
5 Evaluation
For evaluation we used data collected from the
CLEF Dutch QA tracks. The CLEF text collec-
tion contains 4 years of newspaper text, approxi-
mately 80 million words and Dutch Wikipedia, ap-
proximately 50 million words. We used the ques-
tion sets from the competitions of the Dutch QA
track in 2003, 2004, and 2005 (774 in total). Ques-
tions in these sets are annotated with valid answers
found by the participating teams including IDs of
supporting documents in the given text collection.
We expanded these list of valid answers where nec-
essary.
We calculated for each run the Mean Reciprocal
Rank (MRR).5 The MRR measures the percentage
of passages for which a correct answer was found
in the top-k passages returned by the system. The
MRR score is the average of 1/R where R is the
rank of the first relevant passage computed over
the 20 highest ranked passages. Passages retrieved
were considered relevant when one of the possible
answer strings was found in that passage.
6 Results
In Table 2 the MRR (Mean Reciprocal Rank) is
given for the various expansion techniques. Scores
are given for expanding the several syntactic cat-
egories, where possible. The baseline does not
5We used MRR instead of other common evaluation mea-
sures because of its stronger correlation with the overall per-
formance of our QA system than, for example, coverage and
redundancy (see Tiedemann and Mur (2008)).
53
MRR
SynCat EWN Syntax Align Proxi Cat.NEs
Nouns 51.52 51.15 51.21 51.38 51.75
Adj 52.33 52.27 52.38 51.71
Verbs 52.40 52.33 52.21 52.62
Proper 52.59 50.16 53.94 55.68
All 51.65 51.21 51.02 53.36 55.29
Table 2: MRR scores for the IR component with
query expansion from several sources
#questions (+/-)
SynCat EWN Syntax Align Proxi Cat.NEs
Nouns 27/50 28/61 17/58 64/87 17/37
Adj 3/6 1/2 1/2 31/47
Verbs 31/51 5/10 8/32 51/56
Proper 3/2 30/80 76/48 157/106
All 56/94 56/131 25/89 161/147 168/130
Table 3: Number of questions that receive a higher
(+) or lower (-) RR when using expansions from
several sources
make use of any expansion for any syntactic cat-
egory and amounts to 52.36.
In Table 3 the number of questions that get a
higher and lower reciprocal rank (RR) after ap-
plying the individual lexico-semantic resources are
given. Apart from expansions on adjectives and
proper names from EWN, the impact of the expan-
sion is substantial. The fact that adjectives have
so little impact is due to the fact that there are not
many adjectives among the query terms.6
The negligible impact of the proper names from
EWN is surprising since EWN provides more en-
tries for proper names than the proximity-based
method (1.2K vs 1.4K, as can be seen in 1). The
proximity-based method clearly provides informa-
tion about proper names that are more relevant for
the corpus used for QA, as it is built from a subset
of that same corpus. This shows the advantage of
using corpus-based methods. The impact of the ex-
pansions resulting from the syntax-based method
lies in between the two previously mentioned ex-
pansions. It uses a corpus of which the corpus used
for QA is a subset.
The type of expansions that result from the
proximity-based method have a larger effect on
the performance of the system than those result-
ing from the syntax-based method. In Chapter 5 of
van der Plas (2008) we explain in greater detail that
the proximity-based method uses frequency cut-
6Moreover, the adjectives related to countries, such as
German and French and their expansion Germany, France are
handled by a separate list.
offs to keep the co-occurrence matrix manageable.
The larger impact of the proximity-based nearest
neighbours is probably partly due to this decision.
The cutoffs for the alignment-based and syntax-
based method have been determined after evalu-
ations on EuroWordNet (Vossen, 1998) (see also
van der Plas (2008)).
The largest impact results from expanding
proper names with categorised named entities. We
know from Table 1 in section 3, that this resource
has 70 times more data than the proximity-based
resource.
For most of the resources the number of ques-
tions that show a rise in RR is smaller than the
number of questions that receive a lower RR, ex-
cept for the expansion of proper names by the cat-
egorised named entities and the proximity-based
method.
The expansions resulting from the syntax-based
method do not result in any improvements. As
expected, the expansion of proper names from
the syntax-based method hurts the performance
most. Remember that the nearest neighbours of the
syntax-based method often include co-hyponyms.
For example, Germany would get The Netherlands
and France as nearest neighbours. It does not seem
to be a good idea to expand the word Germany
with other country names when a user, for exam-
ple, asks the name of the Minister of Foreign Af-
fairs of Germany. However, also the synonyms
from EWN and the alignment-based method do not
result in improvements.
The categorised named entities provide the most
successful lexico-semantic information, when
used to expand named entities with their category
label. The MRR is augmented by almost 3,5%. It
is clear that using the same information in the other
direction, i.e. to expand nouns with named enti-
ties of the corresponding category hurts the scores.
The proximity-based nearest neighbours of proper
names raises the MRR scores with 1,5%.
Remember from the introduction that we made
a distinction between the terminological gap and
the knowledge gap. The lexico-semantic re-
sources that are suited to bridge the terminolog-
ical gap, such as synonyms from the alignment-
based method and EWN, do not result in improve-
ments in the experiments under discussion. How-
ever, the lexico-semantic resources that may be
used to bridge the knowledge gap, i.e. associations
from the proximity-based method and categorised
54
CLEF score
EWN Syntax Align Proxi Cat.NEs Baseline
46.3 47.0 46.6 47.6 47.9 46.8
Table 4: CLEF scores of the QA system with query
expansion from several sources
named entities, do result in improvements of the
IR component.
To determine the effect of query expansion on
the QA system as a whole we determined the av-
erage CLEF score when using the various types
of lexico-semantic information for the IR com-
ponent. The CLEF score gives the precision of
the first (highest ranked) answer only. For EWN,
the syntax-based, and the alignment-based nearest
neighbours we have used all expansions for all syn-
tactic categories together. For the proximity-based
nearest neighbours and the categorised named en-
tities we have limited the expansions to the proper
names as these performed rather well.
The positive effect of using categorised named
entities and proximity-based nearest neighbours
for query expansion is visible in the CLEF scores
as well, although less apparent than in the MRR
scores from the IR component in Table 2.
6.1 Error analysis
Let us first take a look at the disappointing re-
sults regarding the terminological gap, before we
move to the more promising results related to the
knowledge gap. We expected that the expansions
of verbs would be particularly helpful to overcome
the terminological gap that is large for verbs, since
there is much variation. We will give some exam-
ples of expansion from EWN and the alignment-
based method.
(2) Wanneer werd het Verdrag van Rome getekend?
?When was the Treaty of Rome signed??
Align: teken ?sign?? typeer ?typify?, onderteken ?sign?
EWN: teken ?sign? ? typeer ?typify?, kentekenen ?charac-
terise?, kenmerk ?characterise?, schilder ?paint?, kenschets
?characterise?, signeer ?sign?, onderteken ?sign?, schets
?sketch?, karakteriseer ?characterise?.
For the example in (2) both the alignment-based
expansions and the expansion from EWN result in
a decrease in RR of 0.5. The verb teken ?sign? is
ambiguous. We see three senses of the verb repre-
sented in the EWN list, i.e. drawing, characteris-
ing, and signing as in signing an official document.
One out of the two expansions for the alignment-
based method and 2 out of 9 for EWN are in princi-
ple synonyms of teken ?sign? in the right sense for
this question. However, the documents that hold
the answer to this question do not use synonyms
for the word teken. The expansions only introduce
noise.
We found a positive example in (3). The RR
score is improved by 0.3 for both the alignment-
based expansions and the expansions from EWN,
when expanding explodeer ?explode? with ontplof
?blow up?.
(3) Waar explodeerde de eerste atoombom?
?Where did the first atomic bomb explode??
Align: explodeer ?explode? ? ontplof ?blow up?.
EWN: explodeer ?explode?? barst los ?burst?, ontplof ?blow
up?, barst uit ?crack?, plof ?boom?.
To get an idea of the amount of terminologi-
cal variation between the questions and the doc-
uments, we determined the optimal expansion
words for each query, by looking at the words
that appear in the relevant documents. When in-
specting these, we learned that there is in fact lit-
tle to be gained by terminological variation. In
the 25 questions we inspected we found 1 near-
synonym only that improved the scores: gekke-
koeienziekte ?mad cow disease? ? Creutzfeldt-
Jacob-ziekte ?Creutzfeldt-Jacob disease?.
The fact that we find only few synonyms might
be related to a point noted by Mur (2006): Some
of the questions in the CLEF track that we use for
evaluation look like back formulations.
After inspecting the optimal expansions, we
were under the impression that most of the expan-
sions that improved the scores were related to the
knowledge gap, rather than the terminological gap.
We will now give some examples of good and bad
expansions related to the knowledge gap.
The categorised named entities result in the best
expansions, followed by the proximity-based ex-
pansions. In (4) an example is given for which cat-
egorised named entities proved very useful:
(4) Wie is Keith Richard?
?Who is Keith Richard??
Cat. NEs: Keith Richard ? gitarist ?guitar player?, lid
?member?, collega ?colleague?, Rolling Stones-gitarist
?Rolling Stones guitar player?, Stones-gitarist ?Stones guitar
player?.
It is clear that this type of information helps a lot
in answering the question in (4). It contains the
answer to the question. The RR for this question
goes from 0 to 1. We see the same effect for the
55
question Wat is NASA? ?What is NASA??.
It is a known fact that named entities are an im-
portant category for QA. Many questions ask for
named entities or facts related to named entities.
From these results we can see that adding the ap-
propriate categories to the named entities is useful
for IR in QA.
The categorised named entities were not always
successful. In (5) we show that the proximity-
based expansion proved more helpful in some
cases.
(5) Welke bevolkingsgroepen voerden oorlog in
Rwanda?
?What populations waged war in Rwanda??
Proximity: Rwanda? Za??re, Hutu, Tutsi, Ruanda, Rwandees
?Rwandese?.
Cat. NEs: Rwanda ? bondgenoot ?ally?, land ?country?,
staat ?state?, buurland ?neighbouring country?.
In this case the expansions from the proximity-
based method are very useful (except for Zaire),
since they include the answer to the question. That
is not always the case, as can be seen in (6). How-
ever, the expansions from the categorised named
entities are not very helpful in this case either.
(6) Wanneer werd het Verdrag van Rome getekend?
?When was the treaty of Rome signed??
Proximity: Rome ? paus ?pope?, Italie?, bisschop ?bishop?,
Italiaans ?Italian?, Milaan ?Milan?.
Cat. NEs: Rome ? provincie ?province?, stad ?city?,
hoofdstad ?capital?, gemeente ?municipality?.
IR does identify Verdrag van Rome ?Treaty of
Rome? as a multi-word term, however it adds the
individual parts of multi-word terms as keywords
as a form of compound analysis. It might be bet-
ter to expand the multi-word term only and not
its individual parts to decrease ambiguity. Ver-
drag van Rome ?Treaty of Rome? is not found in
the proximity-based nearest neighbours, because it
does not include multi-word terms.
Still, it is not very helpful to expand the word
Rome with pope for this question that has nothing
to do with religious affairs. We can see this as a
problem of word sense disambiguation. The as-
sociation pope belongs to Rome in the religious
sense, the place where the Catholic Church is
seated. Rome is often referred to as the Catholic
Church itself, as in Henry VIII broke from Rome.
Gonzalo et al (1998) showed in an experiment,
where words were manually disambiguated, that
a substantial increase in performance is obtained
when query words are disambiguated, before they
are expanded.
We tried to take care of these ambiguities by
using an overlap method. The overlap method
selects expansions that were found in the near-
est neighbours of more than two query words.
Unfortunately, as Navigli and Velardi (2003),
who implement a similar technique, using lexico-
semantic information from WordNet, note, the
COMMON NODES EXPANSION TECHNIQUE works
very badly. Also, Voorhees (1993) who uses a
similar method to select expansions concludes that
the method has the tendency to select very general
terms that have more than one sense themselves.
In future work we would like to implement the
method by Qiu and Frei (1993), as discussed in
section 2, that uses a more sophisticated technique
to combine the expansions of several words in the
query.
7 Conclusion
We can conclude from these experiments on query
expansion for passage retrieval that query expan-
sion with synonyms to overcome the terminolog-
ical gap is not very fruitful. We believe that the
noise introduced by ambiguity of the query terms
is stronger than the positive effect of adding lexi-
cal variants. This is in line with findings by Yang
and Chua (2003). On the contrary, Pasc?a and
Harabagiu (2001) were able to improve their QA
system by using lexical and semantic alternations
from WordNet using feedback loops.
The disappointing results might also be due to
the small amount of terminological variation be-
tween questions and document collection.
However, adding extra information with regard
to the subject field of the query, query expansions
that bridge the knowledge gap, proved slightly
beneficial. The proximity-based expansions aug-
ment the MRR scores with 1.5%. Most successful
are the categorised named entities. These expan-
sions were able to augment the MRR scores with
nearly 3.5%.
The positive effect of using categorised named
entities and proximity-based nearest neighbours
for query expansion is visible in the CLEF scores
for the QA system overall as well. However, the
improvements are less apparent than in the MRR
scores from the IR component.
56
Acknowledgements
This research was carried out in the project
Question Answering using Dependency Relations,
which is part of the research program for Interac-
tive Multimedia Information eXtraction, IMIX, fi-
nanced by NWO, the Dutch Organisation for Scien-
tific Research and partly by the European Commu-
nity?s Seventh Framework Programme (FP7/2007-
2013) under grant agreement n 216594 (CLASSIC
project: www.classic-project.org).
References
Gonzalo, J., F. Verdejo, I. Chugur, and J. Cigarran.
1998. Indexing with WordNet synsets can improve
text retrieval. In Proceedings of the COLING/ACL
Workshop on Usage of WordNet for NLP.
Grefenstette, G. 1992. Use of syntactic context to pro-
duce term association lists for text retrieval. In Pro-
ceedings of the Annual International Conference on
Research and Development in Information Retrieval
(SIGIR).
Grefenstette, G. 1994. Explorations in automatic the-
saurus discovery. Kluwer Academic Publishers.
Harris, Z. S. 1968. Mathematical structures of lan-
guage. Wiley.
Jakarta, Apache. 2004. Apache Lucene - a high-
performance, full-featured text search engine library.
http://lucene.apache.org/java/docs/index.html.
Kaisser, M. and B. Webber. 2007. Question answering
based on semantic roles. In Proceedings of de ACL
workshop on deep linguistic processing.
Koehn, P. 2003. Europarl: A multilingual corpus for
evaluation of machine translation.
Moldovan, D., M. Passc?a, S. Harabagiu, and M. Sur-
deanu. 2002. Performance issues and error analysis
in an open-domain question answering system. In
Proceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
Moldovan, D., M. Pasc?a, S. Harabagiu, and M. Sur-
deanu. 2003. Performance issues and error analysis
in an open-domain question answering system. ACM
Transactions on Information Systems., 21(2):133?
154.
Monz, C. 2003. From Document Retrieval to Question
Answering. Ph.D. thesis, University of Amsterdam.
Mur, J. 2006. Increasing the coverage of answer ex-
traction by applying anaphora resolution. In Fifth
Slovenian and First International Language Tech-
nologies Conference (IS-LTC).
Navigli, R. and P. Velardi. 2003. An analysis of
ontology-based query expansion strategies. In Pro-
ceedings of the Workshop on Adaptive Text Extrac-
tion and Mining (ATEM), in the 14th European Con-
ference on Machine Learning (ECML 2003).
Pantel, P. and D. Ravichandran. 2004. Automati-
cally labeling semantic classes. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing
(HLT/EMNLP).
Pasc?a, M. and S Harabagiu. 2001. The informative role
of wordnet in open-domain question answering. In
Proceedings of the NAACL 2001 Workshop on Word-
Net and Other Lexical Resources.
Qiu, Y. and H.P. Frei. 1993. Concept-based query ex-
pansion. In Proceedings of the Annual International
Conference on Research and Development in Infor-
mation Retrieval (SIGIR), pages 160?169.
Shen, D. and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
EMNLP.
Tiedemann, J. and J. Mur. 2008. Simple is best: Exper-
iments with different document segmentation strate-
gies for passage retrieval. In Proceedings of the
Coling workshop Information Retrieval for Question
Answering. To appear.
van der Plas, L. and J. Tiedemann. 2006. Finding
synonyms using automatic word alignment and mea-
sures of distributional similarity. In Proceedings of
COLING/ACL.
van der Plas, Lonneke. 2008. Automatic lexico-
semantic acquisition for question answering. Ph.D.
thesis, University of Groningen. To appear.
van Noord, G. 2006. At last parsing is now operational.
In Actes de la 13eme Conference sur le Traitement
Automatique des Langues Naturelles.
Voorhees, E.M. 1993. Query expansion using lexical-
semantic relations. In Proceedings of the Annual
International Conference on Research and Develop-
ment in Information Retrieval (SIGIR).
Vossen, P. 1998. EuroWordNet a multilingual database
with lexical semantic networks.
Yang, H. and T-S. Chua. 2003. Qualifier: question an-
swering by lexical fabric and external resources. In
Proceedings of the Conference on European Chap-
ter of the Association for Computational Linguistics
(EACL).
Yang, H., T-S. Chua, Sh. Wang, and Ch-K. Koh. 2003.
Structured use of external knowledge for event-based
open domain question answering. In Proceedings
of the Annual International Conference on Research
and Development in Information Retrieval (SIGIR).
57
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 113?117,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Cross-lingual Validity of PropBank in the Manual Annotation of French
Lonneke van der Plas Tanja Samardz?ic?
Linguistics Department
University of Geneva
Rue de Candolle 5, 1204 Geneva
Switzerland
{Lonneke.vanderPlas,Tanja.Samardzic,Paola.Merlo}@unige.ch
Paola Merlo
Abstract
Methods that re-use existing mono-lingual
semantic annotation resources to annotate
a new language rely on the hypothesis that
the semantic annotation scheme used is
cross-lingually valid. We test this hypoth-
esis in an annotation agreement study. We
show that the annotation scheme can be
applied cross-lingually.
1 Introduction
It is hardly a controversial statement that elegant
language subtleties and powerful linguistic im-
agery found in literary writing are lost in trans-
lation. Yet, translation preserves enough meaning
across language pairs to be useful in many appli-
cations and for many text genres.
The belief that this layer of meaning which is
preserved across languages can be formally rep-
resented and automatically calculated underlies
methods that use parallel corpora for the automatic
generation of semantic annotations through cross-
lingual transfer (Pado?, 2007; Basili et al, 2009).
A methodology similar in spirit ? re-use of the
existing resources in a different language ? has
also been applied in developing manually anno-
tated resources. Monachesi et al (2007) annotate
Dutch sentences using the PropBank annotation
scheme (Palmer et al, 2005), while Burchardt et
al. (2009) use the FrameNet framework (Fillmore
et al, 2003) to annotate a German corpus. In-
stead of building special lexicons containing the
specific semantic information needed for the an-
notation for each language separately, which is a
complex and time-consuming endeavour in itself,
these approaches rely on the lexicons already de-
veloped for English.
In this paper, we hypothesize that the level
of abstraction that is necessary to develop a se-
mantic lexicon/ontology for a single language
based on observable linguistic behaviour ? that
is a mono-lingual, item-specific annotation ? is
cross-linguistically valid. We test this hypothe-
sis by manually annotating French sentences using
the PropBank frame files developed for English.
It has been claimed that semantic parallelism
across languages is smaller when using the
PropBank semantic annotations instead of the
FrameNet scheme, because FrameNet is more ab-
stract and less verb-specific (Pado?, 2007). We are
working with the PropBank annotation scheme,
contrary to other works that use the FrameNet
scheme, such as Pado? (2007) and Basili et al
(2009). We choose this annotation for two main
reasons. First, the primary use of our annotation is
to serve as a gold standard in the task of syntactic-
semantic parsing. FrameNet does not have a prop-
erly sampled hand-annotated corpus of English,
by design. So we cannot use it for this task. Sec-
ond, in Merlo and Van der Plas (2009), the seman-
tic annotations schemes of PropBank and VerbNet
(Kipper, 2005) are compared, based on annotation
of the SemLink project (Loper et al, 2007). The
authors conclude that PropBank is the preferred
annotation for a joint syntactic-semantic setting.
If the PropBank annotation scheme is cross-
lingually valid, annotators can reach a consensus
and can do so swiftly. Thus, cross-lingual valid-
ity is measured by how well-defined the manual
annotation task is (inter-annotator agreement) and
by how hard it is to reach an agreement (pre- and
post-consensus inter-annotator agreement). In ad-
dition, we measure the impact of the level of ab-
straction of the predicate labels. Conversely, how
often labels do not transfer and distributions of dis-
agreements are indicators of lack of parallelism
across languages that we study both by quantita-
tive and qualitative analysis.
To preview the results, we find that the Prop-
Bank annotation scheme developed for English
can be applied for a large portion of French sen-
113
tences without adjustments, which confirms its
cross-lingual validity. A high level of inter-
annotator agreement is reached when the verb-
specific PropBank labels are replaced by less fine-
grained verb classes after annotating. Non-parallel
cases are mostly due to idioms and collocations.
2 Materials and Methods
Our choices of formal representation and of la-
belling scheme are driven by the goal of produc-
ing useful annotations for syntactic-semantic pars-
ing in a setting based on an aligned corpus. In the
following subsections we describe the annotation
scheme and procedure, the corpus, and phases of
annotation.
2.1 The PropBank Annotation Framework
We use the PropBank scheme for the manual anno-
tations. PropBank is a linguistic resource that con-
tains information on the semantic structure of sen-
tences. It consists of a one-million-word corpus
of naturally occurring sentences annotated with
semantic structures and a lexicon (the PropBank
frame files) that lists all the predicates (verbs) that
can be found in the annotated sentences and the
sets of semantic roles they introduce.
Predicates are marked with labels that specify
the sense of the verb in the particular sentence. Ar-
guments are marked with the labels A0 to A5. The
labels A0 and A1 have approximately the same
value with all verbs. They are used to mark in-
stances of typical AGENTS (A0) and PATIENTS
(A1). The value of other numbers varies across
verbs. Modifiers are annotated in PropBank with
the label AM. This label can have different exten-
sions depending on the semantic type of the con-
stituent, for example locatives and adverbials.
2.2 Annotation Procedure
Annotators have access to PropBank frame files
and guidelines adapted for the current task. The
frame files provide verb-specific descriptions of all
possible semantic roles and illustrate these roles
with examples as shown for the verb paid in (1)
and the verb senses of pay in Table 1. Annotators
need to look up each verb in the frame files to be
able to label it with the right verb sense and to be
able to allocate the arguments consistently.
(1) [A0 The Latin American nation] has
[REL?PAY.01 paid] [A1 very little] [A3 on its
debt] [AM?TMP since early last year].
Frame Semantic roles
pay.01 A0: payer or buyer
A1: money or attention
A2: person being paid, destination of attention
A3: commodity, paid for what
pay.02 A0: payer
pay off A1: debt
A2: owed to whom, person paid
pay.03 A0: payer or buyer
pay out A1: money or attention
A2: person being paid, destination of attention
A3: commodity, paid for what
pay.04 A1: thing succeeding or working out
pay.05 A1: thing succeeding or working out
pay off
pay.06 A0: payer
pay down A1: debt
Table 1: The PropBank lexicon entry for pay.
In our cross-lingual setting, annotators used
the English PropBank frame files to annotate the
French sentences. This means that for every pred-
icate they find in the French sentence, they need
to translate it, and find an English verb sense that
is applicable to the French verb. If an appropri-
ate entry cannot be found in the frame files for a
given predicate, the annotator is instructed to use
the ?dummy? label for the predicate and fill in the
roles according to their own insights.
For the annotation of sentences we use an adap-
tation of the user-friendly, freely available Tree
Editor (TrEd, Pajas and S?te?pa?nek, 2008). The tool
shows the syntactic analysis and the plain sentence
in the same window allowing the user to add se-
mantic arcs and labels to the nodes in the syntactic
dependency tree.
The decision to show syntactic information is
merely driven by the fact that we want to guide the
annotator in selecting the heads of phrases during
the annotation process. The sentences are parsed
by a syntactic parser (Titov and Henderson, 2007)
that we trained on syntactic dependency annota-
tions for French (Candito et al, 2009). Although
the parser is state-of-the-art (87.2% Labelled At-
tachment Score), in case of parse errors, we ask
annotators to ignore the errors of the parser and
put the label on the actual head.
2.3 Corpus
We selected the French sentences for the man-
ual annotation from the parallel Europarl corpus
(Koehn, 2005). Because translation shifts are
known to pose problems for the automatic cross-
lingual transfer of semantic roles (Pado?, 2007)
and for machine translation (Ozdowska and Way,
114
2009), and these are more likely to appear in in-
direct translations, we decided to select only those
parallel sentences, for which we can infer from the
labels used in Europarl that they are direct trans-
lations from English to French, or vice versa. We
selected 1040 sentences for annotation (40 in to-
tal for the two training phases, 100 for calibration,
and 900 for the main annotation phase.)1
2.4 Annotation Phases
The training procedure described in Figure 1
is inspired by the methodology indicated in
Pado? (2007). A set of 130 sentences were anno-
tated manually by four annotators with very good
proficiency in both French and English for the
training and the calibration phase. The remaining
900 sentences are annotated by one annotator (out
of those four), a trained linguist. Inter-annotator
agreement was measured at several points in the
annotation process marked with an arrow in Fig-
ure 1. The guidelines were adjusted after the train-
ing phase.
? Training phase
-TrainingA: 10 sentences, all annotators together
-TrainingB: 30 sentences, all annotators individually?
-Reach consensus on Training B?
? Calibration phase
-100 sentences by main annotator, one third of those by
each of the other 3 annotators?
? Main annotation phase
-900 sentences by main annotator
Figure 1: The annotation phases.
3 Results
Cross-lingual validity is measured by comparing
inter-annotator agreement at several stages in the
annotation, by measuring the agreement on less
specific predicate labelling, and by a quantitative
and qualitative analysis of non-parallel cases.
3.1 Inter-annotator Agreement for Several
Annotation Phases
To assess the quality of the manual annotations we
measured the agreement between annotators as the
average F-measure of all pairs of annotators after
each phase of the annotation procedure.2 The first
1As usual practice in preprocessing for automatic align-
ment, the datasets were tokenised and lowercased and only
sentence pairs corresponding to a 1-to-1 alignment with
lengths ranging from 1 to 40 tokens on both French and En-
glish sides were considered.
2It is a known fact that measuring annotator agreement us-
ing the kappa score is problematic in categorisation tasks that
Predicates Arguments
Lab. F Unl. F Lab. F Unl. F
TrainingB 46 85 62 75
TrainingB(cons.) 95 97 91 95
Calibration 59 93 69 84
Table 2: Percent inter-annotator agreement (F-
measure) for labelled/unlabelled predicates and
for labelled/unlabelled arguments
row of Table 2 shows that the task is hard. But
the difference between the first row and the sec-
ond row shows that there were many differences
between annotators that could be resolved. After
discussions and individual corrections the scores
are between 91% and 95%. This indicates that
the task is well-defined. Row three shows that the
agreement in the calibration phase increases a lot
compared to the last training phase (row 1). This
might in part be due to the fact that the guidelines
were adjusted by the end of the training phase, but
could also be because the annotators are getting
more acquainted to the task and the software.
As expected, because annotators used the En-
glish PropBank frame files to annotate French
verbs, the task of labelling predicates proved more
difficult than labelling semantic roles. It results in
the lowest agreement scores overall. In the follow-
ing subsections we study the sources of disagree-
ment in predicate labelling in more detail.
3.2 Inter-annotator Agreement in Predicate
Labellings
Predicate labels in PropBank apply to particular
verb senses, for example walk.01 for the first sense
of the verb walk. Even though the senses are
coarser than, for example, the senses in Word-
Net (Fellbaum, 1998), the labels are rather spe-
cific. This specificity possibly poses problems
when working in a cross-lingual setting.
We compare the agreement reached using Prop-
Bank verb sense labels with the agreement reached
using the verb classifications from VerbNet (Kip-
per, 2005) and the mapping to PropBank labels
as provided in the type mappings of the SemLink
project3 (Loper et al, 2007). If two annotators
used two different predicate labels to annotate the
do not have a fixed number of items and categories (Burchardt
et al, 2006). The F-measure is a well-known measure used
for the evaluation of many task such as syntactic-semantic
parsing, the task that is the motivation for this paper. The
choice of the F-measure makes the comparison to the perfor-
mance of the future parser easier.
3(http://verbs.colorado.edu/semlink/)
115
same verb, but those verb senses belong to the
same verb class, we count those as correct4.
The average inter-annotator agreement is rela-
tively low when we compare the annotations on
the PropBank verb sense level: 59%. However, at
the level of verb classes, the inter-annotator agree-
ment increases to 81%. This raises the issue of
whether we should not label the predicates with
verb classes instead of verb senses. By using Prop-
Bank labels for the manual annotation and replac-
ing these with verb classes in post-processing, the
benefits are two-fold: We are able to reach a high
level of cross-lingual parallelism on the annota-
tions, while keeping the manual annotation task as
specific and less abstract as possible.
3.3 Analysis of Non-Parallel Cases
For a single annotator, the main measure of cross-
lingual validity is the percentage of dummy pred-
icates in the annotation. In the sentences from the
calibration and the main annotation phase from the
main annotator (1000 sentences in total), we find
130 predicates (tokens) for which the annotator
used the ?dummy? label.
Manual inspection reveals that the ?dummy? la-
bel is mainly used for French multi-word expres-
sions (82%), most of which can be translated by
a single English verb (47%), whereas others can-
not, because they are translated by a combination
that includes a form of ?be? that is not annotated
in PropBank (25%). The 47% of multi-word ex-
pressions that receive the ?dummy? label show the
annotator?s reluctance to put a single verb label on
a French multi-word expression. The annotation
guidelines could be adapted to instruct annotators
not to hesitate in such cases.
Similarly, collocations and idiomatic expres-
sions are the main sources of disagreement in
predicate labellings among annotators. We can
conclude that, as shown in studies on other lan-
guage pairs (Burchardt et al, 2009), collocations
and idiomatic expressions were identified as verb
uses where the verb?s predicate label cannot be
transferred directly from one language to another.
4 Discussion and Related Work
Burchardt et al (2009) use English FrameNet to
4The mappings from PropBank verb sense labels to Verb-
Net verb classes are one-to-many and not complete. We
counted a pair as matching if there exists a class to which
both verb senses belong. We found a verb class for both verb
senses in about 78% of the cases and discarded the rest.
annotate a corpus of German sentences manually.
They find that the vast majority of frames can be
applied to German directly. However, around one
third of the verb senses identified in the German
corpus were not covered by FrameNet. Also, a
number of German verbs were found to be under-
specified. Finally, some problems related to treat-
ing particular verb uses were identified, such as id-
ioms, metaphors, and support verb constructions.
Monachesi et al (2007) use PropBank labels for
semi-automatic annotation of a corpus of Dutch
sentences. Semantic roles were first annotated
using a rule-based semantic parser and then cor-
rected by one annotator. Although not all Dutch
verbs could be translated to an equivalent verb
sense in English, these cases were assessed as rel-
atively rare. What proved to be problematic was
identifying the correct label for modifiers.
Bittar (2009) makes use of cross-lingual lexi-
cal transfer in annotating French verbs with event
types, by adapting a small-scale English verb lex-
icon with specified event structure (TimeML).
The inter-annotator agreement in labelling pred-
icates reported in Burchardt et al (2009) reaches
85%, while our best score (when falling back to
verb classes) is 81%. However, unlike Burchardt
et al (2009) we did not introduce any new French
labels. We find, like Monachesi et al (2007), that
non-parallel cases are less frequent than what is re-
ported in Burchardt et al (2009), which could be
due to the properties of the annotations schemes.
5 Conclusions
We can conclude that the general task of anno-
tating French sentences using English PropBank
frame files is well-defined. Nevertheless, it is a
hard task that requires linguistic training. With re-
spect to the disagreements on labelling predicates,
we can conclude that a large part can be resolved
if we compare the annotations at the level of verb
classes instead of at the very fine-grained level of
verb senses. Non-parallel cases are mostly due to
idioms and collocations. Their rate is relatively
low and can be further reduced by adapting anno-
tation guidelines.
Acknowledgments
The research leading to these results has received fund-
ing from the EU FP7 programme (FP7/2007-2013) under
grant agreement nr 216594 (CLASSIC project: www.classic-
project.org). We would like to thank Goljihan Kashaeva and
James Henderson for valuable comments.
116
References
R. Basili, D. De Cao, D. Croce, B. Coppola, and A. Moschitti,
2009. Computational Linguistics and Intelligent Text Pro-
cessing, chapter Cross-Language Frame Semantics Trans-
fer in Bilingual Corpora, pages 332?345. Springer Berlin
/ Heidelberg.
A. Bittar. 2009. Annotation of events and temporal expres-
sions in French texts. In Proceedings of the third Linguis-
tic Annotation Workshop (LAW III), pages 48?51, Suntec,
Singapore.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado?, and
M. Pinkal. 2006. The SALSA corpus: a German cor-
pus resource for lexical semantics. In Proceedings of the
5th International Conference on Language Resources and
Evaluation (LREC 2006), pages 969?974, Genoa, Italy.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado, and
M. Pinkal, 2009. Multilingual FrameNets in Computa-
tional Lexicography: Methods and Applications, chapter
FrameNet for the semantic analysis of German: Annota-
tion, representation and automation, pages 209?244. De
Gruyter Mouton, Berlin.
M.-H. Candito, B. Crabbe?, P. Denis, and F. Gue?rin.
2009. Analyse syntaxique du franc?ais : des constitu-
ants aux de?pendances. In Proceedings of la Confe?rence
sur le Traitement Automatique des Langues Naturelles
(TALN?09), Senlis, France.
C. Fellbaum. 1998. WordNet, an electronic lexical database.
MIT Press.
C. J. Fillmore, R. Johnson, and M.R.L. Petruck. 2003. Back-
ground to FrameNet. International journal of lexicogra-
phy, 16.3:235?250.
K. Kipper. 2005. VerbNet: A broad-coverage, comprehen-
sive verb lexicon. Ph.D. thesis, University of Pennsylvnia.
P. Koehn. 2005. Europarl: A parallel corpus for statistical
machine translation. In Proceedings of the MT Summit,
pages 79?86, Phuket, Thailand.
E. Loper, S-T Yi, and M. Palmer. 2007. Combining lexical
resources: Mapping between PropBank and VerbNet. In
Proceedings of the 7th International Workshop on Com-
putational Semantics (IWCS-7), pages 118?129, Tilburg,
The Netherlands.
P. Merlo and L. van der Plas. 2009. Abstraction and gen-
eralisation in semantic role labels: PropBank, VerbNet
or both? In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of the
AFNLP, pages 288?296, Suntec, Singapore.
P. Monachesi, G. Stevens, and J. Trapman. 2007. Adding
semantic role annotation to a corpus of written Dutch.
In Proceedings of the Linguistic Annotation Workshop
(LAW), pages 77?84, Prague, Czech republic.
S. Ozdowska and A. Way. 2009. Optimal bilingual data for
French-English PB-SMT. In Proceedings of the 13th An-
nual Conference of the European Association for Machine
Translation (EAMT?09), pages 96?103, Barcelona, Spain.
S. Pado?. 2007. Cross-lingual Annotation Projection Mod-
els for Role-Semantic Information. Ph.D. thesis, Saarland
University.
P. Pajas and J. S?te?pa?nek. 2008. Recent advances in a feature-
rich framework for treebank annotation. In Proceedings of
the 22nd International Conference on Computational Lin-
guistics (Coling 2008), pages 673?680, Manchester, UK.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposi-
tion Bank: An annotated corpus of semantic roles. Com-
putational Linguistics, 31:71?105.
I. Titov and J. Henderson. 2007. A latent variable model
for generative dependency parsing. In Proceedings of the
International Conference on Parsing Technologies (IWPT-
07), pages 144?155, Prague, Czech Republic.
117
Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 28?37,
Beijing, August 2010
Finding Medical Term Variations using Parallel Corpora and
Distributional Similarity
Lonneke van der Plas
Department of Linguistics
University of Geneva
lonneke.vanderplas@unige.ch
Jo?rg Tiedemann
Department of Linguistics and Philology
Uppsala University
jorg.tiedemann@lingfil.uu.se
Abstract
We describe a method for the identifica-
tion of medical term variations using par-
allel corpora and measures of distribu-
tional similarity. Our approach is based
on automatic word alignment and stan-
dard phrase extraction techniques com-
monly used in statistical machine transla-
tion. Combined with pattern-based filters
we obtain encouraging results compared
to related approaches using similar data-
driven techniques.
1 Introduction
Ontologies provide a way to formally represent
knowledge, for example for a specific domain.
Ontology building has received a lot of atten-
tion in the medical domain. This interest is re-
flected in the existence of numerous medical on-
tologies, such as the Unified Medical Language
System (UMLS) (McCray and Hole, 1990) with
its metathesaurus, semantic network, and special-
ist lexicon. Although the UMLS includes infor-
mation for languages other than English, the cov-
erage for other languages is generally smaller.
In this paper we describe an approach to acquire
lexical information for the Dutch medical domain
automatically. In the medical domain variations in
terminology often include multi-word terms such
as aangeboren afwijking ?birth defect? for con-
genitale aandoening ?congenital disorder?. These
multiple ways to refer to the same concept using
distinct (multi-word) terms are examples of syn-
onymy1 but are often referred to as term varia-
1Spelling variants are a type of term variations that are
not included in the definition of synonymy.
tions. These term variations could be used to en-
hance existing medical ontologies for the Dutch
language.
Our technique builds on the distributional hy-
pothesis, the idea that semantically related words
are distributed similarly over contexts (Harris,
1968). This is in line with the Firthian saying that,
?You shall know a word by the company it keeps.?
(Firth, 1957). In other words, you can grasp the
meaning of a word by looking at its contexts.
Context can be defined in many ways. Previous
work has been mainly concerned with the syntac-
tic contexts a word is found in (Lin, 1998; Cur-
ran, 2003). For example, the verbs that are in
a subject relation with a particular noun form a
part of its context. In accordance with the Firthian
tradition these contexts can be used to determine
the semantic relatedness of words. For instance,
words that occur in a object relation with the verb
to drink have something in common: they are liq-
uid. Other work has been concerned with the bag-
of-word context, where the context of a word are
the words that are found in its proximity (Wilks et
al., 1993; Schu?tze, 1992).
Yet another context, that is much less studied, is
the translational context. The translational context
of a word is the set of translations it gets in other
languages. For example, the translational context
of cat is kat in Dutch and chat in French. This
requires a rather broad understanding of the term
context. The idea is that words that share a large
number of translations are similar. For example
both autumn and fall get the translation herfst in
Dutch, Herbst in German, and automne in French.
This indicates that autumn and fall are synonyms.
28
A straightforward place to start looking for
translational context is in bilingual dictionaries.
However, these are not always publicly available
for all languages. More importantly, dictionar-
ies are static and therefore often incomplete re-
sources. We have chosen to automatically acquire
word translations in multiple languages from text.
Text in this case should be understood as multi-
lingual parallel text. Automatic alignment gives
us the translations of a word in multiple lan-
guages. The so-called alignment-based distribu-
tional methods described in Van der Plas (2008)
apply the translational context for the discovery
of single word synonyms for the general domain.
Any multilingual parallel corpus can be used for
this purpose. It is thus possible to focus on
a special domain, such as the medical domain
we are considering in this paper. The automatic
alignment provides us also with domain-specific
frequency information for every translation pair,
which is helpful in case words are ambiguous.
Aligned parallel corpora have often been used
in the field of word sense discovery, the task of
discriminating the different senses words have.
The idea behind it is that a word that receives dif-
ferent translations might be polysemous. For ex-
ample, a word such as wood receives the transla-
tion woud and hout in Dutch, the former referring
to an area with many trees and the latter referring
to the solid material derived from trees. Whereas
this type of work is all built upon the divergence of
translational context, i.e. one word in the source
language is translated by many different words in
the target language, we are interested in the con-
vergence of translations, i.e. two words in the
source language receiving the same translation in
the target language. Of course these two phenom-
ena are not independent. The alleged conversion
of the target language might well be a hidden di-
version of the source language. Since the English
word might be polysemous, the fact that woud and
hout in Dutch are both translated in English by
wood does not mean that woud and hout in Dutch
are synonyms. However, the use of multiple lan-
guages overshadows the noise resulting from pol-
ysemy (van der Plas, 2008).
Van der Plas (2008) shows that the way the
context is defined influences the type of lexico-
semantic knowledge that is discovered. After
gold standard evaluations and manual inspection
the author concludes that when using translational
contexts more tight semantic relations such as
synonymy are found whereas the conventional
syntax-based approaches retrieve hypernyms, co-
hyponyms, and antonyms of the target word. The
performance on synonym acquisition when using
translational contexts is almost twice as good as
when using syntactic contexts, while the amount
of data used is much smaller. Van der Plas (2008)
ascribed the fact that the syntax-based method be-
haves in this way to the fact that loosely related
words, such as wine and beer, are often found in
the same syntactic contexts. The alignment-based
method suffers less from this indiscriminant ac-
ceptance because words are typically translated by
words with the same meaning. The word wine is
typically not translated with a word for beverage
nor with a word for beer, and neither is good trans-
lated with the equivalence of bad.
In this paper we are concerned with medical
term variations that are in fact (multi-word) syn-
onyms. We will use the translational context to
compute similarity between terms. The transla-
tional context is not only very suitable to find
tight relations between words, the transition from
single-word synonyms to multi-word term varia-
tions is also straightforward due to advances in
phrase-based machine translation. We will use
word alignment techniques in combination with
phrase extraction techniques from statistical ma-
chine translation to extract phrases and their trans-
lations from a medical parallel corpus. We com-
bine this approach with Part-of-Speech (PoS) pat-
terns from the term extraction literature to extract
candidate terms from the phrase tables. Using
similarity measures used in distributional methods
we finally compute ranked lists of term variations.
We already noted that these term variations
could be used to enhance existing ontologies for
the Dutch language. On top of that we believe that
the multi-lingual method that uses translations of
multi-word terms in several languages could be
used to expand resources built for English with
translations in other languages (semi-) automati-
cally. This last point falls outside the scope of this
paper.
29
In the following section we will describe the
alignment-based approaches to distributional sim-
ilarity. In section 3 we will describe the method-
ology we followed in this paper in detail. We de-
scribe our evaluation in section 4 and discuss the
results in section 5. Section 6 concludes this pa-
per.
2 Alignment-based methods
In this section we explain the alignment-based ap-
proaches to distributional similarity. We will give
some examples of translational context and we
will explain how measures serve to determine the
similarity of these contexts. We end this section
with a discussion of related work.
2.1 Translational context
The translational context of a word or a multi-
word term is the set of translations it gets in other
languages. For the acquisition of translations for
the Dutch medical terms we rely on automatic
word alignment in parallel corpora.
Figure 1: Example of bidirectional word align-
ments of two parallel sentences
Figure 1 illustrates the automatic word alignment
between a Dutch and an English phrase as a re-
sult of using the IBM alignment models (Brown
et al, 1993) implemented in the open-source tool
GIZA++ (Och, 2003). The alignment of two texts
is bi-directional. The Dutch text is aligned to
the English text and vice versa (dotted lines ver-
sus continuous lines). The alignment models pro-
duced are asymmetric. Several heuristics exist
to combine directional word alignments which is
usually called ?symmetrization?. In order to cover
multi-word terms standard phrase extraction tech-
niques can be used to move from word alignment
to linked phrases (see section 3.2 for more de-
tails).
2.2 Measures for computing similarity
Translational co-occurrence vectors are used to
find distributionally similar words. For ease of
reading, we give an example of a single-word
term kat in Table 1. In our current setting the
terms can be both single- or multi-word terms
such as werkzame stof ?active ingredient?. Ev-
ery cell in the vector refers to a particular transla-
tional co-occurrence type. For example, kat ?cat?
gets the translation Katze in German. The value
of these cells indicate the number of times the co-
occurrence type under consideration is found in
the corpus.
Each co-occurrence type has a cell frequency.
Likewise each head term has a row frequency.
The row frequency of a certain head term is the
sum of all its cell frequencies. In our example the
row frequency for the term kat ?cat? is 65. Cut-
offs for cell and row frequency can be applied to
discard certain infrequent co-occurrence types or
head terms respectively.
DE FR IT EN total
Katze chat gatto cat
kat 17 26 8 13 64
Table 1: Translational co-occurrence vector for
kat (?cat?) based on four languages
The more similar the vectors are, the more dis-
tributionally similar the head terms are. We need a
way to compare the vectors for any two head terms
to be able to express the similarity between them
by means of a score. Various methods can be used
to compute the distributional similarity between
terms. We will explain in section 3 what measures
we have chosen in the current experiments.
2.3 Related work
Multilingual parallel corpora have mostly been
used for tasks related to word sense disambigua-
tion such as separation of senses (Resnik and
Yarowsky, 1997; Dyvik, 1998; Ide et al, 2002).
However, taking sense separation as a basis,
Dyvik (2002) derives relations such as synonymy
and hyponymy by applying the method of se-
mantic mirrors. The paper illustrates how the
method works. First, different senses are iden-
tified on the basis of manual word translations
in sentence-aligned Norwegian-English data (2,6
million words in total). Second, senses are
grouped in semantic fields. Third, features are
30
assigned on the basis of inheritance. Lastly, se-
mantic relations such synonymy and hyponymy
are detected based on intersection and inclusion
among feature sets .
Improving the syntax-based approach for syn-
onym identification using bilingual dictionaries
has been discussed in Lin et al (2003) and Wu and
Zhou (2003). In the latter parallel corpora are also
applied as a reference to assign translation likeli-
hoods to candidates derived from the dictionary.
Both of them are limited to single-word terms.
Some researchers employ multilingual corpora
for the automatic acquisition of paraphrases (Shi-
mota and Sumita, 2002; Bannard and Callison-
Burch, 2005; Callison-Burch, 2008). The last two
are based on automatic word alignment as is our
approach.
Bannard and Callison-Burch (2005) use a
method that is also rooted in phrase-based statis-
tical machine translation. Translation probabili-
ties provide a ranking of candidate paraphrases.
These are refined by taking contextual informa-
tion into account in the form of a language model.
The Europarl corpus (Koehn, 2005) is used. It has
about 30 million words per language. 46 English
phrases are selected as a test set for manual evalu-
ation by two judges. When using automatic align-
ment, the precision reached without using contex-
tual refinement is 48.9%. A precision of 55.3%
is reached when using context information. Man-
ual alignment improves the performance by 26%.
A precision score of 55% is attained when using
multilingual data.
In a more recent publication Callison-Burch
(2008) improved this method by using syntac-
tic constraints and multiple languages in parallel.
We have implemented a combination of Bannard
and Callison-Burch (2005) and Callison-Burch
(2008), in which we use PoS filters instead of
syntactic constraints to compare our results with.
More details can be found in the Section 5.
Apart from methods that use parallel corpora
mono-lingual pattern-based methods have been
used to find term variations. Fahmi (2009) ac-
quired term variation for the medical domain us-
ing a two-step model. As a first step an initial list
of synonyms are extracted using a method adapted
from DIPRE (Brin, 99). During this step syntactic
patterns guide the extraction of candidate terms in
the same way as they will guide the extraction in
this paper. This first step results in a list of candi-
date synonyms that are further filtered following a
method described in Lin et al (2003), which uses
Web pages as an external source to measure the
synonym compatibility hits of each pair. The pre-
cision and recall scores presented in Fahmi (2009)
are high. We will give results for this method
on our test set in Section 5 and refer to it as the
pattern- and web-based approach.
3 Materials and methods
In the following subsections we describe the setup
for our experiments.
3.1 Data collection
Measures of distributional similarity usually re-
quire large amounts of data. For the alignment
method we need a parallel corpus of reasonable
size with Dutch either as source or as target lan-
guage coming from the domain we are interested
in. Furthermore, we would like to experiment
with various languages aligned to Dutch.
The freely available EMEA corpus (Tiede-
mann, 2009) includes 22 languages in parallel
with a reasonable size of about 12-14 million to-
kens per language. The entire corpus is aligned
at the sentence level for all possible combinations
of languages. Thus, for acquiring Dutch syn-
onyms we have 21 language pairs with Dutch as
the source language. Each language pair includes
about 1.1 million sentence pairs. Note that there
is a lot of repetition in EMEA and the number
of unique sentences (sentence fragments) is much
smaller: around 350,000 sentence pairs per lan-
guage pair with about 6-7 million tokens per lan-
guage.
3.2 Word alignment and phrase extraction
For sentence alignment we applied hunalign
(Varga et al, 2005) with the ?realign? function that
induces lexical features from the bitext to be com-
bined with length based features. Word alignment
has been performed using GIZA++ (Och, 2003).
We used standard settings defined in the Moses
toolkit (Koehn et al, 2007) to generate Viterbi
word alignments of IBM model 4 for sentences
31
not longer than 80 tokens. In order to improve
the statistical alignment we used lowercased to-
kens and lemmas in case we had them available
(produced by the Tree-Tagger (Schmid, 1994) and
the Alpino parser (van Noord, 2006)).
We used the grow heuristics to combine the
asymmetric word alignments which starts with
the intersection of the two Viterbi alignments and
adds block-neighboring points to it in a second
step. In this way we obtain high precision links
with some many-to-many alignments. Finally we
used the phrase extraction tool from Moses to ex-
tract phrase correspondences. Phrases in statisti-
cal machine translation are defined as sequences
of consecutive words and phrase extraction refers
to the exhaustive extraction of all possible phrase
pairs that are consistent with the underlying word
alignment. Consistency in this case means that
words in a legal phrase are only aligned to words
in the corresponding phrase and not to any other
word outside of that phrase. The extraction mech-
anism can be restricted by setting a maximum
phrase length which is seven in the default set-
tings of Moses. However, we set the maximum
phrase length to four, because we do not expect
many terms in the medical domain to be longer
than 4 words.
As explained above, word alignment is carried
out on lowercased and possibly lemmatised ver-
sions of the corpus. However, for phrase extrac-
tion, we used surface wordforms and extracted
them along with the part-of-speech (PoS) tags for
Dutch taken from the corresponding Alpino parse
trees. This allows us to lowercase all words except
the words that have been tagged as name. Further-
more, the inclusion of PoS tags enabled us to fil-
ter the resulting phrase table according to typical
patterns of multi-word terms. We also removed
phrases that consist of only non-alphabetical char-
acters. Note that we rely entirely on automatic
processing of our data. Thus, the results from
automatic tagging, lemmatisation and word align-
ment include errors. Bannard and Callison-Burch
(2005) show that when using manual alignment
the percentage of correct paraphrases significantly
rises from 48.9% to 74.9%.
3.3 Selecting candidate terms
As we explained above we can select those
phrases that are more likely to be good terms
by using a regular expression over PoS tags.
We apply a pattern using adjectives (A), nouns
(NN), names (NM) and prepositions (P) as its
components based on Justeson and Katz. (1995)
which was adapted to Dutch by Fahmi (2009):
((A|NN|NM)+|(((A|NN|NM)*
(NN|NM P)?)(A|NN|NM)*))NN+
To explain this regular expression in words, a
candidate term is either a sequence of adjectives
and/or nouns and/or names, ending in a noun or
name or it consists of two such strings, separated
by a single preposition.
After applying the filters and removing all ha-
paxes we are left with 9.76 M co-occurrences of a
Dutch (multi-word) term and a foreign translation.
3.4 Comparing vectors
To compare the vectors of the terms we need a
similarity measures. We have chosen to describe
the functions used in this paper using an extension
of the notation used by Lin (1998), adapted by
Curran (2003). Co-occurrence data is described
as tuples: ?word, language, word??, for example,
?kat, EN, cat?.
Asterisks indicate a set of values ranging over
all existing values of that component of the rela-
tion tuple. For example, (w, ?, ?) denotes for a
given word w all translational contexts it has been
found in in any language. For the example of
kat in, this would denote all values for all transla-
tional contexts the word is found in: Katze DE:17,
chat FR:26 etc. Everything is defined in terms
of co-occurrence data with non-zero frequencies.
The set of attributes or features for a given corpus
is defined as:
(w, ?, ?) ? {(r, w?)|?(w, r, w?)}
Each pair yields a frequency value, and the se-
quence of values is a vector indexed by r:w? val-
ues, rather than natural numbers. A subscripted
asterisk indicates that the variables are bound to-
gether:
?
(wm, ?r, ?w?) ? (wn, ?r, ?w?)
32
The above refers to a dot product of the vectors
for term wm and term wn summing over all the
r:w? pairs that these two terms have in common.
For example we could compare the vectors for kat
and some other term by applying the dot product
to all bound variables.
We have limited our experiments to using Co-
sine2. We chose this measure, since it performed
best in experiments reported in Van der Plas
(2008). Cosine is a geometrical measure. It re-
turns the cosine of the angle between the vectors
of the words and is calculated as the dot product
of the vectors:
Cosine =
? (W1, ?r, ?w?) ? (W2, ?r, ?w?)?? (W1, ?, ?)2 ?? (W2, ?, ?)2
If the two words have the same distribution the
angle between the vectors is zero.
3.5 Post-processing
A well-known problem of phrase-based meth-
ods to paraphrase or term variation acquisition
is the fact that a large proportion of the term
variations or paraphrases proposed by the sys-
tem are super- or sub-strings of the original term
(Callison-Burch, 2008). To remedy this prob-
lem we removed all term variations that are ei-
ther super- or sub-strings of the original term from
the lists of candidate term variations output by the
system.
4 Evaluation
There are several evaluation methods available to
assess lexico-semantic data. Curran (2003) distin-
guishes two types of evaluation: direct evaluation
and indirect evaluation. Direct evaluation meth-
ods compare the semantic relations given by the
2Feature weights have been used in previous work for
syntax-based methods to account for the fact that co-
occurrences have different information values. Selectionally
weak (Resnik, 1993) or light verbs such as hebben ?to have?
have a lower information value than a verb such as uitpersen
?squeeze? that occurs less frequently. Although weights that
promote features with a higher information value work very
well for syntax-based methods, Van der Plas (2008) showed
that weighting only helps to get better synonyms for very in-
frequent nouns when applied in alignment-based approaches.
In the current setting we do not consider very infrequent
terms so we did not use any weighting.
system against human performance or expertise.
Indirect approaches evaluate the system by mea-
suring its performance on a specific task.
Since we are not aware of a task in which we
could test the term variations for the Dutch medi-
cal domain and ad-hoc human judgments are time
consuming and expensive, we decided to com-
pare against a gold standard. Thereby denying
the common knowledge that the drawback of us-
ing gold standard evaluations is the fact that gold
standards often prove to be incomplete. In previ-
ous work on synonym acquisition for the general
domain, Van der Plas and Tiedemann (2006) used
the synsets in Dutch EuroWordnet (Vossen, 1998)
for the evaluation of the proposed synonyms. In
an evaluation with human judgments, Van der Plas
and Tiedemann (2006) showed that in 37% of the
cases the majority of the subjects judged the syn-
onyms proposed by the system to be correct even
though they were not found to be synonyms in
Dutch EuroWordnet. For evaluating medical term
variations in Dutch there are not many gold stan-
dards available. Moreover, the gold standards that
are available are even less complete than for the
general domain.
4.1 Gold standard
We have chosen to evaluate the nearest neighbours
of the alignment-based method on the term vari-
ations from the Elseviers medical encyclopedia
which is intended for the general audience con-
taining 379K words. The encyclopedia was made
available to us by Spectrum B.V.3.
The test set is comprised of 848 medical terms
from aambeeld ?incus? to zwezerik ?thymus? and
their term variations. About 258 of these entries
contain multiword terms. For most of the terms
the list from Elseviers medical encyclopedia gives
only one term variation, 146 terms have two term
variations and only one term has three variations.
For each of these medical terms in the test set the
system generates a ranked list of term variations
that will be evaluated against the term variations
in the gold standard.
3http://www.kiesbeter.nl/medischeinformatie/
33
5 Results and Discussion
Before we present our results and give a detailed
error analysis we would like to remind the reader
of the two methods we compare our results with
and give some more detail on the implementation
of the second method.
5.1 Two methods for comparison
The first method is the pattern- and web-based ap-
proach described in Fahmi (2009). Note that we
did not re-implement the method, so we were not
able to run the method on the same corpus we
are using in our experiments. The corpus used
in Fahmi (2009) is a medical corpus developed
in Tilburg University (http://ilk.uvt.nl/rolaquad).
It consists of texts from a medical encyclopedia
and a medical handbook and contains 57,004 sen-
tences. The system outputs a ranked list of term
variation pairs. We selected the top-100 pairs
that are output by the system and evaluated these
on the test set described in Subsection 4.1. The
method is composed of two main steps. In the
first step candidate terms are extracted from the
corpus using a PoS filter, that is similar to the
PoS filter we applied. In the second step pairs of
candidate term variations are re-ranked on the ba-
sis of information from the Web. Phrasal patterns
such as XorY are used to get synonym compat-
ibility hits as opposed to XandY that points to
non-synonymous terms.
The second method we compare with is the
phrase-based translation method first introduced
by Bannard and Callison-Burch (2005). Statisti-
cal word alignment can be used to measure the re-
lation between source language items. Here, one
makes use of the estimated translation likelihoods
of phrases (p(f |e) and p(e|f)) that are used to
build translation models in standard phrase-based
statistical machine translation systems (Koehn et
al., 2007). Bannard and Callison-Burch (2005)
define the problem of paraphrasing as the follow-
ing search problem:
e?2 = argmaxe2:e2 6=e1p(e2|e1) where
p(e2|e1) ?
?
f
p(f |e1)p(e2|f)
Certainly, for paraphrasing we are not only inter-
ested in e?2 but for the top-ranked paraphrase can-
didates but this essentially does not change the al-
gorithm. In their paper, Bannard and Callison-
Burch (2005) also show that systematic errors
(usually originating from bad word alignments)
can be reduced by summing over several language
pairs.
e?2 ? argmaxe2:e2 6=e1
?
C
?
fC
p(fC |e1)p(e2|fC)
This is the approach that we also adapted for our
comparison. The only difference in our imple-
mentation is that we applied a PoS-filter to extract
candidate terms as explained in section 3.3. In
some sense this is a sort of syntactic constraint in-
troduced in Callison-Burch (2008). Furthermore,
we set the maximum phrase length to 4 and ap-
plied the same post-processing as described in
Subsection 3.5 to obtain comparable results.
5.2 Results
Table 2 shows the results for our method com-
pared with the method adapted from Bannard and
Callison-Burch (2005) and the method by Fahmi
(2009). Precision and recall are given at several
values of k. At k=1, only the top-1 term varia-
tions the system proposes are taken into account.
At k=3 the top-3 candidate term variations are in-
cluded in the calculations.
The last column shows the coverage of the sys-
tem. A coverage of 40% means that for 40% of the
850 terms in the test set one or more term varia-
tions are found. Recall is measured for the terms
covered by the system.
From Table 2 we can read that the method we
propose is able to get about 30% of the term vari-
ations right, when only the top-1 candidates are
considered. It is able to retrieve roughly a quarter
of the term variations provided in the gold stan-
dard4. If we increase k precision goes down and
recall goes up. This is expected, because the sys-
tem proposes a ranked list of candidate term vari-
ations so at higher values of k the quality is lower,
but more terms from the gold standard are found.
4Note that a recall of 100% is not possible, because some
terms have several term variations.
34
Method k=1 k=2 k=3 Coverage
P R P R P R
Phrase-based Distr. Sim 28.9 22.8 21.8 32.7 17.3 37.2 40.0
Bannard&Callison-Burch (2005) 18.4 15.3 16.9 27.3 13.7 32.3 48.1
Fahmi (2009) 38.2 35.1 37.1 35.1 37.1 35.1 4.0
Phrase-based Distr. Sim (hapaxes) 25.4 20.9 20.4 32.1 16.1 36.8 47.8
Table 2: Percent precision and recall at several values of k and percent coverage for the method pro-
posed in this paper (plus a version including hapaxes), the method adapted from Bannard and Callison-
Burch (2005) and the output of the system proposed by Fahmi (2009)
In comparison, the scores resulting from our
adapted implementation of Bannard and Callison-
Burch (2005) are lower. They do however, man-
age to find more terms from the test set covering
around 48% of the words in the gold standard.
This is due to the cut-off that we use when cre-
ating the co-occurrence vector to remove unreli-
able data points. In our approach we discarded
hapaxes, whereas for the Bannard and Callison-
Burch approach the entire phrase table is used.
We therefore ran our system once again without
this cut-off. As expected, the coverage went up
in that setting ? actually to 48% as well.5 How-
ever, we can see that the precision and recall re-
mained higher, than the scores we got with the
implementation following Bannard and Callison-
Burch (2005). Hence, our vector-based approach
seems to outperform the direct use of probabilities
from phrase-based MT.
Finally, we also compare our results with the
data set extracted using the pattern- and web-
based approach from Fahmi (2009). The precision
and recall figures of that data set are the highest in
our comparison. However, since the coverage of
this method is very low (which is not surprising
since a smaller corpus is used to get these results)
the precision and recall are calculated on the ba-
sis of a very small number of examples (35 to be
precise). The results are therefore not very reli-
able. The precision and recall figures presented
in Fahmi (2009), however, point in the same di-
rection. To get an idea of the actual coverage of
this method we would need to apply this extrac-
tion technique to the EMEA corpus. This is espe-
cially difficult due to the heavy use of web queries
5The small difference in coverage is due to some mistakes
in tokenisation for our method.
which makes it problematic to apply this method
to large data sets.
5.3 Error analysis
The most important finding we did, when closely
inspecting the output of the system is that many of
the term variations proposed by the system are not
found in the gold standard, but are in fact correct.
Here, we give some examples below:
arts, dokter (?doctor?)
ademnood, ademhalingsnood (?respiratory distress?)
aangezichtsverlamming, gelaatsparalyse (?facial paralysis?)
alvleesklierkanker, pancreaskanker (?cancer of the pan-
creas?)
The scores given in Table 2 are therefore pes-
simistic and a manual evaluation with domain spe-
cialist would certainly give us more realistic and
probably much higher scores. We also found some
spelling variants which are usually not covered by
the gold standard. Look, for instance, at the fol-
lowing examples:
astma, asthma (?asthma?)
atherosclerose, Artherosclerosis (?atherosclerosis?)
autonoom zenuwstelsel, autonome zenuwstelsel (?autonomic
nervous system?)
Some mistakes could have been avoided using
stemming or proper lemmatisation (plurals that
are counted as wrong):
abortus, zwangerschapsafbrekingen (?abortion?)
adenoom, adenomen (?adenoma?)
indigestie, spijsverteringsstoornissen (?indigestion?)
After removing the previous cases from the data,
some of the remaining mistakes are related to the
problem we mentioned in section 3.5: Phrase-
35
based methods to paraphrase or term variation ac-
quisition have the tendency to propose term vari-
ations that are super- or sub-strings of the origi-
nal term. We were able to filter out these super-
or sub-strings, but not in cases where a candidate
term is a term variation of a super- or sub-string of
the original term. Consider, for example the term
bloeddrukverlaging ?blood pressure decrease? and
the candidate afname ?decrease?, where afname is
a synonym for verlaging.
6 Conclusions
In this article we have shown that translational
context together with measures of distributional
similarity can be used to extract medical term vari-
ations from aligned parallel corpora. Automatic
word alignment and phrase extraction techniques
from statistical machine translation can be applied
to collect translational variations across various
languages which are then used to identify seman-
tically related words and phrases. In this study, we
additionally apply pattern-based filters using part-
of-speech labels to focus on particular patterns of
single and multi-word terms. Our method out-
performs another alignment-based approach mea-
sured on a gold standard taken from a medical en-
cyclopedia when applied to the same data set and
using the same PoS filter. Precision and recall are
still quite poor according to the automatic evalu-
ation. However, manual inspection suggests that
many candidates are simply misjudged because of
the low coverage of the gold standard data. We
are currently setting up a manual evaluation. Alto-
gether our approach provides a promising strategy
for the extraction of term variations using straight-
forward and fully automatic techniques. We be-
lieve that our results could be useful for a range of
applications and resources and that the approach
in general is robust and flexible enough to be ap-
plied to various languages and domains.
Acknowledgements
The research leading to these results has received
funding from the EU FP7 programme (FP7/2007-
2013) under grant agreement nr 216594 (CLAS-
SIC project: www.classic-project.org).
References
Bannard, C. and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceedings
of the annual Meeting of the Association for Com-
putational Linguistics (ACL).
Brin, S. 99. Extracting patterns and relations from the
World Wide Web. In WebDB ?98: Selected papers
from the International Workshop on The World Wide
Web and Databases.
Brown, P.F., S.A. Della Pietra, V.J. Della Pietra, and
R.L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?296.
Callison-Burch, C. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
Curran, J.R. 2003. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
Dyvik, H. 1998. Translations as semantic mirrors.
In Proceedings of Workshop Multilinguality in the
Lexicon II (ECAI).
Dyvik, H. 2002. Translations as semantic mirrors:
from parallel corpus to wordnet. Language and
Computers, Advances in Corpus Linguistics. Pa-
pers from the 23rd International Conference on En-
glish Language Research on Computerized Corpora
(ICAME 23), 16:311?326.
Fahmi, I. 2009. Automatic Term and Relation Extrac-
tion for Medical Question Answering System. Ph.D.
thesis, University of Groningen.
Firth, J.R. 1957. A synopsis of linguistic theory 1930-
1955. Studies in Linguistic Analysis (special vol-
ume of the Philological Society), pages 1?32.
Harris, Z.S. 1968. Mathematical structures of lan-
guage. Wiley.
Ide, N., T. Erjavec, and D. Tufis. 2002. Sense discrim-
ination with parallel corpora. In Proceedings of the
ACL Workshop on Sense Disambiguation: Recent
Successes and Future Directions.
Justeson, J. and S. Katz. 1995. Technical terminol-
ogy: some linguistic properties and an algorithm for
identification in text. Natural Language Engineer-
ing, 1:9?27.
Koehn, P., H. Hoang, A. Birch, C. Callison-Burch,
M.Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A.Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics.
36
Koehn, P. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In Proceedings of the MT
Summit, pages 79?86, Phuket, Thailand.
Lin, D., S. Zhao, L. Qin, and M. Zhou. 2003. Identify-
ing synonyms among distributionally similar words.
In Proceedings of the International Joint Confer-
ence on Artificial Intelligence (IJCAI).
Lin, D. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING/ACL.
McCray, A. and W. Hole. 1990. The scope and struc-
ture of the first version of the umls semantic net-
work. In Symposium on Computer Applications in
Primary Care (SCAMC-90), IEEE Computer Soci-
ety, pages 126?130, , Washington DC, IEEE Com-
puter Society. 126-130.
Och, F.J. 2003. GIZA++: Training of sta-
tistical translation models. Available from
http://www.isi.edu/?och/GIZA++.html.
Resnik, P. and D. Yarowsky. 1997. A perspective on
word sense disambiguation methods and their eval-
uation. In Proceedings of ACL SIGLEX Workshop
on Tagging Text with Lexical Semantics: Why, what,
and how?
Resnik, P. 1993. Selection and information. Unpub-
lished doctoral thesis, University of Pennsylvania.
Schmid, Helmut. 1994. Probabilistic part-of-
speech tagging using decision trees. In Pro-
ceedings of International Conference on New
Methods in Language Processing, pages 44?49,
Manchester, UK, September. http://www.ims.uni-
stuttgart.de/?schmid/.
Schu?tze, H. 1992. Dimensions of meaning. In Pro-
ceedings of the ACM/IEEE conference on Super-
computing.
Shimota, M. and E. Sumita. 2002. Automatic para-
phrasing based on parallel corpus for normalization.
In Proceedings of the International Conference on
Language Resources and Evaluation (LREC).
Tiedemann, Jo?rg. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and
interfaces. In Nicolov, N., K. Bontcheva, G. An-
gelova, and R. Mitkov, editors, Recent Advances
in Natural Language Processing, volume V, pages
237?248, Borovets, Bulgaria. John Benjamins, Am-
sterdam/Philadelphia.
van der Plas, L. and J. Tiedemann. 2006. Finding syn-
onyms using automatic word alignment and mea-
sures of distributional similarity. In Proceedings of
COLING/ACL.
van der Plas. 2008. Automatic lexico-semantic acqui-
sition for question answering. Groningen disserta-
tions in linguistics.
van Noord, G. 2006. At last parsing is now oper-
ational. In Actes de la 13eme Conference sur le
Traitement Automatique des Langues Naturelles.
Varga, D., L. Nmeth, P. Halcsy, A. Kornai, V. Trn, and
V. Nagy. 2005. Parallel corpora for medium density
languages. In Proceedings of RANLP 2005, pages
590?596.
Vossen, P. 1998. EuroWordNet a multilingual
database with lexical semantic networks.
Wilks, Y., D. Fass, Ch. M. Guo, J. E. McDonald,
and B. M. Slator T. Plate. 1993. Providing ma-
chine tractable dictionary tools. Machine Transla-
tion, 5(2):99?154.
Wu, H. and M. Zhou. 2003. Optimizing synonym ex-
traction using monolingual and bilingual resources.
In Proceedings of the International Workshop on
Paraphrasing: Paraphrase Acquisition and Appli-
cations (IWP).
37
