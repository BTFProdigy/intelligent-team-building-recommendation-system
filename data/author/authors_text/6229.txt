Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 296?305,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
How well does active learning actually work? Time-based evaluation of
cost-reduction strategies for language documentation.
Jason Baldridge
Department of Linguistics
The University of Texas at Austin
jbaldrid@mail.utexas.edu
Alexis Palmer
Computational Linguistics
Saarland University
apalmer@coli.uni-sb.de
Abstract
Machine involvement has the potential to
speed up language documentation. We as-
sess this potential with timed annotation
experiments that consider annotator exper-
tise, example selection methods, and sug-
gestions from a machine classifier. We
find that better example selection and la-
bel suggestions improve efficiency, but ef-
fectiveness depends strongly on annota-
tor expertise. Our expert performed best
with uncertainty selection, but gained lit-
tle from suggestions. Our non-expert per-
formed best with random selection and
suggestions. The results underscore the
importance both of measuring annotation
cost reductions with respect to time and of
the need for cost-sensitive learning meth-
ods that adapt to annotators.
1 Introduction
Data annotated with linguistically interesting la-
bels is used in a wide variety of contexts. Com-
putational linguists generally use annotated data
as training and evaluation material for natural lan-
guage processing systems; corpus linguists use it
to test hypotheses about language; documentary
linguists create interlinear glossed texts to pre-
serve examples of endangered languages and hy-
potheses about the grammars of those languages.
Regardless of the context, creating annotated data
is costly in terms of time and/or money. Since both
time and money are undeniably in limited supply,
there is a widely shared desire to reduce this cost.
Reducing cost involves strategies that do more
with fewer human-annotated labels and/or reduce
the per-label cost. An example of the former is ac-
tive learning, which focuses annotation effort on
data points selected by the learner(s) for their ex-
pected utility in developing a more accurate model
(Settles, 2009). Examples of the latter include
providing suggestions from a machine labeler and
using extremely cheap human labelers, e.g. with
the Amazon Mechanical Turk (Snow et al, 2008).
Different techniques may be more or less appli-
cable depending on the language being annotated,
the kind of labels which are desired (tags, syntac-
tic structures, etc.), and the desired use of the an-
notated data (e.g., for training models, testing lin-
guistic hypotheses, or preserving a language).
This paper discusses experiments that measure
the effectiveness of machine-aided annotation for
language documentation using both active learn-
ing simulation experiments and annotation ex-
periments which involve actual documentary lin-
guists interacting with machine example selec-
tion and label suggestion. Specifically, we deal
with the task of labeling morphemes of the Mayan
language Uspanteko with fine-grained parts-of-
speech. We also run active learning simulation
experiments for part-of-speech tagging for Dan-
ish, Dutch, English, Swedish, and Uspanteko to
show the validity of our models and methods in a
standard setting. For Uspanteko, we provide re-
sults from annotation experiments in which anno-
tation cost is measured in terms of the actual an-
notation time required while varying three factors:
(1) example selection, (2) machine label sugges-
tions, and (3) annotator expertise.
Our findings indicate that there is consider-
able promise for reducing the cost of produc-
ing IGT, but they also demonstrate considerable
variation due to the interaction of these factors.
This suggests different prescriptions for appropri-
ate strategies in different contexts. Most clearly,
the worst performing strategy?by far?is that
used in nearly all documentary work: sequential
annotation without automation. Also, our expert
annotator did best with examples picked by un-
certainty selection, while our non-expert did best
with random selection aided by machine label sug-
296
Language #words-tr #words-dev #tags #sents-tr #sents-dev Avg.sent Avg.tr.sent Avg.dev.sent
Danish 62825 31561 10 3570 1618 18.18 17.60 19.50
Dutch 129586 65483 13 9365 3982 14.61 13.84 16.44
English 167593 131768 45 6945 5527 24.00 24.13 23.84
Swedish 127684 63783 41 7326 3714 17.34 17.43 17.17
Uspanteko 43473 19906 69 7423 3288 5.92 5.86 6.05
Table 1: Corpora: number of words and sentences, number of possible tags, and average sentence length.
gestions. This difference confirms the importance
of cost-sensitive active learning strategies that are
not just learner-guided, but also take into account
modeling of the annotators (Settles et al, 2008;
Haertel et al, 2008; Vijayanarasimhan and Grau-
man, 2008). Finally, we confirm the importance
of using actual annotation time to measure annota-
tion cost: a unit-cost assumption?even at a fine-
grained level?can dramatically misrepresent the
actual effectiveness of different strategies.
2 Task and data
Annotation task: language documentation
The amount of money spent on obtaining human
annotations is an extremely important concern in
much language annotation. However, there is a
further urgency for annotation in the case of lan-
guage documentation: languages are dying at the
rate of two each month. By the end of this cen-
tury, half of the approximately 6000 extant spoken
languages will cease to be transmitted effectively
from one generation of speakers to the next (Crys-
tal, 2000). Recorded and transcribed texts anno-
tated with detailed linguistic information create an
important multi-faceted record of these languages,
but there are few trained linguists with adequate
time and appropriate levels of funding relative to
the size of the problem. Annotation cost?in both
time and money?is thus keenly felt in the work
of documenting and describing endangered lan-
guages. Active learning and automated label sug-
gestions could help deal with this language docu-
mentation bottleneck.
We focus on one stage of language documen-
tation, the production of interlinear glossed text
(IGT), a standard form of annotation that in-
volves both morphological and grammatical anal-
ysis. IGT is generally created following transcrip-
tion and translation of recorded speech, with the
annotations often being provided by trained anno-
tators with varying levels of expertise. The result
is generally a small amount of IGT annotated data
and a greater amount of unannotated data.
Data We use a collection of 32 interlinear
glossed texts (IGT) in the Mayan language Uspan-
teko. This corpus was cleaned up and adapted by
Palmer et al (2009) from an original collection of
67 texts that were collected, transcribed, translated
and annotated by the OKMA language documen-
tation project (Pixabaj et al, 2007).
Two core tasks in creating IGT are morpholog-
ical analysis and tagging morphemes with their
glosses (labels indicating part-of-speech and/or
grammatical function). We deal with the latter task
and assume texts are morphologically segmented.
Standard four-line IGT has morphemes on one line
and their glosses on the next. The gloss line in-
cludes labels for grammatical morphemes (e.g. PL
or COM) and translations of stems (e.g. hablar or
idioma). The following is an Uspanteko example:
(1) TEXT: Kita? tinch?ab?ej laj inyolj iin
MORPH:
GLOSS:
POS:
kita?
NEG
PART
t-in-ch?abe-j
INC-E1S-hablar-SC
TAM-PERS-VT-SUF
laj
PREP
PREP
in-yol-j
A1S-idioma-SC
PERS-S-SUF
iin
yo
PRON
TRANS: ?No le hablo en mi idioma.?
We use a single layer that is a combination of the
GLOSS and POS layers (Palmer et al, 2009). For
(1), the morphemes and labels for our task are:
(2) kita?
NEG
t-
INC
in-
E1S
ch?abe
VT
-j
SC
laj
PREP
in-
A1S
yol
S
-j
SC
iin
PRON
We also consider POS-tagging for Danish,
Dutch, English, and Swedish; the English is from
sections 00-05 (as training set) and 19-21 (as de-
velopment set) of the Penn Treebank (Marcus et
al., 1993), and the other languages are from the
CoNLL-X dependency parsing shared task (Buch-
holz and Marsi, 2006).
1
We split the original train-
ing data into training and development sets. Ta-
ble 1 shows the number of words and sentences
in each split of each dataset, as well as the num-
ber of possible labels and the average sentence
length. The Uspanteko data is counted in mor-
phemes rather than words; also, the Uspanteko
texts are divided at the clause rather than sentence
level. This gives the corpus a much lower average
clause length than the other languages (Table 1).
1
The subset of the Penn Treebank was chosen to be of
comparable size to the CoNLL datasets.
297
3 Model and methods
Classification model. We use a standard maxi-
mum entropy classifier for tagging Danish, Dutch,
English, and Swedish words with POS-tags and
tagging Uspanteko morphemes with Gloss/POS
tags. The label for a word/morpheme is pre-
dicted based on the word/morpheme itself plus
a window of two units before and after. Stan-
dard part-of-speech tagging features (Ratnaparkhi,
1998; Curran and Clark, 2003) are extracted from
the morpheme to help with predicting labels for
previously unseen morphemes. This is a strong
but standard model; better, more complex models
could be used, but the gains are likely to be small.
Thus, we opted for simplicity in our model so as to
focus more on the interaction between the annota-
tor and different levels of machine involvement.
The accuracy of the tagger on the datasets when
trained on all available training material is given
in the following table, along with accuracy of a
unigram model (learned from the training set and
constrained by a tag dictionary for known words).
Unigram Model
Danish 91.62% 95.58%
Dutch 90.92% 93.57%
English 87.87% 93.25%
Swedish 84.91% 87.74%
Uspanteko 77.84% 79.39%
Sample selection. We consider three sample
selection methods: sequential, random, and
uncertainty. Sequential selection is important
to consider as it is the default in documentary
projects. It is sub-optimal for corpora with con-
tiguous sub-domains, since it necessitates working
through many similar examples before getting to
possibly more informative examples. Random se-
lection is a model-free method that avoids the sub-
domain trap by sampling freely from the entire
corpus. It generally works better than sequential
selection and provides a strong baseline against
which to compare learner-guided selection.
Uncertainty selection (Cohn et al, 1995) iden-
tifies examples the model is least confident about.
We measure uncertainty as the entropy of the la-
bel distribution predicted by the maximum en-
tropy model for each example. Uncertainty for
a clause is calculated as the average entropy per
morpheme; clauses with the highest average en-
tropy are selected for labeling.
A recent development in active learning is cost-
sensitive selection that is guided not only by the
learner but also by the expected cost of labeling an
example based on its likely complexity and/or the
reliability of the annotator. Settles et al (2008)
provide empirical validation for cost-related in-
tuitions; for example, that cost of annotation is
static neither per example nor per annotator. Also,
they show that taking annotation cost into account
can improve active learning effectiveness, but that
learning to predict annotation cost is not yet well-
understood. A cost-sensitive Return on Investment
heuristic is developed in Haertel et al (2008) and
tested in a simulated POS-tagging context. Our
experiments do not employ cost-sensitive selec-
tion, but our results?from live (non-simulated)
active learning experiments of real-world scale?
empirically support the need to consider cost-
sensitive selection if better cost reductions are to
be achieved.
Annotation setup. We compare results from
two annotators with different levels of exposure to
Uspanteko. Both are documentary linguists with
extensive field experience. Our expert annota-
tor is a native speaker of K?ichee?, a closely re-
lated Mayan language, and has worked extensively
on Uspanteko. Our non-expert annotator had no
prior experience with Uspanteko and only limited
exposure to Mayan languages. During annotation,
he used an Uspanteko-Spanish dictionary.
For each selection method, we consider two
conditions for providing classifier labels: a do-
suggest (ds) condition where the labels predicted
by the machine learner are shown to the annotator,
and a no-suggest (ns) condition where the annota-
tor does not see the predictions. With ds, the anno-
tator is shown the most probable label and a ranked
list of all labels assigned a probability greater than
half that of the best label. For ns, the annotator
sees a frequency-ranked list of labels previously
seen in training data for the given morpheme.
Annotators improve as they see more examples.
To minimize the impact of this learning process,
annotation is done in rounds. Each round con-
sists of sixty clauses?six batches of ten each for
the six experimental cases. The annotator is free
to break between batches. Following annotation,
the newly-labeled clauses are added to the train-
ing data, and a new model is trained and evaluated.
Both annotators completed fifty-six rounds of an-
notation. See Palmer et al (2009) for more details
on the annotation setup.
298
Measuring annotation cost. Active learning
studies usually simulate annotation and use a unit
cost assumption that each word, sentence, con-
stituent, document, etc. takes the same time to an-
notate. This is often the only option since corpora
typically do not retain annotation time, but it is
likely to exaggerate the annotation cost reductions
achieved. This is exacerbated with active learn-
ing: the informative examples it seeks to find are
typically harder to annotate (Hachey et al, 2005).
Baldridge and Osborne (2008) correlate a unit
cost in terms of discriminants (decisions made
by annotators about valid parses) to annotation
time. This is a better approximation than unit costs
where such a relationship cannot be established.
However, it is based on a static measurement of
annotation time, and clearly the time taken to an-
notate an example is not a function of the example
alone. Annotation time is actually dynamic in that
it is dependent on how many and what kinds of
examples have already been annotated. An ?infor-
mative? example is likely to take longer to anno-
tate if selected early than it would after the anno-
tator has seen many other examples.
Thus, it is important to measure annotation time
embedded in the context of a particular annota-
tion experiment with the sample selection/labeling
strategies of interest. In our annotation experi-
ments, we measure the exact time taken to anno-
tate each example by each annotator and use this
as the cost metric, inspired by Ngai and Yarowsky
(2000). In the simulation studies, as we are un-
able to measure time, we measure cost by sen-
tence/clause and word/morpheme.
Learning curve comparison. We are interested
in comparative evaluation of many different exper-
imental settings, across which we vary selection
methods, use of label suggestions, and annotators.
To achieve this, it is useful to have a summary
value for comparing the results from two individ-
ual experiments. One such measure is the percent-
age error reduction (PER), measured over a dis-
crete set of points on the first 20% of the points on
the learning curve (Melville and Mooney, 2004).
2
We use a new related measure, which we call
the overall percentage error reduction (OPER),
that uses the entire area under the curves given by
2
This is justified in standard conditions, sampling from a
finite corpus: active learning runs out of interesting examples
after considering a fraction of the data, so the curve is artifi-
cially pulled down by the remaining, boring examples.
fitted nonlinear regression models rather than av-
eraging over a subset of data points. Specifically,
we fit a modified Michaelis-Menton model:
f(cost, (K,V
m
, A)) =
V
m
(A + cost)
K + cost
The (original) parameters V
m
and K respectively
correspond to the horizontal asymptote and the
cost where accuracy is halfway between 0 and V
m
.
The additional parameter A allows for a better fit
to our data by allowing for less sharp elbows and
letting cost be zero. Model parameters were de-
termined with nls in R (Ritz and Streibig, 2008).
With the fitted regression models, it is straight-
forward to calculate the area under the curve be-
tween a start cost c
i
and end cost c
j
by taking the
integral from c
i
to c
j
. The overall accuracy for
the experiment is given by dividing that area by
100 ? (c
j
? c
i
). Call this the overall curve accu-
racy (OCA). Then, for experiment A compared to
experiment B, OPER(A,B) =
OCA
A
?OCA
B
100?OCA
B
. For
the simulation experiments we calculate OPER for
only the first 20% of cost units, like Melville and
Mooney. For the annotation experiments, we cal-
culate it for the minimum amount of time spent on
any of the experiments (which ended up using less
than 10% of all available morphemes).
4 Simulation experiments
We verify that our tagger and dataset behave as
expected in standard active learning experiments
by running simulations on the Uspanteko data set,
and on POS-tagging for Danish, Dutch, English,
and Swedish. Here, we vary only the selection
method: sequential, random, or uncertainty.
For each language, we randomly select a seed
set of 10 labeled sentences. The number of exam-
ples selected to be labeled in each round begins
at 10 and doubles after every 20 rounds. For rand
and unc, each batch of examples is selected from a
pool (size of 1000) that is itself randomly selected
from the entire set of remaining unlabeled exam-
ples. rand and unc experiments for each language
are replicated 5 times; splines and regressions are
computed over all runs for each condition.
Figure 1 gives learning curves for the Uspan-
teko simulations, with cost measured in terms of
(a) clauses and (b) morphemes. Both graphs show
the usual behavior found in active learning exper-
iments. rand and unc both rise more quickly than
seq, and unc is well above rand. The relation-
ship between the methods is the same regardless
299
0 2000 4000 6000
50
55
60
65
70
75
80
Number of clauses selected
Accu
racy
 on a
ll tok
ens
UncertaintyRandomSequential
0 10000 20000 30000 40000
50
55
60
65
70
75
80
Number of morphemes selected
Accu
racy
 on a
ll tok
ens
UncertaintyRandomSequential
(a) (b)
Figure 1: Learning curves for simulations; (a) clause cost and (b) morphemes cost. The dashed vertical
lines indicate (a) #clauses=1485 and (b) #morphemes=8695 (to compare OPER values).
rand
seq
unc
seq
unc
rand
Uspanteko-Clauses 5.86 13.27 7.86
Uspanteko-Morphs 7.47 11.68 4.55
Table 2: OPER values for Uspanteko simulations,
comparing clause and morpheme cost.
A
B
indi-
cates we compute OPER(A,B).
of the cost metric, but the relative differences in
cost-savings are not, which we see when we look
at OPER values.
The dashed vertical lines in the two graphs cor-
respond to the 20% mark used to calculate OPER
values, which are given in Table 2. Most impor-
tantly, note the much larger OPER for unc over
rand with clause cost (7.86 vs 4.55). Also note
that OPER(rand,seq) is lower with clause cost?
this indicates that the beginning portions of the
corpus contain longer sentences with more mor-
phemes, an accident which overstates how well
seq would likely work in general.
Since rand is unbiased with respect to pick-
ing longer sentences, the large increase of
OPER(unc,rand) from 4.55 to 7.86 is a clear in-
dication of the well-known?but not always at-
tended to?tendency of uncertainty sampling to
select longer sentences. Consequently, one should
at least use sub-sentence cost in order not to over-
state the gains from active learning. The annota-
tion experiments in the next section take this word
rand
seq
unc
seq
unc
rand
Danish 4.58 6.95 2.48
Dutch 21.95 23.68 2.20
English 6.55 8.00 1.56
Swedish 9.56 9.29 -0.30
Uspanteko 7.47 11.68 4.55
Table 3: OPER values for morpheme cost for sim-
ulations.
A
B
indicates we compute OPER(A,B).
of caution one step further: even sub-sentence cost
(morpheme cost, in our setting) can overestimate
gains since the morphemes selected are actually
harder to annotate and thus take more time.
Table 3 gives overall percentage error reduc-
tions (OPER) between different selection methods
based on word/morpheme cost, for each language.
For all languages, rand and unc are better than
seq. Only in the case of Swedish is there no ben-
efit from unc over rand. For Dutch, the large
gains over seq for both rand and unc accurately
reflect the heterogeneity of the underlying Alpino
corpus.
3
Most importantly, for Uspanteko, there
are large reductions from unc to rand to seq, mir-
roring the clear trends in Figure 1b.
These simulations have an unrealistic ?perfect?
annotator, the corpus. Next, we discuss results
with real annotators?who may be fallible or may
(reasonably) beg to differ with the corpus analysis.
3
http://www.let.rug.nl/vannoord/trees/
300
0 1000 2000 3000 4000
20
30
40
50
60
70
Morphemes annotated
Accu
racy
 on a
ll tok
ens
Non?Expert, No Suggest, SequentialNon?Expert, Suggest, RandomExpert, No Suggest, SequentialExpert, No Suggest, Uncertainty
0 5000 10000 15000 20000 25000
20
30
40
50
60
70
Cumulative annotation time
Accu
racy
 on a
ll tok
ens
Non?Expert, No Suggest, SequentialNon?Expert, Suggest, RandomExpert, No Suggest, SequentialExpert, No Suggest, Uncertainty
(a) (b)
Figure 2: A sample of the learning curves with (a) morpheme cost and (b) time cost. Morpheme cost
ranks strategies for a given annotator similarly to time cost, but it gives dramatically different results
from time cost when used to compare different annotators.
5 Annotation experiments
With two annotators (expert, non-expert), three
selection methods (seq, rand, unc), and two ma-
chine labeling settings (ns, ds), we obtain 12 dif-
ferent experiments. Each experiment measures ac-
curacy in terms of all words and unknown words
and cost in terms of clauses, morphemes and time;
this produces six views on every experiment. In
this paper we focus on one view: accuracy over all
words with time-based evaluation of cost.
As with the simulations, clause cost in the an-
notation experiments overestimates the cost reduc-
tions. For morpheme cost, the annotation experi-
ments show that (a) it also overstates cost reduc-
tions compared to time, and (b) it can mis-state
relative effectiveness when comparing annotators.
The big picture. Figure 2 shows curves for four
experiments: seq-ns for both annotators
4
and the
most effective overall condition for each annota-
tor. Figure 2a uses morpheme cost evaluation; on
that metric, both annotators appear to be about
equally effective with seq-ns and much more ef-
fective with machine involvement (unc or ds) than
without. Additionally, the non-expert?s rand-ds
appears to beat the expert?s unc-ns. However, the
time cost evaluation in Figure 2b tells a dramat-
ically different story. Each annotator?s machine-
4
Recall that sequential annotation is the default mode for
producing IGT, so this strategy is of particular interest.
involved experiment is much better than their seq-
ns, but now the expert?s best is clearly better than
the non-expert?s. We see this as clear evidence for
the need for cost-sensitive learning over vanilla ac-
tive learning (as we do here).
5
The non-expert with rand-ds caught up to and
surpassed the unaided expert in about six hours
total annotation time, and he caught up to her
unc-ns curve after 35 hours. This is encourag-
ing since often language documentation projects
have participants with a wide range of expertise
levels, and these results suggest that assistance
from machine learning, if done properly, may in-
crease the effectiveness of participants with less
language-specific expertise. We are also encour-
aged, with respect to the effectiveness of active
learning, that the expert?s best performance is ob-
tained with uncertainty-based selection.
Within annotator comparisons. Figure 3
shows both actual measurements and the fitted
nonlinear regression curves used to compute
OPER. Figure 3a, the expert without suggestions,
exhibits typical active learning behavior similar to
that seen in the simulation experiments. Figure 3b,
5
It is also clear to see that, unsurprisingly, the expert spent
much less time to complete the 56 rounds than the non-expert.
In general, the expert annotator was much quicker, particu-
larly in early rounds, averaging 4.1 seconds per morpheme
annotated against the non-expert?s 8.0 second average. See
Palmer et al (2009) for more details.
301
ll
ll
lll
llll
lllllllll
lllllllllll
lllllllllllllllll
llllll
0 5000 10000 15000 20000 25000 30000
30
40
50
60
70
Cumulative annotation time
Accu
racy
 on a
ll tok
ens
l Expert, No Suggest, UncertaintyExpert, No Suggest, RandomExpert, No Suggest, Sequential
l
l
l
l
llll
ll
lll
ll llllll
l ll
lllllllllll
lllllllllllllll
lllllll
0 5000 10000 15000 20000 25000 30000
30
40
50
60
70
Cumulative annotation time
Accu
racy
 on a
ll tok
ens
l Non?expert, Suggest, UncertaintyNon?expert, Suggest, RandomNon?expert, Suggest, SequentialNon?expert, No Suggest, Sequential
(a) (b)
Figure 3: Sample measurements and fitted nonlinear regression curves for (a) the expert and (b) the
non-expert. Note that the scale is consistent for comparability. The dashed vertical lines indicate 12,500
seconds (about 35 hours), which is the upper limit used in computing OPER values for Table 4.
the non-expert with suggestions, shows that in the
ds conditions the non-expert was less effective
with unc. This is not unexpected: uncertainty
selects harder examples that will either take
longer to annotate or are easier to get wrong,
especially if the annotator trusts the classifier and
especially on examples the classifier is uncertain
about. Nonetheless, in all ds cases, the non-expert
performs better than with seq-ns.
OPER. Table 4 provides OPER values from
time 0 to 12,500 seconds (about 35 hours), the
minimum amount of annotation time logged in any
one of the twelve experiments.
6
The table mixes
three types of comparison: (1) the boxed values
on the diagonal give OPER for the expert versus
the non-expert given the same selection and sug-
gestion conditions; (2) the upper (right) triangle
gives OPER for the expert versus herself for dif-
ferent conditions; and (3) the lower (left) trian-
gle is the non-expert versus himself. For exam-
ple: (1) the expert obtained an 11.52 OPER versus
the non-expert when both used rand-ns; (2) the
expert obtained a 10.52 OPER by using rand-ds
rather than seq-ns; and (3) the non-expert obtained
a 5.93 OPER over rand-ns by using rand-ds.
A number of patterns emerge. Quite unsurpris-
6
Stopping at 12,500 seconds ensures a fair comparison,
for example, between the expert and the non-expert because
it requires no extrapolation of the expert?s performance.
X
X
X
X
X
X
non-exp
exp
seq-ns rand-ns unc-ns seq-ds rand-ds unc-ds
seq-ns 15.99 8.85 14.17 6.34 10.52 14.50
rand-ns 13.46 11.52 5.83 -2.76 1.83 6.20
unc-ns 19.20 6.63 10.76 -9.12 -4.25 0.39
seq-ds 10.24 -3.72 -11.09 12.34 4.46 8.72
rand-ds 18.59 5.93 -0.76 9.30 7.67 4.45
unc-ds 11.19 -2.62 -9.91 1.06 -9.09 19.13
Table 4: Overall percentage error reduction
(OPER) comparisons, with timing cost. See ex-
planation of table in the OPER subsection.
ingly, the values on the diagonal show that the ex-
pert is more effective than the non-expert in all
conditions. Also, every other condition is more ef-
fective than seq-ns for both annotators (first row
for the expert, first column for the non-expert).
unc-ns and rand-ds are particularly effective for
the non-expert, giving OPERs of 19.20 and 18.59
over seq-ns, respectively. These reductions, big-
ger than the expert?s reductions of 14.17 and 10.52
for the same conditions, considerably reduce the
large gap in seq-ns effectiveness between the two
annotators (see Figure 2b).
The expert actually gains very little from ds for
both rand and unc: adding suggestions gave OP-
ERs of just 1.83 and .39, respectively. In con-
trast, the non-expert obtains an improvement of
5.93 OPER when suggestions are used with rand,
302
but performs worse when used with unc (-9.91
OPER). Even more striking: the non-expert?s
unc-ds is worse than rand-ns (-2.62 OPER), a
completely model-free setting. These variations
demonstrate the importance of modeling annotator
fallibility and sensitivity to cost, as well as char-
acteristics of the annotation task itself, if learner-
guided selection and suggestion are to be used
(Donmez and Carbonell, 2008; Arora et al, 2009).
Annotator accuracy. Another factor which
must be considered when annotation is done by
human annotators (rather than being simulated)
is the accuracy of the humans? labels. Table 5
shows the overall accuracy of the annotators? la-
bels for each condition (after 56 rounds) as mea-
sured against the original OKMA annotations.
Unsurprisingly, unc selection picks examples that
are more difficult to annotate: accuracy for both
annotators suffers in both unc-ns and unc-ds.
It may seem surprising that the non-expert?s ac-
curacies are generally higher than the expert?s.
The main reason for this is that the non-expert
took nearly twice as long to annotate his examples,
so each one was done with more care. However,
this difference also highlights challenges that arise
when we bring active learning into non-simulated
annotation contexts. The typical assumption is
that gold standard labeled data represents a true,
fixed target, against which annotator or machine-
predicted labels should be measured. In language
documentation, though, the analysis of the lan-
guage is continually evolving, and analysis and
annotation each inform the other. In fact, the ex-
pert recognized (in the morphological segmenta-
tion) several linguistic phenomena for which the
analysis has changed since the original OKMA an-
notations were done. As she changed her analy-
ses, her labels diverged from those of the original
corpus?another reason for her ?lower? accuracy.
This is to say that the ground truth of the current
OKMA annotations we had to work with can be
viewed as one (valid) stage in the iterative reanal-
ysis process that language documentation is.
Error analysis. Preliminary analysis of ?errors?
made by the annotators supports the idea that
the results seen in Table 5 are heavily influenced
by changes in the expert?s analysis of the lan-
guage. Some duplicate clause annotation oc-
curred for each annotator, because each of the
twelve annotator-selection-suggestion conditions
expert non-expert
seq-ns 73.17% 75.09%
rand-ns 69.90% 74.37%
unc-ns 61.23% 60.04%
seq-ds 67.48% 73.13%
rand-ds 68.34% 73.03%
unc-ds 59.79% 60.27%
Table 5: Overall accuracy of annotators? labels,
measured against OKMA annotations.
drew from the same global set of unlabeled ex-
amples. This duplication allows us to measure
the consistency of each annotator on labeling such
duplicate clauses. Table 6 shows the percentage
of morphemes labeled consistently by each anno-
tator. Numbers for the expert appear in the top
(right) triangle, and for the non-expert in the bot-
tom (left) triangle. Overall intra-annotator consis-
tency is much higher for the expert (88.38%) than
for the non-expert (81.64%), suggesting that the
expert maintained a more consistent mental model
of the language, but one which disagrees in some
areas with the original annotations.
Another key error source comes from differ-
ences in use of one individual label: the annota-
tors could assign a label that does not appear in
the original corpus. This is yet another issue that
does not?in fact, cannot?arise in simulated ac-
tive learning. The label ESP was introduced for la-
beling Spanish loans or insertions (such as the dis-
course marker entonces) which do not have a clear
function in Uspanteko grammar. Such tokens are
inconsistently labeled in the original corpus, usu-
ally with catch-all categories like particle or ad-
verb. The annotators felt that the best analysis was
to mark the tokens as of Spanish origin. The expert
annotator used the ESP label for 2086 of 24129 to-
kens (8.65%) versus 221 of 22819 tokens (0.97%)
for the non-expert. Any such token labeled with
ESP is scored as incorrect when compared to the
OKMA standard, so this label alone accounts for
more than 7% of the expert annotator?s total error.
Finally, Table 7 presents inter-annotator agree-
ment measured as percent agreement on mor-
phemes in clauses labeled by both annotators.
Note that in general agreement seems to be low-
est for clauses duplicated in unc conditions, sup-
porting the expected result that uncertainty-based
selection does indeed select clauses that are more
difficult for human annotators to label.
303
PP
P
P
P
P
non
exp
seq-ns rand-ns unc-ns seq-ds rand-ds unc-ds
seq-ns ? 95.00% (41) 87.10% (56) 92.39% (60) 91.02% (28) 88.83% (51)
rand-ns 90.11% (49) ? 90.91% (57) 87.57% (35) 90.94% (50) 89.53% (57)
unc-ns 80.80% (44) 81.68% (54) ? 81.35% (41) 89.10% (40) 87.82% (332)
seq-ds 90.00% (54) 87.94% (44) 77.97% (48) ? 86.13% (42) 82.14% (42)
rand-ds 90.15% (52) 86.64% (45) 79.46% (62) 81.43% (44) ? 87.06% (49)
unc-ds 84.15% (47) 78.55% (52) 77.68% (328) 78.81% (35) 77.95% (60) ?
Table 6: Annotation consistency, expert and non-expert, (number of duplicate clauses, of 560 possible)
P
P
P
P
P
P
non
exp
seq-ns rand-ns unc-ns seq-ds rand-ds unc-ds
seq-ns 69.91% (523) 70.82% (42) 62.42% (48) 72.35% (54) 74.25% (28) 67.82% (47)
rand-ns 71.32% (48) 83.94% (39) 66.56% (47) 66.15% (43) 73.75% (42) 67.55% (52)
unc-ns 66.31% (48) 67.87% (53) 62.31% (301) 58.87% (51) 73.31% (40) 61.10% (298)
seq-ds 73.35% (60) 75.56% (34) 56.39% (37) 60.02% (540) 66.00% (44) 61.01% (36)
rand-ds 68.67% (50) 76.40% (63) 66.67% (58) 65.88% (47) 76.33% (42) 66.99% (64)
unc-ds 65.41% (50) 67.98% (55) 60.43% (263) 58.13% (38) 70.74% (57) 60.40% (275)
Table 7: IAA: expert v. non-expert, percentage of morphemes in agreement, (number of duplicate
clauses, of 560 possible)
6 Conclusion
Through actual annotation experiments that con-
trol for several factors, we have evaluated the po-
tential of incorporating active learning and label
suggestions to speed up morpheme glossing in a
realistic language documentation context. Some
configurations of learner-guided example selec-
tion and machine label suggestions perform far
better than the standard strategy of sequential se-
lection without suggestions. However, the effec-
tiveness of any given strategy depends on annota-
tor expertise. The impact of differences between
annotators directly bears on the point made by
Donmez and Carbonell (2008) that if cost reduc-
tions are to be reliably obtained with active learn-
ing techniques, annotators? fallibility, unreliabil-
ity, and sensitivity to cost must be modeled.
Our results suggest some possible prescriptions
for tuning techniques according to annotator ex-
pertise. However, even if we can estimate a rela-
tive level of expertise, following such broad pre-
scriptions is unlikely to be more robust than an ap-
proach which adapts selection and suggestion to
the individual annotator, perhaps working within
an annotation group. Indeed, it seems that dealing
with variation in annotators/oracles may be more
important than devising better selection strategies.
The difference in performance due to expertise
suggests that using multiple annotators to check
relative annotation rate and accuracy of different
annotators could be a key ingredient in any actu-
ally deployed active learning system. This could
provide for better modeling of individual anno-
tators as part of an annotation group they can be
compared against, allowing the system, for exam-
ple, to throttle active selection if an annotator ap-
pears to be too slow or inaccurate.
Another major issue we highlight is the uncer-
tainty around the question of whether active learn-
ing works in practical applications. Respondents
to the survey of Tomanek and Olsson (2009) in-
dicated that this uncertainty?will active learn-
ing work? what methods or techniques will work
best??is one of the reasons active learning is not
widely used in actual annotation. In addition, cre-
ating the necessary software infrastructure to build
an active learning enabled annotation system?
a system which must interface robustly between
data, annotator, and machine classifier, yet still
be easy to use?is a substantial hurdle. It seems
unlikely that there will be much uptake until a)
consistent, large cost reductions can be shown in
actual annotation studies, and b) appropriate, tun-
able, widely-available software exists.
Acknowledgments
This work is funded by NSF grant BCS 06651988
?Reducing Annotation Effort in the Documenta-
tion of Languages using Machine Learning and
Active Learning.? Thanks to Eric Campbell, Ka-
trin Erk, Michel Jacobson, Taesun Moon, Telma
Kaan Pixabaj, and Elias Ponvert.
304
References
Shilpa Arora, Eric Nyberg, and Carolyn P. Ros?e. 2009.
Estimating annotation cost for active learning in a
multi-annotator environment. In Proceedings of the
NAACL HLT Workshop on Active Learning for Nat-
ural Language Processing, pages 18?26, Boulder,
CO.
Jason Baldridge and Miles Osborne. 2008. Active
learning and logarithmic opinion pools for HPSG
parse selection. Natural Language Engineering,
14(2):199?222.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning (CoNLL-X),
pages 149?164, New York City, June. Association
for Computational Linguistics.
David A. Cohn, Zoubin Ghahramani, and Michael I.
Jordan. 1995. Active learning with statistical mod-
els. In G. Tesauro, D. Touretzky, and T. Leen, ed-
itors, Advances in Neural Information Processing
Systems, volume 7, pages 705?712. The MIT Press.
David Crystal. 2000. Language Death. Cambridge
University Press, Cambridge.
James R. Curran and Stephen Clark. 2003. Investigat-
ing GIS and smoothing for maximum entropy tag-
gers. In Proceedings of the 10th Conference of the
European Association for Computational Linguis-
tics, pages 91?98.
Pinar Donmez and Jaime G. Carbonell. 2008. Proac-
tive learning: Cost-sensitive active learning with
multiple imperfect oracles. In Proceedings of
CIKM08, Napa Valley, CA.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In Proceedings of the 9th Confer-
ence on Computational Natural Language Learning,
Ann Arbor, MI.
Robbie A. Haertel, Kevin D. Seppi, Eric K. Ringger,
and James L. Carroll. 2008. Return on invest-
ment for active learning. In Proceedings of the NIPS
Workshop on Cost-Sensitive Learning.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional linguistics, 19:313?330.
Prem Melville and Raymond J. Mooney. 2004. Di-
verse ensembles for active learning. In Proceed-
ings of the 21st International Conference on Ma-
chine Learning, pages 584?591, Banff, Canada.
Grace Ngai and David Yarowsky. 2000. Rule writ-
ing or annotation: cost-efficient resource usage for
base noun phrase chunking. In Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics, pages 117?125, Hong Kong.
Alexis Palmer, Taesun Moon, and Jason Baldridge.
2009. Evaluating automation strategies in language
documentation. In Proceedings of the NAACL HLT
2009 Workshop on Active Learning for Natural Lan-
guage Processing, pages 36?44, Boulder, CO.
Telma Can Pixabaj, Miguel Angel Vicente M?endez,
Mar??a Vicente M?endez, and Oswaldo Ajcot Dami?an.
2007. Text Collections in Four Mayan Languages.
Archived in The Archive of the Indigenous Lan-
guages of Latin America.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania, Philadelphia, PA.
Christian Ritz and Jens Carl Streibig. 2008. Nonlinear
Regression with R. Springer.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Pro-
ceedings of the NIPS Workshop on Cost-Sensitive
Learning.
Burr Settles. 2009. Active learning literature survey.
Technical Report Computer Sciences Technical Re-
port 1648, University of Wisconsin-Madison.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP 2008,
pages 254?263.
Katrin Tomanek and Fredrik Olsson. 2009. A Web
Survey on the Use of Active learning to support an-
notation of text data. In Proceedings of the NAACL
HLT Workshop on Active Learning for Natural Lan-
guage Processing, pages 45?48, Boulder, CO.
Sudheendra Vijayanarasimhan and Kristen Grauman.
2008. Multi-level active prediction of useful im-
age annotations for recognition. In Proceedings of
NIPS08, Vancouver, Canada.
305
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 896?903,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Sequencing Model for Situation Entity Classification
Alexis Palmer, Elias Ponvert, Jason Baldridge, and Carlota Smith
Department of Linguistics
University of Texas at Austin
{alexispalmer,ponvert,jbaldrid,carlotasmith}@mail.utexas.edu
Abstract
Situation entities (SEs) are the events, states,
generic statements, and embedded facts and
propositions introduced to a discourse by
clauses of text. We report on the first data-
driven models for labeling clauses according
to the type of SE they introduce. SE classifi-
cation is important for discourse mode iden-
tification and for tracking the temporal pro-
gression of a discourse. We show that (a)
linguistically-motivated cooccurrence fea-
tures and grammatical relation information
from deep syntactic analysis improve clas-
sification accuracy and (b) using a sequenc-
ing model provides improvements over as-
signing labels based on the utterance alone.
We report on genre effects which support the
analysis of discourse modes having charac-
teristic distributions and sequences of SEs.
1 Introduction
Understanding discourse requires identifying the
participants in the discourse, the situations they par-
ticipate in, and the various relationships between and
among both participants and situations. Coreference
resolution, for example, is concerned with under-
standing the relationships between references to dis-
course participants. This paper addresses the prob-
lem of identifying and classifying references to situ-
ations expressed in written English texts.
Situation entities (SEs) are the events, states,
generic statements, and embedded facts and propo-
sitions which clauses introduce (Vendler, 1967;
Verkuyl, 1972; Dowty, 1979; Smith, 1991; Asher,
1993; Carlson and Pelletier, 1995). Consider the
text passage below, which introduces an event-type
entity in (1), a report-type entity in (2), and a state-
type entity in (3).
(1) Sony Corp. has heavily promoted the VideoWalkman
since the product?s introduction last summer ,
(2) but Bob Gerson , video editor of This Week in Con-
sumer Electronics , says
(3) Sony conceives of 8mm as a ?family of products ,
camcorders and VCR decks , ?
SE classification is a fundamental component in de-
termining the discourse mode of texts (Smith, 2003)
and, along with aspectual classification, for tempo-
ral interpretation (Moens and Steedman, 1988). It
may be useful for discourse relation projection and
discourse parsing.
Though situation entities are well-studied in lin-
guistics, they have received very little computational
treatment. This paper presents the first data-driven
models for SE classification. Our two main strate-
gies are (a) the use of linguistically-motivated fea-
tures and (b) the implementation of SE classification
as a sequencing task. Our results also provide empir-
ical support for the very notion of discourse modes,
as we see clear genre effects in SE classification.
We begin by discussing SEs in more detail. Sec-
tion 3 describes our two annotated data sets and pro-
vides examples of each SE type. Section 4 discusses
feature sets, and sections 5 and 6 present models,
experiments, and results.
896
2 Discourse modes and situation entities
In this section, we discuss some of the linguistic mo-
tivation for SE classification and the relation of SE
classification to discourse mode identification.
2.1 Situation entities
The categorization of SEs into aspectual classes is
motivated by patterns in their linguistic behavior.
We adopt an expanded version of a paradigm relat-
ing SEs to discourse mode (Smith, 2003) and char-
acterize SEs with four broad categories:
1. Eventualities. Events (E), particular states (S),
and reports (R). R is a sub-type of E for SEs
introduced by verbs of speech (e.g., say).
2. General statives. Generics (G) and generaliz-
ing sentences (GS). The former are utterances
predicated of a general class or kind rather than
of any specific individual. The latter are habit-
ual utterances that refer to ongoing actions or
properties predicated of specific individuals.
3. Abstract entities. Facts (F) and proposi-
tions (P).1
4. Speech-act types. Questions (Q) and impera-
tives (IMP).
Examples of each SE type are given in section 3.2.
There are a number of linguistic tests for iden-
tifying situation entities (Smith, 2003). The term
linguistic test refers to a rule which correlates an
SE type to particular linguistic forms. For exam-
ple, event-type verbs in simple present tense are a
linguistic correlate of GS-type SEs.
These linguistic tests vary in their precision and
different tests may predict different SE types for
the same clause. A rule-based implementation us-
ing them to classify SEs would require careful rule
ordering or mediation of rule conflicts. However,
since these rules are exactly the sort of information
extracted as features in data-driven classifiers, they
1In our system these two SE types are identified largely as
complements of factive and propositional verbs as discussed
in Peterson (1997). Fact and propositional complements have
some linguistic as well as some notional differences. Facts may
have causal effects, and facts are in the world. Neither of these
is true for propositions. In addition, the two have somewhat
different semantic consequences of a presuppositional nature.
can be cleanly integrated by assigning them empiri-
cally determined weights. We use maximum entropy
models (Berger et al, 1996), which are particularly
well-suited for tasks (like ours) with many overlap-
ping features, to harness these linguistic insights by
using features in our models which encode, directly
or indirectly, the linguistic correlates to SE types.
The features are described in detail in section 4.
2.2 Basic and derived situation types
Situation entities each have a basic situation type,
determined by the verb plus its arguments, the verb
constellation. The verb itself plays a key role in de-
termining basic situation type but it is not the only
factor. Changes in the arguments or tense of the verb
sometimes change the basic situation types:
(4) Mickey painted the house. (E)
(5) Mickey paints houses. (GS)
If SE type could be determined solely by the verb
constellation, automatic classification of SEs would
be a relatively straightforward task. However, other
parts of the clause often override the basic situation
type, resulting in aspectual coercion and a derived
situation type. For example, a modal adverb can
trigger aspectual coercion:
(6) Mickey probably paints houses. (P)
Serious challenges for SE classification arise from
the aspectual ambiguity and flexibility of many
predicates as well as from aspectual coercion.
2.3 Discourse modes
Much of the motivation of SE classification is
toward the broader goal of identifying discourse
modes, which provide a linguistic characterization
of textual passages according to the situation enti-
ties introduced. They correspond to intuitions as to
the rhetorical or semantic character of a text. Pas-
sages of written text can be classified into modes
of discourse ? Narrative, Description, Argument, In-
formation, and Report ? by examining concrete lin-
guistic cues in the text (Smith, 2003). These cues
are of two forms: the distribution of situation entity
types and the mode of progression (either temporal
or metaphorical) through the text.
897
For example, the Narration and Report modes
both contain mainly events and temporally bounded
states; they differ in their principles of temporal pro-
gression. Report passages progress with respect to
(deictic) speech time, whereas Narrative passages
progress with respect to (anaphoric) reference time.
Passages in the Description mode are predominantly
stative, and Argument mode passages tend to be
characterized by propositions and Information mode
passages by facts and states.
3 Data
This section describes the data sets used in the ex-
periments, the process for creating annotated train-
ing data, and preprocessing steps. Also, we give ex-
amples of the ten SE types.
There are no established data sets for SE classifi-
cation, so we created annotated training data to test
our models. We have annotated two data sets, one
from the Brown corpus and one based on data from
the Message Understanding Conference 6 (MUC6).
3.1 Segmentation
The Brown texts were segmented according to SE-
containing clausal boundaries, and each clause was
labeled with an SE label. Segmentation is itself a
difficult task, and we made some simplifications.
In general, clausal complements of verbs like say
which have clausal direct objects were treated as
separate clauses and given an SE label. Clausal com-
plements of verbs which have an entity as a direct
object and second clausal complement (such as no-
tify) were not treated as separate clauses. In addi-
tion, some modifying and adjunct clauses were not
assigned separate SE labels.
The MUC texts came to us segmented into ele-
mentary discourse units (EDUs), and each EDU was
labeled by the annotators. The two data sets were
segmented according to slightly different conven-
tions, and we did not normalize the segmentation.
The inconsistencies in segmentation introduce some
error to the otherwise gold-standard segmentations.
3.2 Annotation
Each text was independently annotated by two ex-
perts and reviewed by a third. Each clause was as-
signed precisely one SE label from the set of ten
possible labels. For clauses which introduce more
SE Text
S That compares with roughly paperback-book
dimensions for VHS.
G Accordingly, most VHS camcorders are usually
bulky and weigh around eight pounds or more.
S ?Carl is a tenacious fellow,?
R said a source close to USAir.
GS ?He doesn?t give up easily
GS and one should never underestimate what he can
or will do.?
S For Jenks knew
F that Bari?s defenses were made of paper.
E Mr. Icahn then proposed
P that USAir buy TWA,
IMP ?Fermate?!
R Musmanno bellowed to his Italian crewmen.
Q What?s her name?
S Quite seriously, the names mentioned as possibilities
were three male apparatchiks from the Beltway?s
Democratic political machine
N By Andrew B. Cohen Staff Reporter of The WSJ
Table 1: Example clauses and their SE annota-
tion. Horizontal lines separate extracts from differ-
ent texts.
than one SE, the annotators selected the most salient
one. This situation arose primarily when comple-
ment clauses were not treated as distinct clauses, in
which case the SE selected was the one introduced
by the main verb. The label N was used for clauses
which do not introduce any situation entity.
The Brown data set consists of 20 ?popular lore?
texts from section cf of the Brown corpus. Seg-
mentation of these texts resulted in a total of 4390
clauses. Of these, 3604 were used for training and
development, and 786 were held out as final test-
ing data. The MUC data set consists of 50 Wall
Street Journal newspaper articles segmented to a to-
tal of 1675 clauses. 137 MUC clauses were held
out for testing. The Brown texts are longer than
the MUC texts, with an average of 219.5 clauses
per document as compared to MUC?s average of
33.5 clauses. The average clause in the Brown data
contains 12.6 words, slightly longer than the MUC
texts? average of 10.9 words.
Table 1 provides examples of the ten SE types as
well as showing how clauses were segmented. Each
SE-containing example is a sequence of EDUs from
the data sets used in this study.
898
WWORDS words & punctuation
WT
W (see above)
POSONLY POS tag for each word
WORD/POS word/POS pair for each word
WTL
WT (see above)
FORCEPRED T if clause (or preceding clause)
contains force predicate
PROPPRED T if clause (or preceding clause)
contains propositional verb
FACTPRED T if clause (or preceding clause)
contains factive verb
GENPRED T if clause contains generic predicate
HASFIN T if clause contains finite verb
HASMODAL T if clause contains modal verb
FREQADV T if clause contains frequency adverb
MODALADV T if clause contains modal adverb
VOLADV T if clause contains volitional adverb
FIRSTVB lexical item and POS tag for first verb
WTLG
WTL (see above)
VERBS all verbs in clause
VERBTAGS POS tags for all verbs
MAINVB main verb of clause
SUBJ subject of clause (lexical item)
SUPER CCG supertag
Table 2: Feature sets for SE classification
3.3 Preprocessing
The linguistic tests for SE classification appeal to
multiple levels of linguistic information; there are
lexical, morphological, syntactic, categorial, and
structural tests. In order to access categorial and
structural information, we used the C&C2 toolkit
(Clark and Curran, 2004). It provides part-of-speech
tags and Combinatory Categorial Grammar (CCG)
(Steedman, 2000) categories for words and syntac-
tic dependencies across words.
4 Features
One of our goals in undertaking this study was to
explore the use of linguistically-motivated features
and deep syntactic features in probabilistic models
for SE classification. The nature of the task requires
features characterizing the entire clause. Here, we
describe our four feature sets, summarized in table 2.
The feature sets are additive, extending very basic
feature sets first with linguistically-motivated fea-
tures and then with deep syntactic features.
2svn.ask.it.usyd.edu.ap/trac/candc/wiki
4.1 Basic feature sets: W and WT
The WORDS (W) feature set looks only at the words
and punctuation in the clause. These features are
obtained with no linguistic processing.
WORDS/TAGS (WT) incorporates part-of-speech
(POS) tags for each word, number, and punctuation
mark in the clause and the word/tag pairs for each
element of the clause. POS tags provide valuable in-
formation about syntactic category as well as certain
kinds of shallow semantic information (such as verb
tense). The tags are useful for identifying verbs,
nouns, and adverbs, and the words themselves repre-
sent lexico-semantic information in the feature sets.
4.2 Linguistically-motivated feature set: WTL
The WORDS/TAGS/LINGUISTIC CORRELATES
(WTL) feature set introduces linguistically-
motivated features gleaned from the literature
on SEs; each feature encodes a linguistic cue that
may correlate to one or more SE types. These
features are not directly annotated; instead they are
extracted by comparing words and their tags for
the current and immediately preceding clauses to
lists containing appropriate triggers. The lists are
compiled from the literature on SEs.
For example, clauses embedded under predicates
like force generally introduce E-type SEs:
(7) I forced [John to run the race with me].
(8) * I forced [John to know French].
The feature force-PREV is extracted if a member
of the force-type predicate word list occurs in the
previous clause.
Some of the correlations discussed in the litera-
ture rely on a level of syntactic analysis not available
in the WTL feature set. For example, stativity of the
main verb is one feature used to distinguish between
event and state SEs, and particular verbs and verb
tenses have tendencies with respect to stativity. To
approximate the main verb without syntactic analy-
sis, WTL uses the lexical item of the first verb in the
clause and the POS tags of all verbs in the clause.
These linguistic tests are non-absolute, making
them inappropriate for a rule-based model. Our
models handle the defeasibility of these correlations
probabilistically, as is standard for machine learning
for natural language processing.
899
4.3 Addition of deep features: WTLG
The WORDS/TAGS/LINGUISTIC CORRE-
LATES/GRAMMATICAL RELATIONS (WTLG)
feature set uses a deeper level of syntactic analysis
via features extracted from CCG parse representa-
tions for each clause. This feature set requires an
additional step of linguistic processing but provides
a basis for more accurate classification.
WTL approximated the main verb by sloppily tak-
ing the first verb in the clause; in contrast, WTLG
uses the main verb identified by the parser. The
parser also reliably identifies the subject, which is
used as a feature.
Supertags ?CCG categories assigned to words?
provide an interesting class of features in WTLG.
They succinctly encode richer grammatical informa-
tion than simple POS tags, especially subcategoriza-
tion and argument types. For example, the tag S\NP
denotes an intransitive verb, whereas (S\NP)/NP
denotes a transitive verb. As such, they can be seen
as a way of encoding the verbal constellation and its
effect on aspectual classification.
5 Models
We consider two types of models for the automatic
classification of situation entities. The first, a la-
beling model, utilizes a maximum entropy model
to predict SE labels based on clause-level linguistic
features as discussed above. This model ignores the
discourse patterns that link multiple utterances. Be-
cause these patterns recur, a sequencing model may
be better suited to the SE classification task. Our
second model thus extends the first by incorporating
the previous n (0 ? n ? 6) labels as features.
Sequencing is standardly used for tasks like part-
of-speech tagging, which generally assume smaller
units to be both tagged and considered as context
for tagging. We are tagging at the clause level rather
than at the word level, but the structure of the prob-
lem is essentially the same. We thus adapted the
OpenNLP maximum entropy part-of-speech tagger3
(Hockenmaier et al, 2004) to extract features from
utterances and to tag sequences of utterances instead
of words. This allows the use of features of adjacent
clauses as well as previously-predicted labels when
making classification decisions.
3http://opennlp.sourceforge.net.
6 Experiments
In this section we give results for testing on Brown
data. All results are reported in terms of accu-
racy, defined as the percentage of correctly-labeled
clauses. Standard 10-fold cross-validation on the
training data was used to develop models and fea-
ture sets. The optimized models were then tested on
the held-out Brown and MUC data.
The baseline was determined by assigning S
(state), the most frequent label in both training sets,
to each clause. Baseline accuracy was 38.5% and
36.2% for Brown and MUC, respectively.
In general, accuracy figures for MUC are much
higher than for Brown. This is likely due to the fact
that the MUC texts are more consistent: they are all
newswire texts of a fairly consistent tone and genre.
The Brown texts, in contrast, are from the ?popular
lore? section of the corpus and span a wide range
of topics and text types. Nonetheless, the patterns
between the feature sets and use of sequence predic-
tion hold across both data sets; here, we focus our
discussion on the results for the Brown data.
6.1 Labeling results
The results for the labeling model appear in the two
columns labeled ?n=0? in table 3. On Brown, the
simple W feature set beats the baseline by 6.9% with
an accuracy of 45.4%. Adding POS information
(WT) boosts accuracy 4.5% to 49.9%. We did not
see the expected increase in performance from the
linguistically motivated WTL features, but rather a
slight decrease in accuracy to 48.9%. These features
may require a greater amount of training material to
be effective. Addition of deep linguistic information
with WTLG improved performance to 50.6%, a gain
of 5.2% over words alone.
6.2 Oracle results
To determine the potential effectiveness of sequence
prediction, we performed oracle experiments on
Brown by including previous gold-standard labels as
features. Figure 1 illustrates the results from ora-
cle experiments incorporating from zero to six pre-
vious gold-standard SE labels (the lookback). The
increase in performance illustrates the importance of
context in the identification of SEs and motivates the
use of sequence prediction.
900
42
44
46
48
50
52
54
56
58
60
0 1 2 3 4 5 6
Acc
Lookback
WWTWTLWTLG
Figure 1: Oracle results on Brown data.
6.3 Sequencing results
Table 3 gives the results of classification with the se-
quencing model on the Brown data. As with the la-
beling model, accuracy is boosted by WT and WTLG
feature sets. We see an unexpected degradation in
performance in the transition from WT to WTL.
The most interesting results here, though, are the
gains in accuracy from use of previously-predicted
labels as features for classification. When labeling
performance is relatively poor, as with feature set W,
previous labels help very little, but as labeling accu-
racy increases, previous labels begin to effect notice-
able increases in accuracy. For the best two feature
sets, considering the previous two labels raises the
accuracy 2.0% and 2.5%, respectively.
In most cases, though, performance starts to de-
grade as the model incorporates more than two pre-
vious labels. This degradation is illustrated in Fig-
ure 2. The explanation for this is that the model is
still very weak, with an accuracy of less than 54%
for the Brown data. The more previous predicted la-
bels the model conditions on, the greater the likeli-
hood that one or more of the labels is incorrect. With
gold-standard labels, we see a steady increase in ac-
curacy as we look further back, and we would need
a better performing model to fully take advantage of
knowledge of SE patterns in discourse.
The sequencing model plays a crucial role, partic-
ularly with such a small amount of training material,
and our results indicate the importance of local con-
text in discourse analysis.
42
44
46
48
50
52
54
0 1 2 3 4 5 6
WWTWTLWTLG
Figure 2: Sequencing results on Brown data.
BROWN Lookback (n)
0 1 2 3 4 5 6
W 45.4 45.2 46.1 46.6 42.8 43.0 42.4
WT 49.9 52.4 51.9 49.2 47.2 46.2 44.8
WTL 48.9 50.5 50.1 48.9 46.7 44.9 45.0
WTLG 50.6 52.9 53.1 48.1 46.4 45.9 45.7
Baseline 38.5
Table 3: SE classification results with sequencing
on Brown test set. Bold cell indicates accuracy at-
tained by model parameters that performed best on
development data.
6.4 Error analysis
Given that a single one of the ten possible labels
occurs for more than 35% of clauses in both data
sets, it is useful to look at the distribution of er-
rors over the labels. Table 4 is a confusion matrix
for the held-out Brown data using the best feature
set.4 The first column gives the label and number
of occurrences of that label, and the second column
is the accuracy achieved for that label. The next
two columns show the percentage of erroneous la-
bels taken by the labels S and GS. These two labels
are the most common labels in the development set
(38.5% and 32.5%). The final column sums the per-
centages of errors assigned to the remaining seven
labels. As one would expect, the model learns the
predominance of these two labels. There are a few
interesting points to make about this data.
First, 66% of G-type clauses are mistakenly as-
signed the label GS. This is interesting because
these two SE-types constitute the broader SE cat-
4Thanks to the anonymous reviewer who suggested this use-
ful way of looking at the data.
901
% Correct % Incorrect
Label Label S GS Other
S(278) 72.7 n/a 14.0 13.3
E(203) 50.7 37.0 11.8 0.5
GS(203) 44.8 46.3 n/a 8.9
R(26) 38.5 30.8 11.5 19.2
N(47) 23.4 31.9 23.4 21.3
G(12) 0.0 25.0 66.7 8.3
IMP(8) 0.0 75.0 25.0 0.0
P(7) 0.0 71.4 28.6 0.0
F(2) 0.0 100.0 0.0 0.0
Table 4: Confusion matrix for Brown held-out test
data, WTLG feature set, lookback n = 2. Numbers
in parentheses indicate how many clauses have the
associated gold standard label.
egory of generalizing statives. The distribution of
errors for R-type clauses points out another interest-
ing classification difficulty.5 Unlike the other cat-
egories, the percentage of false-other labels for R-
type clauses is higher than that of false-GS labels.
80% of these false-other labels are of type E. The
explanation for this is that R-type clauses are a sub-
type of the event class.
6.5 Genre effects in classification
Different text domains frequently have different
characteristic properties. Discourse modes are one
way of analyzing these differences. It is thus in-
teresting to compare SE classification when training
and testing material come from different domains.
Table 5 shows the performance on Brown when
training on Brown and/or MUC using the WTLG
feature set with simple labeling and with sequence
prediction with a lookback of two. A number of
things are suggested by these figures. First, the la-
beling model (lookback of zero), beats the baseline
even when training on out-of-domain texts (43.1%
vs. 38.5%), but this is unsurprisingly far below
training on in-domain texts (43.1% vs. 50.6%).
Second, while sequence prediction helps with in-
domain training (53.1% vs 50.6%), it makes no
difference with out-of-domain training (42.9% vs
43.1%). This indicates that the patterns of SEs in a
text do indeed correlate with domains and their dis-
course modes, in line with case-studies in the dis-
course modes theory (Smith, 2003). Finally, mix-
5Thanks to an anonymous reviewer for bringing this to our
attention.
lookback Brown test set
WTLG
train:Brown 0 50.6
2 53.1
train:MUC 0 43.1
2 42.9
train:all 0 50.4
2 49.5
Table 5: Cross-domain SE classification
ing out-of-domain training material with in-domain
material does not hurt labelling accuracy (50.4% vs
50.6%), but it does take away the gains from se-
quencing (49.5% vs 53.1%).
These genre effects are suggestive, but inconclu-
sive. A similar setup with much larger training and
testing sets would be necessary to provide a clearer
picture of the effect of mixed domain training.
7 Related work
Though we are aware of no previous work in SE
classification, others have focused on automatic de-
tection of aspectual and temporal data.
Klavans and Chodorow (1992) laid the founda-
tion for probabilistic verb classification with their
interpretation of aspectual properties as gradient and
their use of statistics to model the gradience. They
implement a single linguistic test for stativity, treat-
ing lexical properties of verbs as tendencies rather
than absolute characteristics.
Linguistic indicators for aspectual classification
are also used by Siegel (1999), who evaluates 14 in-
dicators to test verbs for stativity and telicity. Many
of his indicators overlap with our features.
Siegel and McKeown (2001) address classifica-
tion of verbs for stativity (event vs. state) and
for completedness (culminated vs. non-culminated
events). They compare three supervised and one un-
supervised machine learning systems. The systems
obtain relatively high accuracy figures, but they are
domain-specific, require extensive human supervi-
sion, and do not address aspectual coercion.
Merlo and Stevenson (2001) use corpus-based
thematic role information to identify and classify
unergative, unaccusative, and object-drop verbs.
Stevenson and Merlo note that statistical analysis
cannot and should not be separated from deeper lin-
guistic analysis, and our results support that claim.
902
The advantages of our approach are the broadened
conception of the classification task and the use of
sequence prediction to capture a wider context.
8 Conclusions
Situation entity classification is a little-studied but
important classification task for the analysis of dis-
course. We have presented the first data-driven mod-
els for SE classification, motivating the treatment of
SE classification as a sequencing task.
We have shown that linguistic correlations to sit-
uation entity type are useful features for proba-
bilistic models, as are grammatical relations and
CCG supertags derived from syntactic analysis of
clauses. Models for the task perform poorly given
very basic feature sets, but minimal linguistic pro-
cessing in the form of part-of-speech tagging im-
proves performance even on small data sets used for
this study. Performance improves even more when
we move beyond simple feature sets and incorpo-
rate linguistically-motivated features and grammat-
ical relations from deep syntactic analysis. Finally,
using sequence prediction by adapting a POS-tagger
further improves results.
The tagger we adapted uses beam search; this al-
lows tractable use of maximum entropy for each la-
beling decision but forgoes the ability to find the
optimal label sequence using dynamic programming
techniques. In contrast, Conditional Random Fields
(CRFs) (Lafferty et al, 2001) allow the use of max-
imum entropy to set feature weights with efficient
recovery of the optimal sequence. Though CRFs are
more computationally intensive, the small set of SE
labels should make the task tractable for CRFs.
In future, we intend to test the utility of SEs in dis-
course parsing, discourse mode identification, and
discourse relation projection.
Acknowledgments
This work was supported by the Morris Memorial
Trust Grant from the New York Community Trust.
The authors would like to thank Nicholas Asher,
Pascal Denis, Katrin Erk, Garrett Heifrin, Julie
Hunter, Jonas Kuhn, Ray Mooney, Brian Reese, and
the anonymous reviewers.
References
N. Asher. 1993. Reference to Abstract objects in Dis-
course. Kluwer Academic Publishers.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
G. Carlson and F. J. Pelletier, editors. 1995. The Generic
Book. University of Chicago Press, Chicago.
S. Clark and J. R. Curran. 2004. Parsing the WSJ using
CCG and log?linear models. In Proceedings of ACL?
04, pages 104?111, Barcelona, Spain.
D. Dowty. 1979. Word Meaning and Montague Gram-
mar. Reidel, Dordrecht.
J. Hockenmaier, G. Bierner, and J. Baldridge. 2004. Ex-
tending the coverage of a CCG system. Research on
Language and Computation, 2:165?208.
J. L. Klavans and M. S. Chodorow. 1992. Degrees of
stativity: The lexical representation of verb aspect. In
Proceedings of COLING 14, Nantes, France.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labelling sequence data. In Proceedings
of ICML, pages 282?289, Williamstown, USA.
P. Merlo and S. Stevenson. 2001. Automatic verb clas-
sification based on statistical distributions of argument
structure. Computational Linguistics.
M. Moens and M. Steedman. 1988. Temporal ontol-
ogy and temporal reference. Computational Linguis-
tics, 14(2):15?28.
P. Peterson. 1997. Fact Proposition Event. Kluwer.
E. V. Siegel and K. R. McKeown. 2001. Learning meth-
ods to combine linguistic indicators: Improving as-
pectual classification and revealing linguistic insights.
Computational Linguistics, 26(4):595?628.
E. V. Siegel. 1999. Corpus-based linguistic indicators
for aspectual classification. In Proceedings of ACL37,
University of Maryland, College Park.
C. S. Smith. 1991. The Parameter of Aspect. Kluwer.
C. S. Smith. 2003. Modes of Discourse. Cambridge
University Press.
M. Steedman. 2000. The Syntactic Process. MIT
Press/Bradford Books.
Z. Vendler, 1967. Linguistics in Philosophy, chapter
Verbs and Times, pages 97?121. Cornell University
Press, Ithaca, New York.
H. Verkuyl. 1972. On the Compositional Nature of the
Aspects. Reidel, Dordrecht.
903
Proceedings of the Linguistic Annotation Workshop, pages 176?183,
Prague, June 2007. c?2007 Association for Computational Linguistics
IGT-XML: an XML format for interlinearized glossed texts
Alexis Palmer, Katrin Erk
Department of Linguistics
University of Texas at Austin
{alexispalmer,katrin.erk}@mail.utexas.edu
Abstract
We propose a new XML format for repre-
senting interlinearized glossed text (IGT),
particularly in the context of the documen-
tation and description of endangered lan-
guages. The proposed representation, which
we call IGT-XML, builds on previous mod-
els but provides a more loosely coupled and
flexible representation of different annota-
tion layers. Designed to accommodate both
selective manual reannotation of individual
layers and semi-automatic extension of an-
notation, IGT-XML is a first step toward par-
tial automation of the production of IGT.
1 Introduction
Much previous work on linguistic annotation has
necessarily focused on resource-rich languages, as it
is these languages for which we have large corpora
in need of linguistic annotation. In contrast, devel-
opment of annotation schemata and methodologies
to be used with language data from endangered lan-
guages has been left largely to individual documen-
tary and/or descriptive linguists working with partic-
ular languages.
This paper addresses linguistic annotation in the
context of the documentation and description of
endangered languages. One interesting feature of
language documentation projects is that, while the
languages studied differ widely, there is a quasi-
standard for presenting the material, in the form of
interlinearized glossed text (IGT). IGT typically
comprises at least four levels: (1) the original text,
(2) a separation of the original text into individual
morphemes, (3) a detailed morpheme-by-morpheme
gloss, and (4) a free translation of each sentence.
Another characteristic of language documentation
projects is the tentative nature of many analyses,
given that linguistic analysis is often occurring in
tandem with the annotation process, sometimes for
the first time in the recorded history of the language.
Furthermore, language documentation projects re-
quire long-term accessibility of the collected lan-
guage data as well as easy accessibility to commu-
nity members as well as to linguists.
In this paper we propose a new XML format for
representing IGT, which we call IGT-XML. We
build on the model of Hughes et al(2003) (the BHB
model from now on), who first proposed using the
IGT structure directly as a basis for an XML format.
While their format shows closely integrated annota-
tion layers using XML embedding, our model has
a more loosely coupled and flexible representation
of different annotation layers, to accommodate (a)
selective manual reannotation of individual layers,
and (b) the (semi-)automatic extension of annota-
tion, without the format posing an a priori restriction
on the annotation levels that can be added. The IGT-
XML representation is thus a first step toward par-
tial automation of the production of IGT, which in
turn is part of a larger project using techniques from
machine learning and natural language processing to
significantly reduce the time and money required to
produce annotated texts.
Besides the BHB model, we build on the Open
Languages Archiving Community (OLAC)1 meta-
data standard. OLAC is developing best practice
guidelines for archiving language resources digi-
tally, including a list of metadata entries to record
1http://www.language-archives.org
176
with language data.
Plan of the paper. After discussing interlin-
earized glosses in Section 2, we show the BHB
model and corresponding XML format in Section 3.
Section 4 presents the IGT-XML format that we pro-
pose. Section 5 demonstrates the applicability of
IGT-XML to data from different languages and dif-
ferent documentation projects, and Section 6 con-
cludes.
2 Interlinearized glossed text
IGT is a way of encoding linguistic data commonly
used to present linguistic examples. The example
below is a segment of IGT taken from Kuhn and
Mateo-Toledo (2004). The language is Q?anjob?al,
a Mayan language of Guatemala.
(1) Maxab? ek?elteq ix unin yet
sq?inib?alil tu.
(2) max-ab?
COM-EV
ek?-el-teq
pass-DIR-DIR
ix
CL
unin
child
y-et
E3S-when
s-q?inib?-al-il
E3S-early-ABS-ABS
tu
DEM
?The child came out early that morning (they say)? 2
The format of the IGT in this example is typical of
the presentation of individual examples in the lin-
guistics literature. The raw, unannotated text (1) is
associated with three layers of annotation, shown in
(2). The first annotation layer shows the same text
with each word segmented into its constituent mor-
phemes. The next layer, the gloss layer, is a combi-
nation of English translations of the Q?anjob?al lem-
mas and tags representing the linguistic information
encoded by affixes on the lemmas. The third layer is
an English translation.
IGT formats vary more widely in language doc-
umentation, where IGT is typically the product of
linguistic analysis of texts transcribed from audio or
audiovisual recordings. A broad survey of formats
for interlinear texts (Bow et al, 2003) found vari-
ation in the number of rows, the type of analysis
found in each row, as well as the level of granularity
of analysis in each row.3
2KEY: ABS=abstract, COM=completive, CL=classifier,
DEM=demonstrative, E=ergative, EV=evidential, S=singular,
3=third person
3Hughes et al(2003) also discuss variation in presentational
factors, which we choose not to encode in our XML format.
Tools using IGT Shoebox/Toolbox4 (Shoebox in
following text) is a system that is widely used in doc-
umentary linguistics for storing and managing lan-
guage data. It provides facilities for lexicon man-
agement as well as text interlinearization.
Figure 1 shows one sentence of Q?anjob?al IGT in
the Shoebox output format.5 Shoebox exports texts
as plain text files. The different annotation layers
are marked by labels at the beginning of the line.
For example, in Figure 1 the label \tx marks the
original text and the line starting with \dm contains
its morphological segmentation.
One important test case for any XML format for
IGT is whether it can represent existing IGT data.
As Shoebox is a widely used tool, we take the
Shoebox data format as a representative case study.
Specifically, in Section 5 we show how texts from
two different languages, interlinearized using Shoe-
box and represented in the Shoebox output format,
can be encoded in IGT-XML.
In this paper we focus on the question of repre-
sentation rather than format transformation. Each
system managing IGT data will have different out-
put formats, requiring different techniques for trans-
forming the data to XML. The aim of this paper is
simply to describe and demonstrate the IGT-XML
format; a detailed automatic transformation method
mapping other formats to IGT-XML is beyond the
scope of this paper and will be addressed separately.
3 Previous work
This section discusses previous work on representa-
tion formats and specifically XML formats for inter-
linear text.
The BHB model: four levels of interlinear text.
Building on Bow et al?s (2003) analysis of differ-
ent IGT formats used in the literature, Hughes et
al. (2003) propose a four-level hierarchical model
for representing interlinear text. The four levels en-
code elements common to most instances of IGT:
text, phrase, word, and morpheme. One text may
consist of several individual phrases. A phrase con-
sists of one or more words, each of which consists
4http://www.sil.org/computing/catalog/
show software.asp?id=79
5Data from B?alam Mateo-Toledo, p.c.
177
\ref txt080_p2.002
\tx Exx a yet junxa tyempohal, ayin ti? xiwil+
\dm exxx a y- et jun - xa tyempo -al, ayin ti xiwil+
\ge INTJ ENF E3- de/cuando ART/uno - ya tiempo -ABS yo DEM muchos
\cp intj part pref- sr num - adv s -suf pro part adv
\tes Eee en otro tiempo yo vi
Figure 1: Shoebox output: Q?anjob?al
<resource>
<interlinear_text>
<item type="title">Example</item>
<phrases>
<phrase>
<item type="gls">The child came out
early that morning (they say)</item>
<words>
<word>
<item type="txt">ek?elteq</item>
<morphemes>
<morph>
<item type="txt">ek?</item>
<item type="gls">pass</item>
</morph>
<morph>
<item type="txt">el</item>
<item type="gls">DIR</item>
</morph>
<morph>
<item type="txt">teq</item>
<item type="gls">DIR</item>
</morph>
</morphemes>
</word>
</words>
</phrase>
</phrases>
</interlinear_text>
</resource>
Figure 2: BHB IGT representation format:
Q?anjob?al
of one or more morphemes. To make this more con-
crete, the example in (1) shows a single phrase (or a
one-phrase text). The three annotation layers in (2)
are situated at different levels in the hierarchy: The
first and second annotation layers are both situated
at the morpheme level, showing a separation of the
original phrase into its constituent morphemes and
a morpheme-by-morpheme gloss, respectively. The
third annotation layer, the translation, is again situ-
ated at the phrase level, like the original text in (1).
The BHB model was originally developed in the
context of the EMELD project,6 which has focused
on advancing the state of technologies, data repre-
sentation formats, and methodologies for digital lan-
guage documentation.
The BHB XML format. Figure 2 shows an exam-
ple of the BHB XML format, which articulates the
four nested levels of structure of the BHB model.
It directly expresses the hierarchy of annotation lev-
els in a nested XML structure, in which, for exam-
ple, <morph> elements representing morphemes
are embedded in <word> elements representing
the corresponding words. The model maintains
the link between the source text morpheme and
the morpheme-level gloss annotation by embedding
both as <item> elements within the <morph>
and distinguishing the two by an attribute called
type.
While this representation provides the needed link
between morphemes and their glosses, it is rather in-
flexible because it is not modular: To add an addi-
tional annotation layer at the word level, one would
need to access and change the representation of each
word of each phrase. In this way, the BHBXML for-
mat is not ideally suited for an extensible annotation
that would need to add additional layers of linguistic
information in a flexible way.
6http://linguistlist.org/emeld
178
4 IGT-XML
In this section we propose a new XML representa-
tion for IGT, IGT-XML. Like the BHB XML for-
mat, it is based on the BHB four-level model, but
it modularizes annotation levels. Linking between
annotation levels is achieved via unique IDs.
The IGT-XML format.
Figure 3 illustrates the new IGT-XML format, show-
ing a representation of the Q?anjob?al example of
Figure 1, mostly restricted to a single word, tyem-
pohal, for simplicity.
The IGT-XML format contains (at least) three
main components:
? a plaintext component comprising phrases as
well as the individual words making up each
phrase, encased in the <phrases> XML el-
ement,
? a morpheme component giving a morphologi-
cal analysis of the source text, encased in the
<morphemes> XML element, and
? a gloss component including glosses at both the
phrase and the word level.
Further annotation layers can be added by extend-
ing the format with additional components beyond
these three, which describe the core four levels of
interlinear text.
Within the <phrases> block, each individual
phrase is encased in a <phrase> element, which
includes the plain text within the <plaintext>
element as well as each individual word of the plain
text in a <word> element. Each <phrase> and
each <word> has a globally unique ID, assigned
in an id attribute. We choose to give explicit IDs
to words, rather than rely on character offsets, to
avoid possible problems with character encodings
and mis-represented special characters.
The morphemes in the<morphemes> block are
again organized by<phrase>. Each<phrase>
in the <morphemes> block refers to the corre-
sponding phrase in the <phrases> block by that
phrase?s unique ID.
Each individual morpheme, represented by a
<morph> element, refers to the unique ID corre-
sponding to the word of which it is a part. The lin-
ear order of morphemes belonging to the same word
is reflected in the order in which <morph> ele-
ments appear, as well as in the running id of the
morphemes. Morphemes have id attributes of their
own such that further annotation levels can refer to
the morphological segmentation of the source text,
as is the case for the morpheme-by-morpheme gloss
in the example in (2).
Whole-sentence glosses are collected in the
<translations> block, while word-by-word
glosses reside in the <gloss> block. Again,
glosses are organized by <phrase>, linked to the
original phrases by idref attributes. The glosses
in <gloss> refer to individual morphemes, hence
their idref attributes point to id attributes of the
<morphemes> block.
Metadata information in the file header
As suggested in Figure 3, IGT-XML is easily ex-
tended with metadata for each text. We adopt the
OLAC metadata set which uses the fifteen elements
defined in the Dublin Core metadata standard (Bird
and Simons, 2003a; Bird and Simons, 2001). These
elements provide a framework for specifying key in-
formation such as annotators, format, and language
of the text. In addition, the OLAC standard incorpo-
rates a number of qualifiers specific to the language-
resource community, such as discourse types (story,
conversation, etc.) and linguistic data types (lexi-
con, language description, primary text, etc.), and a
process for adopting further extensions.
In addition to the metadata block at the head of the
document, it would be possible to intersperse addi-
tional metadata blocks throughout the document, if
for example we wanted to indicate change of speaker
from one phrase to another in recorded conversation.
Discussion
Feature overview. The IGT-XML format we have
presented groups annotation into blocks in a mod-
ular fashion. Each block represents an annotation
layer. The format uses globally unique IDs (via
id and idref attributes) rather than XML em-
bedding for linking annotation layers. In particular,
<morph> and <word> annotation is kept sepa-
rate, such that additional layers of annotation at the
word and morpheme levels can be added modularly
without interfering with each other.
In its minimal form, the format has three blocks,
179
<text id="T1" lg="kjb" source_id="txt080_p2" title="Pixanej">
<metadata idref="T1">
<!-- incorporate OLAC metadata standard -->
</metadata>
<body>
<phrases>
<phrase id="T1.P2" source_id="txt080_p2.002">
<plaintext>Exx a yet junxa tyempohal, ayin ti? xiwil+</plaintext>
<word id="T1.P2.W5" text="tyempohal"/>
</phrase>
</phrases>
<morphemes source_layer="\dm">
<phrase idref="T1.P2">
<morph idref="T1.P2.W5" id="T1.P2.W5.M1" text="tyempo"/>
<morph idref="T1.P2.W5" id="T1.P2.W5.M2" text="al">
<type l="suf"/>
</morph>
</phrase>
</morphemes>
<gloss source_layer="\ge">
<phrase idref="T1.P2">
<gls idref="T1.P2.W5.M1" text="tiempo"/>
<gls idref="T1.P2.W5.M2" text="ABS"/>
</phrase>
</gloss>
<translations>
<phrase idref="T1.P2">
<trans id="T1.P2.Tr1" lg="en">Eee en otro tiempo yo vi</trans>
</phrase>
</translations>
</body>
</text>
Figure 3: IGT-XML representation format: Q?anjob?al
180
for phrases, morphemes, and glosses, but it is exten-
sible by further blocks, for example for POS-tags. It
is also possible to have different types of annotation
at the same linguistic level, for example manually
created as well as automatically assigned POS-tags.
Mildly standoff annotation. The IGT-XML for-
mat keeps the plain text separate from all levels of
annotation. However, it is not standoff in the strict
sense of having all annotation levels refer to the
plain text only and never to one another. The rea-
son for this is that there is no clear ?basic? level to
which all other annotation could refer.
One obvious candidate is the plain text, but the
morpheme-by-morpheme gloss refers not to words,
but to the morpheme segmentation of the source
text, as can be seen in example (2). This makes the
morpheme-segmented source text another candidate
for the basic level, but it is not guaranteed that this
level of annotation will always be available. At the
start of the annotation process the documentary lin-
guist likely has a transcription and a translation, but
he or she may or may not have determined the mor-
photactics of the language or even how to identify
word boundaries.
So, in order (a) not to commit the annotator to one
single order of annotation, or the presence of any
particular annotation level besides the plain text, and
(b) to allow annotation to refer to each of the levels
identified in the BHB model ? text, phrase, word,
and morpheme ?, we allow annotation levels to refer
to each other via unique IDs.
Requirements for IGT formats. Given the nature
of language documentation projects and IGT data,
an IGT representation format should (1) support
long term archiving of language data (Bird and Si-
mons, 2003b), which requires platform-independent
encoding, and it should (2) support a range of for-
mats. IGT data from different sources may show
differences in format and in what is annotated (Bow
et al, 2003), and may be produced using different
software systems. (3) It should be possible to add
or exchange layers of annotation in a modular fash-
ion. This is important because linguistic analysis
in language documentation, which typically targets
languages that are not well-studied, is often tenta-
tive and subject to change. This will also become
increasingly important with the use of automation
to aid and speed up language documentation: Au-
tomation techniques will typically target individual
annotation layers, and it is desirable to be able to
exchange automatic analysis tools freely.
Point (1), platform independence, is achieved by
almost any XML format, since XML formats are
plain text-based and mostly human-readable. Point
(2), the coverage of IGT formats in all variants, can
be achieved by adoption of the BHB model. Flexi-
bility and modularity (point (3)) are the main moti-
vations in the introduction of IGT-XML.
Beyond word-level annotation. For now the an-
notation focus in language documentation projects
is mostly on the word level, especially on morphol-
ogy and POS-tags. For annotation at the syntactic
level, it is an open question what the features of a
universally applicable annotation format should be.
At the moment, TIGER XML (Mengel and Lezius,
2000), with its capability to represent discontinuous
constituents, and constituent as well as dependency
information, seems like a good candidate. Syntac-
tic information could be represented in a separate
top-level XML element, linking tree terminals to
<word> elements by their ID attributes.
5 Data
An important goal of this research is to develop an
XML format which will be viable for use in the
broadest possible range of language documentation
contexts. To that end, the format needs to stretch
and morph with the needs and desires of the individ-
ual user. This section discusses some issues arising
from actual use of the format. The points are illus-
trated with pieces of the XML representation rather
than complete XML documents.
IGT-XML has been used to encode portions of
texts from the Mayan language Q?anjob?al and the
Mixe-Zoquean language Soteapanec (more com-
monly known as Sierra Popoluca). Q?anjob?al is
spoken primarily in the northwestern regions of
Guatemala, and Soteapanec is spoken in the south-
ern part of the state of Veracruz, Mexico. Both texts
come from ongoing documentation efforts, and both
were first interlinearized using Shoebox.
181
5.1 Q?anjob?al
Figure 1 shows a Q?anjob?al sentence in the Shoe-
box export format. The annotation comprises origi-
nal text (\tx level), morphological analysis (\dm),
morpheme gloss (\ge), and parts of speech (\cp).
The Q?anjob?al texts we received preserve links be-
tween Shoebox annotation layers only through typo-
graphical alignment. The IGT-XML representation
makes these links explicit through global IDs using
id and idref attributes. It also splits off punctua-
tion, treating punctuation marks as separate words:
<word id="T1.P2.W5" text="tyempohal"/>
<word id="T1.P2.W6" text=","/>
<word id="T1.P2.W7" text="ayin"/>
In the part of speech annotation level (line \cp),
the annotator has additionally marked prefixes and
suffixes, using the labels pref- and -suf, respec-
tively. In the IGT-XML, we have incorporated this
information in the<morphemes> level as type in-
formation on a morpheme. Figure 3 shows an exam-
ple of this, extended below:
<morph idref="T1.P2.W5" id="T1.P2.W5.M1"
text="tyempo"/>
<morph idref="T1.P2.W5" id="T1.P2.W5.M2"
text="al">
<type l="suf"/>
</morph>
<morph idref="T1.P2.W6" id="T1.P2.W6.M1"
text=",">
<type l="punct"/>
</morph>
By encoding morpheme type as a <type> ele-
ment embedded in the <morph>, we can allow a
single morpheme to bear more than one type label.
For example, an annotator may want to mark a single
morpheme as being an inflectional morpheme which
appears in a suffixal position. This would be in-
dicated by associating multiple <type> elements
with a single<morph> element, differentiating the
<type> elements through use of the label (l) at-
tribute, as shown in the constructed example below.
<morph idref="T3.P1.W3" id="T3.P1.W3.M2"
text="al">
<type l="suf"/>
<type l="infl"/>
</morph>
Furthermore, as the type label is specified in an at-
tribute value, each documentation project can spec-
ify its own list of possible labels.
\ref Jovenes 002
\t Weenyi woony=jaych@@x+tyam
\mb weenyi woonyi=jay.ty@@xi+tam
\gs algunos varon+HPL
\t yo7om@7yyajpa+m
\mb 0+yoomo.7@7y-yaj-pa+m
\gs 3ABS+casar con mujer-3PL-INC+ALR
\f Algunos nin*os se casan.
Figure 4: Shoebox output: Soteapanec
5.2 Soteapanec
Figure 4 shows the Shoebox output for a Soteapanec
phrase.7 In the notation chosen in this project, the
characters ?7? and ?@? refer to phonemes (glottal
stop and mid high unrounded vowel, respectively),
while ?-?, ?+?, ?>?, ?=? and ?.? all mark morpheme
boundaries. Clitic boundaries are marked by ?+?, in-
flectional boundaries by ?-?, derivational boundaries
by ?>? or ?.?, and compounds are indicated with ?=?.
The four different morpheme boundaries translate
to morpheme types in the IGT-XML, which are en-
coded as in the Q?anjob?al case:
<morph idref="T1.P2.W1" id="T1.P2.W1.M1"
text="weenyi"/>
<morph idref="T1.P2.W2" id="T1.P2.W2.M1"
text="woonyi=jay">
<type l="compound"/>
</morph>
<morph idref="T1.P2.W2" id="T1.P2.W2.M2"
text="ty@@xi"/>
<morph idref="T1.P2.W2" id="T1.P2.W2.M3"
text="tam">
<type l="suf"/>
</morph>
The encoding of the compound represents one of
many choices to be made by users of IGT-XML. We
have chosen to present the compound woonyi=jay as
a single morpheme, in line with the linguist?s choice
to notate the compounds this way in the text. An al-
ternative would be to break the compound into two
separate morphemes, each marked as a compound
via the l attribute of the <type> element.
A similar choice exists with respect to the repre-
sentation of other derivational morphology, both at
the level of morphological segmentation and at the
level of the plaintext. In this case, the plaintext of the
Soteapanec includes boundary markers. IGT-XML
7Data from (Franco and de Jong Boudreault, 2005).
182
can accommodate this type of text as well as it can a
truly plain text.
In this Shoebox output, there is no typograph-
ical alignment between annotation levels. So the
manual transformation to IGT-XML had to rely on
counting morphemes. However there are frequent
mismatches between the number of morphemes in
the morphological level (\mb) and the gloss level
(\gs). The second group of lines in Figure 4 shows
an example: There are six morphemes on the \mb
level, but seven on the \gs level. We envision that
automatic transformation to IGT-XMLwill flag such
cases as mismatched, thus functioning as error de-
tection for the annotation. Even in the manual trans-
formation process, we have marked mismatches at
the gloss level to facilitate adjudication by the anno-
tator.
<morph idref="T1.P2.W2.M4"><gls text="HPL"
flag="mismatch" flagsrc="amp"
flagdate="031507"/>
</morph>
We also include the source and date of the flag, at-
tributes which could easily be obtained automati-
cally.
This section provides only a sample of the is-
sues encountered using IGT-XML. One of our next
steps is to work on automatic transformations from
Shoebox data formats to IGT-XML, a stage at which
many of these challenges will necessarily be ad-
dressed.
6 Conclusion
In this paper we have introduced a new XML for-
mat for representing language documentation data,
IGT-XML. At the heart of the model is a represen-
tation of interlinearized glossed text (IGT). Building
on the BHB model (Hughes et al, 2003), IGT-XML
represents original text, its translation, a morpholog-
ical analysis of the original text, and a morpheme-
by-morpheme gloss. Different annotation layers are
represented separately in a modular fashion, allow-
ing for flexible annotation of individual layers as
well as the extension by further annotation layers.
Layers are linked explicitly via globally unique IDs,
using id and idref attributes.
One main aim in the design of the IGT-XML for-
mat is to facilitate the (semi-)automatic annotation
of language documentation data. In fact, our next
step will be to explore the use of computational tools
for speeding up and extending the annotation of less-
studied languages. This connection of documentary
and computational linguistics has the potential to be
very useful to documentary linguists. It also repre-
sents an interesting opportunity for the use of semi-
supervised machine learning techniques like active
learning on a novel application.
Acknowledgments
We would like to thank Lynda de Jong Boudreault
and B?alam Mateo-Toledo for sharing with us data
collected in their documentation efforts.
References
Steven Bird and Gary Simons. 2001. The OLAC meta-
data set and controlled vocabularies. In Proceedings
of ACL Workshop on Sharing Tools and Resources for
Research and Education, pages 7?18, Toulouse.
Steven Bird and Gary Simons. 2003a. Extending Dublin
Core Metadata to support the description and discov-
ery of language resources. Computing and the Hu-
manities, 37:375?388.
Steven Bird and Gary Simons. 2003b. Seven dimensions
of portability for language documentation and descrip-
tion. Language, 79(3):557?582.
Catherine Bow, Baden Hughes, and Steven Bird. 2003.
Towards a general model of interlinear text. In Pro-
ceedings of EMELD Workshop 2003: Digitizing and
Annotating Texts and Field Recordings, LSA Institute:
Lansing MI, USA.
Julia Albino Franco and Lynda de Jong Boudreault.
2005. Jovenes. Unpublished annotated text. Univer-
sity of Texas at Austin.
Baden Hughes, Steven Bird, and Catherine Bow. 2003.
Encoding and presenting interlinear text using XML
technologies. In Alistair Knott and Dominique Esti-
val, editors, Proceedings of the Australasian Language
Technology Workshop, pages 105?113.
Jonas Kuhn and B?alam Mateo-Toledo. 2004. Applying
computational linguistic techniques in a documentary
project for Q?anjob?al (Mayan, Guatemala). In Pro-
ceedings of LREC 2004, Lisbon, Portugal.
Andreas Mengel and Wolfgang Lezius. 2000. An XML-
based encoding format for syntactically annotated cor-
pora. In Proceedings of LREC 2000, Athens, Greece.
183
Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 36?44,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Evaluating Automation Strategies in Language Documentation
Alexis Palmer, Taesun Moon, and Jason Baldridge
Department of Linguistics
The University of Texas at Austin
Austin, TX 78712
{alexispalmer,tsmoon,jbaldrid}@mail.utexas.edu
Abstract
This paper presents pilot work integrating ma-
chine labeling and active learning with human
annotation of data for the language documen-
tation task of creating interlinearized gloss
text (IGT) for the Mayan language Uspanteko.
The practical goal is to produce a totally an-
notated corpus that is as accurate as possible
given limited time for manual annotation. We
describe ongoing pilot studies which examine
the influence of three main factors on reduc-
ing the time spent to annotate IGT: sugges-
tions from a machine labeler, sample selection
methods, and annotator expertise.
1 Introduction
Languages are dying at the rate of two each month.
By the end of this century, half of the approxi-
mately 6000 extant spoken languages will cease to
be transmitted effectively from one generation of
speakers to the next (Crystal, 2000). Under this
immense time pressure, documentary linguists seek
to preserve a record of endangered languages while
there are still communities of speakers to work with.
Many language documentation projects target lan-
guages about which our general linguistic knowl-
edge is nonexistent or much less than for more
widely-spoken languages. The vast majority of these
are individual or small-group endeavors on small
budgets with little or no institutional guidance by
the greater documentary linguistic community. The
focus in such projects is often first on collection of
data (documentation), with a following stage of lin-
guistic analysis and description. A key part of the
analysis process, detailed linguistic annotation of the
recorded texts, is a time-consuming and tedious task
usually occurring late in the project, if it occurs at
all.
Text annotation typically involves producing in-
terlinearized glossed text (IGT), labeling for mor-
phology, parts-of-speech, etc., which greatly facil-
itates further exploration and analysis of the lan-
guage. The following is IGT for the phrase xelch
li from the Mayan language Uspanteko:1
(1) x-
COM-
el
salir
-ch
-DIR
li
DEM
Spanish: ?Salio entonces.? English:?Then he left.?
The levels of analysis include morpheme segmenta-
tion, transliteration of stems, and labeling of stems
and morphemes with tags, some corresponding to
parts-of-speech and others to semantic distinctions.
There is no single standard format for IGT. The
IGT systems developed by documentation projects
tend to be idiosyncratic: they may be linguistically
well-motivated and intuitive, but they are unlikely to
be compatible or interchangeable with systems de-
veloped by other projects. They may lack internal
consistency as well. Nonetheless, IGT in a read-
ily accessible format is an important resource that
can be used fruitfully by linguists to examine hy-
potheses on novel data (e.g. Xia and Lewis (2007;
2008), Lewis and Xia (2008)). Furthermore, it can
be used by educators and language activists to create
curriculum material for mother language education
and promote the survival of the language.
Despite the urgent need for such resources, IGT
annotations are time consuming to create entirely by
hand, and both human and financial resources are
extremely limited in this domain. Thus, language
1KEY: COM=completive aspect, DEM=demonstrative,
DIR=directional
36
documentation presents an interesting test case and
an ideal context for use of machine labeling and ac-
tive learning. This paper describes a series of ex-
periments designed to assess this promise in a re-
alistic documentation context: creation of IGT for
the Mayan language Uspanteko. We systematically
compare varying degrees of machine involvement in
the development of IGT, from minimally involved
situations where examples for tagging are selected
sequentially to active learning situations where the
machine learner selects samples for human tagging
and suggests labels. We also discuss the challenges
faced by linguists in having to learn, transcribe, ana-
lyze, and annotate a language almost simultaneously
and discuss whether machine involvement reduces
or compounds those challenges.
In the experiments, two documentary linguists an-
notate IGT for Uspanteko texts using different lev-
els of support from a machine learned classifier. We
consider the interaction of three main conditions: (1)
sequential, random, or uncertainty sampling for re-
questing labels from an annotator, (2) suggestions or
no suggestions from a machine labeler, and (3) ex-
pert versus non-expert annotator. All annotator deci-
sions are timed, enabling the actual time cost of an-
notation to be measured within the context of each
condition. This paper describes the Uspanteko data
set we adapted for the experiments, expands on the
choices described above, and reports on preliminary
results from our ongoing annotation experiments.
2 Data: Uspanteko IGT
This section describes the Uspanteko corpus used
for the experiments, our clean-up of the corpus, and
the specific task?labeling part-of-speech and gloss
tags?addressed by the experiments.
2.1 OKMA Uspanteko corpus
Our primary dataset is a corpus of texts (Pixabaj et
al., 2007) in the Mayan language Uspanteko that
were collected, transcribed, translated (into Span-
ish) and annotated as part of the OKMA language
documentation project.2 Uspanteko, a member of
the K?ichee? branch of the Mayan language family,
is spoken by approximately 1320 people in central
Guatemala (Richards, 2003).
2http://www.okma.org
The corpus contains 67 texts, 32 of them glossed.
Four textual genres are represented in the glossed
portion of the corpus: oral histories (five texts) usu-
ally have to do with the history of the village and the
community, personal experience texts (five texts) re-
count events from the lives of individual people in
the community, and stories (twenty texts) are pri-
marily folk stories and children?s stories. The corpus
also contains one recipe and one advice text in which
a speaker discusses what the community should be
doing to better preserve and protect the environment.
The transcriptions are based on spoken data, with
attendant dysfluencies, repetitions, false starts, and
incomplete sentences. Of the 284,455 words, 74,298
are segmented and glossed. This is a small dataset
by computational linguistics standards but rather
large for a documentation project.
2.2 Interlinearized Glossed Text
Once recordings have been made, the next tasks are
typically to produce translations and transcription of
the audio. Transcription is a complex and difficult
process, often involving the development of an or-
thography for the language in parallel. The product
of the transcription is raw text like the Uspanteko
sample shown below (text 068, clauses 283-287):
Non li in yolow rk?il kita?
tinch?ab?ex laj inyolj iin, si no ke
laj yolj jqaaj tinch?ab?ej i non qe li
xk?am rib? chuwe, non qe li lajori
non li iin yolow rk?ilaq.3
Working with the transcription, the translation, and
any previously-attained knowledge about the lan-
guage, the linguist next makes decisions about the
division of words into morphemes and the contribu-
tions made by individual morphemes to the meaning
of the word or of the sentence. IGT efficiently brings
together and presents all of this information.
In the traditional four-line IGT format, mor-
phemes appear on one line and glosses for those
morphemes on the next. The gloss line includes both
labels for grammatical morphemes (e.g. PL or COM)
and translations of stems (e.g. salir or ropa). See
the following example from Uspanteko:4
3Spanish: Solo asi yo aprendi con e?l. No le hable en el
idioma mio. Si no que en el idioma su papa? le hablo. Y solo asi
me fui acostumbrando. Solo asi ahora yo platico con ellos.
4KEY: E1S=singular first person ergative, INC=incompletive,
PART=particle, PREP=preposition, PRON=pronoun, NEG=negation,
37
(2) Kita? tinch?ab?ej laj inyolj iin
(3) kita?
NEG
PART
t-in-ch?abe-j
INC-E1S-hablar-SC
TAM-PERS-VT-SUF
laj
PREP
PREP
in-yolj
E1S-idioma
PERS-S
iin
yo
PRON
?No le hablo en mi idioma.?
(?I don?t speak to him in my language.?)
Most commonly, IGT is presented in a four-tier
format. The first tier (2) is the raw, unannotated
text. The second (first line of (3)) is the same text
with each word morphologically segmented. The
third tier (second line of (3)), the gloss line, is a
combination of Spanish translations of the Uspan-
teko stems and gloss tags representing the grammat-
ical information encoded by affixes and stand-alone
morphemes. The fourth tier (fourth line of (3)) is a
translation in the target language of documentation.
Some interlinear texts include other project-
defined tiers. OKMA uses a fifth tier (third line of
(3)), described as the word-class line. This line is
a mix of traditional POS tags, positional labels (e.g.
suffix, prefix), and broader linguistic categories like
TAM for tense-aspect-mood.
2.3 Cleaning up the OKMA annotations
The OKMA annotations were created using Shoe-
box,5 a standard tool used by documentary linguists
for lexicon management and IGT creation. To de-
velop a corpus suitable for these studies, it was nec-
essary to put considerable effort into normalizing
the original OKMA source annotations. Varied lev-
els of linguistic training of the original annotators
led to many inconsistencies in the original annota-
tions. Also, Shoebox (first developed in 1987) uses
a custom, pre-XML whitespace delimited data for-
mat, making normalization especially challenging.
Finally, not all of the texts are fully annotated. Al-
most half of the 67 texts are just transcriptions, sev-
eral texts are translated but not further analyzed, and
several others are only partially annotated at text
level, clause level, word level, or morpheme level. It
was thus necessary to identify complete texts for use
in our experiments. Some missing labels in nearly-
complete texts were filled in by the expert annotator.
A challenge for representing IGT in a machine-
readable format is maintaining the links between
S=sustantivo (noun), SC=category suffix, SUF=suffix,
TAM=tense/aspect/mood, VT=transitive verb
5http://www.sil.org/computing/shoebox/
the source text morphemes in the second tier and
the morpheme-by-morpheme glosses in the third
tier. The standard Shoebox output format, for ex-
ample, enforces these links through management of
the number of spaces between items in the output.
To address this, we converted the cleaned annota-
tions into IGT-XML (Palmer and Erk, 2007) with
help from the Shoebox/Toolbox interfaces provided
in the Natural Language Toolkit (Robinson et al,
2007). Automating the transformation from Shoe-
box format to IGT-XML?s hierarchical format re-
quired cleaning up tier-to-tier alignment and check-
ing segmentation in some cases where morphemes
and glosses were misaligned, as in (5) below.6
(4) Non li in yolow rk?il
(5) Non
DEM
DEM
li
DEM
DEM
in
yo
PRON
yolow
platicar
VI
r-k?il
AP
SUF
E3s.-SR
PERS SREL
?Solo asi yo aprendi con e?l.?
Here, the number of elements in the morpheme tier
(first line of (5)) does not match the number of el-
ements in the gloss tier (second line of (5)). The
problem is a misanalysis of yolow: it should be
segmented yol-ow with the gloss platicar-AP.
Automating this transformation has the advantage of
identifying such inconsistencies and errors.
There also were many low-level issues that had
to be handled, such as checking and enforcing con-
sistency of tags. For example, the tag E3s. in the
gloss tier of (5) is a typo; the correct tag is E3S. The
annotation tool used in these studies does not allow
such inconsistencies to occur.
2.4 Target labels
There are two main tasks in producing IGT: word
segmentation (determination of stems and affixes)
and glossing each segment. Stems and affixes each
get a different type of gloss: the gloss of a stem is
typically its translation whereas the gloss of an affix
is a label indicating its grammatical role. The addi-
tional word-class line provides part-of-speech infor-
mation for the stems, such as VT for salir.
Complete prediction of segmentation, gloss trans-
lations and labels is our ultimate goal for aiding IGT
6KEY: AP=antipassive, DEM=demonstrative, E3S=singular third
person ergative, PERS=person marking, SR/SREL=relational noun,
VI=intransitive verb
38
creation with automation. Here, we study the poten-
tial for improving annotation efficiency for the more
limited task of predicting the gloss label for each af-
fix and the part-of-speech label for each stem. Thus,
the experiments aim to produce a single label for
each morpheme. We assume that words have been
pre-segmented and we ignore the gloss translations.
The target representation in these studies is an ad-
ditional tier which combines gloss labels for affixes
and stand-alone morphemes with part-of-speech la-
bels for stems. Example (6) repeats the clause in (4),
adding this new combined tier. Stem labels are given
in bold text, and affix labels in plain text.
(6) Non li in yolow rk?il
(7) Non
DEM
li
DEM
in
PRON
yol-ow
VI-AP
r-k?il
E3S-SR
?Solo asi yo aprendi con e?l.?
A simple procedure was used to create the new tier.
For each morpheme, if a gloss label (such as DEM
or E3S) appears on the gloss line (second line of
(3)), we select that label. If what appears is a stem
translation, we instead select the part-of-speech la-
bel from the next tier down (third line of (3)).
In the entire corpus, sixty-nine different labels
appear in this combined tier. The following table
shows the five most common part-of-speech labels
(left) and the five most common gloss labels (right).
The most common label, S, accounts for 11.3% of
the tokens in the corpus.
S noun 7167 E3S sg.3p. ergative 3433
ADV adverb 6646 INC incompletive 2835
VT trans. verb 5122 COM completive 2586
VI intrans. verb 3638 PL plural 1905
PART particle 3443 SREL relational noun 1881
3 Integrated annotation and automation
The experimental framework described in this sec-
tion is designed to model and evaluate real-time inte-
gration of human annotation, active learning strate-
gies, and output from machine-learned classifiers.
The task is annotation of morpheme-segmented texts
from a language documentation project (sec. 2).
3.1 Tools and resources
Integrating automated support and human annota-
tion in this context requires careful coordination of
three components: 1) presenting examples to the an-
notator and storing the annotations, 2) training and
evaluation of tagging models using data labeled by
the annotator, and 3) selecting new examples for an-
notation. The processes are managed and coordi-
nated using the OpenNLP IGT Editor.7 The anno-
tation component of the tool, and in particular the
user interface, is built on the Interlinear Text Editor
(Lowe et al, 2004).
For tagging we use a strong but simple standard
classifier. There certainly are many other modeling
strategies that could be used, for example a condi-
tional random field (as in Settles and Craven (2008)),
or a model that deals differently with POS labels and
morpheme gloss labels. Nonetheless, a documen-
tary linguistics project would be most likely to use a
straightforward, off-the-shelf labeler, and our focus
is on exploring different annotation approaches in a
realistic documentation setting rather than building
an optimal classifier. To that end, we use a standard
maximum entropy classifier which predicts the label
for a morpheme based on the morpheme itself plus
a window of two morphemes before and after. Stan-
dard features used in part-of-speech taggers are ex-
tracted from the morpheme to help with predicting
labels for previously unseen stems and morphemes.
3.2 Annotators and annotation procedures
A practical goal of these studies is to explore best
practices for using automated support to create fully-
annotated texts of the highest quality possible within
fixed resource limits. For producing IGT, one of the
most valuable resources is the time of a linguist with
language-specific expertise. Documentary projects
may also (or instead) have access to a trained lin-
guist without prior experience in the language. We
compare results from two annotators with different
levels of exposure to the language. Both are trained
linguists who specialize in language documentation
and have extensive field experience.8
The first, henceforth referred to as the expert
annotator, has worked extensively on Uspanteko,
including writing a grammar of the language and
7http://igt.sourceforge.net/
8It should be noted that these are pilot studies. With just
two annotators, the annotation comparisons are suggestive but
not conclusive. Even so, this scenario accurately reflects the
resource limitations encountered in documentation projects.
39
contributing to the publication of an Uspanteko-
Spanish dictionary (A?ngel Vicente Me?ndez, 2007).
She is a native speaker of K?ichee?, a closely-related
Mayan language. The second annotator, the non-
expert annotator, is a doctoral student in language
documentation with no prior experience with Us-
panteko and only limited previous knowledge of
Mayan languages. Throughout the annotation pro-
cess, the non-expert annotator relied heavily on the
Uspanteko-Spanish dictionary. Both annotators are
fluent speakers of Spanish, the target translation and
glossing language for the OKMA texts.
In many annotation projects, labeling of training
data is done with reference to a detailed annotation
manual. In the language documentation context, a
more usual situation is for the annotator(s) to work
from a set of agreed-upon conventions but without
strict annotation guidelines. This is not because doc-
umentary linguists lack motivation or discipline but
simply because many aspects of the language are un-
known and the analysis is constantly changing.
In the absence of explicit written annotation
guidelines, we use an annotation training process for
the annotators to learn the OKMA annotation con-
ventions. Two seed sets of ten clauses each were se-
lected to be used both for human annotation training
and for initial classifier training. The first ten clauses
of the first text in the training data were used to seed
model training for the sequential selection cases (see
3.4). The second set of ten were randomly selected
from the entire corpus and used to seed model train-
ing for both random and uncertainty sampling.
These twenty clauses were used to provide initial
guidance to the annotators. With the aid of a list of
possible labels and the grammatical categories they
correspond to, each annotator was asked to label the
seed clauses, and these labels were compared to the
gold standard labels. Annotators were told which
labels were correct and which were incorrect, and
the process was repeated until all morphemes were
correctly labeled. In some cases during this training
phase, the correct label for a morpheme was sup-
plied to the annotator after several incorrect guesses.
3.3 Suggesting labels
We consider two situations with respect to the con-
tribution of the classifier: a suggest condition in
which the labels predicted by the machine learner
are shown to the annotator as she begins labeling a
selected clause, and a no-suggest condition in which
the annotator does not see the predicted labels.
In the suggest cases, the annotator is shown the la-
bel assigned the greatest likelihood by the tagger as
well as a list of several highly-likely labels, ranked
according to likelihood. To be included on this list,
a label must be assigned a probability greater than
half that of the most-likely label. In the no-suggest
cases, the annotator has access to a list of the la-
bels previously seen in the training data for a given
morpheme, ranked in order of frequency of occur-
rence with the morpheme in question; this is similar
to the input an annotator gets while glossing texts in
Shoebox/Toolbox. Specifically, Shoebox/Toolbox
presents previously seen glosses and labels for a
given morpheme in alphabetic order.
3.4 Sample selection
We consider three methods of selecting examples
for annotation?sequential (seq), random (rand), and
uncertainty sampling (al)?and the performance of
each method in both the suggest and the no-suggest
setups. For uncertainty sampling, we measure un-
certainty of a clause as the average entropy per mor-
pheme (i.e., per labeling decision).
3.5 Measuring annotation cost
Not all examples take the same amount of effort to
annotate. Even so, the bulk of the literature on active
learning assumes some sort of unit cost to determine
the effectiveness of different sample selection strate-
gies. Examples of unit cost measurements include
the number of documents in text classification, the
number of sentences in part-of-speech tagging (Set-
tles and Craven, 2008), or the number of constituents
in parsing (Hwa, 2000). These measures are conve-
nient for performing active learning simulations, but
awareness has grown that they are not truly repre-
sentative measures of the actual cost of annotation
(Haertel et al, 2008a; Settles et al, 2008), with Ngai
and Yarowsky (2000) being an early exception to the
unit-cost approach. Also, Baldridge and Osborne
(2004) use discriminants in parse selection, which
are annotation decisions that they later showed cor-
relate with timing information (Baldridge and Os-
borne, 2008).
The cost of annotation ultimately comes down to
40
money. Since annotator pay may be variable but will
(under standard assumptions) be constant for a given
annotator, the best approximation of likely cost sav-
ings is to measure the time taken to annotate under
different levels of automated support. This is es-
pecially important in sample selection and its inter-
action with automated suggestions: active learning
seeks to find more informative examples, and these
will most likely involve more difficult decisions, de-
creasing annotation quality and/or increasing anno-
tation time (Hachey et al, 2005). Thus, we measure
cost in terms of the time taken by each annotator on
each example. This allows us to measure the actual
time taken to produce a given labeled data set, and
thus compare the effectiveness of different levels of
automated support plus their interaction with anno-
tators of different levels of expertise.
Recent work shows that paying attention to pre-
dicted annotation cost in sample selection itself can
increase the effectiveness of active learning (Settles
et al, 2008; Haertel et al, 2008b). Though we have
not explored cost-sensitive selection here, the sce-
nario described here is an appropriate test ground for
it: in fact, the results of our experiments, reported in
the next section, provide strong evidence for a real
natural language annotation task that active learning
selection with cost-sensitivity is indeed sub-optimal.
4 Discussion
This section presents and discusses preliminary re-
sults from the ongoing annotation experiments. The
Uspanteko corpus was split into training, develop-
ment, and held-out test sets, roughly 50%, 25%,
and 25%. Specifically, the training set of 21 texts
contains 38802 words, the development set of 5
texts contains 16792 words, and the held-out test
set, 6 texts, contains 18704 words. These are small
datasets, but the size is realistic for computational
work on endangered languages.
When measuring the performance of annotators,
factors like fatigue, frustration, and especially the
annotator?s learning process must be considered.
Annotators improve as they see more examples (es-
pecially the non-expert annotator). To minimize the
impact of the annotator?s learning process on the re-
sults, annotation is done in rounds. Each round con-
sists of ten clauses from each of the six experimental
0 10 20 30 40 50
0
10
20
30
40
Number of Annotation Rounds
Seco
nds 
per M
orph
eme
Non?expertExpert
Figure 1: Average annotation time (in seconds per mor-
pheme) over annotation rounds, averaged over all six con-
ditions for each annotator.
cases for each annotator. The newly-labeled clauses
are then added to the labeled training data, and a new
tagging model is trained on the updated training set
and evaluated on the development set. Both annota-
tors have completed fifty-one rounds of annotation
so far, labeling 510 clauses for each of the six ex-
perimental conditions. The average number of mor-
phemes labeled is 3059 per case. Because the anno-
tation experiments are ongoing, we discuss results in
terms of the trends seen thus far.
4.1 Annotator speed
The expert annotator showed a small increase in
speed after an initial familiarization period, and the
non-expert showed a dramatic increase. Figure 1
plots the number of seconds taken per morpheme
over the course of annotation, averaged over all six
conditions for each annotator. The slowest, fastest,
and mean rates, in seconds per morpheme, for the
expert annotator were 12.60, 1.89, and 4.14, respec-
tively. For the non-expert, they were 59.71, 1.90,
and 8.03.
4.2 Accuracy of model on held-out data
Table 1 provides several measures of the current
state of annotation in all 12 conditions after 51
rounds of annotation. The sixth column, labeled
41
Anno Suggest Select Time (sec) #Morphs Model Accuracy Total Accuracy of Annotation
NonExp N Seq 23739.79 3314 63.28 63.92
NonExp N Rand 22721.11 2911 68.36 68.69
NonExp N AL 23755.71 2911 68.26 67.84
NonExp Y Seq 21514.05 2887 66.55 66.89
NonExp Y Rand 22189.68 3002 68.41 68.73
NonExp Y AL 25731.57 2750 67.63 67.30
Exp N Seq 11862.39 3354 61.15 61.88
Exp N Rand 11665.10 3043 64.60 64.91
Exp N AL 13894.14 3379 66.74 66.47
Exp Y Seq 11758.74 2892 61.12 61.48
Exp Y Rand 11426.85 2979 60.13 60.57
Exp Y AL 16253.40 3296 63.30 63.15
Table 1: After 51 rounds of annotation: ModelAcc=accuracy on development set, TotalAnnoAcc=accuracy of fully-labeled corpus
ModelAcc, shows the accuracy of models on the
development data. This represents a unit cost as-
sumption at the clause level: measured this way, the
results would suggest that the non-expert was best
served by random selection, with no effect from ma-
chine suggestions. For the expert, they suggest ac-
tive learning without suggestions is best, and that
suggestions actually hurt effectiveness.
4.3 Accuracy of fully-labeled corpus
We are particularly concerned with the question of
how to develop a fully-labeled corpus with the high-
est level of accuracy, given a finite set of resources.
Thus, we combine the portion of the training set la-
beled by the human annotator with the results of tag-
ging the remainder of the training set with the model
trained on those annotations. The rightmost column
of Table 1, labeled Total Accuracy of Annotation,
shows the accuracy of the fully labeled training set
(part human, part machine labels) after 51 rounds.
These accuracies parallel the model accuracies: ran-
dom selection is best for the non-expert annotator,
and uncertainty selection is best for the expert.
Since this tagging task involves labeling mor-
phemes, a clause cost assumption is not ideal?e.g.,
active learning tends to select longer clauses and
thereby obtains more labels. To reflect this, a sub-
clause cost can help: here we use the number of
morphemes annotated. The column labeled Tokens
in Table 2 shows the total accuracy achieved in each
condition when human annotation ceases at 2750
morphemes. The figure in parentheses is the cumu-
lative annotation time at the morpheme cut-off point.
Here, the non-expert does best: he took great care
with the annotations and was clearly not tempted to
Anno Suggest Select Time Tokens (time)
(11427 sec) (2750 morphs)
NonExp N Seq 55.01 59.80 (21678 secs)
NonExp N Rand 59.95 68.68 (22069 secs)
NonExp N AL 59.86 67.70 (22879 secs)
NonExp Y Seq 60.27 66.79 (21053 secs)
NonExp Y Rand 62.96 68.38 (21194 secs)
NonExp Y AL 59.18 67.30 (25732 secs)
Exp N Seq 61.21 59.18 (10110 secs)
Exp N Rand 64.92 64.42 (10683 secs)
Exp N AL 65.72 65.74 (11826 secs)
Exp Y Seq 61.47 61.47 (11436 secs)
Exp Y Rand 60.57 61.16 (10934 secs)
Exp Y AL 61.54 62.87 (13957 secs)
Table 2: For given cost, accuracy of fully-labeled corpus.
accept erroneous suggestions from the machine la-
beler. In contrast, the expert does seem to have ac-
cepted many bad machine suggestions.
Morpheme unit cost is more fine-grained than
clause-level cost, but it hides the fact that the ex-
pert annotator needed far less time to produce a cor-
pus of higher overall labeled quality than the non-
expert. This can be seen in the Time column of
Table 2, which gives the total annotation accuracy
when 11427 seconds are alloted for human label-
ing. The expert annotator achieved the highest accu-
racy for total labeling of the training set using active
learning without machine label suggestions. Active
learning helps the non-expert as well, but his best
condition is random selection with machine labels.
4.4 Annotator accuracy by round
Active learning clearly selects harder examples that
hurt the non-expert?s performance. To see this
clearly, we measured the accuracy of the annotators?
labels for each round of each experimental setup,
42
0 10 20 30 40 50
50
60
70
80
90
100
Number of Annotation Rounds
Sing
le R
oun
d Ac
cura
cy
Suggest + UncertaintySuggest + RandomSuggest + SequentialNo?suggest + UncertaintyNo?suggest + RandomNo?suggest + Sequential
0 10 20 30 40 50
50
60
70
80
90
100
Number of Annotation Rounds
Sing
le R
oun
d Ac
cura
cy
Suggest + UncertaintySuggest + RandomSuggest + SequentialNo?suggest + UncertaintyNo?suggest + RandomNo?suggest + Sequential
(a) (b)
Figure 2: Single round accuracy per round for each experiment type by: (a) non-expert annotator, (b) expert annotator
given in Fig. 2. It is not clear at this stage whether
the tag suggestions by the machine labeler are help-
ful to human annotation. It is useful to compare the
cases where the machine learner is not involved in
example selection (i.e. random and sequential) to
uncertainty sampling, which does involve the ma-
chine learner. One thing that is apparent is that when
active learning is used to select samples for annota-
tion, both the expert and non-expert annotator have
a harder time providing correct tags. A point of con-
trast between the expert and non-expert is that the
non-expert generally outperforms the expert on label
accuracy in the non-active learning scenarios. The
non-expert was very careful with his labeling deci-
sions, but also much slower than the expert. In the
end, speedier annotation rates allowed the expert an-
notator to achieve higher accuracies in less time.
5 Conclusion
We have described a set of ongoing pilot experi-
ments designed to test the utility of machine label-
ing and active learning in the context of documen-
tary linguistics. The production of IGT is a realistic
annotation scenario which desperately needs label-
ing efficiency improvements. Our preliminary re-
sults suggest that both machine labeling and active
learning can increase the effectiveness of annotators,
but they interact quite strongly with the expertise of
the annotators. In particular, though active learn-
ing works well with the expert annotator, for a non-
expert annotator it seems that random selection is
a better choice. However, we stress that our anno-
tation experiments are ongoing. Active learning is
often less effective early in the learning curve, es-
pecially when automated label suggestions are pro-
vided, because the model is not yet accurate enough
to select truly useful examples, nor to suggest labels
for them reliably (Baldridge and Osborne, 2004).
Thus, we expect automation via uncertainty sam-
pling and/or suggestion may gather momentum and
outpace random selection and/or no suggestions by
wider margins as annotation continues.
Acknowledgments
This work is funded by NSF grant BCS 06651988
?Reducing Annotation Effort in the Documentation
of Languages using Machine Learning and Active
Learning.? Thanks to Katrin Erk, Nora England,
Michel Jacobson, and Tony Woodbury; and to anno-
tators Telma Kaan Pixabaj and Eric Campbell. Fi-
nally, thanks to the anonymous reviewers for valu-
able feedback.
43
References
Miguel A?ngel Vicente Me?ndez. 2007. Diccionario bil-
ingu?e Uspanteko-Espan?ol. Cholaj Tzijb?al li Uspan-
teko. Okma y Cholsamaj, Guatemala.
Jason Baldridge and Miles Osborne. 2004. Active learn-
ing and the total cost of annotation. In Proceedings of
Empirical Approaches to Natural Language Process-
ing (EMNLP).
Jason Baldridge and Miles Osborne. 2008. Active learn-
ing and logarithmic opinion pools for HPSG parse se-
lection. Natural Language Engineering, 14(2):199?
222.
David Crystal. 2000. Language Death. Cambridge Uni-
versity Press, Cambridge.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In Proceedings of the 9th Conference
on Computational Natural Language Learning, Ann
Arbor, MI.
Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-
roll, and McClanahan Peter. 2008a. Assessing the
costs of sampling methods in active learning for anno-
tation. In Proceedings of ACL-08: HLT, Short Papers,
pages 65?68, Columbus, Ohio, June. Association for
Computational Linguistics.
Robbie A. Haertel, Kevin D. Seppi, Eric K. Ringger, and
James L. Carroll. 2008b. Return on investment for
active learning. In Proceedings of the NIPS Workshop
on Cost-Sensitive Learning. ACL Press.
Rebecca Hwa. 2000. Sample selection for statistical
grammar induction. In Proceedings of the 2000 Joint
SIGDAT Conference on EMNLP and VLC, pages 45?
52, Hong Kong, China, October.
William Lewis and Fei Xia. 2008. Automatically iden-
tifying computationally relevant typological features.
In Proceedings of IJCNLP-2008, Hyderabad, India.
John Lowe, Michel Jacobson, and Boyd Michailovsky.
2004. Interlinear text editor demonstration and projet
archivage progress report. In 4th EMELD workshop
on Linguistic Databases and Best Practice, Detroit,
MI.
Grace Ngai and David Yarowsky. 2000. Rule Writing or
Annotation: Cost-efficient Resource Usage for Base
Noun Phrase Chunking. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 117?125, Hong Kong.
Alexis Palmer and Katrin Erk. 2007. IGT-XML: An
XML format for interlinearized glossed text. In Pro-
ceedings of the Linguistic Annotation Workshop (LAW-
07), ACL07, Prague.
Telma Can Pixabaj, Miguel Angel Vicente Me?ndez,
Mar??a Vicente Me?ndez, and Oswaldo Ajcot Damia?n.
2007. Text collections in Four Mayan Languages.
Archived in The Archive of the Indigenous Languages
of Latin America.
Michael Richards. 2003. Atlas lingu???stico de Guatemala.
Servipresna, S.A., Guatemala.
Stuart Robinson, Greg Aumann, and Steven Bird. 2007.
Managing fieldwork data with Toolbox and the Natu-
ral Language Toolki t. Language Documentation and
Conservation, 1:44?57.
Burr Settles and Mark Craven. 2008. An analysis of
active learning strategies for sequence labeling tasks.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1070?1079, Honolulu, Hawaii, October. Association
for Computational Linguistics.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Proceed-
ings of the NIPS Workshop on Cost-Sensitive Learn-
ing, pages 1069?1078. ACL Press.
Fei Xia and William Lewis. 2007. Multilingual struc-
tural projection across interlinear text. In Proceedings
of HLT/NAACL 2007, Rochester, NY.
Fei Xia and William Lewis. 2008. Repurposing theoreti-
cal linguistic data for tool development antd search. In
Proceedings of IJCNLP-2008, Hyderabad, India.
44
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 949?957,
Beijing, August 2010
Bringing Active Learning to Life
Ines Rehbein
Computational Linguistics
Saarland University
Josef Ruppenhofer
Computational Linguistics
Saarland University
{rehbein|josefr|apalmer}@coli.uni-sb.de
Alexis Palmer
Computational Linguistics
Saarland University
Abstract
Active learning has been applied to dif-
ferent NLP tasks, with the aim of limit-
ing the amount of time and cost for human
annotation. Most studies on active learn-
ing have only simulated the annotation
scenario, using prelabelled gold standard
data. We present the first active learning
experiment for Word Sense Disambigua-
tion with human annotators in a realistic
environment, using fine-grained sense dis-
tinctions, and investigate whether AL can
reduce annotation cost and boost classifier
performance when applied to a real-world
task.
1 Introduction
Active learning has recently attracted attention as
having the potential to overcome the knowledge
acquisition bottleneck by limiting the amount of
human annotation needed to create training data
for statistical classifiers. Active learning has been
shown, for a number of different NLP tasks, to re-
duce the number of manually annotated instances
needed for obtaining a consistent classifier perfor-
mance (Hwa, 2004; Chen et al, 2006; Tomanek et
al., 2007; Reichart et al, 2008).
The majority of such results have been achieved
by simulating the annotation scenario using prela-
belled gold standard annotations as a stand-in for
real-time human annotation. Simulating annota-
tion allows one to test different parameter set-
tings without incurring the cost of human anno-
tation. There is, however, a major drawback: we
do not know whether the results of experiments
performed using hand-corrected data carry over to
real-world scenarios in which individual human
annotators produce noisy annotations. In addi-
tion, we do not know to what extent error-prone
annotations mislead the learning process. A sys-
tematic study of the impact of erroneous annota-
tion on classifier performance in an active learn-
ing (AL) setting is overdue. We need to know a)
whether the AL approach can really improve clas-
sifier performance and save annotation time when
applied in a real-world scenario with noisy data,
and b) whether AL works for classification tasks
with fine-grained or complex annotation schemes
and a low inter-annotator agreement.
In this paper we bring active learning to life in
the context of frame semantic annotation of Ger-
man texts within the SALSA project (Burchardt
et al, 2006). Specifically, we apply AL methods
for learning to assign semantic frames to predi-
cates, following Erk (2005) in treating frame as-
signment as a Word Sense Disambiguation task.
Under our fine-grained annotation scheme, anno-
tators have to deal with a high level of ambigu-
ity, resulting in low inter-annotator agreement for
some word senses. This fact, along with the po-
tential for wrong annotation decisions or possi-
ble biases from individual annotators, results in
an annotation environment in which we get noisy
data which might mislead the classifier. A sec-
ond characteristic of our scenario is that there is no
gold standard for the newly annotated data, which
means that evaluation is not straightforward. Fi-
nally, we have multiple annotators whose deci-
949
sions on particular instances may diverge, raising
the question of which annotations should be used
to guide the AL process. This paper thus investi-
gates whether active learning can be successfully
applied in a real-world scenario with the particular
challenges described above.
Section 2 of the paper gives a short overview
of the AL paradigm and some related work, and
Section 3 discusses the multi-annotator scenario.
In Section 4 we present our experimental design
and describe the data we use. Section 5 presents
results, and Section 6 concludes.
2 Active Learning
The active learning approach aims to reduce the
amount of manual annotation needed to create
training data sufficient for developing a classifier
with a given performance. At each iteration of
the AL cycle, the actual knowledge state of the
learner guides the learning process by determin-
ing which instances are chosen next for annota-
tion. The main goal is to advance the learning
process by selecting instances which provide im-
portant information for the machine learner.
In a typical active learning scenario, a small set
of manually labelled seed data serves as the ini-
tial training set for the classifier (learner). Based
on the predictions of the classifier, a large pool
of unannotated instances is queried for the next
instance (or batch of instances) to be presented
to the human annotator (sometimes called the or-
acle). The underlying active learning algorithm
controlling the learning process tries to select the
most informative instances in order to get a strong
boost in classifier performance. Different meth-
ods can be used for determining informativity of
instances. We use uncertainty sampling (Cohn et
al., 1995) in which ?most informative? instances
are those for which the classifier has the lowest
confidence in its label predictions. The rough in-
tuition behind this selection method is that it iden-
tifies instance types which have yet to be encoun-
tered by the classifier. The learning process pro-
ceeds by presenting the selected instances to the
human annotator, who assigns the correct label.
The newly-annotated instances are added to the
seed data and the classifier is re-trained on the new
data set. The newly trained classifier now picks
the next instances, based on its updated knowl-
edge, and the process repeats. If the learning pro-
cess can provide precisely that information which
the classifier still needs to learn, a smaller number
of instances should suffice to achieve the same ac-
curacy as on a larger training set of randomly se-
lected training examples.
Active learning has been applied to a num-
ber of natural language processing tasks like
POS tagging (Ringger et al, 2007), NER (Laws
and Schu?tze, 2008; Tomanek and Hahn, 2009),
syntactic parsing (Osborne and Baldridge, 2004;
Hwa, 2004), Word Sense Disambiguation (Chen
et al, 2006; Chan and Ng, 2007; Zhu and Hovy,
2007; Zhu et al, 2008) and morpheme gloss-
ing for language documentation (Baldridge and
Palmer, 2009). While most of these studies suc-
cessfully show that the same classification accu-
racy can be achieved with a substantially smaller
data set, these findings are mostly based on simu-
lations using gold standard data.
For our task of Word Sense Disambiguation
(WSD), mixed results have been achieved. AL
seems to improve results in a WSD task with
coarse-grained sense distinctions (Chan and Ng,
2007), but the results of (Dang, 2004) raise doubts
as to whether AL can successfully be applied to
a fine-grained annotation scheme, where Inter-
Annotator Agreement (IAA) is low and thus the
consistency of the human annotations decreases.
In general, AL has been shown to reduce the cost
of annotation when applied to classification tasks
where a single human annotator predicts labels for
new data points with a reasonable consistency and
accuracy. It is not clear whether the same settings
can be applied to a multi-annotator environment
where IAA is low.
3 Active Learning in a realistic task
including multiple annotators
Another possible difference between active learn-
ing simulations and real-world scenarios is the
multi-annotator environment. In such a setting,
two or more annotators assign labels to the same
instances, which are then merged to check for con-
flicting decisions from different annotators. This
is standard practise in many annotation projects
doing fine-grained semantic annotation with a
950
high level of ambiguity, and it necessitates that all
annotators work on the same data set.
Replicating an active learning simulation on
hand-corrected data, starting with a fixed set of
seed data and fixed parameter settings, using the
same algorithm, will always result in the same
training set selected from the pool. Human anno-
tators, however, will assign different labels to the
same instances, thus influencing the selection of
the next instance from the pool. This means that
individual annotators might end up with very dif-
ferent sets of annotated data, depending on factors
like their interpretation of the annotation guide-
lines, an implicit bias towards a particular label,
or simply errors made during annotation.
There is not much work addressing this prob-
lem. (Donmez and Carbonell, 2008) consider
modifications of active learning to accommodate
variability of annotators. (Baldridge and Palmer,
2009) present a real-world study with human an-
notators in the context of language documenta-
tion. The task consists of producing interlin-
ear glossed text, including morphological and
grammatical analysis, and can be described as
a sequence labelling task. Annotation cost is
measured as the actual time needed for annota-
tion. Among other settings, the authors compare
the performance of two annotators with different
grades of expertise. The classifier trained on the
data set created by the expert annotator in an ac-
tive learning setting does obtain a higher accuracy
on the gold standard. For the non-expert annota-
tor, however, the active learning setting resulted
in a lower accuracy than for a classifier trained on
a randomly selected data set. This finding sug-
gests that the quality of annotation needs to be
high enough for active learning to actually work,
and that annotation noise is a problem for AL.
There are two problems arising from this:
1. It is not clear whether active learning will
work when applied to noisy data
2. It is not straightforward to apply active learn-
ing to a real-world scenario, where low IAA
asks for multiple annotators
In our experiment we address these questions
by systematically investigating the impact of an-
notation noise on classifier performance and on
the composition of the training set. The next sec-
tion presents the experimental design and the data
used in our experiment.
4 Experimental Design
In the experiment we annotated 8 German cau-
sation nouns, namely Ausgang, Anlass, Ergeb-
nis, Resultat, Grund, Konsequenz, Motiv, Quelle
(outcome, occasion, effect, result, reason, con-
sequence, motive, source of experience). These
nouns were chosen because they exhibit a range
of difficulty in terms of the number of senses they
have in our annotation scheme. They all encode
subtle distinctions between different word senses,
but some of them are clearly easier to disam-
biguate than others. For instance, although Aus-
gang has 9 senses, they are easier to distinguish
for humans than the 4 senses of Konsequenz.
Six annotators participated in the experiment.
While all annotators were trained, having at least
one year experience in frame-semantic annota-
tion, one of the annotators is an expert with several
years of training and working experience in the
Berkeley FrameNet Project. This annotator also
defined the frames (word senses) used in our ex-
periment.
Prior to the experiment, all annotators were
given 100 randomly chosen sentences. After
annotating the training data, problematic cases
were discussed to make sure that the annotators
were familiar with the fine-grained distinctions
between word senses in the annotation scheme.
The data sets used for training were adjudicated
by two of the annotators (one of them being the
expert) and then used as a gold standard to test
classifier performance in the active learning pro-
cess.
4.1 Data and Setup
For each lemma we extracted sentences from the
Wahrig corpus1 containing this particular lemma.
The annotators had to assign word senses to 300
instances for each target word, split into 6 pack-
ages of 50 sentences each. This resulted in 2,400
annotated instances per annotator (14,400 anno-
tated instances in total). The annotation was done
1The Wahrig corpus includes more than 113 mio. sen-
tences from German newspapers and magazines covering
topics such as politics, science, fashion, and others.
951
Anlass Motiv Konsequenz Quelle Ergebnis / Resultat Ausgang Grund
Occasion (37) Motif (47) Causation (32) Relational nat feat.(3) Causation (4/10) Outcome (67) Causation (24)
Reason (63) Reason(53) Level of det.(6) Source of getting (14) Competitive score(12/36) Have leave (4) Reason (58)
Response (61) Source of exp. (14) Decision (11/6) Portal (21) Death (1)
MWE1 (1) Source of info. (56) Efficacy (2/3) Outgoing goods (4) Part orientation. (0)
Well (6) Finding out (24/23) Ostomy (0) Locale by owner(3)
Emissions source (7) Mathematics (1/0) Origin (5) Surface earth (0)
Operating result (36/5) Tech output (7) Bottom layer (0)
Outcome (10/17) Process end (2) Soil (1)
Departing (1) CXN1 (0)
CXN2 (0)
MWE1 (0)
MWE2 (10)
MWE3 (0)
MWE4 (3)
MWE5 (0)
MWE6 (0)
Fleiss? kappa for the 6 annotators for the 150 instances annotated in the random setting
0.67 0.79 0.55 0.77 0.63 / 0.59 0.82 0.43
Table 1: 8 causation nouns and their word senses (numbers in brackets give the distribution of word
senses in the gold standard (100 sentences); CXN: constructions, MWE: multi-word expressions; note
that Ergebnis and Resultat are synonyms and therefore share the same set of frames.)
using a Graphical User Interface where the sen-
tence was presented to the annotator, who could
choose between all possible word senses listed in
the GUI. The annotators could either select the
frame by mouse click or use keyboard shortcuts.
For each instance we recorded the time it took
the annotator to assign an appropriate label. To
ease the reading process the target word was high-
lighted.
As we want to compare time requirements
needed for annotating random samples and sen-
tences selected by active learning, we had to con-
trol for training effects which might speed up the
annotation. Therefore we changed the annotation
setting after each package, meaning that the first
annotator started with 50 sentences randomly se-
lected from the pool, then annotated 50 sentences
selected by AL, followed by another 50 randomly
chosen sentences, and so on. We divided the an-
notators into two groups of three annotators each.
The first group started annotating in the random
setting, the second group in the AL setting. The
composition of the groups was changed for each
lemma, so that each annotator experienced all dif-
ferent settings during the annotation process. The
annotators were not aware of which setting they
were in.
Pool data For the random setting we randomly
selected three sets of sentences from the Wahrig
corpus which were presented for annotation to all
six annotators. This allows us to compare annota-
tion time and inter-annotator agreement between
the annotators. For the active learning setting we
randomly selected three sets of 2000 sentences
each, from which the classifier could pick new in-
stances during the annotation process. This means
that for each trial the algorithm could select 50 in-
stances out of a pool of 2000 sentences. On any
given AL trial each annotator uses the same pool
as all the other annotators. In an AL simulation
with fixed settings and gold standard labels this
would result in the same subset of sentences se-
lected by the classifier. For our human annotators,
however, due to different annotation decisions the
resulting set of sentences is expected to differ.
Sampling method Uncertainty sampling is a
standard sampling method for AL where new in-
stances are selected based on the confidence of the
classifier for predicting the appropriate label. Dur-
ing early stages of the learning process when the
classifier is trained on a very small seed data set,
it is not beneficial to add the instances with the
lowest classifier confidence. Instead, we use a dy-
namic version of uncertainty sampling (Rehbein
and Ruppenhofer, 2010), based on the confidence
of a maximum entropy classifier2, taking into ac-
count how much the classifier has learned so far.
In each iteration one new instance is selected from
the pool and presented to the oracle. After anno-
tation the classifier is retrained on the new data
set. The modified uncertainty sampling results in
a more robust classifier performance during early
stages of the learning process.
2http://maxent.sourceforge.net
952
Anlass Motiv Konsequenz Quelle Ergebnis Resultat Ausgang Grund
R U R U R U R U R U R U R U R U
A1 8.6 9.6 5.9 6.6 10.7 10.5 6.0 4.8 10.5 7.4 10.1 9.6 6.4 10.0 10.2 11.1
A2 4.4 5.7 4.8 5.9 8.2 9.2 4.9 4.9 6.4 4.4 11.7 8.5 5.1 7.7 9.0 9.3
A3 9.9 9.2 6.8 6.7 6.8 8.3 7.4 6.1 9.4 7.6 9.0 12.3 7.5 8.5 11.7 10.2
A4 5.8 4.9 3.6 3.6 9.9 11.3 4.8 3.5 7.9 7.1 9.7 11.1 3.6 4.1 9.9 9.4
A5 3.0 3.5 3.0 2.6 4.8 4.9 3.8 3.0 6.8 4.8 6.7 6.1 3.1 3.5 6.3 6.0
A6 5.4 6.3 5.3 4.7 6.7 8.6 5.4 4.6 7.8 6.1 8.7 9.0 6.9 6.6 9.3 8.5
? 6.2 6.5 4.9 5.0 7.8 8.8 5.4 4.5 8.1 6.2 9.3 9.4 5.4 6.7 9.4 9.1
sl 25.8 27.8 27.8 26.0 24.2 25.8 24.9 26.5 25.7 25.2 29.0 35.9 25.5 27.9 26.8 29.7
Table 2: Annotation time (sec/instance) per target/annotator/setting and average sentence length (sl)
5 Results
The basic idea behind active learning is to se-
lect the most informative instances for annotation.
The intuition behind ?more informative? is that
these instances support the learning process, so we
might need fewer annotated instances to achieve
a comparable classifier performance, which could
decrease the cost of annotation. On the other
hand, ?more informative? also means that these
instances might be more difficult to annotate, so it
is only fair to assume that they might need more
time for annotation, which increases annotation
cost. To answer the question of whether AL re-
duces annotation cost or not we have to check a)
how long it took the annotators to assign labels
to the AL samples compared to the randomly se-
lected instances, and b) how many instances we
need to achieve the best (or a sufficient) perfor-
mance in each setting. Furthermore, we want to
investigate the impact of active learning on the
distribution of the resulting training sets and study
the correlation between the performance of the
classifier trained on the annotated data and these
factors: the difficulty of the annotation task (as-
sessed by IAA), expertise and individual proper-
ties of the annotators.
5.1 Does AL speed up the annotation process
when working with noisy data?
Table 2 reports annotation times for each annota-
tor and target for random sampling (R) and uncer-
tainty sampling (U). For 5 out of 8 targets the time
needed for annotating in the AL setting (averaged
over all annotators) was higher than for annotat-
ing the random samples. To investigate whether
this might be due to the length of the sentences
in the samples, Table 2 shows the average sen-
tence length for random samples and AL samples
for each target lemma. Overall, the sentences se-
lected by the classifier during AL are longer (26.2
vs. 28.1 token per sentence), and thus may take
the annotators more time to read.3 However, we
could not find a significant correlation (Spearman
rank correlation test) between sentence length and
annotation time, nor between sentence length and
classifier confidence.
The three target lemmas which took longer to
annotate in the random setting are Ergebnis (re-
sult), Grund (reason) and Quelle (source of expe-
rience). This observation cannot be explained by
sentence length. While sentence length for Ergeb-
nis is nearly the same in both settings, for Grund
and Quelle the sentences picked by the classi-
fier in the AL setting are significantly longer and
therefore should have taken more time to anno-
tate. To understand the underlying reason for this
we have to take a closer look at the distribution of
word senses in the data.
5.2 Distribution of word senses in the data
In the literature it has been stated that AL implic-
itly alleviates the class imbalance problem by ex-
tracting more balanced data sets, while random
sampling tends to preserve the sense distribution
present in the data (Ertekin et al, 2007). We could
not replicate this finding when using noisy data
to guide the learning process. Table 3 shows the
distribution of word senses for the target lemma
Ergebnis a) in the gold standard, b) in the random
samples, and c) in the AL samples.
The variance in the distribution of word senses
in the random samples and the gold standard can
3The correlation between sentence length and annotation
time is not obvious, as the annotators only have to label one
target in each sentence. For ambiguous sentences, however,
reading time may be longer, while for the clear cases we do
not expect a strong effect.
953
Ergebnis
Frame gold (%) R (%) U (%)
Causation 4.0 4.8 3.7
Outcome 10.0 17.8 10.5
Finding out 24.0 26.2 8.2
Efficacy 2.0 0.8 0.1
Decision 11.0 5.1 3.2
Mathematics 1.0 1.6 0.4
Operating result 36.0 24.5 66.7
Competitive score 12.0 19.2 7.2
Table 3: Distribution of frames (word senses) for
the lemma Ergebnis in the gold standard (100 sen-
tences), in the random samples (R) and AL sam-
ples (U) (150 sentences each)
be explained by low inter-annotator agreement
caused by the high level of ambiguity for the tar-
get lemmas. The frame distribution in the data
selected by uncertainty sampling, however, cru-
cially deviates from those of the gold standard
and the random samples. A disproportionately
high 66% of the instances selected by the classi-
fier have been assigned the label Operating result
by the human annotators. This is the more sur-
prising as this frame is fairly easy for humans to
distinguish.
The classifier, however, proved to have seri-
ous problems learning this particular word sense
and thus repeatedly selected more instances of this
frame for annotation. As a result, the distribution
of word senses in the training set for the uncer-
tainty samples is highly skewed, having a nega-
tive effect on the overall classifier performance.
The high percentage of instances of the ?easy-to-
decide? frame Operating result explains why the
instances for Ergebnis took less time to annotate
in the AL setting. Thus we can conclude that an-
notating the same number of instances on average
takes more time in the AL setting, and that this
effect is not due to sentence length.
5.3 What works, what doesn?t, and why
For half of the target lemmas (Motiv, Konsequenz,
Quelle, Ausgang), we did obtain best results in
the AL setting (Table 4). For Ausgang and Mo-
tiv AL gives a substantial boost in classifier per-
formance of 5% and 7% accuracy, while the gains
for Konsequenz and Quelle are somewhat smaller
with 2% and 1%, and for Grund the highest accu-
racy was reached on both the AL and the random
Random Uncertainty
50 100 150 50 100 150
Anlass 0.85 0.86 0.85 0.84 0.85 0.84
Motiv 0.57 0.62 0.63 0.64 0.67 0.70
Konseq. 0.55 0.59 0.60 0.61 0.62 0.62
Quelle 0.56 0.53 0.54 0.52 0.52 0.57
Ergebnis 0.39 0.42 0.41 0.39 0.37 0.38
Resultat 0.31 0.35 0.37 0.32 0.34 0.34
Ausgang 0.67 0.69 0.69 0.68 0.72 0.74
Grund 0.48 0.47 0.47 0.47 0.44 0.48
Table 4: Avg. classifier performance (acc.) over
all annotators for the 8 target lemmas when train-
ing on 50, 100 and 150 annotated instances for
random samples and uncertainty samples
sample.
Figure 1 (top row) shows the learning curves
for Resultat, our worst-performing lemma, for the
classifier trained on the manually annotated sam-
ples for each individual annotator. The solid black
line represents the majority baseline, obtained by
assigning the most frequent word sense in the gold
standard to all instances. For both random and AL
settings, results are only slightly above the base-
line. The curves for the AL setting show how erro-
neous decisions can mislead the classifier, result-
ing in classifier accuracy below the baseline for
two of the annotators, while the learning curves
for these two annotators on the random samples
show the same trend as for the other 4 annotators.
For Konsequenz (Figure 1, middle), the classi-
fier trained on the AL samples yields results over
the baseline after around 25 iterations, while in
the random sampling setting it takes at least 100
iterations to beat the baseline. For Motiv (Figure
1, bottom row), again we observe far higher re-
sults in the AL setting. A possible explanation for
why AL seems to work for Ausgang, Motiv and
Quelle might be the higher IAA4 (? 0.825, 0.789,
0.768) as compared to the other target lemmas.
This, however, does not explain the good results
achieved on the AL samples for Konsequenz, for
which IAA was quite low with ? 0.554.
Also startling is the fact that AL seems to work
particularly well for one of the annotators (A6,
Figure 1) but not for others. Different possible ex-
planations come to mind: (a) the accuracy of the
annotations for this particular annotator, (b) the
4IAA was computed on the random samples, as the AL
samples do not include the same instances.
954
0 50 100 150
0.
10
0.
20
0.
30
0.
40
Resultat (Random Sampling)
no. of iterations
a
cc
u
ra
cy
Annotator
A1
A2
A3
A4
A5
A6
0 50 100 150
0.
10
0.
20
0.
30
0.
40
Resultat (Uncertainty Sampling)
no. of iterations
a
cc
u
ra
cy
0 50 100 150
0.
3
0.
4
0.
5
0.
6
0.
7
Konsequenz (Random Sampling)
no. of iterations
a
cc
u
ra
cy
0 50 100 150
0.
3
0.
4
0.
5
0.
6
0.
7
Konsequenz (Uncertainty Sampling)
no. of iterations
a
cc
u
ra
cy
0 50 100 150
0.
50
0.
60
0.
70
Motiv (Random Sampling)
no. of iterations
a
cc
u
ra
cy
0 50 100 150
0.
50
0.
60
0.
70
Motiv (Uncertainty Sampling)
no. of iterations
a
cc
u
ra
cy
Figure 1: Active learning curves for Resultat, Konsequenz and Motiv (random sampling versus uncer-
tainty sampling; the straight black line shows the majority baseline)
955
Konsequenz A1 A2 A3 A4 A5 A6
human 0.80 0.72 0.89 0.73 0.89 0.76
maxent 0.60 0.63 0.67 0.60 0.63 0.64
Table 5: Acc. for human annotators against the
adjudicated random samples and for the classifier
instances selected by the classifier based on the
annotation decisions of the individual annotators,
and (c) the distribution of frames in the annotated
training sets for the different annotators.
To test (a) we evaluated the annotated ran-
dom samples for Konsequenz for each annotator
against the adjudicated gold standard. Results
showed that there is no strong correlation between
the accuracy of the human annotations and the
performance of the classifier trained on these an-
notations. The annotator for whom AL worked
best had a medium score of 0.76 only, while the
annotator whose annotations were least helpful
for the classifier showed a good accuracy of 0.80
against the gold standard.
Next we tested (b) the impact of the particu-
lar instances in the AL samples for the individ-
ual annotators on classifier performance. We took
all instances in the AL data set from A6, whose
annotations gave the greatest boost to the clas-
sifier, removed the frame labels and gave them
to the remaining annotators for re-annotation.
Then we trained the classifier on each of the re-
annotated samples and compared classifier perfor-
mance. Results for 3 of the remaining annotators
were in the same range or even higher than the
ones for A6 (Figure 2). For 2 annotators, however,
results remained far below the baseline.
This again shows that the AL effect is not di-
rectly dependent on the accuracy of the individual
annotators, but that particular instances are more
informative for the classifier than others. Another
crucial point is (c) the distribution of frames in
the samples. In the annotated samples for A1 and
A2 the majority frame for Konsequenz is Causa-
tion, while in the samples for the other annotators
Response was more frequent. In our test set Re-
sponse also is the most frequent frame, therefore it
is not surprising that the classifiers trained on the
samples of A3 to A6 show a higher performance.
This means that high-quality annotations (identi-
fied by IAA) do not necessarily provide the in-
0 50 100 150
0.
3
0.
4
0.
5
0.
6
0.
7
Konsequenz: Re?annotated samples
no. of iterations
ac
cu
ra
cy
Annotator
A1
A2
A3
A4
A5
A6
Figure 2: Re-annotated instances for Konsequenz
(AL samples from annotator A6)
formation from which the classifier benefits most,
and that in a realistic annotation task address-
ing the class imbalance problem (Zhu and Hovy,
2007) is crucial.
6 Conclusions
We presented the first experiment applying AL in
a real-world scenario by integrating the approach
in an ongoing annotation project. The task and
annotation environment pose specific challenges
to the AL paradigm. We showed that annotation
noise caused by biased annotators as well as erro-
neous annotations mislead the classifier and result
in skewed data sets, and that for this particular task
no time savings are to be expected when applied
to a realistic scenario. Under certain conditions,
however, classifier performance can improve over
the random sampling baseline even on noisy data
and thus yield higher accuracy in the active learn-
ing setting. Critical features which seem to influ-
cence the outcome of AL are the amount of noise
in the data as well as the distribution of frames
in training- and test sets. Therefore, addressing
the class imbalance problem is crucial for apply-
ing AL to a real annotation task.
956
Acknowledgments
This work was funded by the German Research
Foundation DFG (grant PI 154/9-3 and the MMCI
Cluster of Excellence).
References
Baldridge, Jason and Alexis Palmer. 2009. How well
does active learning actually work?: Time-based
evaluation of cost-reduction strategies for language
documentation. In Proceedings of EMNLP 2009.
Burchardt, Aljoscha, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pado?, and Manfred Pinkal.
2006. The salsa corpus: a german corpus resource
for lexical semantics. In Proceedings of LREC-
2006.
Chan, Yee Seng and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense dis-
ambiguation. In Proceedings of ACL-2007.
Chen, Jinying, Andrew Schein, Lyle Ungar, and
Martha Palmer. 2006. An empirical study of the
behavior of active learning for word sense disam-
biguation. In Proceedings of NAACL-2006, New
York, NY.
Cohn, David A., Zoubin Ghahramani, and Michael I.
Jordan. 1995. Active learning with statistical mod-
els. In Tesauro, G., D. Touretzky, and T. Leen, ed-
itors, Advances in Neural Information Processing
Systems, volume 7, pages 705?712. The MIT Press.
Dang, Hoa Trang. 2004. Investigations into the role
of lexical semantics in word sense disambiguation.
PhD dissertation, University of Pennsylvania, Penn-
sylvania, PA.
Donmez, Pinar and Jaime G. Carbonell. 2008. Proac-
tive learning: Cost-sensitive active learning with
multiple imperfect oracles. In Proceedings of
CIKM08.
Erk, Katrin. 2005. Frame assignment as word sense
disambiguation. In Proceedings of the IWCS-6.
Ertekin, S?eyda, Jian Huang, L?eon Bottou, and Lee
Giles. 2007. Learning on the border: active learn-
ing in imbalanced data classification. In Proceed-
ings of CIKM ?07.
Hwa, Rebecca. 2004. Sample selection for statisti-
cal parsing. Computational Linguistics, 30(3):253?
276.
Laws, Florian and Heinrich Schu?tze. 2008. Stopping
criteria for active learning of named entity recogni-
tion. In Proceedings of Coling 2008.
Osborne, Miles and Jason Baldridge. 2004.
Ensemble-based active learning for parse selection.
In Proceedings of HLT-NAACL 2004.
Rehbein, Ines and Josef Ruppenhofer. 2010. Theres
no data like more data? revisiting the impact of
data size on a classification task. In Proceedings
of LREC-07, 2010.
Reichart, Roi, Katrin Tomanek, Udo Hahn, and Ari
Rappoport. 2008. Multi-task active learning for
linguistic annotations. In Proceedings of ACL-08:
HLT.
Ringger, Eric, Peter Mcclanahan, Robbie Haertel,
George Busby, Marc Carmen, James Carroll, and
Deryle Lonsdale. 2007. Active learning for part-
of-speech tagging: Accelerating corpus annotation.
In Proceedings of ACL Linguistic Annotation Work-
shop.
Tomanek, Katrin and Udo Hahn. 2009. Reducing
class imbalance during active learning for named
entity annotation. In Proceedings of the 5th Interna-
tional Conference on Knowledge Capture, Redondo
Beach, CA.
Tomanek, Katrin, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction
which cuts annotation costs and maintains corpus
reusability of annotated data. In Proceedings of
EMNLP-CoNLL 2007.
Zhu, Jingbo and Ed Hovy. 2007. Active learning
for word sense disambiguation with methods for ad-
dressing the class imbalance problem. In Proceed-
ings of EMNLP-CoNLL 2007.
Zhu, Jingbo, Huizhen Wang, Tianshun Yao, and Ben-
jamin K. Tsou. 2008. Active learning with sam-
pling by uncertainty and density for word sense dis-
ambiguation and text classification. In Proceedings
of Coling 2008.
957
Coling 2010: Poster Volume, pages 928?936,
Beijing, August 2010
Evaluating FrameNet-style semantic parsing:
the role of coverage gaps in FrameNet
Alexis Palmer and Caroline Sporleder
Computational Linguistics
Saarland University
{apalmer, csporled}@coli.uni-saarland.de
Abstract
Supervised semantic role labeling (SRL)
systems are generally claimed to have ac-
curacies in the range of 80% and higher
(Erk and Pado?, 2006). These numbers,
though, are the result of highly-restricted
evaluations, i.e., typically evaluating on
hand-picked lemmas for which training
data is available. In this paper we con-
sider performance of such systems when
we evaluate at the document level rather
than on the lemma level. While it is well-
known that coverage gaps exist in the re-
sources available for training supervised
SRL systems, what we have been lacking
until now is an understanding of the pre-
cise nature of this coverage problem and
its impact on the performance of SRL sys-
tems. We present a typology of five differ-
ent types of coverage gaps in FrameNet.
We then analyze the impact of the cov-
erage gaps on performance of a super-
vised semantic role labeling system on full
texts, showing an average oracle upper
bound of 46.8%.
1 Introduction
A lot of progress has been made in semantic
role labeling over the past years, but the per-
formance of state-of-the-art systems is still rel-
atively low, especially for deep, FrameNet-style
semantic parsing. Furthermore, many of the re-
ported performance figures are somewhat unre-
alistic because system performance is evaluated
on hand-selected lemmas, usually under the im-
plicit assumptions that (i) all relevant word senses
(frames) of each lemma are known, and (ii) there
is a suitable amount of training data for each
sense. This approach to evaluation arises from the
limited coverage of the available hand-coded data
against which to evaluate. More realistic evalua-
tions test systems on full text, but these same cov-
erage limitations mean that the assumptions made
in more restricted evaluations do not necessarily
hold for full text. This paper provides an analysis
of the extent and nature of the coverage gaps in
FrameNet. A more precise understanding of the
limitations of existing resources with respect to
robust semantic analysis of texts is an important
foundational component both for improving ex-
isting systems and for developing future systems,
and it is in this spirit that we make our analysis.
Full-text semantic analysis
Automated frame-semantic analysis aims to ex-
tract from text the key event-denoting predicates
and the semantic argument structure for those
predicates. The semantic argument structure of
a predicate describing an event encodes relation-
ships between the participants involved in the
event, e.g. who did what to whom. Knowledge of
semantic argument structure is essential for lan-
guage understanding and thus important for ap-
plications such as information extraction (Mos-
chitti et al, 2003; Surdeanu et al, 2003), ques-
tion answering (Shen and Lapata, 2007), or recog-
nizing textual entailment (Burchardt et al, 2009).
Evaluating an existing system for its ability to aid
such tasks is unrealistic if the evaluation is lemma-
based rather than text-based. Consequently, there
continues to be significant interest in developing
semantic role labeling (SRL) systems able to au-
tomatically compute the semantic argument struc-
tures in an input text.
Performance on the full text task, though, is
typically much lower than for the more restricted
evaluations. The SemEval 2007 Task on ?Frame
Semantic Structure Extraction,? for example, re-
quired systems to identify key predicates in texts,
928
assign a semantic frame to the relevant predi-
cates, identify the semantic arguments for the
predicates, and finally label those arguments with
their semantic roles. The systems participating
in this task only obtained F-Scores between 55%
and 78% for frame assignment, despite the fact
that the task organizers adopted a lenient evalu-
ation scheme which gave partial credit for near-
misses (Baker et al, 2007). For the combined task
of frame assigment and role labeling the perfor-
mance was even lower, ranging from 35% to 54%
F-Score.
Note that this distinction between evaluation
schemes for SRL systems corresponds to the dis-
tinction between ?lexical sample? and ?all words?
evaluations in word sense disambiguation, where
results for the latter scheme are also typically
lower (McCarthy, 2009).
The low performances are at least partly due
to coverage problems. For example, Baker et
al. (2007) annotated three new texts for their
SemEval 2007 task. Although these new texts
overlap in domain with existing FrameNet data,
the task organizers had to create 40 new frames
in order to complete annotation. The new frames
were for word senses found in the test set but
missing from FrameNet. The test set contained
only 272 frames (types), meaning that nearly 15%
of the frames therein were not yet defined in
FrameNet. Obviously, coverage issues of this de-
gree make full SRL a difficult task, but this is a
realistic scenario that will be encountered in real
applications as well.
As mentioned above, for many tasks it is neces-
sary to compute the semantic argument structures
for the whole text, or at least for multi-sentence
passages. Due to non-local relations between ar-
gument structures this is also true for tasks like
question answering, where it might be possible
to automatically determine a subset of lemmas
which are relevant for the task. For example, in (1)
it might be possible to determine that the second
sentence contains the answer to the question ?Was
Thomas Preston acquitted of theft?? However,
to correctly answer this question, it is necessary
to resolve the null instantiation of the CHARGES
role of the VERDICT frame. This null instantiation
links back to the previous sentence, and resolving
it might require obtaining an analysis of the word
tried.
(1) [Captain Thomas Preston]Defendantiwas triedTry defendanti for[murder]Chargesi,j .
In the end [he]Defendantj was
acquittedVerdictj [?]Chargesj .
Performance levels obtained for full text are
usually not sufficient for this kind of real-world
task. FrameNet-style semantic role labeling has
been shown to, in principle, be beneficial for ap-
plications that need to generalise over individual
lemmas, such as recognizing textual entailment or
question answering. However, studies also found
that state-of-the-art FrameNet-style SRL systems
perform too poorly to provide any substantial ben-
efit to real applications (Burchardt et al, 2009;
Shen and Lapata, 2007).
Extending the value of automated semantic
parsing for a variety of applications requires im-
proving the ability of systems to process unre-
stricted text. Several methods have been pro-
posed to address different aspects of the cover-
age problem, ranging from automatic data expan-
sion and semi-supervised semantic role labelling
(Fu?rstenau and Lapata, 2009b; Fu?rstenau and La-
pata, 2009a; Deschacht and Moens, 2009; Gordon
and Swanson, 2007; Pado? et al, 2008) to systems
which can infer missing word senses (Pennac-
chiotti et al, 2008b; Pennacchiotti et al, 2008a;
Cao et al, 2008; Burchardt et al, 2005). How-
ever, so far there has not been a detailed analysis
of the problem. In this paper we provide that de-
tailed analysis, by defining different types of cov-
erage problems and performing analysis of both
coverage and performance of an automated SRL
system on three different data sets.
Section 2 of the paper provides an introduction
to FrameNet and introduces the basic terminol-
ogy. Section 4 describes our approach to coverage
evaluation, Section 3 discusses the texts analyzed,
and the analysis itself appears in Section 5. Sec-
tion 6 then looks at one possibility for addressing
the coverage problem. The final section presents
some discussion and conclusions.
929
(a) (b)
Figure 1: Terminology: (a) Frame with core frame elements (FEs) and frame-evoking elements (FEEs)
(b) Target with possible frame assignments and resultant lexical units (LUs)
2 FrameNet
Manual annotation of corpora with semantic ar-
gument structure information has enabled the de-
velopment of statistical and supervised machine
learning techniques for semantic role labeling
(Toutanova et al, 2008; Moschitti et al, 2008;
Gildea and Jurafsky, 2002).
The two main resources are PropBank (Palmer
et al, 2005) and FrameNet (Ruppenhofer et al,
2006). PropBank aims to provide a semantic role
annotation for every verb in the Penn TreeBank
(Marcus et al, 1994) and assigns roles on a verb-
by-verb basis, without making higher-level gener-
alizations. Whether two distinct usages of a given
verb are viewed as different senses or not is thus
driven by both syntax (namely, differences in syn-
tactic argument structure) and semantics (via ba-
sic, easily-discernable differences in meaning).
FrameNet1 is a lexicographic project whose
aim it is to create a lexical resource documenting
valence structures for different word senses and
their possible mappings to underlying semantic
argument structure (Ruppenhofer et al, 2006). In
contrast to PropBank, FrameNet is primarily se-
mantically driven; word senses (frames)2 are de-
fined mainly based on sometimes-subtle meaning
differences and can thus generalise across individ-
ual lemmas, and often also across different parts-
of-speech. Because FrameNet focusses on seman-
tics it is not restricted to verbs but also provides
1http://framenet.icsi.berkeley.edu/
2We follow Erk (2005) in treating frame assignment as a
word sense disambiguation task. Thus in this paper we use
the terms frame and sense interchangeably.
semantic argument annotations for nouns, adjec-
tives, adverbs, prepositions and even multi-word
expressions. For example, the sentence in (2) and
the NP in (3) have identical argument structures
because the verb speak and the noun comment
evoke the same frame STATEMENT.
(2) [The politician]Speaker spokeStatement
[about recent developments on the labour
market]Topic.
(3) [The politician?s]Speaker com-
mentsStatement [on recent developments
on the labour market]Topic
Since FrameNet annotations are semanti-
cally driven they are considerably more time-
consuming to create than PropBank annotations.
However, FrameNet alo provides ?deeper? and
more informative annotations than PropBank
analyses (Ellsworth et al, 2004). For instance,
the fact that (2) and (3) refer to the same state-
of-affairs is not captured by PropBank sense dis-
tinctions.
FrameNet Terminology
The English FrameNet data consist of an inven-
tory of frames (i.e. word senses), a set of lexi-
cal entries, and a set of annotated examples ex-
emplifying different syntactic realizations for se-
lected frames (known as the lexicographic anno-
tations). Frames are conceptual structures that
describe types of situations or events together
with their participants. Frame-evoking elements
(FEEs) are predicate usages which evoke a par-
ticular frame. A given lemma can evoke different
930
frames in different contexts; each instance of the
lemma is a separate target for semantic analysis.
For example, (4) and (5) illustrate two different
frames of the lemma speak.
(4) [The politician]Speaker spokeStatement
[about recent developments on the labour
market]Topic.
(5) [She]Interlocutor1 doesn?t speakChatting
to [anyone]Interlocutor2 .
In this paper we follow standard use of
FrameNet terminology, with the possible excep-
tion of the term lexical unit. Figure 1 illus-
trates our use of FrameNet-related terminology,
focussing on (a) the CAUSE TO MAKE NOISE
frame and (b) the target verb lemma ring.
The definition of a frame determines the avail-
able roles (frame elements or FEs) of the se-
mantic argument structure for the particular use
of the predicate, as well as the status?core or
peripheral?of those roles. For example, the FE
TOPIC is a core role under the STATEMENT frame,
but a peripheral role under the CHATTING frame.
The lexical entry of a lemma in FrameNet spec-
ifies a list of frames which the lemma can evoke,
and the pairing of a word with a particular frame is
called a lexical unit (LU). Ideally there should be
annotated examples for each lexical unit, exem-
plifying different syntactic constructions which
can realize this LU. However, as we will see
later (Section 5) annotated examples can be miss-
ing. Also, because FrameNet is a lexicographic
project, the examples were extracted to illustrate
particular usages, i.e., they are not meant to be sta-
tistically representative.
3 Data
Having introduced the basic FrameNet terminol-
ogy, we now describe in more detail the data
sets used in the analysis. FrameNet Release 1.3
(FN1.3), the latest release from the Berkeley
FrameNet project, includes both a corpus of lex-
icographic annotations (FNL), which we referred
to in Section 2, and a corpus of texts fully-
annotated with frames and semantic role labels
(FNF). Annotations in the two corpora of course
cover different sets of predicates and frames, and
FNL is the corpus commonly used as the basis for
training supervised FrameNet-based SRL systems
(Erk and Pado?, 2006).
In our analysis, we look at three data sets: the
lexicographic annotations from FN1.3, the full
text annotations from FN1.3, and a new data set
of running text that was annotated for the SemEval
2010 Task-10 (see Table 1 for details).
FrameNet Lexicographic (FNL) FrameNet
started as a lexicographic project, aiming to draw
up an inventory of frames and lexical units, sup-
ported by corpus evidence, to document the range
of syntactic and semantic usages of each lexical
unit. The annotated example sentences in this part
of FN1.3 are taken from the British National Cor-
pus (BNC). BNC is a balanced corpus, hence FNL
covers, in principle, a variety of domains.
For each LU, a subset of the sentences in which
it occurs was selected for annotation, and in each
extracted sentence, only the target LU was anno-
tated. The sentences were not chosen randomly
but with a set of lexicographic constraints in mind.
In particular the sentences should exemplify dif-
ferent usage. Thus ideally selected sentences
would be easy to understand and not too long or
complex. As a consequence of this linguistically-
driven selection procedure, the annotated sen-
tences are not statistically representative in any
way. FNL provides annotations for just under
140,000 FEEs (tokens). On average, around 20
sentences are annotated for each LU. FrameNet?s
frame inventory contains 722 frames.3
FrameNet Full Texts (FNF) Starting with re-
lease 1.3, FrameNet alo provides annotations of
running texts. In this annotation mode, all LUs
in a sentence and all sentences in a text are an-
notated. FN1.3 contains two subsets of full text
annotations. The first of these (PB) contains five
texts which were also annotated by the PropBank
project. While all texts come from the Wall Street
Journal, they are not prototypical examples of the
financial domain, rather they are longer essays
covering a wide variety of general interest topics
3Only lexical frames are included in this number. In addi-
tion to those, FrameNet 1.3 defines another 74 frames which
cannot be lexicalised but are included because they provide
useful generalisations in the frame hierarchy.
931
FEEs Frames
Data Genre / Domain Tokens Types Types
FNL mixed 139,439 8370 722
PB essays, general interest 1580 680 319
NTI reports, foreign affairs 8271 1305 434
SE fiction, crime 1530 680 320
Table 1: Statistics for the three data sets
(ranging from ?Bell Ringing? to ?Earthquakes?).
The second subset (NTI) contains 12 texts from
the Nuclear Threat Initiative website.4 These texts
are intelligence reports which summarize and dis-
cuss the status of various countries with regard to
the development of weapons and missile systems.
Statistics for both data sets are given in Table 1.
SemEval 2010 Task-10 Full Texts (SE) While
the FrameNet full texts allow us to estimate cover-
age gaps that arise from limited training data, they
do not allow us to gauge coverage problems aris-
ing from missing frames in the FN1.3 inventory.
The reason for this is that the frame inventory re-
flects the annotations of both the lexicographic
and the full text part of FN1.3, i.e., every frame
annotated in one of these subsets will also be part
of the inventory. To estimate the frame coverage
problem on completely new texts, we therefore in-
cluded a third (full text) data set that was anno-
tated for the SemEval 2010 Task 10 on ?Linking
Events and Their Participants in Discourse? (Rup-
penhofer et al, 2009).5 The text is taken from
Arthur Conan Doyle?s ?The Adventure of Wiste-
ria Lodge?. It thus comes from the fiction domain.
The text was manually annotated with frame-
semantic argument structure by two experienced
annotators. Similar to the FNF texts, the annota-
tors aimed to annotate all LUs in the text. To do
so, some new frames had to be created for pre-
viously un-encountered LUs. These new frames
are not part of FN1.3 and we can thus use them to
estimate coverage problems arising from missing
frames. Details for the data set can be found in
Table 1. This data set is very similar to the PB set
in terms of size, FEE type-token ratio and number
of frames (types).
4http://www.nti.org
5The data set is available from http://semeval2.
fbk.eu/semeval2.php?location=data.
4 Types of Coverage Gaps
Semantic role labelling systems have to perform
two sub-tasks: (i) identifying the correct frame
for a given lemma and context, and (ii) identifying
and labeling the frame elements. The most severe
coverage problems typically arise with the first
subtask. Furthermore, coverage problems related
to frame identification have a knock-on effect on
role identification and labeling because the choice
of the correct frame determines which roles are
available. Therefore, we focus on the frame iden-
tification task in this paper.
Attempts to do automated frame assignment
on unrestricted text invariably encounter prob-
lems associated with limited coverage of frame-
evoking elements in FrameNet. However, not ev-
ery coverage gap is the same, and the precise na-
ture of a coverage gap influences potential strate-
gies for addressing it. In this section we describe
the different types of coverage gaps. We pro-
ceed from less problematic coverage gaps to more
problematic ones, in the sense that the former can
be addressed more straighforwardly by automated
systems than can the latter.
4.1 NOTR gaps
Some coverage gaps occur when lexical units
(LUs) defined in FrameNet lack corresponding
annotated examples; these gaps are the result of
lacking training data, hence we call them NOTR
gaps. To give a sense of the abundance of such
gaps, of the 10,191 LUs defined in FN1.3, anno-
tated examples are available for only 6727.
NOTR-LU: lexical unit with no training data.
In many cases, an LU ? a specific pairing of a
target lemma with one frame ? may be defined
in FrameNet, thus potentially accessible to an
automated system, but lacking labeled training
material. For example, FrameNet defines two
LUs for the noun ringer: with the frames CAUSE
TO MAKE NOISE and SIMILARITY. It is clear
that the occurrence of ringer in (6) belongs to
the former LU, even given a very limited context.
The lexicographic annotations, though, provide
training material only for the SIMILARITY frame.
932
(6) Then, at a signal, the ringers begin vary-
ing the order in which the bells sound
without altering the steady rhythm of the
striking.
NOTR-LU gaps pose particular problems to a
fully-supervised SRL system, because such a sys-
tem cannot learn anything about the context in
which the CAUSE TO MAKE NOISE frame is more
appropriate. A NOTR-LU gap is identified for
an LU even if training data is available for other
senses (i.e. other LUs) of the target lemma.
NOTR-TGT: target with no training data. In
other cases, a target lemma may be defined as par-
ticipating in one or more LUs, but with no training
data available for any of them. In other words, a
supervised automated system trained only on the
available annotated examples will fail to learn any
potential frame assignments for the target lemma.
Such is the case for art, which in FrameNet is
assigned the single frame CRAFT, but for which
FNL contains no training data.
(7) The art of change-ringing is peculiar to
the English, and, like most English pe-
culiarities, unintelligible to the rest of the
world.
Whereas a NOTR-LU gap obscures a particular
frame assignment for a target lemma, a NOTR-
TGT gap indicates a complete absence in the lexi-
cographic corpus of annotated data for the lemma.
4.2 UNDEF gaps
The previous coverage problems arise from a lack
of annotated data, an issue which conceivably
could be addressed through further annotation.
More serious problems arise when a text contains
word senses, words, or frames not contained in
FrameNet. We call such elements ?undefined?;
specifically, they receive no treatment in FN1.3.
UNDEF-LU: lexical unit not defined. Cover-
age gaps of this sort occur when the frame inven-
tory for a given lemma is not complete. In other
words, at least one LU for the lemma exists in
FrameNet, but one or more other LUs are miss-
ing. For example, the noun installation occurs
in FrameNet with the frames LOCALE BY USE
and INSTALLING. The sense of an art installation,
which is an instance of the frame PHYSICAL ART-
WORKS, is missing.
UNDEF-TGT: target not addressed. In the
worst case, all LUs for a target lemma might be
missing, i.e., the lemma does not occur in the
FrameNet lexicon at all. The noun fabric is an ex-
ample. Though it has at least two distinct senses?
that of cloth or material and that of a framework
(e.g. the fabric of society)?FrameNet provides
no help for determining appropriate frames for in-
stances of this lemma.
UNDEF-FR: frame not defined. Finally, it
may be not only that the LU is missing, but that
there is no definition in FrameNet for the cor-
rect frame given the context. For example, in
the sports domain the lemma ringer can have the
sense of (a horseshoe thrown so that it encircles
the peg); to our knowledge, this sense is not avail-
able in FrameNet.
5 Coverage gaps and automated
processing
With the exception of work on extending cov-
erage, most FrameNet-style semantic role label-
ing studies draw both training and evaluation data
from FNL. This is an unrealistic evaluation sce-
nario for full-text semantic analysis, as such eval-
uation limits the domain for which prediction can
occur to those lexical entries treated in FNL. For
systems which do not attempt any generalization
beyond those lexical entries with training data,
this limits the system to 5864 lemmas for which it
can make predictions regarding frame assignment
and role labeling.
Disregarding whether annotations have yet
been provided for the lexical units in FNL still
limits us to 8370 frame-evoking elements (tar-
gets). To better understand the potential of cur-
rent frame-semantic resources for semantic anal-
ysis of unrestricted text, we evaluate coverage of
the FNL annotations against the texts in FNF, as
well as against the SemEval text. We then analyze
the performance of an off-the-shelf, supervised
SRL system, Shalmaneser (Erk and Pado?, 2006),
on the same texts, with a focus on the types of
933
Dataset TR-LU NOTR-LU NOTR-TGT UNDEF-LU UNDEF-FR
PB 42.66 9.56 47.78 ? ?
NTI 46.77 7.77 45.46 ? ?
SE 51.64 6.86 26.01 3.40 12.09
Table 2: FrameNet coverage for analyzed texts
errors made and the upper bound on performance
for this system.
5.1 FrameNet coverage
As described in Section 4, in many cases a lex-
ical unit, a frame-evoking element, or a frame
may simply not be represented in FrameNet. In
other cases, the entity may be in FN1.3 but lack-
ing training data. Of the 722 frames defined in
FN1.3, for example, annotations exist for 502.
For the three data sets analyzed, Table 2 shows
the degree of coverage provided by FNL for the
gold-standard frame annotations. First, the TR-
LU column shows the non-problematic cases, for
which the correct frame annotation is available
in FrameNet, with training data. The next two
columns represent training gaps related to lack
of training data: NOTR-LU are cases for which
training data exists for the target, but not for the
correct sense of the target, and NOTR-TGT in-
stances are those for which no training data at all
exists for the target.
Because all targets annotated in the FNF texts
(i.e. PB and NTI above) are incorporated in
FN1.3, gaps due to missing LUs, targets, or
frames do not exist for those texts. The same
does not hold for the SemEval (SE) text. For
3.4% of the annotated SemEval targets, an LU is
entirely missing from the lemma?s frame inven-
tory in FrameNet, and in just over 12% of cases
both the lemma and the frame are missing. In to-
tal, more than 15% of LUs appearing in the gold-
standard SemEval annotations are not defined at
all within FrameNet. This figure accords with that
found by Baker et al (2007).
5.2 Error analysis of full-text frame
assignment
Here we examine the errors made by Shalmaneser
for frame assignment on the three data sets. The
upper bound on apparent performance is fixed by
Dataset Correct Type(i) Type(ii) Type(iii)
PB 36.71 5.95 9.56 47.78
NTI 41.22 5.55 7.77 45.46
SE 46.67 4.97 6.86 41.50
Table 3: Shalmaneser performance on texts
the number of targets for which Shalmaneser has
seen training data, namely the sum of TR-LU and
NOTR-LU in Table 2.6
We consider three categories of errors: (i) nor-
mal or true errors are misclassifications when the
correct label has been seen in the training data. In
this category we also count errors resulting from
incorrect lemmatization. (ii) label-not-seen errors
are misclassifications when the correct label does
not appear in the training data and thus is unavail-
able to the classifier. Finally, (iii) no-chance er-
rors occur when the system has no information
for either a given target or a given frame. Ta-
ble 3 shows the prevalence of each error type for
each data set, given as the percentage of all frame-
assignment targets.
It can be seen that the frame assignment accu-
racy is relatively low for all three texts (between
37% and 47%). However, only a relatively small
proportion of the misclassifications are due to true
errors made by the system. Furthermore, a large
amount of errors (41% to 48%, with an average
of 46.8%) is due to cases where important infor-
mation is missing from FrameNet (Type (iii) er-
rors). Consequently, improving the semantic role
labeller by optimising the feature space or the ma-
chine learning framework is going to have very
little effect. A much more promising path would
be to investigate methods which might enable the
SRL system to deal gracefully with unseen data.
One possible strategy is discussed in the next sec-
tion.
6By ?apparent performance? we mean the system?s own
evaluation of its accuracy on frame assignment.
934
6 Frame and lemma overlap
One potential strategy for improving full-text se-
mantic analysis without performing additional an-
notation is to take advantage of semantic overlap
as it is represented in FrameNet. We can look
at two different types of overlap in FrameNet:
lemma overlap and frame overlap.
6.1 Lemma overlap
The approach of treating frame assignment as a
word sense disambiguation task (as, e.g., by Shal-
maneser) relies on the overlap of LUs with the
same lemma and trains lemma-based classifiers
on all training instances for all LUs involving that
lemma. One way to consider using labeled mate-
rial in FrameNet to improve performance on tar-
gets for which we have no labeled material is to
generalize over lemmas associated with the same
frame. The idea is to use training instances from
related lemmas to build a larger training set for
lemmas with little or no annotated data.
Of the 8370 lemmas in FN, 8358 share a single
frame with at least one other lemma. 890 overlap
on two frames with at least one other lemma, and
111 have 3-frame overlap with at least one other
lemma. Only 16 lemmas show an overlap of four
or more frames. These groupings are:
1. clang.v, clatter.v, click.v, thump.v
2. hit.v, smack.v, swing.v, turn.v
3. drop.v, rise.v
4. remember.v, forget.v
5. examine.v, examination.n
6. withdraw.v, withdrawal.n
The first two groupings are sets of words that
are closely semantically related, the second two
are opposite pairs, and the third two are verb-
nominalization pairs.
The lemma overlap groups differ with respect
to how much training data they make accessible.
6.2 Frame overlap
Another possibility to be considered is general-
ization over all instances of a given frame. For
the 502 frames with annotated examples, the num-
ber of annotated instances ranges from one (SAFE
SITUATION, BOARD VEHICLE, and ACTIVITY
START to 6233 (SELF MOTION), with an average
of 278 training instances per frame.
In future work we will examine the effective-
ness of binary frame-based classifiers, abstract-
ing away from individual predicates to predict
whether a given lemma belongs to the frame in
question (for a related study see Johansson and
Nugues (2007)). A potential drawback to this ap-
proach is the loss of predicate-specific informa-
tion. We know, for example, about verbs that they
tend to have typical argument structures and typi-
cal syntactic realizations of those argument struc-
tures.
In addition to this frame-overlap approach, we
will consider the impact on coverage of using
coarser-grained versions of FrameNet in which
frames have been merged according to frame rela-
tions defined over the FrameNet hierarchy, using
the FrameNet Transformer tool described in (Rup-
penhofer et al, 2010).
7 Conclusions
Although it is clear that the capability to do shal-
low semantic analysis on unrestricted text, and on
complete documents or text passages, would help
performance on a number of key tasks, currently-
available resources seriously limit our potential
for achieving this with supervised systems. The
analysis in this paper aims for a better understand-
ing of the precise nature of these limitations in
order to address them more deliberately and with
a principled understanding of the coverage prob-
lems faced by current systems.
To this end, we outline a typology of coverage
gaps and analyze both coverage of FrameNet and
performance of a supervised semantic role label-
ing system on three different full-text data sets, to-
taling over 150,000 frame-assignment targets. We
find that, on average, 46.8% of targets are not cov-
ered under straight supervised-classification ap-
proaches to frame assignment.
Acknowledgments
This research has been funded by the German Re-
search Foundation DFG under the MMCI Cluster
of Excellence. Thanks to the anonymous review-
ers, Josef Ruppenhofer, Ines Rehbein, and Hagen
Fu?rstenau for interesting and helpful comments
and discussions, and to Collin Baker for assistance
with data.
935
References
C. Baker, M. Ellsworth, K. Erk. 2007. Semeval-2007
task 19: Frame semantic structure extraction. In
Proceedings of SemEval-2007.
A. Burchardt, K. Erk, A. Frank. 2005. A WordNet
Detour to FrameNet. In Proceedings of the GLDV-
05 Workshop GermaNet II.
A. Burchardt, M. Pennacchiotti, S. Thater, M. Pinkal.
2009. Assessing the impact of frame semantics on
textual entailment. Journal of Natural Language
Engineering, Special Issue on Textual Entailment,
15(4):527?550.
D. D. Cao, D. Croce, M. Pennacchiotti, R. Basili.
2008. Combining word sense and usage for mod-
eling frame semantics. In Proceedings of STEP-08.
K. Deschacht, M.-F. Moens. 2009. Semi-supervised
Semantic Role Labeling Using the Latent Words
Language Model. In Proceedings of EMNLP-09.
M. Ellsworth, K. Erk, P. Kingsbury, S. Pado?. 2004.
PropBank, SALSA, and FrameNet: How Design
Determines Product. In Proceedings LREC 2004
Workshop on Building Lexical Resources from Se-
mantically Annotated Corpora.
K. Erk, S. Pado?. 2006. Shalmaneser ? a toolchain for
shallow semantic parsing. In Proceedings of LREC-
06.
K. Erk. 2005. Frame assignment as word sense disam-
biguation. In Proceedings of IWCS 6.
H. Fu?rstenau, M. Lapata. 2009a. Graph alignment for
semi-supervised semantic role labeling. In Proceed-
ings of EMNLP 2009.
H. Fu?rstenau, M. Lapata. 2009b. Semi-supervised se-
mantic role labeling. In Proceedings of EACL 2009.
D. Gildea, D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
A. Gordon, R. Swanson. 2007. Generalizing semantic
role annotations across syntactically similar verbs.
In Proceedings of ACL 2007.
R. Johansson, P. Nugues. 2007. Using WordNet to
extend FrameNet coverage. In Proceedings of the
Workshop on Building Frame-semantic Resources
for Scandinavian and Baltic Languages, NODAL-
IDA.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R. MacIn-
tyre, A. Bies, M. Ferguson, K. Katz, B. Schasberger.
1994. The Penn Treebank: Annotating predicate ar-
gument structure. In ARPA Human Language Tech-
nology Workshop.
D. McCarthy. 2009. Word Sense Disambiguation:
An Overview. Language and Linguistics Compass,
3(2):537?558.
A. Moschitti, P. Morarescu, S. Harabagiu. 2003.
Open-domain information extraction via automatic
semantic labeling. In Proceedings of FLAIRS.
A. Moschitti, D. Pighin, R. Basili. 2008. Tree Kernels
for Semantic Role Labeling. Computational Lin-
guistics, 34(2).
S. Pado?, M. Pennacchiotti, C. Sporleder. 2008. Se-
mantic role assignment for event nominalisations by
leveraging verbal data. In Proceedings of Coling
2008.
M. Palmer, D. Gildea, P. Kingsbury. 2005. The Propo-
sition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71?105.
M. Pennacchiotti, D. D. Cao, R. Basili, D. Croce,
M. Roth. 2008a. Automatic induction of FrameNet
lexical units. In Proceedings of EMNLP-08.
M. Pennacchiotti, D. D. Cao, P. Marocco, R. Basili.
2008b. Towards a Vector Space Model for
FrameNet-like Resources. In Proceedings of LREC-
08.
J. Ruppenhofer, M. Ellsworth, M. R. L. Petruck, C. R.
Johnson, J. Scheffczyk. 2006. FrameNet II: Ex-
tended Theory and Practice.
J. Ruppenhofer, C. Sporleder, R. Morante, C. Baker,
M. Palmer. 2009. SemEval-2010 Task 10: Link-
ing Events and Their Participants in Discourse. In
Proceedings of SEW-2009.
J. Ruppenhofer, M. Pinkal, J. Sunde. 2010. Generat-
ing FrameNets of various granularities. In Proceed-
ings of LREC 2010.
D. Shen, M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
EMNLP-2007.
M. Surdeanu, S. Harabagiu, J. Williams, P. Aarseth.
2003. Using predicate-argument structures for in-
formation extraction. In Proceedings of ACL 2003.
K. Toutanova, A. Haghighi, C. D. Manning. 2008.
A Global Joint Model for Semantic Role Labeling.
Computational Linguistics, 34(2).
936
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 517?523,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Automatic prediction of aspectual class of verbs in context
Annemarie Friedrich and Alexis Palmer
Department of Computational Linguistics
Saarland University, Saarbr?ucken, Germany
{afried,apalmer}@coli.uni-saarland.de
Abstract
This paper describes a new approach to
predicting the aspectual class of verbs in
context, i.e., whether a verb is used in a
stative or dynamic sense. We identify two
challenging cases of this problem: when
the verb is unseen in training data, and
when the verb is ambiguous for aspec-
tual class. A semi-supervised approach us-
ing linguistically-motivated features and a
novel set of distributional features based
on representative verb types allows us to
predict classes accurately, even for unseen
verbs. Many frequent verbs can be either
stative or dynamic in different contexts,
which has not been modeled by previous
work; we use contextual features to re-
solve this ambiguity. In addition, we intro-
duce two new datasets of clauses marked
for aspectual class.
1 Introduction
In this work, we focus on the automatic prediction
of whether a verb in context is used in a stative or
in a dynamic sense, the most fundamental distinc-
tion in all taxonomies of aspectual class. The as-
pectual class of a discourse?s finite verbs is an im-
portant factor in conveying and interpreting tem-
poral structure (Moens and Steedman, 1988; Dorr,
1992; Klavans and Chodorow, 1992); others are
tense, grammatical aspect, mood and whether the
utterance represents an event as completed. More
accurate temporal information processing is ex-
pected to be beneficial for a variety of natural lan-
guage processing tasks (Costa and Branco, 2012;
UzZaman et al, 2013).
While most verbs have one predominant inter-
pretation, others are more flexible for aspectual
class and can occur as either stative (1) or dynamic
(2) depending on the context. There are also cases
that allow for both readings, such as (3).
(1) The liquid fills the container. (stative)
(2) The pool slowly filled with water. (dynamic)
(3) Your soul was made to be filled with God
Himself. (both) (Brown corpus, religion)
Cases like (3) do not imply that there is a third
class, but rather that two interpretations are avail-
able for the sentence, of which usually one will be
chosen by a reader.
Following Siegel and McKeown (2000), we aim
to automatically classify clauses for fundamental
aspectual class, a function of the main verb and
a select group of complements, which may dif-
fer per verb (Siegel and McKeown, 2000; Siegel,
1998b). This corresponds to the aspectual class
of the clause?s main verb when ignoring any as-
pectual markers or transformations. For exam-
ple, English sentences with perfect tense are usu-
ally considered to introduce states to the discourse
(Smith, 1991; Katz, 2003), but we are interested in
the aspectual class before this transformation takes
place. The clause John has kissed Mary introduces
a state, but the fundamental aspectual class of the
?tenseless? clause John kiss Mary is dynamic.
In contrast to Siegel and McKeown (2000), we
do not conduct the task of predicting aspectual
class solely at the type level, as such an approach
ignores the minority class of ambiguous verbs. In-
stead we predict the aspectual class of verbs in
the context of their arguments and modifiers. We
show that this method works better than using only
type-based features, especially for verbs with am-
biguous aspectual class. In addition, we show
that type-based features, including novel distribu-
tional features based on representative verbs, accu-
rately predict predominant aspectual class for un-
seen verb types. Our work differs from prior work
in that we treat the problem as a three-way clas-
sification task, predicting DYNAMIC, STATIVE or
BOTH as the aspectual class of a verb in context.
517
2 Related work
Aspectual class is well treated in the linguistic lit-
erature (Vendler, 1957; Dowty, 1979; Smith, 1991,
for example). Our notion of the stative/dynamic
distinction corresponds to Bach?s (1986) distinc-
tion between states and non-states; to states ver-
sus occurrences (events and processes) according
to Mourelatos (1978); and to Vendler?s (1957) dis-
tinction between states and the other three classes
(activities, achievements, accomplishments).
Early studies on the computational modeling
of aspectual class (Nakhimovsky, 1988; Passon-
neau, 1988; Brent, 1991; Klavans and Chodorow,
1992) laid foundations for a cluster of papers pub-
lished over a decade ago (Siegel and McKeown,
2000; Siegel, 1998b; Siegel, 1998a). Since then,
it has mostly been treated as a subtask within
temporal reasoning, such as in efforts related to
TimeBank (Pustejovsky et al, 2003) and the Tem-
pEval challenges (Verhagen et al, 2007; Verha-
gen et al, 2010; UzZaman et al, 2013), where
top-performing systems (Jung and Stent, 2013;
Bethard, 2013; Chambers, 2013) use corpus-based
features, WordNet synsets, parse paths and fea-
tures from typed dependencies to classify events
as a joint task with determining the event?s span.
Costa and Branco (2012) explore the usefulness of
a wider range of explicitly aspectual features for
temporal relation classification.
Siegel and McKeown (2000) present the most
extensive study of predicting aspectual class,
which is the main inspiration for this work. While
all of their linguistically motivated features (see
section 4.1) are type-based, they train on and eval-
uate over labeled verbs in context. Their data
set taken from medical discharge summaries com-
prises 1500 clauses containing main verbs other
than be and have which are marked for aspectual
class. Their model fails to outperform a baseline
of memorizing the most frequent class of a verb
type, and they present an experiment testing on un-
seen verb types only for the related task of classi-
fying completedness of events. We replicate their
method using publicly available software, create
a similar but larger corpus,
1
and show that it is
indeed possible to predict the aspectual class of
unseen verbs. Siegel (1998a) investigates a classi-
fication method for the verb have in context; in-
1
Direct comparison on their data is not possible; feature
values for the verbs studied are available, but full texts and
the English Slot Grammar parser (McCord, 1990) are not.
COMPLETE W/O have/be/none
genre clauses ? clauses ?
jokes 3462 0.85 2660 0.77
letters 1848 0.71 1444 0.62
news 2565 0.79 2075 0.69
all 7875 0.80 6161 0.70
Table 1: Asp-MASC: Cohen?s observed un-
weighted ?.
DYNAMIC STATIVE BOTH
DYNAMIC 4464 164 9
STATIVE 434 1056 29
BOTH 5 0 0
Table 2: Asp-MASC: confusion matrix for two
annotators, without have/be/none clauses, ? is 0.7.
spired by this work, our present work goes one
step further and uses a larger set of instance-based
contextual features to perform experiments on a
set of 20 verbs. To the best of our knowledge, there
is no previous work comprehensively addressing
aspectual classification of verbs in context.
3 Data
Verb type seed sets Using the LCS Database
(Dorr, 2001), we identify sets of verb types whose
senses are only stative (188 verbs, e.g. belong,
cost, possess), only dynamic (3760 verbs, e.g. al-
ter, knock, resign), or mixed (215 verbs, e.g. fill,
stand, take), following a procedure described by
Dorr and Olsen (1997).
Asp-MASC The Asp-MASC corpus consists of
7875 clauses from the letters, news and jokes sec-
tions of MASC (Ide et al, 2010), each labeled
by two annotators for the aspectual class of the
main verb.
2
Texts were segmented into clauses us-
ing SPADE (Soricut and Marcu, 2003) with some
heuristic post-processing. We parse the corpus us-
ing the Stanford dependency parser (De Marneffe
et al, 2006) and extract the main verb of each seg-
ment. We use 6161 clauses for the classification
task, omitting clauses with have or be as the main
verb and those where no main verb could be iden-
tified due to parsing errors (none). Table 1 shows
inter-annotator agreement; Table 2 shows the con-
fusion matrix for the two annotators. Our two an-
notators exhibit different preferences on the 598
cases where they disagree between DYNAMIC and
STATIVE. Such differences in annotation prefer-
2
Corpus freely available from
www.coli.uni-saarland.de/
?
afried.
518
DYNAMIC STATIVE BOTH
DYNAMIC 1444 201 54
STATIVE 168 697 20
BOTH 44 31 8
Table 3: Asp-Ambig: confusion matrix for two
annotators. Cohen?s ? is 0.6.
ences are not uncommon (Beigman Klebanov et
al., 2008). We observe higher agreement in the
jokes and news subcorpora than for letters; texts
in the letters subcorpora are largely argumentative
and thus have a different rhetorical style than the
more straightforward narratives and reports found
in jokes. Overall, we find substantial agreement.
The data for our experiments uses the label DY-
NAMIC or STATIVE whenever annotators agree,
and BOTH whenever they disagree or when at least
one annotator marked the clause as BOTH, assum-
ing that both readings are possible in such cases.
Because we don?t want to model the authors? per-
sonal view of the theory, we refrain from applying
an adjudication step and model the data as is.
Asp-Ambig: (Brown) In order to facilitate a
first study on ambiguous verbs, we select 20 fre-
quent verbs from the list of ?mixed? verbs (see
section 3) and for each mark 138 sentences. Sen-
tences are extracted randomly from the Brown cor-
pus, such that the distribution of stative/dynamic
usages is expected to be natural. We present
entire sentences to the annotators who mark the
aspectual class of the verb in question as high-
lighted in the sentence. The data is processed in
the same way as Asp-MASC, discarding instances
with parsing problems. This results in 2667 in-
stances. ? is 0.6, the confusion matrix is shown in
Table 3. Details are listed in Table 10.
4 Model and Features
For predicting the aspectual class of verbs in con-
text (STATIVE, DYNAMIC, BOTH), we assume a
supervised learning setting and explore features
mined from a large background corpus, distribu-
tional features, and instance-based features. If not
indicated otherwise, experiments use a Random
Forest classifier (Breiman, 2001) trained with the
implementation and standard parameter settings
from Weka (Hall et al, 2009).
4.1 Linguistic indicator features (LingInd)
This set of corpus-based features is a reimple-
mentation of the linguistic indicators of Siegel
FEATURE EXAMPLE FEATURE EXAMPLE
frequency - continuous continually
present says adverb endlessly
past said evaluation better
future will say adverb horribly
perfect had won manner furiously
progressive is winning adverb patiently
negated not/never temporal again
particle up/in/... adverb finally
no subject - in-PP in an hour
for-PP for an hour
Table 4: LingInd feature set and examples for lex-
ical items associated with each indicator.
FEATURE VALUES
part-of-speech tag of the verb VB, VBG, VBN, ...
tense present, past, future
progressive true/false
perfect true/false
voice active/passive
grammatical dependents WordNet lexname/POS
Table 5: Instance-based (Inst) features
and McKeown (2000), who show that (some of)
these features correlate with either stative or dy-
namic verb types. We parse the AFE and XIE sec-
tions of Gigaword (Graff and Cieri, 2003) with
the Stanford dependency parser. For each verb
type, we obtain a normalized count showing how
often it occurs with each of the indicators in Ta-
ble 4, resulting in one value per feature per verb.
For example, for the verb fill, the value of the
feature temporal-adverb is 0.0085, meaning
that 0.85% of the occurrences of fill in the corpus
are modified by one of the temporal adverbs on the
list compiled by Siegel (1998b). Tense, progres-
sive, perfect and voice are extracted using a set of
rules following Loaiciga et al (2014).
3
4.2 Distributional Features (Dist)
We aim to leverage existing, possibly noisy sets
of representative stative, dynamic or mixed verb
types extracted from LCS (see section 3), mak-
ing up for unseen verbs and noise by averaging
over distributional similarities. Using an exist-
ing large distributional model (Thater et al, 2011)
estimated over the set of Gigaword documents
marked as stories, for each verb type, we build
a syntactically informed vector representing the
contexts in which the verb occurs. We compute
three numeric feature values per verb type, which
correspond to the average cosine similarities with
the verb types in each of the three seed sets.
3
We thank the authors for providing us their code.
519
FEATURES ACCURACY (%)
Baseline (Lemma) 83.6
LingInd 83.8
Inst 70.8
Inst+Lemma 83.7
Dist 83.4
LingInd+Inst+Dist+Lemma 84.1
Table 6: Experiment 1: SEEN verbs, using Asp-
MASC. Baseline memorizes most frequent class
per verb type in training folds.
4.3 Instance-based features (Inst)
Table 5 shows our set of instance-based syntac-
tic and semantic features. In contrast to the above
described type-based features, these features do
not rely on a background corpus, but are ex-
tracted from the clause being classified. Tense,
progressive, perfect and voice are extracted from
dependency parses as described above. For fea-
tures encoding grammatical dependents, we focus
on a subset of grammatical relations. The fea-
ture value is either the WordNet lexical filename
(e.g. noun.person) of the given relation?s argu-
ment or its POS tag, if the former is not avail-
able. We simply use the most frequent sense for
the dependent?s lemma. We also include features
that indicate, if there are any, the particle of the
verb and its prepositional dependents. For the
sentence A little girl had just finished her first
week of school, the instance-based feature values
would include tense:past, subj:noun.person,
dobj:noun.time or particle:none.
5 Experiments
The experiments presented in this section aim to
evaluate the effectiveness of the feature sets de-
scribed in the previous section, focusing on the
challenging cases of verb types unseen in the train-
ing data and highly ambiguous verbs. The feature
Lemma indicates that the verb?s lemma is used as
an additional feature.
Experiment 1: SEEN verbs
The setting of our first experiment follows Siegel
and McKeown (2000). Table 6 reports results for
10-fold cross-validation, with occurrences of all
verbs distributed evenly over the folds. No feature
combination significantly
4
outperforms the base-
line of simply memorizing the most frequent class
4
According to McNemar?s test with Yates? correction for
continuity, p < 0.01.
FEATURES ACCURACY (%)
1 Baseline 72.5
2 Dist 78.3?
3 LingInd 80.4?
4 LingInd+Dist 81.9*?
Table 7: Experiment 2: UNSEEN verb types, Lo-
gistic regression, Asp-MASC. Baseline labels ev-
erything with the most frequent class in the train-
ing set (DYNAMIC). *Significantly
4
different from
line 1. ?Significantly
4
different from line 3.
DATA FEATURES ACC. (%)
one-label Baseline 92.8
verbs LingInd 92.8
Dist 92.6
(1966 inst.) Inst+Lemma 91.4?
LingInd+Inst+Lemma 92.4
multi-label Baseline 78.9
verbs LingInd 79.0
Dist 79.0
(4195 inst.) Inst 67.4?
Inst+Lemma 79.9
LingInd+Inst+Lemma 80.9*
LingInd+Inst+Lemma+Dist 80.2*
Table 8: Experiment 3: ?ONE- VS. MULTI-
LABEL? verbs, Asp-MASC. Baseline as in Table
6. *Indicates that result is significantly
4
different
from the respective baseline.
of a verb type in the respective training folds.
Experiment 2: UNSEEN verbs
This experiment shows a successful case of semi-
supervised learning: while type-based feature val-
ues can be estimated from large corpora in an un-
supervised way, some labeled training data is nec-
essary to learn their best combination. This exper-
iment specifically examines performance on verbs
not seen in labeled training data. We use 10-fold
cross validation but ensure that all occurrences of
a verb type appear in the same fold: verb types
in each test fold have not been seen in the re-
spective training data, ruling out the Lemma fea-
ture. A Logistic regression classifier (Hall et al,
2009) works better here (using only numeric fea-
tures), and we present results in Table 7. Both the
LingInd and Dist features generalize across verb
types, and their combination works best.
Experiment 3: ONE- vs. MULTI-LABEL verbs
For this experiment, we compute results sepa-
rately for one-label verbs (those for which all in-
stances in Asp-MASC have the same label) and
520
SYSTEM CLASS ACC. P R F
baseline micro-avg. 78.9 0.75 0.79 0.76
LingInd DYNAMIC 0.84 0.95 0.89
+Inst STATIVE 0.76 0.69 0.72
+Lemma BOTH 0.51 0.24 0.33
micro-avg. 80.9* 0.78 0.81 0.79
Table 9: Experiment 3: ?MULTI-LABEL?, preci-
sion, recall and F-measure, detailed class statistics
for the best-performing system from Table 8.
for multi-label verbs (instances have differing la-
bels in Asp-MASC). We expect one-label verbs
to have a strong predominant aspectual class, and
multi-label verbs to be more flexible. Otherwise,
the experimental setup is as in experiment 1. Re-
sults appear in Table 8. In each case, the linguistic
indicator features again perform on par with the
baseline. For multi-label verbs, the feature combi-
nation Lemma+LingInd+Inst leads to significant
4
improvement of 2% gain in accuracy over the
baseline; Table 9 reports detailed class statistics
and reveals a gain in F-measure of 3 points over
the baseline. To sum up, Inst features are essential
for classifying multi-label verbs, and the LingInd
features provide some useful prior. These results
motivate the need for an instance-based approach.
Experiment 4: INSTANCE-BASED classification
For verbs with ambiguous aspectual class, type-
based classification is not sufficient, as this ap-
proach selects a dominant sense for any given verb
and then always assigns that. Therefore we pro-
pose handling ambiguous verbs separately. As
Asp-MASC contains only few instances of each of
the ambiguous verbs, we turn to the Asp-Ambig
dataset. We perform a Leave-One-Out (LOO)
cross validation evaluation, with results reported
in Table 10.
5
Using the Inst features alone (not
shown in Table 10) results in a micro-average ac-
curacy of only 58.1%: these features are only use-
ful when combined with the feature Lemma. For
classifying verbs whose most frequent class oc-
curs less than 56% of the time, Lemma+Inst fea-
tures are essential. Whether or not performance
is improved by adding LingInd/Dist features, with
their bias towards one aspectual class, depends
on the verb type. It is an open research question
which verb types should be treated in which way.
5
The third column also shows the outcome of using ei-
ther only the Lemma, only LingInd or only Dist in LOO; all
have almost the same outcome as using the majority class,
numbers differ only after the decimal point.
I
n
s
t
+
L
e
m
m
a
I
n
s
t
+
L
e
m
m
a
+
L
i
n
g
I
n
d
+
D
i
s
t
# OF MAJORITY
VERB INST. CLASS
5
feel 128 96.1 STAT 93.0 93.8
say 138 94.9 DYN 93.5 93.5
make 136 91.9 DYN 91.9 91.2
come 133 88.0 DYN 87.2 87.2
take 137 85.4 DYN 85.4 85.4
meet 130 83.9 DYN 86.2 87.7
stand 130 80.0 STAT 79.2 83.1
find 137 74.5 DYN 69.3 68.8
accept 134 70.9 DYN 64.9 65.7
hold 134 56.0 BOTH 43.3 49.3
carry 136 55.9 DYN 55.9 58.1
look 138 55.8 DYN 72.5 74.6
show 133 54.9 DYN 69.2 68.4
appear 136 52.2 STAT 64.7 61.0
follow 122 51.6 BOTH 69.7 65.6
consider 138 50.7 DYN 61.6 70.3
cover 123 50.4 STAT 46.3 54.5
fill 134 47.8 DYN 66.4 62.7
bear 135 47.4 DYN 70.4 67.4
allow 135 37.8 DYN 48.9 51.9
micro-avg. 2667 66.3 71.0* 72.0*
Table 10: Experiment 4: INSTANCE-BASED.
Accuracy (in %) on Asp-Ambig. *Differs
significantly
4
from the majority class baseline.
6 Discussion and conclusions
We have described a new, context-aware approach
to automatically predicting aspectual class, includ-
ing a new set of distributional features. We have
also introduced two new data sets of clauses la-
beled for aspectual class. Our experiments show
that in any setting where labeled training data
is available, improvement over the most frequent
class baseline can only be reached by integrating
instance-based features, though type-based fea-
tures (LingInd, Dist) may provide useful priors
for some verbs and successfully predict predom-
inant aspectual class for unseen verb types. In or-
der to arrive at a globally well-performing system,
we envision a multi-stage approach, treating verbs
differently according to whether training data is
available and whether or not the verb?s aspectual
class distribution is highly skewed.
Acknowledgments We thank the anonymous
reviewers, Omri Abend, Mike Lewis, Manfred
Pinkal, Mark Steedman, Stefan Thater and Bonnie
Webber for helpful comments, and our annotators
A. Kirkland and R. K?uhn. This research was sup-
ported in part by the MMCI Cluster of Excellence,
and the first author is supported by an IBM PhD
Fellowship.
521
References
Emmon Bach. 1986. The algebra of events. Linguis-
tics and philosophy, 9(1):5?16.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing disagreements. In Pro-
ceedings of the Workshop on Human Judgements in
Computational Linguistics, pages 2?7. Association
for Computational Linguistics.
Steven Bethard. 2013. ClearTK-TimeML: A minimal-
ist approach to TempEval 2013. In Second Joint
Conference on Lexical and Computational Seman-
tics (* SEM), volume 2, pages 10?14.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32.
Michael R. Brent. 1991. Automatic semantic classifi-
cation of verbs from their syntactic contexts: an im-
plemented classifier for stativity. In Proceedings of
the fifth conference on European chapter of the As-
sociation for Computational Linguistics, pages 222?
226. Association for Computational Linguistics.
Nathanael Chambers. 2013. Navytime: Event and
time ordering from raw text. In Second Joint Con-
ference on Lexical and Computational Semantics (*
SEM), volume 2, pages 73?77.
Francisco Costa and Ant?onio Branco. 2012. Aspec-
tual type and temporal relation classification. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 266?275. Association for Computa-
tional Linguistics.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of LREC, volume 6, pages
449?454.
Bonnie J. Dorr and Mari Broman Olsen. 1997. De-
riving verbal and compositional lexical aspect for
NLP applications. In Proceedings of the eighth con-
ference on European chapter of the Association for
Computational Linguistics, pages 151?158. Associ-
ation for Computational Linguistics.
Bonnie J. Dorr. 1992. A two-level knowledge repre-
sentation for machine translation: lexical semantics
and tense/aspect. In Lexical Semantics and Knowl-
edge Representation, pages 269?287. Springer.
Bonnie J. Dorr. 2001. LCS verb database. Online
software database of Lexical Conceptual Structures,
University of Maryland, College Park, MD.
David Dowty. 1979. Word Meaning and Montague
Grammar. Reidel, Dordrecht.
David Graff and Christopher Cieri. 2003. English gi-
gaword.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
ACM SIGKDD explorations newsletter, 11(1):10?
18.
Nancy Ide, Christiane Fellbaum, Collin Baker, and Re-
becca Passonneau. 2010. The manually annotated
sub-corpus: a community resource for and by the
people. In Proceedings of the ACL 2010 Conference
Short Papers, pages 68?73. Association for Compu-
tational Linguistics.
Hyuckchul Jung and Amanda Stent. 2013. ATT1:
Temporal annotation using big windows and rich
syntactic and semantic features. In Second Joint
Conference on Lexical and Computational Seman-
tics (* SEM), volume 2, pages 20?24.
Graham Katz. 2003. On the stativity of the english
perfect. Perfect explorations, pages 205?234.
Judith L. Klavans and Martin Chodorow. 1992. De-
grees of stativity: the lexical representation of verb
aspect. In Proceedings of the 14th conference on
Computational Linguistics, pages 1126?1131. Asso-
ciation for Computational Linguistics.
Sharid Loaiciga, Thomas Meyer, and Andrei Popescu-
Belis. 2014. English-French Verb Phrase Align-
ment in Europarl for Tense Translation Modeling.
In Language Resources and Evaluation Conference
(LREC), Reykjavik, Iceland.
Michael C. McCord. 1990. Slot Grammar. Springer.
Marc Moens and Mark J. Steedman. 1988. Tempo-
ral ontology and temporal reference. Computational
Linguistics, 14(2):15?28.
Alexander P.D. Mourelatos. 1978. Events, processes,
and states. Linguistics and philosophy, 2(3):415?
434.
Alexander Nakhimovsky. 1988. Aspect, aspectual
class, and the temporal structure of narrative. Com-
putational Linguistics, 14(2):29?43.
Rebecca Passonneau. 1988. A computational model
of the semantics of tense and aspect. Computational
Linguistics, Spring 1988.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al 2003. The timebank corpus. In Corpus
linguistics, volume 2003, page 40.
Eric V. Siegel and Kathleen R. McKeown. 2000.
Learning methods to combine linguistic indica-
tors: Improving aspectual classification and reveal-
ing linguistic insights. Computational Linguistics,
26(4):595?628.
522
Eric V. Siegel. 1998a. Disambiguating verbs with the
WordNet category of the direct object. In Proceed-
ings of Workshop on Usage of WordNet in Natural
Language Processing Systems, Universite de Mon-
treal.
Eric V. Siegel. 1998b. Linguistic Indicators for
Language Understanding: Using machine learn-
ing methods to combine corpus-based indicators for
aspectual classification of clauses. Ph.D. thesis,
Columbia University.
Carlota S. Smith. 1991. The Parameter of Aspect.
Kluwer, Dordrecht.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology-Volume 1, pages 149?156. Association
for Computational Linguistics.
Stefan Thater, Hagen F?urstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and ef-
fective vector model. In IJCNLP, pages 1134?1143.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, Marc Verhagen, James Allen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating time expressions, events, and temporal
relations. In Second joint conference on lexical and
computational semantics (* SEM), volume 2, pages
1?9.
Zeno Vendler, 1957. Linguistics in Philosophy, chapter
Verbs and Times, pages 97?121. Cornell University
Press, Ithaca, New York.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal
relation identification. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 75?80. Association for Computational Lin-
guistics.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 task 13:
TempEval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 57?
62. Association for Computational Linguistics.
523
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 109?114,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
lex4all: A language-independent tool for building and evaluating
pronunciation lexicons for small-vocabulary speech recognition
Anjana Vakil, Max Paulus, Alexis Palmer, and Michaela Regneri
Department of Computational Linguistics, Saarland University
{anjanav,mpaulus,apalmer,regneri}@coli.uni-saarland.de
Abstract
This paper describes lex4all, an open-
source PC application for the generation
and evaluation of pronunciation lexicons
in any language. With just a few minutes
of recorded audio and no expert knowl-
edge of linguistics or speech technology,
individuals or organizations seeking to
create speech-driven applications in low-
resource languages can build lexicons en-
abling the recognition of small vocabular-
ies (up to 100 terms, roughly) in the target
language using an existing recognition en-
gine designed for a high-resource source
language (e.g. English). To build such lex-
icons, we employ an existing method for
cross-language phoneme-mapping. The
application also offers a built-in audio
recorder that facilitates data collection, a
significantly faster implementation of the
phoneme-mapping technique, and an eval-
uation module that expedites research on
small-vocabulary speech recognition for
low-resource languages.
1 Introduction
In recent years it has been demonstrated that
speech recognition interfaces can be extremely
beneficial for applications in the developing world
(Sherwani and Rosenfeld, 2008; Sherwani, 2009;
Bali et al., 2013). Typically, such applications
target low-resource languages (LRLs) for which
large collections of speech data are unavailable,
preventing the training or adaptation of recogni-
tion engines for these languages. However, an ex-
isting recognizer for a completely unrelated high-
resource language (HRL), such as English, can
be used to perform small-vocabulary recognition
tasks in the LRL, given a pronunciation lexicon
mapping each term in the target vocabulary to a
sequence of phonemes in the HRL, i.e. phonemes
which the recognizer can model.
This is the motivation behind lex4all,
1
an open-
source application that allows users to automati-
cally create a mapped pronunciation lexicon for
terms in any language, using a small number of
speech recordings and an out-of-the-box recog-
nition engine for a HRL. The resulting lexicon
can then be used with the HRL recognizer to add
small-vocabulary speech recognition functionality
to applications in the LRL, without the need for
the large amounts of data and expertise in speech
technologies required to train a new recognizer.
This paper describes the lex4all application and
its utility for the rapid creation and evaluation of
pronunciation lexicons enabling small-vocabulary
speech recognition in any language.
2 Background and related work
Several commercial speech recognition systems
offer high-level Application Programming Inter-
faces (APIs) that make it extremely simple to add
voice interfaces to an application, requiring very
little general technical expertise and virtually no
knowledge of the inner workings of the recogni-
tion engine. If the target language is supported by
the system ? the Microsoft Speech Platform,
2
for
example, supports over 20 languages ? this makes
it very easy to create speech-driven applications.
If, however, the target language is one of the
many thousands of LRLs for which high-quality
recognition engines have not yet been devel-
oped, alternative strategies for developing speech-
recognition interfaces must be employed. Though
tools for quickly training recognizers for new lan-
guages exist (e.g. CMUSphinx
3
), they typically
require many hours of training audio to produce
effective models, data which is by definition not
1
http://lex4all.github.io/lex4all/
2
http://msdn.microsoft.com/en-us/library/hh361572
3
http://www.cmusphinx.org
109
available for LRLs. In efforts to overcome this
data scarcity problem, recent years have seen
the development of techniques for rapidly adapt-
ing multilingual or language-independent acoustic
and language models to new languages from rela-
tively small amounts of data (Schultz and Waibel,
2001; Kim and Khudanpur, 2003), methods for
building resources such as pronunciation dictio-
naries from web-crawled data (Schlippe et al.,
2014), and even a web-based interface, the Rapid
Language Adaptation Toolkit
4
(RLAT), which al-
lows non-expert users to exploit these techniques
to create speech recognition and synthesis tools
for new languages (Vu et al., 2010). While they
greatly reduce the amount of data needed to build
new recognizers, these approaches still require
non-trivial amounts of speech and text in the target
language, which may be an obstacle for very low-
or zero-resource languages. Furthermore, even
high-level tools such as RLAT still demand some
understanding of linguistics/language technology,
and thus may not be accessible to all users.
However, many useful applications (e.g. for ac-
cessing information or conducting basic transac-
tions by telephone) only require small-vocabulary
recognition, i.e. discrimination between a few
dozen terms (words or short phrases). For ex-
ample, VideoKheti (Bali et al., 2013), a text-
free smartphone application that delivers agricul-
tural information to low-literate farmers in In-
dia, recognizes 79 Hindi terms. For such small-
vocabulary applications, an engine designed to
recognize speech in a HRL can be used as-is to
perform recognition of the LRL terms, given a
grammar describing the allowable combinations
and sequences of terms to be recognized, and a
pronunciation lexicon mapping each target term to
at least one pronunciation (sequence of phonemes)
in the HRL (see Fig. 1 for an example).
This is the thinking behind Speech-based Auto-
mated Learning of Accent and Articulation Map-
ping, or ?Salaam? (Sherwani, 2009; Qiao et al.,
2010; Chan and Rosenfeld, 2012), a method of
cross-language phoneme-mapping that discovers
accurate source-language pronunciations for terms
in the target language. The basic idea is to discover
the best pronunciation (phoneme sequence) for a
target term by using the source-language recog-
nition engine to perform phone decoding on one
or more utterances of the term. As commercial
4
http://i19pc5.ira.uka.de/rlat-dev
<lexicon version="1.0" xmlns="http://www
.w3.org/2005/01/pronunciation-
lexicon" xml:lang="en-US" alphabet
="x-microsoft-ups">
<lexeme>
<grapheme>beeni</grapheme>
<phoneme>B E NG I</phoneme>
<phoneme>B EI N I I</phoneme>
</lexeme>
</lexicon>
Figure 1: Sample XML lexicon mapping the
Yoruba word beeni (?yes?) to two possible se-
quences of American English phonemes.
recognizers such as Microsoft?s are designed for
word-decoding, and their APIs do not usually al-
low users access to the phone-decoding mode, the
Salaam approach uses a specially designed ?super-
wildcard? recognition grammar to mimic phone
decoding and guide pronunciation discovery (Qiao
et al., 2010; Chan and Rosenfeld, 2012). This al-
lows the recognizer to identify the phoneme se-
quence best matching a given term, without any
prior indication of how many phonemes that se-
quence should contain.
Given this grammar and one or more audio
recordings of the term, Qiao et al. (2010) use an it-
erative training algorithm to discover the best pro-
nunciation(s) for that term, one phoneme at a time.
Compared to pronunciations hand-written by a lin-
guist, pronunciations generated automatically by
this algorithm yield substantially higher recog-
nition accuracy: Qiao et al. (2010) report word
recognition accuracy rates in the range of 75-95%
for vocabularies of 50 terms. Chan and Rosen-
feld (2012) improve accuracy on larger vocabu-
laries (up to approximately 88% for 100 terms)
by applying an iterative discriminative training al-
gorithm, identifying and removing pronunciations
that cause confusion between word types.
The Salaam method is fully automatic, demand-
ing expertise neither in speech technology nor
in linguistics, and requires only a few recorded
utterances of each word. At least two projects
have successfully used the Salaam method to add
voice interfaces to real applications: an Urdu
telephone-based health information system (Sher-
wani, 2009), and the VideoKheti application men-
tioned above (Bali et al., 2013). What has not ex-
isted before now is an interface that makes this ap-
proach accessible to any user.
110
Given the established success of the Salaam
method, our contribution is to create a more time-
efficient implementation of the pronunciation-
discovery algorithm and integrate it into an easy-
to-use graphical application. In the following sec-
tions, we describe this application and our slightly
modified implementation of the Salaam method.
3 System overview
We have developed lex4all as a desktop applica-
tion for Microsoft Windows,
5
since it relies on the
Microsoft Speech Platform (MSP) as explained in
Section 4.1. The application and its source code
are freely available via GitHub.
6
The application?s core feature is its lexicon-
building tool, the architecture of which is illus-
trated in Figure 2. A simple graphical user in-
terface (GUI) allows users to type in the written
form of each term in the target vocabulary, and
select one or more audio recordings (.wav files)
of that term. Given this input, the program uses
the Salaam method to find the best pronuncia-
tion(s) for each term. This requires a pre-trained
recognition engine for a HRL as well as a series
of dynamically-created recognition grammars; the
engine and grammars are constructed and man-
aged using the MSP. We note here that our imple-
mentation of Salaam deviates slightly from that of
Qiao et al. (2010), improving the time-efficiency
and thus usability of the system (see Sec. 4).
Once pronunciations for all terms in the vocab-
ulary have been generated, the application outputs
a pronunciation lexicon for the given terms as an
XML file conforming to the Pronunciation Lexi-
con Specification.
7
This lexicon can then be di-
rectly included in a speech recognition application
built using the MSP API or a similar toolkit.
4 Pronunciation mapping
4.1 Recognition engine
For the HRL recognizer we use the US English
recognition engine of the MSP. The engine is used
as-is, with no modifications to its underlying mod-
els. We choose the MSP for its robustness and
ease of use, as well as to maintain comparability
with the work of Qiao et al. (2010) and Chan and
Rosenfeld (2012). Following these authors, we
use an engine designed for server-side recognition
5
Windows 7 or 8 (64-bit).
6
http://github.com/lex4all/lex4all
7
http://www.w3.org/TR/pronunciation-lexicon/
Figure 2: Overview of the core components of the
lex4all lexicon-building application.
of low-quality audio, since we aim to enable the
creation of useful applications for LRLs, includ-
ing those spoken in developing-world communi-
ties, and such applications should be able to cope
with telephone-quality audio or similar (Sherwani
and Rosenfeld, 2008).
4.2 Implementation of the Salaam method
Pronunciations (sequences of source-language
phonemes) for each term in the target vocabu-
lary are generated from the audio sample(s) of
that term using the iterative Salaam algorithm
(Sec. 2), which employs the source-language rec-
ognizer and a special recognition grammar. In
the first pass, the algorithm finds the best candi-
date(s) for the first phoneme of the sample(s), then
the first two phonemes in the second pass, and so
on until a stopping criterion is met. In our im-
plementation, we stop iterations if the top-scoring
sequence for a term has not changed for three con-
secutive iterations (Chan and Rosenfeld, 2012), or
if the best sequence from a given pass has a lower
confidence score than the best sequence from the
111
previous pass (Qiao et al., 2010). In both cases, at
least three passes are required.
After the iterative training has completed, the n-
best pronunciation sequences (with n specified by
users ? see Sec. 5.2) for each term are written to
the lexicon, each in a <phoneme> element corre-
sponding to the <grapheme> element containing
the term?s orthographic form (see Fig. 1).
4.3 Running time
A major challenge we faced in engineering a user-
friendly application based on the Salaam algo-
rithm was its long running time. The algorithm
depends on a ?super-wildcard? grammar that al-
lows the recognizer to match each sample of a
given term to a ?phrase? of 0-10 ?words?, each
word comprising any possible sequence of 1, 2, or
3 source-language phonemes (Qiao et al., 2010).
Given the 40 phonemes of US English, this gives
over 65,000 possibilities for each word, resulting
in a huge training grammar and thus a long pro-
cessing time. For a 25-term vocabulary with 5
training samples per term, the process takes ap-
proximately 1-2 hours on a standard modern lap-
top. For development and research, this long train-
ing time is a serious disadvantage.
To speed up training, we limit the length of each
?word? in the grammar to only one phoneme, in-
stead of up to 3, giving e.g. 40 possibilities in-
stead of tens of thousands. The algorithm can still
discover pronunciation sequences of an arbitrary
length, since, in each iteration, the phonemes dis-
covered so far are prepended to the super-wildcard
grammar, such that the phoneme sequence of the
first ?word? in the phrase grows longer with each
pass (Qiao et al., 2010). However, the new imple-
mentation is an order of magnitude faster: con-
structing the same 25-term lexicon on the same
hardware takes approximately 2-5 minutes, i.e.
less than 10% of the previous training time.
To ensure that the new implementation?s vastly
improved running time does not come at the cost
of reduced recognition accuracy, we evaluate and
compare word recognition accuracy rates using
lexicons built with the old and new implementa-
tions. The data we use for this evaluation is a
subset of the Yoruba data collected by Qiao et al.
(2010), comprising 25 Yoruba terms (words) ut-
tered by 2 speakers (1 male, 1 female), with 5
samples of each term per speaker. To determine
same-speaker accuracy for each of the two speak-
Old New p
Female average 72.8 73.6 0.75
Male average 90.4 90.4 1.00
S
a
m
e
-
s
p
e
a
k
e
r
Overall average 81.6 82 0.81
Trained on male 70.4 66.4 ?
Trained on female 76.8 77.6 ?
C
r
o
s
s
-
s
p
e
a
k
e
r
Average 73.6 72 0.63
Table 1: Word recognition accuracy for Yoruba us-
ing old (slower) and new (faster) implementations,
with p-values from t-tests for significance of dif-
ference in means. Bold indicates highest accuracy.
ers, we perform a leave-one-out evaluation on the
five samples recorded per term per speaker. Cross-
speaker accuracy is evaluated by training the sys-
tem on all five samples of each term recorded by
one speaker, and testing on all five samples from
the other speaker. We perform paired two-tailed t-
tests on the results to assess the significance of the
differences in mean accuracy.
The results of our evaluation, given in Table 1,
indicate no statistically significant difference in
accuracy between the two implementations (all p-
values are above 0.5 and thus clearly insignifi-
cant). As our new, modified implementation of the
Salaam algorithm is much faster than the original,
yet equally accurate, lex4all uses the new imple-
mentation by default, although for research pur-
poses we leave users the option of using the origi-
nal (slower) implementation (see Section 5.2).
4.4 Discriminative training
Chan and Rosenfeld (2012) achieve increased ac-
curacy (gains of up to 5 percentage points) by
applying an iterative discriminative training al-
gorithm. This algorithm takes as input the set
of mapped pronunciations generated using the
Salaam algorithm; in each iteration, it simulates
recognition of the training audio samples using
these pronunciations, and outputs a ranked list of
the pronunciations in the lexicon that best match
each sample. Pronunciations that cause ?confu-
sion? between words in the vocabulary, i.e. pro-
nunciations that the recognizer matches to sam-
ples of the wrong word type, are thus identified
and removed from the lexicon, and the process is
repeated in the next iteration.
We implement this accuracy-boosting algorithm
in lex4all, and apply it by default. To enable fine-
112
tuning and experimentation, we leave users the
option to change the number of passes (4 by de-
fault) or to disable discriminative training entirely,
as mentioned in Section 5.2.
5 User interface
As mentioned above, we aim to make the creation
and evaluation of lexicons simple, fast, and above
all accessible to users with no expertise in speech
or language technologies. Therefore, the applica-
tion makes use of a simple GUI that allows users
to quickly and easily specify input and output file
paths, and to control the parameters of the lexicon-
building algorithms.
Figure 3 shows the main interface of the lex4all
lexicon builder. This window displays the terms
that have been specified and the number of audio
samples that have been selected for each word.
Another form, accessed via the ?Add word? or
?Edit? buttons, allows users to add to or edit the
vocabulary by simply typing in the desired ortho-
graphic form of the word and selecting the audio
sample(s) to be used for pronunciation discovery
(see Sec. 5.1 for more details on audio input).
Once the target vocabulary and training audio
have been specified, and the additional options
have been set if desired, users click the ?Build
Lexicon? button and specify the desired name and
target directory of the lexicon file to be saved, and
pronunciation discovery begins. When all pronun-
ciations have been generated, a success message
displaying the elapsed training time is displayed,
and users may either proceed to the evaluation
module to assess the newly created lexicon (see
Sec. 6), or return to the main interface to build an-
other lexicon.
5.1 Audio input and recording
The GUI allows users to easily browse their file
system for pre-recorded audio samples (.wav
files) to be used for lexicon training. To simplify
data collection and enable the development of lexi-
cons even for zero-resource languages, lex4all also
offers a simple built-in audio recorder to record
new speech samples.
The recorder, built using the open-source library
NAudio,
8
takes the default audio input device as
its source and records one channel with a sampling
rate of 8 kHz, as the recognition engine we employ
is designed for low-quality audio (see Section 4.1).
8
http://naudio.codeplex.com/
Figure 3: Screenshot of the lexicon builder.
5.2 Additional options
As seen in Figure 3, lex4all includes optional con-
trols for quick and easy fine-tuning of the lexicon-
building process (the default settings are pictured).
First of all, users can specify the maxi-
mum number of pronunciations (<phoneme> el-
ements) per word that the lexicon may contain;
allowing more pronunciations per word may im-
prove recognition accuracy (Qiao et al., 2010;
Chan and Rosenfeld, 2012). Secondly, users may
train the lexicon using our modified, faster imple-
mentation of the Salaam algorithm or the origi-
nal implementation. Finally, users may choose
whether or not discriminative training is applied,
and if so, how many passes are run (see Sec. 4.4).
6 Evaluation module for research
In addition to its primary utility as a lexicon-
building tool, lex4all is also a valuable research
aide thanks to an evaluation module that allows
users to quickly and easily evaluate the lexicons
they have created. The evaluation tool allows users
to browse their file system for an XML lexicon file
that they wish to evaluate; this may be a lexicon
created using lex4all, or any other lexicon in the
same format. As in the main interface, users then
select one or more audio samples (.wav files)
for each term they wish to evaluate. The system
then attempts to recognize each sample using the
given lexicon, and reports the counts and percent-
ages of correct, incorrect, and failed recognitions.
113
Users may optionally save this report, along with
a confusion matrix of word types, as a comma-
separated values (.csv) file.
The evaluation module thus allows users to
quickly and easily assess different configurations
of the lexicon-building tool, by simply changing
the settings using the GUI and evaluating the re-
sulting lexicons. Furthermore, as the applica-
tion?s source code is freely available and modifi-
able, researchers may even replace entire modules
of the system (e.g. use a different pronunciation-
discovery algorithm), and use the evaluation mod-
ule to quickly assess the results. Therefore, lex4all
facilitates not only application development but
also further research into small-vocabulary speech
recognition using mapped pronunciation lexicons.
7 Conclusion and future work
We have presented lex4all, an open-source appli-
cation that enables the rapid automatic creation
of pronunciation lexicons in any (low-resource)
language, using an out-of-the-box commercial
recognizer for a high-resource language and the
Salaam method for cross-language pronunciation
mapping (Qiao et al., 2010; Chan and Rosen-
feld, 2012). The application thus makes small-
vocabulary speech recognition interfaces feasible
in any language, since only minutes of training au-
dio are required; given the built-in audio recorder,
lexicons can be constructed even for zero-resource
languages. Furthermore, lex4all?s flexible and
open design and easy-to-use evaluation module
make it a valuable tool for research in language-
independent small-vocabulary recognition.
In future work, we plan to expand the selection
of source-language recognizers; at the moment,
lex4all only uses US English as the source lan-
guage, but any of the 20+ other HRLs supported
by the MSP could be added. This would enable in-
vestigation of the target-language recognition ac-
curacy obtained using different source languages,
though our initial exploration of this issue sug-
gests that phonetic similarity between the source
and target languages might not significantly affect
accuracy (Vakil and Palmer, 2014). Another future
goal is to improve and extend the functionality of
the audio-recording tool to make it more flexible
and user-friendly. Finally, as a complement to the
application, it would be beneficial to create a cen-
tral online data repository where users can upload
the lexicons they have built and the speech sam-
ples they have recorded. Over time, this could be-
come a valuable collection of LRL data, enabling
developers and researchers to share and re-use data
among languages or language families.
Acknowledgements
The first author was partially supported by a
Deutschlandstipendium scholarship sponsored by
IMC AG. We thank Roni Rosenfeld, Hao Yee
Chan, and Mark Qiao for generously sharing their
speech data and valuable advice, and Dietrich
Klakow, Florian Metze, and the three anonymous
reviewers for their feedback.
References
Kalika Bali, Sunayana Sitaram, Sebastien Cuendet,
and Indrani Medhi. 2013. A Hindi speech recog-
nizer for an agricultural video search application. In
ACM DEV ?13.
Hao Yee Chan and Roni Rosenfeld. 2012. Discrimi-
native pronunciation learning for speech recognition
for resource scarce languages. In ACM DEV ?12.
Woosung Kim and Sanjeev Khudanpur. 2003. Lan-
guage model adaptation using cross-lingual infor-
mation. In Eurospeech.
Fang Qiao, Jahanzeb Sherwani, and Roni Rosenfeld.
2010. Small-vocabulary speech recognition for
resource-scarce languages. In ACM DEV ?10.
Tim Schlippe, Sebastian Ochs, and Tanja Schultz.
2014. Web-based tools and methods for rapid pro-
nunciation dictionary creation. Speech Communica-
tion, 56:101?118.
Tanja Schultz and Alex Waibel. 2001. Language-
independent and language-adaptive acoustic model-
ing for speech recognition. Speech Communication,
35(1-2):31 ? 51.
Jahanzeb Sherwani and Roni Rosenfeld. 2008. The
case for speech technology for developing regions.
In HCI for Community and International Develop-
ment Workshop, Florence, Italy.
Jahanzeb Sherwani. 2009. Speech interfaces for in-
formation access by low literate users. Ph.D. thesis,
Carnegie Mellon University.
Anjana Vakil and Alexis Palmer. 2014. Cross-
language mapping for small-vocabulary ASR in
under-resourced languages: Investigating the impact
of source language choice. In SLTU ?14.
Ngoc Thang Vu, Tim Schlippe, Franziska Kraus, and
Tanja Schultz. 2010. Rapid bootstrapping of five
Eastern European languages using the Rapid Lan-
guage Adaptation Toolkit. In Interspeech.
114
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 286?295, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Using the text to evaluate short answers for reading comprehension exercises
Andrea Horbach, Alexis Palmer and Manfred Pinkal
Department of Computational Linguistics, Saarland University, Saarbru?cken, Germany
(andrea|apalmer|pinkal)@coli.uni-saarland.de
Abstract
Short answer questions for reading compre-
hension are a common task in foreign lan-
guage learning. Automatic short answer scor-
ing is the task of automatically assessing the
semantic content of a student?s answer, mark-
ing it e.g. as correct or incorrect. While pre-
vious approaches mainly focused on compar-
ing a learner answer to some reference an-
swer provided by the teacher, we explore the
use of the underlying reading texts as addi-
tional evidence for the classification. First, we
conduct a corpus study targeting the links be-
tween sentences in reading texts for learners of
German and answers to reading comprehen-
sion questions based on those texts. Second,
we use the reading text directly for classifi-
cation, considering three different models: an
answer-based classifier extended with textual
features, a simple text-based classifier, and a
model that combines the two according to con-
fidence of the text-based classification. The
most promising approach is the first one, re-
sults for which show that textual features im-
prove classification accuracy. While the other
two models do not improve classification ac-
curacy, they do investigate the role of the text
and suggest possibilities for developing auto-
matic answer scoring systems with less super-
vision needed from instructors.
1 Introduction
Reading comprehension exercises are a common
means of assessment for language teaching: students
read a text in the language they are learning and are
then asked to answer questions about the text. The
types of questions asked of the learner may vary in
their scope and in the type of answers they are de-
signed to elicit; in this work we focus on ?short an-
swer? responses, which are generally in the range of
1?3 sentences.
The nature of the reading comprehension task is
that the student is asked to show that he or she has
understood the text at hand. Questions focus on one
or more pieces of information from the text, and cor-
rect responses should contain the relevant semantic
content. In the language learning context, responses
classified as correct might still contain grammatical
or spelling errors; the focus lies on the content rather
than the form of the learner answer.
Automatic scoring of short answer responses to
reading comprehension questions is in essence a tex-
tual entailment task, with the additional complica-
tion that, in order to answer a question correctly, the
learner must have identified the right portion of the
text. It isn?t enough that a student answer is en-
tailed by some part of the reading text; it must be
entailed by the part of the text which is responsive
to the question under discussion.
Previous approaches to automatic short answer
scoring have seldom considered the reading text it-
self, instead comparing student answers to target an-
swers supplied by instructors; we will refer to these
as answer-based models. In this paper we explore
the role of the text for short answer scoring, evalu-
ating several models for considering the text in au-
tomatic scoring, and presenting results of an anno-
tation study regarding the semantic links between
reading texts and answers to reading comprehension
questions.
286
TEXT: SCHLOSS PILLNITZ
This palace, which lies in the east of Dresden, is to me the most beautiful palace in the Dresden area. (. . . ) One special
attraction in the park is the camellia tree. In 1992, the camellia, which is more than 230 years old and 8.90 meters tall, got a
new, moveable home, in which temperature, ventilation, humidity, and shade are controlled by a climate regulation computer.
In the warm seasons, the house is rolled away from the tree. During the Blossom Time, from the middle of February until April,
the camellia has tens of thousands of crimson red blossoms. Every year, a limited number of shoots from the Pillnitz camellia
are sold during the Blossom Time, making it an especially worthwhile time to visit.
QUESTION:
A friend of yours would like to see the historic camellia tree. When should he go to Pillnitz, and why exactly at this time?
TARGET ANSWERS:
? From the middle of February until April is the Blossom Time.
? In spring the camellia has tens of thousands of crimson red blossoms.
LEARNER ANSWERS:
? [correct] He should go from the middle of February until April, because then the historic camellia has tens of thousands
of crimson red blossoms.
? [incorrect] Every year, a limited number of Pillnitz camellia are sold during the Blossom Time.
? [incorrect] All year round against temperature and humidity are controlled by a climate regulation computer.
Figure 1: Example of reading text with question and answers (translation by authors)
These investigations are done for German lan-
guage texts, questions, and answers. Figure 1 shows
a (translated) sample reading text, question, set of
target answers, and set of learner answers.
We show that the use of text-based features
improves classification performance over purely
answer-based models. We also show that a very sim-
ple text-based classifier, while it does not achieve
the same performance as the answer-based classifier,
does reach an accuracy of 76% for binary classifica-
tion (correct/incorrect) of student answers. The im-
plication of this for automatic scoring is that reason-
able results may be achievable with much less effort
on the part of instructors; namely, a classifier trained
on the supervision provided by marking the region
of a text relevant to a given question performs rea-
sonably well, though not as well as one trained on
full target answers.
The paper proceeds as follows: in Section 2 we
discuss the task and related approaches. In Sec-
tion 3, we describe our baseline model and the data
set we use. In Section 4 and Section 5 we discuss
our text-based models and present experiments and
results.
2 Approaches to short answer scoring
In short answer scoring (SAS) the task is to auto-
matically assign labels to individual learner answers.
Those labels can either be binary, a value on some
scale of points or grades, or a more fine-grained di-
agnosis. For example, one fine-grained set of labels
(Bailey, 2008) classifies answers as (among others)
correct, as missing a necessary concept or concepts,
containing extra content, or as failing to answer the
question. Our present study is restricted to binary
classification.
Previous work on SAS, including early systems
like (Leacock and Chodorow, 2003; Pulman and
Sukkarieh, 2005; Sukkarieh and Pulman, 2005) is
of course not only in the domain of foreign lan-
guage learning. For example, Mohler et al (2011)
and Mohler and Mihalcea (2009) use semantic graph
alignments and semantic similarity measures to as-
sess student answers to computer science questions,
comparing them to sample solutions provided by a
teacher. Accordingly, not all SAS settings include
reading or other reference texts; many involve only
questions, target answers, and learner answers. Our
approach is relevant for scenarios in which some sort
287
of reference text is available.
The work we present here is strongly based on
approaches towards SAS by Meurers and colleagues
(Bailey and Meurers, 2008; Meurers et al, 2011a;
Meurers et al, 2011b; Ziai et al, 2012). Specifically,
the sentence alignment model described in Section 3
(and again discussed in Section 4) is modeled after
the one used by Meurers et al to align target answers
and student answers.
Rather than using answers provided by instruc-
tors, Nielsen et al (2008) represent target answers to
science questions as a set of hand-annotated facets,
i.e. important aspects of the answer, typically repre-
sented by a pair of words and the relation that con-
nects them. Student answers, and consequently stu-
dents? understanding of target science concepts, are
then assessed by determining whether the relevant
facets are addressed by the learner answers.
Evaluating short answers on the basis of associ-
ated reading texts, as we do here, is a task related
to textual entailment. In the context of tutoring sys-
tems, Bethard et al (2012) identify students? mis-
conceptions of science concepts in essay writing us-
ing textual entailment techniques. They align stu-
dents? writings to extracted science concepts in or-
der to identify misconceptions, using a similar ap-
proach to identify the correct underlying concept.
An excellent and more detailed overview of re-
lated work can be found in Ziai et al (2012).
To our knowledge, there is no previous work that
uses reading texts as evidence for short answer scor-
ing in the context of foreign language learning.
3 Answer-based models
In order to compare to previous work, we first im-
plement an alignment-based model following that
proposed in (Meurers et al, 2011b). We refer to
this class of models as answer-based because they
function by aligning learner answers to instructor-
supplied target answers along several different di-
mensions, discussed below. Answers are then clas-
sified as correct or incorrect on the basis of features
derived from these alignments.
Wherever possible/practical, we directly re-
implement the Meurers model for German data.
In this section we describe relevant aspects of the
Meurers model, along with modifications and exten-
sions in our implementation of that model.1
Preprocessing
We preprocess all material (learner answers, target
answers, questions and reading texts) using stan-
dard NLP tools for sentence splitting and tokeniza-
tion (both OpenNLP2), POS tagging and stemming
(both Treetagger (Schmid, 1994)), NP chunking
(OpenNLP), and dependency parsing (Zurich Parser
(Sennrich et al, 2009)). We use an NE Tagger
(Faruqui and Pado?, 2010) to annotate named enti-
ties. Synonyms and semantic types are extracted
from GermaNet (Hamp and Feldweg, 1997).
For keywords, which serve to give more emphasis
to content words in the target answer, we extract all
nouns from the target answer.
Given that we are dealing with learner language,
but do not want to penalize answers for typical
learner errors, spellchecking (and subsequent cor-
rection of spelling errors) is especially important for
this task. Our approach is as follows: we first iden-
tify all words from the learner answers that are not
accepted by a German spellchecker (aspell3). We
then check for each word whether the word never-
theless occurs in the target answer, question or read-
ing text. If so, we accept it as correct. Otherwise, we
try to identify (using Levenshtein distance) which
word from the target answer, question, or reading
text is most likely to be the form intended by the
student.
Prior to alignment, we remove from the answer
all punctuation, stopwords (restricted to determiners
and auxiliaries), and material present in the question.
Alignment
The alignment process in short answer scoring ap-
proximates determination of semantic equivalence
between target answer and learner answer. Dur-
ing alignment, we identify matches between an-
swer pairs on a number of linguistic levels: tokens,
chunks, and dependency triples.
On the token level, we consider a number of dif-
ferent metrics for identity between tokens, with each
1Some extensions were made in order to bring performance
of our re-implementation closer to the figures reported in previ-
ous work.
2http://opennlp.apache.org/index.html
3http://aspell.net/
288
metric associated with a certain alignment weight.
After weights have been determined for all possible
token pairs, the best applicable weight is used as in-
put for a traditional marriage alignment algorithm
(Gale and Shapley, 1962).
We use the following types of identity (id),
weighted in the following order:
token id > lemma id >
spelling id > synonym & NE id >
similarity id>
NE type, semantic type & POS id
For synonym identity, we take a broad notion of
synonymy, extracting (from GermaNet) as potential
synonyms all words which are at most two levels (in
either direction) away from the target word. Simi-
larity identity is defined as two words having a Ger-
maNet path relatedness above some threshhold. In
order to have semantic type identity, two words must
have a common GermaNet hypernym (from a pre-
determined set of relevant hypernyms). Only some
closed-class words are eligible for POS identity. We
treat e.g. all types of determiners as POS identical.
Unlike, for example, alignment in machine trans-
lation, in which every token pair is considered a can-
didate for alignment, under the Meurers model only
candidates with at least one type of token identity are
available for alignment. This aims to prevent com-
pletely unrelated word pairs from being considered
for alignment.
In order to favor alignment of content words over
alignment of function words, and in departure from
the Meurers model, we use a content word multiplier
for alignment weights.
Chunks can only be aligned if at least one pair
of tokens within the respective chunks has been
aligned, and the percentage of aligned tokens be-
tween learner and target answer chunks is used as
input for the alignment process. Dependency triple
pairs are aligned when they share dependency rela-
tion, head lemma, and dependent lemma.
Features and classifier
After answers have been aligned, the following fea-
tures are extracted as input for the classifier: key-
word overlap (percentage of aligned keywords), tar-
get token overlap (percentage of aligned target to-
kens), learner token overlap (percentage of aligned
learner tokens), token match (percentage of token
alignments that are token identical), lemma match,
synonym match, type match, target triple overlap,
learner triple overlap, target chunk overlap, learner
chunk overlap, target bigram overlap, learner bigram
overlap, target trigram overlap, learner trigram over-
lap, and variety of alignment (number of different
token alignment types).
The n-gram features are the only new features in
our re-implementation of the Meurers model, hoping
to capture the influence of linear ordering of aligned
tokens. These features did not in the end improve
the model?s performance.
For classification, we use the timbl toolkit (Daele-
mans et al, 2009) for k-nearest neighbors classi-
fication. We treat all features as numeric values
and evaluate performance via leave-one-out cross-
validation. Further details appear in Section 5.
Data
For all work reported in this paper, we use the Ger-
man CREG corpus (Ott et al, 2012) of short answers
to questions for reading comprehension tasks. More
specifically, we use a balanced subset of the CREG
corpus containing a total of 1032 learner answers.
This corpus consists of 30 reading texts with an av-
erage of 5.9 questions per text. Each question is as-
sociated with one or more target answers, specified
by a teacher. For each question in turn there are an
average of 5.8 learner answers, each manually anno-
tated according to both binary and fine-grained la-
beling schemes. When there are several target an-
swers for a question, the best target answer for each
learner answer is indicated.
4 Text-based approach
Previous approaches to this task take the instructor-
supplied target answer(s) as a sort of supervision;
the target answer is meant to indicate the seman-
tic content necessary for a correct student answer.
Alignment between student answer and target an-
swer is then taken as a way of approximating se-
mantic equivalence. The key innovation of the cur-
rent study is to incorporate the reading text into the
evaluation of student answers. In this section we de-
scribe and evaluate three approaches to incorporat-
ing the text. The aim is to consider the semantic
289
relationships between target answer, learner answer,
and the text itself.4
A target answer is in fact just one way of ex-
pressing the requisite semantic content. Teachers
who create such exercises are obviously looking at
the text while creating target answers, and target an-
swers are often paraphrases of one or more sentences
of the reading text. Some learner answers which are
scored as incorrect by the answer-based system may
in fact be variant expressions of the same semantic
content as the target answer. Due to the nature of the
reading comprehension task, in which students are
able to view the text while answering questions, we
might expect students to express things in a man-
ner similar to the text. This is especially true for
language learners, as they are likely to have a lim-
ited range of options both for lexical expression and
grammatical constructions.
Along similar lines, one potential source of incor-
rect answers is an inability on the part of the stu-
dent to correctly identify the portion of the text that
is relevant to the question at hand. Our hypothesis
therefore is that a learner answer which links to the
same portion of the reading text as the target answer
is likely to be a correct answer. Similarly, a learner
answer which closely matches some part of the text
that is not related to the target answer is likely to be
incorrect.
Our text-based models investigate this hypothesis
in several different ways, described in Section 4.2.
4.1 Annotation study
The CREG data includes questions, learner answers,
target answers, and reading texts; associations be-
tween text and answers are not part of the anno-
tations. We undertook an annotation project in or-
der to have gold-standard source sentences for both
learner and target answers. This gold-standard is
then used to inform the text-based models described
below.
After removing a handful of problematic ques-
tions and their associated answers, we acquired hu-
man annotations for 889 of the 1032 learner an-
swers from the balanced subset of the CREG cor-
pus, in addition to 294 target answers. Each answer
4In future work we will also consider semantic relationships
between the question and the text.
was labeled separately by two (of three) annotators,
who were given the reading text and the question
and asked to identify the single best source sentence
from the text. Annotators were not told whether any
given instance was a target or learner answer, nor
whether learner answers were correct or incorrect.
Although we expected most answers to corre-
spond directly to a single text passage (Meurers et
al., 2011b), annotators were asked to look for (and
annotate appropriately) two different conditions in
which more than one source sentence may be rele-
vant. We refer to these as the repeated content con-
dition and the distributed content condition.
In the repeated content condition, the same se-
mantic content may be fully represented in more
than one sentence from the original text. In such
cases, we would expect the text to contain sen-
tences that are paraphrases or near-paraphrases of
one another. The distributed content condition oc-
curs when the relevant semantic content spans multi-
ple sentences, and some degree of synthesis or even
inference may be required to arrive at the answer.
Annotators were instructed to assume that pronouns
had been resolved; in other words, a sentence should
not be considered necessary semantic content sim-
ply because it contains the NP to which a pronoun in
another sentence refers. For both of these multiple-
sentence conditions, annotators were asked to select
one single-best source sentence from among the set
and also to mark the alternative source sentences.
For 31.2% of the answers annotated, one or more
annotator provided more than one possible source
sentence. Upon closer inspection, though, the an-
notations for these conditions are not very consis-
tent. In the repeated content condition, there is very
little agreement between annotators regarding when
the text contains more than one full-sentence source
for the answer. In the distributed content condition,
sometimes annotators disagree on the primary sen-
tence, and in many instances, one annotator identi-
fied multiple sentences and the other only one. Due
to these inconsistencies, for the purpose of this study
we decided to treat the multiple-sentence conditions
in an underspecified fashion. When an annotator has
identified either of these conditions, we convert the
annotations to a single-best sentence and a set of al-
ternatives.
The annotations were processed to automatically
290
Answer type agree altagree disagree nolink
Learner answers (all) 70.3% 9.4% 16.9% 3.4%
Learner answers (correct) 75.1% 11.7% 12.7% 0.5%
Learner answers (incorrect) 65.9% 7.3% 20.7% 6.4%
Target 73.1% 8.1% 17.3% 1.4%
Table 1: Inter-annotator agreement for linking answers to source sentences in text
produce a gold-standard set of source sentence IDs,
indicating the single sentence in the reading text to
which each answer is most closely linked. We iden-
tify four distinct categories with respect to agree-
ment between annotators. Agreement figures appear
in Table 1.
** agree: In this case, both annotators linked the
answer to the same source sentence, and that sen-
tence is identified as the gold-standard link to the
answer.
** altagree: This category covers two different
situations in which the two annotators fail to agree
on the single-best sentence. First, there are cases in
which the best sentence selected by one annotator
is a member of the set of alternatives indicated by
the other. Second, in a small number of cases, both
annotators agree on one member of the set of alter-
natives. In other words, the single sentence in the
intersection of the sets of sentences identified by the
two annotators is taken as the gold-standard annota-
tion. There was no (non-agree) case in which that
intersection contained more than one sentence.
** disagree: This category also includes two dif-
ferent types of cases. In the first, one of the two
annotators failed to identify a source sentence to
link with the answer. In that case, we consider
the annotators to be in disagreement, and for the
gold-standard we use the sentence ID provided by
the one responding annotator. In the second case,
the annotators disagree on the single-best sentence
and there is no overlap between indicated alterna-
tive sentences. In those cases, for the gold standard
we choose from the two source sentences that which
appears first in the reading text.5
** nolink: For a small number of answers (n=34),
5This is a relatively arbitrary decision motivated by the de-
sire to have a source sentence associated with as many answers
as possible. Future work may include adjudication of annota-
tions to reduce the noise introduced to the gold standard by this
category of responses.
both annotators found no link to the text. One ex-
ample of such a case is an answer given entirely in
English. For these cases, the gold standard provides
no best source sentence.
If we consider both altagree and nolink to be
forms of agreement, interannotator agreement is
about 74% for both learner and target answers.
4.2 Text-based models
In this paper we consider two different models for
incorporating the reading text into automatic short
answer scoring. In the first approach, we employ
a purely text-based model. The second combines
either text-based features or the text-based model
with the answer-based model described in Section 3.
Evaluation of all three approaches appears in Sec-
tion 5.
4.2.1 Simple text-based model
This model classifies student answers by compar-
ing the source sentence most closely associated with
the student answer to that associated with the tar-
get answer. If the two sentences are identical, the
answer is classified as correct, and otherwise as in-
correct.
We consider both the annotated best sentences
(goldlink) and automatically-identified answer-
sentence pairs (autolink). For automatic identifica-
tion, we use the alignment model described in Sec-
tion 3 to identify the best matching source sentence
in the text for both learner and target answers. We
use the token alignment process to align a given an-
swer with each sentence from its respective reading
text; the best-matching source sentence is that with
the highest alignment weight. Chunk alignments are
used only for correction of token alignments, and
dependency alignments are not considered.
This model takes an extremely simple approach to
answer classification, and could certainly be refined
and improved. At the same time, its relatively strong
291
performance (see Table 3) suggests that the mini-
mal level of supervision offered by teachers simply
marking the sentence of a text most relevant to a
given reading comprehension question may be ben-
eficial for automatic answer scoring.
4.2.2 Combining text-based and answer-based
models
In addition to the purely text-based model, we ex-
plore two ways of combining text- and answer-based
models.
Textual features in the answer-based model In
the first, we extract four features from the align-
ments between answers and source sentences and in-
corporate these as additional features in the answer-
based model.
Features 1, 3, and 4 are each computed in two
versions, using source sentences from either the an-
notated gold standard (goldlink), or the alignment
model (autolink).
1. SourceAgree This boolean feature is true if
both learner and target answer link to the same
source sentence, and false otherwise (also if no
source sentence was annotated or automatically
found).
2. SourceEntropy For this feature we look at the
two most-likely source sentences for the learner
answer, as determined by automatic alignment
scores. We treat the alignment weights as
probabilities, normalizing so that they sum up
to one. We then take the entropy between
these two alignment weights as indicative of the
confidence of the automatic alignment for the
learner answer.
3. AgreeEntropy Here we weight the first feature
according to the second, taking the entropy as a
confidence score for the binary feature. Specif-
ically, we value SourceAgree at 0.5 when the
feature is true, ?0.5 when false, and multiply
this with (1? entropy).
4. TextAdjacency This feature captures the dis-
tance (in number of sentences) between the
source sentence linked to the learner answer
and that linked to the target answer. With this
feature we aim to capture the tendency of adja-
cent passages in a text to exhibit topical coher-
ence (Mirkin et al, 2010).
Classifier combination In the second approach,
we combine the output of the answer-based and
text-based classifiers to arrive at a final classifica-
tion system, allowing the text-based classifier to pre-
dominate in those cases for which it is most con-
fident and falling back to the answer-based classi-
fier for other cases. Confidence of the text-based
classifier is determined based on entropy of the two
highest-scoring alignments between learner answer
and source sentence. The entropy threshhold was
determined empirically to 0.5.
5 Experiments and results
This section discusses experiments on short an-
swer scoring (binary classification) for German, in
the context of reading comprehension for language
learning. Specifically, we investigate the text-based
models described in Section 4.2. In all cases, fea-
tures and parameter settings were tuned on a de-
velopment set which was extracted from the larger
CREG corpus. In other words, there is no over-
lap between test and development data. For test-
ing, we perform leave-one-out cross-validation on
the slightly-smaller subset of the corpus which was
used for annotation.
5.1 Answer-based baseline
As a baseline for our text-based models we take
our implementation of the answer-based model from
(Meurers et al, 2011b). As previously mentioned,
our implementation diverges from theirs at some
points, and we do not quite reach the performance
reported for their model (accuracy of 84.6% on the
balanced CREG corpus) and are far from reaching
the current state of the art accuracy of 86.3%, as re-
ported in Hahn and Meurers (2012).
Our answer-based model appears as baseline in
Table 2. During development, the one extension to
the baseline which helped most was the use of ex-
tended synonyms. This variant of the model appears
in the results table with the annotation +syn.
292
model k=5 k=15 k=30
baseline 0.817 0.820 0.822
baseline+syn 0.822 0.826 0.825
text: goldlink 0.827 0.827 0.829
text+syn:goldlink 0.830 0.835* 0.837*
text:autolink 0.837* 0.836* 0.825
text+syn:autolink 0.844* 0.836* 0.832
combined 0.810 0.819 0.816
combined+syn 0.817 0.822 0.818
Table 2: Classification accuracy for answer-based base-
line (baseline), answer-based plus textual features (text),
and classifier combination (combined). +syn indicates
expanded synonymy features, goldlink indicates identi-
fying the source sentences via annotated links, autolink
indicates determining source sentences using the align-
ment model, k=number of neighbors. Results marked
with * are significant compared to the best baseline
model. See Section 5.2.1 for details.
5.2 Text-based models
As described in Section 4.2, we consider three dif-
ferent approaches for incorporating the reading text
into answer classification: use of textual features
in the answer-based model, combination of separate
answer-based and text-based models, and a simple
text-based classifier.
5.2.1 Combining text-based and answer-based
models
We explore two ways of combining text- and
answer-based models.
Adding textual features to the answer-based
model
We evaluate the contribution of the four new text-
based features, computed in two variations: with
source sentences as they are identified in the gold
standard (goldlink) and as they are computed using
the alignment model (autolink). We add those ad-
ditional features to the two answer-based systems:
the baseline (text) and the baseline with extended
synonym set (text+syn). Results are presented in
Table 2.
We present results for using the 5, 15, and 30 near-
est neighbors for classification, as the influence of
various features changes with the number of neigh-
bors. We calculate the significance for the difference
autolink goldlink alt-set
Accuracy 0.762 0.722 0.747
P correct 0.805 0.781 0.753
R correct 0.667 0.585 0.702
F correct 0.729 0.668 0.727
P incorrect 0.735 0.689 0.742
R incorrect 0.851 0.849 0.788
F incorrect 0.789 0.761 0.764
Table 3: Classification accuracy, precision, recall, and f-
score for simple text-based classifier, under three differ-
ent conditions. See Section 5.2.2 for details.
between the best baseline model (0.826) and each
model which uses textual features, using a resam-
pling test (Edgington, 1986). The results marked
with a * in the Table 2 are significant at p ? 0.01.
Although the impact of the textual features is
clearly not as big with a stronger baseline model,
we still see a clear pattern of improved accuracy.
We may expect this difference to increase with more
data and with additional and/or improved text-based
features.
Classifier combination
Combining the two classifiers (answer-based and
text-based) according to confidence levels results in
decreased performance compared to the baseline.
These results appear in Table 2 as combined.
5.2.2 Simple text-based classification
We have seen that textual features improve clas-
sification accuracy over the answer-driven model,
yet this approach still requires the supervision pro-
vided by teacher-supplied target answers. In our
third model, we investigate how the system performs
without this degree of supervision, considering how
far we can get by using only the text.
The simple text-based classifier, rather than tak-
ing a feature-based approach to classification, bases
its decision solely on whether or not the learner and
target answers link to the same source sentence. We
compare three different methods for obtaining these
links. The first approach (autolink) automatically
links each answer to a source sentence from the
text, based on alignments as described in Section 3.
The second (goldlink) uses links as provided by the
gold standard; in this case, learner answers without
293
a linked sentence (e.g. nolink cases) are immedi-
ately classified as incorrect. The third approach (alt-
set) exploits that fact that in many cases annotators
provided alternate source sentences. Under this ap-
proach, an answer is classified as correct provided
that there is a non-empty intersection between the
set of possible source sentences for the learner an-
swer and that for the target answer. For the second
and third approaches, we classify as incorrect those
learner answers lacking a gold-standard annotation
for the corresponding target answer.
In Table 3 we present classification accuracy, pre-
cision, recall, and f-score for the three different con-
ditions. Precision, recall, and f-score are reported
separately for correct and incorrect learner answers.
The 76% accuracy reached using the simple text-
based classifier suggests that a system which has
teachers supply source sentences instead of target
answers and then automatically aligns learner an-
swers to the text, while nowhere near comparable to
the state-of-the-art supervised system, still achieves
a reasonably accurate classification.
6 Conclusion
In this paper we have presented the first use of
reading texts for automatic short answer scoring in
the context of foreign language learning. We show
that, for German, the use of simple text-based fea-
tures improves classification accuracy over purely
answer-based models. We plan in the future to inves-
tigate a wider range of text-based features. We have
also shown that a simple classification model based
only on linking answers to source sentences in the
text achieves a reasonable classification accuracy.
This finding has the potential to reduce the amount
of teacher supervision necessary for authoring short
answer exercises within automatic answer scoring
systems. In addition to these findings, we have pre-
sented the results of an annotation study linking both
target and learner answers to source sentences.
In the near-term future we plan to further inves-
tigate the role of the reading text for short answer
scoring along three lines. First, we will address the
question of the best size of text unit for alignment.
In many cases, the best answers are linked not to
entire sentences but to regions of sentences; in oth-
ers, answers may correspond to more than one sen-
tence. Our current approach ignores this issue. Sec-
ond, we are interested in the variety of semantic re-
lationships holding between questions, answers and
texts. Along these lines, we will further investigate
the sets of alternatives provided by annotators, as
well as bringing in notions from work on paraphras-
ing and recognizing textual entailment. Finally, we
are interested in moving from simple binary classi-
fication to the fine-grained level of diagnosis.
Acknowledgments
We would like to thank Erik Hahn, David Alejandro
Przybilla and Jonas Sunde for carrying out the an-
notations. We thank the three anonymous reviewers
for their helpful comments. This work was funded
by the Cluster of Excellence ?Multimodal Comput-
ing and Interaction? of the German Excellence Ini-
tiative and partially funded through the INTERREG
IV A programme project ALLEGRO (Project No.:
67 SMLW 11137).
References
Stacey Bailey and Detmar Meurers. 2008. Diagnosing
meaning errors in short answers to reading comprehen-
sion questions. In Proceedings of the Third Workshop
on Innovative Use of NLP for Building Educational
Applications, pages 107?115, Columbus, Ohio, June.
Stacey Bailey. 2008. Content Assessment in Intelligent
Computer-Aided Language Learning: Meaning Error
Diagnosis for English as a Second Language. Ph.D.
thesis, The Ohio State University.
Steven Bethard, Haojie Hang, Ifeyinwa Okoye, James H.
Martin, Md. Arafat Sultan, and Tamara Sumner. 2012.
Identifying science concepts and student misconcep-
tions in an interactive essay writing tutor. In Proceed-
ings of the Seventh Workshop on Building Educational
Applications Using NLP, pages 12?21.
Walter Daelemans, Jakub Zavrel, Ko Sloot, and Antal
Van Den Bosch. 2009. TiMBL: Tilburg Memory-
Based Learner, version 6.2, Reference Guide. ILK
Technical Report 09-01.
Eugene S Edgington. 1986. Randomization tests. Mar-
cel Dekker, Inc., New York, NY, USA.
Manaal Faruqui and Sebastian Pado?. 2010. Training and
evaluating a German named entity recognizer with se-
mantic generalization. In Proceedings of KONVENS
2010, Saarbru?cken, Germany.
David Gale and Lloyd S. Shapley. 1962. College ad-
missions and the stability of marriage. The American
Mathematical Monthly, 69(1):9?15.
294
Michael Hahn and Detmar Meurers. 2012. Evaluat-
ing the meaning of answers to reading comprehension
questions: A semantics-based approach. In Proceed-
ings of the 7th Workshop on Innovative Use of NLP
for Building Educational Applications (BEA7), pages
326?336, Montreal, Canada. Association for Compu-
tational Linguistics.
Birgit Hamp and Helmut Feldweg. 1997. Germanet - a
lexical-semantic net for German. In In Proceedings of
ACL workshop Automatic Information Extraction and
Building of Lexical Semantic Resources for NLP Ap-
plications, pages 9?15.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey Bai-
ley. 2011a. Integrating parallel analysis modules to
evaluate the meaning of answers to reading compre-
hension questions. Special Issue on Free-text Auto-
matic Evaluation. International Journal of Continu-
ing Engineering Education and Life-Long Learning
(IJCEELL), 21(4):355?369.
Detmar Meurers, Ramon Ziai, Niels Ott, and Janina
Kopp. 2011b. Evaluating answers to reading com-
prehension questions in context: Results for German
and the role of information structure. In Proceedings
of the TextInfer 2011 Workshop on Textual Entailment,
pages 1?9, Edinburgh, Scottland, UK.
Shachar Mirkin, Ido Dagan, and Sebastian Pado?. 2010.
Assessing the role of discourse references in entail-
ment inference. In ACL.
Michael Mohler and Rada Mihalcea. 2009. Text-to-text
semantic similarity for automatic short answer grad-
ing. In Alex Lascarides, Claire Gardent, and Joakim
Nivre, editors, EACL, pages 567?575.
Michael Mohler, Razvan C. Bunescu, and Rada Mihal-
cea. 2011. Learning to grade short answer questions
using semantic similarity measures and dependency
graph alignments. In Dekang Lin, Yuji Matsumoto,
and Rada Mihalcea, editors, ACL, pages 752?762.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008. Learning to assess low-level conceptual under-
standing. In David Wilson and H. Chad Lane, editors,
FLAIRS Conference, pages 427?432.
Niels Ott, Ramon Ziai, and Detmar Meurers. 2012. Cre-
ation and analysis of a reading comprehension exer-
cise corpus: Towards evaluating meaning in context.
In Thomas Schmidt and Kai Wo?rner, editors, Mul-
tilingual Corpora and Multilingual Corpus Analysis,
Hamburg Studies in Multilingualism (HSM), pages
47?69. Benjamins, Amsterdam.
Stephen G. Pulman and Jana Z. Sukkarieh. 2005. Au-
tomatic short answer marking. In Proceedings of the
second workshop on Building Educational Applica-
tions Using NLP, EdAppsNLP 05, pages 9?16.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, Manchester, United Kingdom.
Rico Sennrich, Gerold Schneider, Martin Volk, and Mar-
tin Warin. 2009. A new hybrid dependency parser
for German. In Christian Chiarcos, Richard Eckart
de Castilho, and Manfred Stede, editors, Von der Form
zur Bedeutung: Texte automatisch verarbeiten ? From
Form to Meaning: Processing Texts Automatically.
Proceedings of the Biennial GSCL Conference 2009,
pages 115?124. Narr, Tu?bingen.
Jana Z. Sukkarieh and Stephen G. Pulman. 2005. In-
formation extraction and machine learning: Auto-
marking short free text responses to science questions.
In Chee-Kit Looi, Gordon I. McCalla, Bert Bredeweg,
and Joost Breuker, editors, AIED, volume 125 of Fron-
tiers in Artificial Intelligence and Applications, pages
629?637.
Ramon Ziai, Niels Ott, and Detmar Meurers. 2012.
Short answer assessment: Establishing links between
research strands. In Proceedings of the 7th Workshop
on Innovative Use of NLP for Building Educational
Applications (BEA7), Montreal, Canada.
295
Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 30?34,
Avignon, France, April 23 - 24 2012. c?2012 Association for Computational Linguistics
Visualising Typological Relationships: Plotting WALS with Heat Maps
Richard Littauer
University of Saarland
Computational Linguistics
Saarbru?cken, Germany
richard.littauer@gmail.com
Rory Turnbull
Ohio State University
Department of Linguistics
Columbus, Ohio
turnbull@ling.osu.edu
Alexis Palmer
University of Saarland
Computational Linguistics
Saarbru?cken, Germany
apalmer@coli.uni-sb.de
Abstract
This paper presents a novel way of vi-
sualising relationships between languages.
The key feature of the visualisation is that
it brings geographic, phylogenetic, and
linguistic data together into a single im-
age, allowing a new visual perspective on
linguistic typology. The data presented
here is extracted from the World Atlas of
Language Structures (WALS) (Dryer and
Haspelmath, 2011). After pruning due to
low coverage of WALS, we filter the typo-
logical data by geographical proximity in
order to ascertain areal typological effects.
The data are displayed in heat maps which
reflect the strength of similarity between
languages for different linguistic features.
Finally, the heat maps are annotated for lan-
guage family membership. The images so
produced allow a multi-faceted perspective
on the data which we hope will facilitate the
interpretation of results and perhaps illumi-
nate new areas of research in linguistic ty-
pology.
1 Introduction
This paper presents a novel way of visualising re-
lationships between languages. Relationships be-
tween languages can be understood with respect
to linguistic features of the languages, their geo-
graphical proximity, and their status with respect
to historical development. The visualisations pre-
sented in this paper are part of a new attempt to
bring together these three perspectives into a sin-
gle image. One line of recent work brings com-
putational methods to bear on the formation and
use of large typological databases, often using so-
phisticated statistical techniques to discover rela-
tions between languages (Cysouw, 2011; Daume?
III and Campbell, 2007; Daume? III, 2009, among
others), and another line of work uses typolog-
ical data in natural language processing (Georgi
et al, 2010; Lewis and Xia, 2008, for example).
The task of visually presenting the resulting data
in this way has been only infrequently addressed.
We are aware of some similar work (Mayer et al,
2010; Rohrdantz et al, 2010) in visualising dif-
ferences in linguistic typology, phylogeny (Mul-
titree, 2009), and geographical variation (Wiel-
ing et al, 2011). Here, we present our method
for addressing the visualisation gap, bringing to-
gether phylogeny, typology, and geography by us-
ing data from the World Atlas of Language Struc-
tures (Dryer and Haspelmath, 2011) to develop
heat maps that can visually show the intercon-
nected relationships between languages and lan-
guage families.
The main envisioned application of our visual-
isations is in the area of linguistic typology. Ty-
pology has been used to derive implications about
possible languages, and about the ordering of the
human mind. Different theorists have taken dif-
ferent views on the relationship between typology
and the universality of languages. For example,
Greenberg (1963), a foundational work, identified
a number of cross-linguistic typological proper-
ties and implications and aimed to present them
as truly universal ? relevant for all languages. In a
similar vein, typological universals have been em-
ployed as evidence in a generative story regarding
language learning (Chomsky, 2000).
Taking a different perspective, Dunn et al
(2011) argued that a language?s typology relies
upon the previous generations? language more
than on any biological, environmental or cogni-
tive constraints, and that there are pathways which
30
are generally followed in language change based
on the previous parent language. What these argu-
ments have in common is a reliance on a view of
linguistic typology that is potentially restricted in
its scope, due to insufficient access to broad-scale
empirical data, covering many features of many
languages of the world.
The most comprehensive computational re-
source for linguistic typology currently avail-
able is the World Atlas of Language Structures
(WALS).1 WALS is a large database of details
of structural properties of several thousand lan-
guages (Dryer and Haspelmath, 2011). The prop-
erties were collected from descriptive sources by
the project?s 55 authors.
However, of the 2,678 languages and 192 fea-
tures in WALS, only 16% of the possible data
points are actually specified?the data are sparse,
and the sparsity of the data naturally makes it dif-
ficult to perform reliable statistical analysis. One
way to work around this limitation is to seek
meaningful visualisations of the data in WALS,
instead of simply relying on raw numbers. This is
our approach.
In this paper, we first discuss in more detail
the source data and the types of information ex-
tracted, followed by a discussion of some diffi-
culties presented by the available data and our
approaches for addressing those difficulties. Fi-
nally, we present a sample of the resulting visual-
isations.
2 Aspects of the Visualisations
The visualisations described here bring together
three types of information: linguistic features, ge-
ographical distance, and phylogenetic distance.
For the current study, all three types of informa-
tion are extracted from the WALS database. In
future work, we would explore alternate sources
such as Ethnologue (Lewis, 2009) or MultiTree
(2009) for alternate phylogenetic hierarchies.
2.1 Linguistic features
At the time of writing, WALS contains infor-
mation for 2,678 languages. The linguistic fea-
tures covered in WALS range from phonetic and
phonological features, over some lexical and mor-
phological features, to syntactic structures, word
1As of 2008, WALS is browsable online (http://
www.wals.info).
order tendencies, and other structural phenomena.
A total of 192 features are represented, grouped
in 144 different chapters, with each chapter ad-
dressing a set of related features. Ignoring the fact
that a language having certain features will can-
cel out the possibility (or diminish the probabil-
ity) of others, only 15.8% of WALS is described
fully. In other words, if we consider WALS to be
a 2,678x192 grid, fewer than 16% of the grid?s
squares contain feature values.
The coverage of features/chapters varies dra-
matically across languages, with an average of 28
feature values per language. The most populated
feature has data for 1,519 languages. Because of
the extreme sparsity of the data, we restricted our
treatment to only languages with values for 30%
or more of the available features?372 languages,
with a total of 36k feature values.
2.2 Phylogenetic distance
Languages are related phylogenetically either ver-
tically, by lineage, or horizontally, by contact.
In WALS, each language is placed in a tree hi-
erarchy that specifies phylogenetic relations. In
the WALS data files, this is specified by linking
at three different levels: family, such as ?Sino-
Tibetan?, sub-family, such as ?Tibeto-Burman?,
and genus, such as ?Northern Naga?. The WALS
phylogenetic hierarchies do not take into account
language contact. For that, we used geographic
coordinates, which are present in WALS, as a
proxy for contact.
2.3 Geographic distance
Geographic distance is an important aspect of ty-
pological study because neighbouring languages
often come to share linguistic features, even in
the absence of genetic relationship between the
languages. Each language in WALS is associ-
ated with a geographical coordinate representing
a central point for the main population of speakers
of that language. We use these data to determine
geographic distance between any two languages,
using the haversine formula for orthodromic dis-
tance.2 A crucial aspect of our visualisations
is that we produce them only for sets of lan-
guages within a reasonable geographic proximity
2This measure is inexact, especially over long distances,
due to the imperfect topography and non-spherical shape of
the earth, but it is computationally simple and is accurate
enough for our present purposes.
31
and with sufficient feature coverage in WALS.
For this study, we used two approaches to
clustering languages according to geographic dis-
tance. First, we chose an arbitrary radius in or-
der to create a decision boundary for clustering
neighbouring languages. For each language, that
language?s location is fixed as the centroid of the
cluster and every language within the given radius
is examined. We found that a radius of 500 kilo-
metres provides a sufficient number of examples
even after cleaning low-coverage languages from
the WALS data.
The second approach selected an arbitrary
lower bound for the number of languages in the
geographic area under consideration. If a suffi-
cient percentage (enough to graph) of the total
number of languages in the area remained after
cleaning the WALS data, we took this as a useful
area and did mapping for that area. This num-
ber is clearly under-representative of the amount
of contact languages, as only half of the world?s
languages are present in WALS with any degree
of coverage. This proxy was not as good as the
radius method at choosing specific, useful exam-
ples for the n-nearest neighbours, as the languages
chosen were often quite distant from one another.
3 Heat Map Visualisations
We focused on producing visualisations only for
features that are salient for the maximal number
of selected languages. We choose two heat maps
for display here, from the least sparse data avail-
able, to demonstrate the output of the visualisa-
tion method. The remaining visualisations, along
with all code used to produce the visualisations,
are available in a public repository.3
All data was downloaded freely from WALS,
all coding was done in either Python or R. The
code was not computationally expensive to run,
and the programming languages and methods are
quite accessible.
In a two-dimensional heat map, each cell of
a matrix is filled with a colour representing that
cell?s value. In our case, the colour of the cell rep-
resents the normalised value of a linguistic feature
according to WALS. Languages with the same
colour in a given row have the same value for
3https://github.com/RichardLitt/
visualizing-language
Ham
tai Una
Imo
nda
Was
kia
Am
ele
Awt
uw Usa
n
Kob
on
Yim
as
Alam
blak
Ara
pes
h (M
oun
tain
)
Kew
a
Tau
ya Hua
Yag
aria Dum
o
Sen
tani
Order of Object and Verb
Order of Negative Morpheme and Verb
Preverbal Negative Morphemes
Postverbal Negative Morphemes
Position of Negative Word With Respect to S, O, & V
Negative Morphemes
Order of Genitive and Noun
Position of Tense?Aspect Affixes
Order of Adposition and Noun Phrase
Order of Adjective and Noun
O & V Ordering and the Adj & N Ordering
Gender Distinctions in Independent Personal Pronouns
Person Marking on Adpositions
O & V Ordering and the Adposition & NP Ordering
Alignment of Verbal Person Marking
Figure 1: Geographically-focused heat map; see text
for details. The bar at the top of the image repre-
sents the language family of the language in that col-
umn: Pink = Border; Red = Trans-New Guinea; Blue
= Sepik; Brown = Lower Sepik-Ramu; Purple = Torri-
celli; Green = Skou; and Orange = Sentani.
that typological feature.4 Below we discuss two
types of heat maps, focusing first on geographic
and then on phylogenetic features.
3.1 Geographically-focused heat maps
For the geographic distance maps, for each lan-
guage present in the cleaned data, we identified
all possible languages that lay within 500km, and
sorted these languages until only the 16 closest
neighbours were selected. Once the set of lan-
guages was determined, we selected for graph-
ing only the most commonly-occurring features
across that set of languages.
To present the visualisation, we first centred
the source language in the map. This decision
was made in order to reduce the effect of one of
the primary issues with using distance on a two
dimensional graph; distance between two non-
source languages is not shown, meaning that one
could be to the north and another to the south.
This means that the languages on the extremes of
the map may be far apart from each other, and
should be viewed with caution.
Figure 1 shows a geographically-focused heat
map with values for various morphological and
word order features. The map is centred on Yi-
mas, a language spoken in New Guinea. The
features presented represent a particularly non-
4Due to this reliance on colour, we strongly suggest view-
ing the heat maps presented here in colour.
32
sparse section of WALS for this language area.
A number of insights can be gleaned here. Most
prominently, these languages are quite homoge-
nous with respect to the selected features. Given
that most of the languages do indeed belong to the
same language family (cf. top bar of the graph),
this is unlikely to be a chance effect. In the 5th
row (?O&V Ordering and the Adj&N Ordering?),
we see via the cluster of red cells a partial group-
ing of languages close to Yimas, with less sim-
ilarity at a greater distance. The nearly alter-
nating pattern we see for ?Position of Negative
Word With Respect to S,O,&V? may suggest areal
groups that have been split by the data-centring
function. Also, the checkerboard pattern for this
feature and the one below (?Postverbal Negative
Morphemes?) suggests a possible negative corre-
lation between these two linguistic features.
3.2 Phylogenetically-focused heat maps
To produce phylogenetically-focused visualisa-
tions, for each language we identified other lan-
guages coming from the same family, subfam-
ily, or genus. Figure 2 shows a phylogenetically-
focused heat map for Niger-Congo languages, ar-
ranged from west to east. A number of the west-
ern languages show red cells for features related
to relative clauses; these can be compared to
mostly blue cells in the eastern languages. We
also see some apparent groupings for variable
word order in negative clauses (red cells in west-
ern languages) and for NegSVO Order (purple
cells in western languages). For some pairs of
adjacent languages (most notably Bambara and
Supyire), we see clusters of shared features. Es-
pecially give the importance of Bambara for syn-
tactic argumentation (Culy, 1985), this graph is an
excellent example of visualisation pointing out an
intriguing area for closer analysis.
4 Conclusion
In this paper we present a new approach to visual-
ising relationships between languages, one which
allows for the simultaneous viewing of linguistic
features together with phylogenetic relationships
and geographical location and proximity. These
visualisations allow us to view language relation-
ships in a multi-faceted way, seeking to work
around the sparseness of available data and facili-
tate new insights into linguistic typology.
In this work we placed strong restrictions on
Diol
a?F
ogn
y
Wol
of
Tem
ne Kisi Gre
bo
Bam
bara Sup
yire Aka
n
Koro
mfe
Dag
ban
i
Ewe Yoru
ba
Ijo (
Kolo
kum
a)
Gwa
ri Igbo Biro
m
Fye
m
Bab
ung
o
Fula
 (Ni
geri
an)
Mum
uye
Ewo
ndo
Doy
ayo Kon
go
Gbe
ya B
oss
ang
oa
San
go
Luv
ale
Nko
re?K
iga Zulu Swa
hili
Order of Numeral and Noun
Postnominal relative clauses
Double?headed relative clauses
Reciprocal Constructions
Correlative relative clauses
Position of Polar Question Particles
Order of Person Markers on the Verb
Different word order in negative clauses
Optional Double Negation in SVO languages
NegSVO Order
SNegVO Order
SVNegO Order
Adjoined relative clauses
Order of Degree Word and Adjective
Comparative Constructions
Figure 2: Phylogenetic heat-map of Niger-Congo lan-
guages, arranged from west to east.
both feature coverage and selection of salient fea-
tures for representation, reducing the number of
graphs produced to 6 with geographic focus and
8 with phylogenetic focus. One topic for future
work is to explore other ways of working with
and expanding the available data in order to ac-
cess even more useful visualisations. In addition,
it would be very interesting to apply this visuali-
sation method to data from other sources, for ex-
ample, data from multiple related dialects. In such
cases, coverage is likely to be better, and the lan-
guages in question will have been selected already
for their relatedness, thus avoiding some of the
data-filtering issues that arise. Finally, we would
like to investigate more principled approaches to
selection, presentation, and ordering of linguistic
features in the heat maps.
Acknowledgments
We are grateful to the three anonymous reviewers
for helpful comments on the current paper as well
as interesting suggestions for future work.
References
Noam Chomsky. 2000. New Horizons in the Study of
Language and Mind. Cambridge University Press,
Cambridge, UK.
Christopher Culy. 1985. The complexity of the vo-
cabulary of Bambara. Linguistics and Philosophy,
8:345?351. 10.1007/BF00630918.
Michael Cysouw. 2011. Quantitative explorations
of the world-wide distribution of rare characteris-
tics, or: the exceptionality of northwestern european
languages. In Horst Simon and Heike Wiese, edi-
33
tors, Expecting the Unexpected, pages 411?431. De
Gruyter Mouton, Berlin, DE.
Hal Daume? III and Lyle Campbell. 2007. A Bayesian
model for discovering typological implications. In
Conference of the Association for Computational
Linguistics (ACL), Prague, Czech Republic.
Hal Daume? III. 2009. Non-parametric Bayesian
model areal linguistics. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL), Boulder, CO.
Matthew Dryer and Martin Haspelmath, editors. 2011.
The World Atlas of Language Structures Online.
Max Planck Digital Library, Munich, 2011 edition.
Michael Dunn, Simon Greenhill, Stephen Levinson,
and Russell Gray. 2011. Evolved structure of lan-
guage shows lineage-specific trends in word-order
universals. Nature, 473(7345):79?82.
Ryan Georgi, Fei Xia, and Will Lewis. 2010.
Comparing language similarity across genetic and
typologically-based groupings. In Proceedings of
COLING 2010.
Joseph Greenberg. 1963. Some universals of grammar
with particular reference to the order of meaningful
elements. In Joseph Greenberg, editor, Universals
of Language, pages 58?90. MIT Press, Cambridge,
MA.
William Lewis and Fei Xia. 2008. Automati-
cally identifying computationally relevant typolog-
ical features. In Proceedings of IJCNLP 2008.
M. Paul Lewis, editor. 2009. Ethnologue: Languages
of the World. SIL International, Dallas, TX, six-
teenth edition.
Thomas Mayer, Christian Rohrdantz, Frans Plank, Pe-
ter Bak, Miriam Butt, and Daniel Keim. 2010.
Consonant co-occurrence in stems across lan-
guages: automatic analysis and visualization of
a phonotactic constraint. In Proceedings of the
2010 Workshop on NLP and Linguistics: Finding
the Common Ground, NLPLING ?10, pages 70?78,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Multitree. 2009. Multitree: A digital library of lan-
guage relationships. Institute for Language Infor-
mation and Techology (LINGUIST List), Eastern
Michigan University, Ypsilanti, MI, 2009 edition.
Christian Rohrdantz, Thomas Mayer, Miriam Butt,
Frans Plank, and Daniel Keim. 2010. Compara-
tive visual analysis of cross-linguistic features. In
Proceedings of the International Symposium on Vi-
sual Analytics Science and Technology (EuroVAST
2010), pages 27?32. Poster paper; peer-reviewed
(abstract).
Martijn Wieling, John Nerbonne, and R. Harald
Baayen. 2011. Quantitative social dialectology:
Explaining linguistic variation geographically and
socially. PLoS ONE, 6(9):e23613, 09.
34
Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 77?85,
Baltimore, Maryland, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
SeedLing: Building and using a seed corpus
for the Human Language Project
Guy Emerson, Liling Tan, Susanne Fertmann, Alexis Palmer, and Michaela Regneri
Universit?at des Saarlandes
66123 Saarbr?ucken, Germany
{emerson, liling, susfert, apalmer, regneri}
@coli.uni-saarland.de
Abstract
A broad-coverage corpus such as the Hu-
man Language Project envisioned by Ab-
ney and Bird (2010) would be a powerful
resource for the study of endangered lan-
guages. Existing corpora are limited in
the range of languages covered, in stan-
dardisation, or in machine-readability. In
this paper we present SeedLing, a seed
corpus for the Human Language Project.
We first survey existing efforts to compile
cross-linguistic resources, then describe
our own approach. To build the foundation
text for a Universal Corpus, we crawl and
clean texts from several web sources that
contain data from a large number of lan-
guages, and convert them into a standard-
ised form consistent with the guidelines
of Abney and Bird (2011). The result-
ing corpus is more easily-accessible and
machine-readable than any of the underly-
ing data sources, and, with data from 1451
languages covering 105 language fami-
lies, represents a significant base corpus
for researchers to draw on and add to in
the future. To demonstrate the utility of
SeedLing for cross-lingual computational
research, we use our data in the test appli-
cation of detecting similar languages.
1 Introduction
At the time of writing, 7105 living languages
are documented in Ethnologue,
1
but Simons and
Lewis (2011) calculated that 37% of extant lan-
guages were at various stages of losing trans-
misson to new generations. Only a fraction
of the world?s languages are well documented,
fewer have machine-readable resources, and fewer
again have resources with linguistic annotations
1
http://www.ethnologue.com
(Maxwell and Hughes, 2006) - so the time to work
on compiling these resources is now.
Several years ago, Abney and Bird (2010; 2011)
posed the challenge of building a Universal Cor-
pus, naming it the Human Language Project. Such
a corpus would include data from all the world?s
languages, in a consistent structure, facilitating
large-scale cross-linguistic processing. The chal-
lenge was issued to the computational linguistics
community, from the perspective that the language
processing, machine learning, and data manipula-
tion and management tools well-known in com-
putational linguistics must be brought to bear on
the problems of documentary linguistics, if we
are to make any serious progress toward build-
ing such a resource. The Universal Corpus as
envisioned would facilitate broadly cross-lingual
natural language processing (NLP), in particular
driving innovation in research addressing NLP for
low-resource languages, which in turn supports
the language documentation process.
We have accepted this challenge and have be-
gun converting existing resources into a format
consistent with Abney and Bird?s specifications.
We aim for a collection of resources that includes
data: (a) from as many languages as possible, and
(b) in a format both in accordance with best prac-
tice archiving recommendations and also readily
accessible for computational methods. Of course,
there are many relevant efforts toward producing
cross-linguistic resources, which we survey in sec-
tion 2. To the best of our knowledge, though, no
existing effort meets these two desiderata to the
extent of our corpus, which we name SeedLing: a
seed corpus for the Human Language Project.
To produce SeedLing, we have drawn on four
web sources, described in section 3.2. To bring
the four resources into a common format and
data structure (section 3.1), each required differ-
ent degrees and types of cleaning and standardis-
ation. We describe the steps required in section 4,
77
presenting each resource as a separate mini-case
study. We hope that the lessons we learned in
assembling our seed corpus can guide future re-
source conversion efforts. To that end, many of the
resources described in section 2 are candidates for
inclusion in the next stage of building a Universal
Corpus.
We believe the resulting corpus, which at
present covers 1451 languages from 105 language
families, is the first of its kind: large enough and
consistent enough to allow broadly multilingual
language processing. To test this claim, we use
SeedLing in a sample application (section 5): the
task of language clustering. With no additional
pre-processing, we extract surface-level features
(frequencies of character n-grams and words) to
estimate the similarity of two languages. Unlike
most previous approaches to the task, we make
no use of resources curated for linguistic typol-
ogy (e.g. values of typological features as in
WALS (Dryer and Haspelmath, 2013), Swadesh
word lists). Despite our approach being highly
dependent on orthography, our clustering perfor-
mance matches the results obtained by Georgi
et al. (2010) using typolological features, which
demonstrates SeedLing?s utility in cross-linguistic
research.
2 Related Work
In this section, we review existing efforts to com-
pile multilingual machine-readable resources. Al-
though some commercial resources are available,
we restrict attention to freely accessible data.
2
Traditional archives. Many archives exist to
store the wealth of traditional resources produced
by the documentary linguistics community. Such
documents are increasingly being digitised, or
produced in a digital form, and there are a number
of archives which now offer free online access.
Some archives aim for a universal scope, such
as The Language Archive (maintained by the
Max Planck Institute of Psycholinguistics), Col-
lection Pangloss (maintained by LACITO), and
The Endangered Languages Archive (maintained
by SOAS). Most archives are regional, including
AILLA, ANLA, PARADISEC, and many others.
However, there are two main problems common
to all of the above data sources. Firstly, the data
2
All figures given below were correct at the time of writ-
ing, but it must be borne in mind that most of these resources
are constantly growing.
is not always machine readable. Even where the
data is available digitally, these often take the form
of scanned images or audio files. While both can
provide invaluable information, they are extremely
difficult to process with a computer, requiring an
impractical level of image or video pre-processing
before linguistic analysis can begin. Even textual
data, which avoids these issues, may not be avail-
able in a machine-readable form, being stored as
pdfs or other opaque formats. Secondly, when data
is machine readable, the format can vary wildly.
This makes automated processing difficult, espe-
cially if one is not aware of the details of each
project. Even when metadata standards and en-
codings agree, there can be idiosyncractic markup
or non-linguistic information, such as labels for
speakers in the transcript of a conversation.
We can see that there is still much work to be
done by individual researchers in digitising and
standardising linguistic data, and it is outside of
the scope of this paper to attempt this for the above
archives. Guidelines for producing new materi-
als are available from the E-MELD project (Elec-
tronic Metastructure for Endangered Languages
Data), which specifically aimed to deal with the
expanding number of standards for linguistic data.
It gives best practice recommendations, illustrated
with eleven case studies, and provides input tools
which link to the GOLD ontology language, and
the OLAC metadata set. Further recommenda-
tions are given by Bird and Simons (2003), who
describe seven dimensions along which the porta-
bility of linguistic data can vary. Various tools are
available from The Language Archive at the Max
Planck Institute for Psycholinguistics.
Many archives are part of the Open Language
Archive Community (OLAC), a subcommunity
of the Open Archives Initiative. OLAC main-
tains a metadata standard, based on the 15-element
Dublin Core, which allows a user to search
through all participating archives in a unified fash-
ion. However, centralising access to disparate re-
sources, while of course extremely helpful, does
not solve the problem of inconsistent standards.
Indeed, it can even be hard to answer simple ques-
tions like ?how many languages are represented??
In short, while traditional archives are invalu-
able for many purposes, for large-scale machine
processing, they leave much to be desired.
Generic corpus collections. Some corpus col-
lections exist which do not focus on endangered
78
languages, but which nonetheless cover an in-
creasing number of languages.
MetaShare (Multilingual Europe Technology
Alliance) provides data in a little over 100 lan-
guages. While language codes are used, they have
not been standardised, so that multiple codes are
used for the same language. Linguistic Data Con-
sortium (LDC) and the European Language Re-
sources Association (ELRA) both offer data in
multiple languages. However, while large in size,
they cover only a limited number of languages.
Furthermore, the corpora they contain are stored
separately, making it difficult to access data ac-
cording to language.
Parallel corpora. The Machine Translation
community has assembled a number of parallel
corpora, which are crucial for statistical machine
translation. The OPUS corpus (Tiedemann, 2012)
subsumes a number of other well-known parallel
corpora, such as Europarl, and covers documents
from 350 languages, with various language pairs.
Web corpora. There has been increasing inter-
est in deriving corpora from the web, due to the
promise of large amounts of data. The majority
of web corpora are however aimed at either one or
a small number of languages, which is perhaps to
be expected, given that the majority of online text
is written in a handful of high-resource languages.
Nonetheless, there have been a few efforts to apply
the same methods to a wider range of languages.
HC Corpora currently provides download of
corpora in 68 different language varieties, which
vary in size from 2M to 150M words. The cor-
pora are thus of a respectable size, but only 1% of
the world?s languages are represented. A further
difficulty is that languages are named, without the
corresponding ISO language codes.
The Leipzig Corpora Collection (LCC)
3
(Bie-
mann et al., 2007) provides download of corpora
in 117 languages, and dictionaries in a number of
others, bringing the total number of represented
languages up to 230. The corpora are large, read-
ily available, in plain-text, and labelled with ISO
language codes.
The Cr?ubad?an Project aims to crawl the web for
text in low-resource languages, and data is cur-
rently available for 1872 languages. This rep-
resents a significant portion of the world?s lan-
guages; unfortunately, due to copyright restric-
3
http://corpora.uni-leipzig.de
tions, only lists of n-grams and their frequencies
are publically available, not the texts themselves.
While the breadth of languages covered makes this
a useful resource for cross-linguistic research, the
lack of actual texts means that only a limited range
of applications are possible with this data.
Cross-linguistic projects. Responding to the
call to document and preserve the world?s lan-
guages, highly cross-linguistic projects have
sprung up, striving towards the aim of universality.
Of particular note are the Endangered Languages
Project, and the Rosetta Project. These projects
are to be praised for their commitment to univer-
sality, but in their current forms it is difficult to use
their data to perform large-scale NLP.
3 The Data
3.1 Universal Corpus and Data Structure
Building on their previous paper, Abney and Bird
(2011) describe the data structure they envisage
for a Universal Corpus in more detail, and we aim
to adopt this structure where possible. Two types
of text are distinguished:
Aligned texts consist of parallel documents,
aligned at the document, sentence, or word level.
Note that monolingual documents are viewed as
aligned texts only tied to a single language.
Analysed texts, in addition to the raw text, con-
tain more detailed annotations including parts of
speech, morphological information, and syntactic
relations. This is stored as a table, where rows rep-
resent words, and columns represent: document
ID, language code, sentence ID, word ID, word-
form, lemma, morphological information, part of
speech, gloss, head/governor, and relation/role.
Out of our data sources, three can be straight-
forwardly represented in the aligned text struc-
ture. However, ODIN contains richer annotations,
which are in fact difficult to fit into Abney and
Bird?s proposal, and which we discuss in section
3.2 below.
3.2 Data Sources
Although data size matters in general NLP, uni-
versality is the top priority for a Universal Corpus.
We focus on the following data sources, because
they include a large number of languages, include
several parallel texts, and demonstrate a variety of
data types which a linguist might encounter (struc-
tured, semi-structured, unstructured): the Online
79
Langs. Families Tokens Size
ODIN 1,270 100 351,161 39 MB
Omniglot 129 20 31,318 677 KB
UDHR 352 46 640,588 5.2 MB
Wikipedia 271 21 37 GB
Combined 1,451 105
Table 1: Corpus Coverage
Database of Interlinear Text (ODIN), the Om-
niglot website, the Universal Declaration of Hu-
man Rights (UHDR), and Wikipedia.
Our resulting corpus runs the full gamut of text
types outlined by Abney and Bird, ranging from
single-language text (Wikipedia) to parallel text
(UDHR and Omniglot) to IGTs (ODIN). Table 1
gives some coverage statistics, and we describe
each source in the following subsections. For 332
languages, the corpus contains data from more
than one source.
Universal Declaration of Human Rights. The
Universal Declaration of Human Rights (UDHR)
is a document released by the United Nations in
1948, and represents the first global expression of
human rights. It consists of 30 articles, amounting
to about four pages of text. This is a useful doc-
ument for NLP, since it has been translated into a
wide variety of languages, providing a highly par-
allel text.
Wikipedia. Wikipedia is a collaboratively-
edited encyclopedia, appealing to use for NLP
because of its large size and easy availability.
At the time of writing, it contained 30.8 million
articles in 286 languages, which provides a
sizeable amount of monolingual text in a fairly
wide range of languages. Text dumps are made
regularly available, and can be downloaded from
http://dumps.wikimedia.org.
Omniglot. The Omniglot website
4
is an online
encyclopedia of writing systems and languages.
We extract information from pages on ?Useful for-
eign phrases? and the ?Tower of Babel? story, both
of which give us parallel data in a reasonably large
number of languages.
ODIN. ODIN (The Online Database of Inter-
linear Text) is a repository of interlinear glossed
texts (IGTs) extracted from scholarly documents
(Lewis, 2006; Lewis and Xia, 2010). Compared to
other resources, it is notable for the breadth of lan-
4
http://www.omniglot.com
guages included and the level of linguistic annota-
tion. An IGT canonically consists of three lines:
(i) the source, a sentence in a target language, (ii)
the gloss, an analysis of each source element, and
(iii) the translation, done at the sentence level. The
gloss line can additionally include a number of lin-
guistic terms, which means that the gloss is written
in metalanguage rather than natural language. In
ODIN, translations are into English, and glosses
are written in an English-based metalanguage. An
accepted set of guidelines are given by the Leipzig
Glossing Rules,
5
where morphemes within words
are separated by hyphens (or equal signs, for cli-
tics), and the same number of hyphens should ap-
pear in each word of the source and gloss.
The data from ODIN poses the first obstacle to
straightforwardly adopting Abney and Bird?s pro-
posal. The suggested data structure is aligned at
the word level, and includes a specific list of rel-
evant features which should be used to annotate
words. When we try to adapt IGTs into this for-
mat, we run into certain problems. Firstly, there
is the problem that the most fundamental unit of
analysis according to the Leipzig Glossing Rules
is the morpheme, not the word. Ideally, we should
encode this information explicitly in a Universal
Corpus, assigning a unique identifier to each mor-
pheme (instead of, or in addition to each word).
Indeed, Haspelmath (2011) argues that there is no
cross-linguistically valid definition of word, which
undermines the central position of words in the
proposed data structure.
Secondly, it is unclear how to represent the
gloss. Since the gloss line is not written in a natu-
ral language, we cannot treat it as a simple trans-
lation. However, it is not straightforward to incor-
porate it into the proposed structure for analysed
texts, either. One possible resolution is to move
all elements of the gloss written in capital letters to
the MORPH field (as functional elements are usu-
ally annotated in this way), and all remaining el-
ements to the GLOSS field. However, this loses
information, since we no longer know which mor-
pheme has which meaning. To keep all informa-
tion encoded in the IGT, we need to modify Abney
and Bird (2011)?s proposal.
The simplest solution we can see is to allow
morphemes to be a level of structure in the Uni-
versal Corpus, just as documents, sentences, and
5
http://www.eva.mpg.de/lingua/
resources/glossing-rules.php
80
Figure 1: Heatmap of languages in SeedLing according to endangerment status
words already are. The overall architecture re-
mains unchanged. We must then decide how to
represent the glosses.
Even though glosses in ODIN are based on
English, having been extracted from English-
language documents, this is not true of IGTs in
general. For example, it is common for documen-
tary linguists working on indigenous languages of
the Americas to provide glosses and translations
based on Spanish. For this reason, we believe it
would be wise to specify the language used to pro-
duce the gloss. Since it is not quite the language
itself, but a metalanguage, one solution would be
to use new language codes that make it clear both
that a metalanguage is being used, and also what
natural language it is based on. The five-letter
code gloss cannot be confused with any code
in any version of ISO 639 (with codes of length
two to four). Following the convention that sub-
varieties of a language are indicated with suffixes,
we can append the code of the natural language.
For example, glosses into English and Spanish-
based metalanguages would be given the codes
gloss-eng and gloss-spa, respectively.
One benefit of this approach is that glossed texts
are treated in exactly the same way as parallel
texts. There is a unique identifier for each mor-
pheme, and glosses are stored under this identifier
and the corresponding gloss code. Furthermore,
to motivate the important place of parallel texts in
a Universal Corpus, Abney and Bird view trans-
lations into a high-resource reference language as
a convenient surrogate of meaning. By the same
reasoning, we can use glosses to provide a more
detailed surrogate of meaning, only written in a
metalanguage instead of a natural one.
3.3 Representation and Universality
According to Ethnologue, there are 7105 liv-
ing languages, and 147 living language families.
Across all our data sources, we manage to cover
1451 languages in 105 families, which represents
19.0% of the world?s languages. To get a bet-
ter idea of the kinds of languages represented,
we give a breakdown according to their EGIDS
scores (Expanded Graded Intergenerational Dis-
ruption Scale) (Lewis and Simons, 2010) in Fig-
ure 1. The values in each cell have been colored
according to proportion of languages represented,
with green indicating good coverage and red poor.
It?s interesting to note that vigorous languages (6a)
are poorly represented across all data sources, and
worse than more endangered categories. In terms
of language documentation, vigorous languages
are less urgent goals than those in categories 6b
and up, but this highlights an unexpected gap in
linguistic resources.
4 Data Clean-Up, Consistency, and
Standardisation
Consistency in data structures and formatting is
essential to facilitate use of data in computational
linguistics research (Palmer et al., 2010). In the
following subsections, we describe the process-
ing required to convert the data into a standardised
form. We then discuss standardisation of language
codes and file formats.
81
4.1 Case Studies
UDHR. We used the plain-text UDHR files
available from the Unicode website
6
which uses
UTF-8 encoding for all languages. The first four
lines of each file record metadata, and the rest is
the translation of the UDHR. This dataset is ex-
tremely clean, and simply required segmentation
into sentences.
Wikipedia. One major issue with using the
Wikipedia dump is the problem of separating text
from abundant source-specific markup. To con-
vert compressed Wikipedia dumps to textfiles, we
used the WikiExtractor
7
tool. After conversion
into textfiles, we used several regular expressions
to delete residual Wikipedia markup and so-called
?magic words?.
8
Omniglot. The main issue with extracting the
Omniglot data is that the pages are designed to
be human-readable, not machine-readable. Clean-
ing this data required parsing the HTML source,
and extracting the relevant content, which required
different code for the two types of page we con-
sidered (?Useful foreign phrases? and ?Tower of
Babel?). Even after automatic extraction, some
noise in the data remained, such as explanatory
notes given in parentheses, which are written in
English and not the target language. Even though
the total amount of data here is small compared to
our other sources, the amount of effort required
to process it was not, because of these idiosyn-
cracies. We expect that researchers seeking to
convert data from human-readable to machine-
readable formats will encounter similar problems,
but unfortunately there is unlikely to be a one-size-
fits-all solution to this problem.
ODIN. The ODIN data is easily accessible in
XML format from the online database
9
. Data
for each language is saved in a separate XML
file and the IGTs are encoded in tags of the form
<igt><example>...</example></igt>.
For example, the IGT in Figure 2 is represented
by the XML snippet in Figure 3.
The primary problem in extracting the data is a
lack of consistency in the IGTs. In the above ex-
6
http://unicode.org/udhr/d
7
http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor
8
http://en.wikipedia.org/wiki/Help:
Magic_words
9
http://odin.linguistlist.org/download
21 a. o lesu mai
2sg return here
?You return here.?
Figure 2: Fijian IGT from ODIN
<igt>
<example>
<line>21 a. o lesu mai</line>
<line>2sg return here</line>
<line>?You return here.?</line>
</example>
</igt>
Figure 3: Fijian IGT in ODIN?s XML format
amples, the sentence is introduced by a letter or
number, which needs to be removed; however, the
form of such indexing elements varies. In addi-
tion, the source line in Figure 4 includes two types
of metadata: the language name, and a citation,
both of which introduce noise. Finally, extrane-
ous punctuation such as the quotation marks in the
translation line need to be removed. We used regu-
lar expressions for cleaning lines within the IGTs.
4.2 Language Codes
As Xia et al. (2010) explain, language names do
not always suffice to identify languages, since
many names are ambiguous. For this reason, sets
of language codes exist to more accurately identify
languages. We use ISO 639-3
10
as our standard set
of codes, since it aims for universal coverage, and
has widespread acceptance in the community. The
data from ODIN and the UDHR already used this
standard.
To facilitate the standardization of language
codes, we have written a python API that can be
used to query information about a language or a
code, fetching up-to-date information from SIL
International (which maintains the ISO 639-3 code
set), as well as from Ethnologue.
Wikipedia uses its own set of language codes,
most of which are in ISO 639-1 or ISO 639-3.
The older ISO 639-1 codes are easy to recognise,
being two letters long instead of three, and can
be straightforwardly converted. However, a small
number of Wikipedia codes are not ISO codes at
all - we converted these to ISO 639-3, following
10
http://www-01.sil.org/iso639-3/
default.asp
82
<igt>
<example>
<line>(69) na-Na-tmi-kwalca-t
Yimas (Foley 1991)</line>
<line>3sgA-1sgO-say-rise-PERF
</line>
<line>?She woke me up?
(by verbal action)</line>
</example>
</igit>
Figure 4: Yimas IGT in ODIN?s XML format
documentation from the Wikimedia Foundation.
11
Omniglot does not give codes at all, but only the
language name. To resolve this issue, we automat-
ically converted language names to codes using in-
formation from the SIL website.
Some languages have more than one orthog-
raphy. For example, Mandarin Chinese is writ-
ten with either traditional or simplified charac-
ters; Serbian is written with either the Cyrillic or
the Roman alphabet. For cross-linguistic NLP, it
could be helpful to have standard codes to identify
orthographies, but at present none exist.
4.3 File Formats
It is important to make sure that the data we have
compiled will be available to future researchers,
regardless of how the surrounding infrastructure
changes. Bird and Simons (2003) describe a set of
best practices for maintaining portability of digi-
tal information, outlining seven dimensions along
which this can vary. Following this advice, we
have ensured that all our data is available as plain-
text files, with UTF-8 encoding, labelled with the
relevant ISO 639-3 code. Metadata is stored sepa-
rately. This allows users to easily process the data
using the programming language or software of
their choice.
To allow access to the data following Abney
and Bird?s guidelines, as discussed in section 3,
we have written an API, which we distribute along
with the data. Abney and Bird remain agnostic
to the specific file format used, but if an alterna-
tive format would be preferred, the data would
be straightfoward to convert since it can be ac-
cessed according to these guidelines. As exam-
ples of functionality, our API allows a user to fetch
all sentences in a given language, or all sentences
from a given source.
11
http://meta.wikimedia.org/wiki/
Special_language_codes
5 Detecting Similar Languages
To exemplify the use of SeedLing for compu-
tational research on low-resource languages, we
experiment with automatic detection of similar
languages. When working on endangered lan-
guages, documentary and computational linguists
alike face a lack of resources. It is often helpful to
exploit lexical, syntactic or morphological knowl-
edge of related languages. For example, similar
high-resource languages can be used in bootstrap-
ping approaches, such as described by Yarowsky
and Ngai (2001) or Xia and Lewis (2007).
Language classification can be carried out in
various ways. Two common approaches are ge-
nealogical classification, mapping languages onto
family trees according to their historical related-
ness (Swadesh, 1952; Starostin, 2010); and ty-
pological classification, grouping languages ac-
cording to linguistic features (Georgi et al., 2010;
Daum?e III, 2009). Both of these approaches re-
quire linguistic analysis. By contrast, we use
surface features (character n-gram and word uni-
gram frequencies) extracted from SeedLing, and
apply an off-the-shelf hierarchical clustering al-
gorithm.
12
Specifically, each language is repre-
sented as a vector of frequencies of character bi-
grams, character trigrams, and word unigrams.
Each of these three components is normalised to
unit length. Data was taken from ODIN, Om-
niglot, and the UDHR.
Experimental Setup. We first perform hierar-
chical clustering, which produces a tree structure:
each leaf represents a language, and each node
a cluster. We use linkage methods, which recur-
sively build the tree starting from the leaves. Ini-
tially, each language is in a separate cluster, then
we iteratively find the closest two clusters and
merge them. Each time we do this, we take the
two corresponding subtrees, and introduce a new
node to join them.
We define the distance between two clusters by
considering all possible pairs of languages, with
one from each cluster, and taking the largest dis-
tance. We experimented with other ways to de-
fine the distance between clusters, but results were
poor and we omit results for brevity.
To ease evaluation, we produce a partitional
clustering, by stopping when we reach a certain
number of clusters, set in advance.
12
http://www.scipy.org
83
Precision Recall F-score
SeedLing 0.255 0.205 0.150
Base. 1: random 0.184 0.092 0.068
Base. 2: together 0.061 1.000 0.112
Base. 3: separate 1.000 0.086 0.122
Table 2: Clustering compared with baselines
Figure 5: Performance against number of clusters
Evaluation. We compare our clustering to the
language families in Ethnologue. However, there
are many ways to evaluate clustering quality.
Amig?o et al. (2009) propose a set of criteria which
a clustering evaluation metric should satisfy, and
demonstrate that most popular metrics fail to sat-
isfy at least one of these criteria. However, they
prove that all criteria are satisfied by the BCubed
metric, which we therefore adopt. To calculate the
BCubed score, we take the induced cluster and
gold standard class for each language, and cal-
culate the F-score of the cluster compared to the
class. These F-scores are then averaged across all
languages.
In Table 2, we set the number of clusters to be
105, the number of language families in our data,
and compare this with three baselines: a random
baseline (averaged over 20 runs); putting all lan-
guages in a single cluster; and putting each lan-
guage in a separate cluster. Our clustering outper-
forms all baselines. It is worth noting that pre-
cision is higher than recall, which is perhaps ex-
pected, given that related languages using wildly
differing orthographies will appear distinct.
To allow a closer comparison with Georgi et al.
(2010), we calculate pairwise scores - i.e. consid-
ering if pairs of languages are in the same cluster
or the same class. For 105 clusters, we achieve
a pairwise f-score of 0.147, while Georgi et al.
report 0.140. The figures are not quite compa-
rable since we are evaluating over a different set
of languages; nonetheless, we only use surface
features, while Georgi et al. use typological fea-
tures from WALS. This suggests the possibility for
cross-linguistic research to be conducted based on
shallow features.
In Figure 5, we vary the number of clusters. The
highest f-score is obtained for 199 clusters. There
is a notable jump in performance between 98 and
99, just before the true number of families, 105.
Interpreting the clusters directly is difficult, be-
cause they are noisy. However, the distribution of
cluster sizes mirrors the true distribution - for 105
clusters, we have 48 clusters of size 1 or 2, with
the largest cluster of size 130; while in our gold
standard, there are 51 families with only 1 or 2
languages in the data, with the largest of size 150.
6 Conclusion and Outlook
In this paper, we have described the creation of
SeedLing, a foundation text for a Universal Cor-
pus, following the guidelines of Abney and Bird
(2010; 2011). To do this, we cleaned and standard-
ised data from several multilingual data sources:
ODIN, Omniglot, the UDHR, Wikipedia. The
resulting corpus is more easily machine-readable
than any of the underlying data sources, and has
been stored according to the best practices sug-
gested by Bird and Simons (2003). At present,
SeedLing has data from 19% of the world?s liv-
ing languages, covering 72% of language families.
We believe that a corpus with such diversity of lan-
guages, uniformity of format, cleanliness of data,
and ease of access provides an excellent seed for a
Universal Corpus. It is our hope that taking steps
toward creating this resource will spur both further
data contributions and interesting computational
research with cross-linguistic or typological per-
spectives; we have here demonstrated SeedLing?s
utility for such research by using the data to per-
form language clustering, with promising results.
SeedLing (data, API and documentation) is cur-
rently available via a GitHub repository.
13
We
have yet to fully address questions of long-term
access, and we welcome ideas or collaborations
along these lines.
13
https://github.com/alvations/SeedLing
84
Acknowledgements
We thank the three anonymous reviewers for their
helpful comments. This research was supported
in part by the Cluster of Excellence ?Multi-modal
Computing and Interaction? in the German Excel-
lence Initiative.
References
Steven Abney and Steven Bird. 2010. The Hu-
man Language Project: Building a Universal Cor-
pus of the world?s languages. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 88?97. Association for
Computational Linguistics.
Steven Abney and Steven Bird. 2011. Towards a data
model for the Universal Corpus. In Proceedings of
the 4th Workshop on Building and Using Compa-
rable Corpora: Comparable Corpora and the Web,
pages 120?127. Association for Computational Lin-
guistics.
Enrique Amig?o, Julio Gonzalo, Javier Artiles, and Fe-
lisa Verdejo. 2009. A comparison of extrinsic
clustering evaluation metrics based on formal con-
straints. Information retrieval, 12(4):461?486.
Chris Biemann, Gerhard Heyer, Uwe Quasthoff, and
Matthias Richter. 2007. The Leipzig Corpora
Collection-monolingual corpora of standard size.
Proceedings of Corpus Linguistic 2007.
Steven Bird and Gary Simons. 2003. Seven dimen-
sions of portability for language documentation and
description. Language, pages 557?582.
Hal Daum?e III. 2009. Non-parametric bayesian areal
linguistics. In Proceedings of human language tech-
nologies: The 2009 annual conference of the north
american chapter of the association for computa-
tional linguistics, pages 593?601. Association for
Computational Linguistics.
Matthew S. Dryer and Martin Haspelmath, editors.
2013. WALS Online. Max Planck Institute for Evo-
lutionary Anthropology, Leipzig.
Ryan Georgi, Fei Xia, and William Lewis. 2010.
Comparing language similarity across genetic and
typologically-based groupings. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics, pages 385?393. Association for
Computational Linguistics.
Martin Haspelmath. 2011. The indeterminacy of word
segmentation and the nature of morphology and syn-
tax. Folia Linguistica, 45(1):31?80.
M Paul Lewis and Gary F Simons. 2010. Assessing
endangerment: expanding fishman?s GIDS. Revue
roumaine de linguistique, 2:103?119.
William D Lewis and Fei Xia. 2010. Developing
ODIN: A multilingual repository of annotated lan-
guage data for hundreds of the world?s languages.
Literary and Linguistic Computing, 25(3):303?319.
William D Lewis. 2006. ODIN: A model for adapt-
ing and enriching legacy infrastructure. In e-Science
and Grid Computing, 2006. e-Science?06. Second
IEEE International Conference on, pages 137?137.
IEEE.
Mike Maxwell and Baden Hughes. 2006. Frontiers in
linguistic annotation for lower-density languages. In
Proceedings of the workshop on frontiers in linguis-
tically annotated corpora 2006, pages 29?37. Asso-
ciation for Computational Linguistics.
Alexis Palmer, Taesun Moon, Jason Baldridge, Katrin
Erk, Eric Campbell, and Telma Can. 2010. Compu-
tational strategies for reducing annotation effort in
language documentation. Linguistic Issues in Lan-
guage Technology, 3.
Gary F Simons and M Paul Lewis. 2011. The world?s
languages in crisis: A 20-year update. In 26th
Linguistic Symposium: Language Death, Endanger-
ment, Documentation, and Revitalization. Univer-
sity of Wisconsin, Milwaukee, pages 20?22.
George Starostin. 2010. Preliminary lexicostatistics as
a basis for language classification: a new approach.
Journal of Language Relationship, 3:79?117.
Morris Swadesh. 1952. Lexico-statistic dating of pre-
historic ethnic contacts: with special reference to
north american indians and eskimos. Proceedings of
the American philosophical society, pages 452?463.
J?org Tiedemann. 2012. Parallel data, tools and inter-
faces in OPUS. In LREC, pages 2214?2218.
Fei Xia and William D Lewis. 2007. Multilingual
structural projection across interlinear text. In HLT-
NAACL, pages 452?459.
Fei Xia, Carrie Lewis, and William D Lewis. 2010.
The problems of language identification within
hugely multilingual data sets. In LREC.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust
projection across aligned corpora. In Proceedings
of NAACL-2001, pages 200?207.
85
Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 86?90,
Baltimore, Maryland, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
Short-term projects, long-term benefits:
Four student NLP projects for low-resource languages
Alexis Palmer and Michaela Regneri
Department of Computational Linguistics
Saarland University
Saarbr?ucken, Germany
{apalmer,regneri}@coli.uni-saarland.de
Abstract
This paper describes a local effort to
bridge the gap between computational and
documentary linguistics by teaching stu-
dents and young researchers in computa-
tional linguistics about doing research and
developing systems for low-resource lan-
guages. We describe four student software
projects developed within one semester.
The projects range from a front-end for
building small-vocabulary speech recogni-
tion systems, to a broad-coverage (more
than 1000 languages) language identifi-
cation system, to language-specific sys-
tems: a lemmatizer for the Mayan lan-
guage Uspanteko and named entity recog-
nition systems for both Slovak and Per-
sian. Teaching efforts such as these are an
excellent way to develop not only tools for
low-resource languages, but also computa-
tional linguists well-equipped to work on
endangered and low-resource languages.
1 Introduction
There is a strong argument to be made for bring-
ing together computational and documentary lin-
guistics in order to support the documentation and
description of endangered languages (Abney and
Bird, 2010; Bird, 2009). Documentation, de-
scription, and revitalization work for endangered
languages, as well as efforts to produce digi-
tal and machine-readable resources for languages
currently lacking such data, benefit from techno-
logical support in many different ways. Here we
focus on support via (a) tools facilitating more effi-
cient development of resources, with easy learning
curves, and (b) linguistic analysis tools.
Various meetings and workshops in recent years
have helped to bring the two fields closer to-
gether, but a sizeable gap remains. We?ve come
far enough to, for example, have a relevant work-
shop at a major computational linguistics confer-
ence, but not so far that issues around language en-
dangerment are well-known to even a large subset
of the computational linguistics community. One
way to get computational linguists thinking about
issues related to endangered languages is for them
to get their hands dirty ? to work directly on re-
lated projects. In this paper we describe our own
local effort to bridge this gap: a course for Mas-
ter?s and Bachelor?s students in computational lin-
guistics in which small teams of students each pro-
duced working, non-trivial natural language pro-
cessing (NLP) tools for low-resource languages
(LRLs) over the span of a single semester. The
individual projects are described in Section 3.
Such a course benefits the students in a num-
ber of ways. They get hands-on experience in
system building, they learn about a new subfield
within computational linguistics, with a different
set of concerns (some of these are discussed in
Section 2), and, in some cases, they get the op-
portunity to develop tools for their own native lan-
guages. From the perspective of computational
work on endangered languages, the positive out-
comes are not only a new set of NLP tools, but
also a group of students and young researchers
armed with experience working on low-resource
languages and better equipped to take on similar
projects in the future.
2 Teaching NLP for LRLs
Working on LRLs from a computational perspec-
tive requires training beyond the typical compu-
tational linguistics curriculum. It is not the case
that the most widely-used methods from computa-
tional linguistics can be straightforwardly adapted
for any arbitrarily-selected language. Thus an im-
portant part of our teaching agenda in this context
is to familiarize students with the challenges inher-
ent to NLP for LRLs as well as some of the main
86
approaches for addressing these same challenges.
This section briefly surveys some of the relevant
issues, with pointers to representative studies.
The first and most obvious concern is data spar-
sity. Many of the most successful and widely-
taught methods and models in computational lin-
guistics rely on either large amounts of labeled
data or massive amounts of unlabeled data. Meth-
ods and models explicitly addressing LRLs need
to maximize the utility of available data. Ap-
proaches for addressing data sparsity range from
data collection proposals (Abney and Bird, 2010)
to leveraging high-resource languages (Xia and
Lewis, 2007) to maximizing annotation effort
(Garrette and Baldridge, 2013). A second con-
cern is model suitability. Many existing models
in computational linguistics implicitly encode or
expect characteristics of high-resource languages
(Bender, 2011); for example, much work on com-
putational syntax uses models that exploit linear
ordering of elements in utterances. Such models
are not straightforwardly applicable for languages
with free or flexible word order, nor for highly
agglutinative languages where, for example, com-
plete utterances are encoded as single words. Ap-
proaches to this issues include adaptation of mod-
els using linguistic knowledge and/or universals
(Boonkwan and Steedman, 2011; Naseem et al.,
2010). The third issue to note is the difficulty
of evaluation. The output of systems or tools
performing automated analysis are predictions of
analyses for new data; these predictions must
be evaluated against a ground truth or human-
supplied analysis of the same data. Evaluation
is difficult in the low-resource setting, both be-
cause of limited availability of expert-labeled data
and because, in some cases, the ground truth
isn?t known, or analyses are shifting as knowledge
about the language develops.
We began the course with a discussion of these
issues, as well as an introduction to a range of ex-
isting tools, projects and resources. We did not
explicitly teach programming skills in the course,
but we also did not require extensive program-
ming background. Rather, we aimed to balance
the teams such that each contained a mix of back-
grounds: a bit more than half of the students
had previous experience with software develop-
ment, and the rest had at least taken one intro-
ductory programming course. The projects were
scoped such that there were clear ways for stu-
dents without programming experience to con-
tribute. For example, in some cases, students with
extensive background in linguistics performed lin-
guistic analysis of the data which informed the de-
sign of the system.
Evaluation of students was designed to empha-
size three objectives: production of a working sys-
tem, communication of challenges faced and so-
lutions to those challenges, and personal devel-
opment of professionally-relevant skills. Students
were graded on their weekly progress (more detail
in Section 3), one 15-20 minute talk per student,
individual written reports detailing specific contri-
butions to the project, and a conference-style end-
of-semester poster and demo session. Systems
were required to be working and demonstratable
both at the midway point of the semester (as a sim-
plified prototype) and at the end of the semester.
3 Four projects in four months
The course described here (?NLP tools for Low-
Resource Languages?) was offered as part of the
regular curriculum for undergraduate and gradu-
ate students in the Computational Linguistics de-
partment at Saarland University. We started with
10 students and formed four teams (based on pref-
erences for general topics and programming lan-
guages). The teams could choose their own project
or select from a set of proposed topics.
During the teaching period, we regularly moni-
tored the student?s progress by using some meth-
ods of agile software development.
1
For each
weekly meeting, each team had to set three goals
which constituted their homework. Goals could be
minor tasks (fixing a certain bug), bigger chunks
(choosing and implementing a strategy for data
standardization) or course requirements (prepar-
ing a talk). Not fulfilling a (project-related) goal
was acceptable, but students had to analyze why
they missed the goal and to learn from the experi-
ence. They were expected over the course of the
semester to become better both at setting reach-
able goals and at estimating how long they would
need to meet each goal. Under this obligation to
make continuous, weekly progress, each team had
a working system within three months. At the end
of month four, systems were suitable for demon-
stration at the poster session.
The projects differ according to their scopes and
goals, as well as their immediate practical utility.
1
http://en.wikipedia.org/wiki/Agile_software_development
87
One project (3.1) makes previous research accessi-
ble to users by developing an easy-to-use frontend;
a second project (3.2) aims to extend the num-
ber of languages addressed for an existing multi-
lingual classification task; and the remaining two
(3.3 and 3.4) implement language-specific solu-
tions for individual language processing tasks. We
additionally required that each project be open-
source; the public code repositories are linked in
the respective sections.
3.1 Small-vocabulary ASR for any language
This project
2
builds on existing research for small-
vocabulary (up to roughly 100 distinct words)
speech recognition. Such technology is desirable
for, among other things, developing speech inter-
faces to mobile applications (e.g. to deliver med-
ical information or weather reports; see Sherwani
(2009)), but dedicated speech recognition engines
are available only for a relatively small number
of languages. For small-vocabulary applications,
though, an existing recognizer for a high-resource
language can be used to do recognition in the tar-
get language, given a pronunciation lexicon map-
ping the relevant target language words into se-
quences of sounds in the high-resource language.
This project produces the required lexicon.
Building on the algorithms developed by Qiao
et al. (2010) and Chan and Rosenfeld (2012), two
students developed an easy-to-use interface that
allows a user with no knowledge of speech tech-
nologies to build and test a system to recognize
words spoken in the target language. In its cur-
rent implementation, the system uses the English-
language recognizer from the freely-available Mi-
crosoft Speech Platform;
3
for this reason, the sys-
tem is available for Windows only. To build a rec-
ognizer for a target language, a user needs only
to specify a written form and upload one or more
audio samples for each word in the vocabulary;
generally, the more audio samples per word, the
better the performance. The students additionally
implemented a built-in recorder; this means a user
can spontaneously make recordings for the desired
words. Finally, the system includes implementa-
tions of two different variants of the algorithm and
an evaluation module, thus facilitating use for both
research and development purposes.
The main challenges for this project involved
managing the interaction between the algorithm
2
https://github.com/lex4all/lex4all
3
http://msdn.microsoft.com/en-us/library/hh361572
and the Microsoft speech recognition platform, as
well as getting familiar with development in Win-
dows. The practical utility of this project is imme-
diately evident: any user with a Windows machine
can install the necessary components and have a
working small-vocabulary recognizer within sev-
eral hours. Of course, more time and data may
be required to improve performance of the rec-
ognizer, which currently reaches in the mid-70s
with five audio samples per word. These results,
as well as further details about the system (includ-
ing where to download the code, and discussion
of substituting other high-resource language rec-
ognizers), are described in Vakil et al. (2014).
3.2 Language ID for many languages
This project
4
addresses the task of language iden-
tification. Given a string of text in an arbitrary lan-
guage, can we train a system to recognize what
language the text is written in? Excellent classifi-
cation rates have been achieved in previous work,
but for a relatively small number of languages, and
the task becomes noticeably more difficult as the
number of languages increases (Baldwin and Lui,
2010; Lui and Baldwin, 2012, for example). With
few exceptions (Brown, 2013; Xia et al., 2010; Xia
et al., 2009), existing systems have only attempted
to distinguish between fewer than 200 of the thou-
sands of written languages currently in use. This
team of three students aimed to expand coverage
of language identification systems as much as pos-
sible given existing sources of data.
To do this, they first needed to gather and stan-
dardize data from various sources. They targeted
three sources of data: the Universal Declaration
of Human Rights, Wikipedia,
5
ODIN (Lewis and
Xia, 2010), and some portions of the data avail-
able from Omniglot.
5
The challenges faced by this
group lay primarily in two areas: issues involv-
ing data and those involving classification. In the
first area, they encountered expected and well-
known issues such as clean-up and standardization
of data, dealing with encoding issues, and manag-
ing large amounts of data. The second set of chal-
lenges have to do with the high degree of skew
in the data collected. Though their system covers
over 1000 languages, the amount of data per lan-
guage ranges from a single sentence to hundreds
of thousands of words. Along the way, the stu-
dents realized that this collection of data in a stan-
4
https://github.com/alvations/SeedLing
5
http://www.wikipedia.com,http://www.omniglot.com
88
dard, machine-readable form is useful for many
other purposes. The corpus and how to access it
are described in Emerson et al. (2014). A second
paper presenting the language identification re-
sults (including those for low-resource languages)
is planned for later this year.
3.3 A lemmatizer for Uspanteko
The third project
6
involved implementing a lem-
matizer for the Mayan language Uspanteko. Us-
ing data that had been cleaned, standardized (as
described in Palmer et al. (2010)), and made avail-
able through the Archive of Indigenous Languages
of Latin America,
7
these three students imple-
mented a tool to identify the citation form for in-
flected word forms in texts. The lemmatization
algorithm is based on longest common substring
matching: the closest match for an inflected form
is returned as the lemma. Additionally, a table for
irregular verb inflections was generated using the
annotated source corpus (roughly 50,000 words)
and an Uspanteko-Spanish dictionary (Can Pix-
abaj et al., 2007), to map inflected forms translated
with the same Spanish morpheme.
This group more than any other faced the chal-
lenge of evaluation. Not all lemmas covered in
the texts appear in the dictionary, and the Uspan-
teko texts, though fully analyzed with morphologi-
cal segmentation and glossing, part of speech tags,
and translation into Spanish, do not include cita-
tion forms. Manual evaluation of 100 sentences,
for which a linguist on the team with knowledge
of Spanish determined citation forms, showed ac-
curacy of 59% for the lemmatization algorithm.
3.4 NER for Slovak & Persian
Finally, the fourth project
8
(two students) chose
to tackle the task of named entity recognition
(NER): identifying instances of named entities
(NEs, e.g. people, locations, geopolitical entities)
in texts and associating them with appropriate la-
bels. The students developed a single platform to
do NER in both Slovak and Persian, their native
languages. The approach is primarily based on us-
ing gazetteers (for person names and locations), as
well as regular expressions (for temporal expres-
sions). The students collected the gazeteers for the
two languages as part of the project. Their sys-
tem builds on a modular design; one can swap out
6
https://code.google.com/p/mayan-lemmatizer/
7
http://www.ailla.utexas.org
8
https://code.google.com/p/named\-entity\-tagger/
gazetteers and a few language-specific heuristic
components to perform NER in a new language.
In this project, resource acquisition and evalua-
tion were the main challenges. The students used
some existing resources for both languages, but
also devoted quite some time to producing new
gazetteers. For Slovak, additional challenges were
presented by the language?s large number of in-
flectional cases and resulting variability in form.
For example, some inflected forms used to re-
fer to people from a given location are string-
identical to the names of the locations with a dif-
ferent case inflection. In Persian, the main chal-
lenges were detection of word boundaries (many
names are multi-word expressions) and frequent
NE/proper noun ambiguities. For evaluation, the
students hand-labeled over 35,000 words of Slo-
vak (with 545 NE instances) and about 600 para-
graphs of Persian data (306 NE instances). Perfor-
mace varies across named entity category: tempo-
ral expression matching is most reliable (f-score
0.96 for Slovak, 0.89 for Persion), followed by
locations (0.78 Slovak, 0.92 Persian) and person
names (0.63 Slovak, 0.87 Persian). Note that for
Persian, only NEs with correctly matched bound-
aries are counted (which are 50% for persons).
4 Conclusion
In this paper we have presented four student soft-
ware projects, each one addressing a different
NLP task relevant for one or more low-resource
languages. The successful outcomes of the four
projects show that much progress can be made
even with limited time and limited prior expe-
rience developing such systems. Local teach-
ing efforts such as these can be highly success-
ful in building a group of young researchers who
are both familiar with issues surrounding low-
resource and endangered languages and prepared
to do research and development in this area in the
future. We think of this as planting seeds for an
early harvest: with one semester?s combined effort
between instructors and students, we reap the re-
wards of both new tools and new researchers who
can continue to work on closing the gap between
computational and documentary linguistics.
Course materials are publicly available from the
course homepage,
9
and from the project reposito-
ries linked from the descriptions in Section 3.
9
http://www.coli.uni-saarland.de/courses/cl4lrl-swp/
89
Acknowledgements
First of all, we want to thank the students who par-
ticipated in our course and put so much effort and
passion in their projects. They are (in alphabeti-
cal order): Christine Bocionek, Guy Emerson, Su-
sanne Fertmann, Liesa Heuschkel, Omid Moradi-
annasab, Michal Petko, Maximilian Paulus, Alek-
sandra Piwowarek, Liling Tan and Anjana Vakil.
Further, we want to thank the anonymous review-
ers for their helpful comments. The second author
was funded by the Cluster of Excellence ?Multi-
modal Computing and Interaction? in the German
Excellence Initiative.
References
Steven Abney and Steven Bird. 2010. The Human
Language Project: Building a universal corpus of the
world?s languages. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 88?97. Association for Computa-
tional Linguistics.
Timothy Baldwin and Marco Lui. 2010. Language
identification: The long and the short of the matter.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 229?237, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Emily M Bender. 2011. On achieving and evaluating
language-independence in NLP. Linguistic Issues in
Language Technology, 6(3):1?26.
Steven Bird. 2009. Natural language processing
and linguistic fieldwork. Computational Linguis-
tics, 35(3):469?474.
Prachya Boonkwan and Mark Steedman. 2011. Gram-
mar induction from text using small syntactic proto-
types. In IJCNLP, pages 438?446.
Ralf D Brown. 2013. Selecting and weighting n-grams
to identify 1100 languages. In Text, Speech, and Di-
alogue, pages 475?483. Springer.
Telma Angelina Can Pixabaj, Oxlajuuj Keej Maya?
Ajtz?iib? (Group) Staff, and Centro Educativo y Cul-
tural Maya Staff. 2007. Jkemiix yalaj li uspanteko.
Cholsamaj Fundacion, Guatemala.
Hao Yee Chan and Roni Rosenfeld. 2012. Discrimi-
native pronunciation learning for speech recognition
for resource scarce languages. In Proceedings of the
2nd ACM Symposium on Computing for Develop-
ment, page 12. ACM.
Guy Emerson, Liling Tan, Susanne Fertmann, Alexis
Palmer, and Michaela Regneri. 2014. SeedLing:
Building and using a seed corpus for the Human
Language Project. In Proceedings of ACL Workshop
on the use of computational methods in the study of
endangered languages (ComputEL).
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of NAACL-HLT, pages 138?147.
William D Lewis and Fei Xia. 2010. Developing
ODIN: A multilingual repository of annotated lan-
guage data for hundreds of the world?s languages.
Literary and Linguistic Computing, 25(3):303?319.
Marco Lui and Timothy Baldwin. 2012. Langid.py:
An off-the-shelf language identification tool. In
Proceedings of the ACL 2012 System Demonstra-
tions, ACL ?12, pages 25?30, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1234?1244. Asso-
ciation for Computational Linguistics.
Alexis Palmer, Taesun Moon, Jason Baldridge, Katrin
Erk, Eric Campbell, and Telma Can. 2010. Compu-
tational strategies for reducing annotation effort in
language documentation. Linguistic Issues in Lan-
guage Technology, 3.
Fang Qiao, Jahanzeb Sherwani, and Roni Rosenfeld.
2010. Small-vocabulary speech recognition for
resource-scarce languages. In Proceedings of the
First ACM Symposium on Computing for Develop-
ment, page 3. ACM.
Jahanzeb Sherwani. 2009. Speech interfaces for in-
formation access by low literate users. Ph.D. thesis,
SRI International.
Anjana Vakil, Max Paulus, Alexis Palmer, and
Michaela Regneri. 2014. lex4all: A language-
independent tool for building and evaluating pronun-
ciation lexicons for small-vocabulary speech recog-
nition. In Proceedings of ACL2014 Demo Session.
Fei Xia and William Lewis. 2007. Multilingual struc-
tural projection across interlinear text. In Proceed-
ings of HLT/NAACL 2007, Rochester, NY.
Fei Xia, William D Lewis, and Hoifung Poon. 2009.
Language ID in the context of harvesting language
data off the web. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 870?878. Associ-
ation for Computational Linguistics.
Fei Xia, Carrie Lewis, and William D Lewis. 2010.
The problems of language identification within
hugely multilingual data sets. In LREC.
90
LAW VIII - The 8th Linguistic Annotation Workshop, pages 149?158,
Dublin, Ireland, August 23-24 2014.
Situation entity annotation
Annemarie Friedrich Alexis Palmer
Department of Computational Linguistics
Saarland University, Saarbr?ucken, Germany
{afried,apalmer}@coli.uni-saarland.de
Abstract
This paper presents an annotation scheme for a new semantic annotation task with relevance for
analysis and computation at both the clause level and the discourse level. More specifically, we
label the finite clauses of texts with the type of situation entity (e.g., eventualities, statements
about kinds, or statements of belief) they introduce to the discourse, following and extending
work by Smith (2003). We take a feature-driven approach to annotation, with the result that
each clause is also annotated with fundamental aspectual class, whether the main NP referent is
specific or generic, and whether the situation evoked is episodic or habitual. This annotation is
performed (so far) on three sections of the MASC corpus, with each clause labeled by at least
two annotators. In this paper we present the annotation scheme, statistics of the corpus in its
current version, and analyses of both inter-annotator agreement and intra-annotator consistency.
1 Introduction
Linguistic expressions form patterns in discourse. Passages of text can be analyzed in terms of the
individuals, concepts, times and situations that they introduce to the discourse. In this paper we intro-
duce a new semantic annotation task which focuses on the latter and in particular their aspectual nature.
Situations are expressed at the clause level; situation entity (SE) annotation is the task of associating
individual clauses of text with the type of SE introduced to the discourse by the clause. Following Smith
(2003), we distinguish the following SE types (see Sec. 3.1): EVENTS, STATES, GENERALIZING SEN-
TENCES, GENERIC SENTENCES, FACTS, PROPOSITIONS, QUESTIONS and IMPERATIVES. Although
these categories are clearly distinct from one another on theoretical grounds, in practice it can be difficult
to cleanly draw boundaries between them. We improve annotation consistency by defining the SE types
in terms of features whose values are easier for annotators to identify, and which provide guidance for
distinguishing the more complex SE types.
As with most complex annotation tasks, multiple interpretations are often possible, and we cannot
expect agreement on all instances. The feature-driven approach (see Sec. 3.2) is a valuable source of
information for investigating annotator disagreements, as the features indicate precisely how annotators
differ in their interpretation of the situation. Analysis of intra-annotator consistency shows that personal
preferences of annotators play a role, and we conclude that disagreements often highlight cases where
multiple interpretations are possible. We further argue that such cases should be handled carefully in
supervised learning approaches targeting methods to automatically classify situation entity types.
As the first phase of the SE annotation project, we are in the process of annotating the written portion
of MASC (Ide et al., 2010), the manually-annotated subcorpus of the Open American National Corpus.
MASC provides texts from 20 different genres and has already been annotated with various linguistic
and semantic phenomena.
1
MASC offers several benefits: it includes text from a wide variety of genres,
it facilitates study of interactions between various levels of analysis, and the data is freely available
with straightforward mechanisms for distribution. In this paper we report results for three of the MASC
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://www.americannationalcorpus.org/MASC/Full_MASC.html
149
genres: news, letters, and jokes. Once a larger portion of MASC has been labeled with SEs and their
associated features, we will add our annotations to those currently available for MASC. We mark the SE
types of clauses with the aim of providing a large corpus of annotated text for the following purposes:
(1) To assess the applicability of SE type classification as described by Smith (2003): to what extent
can situations be classified easily, which borderline cases occur, and how do humans perform on this
task? (see Sec. 4)
(2) Training, development and evaluation of automatic systems classifying situation entities, as well
as sub-tasks which have (partially) been studied by the NLP community, but for which no large
annotated corpora are available (for example, automatically predicting the fundamental aspectual
class of verbs in context (Friedrich and Palmer, 2014) or the genericity of clauses and noun phrases).
(3) To provide a foundation for analysis of the theory of Discourse Modes (Smith, 2003), which we
explain next (Sec. 2).
2 Background and related work
Within a text, one recognizes stretches that are intuitively of different types and can be clustered by their
characteristic linguistic features and interpretations. Smith (2003) posits five discourse modes: Narrative,
Report, Description, Informative and Argument/Commentary. Texts of almost all genre categories have
passages of different modes. The discourse modes are characterized by (a) the type of situations (also
called situation entities) introduced in a text passage, and (b) the principle of text progression in the
mode (temporal or atemporal, and different manners of both temporal and atemporal progression). This
annotation project directly addresses the first of these characteristics, the situation entity types (SE types).
Some previous work has addressed the task of classifying SE types at the clause level. Palmer et al.
(2004) enrich LFG parses with lexical information from both a database of lexical conceptual structures
(Dorr, 2001) and hand-collected groups of predicates associated with particular SE types. The enriched
parses are then fed to an ordered set of transfer rules which encode linguistic features indicative of SE
types. The system is evaluated on roughly 200 manually-labeled clauses. Palmer et al. (2007) investigate
various types of linguistic features in a maximum entropy model for SE type classification. The best
results are still below 50% accuracy (with a most-frequent-class baseline of 38%), and incorporating
features from neighboring clauses is shown to increase performance. Palmer et al. (2007) annotate data
from one section of the Brown corpus and a small amount of newswire text, with two annotators and
no clear set annotation guidelines. In addition, work by Cocco (2012) classifies clauses of French text
according to a six-way scheme that falls somewhere between the SE level and the level of discourse
modes. The types are: narrative, argumentative, descriptive, explicative, dialogal, and injunctive.
Other related works address tasks related to the features we annotate. One strand of work is in auto-
matic classification of aspectual class (Siegel and McKeown, 2000; Siegel, 1999; Siegel, 1998; Klavans
and Chodorow, 1992; Friedrich and Palmer, 2014) and its determination as part of temporal classification
(UzZaman et al., 2013; Bethard, 2013; Costa and Branco, 2012). A second aims to distinguish generic
vs. specific clauses (Louis and Nenkova, 2011) or to identify generic noun phrases (Reiter and Frank,
2010). The latter work leverages data with noun phrases annotated as either generic and specific from
the ACE-2 corpus (Mitchell et al., 2003); their definitions of these two types match ours (see Sec. 3.2.1).
3 Annotation Scheme and Process
In this section, we first present the inventory of SE types (Sec. 3.1). We then describe our feature-
driven approach to annotation (Sec. 3.2) and define the SE types with respect to three situation-related
features: main referent type, fundamental aspectual class, and habituality. Some situation entity types
are easier to recognize than others. While some can be identified on the basis of surface structure and
clear linguistic indicators, others depend on internal temporal (and other) properties of the verb and its
arguments. Annotators take the following approach: first, easily-identifiable SE types (Speech Acts and
Abstract Entities) are marked. If the clause?s SE type is not one of these, values for the three features are
determined, and the final determination of SE type is based on the features.
150
3.1 Situation entity types
Following Smith (2003), we distinguish the following SE types:
Eventualities. These types describe particular situations such as STATES (1a) or EVENTS (2). The type
REPORT, a subtype of EVENT, is used for situations introduced by verbs of speech (1b).
(1) (a) ?Carl is a tenacious fellow?, (STATE)
(b) said a source close to USAir. (EVENT ? REPORT)
(2) The lobster won the quadrille. (EVENT)
General Statives. This class includes GENERALIZING SENTENCES (3), which report regularities re-
lated to specific main referents, and GENERIC SENTENCES (4), which make statements about kinds.
(3) Mary often feeds my cats. (GENERALIZING)
(4) The lion has a bushy tail. (GENERIC)
Abstract Entities are the third class of SE types, and comprise FACTS (5) and PROPOSITIONS (6).
These situations differ from the other types in how they relate to the world: Eventualities and General
Statives are located spatially and temporally in the world, but Abstract Entities are not. FACTS are objects
of knowledge and PROPOSITIONS are objects of belief from the respective speaker?s point of view.
(5) I know that Mary refused the offer. (FACT)
(6) I believe that Mary refused the offer . (PROPOSITION)
We limit the annotation of Abstract Entities to the clausal complements of certain licensing predicates,
as well as clauses modified by a certain class of adverbs, as it is not always possible to identify sentences
directly expressing Facts or Propositions on linguistic grounds (Smith, 2003). In (6), believe is the
licensing predicate, and Mary refused the offer is a situation that is introduced as not being in the world,
but about the world (Smith, 2003). Annotators are asked to additionally label the embedded SE type
when possible. For example, that Mary refused the offer in (5) and (6) would be labeled as EVENT.
Speech Acts. This class comprises QUESTIONS and IMPERATIVE clauses (Searle, 1969).
Derived SE types. In some cases, the SE type of a clause changes based on the addition of some
linguistic indication of uncertainty about the status of the situation described. We refer to these as derived
SE types. More specifically, clauses that would otherwise be marked as EVENT may be coerced to the
type STATE due to negation, modality, future tense, conditionality, and sometimes subjectivity: e.g. John
did not win the lottery, a negated event, introduces a STATE to the discourse.
3.2 Features for distinguishing situation entity types
In this section, we describe three features that allow for the clear expression of differences between SE
types. Fleshing out the descriptions of SE types with these underlying features is useful to convey the
annotation scheme to new annotators, to get partial information when an annotator has trouble making a
decision on SE type, and to analyze disagreements between annotators.
3.2.1 Main referent type: specific or generic
This feature indicates the type of the most central entity mentioned in the clause as a noun phrase. We
refer to this entity as the clause?s main referent. This referent can be found by asking the question: What
is this clause about? Usually, but not always, the main referent of a clause is realized as its grammatical
subject. We appeal to the annotator?s intuitions in order to determine the main referent of a clause. In
case the main referent does not coincide with the grammatical subject as in example (7), this is to be
indicated during annotation.
(7) There are two books on the table. (specific main referent, STATE)
151
Some SE types (STATES, GENERALIZING SENTENCES and GENERIC SENTENCES, for details see
Table 1) are distinguished by whether they make a statement about some specific main referent or about
a generic main referent. Specific main referents are particular entities (8), particular groups of entities (9),
organizations (10), particular situations (11) or particular instantiations of a concept (12).
(8) Mary likes popcorn. (particular entity ? specific, STATE)
(9) The students met at the cafeteria. (a particular group ? specific, STATE)
(10) IBM was a very popular company in the 80s. (organization ? specific, STATE)
(11) That she didn?t answer her phone really upset me. (particular situation ? specific, EVENT)
(12) Today?s weather was really nice. (particular instantiation of a concept ? specific, STATE)
The majority of generic main referents are noun phrases referring to a kind rather than to a particular
entity, and generic mentions of concepts or notions (14). Definite NPs and bare plural NPs (13) are the
main kind-referring NP types (Smith, 2003).
(13) The lion has a bushy tail. / Dinosaurs are extinct. (generic, GENERIC SENTENCE)
(14) Security is an important issue in US electoral campaigns. (generic, GENERIC SENTENCE)
While some NPs clearly make reference to a well-established kind, other cases are not so clear cut,
as humans tend to make up a context in which an NP describes some kind (Krifka et al., 1995). Sen-
tence (15) gives an example for such a case: while lions in captivity are not a generally well-established
kind, this term describes a class of entities rather than a specific group of lions in this context.
(15) Lions in captivity have trouble producing offspring. (generic, GENERIC SENTENCE)
Gerunds may occur as the subject in English sentences. When they describe a specific process as in
(16a), we mark them as specific. If they instead describe a kind of process as in (16b), we mark them as
generic.
(16) (a) Knitting this scarf took me 3 days. (specific, EVENT)
(b) Knitting a scarf is generally fun. (generic, GENERIC SENTENCE)
We also give annotators the option to explicitly mark the main referent as expletive, as in (17).
(17) It seemed like (expletive = no main referent, STATE)
he would win. (specific, STATE)
3.2.2 Fundamental aspectual class: stative or dynamic
Following Siegel and McKeown (2000), we determine the fundamental aspectual class of a clause. This
notion is the extension of lexical aspect or aktionsart, which describe the ?real life shape? of situations
denoted by verbs, to the level of clauses. More specifically, aspectual class is a feature of the main verb
and a select group of modifiers, which may differ per verb. The stative/dynamic distinction is the most
fundamental distinction in taxonomies of aspectual class (Vendler, 1967; Bach, 1986; Mourelatos, 1978).
We allow three labels for this feature: dynamic for cases where the verb and its arguments describe
some event (something happens), stative for cases where they introduce some properties of the main
referent to the discourse, or both for cases where annotators see both interpretations.
It is important to note that the fundamental aspectual class of a verb can be different from the type
of situation entity introduced by the clause as a whole. The basic situation type of building a house is
dynamic, and in the examples below we see this fundamental aspectual class appearing in clauses with
different situation entity types. Example (18) describes an EVENT. Clause (19), on the other hand, is a
GENERALIZING SENTENCE, as it describes a pattern of events; this is a situation with a derived type.
The same is true for example (20), which is a STATE because of its future tense.
(18) John built a house. (EVENT, dynamic fundamental aspectual class)
(19) John builds houses. (GENERALIZING SENTENCE, dynamic fundamental aspectual class)
(20) John is going to build a house. (STATE, dynamic fundamental aspectual class)
152
3.2.3 Habituality
Another dimension along which situations can be distinguished is whether they describe a static state, a
one-time (episodic) event (21) or some regularity of an event (22) or a state (23), which is labeled ha-
bitual. The term habitual as used in this annotation project covers more than what is usually considered
a matter of habit, extending to any clauses describing regularities (24). The discussion related to this
linguistic feature in this section follows Carlson (2005). If one can add a frequency adverbial such as
typically/usually to the clause and the meaning of the resulting sentence differs at most slightly from the
meaning of the original sentence, or the sentence contains a frequency adverbial such as never, the sen-
tence expresses a regularity, i.e., is habitual. Another property of habituals is that they are generalizations
and hence have the property of tolerating exceptions. If we learn that Mary eats oatmeal for breakfast, it
does not necessarily need to be true that she eats oatmeal at every breakfast. It is important to note that
unlike fundamental aspectual class, habituality is an attribute of the entire situation.
(21) Mary ate oatmeal for breakfast this morning. (episodic, EVENT)
(22) Mary eats oatmeal for breakfast. (habitual, GENERALIZING SENTENCE)
(23) I often feel as if I only get half the story. (habitual, stative fundamental aspectual class, GENER-
ALIZING SENTENCE)
(24) Glass breaks easily. (habitual, GENERIC SENTENCE)
3.3 SE types and their features
The feature-driven approach to annotation taken here is defined such that, ideally, each unique combina-
tion of values for the three features leads to one SE type. Table 1 shows the assignment of SE types to
various combinations of feature values. This table covers all SE types except ABSTRACT ENTITIES and
SPEECH ACTS, which are more easily identifiable based on lexical and/or syntactic grounds. Annotators
are also provided with information about linguistic tests for some SE types and feature values, both for
making feature value determinations and to support selection of clause-level SE type labels.
SE type main referent aspectual class habituality
EVENT
specific
eventive episodic
generic
STATE specific stative static
GENERIC SENTENCE generic
eventive habitual
stative static, habitual
GENERALIZING
specific
eventive
habitual
SENTENCE stative
General Stative
specific
eventive habitual
generic
Table 1: Situation entity types and their features.
4 Annotator agreement and consistency
This section presents analyses of inter-annotator agreement and intra-annotator consistency, looking at
agreement for individual feature values as well as clause-level SE type.
4.1 Data and annotators
The current version of our corpus consists of three sections (news, letters and jokes) of MASC corpus
(Ide et al., 2010). We hired three annotators, all either native or highly-skilled speakers of English, and
had a training phase of 3 weeks using several Wikipedia documents. Afterwards, annotation of the texts
began and annotators had no further communication with each other. Two annotators (A and B) each
marked the complete data set, and one additional annotator (C) marked the news section only.
153
ANNOTATORS NUMBER OF MAIN ASPECTUAL HABITUALITY SE TYPE SE TYPE
SEGMENTS REFERENT CLASS (REP=EVT)
A:B 2563 0.35 0.81 0.77 0.56 0.66
A:C 2524 0.29 0.77 0.76 0.55 0.65
B:C 2556 0.45 0.73 0.76 0.76 0.74
average 2545 0.36 0.77 0.76 0.62 0.68
Table 2: Cohen?s ?, for pairs of annotators on the MASC news section.
GENRE NUMBER OF MAIN ASPECTUAL HABITUALITY SE TYPE SE TYPE
SEGMENTS REFERENT CLASS (REP=EVT)
jokes 3455 0.57 0.85 0.81 0.74 0.73
news 2563 0.35 0.81 0.77 0.56 0.66
letters 1851 0.41 0.71 0.65 0.56 0.56
all 7869 0.47 0.80 0.77 0.64 0.68
Table 3: Cohen?s ?, for two annotators on three different sections of MASC.
4.2 Segmentation into clauses
We segment the texts into finite clauses using the SPADE discourse parser (Soricut and Marcu, 2003),
applying some heuristic post-processing and allowing annotators to mark segments that do not contain
a situation (for instance, headlines or by-lines) or that should be merged with another segment in order
to describe a complete situation. We filter out all segments marked by any annotator as having a seg-
mentation problem. Of the 2823 segments automatically created for the news section, 4% were marked
as containing no situation by at least one of the three annotators, and 7% were merged to a different
segment by at least one annotator. All three annotators agree on the remaining 2515 segments (89%). Of
the 9428 automatically-created segments in the full data set, 11.5% were marked as no-situation by at
least one of two annotators, and a further 5% were merged to other segments by at least one annotator.
7869 segments remain for studying agreement between two annotators on the full data set.
The three genres vary as to the average segment length. Segments in the letters texts have the longest
average length (11.1 tokens), segments in jokes are the shortest (6.9 tokens on average), and segments in
news fall in the middle with an average length of 9.9 tokens.
4.3 Inter-annotator agreement
As we allow annotators to mark a segment as Speech Acts or Abstract Entities and in addition mark the
SE type of the embedded situation with a non-surface type, we compute agreement for Eventualities and
General Statives in the following, and present the results for Speech Acts and Abstract Entities separately.
news section, 3 annotators. We compute Cohen?s unweighted ? between all three pairs of annotators
for the news section, as shown in Table 2. We compute agreement for the segments where both respective
annotators agree on the segmention, i.e., that the segment describes a situation. For aspectual class, we
compute agreement over the three labels stative, dynamic and both; for main referents, we compute
agreement over the three labels specific, dynamic and expletive; for habituality, we compute agreement
over the three labels episodic, habitual and static. In each case, we omit segments for which one of the
annotators did not give a label, which in each case are fewer than 26 segments.
We observe good agreement for the features aspectual class and habituality, and for SE type between
annotators B and C. Pairs involving annotator A reach lower agreement; we identify two causes. Anno-
tator A marks many segments marked as REPORT by the others as the corresponding supertype EVENT.
This shows up in Table 2 as higher values of ? when considering REPORT to match its supertype EVENT.
The second cause is A?s different preference for marking main referents, causing lower ? scores for agree-
ment on the main referent type and also influencing agreement for situation entity types. In more than
92% of the 183 clauses on which annotators B and C agree with each other, but disagree with A, B
and C assigned the value specific while A marked the main referent as generic. Early in the annotation
project, a revision was made to the scheme for labeling main referents ? one hypothesis is that A might
not have updated her way of labeling these. We estimate that roughly 40% of these cases were due to
154
A?s misunderstanding of feature value definitions, but around 30% of these cases do allow for both inter-
pretations. In the following sentence, the main referent of the second segment could either refer to the
specific set of all kids in New York, or to the class of children in New York: As governor, I?ll make sure
// that every kid in New York has the same opportunity. Another frequent case is the main referent you,
which can be interpreted in a generic way or as specifically addressing the reader (e.g. of a letter). Such
disagreements at the level of feature annotations allow us to detect cases where several interpretations
are possible. Having annotators with different preferences on difficult cases can actually be a valuable
source of information for identifying such cases.
The distribution of labels for main referents is highly skewed towards specific main referents for the
news section; when comparing B and C, they agree on 2358 segments to have a specific main referent.
However, only 122 segments are labeled as having a generic main referent by at least one annotator, and
they agree only on 43 of them. A further 49 are labeled generic by B but specific by C and a further 30
vice versa. In order to collect more reliable data and agreement numbers for the task of labeling main
referent types, we plan to conduct a focused study with a carefully-balanced data set.
news, jokes, letters: 2 annotators. We report agreement for three sections, corresponding to three
genres, for two annotators (A and B) in Table 3. We observe higher agreement for jokes than for news,
and higher agreement for news than for letters. Figure 1 shows the distribution of situation entity types
per genre. The numbers express averages of percentages of label types assigned to the clauses of one
genre by the two annotators. The letters genre is different in that it has more STATES, far fewer EVENTS,
which are usually easy to detect, and more General Statives. Most cases of confusion between annotators
occur between General Statives and STATES, so the more EVENTS texts have, the higher the agreement.
letters news jokes
0%
20%
40%
60% STATE
EVENT
GENERALIZING SENTENCE
GENERIC SENTENCE
Figure 1: Distribution of situation entity types in three different genres.
Speech Acts and Abstract Entities. Figure 2 shows the percentage of segments of each genre that
were marked as a Speech Act or an Abstract Entity by at least one annotator. QUESTIONS are most
frequent in the jokes genre, but about half of them are just marked by one annotator, which has to do with
how consistently indirect questions are marked. The two annotators agree on almost all segments labeled
as imperatives; while there are only very few IMPERATIVES in the news section, there are more in the
jokes and letters sections. The letters are mainly fund-raising letters, which explains the high percentage
of IMPERATIVES (Please help Goodwill. // Use the enclosed card // and give a generous gift today.).
FACTS and PROPOSITIONS, on the other hand, are rather infrequent in any genre, and annotators tend to
mark them inconsistently. We take from this analysis that we need to offer some help to the annotators in
detecting Abstract Entities. We plan to compile a list of verbs that may introduce Abstract Entities and
specifically highlight potential licensing constructions in order to increase recall for these types.
4.4 Intra-annotator consistency
After the first round of annotation, we identified 11 documents with low inter-annotator agreement on
SE type (5 news, 5 letters, 1 jokes) and presented them to two annotators for re-annotation. For each
annotator, the elapsed time between the first and second rounds was at least 3 weeks. We observe that in
general, the agreement of each annotator with herself is greater than agreement with the other annotator.
This shows that the disagreements are not pure random noise, but that annotators have different prefer-
ences for certain difficult decisions. It is interesting to note that annotator B apparently changed how
155
letrs nrws jroorks
0%02
4%02
6%02
S%02
TA%02 EVNENGRLRNI
Z ktrC??? ?enr
Z ktrC??? ? ?eo?
letrs nrws jroorks0%02
4%02
6%02
S%02
TA%02 ???L
letrs nrws jroorks
0%02
4%02
6%02
S%02
TA%02 R?E?V?LR??
letrs nrws jroorks
0%02
4%02
6%02
S%02
TA%02 ???GLRNI
Figure 2: Percentage of segments marked as Speech Act or Abstract Entity by at least one annotator.
GENRE NUMBER OF MAIN ASPECTUAL HABITUALITY SE TYPE SE TYPE
SEGMENTS REFERENT CLASS (REP=EVT)
A1:B1 636 0.15 0.79 0.64 0.40 0.45
A2:B2 599 0.12 0.78 0.70 0.42 0.48
A1:A2 596 0.79 0.88 0.78 0.75 0.75
B1:B2 620 0.55 0.84 0.78 0.75 0.75
Table 4: Consistency study: Cohen?s ?, for two annotators, comparing against each other and against
themselves (re-annotated data). A1 = annotator A in first pass, B2 = annotator B in second pass etc.
she annotates main referents; possibly this is also due to the above mentioned revision to the annotation
scheme. On the other hand, B annotated very few segments as generic (only 61 segments were marked
as having a generic main referent in either the first or second pass, 27 of them in both passes), which
may also have led to the low ? value. The fact that annotators do disagree with themselves indicates that
there are noisy cases in our data set, where multiple interpretations are possible. However, we want to
point out that the level of noise estimated by this intra-annotator consistency study is an upper bound as
we chose the most difficult documents for re-annotation; the overall level of noise in the data set can be
assumed to be much lower.
5 Conclusion
We have presented an annotation scheme for labeling clauses with their situation entity type along with
features indicating the type of main referent, fundamental aspectual class and habituality. The feature-
driven approach allows for a detailed analysis of annotator disagreements, showing in which way the
annotators? understandings of a clause differ. The analysis in the previous chapter showed that while
good inter-annotator agreement can be reached for most decisions required by our annotation schema,
there remain hard cases, on which annotators disagree with each other or with their own first round
of annotations. We do not yet observe satisfying agreement for main referent types or for identifying
abstract entities. In both cases, data sparseness is a problem; there are only very few generic main
referents and abstract entities in our current corpus. We plan to conduct case studies on data that is
specifically selected for these phenomena.
However, in many of the hard cases, several readings are possible. Rather than using an adjudicated
data set for training and evaluation of supervised classifiers for labeling clauses with situation entities,
we plan to leverage such disagreements for training, following proposals by Beigman Klebanov and
Beigman (2009) and Plank et al. (2014).
The annotation reported here is ongoing; our next goal is to extend annotation to additional genres
within MASC, starting with essays, journal, fiction, and travel guides. Following SE annotation, we will
extend the project to annotation of discourse modes. Finally, we are very interested in exploring and
annotating SEs in other languages, as we expect a similar inventory but different linguistic realizations.
Acknowledgments We thank the anonymous reviewers, Bonnie Webber and Andreas Peldszus for
helpful comments, and our annotators Ambika Kirkland, Ruth K?uhn and Fernando Ardente. This re-
search was supported in part by the MMCI Cluster of Excellence, and the first author is supported by an
IBM PhD Fellowship.
156
References
Emmon Bach. 1986. The algebra of events. Linguistics and philosophy, 9(1):5?16.
Beata Beigman Klebanov and Eyal Beigman. 2009. From annotator agreement to noise models. Computational
Linguistics, 35(4):495?503.
Steven Bethard. 2013. ClearTK-TimeML: A minimalist approach to TempEval 2013. In Second Joint Conference
on Lexical and Computational Semantics (* SEM), volume 2, pages 10?14.
Greg Carlson. 2005. Generics, habituals and iteratives. The Encyclopedia of Language and Linguistics.
Christelle Cocco. 2012. Discourse type clustering using pos n-gram profiles and high-dimensional embeddings.
In Proceedings of the Student Research Workshop at the 13th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, EACL 2012.
Francisco Costa and Ant?onio Branco. 2012. Aspectual type and temporal relation classification. In Proceedings of
the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages
266?275.
Bonnie J. Dorr. 2001. LCS verb database. Online software database of Lexical Conceptual Structures, University
of Maryland, College Park, MD.
Annemarie Friedrich and Alexis Palmer. 2014. Automatic prediction of aspectual class of verbs in context. In
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL). Baltimore,
USA.
Nancy Ide, Christiane Fellbaum, Collin Baker, and Rebecca Passonneau. 2010. The manually annotated sub-
corpus: A community resource for and by the people. In Proceedings of the ACL 2010 conference short papers,
pages 68?73.
Judith L. Klavans and Martin S. Chodorow. 1992. Degrees of stativity: The lexical representation of verb aspect.
In Proceedings of the 14th COLING, Nantes, France.
Manfred Krifka, Francis Jeffry Pelletier, Gregory Carlson, Alice ter Meulen, Gennaro Chierchia, and Godehard
Link. 1995. Genericity: an introduction. The Generic Book, pages 1?124.
Annie Louis and Ani Nenkova. 2011. Automatic identification of general and specific sentences by leveraging
discourse annotations. In Proceedings of IJCNLP 2011.
Alexis Mitchell, Stephanie Strassel, Mark Przybocki, JK Davis, George Doddington, Ralph Grishman, Adam Mey-
ers, Ada Brunstein, Lisa Ferro, and Beth Sundheim. 2003. ACE-2 Version 1.0. Linguistic Data Consortium,
Philadelphia.
Alexander PD Mourelatos. 1978. Events, processes, and states. Linguistics and philosophy, 2(3):415?434.
Alexis Palmer, Jonas Kuhn, and Carlota Smith. 2004. Utilization of multiple language resources for robust
grammar-based tense and aspect classification. In Proceedings of LREC 2004.
Alexis Palmer, Elias Ponvert, Jason Baldridge, and Carlota Smith. 2007. A sequencing model for situation entity
classification. Proceedings of ACL 2007.
Barbara Plank, Dirk Hovy, and Anders S?gaard. 2014. Learning part-of-speech taggers with inter-annotator
agreement loss. In Proceedings of EACL 2014.
Nils Reiter and Anette Frank. 2010. Identifying generic noun phrases. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics (ACL).
John Searle. 1969. Speech Acts. Cambridge University Press.
Eric V Siegel and Kathleen R McKeown. 2000. Learning methods to combine linguistic indicators: Improving
aspectual classification and revealing linguistic insights. Computational Linguistics, 26(4):595?628.
Eric V. Siegel. 1998. Disambiguating verbs with the WordNet category of the direct object. In Proceedings of
Workshop on Usage of WordNet in Natural Language Processing Systems, Universite de Montreal.
Eric V. Siegel. 1999. Corpus-based linguistic indicators for aspectual classification. In Proceedings of ACL37,
University of Maryland, College Park.
157
Carlota S Smith. 2003. Modes of discourse: The local structure of texts. Cambridge University Press.
Radu Soricut and Daniel Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information.
In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational
Linguistics on Human Language Technology-Volume 1, pages 149?156. Association for Computational Linguis-
tics.
Naushad UzZaman, Hector Llorens, Leon Derczynski, Marc Verhagen, James Allen, and James Pustejovsky. 2013.
Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations. In Second joint
conference on lexical and computational semantics (* SEM), volume 2, pages 1?9.
Zeno Vendler, 1967. Linguistics in Philosophy, chapter Verbs and Times, pages 97?121. Cornell University Press,
Ithaca, New York.
158
