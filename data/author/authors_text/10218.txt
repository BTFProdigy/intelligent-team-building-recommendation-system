A Knowledge Based Approach to Identification of Serial Verb Construction in 
Chinese-to-Korean Machine Translation System 
 
Dong-il Kim?, Zheng-Cui, Jinji-Li??, Jong-Hyeok Lee 
Department Computer Science and Engineering, Electrical and Computer Engineering Division, 
Pohang University of Science and Technology (POSTECH)   
and Advanced Information Technology Research Center (AlTrc) 
San 31 Hyoja Dong, Pohang, 790-784, Korea 
E-mail: {dongil, cuizheng, ljj,jhlee}@postech.ac.kr 
 
                                                     ? Also an assistant professor at Yanbian University of Science  
& Technology (YUST) Yanji, Jilin, China. 
?? Also a lecturer at YUST 
Abstraction 
In Chinese language processing, the 
recognition and analysis for serial verb 
constructions (SVCs) is a fascinating research 
topic. Chinese language researchers each may 
have a different definition and interpretation of 
SVC since the structure of SVC makes Chinese 
unique to other languages and contains complex 
semantic and pragmatic information. This paper 
proposes a formal definition of SVC and a 
knowledge based approach for the recognition of 
SVCs, which is adopted in TOTAL-CK, a 
transfer-based MT system from Chinese to 
Korean. The recognition process is carried out in 
two stages: the analysis stage classifies SVCs 
into general categories, and the transfer stage 
performs further classification for Korean 
transfer. Some evaluation result for each stage 
was also given with statistics of each category of 
SVCs 
Introduction 
Many Chinese language researchers have paid 
special attention to the so-called ?serial verb 
constructions (SVCs)?, where two or more 
semantically or pragmatically related verb 
phrases or clauses are juxtaposed together 
without any functional marker. Because of 
different definitions and interpretations of SVCs 
among researchers, their categorizations differ 
according to researchers? viewpoints. 
In a Chinese to Korean machine translation, 
the hidden relation of the serial verbs should be 
expressed with some function words from the 
target language viewpoint. Moreover, the 
conceptual scope of these function words is 
different from the scope of SVC categorizations 
that are classified based on the viewpoint of the 
Chinese language itself. 
 In this paper, we proposes a different 
categorization of SVCs defined by the 
contrastive analysis of the two languages, and 
also an SVC identification method that is 
adopted in a Chinese-to-Korean MT system, 
TOTAL-CK. The TOTAL (Translator Of Three 
Asian Languages: Chinese, Korean and 
Japanese) project has been conducted under a 
hybrid strategy with transfer-based and 
example-based engines.  
1. Language Characteristics between 
Chinese and Korean 
In this section, some contrastive analyses of 
the two languages are introduced for better 
understanding of an SVC sentence. Since 
Chinese is an isolating language, morphological 
or syntactic markers rarely appear in a sentence, 
while in an agglutinative language such as 
Korean, these functional markers are not an 
optional unit but an obligatory unit in a sentence.  
An example is given in (1). Notice that the 
Korean alphabets are written with Yale 
Romanization in this paper. 
 
(1) ? ??  ???(ta kai-men jin-qu) 
Ku-nun  mwun-ul  yel-ko   duleka-nta. 
He-NOM  door-ACC open-CON get in-PRENT-DEC. 
He opens the door and enters (the room). 
In the Korean sentence, ko is a connective 
particle, and also nun, ul, and nta denote a topic 
auxiliary particle, an object case particle and 
declarative terminative ending, respectively. All 
these functional markers should be decided in 
the Korean transfer stage. Specifically, we 
require a process to select one from the possible 
conjugational markers when a Chinese SVC 
sentence is transferred to its Korean counterpart. 
2. Related Works 
A SVC is studied among several researchers 
as different names. But the general syntactic 
form is (NP) V1 (NP) V2 (NP)1. The variance of 
definition for SVC comes from the different 
scope of interpretation for the sentence pattern. 
 We will introduce three typical researches to 
clearly outline our definition of SVC.  The 
narrowest view of scope is suggested in (L?, 
1953). In his interpretation, V1 and V2 have the 
same subject and should be not coordinative, but 
it is difficult to decide which one is main or 
additional.  Zhu (Zhu, 1981) includes all cases 
of L??s and the possibility of adding an adjective 
to substitute for the second verb position. He 
also includes the case where an additional verb 
and a main verb are used, such as  V+? 
expression in V1 position, which indicates that 
V1 is additional and V2 is main. The broadest 
scope is proposed in (Li & Thompson, 1981). 
According to his interpretation, an SVC includes 
not only all the patterns noted above but also a 
pivot construction, a subject/object clause, and a 
coordinate clause, but excludes the pattern with 
an adjective in the V2 position. In this paper, the 
scope of SVC is almost same as Li?s but the 
classification of SVCs differs slightly, detailing 
the categorization in chapter 4.  
 A few computational solutions to identifying 
SVCs have been proposed by some researchers. 
A formal description is shown in  (Chan, 1998) 
using time lapse notation and the related 
definition. However, her method makes it 
difficult to computationally detect SVCs without 
the resources containing the deep level of 
analysis of each lexical, which is not obtainable 
in the current stage of language processing. 
                                                     
1 V1 : first verb, V2 : second verb, NP : noun phrase.  
3. Overview of TOTAL-CK System 
Architecture 
As a typical transfer system, TOTAL-CK 
consists of three parts: Chinese analysis, 
dependency tree transfer, and Korean generation.  
The system architecture of TOTAL-CK is shown 
in figure 1. The design principles and the detail 
descriptions are given in (Kim et al, 2002). 
 
  
 
 
 
 
 
 
 
 
 
Figure 1: TOTAL-CK system architecture 
4. Classification of Serial Verb 
Construction 
In the previous chapter, we mentioned the 
syntactic format of SVCs which is NP V1 (NP) 
V2 (NP) and the different scope of definition of 
SVCs by the Chinese language researches. To 
outline the scope of SVCs, we define SVCs in 
terms of dependency relation such that V1 is the 
head of V2, or V2 is the head of V1. It is 
formally defined as follows: 
Definition 1 
Let N represent a set of nodes in a dependency 
tree, and W a set of words. Further Let V be a 
set of verbs, and P be a set of all parts of speech 
in Chinese. Then the functions: head, nw, and 
npos, are defined as below: 
head(n) =hn where n?N and hn is the head of n 
nw(n) = w where n ?N and  w ?  W 
npos(n) = np where n ?N and  np ?P  
A definition of SVC is: 
Given a node n such that npos(n) ?  V and 
Head(n) = hn, 
If and only if npos(hn) ?  V and hn is the head of 
a given sentence then the sentence is a SVC.  
The three sentences from the top of table 1 
satisfy the given condition. Also the head of 
the node is the sentence head, thus these must 
be SVCs. For the  last sentence, nw(n) is ?
?, and nw(Head(n)) is ?? whose the POS  
is not verb and also whose the node is not the 
sentence head. Thus it is not an SVC. 
Sentence nw nwh SH SVC
?????? ? ? Yes Yes
?????????? ? ? Yes Yes
???????? ? ? Yes Yes
?????????
???1000?? 
?? ?? No No
Table 1: Example of Testing SVC 
Where nw: nw(n); nwh: nw(head(n)); SH :testing if 
head(n) is the sentence head ; n is a given node. 
 
Our definition is employed to recognize a 
SVC in the Chinese analysis stage. First we 
describe the classification that is used in the 
Chinese analysis stage.  
4.1 Categories in Chinese Analysis Stage 
All dependency relations, which are detected 
by the above definition, are classified into five 
categories: separate events, object, subject, 
pivotal construction and descriptive clauses, 
based on the classification of Li (Li & 
Thompson, 1981). 
4.1.1 Separate Events  
The serial verb patterns classified by most 
researchers belong to this group where switching 
V1 to V2 provides us a different meaning. In 
addition, we add the case where transposing V1 
to V2 provides us the same meaning in this 
group. 
4.1.2 Object 
If V2 is the main verb in an object clause or a 
object phrase then it belongs to this group. 
4.1.3 Subject 
If V1 is the main verb in the subject clause or 
subject phrase, it is assigned to this group. 
4.1.4 Pivot 
If the noun phrase between V1 and V2 is the 
object of V1 and the subject of V2, then it is a 
pivot construction. 
4.1.5 Descriptive  
 If V2 describes the noun phrase between V1 
and V2, then it is a descriptive SVC. 
 All categories of SVCs are shown in Table 2 
The corresponding Chinese dependency 
relations to object, subject and pivot 
constructions also appear in the some research in 
Chinese language processing (Zhou & Huang, 
1994) but the other two are not shown due to 
their different viewpoints. 
The descriptive construction is directly able to 
be one-to-one mapped to the Korean counterpart. 
However the separate event SVCs should be 
further classified for Korean transfer since the 
separate event SVC is possibly mapped into 
sentences with several different Korean 
conjunctional particles.  Thus, it is touched in 
the transfer stage. 
Category Example 
Separate Event ?????; ????????? 
Object ?????????? 
Subject ???????? 
Pivot ???????? 
Descriptive ??????????? 
Table 2: Examples of SVC category 
4.2 Subcategories in Transfer Stage 
The separate event SVC for each sense of 
Korean conjunctional particles is classified into 
the following subcategories: restrictive, 
quasi-coordinative, simultaneous, transitional, 
and circumstantial by the Korean language 
viewpoint. 
4.2.1 Restrictive  
 The action of V2 is performed under the 
restriction given by V1.  There are different 
types of restriction, such as space, group-related, 
causal, and instrumental. The examples are 
presented in table 3. 
Sentence V1 V2 R 
type 
????????????
??2? 
?? ?? space 
????????????? ?? ?? group
?????????? ?? ?? causal
????????????
??????? 
?? ?? tool 
Table 3: Examples of Restrictive Separate Events 
 
                                                     
2  The sentence can also be interpreted as purposive 
separate events. But it is included into a restrictive separate 
event SVC because it is impossible to detect the differences 
between restrictive and purposive, as this requires 
pragmatic level information 
 
4.2.2 Quasi-Coordinative 
In quasi-coordinative, two different cases 
exist. First, transposing V1 to V2 never causes a 
meaning shift of the sentence, named alternative. 
The other is that V1 and V2 are only 
sequentially related, called consecutive. 
4.2.3. Simultaneous 
In a simultaneous case, V1 and V2 occur at the 
same time. 
4.2.4 Transitional 
 If the action of V1 is interrupted by the action 
V2, then it is transitional. 
4.2.5 Circumstantial 
When V2 occurs on the condition of the action 
of V1, then it is classified as a circumstantial 
case.  
The examples for rests of the separate event 
are given in Table 4. 
Type Example 
Q-Coordinative ?????????(alternative)  
??????(consecutive) 
Simultaneous  ?????? 
Transitional ?????????? 
Circumstantial ????? 
Table 4: Examples of Separate Events 
 
In restrictive, quasi-coordinate, simultaneous, 
transitional, and circumstantial separate event 
SVC Chinese sentences, all the above verbs are 
mapped into the corresponding Korean verb 
followed by the Korean conjunctional particle 
?se?, ?ko?, ?un-chay-lo?, ?taka? and ?myen?, 
respectively. 
5. Identification of SVCs 
 To recognize SVCs, we divide the identifying 
process into two stages. The general categories 
of SVCs are able to be found at the analysis 
stage and the subcategories of a separated event 
SVC are detected in the transfer stage.  
5.1 Analysis Stage 
To recognize the five general categories of 
SVCs, two resources are used: one is the 
Grammatical Knowledge Base of Contemporary 
Chinese (GKBCC) and the other is a verb list 
with valency information (VLVI) (Zhu et al, 
1995).  Checking a verb in GKBCC allows us 
to simply detect a pivot SVC. The remainders of 
the other types of SVCs should be carefully 
handled. There are two possible ambiguous 
structures of SVCs 
Case 1 : NP V1 V2 (NP2) 
Case 2 : NP V1 NP1 V2 (NP2) 
Where NP, NP1 and NP2 are noun phrases. 
The algorithm for each case is illustrated in 
figure 2 and figure 3. In Figure 3, the test ?V1 
takes NP & VP?  means that the verb ?? can 
have a noun phrase or an object clause as an 
object. The test, ?satisfy valency?  denotes that  
the second verb  ?? takes a human subject, 
and  ??? can be the subject of the verb ??, 
thus it is classified as an object case. For the 
other sentence, since ?? cannot be the subject 
of the verb ??, it is determined as a subject 
case. 
 
  SVC sentence 
 
 
 Y Object SVC 
Ex) ???????   an object exists?
 
N  
Separate Event SVC
Ex) ??????? 
 
 
 
Figure 2: Algorithm of Detecting SVC for Case 1 
 
 
 
 
Subject SVC 
Ex) ??????????
?????? 
 SVC sentence 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   Figure 3: Algorithm of Detecting SVC for Case 2 
Object SVC 
Ex) ??????????
Y V1 takes 
NP &VP 
N 
N 
Y 
 V2 takes VPas Sub. ?
N 
Separate Events SVC 
Ex) ???????????
Y 
Descriptive SVC 
Ex) ????????? 
????? 
Subject SVC 
Ex) ??????? 
?????????????
 V2 takes VP  
as Sub. ? 
N
Satisfy 
Valency ? 
Y 
 
5.2 Transfer Stage 
The simultaneous separate events is easily 
recognized by the lexical (?) attached to the first 
verb. Also, we use a simple heuristic to detect 
the circumstantial separate events with the 
lexical pattern information.  
The resource used in this stage is a Chinese 
thesaurus called Tongyi-ci-cilin (Mei, 1983). 
With the thesaurus the remainders of separate 
event SVCs are processed with great care. If V2 
is related to the interrupt concept then the 
transitional separate events are assigned. The 
most difficult and frequently occurring cases are 
the restrictive separate events and 
quasi-coordinative separate event.  
The key idea of using the thesaurus is based 
on the observation that the verb V2, if restricted 
by V1 makes it possible that the concept of V2 
will also be restricted by the concept of V1. To 
complete the solution, we first define the 
relations: RSTV, RSTL and RSTM as follows:  
Definition 2 
We define the relations: RSTV, RSTL, and 
RSTM, as follows: 
RSTV= {(V1,V2)  where V1 and V2 are the 
first verb and second verb in a given SVC 
sentence and V2 is semantically restricted by 
V1 : (V1,V2)  (V2,V1)} ?
RSTL= {(CL1,CL2) where CL1 and CL2 are 
the low level concept of the first verb and the 
low level concept 3  of second verb in the 
Chinese thesaurus, respectively, and CL2 is 
semantically restricted by CL1 : (CL1,CL2) ?  
(CL2,CL1)} 
RSTM= {(CM1, CM2) where CM1 and CM2 
are the middle level concept of the first verb and 
the middle level concept of second verb in the 
Chinese thesaurus, respectively, and ML2 is 
semantically restricted by ML1 : (ML1,ML2) ?  
(ML2, ML1)} 
The relations RSTV, RSTL, and RSTM are 
not symmetric and not reflexive. Based on the 
definition we derive the following heuristics: 
 if (V1,V2)?  RSRV then (CL1,CL2) ?RSTL 
But if (V1,V2) ? RSTV then not always 
(CM1,CM2) ?  RSTM. 
                                                     
3 The thesaurus consists of three levels of hierarchy. For 
example, H, Hj, and Hj20 correspond to the one of highest 
concept, the next narrow term called middle-level concept 
and the narrowest term called low-level concept, 
respectively. 
All three examples from the top of table 5 
satisfy the condition that, if (V1, V2)  RSTV 
then (CL1,CL2) 
?
?RSTL and (CM1,CM2) ?  
RSTM. If the condition is always true, then we 
use the middle-level concept relation for 
detecting a restrictive separate event in order to 
increase the applicability of our rules. Also, the 
data structure of RSTM is easily represented 
with an adjacent matrix with the size of 21*21 4 
(Sahni, 1998) where the matrix M is a square 
matrix, whose column and row are the 
middle-level concept, and if M(i,j) = 1 then 
concept j is semantically restricted by concept i, 
otherwise (i,j) ?RSTM. 
RSTV RSTL RSTM Example 
V1 V2 CL1 CL2 CM1 CM2
????????
???????
??  
?
?
?
?
Hj20 Hi21 Hj Hi
???????
????? 
?
?
?
?
Hj20 Hj12 Hj Hi
???????
????????
?? 
?
?
?
?
Hj36 Hj14 Hj Hi
 
 
???????
?????? 
?
?
?
?
Hi17 Hj20 Hi Hj
Table 5: Example of RSTV, RSTL and RSTM 
 
However, the last example reveals that the 
condition is not always true since we have the 
result, both (Hi,Hj) and (Hj,Hi) ?  RSTM.    
Thus, it violates the definition of RSTM. Hence, 
we may not directly use the middle-level concept 
adjacent matrix and the size of the low-level 
concept matrix is too large to be used.5  
 We come up with a solution of a frame with 
multi level concepts. The frame consists of three 
parts: the middle-level concept adjacent matrix, 
the low-level concept adjacent lists and the 
collocation serial verb list for detecting a serial 
verb that always appears together. 
Our solution is that the exceptional cases are 
covered by either the collocation verb lists or the 
low-level concept adjacent list. The remaining 
frequently occurring cases are captured by the 
middle-level adjacent matrix. This leads to the 
sparse matrix of the low-level concept which 
                                                     
4 The number of verbs related middle-level concept in the 
Chinese thesaurus is 21. 
5 The number of verbs related low-level concepts in the 
Chinese thesaurus is about 500. 
causes the adaptation of adjacent lists rather than 
an adjacent matrix for the low-level concepts. 
The order of searching the frame is the 
collocation list, the low-level concept list and the 
middle-level concept matrix. In the collocation 
list, if V1 and V2 belongs to the collocation list 
of the restrictive separate events, such as ???
? or the one of quasi-coordinative, such as ??
?? then the sentence is assigned to a restrictive 
case or a quasi-coordinative case, respectively. 
In the low-level concept lists and the 
middle-level concept matrix, if matching 
succeeds, which means that V2 is semantically 
restricted by V1, then a restrictive case is 
assigned; otherwise, a quasi-coordinate case is 
detected6. The detailed process for identifying 
the subcategories of separate events is shown in 
figure 4. 
6. Evaluation 
We randomly selected 1000 SVC sentences 
from 1998 people?s daily newspapers. The 
number of verbs in the sentence is two since our 
dependency parser is still being improved to 
detect the sentences with multiple embedding 
clauses. In table 6, the distribution of each type 
of SVC and the precision are shown. 
Type Frequency Percentage 
Separate events 402 40.2 
Object 479 47.9 
Subject 31 3.1 
Pivot 39 3.9 
Descriptive 1 0.1 
Error 56 5.6 
(Presicion:94.4%)
Total 1000 100 
Table 6: Distribution of Categories of SVC 
 
The precision is 94.4% and some of the errors 
occur from the tagger, thus some sentences are 
not SVCs. The rest of the errors result from 
missing information in the knowledge bases:  
                                                     
6  For a sentence ???????????  where the 
relation (Hj20,Hj12) is not in the low-level adjacent list, but 
(Hj,Hi) is 1 in the middle-level matrix, it is assigned to the 
restrictive case, while for the sentence ?????????
??? where (Hi17,Hj20) is in the low-level adjacent list, 
thus searching is stopped, it is assigned as a restrictive case. 
A sententence ????????  do not satisfy all 
conditions, thus it is detected as Quasi-Coordinate. 
 
 Separate Event SVC 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4: Detection Algorithm of Subcategory of 
Separate Event SVC. 
 
GKBCC and VLVI. We need the complete list 
of verbs, which has a clause as a subject. These 
verbs in the list will be gradually collected in 
future works. 
The evaluation table for the separate event 
SVCs is provided in Table 7. 
Type Frequency Percentage 
Restrictive 153 38.5 
Quasi-coordinative 184 45.7 
Simultaneous 33 8.2 
Transitional 3 0.7 
Circumstantial 12 2.9 
Error 19 4.7 
(Precision:95.3%)
Total 402 100 
Table 7: Result of Separate Event 
 
N 
N
 V2 is Interrupt 
Concept ? 
 V1 & V2 in middle 
-level concept? 
 
N 
Restrictive 
Separate Events
Quasi-Coordinate Separate
Events 
N 
 V1 & V2 in low-  
level concept? 
N 
N 
 
In 
Restrictive ?
Y 
Y 
Y 
Y V1 & V2 in 
Collocation list?  
In Quasi 
Coordinate?
Y 
Y Simultaneous 
Separate Events
 V1 with ? ?
N 
Circumstantial
Separate Events
Y  Match lexical
Pattern ? 
N 
Transitional 
Separate Events
Y 
The precision of identifying the category of 
separate event is 95.3%. The errors resulted from 
a circumstantial case since our heuristics is too 
restrictive to detect all cases, thus, it might be 
revised further, and since the low-level concept 
lists are not completed.  The low-level concept 
lists will be continuously updated for increasing 
coverage in the tuning stage of the machine 
translation system. 
Table 8 shows the distribution of the 
subcategory of restrictive separate events for 
Korean transfer. 
Type Frequency Percentage
Space 86 56.2 
Group-related 38 24.8 
Causal 17 11.1 
Instrumental 12 7.9 
Total 153 100 
Table 8: Category of Restricted Separated Event 
 
In table 9, the frequency for each type of 
accessed resource is listed. Notice that most 
restrictive separate event SVCs are recognized in 
the middle-level matrix. The two cases in 
collocation are all the case of quasi-coordinative, 
thus, the total number is greater than 153. 
Type of accessed 
resource 
Frequency Percentage
Middle-level matrix 121 78.0 
Low-level list 32 20.6 
Collocation list 2 1.29 
Total 155 100 
Table 9: Access Frequencies for Resource Type 
 
 
 
 
 
 
 
 
Figure 5: Demo system of TOTAL-CK 
 
In figure 5, a demo system of TOTAL-CK is 
illustrated. For a given Chinese SVC sentence 
displayed in the top position of the right-most 
window, the corresponding Korean sentence is 
followed in the next row.  The tagged results, 
the segment of chunking, and the Chinese 
dependency tree with indentation are shown in 
each window from left to right.  
Conclusion and Future work 
In this paper, we formally define serial verb 
constructions, and classified the SVC into 
several categories. These categories are related 
to the analysis stage and the transfer stage of 
TALK-CK. We provided a resolution algorithm 
detecting SVCs in each step. Finally, at each 
stage, a promising experimental result is shown.  
Further research must help to better resolve 
the conditional separate event SVC and 
purposive separate event SVC.  
Acknowledgements 
This work was supported by the Korea 
Science and Engineering Foundation 
(KOSEF)  through the Advanced Information 
Technology Research Center(AITrc). 
References  
Chan  Y. W. (1998) Formal Criteria for interpreting 
Chinese Serial Verb Constructions. 
Communications of COLIPS 8(1) ,pp.13-29. 
Kim D.I., Cui Z, Li J.J. and Lee J.H. (2002). 
Resolving Structural Transfer Ambiguity in 
Chinese-to Korean Machine Translation. 2002 
International Conference on Chinese Language 
Computing, Taichung, Taiwan. 
Li C  N. , Thompson S A. (1981), Mandarin 
Chinese: A functional reference grammar. 
University of California Press, USA. 
L? S. X.(1953) Yufa Xuexi (The Study of Chinese 
Grmmar). Bejing, Zhungguo Qingnian  Press. 
Mei J.J.(1983) Chinese Thesaurus 
(Tong-Yi-Ci-Ci-lin). Shanghai Cishu Press. 1983. 
Sahni, S.(1998) Data structures, algorithms, and 
applications in C++. Boston McGraw-Hill, USA.  
Zhou M and Huang C. (1994) Approach to the 
Chinese Dependency Formalism for the Tagging of 
Corpus. Journal of Chinese Information Processing, 
8/3, pp. 35-52, 1994 
Zhu D.X.(1981), Yufa Jiangyi (Lectures on Chinese 
Grammar),Bejing, Xiangwu Press. 
Zhu X. F, Yu S. W and Wang H. (1995) The 
Development of Contemporary Chinese 
Grammatical Knowledge Base and its Applications. 
Communications of COLIPS, 5/1-2 
Automatic Extraction of English-Chinese Transliteration Pairs  
using Dynamic Window and Tokenizer 
Chengguo Jin 
Dept. of Graduate School for Information 
Technology, POSTECH, Korea 
chengguo@postech.ac.kr 
Dong-Il Kim 
Language Engineering Institute, YUST, 
China 
dongil@ybust.edu.cn 
Seung-Hoon Na 
Dept. of Computer Science & Engineering 
POSTECH, Korea 
nsh1979@postech.ac.kr 
Jong-Hyeok Lee 
Dept. of Computer Science & Engineering 
POSTECH, Korea  
jhlee@postech.ac.kr 
 
 
Abstract 
Recently, many studies have been focused 
on extracting transliteration pairs from bi-
lingual texts. Most of these studies are 
based on the statistical transliteration mod-
el. The paper discusses the limitations of 
previous approaches and proposes novel 
approaches called dynamic window and to-
kenizer to overcome these limitations. Ex-
perimental results show that the average 
rates of word and character precision are 
99.0% and 99.78%, respectively. 
1 Introduction 
Machine transliteration is a type of translation 
based on phonetic similarity between two lan-
guages. Chinese Named entities including foreign 
person names, location names and company names, 
etc are usually transliterated from foreign words. 
The main problem of transliteration resulted from 
complex relations between Chinese phonetic sym-
bols and characters. Usually, a foreign word can be 
transliterated into various Chinese words, and 
sometimes this will lead to transliteration complex-
ity.   In addition, dozens of Chinese characters cor-
respond to each pinyin which uses the Latin 
alphabet to represent sounds in Standard Mandarin. 
In order to solve these problems, Chinese 
government published the ?Names of the world's 
peoples?[12] containing 630,000 entries in 1993, 
which took about 40 years. However, some new 
foreign names still cannot be found in the diction-
ary. Constructing an unknown word dictionary is a 
difficult and time consuming job, so in this paper 
we propose a novel approach to automatically con-
struct the resource by efficiently extracting trans-
literation pairs from bilingual texts.  
Recently, much research has been conducted on 
machine transliteration. Machine transliteration is 
classified into two types. One is automatic genera-
tion of transliterated word from the source lan-
guage [6]; the other one is extracting transliteration 
pairs from bilingual texts [2]. Generally, the gen-
eration process performs worse than the extraction 
process. Especially in Chinese, people do not al-
ways transliterate foreign words only by sound but 
also consider the meanings. For example, the word 
?blog? is not transliterated into ???? ? (Bu-
LaoGe) which is phonetically equivalent to the 
source word, but transliterated into ????(BoKe) 
which means ?a lot of guests?. In this case, it is too 
difficult to automatically generate correct translit-
eration words.  Therefore, our approach is based on 
the method of extracting transliteration pairs from 
bilingual texts. 
The type of extraction of transliteration pairs can 
also be further divided into two types. One is ex-
tracting transliteration candidates from each lan-
guage respectively, and then comparing the pho-
netic similarities between those candidates of two 
languages [2, 8]. The other one is only extracting 
transliteration candidates from the source language, 
and using the candidates to extract corresponding 
transliteration words from the target language [1]. 
In Chinese, there is no space between two words 
and no special character set to represent foreign 
words such as Japanese; hence the candidate ex-
traction is difficult and usually results in a low pre-
cision. Therefore, the method presented in [2] 
which extracted transliteration candidates from 
9
Sixth SIGHAN Workshop on Chinese Language Processing
both English and Chinese result in a poor perform-
ance. Compared to other works, Lee[1] only ex-
tracts transliteration candidates from English, and 
finds equivalent Chinese transliteration words 
without extracting candidates from Chinese texts. 
The method works well, but the performance is 
required to be improved. In this paper we present a 
novel approaches to obtain a remarkable result in 
extracting transliteration word pairs from parallel 
texts.  
The remainder of the paper is organized as fol-
lows: Section 2 gives an overview of statistical 
machine transliteration and describes proposed 
approaches. Section 3 describes the experimental 
setup and a quantitative assessment of performance 
of our approaches. Conclusions and future work 
are presented in Section 4. 
 
2 Extraction of English-Chinese translit-
eration pairs 
In this paper, we first extract English named en-
tities from English-Chinese parallel texts, and se-
lect only those which are to be transliterated into 
Chinese. Next we extract Chinese transliteration 
words from corresponding Chinese texts. [Fig. 1] 
shows the entire process of extracting translitera-
tion word pairs from English-Chinese parallel texts. 
 
 
[Fig 1]. The process of extracting transliteration pairs from 
English-Chinese parallel corpus 
 
2.1 Statistical machine transliteration model 
  Generally, the Chinese Romanization system pin-
yin which is used to represent the pronunciation of 
each Chinese character is adopted in Chinese trans-
literation related studies. For example, the Chinese 
word ????? is first transformed to pinyin ?Ke 
Lin Dun?, and we compare the phonetic similarities 
between ?Clinton? and ?KeLinDun?. In this paper, 
we assume that E is written in English, while C is 
written in Chinese, and TU represents translitera-
tion units. So P(C|E), ??P( ? |Clinton) can be 
transformed to P(KeLinDun|Clinton). In this paper 
we define English TU as unigram, bigram, and tri-
gram; Chinese TU is pinyin initial, pinyin final and 
the entire pinyin. With these definitions we can 
further write the probability, ??P( ?|Clinton), as 
follows:  
(P ??? | Clinton ) ? )|( ClintonkelindunP   
 ?  )|()|()|()|()|( onunPdtPininPllPCkeP (1) 
 
 
[Fig 2]. TU alignment between English and Chinese pinyin 
 
[Fig 2] shows the possible alignment between Eng-
lish word ?Clinton? and Chinese word ??????s 
pinyin ?KeLinDun?.  
In [1], the authors add the match type informa-
tion in Eq. (1). The match type is defined with the 
lengths of TUs of two languages. For example, in 
the case of )|( CkeP the match type is 2-1, be-
cause the size of Chinese TU ke is 2 and the size 
of English TU C is 1. Match type is useful when 
estimating transliteration model?s parameters with-
out a pronunciation dictionary. In this paper, we 
use the EM algorithm to estimate transliteration 
model?s parameters without a pronunciation dic-
tionary, so we applied match type to our model. 
Add Match type(M) to Eq.(1) to formulate as fol-
lows: 
  
)|(),|(max)|( EMPEMCPECP
M
?  
    )(),|(max MPEMCP
M
?               (2) 
( )?
=
+?
N
i
iiiM
mPuvPECP
1
)(log)|((logmax)|(log  (3) 
 
10
Sixth SIGHAN Workshop on Chinese Language Processing
where u, v are English TU and Chinese TU, re-
spectively and m is the match type of u and v. 
 
[Fig 3]. The alignment of the English word and the Chinese 
sentence containing corresponding transliteration word 
 
[Fig 3] shows how to extract the correct Chinese 
transliteration ?? ??(KeLinDun) with the given 
English word ?Clinton? from a Chinese sentence.  
 
2.2 Proposed methods 
  When the statistical machine transliteration is 
used to extract transliteration pairs from a parallel 
text, the problems arise when there is more than 
one Chinese character sequence that is phonetically 
similar to the English word. In this paper we pro-
pose novel approaches called dynamic window and 
tokenizer to solve the problems effectively.  
 
2.2.1 Dynamic window method 
The dynamic window approach does not find the 
transliteration at once, but first sets the window 
size range according to the English word candi-
dates, and slides each window within the range to 
find the correct transliterations. 
 
[Fig 4]. Alignment result between English word ?Clinton? 
and correct Chinese transliteration, add a character into correct 
Chinese transliteration, and eliminate a character from correct 
Chinese transliteration. 
 
If we know the exact Chinese transliteration?s 
size, then we can efficiently extract Chinese trans-
literations by setting the window with the length of 
the actual Chinese transliteration word. For exam-
ple, in [Fig 4] we do alignment between the Eng-
lish word ?Clinton? and correct Chinese translit-
eration ?????(KeLinDun), add a character into 
correct Chinese transliteration ? ? ? ?
??(KeLinYiDun), and eliminate a character from 
correct Chinese transliteration ? ? ?(LinDun) 
respectively. The result shows that the highest 
score is the alignment with correct Chinese trans-
literation. This is because the alignment between 
the English word and the correct Chinese translit-
eration will lead to more alignments between Eng-
lish TUs and Chinese TUs, which will result in 
highest scores among alignment with other Chi-
nese sequences. This characteristic does not only 
exist between English and Chinese, but also exists 
between other language pairs. 
However, in most circumstances, we can hardly 
determine the correct Chinese transliteration?s 
length. Therefore, we analyze the distribution be-
tween English words and Chinese transliterations 
to predict the possible range of Chinese translitera-
tion?s length according to the English word. We 
11
Sixth SIGHAN Workshop on Chinese Language Processing
present the algorithm for the dynamic window ap-
proach as follows:  
Step 1: Set the range of Chinese transliteration?s 
length according to the extracted English word 
candidate.  
Step 2: Slide each window within the range to 
calculate the probability between an English word 
and a Chinese character sequence contained in the 
current window using Eq 3. 
Step 3: Select the Chinese character sequence 
with highest score and back-track the alignment 
result to extract the correct transliteration word. 
[Fig 5] shows the entire process of using the dy-
namic window approach to extract the correct 
transliteration word.  
 
English Word Ziegler 
Chinese Sentence ?? ? ??? ?? ?1963 ?? ??? 
English Sentence Ziegler and Italian Chemist Julio re-ceived the Nobel prize of 1963 together. 
Extracted translit-
eration without 
using dynamic 
window 
?? ? (JiaJuLiAo) 
Correct translitera-
tion ??  (QiGeLe) 
Steps 
1. Set Chinese transliteration?s range according to English 
word ?Ziegler? to [2, 7] (After analyzing the distribution be-
tween an English word and a Chinese transliteration word, we 
found that if the English word length is ?, then the Chinese 
transliteration word is between ?/3 and?.) 
2. Slide each window to find sequence with highest score. 
3 Select the Chinese character sequence with highest score and 
back-track the alignment result to extract a correct translitera-
tion word. 
Win-
dow 
size 
Chinese character sequence with high-
est score of each window (underline 
the back-tracking result) 
Score 
(normal-
ize with 
window 
size) 
2 ? (QiGe) -9.327 
3 ??  (QiGeLe) -6.290 
4 ?? ? (QiGeLeYu) -8.433 
5 ?? ?  (QiGeLeYuYi) -9.719 
6 ?? ??  (JiaJuLiAoGongTong) -10.458 
7 ?? ?  (QiGeLeYuYiDaLi) -10.721 
[Fig 5]. Extract the correct transliteration using the dynamic 
window method 
 
The dynamic window approach can effectively 
solve the problem shown in [Fig 5] which is the 
most common problem that arises from using sta-
tistical machine transliteration model to extract a 
transliteration from a Chinese sentence. However, 
it can not handle the case that a correct translitera-
tion with correct window size can not be extracted.   
Moreover, when the dynamic window approach is 
used, the processing time will increase severely. 
Hence, the following approach is presented to deal 
with the problem as well as to improve the per-
formance. 
 
2.2.2 Tokenizer method 
The tokenizer method is to divide a sentence 
with characters which have never been used in 
Chinese transliterations and applies the statistical 
transliteration model to each part to extract a cor-
rect transliteration.  
There are certain characters that are frequently 
used for transliterating foreign words, such as?
(shi)? (de)? (le)? (he) ??. On the other 
hand, there are other characters, such as ? (shi), 
(de)? (le)? (he),??, that have never been 
used for Chinese transliteration, while they are 
phonetically equivalent with the above characters. 
These characters are mainly particles, copulas and 
non-Chinese characters etc., and always come with 
named entities and sometimes also cause some 
problems. For example, when the English word 
?David? is transliterated into Chinese, the last pho-
neme is omitted and transliterated into ?
??(DaWei). In this case of a Chinese character 
such as ? ?(De) which is phonetically similar 
with the omitted syllable ?d?, the statistical translit-
eration model will incorrectly extract ? ?
?(DaWeiDe) as transliteration of ?David?. In [1], 
the authors deal with the problem through a post-
process using some linguistic rules. Lee and Chang 
[1] merely eliminate the characters which have 
never been used in Chinese transliteration such as 
? ?(De) from the results. Nevertheless, the ap-
proach cannot solve the problem shows in [Fig 6], 
because the copula ? ?(Shi) combines with the 
other character ? ?(zhe) to form the character 
sequence ? ?(ZheShi) which is phonetically 
similar with the English word ?Jacey?, and is in-
correctly recognized as a transliteration of ?Jacey?. 
Thus, in this case, although the copula ? ?(Shi) is 
12
Sixth SIGHAN Workshop on Chinese Language Processing
eliminated from the result through the post-process 
method presented in [1], the remaining part is not 
the correct transliteration. Compared with the 
method in [1], our tokenizer approach eliminates 
copula ? ?(Shi) at pre-processing time and then 
the phonetic similarity between ?Jacey? and the 
remaining part ? ?(Zhe) becomes very low; hence 
our approach overcomes the problem  prior to the 
entire process. In addition, the tokenizer approach 
also reduces the processing time dramatically due 
to separating a sentence into several parts. [Fig 6] 
shows the process of extracting a correct translit-
eration using the tokenizer method.  
 
English Word Jacey 
Chinese Sentence ? ? ?
? ? ? 
English Sentence The authors of this book are Peni-nah  Thomson and Jacey  Grahame. 
Incorrectly extracted 
transliteration (ZheShi) 
Correct transliteration ? (JieXi) 
Steps 
1. Separate the Chinese sentence with characters, ??, , , 
?? (including non-Chinese characters such as punctuation, 
number, English characters etc.), which have never been used 
in Chinese transliteration as follows: 
? ? ? ???? 
2. Apply statistical transliteration model to each part and se-
lect the part with highest score, and back-track the part to ex-
tract a correct transliteration.  
No. 
Chinese character sequence of 
each part (underline the back-
tracking result) 
Score 
(normalize with 
window size) 
1 ? (BenShu) -24.79 
2  (ZuoZhe) -15.83 
3 ?(PeiNiNaTangMuShen) -16.32 
4 ? ???? (JieXi) -10.29 
[Fig 6]. Extracting the correct transliteration using the to-
kenizer method. 
 
In conclusion, the two approaches complement 
each other; hence using them together will lead to 
a better performance. 
3  Experiments 
  In this section, we focus on the setup for the ex-
periments and a performance evaluation of the 
proposed approaches to extract transliteration word 
pairs from parallel corpora. 
3.1 Experimental setup 
We use 300 parallel English-Chinese sentences 
containing various person names, location names, 
company names etc. The corpus for training con-
sists of 860 pairs of English names and their Chi-
nese transliterations. The performance of translit-
eration pair extraction was evaluated based on pre-
cision and recall rates at the word and character 
levels. Since we consider exactly one proper name 
in the source language and one transliteration in 
the target language at a time, the word recall rates 
are the same as the word precision rates.  In order 
to demonstrate the effectiveness of our approaches, 
we perform the following experiments: firstly, only 
use STM(Statistical transliteration model) which is 
the baseline of our experiment; secondly, we apply 
the dynamic window and tokenizer method with 
STM respectively; thirdly, we apply these two 
methods together; at last, we perform experiment 
presented in [1] to compare with our methods. 
3.2 Evaluation of dynamic window and to-
kenizer methods 
 
  [table 1]. The experimental results of extracting 
transliteration pairs using proposed methods 
Methods Word  precision 
Character 
precision 
Character 
recall 
STM (baseline) 75.33% 86.65% 91.11% 
STM+DW 96.00% 98.51% 99.05% 
STM+TOK 78.66% 85.24% 86.94% 
STM+DW+TOK 99.00% 99.78% 99.72% 
STM+CW 98.00% 98.81% 98.69% 
STM+CW+TOK 99.00% 99.89% 99.61% 
 
As shown in table 1, the baseline STM achieves 
a word precision rate of 75%.  The STM works 
relatively well with short sentences, but as the 
length of sentences increases the performance sig-
nificantly decreases. The dynamic window ap-
proach overcomes the problem effectively. If the 
dynamic window method is applied with STM, the 
model will be tolerant with the length of sentences. 
The dynamic window approach improves the per-
formance of STM around 21%, and reaches the 
average word precision rate of 96% (STM+DW). 
In order to estimate the highest performance that 
the dynamic window approach can achieve, we 
apply the correct window size which can be ob-
tained from the evaluation data set with STM. The 
result (STM+CW) shows around 98% word preci-
13
Sixth SIGHAN Workshop on Chinese Language Processing
sion rate and about 23% improvement over the 
baseline. Therefore, dynamic window approach is 
remarkably efficient; it shows only 2% difference 
with theoretically highest performance.  However, 
the dynamic window approach increases the proc-
essing time too much.  
When using tokenizer method (STM+TOK), 
only about 3% is approved over the baseline. Al-
though the result is not considerably improved, it is 
extremely important that the problems that the dy-
namic window method cannot solve are managed 
to be solved. Thus, when using both dynamic win-
dow and tokenizer methods with STM (STM+ 
DW+TOK), it is found that around 3% improve-
ment is achieved over using only the dynamic win-
dow (STM+DW), as well as word precision rates 
of 99%.  
 
[table 2]. Processing time evaluation of proposed methods 
Methods Processing time 
STM (baseline) 5 sec (5751 milisec) 
STM+DW 2min 34sec (154893 milisec) 
STM+TOK 4sec (4574 milisec) 
STM+DW+TOK 32sec (32751 milisec) 
  
  Table 2 shows the evaluation of processing time 
of dynamic window and tokenizer methods. Using 
the dynamic window leads to 27 times more proc-
essing time than STM, while using the tokenizer 
method with the dynamic window method reduces 
the processing time around 5 times than the origi-
nal. Hence, we have achieved a higher precision as 
well as less processing time by combining these 
two methods.  
 
3.3 Comparing experiment 
  In order to compare with previous methods, we 
perform the experiment presented in [1]. Table 3 
shows using the post-processing method presented 
in [1] achieves around 87% of word precision rates, 
and about 12% improvement over the baseline. 
However, our methods are 11% superior to the 
method in [1].  
 
[Table 3] Comparing experiment with previous work 
4 Conclusions and future work 
  In this paper, we presented two novel approaches 
called dynamic window and tokenizer based on the 
statistical machine transliteration model. Our ap-
proaches achieved high precision without any post-
processing procedures. The dynamic window ap-
proach was based on a fundamental property, 
which more TUs aligned between correct translit-
eration pairs. Also, we reasonably estimated the 
range of correct transliteration?s length to extract 
transliteration pairs in high precision. The token-
izer method eliminated characters that have never 
been used in Chinese transliteration to separate a 
sentence into several parts. This resulted in a cer-
tain degree of improvement of precision and sig-
nificantly reduction of processing time.  These two 
methods are both based on common natures of all 
languages; thus our approaches can be readily port 
to other language pairs.  
In this paper, we only considered the English 
words that are to be transliterated into Chinese. 
Our work is ongoing, and in near future, we will 
extend our works to extract transliteration pairs 
from large scale comparable corpora. In compara-
ble corpora, there are many uncertainties, for ex-
ample, the extracted English word may be not 
transliterated into Chinese or there may be no cor-
rect transliteration in Chinese texts. However, with 
large comparable corpora, a word will appear sev-
eral times, and we can use the frequency or entropy 
information to extract correct transliteration pairs 
based on the proposed   perfect algorithm. 
 
Acknowledgement 
This work was supported by the Korea Science and Engineer-
ing Foundation (KOSEF) through the Advanced Information 
Technology Research Center (AITrc), also in part by the BK 
21 Project and MIC & IITA through IT Leading R&D Support 
Project in 2007. 
 
Reference 
 [1] C.-J. Lee, J.S. Chang, J.-S.R. Jang, Extraction of translit-
eration pairs from parallel corpora using a statistical translit-
eration model, in: Information Sciences 176, 67-90 (2006) 
[2] Richard Sproat, Tao Tao, ChengXiang Zhai, Named Entity 
Transliteration with Comparable Corpora, in: Proceedings of 
the 21st International Conference on Computational Linguis-
tics. (2006) 
[3] J.S. Lee and K.S. Choi, "English to Korean statistical 
transliteration for information retrieval," International Journal 
of Computer Processing of Oriental Languages, pp.17?37, 
(1998). 
Methods Word  Precision 
Character 
Precision 
Character 
Recall 
STM (baseline) 75.33% 86.65% 91.11% 
STM+DW+TOK 99.00% 99.78% 99.72% 
STM+[1]?s 
method 87.99% 90.17% 91.11% 
14
Sixth SIGHAN Workshop on Chinese Language Processing
[4] K. Knight, J. Graehl, Machine transliteration, Computa-
tional Linguistics 24 (4), 599?612, (1998). 
[5] W.-H. Lin, H.-H. Chen, Backward transliteration by learn-
ing phonetic similarity, in: CoNLL-2002, Sixth Conference on 
Natural Language Learning, Taipei, Taiwan, (2002). 
[6] J.-H. Oh, K.-S. Choi, An English?Korean transliteration 
model using pronunciation and contextual rules, in: Proceed-
ings of the 19th International Conference on Computational 
Linguistics (COLING), Taipei, Taiwan, pp. 758?764, (2002). 
[7] C.-J. Lee, J.S. Chang, J.-S.R. Jang, A statistical approach 
to Chinese-to-English Backtransliteration, in: Proceedings of 
the 17th Pacific Asia Conference on Language, Information, 
and Computation (PACLIC), Singapore, pp. 310?318, (2003). 
[8] Jong-Hoon Oh, Sun-Mee Bae, Key-Sun Choi, An Algo-
rithm for extracting English-Korean Transliteration pairs using 
Automatic E-K Transliteration In Proceedings of Korean In-
formation Science Socieity (Spring). (In Korean), (2004). 
[9] Jong-Hoon Oh, Jin-Xia Huang, Key-Sun Choi, An Align-
ment Model for Extracting English-Korean Translations of 
Term Constituents, Journal of Korean Information Science 
Society, SA, 32(4), (2005) 
[10] Chun-Jen Lee, Jason S. Chang, Jyh-Shing Roger Jang: 
Alignment of bilingual named entities in parallel corpora us-
ing statistical models and multiple knowledge sources. ACM 
Trans. Asian Lang. Inf. Process. 5(2): 121-145 (2006) 
[11] Lee, C. J. and Chang, J. S., Acquisition of English-
Chinese Transliterated Word Pairs from Parallel-Aligned 
Texts Using a Statistical Machine Transliteration Model, In. 
Proceedings of HLT-NAACL, Edmonton, Canada, pp. 96-103, 
(2003). 
[12] Xinhua Agency, Names of the world's peoples: a com-
prehensive dictionary of names in Roman-Chinese ( ?
? ? ), (1993) 
15
Sixth SIGHAN Workshop on Chinese Language Processing
Segmentation of Chinese Long Sentences Using Commas 
Mei xun Jin1, Mi-Young Kim2, Dongil Kim3 and Jong-Hyeok Lee4 
Graduate School for Informa-
tion Technology1, 
Div. of Electrical and 
Computer Engineering24,
Pohang University of Science and Technology 
Advanced Information Technology Research Center(Altrc)
{meixunj1,colorful2,jhlee4}@ 
postech.ac.kr 
Language Engineering Institute3
Div. of Computer, Electronics 
and Telecommunications 
Yanbian University of Science 
and Technology 
dongil@ybust.edu.cn 
 
 
 
 
Abstract 
The comma is the most common form of 
punctuation. As such, it may have the 
greatest effect on the syntactic analysis of a 
sentence. As an isolate language, Chinese 
sentences have fewer cues for parsing. The 
clues for segmentation of a long Chinese 
sentence are even fewer. However, the av-
erage frequency of comma usage in Chi-
nese is higher than other languages. The 
comma plays an important role in long 
Chinese sentence segmentation. This paper 
proposes a method for classifying commas 
in Chinese sentences by their context, then 
segments a long sentence according to the 
classification results. Experimental results 
show that accuracy for the comma classifi-
cation reaches 87.1 percent, and with our 
segmentation model, our parser?s depend-
ency parsing accuracy improves by 9.6 per-
cent.  
 
1 Introduction 
Chinese is a language with less morphology and 
no case marker. In Chinese, a subordinate clause or 
coordinate clause is sometimes connected without 
any conjunctions in a sentence. Because of these 
characteristics, Chinese has a rather different set of 
salient ambiguities from the perspective of statisti-
cal parsing (Levy and Manning, 2003). In addition, 
the work for clause segmentation is also rather dif-
ferent compared with other languages.  
    However, in written Chinese, the comma is used 
more frequently (Lin, 2000). In English, the average 
use of comma per sentence is 0.869 (Jones, 1996a)1 
~1.04(Hill, 1996), and in Chinese it is 1.792, which 
is one and a half to two more times as it is used in 
English. In Korean, the comma is used even less 
than it is in English (Lin, 2000). 
   Since Chinese has less morphology and no case 
marker, and the comma is frequently used, the 
comma becomes an important cue for long Chinese 
sentence parsing. Because more commas may ap-
pear in longer sentences, the necessity of analyzing 
the comma also increases.  
    Some handbooks about standard Chinese gram-
mars list ten to twenty uses of the comma, accord-
ing to the context. Among these uses, is occurrence 
at the end of a clause3 in a sentence (Lin, 2000). 
About 30% of commas are used to separate the 
clause from its main sentence or neighbor clause(s). 
If the comma appears at the end of a clause, the 
position can naturally be set as the clause segmen-
tation point.  
     This paper proposes Chinese long sentence 
segmentation by classifying the comma. In section 
2, related work in clause segmentation and punc-
tuation processing is presented. Comma classifica-
tion criteria are then introduced, and the 
classification model follows. Afterwards, some 
experimental results show how the proposed 
comma classification and long sentence segmenta-
tion are effective in Chinese parsing. Finally, a 
conclusion will be given.  
                                                          
1 The frequency of comma per sentence is calculated as = Total frequency of 
commas/(Total frequency of full stop + Total frequency of Question mark),  
based on the punctuation statistics of Jone?s Phd. thesis P56,57.  
2 The calculation is based on People?s Daily Corpus?98. 
3 Clause in this paper, is a predicate with its full complements, subject, object(s). 
According to the type of a predicate and the context, subject or object may or 
may not appear. Adjunct of the predicate may or may not be included in the 
clause.  
 
 
 
 
?????????????  
Figure1: example of a dependency relation  
 
 
 
??????????????????? 
 
 
 
Figure2: example of a dependency relation  
 
 
 
 
???????????????? 
 
 
Figure 3: example of a dependency relation 
Examples:  
(1) ????????????? 
They have class in the morning and do experiments in 
the afternoon. 
(2) ??????????????????? 
Several years ago,  BeiHai City was only an unknown 
small fishing village. 
(3) ???????????????? 
The students prefer young and beautiful teachers.  
(4) ?????????????? 
Xiao Ming is doing homework and his mom is 
knitting. 
(5) ?????????????? 
Though he studies very hard, his score is not satisfiable.   
(6) ??????????????????????
???? 
The change of domestic economic development in 
Russia has promoted the trade exchange between two 
countries. 
(7) ???????????????????? 
Bank of China invited a Japanese company as its con-
soler last October.  
(8) ????????????????????? 
He is a good leader in the company as well as a good 
daddy at home. 
(9) ?????????????????????
?? 
The quick transfer of the scientific research achieve-
ment to industry is the characteristic of this develop-
ment district. 
(10) ??????????????? 
The students happily come to the playground. 
(11) ??????????????????????
?????????? 
The investment from Korea to DaLian city has grown 
for three years, and all Korean investment companies 
in DaLian  receive preferential treatment. 
(12) ????????????????????   
The statistics show that the exportation from DaLian to 
Korea is reach to USD100,000,000. 
(13) ??????????????????????
?? 
In 1994, TongYong Company purchased goods worthy 
of more than  USD40,000,000.  
(14) ???????????????? 
She gets up early, and does physical exercise every 
morning. 
(15) ?????????????????????
?? 
The occupation of the first products is less than 3/10 
and the portion of the second ones is more than 7/10.   
2 Related Work  
2.1 Related Work for Clause Segmenta-
tion 
Syntactic ambiguity problems increase drasti-
cally as the input sentence becomes longer. Long 
sentence segmentation is a way to avoid the prob-
lem. Many studies have been made on clause seg-
mentation (Carreras and Marquez, 2002, Leffa, 1998, 
Sang and Dejean,2001). In addition, many studies 
also have been done on long sentences segmenta-
tion by certain patterns (Kim and Zhang, 2001, Li and 
Pei, 1990, Palmer and Hearst, 1997).  
However, some researchers merely ignore punc-
tuation, including the comma, and some research-
ers use a comma as one feature to detect the 
segmentation point, not fully using the information 
from the comma.  
2.2 Related Work for Punctuation Proc-
essing  
Several researchers have provided descriptive 
treatment of the role of punctuations: Jones (1996b) 
determined the syntactic function of the punctua-
tion mark. Bayraktar and Akman (1998) classified 
commas by means of the syntax-patterns in which 
they occur. However, theoretical forays into the 
syntactic roles of punctuation were limited. 
Many researchers have used the punctuation 
mark for syntactic analysis and insist that punctua-
tion indicates useful information. Jones (1994) suc-
cessfully shows that grammar with punctuation 
outperforms one without punctuation. Briscoe and 
Carroll 1995) also show the importance of punc-
tuation in reducing syntactic ambiguity. Collins 
(1999), in his statistical parser, treats a comma as an 
important feature. Shiuan and Ann (1996) separate 
complex sentences with respect to the link word, 
including the comma. As a result, their syntactic 
parser performs an error reduction of 21.2% in its 
accuracy.  
(
                                                          
Say (1997) provides a detailed introduction to us-
ing punctuation for a variety of other natural lan-
guage processing tasks.  
All of these approaches prove that punctuation 
analyses improve various natural language process-
ing performance, especially in complex sentence 
segmentation. 
3 Types of Commas  
The comma is the most common punctuation, 
and the one that might be expected to have the 
greatest effect on syntactic parsing. Also, it seems 
natural to break a sentence at the comma position 
in Chinese sentences. The procedure for syntactic 
analysis of a sentence, including the segmentation 
part, is as follows: 
1st step: segment the sentence at a comma 
2nd step: do the dependency analysis for each 
segment 
3rd step: set the dependency relation between  
segment pairs  
In Chinese dependency parsing, not all commas 
are proper as segmentation points.  
First, segmentation at comma in some sentences, 
will cause some of the words fail to find their heads. 
Figure 2 shows, in example (2), there are two 
words, ??  (BeiHai City)  and ?  (preposition) 
from the left segment have dependency relation 
with the word ?(is) of the right segment. So, the 
segmentation at comma , will cause two of  words 
?? (BeiHai City)  and ? (preposition) in the left 
segment, cannot find their head in the second step 
of syntactic parsing stage.  
Second, segmentation at commas can cause 
some words to find the wrong head. Example (3) of 
figure 3 shows two pairs of words with dependency 
relations. For each pair, one word is from the left 
segment, and one word is from the right segment :
?? (like) from the left segment and ??(teacher)  
from the right, ?? (young) from the left and ? 
(of) from the right. Segmentation at the comma 
will cause the word ??(young) to get the word ?
? (like) as its head, which is wrong. 
Example (2) and (3) demonstrate improper sen-
tence segmentation at commas. In figure 2 and fig-
ure 3, there are two dependency lines that cross 
over the commas for both sentences. We call these 
kinds of commas mul_dep_lines_cross comma 
(multiple lines cross comma). In figure 1, there is 
only one dependency line cross over the comma. 
We call these kinds of commas one_dep_line_cross 
comma.    
Segmentation at one_dep_line_cross comma is 
helpful for reducing parsing complexity and can 
contribute to accurate parsing results. However, we 
should avoid segmenting at the position of 
mul_dep_lines_cross comma. It is necessary to 
check each comma according to its context.  
3.1 Delimiter Comma and Separator 
Comma 
Nunberg (1990) classified commas in English 
into two categories, as a delimiter comma and a 
separator comma, by whether the comma is used to 
separate the elements of the same type 4  or not. 
While a delimiter comma is used to separate differ-
ent syntactic types, a separator comma is used to 
separate members of conjoined elements. The 
commas in Chinese can also be classified into these 
two categories. The commas in example (3) and (4) 
are separators, while those in (2) and (5), are 
delimiters. 
However, both delimiter comma and separator 
commas can be mul_dep_line_cross commas. In 
example (2), the comma is a delimiter comma as 
well as a mul_dep_line_cross comma. As a separa-
tor comma, the comma in example (3), is also a 
mul_dep_line_cross comma. Nunberg?s classifica-
tion cannot help to identify mul_dep_line_cross 
commas. 
We therefore need a different kind of classifica-
tion of comma. Both delimiter comma and separa-
tor comma can occur within a clause or at the end 
of a clause. Commas that appear at the end of a 
clause are clearly one_dep_line_cross commas. 
The segmentation at these kinds of comma is valid. 
4 Same type means that it has the same syntactic role in the sentence, it can be a 
coordinate phrase or coordinate clause. 
3.2 Inter-clause Comma and Intra-clause 
Comma 
Commas occurring within a clause are here 
called intra-clause commas. Similarly, commas at 
the end of a clause will be called inter-clause 
commas. Example  (2), (3) include intra-clause 
commas, and example (4), (5) include inter-clause 
commas. 
3.2.1 Constituents of the Two Segments 
Adjoining a Comma 
A segment is a group of words between two 
commas or a group of words from the beginning 
(or end) of a sentence to its nearest comma. 
To identify whether a comma is an inter-clause 
comma or an intra-clause comma, we assign values 
to each comma. These values reflect the nature of 
the two segments next to the comma. Either the left 
or right segment of a comma, can be deduced as a 
phrase5, or several non-overlapped phrases, or a 
clause.(see examples (6)~(15)). The value we as-
sign to a comma is a two-dimensional value 
(left_seg, right_seg). The value of left_seg and 
right_seg can be p(hrase) or c(lause), therefore the 
assigned value for each comma can be (p,p), (p,c), 
(c,p) or (c,c).  
 Commas with (p,p) as the assigned value, in-
clude the case when the left and right segment of 
the comma can be deduced as one phrase, as shown 
in example (6) or several non-overlapped phrases, 
as described in example (7).  
We can assign the value of (c,p) to commas in 
example (8), (9) and (10),  indicating the left ad-
joining segment is a clause and the right one is a 
phrase or several non-overlapped phrases. In a 
similar way, commas in example (11)~(13) are 
case of (p,c). 
 If a comma has (c,c) as the assigned value, both 
the left segment and the right segment can be de-
duced as a clause. The relation between the two 
clauses can be coordinate (example (14)) or subor-
dinate (example (15)).  
                                                          
5 Phrase is the group of words that can be deduced as the phrase in Chinese Penn 
Tree Bank 2.0. A phrase may contain an embedded clause as its adjunct or 
complement.  
(a), ??????????? 
(b) ????????? 
In example (a) ,the PP has the embedded clause as its complement. And in 
example (b), the embedded clause is the adjunct of the NP. 
3.2.2 Syntactic Relation between Two Ad-
joining Segments 
A word (some words) in the left segment and a 
word (some words) in the right segment of a 
comma may or may not have a dependency rela-
tion(s). For a comma, if at least one word from the 
left segment has a dependency relation with a word 
from the right segment, we say the left segment and 
the right segment have a syntactic relation. Other-
wise the two segments adjoining the comma have 
no syntactic relations. Rel() functions are defined 
in table-1. 
Table 1: functions Rel(), Dir() and Head() 
Rel() 
? To check if any words of the left segment has a 
dependency relation with the word of the right 
segment. 
? If there is, Rel()=1  
Otherwise Rel()=0. 
Dir() 
? To indicate how many direction(s) of the de-
pendency relations the left and right segment 
have. when Rel()=1. 
? For one_dep_line_cross comma, Dir()=1. 
? For mul_dep_line_cross comma, if the directions 
of the dependency relations are the same, 
Dir()=1, else Dir()=2. 
Head() 
? To indicate which side of segment contains the 
head of any words of the other side, when 
Rel()=1. 
? When Dir()=1, if the left segment contains any 
word as the head of a word of the right, Head() = 
left; Otherwise Head()=right. 
? When Dir()=2,   
1. According to the direction of dependency 
relation of these two segments, to find the 
word which has no head. 
2. If the word is on the left, Head()=left, other-
wise, Head()=right.  
 
For the one_dep_line_cross comma, the left and 
right segments have syntactic relation, and only 
one word from a segment has a dependency rela-
tion with a word from the other segment. For 
mul_dep_line_cross comma, at least two pairs of 
words from each segment have dependency rela-
tions. We then say that the left and right segments 
adjacent to the comma have multiple dependency 
relations. The directions of each relation may differ 
or not. We define a function Dir() as follows : if all 
the directions of the relations are the same, get 1 as 
its value, else 2 for its value. This is in table-1. We 
also define function Head() to indicate whether the 
left segment or the right segment contains the head 
word of the other when the two segments have syn-
tactic relation. This is also shown in table 1. 
In example (3) as figure 3 shows, Rel()=1, 
Dir()=2 and Head()=left. 
3.2.3 Inter-clause Comma and Intra-
clause Comma  
For commas assigned values  (p,p) or (c,c), the 
function Rel() is always 1. Commas with values (c, 
p) or (p,c) can be further divided into two sub-cases. 
Table 2 shows the sub-case of (c,p), and table 3 
shows the sub-cases of (p,c). 
 
Rel() =0 The 2nd comma of Example (8); 
Example (9); 
Head() =right = p 
(c,p)-
I 
Rel()=1 Example (10); 
Head() = left=c 
(c,p)-
II 
Table 2: sub-cases of commas with value of (c,p) 
 
Rel()=0 Example (11); 
Example (12); 
Head() =left = p 
(p,c)-
I 
Rel()=1 Example (13); 
Head() = right=c 
(p,c)-
II 
Table3: sub-cases of commas with value of (p,c) 
 
Commas with the value of (p,p), (c,p)-II and 
(p,c)-II are used to connect coordinate phrases or to 
separate two constituents of a clause. These com-
mas are intra-clause commas. 
Commas with (c,c), (c,p)-I and (p,c)-I are used 
as a clause boundaries. These are inter-clause 
commas. 
An inter-clause comma joins the clauses together 
to form a sentence. The commas that belong to an 
inter-clause category are safe as segmentation 
points (Kim, 2001).   
4 Feature Selection  
To identify the inter-clause or intra-clause role 
of a comma, we need to estimate the right and left 
segment conjuncts to the comma, using informa-
tion from both segments. Any information to iden-
tify a segment as a clause or a phrase or phrases is 
useful. Carreras and Marquez (2001) prove that 
using features containing relevant information 
about a clause leads to more efficient clause identi-
fication. Their system outperforms all other sys-
tems in CoNLL?01 clause identification shared task 
(Sang & Dejean, 2001). Given this consideration, we 
select two categories of features as follows. 
 (1) Direct relevant feature category: predicate 
and its complements. 
 (2) Indirect relevant feature category: auxiliary 
words or adverbials or prepositions or clausal 
conjunctions. 
Directly relevant features 
VC: if a copula ? appears 
VA: if an adjective appears 
VE: if ? as the main verb appears 
VV: if a verb appears 
CS: if a subordinate conjunction appears 
Table 4: feature types for classification 
Indirectly relevant features 
AD: if an adverb appears 
AS: if an aspect marker appears 
P: if a preposition appears 
DE: if ? appears 
DEV:if ? appears  
DER: if ? appears 
BA_BEI: if ? or ? appears 
LC: if a localizer appears 
FIR_PR : if the first word is a pronoun 
LAS_LO: if the last word is a localizer 
LAS_T : if the last word is a time 
LAS_DE_N : if the last word is a noun that follows ? 
No_word : if the length of a word is more than 5 
no_verb: if no verb(including VA)  
DEC: if there is relative clause 
ONE: if the segment has only one word 
 
To detect whether a segment is a clause or 
phrase, the verbs are important. However, Chinese 
has no morphological paradigms and a verb takes 
various syntactic roles besides the predicate, with-
out any change of its surface form. This means that  
information about the verb is not sufficient, in itself, 
to determine whether segment is a clause.  
When the verb takes other syntactic roles besides 
the predicate, it?s frequently accompanied by func-
tion words. For example, a verb can be used as the 
complement of the auxiliary word ? or ?(Xia, 
2000), to modify the following verb or noun. In 
these cases, the auxiliary words are helpful for de-
ciding the syntactic role of the verb. Other function 
words around the verb also help us to estimate the 
syntactic role of the verb. Under this consideration, 
we employ all the function words as features, 
where they are composed as the indirect relevant 
feature category.  
Table 4 gives the entire feature set. The label of 
each feature type is same as the tag set of Chinese 
Penn Treebank 2.0 (see Xia (2000) for more de-
tailed description). If the feature appears at the left 
segment, we label it as L_feature type, and if it is 
on the right, it?s labeled as R_ feature type, where 
feature type is the feature that is shown on table 4.  
The value for each feature is either 0 or 1. When 
extracting features of a sentence, if any feature in 
the table 4, appears in the sentence, we assign the 
value as 1 otherwise 0. The features of example (12) 
are extracted as table 5 describes. All of these val-
ues are composed as an input feature vector for 
comma classification. 
Table 5: the extracted features of example (12)  
5 Experiments 
For training and testing, we use the Chinese 
Penn Treebank 2.0 corpus based on 10-fold valida-
tion. First, using bracket information, we extract 
the type (inter-clause comma or intra-clause 
comma) for each comma, as we defined. The ex-
tracted information is used as the standard answer 
sheet for training and testing. 
 We extract the feature vector for each comma, 
and use support vector machines (SVM) to perform 
the classification work.  
Performances are evaluated by the following 
four types of measures: accuracy, recall, F?=1/2 for 
inter-clause and intra-clause comma respectively, 
and total accuracy. Each evaluation measure is cal-
culated as follows. 
Inter(or intra)-clause comma accuracy6 =  
 
identifiedofnumberthe
identifiedcorrectlyofnumberthe  
Inter(or intra)-clause comma recall  = 
 
classtheofnumbertotal
identifiedcorrectlyofnumberthe  
Inter(or intra)-clause comma F ?=1/2 = 
recall) comma clauseintra)inter(or              
precision comma clauseintra)(inter(or 
)recallcommaclauseintra)inter(or               
precisioncommaclauseintra)inter(or (2
?
+?
?
???
 
Total accuracy = 
 
commasofnumbertotal
identifiedcorrectlyofnumbertotal  
5.1 Classification Using SVM 
 Support vector machines (SVM) are one of the 
binary classifiers based on maximum margin strat-
egy introduced by Vapnik (Vapnik, 1995). For many 
classification works, SVM outputs a state of the art 
performance.  
L_VC =0 L_VA =0 L_VE =0  
R_VC = 0 R_VA =0 R_VE =0 
L_VV = 0 L_CS =0 L_AD =0 
R_VV =1 R_CS =0 R_AD =0 
L_AS =0 L_P =0  L_DE =0 
R_AS =1 R_P =0 R_DE =1 
L_DEV =0 L_DER = 0 L_BA_BEI =0 
R_DEV =0 R_DER =0 R_BA_BEI=0 
L_LC =0 L_DEC = 0 L_FIR_PR  =0 
R_LC =0 R_DEC =0 R_FIR_PR=0  
L_LAS_LO =0 L_LAS_T =1 L_LAS_DE_N=0 
R_LAS_LO=0 R_LAS_T=0 R_LAS_DE_N=1 
L_No_word=0 L_no_verb =1 L_ONE = 0 
R_No_word =1 R_no_verb =0 R_ONE =0 
There are two advantages in using SVM for clas-
sification: 
(1) High generalization performance in high di-
mensional feature spaces. 
(2) Learning with combination of multiple fea-
tures is possible via various kernel functions.  
Because of these characteristics, many research-
ers use SVM for natural language processing and 
obtain satisfactory experimental results (Yamada, 
2003). 
In our experiments, we use SVMlight (Joachims, 
1999) as a classification tool. 
5.2 Experimental Results 
First, we set the entire left segment and right 
segment as an input window. Table 6 gives the per-
formance with different kernel functions. The RBF 
kernel function with ? =1.5 outputs the best per-
formance. Therefore, in the following experiments, 
we use this kernel function only.  
Next, we perform several experiments on how 
the selection of word window affects performance. 
First, we select the adjoining 3 words of the right 
and left segment each, indicated as win-3 in table 7. 
                                                          
6 The inter-clause comma precision is abbreviated  as inter-P. Same way, Inter-R 
for inter-clause comma recall, ..etc. 
Second, we select the first 2 words and last 3 words 
of the left segment and the first 3 and last 2 of the 
right segment, indicated as win 2-3 in table 7. Fi-
nally, we use the part of speech sequence as input. 
As the experimental results show, the part of 
speech sequence is not a good feature. The features 
with clausal relevant information obtain a better 
output. We also find that the word window of first 
2-last 3 obtains the best total precision, better than 
using the entire left and right segments. From this, 
we conclude that the words at the beginning and 
end of the segment reveal segment clausal informa-
tion more effectively than other words in the seg-
ment. 
5.3 Comparison of Parsing Accuracy with 
and without Segmentation Model 
The next experiment tests how the segmentation 
model contributes to parsing performance. We use 
a Chinese dependency parser, which was imple-
mented with the architecture presented by Kim 
(2001) presents.  
After integrating the segmentation model, the 
parsing procedure is as follows: 
- Part of speech tagging. 
- Long sentence segmentation by comma. 
- Parsing based on segmentation. 
Table 9 gives a comparison of the results of the 
original parser with the integrated parser.  
5.4 Comparison with Related Work  
Shiuan and Ann?s (1996) system obtains the 
clues for segmenting a complex sentence in Eng-
lish by disambiguating the link words, including 
the comma. The approach to find the segmentation 
point by analyzing the specific role of the comma 
in the sentence seems similar with our approach. 
However, our system differs from theirs as follows: 
(1) Shiuan and Ann?s system sieves out just two 
roles for the comma, while ours gives an 
analysis for the complete usages of the 
comma. 
(2) Shiuan and Ann?s system also analyzes the 
clausal conjunction or subordinating preposi-
tion as the segmentation point. 
Although the language for analysis is different, 
and the training and testing data also differ, the 
motivation of the two systems is the same. In addi-
tion, both systems are evaluated by integrating the 
original parser. The average accuracy of comma 
disambiguation in Shiuan and Ann?s is 93.3% that 
is higher than ours by 6.2%. However, for parsing 
accuracy, Shiuan and Ann?s system improves by 
4%(error reduction of 21.2%), while ours improves 
by 9.6 percent. 
 
Kernel 
function Inter-P Inter-R Intra-P Intra-R Inter-F Intra-F Total-P
linear 
74.22
% 
77.87
% 
72.52
% 
70.61
% 
76.00
% 
71.56
% 
73.14
% 
Polynomial 
d=2 
79.84
% 
81.15
% 
84.51
% 
83.77
% 
80.49
% 
84.14
% 
82.86
% 
Polynomial 
d=3 
78.57
% 
81.15
% 
88.39
% 
86.84
% 
79.84
% 
87.61
% 
84.86
% 
RBF    ? = 0.5 78.46% 83.61% 88.64% 85.53% 80.95% 87.05% 84.86% 
RBF  ? = 1.5 78.69% 78.69% 89.04% 89.04% 78.69% 89.04% 85.43% 
RBF  ? = 2.5 80.62% 85.25% 88.24% 85.53% 82.87% 86.86% 85.43% 
RBF ? = 3.5 79.41% 88.52% 85.05% 79.82% 83.72% 82.35% 82.86% 
Table 6: experimental results with  
different kernel functions 
 
Word Win-
dow Inter-P Inter-R Intra-P Intra-R Inter-F Intra-F Total-P
Win3 
80.45
% 
87.70
% 
84.33
% 
80.26
% 
83.92
% 
82.25
% 
82.86
% 
Win2-3 
85.60
% 
87.70
% 
88.00
% 
86.84
% 
86.64
% 
87.42
% 
87.14
% 
Table 7: experimental results for  
word window size  
 
 Inter-P Inter-R Intra-P Intra-R Inter-F Intra-F Total-P
POS 
sequence 
75.42
% 
72.95
% 
80.60
% 
82.02
% 
74.17
% 
81.30
% 
78.86
% 
Table 8: experimental results for using  
part of speech sequence  
 
 Original 
parser 
Integrated 
parser 
Average dependency pars-
ing accuracy7 
73.8% 83.4% 
Average complete sentence 
accuracy 
23.8% 25.4% 
Table 9: comparison of parsing accuracy of the 
original parser with the integrated parser 
                                                          
7 The evaluation measures are used as it is defined in Kim (2001). 
6 Conclusion 
In this paper, we propose a method to segment a 
Chinese sentence by classification of the comma.  
We define the criteria for classification, and ac-
cording to the criteria, a model for classification of 
the comma is given. The segmentation at the 
comma position seems to be efficient for improv-
ing the accuracy of dependency parsing by 
9.6percent. Moreover, since commas more fre-
quently appear in Chinese language, we expect our 
approach including salient and refined analysis of 
comma usages provides feasible solutions for seg-
mentation. 
However, the accuracy for the segmentation is 
not yet satisfactory. Since erroneous segmentation 
may cause a parsing failure for the entire sentence, 
errors can be serious. Further research should be 
done to improve the performance and reduce side 
effects for parsing the entire sentence.  
Acknowledgments 
This work was supported by the KOSEF through 
the Advanced Information Technology Research 
Center (AITrc), and by the BK21 Project. 
References 
M. Bayparktar,  B. Say and V. Akman 1998, An analysis 
of English punctuation: the special case of comma, 
International Journal of Corpus Linguistics, 1998 
X. Carreras, L. Marquez, V. Punyakanok, and D. Roth 
2002, Learning and inference for clause identification, 
Proceeding of 13th European Conference on Machine 
Learning, Finland, 2002 
R.L. Hill 1996, A comma in parsing: A study into the 
influence of punctuation (commas) on contextually 
isolated "garden-path" sentences.  M.Phil disseration, 
Dundee University, 1996 
T.Joachims 1999, Making large-Scale SVM Learning 
Practical. Advances in Kernel Methods - Support 
Vector Learning, B. Sch?lkopf and C. Burges and A. 
Smola (ed.), MIT-Press, 1999 
B. Jones 1994, Exploring the role of punctuation in pars-
ing natural text, Proceedings of COLING-94, pages 
421-425 
B. Jones 1996a, What?s the point? A (computational)  
theory of punctuation, PhD Thesis, Centre for Cogni-
tive Science, University of Edinburgh, Edinburgh, 
UK, 1996 
B. Jones 1996b, Towards testing the syntax of punctua-
tion, Proceeding of 34th ACL, 1996 
M.Y.Kim, S.J. Kang, J.H. Lee 2001, Resolving ambigu-
ity in Inter-chunk dependency parsing, Proceedings 
of the sixth Natural Language Processing Pacific Rim 
Symposium, Tokyo, Japan, 2001 
S. Kim, B.Zhang and Y. Kim 2001, Learning-based 
intrasentence segmentation for efficient translation of 
long sentences, Machine Translation, Vol.16, no.3, 
2001 
Roger Levy and Christopher Manning. 2003. Is it harder 
to parse Chinese, or the Chinese Treebank? In 
Proceeding of ACL-2003. 
V.J. Leffa 1998, clause processing in complex sentences, 
Proceeding of 1st International Conference on Lan-
guage Resources and Evaluation, Spain,1998 
W.C. Li, T.Pei, B.H. Lee and Chiou, C.F. 1990, Parsing 
long English sentences with pattern rules, Proceeding 
of 13th International Conference on Computational 
Linguistics, Finland, 1990 
Shui-fang Lin 2000. study and application of punctua-
tion(??????????). People?s Publisher, 
P.R.China. (in Chinese) 
Geoffrey Nunberg 1990. the linguistics of punctua-
tion .CSLI lecture notes. 18, Stanford, California. 
D.D. Palmer and M.A. Hearst 1997, Adaptive multilin-
gual sentence boundary disambiguation, Computa-
tional Linguistics, Vol.27, 1997 
E.F.T.K. Sang and H.Dejean. 2001, Introduction to the 
CoNLL-2001 shared task: clause identification, Pro-
ceeding of CoNLL-2001 
B. Say and V. Akman 1997, current approaches to 
punctuation in computational linguistics, Computers 
and the Humanities, 1997 
P.L. Shiuan and C.T.H. Ann 1996, A divide-and-
conquer strategy for parsing, Proceedings of the 
ACL/SIGPARSE 5th international workshop on pars-
ing technologies, Santa Cruz, USA, pp57-66 
Fei Xia 2000, The bracketing Guidelines for the Penn 
Chinese Treebank(3.0) 
Vladimir N Vapnik 1995 The nature of statistical learn-
ing theory. New York, 1995 
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 190?196,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Chinese Syntactic Reordering for Adequate Generation of Korean 
Verbal Phrases in Chinese-to-Korean SMT 
Jin-Ji Li, Jungi Kim, Dong-Il Kim*, and Jong-Hyeok Lee 
Department of Computer Science and Engineering,  
Electrical and Computer Engineering Division, 
Pohang University of Science and Technology (POSTECH), 
San 31 Hyoja Dong, Pohang, 790-784, R. of Korea 
E-mail: {ljj, yangpa, jhlee}@postech.ac.kr 
 
*Language Engineering Institute,  
Department of Computer, Electron and Telecommunication Engineering, 
Yanbian University of Science and Technology (YUST), 
Yanji, Jilin, 133-000, P.R. of China 
E-mail: {dongil}@ybust.edu.cn 
 
Abstract 
Chinese and Korean belong to different lan-
guage families in terms of word-order and 
morphological typology. Chinese is an SVO 
and morphologically poor language while Ko-
rean is an SOV and morphologically rich one. 
In Chinese-to-Korean SMT systems, systemat-
ic differences between the verbal systems of 
the two languages make the generation of Ko-
rean verbal phrases difficult. To resolve the 
difficulties, we address two issues in this paper. 
The first issue is that the verb position is dif-
ferent from the viewpoint of word-order ty-
pology. The second is the difficulty of com-
plex morphology generation of Korean verbs 
from the viewpoint of morphological typology. 
We propose a Chinese syntactic reordering 
that is better at generating Korean verbal 
phrases in Chinese-to-Korean SMT. Specifi-
cally, we consider reordering rules targeting 
Chinese verb phrases (VPs), preposition 
phrases (PPs), and modality-bearing words 
that are closely related to Korean verbal phras-
es. We verify our system with two corpora of 
different domains. Our proposed approach 
significantly improves the performance of our 
system over a baseline phrased-based SMT 
system. The relative improvements in the two 
corpora are +9.32% and +5.43%, respectively. 
1 Introduction 
Recently, there has been a lot of research on en-
coding syntactic information into statistical ma-
chine translation (SMT) systems in various forms 
and in different stages of translation processes. 
During preprocessing source language sen-
tences undergo reordering and morpho-syntactic 
reconstruction phases to generate more target 
language-like sentences. Also, fixing erroneous 
words, generating complex morphology, and re-
ranking translation results in post-processing 
phases may utilize syntactic information of both 
source and target languages. A syntax-based 
SMT system encodes the syntactic information in 
its translation model of the decoding step. 
A number of researchers have proposed syn-
tactic reordering as a preprocessing step (Xia and 
McCord, 2004; Collins et al, 2005; Wang et al, 
2007). In these syntactic reordering approaches, 
source sentences are first parsed and a series of 
reordering rules are applied to the parsed trees to 
reorder the source sentences into target language-
like word orders. Such an approach is an effec-
tive method for a phrase-based SMT system that 
employs a relatively simple distortion model in 
the decoding phase. 
This paper concentrates upon reordering 
source sentences in the preprocessing step of a 
Chinese-to-Korean phrase-based SMT system 
using syntactic information. Chinese-to-Korean 
SMT has more difficulties than the language 
pairs studied in previous research (French-
English, German-English, and Chinese-English). 
From the viewpoint of language typology, these 
language pairs are all SVO languages and they 
have relatively simpler morphological inflections. 
On the other hand, Korean is an SOV and agglu-
tinative language with relatively free word order 
and with complex and rich inflections. 
For the Chinese-to-Korean SMT, these syste-
matic differences of the two languages make the 
generation of Korean verbal phrases very diffi-
cult. Firstly, the difference in the verb position of 
the two languages may not be reflected in the 
simple distortion model of a phrase-based SMT 
system. Secondly, Morphology generation of 
190
Korean verbs is difficult because of its complexi-
ty and the translation direction from a low-
inflection language to a high-inflection language. 
In the following sections, we describe the cha-
racteristics of Korean verbal phrases and their 
corresponding Chinese verbal phrases, and 
present a set of hand-written syntactic reordering 
rules including Chinese verb phrases (VPs), pre-
position phrases (PPs), and modality-bearing 
words. In the latter sections, we empirically veri-
fy that our reordering rules effectively reposition 
source words to target language-like order and 
improve the translation results. 
2 Contrastive analysis of Chinese and 
Korean with a focus on Korean verbal 
phrase generations 
In the Chinese-to-Korean SMT, the basic transla-
tion units are morphemes. For Chinese, sentences 
are segmented into words. As a typical isolating 
language, each segmented Chinese word is a 
morpheme. Korean is a highly agglutinative lan-
guage and an eojeol refers to a fully inflected 
lexical form separated by a space in a sentence. 
Each eojeol in Korean consists of one or more 
base forms (stem morphemes or content mor-
phemes) and their inflections (function mor-
phemes). Inflections usually include postposi-
tions and verb endings (verb affixes) of verbs 
and adjectives. These base forms and inflections 
are grammatical units in Korean, and they are 
defined as morphemes. As for the translation unit, 
eojeol cause data sparseness problems hence we 
consider a morpheme as a translation unit for 
Korean.  
As briefly mentioned in the previous section, 
Chinese and Korean belong to different word-
order typologies. The difference of verb position 
causes the difficulty in generating correct Korean 
verbal phrases. Also, the complexity of verb af-
fixes in Korean verbs is problematic in SMT sys-
tems targeting Korean, especially if the source 
language is isolated. 
In the Dong-A newspaper corpus on which we 
carry out our experiments in Section 4, Korean 
function morphemes occupy 41.3% of all Korean 
morphemes. Verb endings consist of 40.3% of all 
Korean function words, and the average number 
of function morphemes inflected by a verb or an 
adjective is 1.94 while that of other content mor-
phemes is only 0.7.  
These statistics indicate that the morphological 
form of Korean verbal phrases (Korean verbs) 1 
are significantly complex. A verbal phrase in 
Korean consists of a series of verb affixes along 
with a verb stem. A verb stem cannot be used by 
itself but should take at least one affix to form a 
verbal complex. Verb affixes in Korean are or-
dered in a relative sequence within a verbal com-
plex (Lee, 1991) and express different modality 
information2: tense, aspect, mood, negation, and 
voice (Figure 1). These five grammatical catego-
ries are the major constituents of modal expres-
sion in Korean. 
 
K1: ?(stem) +?_?(aspect prt.) +  ?(aspect prt.)
+ ?(tense prt.) + ?(mood prt.) 
E1: had been eating 
K2. ?(stem) + ?(passive prt.) + ?(aspect prt.) + 
?_ ?_?(modality prt.)  + ?(mood prt.) 
E2: might have been caught 
Figure 1. Verbal phrases in Korean. Bold-faced 
content morphemes followed by functional ones 
with ?+? symbols. Prt. is an acronym for particle. 
 
The modality of Korean is expressed inten-
sively by verb affixes. However, Chinese ex-
presses modality using discontinuous morphemes 
scattered throughout a sentence (Figure 2). Also, 
the prominence of grammatical categories ex-
pressing modality information is different from 
language to language, and correlations of such 
categories in a language are also different. The 
differences between the two languages lead to 
difficulties in alignment and cause linking obscu-
rities. 
 
C3: ??(thief)/??(might)/?(passive prt.)/??
(police)/?(catch)/?(aspect prt.)/? 
 
K3: ??(thief)+?   ??(police)+??   
?(catch)+?(passive prt.)+?(aspect prt.)+?_?_?
(modality prt.)+?(mood prt.)+./ 
E3: The thief might have been caught by the police.
Figure 2. Underlined morphemes are modality-
bearing morphemes in Chinese and Korean sen-
tences. Chinese words are separated by a ?/? 
symbol and Korean eojeols by a space.  
                                                 
1 ?Korean verbal phrase? or ?Korean verbs? in this paper 
refer to Korean predicates (verbs or adjectives) in a sentence.  
2  Modality system refers to five grammatical categories: 
tense, aspect, mood (modality & mood), negation, and voice. 
The definition of these categories is described in detail in 
(Li et al, 2005). 
191
We consider two issues for generating ade-
quate Korean verbal phrases. First is the correct 
position of verbal phrases, and the second is the 
generation of verb affixes which convey modali-
ty information. 
3 Chinese syntactic reordering rules 
In this section, we describe a set of manually 
constructed Chinese syntactic reordering rules. 
Chinese sentences are first parsed by Stanford 
PCFG parser which uses Penn Chinese Treebank 
as the training corpus (Levy and Manning, 2003). 
Penn Chinese Treebank adopts 23 tags for phras-
es (Appendix A). We identified three categories 
in Chinese that need to be reordered: verb phras-
es (VPs), preposition phrases (PPs), and modali-
ty-bearing words.  
3.1 Verb phrases 
Korean is a verb-final language, and verb phrase 
modifiers and complements occur in the pre-
verbal positions. However, in Chinese, verb 
phrase modifiers occur in the pre-verbal or post-
verbal positions, and complements mostly occur 
in post-verbal positions. 
We move the verb phrase modifiers and com-
plements located before the verbal heads to the 
post-verbal position as demonstrated in the fol-
lowing examples. A verbal head consists of a 
verb (including verb compound) and an aspect 
sequence (Xue and Xia, 2000). Therefore, aspect 
markers such as ?? (perfective prt.)?, ??
(durative prt.)?, ??(experiential prt.)? positioned 
immediately after a verb should remain in the 
relatively same position with the preceding verb. 
The third one in the example reordering rules 
shows this case. Mid-sentence punctuations are 
also considered when constructing the reordering 
rules. 
 
Examples of reordering rules of VPs3: 
 
VV0 NP1 ? NP1 VV0 
VV0 IP1 ? IP1 VV0 
VV0 AS1 NP2 ? NP2 VV0 AS1 
VV0 PU1 IP2 ? IP2 PU1 VV0 
 
Original parse tree: 
VP 
      PP (P ?) 
          NP (NN ??) 
      PP (P ?) 
                                                 
3 VV: common verb; AS: aspect marker; P: preposi-
tion; PU: punctuation; PN: pronoun; 
          NP (PN ??) 
      VP (VV ??) 
          NP (NN ??) 
 
Reordered parse tree: 
VP 
      PP (P ?) 
          NP (NN ??) 
      PP (P ?) 
          NP (PN ??) 
      NP (NN ??) 
VP (VV ??) 
 
3.2 Preposition phrases 
Chinese prepositions originate from verbs, and 
they preserve the characteristics of verbs. Chi-
nese prepositions are translated into Korean 
verbs, other content words, or particles. We only 
consider the Chinese prepositions that translate 
into verbs and other content words. We swap the 
prepositions with their objects as demonstrated in 
the following examples.  
 
Examples of reordering rules of PPs: 
 
Case 1: translate into Korean verbs 
P(?)0 NP1 ? NP1 P(?)0 
P(??)0 IP1 ? IP1 P(??)0 
P(??)0 LCP1 ? LCP1 P(??)0 
 
Case 2: translate into other content words 
P(??)0 IP1 ? IP1 P(??)0 
P(??)0 NP1 ? NP1 P(??)0 
 
Original parse tree: 
VP 
      PP (P ?) 
          NP (NN ??) 
      PP (P ?) 
          NP (PN ??) 
      VP (VV ??) 
          NP (NN ??) 
 
Reordered parse tree: 
VP 
  NP (NN ??) 
PP (P ?)  
      PP (P ?) 
          NP (PN ??) 
      VP (VV ??) 
          NP (NN ??) 
 
192
3.3 Modality-bearing words 
Verb affixes in Korean verbal phrases indicate 
modality information such as tense, aspect, mood, 
negation, and voice. The corresponding modality 
information is implicitly or explicitly expressed 
in Chinese. It is important to figure out what fea-
tures are used to represent modality information. 
Li et al (2008) describes in detail the features in 
Chinese that express modality information. 
However, since only lexical features can be reor-
dered, we consider explicit modality features 
only. 
Modality-bearing words are scattered over an 
entire sentence. We move them near their verbal 
heads because their correspondences in Korean 
sentences are always placed right after their 
verbs. 
When constructing reordering rules, we con-
sider temporal adverbs, auxiliary verbs, negation 
particles, and aspect particles only. The follow-
ing example sentences show the results of a few 
of our reordering rules for modality-bearing 
words. 
 
Examples of reordering rules of modality-
bearing words: 
 
Original parse tree: 
VP 
      ADVP (AD ?)                   ? Temporal adverb 
          PP (P ?) 
              LCP 
                  NP (NN ??) (NN ??) (NN ??) 
                  (LC?) 
          VP (VV ??) 
              NP (NN ??) 
 
Reordered parse tree: 
VP 
      PP (P ?) 
          LCP  
              NP (NN ??) (NN ??) (NN ??) 
              (LC ?) 
      ADVP (AD ?)  
VP (VV ??) 
          NP (NN ??) 
 
Original parse tree: 
VP (VV ?)                                ? Auxiliary verb 
       VP 
           PP (P ?) 
               LCP 
                   NP (NN ??) (NN ?) 
                   (LC ?) 
           VP (VV ??) 
Reordered parse tree: 
VP 
        PP (P ?) 
             LCP 
                  NP (NN ??) (NN ?) 
                  (LC ?) 
      VP (VV ?) 
VP (VV ??) 
 
Original parse tree: 
VP 
ADVP (AD ?)                ? Negation particle 
   VP (VV ??)                       ? Auxiliary verb 
   VP 
        PP (P ?) 
           NP (NN ???) (NN ??) 
             VP (VV ??) 
 
Reordered parse tree: 
VP 
     PP (P ?) 
         NP (NN ???) (NN ??) 
ADVP (AD ?) 
VP (VV ??) 
          VP (VV ??) 
 
Generally speaking, Chinese does not have 
grammatical forms for voice. Although, voice is 
also a grammatical category expressing modality 
information, we have left it out of the current 
phase of our experiment since voice detection is 
another research issue and reordering rules for 
voice are unavoidably complicated. 
4 Experiment 
Our baseline system is a popular phrase-based 
SMT system, Moses (Koehn et al, 2007), with 
5-gram SRILM language model (Stolcke, 2002), 
tuned with Minimum Error Training (Och, 2003). 
We adopt NIST (NIST, 2002) and BLEU (Papi-
neni et al, 2001) as our evaluation metrics.  
Chinese sentences in training and test corpora 
are first parsed and are applied a series of syntac-
tic reordering rules. To evaluate the contribution 
of the three categories of syntactic reordering 
rules, we perform the experiments applying each 
category independently. Experiments of various 
combinations are also carried out. 
4.1 Corpus profile 
We automatically collected and constructed a 
sentence-aligned parallel corpus from the online 
193
Dong-A newspaper 4 . Strictly speaking, it is a 
non-literally translated Korean-to-Chinese cor-
pus. The other corpus is provided by MSRA 
(Microsoft Research Asia). It is a Chinese-
Korean-English trilingual corpus of technical 
manuals and a literally translated corpus. 
Chinese sentences are segmented by Stanford 
Chinese word segmenter (Tseng et al, 2005), 
and parsed by Stanford Chinese parser (Levy and 
Manning, 2003). Korean sentences are seg-
mented into morphemes by an in-house morpho-
logical analyzer. 
The detailed corpus profiles are displayed in 
Table 1 and 2. The Dong-A newspaper corpus is 
much longer than the MSRA technical manual 
corpus. In Korean, we report the length of con-
tent and function words. 
 
 Training (99,226 sentences) 
Chinese Korean Content Function
# of words 2,692,474 1,859,105 1,277,756 
# of singletons 78,326 67,070 514
avg. sen. length 27.13 18.74 12.88
 Development (500 sentences) 
Chinese Korean Content Function
# of words 14,485 9,863 6,875
# of singletons 4,029 4,166 163
avg. sen. length 28.97 19.73 13.75
 Test (500 sentences) 
Chinese Korean Content Function
# of words 14,657 10,049 6,980
# of singletons 4,027 4,217 164
avg. sen. length 29.31 20.10 13.96
Table 1. Corpus profile of Dong-A newspaper. 
 
 Training (29,754 sentences) 
Chinese Korean Content Function
# of words 425,023  316,289 207,909
# of singletons 5,746 4,689 197
avg. sen. length 14.29 10.63 6.99
 Development (500 sentences) 
Chinese Korean Content Function
# of words 6,380 4,853 3,214
# of singletons 1,174 975 93
avg. sen. length 12.76 9.71 6.43
 Test (500 sentences) 
Chinese Korean Content Function
                                                 
4 http://www.donga.com/news/ (Korean) and 
http://chinese.donga.com/gb/index.html (Chinese) 
# of words 7,451 5,336 3,548
# of singletons 1,182 964 99
avg. sen. length 14.90 10.67 7.10
Table 2. Corpus profile of MSRA technical ma-
nual. 
4.2 Result and discussion 
The experimental results are displayed in Table 3 
and 4. Besides assessing the effectiveness of 
each reordering category, we test various combi-
nations of the three categories. 
 
Method NIST BLEU 
Baseline 5.7801 20.49 
Reorder.VP 5.8402  22.12 (+7.96%) 
Reorder.PP 5.7773  20.10 (-1.90%) 
Reorder.Modality 5.7682  20.93 (+2.15%) 
Reorder.VP+PP 5.8176  21.96 (+7.17%) 
Reorder.VP+Modality 5.9198  22.24 (+8.54%) 
Reorder.All 5.9361 22.40 (+9.32%) 
Table 3. Experimental results on the Dong-A 
newspaper corpus. 
 
Method NIST BLEU 
Baseline 7.2596 44.03 
Reorder.VP 7.2238  44.57 (+1.23%) 
Reorder.PP 7.2793  44.22 (+0.43%) 
Reorder.Modality 7.3110  44.25 (+0.50%) 
Reorder.VP+PP 7.3401  45.28 (+2.84%) 
Reorder.VP+Modality 7.4246  46.42 (+5.43%) 
Reorder.All 7.3849  46.33 (+5.22%) 
Table 4. Experimental results on the MSRA 
technical manual corpus. 
 
From the experimental result of the Dong-A 
newspaper corpus, we find that the most effec-
tive category is the reordering rules of VPs. 
When the VP reordering rules are combined with 
the modality ones, the performance is even better. 
The gain of BLEU is not significant, but the gain 
of NIST is significant from 5.8402 to 5.9198. 
The PP reordering rules do not contribute to the 
performance when they are singly applied. How-
ever, when combined with the other two catego-
ries, they contribute to the performance. The best 
performance is achieved when all three catego-
ries? reordering rules are applied and the relative 
improvement is +9.32% over the baseline system. 
In the MSRA corpus, the performance of vari-
ous combinations of the three categories is better 
than those of the individual categories. The PP 
category shows improvement when it is com-
bined with the VP category. The combination of 
VP and modality category improves the perfor-
mance by +5.43% over the baseline. 
194
These results agree with our expectations: re-
solving the word order and modality expression 
differences of verbal phrases between Chinese 
and Korean is an effective approach.  
4.3 Error Analysis 
We adopt an error analysis method proposed by 
Vilar et al (2006). They presented a framework 
for classifying error types of SMT systems. (Ap-
pendix B.)  
Since our approach focuses on verbal phrase 
differences between Chinese and Korean, we 
carry out the error analysis only on the verbal 
heads. Three types of errors are considered:  
word order, missing words, and incorrect words. 
We further classify the incorrect words category 
into two sub-categories: wrong lexical 
choice/extra word, and incorrect form of modali-
ty information. 50 sentences are selected from 
each test corpus on which to perform the error 
analysis. For each corpus, we choose the best 
system: Reorder.All for the Dong-A corpus and 
Reorder.VP+modality for the MSRA corpus. 
 The most frequent error type is wrong word 
order in both corpora. When a verb without any 
modality information appears in a wrong position, 
we only count it as a wrong word order but not 
as a wrong modality. Therefore, the number of 
wrong modalities is not as frequent as it should 
be. 
Table 5 and 6 indicate that our proposed me-
thod helps improve the SMT system to reduce 
the number of error types related to verbal phras-
es. 
 
Error type Frequency Baseline Reorder.All
wrong word order  34 7
missing content word  18 5
wrong lexical choice/ 
extra word  6 1
wrong modality  10 6
Table 5. Error analysis of the Dong-A newspaper 
corpus. 
 
Error type 
Frequency 
Baseline Reorder. VP+Modality
wrong word order 19 11
missing content word 4 2
wrong lexical choice/ 
extra word 8 3
wrong modality  11 6
Table 6. Error analysis of the MSRA technical 
manual corpus. 
 
5 Conclusion and future work 
In this paper, we proposed a Chinese syntactic 
reordering more suitable to adequately generate 
Korean verbal phrases in Chinese-to-Korean 
SMT. Specifically, we considered reordering 
rules targeting Chinese VPs, PPs, and modality-
bearing words that are closely related to Korean 
verbal phrases. 
Through a contrastive analysis between the 
two languages, we first showed the difficulty of 
generating Korean verbal phrases when translat-
ing from a morphologically poor language, Chi-
nese. Then, we proposed a set of syntactic reor-
dering rules to reorder Chinese sentences into a 
more Korean like word order. 
We conducted several experiments to assess 
the contributions of our method. The reordering 
of VPs is the most effective, and improves the 
performance even more when combined with the 
reordering rules of modality-bearing words. Ap-
plied to the Dong-A newspaper corpus and the 
MSRA technical manual corpus, our proposed 
approach improved the baseline systems by 
9.32% and 5.43%, respectively. We also per-
formed error analysis with a focus on verbal 
phrases. Our approach effectively decreased the 
size of all errors.  
There remain several issues as possible future 
work. We only considered the explicit modality 
features and relocated them near the verbal heads. 
In the future, we may improve our system by 
extracting implicit modality features. 
In addition to generating verbal phrases, there 
is the more general issue of generating complex 
morphology in SMT systems targeting Korean, 
such as generating Korean case markers. There 
are several previous studies on this topic (Min-
kov et al, 2007; Toutanova et al, 2008). This 
issue will also be the focus of our future work in 
both the phrase- and syntax-based SMT frame-
works. 
 
Acknowledgments 
This work was supported in part by MKE & II-
TA through the IT Leading R&D Support Project 
and also in part by the BK 21 Project in 2009. 
References  
Charles N. Li, and Sandra A. Thompson 1996. Man-
darin Chinese: A functional reference grammar, 
University of California Press, USA. 
David Vilar, Jia Xu, Luis Fernando D?Haro, and 
Hermann Ney. 2006. Error Analysis of Statistical 
195
Machine Translation Output. In Proceedings of 
LREC. 
Einat Minkov, Kristina Toutanova, and Hisami Suzu-
ki. 2007. Generating Complex Morphology for 
Machine Translation. In Proceedings of ACL.  
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned re-
write patterns. In Proceedings of COLING. 
Huihsin Tseng, Pichuan Chang, Galen Andrew, Da-
niel Jurafsky and Christopher Manning. 2005. A 
Conditional Random Field Word Segmenter. In 
Fourth SIGHAN Workshop on Chinese Language 
Processing.  
HyoSang Lee 1991. Tense, aspect, and modality: A 
discourse-pragmatic analysis of verbal affixes in 
Korean from a typological perspective, PhD thesis, 
Univ. of California, Los Angeles. 
Jin-Ji Li, Ji-Eun Roh, Dong-Il Kimand Jong-Hyeok 
Lee. 2005. Contrastive Analysis and Feature Selec-
tion for Korean Modal Expression in Chinese-
Korean Machine Translation System. International 
Journal of Computer Processing of Oriental Lan-
guages, 18(3), 227--242. 
 Jin-Ji Li, Dong-Il Kim and Jong-Hyeok Lee. 2008. 
Annotation Guidelines for Chinese-Korean Word 
Alignment. In Proceedings of LREC. 
Kristina Toutanova, Hisami Suzuki, and Achim 
Puopp. 2008. Applying Morphology Generation 
Models to Machine Translation. In Proceedings of 
ACL. 
Nianwen Xue, and Fei Xia. 2000. The bracketing 
guidelines for the Penn Chinese Treebank (3.0). 
IRCS technical report, University of Pennsylvania. 
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha 
Palmer. 2005. The Penn Chinese Treebank: Phrase 
structure annotation of a large corpus. Natural 
Language Engineering, 11(2):207?238. 
Michael Collins, Philipp Koehn, and Ivona 
Ku?cerov?a. 2005. Clause restructuring for statis-
tical machine translation. In Proceedings of ACL, 
pages 531?540. 
NIST. 2002. Automatic evaluation of machine trans-
lation quality using n-gram co-occurrence statis-
tics. 
Och, F. J. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of 
ACL. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris-
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constrantin, and Evan Herbst. 2007. Moses: Open 
source toolkit for statistical machine translation. In 
Proceedings of ACL, Demonstration Session.  
Roger Levy and Christopher D. Manning. 2003. Is it 
harder to parse Chinese, or the Chinese Treebank? 
In Proceedings of ACL. 
Stolcke, A. 2002. SRILM - an extensible language 
modeling toolkit. In Proceedings of ICSLP, 2:901-
904. 
Appendix A. Tag for phrases in Penn 
Chinese Treebank. 
ADJP adjective phrase 
ADVP adverbial phrase headed by AD (adverb)
CLP  classifier phrase 
CP   clause headed by C (complementizer) 
DNP  phrase formed by ?XP+DEG? 
DP   determiner phrase 
DVP  phrase formed by ?XP+DEV? 
FRAG fragment 
IP   simple clause headed by I (INFL) 
LCP  phrase formed by ?XP+LC? 
LST  list marker 
NP   noun phrase 
PP   preposition phrase 
PRN  parenthetical 
QP   quantifier phrase 
UCP  unidentical coordination phrase 
VP   verb phrase 
Appendix B. Classification of translation 
errors proposed by Vilar et al (2006). 
 
196
