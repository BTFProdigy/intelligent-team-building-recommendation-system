Proceedings of the 8th International Conference on Computational Semantics, pages 73?89,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
A Conceptual and Operational Model for
Procedural Texts and its Use in Textual Integration
Isabelle Dautriche
(1,2)
, Patrick Saint-Dizier
(1)
(1) IRIT-CNRS, Toulouse, France
(2) INSA, Toulouse France
isabelle.dautriche@gmail.com, stdizier@irit.fr
Abstract
In this paper, we propose a conceptual and operational model for
procedural texts, within Action theory. We propose some elements to
model the complexity and expliciteness of instructions, showing, how,
for a given execution of the procedure, the success of the goal can be
measured. Next, we show how a procedure can be enriched via textual
integration in order to improve the success rate of the goal.
1 Introduction
Procedural texts consist of a sequence of instructions, designed with some
accuracy in order to reach a goal (e.g. assemble a computer). Procedural
texts may also include subgoals. Goals and subgoals are most of the time
realized by means of titles and subtitles. The user must carefully follow step
by step the given instructions in order to reach the goal. A procedure is in
general a finite sequence of subactions, terminal elements being instructions.
Instruction execution may be conditional, associated with preferences or
advices. Instructions may contain continuous processes, loops and may be
organized as a cluster in a loop, till some result is reached. This is presented
in (Delpech et al 2008). Not surprisingly, procedures have a structure that
resembles what is usually found in imperative programming.
The main goal of our project is to analyze the structure of procedural
texts in order to efficiently and accurately respond to How-to? questions.
This means identifying titles (which basically convey the main goals of a
procedure in a number of domains), sequences of instructions serving these
goals, and a number of additional structures such as prerequisites, warnings,
advices, illustrations, etc. (Takechi et al. 2003), (Adam, 2001). A response
73
to an How-to question is then the well-formed text portion within the scope
of the title that matches the question (Delpech et al 2007, 2008). Quite
often, we have several texts candidates for the response. It is often difficult
to make a choice a priori, therefore, we offer the user two approaches which
may complement each other:
? the possibility to get a few relevant documents, undergoing a few
constraints (e.g. good typography), associated with inter-document
navigation tools based on contents (experiments related to user nav-
igation strategies have been carried out using the Navitexte software
(http://panini.u-paris10.fr/jlm/?Start:projets:NaviTexte)),
? the selection of a main document and the integration into this docu-
ment of additional information such as missing manners, instruments,
durations, etc., coming from the other candidate procedures, possibly
in an interactive way, upon user?s request. Obviously, this is very chal-
lenging, and it may be necessary e.g. to indicate integration situations
to the user in case some incoherences arise.
In this paper, besides developing the foundational aspects of procedural
texts within the perspective of Action Theory (originating from (Davidson
80), see also: http://actiontheory.free.fr/), we develop a first experiment we
carried out for textual integration. Text fusion or integration is in general
extremely difficult, however, procedural texts being strongly restricted in
form and contents make such an experiment possible and of much interest.
What is presented here is basically a feasability study, identifying difficulties,
deadlocks, and the linguistic and reasoning resources which may be needed.
Evaluation results are preliminary and therefore indicative.
2 Dealing with Procedural Texts
In our perspective, procedural texts range from apparently simple cooking
recipes to large maintenance manuals. They also include documents as di-
verse as teaching texts, medical notices, social behavior recommendations,
directions for use, assembly notices, do-it-yourself notices, itinerary guides,
advice texts, savoir-faire guides etc. (Aouladomar et al, 2005). Procedural
texts follow a number of structural criteria, whose realization may depend
on the author?s writing abilities, on the target user, and on traditions asso-
ciated with a given domain. Procedural texts can be regulatory, procedural,
programmatory, prescriptive or injunctive.
74
We have developed a quite detailed analysis of procedural texts from
a manual corpus analysis, identifying their main basic components as well
as their global structure. For that purpose, we have defined two levels: a
segmentation level that basically identifies structures considered as terminal
structures (titles, instructions, prerequisites, connectors, etc.) and a gram-
mar level that binds these terminal structures to give a global structure to
procedural texts (Delpech et al 2008) (Fontan et al 2008). This structure
is textual and dedicated only to elements relevant to procedurality. Our
study was carried out on a development corpus of 1700 French texts taken
from the Web from most of the areas cited above, and extracted from our
more global corpus of 8000 texts.
Procedural texts are complex structures, they often exhibit a quite com-
plex rational (the instructions) and ?irrational? structure which is mainly
composed of advices, conditions, preferences, evaluations, user stimulations,
etc. They form what we call the explanation structure, which motivates and
justifies the goal-instructions structure, viewed as the backbone of procedu-
ral texts. A number of these elements are forms of argumentation, they ap-
pear to be very useful, sometimes as important as instructions: they provide
a strong and essential internal cohesion and coherence to procedural texts.
They also indicate, among other things, the consequences on the target goal
of an incorrect or incomplete execution of the associated instruction.
A conceptual and operational semantics for procedural texts can be de-
rived from these considerations, where, in fact, what is essential is to model
the complexity of a procedure, the risks, and a series of indicators of the
difficulty to reach the goal.
As an illustration, consider the cooking receipe extract in Figure 1 (our
work is realized on French, with English glosses), describing the prepara-
tion of a pizza, arguments are in italics. As the reader can note, warnings
and advices operate at various levels, some are general recommendations,
while others clearly indicate the risks of a failure, complete or partial, if the
instruction is not correctly carried out.
3 A Formal Model for Procedural Texts within
Action Theory
Action Theory is a general framework that deals with various facets of ac-
tions, including e.g. planning issues, modelling causality and psychological
investigations related to the notion of goal. Action theory seems to be a
convenient framework to give a conceptual semantics to procedurality, and
75
HOW TO MAKE PIZZA
Making Pizza Dough
Pour the warm water in a large mixing bowl. Add the sugar and stir until
dissolved otherwise it may burn when baking.
Add the yeast and gently stir the mixture until the yeast is dissolved.
Let the mixture sit for 10 minutes to allow the yeast to become ?active?, do not
wait for more otherwise its effect will decrease.
The mixture should become foamy at the surface and appear cloudy, and will
begin to release its familiar, ?yeasty? aroma.
Add the salt and olive oil and stir again to combine the ingredients, make sure
they are well mixed otherwise the oil will burn.
Add 1 cup of flour to the mixture and whisk in until fully dissolved, otherwise
you?ll get lumps later .
Add the second cup of flour and whisk it in until the mixture is smooth, alter-
natively, you can add the flour gradually.
You may need to add a dusting of flour from time to time to reduce the stickiness
of the dough as you work it with your hands. Be patient, folding the dough
mixture in on itself, over and over again.
Figure 1: Arguments in a procedure
to one of its main language realizations: procedural texts.
Since argumentation is a major feature of procedural texts it will play a
major role in our modelling. We then develop the main facets of a conceptual
semantics for procedural texts, with a focus on the measure of the success in
reaching a goal, based on arguments. This leads us to investigate measures
of instructions complexity and expliciteness. The second part of the paper is
an experiment showing how complexity and expliciteness can be dealt with
in order to guarantee a higher success rate to the procedure using textual
integration.
3.1 Structure of Arguments
Roughly, argumentation is a process that allows speakers to construct state-
ments for or against another statement called the conclusion. These former
statements are called supports. The general form of an argument is : Con-
clusion ?because? Support (noted as C because S). In natural language,
conclusions usually appear before the support, but they may also appear
after, to stress the support. A conclusion may receive several supports, pos-
sibly of different natures (advices and warnings): don?t add natural fertilizer,
76
this may attract insects, which will damage your young plants. Arguments
may be more or less strong, they bear in general a certain weight, mostly
induced from the words they contain or from their syntactic construction
(Anscombre et al 1981), (Moeschler 1985), (Amgoud et al. 2001). In nat-
ural contexts, this weight is somewhat vague and therefore quite difficult to
precisely evaluate.
In procedural texts, arguments are associated with instructions or groups
of related instructions (that we call instructional compounds). Contrary to
works in artificial intelligence, arguments appear here in isolation, they do
not attack each other, they simply contribute to the success of the goal the
procedure encodes.
3.2 Modelling Procedural Texts
Let G be a goal which can be reached by the sequence of instructions
A
i
, i ? [1, n]. We will not go here into the details of their exact tem-
poral structure, which is often basically sequential. A correct execution of
all the instructions guarantees the success of the goal G.
Let us then assume that any A
i
is associated with a support S
i
(possibly
not realized). The absence of an explicit support does not mean that the
instruction is not important, but simply that failing to realize it has obvi-
ous consequences on the goal G. A procedure is then a temporally ordered
sequence of A
i
S
i
.
Let us next associate with every instruction A
i
S
i
(or, instructional com-
pound, no distinction is made in this paper) a vector (p
i
, g
i
, d
i
, t
i
) where:
? p
i
is the associated penalty when the user, for a given execution, partly
or fully fails to realize A
i
,
? g
i
is the gain associated with A
i
: there is a gain only in case where A
i
is an advice, aimed at improving the quality of G,
? d
i
is the intrinsic difficulty of the instruction A
i
,
? t
i
is the degree of explicitness of A
i
.
These parameters are obviously quite difficult to elaborate. This is devel-
oped in the next subsections.
Let us concentrate here on the realization of the goal G, and on measuring
its success, given a certain execution U by a user. It is important to note
that failing to properly realize an instruction does not necessarily mean that
the goal cannot be reached, but the result of the procedure won?t be as nice
77
as it could have been. In the natural language expressions of conclusions
(the A
j
) as well as of supports, there are many modals or classes of verbs
(like risk verbs) that indeed modulate those consequences on G, contrast for
example:
use professional products to clean your leathers, they will give them a brighter
aspect. with:
carefully plug in your mother card vertically, otherwise you will most likely
damage its connectors.
In the latter case, the goal ?mounting your own PC? is likely to fail if the
instruction is not correctly realized (the instruction at stake will be assigned
a high penalty), whereas in the former, the goal ?cleaning your leathers? will
just be less successful in a certain sense, where there is an explicit gain
associated if professional products are used. These parameters and their
language realizations are studied in (Fontan et al 2008).
Given a certain realization by a user, the success of a goal G can be
evaluated by the sum of the gains on the one hand, and the sum of the
penalties on the other hand. Gains and penalties a priori do not compen-
sate each other: they operate at different levels. Since any A
i
is in fact
realized successfully to a certain degree by the user for a given an execution
U, gains and penalties need to be weighted for that execution, i.e. paired
with a success measure for each instruction. Let us introduce respectively
?
i
and ?
i
for gains and penalties on A
i
, each of these weights being included
in [0, 1]. Then, for a given execution of the goal G, we have:
gain(G) =
?
n
i=1
g
i
? ?
i
, penalty(G) =
?
n
i=1
p
i
? ?
i
3.3 Elaborating penalties and gains
A penalty is equal to 0 when the action is correctly realized. Otherwise it
has a positive value. Gains are also positive values and are only associated
with advices (optional instructions), therefore, gains are positive only when
advice are executed, otherwise they are null.
The difficulty is to elaborate a model that can describe in a simple way
the form penalties may have. For that purpose, and as a first experimenta-
tion, we introduce a three place vector representing three types of execution
levels, to which penalty costs can be associated. The vector represents penal-
ities according to three levels of success for an instructions: (good execution,
average, failure). To make it representative of the various importance of the
instructions in a procedure, we introduce four levels of importance:
78
- Essential action, with vector: (0, N, infinite),
- Important action : (0, 1, N),
- Useful action : (0,0,1),
- Optionnal action : (0,0,0).
The value of N remains a parameter that needs to be adjusted experimen-
tally. In case of an essential action, failure entails the goal failure, since the
value of the penalty is infinite. The four levels of importance of actions is a
parameter quite frequently encountered in experimental psychology, didac-
tics and cognitive ergonomy when users have to evaluate the importance of
a task, e.g. in emergency stituations.
For each instruction, the level of importance can be estimated from the
terms in the instruction (illocutionary force captured in warnings) or be
specified by the author of the text, e.g. via appropriate devices like icons,
which abound in large public procedural texts.
We can have a similar characterization for gains:
- Important advice: (0, 1, M),
- Useful if done completely: (0, 0, 1),
- No advice (0, 0, 0).
The value of N needs also to be experimentally elaborated.
3.4 Measuring the intrinsic difficulty rate d of an instruction
It is of much interest to be able to measure the inherent complexity or
difficulty of an instruction. This notion obviously depends on the reader,
and may be somewhat dependent on the other actions around it. Neverthe-
less, we think that some features introduce in any situation some inherent
difficulties. We give here some elements found in procedures identified as
introducing some complexity, independently of the domain at stake. Those
elements are essentially structures like verbs, PPs and adverbs.
Considering that procedural texts must limit as much as possible the
distance between the text and the action (they are oriented towards action,
not inferencing), we have identified elements inducing complexity by asking
users to indicate and possibly comment every indication in instructions for
which they had to make an elaboration (imagine a gesture, identify a tool
among a set of closely related tools, evaluate a manner (slowly, cautiously),
etc.). The protocol is simple and informal, but nevertheless gives interesting
indications for parameters inducing complexity.
The most frequently encountered parameters are, informally:
? presence of ?complex? manners (e. g. very slowly), by complex we mean
79
either a manner which is inherently difficult to realize or a manner
reinforced by an adverb of intensity,
? technical complexity of the verb or the verb compound used: if most
instructions include a verb which is quite simple, some exhibit quite
technical verbs, metaphorical uses, or verbs applied to unexpected
situations, for which an elaboration is needed. This is for example
relatively frequent in cooking (with specialised verbs like ?re?server?
which have a different meaning than the standard one), or in do-it-
yourself texts written by technicians of the domain.
? duration of execution as specified in the instruction (the longer the
more difficult),
? synchronization between actions, in particular in instructional com-
pounds,
? uncommon tools, or uncommon uses of basic tools (open the box with
a sharp knife),
? presence of evaluation statements or resulting states, for example to
indicate the termination of the action (as soon as the sauce turns brown
add flour).
At the moment, we have lists of a priori complex verbs and manners for each
domain collected from our development corpus. The task is to organize these
terms by increasing complexity, via the design of adequate scales. Manners
are relatively limited and have been ordered manually quite easily. For larger
groups of terms, we can also simply form groups of terms classified according
to complexity, without introducing scales and accurate order analysis.
Obviously, these observations allow us to introduce a very preliminary
measure of complexity, since more empirical measures need to be realized.
To be able to have an indicative evaluation, each of the points above counts
for 1, independently of its importance or strength in the text. Complexity c
therefore ranges from 0 to 6. The complexity rate d
i
of instruction i is c/6
to keep it in the [0,1] range.
It is important to note that the higher the difficulty d
i
is, the more
risky the instruction is in terms of failure. Since it is not in general easy
to decompose a difficult action into several simpler ones, a strategy to limit
risks is to enrich a difficult instruction as much as possible so that all the
details are given: this can be measured by the expliciteness criteria.
80
3.5 Measuring the expliciteness rate t of an instruction
Expliciteness characterizes the degree of accuracy of an instruction. Several
marks, independently of the domain, contribute to making more explicit an
instruction:
? when appropriate: existence of means or instruments,
? pronominal references as minimal as possible, and predicate argument
constructions as comprehensive as possible,
? length of action explicit when appropriate (stir during 10 minutes),
? list of items to consider as explicit and low level as possible (mix the
flour with the sugar, eggs and oil),
? presence of an argument, advice or warning,
? presence of some help elements like images, diagrams, or elaborations,
variants, etc.
Those criteria may be dependent on the domain, for example length of
an action is very relevant in cooking, somewhat in do-it-yourself, and much
less in the society domain. Similarly as for d, each item counts for 1 at the
moment, expliciteness e therefore ranges from 0 to 6. The expliciteness rate
is t
i
= e/6 to keep it in the [0,1] range.
Note also that the higher t
i
is, the more chances the instruction has to
succeed since it is very explicit and has a lot of details.
Now, if we consider the product d
i
? (1? t
i
), the more it tends towards
1, the higher the risk is for the action to fail. Therefore, when d
i
is high, it
is also necessary that t
i
is high to compensate the difficulty. Given that d
i
remains unchanged (if the instruction cannot be simplified), the strategy is
then to increase t
i
as much as possible.
4 Enhancing expliciteness: towards procedure in-
tegration
An approach to enhancing t is to add information to those instructions which
are judged difficult, wherever possible. This technique is called document
integration. It consists in considering a ?reference? document that is gradu-
ally enriched from information contained in other closely related documents
(i.e. procedures with the same goal). A difficulty is to keep the coherence
81
and cohesion of the document. Information may be added gradually, by in-
struction or by theme (e.g. instrument, duration), over the whole procedure
depending e.g. on the user?s needs or requirements.
Integrating information into an already existing document is a very diffi-
cult task. The work presented here is essentially a feasability study, applied
to a very restricted type of text: procedural texts, where the text can be
segmented into small items (instructions), which have a quite homogeneous
and simple style. This allows us to evaluate feasability, difficulties, needs
and to propose some elements of a method. We view document integra-
tion as a kind of inference mechanism that includes several aspects, among
which:
? evaluation of the relevance of adding or revising a certain type of
information, w.r.t. expliciteness requirements, e.g. based on Grice?s
maxims,
? maintenance of the coherence of the text, while keeping integration a
monotonic process.
Besides being basically a monotonic process, integration is also modular
since it can operate by themes, possibly following different strategies.
Two main types of structures may be integrated: (1) additional infor-
mation within an instruction or (2) additional instructions between two in-
structions. We will concentrate here on the first, which is probably the less
complex. By adding information, we mean in particular adding a role not
specified so far (e.g. instrument, duration) or updating an already existing
data (e.g. cheese + mozarella ? ?cheese, e.g. mozarella?, the symbol ?+?
being in this paper the integration operator).
In this section, we focus on texts which are prototypically procedural
such as cooking receipes, do-it-yourself, gardening, etc... We propose some
criteria to select a reference procedure among a choice of procedures related
to the same goal. Then we propose criteria to align instructions judged to be
closely related, based on a semantic tagging of instructions. We propose a
simple form of similarity ranking and, finally, show how information can be
integrated into the reference document, generating well-formed instructions.
4.1 A task-oriented tagging of instruction contents
Let us first introduce the tags we consider, inspired from thematic roles, but
which may apply to a variety of syntactic constructs, besides NPs and PPs,
such as adverbs or adjectives. These roles are basically action-oriented, they
82
form the prototypical pattern one may associate with an instruction. We
defined them from thematic roles and stabilized their definition and scope
from a number observations over manual annotations. We have the following
main roles: Themes: are basically ingredients, tools or objects on which an
action is realized (preheat the oven) or shapes (form a ball of dough). They
can also be persons in the social relation domain.
Manner: indicates how the action is carried out, it does not modify the
theme, but indicates the way the instruction must be realized (serrez la
poigne d?un demi tour., turn the handle of half a tour).
Means: identifies the entity via which the action is realized, this entity is in
general an element that interacts with the theme in some way, e.g. to form
a whole with it (arroser les endives avec jus, put sauce on the endives).
Instrument: the object used to realize an action, controlled by the agent,
and that participates at various degrees to the sucess of the action.
Goal: refers to the abstract or concrete objective towards which the action
is directed.
Result: refers to the expression that characterizes the consequence of the
action (make notes of your expenses to be able to evaluate how much you
spend over a week).
Condition: expression that indicates the constraints under which the action
can be realized or should be considered.
Localization: source, fixed or destination applied to time or space, possibly
to abstract entities.
We also observed a number of rhetorical relations within the instruction
(or instructional compound), but this is not dealt with at the moment. For
example, we will not integrate advices or elaborations.
In terms of parsing and tagging, we use the last version of Cordial
(http://www.synapse-fr.com/correcteur orthographe grammaire.htm), that
produces a parse which is appropriate for our needs: constituents are cor-
rectly identified and are paired with semantic annotations that we use to
assign roles, in conjunction with the lexicon of the predicates of the domain
that indicates the roles of arguments with their semantic type. Adjuncts are
labelled on the basis of the preposition that introduces them (via our Prep-
Net database) and the semantic type given by Cordial. For cooking receipe
and gardening texts the accuracy is at the moment about 72% which is a
moderately good result. Most errors come from incorrect style in the proce-
dure, and lack of domain specific descriptions for prepositions introducing
adjuncts.
83
4.2 Selection of the reference text
There are different strategies for selecting the text, called the reference text
(noted as R), that will be enriched from other procedural texts dealing with
the same procedure. We assume at this stage that we have texts dealing
exactly with the same procedure, possibly with minor variants. The text
with the largest number of instructions is selected as the reference. The idea
behind is that it is probably the most detailed one (but the correlation is
not absolute), and therefore also the simplest one since, for a given task, its
decomposition level into instructions is probably the highest. When we have
several procedures with approximately the same number of instructions, the
decision may be more difficult to make. Two criteria are considered: the
quality of the typography, which is an important feature for users, and
secondly, the origin (profesionnal site prefered to a blog).
4.3 Aligning instructions
Given a reference text (noted as R) and another procedure (noted as E)
used to enrich the former, the first challenge is to align instructions dealing
exactly with the same action. This is not necessarily a one to one relation:
several instructions in E may correspond to a single one in R or vice versa.
Next, instructions in the two texts may be organized differently: we observed
slightly different temporal organizations (in particular for minor operations,
e.g.: add salt and pepper which may appear at several places).
Let us concentrate here on simple situations where two instructions are
comparable. Our starting point are instructions tagged as indicated above.
Let A
R,i
be such an instruction in R and A
E,j
such an instruction from E.
The procedure is roughly the following:
(1) aligning verbs: a first parameter, VA, encodes the alignement quality
of the main verb of the two instructions. The situations are the following:
(1) identical, (2) quasi-synonym, as stated in our domain verb ontology,
(3) related via lexical inference (arroser (Theme: la garniture) (manner:
de sauce) versus verser (theme: la sauce) (fixed-position: sur la garniture),
gloss: baste/pour sauce on the garnish), (4) support construction synonym
of the other construction (mettre un torchon sur / couvrir, put a dish towel
on /cover), and (5) different. A mark is associated with each of these levels,
tentatively 4 for (1), 2 for (2) to (4) which are of a similar complexity and
0 for (5).
(2) aligning and comparing arguments: arguments tagged by the same
role (or those shown to be identical via lexical inference) are considered.
84
The objects included into those common roles must be closely related: this
is evaluated via a conceptual metrics (that basically considers sisters and
immediate ancestors at this stage) applied on the domain ontology, cooking
in our experiment. Success or failure is measured via a second parameter AA.
Furthemore, roles are distributed into 3 categories, the first one having the
highest weight. For the cooking domain, we have: (1) theme and temporal
localizations (duration or position), (2) spatial localization (fixed, source,
destination), (3) means, instrument. The other roles are quite difficult to
compare in the cooking domain and are so far kept in level 3.
For each tagged role which is comparable, a mark is given that takes into
account the level of the tag (1 to 3 above). The mark we get per aligned
argument (k) in instructions A
R,i
(the instruction from the referent text)
and A
E,j
(the one from the enriching text) is:
m
k
= f(A
R,i,k
, A
E,j,k
)
where f(A
R,i,k
, A
E,j,k
) is the metrics that assigns a similarity measure for
the argument k, as indicated above. The global rate associated with n
arguments successfully aligned between the two instructions is:
MA
i
=
?
n
i=1
?
i
?m
i
where ? is the weight that corresponds to the three categories of roles given
in the paragraph above. The global alignement rate is then:
rate(A
R,i
, A
E,j
) = ?? V A
i
+ ? ?MA
i
/n.
where ? and ? encode respectively the relative importance of verbs and
the arguments in the alignment. These parameters need to be adjusted
experimentally. At the moment, they are both equal to 1, meaning that
the verb and the arguments in total have the same weight in the alignment
rate. As can be noted, we do not take into account at this level the possible
negative effects of arguments not aligned, since they may contain, in fact,
useful information to add to the reference text.
All the instructions in R and E that can potentially align are compared,
independently of the temporal relations in the texts. As a result a matrix
containing all the alignement rates is produced. For each instruction in R,
the best rate, if it is above a certain minimal threshold, is considered and
the corresponding instruction in E is considered for information integration.
In the case where there are several instructions in E which can be aligned
with an instruction in R, then two factors are considered, in the following
priority order:
- whether these instructions also have good alignement rates with other
instructions in R, in which case, they may be selected for alignement with
one of these other instructions,
- whether they temporally coincide in the sequence of instructions in R and
85
E. It should however be noted that, although procedural texts have a strong,
but simple, temporal structure, we observed many permutations which make
this criteria less strong than it could have been.
At this stage, it is quite difficult to evaluate the accuracy of the aligne-
ment strategy since we do not cover all the cases, which may entail er-
rors. However, considering just the cases where we have a one-to-one aligne-
ment strategy, over cooking and gardening domains, tested on 14 reference-
enriching pairs of texts (with an average of 18 instructions for reference
texts), we get a recall of about 74% for a precision of about 90%. Those
results are quite encouraging, but we need to analyze the reasons of errors.
We observed more complex cases where alignement must be realized on
the basis of patterns, i.e. groups of instructions (possibly discontinuous) and
not single, isolated instructions. This is in particular the case with iterative
structures (stir again for one minute) which are repeated with variants. We
have not explored these situations, but it is clear that the detection of such
patterns is an important aspect.
4.4 Integrating information
Given two aligned instructions, the next step is to add information to the
reference document instruction A
R
. The verb in A
R
is kept, while the infor-
mation which is added is basically the tagged elements, which are arguments
or adjuncts. All the tagged elements in the enriching document instruction
(A
E
) are considered, even if A
R
already has an element based on the same
tag. Let us define an integration operator, noted as ?+?, composed of a left
context element, fragment of A
R
, and a right context, fragment of A
E
. The
result is an enriched instruction. This operation is monotonic: the amount
of information resulting from one or more applications of information inte-
gration is always increasing.
Adding information follows a strategy with two main aspects:
(1) parameterized adjunction: of missing elements (tags not present in
A
R
), according to two parameters:
- information explicity requested by the user (if this possibility is offered),
such as instruments,
- information a priori essential to a domain: for example adding tools,
manners and durations is important in cooking to make instructions more
explicit, and therefore less difficult to realize. Duration may be less relevant
in do-it yourself texts.
(2) integrating already existing information: in A
R
, where the three
following points are treated so far:
86
- disjunctive knowledge integration: add (soya milk + rice milk) ? add soya
or rice milk.
- exemplification: add (cheese + mozarella) ? add cheese, for example
mozarella.
- blocking ill-formed configurations: for example, in the case of incorpora-
tion: (butter the bread + with butter ? butter the bread) ?with butter? is
not included because the noun butter is already incorporated into the verb.
Let us now consider the surface realization of the resulting enriched in-
struction. For the first aspect above, this process starts from the tagged
elements so that a correct linguistic realization can be produced. We do not
just copy and paste the additional information to A
R
, but, based on the
annotated elements, we introduce insertion rules, that indicate where the
information must be inserted into the proposition. For example, [stir man-
ner: gently duration: for 4 minutes] + [theme: the sauce] must be realized
as:
[stir manner: gently theme: the sauce duration: for 4 minutes]
where the theme is inserted between the manner and the duration. The
resulting instruction is then:
Stir gently the sauce during 4 minutes.
This is realized by an insertion rule that says:
Verb (manner:X duration:Y) + (theme:Z) ? Verb manner:X theme:Z du-
ration:Y.
From the needs in terms of integration and from the observation of the
structure of instructions, we have defined at the moment 19 insertion rules.
These are now being generalized so that more generic situations can be
captured. In particular, we rephrased these rules in terms well-formedness
conditions essentially based on thematic linear precedence, a widely used
approach in language generation:
instrument < duration, meaning that any instrument always precedes a du-
ration expression. Similarly, we have:
theme < instrument, duration, localization.
In this approach, we keep the ?base? form used in most instructions since
there is almost never any syntactic alternation. These precedence specifica-
tions are very close to thematic role hierarchies, except that we deal here
also with a number of adjuncts (goals, manners, etc.) that may occupy a
large number of positions in the instruction. These adjuncts are however
subject to constraints, e.g.:
goal, condition < manner, means, result.
Finally, we apply the ?heavy-NP shift? movement when the argument is large,
for example a long theme will be moved at the end of the instruction.
87
At the visualization level, integrated information is underlined so that
the user knows that it has been imported, and may consider it with some
care or may get the original enriching instruction.
5 Conclusion
In this paper, we have presented a model for the structure of procedural texts
based on Action Theory, where the success of a goal is evaluated via several
parameters. We have in particular investigated the notions of complexity
and expliciteness of a procedure, and proposed an approach to improve the
success rate of a procedure via the integration of additional knowledge from
other procedures into a reference procedure. This latter point has entailed
the introduction of a semantic tagging, task-oriented, of instructions, the
development of an alignment strategy and a simple language realization
strategy.
Obviously what we propose here is basically exploratory, to evaluate
complexity and feasability. Document integration is a very hard task. Al-
thought restricting ourselves to very prototypical procedural texts does help
to propose solutions, it is clear that this work needs a lots of adjustments
and testing.
Acknowledgements This work was supported by the French ANR re-
search programme, under the TextCoop project.
References
[1] Adam, J.M., Types de Textes ou genres de Discours? Comment Classer les
Textes qui Disent De et Comment Faire, Langages, 141, pp. 10-27, 2001.
[2] Amgoud, L., Bonnefon, J.F., Prade, H., An Argumentation-based Approach to
Multiple Criteria Decision, in 8th European Conf. on Symbolic and Quantitative
Approaches to Reasoning with Uncertainty, ECSQARU?2005, Barcelona.
[3] Amgoud, L., Parsons, S., Maudet, N., Arguments, Dialogue, and Negotiation,
in: 14th European Conference on Artificial Intelligence, Berlin, 2001.
[4] Anscombre, J.-Cl. Ducrot, O., Interrogation et Argumentation, in Langue fran-
caise, no 52, L?interrogation, 5 - 22, 1981.
[5] Aouladomar, F., Towards Answering Procedural Questions. Dans : KRAQ?05
- IJCAI workshop, Edinburgh, F. Benamara, P. Saint-Dizier (Eds.), p. 24-36,
juillet 2005.
[6] Davidson, D., Essays on Actions and Events, Oxford: Oxford University Press,
1980.
88
[7] Delin, J., Hartley, A., Paris, C., Scott, D., Vander Linden, K., Expressing Pro-
cedural Relationships in Multilingual Instructions, Proc. 7th Int. Workshop on
Natural Language Generation, USA, 1994.
[8] Delpech, E., Saint-Dizier, P., A Two-Level Strategy for Parsing Procedural
Texts, proc. VSST07, Marrackech, 2007.
[9] Delpech, E., Saint-Dizier, P., Investigating the Structure of Procedural Texts
for Answering How-to Questions, LREC 2008, Marrakech.
[10] Fontan, L., Saint-Dizier, P., Analyzing the explanation structure of procedural
texts: dealing with Advices and Warnings, STEP, semantics in text processing,
College publications, J. Bos and R. delmonte (edts), 2008.
[11] Moschler, J., Argumentation et Conversation, Ele?ments pour une Analyse
Pragmatique du Discours, Hatier - Cre?dif, 1985.
[12] Takechi, M., Tokunaga, T., Matsumoto, Y., Tanaka, H., Feature Selection
in Categorizing Procedural Expressions, 6th Int?l Workshop on Information Re-
trieval with Asian Languages (IRAL2003), pp.49-56, 2003.
89
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2326?2334, Dublin, Ireland, August 23-29 2014.
Unsupervised Word Segmentation in Context
Gabriel Synnaeve and Isabelle Dautriche
LSCP, DEC
ENS Ulm, Paris, France
gabriel.synnaeve@gmail.com
isabelle.dautriche@gmail.com
Benjamin B
?
orschinger
Institut f?ur Computerlinguistik
Universit?at Heidelberg, Heidelberg, Germany.
benjamin.boerschinger@gmail.com
Mark Johnson
Department of Computer Science
Macquarie University, Sydney, Australia
mark.johnson@mq.edu.au
Emmanuel Dupoux
LSCP, DEC
EHESS, Paris, France
emmanuel.dupoux@gmail.com
Abstract
This paper extends existing word segmentation models to take non-linguistic context into ac-
count. It improves the token F-score of a top performing segmentation models by 2.5% on a 27k
utterances dataset. We posit that word segmentation is easier in-context because the learner is
not trying to access irrelevant lexical items. We use topics from a Latent Dirichlet Allocation
model as a proxy for ?activities? contexts, to label the Providence corpus. We present Adaptor
Grammar models that use these context labels, and we study their performance with and without
context annotations at test time.
1 Introduction and Previous Works
Segmentation of the speech stream into lexical units plays a central role in early language acquisition.
Because words are generally not uttered in isolation, one of the first task for infants learning a language is
to extract the words that make up the utterances they hear. Experimental research has shown that infants
are able to segment fluent speech into word-like units within the first year of life (Jusczyk and Aslin,
1995). How does this ability emerge? There is evidence that infants use a broad array of linguistic cues
to perform word segmentation (e.g., phonotactics (Jusczyk et al., 1993a), prosodic information (Jusczyk
et al., 1993b), statistical regularities (Saffran et al., 1996)). Past experimental and modeling research on
speech segmentation has mainly focused on linguistic cues, treating them as independent from other non-
linguistic cues naturally occurring in the child learning environment. Yet, language appears in context
and is constrained by the events occurring in the daily life of the child. For example, during an eating
event one is most likely to speak about food, while during a zoo-visit event, people are more likely
to talk about the animals they see. Activity contexts may provide a natural structure to speech that
would be readily be accessible to children. A recent study using dense recordings of a single child?s
language development (Roy et al., 2006) showed that words appearing in specific activity contexts are
learned faster (Roy et al., 2012). Relatedly, Johnson et al. (2010) showed that Adaptor Grammars (AGs)
performed better on a segmentation task when the model has access to a hand-annotated set of objects
present in the environment, that it can use to learn simultaneously word-object associations (see also
(Frank et al., 2009)). This supports the view that integrating multiple sources of information, linguistic
and non-linguistic, can improve learning.
Following this idea, we posit that information from the broader context in which a word has been
uttered may simplify the learning problem faced by the child. In particular, our hypothesis postulates
that speech segmentation is easier when using vocabularies that are related to a specific activity (eating,
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
2326
Table 1: Most probable words in the 7 final topics
egg book ball truck name color block
apple shape cat car school bear battery
banana square hat fire time crayon minute
milk circle tree piece today hair phone
butter triangle fish train day head puzzle
?food ?shapes ?playing ?toys ?time ?drawing ?garbage?
playing...), or place (kitchen, bedroom...). To evaluate this hypothesis, we applied topic modeling (Blei
et al., 2003) to automatically derive activity contexts on a corpus of child directed speech, the Provi-
dence corpus (Demuth et al., 2006), and tested the influence of such topics on a word segmentation task
extending the AG models used in (B?orschinger et al., 2012). We found that a model augmented with
the assumption that words are dependent upon the topic of the discourse (as a proxy for activity context)
performs better than the same model without access to the discourse topic. This suggests that the broader
context in which sentences are uttered may help in the word segmentation process, and could presumably
be used at various stages of language development.
The paper is structured as follows. Section 2 presents a novel approach to augment a corpus with
contextual annotations derived from topic models. Section 3 quickly explains Adaptor Grammars, the
framework that we used to express all our models. Section 4 presents all the models that were used in
the results. Section 5 describes the Providence corpus and the experimental setup. Section 6 shows our
quantitative and qualitative results. Finally, we discuss the implications for models of language learning.
2 Topics as Proxies for Contexts
Roy et al. (2012) found high correlations between human-annotated activity contexts and topics from a
latent Dirichlet allocation model (LDA) (Blei et al., 2003), thus showing that using topics as proxies for
contexts is a sound approach. Topic modeling infers a topic distribution for each ?document? (a bag of
words) in the corpus. Since ?documents? were not annotated in our corpus, we developed the following
3-step approach to automatically segment it into documents.
Firstly, for all the children of the Providence corpus, we used recording sessions as hard document
boundaries. We considered as a ?possible document? every contiguous sequence of sentences separated
by at least 10 seconds of silence, according to the orthographic transcript. We also identified ?possible
documents? using cues such as ?bye/hi?, indicating a change of participants. This segmentation resulted
in an over-segmented corpus (compared to context switches), yielding a total of 16, 742 documents.
Secondly, we used the gensim software (
?
Reh?u?rek and Sojka, 2010) to train a topic model (LDA)
1
,
and get the topic distributions for each of these documents. We used the symmetric KL-divergence to
measure the distance between two topic distributions before and after a ?possible document? boundary.
If the distance was above a threshold, we considered this boundary as a document boundary. Otherwise
we merged both ?possible documents? through this silence. The threshold was set empirically to dis-
criminate between two topic distributions that correspond to different activity contexts. After this step,
we assume that each of the resulting 8, 634 documents maps to an activity context.
Thirdly, we applied LDA again on this new segmentation to get the topic distribution, hence the activity
context, of each document. The number of topics is qualitatively chosen to correspond to the number of
main activity contexts (eating / playing / drawing / etc.) that occur in the Providence dataset (we used 7
topics), the resulting most topic specific words are shown in Table 1. Finally, for each document, we got
a distribution on topics, and we annotated the document with the most probable topic. By doing that, we
throw away graded information about the distribution on topics for each document. We could make use
of the full distribution, but here we are only interested in the most probable topic as a proxy for activity
context. We do not posit that the infants learn the topic models on linguistic cues while bootstrapping
speech and segmentation, but rather that they get activity context from non-linguistic cues.
1
We did LDA only on nouns (as they contain most of the semantics), weighted by TF-IDF.
2327
3 Adaptor Grammars
Adaptor Grammars (Johnson et al., 2007) are an extension of probabilistic context-free gram-
mars (PCFGs) that learn probability of entire subtrees as well as probabilities of rules. A PCFG
(N,W,R, S, ?) consists of a start symbol S, N and W disjoints sets of nonterminals and terminal sym-
bols respectively. R is a set of rules producing elements of N or W . Finally, ? is a set of distributions
over the rules R
X
,?X ? N (R
X
are the rules that expand X). An AG (N,W,R, S, ?, A,C) extends
the above PCFG with a subset (A ? N ) of adapted nonterminals, each of them (X ? A) having an
associated adaptor (C
X
? C). An AG defines a distribution over trees G
X
, ?X ? N ?W . If X /? A,
then G
X
is defined exactly as for a PCFG:
G
X
=
?
X?Y
1
...Y
n
?R
X
?
X?Y
1
...Y
n
TD
X
(G
Y
1
. . . G
Y
n
)
With TD
X
(G
1
. . . G
n
) the distribution over trees with root node X and each subtree t
i
? G
i
i.i.d. If
X ? A, then there is an additional indirection (composition) with the distribution H
X
:
G
X
=
?
X?Y
1
...Y
n
?R
X
?
X?Y
1
...Y
n
TD
X
(H
Y
1
. . . H
Y
n
)
H
X
? C
X
(G
X
)
We used C
X
adaptors following the Pitman-Yor process (PYP) (Perman et al., 1992; Teh, 2006) with
parameters a and b. The PYP generates (Zipfian) type frequencies that are similar to those that occur
in natural language (Goldwater et al., 2011). Metaphorically, if there are n customers and m tables, the
n+ 1th customer is assigned to table z
n+1
according to (?
k
is the Kronecker delta function):
z
n+1
|z
1
. . . z
n
?
ma+ b
n+ b
?
m+1
+
m
?
k=1
n
k
? a
n+ b
?
k
For an AG, this means that adapted non-terminals (X ? A) either expand to a previously generated
subtree (T (X)
k
) with probability proportional to how often it was visited (n
k
), or to a new subtree
(T (X)
m+1
) generated through the PCFG with probability proportional to ma+ b.
4 Word segmentation models
4.1 Unigram model
This most basic model just generates words as sequences of phonemes. AsWord is underlined, it means
it is adapted, and thus we learn a ?word unit -like? vocabulary. Phon is a nonterminal that expands to
all the phonemes of the language under consideration.
Sentence?Word
+
Word? Phon
+
where :
Word
+
?
{
Words?Word
Words?Word Words
4.2 Collocations and Syllabification
The baseline that we are using is commonly called the ?colloc-syll? model (Johnson, 2008; B?orschinger
et al., 2012) and is reported at 78% token F-score on the standard Brent version of the Bernstein-Ratner
corpus corpus (Johnson, 2008). It posits that sentences are collocations of words, and words are com-
posed of syllables. (Goldwater et al., 2009) showed how an assumption of independence between words
(a unigram model) led to under-segmentation. So, above the Word level, we take the collocations (co-
occurring sequences) of words into account.
2328
Furthermore, there is evidence that 8-month-old infants track syllable frequencies (Saffran et al.,
1996), and the ?colloc-syll? model can take that into account. Word splits into general syllables and
initial- or final- specific syllables. Syllables consist of onsets or codas (producing consonants), and nu-
clei (vowels). Onsets, nuclei and codas are adapted, thus allowing this model to memorize sequences or
consonants or sequences of vowels, dependent on their position in the word. Consonants and vowels are
the pre-terminals, their derivation is specified in the grammar into phonemes of the language.
Sentence? Colloc
+
Colloc?Word
+
Word? StructSyll
For notations purposes, all this syllabification is appended after Word by Word ? StructSyll.
All details about the collocations and syllabification grammars can be found in (Johnson, 2008). Here
is an example of a (good) parse of ?yuwanttusiD6bUk? with this model, skipping the StructSyll
derivations:
Sentence
Colloc
Word
bUk
Word
D6
Colloc
Word
si
Colloc
Word
tu
Word
want
Word
yu
4.3 Including topics (contexts)
To allow for the model to make use of the topics (used as proxies for contexts), we modify the grammar
by prefixing utterances with topic number (similarly to (Johnson et al., 2010)), ?K ? #topics:
Sentence? tK Colloc
+
tK
Colloc
tK
?Word
+
tK
For each Word
tK
, we can derive it into a common adapted Word by Word
tK
?Word. Consider this
lower level adaptor (Word): it learns a shared vocabulary independently of the topic (all contexts that
will derive b U k will increment the Word(b U k) pseudo-count). This Word-hierarchical model is
called share vocab.
Alternatively, we can learn a separate vocabulary for each topic, by having directly: Word
tK
?
StructSyll (note that all words then share the same syllabic structure). Words are split across different
topics and need to be adapted for each topic in which they appear. This flat structure vocabulary model
is called split vocab.
4.4 Allowing for non context-specific words
Sentences are not composed only of context-specific words, thus we need a third type of extension that
allows for topic-independent and topic-specific words to mix. For this, we add topic-independent types
of Colloc and Word that can be used across all topics, but we force each sentence to have at least one
topical collocation:
Sentence ? tK (Colloc
+
|Colloc
+
tK
) Colloc
+
tK
(Colloc
+
|Colloc
+
tK
)
Colloc
tK
? Word
+
tK
Colloc ? Word
+
Word
tK
? StructSyll
Word ? StructSyll
2329
Parentheses denote that these terms are optionals, and ?|? denotes ?or?. Both Word
tK
and Word are
adapted, but this time on the same level of hierarchy. This model allows the use of both topic-specific and
common words in sentences, and it learns #topics + 1 vocabularies. We call this model with common.
An example of a correct parse with this model is given by:
Sentence
Colloc t3
Word t3
bUk
Word t3
D6
Word t3
si
Colloc
Word
tu
Word
want
Word
yu
t3
5 Experimental setup
The Providence corpus (Demuth et al., 2006) consists of audio and video, weekly or bi-weekly, record-
ings of 6 monolingual English-speaking children home interactions. Each recording is approximatively
1 hour long. This corpus spans approximatively from their first to third year. We used the whole corpus
to extract the topics to get more stable and general activity contexts. For all the following results, we
used only the Naima portion between 11 months and 24 months, consisting in 26,425 utterances (sen-
tences) and 135,389 tokens (words). The input consist in DARPABET-encoded sequences of phonemes
with about 4200 word-types in the Naima subset. We followed the same preparation procedure as in
(B?orschinger et al., 2012), where more details about the corpus can be found.
We used the last version of Mark Johnson?s Adaptor Grammars software
2
. All the additional code
(preparation, topics, grammars, learning) to reproduce these experiments and results is freely available
online
3
, along with the datasets annotations derived from topic modeling
4
. For the adaptors, we used a
Beta(1, 1) (uniform) prior on the PYP a parameter, and a sparse Gamma(100, 0.01) prior on the PYP
b parameter. We ran 500 iterations (finishing at ? 0.05% of log posterior variation between the lasts
iterations) with several runs for each subset of the Naima dataset.
6 Results
6.1 Unsupervised words segmentation
Table 2: Mean (token and boundary) F-scores (f), precisions (p), and recalls (r) for different models
depending on the size of dataset (age range).
months baseline share vocab split vocab with common
token f p r f p r f p r f p r
11-12 .80 .79 .81 .77 .76 .78 .77 .75 .78 .77 .75 .78
11-15 .81 .81 .82 .76 .78 .75 .81 .79 .82 .82 .81 .83
11-19 .82 .82 .83 .77 .78 .76 .81 .81 .82 .83 .82 .84
11-22 .81 .82 .81 .77 .79 .75 .82 .81 .83 .83 .82 .84
boundary f p r f p r f p r f p r
11-12 .90 .88 .91 .88 .87 .89 .87 .85 .90 .88 .85 .90
11-15 .91 .91 .92 .89 .91 .86 .91 .89 .92 .91 .90 .93
11-19 .92 .92 .93 .90 .92 .88 .92 .91 .93 .92 .91 .94
11-22 .92 .93 .91 .90 .93 .87 .92 .91 .93 .93 .91 .94
The key metric of interest is the token F-score (harmonic mean of precision and recall of words).
Table 2 gives all the scores for an increasingly large dataset (as in (B?orschinger et al., 2012)). Figure 1
shows the month-by-month evolution of the token F-score of the different models. We can see that
2
http://web.science.mq.edu.au/
?
mjohnson/
3
https://github.com/SnippyHolloW/contextual_word_segmentation
4
https://github.com/SnippyHolloW/contextual_word_segmentation/tree/master/
ProvidenceFinal/Final
2330
Figure 1: Token F-scores (and standard deviations) evolution with an increasingly bigger and richer
dataset (11 months to ?X-axis value? months), computed on 8 runs of 500 iterations per data point.
0.750
0.775
0.800
0.825
24232221201918171615141312 age in months
token f
?score modelbaselineshare vocabsplit vocabwith common
context-based models need more data to get good performances (several vocabularies to learn), but they
seem more resilient to over-segmentation.
Preliminary results confirm the trend of baseline scores getting slowly worse at 25 and 26 months
while with common and split vocab stabilize (not plotted here). We also tried models for which we
can have the ?common vocabulary? derived only at the level of the collocations (making topic-specific
collocations topic-pure as in split vocab for instance), or only at the level of the words (allowing for
topic-specific collocations deriving in only common words if needed). Both models are worse than split
vocab and with common.
Using a shared global vocabulary while being able to learn (through adaptation) different topic-specific
vocabularies does not seem to be a solution: share vocab performs worse than the baseline. Token recall
and boundary recall are worse off (see Table 2), suggesting that fewer words are correctly adapted.
Maybe that is because this is the only model with two levels of adapted word hierarchies (Word
tK
and
Word). Sharing a lower-level vocabulary (Word) still does not allow for context vocabularies (Word
tK
)
to mix, thus is simply harder to train. Having only one vocabulary per context (split vocab) is a slight
improvement over the baseline, even though it is not significant (95% confidence interval) before 22
months. Models allowing for both topic-specific vocabularies and a common vocabulary to be learned
are the best: with common is significantly (95% confidence interval) better than the baseline, starting
from 20 months (Figure 1). The improvement seems to be due to better token (and boundary) recall
(Table 2), suggesting that more words are learned. By looking at their lexicons at 24 months, topic-
dependent models have slightly larger lexicon recalls and worse lexicon precisions than the baseline.
This means that the additional true word-types that they learn are more frequently correctly used than the
false word-types (otherwise the token F-scores would be reversed, e.g. between split vocab and baseline).
2331
Figure 2: Mean token F-scores (and standard deviations) on 20% held-out test data for 6 different random
splits of Naima from 11 to 22 months, 500 iterations each. Grey for baseline on test, green and blue for
context-dependent models on test and no prefix conditions respectively.
Table 3: Most probable words (? P (word|topic = k)) in the 7 recovered topics at test time without topic
annotations (no prefix condition) for the with common model (we omitted phonemes clusters yielding
non-words).
bread elephant lego Michael skinny stick bubble
delicious owl doctor shorts massage remember pasta
avocado wearing brush towel ostrich track spirals
porridge turkey change shirts nurse forget squirrels
raisin haircut squeeze pirates hammer oink thumb
biscuit turtle music tangled ruby towed pentagon
food animals play clothes (messy) verbs ?shapes
6.2 Recovery of the topics on held-out data
To check whether these models generalize to unseen utterances, and possibly unseen vocabulary, we
looked at the scores of held-out data (80/20% train/test split of the Naima 11 to 22 months dataset).
Token F-scores for this test condition are shown in green and grey in Figure 2. This separates low-
frequency collocations to be used at test time and those seen at training time, both for context aware
models and the basic baseline model. The F-scores show the same pattern as in the previous experiment,
with context-aware models (with common and split vocab here) performing better than the baseline.
The topics are learned on the orthographic transcription of the whole Providence corpus (6 children),
while we test only on the Naima dataset. Still, to check that these results are not simply due to additional
information (leaked somehow in the form of the tK prefix), we produced another held-out condition,
without topic ( tK) prefixes. Models can use topic-specific vocabularies learned during training, but
they are given no context information at test time. Token F-scores for this no prefix condition are shown
in blue (and grey for the baseline) in Figure 2. The fact that no prefix performance is on par with the
test condition means that contextual cues are not only important at test time, but particularly so while
learning the vocabulary. In other words, the model acquires its vocabularies making use of the additional
context. In the test setting, it is evaluated on novel utterances for which additional context information
is available. In the no prefix condition it is evaluated on novel utterances for which no additional context
information is available. This means that topic-specific vocabulary learned during training is successfully
used in a consistent way at test time. To confirm this qualitatively, we looked at the most probable words
(after unsupervised segmentation from the phonemic input) in recovered topics at test time in the no
prefix condition. They are shown in Table 3, and they exhibit some of the topics that were found on the
orthographic transcript (as they are not limited to nouns, a topic for ?verbs? appears).
2332
7 Conclusion
We have shown that contextual information helps segmenting speech into word-like units. We used
topic modeling as a proxy for richer contextual annotations, as (Roy et al., 2012) have shown high cor-
relation between contexts and automatically derived topics. We modified existing Adaptor Grammar
segmentation models (Johnson, 2008; Johnson and Goldwater, 2009), to be able to learn topic-specific
vocabularies. We applied this approach to a large child directed speech corpus that was previously used
for segmentation (B?orschinger et al., 2012). Our model with the capacity to use both a topic-specific
vocabulary and a common vocabulary (with common) produces better segmentation scores, ending up
with at least 2.5% better absolute F-scores than its context-oblivious counterpart (baseline). More gen-
erally, both models that learn specialized vocabularies do not get worse F-scores with increasing data
(Figure 1). Particularly, they seem to fix a well-known problem of previous models like ?colloc-syll?
(our baseline), that ?overlearn? by over-segmenting frequent morphemes as single words (B?orschinger
et al., 2012). We have controlled for the additional information of giving the topic ( tK), and we have
found out that contextual information helps at training time.
It would be interesting to look into the link between semantics and syntax in recovered topics. Fur-
ther work should integrate syntax (e.g. function words), stress cues and prosody from the audio signal
(B?orschinger and Johnson, 2014), use even less supervision for contexts, and be applied to other lan-
guages. We believe that language acquisition is not a simple sequential process and that segmentation,
syntax, and word meaning bootstrap each others. This is only a first step towards integrating multiple
sources of information and different modalities at all steps of language acquisition.
Acknowledgments
This project is funded in part by the European Research Council (ERC-2011-AdG-295810 BOOT-
PHON), the Agence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, ANR-10-IDEX-0001-02
PSL*), the Fondation de France, the Ecole de Neurosciences de Paris, and the Region Ile de France (DIM
cerveau et pense).
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res.,
3:993?1022, March.
Benjamin B?orschinger and Mark Johnson. 2014. Exploring the role of stress in Bayesian word segmentation using
Adaptor Grammars. Transactions of the Association of Computational Linguistics, 2:93?104, February.
Benjamin B?orschinger, Katherine Demuth, and Mark Johnson. 2012. Studying the effect of input size for bayesian
word segmentation on the providence corpus. In COLING, pages 325?340.
Katherine Demuth, Jennifer Culbertson, and Jennifer Alter. 2006. Word-minimality, epenthesis and coda licensing
in the early acquisition of english. Language and Speech, 49(2):137?173.
Michael C Frank, Noah D Goodman, and Joshua B Tenenbaum. 2009. Using speakers? referential intentions to
model early cross-situational word learning. Psychological Science, 20(5):578?585.
Sharon Goldwater, Thomas L Griffiths, and Mark Johnson. 2009. A bayesian framework for word segmentation:
Exploring the effects of context. Cognition, 112(1):21?54.
Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2011. Producing power-law distributions and damping
word frequencies with two-stage language models. Journal of Machine Learning Research, 12(Jul):2335?2382.
Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,
pages 317?325. Association for Computational Linguistics.
Mark Johnson, Thomas L Griffiths, and Sharon Goldwater. 2007. Adaptor grammars: A framework for specifying
compositional nonparametric bayesian models. Advances in neural information processing systems, 19:641.
2333
Mark Johnson, Katherine Demuth, Michael Frank, and Bevan Jones. 2010. Synergies in learning words and their
referents. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances in
Neural Information Processing Systems 23, pages 1018?1026.
Mark Johnson. 2008. Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic
structure. In ACL, pages 398?406.
Peter W. Jusczyk and Richard N. Aslin. 1995. Infants detection of the sound patterns of words in fluent speech.
Cognitive psychology, 29(1):123.
Peter W. Jusczyk, Anne Cutler, and Nancy J. Redanz. 1993a. Infants? preference for the predominant stress
patterns of english words. Child development, 64(3):675687.
Peter W. Jusczyk, Angela D. Friederici, Jeanine MI Wessels, Vigdis Y. Svenkerud, and Ann Marie Jusczyk.
1993b. Infants sensitivity to the sound patterns of native language words. Journal of Memory and Language,
32(3):402420.
Mihael Perman, Jim Pitman, and Marc Yor. 1992. Size-biased sampling of poisson point processes and excursions.
Probability Theory and Related Fields, 92(1):21?39.
Radim
?
Reh?u?rek and Petr Sojka. 2010. Software Framework for Topic Modelling with Large Corpora. In Proceed-
ings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45?50, Valletta, Malta, May.
ELRA. http://is.muni.cz/publication/884893/en.
Deb Roy, Rupal Patel, Philip DeCamp, Rony Kubat, Michael Fleischman, Brandon Roy, Nikolaos Mavridis, Ste-
fanie Tellex, Alexia Salata, Jethran Guinness, et al. 2006. The human speechome project. In Symbol Grounding
and Beyond, pages 192?196. Springer.
Brandon C Roy, Michael C Frank, and Deb Roy. 2012. Relating activity contexts to early word learning in dense
longitudinal data. In Proceedings of the 34th Annual Cognitive Science Conference.
Jenny R. Saffran, Richard N. Aslin, and Elissa L. Newport. 1996. Statistical learning by 8-month old infants.
Science, 274(5294):1926?1928.
Yee Whye Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings
of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 985?992.
2334
