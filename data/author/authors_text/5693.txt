Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 21?24,
New York, June 2006. c?2006 Association for Computational Linguistics
Class Model Adaptation for Speech Summarisation
Pierre Chatain, Edward W.D. Whittaker, Joanna Mrozinski and Sadaoki Furui
Dept. of Computer Science
Tokyo Institute of Technology
2-12-1 Ookayama, Meguro-ku, Tokyo 152-8552, Japan
{pierre, edw, mrozinsk, furui}@furui.cs.titech.ac.jp
Abstract
The performance of automatic speech
summarisation has been improved in pre-
vious experiments by using linguistic
model adaptation. We extend such adapta-
tion to the use of class models, whose ro-
bustness further improves summarisation
performance on a wider variety of objec-
tive evaluation metrics such as ROUGE-2
and ROUGE-SU4 used in the text sum-
marisation literature. Summaries made
from automatic speech recogniser tran-
scriptions benefit from relative improve-
ments ranging from 6.0% to 22.2% on all
investigated metrics.
1 Introduction
Techniques for automatically summarising written
text have been actively investigated in the field of
natural language processing, and more recently new
techniques have been developed for speech sum-
marisation (Kikuchi et al, 2003). However it is
still very hard to obtain good quality summaries.
Moreover, recognition accuracy is still around 30%
on spontaneous speech tasks, in contrast to speech
read from text such as broadcast news. Spontaneous
speech is characterised by disfluencies, repetitions,
repairs, and fillers, all of which make recognition
and consequently speech summarisation more diffi-
cult (Zechner, 2002). In a previous study (Chatain
et al, 2006), linguistic model (LiM) adaptation us-
ing different types of word models has proved use-
ful in order to improve summary quality. However
sparsity of the data available for adaptation makes it
difficult to obtain reliable estimates of word n-gram
probabilities. In speech recognition, class models
are often used in such cases to improve model ro-
bustness. In this paper we extend the work previ-
ously done on adapting the linguistic model of the
speech summariser by investigating class models.
We also use a wider variety of objective evaluation
metrics to corroborate results.
2 Summarisation Method
The summarisation system used in this paper is es-
sentially the same as the one described in (Kikuchi
et al, 2003), which involves a two step summarisa-
tion process, consisting of sentence extraction and
sentence compaction. Practically, only the sentence
extraction part was used in this paper, as prelimi-
nary experiments showed that compaction had little
impact on results for the data used in this study.
Important sentences are first extracted accord-
ing to the following score for each sentence
W = w1, w2, ..., wn, obtained from the automatic
speech recognition output:
S(W ) =
1
N
N?
i=1
{?CC(wi)+?II(wi)+?LL(wi)},
(1)
where N is the number of words in the sentence
W , and C(wi), I(wi) and L(wi) are the confidence
score, the significance score and the linguistic score
of word wi, respectively. ?C , ?I and ?L are the
respective weighting factors of those scores, deter-
mined experimentally.
For each word from the automatic speech recogni-
21
tion transcription, a logarithmic value of its posterior
probability, the ratio of a word hypothesis probabil-
ity to that of all other hypotheses, is calculated using
a word graph obtained from the speech recogniser
and used as a confidence score.
For the significance score, the frequencies of oc-
currence of 115k words were found using the WSJ
and the Brown corpora.
In the experiments in this paper we modified the
linguistic component to use combinations of dif-
ferent linguistic models. The linguistic component
gives the linguistic likelihood of word strings in
the sentence. Starting with a baseline LiM (LiMB)
we perform LiM adaptation by linearly interpolat-
ing the baseline model with other component mod-
els trained on different data. The probability of a
given n-gram sequence then becomes:
P (wi|wi?n+1..wi?1) = ?1P1(wi|wi?n+1..wi?1)
+... + ?nPn(wi|wi?n+1..wi?1), (2)
where
?
k ?k = 1 and ?k and Pk are the weight and
the probability assigned by model k.
In the case of a two-sided class-based model,
Pk(wi|wi?n+1..wi?1) = Pk(wi|C(wi)) ?
Pk(C(wi)|C(wi?n+1)..C(wi?1)), (3)
where Pk(wi|C(wi)) is the probability of the
word wi belonging to a given class C, and
Pk(C(wi)|C(wi?n+1)..C(wi?1)) the probability of
a certain word class C(wi) to appear after a history
of word classes, C(wi?n+1), ..., C(wi?1).
Different types of component LiM are built, com-
ing from different sources of data, either as word
or class models. The LiMB and component LiMs
are then combined for adaptation using linear inter-
polation as in Equation (2). The linguistic score is
then computed using this modified probability as in
Equation (4):
L(wi) = logP (wi|wi?n+1..wi?1). (4)
3 Evaluation Criteria
3.1 Summarisation Accuracy
To automatically evaluate the summarised speeches,
correctly transcribed talks were manually sum-
marised, and used as the correct targets for evalua-
tion. Variations of manual summarisation results are
merged into a word network, which is considered to
approximately express all possible correct summari-
sations covering subjective variations. The word ac-
curacy of automatic summarisation is calculated as
the summarisation accuracy (SumACCY) using the
word network (Hori et al, 2003):
Accuracy = (Len?Sub?Ins?Del)/Len?100[%],
(5)
where Sub is the number of substitution errors, Ins
is the number of insertion errors, Del is the number
of deletion errors, and Len is the number of words
in the most similar word string in the network.
3.2 ROUGE
Version 1.5.5 of the ROUGE scoring algorithm
(Lin, 2004) is also used for evaluating results.
ROUGE F-measure scores are given for ROUGE-
2 (bigram), ROUGE-3 (trigram), and ROUGE-SU4
(skip-bigram), using the model average (average
score across all references) metric.
4 Experimental Setup
Experiments were performed on spontaneous
speech, using 9 talks taken from the Translanguage
English Database (TED) corpus (Lamel et al, 1994;
Wolfel and Burger, 2005), each transcribed and
manually summarised by nine different humans for
both 10% and 30% summarization ratios. Speech
recognition transcriptions (ASR) were obtained for
each talk, with an average word error rate of 33.3%.
A corpus consisting of around ten years of con-
ference proceedings (17.8M words) on the subject
of speech and signal processing is used to generate
the LiMB and word classes using the clustering al-
gorithm in (Ney et al, 1994).
Different types of component LiM are built and
combined for adaptation as described in Section 2.
The first type of component linguistic models are
built on the small corpus of hand-made summaries
described above, made for the same summarisation
ratio as the one we are generating. For each talk
the hand-made summaries of the other eight talks
(i.e. 72 summaries) were used as the LiM training
corpus. This type of LiM is expected to help gener-
ate automatic summaries in the same style as those
made manually.
22
Baseline Adapted
SumACCY R-2 R-3 R-SU4 SumACCY R-2 R-3 R-SU4
10% Random 34.4 0.104 0.055 0.142 - - - -
Word 63.1 0.186 0.130 0.227 67.8 0.193 0.140 0.228
Class 65.1 0.195 0.131 0.226 72.6 0.210 0.143 0.234
Mixed 63.6 0.186 0.128 0.218 71.8 0.211 0.139 0.231
30% Random 71.2 0.294 0.198 0.331 - - - -
Word 81.6 0.365 0.271 0.395 83.3 0.365 0.270 0.392
Class 83.1 0.374 0.279 0.407 92.9 0.415 0.325 0.442
Mixed 83.1 0.374 0.279 0.407 92.9 0.415 0.325 0.442
Table 1: TRS baseline and adapted results.
The second type of component linguistic models
are built from the papers in the conference proceed-
ings for the talk we want to summarise. This type
of LiM, used for topic adaptation, is investigated be-
cause key words and important sentences that appear
in the associated paper are expected to have a high
information value and should be selected during the
summarisation process.
Three sets of experiments were made: in the first
experiment (referred to as Word), LiMB and both
component models are word models, as introduced
in (Chatain et al, 2006). For the second one (Class),
both LiMB and the component models are class
models built using exactly the same data as the word
models. For the third experiment (Mixed), the LiMB
is an interpolation of class and word models, while
the component LiMs are class models.
To optimise use of the available data, a rotating
form of cross-validation (Duda and Hart, 1973) is
used: all talks but one are used for development, the
remaining talk being used for testing. Summaries
from the development talks are generated automati-
cally by the system using different sets of parameters
and the LiMB . These summaries are evaluated and
the set of parameters which maximises the develop-
ment score for the LiMB is selected for the remain-
ing talk. The purpose of the development phase is
to choose the most effective combination of weights
?C , ?I and ?L. The summary generated for each
talk using its set of optimised parameters is then
evaluated using the same metric, which gives us our
baseline for this talk. Using the same parameters as
those that were selected for the baseline, we gener-
ate summaries for the lectures in the development set
for different LiM interpolation weights ?k. Values
between 0 and 1 in steps of 0.1, were investigated
for the latter, and an optimal set of ?k is selected.
Using these interpolation weights, as well as the set
of parameters determined for the baseline, we gen-
erate a summary of the test talk, which is evaluated
using the same evaluation metric, giving us our fi-
nal adapted result for this talk. Averaging those re-
sults over the test set (i.e. all talks) gives us our final
adapted result.
This process is repeated for all evaluation metrics,
and all three experiments (Word, Class, and Mixed).
Lower bound results are given by random sum-
marisation (Random) i.e. randomly extracting sen-
tences and words, without use of the scores present
in Equation (1) for appropriate summarisation ratios.
5 Results
5.1 TRS Results
Initial experiments were made on the human tran-
scriptions (TRS), and results are given in Table 1.
Experiments on word models (Word) show relative
improvements in terms of SumACCY of 7.5% and
2.1% for the 10% and 30% summarisation ratios, re-
spectively. ROUGE metrics, however, do not show
any significant improvement.
Using class models (Class and Mixed), for all
ROUGE metrics, relative improvements range from
3.5% to 13.4% for the 10% summarisation ratio, and
from 8.6% to 16.5% on the 30% summarisation ra-
tio. For SumACCY, relative improvements between
11.5% to 12.9% are observed.
5.2 ASR Results
ASR results for each experiment are given in Ta-
ble 2 for appropriate summarisation ratios. As for
23
Baseline Adapted
SumACCY R-2 R-3 R-SU4 SumACCY R-2 R-3 R-SU4
10% Random 33.9 0.095 0.042 0.140 - - - -
Word 48.6 0.143 0.064 0.182 49.8 0.129 0.060 0.173
Class 50.0 0.133 0.063 0.170 55.1 0.156 0.077 0.193
Mixed 48.5 0.134 0.068 0.176 56.2 0.142 0.077 0.191
30% Random 56.1 0.230 0.124 0.283 - - - -
Word 66.7 0.265 0.157 0.314 68.7 0.271 0.161 0.328
Class 66.1 0.277 0.165 0.324 71.1 0.300 0.180 0.348
Mixed 64.9 0.268 0.160 0.312 70.5 0.304 0.192 0.351
Table 2: ASR baseline and adapted results.
the TRS, LiM adaptation showed improvements in
terms of SumACCY, but ROUGE metrics do not cor-
roborate those results for the 10% summarisation ra-
tio. Using class models, for all ROUGE metrics, rel-
ative improvements range from 6.0% to 22.2% and
from 7.4% to 20.0% for the 10% and 30% summari-
sation ratios, respectively. SumACCY relative im-
provements range from 7.6% to 15.9%.
6 Discussion
Compared to previous experiments using only word
models, improvements obtained using class models
are larger and more significant for both ROUGE and
SumACCY metrics. This can be explained by the
fact that the data we are performing adaptation on
is very sparse, and that the nine talks used in these
experiments are quite different from each other, es-
pecially since the speakers also vary in style. Class
models are more robust to this spontaneous speech
aspect than word models, since they generalise bet-
ter to unseen word sequences.
There is little difference between the Class and
Mixed results, since the development phase assigned
most weight to the class model component in the
Mixed experiment, making the results quite similar
to those of the Class experiment.
7 Conclusion
In this paper we have investigated linguistic model
adaptation using different sources of data for an au-
tomatic speech summarisation system. Class mod-
els have proved to be much more robust than word
models for this process, and relative improvements
ranging from 6.0% to 22.2% were obtained on a va-
riety of evaluation metrics on summaries generated
from automatic speech recogniser transcriptions.
Acknowledgements: The authors would like to
thank M. Wo?lfel for the recogniser transcriptions
and C. Hori for her work on two stage summarisa-
tion and gathering the TED corpus data. This work
is supported by the 21st Century COE Programme.
References
P. Chatain, E.W.D. Whittaker, J. Mrozinski, and S. Fu-
rui. 2006. Topic and Stylistic Adaptation for Speech
Summarization. Proc. ICASSP, Toulouse, France.
R. Duda and P. Hart. 1973. Pattern Classification and
Scene Analysis. Wiley, New York.
C. Hori, T. Hori, and S. Furui. 2003. Evaluation
Method for Automatic Speech Summarization. Proc.
Eurospeech, Geneva, Switzerland, 4:2825?2828.
T. Kikuchi, S. Furui, and C. Hori. 2003. Automatic
Speech Summarization based on Sentence Extraction
and Compaction. Proc. ICASSP, Hong Kong, China,
1:236?239.
L. Lamel, F. Schiel, A. Fourcin, J. Mariani, and H. Till-
mann. 1994. The Translanguage English Database
(TED). Proc. ICSLP, Yokohama, Japan, 4:1795?1798.
Chin-Yew Lin. 2004. ROUGE: a Package for Automatic
Evaluation of Summaries. Proc. WAS, Barcelona,
Spain.
H. Ney, U. Essen, and R. Kneser. 1994. On Structur-
ing Probabilistic Dependences in Stochastic Language
Modelling. Computer Speech and Language, (8):1?
38.
M. Wolfel and S. Burger. 2005. The ISL Baseline Lec-
ture Transcription System for the TED Corpus. Tech-
nical report, Karlsruhe University.
K. Zechner. 2002. Summarization of Spoken Language-
Challenges, Methods, and Prospects. Speech Technol-
ogy Expert eZine, Issue.6.
24
Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 288?291,
New York City, June 2006. c?2006 Association for Computational Linguistics
Factoid Question Answering with Web, Mobile and Speech Interfaces
E.W.D. Whittaker J. Mrozinski S. Furui
Dept. of Computer Science
Tokyo Institute of Technology
2-12-1, Ookayama, Meguro-ku
Tokyo 152-8552 Japan
 
edw,mrozinsk,furui  @furui.cs.titech.ac.jp
Abstract
In this paper we describe the web and
mobile-phone interfaces to our multi-
language factoid question answering (QA)
system together with a prototype speech
interface to our English-language QA sys-
tem. Using a statistical, data-driven ap-
proach to factoid question answering has
allowed us to develop QA systems in five
languages in a matter of months. In the
web-based system, which is accessible
at http://asked.jp, we have com-
bined the QA system output with standard
search-engine-like results by integrating it
with an open-source web search engine.
The prototype speech interface is based
around a VoiceXML application running
on the Voxeo developer platform. Recog-
nition of the user?s question is performed
on a separate speech recognition server
dedicated to recognizing questions. An
adapted version of the Sphinx-4 recog-
nizer is used for this purpose. Once the
question has been recognized correctly it
is passed to the QA system and the re-
sulting answers read back to the user by
speech synthesis. Our approach is mod-
ular and makes extensive use of open-
source software. Consequently, each com-
ponent can be easily and independently
improved and easily extended to other lan-
guages.
1 Introduction
The approach to factoid question answering (QA)
that we adopt was first described in (Whittaker et
al., 2005b) where the details of the mathematical
model and how it was trained for English were
given. The approach has been successfully evalu-
ated in the 2005 text retrieval conference (TREC)
question answering track evaluations (Voorhees and
Trang Dang, 2005) where our group placed eleventh
out of thirty participants (Whittaker et al, 2005a).
Although the TREC QA task is substantially differ-
ent to web-based QA this evaluation showed that our
approach works and provides an objective assess-
ment of its quality. Similarly, for our Japanese lan-
guage system we have evaluated the performance of
our approach on the NTCIR-3 QAC-1 task (Whit-
taker et al, 2005c). Although our Japanese ex-
periments were applied retrospectively, the results
would have placed us in the mid-range of partici-
pating systems. In (Whittaker et al, 2006b) we de-
scribed how our approach could be used for the rapid
development of web-based QA systems in five very
different languages. It was shown that a developer
proficient with the tools, and with access to suitable
training data, could build a system in a new language
in around 10 hours. In (Whittaker et al, 2006a) we
evaluated the performance of the systems for four of
our five languages. We give a brief summary of our
approach to QA in Section 2.
In this paper we introduce our web-based
QA system which is publicly accessible at
http://asked.jp, permitting questions in En-
glish, Japanese, Chinese, Russian and Swedish and
288
is discussed in Section 3. Since answers in factoid
QA are inherently well-suited to display on small
screens we have also made a mobile-phone interface
which is accessible at the same address when using
an HTML browser from a mobile phone. This is dis-
cussed in Section 4. There are several other QA sys-
tems on the web including Brainboost (Brainboost,
2005) and Lexxe (Lexxe, 2005) but they only try to
answer questions in English and do not have conve-
nient mobile interfaces.
Entering whole questions rather than just key-
words is tedious especially on a mobile-phone so
we have also begun to look at speech interfaces. In
this paper we describe a prototype speech interface
to our English-language QA system. This prototype
is currently intended primarily as a platform for fur-
ther research into speech recognition and answering
of questions from an acoustic modelling point-of-
view (e.g. low-bandwidth, low-quality VoIP chan-
nel), from a language modelling perspective (e.g. ir-
regular word order in questions vs. text, and very
large out-of-vocabulary problem) and also in terms
of dialog modelling. There have been several at-
tempts at speech interfaces to QA systems in the lit-
erature e.g. (Schofield and Zheng, 2003) but as far
as we know ours is the only system that is publicly
accessible. We discuss this interface in Section 5.
2 Statistical pattern classification
approach to QA
The answer to a question depends primarily on the
question itself but also on many other factors such
as the person asking the question, the location of the
person, what questions the person has asked before,
and so on. For simplicity, we choose to consider
only the dependence of an answer   on the question

. In particular, we hypothesize that the answer  
depends on two sets of features extracted from

:


	
and  

	
as follows:

 

	

 


	
 (1)
where

can be thought of as a set of Proceedings of ACL-08: HLT, pages 443?451,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Collecting a Why-question corpus for development and evaluation of an
automatic QA-system
Joanna Mrozinski Edward Whittaker
Department of Computer Science
Tokyo Institute of Technology
2-12-1-W8-77 Ookayama, Meguro-ku
Tokyo 152-8552 Japan
{mrozinsk,edw,furui}@furui.cs.titech.ac.jp
Sadaoki Furui
Abstract
Question answering research has only recently
started to spread from short factoid questions
to more complex ones. One significant chal-
lenge is the evaluation: manual evaluation is a
difficult, time-consuming process and not ap-
plicable within efficient development of sys-
tems. Automatic evaluation requires a cor-
pus of questions and answers, a definition of
what is a correct answer, and a way to com-
pare the correct answers to automatic answers
produced by a system. For this purpose we
present a Wikipedia-based corpus of Why-
questions and corresponding answers and arti-
cles. The corpus was built by a novel method:
paid participants were contacted through a
Web-interface, a procedure which allowed dy-
namic, fast and inexpensive development of
data collection methods. Each question in the
corpus has several corresponding, partly over-
lapping answers, which is an asset when es-
timating the correctness of answers. In ad-
dition, the corpus contains information re-
lated to the corpus collection process. We be-
lieve this additional information can be used to
post-process the data, and to develop an auto-
matic approval system for further data collec-
tion projects conducted in a similar manner.
1 Introduction
Automatic question answering (QA) is an alternative
to traditional word-based search engines. Instead of
returning a long list of documents more or less re-
lated to the query parameters, the aim of a QA sys-
tem is to isolate the exact answer as accurately as
possible, and to provide the user only a short text
clip containing the required information.
One of the major development challenges is eval-
uation. The conferences such as TREC1, CLEF2
and NTCIR3 have provided valuable QA evaluation
methods, and in addition produced and distributed
corpora of questions, answers and corresponding
documents. However, these conferences have fo-
cused mainly on fact-based questions with short an-
swers, so called factoid questions. Recently more
complex tasks such as list, definition and discourse-
based questions have also been included in TREC in
a limited fashion (Dang et al, 2007). More complex
how- and why-questions (for Asian languages) were
also included in the NTCIR07, but the provided data
comprised only 100 questions, of which some were
also factoids (Fukumoto et al, 2007). Not only is
the available non-factoid data quite limited in size,
it is also questionable whether the data sets are us-
able in development outside the conferences. Lin
and Katz (2006) suggest that training data has to be
more precise, and, that it should be collected, or at
least cleaned, manually.
Some corpora of why-questions have been col-
lected manually: corpora described in (Verberne et
al., 2006) and (Verberne et al, 2007) both com-
prise fewer than 400 questions and corresponding
answers (one or two per question) formulated by na-
tive speakers. However, we believe one answer per
question is not enough. Even with factoid questions
it is sometimes difficult to define what is a correct
1http://trec.nist.gov/
2http://www.clef-campaign.org/
3http://research.nii.ac.jp/ntcir/
443
answer, and complex questions result in a whole new
level of ambiguity. Correctness depends greatly on
the background knowledge and expectations of the
person asking the question. For example, a correct
answer to the question ?Why did Mr. X take Ms. Y
to a coffee shop?? could be very different depending
on whether we knew that Mr. X does not drink cof-
fee or that he normally drinks it alone, or that Mr. X
and Ms. Y are known enemies.
The problem of several possible answers and, in
consequence, automatic evaluation has been tackled
for years within another field of study: automatic
summarisation (Hori et al, 2003; Lin and Hovy,
2003). We believe that the best method of provid-
ing ?correct? answers is to do what has been done in
that field: combine a multitude of answers to ensure
both diversity and consensus among the answers.
Correctness of an answer is also closely related to
the required level of detail. The Internet FAQ pages
were successfully used to develop QA-systems (Jijk-
oun and de Rijke, 2005; Soricut and Brill, 2006), as
have the human-powered question sites such as An-
swers.com, Yahoo Answers and Google Answers,
where individuals can post questions and receive an-
swers from peers (Mizuno et al, 2007). Both re-
sources can be assumed to contain adequately error-
free information. FAQ pages are created so as to
answer typical questions well enough that the ques-
tions do not need to be repeated. Question sites typ-
ically rank the answers and offer bonuses for peo-
ple providing good ones. However, both sites suffer
from excess of information. FAQ-pages tend to also
answer questions which are not asked, and also con-
tain practical examples. Human-powered answers
often contain unrelated information and discourse-
like elements. Additionally, the answers do not al-
ways have a connection to the source material from
which they could be extracted.
One purpose of our project was to take part in
the development of QA systems by providing the
community with a new type of corpus. The cor-
pus includes not only the questions with multiple
answers and corresponding articles, but also certain
additional information that we believe is essential to
enhance the usability of the data.
In addition to providing a new QA corpus, we
hope our description of the data collection process
will provide insight, resources and motivation for
further research and projects using similar collection
methods. We collected our corpus through Amazon
Mechanical Turk service 4 (MTurk). The MTurk
infrastructure allowed us to distribute our tasks to
a multitude of workers around the world, without
the burden of advertising. The system also allowed
us to test the workers suitability, and to reward the
work without the bureaucracy of employment. To
our knowledge, this is the first time that the MTurk
service has been used in equivalent purpose.
We conducted the data collection in three steps:
generation, answering and rephrasing of questions.
The workers were provided with a set of Wikipedia
articles, based on which the questions were created
and the answers determined by sentence selection.
The WhyQA-corpus consists of three parts: original
questions along with their rephrased versions, 8-10
partly overlapping answers for each question, and
the Wikipedia articles including the ones corre-
sponding to the questions. The WhyQA-corpus is
in XML-format and can be downloaded and used
under the GNU Free Documentation License from
www.furui.cs.titech.ac.jp/ .
2 Setup
Question-answer pairs have previously been gen-
erated for example by asking workers to both ask
a question and then answer it based on a given
text (Verberne et al, 2006; Verberne et al, 2007).
We decided on a different approach for two reasons.
Firstly, based on our experience such an approach is
not optimal in the MTurk framework. The tasks that
were welcomed by workers required a short atten-
tion span, and reading long texts was negatively re-
ceived with many complaints, sloppy work and slow
response times. Secondly, we believe that the afore-
mentioned approach can produce unnatural ques-
tions that are not actually based on the information
need of the workers.
We divided the QA-generation task into two
phases: question-generation (QGenHIT) and an-
swering (QAHIT). We also trimmed the amount of
the text that the workers were required to read to cre-
ate the questions. These measures were taken both
in order to lessen the cognitive burden of the task
4http://www.mturk.com
444
and to produce more natural questions.
In the first phase the workers generated the ques-
tions based on a part of Wikipedia article. The re-
sulting questions were then uploaded to the system
as new HITs with the corresponding articles, and
answered by available (different) workers. Our hy-
pothesis is that the questions are more natural if their
answer is not known at the time of the creation.
Finally, in an additional third phase, 5 rephrased
versions of each question were created in order to
gain variation (QRepHIT). The data quality was en-
sured by requiring the workers to achieve a certain
result from a test (or a Qualification) before they
could work on the aforementioned tasks.
Below we explain the MTurk system, and then our
collection process in detail.
2.1 Mechanical Turk
Mechanical Turk is a Web-based service, offered by
Amazon.com, Inc. It provides an API through which
employers can obtain a connection to people to per-
form a variety of simple tasks. With tools provided
by Amazon.com, the employer creates tasks, and up-
loads them to the MTurk Web-site. Workers can then
browse the tasks and, if they find them profitable
and/or interesting enough, work on them. When the
tasks are completed, the employer can download the
results, and accept or reject them. Some key con-
cepts of the system are listed below, with short de-
scriptions of the functionality.
? HIT Human Intelligence Task, the unit of a
payable chore in MTurk.
? Requester An ?employer?, creates and uploads
new HITs and rewards the workers. Requesters
can upload simple HITs through the MTurk Re-
quester web site, and more complicated ones
through the MTurk Web Service APIs.
? Worker An ?employee?, works on the hits
through the MTurk Workers? web site.
? Assignment. One HIT consists of one or more
assignments. One worker can complete a sin-
gle HIT only once, so if the requester needs
multiple results per HIT, he needs to set the
assignment-count to the desired figure. A HIT
is considered completed when all the assign-
ments have been completed.
? Rewards At upload time, each HIT has to be
assigned a fixed reward, that cannot be changed
later. Minimum reward is $0.01. Amazon.com
collects a 10% (or a minimum of $0.05) service
fee per each paid reward.
? Qualifications To improve the data quality,
a HIT can also be attached to certain tests,
?qualifications? that are either system-provided
or created by the requester. An example of
a system-provided qualification is the average
approval ratio of the worker.
Even if it is possible to create tests that workers
have to pass before being allowed to work on a HIT
so as to ensure the worker?s ability, it is impossible
to test the motivation (for instance, they cannot be
interviewed). Also, as they are working through the
Web, their working conditions cannot be controlled.
2.2 Collection process
The document collection used in our research was
derived from the Wikipedia XML Corpus by De-
noyer and Gallinari (2006). We selected a total of
84 articles, based on their length and contents. A
certain length was required so that we could expect
the article to contain enough interesting material to
produce a wide selection of natural questions. The
articles varied in topic, degree of formality and the
amount of details; from ?Horror film? and ?Christ-
mas worldwide? to ?G-Man (Half-Life)? and ?His-
tory of London?. Articles consisting of bulleted lists
were removed, but filtering based on the topic of the
article was not performed. Essentially, the articles
were selected randomly.
2.2.1 QGenHIT
The first phase of the question-answer generation
was to generate the questions. In QGenHIT we pre-
sented the worker with only part of a Wikipedia ar-
ticle, and instructed them to think of a why-question
that they felt could be answered based on the origi-
nal, whole article which they were not shown. This
approach was expected to lead to natural curiosity
and questions. Offering too little information would
have lead to many questions that would finally be
left unanswered, and it also did not give the workers
enough to work on. Giving too much information
445
Qualification The workers were required to pass a test before working on the HITs.
QGenHIT Questions were generated based on partial Wikipedia articles. These questions were
then used to create the QAHITs.
QAHIT Workers were presented with a question and a corresponding article. The task was to
answer the questions (if possible) through sentence selection.
QRepHIT To ensure variation in the questions, each question was rephrased by 5 different workers.
Table 1: Main components of the corpus collection process.
Article topic: Fermi paradox
Original question Why is the moon crucial to the rare earth hypothesis?
Rephrased Q 1 How does the rare earth theory depend upon the moon?
Rephrased Q 2 What makes the moon so important to rare earth theory?
Rephrased Q 3 What is the crucial regard for the moon in the rare earth hypothesis?
Rephrased Q 4 Why is the moon so important in the rare earth hypothesis?
Rephrased Q 5 What makes the moon necessary, in regards to the rare earth hypothesis?
Answer 1. Sentence ids: 20,21. Duplicates: 4. The moon is important because its gravitational pull
creates tides that stabilize Earth?s axis. Without this stability, its variation, known as precession of
the equinoxes, could cause weather to vary so dramatically that it could potentially suppress the more
complex forms of life.
Answer 2. Sentence ids: 18,19,20. Duplicates: 2. The popular Giant impact theory asserts that it
was formed by a rare collision between the young Earth and a Mars-sized body, usually referred to as
Orpheus or Theia, approximately 4.45 billion years ago. The collision had to occur at a precise angle,
as a direct hit would have destroyed the Earth, and a shallow hit would have deflected the Mars-sized
body. The moon is important because its gravitational pull creates tides that stabilize Earth?s axis.
Answer 3. Sentence ids: 20,21,22. Duplicates: 2. The moon is important because its gravitational
pull creates tides that stabilize Earth?s axis. Without this stability, its variation, known as precession
of the equinoxes, could cause weather to vary so dramatically that it could potentially suppress the
more complex forms of life. The heat generated by the Earth/Theia impact, as well as subsequent
Lunar tides, may have also significantly contributed to the total heat budget of the Earth?s interior,
thereby both strengthening and prolonging the life of the dynamos that generate Earth?s magnetic field
Dynamo 1.
Answer 4. Sentence ids: 18,20,21. No duplicates. The popular Giant impact theory asserts that
it was formed by a rare collision between the young Earth and a Mars-sized body, usually referred
to as Orpheus or Theia, approximately 4.45 billion years ago. The moon is important because its
gravitational pull creates tides that stabilize Earth?s axis. Without this stability, its variation, known
as precession of the equinoxes, could cause weather to vary so dramatically that it could potentially
suppress the more complex forms of life.
Answer 5. Sentence ids: 18,21. No duplicates. The popular Giant impact theory asserts that it
was formed by a rare collision between the young Earth and a Mars-sized body, usually referred to as
Orpheus or Theia, approximately 4.45 billion years ago. Without this stability, its variation, known
as precession of the equinoxes, could cause weather to vary so dramatically that it could potentially
suppress the more complex forms of life.
Table 2: Data example: Question with rephrased versions and answers.
446
(long excerpts from the articles) was severely dis-
liked among the workers simply because it took a
long time to read.
We finally settled on a solution where the partial
content consisted of the title and headers of the arti-
cle, along with the first sentences of each paragraph.
The instructions to the questions demanded rigidly
that the question starts with the word ?Why?, as it
was surprisingly difficult to explain what we meant
by why-questions if the question word was not fixed.
The reward per HIT was $0.04, and 10 questions
were collected for each article. We did not force the
questions to be different, and thus in the later phase
some of the questions were removed manually as
they were deemed to mean exactly the same thing.
However, there were less than 30 of these duplicate
questions in the whole data set.
2.2.2 QAHIT
After generating the questions based on partial ar-
ticles, the resulting questions were uploaded to the
system as HITs. Each of these QAHITs presented a
single question with the corresponding original arti-
cle. The worker?s task was to select either 1-3 sen-
tences from the text, or a No-answer-option (NoA).
Sentence selection was conducted with Javascript
functionality, so the workers had no chance to in-
clude freely typed information within the answer (al-
though a comment field was provided). The reward
per HIT was $0.06. At the beginning, we collected
10 answers per question, but we cut that down to 8
because the HITs were not completed fast enough.
The workers for QAHITs were drawn from the
same pool as the workers for QGenHIT, and it was
possible for the workers to answer the questions they
had generated themselves.
2.2.3 QRepHIT
As the final step 5 rephrased versions of each
question were generated. This was done to com-
pensate the rigid instructions of the QGenHIT and
to ensure variation in the questions. We have not yet
measured how well the rephrased questions match
the answers of their original versions. In the final
QRepHIT questions were grouped into groups of 5.
Each HIT consisted of 5 assignments, and a $0.05
reward was offered for each HIT.
QRepHIT required the least amount of design and
trials, and workers were delighted with the task. The
HITs were completed fast and well even in the case
when we accidentally uploaded a set of HITs with
no reward.
As with QAHIT, the worker pool for creating and
rephrasing questions was the same. The questions
were rephrased by their creator in 4 cases.
2.3 Qualifications
To improve the data quality, we used the qualifi-
cations to test the workers. For the QGenHITs we
only used the system-provided ?HIT approval rate?-
qualification. Only workers whose previous work
had been approved in 80% of the cases were able to
work on our HITs.
In addition to the system-provided qualification,
we created a why-question-specific qualification.
The workers were presented with 3 questions, and
they were to answer each by either selecting 1-
3 most relevant sentences from a list of about 10
sentences, or by deciding that there is no answer
present. The possible answer-sentences were di-
vided into groups of essential, OK and wrong, and
one of the questions did quite clearly have no an-
swer. The scoring was such that it was impossible
to get approved results if not enough essential sen-
tences were included. Selecting sentences from the
OK-group only was not sufficient, and selecting sen-
tences from the wrong-group was penalized. A min-
imum score per question was required, but also the
total score was relevant ? component scores could
compensate each other up to a point. However, if
the question with no answer was answered, the score
could not be of an approvable level. This qualifica-
tion was, in addition to the minimum HIT approval
rate of 80%, a prerequisite for both the QRepHITs
and the QAHITs.
A total of 2355 workers took the test, and 1571
(67%) of them passed it, thus becoming our avail-
able worker pool. However, in the end the actual
number of different workers was only 173.
Examples of each HIT, their instructions and the
Qualification form are included in the final corpus.
The collection process is summarised in Table 1.
447
3 Corpus description
The final corpus consists of questions with their
rephrased versions and answers. There are total of
695 questions, of which 159 were considered unan-
swerable based on the articles, and 536 that have 8-
10 answers each. The total cost of producing the
corpus was about $350, consisting of $310 paid in
workers rewards and $40 in Mechanical Turk fees,
including all the trials conducted during the devel-
opment of the final system.
Also included is a set of Wikipedia documents
(WikiXML, about 660 000 articles or 670MB in com-
pressed format), including the ones corresponding to
the questions (84 documents). The source of Wik-
iXML is the English part of the Wikipedia XML
Corpus by Denoyer and Gallinari (2006). In the
original data some of the HTML-structures like lists
and tables occurred within sentences. Our sentence-
selection approach to QA required a more fine-
grained segmentation and for our purpose, much
of the HTML-information was redundant anyway.
Consequently we removed most of the HTML-
structures, and the table-cells, list-items and other
similar elements were converted into sentences.
Apart from sentence-information, only the section-
title information was maintained. Example data is
shown in Table 2.
3.1 Task-related information
Despite the Qualifications and other measures taken
in the collection phase of the corpus, we believe the
quality of the data remains open to question. How-
ever, the Mechanical Turk framework provided addi-
tional information for each assignment, for example
the time workers spent on the task. We believe this
information can be used to analyse and use our data
better, and have included it in the corpus to be used
in further experiments.
? Worker Id Within the MTurk framework, each
worker is assigned a unique id. Worker id can
be used to assign a reliability-value to the work-
ers, based on the quality of their previous work.
It was also used to examine whether the same
workers worked on the same data in different
phases: Of the original questions, only 7 were
answered and 4 other rephrased by the same
worker they were created by. However, it has
to be acknowledged that it is also possible for
one worker to have had several accounts in the
system, and thus be working under several dif-
ferent worker ids.
? Time On Task The MTurk framework also
provides the requester the time it took for the
worker to complete the assignment after ac-
cepting it. This information is also included in
the corpus, although it is impossible to know
precisely how much time the workers actually
spent on each task. For instance, it is possible
that one worker had several assignments open
at the same time, or that they were not concen-
trating fully on working on the task. A high
value of Time On Task thus does not necessar-
ily mean that the worker actually spent a long
time on it. However, a low value indicates that
he/she did only spend a short time on it.
? Reward Over the period spent collecting the
data, we changed the reward a couple of times
to speed up the process. The reward is reported
per HIT.
? Approval Status Within the collection pro-
cess we encountered some clearly unacceptable
work, and rejected it. The rejected work is also
included in the corpus, but marked as rejected.
The screening process was by no means per-
fect, and it is probable that some of the ap-
proved work should have been rejected.
? HIT id, Assignment id, Upload Time HIT and
assignment ids and original upload times of the
HITs are provided to make it possible to retrace
the collection steps if needed.
? Completion Time Completion time is the
timestamp of the moment when the task was
completed by a worker and returned to the sys-
tem. The time between the completion time
and the upload time is presumably highly de-
pendent on the reward, and on the appeal of the
task in question.
3.2 Quality experiments
As an example of the post-processing of the data,
we conducted some preliminary experiments on the
answer agreement between workers.
448
Out of the 695 questions, 159 were filtered out in
the first part of QAHIT. We first uploaded only 3 as-
signments, and the questions that 2 out of 3 work-
ers deemed unanswerable were filtered out. This
left 536 questions which were considered answered,
each one having 8-10 answers from different work-
ers. Even though in the majority of cases (83% of the
questions) one of the workers replied with the NoA,
the ones that answered did agree up to a point: of
all the answers, 72% were such that all of their sen-
tences were selected by at least two different work-
ers. On top of this, an additional 17% of answers
shared at least one sentence that was selected by
more than one worker.
To understand the agreement better, we also cal-
culated the average agreement of selected sentences
based on sentence ids and N-gram overlaps between
the answers. In both of these experiments, only
those 536 questions that were considered answer-
able were included.
3.2.1 Answer agreement on sentence ids
As the questions were answered by means of sen-
tence selection, the simplest method to check the
agreement between the workers was to compare
the ids of the selected sentences. The agreement
was calculated as follows: each answer was com-
pared to all the other answers for the same ques-
tion. For each case, the agreement was defined as
Agreement = CommonIdsAllIds , where CommonIds
is the number of sentence ids that existed in both
answers, and AllIds is the number of different ids
in both answers. We calculated the overall average
agreement ratio (Total Avg) and the average of the
best matches between two assignments within one
HIT (Best Match). We ran the test for two data sets:
The most typical case of the workers cheating was
to mark the question unaswerable. Because of this
the first data set included only the real answers, and
the NoAs were removed (NoA not included, 3872
answers). If an answer was compared with a NoA,
the agreement was 0, and if two NoAs were com-
pared, the agreement was 1. We did, however, also
include the figures for the whole data set (NoA in-
cluded, 4638 answers). The results are shown in Ta-
ble 3.
The Best Match -results were quite high com-
pared to the Total Avg. From this we can conclude
Total Avg Best Match
NoA not included 0.39 0.68
NoA included 0.34 0.68
Table 3: Answer agreement based on sentence ids.
that in the majority of cases, there was at least one
quite similar answer among those for that HIT. How-
ever, comparing the sentence ids is only an indica-
tive measure, and it does not tell the whole story
about agreement. For each document there may ex-
ist several separate sentences that contain the same
kind of information, and so two answers can be alike
even though the sentence ids do not match.
3.2.2 Answer agreement based on ROUGE
Defining the agreement over several passages of
texts has for a long time been a research prob-
lem within the field of automatic summarisation.
For each document it is possible to create several
summarisations that can each be considered cor-
rect. The problem has been approached by using
the ROUGE-metric: calculating the N-gram over-
lap between manual, ?correct? summaries, and the
automatic summaries. ROUGE has been proven to
correlate well with human evaluation (Lin and Hovy,
2003).
Overlaps of higher order N-grams are more usable
within speech summarisation as they take the gram-
matical structure and fluency of the summary into
account. When selecting sentences, this is not an is-
sue, so we decided to use only unigram and bigram
counts (Table 4: R-1, R2), as well as the skip-bigram
values (R-SU) and the longest common N-gram met-
ric R-L. We calculated the figures for two data sets
in the same way as in the case of sentence id agree-
ment. Finally, we set a lower bound for the results
by comparing the answers to each other randomly
(the NoAs were also included).
The final F-measures of the ROUGE results are
presented in Table 4. The figures vary from 0.37 to
0.56 for the first data set, and from 0.28 to 0.42 to
the second. It is debatable how the results should
be interpreted, as we have not defined a theoretical
upper bound to the values, but the difference to the
randomised results is substantial. In the field of au-
tomatic summarisation, the overlap of the automatic
449
results and corresponding manual summarisations is
generally much lower than the overlap between our
answers (Chali and Kolla, 2004). However, it is dif-
ficult to draw detailed conclusions based on compar-
ison between these two very different tasks.
R-1 R-2 R-SU R-L
NoA not included 0.56 0.46 0.37 0.52
NoA included 0.42 0.35 0.28 0.39
Random Answers 0.13 0.01 0.02 0.09
Table 4: Answer agreement: ROUGE-1, -2, -SU and -L.
The sentence agreement and ROUGE-figures do
not tell us much by themselves. However, they are
an example of a procedure that can be used to post-
process the data and in further projects of similar
nature. For example, the ROUGE similarity could
be used in the data collection phase as a tool of au-
tomatic approval and rejection of workers? assign-
ments.
4 Discussion and future work
During the initial trials of data collection we encoun-
tered some unexpected phenomena. For example,
increasing the reward did have a positive effect in
reducing the time it took for HITs to be completed,
however it did not correlate in desirable way with
data quality. Indeed the quality actually decreased
with increasing reward. We believe that this unex-
pected result is due to the distributed nature of the
worker pool in Mechanical Turk. Clearly the moti-
vation of some workers is other than monetary re-
ward. Especially if the HIT is interesting and can
be completed in a short period of time, it seems that
there are people willing to work on them even for
free.
MTurk requesters cannot however rely on this
voluntary workforce. From MTurk Forums it is clear
that some of the workers rely on the money they
get from completing the HITs. There seems to be a
critical reward-threshold after which the ?real work-
force?, i.e. workers who are mainly interested in per-
forming the HITs as fast as possible, starts to partic-
ipate. When the motivation changes from voluntary
participation to maximising the monetary gain, the
quality of the obtained results often understandably
suffers.
It would be ideal if a requester could rely on the
voluntary workforce alone for results, but in many
cases this may result either in too few workers and/or
too slow a rate of data acquisition. Therefore it is of-
ten necessary to raise the reward and rely on efficient
automatic validation of the data.
We have looked into the answer agreement of
the workers as an experimental post-processing step.
We believe that further work in this area will provide
the tools required for automatic data quality control.
5 Conclusions
In this paper we have described a dynamic and inex-
pensive method of collecting a corpus of questions
and answers using the Amazon Mechanical Turk
framework. We have provided to the community
a corpus of questions, answers and corresponding
documents, that we believe can be used in the de-
velopment of QA-systems for why-questions. We
propose that combining several answers from dif-
ferent people is an important factor in defining the
?correct? answer to a why-question, and to that goal
have included several answers for each question in
the corpus.
We have also included data that we believe is
valuable in post-processing the data: the work his-
tory of a single worker, the time spent on tasks, and
the agreement on a single HIT between a set of dif-
ferent workers. We believe that this information, es-
pecially the answer agreement of workers, can be
successfully used in post-processing and analysing
the data, as well as automatically accepting and re-
jecting workers? submissions in similar future data
collection exercises.
Acknowledgments
This study was funded by the Monbusho Scholar-
ship of Japanese Government and the 21st Century
COE Program ?Framework for Systematization and
Application of Large-scale Knowledge Resources
(COE-LKR)?
References
Yllias Chali and Maheedhar Kolla. 2004. Summariza-
tion Techniques at DUC 2004. In DUC2004.
Hoa Trang Dang, Diane Kelly, and Jimmy Lin. 2007.
Overview of the TREC 2007 Question Answering
450
Track. In E. Voorhees and L. P. Buckland, editors, Six-
teenth Text REtrieval Conference (TREC), Gaithers-
burg, Maryland, November.
Ludovic Denoyer and Patrick Gallinari. 2006. The
Wikipedia XML Corpus. SIGIR Forum.
Junichi Fukumoto, Tsuneaki Kato, Fumito Masui, and
Tsunenori Mori. 2007. An Overview of the 4th Ques-
tion Answering Challenge (QAC-4) at NTCIR work-
shop 6. In Proceedings of the Sixth NTCIR Workshop
Meeting, pages 433?440.
Chiori Hori, Takaaki Hori, and Sadaoki Furui. 2003.
Evaluation Methods for Automatic Speech Summa-
rization. In In Proc. EUROSPEECH, volume 4, pages
2825?2828, Geneva, Switzerland.
Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving
Answers from Frequently Asked Questions Pages on
the Web. In CIKM ?05: Proceedings of the 14th ACM
international conference on Information and knowl-
edge management, pages 76?83, New York, NY, USA.
ACM Press.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic Eval-
uation of Summaries Using N-gram Co-occurrence
Statistics. In Human Technology Conference (HLT-
NAACL), Edmonton, Canada.
Jimmy Lin and Boris Katz. 2006. Building a Reusable
Test Collection for Question Answering. J. Am. Soc.
Inf. Sci. Technol., 57(7):851?861.
Junta Mizuno, Tomoyosi Akiba, Atsushi Fujii, and
Katunobu Itou. 2007. Non-factoid Question Answer-
ing Experiments at NTCIR-6: Towards Answer Type
Detection for Realworld Questions. In Proceedings of
the 6th NTCIR Workshop Meeting on Evaluation of In-
formation Access Technologies, pages 487?492.
Radu Soricut and Eric Brill. 2006. Automatic Question
Answering Using the Web: Beyond the Factoid. Inf.
Retr., 9(2):191?206.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-
Arno Coppen. 2006. Data for Question Answering:
the Case of Why. In LREC.
Susan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-
Arno Coppen. 2007. Discourse-based Answer-
ing of Why-questions. Traitement Automatique des
Langues, 47(2: Discours et document: traitements
automatiques):21?41.
451
