Coling 2008: Companion volume ? Posters and Demonstrations, pages 173?176
Manchester, August 2008
Entailment-based Question Answering  
for Structured Data 
Bogdan Sacaleanu?, Constantin Orasan?, Christian Spurk?, Shiyan 
Ou?, Oscar Ferrandez?, Milen Kouylekov? and Matteo Negri?  
?LT-Lab, DFKI GmbH / Saarbr?cken, Germany 
?RIILP, University of Wolverhampton / Wolverhampton, UK 
?Fondazione Bruno Kessler (FBK) / Trento, Italy 
?University of Alicante / Alicante, Spain 
 
 
Abstract  
This paper describes a Question Answer-
ing system which retrieves answers from 
structured data regarding cinemas and 
movies. The system represents the first 
prototype of a multilingual and multi-
modal QA system for the domain of tour-
ism. Based on specially designed domain 
ontology and using Textual Entailment as 
a means for semantic inference, the sys-
tem can be used in both monolingual and 
cross-language settings with slight ad-
justments for new input languages. 
1 Introduction 
Question Answering over structured data has 
been traditionally addressed through a deep 
analysis of the question in order to reconstruct a 
logical form, which is then translated in the query 
language of the target data (Androutsopoulos et 
al, 1995, Popescu et al 2003). This approach im-
plies a complex mapping between linguistic ob-
jects (e.g. lexical items, syntactic structures) and 
against data objects (e.g. concepts and relations 
in a knowledge base). Unfortunately, such a 
mapping requires extensive manual work, which 
in many cases represents a bottleneck preventing 
the realization of large scale and portable natural 
language interfaces to structured data.  
This paper presents the first prototype of a 
question answering system which can answer 
questions in several languages about movies and 
cinema using a multilingual ontology and textual 
                                                 
 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
entailment. The remainder of the paper is struc-
tured as follows: Section 2 presents the concept 
of entailment-based question answering; Section 
3 describes our prototype which implements this 
concept; A brief evaluation is presented in Sec-
tion 4, followed by conclusions in Section 5. 
2 Entailment-based QA 
Recently Textual Entailment (TE) has been pro-
posed as a unifying framework for applied se-
mantics (Dagan and Glickman, 2004), where the 
need for an explicit representation of a mapping 
between linguistic objects and data objects can 
be, at least partially, bypassed through the defini-
tion of semantic inferences at a textual level. In 
this framework, a text (T) is said to entail a hy-
pothesis (H) if the meaning of H can be derived 
from the meaning of T. 
On the basis of the TE framework, the QA 
problem can be recast as an entailment problem, 
where the text (T) is the question (or its affirma-
tive version) and the hypothesis (H) is a rela-
tional answer pattern, which is associated to in-
structions for retrieving the answer to the input 
question. In this framework, given a question Q 
and a set of relational answer patterns P, a QA 
system needs to select those patterns in P that are 
entailed by Q. Instructions associated to answer 
patterns may be viewed as high precision proce-
dures for answer extraction, which are dependent 
on the specific source which is asked for. In case 
of QA over structured data, instructions could be 
queries to a database; whilst in case of QA on the 
Web, an instruction could be the URL of a Web 
page containing the answer to a question or some 
form of IR query to a search engine. 
Therefore, the underlying idea of an entail-
ment-based QA system is to match the user?s re-
quest to a set of predefined question patterns in 
order to get some kind of analysis for the request. 
173
As an example consider the question ?Where 
can I watch the movie ?Dreamgirls? next Satur-
day?? and the predefined question patterns: 
? Which movies are currently running in 
[CINEMA]?  EAT = [MOVIE] 
? Where can I watch the movie [MOVIE] 
on [WEEKDAY]?  EAT = [CINEMA] 
? Where can I see [MOVIE]? 
 EAT = [CINEMA] 
In the example, each of the patterns contains 
placeholders for relevant named entities and has 
an expected answer type (EAT) associated with 
it. The entailment-based QA system should re-
turn that pattern (2) is entailed by the question 
and as a result the retrieval instructions associ-
ated to it will be used to answer the question. 
3 Description of system 
Our question answering system implements the 
concept of entailment-based question answering 
described in the previous section. The overall 
structure of our system is presented in Figure 1.  
Given a question asked by a user of the system 
in a known location, the QA planner forwards it 
to the Instance Annotator in order to find any 
concepts that might be related to the targeted 
domain (i.e. cinema, city, movie). The result is 
then analyzed by the Relation Matcher, which on 
the basis of entailment can either select the most 
appropriate interpretation of the question and im-
plicitly its associated procedure of answering the 
question, or decide that the user request is out-of-
coverage if no such interpretation is available. 
The cross-linguality of our system and, to a 
certain extent, the interaction between its compo-
nents is ensured by a domain ontology which is 
used for all four languages involved in the pro-
ject: English, German, Italian and Spanish, and 
its modules (Ou et al, 2008). Concepts from the 
ontology are used to annotate the user questions 
as well as data from which the answer is ex-
tracted. In the current stage of the project, the 
answers are contained in databases obtained from 
content provides or built from structured web 
pages. As a result, the information in the database 
tables was annotated with concepts from the on-
tology and then converted into an RDF graph to 
Figure 1. System Architecture 
174
facilitate retrieval using SPARQL query lan-
guage (Prud'hommeaux and Seaborne, 2006). 
Question patterns corresponding to one or several 
ontological relations were produced after ques-
tions for users were collected and used in the en-
tailment module. The question patterns used by 
the system are very similar to those presented in 
the previous section and contain placeholders for 
the actual entities that are expected to appear in a 
question. 
The SPARQL query associated with a pattern 
selected for a user question is used to retrieve the 
answers from the knowledge base and prepare for 
presentation. Given that our system is not limited 
to returning only textual information, further 
processing can be applied to the retrieved data. 
For example, for proximity questions the list of 
answers consists of cinema names and their GPS-
coordinates, which are used by the Answer Sort-
ing component to reorder the list of answers on 
the basis of their distance to the user?s location. 
Besides presenting the possible answers to a 
given question, the system can offer additional 
information based on the answer?s type: 
? a map for answers that are location 
names, 
? a route description for answers that are 
cinema names, 
? a video-trailer for answers that are movie 
names and 
? an image for answers that are person 
names. 
Due to the fact that a common semantics is 
shared by all four languages by way of a domain 
ontology, the system can be used not only in a 
monolingual setting, but also in a cross-language 
setting. This corresponds to a user-scenario 
where a tourist asks for information in their own 
language in a foreign location (i.e. English 
speaker in Italy). The only difference between 
monolingual and cross-language scenarios is that 
in the cross-language setting, the QA Core sub-
system (Figure 1) selects a Find Entailed Rela-
tion component according to the user input?s lan-
guage. This is due to the entailment algorithms 
that tend to use language specific resources in 
order to attain high accuracy results of matching 
the user request with one of the lexicalized rela-
tions (patterns). It is only the entailment compo-
nent that has to be provided in order to adapt the 
system to new input languages, once the lexical-
ized relations have been translated either manual 
or automatically. 
Both the Instance Annotator and the Answer 
Retriever are language independent, but location 
dependent (Figure 2). The Answer Retriever de-
pends on the location since it is querying data 
found at that place (i.e. Italy), while the Instance 
Annotator looks up instances of the data in the 
user?s question (i.e. annotates an English ques-
tion). They are language independent since they 
are working with data abstractions like SPARQL 
queries (Answer Retriever) or work at character 
level and do not consider language specific as-
pects, like words, in their look-up process (In-
stance Annotator). 
The current version of the system1 is designed 
according to the SOA (Service Oriented Archi-
tecture) and is implemented as point-to-point in-
tegrated web services. Any of the system?s com-
ponents can be substituted by alternative imple-
mentations with no need for further changes as 
long as the functionality remains the same. 
                                                 
1
 http://attila.dfki.uni-sb.de:8282/ QallMe_Proto-
type_WEB_Update/faces/Page6.jsp 
Figure 2. Cross-language Setting 
175
4 Evaluation 
A preliminary evaluation of the first prototype 
was carried out on randomly selected questions 
from a benchmark specifically designed for the 
project. This benchmark was developed to con-
tain questions about various aspects from the 
domain of tourism and for this reason we filtered 
out questions not relevant to cinema or movies. 
The evaluation of the system did not assess 
whether it can extract the correct answer. Instead, 
it measured to what extent the system can select 
the right SPARQL pattern. The explanation for 
this can be found in the fact that once a correct 
question pattern is selected, the extraction of the 
answer requires only retrieval of the answer from 
the database. Moreover, it should be pointed out 
that the main purpose of this preliminary evalua-
tion was to test the interaction between compo-
nents and indicate potential problems, and it was 
less about their performances.  
Table 1 summarises the results of the evalua-
tion. The number of questions used in the evalua-
tion is different from one language to another. 
This can be explained by the fact that for each 
language a number of questions (in general 500) 
was randomly selected from the benchmark and 
only the ones which referred to cinema or movies 
were selected. The column Questions indicates 
the number of questions assessed. The Correct 
column indicates for how many questions a cor-
rect SPARQL was generated. The Wrong column 
corresponds to the number of questions where a 
wrong or incomplete SPARQL was generated. 
This number also includes cases where no 
SPARQL was generated due to lack of corre-
sponding answer pattern. 
 
 Questions Correct Wrong 
English 167 74 (44.31%) 93 (55.68%) 
German 214 120 (56.04%) 94 (43.92%) 
Spanish 58 50 (86.20%) 8 (13.79%) 
Italian 99 46 (46.46%) 53 (53.53%) 
Table 1: Evaluation results 
As can be seen, the results are very different 
from one language to another. This can be ex-
plained by the fact that different entailment en-
gines are used for each language. In addition, 
even though the benchmark was built using a 
common set of guidelines, the complexity of 
questions varies from one language to another. 
For this reason, for some questions it is more dif-
ficult to find the correct pattern than for others.  
Analysis of the results revealed that one of the 
easiest ways to improve the performance of the 
system is to increase the number of patterns. Cur-
rently the average number of patterns per lan-
guage is 42. Improvement of the entailment en-
gines is another direction which needs to be pur-
sued. Most of the partners involved in the project 
have more powerful entailment engines than 
those integrated in the prototype which were 
ranked highly in RTE competitions. Unfortu-
nately, many of these engines cannot be used di-
rectly in our system due to their slow speed. Our 
system is supposed to give users results in real 
time which imposes some constraints on the 
amount of processing that can be done. 
5 Conclusions 
This paper presented the first prototype of an 
entailment-based QA system, which can answer 
questions about movies and cinema. The use of a 
domain ontology ensures that the system is cross-
language and can be extended to new languages 
with slight adjustments at the entailment engine. 
The system is implemented as a set of web ser-
vices and along a Service Oriented Architecture. 
6 Acknowledgements 
This work is supported by the EU-funded pro-
ject QALL-ME (FP6 IST-033860).  
References 
Androutsopoulos, I. and G.D. Ritchie and P. Thanisch. 
1995. Natural Language Interfaces to Databases -- 
An Introduction, Journal of Natural Language En-
gineering, vol.1, no.1, Cambridge University Press. 
Popescu Ana-Marie, Oren Etzioni, and Henry Kautz. 
2003. Towards a theory of natural language inter-
faces to databases. In Proceedings of the confer-
ence on Intelligent User Interfaces. 
Dagan Ido and Oren Glickman. 2004. Probabilistic 
textual entailment: Generic applied modeling of 
language variability. In PASCAL Workshop on 
Learning Methods for Text Understanding and 
Mining, Grenoble. 
Ou Shiyan, Viktor Pekar, Constantin Orasan, Chris-
tian Spurk, Matteo Negri. 2008. Development and 
alignment of a domain-specific ontology for ques-
tion answering. In Proceedings of the 6th Edition of 
the Language Resources and Evaluation Confer-
ence (LREC-08).  
Prud'hommeaux Eric, Andy Seaborne (eds.). 2006. 
SPARQL Query Language for RDF. RDF Data Ac-
cess Working Group. 
176
Is It the Right Answer?
Exploiting Web Redundancy for Answer Validation
Bernardo Magnini, Matteo Negri, Roberto Prevete and Hristo Tanev
ITC-Irst, Centro per la Ricerca Scientifica e Tecnologica
[magnini,negri,prevete,tanev]@itc.it
Abstract
Answer Validation is an emerging topic
in Question Answering, where open do-
main systems are often required to rank
huge amounts of candidate answers. We
present a novel approach to answer valida-
tion based on the intuition that the amount
of implicit knowledge which connects an
answer to a question can be quantitatively
estimated by exploiting the redundancy of
Web information. Experiments carried out
on the TREC-2001 judged-answer collec-
tion show that the approach achieves a
high level of performance (i.e. 81% suc-
cess rate). The simplicity and the effi-
ciency of this approach make it suitable to
be used as a module in Question Answer-
ing systems.
1 Introduction
Open domain question-answering (QA) systems
search for answers to a natural language question
either on the Web or in a local document collec-
tion. Different techniques, varying from surface pat-
terns (Subbotin and Subbotin, 2001) to deep seman-
tic analysis (Zajac, 2001), are used to extract the text
fragments containing candidate answers. Several
systems apply answer validation techniques with the
goal of filtering out improper candidates by check-
ing how adequate a candidate answer is with re-
spect to a given question. These approaches rely
on discovering semantic relations between the ques-
tion and the answer. As an example, (Harabagiu
and Maiorano, 1999) describes answer validation as
an abductive inference process, where an answer is
valid with respect to a question if an explanation for
it, based on background knowledge, can be found.
Although theoretically well motivated, the use of se-
mantic techniques on open domain tasks is quite ex-
pensive both in terms of the involved linguistic re-
sources and in terms of computational complexity,
thus motivating a research on alternative solutions
to the problem.
This paper presents a novel approach to answer
validation based on the intuition that the amount of
implicit knowledge which connects an answer to a
question can be quantitatively estimated by exploit-
ing the redundancy of Web information. The hy-
pothesis is that the number of documents that can
be retrieved from the Web in which the question and
the answer co-occur can be considered a significant
clue of the validity of the answer. Documents are
searched in the Web by means of validation pat-
terns, which are derived from a linguistic process-
ing of the question and the answer. In order to test
this idea a system for automatic answer validation
has been implemented and a number of experiments
have been carried out on questions and answers pro-
vided by the TREC-2001 participants. The advan-
tages of this approach are its simplicity on the one
hand and its efficiency on the other.
Automatic techniques for answer validation are
of great interest for the development of open do-
main QA systems. The availability of a completely
automatic evaluation procedure makes it feasible
QA systems based on generate and test approaches.
In this way, until a given answer is automatically
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 425-432.
                         Proceedings of the 40th Annual Meeting of the Association for
proved to be correct for a question, the system will
carry out different refinements of its searching crite-
ria checking the relevance of new candidate answers.
In addition, given that most of the QA systems rely
on complex architectures and the evaluation of their
performances requires a huge amount of work, the
automatic assessment of the relevance of an answer
with respect to a given question will speed up both
algorithm refinement and testing.
The paper is organized as follows. Section 2
presents the main features of the approach. Section 3
describes how validation patterns are extracted from
a question-answer pair by means of specific question
answering techniques. Section 4 explains the basic
algorithm for estimating the answer validity score.
Section 5 gives the results of a number of experi-
ments and discusses them. Finally, Section 6 puts
our approach in the context of related works.
2 Overall Methodology
Given a question   and a candidate answer  the an-
swer validation task is defined as the capability to as-
sess the relevance of  with respect to   . We assume
open domain questions and that both answers and
questions are texts composed of few tokens (usually
less than 100). This is compatible with the TREC-
2001 data, that will be used as examples throughout
this paper. We also assume the availability of the
Web, considered to be the largest open domain text
corpus containing information about almost all the
different areas of the human knowledge.
The intuition underlying our approach to an-
swer validation is that, given a question-answer pair
([   ,  ]), it is possible to formulate a set of valida-
tion statements whose truthfulness is equivalent to
the degree of relevance of  with respect to   . For
instance, given the question ?What is the capital of
the USA??, the problem of validating the answer
?Washington? is equivalent to estimating the truth-
fulness of the validation statement ?The capital of
the USA is Washington?. Therefore, the answer
validation task could be reformulated as a problem
of statement reliability. There are two issues to be
addressed in order to make this intuition effective.
First, the idea of a validation statement is still insuf-
ficient to catch the richness of implicit knowledge
that may connect an answer to a question: we will
attack this problem defining the more flexible idea
of a validation pattern. Second, we have to design
an effective and efficient way to check the reliability
of a validation pattern: our solution relies on a pro-
cedure based on a statistical count of Web searches.
Answers may occur in text passages with low
similarity with respect to the question. Passages
telling facts may use different syntactic construc-
tions, sometimes are spread in more than one sen-
tence, may reflect opinions and personal attitudes,
and often use ellipsis and anaphora. For instance, if
the validation statement is ?The capital of USA is
Washington?, we have Web documents containing
passages like those reported in Table 1, which can
not be found with a simple search of the statement,
but that nevertheless contain a significant amount of
knowledge about the relations between the question
and the answer. We will refer to these text fragments
as validation fragments.
1. Capital Region USA: Fly-Drive Holidays in
and Around Washington D.C.
2. the Insider?s Guide to the Capital Area Music
Scene (Washington D.C., USA).
3. The Capital Tangueros (Washington, DC
Area, USA)
4. I live in the Nation?s Capital, Washington
Metropolitan Area (USA).
5. in 1790 Capital (also USA?s capital): Wash-
ington D.C. Area: 179 square km
Table 1: Web search for validation fragments
A common feature in the above examples is the
co-occurrence of a certain subset of words (i.e.
?capital?,?USA? and ?Washington?). We will make
use of validation patterns that cover a larger portion
of text fragments, including those lexically similar
to the question and the answer (e.g. fragments 4 and
5 in Table 1) and also those that are not similar (e.g.
fragment 2 in Table 1). In the case of our example
a set of validation statements can be generalized by
the validation pattern:
[capital  text  USA  text  Washington]
where  text  is a place holder for any portion of
text with a fixed maximal length.
To check the correctness of  with respect to  
we propose a procedure that measures the number
of occurrences on the Web of a validation pattern
derived from  and   . A useful feature of such pat-
terns is that when we search for them on the Web
they usually produce many hits, thus making statis-
tical approaches applicable. In contrast, searching
for strict validation statements generally results in a
small number of documents (if any) and makes sta-
tistical methods irrelevant. A number of techniques
used for finding collocations and co-occurrences of
words, such as mutual information, may well be
used to search co-occurrence tendency between the
question and the candidate answer in the Web. If we
verify that such tendency is statistically significant
we may consider the validation pattern as consistent
and therefore we may assume a high level of correla-
tion between the question and the candidate answer.
Starting from the above considerations and given
a question-answer pair     , we propose an answer
validation procedure based on the following steps:
1. Compute the set of representative keywords
	
  and 	  both from   and from  ; this step is
carried out using linguistic techniques, such as
answer type identification (from the question)
and named entities recognition (from the an-
swer);
2. From the extracted keywords compute the vali-
dation pattern for the pair [    ];
3. Submit the patterns to the Web and estimate an
answer validity score considering the number
of retrieved documents.
3 Extracting Validation Patterns
In our approach a validation pattern consists of two
components: a question sub-pattern (Qsp) and an
answer sub-pattern (Asp).
Building the Qsp. A Qsp is derived from the input
question cutting off non-content words with a stop-
words filter. The remaining words are expanded
with both synonyms and morphological forms in
order to maximize the recall of retrieved docu-
ments. Synonyms are automatically extracted from
the most frequent sense of the word in WordNet
(Fellbaum, 1998), which considerably reduces the
risk of adding disturbing elements. As for morphol-
ogy, verbs are expanded with all their tense forms
(i.e. present, present continuous, past tense and past
participle). Synonyms and morphological forms are
added to the Qsp and composed in an OR clause.
The following example illustrates how the Qsp
is constructed. Given the TREC-2001 question
?When did Elvis Presley die??, the stop-words filter
removes ?When? and ?did? from the input. Then
synonyms of the first sense of ?die? (i.e. ?decease?,
?perish?, etc.) are extracted from WordNet. Finally,
morphological forms for all the corresponding verb
tenses are added to the Qsp. The resultant Qsp will
be the following:
[Elvis  text  Presley  text  (die OR died OR
dying OR perish OR ...)]
Building the Asp. An Asp is constructed in two
steps. First, the answer type of the question is iden-
tified considering both morpho-syntactic (a part of
speech tagger is used to process the question) and
semantic features (by means of semantic predicates
defined on the WordNet taxonomy; see (Magnini et
al., 2001) for details). Possible answer types are:
DATE, MEASURE, PERSON, LOCATION, ORGANI-
ZATION, DEFINITION and GENERIC. DEFINITION
is the answer type peculiar to questions like ?What
is an atom?? which represent a considerable part
(around 25%) of the TREC-2001 corpus. The an-
swer type GENERIC is used for non definition ques-
tions asking for entities that can not be classified as
named entities (e.g. the questions: ?Material called
linen is made from what plant?? or ?What mineral
helps prevent osteoporosis??)
In the second step, a rule-based named entities
recognition module identifies in the answer string
all the named entities matching the answer type cat-
egory. If the category corresponds to a named en-
tity, an Asp for each selected named entity is cre-
ated. If the answer type category is either DEFINI-
TION or GENERIC, the entire answer string except
the stop-words is considered. In addition, in order
to maximize the recall of retrieved documents, the
Asp is expanded with verb tenses. The following
example shows how the Asp is created. Given the
TREC question ?When did Elvis Presley die?? and
the candidate answer ?though died in 1977 of course
some fans maintain?, since the answer type category
is DATE the named entities recognition module will
select [1977] as an answer sub-pattern.
4 Estimating Answer Validity
The answer validation algorithm queries the Web
with the patterns created from the question and an-
swer and after that estimates the consistency of the
patterns.
4.1 Querying the Web
We use a Web-mining algorithm that considers the
number of pages retrieved by the search engine. In
contrast, qualitative approaches to Web mining (e.g.
(Brill et al, 2001)) analyze the document content,
as a result considering only a relatively small num-
ber of pages. For information retrieval we used the
AltaVista search engine. Its advanced syntax allows
the use of operators that implement the idea of vali-
dation patterns introduced in Section 2. Queries are
composed using NEAR, OR and AND boolean opera-
tors. The NEAR operator searches pages where two
words appear in a distance of no more than 10 to-
kens: it is used to put together the question and the
answer sub-patterns in a single validation pattern.
The OR operator introduces variations in the word
order and verb forms. Finally, the AND operator is
used as an alternative to NEAR, allowing more dis-
tance among pattern elements.
If the question sub-pattern 
 does not return
any document or returns less than a certain thresh-
old (experimentally set to 7) the question pattern
is relaxed by cutting one word; in this way a new
query is formulated and submitted to the search en-
gine. This is repeated until no more words can be
cut or the returned number of documents becomes
higher than the threshold. Pattern relaxation is per-
formed using word-ignoring rules in a specified or-
der. Such rules, for instance, ignore the focus of the
question, because it is unlikely that it occurs in a
validation fragment; ignore adverbs and adjectives,
because are less significant; ignore nouns belonging
to the WordNet classes ?abstraction?, ?psychologi-
cal feature? or ?group?, because usually they specify
finer details and human attitudes. Names, numbers
and measures are preferred over all the lower-case
words and are cut last.
4.2 Estimating pattern consistency
The Web-mining module submits three searches to
the search engine: the sub-patterns [Qsp] and [Asp]
and the validation pattern [QAp], this last built as
the composition [Qsp NEAR Asp]. The search en-
gine returns respectively: 
 ,    
		 Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 30?37,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Evaluating Knowledge-based Approaches to the Multilingual Extension of
a Temporal Expression Normalizer
Matteo Negri
ITC-irst
Povo - Trento, Italy
negri@itc.it
Estela Saquete, Patricio Mart??nez-Barco, Rafael Mun?oz
DLSI, University of Alicante
Alicante, Spain
{stela,patricio,rafael}@dlsi.ua.es
Abstract
The extension to new languages is a well
known bottleneck for rule-based systems.
Considerable human effort, which typi-
cally consists in re-writing from scratch
huge amounts of rules, is in fact required
to transfer the knowledge available to the
system from one language to a new one.
Provided sufficient annotated data, ma-
chine learning algorithms allow to mini-
mize the costs of such knowledge trans-
fer but, up to date, proved to be ineffec-
tive for some specific tasks. Among these,
the recognition and normalization of tem-
poral expressions still remains out of their
reach. Focusing on this task, and still ad-
hering to the rule-based framework, this
paper presents a bunch of experiments on
the automatic porting to Italian of a system
originally developed for Spanish. Differ-
ent automatic rule translation strategies are
evaluated and discussed, providing a com-
prehensive overview of the challenge.
1 Introduction
In recent years, inspired by the success of MUC
evaluations, a growing number of initiatives (e.g.
TREC1, CLEF2, CoNLL3, Senseval4) have been
developed to boost research towards the automatic
understanding of textual data. Since 1999, the Au-
tomatic Content Extraction (ACE) program5 has
been contributing to broaden the varied scenario
of evaluation campaigns by proposing three main
1http://trec.nist.gov
2http://clef-campaign.org
3http://www.cnts.ua.ac.be/conll
4http://www.senseval.org
5http://www.nist.gov/speech/tests/ace
tasks, namely the recognition of entities, rela-
tions, and events. In 2004, the Timex2 Detec-
tion and Recognition task6 (also known as TERN,
for Time Expression Recognition and Normaliza-
tion) has been added to the ACE program, making
the whole evaluation exercise more complete. The
main goal of the task was to foster research on sys-
tems capable of automatically detecting temporal
expressions (TEs) present in an English text, and
normalizing them with respect to a specifically de-
fined annotation standard.
Within the above mentioned evaluation exer-
cises, the research activity on monolingual tasks
has gradually been complemented by a consid-
erable interest towards multilingual and cross-
language capabilities of NLP systems. This trend
confirms how portability across languages has
now become one of the key challenges for Natu-
ral Language Processing research, in the effort of
breaking the language barrier hampering systems?
application in many real use scenarios. In this di-
rection, machine learning techniques have become
the standard approach in many NLP areas. This
is motivated by several reasons, including i) the
fact that considerable amounts of annotated data,
indispensable to train ML-based algorithms, are
now available for many tasks, and ii) the difficulty,
inherent to rule-based approaches, of porting lan-
guage models from one language to new ones. In
fact, while supervised ML algorithms can be eas-
ily extended to new languages given an annotated
training corpus, rule-based approaches require to
redefine the set of rules, adapting them to each new
language. This is a time consuming and costly
work, as it usually consists in manually rewriting
from scratch huge amounts of rules.
6http://timex2.mitre.org
30
In spite of their effectiveness for some tasks,
ML techniques still fall short from providing ef-
fective solutions for others. This is confirmed by
the outcomes of the TERN 2004 evaluation, which
provide a clear picture of the situation. In spite
of the good results obtained in the TE recognition
task (Hacioglu et al, 2005), the normalization by
means of ML techniques has not been tackled yet,
and still remains an unresolved problem.
Considering the inadequacy of ML techniques
to deal with the normalization problem, and fo-
cusing on portability across languages, this pa-
per extends and completes the previous work pre-
sented in (Saquete et al, 2006b) and (Saquete et
al., 2006a). More specifically, we address the fol-
lowing crucial issue: how to minimize the costs
of building a rule-based TE recognition system
for a new language, given an already existing sys-
tem for another language. Our goal is to experi-
ment with different automatic porting procedures
to build temporal models for new languages, start-
ing from previously defined ones. Still adhering
to the rule-based paradigm, we analyse different
porting methodologies that automatically learn the
TE recognition model used by the system in one
language, adjusting the set of normalization rules
for the new target language.
In order to provide a clear and comprehen-
sive overview of the challenge, an incremental ap-
proach is proposed. Starting from the architecture
of an existing system developed for Spanish (Sa-
quete et al, 2005), we present a bunch of exper-
iments which take advantage of different knowl-
edge sources to build an homologous system for
Italian. Building on top of each other, such exper-
iments aim at incrementally analyzing the contri-
bution of additional information to attack the TE
normalization task. More specifically, the follow-
ing information will be considered:
? The output of online translators;
? The information mined from a manually an-
notated corpus;
? A combination of the two.
2 The task: TE recognition and
normalization
The TERN task consists in automatically detect-
ing, bracketing, and normalizing all the time ex-
pressions mentioned within an English text. The
recognized TEs are then annotated according to
the TIMEX2 annotation standard described in
(Ferro et al, 2005). Markable TEs include both
absolute (or explicit) expressions (e.g. ?April 15,
2006?), and relative (or anaphoric) expressions
(e.g. ?three years ago?). Also markable are du-
rations (e.g. ?two weeks?), event-anchored ex-
pressions (e.g. ?two days before departure?), and
sets of times (e.g. ?every week?). Detection and
bracketing concern systems? capability to recog-
nize TEs within an input text, and correctly deter-
mine their extension. Normalization concerns the
ability of the system to correctly assign, for each
detected TE, the correct values to the TIMEX2
normalization attributes. The meaning of these at-
tributes can be summarized as follows:
? VAL: contains the normalized value of a TE
(e.g. ?2004-05-06? for ?May 6th, 2004?)
? ANCHOR VAL: contains a normalized form
of an anchoring date-time.
? ANCHOR DIR: captures the relative
direction-orientation between VAL and
ANCHOR VAL.
? MOD: captures temporal modifiers (pos-
sible values include: ?approximately?,
?more than?, ?less than?)
? SET: identifies expressions denoting sets of
times (e.g. ?every year?).
2.1 The evaluation benchmark
Moving to a new language, an evaluation bench-
mark is necessary to test systems performances.
For this purpose, the temporal annotations of the
Italian Content Annotation Bank (I-CAB-temp7)
have been selected.
I-CAB consists of 525 news documents
taken from the Italian newspaper L?Adige
(http://www.adige.it), and contains around
182,500 words. Its 3,830 temporal expressions
(2,393 in the training part of the corpus, and 1,437
in the test part) have been manually annotated
following the TIMEX2 standard with some adap-
tations to the specific morpho-syntactic features
of Italian, which has a far richer morphology than
English (see (Magnini et al, 2006) for further
details).
7I-CAB is being developed as part of the three-year
project ONTOTEXT funded by the Provincia Autonoma di
Trento, Italy. See http://tcc.itc.it/projects/ontotext
31
3 The starting point: TERSEO
As a starting point for our experiments we used
TERSEO, a system originally developed for the
automatic annotation of TEs appearing in a Span-
ish written text in compliance with the TIMEX2
standard (see (Saquete, 2005) for a thorough de-
scription of TERSEO?s main features and func-
tionalities).
TEXT
POS 
TAGGER
RECOGNITION: 
PARSER
Lexical and
morphological
information
Temporal 
expression
recognition
DATE
ESTIMATION
Dictionary
Temporal
Expression
Grammar
TEMPORAL
EXPRESSION
NORMALIZATION
EVENT 
ORDERING
ORDERED
TEXT
Documental 
DataBase
Figure 1: System?s architecture.
Basically (see Figure 1), the TE recognition
and normalization process is carried out in two
phases. The first phase (recognition) includes a
pre-processing of the input text, which is tagged
with lexical and morphological information that
will be used as input to a temporal parser. The
temporal parser is implemented using an as-
cending technique (chart parser) and relies on a
language-specific temporal grammar. As TEs can
be divided into absolute and relative ones, such
grammar is tuned for discriminating between the
two groups. On the one hand, absolute TEs di-
rectly provide and fully describe a date. On the
other hand, relative TEs require some degree of
reasoning (as in the case of anaphora resolution).
In the second phase of the process, in order to
translate these expressions into their normalized
form, the lexical context in which they occur is
considered. At this stage, a normalization unit
is in charge of determining the appropriate refer-
ence date (anchor) associated to each anaphoric
TE, calculating its value, and finally generating the
corresponding TIMEX2 tag.
?From a multilingual perspective, an impor-
tant feature of TERSEO is the distinction between
recognition rules, which are language-specific,
and normalization rules, which are language-
independent and potentially reusable for any other
language. Taking the most from the modular ar-
chitecture of the system, a first multilingual exten-
sion has been evaluated over the English TERN
2004 test set. In that extension, the English
temporal model was automatically obtained from
the Spanish one, through the automatic transla-
tion into English8 of the Spanish TEs recognized
by the system (Saquete et al, 2004). The re-
sulting English TEs were then mapped onto the
corresponding language-independent normaliza-
tion rules, with good results (compared with other
participants to the competition) both in terms of
precision and recall. These results are shown in
Table 1.
Prec Rec F
timex2 0.673 0.728 0.699
anchor dir 0.658 0.877 0.752
anchor val 0.684 0.912 0.782
set 0.800 0.667 0.727
text 0.770 0.620 0.690
val 0.757 0.735 0.746
Table 1: Evaluation of English-TERSEO over the
TERN 2004 test set
The positive results of this experience demon-
strated the viability of the adopted solutions, and
motivate our further investigation with Italian as a
new target language.
4 Porting TERSEO to Italian
Due to the separation between language-specific
recognition rules and language-independent nor-
malization rules, the bulk of the porting process
relies on the adaptation of the recognition rules
to the new target language. Taking advantage of
different knowledge sources (either alone or in
combination), an incremental approach has been
adopted, in order to determine the contribution of
additional information on the performance of the
resulting system for Italian.
8Altavista Babel Fish Translation has been used for this
purpose (http://world.altavista.com).
32
4.1 Using online translators
As a first experiment, the same procedure adopted
for the extension to English has been followed.
This represents the simplest approach for porting
TERSEO to other languages, and will be consid-
ered as a baseline for comparison with the results
achieved in further experiments. The only minor
difference with respect to the original procedure
is that now, since two aligned sets of recognition
rules (i.e. for Spanish and for English) are avail-
able, both models have been used. The reason for
considering both models is the fact that they com-
plement each other: on the one hand, the Span-
ish model was obtained manually and showed high
precision values in detection (88%); on the other
hand, although the English model showed lower
precision results in detection (77%), the on-line
translators from English to Italian perform better
than translators from Spanish to Italian.
The process is carried out in the following four
steps.
1. Eng-Ita translation. All the English TEs
known by the system are translated into Ital-
ian9. Starting English, the probability of ob-
taining higher quality translations is maxi-
mized.
2. Spa-Ita translation. For each English TE
without an Italian translation, the correspond-
ing Spanish expression is translated into Ital-
ian. Also the Spanish TEs that do not have an
English equivalent are translated from Span-
ish10 into Italian. This way, the coverage
of the resulting model is maximized, becom-
ing comparable to the hand-crafted Spanish
model.
3. TE Filtering. A filtering module is used to
guarantee the correctness of the translations.
For this purpose, the translated expressions
are searched in the Web with Google. If an
expression is not found by Google it is given
up; otherwise it is considered as a valid Ital-
ian TE. The inconvenience of adopting this
simple filtering strategy occurs in case of am-
biguous expressions, i.e. when a correct ex-
pression is obtained through translation, and
9Also for English to Italian translation, Altavista Babel
Fish Translation has been used
10Using the Spanish-Italian translator available at
http://www.tranexp.com:2000/Translate/result.shtml
Google returns at least on document contain-
ing it, but the expression is not a tempo-
ral one. In these cases the system will er-
roneously store in its database non-temporal
expressions. In this experiment the results
returned by Google have not been analyzed
(only the number of hits has been taken into
account), nor the impact of these errors has
been estimated. A more precise analysis of
the output of the web search has been left as
a future improvement direction.
4. Normalization rules assignment. Finally,
the resulting Italian translations are mapped
onto the language-independent normalization
rules associated with the original English and
Spanish TEs.
The development of this first automatic porting
procedure required one person/week for software
implementation, and less than an hour to obtain
the new model for Italian. The performance of the
resulting system, evaluated over the test set of I-
CAB, is shown in table 2.
Prec Rec F
timex2 0.725 0.833 0.775
anchor dir 0.211 0.593 0.311
anchor val 0.203 0.571 0.300
set 0.152 1.000 0.263
text 0.217 0.249 0.232
val 0.364 0.351 0.357
Table 2: Porting to Italian based on translations
The results achieved by the translation-based
approach are controversial. On the one hand, we
observe a detection performance in line with the
English version of the system. The timex2 at-
tribute, which indicates the proportion of detected
TEs11, has even higher scores, both in terms of
precision (+5%) and recall (+11%), with respect
to the English system. On the other hand, both
bracketing (see the text attribute, which indicates
the quality of extent recognition) and normaliza-
tion (described by the other attributes) show a per-
formance drop. Unfortunately, the reasons of this
drop are still unclear. One possible explanation
is that, due to the intrinsic difficulties presented
by the Italian language, the translation-based ap-
proach falls short from providing an adequate cov-
erage of the many possible TE variants. While
11At least one overlapping character in the extent of the
reference and the system output is required for tag aligment.
33
the presence of lexical triggers denoting a TE ap-
pearing in a text (e.g. the Italian translations of
?years?, ?Monday?, ?afternoon?, ?yesterday?) can
be easily captured by this approach, the complex-
ity of many language-specific constructs is out of
its reach.
4.2 Using an annotated corpus
In a second experiment, the annotations of the
training portion of I-CAB have been used as a pri-
mary knowledge source. The main purpose of this
approach is to maximize the coverage of the Ital-
ian TEs, starting from language-specific knowl-
edge mined from the corpus. The basic hypothe-
sis is that a bottom-up porting methodology, led by
knowledge in the target language, is more effective
than the top-down approach based on knowledge
derived from models built for other languages.
The former, in fact, is in principle more suitable to
capture language-specific TE variations. In order
to test the validity of ths hypothesis, the following
two-step process has been set up:
1. TE Collection and translation. The Italian ex-
pressions are collected from the I-CAB train-
ing portion, and translated both into Spanish
and English.
2. Normalization rules assignment. Italian TEs
are assigned to the appropriate normalization
rules. For each Italian TE mined from the
corpus, the selection is done considering the
normalization rules assigned to its transla-
tions. If both the Spanish and English ex-
pressions are found in their respective mod-
els, and are associated with the same normal-
ization rule, then this rule is assigned also to
the Italian expression. Also, when only one
of the translated expressions is found in the
existing models, the normalization rule is as-
signed. In case of discrepancies, i.e. if both
expressions are found, but are not associated
to the same normalization rule, then one of
the languages must be prioritized. Since the
manually obtained Spanish model has shown
a higher precision, Spanish rules are pre-
ferred.
As the corpus-based approach is mostly built on
the same software used for the translation-based
porting procedure, it did not require additional
time for implementation. Also in this case, the
new model for Italian has been obtained in less
than one hour. Performance results calculated over
the I-CAB test set are reported in Table 3.
Prec Rec F
timex2 0.730 0.839 0.781
anchor dir 0.412 0.414 0.413
anchor val 0.339 0.340 0.339
set 0.030 1.000 0.059
text 0.222 0.255 0.238
val 0.285 0.274 0.279
Table 3: Porting based on corpus annotations
These results partially confirm our working hy-
pothesis, showing a performance increase in terms
of the Italian TEs correctly recognized by the sys-
tem. In fact, both the timex2 attribute, which
indicates the coverage of the system (detection),
and the text attribute, which refers to the TEs
extent determination (bracketing), are slightly in-
creased. This may lead to the conclusion that auto-
matic porting procedures can actually benefit from
language-specific knowledge derived from a cor-
pus.
However, looking at the other TIMEX2 at-
tributes, the situation is not so clear due to the less
coherent behaviour of the system on normaliza-
tion. While for two attributes (anchor dir and an-
chor val) the system performs better, for the other
two (set and val) a performance drop is observed.
A possible reason for that could be related to the
limited number of TE examples that can be ex-
tracted from the Italian corpus (whose dimensions
are relatively small compared to the annotated cor-
pora available for English). In fact, compared to
the sum of English and Spanish examples used for
the translation-based porting procedure, the Ital-
ian expressions present in the corpus are fewer and
repetitive. For instance, with 131, 140, and 30 oc-
currences, the expressions ?oggi? (?today?), ?ieri?
(?yesterday?), and ?domani? (?tomorrow?) repre-
sent around 12.5% of the 2,393 Italian TEs con-
tained in the I-CAB training set.
4.3 Combining online translators and an
annotated corpus
In light of the previous considerations, a third ex-
periment has been conducted combining the top-
down approach proposed in Section 4.1 and the
bottom-up approach proposed in Section 4.2. The
underlying hypothesis is that the induction of an
effective temporal model for Italian can bene-
fit from the combination of the large amount of
examples coming from translations on the one
34
side, and from the more precise language-specific
knowledge derived from the corpus on the other.
To check the validity of this hypothesis, the pro-
cess described in Section 4.2 has been modified
adding an additional phase. In this phase, the set
of TEs derived from I-CAB is augmented with the
expressions already available in the Spanish and
English TE sets. The new porting process is car-
ried out in the following steps:
1. TE Collection and translation. The Italian ex-
pressions are collected from the I-CAB train-
ing portion, and translated both into Spanish
and English.
2. Normalization rules assignment. With the
same methodology described in Section 4.2
(step 2), the Italian TEs mined from the cor-
pus are mapped onto the appropriate normal-
ization rules assigned to their translations.
3. TE set augmentation. The set of Italian TEs
is automatically augmented with new expres-
sions derived from the Spanish and English
TE sets. As described in Section 4.1, these
expressions are first translated into Italian us-
ing on-line translators, then filtered through
Web searches. The remaining TEs are in-
cluded in the Italian model, and related to the
same normalization rules assigned to the cor-
responding Spanish or English TEs.
Also this porting experiment was carried out
with minimal modifications of the existing code.
The automatic acquisition of the new model for
Italian required around one hour. Evaluation re-
sults, calculated over the I-CAB test set are pre-
sented in Table 4.
Prec Rec F
timex2 0.726 0.834 0.776
anchor dir 0.578 0.475 0.521
anchor val 0.516 0.424 0.465
set 0.182 1.000 0.308
text 0.258 0.296 0.276
val 0.564 0.545 0.555
Table 4: Porting based on corpus annotations and
online translators
As can be seen from the table, the combina-
tion of the two approaches leads to an overall per-
formance improvement with respect to the previ-
ous experiments. Apart from a slight decrease in
terms of detection (timex2 attribute), both brack-
eting and normalization performance benefit from
such combination. The improvement on bracket-
ing (text attribute) is around 4% with respect to
both the previous experiments. On average, the
improvement for the normalization attributes is
around 15% with respect to the translation-based
method (ranging from +4,5% for the set attribute,
to +20% for the val attribute), and 20% with re-
spect to the corpus-based method (ranging from
+11% for the anchor dir attribute, to +30% for
the set attribute). These performance improve-
ments are summarized in Table 5, which reports
the F-Measure scores achieved by the three port-
ing approaches.
F-Tran. F-Corpus F-Comb.
timex2 0.775 0.781 0.776
anchor dir 0.311 0.413 0.521
anchor val 0.300 0.339 0.465
set 0.152 0.059 0.308
text 0.263 0.238 0.276
val 0.232 0.279 0.555
Table 5: F-Measure scores comparison
These results confirm the validity of our work-
ing hypothesis, showing that:
? taken in isolation, both the knowledge de-
rived from models built for other languages,
and the language-specific knowledge derived
from an annotated corpus, have a limited im-
pact on the system?s performance;
? taken in combination, the top-down and the
bottom-up approaches can complement each
other, allowing to cope with the complexity
of the porting task.
5 Comparing TERSEO with a
language-specific system
For the sake of completeness, the results achieved
by our combined porting procedure have been
compared with those achieved, over the I-CAB
test set, by a system specifically designed for
Italian. The ITA-Chronos system (Negri and
Marseglia, 2004), a multilingual system for the
recognition and normalization of TEs in Italian
and English, has been used for this purpose. Up to
date, being among the two top performing systems
at TERN 2004, Chronos represents the state-of-
the-art with respect to the TERN task. In addition,
to the best of our knowledge, this is the only sys-
tem effectively dealing with the Italian language.
35
Like all the other state-of-the-art systems ad-
dressing the recognition/normalization task, ITA-
Chronos is a rule-based system. From a design
point of view, it shares with TERSEO a rather
similar architecture which relies on different sets
of rules. These are regular expressions that check
for specific features of the input text, such as the
presence of particular word senses, lemmas, parts
of speech, symbols, or strings satisfying specific
predicates12 . Each set of rules is in charge of
dealing with different aspects of the problem. In
particular, a set of around 350 rules is designed
for TE recognition and is capable of recognizing
with high Precision/Recall rates a broad variety of
TEs. Other sets of regular expressions, for a total
of around 700 rules, are used in the normalization
phase, and are in charge of handling each specific
TIMEX2 normalization attribute. The results ob-
tained by the Italian version of Chronos over the
I-CAB test set are shown in Table 6.
Prec Rec F F-Comb
timex2 0.925 0.908 0.917 0.776 (-14%)
anchor dir 0.733 0.636 0.681 0.521 (-16%)
anchor val 0.495 0.462 0.478 0.465 (-1.3%)
set 0.616 0.500 0.552 0.308 (-24%)
text 0.859 0.843 0.851 0.276 (-57%)
val 0.636 0.673 0.654 0.555 (-10%)
Table 6: Evaluation of ITA-Chronos over the I-
CAB test set
As expected, the distance between the results
obtained by ITA-Chronos and the best Italian sys-
tem automatically obtained from TERSEO (F-
Comb) is considerable. On average, in terms of
F-Measure, the scores obtained by ITA-TERSEO
are 20% lower, ranging from -1.3% for the an-
chor val attribute, to -57% for the text attribute.
However, going beyond the raw numbers, a com-
prehensive evaluation must also take into account
the great difference, in terms of the required time,
effort, and resources deployed in the development
of the two systems. While the implementation of
the manual one took several months, the automatic
porting procedure of TERSEO to Italian (in all the
three modalities described in this paper) is a very
fast process that can be accomplished in less than
an hour. Considering the trade-off between per-
formance and effort required for system?s devel-
12For instance, the predicates ?Weekday-p? and
?Time Unit-p? are respectively satisfied by strings such
as ?Monday?, ?Tuesday?, ..., ?Sunday?, and ?second?,
?minute?, ?hour?, ?day?, ..., ?century?. Of course, this also
holds for the Italian equivalents of these expressions
opment, the proposed methodology represents a
viable solution to attack the porting problem.
6 Conclusions
In this paper, the problem of automatically extend-
ing to new languages a rule-based system for TE
recognition and normalization has been addressed.
Adopting an incremental approach, different port-
ing strategies, for the creation of an Italian system
starting from an already available Spanish system,
have been evaluated and discussed. Each exper-
iment has been carried out considering the con-
tribution of different knowledge sources for rules
translation. Firstly, the contribution given by the
output of online translators has been evaluated,
showing detection performances in line with a pre-
viously developed English extension of the sys-
tem, but a performance drop in terms of normal-
ization performance. Then, the contribution of
knowledge mined from an annotated corpus has
been considered. Results show a performance in-
crease in terms of detection and bracketing, but
a less coherent behaviour in terms of normaliza-
tion. Finally, a combined approach has been ex-
perimented, resulting in an overall performance
increase. System?s performance is still far from
the results obtained by a state-of-the-art system for
Italian but, considering the trade-off between per-
formance and effort required for system?s devel-
opment, results are encouraging.
References
L. Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wil-
son. 2005. Tides.2005 standard for the annotation
of temporal expressions. Technical report, MITRE.
K. Hacioglu, Y. Chen, and B. Douglas. 2005. Time
Expression Labeling for English and Chinese Text.
In Proceedings of CICLing 2005, pages 548?559.
B. Magnini, E. Pianta, C. Girardi, M. Negri, L. Ro-
mano, M. Speranza, and R. Sprugnoli. 2006. I-
CAB: the Italian Content Annotation Bank. In Pro-
ceedings of LREC 2006. To appear.
M. Negri and L. Marseglia. 2004. Recognition and
normalization of time expressions: Itc-irst at tern
2004. Technical report, ITC-irst, Trento.
E. Saquete, P. Martnez-Barco, and R. Muoz. 2004.
Evaluation of the automatic multilinguality for time
expression resolution. In DEXA Workshops, pages
25?30. IEEE Computer Society.
E. Saquete, R. Muoz, and P. Martnez-Barco. 2005.
Event ordering using TERSEO system. Data and
36
Knowledge Engineering Journal, page (To be pub-
lished).
E. Saquete, P. Martinez-Barco, R. Munoz R., M. Ne-
gri, M. Speranza, and R. Sprugnoli R. 2006a. Au-
tomatic resolution rule assignment to multilingual
temporal expressions using annotated corpora. In
Proceedings of the TIME 2006 International Sym-
posium on Temporal Representation and Reasoning.
To Appear.
E. Saquete, P. Martinez-Barco, R. Munoz R., M. Negri,
M. Speranza, and R. Sprugnoli R. 2006b. Multilin-
gual Extension of a Temporal Expression Normal-
izer using Annotated Corpora. In Proceedings of the
EACL Workshop on Cross-Language Knowledge In-
duction.
E. Saquete. 2005. Temporal information Resolution
and its application to Temporal Question Answer-
ing. Phd, Departamento de Lenguages y Sistemas
Informa?ticos. Universidad de Alicante, June.
37
Multilingual Extension of a Temporal Expression Normalizer using
Annotated Corpora
E. Saquete P. Mart??nez-Barco R. Mun?oz
gPLSI
DLSI. UA
Alicante, Spain
fstela,patricio,rafaelg@dlsi.ua.es
M. Negri M. Speranza
ITC-irst
Povo (TN), Italy
fnegri,mansperag@itc.it
R. Sprugnoli
CELCT
Trento, Italy
sprugnoli@celct.it
Abstract
This paper presents the automatic exten-
sion to other languages of TERSEO, a
knowledge-based system for the recogni-
tion and normalization of temporal ex-
pressions originally developed for Span-
ish1. TERSEO was first extended to En-
glish through the automatic translation of
the temporal expressions. Then, an im-
proved porting process was applied to Ital-
ian, where the automatic translation of
the temporal expressions from English and
from Spanish was combined with the ex-
traction of new expressions from an Ital-
ian annotated corpus. Experimental re-
sults demonstrate how, while still adher-
ing to the rule-based paradigm, the devel-
opment of automatic rule translation pro-
cedures allowed us to minimize the ef-
fort required for porting to new languages.
Relying on such procedures, and without
any manual effort or previous knowledge
of the target language, TERSEO recog-
nizes and normalizes temporal expressions
in Italian with good results (72% precision
and 83% recall for recognition).
1 Introduction
Recently, the Natural Language Processing com-
munity has become more and more interested
in developing language independent systems,
in the effort of breaking the language barrier
hampering their application in real use scenar-
ios. Such a strong interest in multilingual-
ity is demonstrated by the growing number of
1This research was partially funded by the Spanish Gov-
ernment (contract TIC2003-07158-C04-01)
international conferences and initiatives plac-
ing systems? multilingual/cross-language capabil-
ities among the hottest research topics, such as
the European Cross-Language Evaluation Forum2
(CLEF), a successful evaluation campaign which
aims at fostering research in different areas of
multilingual information retrieval. At the same
time, in the temporal expressions recognition and
normalization field, systems featuring multilin-
gual capabilities have been proposed. Among
others, (Moia, 2001; Wilson et al, 2001; Negri
and Marseglia, 2004) emphasized the potentiali-
ties of such applications for different information
retrieval related tasks.
As many other NLP areas, research in auto-
mated temporal reasoning has recently seen the
emergence of machine learning approaches trying
to overcome the difficulties of extending a lan-
guage model to other languages (Carpenter, 2004;
Ittycheriah et al, 2003). In this direction, the out-
comes of the first Time Expression Recognition
and Normalization Workshop (TERN 20043) pro-
vide a clear indication of the state of the field. In
spite of the good results obtained in the recog-
nition task, normalization by means of machine
learning techniques still shows relatively poor re-
sults with respect to rule-based approaches, and
still remains an unresolved problem.
The difficulty of porting systems to new lan-
guages (or domains) affects both rule-based and
machine learning approaches. With rule-based ap-
proaches (Schilder and Habel, 2001; Filatova and
Hovy, 2001), the main problems are related to
the fact that the porting process requires rewriting
from scratch, or adapting to each new language,
large numbers of rules, which is costly and time-
2http://www.clef-campaign.org/
3http://timex2.mitre.org/tern.html
1
consuming work. Machine learning approaches
(Setzer and Gaizauskas, 2002; Katz and Arosio,
2001), on the other hand, can be extended with
little human intervention through the use of lan-
guage corpora. However, the large annotated cor-
pora that are necessary to obtain high performance
are not always available. In this paper we describe
a new procedure to build temporal models for new
languages, starting from previously defined ones.
While still adhering to the rule-based paradigm, its
main contribution is the proposal of a simple, but
effective, methodology to automate the porting of
a system from one language to another. In this pro-
cedure, we take advantage of the architecture of an
existing system developed for Spanish (TERSEO,
see (Saquete et al, 2005)), where the recognition
model is language-dependent but the normalizing
procedure is completely independent. In this way,
the approach is capable of automatically learning
the recognition model by adjusting the set of nor-
malization rules.
The paper is structured as follows: Section 2
provides a short overview of TERSEO; Section 3
describes the automatic extension of the system to
Italian; Section 4 presents the results of our evalu-
ation experiments, comparing the performance of
Ita-TERSEO (i.e. our extended system) with the
performance of a state of the art system for Italian.
2 The TERSEO system architecture
TERSEO has been developed in order to automat-
ically recognize temporal expressions (TEs) ap-
pearing in a Spanish written text, and normalize
them according to the temporal model proposed
in (Saquete, 2005), which is compatible with the
ACE annotation standards for temporal expres-
sions (Ferro et al, 2005). As shown in Figure 1,
the first step (recognition) includes pre-processing
of the input texts, which are tagged with lexical
and morphological information that will be used
as input to the temporal parser. The temporal
parser is implemented using an ascending tech-
nique (chart parser) and is based on a temporal
grammar. Once the parser has recognized the TEs
in an input text, these are passed to the normaliza-
tion unit, which updates the value of the reference
according to the date they refer to, and generates
the XML tags for each expression.
As TEs can be categorized as explicit and im-
plicit, the grammar used by the parser is tuned for
discriminating between the two groups. On the
TEXT
POS 
TAGGER
RECOGNITION: 
PARSER
Lexical and
morphological
information
Temporal 
expression
recognition
DATE
ESTIMATION
Dictionary
Temporal
Expression
Grammar
TEMPORAL
EXPRESSION
NORMALIZATION
EVENT 
ORDERING
ORDERED
TEXT
Documental 
DataBase
Figure 1: Graphic representation of the TERSEO
architecture.
one hand, explicit temporal expressions directly
provide and fully describe a date which does not
require any further reasoning process to be inter-
preted (e.g. ?1st May 2005?, ?05/01/2005?). On
the other hand, implicit (or anaphoric) time ex-
pressions (e.g. ?yesterday?, ?three years later?)
require some degree of reasoning (as in the case
of anaphora resolution). In order to translate such
expressions into explicit dates, such reasoning ca-
pabilities consider the information provided by the
lexical context in which they occur (see (Saquete,
2005) for a thorough description of the reasoning
techniques used by TERSEO).
2.1 Recognition using a temporal expression
parser
The parser uses a grammar based on two differ-
ent sets of rules. The first set of rules is in charge
of date and time recognition (i.e. explicit dates,
such as ?05/01/2005?). For this type of TEs, the
grammar adopted by TERSEO recognizes a large
number of date and time formats (see Table 1 for
some examples).
The second set of rules is in charge of the recog-
nition of the temporal reference for implicit TEs,
2
fecha! dd+?/?+mm+?/?+(yy)yy (12/06/1975)
(06/12/1975)
fecha! dd+?-?+mes+?-?+(yy)yy (12-junio-1975)
(12-Jun.-1975)
fecha! dd+?de?+mm+?de?+(yy)yy (12 de junio de 1975)
Table 1: Sample of rules for Explicit Dates Recognition.
reference! ?ayer? (yesterday)
Implicit dates reference! ?man?ana? (tomorrow)
referring to Document Date reference! ?anteayer? (the day before yesterdary)
Concrete reference! ?el pro?ximo d??a? (the next day)
Implicit Dates reference! ?un mes despue?s? (a month later)
Previous Date Period reference! num+?an?os despue?s?(num years later)
Imp. Dates Prev.Date Concrete reference! ?un d??a antes? (a day before)
Implicit Dates reference! ?d??as despue?s? (some days later)
Previous Date Fuzzy reference! ?d??as antes? (some days before)
Table 2: Sample of rules for Implicit Dates recognition.
i.e. TEs that need to be related to an explicit TE
to be interpreted. These can be divided into time
adverbs (e.g. ?yesterday?, ?tomorrow?), and nom-
inal phrases that are referring to temporal relation-
ships (e.g. ?three years later?, ?the day before?).
Table 2 shows some of the rules used for the de-
tection of these kinds of references.
2.2 Normalization
When the system finds an explicit temporal ex-
pression, the normalization process is direct as no
resolution of the expression is necessary. For im-
plicit expressions, an inference engine that inter-
prets every reference previously found in the input
text is used. In some cases references are solved
using the newspaper?s date (FechaP). Other TEs
have to be interpreted by referring to a date named
before in the text that is being analyzed (FechaA).
In these cases, a temporal model that allows the
system to determine the reference date over which
the dictionary operations are going to be done, has
been defined. This model is based on the follow-
ing two rules:
1. The newspaper?s date, when available, is
used as a base temporal referent by default;
otherwise, the current date is used as anchor.
2. In case a non-anaphoric TE is found, it is
stored as FechaA. This value is updated ev-
ery time a non-anaphoric TE appears in the
text.
Table 3 shows some of the entries of the dictio-
nary used in the inference engine.
3 Extending TERSEO: from Spanish
and English to Italian
As stated before, the main purpose of this paper is
to describe a new procedure to automatically build
temporal models for new languages, starting from
previously defined models. In our case, an English
model has been automatically obtained from the
Spanish one through the automatic translation of
the Spanish temporal expressions to English. The
resulting system for the recognition and normal-
ization of English TEs obtains good results both
in terms of precision (P) and recall (R) (Saquete et
al., 2004). The comparison of the results between
the Spanish and the English system is shown in
Table 4.
SPANISH ENGLISH
DOCS 50 100
POS 199 634
ACT 156 511
CORRECT 138 393
INCORRECT 18 118
MISSED 43 123
P 88% 77%
R 69% 62%
F 77% 69%
Table 4: Comparison between Spanish TERSEO
and English TERSEO.
This section presents the procedure we followed
to extend our system to Italian, starting from the
Spanish and English models already available, and
a manually annotated corpus. In this case, both
models have been considered as they can be com-
plemented reciprocally. The Spanish model was
3
REFERENCE DICTIONARY ENTRY
?ayer? Day(FechaP)-1/Month(FechaP)/Year(FechaP)
(yesterday)
?man?ana? Day(FechaP)+1/Month(FechaP)/Year(FechaP)
(tomorrow)
?anteayer? Day(FechaP)-2/Month(FechaP)/Year(FechaP)
(the day before yesterday)
?el pro?ximo d??a? Day(FechaP)+1/Month(FechaP)/Year(FechaP)
(the next day)
?un mes despue?s? [DayI/Month(FechaA)+1/Year(FechaA)--
(a month later) DayF/Month(FechaA)+1/Year(FechaA)]
num+?an?os despue?s? [01/01/Year(FechaA)+num --
(num years later) 31/12/Year(FechaA)+num]
?un d??a antes? Day(FechaA)-1/Month(FechaA)/Year(FechaA)
(a day before)
?d??as despue?s? >>>>FechaA
(some days later)
?d??as antes? <<<<FechaA
(some days before)
Table 3: Normalization rules
manually obtained and evaluated showing high
scores for precision (88%), so better results could
be expected when it is used. However, in spite of
the fact that the English model has shown lower
results on precision (77%), the on-line transla-
tors between Italian and English have better re-
sults than Spanish to Italian translators. As a re-
sult, both models are considered in the following
steps for the multilingual extension:
 Firstly, a set of Italian temporal expressions
is extracted from an Italian annotated corpus
and stored in a database. The selected cor-
pus is the training part of I-CAB, the Ital-
ian Content Annotation Bank (Lavelli et al,
2005). More detailed information about I-
CAB is provided in Section 4.
 Secondly, the resulting set of Italian TEs
must be related to the appropriate normaliza-
tion rule. In order to do that, a double transla-
tion procedure has been developed. We first
translate all the expressions into English and
Spanish simultaneously; then, the normaliza-
tion rules related to the translated expressions
are obtained. If both the Spanish and En-
glish expressions are found in their respec-
tive models in agreement with the same nor-
malization rule, then this rule is also assigned
to the Italian expression. Also, when only
one of the translated expressions is found in
the existing models, the normalization rule
is assigned. In case of discrepancies, i.e. if
both expressions are found, but not coincid-
ing in the same normalization rule, then one
of the languages must be prioritized. As the
Spanish model was manually obtained and
has shown a higher precision, Spanish rules
are preferred. In other cases, the expression
is reserved for a manual assignment.
 Finally, the set is automatically augmented
using the Spanish and English sets of tem-
poral expressions. These expressions were
also translated into Italian by on-line ma-
chine translation systems (Spanish-Italian4 ,
English-Italian5). In this case, a filtering
module is used to guarantee that all the ex-
pressions were correctly translated. This
module searches the web with Google6 for
the translated expression. If the expression
is not frequently found, then the translation
is abandoned. After that, the new Italian ex-
pression is included in the model, and related
to the same normalization rule assigned to the
Spanish or English temporal expression.
The entire translation process has been com-
pleted with an automatic generalization process,
oriented to obtain generalized rules from the con-
crete cases that have been collected from the cor-
4http://www.tranexp.com:2000/Translate/result.shtml
5http://world.altavista.com/
6http://www.google.com/
4
pus. This generalization process has a double ef-
fect. On the one hand, it reduces the number of
recognition rules. On the other hand, it allows the
system to identify new expressions that were not
previously learned. For instance, the expression
?Dieci mesi dopo? (i.e. ?Ten months later?) could
be recognized if the expression ?Nove mesi dopo?
(i.e. Nine months later) was learned.
The multilingual extension procedure (Figure 3)
is carried out in three phases:
Spanish
Temporal 
Recognition
Model
Spanish-Italian
TRANSLATOR
TEs FILTER
KEYWORDS Unit
NEW TEs
FINDER
RULE
ASSIGNMENTS
Google
WordNet
Italian TEs
Italian TEs
Temporal  keywords
New Italian TEs
New Normalizer rule
Italian
Temporal 
Normalizer
Model
Online 
DictionariesITALIAN TEs
GRAMATICS 
Generator
English
Temporal 
Recognition
Model
Italian
I-CAB   
Corpus
English-Italian
TRANSLATOR
Italian-Spanish
TRANSLATOR
Italian-English
TRANSLATOR
Spanish
Temporal 
Normalizer
Model
English
Temporal 
Normalizer
Model
Italian TEs
Italian generalized TEs
Phase
1
Phase 
2 
Phase 
3
Figure 2: Multilingual extension procedure.
 Phase 1: TE Collection. During this phase,
the Italian temporal expressions are col-
lected from I-CAB (Italian Content Annota-
tion Bank), and the automatically translated
Italian TEs are derived from the set of Span-
ish and English TEs. In this case, the TEs
are filtered removing those not being found
by Google.
 Phase 2: TE Generalization. In this phase,
the TEs Gramatics Generator uses the mor-
phological and syntactical information from
the collected TEs to generate the grammat-
ical rules that generalize the recognition of
the TEs. Moreover, the keyword unit is able
to extract the temporal keywords that will be
used to build new TEs. These keywords are
augmented with their synonyms in WordNet
(Vossen, 2000) to generate new TEs.
 Phase 3: TE Normalizing Rule Assignment.
In the last phase, the translators are used to
relate the recognizing rule to the appropriate
normalization rule. For this purpose, the sys-
tem takes advantage of the previously defined
Spanish and English temporal models.
4 Evaluation
The automatic extension of the system to Italian
(Ita-TERSEO) has been evaluated using I-CAB,
which has been divided in two parts: training and
test. The training part has been used, first of all,
in order to automatically extend the system. Af-
ter this extension, the system was evaluated both
against the training and the test corpora. The pur-
pose of this double evaluation experiment was to
compare the recall obtained over the training cor-
pus with the value obtained over the test corpus.
An additional evaluation experiment has also
been carried out in order to compare the perfor-
mance of the automatically developed system with
a state of the art system specifically developed for
Italian and English, i.e. the Chronos system de-
scribed in (Negri and Marseglia, 2004).
In the following sections, more details about I-
CAB and the evaluation process are presented, to-
gether with the evaluation results.
4.1 The I-CAB Corpus
The evaluation has been performed on the tem-
poral annotations of I-CAB (I-CAB-temp) cre-
ated as part of the three-year project ONTOTEXT7
funded by the Provincia Autonoma di Trento.
I-CAB consists of 525 news documents
taken from the local newspaper L?Adige
(http://www.adige.it). The selected news sto-
ries belong to four different days (September, 7th
and 8th 2004 and October, 7th and 8th 2004) and
are grouped into five categories: News Stories,
Cultural News, Economic News, Sports News
and Local News. The corpus consists of around
182,500 words (on average 347 words per file).
The total number of annotated temporal expres-
sions is 4,553; the average length of a temporal
expression is 1.9 words.
The annotation of I-CAB has been carried out
adopting the standards developed within the ACE
program (Automatic Content Extraction8) for the
Time Expressions Recognition and Normalization
7http://tcc.itc.it/projects/ontotext
8http://www.nist.gov/speech/tests/ace
5
tasks, which allows for a semantically rich and
normalized annotation of different types of tempo-
ral expressions (for further details on the TIMEX2
annotation standard for English see (Ferro et al,
2005)).
The ACE guidelines have been adapted to
the specific morpho-syntactic features of Italian,
which has a far richer morphology than English.
In particular, some changes concerning the exten-
sion of the temporal expressions have been in-
troduced. According to the English guidelines,
in fact, definite and indefinite articles are consid-
ered as part of the textual realization of an entity,
while prepositions are not. As the annotation is
word-based, this does not account for Italian artic-
ulated prepositions, where a definite article and a
preposition are merged. Within I-CAB, this type
of preposition has been included as possible con-
stituents of an entity, so as to consistently include
all the articles.
An assessment of the inter-annotator agreement
based on the Dice coefficient has shown that the
task is a well-defined one, as the agreement is
95.5% for the recognition of temporal expressions.
4.2 Evaluation process
The evaluation of the automatic extension of
TERSEO to Italian has been performed in three
steps. First of all, the system has been evaluated
both against the training and the test corpora with
two main purposes:
 Determining if the recall obtained in the eval-
uation of the training part of the corpus is a
bit higher than the one obtained in the eval-
uation of the test part of I-CAB, due to the
fact that in the TE collection phase of the ex-
tension, temporal expressions were extracted
from this part of the corpus.
 Determining the performance of the automat-
ically extended system without any manual
revision of both the Italian translations and
the resolution rules automatically related to
the expressions.
Secondly, we were also interested in verifying
if the performance of the system in terms of pre-
cision could be improved through a manual revi-
sion of the automatically translated temporal ex-
pressions.
Finally, a comparison with a state of the art sys-
tem for Italian has been carried out in order to es-
timate the real potentialities of the proposed ap-
proach. All the evaluation results are compared
and presented in the following sections using the
same metrics adopted at the TERN2004 confer-
ence.
4.2.1 Evaluation of Ita-TERSEO
In the automatic extension of the system, a to-
tal of 1,183 Italian temporal expressions have been
stored in the database. As shown in Table 5, these
expressions have been obtained from the different
resources available:
 ENG ITA: This group of expressions has
been obtained from the automatic translation
into Italian of the English Temporal Expres-
sions stored in the knowledge DB.
 ESP ITA: This group of expressions has been
obtained from the automatic translation into
Italian of the Spanish Temporal Expressions
stored in the knowledge DB.
 CORPUS: This group of expressions has
been extracted directly from the training part
of the I-CAB corpus.
Source N %
ENG ITA 593 50.1
ESP ITA 358 30.3
CORPUS 232 19.6
TOTAL TEs 1183 100.0
Table 5: Italian TEs in the Knowledge DB.
Both the training part and the test part of I-CAB
have been used for evaluation. The results of pre-
cision (P), recall (R) and F-Measure (F) are pre-
sented in Table 6, which provides details about the
system performance over the general recognition
task (timex2), and the different normalization at-
tributes used by the TIMEX2 annotation standard.
As expected, recall performance over the train-
ing corpus is slightly higher. However, although
the temporal expressions have been extracted from
such corpus, in the automatic process of obtain-
ing the normalization rules for these expressions,
some errors could have been introduced.
Comparing these results with those obtained by
the automatic extension of TERSEO to English
and taking into account the recognition task (see
Table 4), precision (P) is slightly better for En-
glish (77% Vs. 72%) whereas recall (R) is better
in the Italian extension (62% Vs. 83%). This is
6
Ita-TERSEO: TRAINING Ita-TERSEO: TEST Chronos: TEST
Tag P R F P R F P R F
timex2 0.694 0.848 0.763 0.726 0.834 0.776 0.925 0.908 0.917
anchor dir 0.495 0.562 0.526 0.578 0.475 0.521 0.733 0.636 0.681
anchor val 0.464 0.527 0.493 0.516 0.424 0.465 0.495 0.462 0.478
set 0.308 0.903 0.459 0.182 1.000 0.308 0.616 0.5 0.552
text 0.265 0.324 0.292 0.258 0.296 0.276 0.859 0.843 0.851
val 0.581 0.564 0.573 0.564 0.545 0.555 0.636 0.673 0.654
Table 6: Results obtained over I-CAB by Ita-TERSEO and Chronos.
due to the fact that in the Italian extension, more
temporal expressions have been covered with re-
spect to the English extension. In this case, in
fact, Ita-TERSEO is not only using the temporal
expressions translated from the English or Spanish
knowledge database, but also the temporal expres-
sions extracted from the training part of I-CAB.
4.2.2 Manual revision of the acquired TEs
A manual revision of the Italian TEs stored in
the Knowledge DB has been done in two steps.
First of all, the incorrectly translated expressions
(from Spanish and English to Italian) were re-
moved from the database. A total of 334 expres-
sions were detected as wrong translated expres-
sions. After this, another revision was performed.
In this case, some expressions were modified be-
cause the expressions have some minor errors in
the translation. 213 expressions were modified in
this second revision cycle. Moreover, since pattern
constituents in Italian might have different ortho-
graphical features (e.g. masculine/feminine, ini-
tial vowel/consonant, etc.), new patterns had to be
introduced to capture such variants. For exam-
ple, as months? names in Italian could start with
a vowel, the temporal expression pattern ?nell?-
MONTH? has been inserted in the Knowledge
DB. After these changes, the total amount of ex-
pressions stored in the DB are shown in Table 7.
Source N %
ING ITA 416 47.9
ESP ITA 201 23.1
CORPUS 232 26.7
REV MAN 20 2.3
TOTAL TEs 869 100.0
Table 7: Italian TEs in the Knowledge DB after
manual revision.
In order to evaluate the system after this manual
revision, the training and the test part of I-CAB
have been used. However, the results of preci-
sion (PREC), recall (REC) and F-Measure were
exactly the same as presented in Table 6. That
is not really surprising. The existence of wrong
expressions in the knowledge database does not
affect the final results of the system, as they will
never be used for recognition or resolution. This
is because these expressions will not appear in real
documents, and are redundant as the correct ex-
pression is also stored in the Knowledge DB.
4.2.3 Comparing Italian TERSEO with a
language-specific system
Finally, in order to compare Ita-TERSEO with
a state of the art system specifically designed for
Italian, we chose Chronos (Negri and Marseglia,
2004), a multilingual system for the recognition
and normalization of TEs in Italian and English.
Like all the other state of the art systems address-
ing the recognition/normalization task, Chronos
is a rule-based system. From a design point of
view, it shares with TERSEO a rather similar ar-
chitecture which relies on different sets of rules.
These are regular expressions that check for spe-
cific features of the input text, such as the pres-
ence of particular word senses, lemmas, parts
of speech, symbols, or strings satisfying specific
predicates. Each set of rules is in charge of deal-
ing with different aspects of the problem. In
particular, a set of around 350 rules is designed
for TE recognition and is capable of recogniz-
ing with high Precision/Recall rates both explicit
and implicit TEs. Other sets of regular expres-
sions, for a total of around 700 rules, are used
in the normalization phase, and are in charge of
handling a specific TIMEX2 attribute (i.e. VAL,
SET, ANCHOR VAL, and ANCHOR DIR). The
results obtained by the Italian version of Chronos
over the test part of I-CAB are shown in the last
three columns of Table 6.
As expected, the distance between the results
obtained by the two systems is considerable. How-
ever, the following considerations should be taken
into account. First, there is a great difference, both
7
in terms of the required time and effort, in the de-
velopment of the two systems. While the imple-
mentation of the manual one took several months,
the porting procedure of TERSEO to Italian is a
very fast process that can be accomplished in less
than an hour. Second, even if an annotated corpus
for a new language is not available, the automatic
porting procedure we present still remains feasi-
ble. In fact, most of the TEs for a new language
that are stored in the Knowledge DB are the result
of the translation of the Spanish/English TEs into
such a target language. In our case, as shown in
Table 5, more than 80% of the acquired Italian TEs
result from the automatic translation of the expres-
sions already stored in the DB. This makes the pro-
posed approach a viable solution which allows for
a rapid porting of the system to other languages,
while just requiring an on-line translator (note that
the Altavista Babel Fish translator9 provides trans-
lations from English to 12 target languages). In
light of these considerations, the results obtained
by Ita-TERSEO are encouraging.
5 Conclusions
In this paper we have presented an automatic ex-
tension of a rule-based approach to TEs recogni-
tion and normalization. The procedure is based
on building temporal models for new languages
starting from previously defined ones. This proce-
dure is able to fill the gap left by machine learning
systems that, up to date, are still far from provid-
ing acceptable performance on this task. As re-
sults illustrate, the proposed methodology (even
though with a lower performance with respect to
language-specific systems) is a viable and effec-
tive solution for a rapid and automatic porting of
an existing system to new languages.
References
B. Carpenter. 2004. Phrasal Queries with LingPipe
and Lucene. In 13th Text REtrieval Conference,
NIST Special Publication. National Institute of Stan-
dards and Technology.
L. Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wil-
son. 2005. TIDES 2005 Standard for the annotation
of temporal expressions. Technical report, MITRE.
E. Filatova and E. Hovy. 2001. Assigning time-stamps
to event-clauses. In ACL, editor, Proceedings of the
2001 ACL Workshop on Temporal and Spatial Infor-
mation Processing, pages 88?95, Toulouse, France.
9http://world.altavista.com/
A. Ittycheriah, L.V. Lita, N. Kambhatla, N. Nicolov,
S. Roukos, and M. Stys. 2003. Identifying and
Tracking Entity Mentions in a Maximum Entropy
Framework. In ACL, editor, Proceedings of the
NAACL Workshop WordNet and Other Lexical Re-
sources: Applications, Extensions and Customiza-
tions.
G. Katz and F. Arosio. 2001. The annotation of tem-
poral information in natural language sentences. In
ACL, editor, Proceedings of the 2001 ACL Work-
shop on Temporal and Spatial Information Process-
ing, pages 104?111, Toulouse, France.
A. Lavelli, B. Magnini, M. Negri, E. Pianta, M. Sper-
anza, and R. Sprugnoli. 2005. Italian Content An-
notation Bank (I-CAB): Temporal expressions (v.
1.0.): T-0505-12. Technical report, ITC-irst, Trento.
T. Moia. 2001. Telling apart temporal locating adver-
bials and time-denoting expressions. In ACL, editor,
Proceedings of the 2001 ACL Workshop on Tempo-
ral and Spatial Information Processing, Toulouse,
France.
M. Negri and L. Marseglia. 2004. Recognition and
normalization of time expressions: Itc-irst at TERN
2004. Technical report, ITC-irst, Trento.
E. Saquete, P. Mart??nez-Barco, and R. Mun?oz. 2004.
Evaluation of the automatic multilinguality for time
expression resolution. In DEXA Workshops, pages
25?30. IEEE Computer Society.
E. Saquete, R. Mun?oz, and P. Mart??nez-Barco. 2005.
Event ordering using terseo system. Data and
Knowledge Engineering Journal, page (To be pub-
lished).
E. Saquete. 2005. Temporal information Resolution
and its application to Temporal Question Answer-
ing. Phd, Departamento de Lenguages y Sistemas
Informa?ticos. Universidad de Alicante, June.
F. Schilder and C. Habel. 2001. From temporal expres-
sions to temporal information: Semantic tagging of
news messages. In ACL, editor, Proceedings of the
2001 ACL Workshop on Temporal and Spatial Infor-
mation Processing, pages 65?72, Toulouse, France.
A. Setzer and R. Gaizauskas. 2002. On the impor-
tance of annotating event-event temporal relations
in text. In LREC, editor, Proceedings of the LREC
Workshop on Temporal Annotation Standards, 2002,
pages 52?60, Las Palmas de Gran Canaria,Spain.
P. Vossen. 2000. EuroWordNet: Building a Multilin-
gual Database with WordNets in 8 European Lan-
guages. The ELRA Newsletter, 5(1):9?10.
G. Wilson, I. Mani, B. Sundheim, and L. Ferro. 2001.
A multilingual approach to annotating and extract-
ing temporal information. In ACL, editor, Pro-
ceedings of the 2001 ACL Workshop on Temporal
and Spatial Information Processing, pages 81?87,
Toulouse, France.
8
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 409?420, Dublin, Ireland, August 23-29 2014.
Machine Translation Quality Estimation Across Domains
Jos
?
e G. C. de Souza
University of Trento
Fondazione Bruno Kessler
Trento, Italy
desouza@fbk.eu
Marco Turchi
Fondazione Bruno Kessler
Trento, Italy
turchi@fbk.eu
Matteo Negri
Fondazione Bruno Kessler
Trento, Italy
negri@fbk.eu
Abstract
Machine Translation (MT) Quality Estimation (QE) aims to automatically measure the quality
of MT system output without reference translations. In spite of the progress achieved in re-
cent years, current MT QE systems are not capable of dealing with data coming from different
train/test distributions or domains, and scenarios in which training data is scarce. We investigate
different multitask learning methods that can cope with such limitations and show that they over-
come current state-of-the-art methods in real-world conditions where training and test data come
from different domains.
1 Introduction
Machine Translation (MT) Quality Estimation (QE) aims to automatically predict the quality of MT
output without using reference translations (Blatz et al., 2003; Specia et al., 2009). QE systems usually
employ supervised machine learning models that use different information extracted from (source, target)
sentence pairs as features along with quality scores as labels. The notion of quality that these models
measure can be indicated by different scores. Some examples are the average number of edits required
to post-edit the MT output, i.e., human translation edit rate
1
(HTER (Snover et al., 2006)), and the time
(in seconds) required to post-edit a translation produced by an MT system (Specia, 2011).
Research on QE has received a strong boost in recent years due to the increase in the usage of MT
systems in real-world applications. Automatic and reference-free MT quality prediction demonstrated
to be useful for different applications, such as: deciding whether the translation output can be published
without post-editing (Soricut and Echihabi, 2010), filtering out low-quality translation suggestions that
should be rewritten from scratch (Specia et al., 2009), selecting the best translation output from a pool
of MT systems (Specia et al., 2010), and informing readers of the translation whether it is reliable or not
(Turchi et al., 2012). Another example is the computer-assisted translation (CAT) scenario, in which it
might be necessary to predict the quality of translation suggestions generated by different MT systems
to support the activity of post editors working with different genres of text.
The dominant QE framework presents some characteristics that can limit models? applicability in
such real-world scenarios. First, the scores used as training labels (HTER, time) are costly to obtain
because they are derived from manual post-editions of MT output. Such requirement makes it difficult
to develop models for domains in which there is a limited amount of labeled data. Second, the learning
methods currently used (for instance in the framework of QE shared evaluation campaigns)
2
assume that
training and test data are sampled from the same distribution. Though reasonable as a first evaluation
setting to promote research in the field, this controlled scenario is not realistic as different data in real-
world applications might be post-edited by different translators, the translations might be generated by
different MT systems and the documents being translated might belong to different domains or genres. To
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Edit distance is calculated as the number of edits (word insertions, deletions, substitutions, and shifts) divided by the
number of words in the reference. Lower HTER values indicate better translations.
2
In the last two editions of the yearly Workshop on Machine Translation, several QE shared tasks have been proposed
(Callison-Burch et al., 2012; Bojar et al., 2013).
409
overcome these limitations a plausible research objective is to exploit techniques that: (i) allow domains
and distributions of features to be different between training and test data, and (ii) that cope with the
scarce amount of training labels by sharing information across domains, a common scenario for transfer
learning.
In this paper we investigate the use of techniques that can exploit the training instances from different
domains to learn a QE model for a specific target domain for which there is a small amount of labeled
data. In particular, we are interested in approaches that allow not only learning from one single source
domain but also from multiple source domains simultaneously, by leveraging the labels from all available
data to improve results in a target domain.
Given these requirements, we experiment with different multitask learning techniques that perform
transfer learning via a common task structure (domain relatedness). Furthermore, we employ an approach
based on feature augmentation that has been successfully used in other natural language processing tasks.
We present a series of experiments over three domains with increasing amounts of training data, showing
that our adaptive approaches outperform competitive baselines.
The contributions of our work are: (i) a first exploration of techniques that overcome the limitation
of current QE learning methods when dealing with data with different training and test distributions and
domains, and (ii) an empirical verification of the amount of training data required by such techniques to
outperform competitive baselines on different target domains. To the best of our knowledge, this is the
first work addressing the challenges posed by domain adaptation in MT QE.
2 Related Work
Quality estimation has recently gained increasing attention, also boosted by two evaluation campaigns
organized within the Workshop on Machine Translation (WMT) (Callison-Burch et al., 2012; Bojar et
al., 2013). The bulk of work done so far has focused on the controlled WMT evaluation framework and,
in particular, on two major aspects of the problem: feature engineering and machine learning methods.
Feature engineering accounts for linguistically-based predictors that aim to model different perspec-
tives of the quality estimation problem. The research ranges from identifying indicators that approximate
the complexity of translating the source sentence and designing features that model the fluency of the
automatically generated translation, to linguistically motivated measures that estimate how adequate the
translation is in comparison to the source sentence in terms of meaning (Blatz et al., 2003; Mehdad et
al., 2012; Hardmeier et al., 2012; Rubino et al., 2012; Specia et al., 2012; de Souza et al., 2013a).
State-of-the-art QE explores different supervised linear or non-linear learning methods for regression
or classification such as Support Vector Machines (SVM), different types of Decision Trees, Neural
Networks, Elastic-Net, Gaussian Processes, Naive Bayes, among others (Specia et al., 2009; Buck,
2012; Beck et al., 2013; Souza et al., 2014). Another aspect related to the learning methods that has
received attention is the optimal selection of features in order to overcome issues related with the high-
dimensionality of the feature space (Soricut et al., 2012; de Souza et al., 2013a; Beck et al., 2013; de
Souza et al., 2013b).
Despite constant improvements, such learning methods have limitations. The main one is that they
assume that both training and test data are independently and identically distributed. As a consequence,
when they are applied to data from a different distribution or domain they show poor performance. This
limitation harms the performance of QE systems for several real-world applications, such as CAT envi-
ronments. Advanced CAT systems currently integrate suggestions obtained from MT engines with those
derived from translation memories (TMs). In such framework, the compelling need to speed up the trans-
lation process and reduce its costs by presenting human translators with good-quality suggestions raises
interesting research challenges for the QE community. In such environments, translation jobs come from
different domains that might be translated by different MT systems and are routed to professional transla-
tors with different idiolect, background and quality standards (Turchi et al., 2013). Such variability calls
for flexible and adaptive QE solutions by investigating two directions: (i) modeling translator behaviour
(Turchi et al., 2014) and (ii) maximize the learning capabilities from all the available data. The second
research objective motivates our investigation on methods that allow the training and test domains and
410
the distributions to be different.
Recent work in QE focused on aspects that are problematic even in the controlled WMT scenario, and
are closely related to the flexibility/adaptability issue. Focusing on the first of the two aforementioned
directions (i.e. modeling translators? behaviour), Cohn and Specia (2013) propose a Multitask Gaussian
Process method that jointly learns a series of annotator-specific models and that outperforms models
trained for each annotator. Our work differs from theirs in that we are interested in the latter research
direction (i.e. coping with domain and distribution diversity) and we use in and out-of-domain data to
learn robust in-domain models. Our scenario represents a more challenging setting than the one tackled
in (Cohn and Specia, 2013), which does not consider different domains.
In transfer learning there are many techniques suitable to fulfill our requirements. The aim of transfer
learning is to extract the knowledge from one or more source tasks and apply it to a target task (Pan
and Yang, 2010). One type of transfer learning is multitask learning (MTL), which uses domain-specific
training signals of related tasks to improve model generalization (Caruana, 1997). Although it was not
originally thought for transferring knowledge to a new task, MTL can be used to achieve this objective
due to its capability to capture task relatedness, which is important knowledge that can be applied to a
new task (Jiang, 2009).
Domain adaptation is a kind of transfer learning in which source and target domains (i.e. training and
test) are different but the tasks are the same (Pan and Yang, 2010). The domain adaptation techniques
that inspire our work have been successfully applied to a variety of NLP tasks (Blitzer et al., 2006;
Jiang and Zhai, 2007). For instance, an effective solution for supervised domain adaptation, EasyAdapt
(SVR FEDA henceforth), was proposed in (Daum?e III, 2007) and applied to named entity recognition,
part-of-speech tagging and shallow parsing. The approach transforms the domain adaptation problem
into a standard learning problem by augmenting the source and target feature set. The feature space is
transformed to be a cross-product of the features of the source and target domains augmented with the
original target domain features. In supervised domain adaptation one has access to out-of-domain labels
and wants to leverage a small amount of available in-domain labeled data to train a model (Daum?e III,
2007), the case of this study. This is different from the semi-supervised case in which in-domain labels
are not available.
3 Adaptation for QE
An important assumption in MTL is that different tasks (domains in our case) are correlated via a certain
structure. Examples of such structures are the hidden layers in a neural network (Caruana, 1997) and
shared feature representation (Argyriou et al., 2007) among others. This common structure allows for
knowledge transfer among tasks and has been demonstrated to improve model generalization over single
task learning (STL) for different problems in different areas. Under this scenario, several assumptions
can be made about the relatedness among the tasks, leading to different transfer structures. We explore
three approaches to MTL that deal with task relatedness in different ways. These are the ?Dirty? approach
to MTL (Jalali et al., 2010), Sparse Trace MTL (Chen et al., 2012) and Robust MTL (Chen et al., 2011).
The three approaches use different regularization techniques that capture task relatedness using norms
over the weights of the features.
Before describing the three approaches, we introduce some basic notation similar to (Chen
et al., 2011). In MTL there are T tasks and each task t ? T has m training samples
{(x
(t)
1
, y
(t)
1
), . . . , (x
(t)
m
, y
(t)
m
)}, with x
(t)
i
? R
d
where d is the number of features and y
(t)
i
? R is the
output (the response variable or label). The input features and labels are stacked together to form two
different matrices X
(t)
= [x
(t)
1
, . . . , x
(t)
m
] and Y
(t)
= [x
(t)
1
, . . . , x
(t)
m
], respectively. The weights of the
features for each task are represented by W , where each column corresponds to a task and each row
corresponds to a feature.
The ?Dirty? approach to MTL follows the idea that different tasks may share the same discriminative
features (Argyriou et al., 2007). However, it also considers that different tasks might have different
discriminative features that are inherent to each task. Therefore, the method encourages shared-sparsity
among tasks and among features in each task. It decomposes W into two components, one is a row-
411
sparsed matrix that corresponds to the features shared among the tasks and the other is an element-wise
sparse matrix that corresponds to the non-shared features that are important for each task independently.
More formally, the ?Dirty? approach is explained by Equation 1.
min
W
T
?
t=1
||(W
(t)
X
(t)
? Y
(t)
)||
2
2
+ ?
s
||S||
1
+ ?
b
||B||
1,?
subject to: W = S +B (1)
where ||(W
(t)
X
(t)
? Y
(t)
)||
2
2
is the least squares loss function, S is the regularization term that en-
courages element-wise sparsity and B is the block-structured row-sparsity regularizer. The ||.||
2
is the
l
2
-norm (Euclidean distance), ||.||
1
is the l
1
-norm (given by
?
i=1
|x
i
|) and ||.||
1,?
is the row grouped l
1
-
norm. The ?
s
and ?
b
are non-negative trade-off parameters that control the amount of regularization
applied to S and B, respectively.
Sparse Trace MTL considers the problem of learning incoherent sparse and low-rank patterns from
multiple related tasks. This approach captures task relationship via a shared low-rank structure of the
weight matrix W . As computing the low-rank structure of a matrix leads to a NP-hard optimization
problem, Chen et al. (2012) proposed to compute the trace norm as a surrogate, making the optimization
problem tractable. In addition to learning the low-rank patterns, this method also considers the fact that
different tasks may have different inherent discriminative features. It decomposes W into two compo-
nents: S, which models element-wise sparsity, and Q, which captures task relationship via the trace
norm. The convex problem minimized by Sparse Trace is given in Equation 2.
min
W
T
?
t=1
||(W
(t)
X
(t)
? Y
(t)
)||
2
2
+ ?
s
||S||
1
subject to: W = S +Q, ||Q||
?
< ?
p
(2)
where ||.||
?
is the trace norm, given by the sum of the singular values ?
i
of W , i.e., ||W ||
?
=
?
i=1
?
i
(W ). Here, ?
p
controls the rank of Q and ?
s
controls the sparsity of S.
The key assumption in MTL is that tasks are related in some way. However, this assumption might not
hold for a series of real-world problems. In situations in which tasks are not related a negative transfer
of information among tasks might occur, harming the generalization of the model. One way to deal
with this problem is to: (i) group related tasks in one structure and share knowledge among them, and
(ii) identify irrelevant tasks maintaining them in a different group that does not share information with
the first group. This is the idea of Robust MTL (RMTL henceforth). The algorithm approximates task
relatedness via a low-rank structure like Sparse Trace and identifies outlier tasks using a group-sparse
structure (column-sparse, at task level). Robust MTL is described by Equation 3. It employs a non-
negative linear combination of the trace norm (the task relatedness component L) and a column-sparse
structure induced by the l
1,2
-norm (the outlier task detection component S). If a task is an outlier it will
have non-zero entries in S.
min
W
T
?
t=1
||(W
(t)
X
(t)
? Y
(t)
)||
2
2
+ ?
l
||L||
?
+ ?
s
||S||
1,2
subject to: W = L+ S (3)
where ||S||
1,2
is the group regularizer that induces sparsity on the tasks.
4 Experimental Setting
In this section we describe the data used for our experiments, the features extracted, the set up of the
learning methods, the baselines used for comparison and the evaluation of the models. The goal of our
experiments is to show that the methods presented in Section 3 outperform competitive baselines and
standard QE learning methods that are not capable of adapting to different domains. We experiment with
three different domains of comparable size and evaluate the performance of the adaptive methods and the
standard techniques with different amounts of training data. The MTL models described in section 3 are
trained with the Malsar toolkit implementation (Zhou et al., 2012). The hyper-parameters are optimized
412
using 5-fold cross-validation in a grid search procedure. The parameter values are searched in an interval
ranging from 10
?3
to 10
3
.
4.1 Data
Our experiments focus on the English-French language pair and encompass three very different domains:
newswire text (henceforth News), transcriptions of Technology Entertainment Design talks (TED) and
Information Technology manuals (IT). Such domains are a challenging combination for adaptive systems
since they come from very different sources spanning speech and written discourse (TED and News/IT,
respectively) as well as a very well defined and controlled vocabulary in the case of IT.
Each domain is composed of 363 tuples formed by the source sentence in English, the French trans-
lation produced by an MT system and a human post-edition of the translated sentence. For each pair
(translation, post-edition) we use as labels the HTER score computed with TERCpp
3
. For the three do-
mains we use half of the data for training (181 instances) and half of the data for testing (182 instances).
The limited amount of instances for training contrasts with the 800 or more instances of the WMT evalu-
ation campaigns and is closer to real-world applications where the availability of large and representative
training sets is far from being guaranteed (e.g. the CAT scenario).
The sentence tuples for the first two domains are randomly sampled from the Trace corpus
4
. The
translations were generated by two different MT systems, a state-of-the-art phrase-based statistical MT
system and a commercial rule-based system. Furthermore, the translations were post-edited by up to four
different translators, as described in (Wisniewski et al., 2013).
Domain No. of tokens Vocab. size Avg. sent. length
TED source 6858 1659 19
TED target 7016 1828 19
IT source 3310 1004 9
IT target 3134 1049 8
News source 7605 2273 21
News target 8230 2346 23
Table 1: Datasets statistics for each domain.
The TED talks domain is formed by subtitles of several talks in a range of topics presented in the TED
conferences. The complete dataset has been used for MT and automatic speech recognition systems
evaluation within the International Workshop on Spoken Language Translation (IWSLT). The News
domain is formed by newswire text used in WMT translation campaigns and covers different topics. The
IT texts come from a software user manual translated by a statistical MT system based on the state-of-
the-art phrase-based Moses toolkit (Koehn et al., 2007) trained on about 2M parallel sentences. The
post-editions were collected from one professional translator operating on the Matecat
5
CAT tool in
real working conditions. Table 1 provides macro-indicators (number of tokens, vocabulary size, average
sentence length) that evidence the large difference between the domains addressed by our experiments
and give an idea of the difficulty of the task.
A peculiarity of the TED domain is that it is formed by manual transcriptions of speech translated by
different MT systems, configuring a different type of discourse than News and IT. In TED, the vocabulary
size in the source and target sentences is lower than that of the News domain but higher than IT. News
presents the most varied vocabulary, which is an evidence of the more varied lexical choice represented
by the several topics that compose the domain. Moreover, News has the highest average sentence length,
a characteristic of non-technical written discourse, which tends to have longer sentences than spoken
discourse and domains dominated by technical jargon. Such a characteristic is exactly what differentiates
IT from the other two domains. IT sentences are technical and present a reduced average number of
3
http://sourceforge.net/projects/tercpp/
4
http://anrtrace.limsi.fr/trace_postedit.tar.bz2
5
www.matecat.com
413
words, as evidenced by the vocabulary size (the smallest among the three domains). These numbers
suggest a divergence between IT and the other two domains, possibly making adaptation more difficult.
4.2 Features
For all the experiments we use the same feature set composed of seventeen features proposed in (Specia et
al., 2009). The set is formed by features that model the complexity of translating the source sentence (e.g.
the average source token length or the number of tokens in the source sentence), and the fluency of the
translated sentence produced by the MT system (e.g. the language model probability of the translation).
The decision to use this feature set is motivated by the fact that it demonstrated to be robust across
language pairs, MT systems and text domains (Specia et al., 2009). The 17 features are:
? number of tokens in the source sentence and in the generated translation;
? average source token length;
? average number of occurences of the target word within the generated translation;
? language model probability of the source sentence and generated translation;
? average number of translations per source word in the sentence: as given by IBM 1 model thresh-
olded so that P (t|s) > 0.2 weighted by the inverse frequency of each word in the source side of the
SMT training corpus?;
? average number of translations per source word in the sentence: as given by IBM 1 model thresh-
olded so that P (t|s) > 0.01 weighted by the inverse frequency of each word in the source side of
the SMT training corpus;
? percentage of unigrams?, bigrams and trigrams? in the first quartile of frequency (lower fre-
quency words) in a corpus of the source language;
? percentage of unigrams?, bigrams and trigrams in the fourth quartile of frequency (higher fre-
quency words) in a corpus of the source language;
? percentage of unigrams in the source sentence seen in the source side of the SMT training corpus;
? number of punctuation marks in the source sentence and in the hypothesis translation;
4.3 Baselines
As a term of comparison, we consider these baselines in our experiments. A simple to implement but
difficult to beat baseline when dealing with regression on tasks with different distributions is to compute
the mean of the training labels and use it as the prediction for each testing point (Rubino et al., 2013).
Hereafter we refer to this baseline as ?. Since supervised domain adaptation techniques should outper-
form models that are trained only on the available in-domain data, we also use as baseline the regressor
built only on the available in-domain data (SVR in-domain). Furthermore, as a third baseline, we train a
regressor by pooling together training data of all domains, combining source and target data without any
kind of task relationship mechanism (SVR Pooling).
The baselines are trained on the feature set described earlier in Section 4.2 with an SVM regression
(SVR) method using the implementation of Scikit-learn (Pedregosa et al., 2011). The radial basis func-
tion (RBF) kernel is used for all baselines. The hyper-parameters of the model are optimized using
randomized search optimization process with 50 iterations as described in (Bergstra and Bengio, 2012)
and used previously for QE in (de Souza et al., 2013a). The best parameters are found using 5-fold
cross-validation on the training data and , ? and C are sampled from exponential distributions scaled at
0.1 for the first two parameters and scaled at 100 for the last one. It is important to notice that the SVR
with RBF kernel methods learn non-linear models that have been shown to perform better than linear
models on the set of features used for predicting HTER. On the contrary, the MTL methods presented in
Section 3 are methods that do not explore kernels or any other kind of non-linear learning method.
414
Source / Target IT
tgt
News
tgt
TED
tgt
IT
src
0.2081 0.2341 0.2232
News
src
0.2368 0.1690 0.2130
TED
src
0.2183 0.2263 0.1928
Table 2: Results of the SVR in-domain baseline trained and evaluated in each domain (average of 50
different shuffles). Rows represent the domain data used to train the model and columns represent the
domain data used to evaluate the model. Scores are MAE.
4.4 Evaluation
The accuracy of the models is evaluated with the mean absolute error (MAE), which was also used in
previous work and in the WMT QE shared tasks (Bojar et al., 2013). MAE is the average of the absolute
difference between the prediction y?
i
of a model and the gold standard response y
i
(Equation 4). As it is
an error measure, lower values mean better performance.
MAE =
1
m
m
?
i=1
|y?
i
? y
i
| (4)
To test the statistical significance of our results we need to perform comparisons of multiple models.
In addition, we would like to test the significance over different training amounts. Given these require-
ments we need to perform multiple hypothesis tests instead of paired tests. It has been shown that for
comparisons of multiple machine learning models, the recommended approach is to use a non-parametric
multiple hypothesis test followed by a post-hoc analysis that compares each pair of hypothesis (Dem?sar,
2006). In our experiments we use the Friedman test (Friedman, 1937; Friedman, 1940) followed by a
post-hoc analysis of the pairs of regressors using Holm?s procedure (Holm, 1979) to perform the pairwise
comparisons when the null hypothesis is rejected. All tests for both Friedman and post-hoc analysis are
run with ? = 0.05. For more details about these methods, we refer the reader to (Dem?sar, 2006; Garcia
and Herrera, 2008) which provide a complete review about the application of multiple hypothesis testing
to machine learning methods.
5 Results and Discussion
Our experiments are organized as follows. First, we evaluate the performance of single task learning
methods on different cross-domain experiments. Then, we report the evaluation for the multitask learning
methods and discuss the results.
5.1 Single Task Learning
With the objective of having an insight about the difference between the domains, we train the SVR
in-domain baseline with all available training data for each domain and evaluate its performance on the
same domain and in the two remaining domains.
Results are reported in Table 2, where the diagonal shows the figures for the in-domain evaluation.
These numbers suggest that the IT domain configures a more difficult challenge for the learning algo-
rithm. The IT in-domain model (IT
src
-IT
tgt
) presents a performance 21% inferior to News and 8%
inferior to TED. For all models trained on a source domain different than the target domain there is a
drop in performance, as it is expected from a system that assumes that training and test data are sampled
from the same distribution. In addition, when predicting IT using the model trained on News, we have a
perfomance drop of 13% whereas using the model trained on TED the performance drops up to 4%.
5.2 Multitask learning
We run the baselines described in Section 4.3 and the methods described in Section 3 on different
amounts of training data, ranging from 18 to 181 instances (10% and 100%, respectively). The mo-
tivation is to verify how much training data is required by the MTL methods to outperform the baselines
for a target domain. Table 3 presents the results for the three domains with models trained on 30, 50 and
415
100% of the training data (54, 90 and 181 instances, respectively). Each method was run on 50 different
train/test splits of the data in order to account for the variability of points in each split.
Method TED News IT
30 % of training data (54 instances)
mean 0.1951 0.1711 0.2174
SVR In-Domain 0.2013 0.1753 0.2235
SVR Pooling 0.1962 0.1899 0.2201
SVR FEDA 0.1952 0.1839 0.2193
MTL Dirty 0.1954 0.1708 0.2193
MTL SparseTrace 0.1976 0.1743 0.2222
MTL RMTL 0.1946 0.1685 0.2162
50% of training data (90 instances)
mean 0.1943 0.1707 0.2170
SVR In-Domain 0.1976 0.1711 0.2183
SVR Pooling 0.1951 0.1865 0.2191
SVR FEDA 0.1937 0.1806 0.2161
MTL Dirty 0.1927 0.1678 0.2148
MTL SparseTrace 0.1922 0.1672 0.2157
MTL RMTL 0.1878 0.1653 0.2119
100% of training data (181 instances)
mean 0.1936 0.1690 0.2162
SVR In-Domain 0.1928 0.1690 0.2081
SVR Pooling 0.1927 0.1849 0.2203
SVR FEDA 0.1908 0.1757 0.2107
MTL Dirty 0.1878 0.1666 0.2083
MTL SparseTrace 0.1881 0.1661 0.2094
MTL RMTL 0.1846 0.1653 0.2075
Table 3: Average performance of fifty runs
of the models on different train and test splits
with 30, 50 and 100 percent of training data.
The average scores reported are the MAE.
Figure 1: Visualization of the RMTL task outlier
model when trained on all the 181 instances of
training data. Cells with darker shades are closer
to zero. Cells with lighter shades are closer to one.
Columns with only black entries are considered in-
lier tasks (domains). From left to right, columns
correspond to News, TED and IT domains. The
first 17 rows correspond to the features used to
train the model and the last row in corresponds to
the bias term.
For all three domains, a general trend is that MTL RMTL is the method that reaches the lowest MAE
when compared to all the other models. Given the difference among the domains, it is very likely that
MTL Dirty and MTL SparseTrace suffer from the negative transfer problem (the assumption that all
tasks are similar does not hold). MTL RMTL is the only method among the methods presented here that
copes with negative transfer among tasks. The significance tests indicate that MTL RMTL improvements
are statistically significant with respect to all baselines depending on the range of training data used to
compute the test.
? For TED, the Friedman test rejects the null hypothesis with p = 4.62
?5
. Post-hoc analysis indicates
that there are differences statistically significant between MTL RMTL and all the three baselines
with p ? 0.002.
? For News, the Friedman test measures significant differences with p = 1.14
?9
and the post-hoc
analysis indicates that MTL RMTL is statistically significant with respect to SVR in-domain and
SVR Pooling with p = 0.002 for varying amounts of training data from 10 to 100%. As can be seen
in Figure 2, MTL RMTL starts with a very high MAE using 10% of the data (approximately 0.21
MAE) but improves dramatically with 20% of the data. Calculating the significance test with 20 to
100% of training data, MTL RMTL is significantly better than all baselines with p ? 2.89
?10
.
? For IT, in a similar situation to the News domain, RMTL is significantly better than all baselines
416
trained on 30% to 100% of the training data (Friedman test?s p = 2.86
?4
and post-hoc analysis?
p ? 3.73
?7
).
Another observed trend is that the MTL models benefit from increasing amounts of training data.
MTL RMTL has an improvement in performance of 5.13% for TED, 4% for News and 1.85% for IT
when trained on 100% of the training data in comparison with the model trained on 30% of training data.
18 36 54 72 90 108 126 144 162 181
Training data points
0.16
0.17
0.18
0.19
0.20
0.21
0.22
M
A
E
news as target domain
Mean
SVR RBF in-domain
SVR Pooling
SVR FEDA
MTL Dirty
MTL SparseTrace
MTL RMTL
Figure 2: Learning curves for the News domain.
The results for the IT domain are in line with the in-domain experiments in which we observed that
IT is a more challenging domain in comparison to TED and News. The MAE of IT is always higher
than for the other domains on in-domain and MTL experiments. Another evidence of this is the model
learned by the RMTL method when using all training data and run on one of the 50 training/test splits. A
graphic representation of the RMTL outlier task detection component (described in Section 3) is shown
in Figure 1.
From left to right, each column represents News, TED, and IT domains, respectively, while each row is
the instantiation of a feature in the corresponding task. Columns with non-black entries represent outlier
tasks. The highest number of entries with lighter shades is in the third column, IT. Several features in
this task are considered outliers with respect to the same features in the other tasks. Consequently, the
learning method takes the weights into consideration to a greater extent when learned with the outlier
model for the IT domain. Entries with the lightest shades in the IT domain correspond to the features
marked with? in Section 4.2. These outlier features are directly affected by the length of the sentences
on which they are computed (source or target) given that the number of tokens influences the final value
of the feature. This outcome goes in the same direction of our analysis of the three domains (Section 4.1)
that indicates a very different vocabulary size and average sentence length for IT when compared to the
other two domains.
To a lesser extent than IT, News and TED domains also present a few lighter-shaded entries in the
outlier component (1st and 2nd column). This suggests that MTL RMTL was capable of transfering
information among the domains in a more efficient way than the other MTL methods analyzed.
417
Overall the experiments presented show encouraging results in the direction of coping with QE data
coming from different domains/genres, translated by different MT systems and post-edited by different
translators. Results show that even in such difficult conditions, the methods investigated are capable of
outperforming competitive baselines based on non-linear models on different domains. As a rationale,
models that consider not only similarity between the domains but also deal with some sort of dissimilarity
should be considered. This is the case of the best performing method, MTL RMTL, which identifies
outlier tasks in order to avoid negative transfer among tasks.
6 Conclusion
In this work we presented an investigation of methods that overcome limitations presented by current
MT QE state-of-the-art systems when applied to real world conditions. In such scenarios (e.g. CAT
environment) the requirements are two-fold: (i) learning in the presence of different train/test feature
and label distributions and across different domains/genres, and (ii) the capability of learning with scarce
training data. In our experiments, we explored transfer learning methods, in particular multitask learning,
and we showed that such methods can cope with the needs of real-world scenarios.
We showed that multitask learning methods are capable to learn robust models for three different
domains that perform better than three strong baselines trained on the same amount of data. The methods
explored here benefit from increasing amounts of training data but also perform well when operating
with very limited amounts of data. We believe that the results obtained in this first exploration of model
adaptation for the problem can encourage the MT QE community to shift the focus from controlled
scenarios to more applicable, real-world contexts that require more robust methods.
Acknowledgements
This work has been partially supported by the EC-funded project MateCat (ICT-2011.4.2-287688).
References
Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. 2007. Multi-task feature learning. In Advances
in neural information processing systems, volume 19.
Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia Specia. 2013. SHEF-Lite: When less is more for translation
quality estimation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 337?342.
James Bergstra and Yoshua Bengio. 2012. Random Search for Hyper-Parameter Optimization. Journal of Ma-
chine Learning Research, 13:281?305.
Joseph John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto San-
chis, and Nicola Ueffing. 2003. Confidence estimation for machine translation. In 20th COLING, pages
315?321.
John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence
learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages
120?128, Morristown, NJ, USA. Association for Computational Linguistics.
Ondej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine
Translation. In Eighth Workshop on Statistical Machine Translation, pages 1?44.
Christian Buck. 2012. Black Box Features for the WMT 2012 Quality Estimation Shared Task. In Proceedings of
the 7th Workshop on Statistical Machine Translation, pages 91?95.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings
of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the 7th Workshop on Statistical
Machine Translation, pages 10?51, Montr{?e}al, Canada, June. Association for Computational Linguistics.
Rich Caruana. 1997. Multitask Learning. Machine learning, 28(1):41?75.
418
Jianhui Chen, Jiayu Zhou, and Jieping Ye. 2011. Integrating low-rank and group-sparse structures for robust multi-
task learning. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and
data mining - KDD ?11, page 42, New York, New York, USA. ACM Press.
Jianhui Chen, Ji Liu, and Jieping Ye. 2012. Learning incoherent sparse and low-rank patterns from multiple tasks.
ACM Transactions on Knowledge Discovery from Data, 5(4):22, February.
Trevor Cohn and Lucia Specia. 2013. Modelling Annotator Bias with Multi-task Gaussian Processes: An applica-
tion to Machine Translation Quality Estimation. In Proceedings of the 51st Annual Meeting of the Association
for Computational Linguistics, pages 32?42.
Hal Daum?e III. 2007. Frustratingly Easy Domain Ddaptation. In Conference of the Association for Computational
Linguistics (ACL).
Jos?e G. C. de Souza, Christian Buck, Marco Turchi, and Matteo Negri. 2013a. FBK-UEdin participation to
the WMT13 Quality Estimation shared-task. In Proceedings of the Eighth Workshop on Statistical Machine
Translation, pages 352?358.
Jos?e G.C. de Souza, Miquel Espl`a-Gomis, Marco Turchi, and Matteo Negri. 2013b. Exploiting qualitative infor-
mation from automatic word alignment for cross-lingual nlp tasks. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Volume 2: Short Papers), pages 771?776, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Janez Dem?sar. 2006. Statistical Comparisons of Classifiers over Multiple Data Sets. The Journal of Machine
Learning Research, 7:1?30, December.
Milton Friedman. 1937. The Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of
Variance. Journal of the American Statistical Association, 32(200):675?701.
Milton Friedman. 1940. A Comparison of Alternative Tests of Significance for the Problem of m Rankings. The
Annals of Mathematical Statistics, 11(1):86?92.
Salvador Garcia and Francisco Herrera. 2008. An Extension on ?Statistical Comparisons of Classifiers over
Multiple Data Sets? for all Pairwise Comparisons. Journal of Machine Learning Research, 9:2677?2694.
Christian Hardmeier, Joakim Nivre, and Jorg Tiedemann. 2012. Tree Kernels for Machine Translation Quality
Estimation. In Proceedings of the 7th Workshop on Statistical Machine Translation, number 2011, pages 109?
113.
Sture Holm. 1979. A Simple Sequentially Rejective Multiple Test Procedure. Scandinavian Journal of Statistics,
6(2):pp. 65?70.
Ali Jalali, PD Ravikumar, S Sanghavi, and C Ruan. 2010. A Dirty Model for Multi-task Learning. In Advances in
Neural Information Processing Systems (NIPS) 23.
Jing Jiang and Chengxiang Zhai. 2007. Instance Weighting for Domain Adaptation in NLP. In Proceedings of the
45th Annual Meeting of the Association for Computational Linguistics, number June, pages 264?271.
Jing Jiang. 2009. Multi-Task Transfer Learning for Weakly-Supervised Relation Extraction. In ACL ?09 Proceed-
ings of the Joint Conference of the 47th Annual Meeting of the ACL, number August, pages 1012?1020.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zenz, Chris Dyer, Ondej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL 2007 Demo and Poster
Sessions, number June, pages 177?180.
Yashar Mehdad, Matteo Negri, and Marcello Federico. 2012. Match without a Referee : Evaluating MT Adequacy
without Reference Translations. In Proceedings of the 7th Workshop on Statistical Machine Translation, pages
171?180.
Sinno Jialin Pan and Qiang Yang. 2010. A Survey on Transfer Learning. IEEE Transactions on Knowledge and
Data Engineering, 22(10):1345?1359, October.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-
ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-
napeau, Mathieu Brucher, Mathieu Perrot, and
?
Edouard Duchesnay. 2011. Scikit-learn : Machine Learning in
Python. Journal of Machine Learning Research, 12:2825?2830.
419
Raphael Rubino, Jennifer Foster, Joachim Wagner, Johann Roturier, Rasul Samad Zadeh Kaljahi, and Fred Hol-
lowood. 2012. DCU-Symantec Submission for the WMT 2012 Quality Estimation Task. In Proceedings of the
Seventh Workshop on Statistical Machine Translation, pages 138?144, Montr{?e}al, Canada, June. Association
for Computational Linguistics.
Raphael Rubino, Jos?e G. C. de Souza, Jennifer Foster, and Lucia Specia. 2013. Topic Models for Translation
Quality Estimation for Gisting Purposes. In Machine Translation Summit (MT Summit) XIV, pages 295?302.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Trans-
lation Edit Rate with Targeted Human Annotation. In Association for Machine Translation in the Americas.
Radu Soricut and A Echihabi. 2010. Trustrank: Inducing trust in automatic translations via ranking. In Proceed-
ings of the 48th Annual Meeting of the Association for Computational Linguistics, number July, pages 612?621.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012. The SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceedings of the 7th Workshop on Statistical Machine Translation, pages
145?151.
Jos?e G. C. de Souza, Jes?us Gonz?alez-Rubio, Christian Buck, Marco Turchi, and Matteo Negri. 2014. FBK-UPV-
UEdin participation in the WMT14 Quality Estimation shared-task. In Proceedings of the Ninth Workshop on
Statistical Machine Translation, Baltimore, MD, USA, June.
Lucia Specia, Marco Turchi, Nello Cristianini, Nicola Cancedda, and Marc Dymetman. 2009. Estimating the
Sentence-Level Quality of Machine Translation Systems. In Proceedings of the 13th Annual Conference of the
EAMT, number May, pages 28?35.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010. Machine translation evaluation versus quality estimation.
Machine Translation, 24(1):39?50, May.
Lucia Specia, Stafford Street, Regent Court, and Mariano Felice. 2012. Linguistic Features for Quality Estimation.
In Proceedings of the 7th Workshop on Statistical Machine Translation, pages 96?103.
Lucia Specia. 2011. Exploiting objective annotations for measuring translation post-editing effort. In Proceedings
of the European Association for Machine Translation, number May, pages 73?80.
Marco Turchi, Josef Steinberger, and Lucia Specia. 2012. Relevance ranking for translated texts. In Proceedings
of the 16th Annual Conference of the European Association for Machine Translation, number May, pages 153?
160.
Marco Turchi, Matteo Negri, and Marcello Federico. 2013. Coping with the Subjectivity of Human Judgements
in MT Quality Estimation. In Proceedings of the 8th Workshop on Statistical Machine Translation (WMT?13),
Sofia, Bulgaria, August.
Marco Turchi, Antonios Anastasopoulos, Jos?e G. C. de Souza, and Matteo Negri. 2014. Adaptive Quality Estima-
tion for Machine Translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational
Linguistics.
Guillaume Wisniewski, Anil Kumar Singh, Natalia Segal, and Franc?ois Yvon. 2013. Design and Analysis of
a Large Corpus of Post-Edited Translations: Quality Estimation, Failure Analysis and the Variability of Post-
Edition. In Machine Translation Summit XIV, pages 117?124.
Jiayu Zhou, Jianhui Chen, and Jieping Ye. 2012. MALSAR: Multi-tAsk Learning via StructurAl Regularization.
420
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1813?1823, Dublin, Ireland, August 23-29 2014.
Quality Estimation for Automatic Speech Recognition
Matteo Negri
(1)
Marco Turchi
(1)
Jos
?
e G. C. de Souza
(1,2)
Daniele Falavigna
(1)
(1)
FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy
(2)
University of Trento, Italy
{negri,turchi,desouza,falavi}@fbk.eu
Abstract
We address the problem of estimating the quality of Automatic Speech Recognition (ASR) out-
put at utterance level, without recourse to manual reference transcriptions and when information
about system?s confidence is not accessible. Given a source signal and its automatic transcription,
we approach this problem as a regression task where the word error rate of the transcribed utter-
ance has to be predicted. To this aim, we explore the contribution of different feature sets and
the potential of different algorithms in testing conditions of increasing complexity. Results show
that our automatic quality estimates closely approximate the word error rate scores calculated
over reference transcripts, outperforming a strong baseline in all the testing conditions.
1 Introduction
In recent years, the increasing usage of large vocabulary continuous speech recognition (LVCSR) systems
to transcribe audio recordings from different sources (e.g. Youtube videos, TV programs, DVD movies,
meetings, etc) has sparked the need of accurate, fast and cost-effective methods to estimate the quality
of ASR output. This need contrasts with the fact that, after decades of progress in ASR research, the
established evaluation protocol is based on computing word error rate scores (WER)
1
over large test
sets of hand-crafted reference transcriptions. Indeed, despite its reliability, reference-based performance
assessment has an evident drawback represented by the cost of acquiring manual transcripts. Besides
increasing the cost-effectiveness of ASR evaluation routines, bypassing this bottleneck has several other
motivations. From an application perspective, for instance, reference-free quality estimation methods
could be used to: i) decide at run-time whether a given input signal has been properly recognized (e.g.
if a user spoken utterance needs to be repeated in a dialogue application), ii) decide if an automatic
transcription is acceptable as is (e.g. if manual revision is needed in an automatic subtitling application),
or iii) select the best transcription among options from multiple ASR systems.
When information about the inner workings of the system used to produce the transcriptions is acces-
sible, current reference-free confidence estimation methods can supply ASR applications with reliable
indicators about output reliability. This condition, however, does not always hold in the aforementioned
scenarios. A clear motivating example is provided by the exponential growth of captioned TED Talks
and Youtube videos,
2
for which no information is available about how transcriptions have been pro-
duced. In this case, neither reference-based methods, nor standard confidence measures can be applied
to obtain useful quality estimates. Nevertheless, in this scenario, supplying reliable indicators of tran-
scription quality has a huge market potential (e.g. to reduce the costs of manual revision/translation)
which motivates our research.
Focusing on these compelling needs, this paper investigates the automatic prediction of ASR out-
put quality when: i) manual reference transcripts are not available and ii) information about the
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
The word error rate is the minimum edit distance between an hypothesis and the reference transcription. Edit distance is
calculated as the number of edits (word insertions, deletions, substitutions) divided by the number of words in the reference.
2
Since 2009, Youtube videos in English can be automatically captioned. In 2012, for the 72 hours of video uploaded per
minute, such functionality was already available for 10 languages. Currently, more than 200 million Youtube videos have either
automatic or human-created captions (source: http://goo.gl/9swYSS).
1813
inner workings of the ASR system is not accessible. Casting the problem as a supervised regression
task, we experiment in a range of testing conditions on a well-known LVCSR setting (i.e. the automatic
transcription of TED talks). In this framework, we analyse the performance of various models (i.e. their
capability to predict utterance-level WER scores) as a function of the different learning algorithms used,
the proposed features, and the amount of training data available.
Our features are categorized according to the type of information they aim to capture. Since the na-
ture of the proposed features is a relevant aspect for the applicability of our approach, an important
distinction is made between ?glass-box? and ?black-box? features, which are respectively informed and
agnostic about systems? internal decoding strategies. The former can play an important role when all the
intermediate processing steps are accessible (e.g. in the selection of the best possible transcription hy-
pothesis). In contrast, black-box features have a wider applicability to situations where such information
is not available (e.g. to estimate the quality of online video subtitles).
Another important aspect relevant to our study is the relation between the accuracy of utterance-level
quality predictions and the degree of homogeneity of training and test data. Indeed, as in any supervised
learning framework, the similarity between training and test data has a direct impact on (classification
and regression) results. In order to fully understand the potential of our approach, we hence measure
performance variations under different levels of similarity between the data used to train the regressor
and the data used for evaluation. To this aim, our experiments account for a range of possible conditions.
These vary from the situation in which training and test are fully homogeneous (i.e. same dataset, with
training instances produced by the same ASR system) to the more challenging situation where training
and test are not homogeneous (i.e. different datasets, with training instances produced by different ASR
systems). Our results, obtained with two different state-of-the-art algorithms for regression, demonstrate
that in all such variable conditions our ASR quality estimation models lead to accurate predictions (i.e.
close the word error rate scores calculated over reference transcripts).
To the best of our knowledge, this paper represents the first extensive investigation on reference-
free and system-agnostic automatic estimation of ASR output quality. Along this direction, our main
contributions can be summarized as follows:
1. We propose a supervised, application-oriented approach to ASR quality estimation that bypasses
the need of manual reference transcriptions and is system-independent.
2. We evaluate our method with different learning algorithms and in different conditions, showing that
its estimates closely approximate the WER scores calculated over reference transcripts.
3. We perform feature analysis, isolating the contribution of each feature set in all the testing condi-
tions.
4. We analyse the learning curves of our best models, investigating the relation between performance
results and the amount of data needed for training.
Overall, these contributions provide useful insights about the feasibility of automatic ASR quality esti-
mation, opening interesting research avenues relevant for system development and for ASR applications.
2 Related Work
As a reference-free automatic evaluation method, our work introduces a valid application-oriented alter-
native to the standard evaluation protocols used within current ASR evaluation campaigns such as IWSLT
(Federico et al., 2011; Federico et al., 2012; Cettolo et al., 2013).
3
Besides that, our approach to ASR
quality estimation (QE) also differs from the well-established confidence estimation (CE) techniques
proposed in previous ASR literature (Sukkar and Lee, 1996; Evermann and Woodland, 2000; Wessel et
al., 2001; Sanchis et al., 2012; Seigel, 2013, inter alia). Such difference firstly relies in the fact that,
while in CE is the system itself that provides an indicator of the reliability of its output transcriptions,
QE aims to provide an external and more objective measure of goodness through WER predictions. A
3
See http://www.iwslt2013.org/ for details about the last edition of the IWSLT Workshop held in 2013.
1814
second (related) difference is that, in contrast with previous CE methods that heavily rely on information
about the internal behaviour of the ASR system, our technique does not necessarily depend on the access
to such information. This extends its applicability to scenarios (out of the scope of CE research) where
the quality of transcriptions produced by (possibly unknown) ASR systems has to be evaluated/compared
solely based on information about the input audio signals and the output transcriptions.
An interesting approach exploiting ASR word accuracy estimates to automatically score the profi-
ciency of non-native English speakers has been proposed by Yoon et al. (2010). To our knowledge this
work is the most similar to the one presented here, although it differs in the application domain and sev-
eral other aspects. First of all, similar to CE methods, it makes some use of glass-box features derived
from knowledge about the ASR internal workings (e.g. word confidence and acoustic/language model
probabilities). Secondly, the domain addressed is constrained to responses to prompted utterances, while
in this paper we address a large unconstrained domain, namely the automatic transcription of lectures
(TED talks) covering different topics. Finally, (Yoon et al., 2010) is based on a rather simple model
whose performance is not carefully analysed from the learning point of view (e.g. by comparing the
contribution different state-of-the-art algorithms) as we do here.
The problem of automating system evaluation without a gold standard has been addressed also in other
NLP areas. For instance, (Louis and Nenkova, 2013) recently addressed the assessment of machine-
generated summaries without model summaries. The strongest parallelism with our work, however,
can be found in the Machine Translation (MT) evaluation field, where the goal of bypassing the need
of manually-created reference translations has motivated a large body of research.
4
Quality estimation
for MT and ASR have a number of commonalities. First, they both deal with a ?source? (respectively
a sentence in a language L and an acoustic utterance) and an ?hypothesis? whose quality has to be
estimated without references (respectively a translation in a language L1 and an automatic transcription
of the audio signal). Second, they can be addressed at various granularities. Indeed, ASR output quality
estimation is similar to its MT counterpart where research focused on quality predictions at word level
(Ueffing and Ney, 2007; Bach et al., 2011), sentence level (Specia et al., 2009; Mehdad et al., 2012)
and document level (Soricut and Echihabi, 2010). Third, both tasks are suitable for supervised machine
learning methods, either for classification (Blatz et al., 2003; Quirk, 2004) or for regression (Specia et
al., 2010; Specia, 2011). Finally, both tasks motivate efforts in designing features capable to capture
the difficulty to process the source, the plausibility of the output hypothesis and (but not necessarily) the
confidence of the decoding process (Felice, 2012; Rubino et al., 2013b).
3 Approach
We approach the automatic estimation of ASR output quality as a supervised regression problem. Given
a training set of (signal, transcription, WER) instances, the task is to predict the WER of each instance
in a test set of unseen (signal, transcription) pairs.
Features. As shown in Table 1, the features used in our experiments (68 in total) can be categorized in
four main groups. The first group (ASR features) includes several glass-box features proposed in previous
literature on ASR confidence estimation (Litman et al., 2000; Gabsdil and Lemon, 2004; Goldwater et al.,
2010; Higgins et al., 2011). These features are suitable only for the ideal situation in which information
about systems? internal decoding strategies is available (as in the experiments discussed in ?4.1). We use
them as a term of comparison to evaluate the usefulness of the other three groups (signal, hybrid and
textual), which belong to the black-box type. These features, which are totally uninformed about the
decoding process, have wider applicability to the system-independent ASR quality estimation tasks that
represent our target scenario (see Sections 4.2 and 4.3). More in detail:
? ASR features aim to capture the confidence of the speech recognizer and the reliability of the whole
decoding process. In our experiments, as we do not have access to decoders of other systems, they
are computed only for the ASR system developed in our labs (Falavigna et al., 2013). These features
4
For a complete overview of the current approaches to MT quality estimation we refer the reader to the WMT12 and WMT13
shared task reports (Callison-Burch et al., 2012; Bojar et al., 2013).
1815
are extracted both from word graphs (WGs) and n-best lists (n=100). In Table 1 ?Total probabil-
ity? is the weighted sum of log Language Model (LM) and log Acoustic Model (AM) probabilities.
LM probability is computed with a 4-gram backoff LM, trained over about 5 billion words using
the IRSTLM toolkit (Federico et al., 2008) and the modified shift-beta smoothing method. AM
probability is computed using a set of tied-state triphone Hidden Markov Models having, as output
state density, a mixture of Gaussian probability densities with diagonal covariance matrices. ?Mean
probability? is obtained dividing the total probability by the number of hypothesized ASR output
items (words + silences). Confidence scores are computed averaging time posterior word proba-
bilities (Evermann and Woodland, 2000). ?Proportion of low confidence words? is the fraction of
words having confidence values ? 0.5. The remaining ASR features are directly extracted from
word graphs and n-best lists scores.
? Signal features aim to capture the difficulty to transcribe a given input looking at the signal as a
whole. They are computed from raw vectors extracted through frame analysis (we employ 20ms
analysis window and 10ms analysis step). For each analysed window, 12 Mel Frequency Cepstral
Coefficients (MFCCs) are evaluated plus log energy. Then, for each given segment, minimum,
maximum and mean values of raw energy, as well as the mean MFCCs values and total segment
duration, are computed to form the signal feature vector.
? Hybrid features provide a more fine-grained way to capture the difficulty of transcribing the signal.
This is done by considering information about word and silence/noise regions, as well as their
respective duration. These features are computed after having performed forced alignment between
the input audio signal and the corresponding automatic hypotheses. Forced alignment is carried
out with our ASR system (Falavigna et al., 2013), in order to detect audio segments related to
words, hesitations and silences in the hypothesis. Pitch features have been computed with the Praat
software tool (Boersma and Weenink, 2005).
? Textual features aim to capture the plausibility (i.e. the fluency) of an output transcription. To
this aim, we consider surface information (such as the number of words and the percentage of
numbers/content-words/nouns/verbs in the hypothesis) as well as information about LM perplexity
and probability of the hypothesis (both at the level of words and parts of speech)
5
.
Feature selection is performed throughout all our experiments to maximize results and, at the same
time, analyse the contribution of the proposed features. To this aim, we use Randomized Lasso, or
stability selection (Meinshausen and B?uhlmann, 2010), which re-samples the training data several times
and fits a Lasso regression model on each sample. Features that appear in a given number of samples are
considered more informative for the task at hand, and hence retained (those marked in bold in Table 1
are the most informative ones based on the experiments described in Sections 4.2 and 4.3).
Learning algorithms. To build our regression models we experimented with two non-parametric learn-
ing approaches: Support Vector Machines (SVMs) (Shawe-Taylor and Cristianini, 2004) and Extremely
Randomized Trees (XT) (Geurts et al., 2006). SVMs are non-parametric deterministic algorithms that
have been widely used in several fields, in particular in NLP where they are the state-of-the-art for various
tasks. Extra-Trees are a tree-based ensemble method for supervised classification and regression that
were also successfully used for MT quality estimation (de Souza et al., 2013; de Souza et al., 2014a). In
XTs each tree can be parametrized differently. When a tree is built, the node splitting step is done at ran-
dom by picking the best split among a random subset of the input features. The results of the individual
trees are combined by averaging their predictions. Hyper-parameter optimization of the SVM (with Ra-
dial basis function kernel ? RBF) and XT models was performed using randomized search (Bergstra and
Bengio, 2012). We used both learning methods as implemented in the Scikit-learn package (Pedregosa
et al., 2011).
5
The PoS LM has been obtained by processing with the TreeTagger (Schmid, 1995) the same data used for the word LM.
6
Hesitations, such as ?uhm?, ?eh? and ?ah? are found through matches with a predefined list. Consecutive repeated words
in the same utterance are also considered as hesitations.
1816
ASR (16)
Total probability of ASR output (w ? logP
LM
+ logP
AM
), mean probability, total
acoustic probability, mean acoustic probability, mean confidence score, Std of confi-
dence scores, confidence scores per second, proportion of low-confidence words, WG
node density, WG transition density, Mean/Std/Min n-best probability, Mean/Std/Min
n-best acoustic probability.
Signal (16)
Total segment duration (sec), Mean/Min/Max raw energy (dB), mean MFCC[1, 2,
3, 4, 5, 6, 7, 8, 9, 10, 11,12].
Hybrid (26)
SNR (dB), mean noise energy (dB), Mean/Min/Max word energy (dB), Min/Max
noise energy (dB), (max word - min noise) energy (dB), # silences, ratio of silences
and words, # words per second, # silences per second, total duration of words
(sec), total duration of silences (sec), mean duration of words (sec), mean duration
of silences (sec), ratio of (tot duration silences) and (tot duration words), Std of word
duration (sec), Std of silence duration (sec), (tot duration words) - (tot duration
silences), Mean/Std/Min./Max. pitch (Hz), # hesitations,
6
frequency of hesitations.
Textual (10)
Number of words, LM log probability of the hypothesis, LM log probability of
POS of the hypothesis, LM log perplexity of POS of the hypothesis, Perplexity of
the hypothesis, % of numbers in the hypothesis, % of tokens in the hypothesis which
do not contain only a-z, % of content words in the hypothesis, % of nouns in the
hypothesis, % of verbs in the hypothesis
Table 1: Full list of the 68 features used in our experiments, divided into four groups. The most predictive
black-box features (resulting from feature selection in the ?4.3 experiments) are marked in bold.
4 Experiments
To evaluate our approach we carried out three sets of experiments. In each set our feature groups are
analysed: i) with the two learning algorithms, ii) in combination/isolation, iii) with/without feature se-
lection. The three sets differ in terms of the difficulty of the quality estimation task from the learning
point of view. To experiment with situations of increasing complexity, we alternate conditions in which
all the features (glass-box and black-box) can be used, training and test sets are non-/homogeneous, the
quality estimator is trained on transcriptions generated by the same/different ASR systems.
Data. The data used in the experiments consists of the audio recordings delivered for the IWSLT 2013
evaluation campaign (Cettolo et al., 2013). One of the tasks of IWSLT 2013 is the automatic tran-
scription of English TED talks, a global set of conferences whose audio/video recordings are publicly
available. The main challenges for ASR in these talks include: the large variability of topics (hence
a large, unconstrained vocabulary), the presence of non-native speakers and a rather informal speaking
style. Each IWSLT participant submitted one primary ASR output run for each of the talks included in
the test set plus some optional contrastive ASR outputs. In addition, participants sent submissions for
the ASR tracks delivered for the 2012 evaluation campaign. Our experiments have been carried out on
the primary submissions, sent by 8 participants, related to the 2012 (consisting in 11 different talks) and
2013 (28 different talks) test sets. The 2012 test set has a total duration of around 1h45sec, it contains
1,118 reference sentences and 18,613 running words. On such dataset, participants? primary submissions
achieved a mean utterance WER ranging from 10.5% to 18.4% (in this work a WER score is computed
for each reference sentence, and mean utterance WER represents the average of sentence WERs). The
2013 test set has a total duration of around 3h55sec, it contains 2,238 reference sentences and 41,545 run-
ning words. On this dataset, primary participants? submissions achieve a mean utterance WER ranging
from 15.9% to 30.8%.
In our experiments, we always use 1,118 utterances for training the regressor and 1,120 for testing. To
this aim, the IWSLT 2013 data is randomly sampled three times in training and test sets of such dimen-
sions. While for the 2012 test set manual utterance segmentation has been provided by the organizers, for
the 2013 data the participants had to employ their own automatic segmentation systems before decoding
the audio tracks (thus resulting in a different number of ASR sentence hypotheses for each team). Hence,
1817
to ensure that each participant has the same number of ASR sentence hypotheses, an alignment with the
reference manual segmentation has been performed in our experiments.
Evaluation. Our evaluation is carried out in terms of Mean Absolute Error (MAE), a standard metric
for regression problems. The MAE is the average of the absolute errors e
i
= |f
i
? y
i
|, where f
i
is
the prediction of the model and y
i
is the actual WER for the i
th
test instance. WER is calculated with
the NIST SCLITE Scoring Package.
7
As it is a measure of error, lower MAE scores indicate that our
predictions are closer to the real WER calculated for each test instance against the reference transcripts.
For each experiment, we report the mean and the standard deviation of the MAE achieved by the best
performing QE models on the IWSLT 2013 test sets.
Baseline. Besides measuring performance in terms of global MAE, each model is compared against a
common baseline for regression tasks. This baseline, which is particularly relevant in settings featuring
different data distributions between training and test sets, is calculated by labelling each test instance
with the mean WER score calculated on the training set. Previous works, also in MT quality estimation,
demonstrated that its results can be particularly hard to beat (Rubino et al., 2013a).
4.1 Experiment 1
In the first set of experiments we consider the easiest situation from the learning perspective. In this
setting we predict the WER of transcriptions produced by our ASR system (denoted by X), whose inner
workings are known (thus enabling the use of glass-box features). To investigate the relation between
prediction accuracy and the degree of homogeneity of training and test data, we experiment both with
similar datasets (disjoint training and test sampled from IWSLT13) and different datasets (IWSLT12
for training and samples from IWSLT13 for test). Results are reported in Table 2, where the notation
?LetterYear - LetterYear? indicates the systems and the datasets used for training and test (respectively
our system X, and data from IWSLT12 and/or IWSLT13).
Train - Test ALL (glass-box + BB COMB) ASR (glass-box) BB COMB (Signal+Hybrid+Textual) Baseline
X13 - X13 11.56?0.29 SVR 12.11?0.29 XT 15.17?0.06 XT 19.84?0.06
X12 - X13 12.61?0.13 XT 13.78?0.16 XT 16.78?0.18 XT 19.06?0.12
Train - Test Signal Hybrid Textual Baseline
X13 - X13 16.42?0.1 XT 17.61?0.12 XT 17.42?0.15 SVR 19.84?0.06
X12 - X13 18.85?0.09
?
XT 18.39?0.22 XT 17.58?0.15 XT 19.06?0.12
Table 2: MAE results using the same system on different datasets, with and without glass-box features.
As can be seen from the table, the two models using ALL the features achieve the largest improvements
over the strong baseline used for comparison (up to 8.2 MAE points in the X13 - X13 setting). This is
not surprising if we consider the high predictive power of ASR (glass-box) features that, when used in
isolation, lead to a considerably lower MAE with respect to the other three groups. However, it?s worth
observing that also the combination of only the black-box features (BB COMB) allows the QE predictors
to significantly outperform the baseline (up to 4.67 MAE points in X13 - X13). Such improvements come
from the joint contribution of each of the three groups, which achieve good results also in isolation.
Indeed, except in one case where the gain over the baseline is not significant
8
(X12 - X13 with the Signal
features), their MAE reduction ranges between 0.67 (X12 - X13 Hybrid) and 3.42 MAE points (X13 -
X13 Signal). The good prediction capability of the black-box features is also shown by the fact that, when
combined with the glass-box features, they lead to improvements between 0.55 and 1.17 MAE points
over the ASR features alone. Considering the privileged condition of the (system-informed) glass-box
features, this is a remarkable result that suggests some complementarity between the two groups.
In general, our supervised approach is sensitive to the similarity between training and test. This is
evidenced by higher MAE results when non-homogeneous datasets (i.e. X12 - X13) are processed. In
7
http://www1.icsi.berkeley.edu/Speech/docs/sctk-1.2/sclite.htm
8
Statistical significance is measured by considering the overlap of confidence intervals defined by the standard deviation
range around the mean. In our tables, the results marked with the ??? symbol are not significantly better than the baseline.
1818
terms of algorithms, XT generally performs better than SVR, in particular when the QE model is trained
and tested on non-homogeneous data. This can be explained by their higher generalization capability
due to variance reductions as explained in (Hastie et al., 2009, Chapter 15).
4.2 Experiment 2
In this set of experiments we consider a situation of intermediate difficulty from the learning perspective.
Our objective is to evaluate, on homogeneous datasets (sampled from IWSLT13), the output of ASR
systems whose inner workings are not known (hence only black-box features can be used). To make
our analysis more complete, we also evaluate the performance of models trained on a given ASR system
to predict the WER of hypotheses produced by a different one. This situation is closer to application
scenarios in which the evaluated ASR system is unknown and different from the one used to train the
quality estimator. Two systems with very different performance are considered for this purpose: the best
and the worst according to the official IWSLT 2013 ranking (respectively denoted by A and Z).
Train - Test BB COMB Signal Hybrid Textual Baseline
A13 - A13 11.18?0.22 SVR 11.91?0.23 SVR 12.76?0.18 SVR 12.57?0.13 SVR 14.35?0.1
Z13 - A13 16.01?0.23 SVR 18.04?0.22 SVR 17.24?0.22 SVR 18.01?0.2 XT 21.58?0.15
Z13 - Z13 15.52?0.6 XT 16.94?0.41 XT 17.04?0.56 SVR 17.84?0.4 XT 19.65?0.43
A13 - Z13 17.36?0.43 XT 18.7?0.53 XT 18.21?0.45 XT 19.38?0.45 XT 21.03?0.51
Table 3: MAE results using different systems on the same dataset, without glass-box features.
The results reported in Table 3 confirm that: i) the combination of black-box features (BB COMB)
always leads to the best QE models, which significantly outperform the baseline, ii) the same holds
also when each single group is used in isolation, iii) with less homogeneous training and test data, XT
performs generally better than SVR.
In addition, it?s worth noting that when a QE model is trained and tested on data transcribed by
the same ASR system the results are significantly better (the MAE is always about 1.0 - 6.0 points
lower). Indeed, as also shown by the same behaviour of our baseline, this condition is simpler and more
suitable for supervised learning methods. This depends on the fact that each ASR system has its own
coherent behaviour, which results in transcriptions with similar characteristics that supervised models
are able to learn (e.g. recurring errors, similar WER distributions). In contrast, when training and test
data are produced by different ASR systems, supervised learning becomes more difficult and the output
predictions less reliable. Each feature group is affected by this situation, but it is interesting to note that
the Hybrid features are more robust than the other two groups to less homogeneous datasets. This can be
explained by the fact that they are extracted after applying forced alignment by means of a third system,
which is likely to normalise and reduce the difference between training and test data. Overall, also in
this more complex scenario where the glass-box features cannot be used, our results demonstrate a good
prediction capability of the QE models, which are still able to beat a strong baseline.
4.3 Experiment 3
In the third set of experiments we consider the hardest case from the learning point of view. In this setting
the evaluated ASR systems are unknown and training/test data are non homogeneous (i.e. training from
IWSLT12, test from samples of IWSLT13). Results are reported in Table 4.
Train - Test BB COMB Signal Hybrid Textual Baseline
A12 - A13 12.81?0.08 XT 13.57?0.13
?
XT 12.85?0.1 XT 13.25?0.23
?
XT 13.65?0.17
Z12 - A13 14.78?0.1 SVR 15.66?0.09
?
XT 13.56?0.09 SVR 13.63?0.24 SVR 15.51?0.35
Z12 - Z13 17.16?0.4 XT 19.34?0.32
?
XT 17.68?0.3 XT 19.59?0.11
?
XT 19.98?0.29
A12 - Z13 19.83?0.23 XT 21.85?0.2 XT 20.68?0.13 XT 22.62?0.08 XT 23.04?0.18
Table 4: MAE results using different systems on different dataset, without glass-box features.
Also in the most challenging scenario our results substantially confirm the previous findings. Indeed,
except in one case (Z12 - A13), the following observations still hold: i) when used in combination, the
1819
0 10 20 30 40 50 60 70 80 90 10012
14
16
18
20
22
24
26
% of Training Data
MA
E
 
 
A12 ? A13
A12 ? Z13
Z12 ? Z13
Z12 ? A13
Figure 1: Learning curves for the best systems of ?Experiment 3? (using BB COMB features).
black-box features (BB COMB) lead to the best QE models, which significantly outperform the baseline,
ii) this holds also when each single group is used in isolation (although not significantly in 5 out of 12
settings), iii) with less homogeneous training and test data, XT performs generally better than SVR.
Unsurprisingly, as also observed in the previous set of experiments, the low homogeneity of training
and test data has an impact on the accuracy of the predictions. The effect of training and testing on
less homogeneous data produced by different systems is now clearly visible. Except for the more robust
Hybrid features, which in the Z12 - A13 setting produce the best model, the results obtained with the two
other groups decreased to the point that their improvement over the baseline is often not significant. Nev-
ertheless, even under the challenging conditions posed by this realistic and application-oriented scenario,
reference-free and system-agnostic ASR evaluation remains a feasible task.
5 Feature Analysis and Learning Curves
In order to gain additional insights about the effectiveness of our method, we performed a further analysis
of the ?Experiment 3? results. In such challenging scenario, the most interesting from the application
perspective, we first identified the most predictive features among those in the BB COMB set. To this
aim, we collected the features that are always chosen by the feature selection algorithm proposed in ?3.
The resulting list contains features from all the three black-box groups (marked in bold in Table 1). This
confirms their complementarity in predicting the quality of a transcribed utterance.
In the same setting, we also investigated the relation between the amount of data used to train our
models and the accuracy of their predictions. To this aim, we measured performance variations when
the same models (i.e. those obtained with the BB COMB set) are trained on different amounts of data.
For each training set, nine subsets were created (with 10%, 20%,..., 90% of the data) by sub-sampling
sentences from a uniform distribution. The process was iterated 5 times. Each subset was used to build
the relative QE regressor, which was then evaluated on our test sets. Figure 1 shows the resulting learning
curves (each point is the average result of the 5 runs on each test set; the error bars show ?1std). As
can be seen from all the curves, after an initial fluctuation of the MAE, performance results with 40% of
the training data are comparable with those obtained using the whole training set. Moreover, it?s worth
remarking that in three out of four cases the models trained with such amount of data already outperform
the baseline (for Z12 - A13 the MAE is only 0.01 point higher). This suggests that reference-free, system-
independent models for ASR quality estimation are able to provide informative predictions even with a
limited amount (?400 manual transcripts) of training instances.
1820
6 Conclusion
We investigated the problem of automatically predicting the word error rate of an automatically-
transcribed utterance in a large vocabulary continuous speech recognition setting. In such scenario,
we proposed a supervised regression approach that bypasses the need of manual reference transcriptions
and does not necessarily depend on information about system?s confidence (first contribution of the pa-
per). Then, by evaluating models obtained with different state-of-the-art learning algorithms, we showed
that our automatic predictions outperform a strong baseline and closely approximate the WER scores
calculated over reference transcripts (second contribution). Different feature groups have been proposed
and their contribution has been analysed in a range of testing conditions of increasing difficulty (third
contribution). This made possible to isolate informative features that significantly contribute to the per-
formance of our quality estimation models, and to get useful insights about the potential of our approach
when different sources of information (glass-box, black box features) are available. Finally, analysing
the relation between prediction performance and the size of the training set, we showed that the results
obtained with 40% of the data are already comparable to our best MAE (fourth contribution).
Our analysis revealed a dependency between the performance of the quality estimation models and
the degree of homogeneity between training and test data. This aspect is particularly relevant from the
application perspective since in real working conditions the availability of large amounts of representa-
tive training instances is far from being guaranteed. In quality estimation for machine translation (a task
featuring strong similarities with ours), these issues have recently motivated studies on domain adapta-
tion and online learning techniques (de Souza et al., 2014b; Turchi et al., 2014). This suggests, as a first
direction for future work, the investigation of approaches capable to better exploit the available training
data and mitigate the impact of large differences between training and test instances.
Acknowledgements
This work has been partially funded by the European project EU-BRIDGE (FP7-287658) and by the
Autonomous Province of Trento, Italy, under the project Wikivoice (L.P. 6/1999).
References
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. 2011. Goodness: a Method for Measuring Machine Translation
Confidence. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pages 211?219. The
Association for Computer Linguistics.
James Bergstra and Yoshua Bengio. 2012. Random Search for Hyper-Parameter Optimization. Journal of Ma-
chine Learning Research, 13:281?305.
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence Estimation for Machine Translation. Summer workshop final report,
JHU/CLSP.
Paul Boersma and David Weenink. 2005. Praat: Doing Phonetics by Computer (Version 4.3.01). Retrieved from
http://www.praat.org/.
Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine
Translation. In Eighth Workshop on Statistical Machine Translation, WMT-2013, pages 1?44, Sofia, Bulgaria.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings
of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical
Machine Translation (WMT?12), pages 10?51, Montr?eal, Canada.
Mauro Cettolo, Jan Niehues, Sebastian St?uker, Luisa Bentivogli, and Marcello Federico. 2013. Report on the 10th
IWSLT Evaluation Campaign. Proceedings of the International Workshop on Spoken Language Translation,
Heidelberg, Germany.
1821
Jos?e G. C. de Souza, Christian Buck, Marco Turchi, and Matteo Negri. 2013. FBK-UEdin participation to the
WMT13 quality estimation shared task. In Proceedings of the Eighth Workshop on Statistical Machine Trans-
lation, pages 352?358, Sofia, Bulgaria, August. Association for Computational Linguistics.
Jos?e G. C. de Souza, Jes?us Gonz?alez-Rubio, Christian Buck, Marco Turchi, and Matteo Negri. 2014a. FBK-UPV-
UEdin participation in the WMT14 Quality Estimation shared-task. In Proceedings of the Ninth Workshop on
Statistical Machine Translation, Baltimore, MD, USA, June.
Jos?e G. C. de Souza, Marco Turchi, and Matteo Negri. 2014b. Predicting Machine Translation Quality Esti-
mation Across Domains. In Proceedings of the 25th International Conference on Computational Linguistics,
COLING?14, Dublin, Ireland.
Gunnar Evermann and Philip C. Woodland. 2000. Large Vocabulary Decoding and Confidence Estimation Using
Word Posterior Probabilities. In Proc. of ICASSP, pages 2366?2369, Istanbul, Turkey, June.
Daniele Falavigna, Roberto Gretter, Fabio Brugnara, Diego Giuliani, and Romain Serizel. 2013. FBK@IWSLT
2013 - ASR Tracks. In Proceedings of the IWSLT 2013 workshop, Heidelberg, Germany.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. IRSTLM: an Open Source Toolkit for Handling
Large Scale Language Models. pages 1618?1621, Brisbane, Australia, September.
Marcello Federico, Luisa Bentivogli, Michael Paul, and Sebastian St?uker. 2011. Overview of the IWSLT 2011
Evaluation Campaign. In International Workshop on Spoken Language Translation, pages 11?27.
Marcello Federico, Mauro Cettolo, Luisa Bentivogli, Michael Paul, and Sebastian St?uker. 2012. Overview of the
IWSLT 2012 Evaluation Campaign. In Proc. of the International Workshop on Spoken Language Translation,
Hong Kong, HK, December.
Mariano Felice. 2012. Linguistic Indicators for Quality Estimation of Machine Translations. Master?s thesis,
University of Wolverhampton, UK.
Malte Gabsdil and Oliver Lemon. 2004. Combining acoustic and pragmatic features to predict recognition perfor-
mance in spoken dialogue systems. pages 344?351.
Pierre Geurts, Damien Ernst, and Louis Wehenkel. 2006. Extremely randomized trees. Mach. Learn., 63(1):3?42,
April.
Sharon Goldwater, Dan Jurafsky, and Christopher Manning. 2010. Which words are hard to recognize? Prosodic,
lexical, and disfluency factors that increase speech recognition error rates. 52(3):181?200.
Trevor Hastie, Robert Tibshirani, Jerome Friedman, T Hastie, J Friedman, and R Tibshirani. 2009. The elements
of statistical learning, volume 2. Springer.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and David Williamson. 2011. A three-stage approach for auto-
mated scoring of spontaneous responses. (25):282?306.
Diane J. Litman, Julia B. Hirschberg, and Marc Swerts. 2000. Predicting Automatic Speech Recognition Perfor-
mance Using Prosodic Cues. In Proceedings of NAACL, pages 218?225.
Annie Louis and Ani Nenkova. 2013. Automatically assessing machine summary content without a gold standard.
Computational Linguistics, 39(2):267?300.
Yashar Mehdad, Matteo Negri, and Marcello Federico. 2012. Match without a Referee: Evaluating MT Adequacy
without Reference Translations. In Proceedings of the Machine Translation Workshop (WMT2012).
Nicolai Meinshausen and Peter B?uhlmann. 2010. Stability selection. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 72(4):417?473.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-
learn : Machine Learning in Python. Journal of Machine Learning Research, 12:2825?2830.
Christopher B. Quirk. 2004. Training a Sentence-Level Machine Translation Confidence Measure. In Proceedings
of LREC 2004.
Raphael Rubino, Jos?e GC de Souza, Jennifer Foster, and Lucia Specia. 2013a. Topic Models for Translation
Quality Estimation for Gisting Purposes. In Proceedings of the Machine Translation Summit XIV.
1822
Raphael Rubino, Joachim Wagner, Jennifer Foster, Johann Roturier, Rasoul Samad Zadeh Kaljahi, and Fred Hol-
lowood. 2013b. DCU-Symantec at the WMT 2013 Quality Estimation Shared Task. In Proceedings of the
Eighth Workshop on Statistical Machine Translation, pages 392?397.
Alberto Sanchis, Alfons Juan, and Enrique Vidal. 2012. A Word-Based Naive Bayes Classifier for Confidence
Estimation in Speech Recognition. 20(12):565?574.
Helmut Schmid. 1995. Improvements in Part-of-Speech Tagging with an Application to German. In Proceedings
of the ACL SIGDAT-Workshop, pages 47?50, Dublin, Ireland.
Matthew Stephen Seigel. 2013. Condence Estimation for Automatic Speech Recognition Hypotheses. University
of Cambridge. PhD Thesis.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel methods for pattern analysis. Cambridge university press.
Radu Soricut and Abdessamad Echihabi. 2010. TrustRank: Inducing Trust in Automatic Translations via Ranking.
In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ?10, pages
612?621, Stroudsburg, PA, USA. Association for Computational Linguistics.
Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco Turchi, and Nello Cristianini. 2009. Estimating the
Sentence-Level Quality of Machine Translation Systems. In Proceedings of the 13th Annual Conference of the
European Association for Machine Translation (EAMT?09), pages 28?35, Barcelona, Spain.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010. Machine Translation Evaluation versus Quality Estimation.
Machine translation, 24(1):39?50.
Lucia Specia. 2011. Exploiting Objective Annotations for Measuring Translation Post-editing Effort. Proceedings
of the 15th Conference of the European Association for Machine Translation, pages 73?80.
Rafic Antoon Sukkar and Chin-Hui Lee. 1996. Vocabulary Independent Discriminative Utterance Verification for
Nonkeyword Rejection in Subword Based Speech Recognition. 6(6):420?429.
Marco Turchi, Antonios Anastasopoulos, Jos?e G. C. de Souza, and Matteo Negri. 2014. Adaptive Quality Estima-
tion for Machine Translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational
Linguistics, ACL?14, Baltimore, MD, USA. Association for Computational Linguistics.
Nicola Ueffing and Hermann Ney. 2007. Word-Level Confidence Estimation for Machine Translation. Comput.
Linguist., 33(1):9?40, March.
Frank Wessel, Ralf Schl?uter, Klaus Macherey, and Hermann Ney. 2001. Confidence Measures for Large Vocabu-
lary Continuous Speech Recognition. 9(3):288?298.
Su-Youn Yoon, Lei Chen, and Klaus Zechner. 2010. Predicting word accuracy for the automatic speech recogni-
tion of non-native speech. In Proc. of INTERSPEECH, pages 773?776, Makuhari,Chiba, Japan.
1823
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 129?132, Dublin, Ireland, August 23-29 2014.
THE MATECAT TOOL
M. Federico and N. Bertoldi and M. Cettolo and M. Negri and M. Turchi
Fondazione Bruno Kessler, Trento (Italy)
M. Trombetti and A. Cattelan and A. Farina and
D. Lupinetti and A. Martines and A. Massidda
Translated Srl, Roma (Italy)
H. Schwenk and L. Barrault and F. Blain
Universit?e du Maine, Le Mans (France)
P. Koehn and C. Buck and U. Germann
The University of Edinburgh (United Kingdom)
www.matecat.com
Abstract
We present a new web-based CAT tool providing translators with a professional work environ-
ment, integrating translation memories, terminology bases, concordancers, and machine transla-
tion. The tool is completely developed as open source software and has been already successfully
deployed for business, research and education. The MateCat Tool represents today probably the
best available open source platform for investigating, integrating, and evaluating under realistic
conditions the impact of new machine translation technology on human post-editing.
1 Introduction
The objective of MateCat
1
is to improve the integration of machine translation (MT) and human transla-
tion within the so-called computer aided translation (CAT) framework. CAT tools represent nowadays the
dominant technology in the translation industry. They provide translators with text editors that can man-
age several document formats and suitably arrange their content into text segments ready to be translated.
Most importantly, CAT tools provide access to translation memories (TMs), terminology databases, con-
cordance tools and, more recently, to machine translation (MT) engines. A TM is basically a repository
of translated segments. During translation, the CAT tool queries the TM to search for exact or fuzzy
matches of the current source segment. These matches are proposed to the user as translation sugges-
tions. Once a segment is translated, its source and target texts are added to the TM for future queries. The
integration of suggestions from an MT engine as a complement to TM matches is motivated by recent
studies (Federico et al., 2012; Green et al., 2013; L?aubli et al., 2013), which have shown that post-editing
MT suggestions can substantially improve the productivity of professional translators. MateCat lever-
ages the growing interest and expectations in statistical MT by advancing the state-of-the-art along three
directions:
? Self-tuning MT, i.e. methods to train statistical MT for specific domains or translation projects;
? User adaptive MT, i.e. methods to quickly adapt statistical MT from user corrections and feedback;
? Informative MT, i.e. supply more information to enhance users? productivity and work experience.
Research along these three directions has converged into a new generation CAT software, which is
both an enterprise level translation workbench (currently used by several hundreds of professional trans-
lators) as well as an advanced research platform for integrating new MT functions, running post-editing
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0/.
1
MateCat, acronym of Machine Translation Enhanced Computer Assisted Translation, is a 3-year research project (11/2011-
10/2014) funded by the European Commission under FP7 (grant agreement no 287688). The project consortium is led by FBK
(Trento, Italy) and includes the University of Edinburgh (United Kingdom), Universit?e du Maine (Le Mans, France), and
Translated Srl (Rome, Italy).
129
Figure 1: The MateCat Tool editing page.
experiments and measuring user productivity. The MateCat Tool, which is distributed under the LGPL
open source license, combines features of the most advanced systems (either commercial, like the pop-
ular SDL Trados Workbench,
2
or free like OmegaT
3
) with new functionalities. These include: i) an
advanced API for the Moses Toolkit,
4
customizable to languages and domains, ii) ease of use through a
clean and intuitive web interface that enables the collaboration of multiple users on the same project, iii)
concordancers, terminology databases and support for customizable quality estimation components and
iv) advanced logging functionalities.
2 The MateCat Tool in a Nutshell
Overview. The MateCat Tool runs as a web-server accessible through Chrome, Firefox and Safari. The
CAT web-server connects with other services via open APIs: the TM server MyMemory
5
, the commer-
cial Google Translate (GT) MT server, and a list of Moses-based servers specified in a configuration file.
While MyMemory?s and GT?s servers are always running and available, customized Moses servers have
to be first installed and set-up. Communication with the Moses servers extends the GT API in order to
support self-tuning, user-adaptive and informative MT functions. The natively supported document for-
mat of MateCat Tool is XLIFF,
6
although its configuration file makes it possible to specify external file
converters. The tool supports Unicode (UTF-8) encoding, including non latin alphabets and right-to-left
languages, and handles texts embedding mark-up tags.
How it works. The tool is intended both for individual translators or managers of translation projects
involving one or more translators. A translation project starts by uploading one or more documents and
specifying the desired translation direction. Then the user can optionally select a MT engine from an
available list and/or a new or existing private TM in MyMemory, by specifying its private key. Notice
that the public MyMemory TM and the GT MT services are assumed by default. The following step is
the volume analysis of the document, which reports statistics about the words to be actually translated
based on the coverage provided by the TM. At this stage, long documents can be also split into smaller
portions to be for instance assigned to different translators or translated at different times. The following
step starts the actual translation process by opening the editing window. All source segments of the
2
http://www.translationzone.com/
3
http://www.omegat.org/
4
http://www.statmt.org/moses/
5
http://mymemory.translated.net
6
http://docs.oasis-open.org/xliff/v1.2/os/xliff-core.html
130
document and their corresponding target segments are arranged side-by-side on the screen. By selecting
one segment, an editing pane opens (Figure 1) including an editable field that is initialized with the best
available suggestion or with the last post-edit. Translation hints are shown right below together with
their origin (MT or TM). Their ranking is based on the TM match score or the MT confidence score. MT
hints with no confidence score are assigned a default score. Tag consistency is automatically checked
during translation and warnings are possibly shown in the editing window. An interesting feature of the
MateCat Tool is that each translation project is uniquely identified by its URL page which also includes
the currently edited segment. This permits for instance more users to simultaneously access and work on
the same project. Moreover, to support simultaneous team work on the same project, translators can mark
the status (draft, translated, approved, rejected) of each segment with a corresponding color (see Figure
1, right blue bar). The user interface is enriched with search and replace functions, a progress report at
the bottom of the page, and several shortcut commands for the skilled users. Finally, the tool embeds a
concordance tool to search for terms in the TM, and a glossary where each user can upload, query and
update her terminology base. Users with a Google account can access a project management page which
permits then to manage all their projects, including storage, deletion, and access to the editing page.
MT support. The tool supports Moses-based servers able to provide an enhanced CAT-MT commu-
nication. In particular, the GT API is augmented with feedback information provided to the MT engine
every time a segment is post-edited as well as enriched MT output, including confidence scores, word
lattices, etc. The developed MT server supports multi-threading to serve multiple translators, properly
handles text segments including tags, and instantly adapts from the post-edits performed by each user
(Bertoldi et al., 2013).
Edit Log. During post-editing the tool collects timing information for each segment, which is updated
every time the segment is opened and closed. Moreover, for each segment, information is collected about
the generated suggestions and the one that has actually been post-edited. This information is accessible at
any time through a link in the Editing Page, named Editing Log. The Editing Log page (Figure 2) shows
a summary of the overall editing performed so far on the project, such as the average translation speed
and post-editing effort and the percentage of top suggestions coming from MT or the TM. Moreover,
for each segment, sorted from the slowest to the fastest in terms of translation speed, detailed statistics
about the performed edit operations are reported. This information, with even more details, can be also
downloaded as a CSV file to perform a more detailed post-editing analysis. While the information shown
in the Edit Log page is very useful to monitor progress of a translation project in real time, the CSV file
is a fundamental source of information for detailed productivity analyses once the project is ended.
3 Applications.
The MateCat Tool has been exploited by the MateCat project to investigate new MT functions (Bertoldi
et al., 2013; Cettolo et al., 2013; Turchi et al., 2013; Turchi et al., 2014) and to evaluate them in a real
professional setting, in which translators have at disposal all the sources of information they are used
to work with. Moreover, taking advantage of its flexibility and ease of use, the tool has been recently
exploited for data collection and education purposes (a course on CAT technology for students in trans-
lation studies). An initial version of the tool has also been leveraged by the Casmacat project
7
to create
a workbench (Alabau et al., 2013), particularly suitable for investigating advanced interaction modalities
such as interactive MT, eye tracking, and handwritten input. Currently the tool is employed by Trans-
lated for their internal translation projects and is being tested by several international companies, both
language service providers and IT companies. This has made possible to collect continuous feedback
from hundreds of translators, which besides helping us to improve the robustness of the tool is also
influencing the way new MT functions will be integrated to supply the best help to the final user.
7
http://www.casmacat.eu
131
Figure 2: The MateCat Tool edit log page.
References
Vicent Alabau, Ragnar Bonk, Christian Buck, Michael Carl, Francisco Casacuberta, Mercedes Garca-Mart??nez,
Jes?us Gonz?alez, Philipp Koehn, Luis Leiva, Bartolom?e Mesa-Lao, Daniel Oriz, Herv?e Saint-Amand, Germ?an
Sanchis, and Chara Tsiukala. 2013. Advanced computer aided translation with a web-based workbench. In
Proceedings of Workshop on Post-editing Technology and Practice, pages 55?62.
Nicola Bertoldi, Mauro Cettolo, and Marcello Federico. 2013. Cache-based Online Adaptation for Machine
Translation Enhanced Computer Assisted Translation. In Proceedings of the MT Summit XIV, pages 35?42,
Nice, France, September.
Mauro Cettolo, Christophe Servan, Nicola Bertoldi, Marcello Federico, Lo??c Barrault, and Holger Schwenk. 2013.
Issues in Incremental Adaptation of Statistical MT from Human Post-edits. In Proceedings of the MT Summit
XIV Workshop on Post-editing Technology and Practice (WPTP-2), pages 111?118, Nice, France, September.
Marcello Federico, Alessandro Cattelan, and Marco Trombetti. 2012. Measuring user productivity in machine
translation enhanced computer assisted translation. In Proceedings of the Tenth Conference of the Association
for Machine Translation in the Americas (AMTA).
Spence Green, Jeffrey Heer, and Christopher D Manning. 2013. The efficacy of human post-editing for language
translation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 439?
448. ACM.
Samuel L?aubli, Mark Fishel, Gary Massey, Maureen Ehrensberger-Dow, and Martin Volk. 2013. Assessing Post-
Editing Efficiency in a Realistic Translation Environment. In Michel Simard Sharon O?Brien and Lucia Specia
(eds.), editors, Proceedings of MT Summit XIVWorkshop on Post-editing Technology and Practice, pages 83?91,
Nice, France.
Marco Turchi, Matteo Negri, and Marcello Federico. 2013. Coping with the subjectivity of human judgements
in MT quality estimation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages
240?251, Sofia, Bulgaria, August. Association for Computational Linguistics.
Marco Turchi, Antonios Anastasopoulos, Jos?e G.C. de Souza, and Matteo Negri. 2014. Adaptive Quality Estima-
tion for Machine Translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational
Linguistics (ACL ?14). Association for Computational Linguistics.
132
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 670?679,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Divide and Conquer:
Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora
Matteo Negri
FBK-irst
Trento, Italy
negri@fbk.eu
Luisa Bentivogli
FBK-irst
Trento, Italy
bentivo@fbk.eu
Yashar Mehdad
FBK-irst and University of Trento
Trento, Italy
mehdad@fbk.eu
Danilo Giampiccolo
CELCT
Trento, Italy
giampiccolo@celct.it
Alessandro Marchetti
CELCT
Trento, Italy
amarchetti@celct.it
Abstract
We address the creation of cross-lingual tex-
tual entailment corpora by means of crowd-
sourcing. Our goal is to define a cheap and
replicable data collection methodology that
minimizes the manual work done by expert
annotators, without resorting to preprocess-
ing tools or already annotated monolingual
datasets. In line with recent works empha-
sizing the need of large-scale annotation ef-
forts for textual entailment, our work aims to:
i) tackle the scarcity of data available to train
and evaluate systems, and ii) promote the re-
course to crowdsourcing as an effective way
to reduce the costs of data collection without
sacrificing quality. We show that a complex
data creation task, for which even experts usu-
ally feature low agreement scores, can be ef-
fectively decomposed into simple subtasks as-
signed to non-expert annotators. The resulting
dataset, obtained from a pipeline of different
jobs routed to Amazon Mechanical Turk, con-
tains more than 1,600 aligned pairs for each
combination of texts-hypotheses in English,
Italian and German.
1 Introduction
Cross-lingual Textual Entailment (CLTE) has been
recently proposed by (Mehdad et al, 2010; Mehdad
et al, 2011) as an extension of Textual Entailment
(Dagan and Glickman, 2004). The task consists of
deciding, given a text (T) and an hypothesis (H) in
different languages, if the meaning of H can be in-
ferred from the meaning of T. As in other NLP appli-
cations, both for monolingual and cross-lingual TE,
the availability of large quantities of annotated data
is an enabling factor for systems development and
evaluation. Until now, however, the scarcity of such
data on the one hand, and the costs of creating new
datasets of reasonable size on the other, have repre-
sented a bottleneck for a steady advancement of the
state of the art.
In the last few years, monolingual TE corpora for
English and other European languages have been
created and distributed in the framework of sev-
eral evaluation campaigns, including the RTE Chal-
lenge1, the Answer Validation Exercise at CLEF2,
and the Textual Entailment task at EVALITA3. De-
spite the differences in the design of the tasks, all
the released datasets were collected through simi-
lar procedures, always involving expensive manual
work done by expert annotators. Moreover, in the
data creation process, large amounts of hand-crafted
T-H pairs often have to be discarded in order to re-
tain only those featuring full agreement, in terms of
the assigned entailment judgements, among multiple
annotators. The amount of discarded pairs is usually
high, contributing to increase the costs of creating
textual entailment datasets4.
The issues related to the shortage of datasets and
the high costs for their creation are more evident
1http://www.nist.gov/tac/2011/RTE/
2http://nlp.uned.es/clef-qa/ave/
3http://www.evalita.it/2009/tasks/te
4For instance, in the first five RTE Challenges, the aver-
age effort needed to create 1,000 pairs featuring full agreement
among 3 annotators was around 2.5 person-months. Typically,
around 25% of the original pairs had to be discarded during the
process, due to low inter-annotator agreement (Bentivogli et al,
2009).
670
in the CLTE scenario, where: i) the only dataset
currently available is an English-Spanish corpus ob-
tained by translating the RTE-3 corpus (Negri and
Mehdad, 2010), and ii) the application of the stan-
dard methods adopted to build RTE pairs requires
proficiency in multiple languages, thus significantly
increasing the costs of the data creation process.
To address these issues, in this paper we devise
a cost-effective methodology to create cross-lingual
textual entailment corpora. In particular, we focus
on the following problems:
(1) Is it possible to collect T-H pairs minimizing
the intervention of expert annotators? To address
this question, we explore the feasibility of crowd-
sourcing the corpus creation process. As a contri-
bution beyond the few works on TE/CLTE data ac-
quisition, we define an effective methodology that:
i) does not involve experts in the most complex (and
costly) stages of the process, ii) does not require pre-
processing tools, and iii) does not rely on the avail-
ability of already annotated RTE corpora.
(2) How can we guarantee good quality of the col-
lected data at a low cost? We address the quality
control issue through the decomposition of a com-
plex task (i.e. creating and annotating entailment
pairs) into smaller sub-tasks. Complex tasks are usu-
ally hard to explain in a simple way understandable
to non-experts, difficult to accomplish, and not suit-
able for the application of the quality-check mecha-
nisms provided by current crowdsourcing services.
Our ?divide and conquer? solution represents the
first attempt to address a complex task involving
content generation and labelling through the defini-
tion of a cheap and reliable pipeline of simple tasks
which are easy to define, accomplish, and control.
(3) Can we adapt such methodology to collect
cross-lingual T-H pairs? We tackle this question
by separating the problem of creating and annotating
TE pairs from the issues related to the multilingual
dimension. Our solution builds on the assumption
that entailment annotations can be projected across
aligned T-H pairs in different languages. In this
case, a complex multilingual task is reduced to a se-
quence of simpler subtasks where the most difficult
one, the generation of entailment pairs, is entirely
monolingual. Besides ensuring cost-effectiveness,
our solution allows us to overcome the problem of
finding workers that are proficient in multiple lan-
guages. Moreover, since the core monolingual tasks
of the process are carried out by manipulating En-
glish texts, we are able to address the very large
community of English speaking workers, with a
considerable reduction of costs and execution time.
Finally, as a by-product of our method, the acquired
pairs are fully aligned for all language combinations,
thus enabling meaningful comparisons between sce-
narios of different complexity (monolingual TE, and
CLTE between close or distant languages).
We believe that, in the same spirit of recent works
promoting large-scale annotation efforts around en-
tailment corpora (Sammons et al, 2010; Bentivogli
et al, 2010), the proposed approach and the resulting
dataset5 will contribute to meeting the strong need
for resources to develop and evaluate novel solutions
for textual entailment.
2 Related Works
Crowdsourcing services, such as Amazon Mechan-
ical Turk6 (MTurk) and CrowdFlower7, have been
recently used with success for a variety of NLP ap-
plications (Callison-Burch and Dredze, 2010). The
idea is that the acquisition and annotation of large
amounts of data needed to train and evaluate NLP
tools can be carried out in a cost-effective manner
by defining simple Human Intelligence Tasks (HITs)
routed to a crowd of non-expert workers (aka ?Turk-
ers?) hired through on-line marketplaces.
As regards textual entailment, the first work ex-
ploring the use of crowdsourcing services for data
annotation is described in (Snow et al, 2008), which
shows high agreement between non-expert annota-
tions of the RTE-1 dataset and existing gold standard
labels assigned by expert labellers.
Focusing on the actual generation of monolingual
entailment pairs, (Wang and Callison-Burch, 2010)
experiments the use of MTurk to collect facts and
counter facts related to texts extracted from an ex-
isting RTE corpus annotated with named entities.
Taking a step beyond the task of annotating exist-
5The CLTE corpora described in this paper will be made
freely available for research purposes through the website of
the funding EU Project CoSyne (http://www.cosyne.eu/).
6https://www.mturk.com/
7Although MTurk is directly accessible only to US citizens,
the CrowdFlower service (http://crowdflower.com/) provides an
interface to MTurk for non-US citizens.
671
ing datasets, and showing the feasibility of involving
non-experts also in the generation of TE pairs, this
approach is more relevant to our objectives. How-
ever, at least two major differences with our work
have to be remarked. First, they still use avail-
able RTE data to obtain a monolingual TE corpus,
whereas we pursue the more ambitious goal of gen-
erating from scratch aligned CLTE corpora for dif-
ferent language combinations. To this aim, we do
not resort to already annotated data, nor language-
specific preprocessing tools. Second, their approach
involves qualitative analysis of the collected data
only a posteriori, after manual removal of invalid
and trivial generated hypotheses. In contrast, our
approach integrates quality control mechanisms at
all stages of the data collection/annotation process,
thus minimizing the recourse to experts to check the
quality of the collected material.
Related research in the CLTE direction is re-
ported in (Negri and Mehdad, 2010), which de-
scribes the creation of an English-Spanish corpus
obtained from the RTE-3 dataset by translating the
English hypotheses into Spanish. Translations have
been crowdsourced adopting a methodology based
on translation-validation cycles, defined as separate
HITs. Although simplifying the CLTE corpus cre-
ation problem, which is recast as the task of translat-
ing already available annotated data, this solution is
relevant to our work for the idea of combining gold
standard units and ?validation HITS? as a way to
control the quality of the collected data at runtime.
3 Quality Control of Crowdsourced Data
The design of data acquisition HITs has to take into
account several factors, each having a considerable
impact on the difficulty of instructing the workers,
the quality and quantity of the collected data, the
time and overall costs of the acquisition. A major
distinction has to be made between jobs requiring
data annotation, and those involving content gener-
ation. In the former case, Turkers are presented with
the task of labelling input data referring to a fixed
set of possible values (e.g. making a choice between
multiple alternatives, assigning numerical scores to
rank the given data). In the latter case, Turkers are
faced with creative tasks consisting in the production
of textual material (e.g. writing a correct translation,
or a summary of a given text).
The ease of controlling the quality of the acquired
data depends on the nature of the job. For annotation
jobs, quality control mechanisms can be easily set up
by calculating Turkers? agreement, by applying vot-
ing schemes, or by adding hidden gold units to the
data to be annotated8. In contrast, the quality of the
results of content generation jobs is harder to assess,
due to the fact that multiple valid results are accept-
able (e.g. the same content can be expressed, trans-
lated, or summarized in different ways). In such sit-
uations the standard quality control mechanisms are
not directly applicable, and the detection of errors
requires either costly manual verification at the end
of the acquisition process, or more complex and cre-
ative solutions integrating HITs for quality check.
Most of the approaches to content generation pro-
posed so far rely on post hoc verification to fil-
ter out undesired low-quality data (Mrozinski et al,
2008; Mihalcea and Strapparava, 2009; Wang and
Callison-Burch, 2010). The few solutions integrat-
ing validation HITs address the translation of sin-
gle sentences, a task that is substantially different
from ours (Negri and Mehdad, 2010; Bloodgood and
Callison-Burch, 2010). Compared to sentence trans-
lation, the task of creating CLTE pairs is both harder
to explain without recurring to notions that are dif-
ficult to understand to non-experts (e.g. ?seman-
tic equivalence?, ?unidirectional entailment?), and
harder to execute without mastering these notions.
To tackle these issues the ?divide and conquer? ap-
proach described in the next section consists in the
decomposition of a difficult content generation job
into easier subtasks that are: i) self-contained and
easy to explain, ii) easy to execute without any NLP
expertise, and iii) suitable for the integration of a va-
riety of runtime control mechanisms (regional qual-
ifications, gold units, ?validation HITs?) able to en-
sure a good quality of the collected material.
8Both MTurk and CrowdFlower provide means to check
workers? reliability, and weed out untrusted ones without money
waste. These include different types of qualification mecha-
nisms, the possibility of giving work only to known trusted
Turkers (only with MTurk), and the possibility of adding hid-
den gold standard units in the data to be annotated (offered as a
built-in mechanism only by CrowdFlower).
672
4 CLTE Corpus Creation Methodology
Our approach builds on a pipeline of HITs routed to
MTurk?s workforce through the CrowdFlower inter-
face. The objective is to collect aligned T-H pairs
for different language combinations, reproducing an
RTE-like annotation style. However, our annotation
is not limited to the standard RTE framework, where
only unidirectional entailment from T to H is con-
sidered. As a useful extension, we annotate any pos-
sible entailment relation between the two text frag-
ments, including: i) bidirectional entailment (i.e.
semantic equivalence between T and H), ii) unidi-
rectional entailment from T to H, and iii) unidirec-
tional entailment from H to T. The resulting pairs
can be easily used to generate not only standard RTE
datasets9, but also general-purpose collections fea-
turing multi-directional entailment relations.
4.1 Data Acquisition and Annotation
We collect large amounts of CLTE pairs carrying out
the most difficult part of the process (the creation of
entailment-annotated pairs) at a monolingual level.
Starting from a set of parallel sentences in n lan-
guages, (e.g. L1, L2, L3), n entailment corpora are
created: one monolingual (L1/L1), and n-1 cross-
lingual (L1/L2, and L1/L3).
The monolingual corpus is obtained by modify-
ing the sentences only in one language (L1). Orig-
inal and modified sentences are then paired and an-
notated to form an entailment dataset for L1. The
CLTE corpora are obtained by combining the mod-
ified sentences in L1 with the original sentences in
L2 and L3, and projecting to the multilingual pairs
the annotations assigned to the monolingual pairs.
In principle, only two stages of the process re-
quire crowdsourcing multilingual tasks, but do not
concern entailment annotations. The first one, at the
beginning of the process, aims to obtain a set of par-
allel sentences to start with, and can be done in dif-
ferent ways (e.g. crowdsourcing the translation of
a set of sentences). The second one, at the end of
the process, consists of translating the modified L1
sentences into other languages (e.g. L2) in order to
extend the corpus to cover new language combina-
9With the positive examples drawn from bidirectional and
unidirectional entailments from T to H, and the negative ones
drawn from unidirectional entailments from H to T.
tions (e.g. L2/L2, L2/L3).
The execution of the two ?multilingual? stages is
not strictly necessary but depends on: i) the avail-
ability of parallel sentences to start the process, and
ii) the actual objectives in terms of language combi-
nations to be covered10.
As regards the first stage, in this work we started
from a set of 467 English/Italian/German aligned
sentences extracted from parallel documents down-
loaded from the Cafebabel European Magazine11.
Concerning the second multilingual stage, we per-
formed only one round of translations from En-
glish to Italian to extend the 3 combinations ob-
tained without translations (ENG/ENG, ENG/ITA,
and ENG/GER) with the new language combina-
tions ITA/ITA, ITA/ENG, and ITA/GER.
STEP1:	 ?Sentence	 ?modifica?on	 ?(monolingual)	 ?
STEP3:	 ?Transla?on	 ?(mul?lingual)	 ?
GER	 ? ENG	 ?
ENG1	 ?
ITA	 ?
ITA1	 ? ITA	 ?ENG	 ? ENG1	 ?
STEP2:	 ?TE	 ?annota?on	 ?(monolingual)	 ?
Monolingual	 ?TE	 ?corpus	 ?
Cross-??lingual	 ?TE	 ?corpus	 ?
ENG1	 ?GER	 ?
ENG1	 ?ITA	 ?
TE	 ?annota?ns	 ?projec?n	 ?	 ?	 ?
ITA1	 ? GER	 ?
ITA1	 ? ENG	 ?
Figure 1: Corpus creation process.
The main steps of our corpus creation process,
depicted in Figure 1, can be summarized as follows:
Step1: Sentence modification. The original
English sentences (ENG) are modified through
(monolingual) generation HITs asking Turkers to:
i) preserve the meaning of the original sentences
using different surface forms, or ii) slightly change
their meaning by adding or removing content. Our
assumption, in line with (Bos et al, 2009), is that
10Starting from parallel sentences in n languages, the n cor-
pora obtained without recurring to translations can be aug-
mented, by means of translation HITs, to create the full set of
language combinations. Each round of translation adds 1 mono-
lingual corpus, and n-1 CLTE corpora.
11http://www.cafebabel.com/
673
another way to think about entailment is to consider
whether one text T1 adds new information to the
content of another text T: if so, then T is entailed by
T1.
The result of this phase is a set of texts (ENG1)
that can be of three types:
1. Paraphrases of the original ENG texts, that will
be used to create bidirectional entailment pairs
(ENG?ENG1);
2. More specific sentences (the outcome of
content addition operations), used to create
ENG?ENG1 unidirectional entailment pairs;
3. More general sentences (the outcome of
content removal operations), used to create
ENG?ENG1 unidirectional entailment pairs.
Step2: TE Annotation. Entailment pairs com-
posed of the original sentences (ENG) and the modi-
fied ones (ENG1) are used as input of (monolingual)
annotation HITs asking Turkers to decide which of
the two texts contains more information. As a re-
sult, each ENG/ENG1 pair is annotated as an ex-
ample of uni-/bidirectional entailment, and stored in
the monolingual English corpus. Since the original
ENG texts are aligned with the ITA and GER texts,
the entailment annotations of ENG/ENG1 pairs can
be projected to the other language pairs and the
ITA/ENG1 and GER/ENG1 pairs are stored in the
CLTE corpus. The possibility of projecting TE an-
notations is based on the assumption that the seman-
tic information is mostly preserved during the trans-
lation process. This particularly holds at the deno-
tative level (i.e. regarding the truth values of the
sentence) which is crucial to semantic inference. At
other levels (e.g. lexical) there might be slight se-
mantic variations which, however, are very unlikely
to play a crucial role in determining entailment rela-
tions.
Step3: Translation. The modified sentences
(ENG1) are translated into Italian (ITA1) through
(multilingual) generation HITs reproducing the ap-
proach described in (Negri and Mehdad, 2010). As
a result, three new datasets are produced by au-
tomatically projecting annotations: the monolin-
gual ITA/ITA1, and the cross-lingual ENG/ITA1 and
GER/ITA1.
Since the solution adopted for sentence transla-
tion does not present novelty factors, the remainder
of this paper will omit further details on it. Instead,
the following sections will focus on the more chal-
lenging tasks of sentence modification and TE anno-
tation.
4.2 Crowdsourcing Sentence Modification and
TE Annotation
Sentence modification and TE annotation have been
decomposed into a pipeline of simpler monolingual
English sub-tasks. Such pipeline, depicted in Figure
2, involves several types of generation/annotation
HITs designed to be easily understandable to non-
experts. Each HIT consists of: i) a set of instruc-
tions for a specific task (e.g. paraphrasing a text),
ii) the data to be manipulated (e.g. an English sen-
tence), and iii) a test to check workers? reliability.
To cope with the quality control issues discussed in
Section 3, such tests are realized using gold stan-
dard units, either hidden in the data to be annotated
(annotation HITs) or defined as test questions that
workers must correctly answer (generation HITs).
Moreover, regional qualifications are applied to all
HITs. As a further quality check, all the annotation
HITs consider Turkers? agreement as a way to filter
out low quality results (only annotations featuring
agreement among 4 out of 5 workers are retained).
The six HITs defined for each subtask can be de-
scribed as follows:
1. Paraphrase (generation). Modify an En-
glish text (ENG), in order to produce a semantically
equivalent variant (ENG1). As a reliability test, be-
fore creating the paraphrase workers are asked to
judge if two English sentences contain the same in-
formation.
2. Grammaticality (annotation). Decide if an
English sentence is grammatically correct. This val-
idation HIT represents a quality check of the out-
put of each generation task (i.e. paraphrasing, and
add/remove information HITs).
3. Bidirectional Entailment (annotation). De-
cide whether two English sentences, the original
ENG and the modified ENG1, contain the same in-
formation (i.e. are semantically equivalent).
4a. Add Information (generation). Modify an
English text to create a more specific one by adding
content. As a reliability test, before generating the
674
Figure 2: Sentence modification and TE annotation pipeline.
new sentence workers are asked to judge which of
two given English sentences is more detailed.
4b. Remove Information (generation). Mod-
ify an English text to create a more general one by
removing part of its content. As a reliability test, be-
fore generating the new sentence workers are asked
to judge which of two given English sentences is less
detailed.
5. Unidirectional Entailment (annotation). De-
cide which of two English sentences (the original
ENG, and a modified ENG1) provides more infor-
mation.
These HITs are combined in an iterative pro-
cess that alternates text generation, grammaticality
check, and entailment annotation steps. As a result,
for each original ENG text we obtain multiple ENG1
variants of the three types (paraphrases, more gen-
eral texts, and more specific texts) and, in turn, a set
of annotated monolingual (ENG/ENG1) TE pairs.
As described in Section 4.1, the resulting mono-
lingual English TE corpus (ENG/ENG1) is used to
create the following mono/cross-lingual TE corpora:
? ITA/ENG1, and GER/ENG1 (by projecting TE
annotations)
? ITA/ITA1, GER/ITA1, and ENG/ITA1 (by
translating the ENG1 texts into Italian, and pro-
jecting TE annotations)
5 The Resulting CLTE Corpora
This section provides a quantitative and qualita-
tive analysis of the results of our corpus creation
methodology, focusing on the collected ENG-ENG1
monolingual dataset. It has to be remarked that, as
an effect of the adopted methodology, all the obser-
vations and the conclusions drawn hold for the col-
lected CLTE corpora as well.
5.1 Quantitative Analysis
Table 1 provides some details about each step of the
pipeline shown in Figure 2. For each HIT the table
presents: i) the number of items (sentences, or pairs
of sentences) given in input, ii) the number of items
(sentences or annotations) produced as output, iii)
the number of items discarded when the agreement
threshold was not reached, iv) the number of entail-
ment pairs added to the corpus, v) the time (days and
hours) required by the MTurk workforce to complete
the job, and vi) the cost of the job.
In HIT-1 (Paraphrase) 1,414 paraphrases were
collected asking three different meaning-preserving
modifications of each of the 467 original sen-
tences12. From a practical point of view, such redun-
dancy aims to ensure a sufficient number of gram-
matically correct and semantically equivalent mod-
ified sentences. From a theoretical point of view,
12Often, crowdsourced jobs return a number of output items
that is slightly larger than required, due to the labour distribution
mechanism internal to MTurk.
675
HIT # Input items # Output items # Discarded items # Pairs to corpus MTurk time Cost ($)
1. Paraphrase 467 1,414 5d+10.5h 45.48
2. Grammaticality 1,414 1,326 88 (6.22%) 1d+15h 56.88
3. Bidirectional Ent. 1,326 1,213 113 (8.52%) 301 3d+2h 53.47
(yes=1,205 no=8)
4a. Add Info 452 916 3d 37.02
4b. Remove Info 452 923 2d+22h 29.73
2. Grammaticality 1,839 1,749 90 (4.89%) 2d+5h 64.37
3. Bidirectional Ent. 1,749 1,438 311 (17.78%) 148 3d+20.5h 70.52
(yes=148 no=1,290)
5. Unidirectional Ent. 1,298 1,171 127 (9.78%) 1,171 8.5h 78.24
(491 + 680)
TOTAL 721 1,620 22d+11h 435.71
Table 1: The monolingual dataset creation pipeline.
collecting many variants of a small pool of origi-
nal sentences aims to create pairs featuring different
entailment relations with similar superficial forms.
This, in principle, should allow to obtain a dataset
which requires TE systems to focus more on deeper
semantic phenomena than on the surface realization
of the pairs.
The collected paraphrases were sent as input to
HIT-2 (Grammaticality). After this validation HIT,
the number of acceptable paraphrases was reduced
to 1,326 (with 88 discarded sentences, correspond-
ing to 6.22% of the total).
The retained paraphrases were paired with their
corresponding original sentences, and sent to HIT-3
(Bidirectional Entailment) to be judged for semantic
equivalence. The pairs marked as bidirectional en-
tailments (1,205) were divided in three groups: 25%
of the pairs (301) were directly stored in the final
corpus, while the ENG1 paraphrases of the remain-
ing 75% (904) were equally distributed to the next
modification steps.
In both HIT-4a (Add Information) and HIT-4b
(Remove information) two new modified sentences
were asked for each of the 452 paraphrases received
as input. The sentences collected in these generation
tasks were respectively 916 and 923.
The new modified sentences were sent back to
HIT-2 (Grammaticality) and HIT-3 (Bidirectional
Entailment). As a result 1,438 new pairs were cre-
ated; out of these, 148 resulted to be bidirectional
entailments and were stored in the corpus.
Finally, the 1,298 entailment pairs judged as non-
bidirectional in the two previously completed HIT-
3 (8+1,290) were given as input to HIT-5 (Unidi-
rectional Entailment). The pairs which passed the
agreement threshold were classified according to the
judgement received, and stored in the corpus as uni-
directional entailment pairs.
The analysis of Table 1 allows to formulate
some considerations. First, the percentage of dis-
carded items confirms the effectiveness of decom-
posing complex generation tasks into simpler sub-
tasks that integrate validation HITs and quality
checks based on non-experts? agreement. In fact, on
average, around 9.5% of the generated items were
discarded without experts? intervention13. Second,
the amount of discarded items gives evidence about
the relative difficulty of each HIT. As expected,
we observe lower rejection rates, corresponding to
higher inter-annotator agreement, for grammatical-
ity HITs (5.55% on average) than for more complex
entailment-related tasks (12.02% on average).
Looking at costs and execution time, it is hard
to draw definite conclusions due to several factors
that influence the progress of the crowdsourced jobs
(e.g. the fluctuations of Turkers? performances, the
time of the day at which jobs are posted, the dif-
ficulty to set the optimal cost for a given HIT14).
On the one hand, as expected, the more creative
?Add Info? task proved to be more demanding than
the ?Remove Info?: even though it was paid more,
13Moreover, it is worthwhile noticing that around 20% of the
collected items were automatically rejected (and not paid) due
to failures on the gold standard controls created both for gener-
ation and annotation tasks.
14The payment for each HIT was set on the basis of a pre-
vious feasibility study aimed at determining the best trade-off
between cost and execution time. However, replicating our ap-
proach would not necessarily result in the same costs.
676
it still took little more time to be completed. On
the other hand, although the ?Unidirectional Entail-
ment? task was expected to be more difficult and
thus rewarded more than the ?Bidirectional Entail-
ment? one, in the end it took notably less time to
be completed. Nevertheless, the overall figures (435
USD, and about 22.5 days of MTurk work to com-
plete the process)15 clearly demonstrate the effec-
tiveness of the approach. Even considering the time
needed for an expert to manage the pipeline (i.e. one
week to prepare gold units, and to handle the I/O of
each HIT), these figures show that our methodology
provides a cheaper and faster way to collect entail-
ment data in comparison with the RTE average costs
reported in Section 1.
As regards the amount of data collected, the re-
sulting corpus contains 1,620 pairs with the fol-
lowing distribution of entailment relations: i) 449
bidirectional entailments, ii) 491 ENG?ENG1 uni-
directional entailments, and iii) 680 ENG?ENG1
unidirectional entailments.
It must be noted that our methodology does not
lead to the creation of pairs where some information
is provided in one text and not in the other, and vice-
versa, as Example 1 shows:
Example 1.
ENG: New theories were emerging in the field of psychology.
ENG1: New theories were rising, which announced a kind of
veiled racism.
These negative examples in both directions repre-
sent a natural extension of the dataset, relevant also
for specific application-oriented scenarios, and their
creation will be addressed in future work.
Besides the achievement of our primary objec-
tives, the adopted approach led to some interesting
by-products. First, the generated corpora are per-
fectly suitable to produce entailment datasets simi-
lar to those used in the traditional RTE evaluation
framework. In particular, considering any possible
entailment relation between two text fragments, our
annotation subsumes the one proposed in RTE cam-
paigns. This allows for the cost-effective genera-
tion of RTE-like annotations from the acquired cor-
15Although by projecting annotations the ENG1/ITA and
ENG1/GER CLTE corpora came for free, the ITA1/ITA,
ITA1/ENG, and ITA1/GER combinations created by crowd-
sourcing translations added 45 USD and approximately 5 days
to these figures.
pora by combining ENG?ENG1 and ENG?ENG1
pairs to form 940 positive examples (449+491),
keeping the 680 ENG?ENG1 as negative exam-
ples. Moreover, by swapping ENG and ENG1 in the
unidirectional entailment pairs, 491 additional nega-
tive examples and 680 positive examples can be eas-
ily obtained.
Finally, the output of HITs 1-2-3 in Table 1 rep-
resents per se a valuable collection of 1,205 para-
phrases. This suggests the great potential of crowd-
sourcing for paraphrase acquisition.
5.2 Qualitative Analysis
Through manual verification of more than 50% of
the corpus (900 pairs), a total number of 53 pairs
(5.9%) were found incorrect. The different errors
were classified as follows:
Type 1: Sentence modification errors. Generation
HITs are a minor source of errors, being responsible
for 10 problematic pairs. These errors are either in-
troduced by generating a false statement (Example
2), or by forming a not fully understandable, awk-
ward, or non-natural sentence (Example 3).
Example 2.
ENG: Kosovo was the subject of major riots in 1989.
ENG1: The Russian city of Kosovo was the subject of ...
Example 3.
ENG: Balat is the Kurdish-Armenian district of Instanbul.
ENG1: Balat is a place, which is the Kurdish-Armenian ...
Type 2: TE annotation errors. The notion of con-
taining more/less information, used in the ?Unidi-
rectional Entailment? HIT, can mostly be applied
straightforwardly to the entailment definition. How-
ever, the concept of ?more/less detailed?, which gen-
erally works for factual statements, in some cases is
not applicable. In fact, the MTurk workers have reg-
ularly interpreted the instructions about the amount
of information as concerning the quantity of con-
cepts contained in a sentence. This is not always cor-
responding to the actual entailment relation between
the sentences. As a consequence, 43 pairs featur-
ing wrong entailment annotations were encountered.
These errors can be classified as follows:
a) 13 pairs, where the added/removed information
changes the meaning of the sentence. In these cases,
the modified sentence was judged more/less specific
677
than the original one, leading to unidirectional en-
tailment annotation. On the contrary, in terms of
the standard entailment definition, the correct anno-
tation is ?no entailment? (as in Example 4, which
was annotated as ENG?ENG1):
Example 4.
ENG: If you decide to live in Bulgaria, you have to like
difficulties because they are not difficulties, they are challenges.
ENG1: You have to like difficulties as they are not difficulties,
they are challenges.
b) 10 pairs where the incorrect annotation is due to
a coreference problem, as in:
Example 5.
ENG: John Smith is the new CEO of the company.
ENG1: He is the new CEO of the company.
These pairs were labelled as unidirectional entail-
ments (in the example above ENG?ENG1), under
the assumption that a proper name is more specific
and informative than a pronoun. However, adher-
ing to the TE definition, co-referring expressions are
equivalent, and their realization does not play any
role in the entailment decision. This implies that the
correct entailment annotation is ?bidirectional?.
c) 9 pairs where the sentences are semantically
equivalent, but contain a piece of information which
is explicit in one sentence, and implicit in the other.
In these cases, Turkers judged the sentence contain-
ing the explicit mention as more specific, and thus
the pair was annotated as unidirectional entailment.
Example 6.
ENG: I hear the click of the trigger and the burst of bullets
reach me immediately.
ENG1: I hear the trigger and the burst of bullets reach me
instantly.
In Example 6, the expression ?the trigger? in ENG1
implicitly means ?the click of the trigger?, mak-
ing the two sentences equivalent, and the entailment
bidirectional (instead of ENG?ENG1).
d) 7 pairs where the information removed from or
added to the sentence is not relevant to the entail-
ment relation. In these cases, the modified sen-
tence was judged less/more specific than the origi-
nal one (and thus considered as unidirectional entail-
ment), even though the correct judgement is ?bidi-
rectional?, as in:
Example 7.
ENG: At the same time, AKP is struggling with its approach to
the EU.
ENG1: AKP is struggling with its approach to the European
Union.
e) 4 pairs where the added/removed information
concerns universally quantified general statements,
about which the interpretation of ?more/less spe-
cific? given by Turkers resulted in the wrong anno-
tation.
Example 8.
ENG: I think the success of multicultural couples depends on
the size of the cultural gap between the two partners
ENG1: I believe the success of the couples depends on the size
of the cultural gap between the 2 partners.
In Example 8, the additional information (?mul-
ticultural?) restricts the set to which it refers
(?couples?) making ENG entailed by ENG1, and
not vice versa as resulted from Turkers? annotation.
In light of this analysis, we conclude that the sen-
tence modification methodology proved to be suc-
cessful, as the low number of Type 1 errors shows.
Considering that the most expensive phase in the
creation of a TE dataset is the generation of the
pairs, this is a significant achievement. Differently,
the entailment assessment phase appears to be more
problematic, accounting for the majority of errors.
As shown by Type 2 errors, this is due to a par-
tial misalignment between the instructions given in
our HITs, and the formal definition of textual en-
tailment. For this reason, further experimentation
will explore different ways to instruct workers (e.g.
asking to consider proper names and pronouns as
equivalent) in order to reduce the amount of errors
produced. As a final remark, considering that in the
creation of a TE dataset the manual check of the an-
notated pairs represents a minor cost, even the in-
volvement of experts to filter out wrong annotations
would not decrease the cost-effectiveness of the pro-
posed methodology.
6 Conclusions
There is an increasing need of annotated data to
develop new solutions to the Textual Entailment
problem, explore new entailment-related tasks, and
set up experimental frameworks targeting real-world
applications. Following the recent trends promot-
ing annotation efforts that go beyond the estab-
lished RTE Challenge framework (unidirectional en-
tailment between monolingual T-H pairs), in this
678
paper we addressed the multilingual dimension of
the problem. Our primary goal was the creation of
large-scale collections of entailment pairs for differ-
ent language combinations. Besides that, we consid-
ered cost effectiveness and replicability as additional
requirements. To achieve our objectives, we devel-
oped a ?divide and conquer? methodology based on
crowdsourcing. Our approach presents several key
innovations with respect to the related works on TE
data acquisition. These include the decomposition
of a complex content generation task in a pipeline
of simpler subtasks accessible to a large crowd of
non-experts, and the integration of quality control
mechanisms at each stage of the process. The result
of our work is the first large-scale dataset contain-
ing both monolingual and cross-lingual corpora for
several combinations of texts-hypotheses in English,
Italian, and German. Among the advantages of our
method it is worth mentioning: i) the full alignment
between the created corpora, ii) the possibility to
easily extend the dataset to new languages, and iii)
the feasibility of creating general-purpose corpora,
featuring multi-directional entailment relations, that
subsume the traditional RTE-like annotation.
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853). The au-
thors would like to thank Emanuele Pianta for the
helpful discussions, and Giovanni Moretti for the
valuable support in the creation of the CLTE dataset.
References
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The Fifth
PASCAL Recognizing Textual Entailment Challenge.
Proceedings of TAC 2009.
Luisa Bentivogli, Elena Cabrio, Ido Dagan, Danilo Gi-
ampiccolo, Medea Lo Leggio, and Bernardo Magnini.
2010. Building Textual Entailment Specialized Data
Sets: a Methodology for Isolating Linguistic Phenom-
ena Relevant to Inference. Proceedings of LREC 2010.
Michael Bloodgood and Chris Callison-Burch. 2010.
Using Mechanical Turk to Build Machine Translation
Evaluation Sets. Proceedings of the NAACL 2010
Workshop on Creating Speech and Language Data
With Amazons Mechanical Turk.
Johan Bos, Fabio Massimo Zanzotto, and Marco Pennac-
chiotti. 2009. Textual Entailment at EVALITA 2009.
Proceedings of EVALITA 2009.
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing Speech and Language Data With Amazons Me-
chanical Turk. Proceedings NAACL-2010 Workshop
on Creating Speech and Language Data With Amazons
Mechanical Turk.
Ido Dagan and Oren Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. Proceedings of the PASCAL Workshop of
Learning Methods for Text Understanding and Min-
ing.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment.
Proceedings of NAACL-HLT 2010.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using Bilingual Parallel Corpora for Cross-
Lingual Textual Entailment. Proceedings of ACL-HLT
2011.
Rada Mihalcea and Carlo Strapparava. 2009. The Lie
Detector: Explorations in the Automatic Recognition
of Deceptive Language. Proceedings of ACL 2009.
Joanna Mrozinski, Edward Whittaker, and Sadaoki Furui.
2008. Collecting a Why-Question Corpus for Devel-
opment and Evaluation of an Automatic QA-System.
Proceedings of ACL 2008.
Matteo Negri and Yashar Mehdad. 2010. Creating a Bi-
lingual Entailment Corpus through Translations with
Mechanical Turk: $100 for a 10-day Rush. Proceed-
ings of the NAACL 2010 Workshop on Creating Speech
and Language Data With Amazons Mechanical Turk.
Mark Sammons, V. G. Vinod Vydiswaran, and Dan Roth.
2010. Ask Not What Textual Entailment Can Do for
You... Proceedings of ACL 2010.
Rion Snow, Brendan O?Connor, Daniel Jurafsky and An-
drew Y. Ng. 2008. Cheap and Fast - But is it Good?
Evaluating Non-Expert Annotations for Natural Lan-
guage Tasks. Proceedings of EMNLP 2008.
Rui Wang and Chris Callison-Burch. 2010. Cheap Facts
and Counter-Facts. Proceedings of the NAACL 2010
Workshop on Creating Speech and Language Data
With Amazons Mechanical Turk.
679
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1643?1653,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Assessing the Impact of Translation Errors
on Machine Translation Quality with Mixed-effects Models
Marcello Federico, Matteo Negri, Luisa Bentivogli, Marco Turchi
FBK - Fondazione Bruno Kessler
Via Sommarive 18, 38123 Trento, Italy
{federico,negri,bentivogli,turchi}@fbk.eu
Abstract
Learning from errors is a crucial aspect of
improving expertise. Based on this no-
tion, we discuss a robust statistical frame-
work for analysing the impact of different
error types on machine translation (MT)
output quality. Our approach is based on
linear mixed-effects models, which allow
the analysis of error-annotated MT out-
put taking into account the variability in-
herent to the specific experimental setting
from which the empirical observations are
drawn. Our experiments are carried out
on different language pairs involving Chi-
nese, Arabic and Russian as target lan-
guages. Interesting findings are reported,
concerning the impact of different error
types both at the level of human perception
of quality and with respect to performance
results measured with automatic metrics.
1 Introduction
The dominant statistical approach to machine
translation (MT) is based on learning from large
amounts of parallel data and tuning the result-
ing models on reference-based metrics that can
be computed automatically, such as BLEU (Pap-
ineni et al., 2001), METEOR (Banerjee and Lavie,
2005), TER (Snover et al., 2006), GTM (Turian
et al., 2003). Despite the steady progress in the
last two decades, especially for few well resourced
translation directions having English as target lan-
guage, this way to approach the problem is quickly
reaching a performance plateau. One reason is
that parallel data are a source of reliable informa-
tion but, alone, limit systems knowledge to ob-
served positive examples (i.e. how a sentence
should be translated) without explicitly modelling
any notion of error (i.e. how a sentence should
not be translated). Another reason is that, as a
development and evaluation criterion, automatic
metrics provide a holistic view of systems? be-
haviour without identifying the specific issues of a
translation. Indeed, the global scores returned by
MT evaluation metrics depend on comparisons be-
tween translation hypotheses and reference trans-
lations, where the causes and the nature of the dif-
ferences between them are not identified.
To cope with these issues and define system
improvement priorities, the focus of MT evalua-
tion research is gradually shifting towards profil-
ing systems? behaviour with respect to various ty-
pologies of errors (Vilar et al., 2006; Popovi?c and
Ney, 2011; Farr?us et al., 2012, inter alia). This
shift has enriched the traditional MT evaluation
framework with a new element, that is the actual
errors done by a system. Until now, most of the
research has focused on the relationship (i.e. the
correlation) between two elements of the frame-
work: humans and automatic evaluation metrics.
As a new element of the framework, which be-
comes a sort of ?evaluation triangle?, the analy-
sis of error annotations opens interesting research
problems related to the relationships between: i)
error types and human perception of MT quality
and ii) error types and the sensitivity of automatic
metrics.
Besides motivating further investigation on met-
rics featuring high correlation with human judge-
ments (a well-established MT research sub-field,
which is out of the scope of this paper), connecting
the vertices of this triangle raises new challenging
questions such as:
(1) Which types of MT errors have the high-
est impact on human perception of translation
quality? Surprisingly, little prior work focused
on this side of the triangle. Error annotations
have been considered to highlight strengths and
weaknesses of MT engines or to investigate the
influence of different error types on post-editors?
work. However, the direct connection between er-
1643
rors and users? preferences has been only partially
understood, mainly from a descriptive standpoint
and through rudimentary techniques unsuitable to
draw clear-cut conclusions or reliable inferences.
(2) To which types of errors are different MT
evaluation metrics more sensitive? This side of
the triangle has been even less explored. For in-
stance, little has been done to understand which
automatic metric is more suitable to assess sys-
tem improvements with respect to a specific issue
(e.g. word order or morphology) or to shed light
on the joint impact of different error types on per-
formance results calculated with different metrics.
To answer these questions, we propose a ro-
bust statistical framework to analyse the im-
pact of different error types, alone and in com-
bination, both on human perception of quality and
on MT evaluation metrics? results. Our analysis
is carried out by employing linear mixed-effects
models, a generalization of linear regression mod-
els suited to model responses with fixed and ran-
dom effects. Experiments are performed on data
covering three translation directions (English to
Chinese, Arabic and Russian). For each direc-
tion, two automatic translations were collected for
around 400 sentences and were manually evalu-
ated by expert translators through absolute quality
judgements and error annotation.
Building on the advantages offered by linear
mixed-effects models, our main contributions in-
clude:
? A rigorous method, novel to MT error anal-
ysis research, to relate MT issues to human
preferences and MT metrics? results;
? The application of such method to three
translation directions having English as
source and different languages as target;
? A number of findings, specific to each lan-
guage direction, which are out of the reach of
the few simpler methods proposed so far.
Overall, our study has clear practical implica-
tions for MT systems? development and evalu-
ation. Indeed, the proposed statistical analysis
framework represents an ideal instrument to: i)
identify translation issues having the highest im-
pact on human perception of quality and ii) choose
the most appropriate evaluation metric to measure
progress towards their solution.
2 Related Work
Error analysis, as a way to identify systems? weak-
nesses and define priorities for their improvement,
is gaining increasing interest in the MT com-
munity (Popovi?c and Ney, 2011; Popovic et al.,
2013). Along this direction, the initial efforts to
develop error taxonomies covering different levels
of granularity (Flanagan, 1994; Vilar et al., 2006;
Farr?us Cabeceran et al., 2010; Stymne and Ahren-
berg, 2012; Lommel et al., 2014) have been re-
cently complemented by investigations on how to
exploit error annotations for diagnostic purposes.
Error annotations of sentences produced by differ-
ent MT systems, in different target languages and
domains, have been used to determine the qual-
ity of translations according to the amount of er-
rors encountered (Popovic et al., 2013), to design
new automatic metrics that take into considera-
tion human annotations (Popovic, 2012; Bojar et
al., 2013), and to train classifiers that can auto-
matic identify fine-grained errors in the MT output
(Popovi?c and Ney, 2011). The impact of edit op-
erations on post-editors? productivity, which im-
plicitly connects the severity of different errors to
human activity, has also been studied (Temnikova,
2010; O?Brien, 2011; Blain et al., 2011), but
few attempts have been made to explicitly model
how fine-grained errors impact on human quality
judgements and automatic metrics.
Recently, the relation between different error
types, their frequency, and human quality judge-
ments has been investigated from a descriptive
standpoint in (Lommel et al., 2014; Popovi?c et al.,
2014). In both works, however, the underlying as-
sumption that the most frequent error has also the
largest impact on quality perception is not verified
(in general and, least of all, across language pairs,
domains, MT systems and post-editors). Another
limitation of the proposed (univariate) analysis lies
in the fact that it exclusively focuses on error types
taken in isolation. This simplification excludes the
possibility that humans, when assigning a global
quality score to a translation, may be influenced
not only by the error types but also by their inter-
action. The implications of such possibility call
for a multivariate analysis capable to model also
error interactions.
In (Kirchhoff et al., 2013), a statistically-
grounded approach based on conjoint analysis has
been used to investigate users? reactions to dif-
ferent types of translation errors. According to
1644
their results, word order is the most dispreferred
error type, and the count of the errors in a sen-
tence is not a good predictor of users? prefer-
ences. Though more sophisticated than methods
based on rough error counts, the conjoint model
is bound to several constraints that limit its us-
ability. In particular, the application of conjoint
analysis in this context requires to: i) operate with
semi-automatically created (hence artificial) data
instead of real MT output, ii) manually define dif-
ferent levels of severity for each error type (e.g.
high/medium/low), and iii) limit the number of er-
ror types considered to avoid the explosion of all
possible combinations. Finally, the conjoint anal-
ysis framework is not able to explicitly model vari-
ance in the translated sentences, the human anno-
tators, and the SMT systems used to translate the
source sentences. Our claim is that avoiding any
possible bias introduced by these factors should be
a priority in the analysis of empirical observations
in a given experimental setting.
So far, the relation between errors and auto-
matic metrics has been analysed by measuring the
correlation between single or total error frequen-
cies and automatic scores (Popovi?c and Ney, 2011;
Farr?us et al., 2012). Using two different error tax-
onomies, both works show that the sum of the er-
rors has a high correlation with BLEU and TER
scores. Similar to the aforementioned works ad-
dressing the impact of MT errors on human per-
ception, these studies disregard error interactions,
and their possible impact on automatic scores.
To overcome these issues, we propose a ro-
bust statistic analysis framework based on mixed-
effects models, which have been successfully ap-
plied to several NLP problems such as sentiment
analysis (Greene and Resnik, 2009), automatic
speech recognition (Goldwater et al., 2010), and
spoken language translation (Ruiz and Federico,
2014). Despite their effectiveness, the use of
mixed-effects models in the MT field is rather re-
cent and limited to the analysis of human post-
editions (Green et al., 2013; L?aubli et al., 2013).
In both studies, the goal was to evaluate the im-
pact of post-editing on the quality and productivity
of human translation assuming an ANOVA mixed
model for a between-subject design, in which hu-
man translators either post-edited or translated the
same texts. Our scenario is rather different as we
employ mixed models to measure the influence of
different MT error types - expressed as continu-
ous fixed effects - on quality judgements and auto-
matic quality metrics. Mixed models, having the
capability to absorb random variability due to the
specific experimental set-up, provide a robust mul-
tivariate method to efficiently analyse the impor-
tance of error types.
Finally, differently from all previous works, our
analysis is run on language pairs having English
as source and languages distant from English (in
term of morphology and word-order) as target.
3 Mixed-effects Models
Mixed-effects models - or simply mixed models
- like any regression model, express the relation-
ship between a response variable and some co-
variates and/or contrast factors. They enhance
conventional models by complementing fixed ef-
fects with so-called random effects. Random ef-
fects are introduced to absorb random variability
inherent to the specific experimental setting from
which the observations are drawn. In general, ran-
dom effects correspond to covariates that are not -
or cannot be - exhaustively observed in an experi-
ment, e.g. the human annotators and the evaluated
systems. Hence, mixed models permit to elegantly
cope with experimental design aspects that hinder
the applicability of conventional regression mod-
els. These are, in particular, the use of repeated
and/or clustered observations that introduce corre-
lations in the response variable that clearly violate
the independence and homoscedasticity assump-
tions of conventional linear, ANOVA, and logis-
tic regression models. Significance testing with
mixed models is in general more powerful, i.e. less
prone to Type II Errors, and also permits to reduce
the chance of Type I Errors in within-subject de-
signs, which are prone to the ?fallacy of language-
as-a-fixed-effect? (Clark, 1973).
Random effects can be directly associated to
the regression model parameters, as random in-
tercepts and random slopes, and have the same
form of the generic error component of the model,
i.e. normally distributed with zero mean and un-
known variance. As random effects introduce hid-
den variables, mixed models are trained with Ex-
pectation Maximization, while significance testing
is performed via likelihood-ratio (LR) tests.
In this work we employ mixed linear models to
measure the influence of different MT error types,
expressed as continuous fixed effects, on quality
1645
judgements or on automatic quality metrics.
1
We illustrate mixed linear models (Baayen et
al., 2008) by referring to our analysis, which ad-
dresses the relationships between a quality metric
(y) and different types of errors (e.g. A, B, and
C)
2
observed at the sentence level. For the sake of
simplicity, we assume to have balanced repeated
observations for one single crossed effect. That is,
we have i ? {1, . . . , I} MT systems (our groups)
each of which translated the same j ? {1, . . . , J}
test sentences. Our response variable y
ij
- a nu-
meric quality score - is computed on each (sen-
tence, system) pair, and we aim to investigate its
relationship with error statistics available for each
MT output, namely A
ij
, B
ij
and C
ij
. A (possible)
linear mixed model for our study would be:
y
ij
= ?
0
+ ?
1
A
ij
+ ?
2
B
ij
+ ?
3
C
ij
+ (1)
b
0,i
+ b
1,i
A
ij
+ b
2,i
B
ij
+ b
3,i
C
i
+ 
ij
The model is split into two lines on purpose. The
first line shows the fixed effect component, that is
intercept (?
0
) and slopes (?
1
, ?
2
, ?
3
) for each error
type. The second line specifies the random struc-
ture of the model, which includes random inter-
cept and slopes for each MT system and the resid-
ual error. Borrowing the notation from (Green
et al., 2013), we conveniently rewrite (1) in the
group-wise arranged matrix notation:
y
i
= x
T
i
? + z
T
i
b
i
+ 
i
(2)
where y
i
is the J ? 1 vector of responses, x
i
is the
J?p design matrix of covariates (including the in-
tercept) with fixed coefficients ? ? R
p?1
, z is the
random structure matrix defined by J ? q covari-
ates with random coefficients b
i
? R
q?1
, and 
i
is
the vector of residuals (in our example, p = 4 and
q = 4). By packing together vectors and matrices
indexed over groups i, we can rewrite the model
in a general form (Baayen et al., 2008), which can
represent any possible crossed-effects and random
structures defined over them allowing, at the same
time, for a compact model specification:
y = X
T
? + Z
T
b+  (3)
 ? N (0, ?
2
I), b ? N (0, ?
2
?), b ? 
1
Although mixed ordinal models (Tutz and Hennevogl,
1996) are in principle more appropriate to target quality
judgements, in our preliminary investigations mixed linear
models showed a significantly higher predictive power.
2
Here, A, B and C represent three generic error classes.
Their actual number in a given experimental setting will de-
pend on the granularity of the reference error taxonomy.
where ? is the relative variance-covariance q ? q
matrix of the random effects (now q = 4I), ?
2
is the variance of the per-observation term , the
symbol ? denotes independence of random vari-
ables, andN indicates the multivariate normal dis-
tribution. While b, ?, and ? are estimated via max-
imum likelihood, the single random intercept and
slope values for each group are calculated subse-
quently. They are referred to as Best Linear Un-
biased Predictors (BLUPS) and, formally, are not
parameters of the model.
The significance of the contribution of each sin-
gle parameter (e.g. single entries of ?) to the
goodness of fit can be tested via likelihood ratio.
In this way, both the fixed and random effect struc-
ture of the model can be investigated with respect
to its actual necessity to the model.
4 Dataset
For our analysis we used a dataset that covers
three translation directions, corresponding to En-
glish to Chinese, Arabic, and Russian. An inter-
national organization provided us a set of English
sentences together with their translation produced
by two anonymous MT systems. For each evalu-
ation item (source sentence and two MT outputs)
three experts were asked to assign quality scores to
the MT outputs, and a fourth expert was asked to
annotate translation errors. The four experts, who
were all professional translators native in the ex-
amined target languages, were carefully trained to
get acquainted with the evaluation guidelines and
the annotation tool specifically developed for these
evaluation tasks (Girardi et al., 2014). The anno-
tation process was carried out in parallel by all an-
notators over one week, resulting in a final dataset
composed of 312 evaluation items for the ENZH
direction, 393 for ENAR, and 437 for ENRU.
4.1 Quality Judgements
Quality judgements were collected by asking the
three experts to rate each automatic translation
according to a 1-5 Likert scale, where 1 means
?incomprehensible translation? and 5 means ?per-
fect translation?. The distribution of the collected
annotations with respect to each quality score is
shown in Figure 1. As we can see, this distri-
bution reflects different levels of perceived qual-
ity across languages. ENZH, for instance, has the
highest number of low quality scores (1 and 2),
while ENRU has the highest number of high qual-
1646
0%	 ?
20%	 ?
40%	 ?
60%	 ?
80%	 ?
100%	 ?
ENZH	 ? ENAR	 ? ENRU	 ?
5	 ?
4	 ?
3	 ?
2	 ?
1	 ?
Figure 1: Distribution of quality scores.
ity scores (4 and 5).
Table 1 shows the average of all the qual-
ity scores assigned by each annototator as well
as the average score obtained for each MT sys-
tem. These values demonstrate the variability
of annotators and systems. A particularly high
variability among human judges is observed for
the ENAR language direction (also reflected by
the inter-annotator agreement scores discussed be-
low), while ENZH shows the highest variability
between systems. As we will see in ?5.1, we suc-
cessfully cope with this variability by considering
systems and annotators as random effects, which
allow the regression models to abstract from these
differences.
Ann1 Ann2 Ann3 Sys1 Sys2
ENZH 2.38 2.69 2.21 2.29 2.56
ENAR 2.76 2.77 1.84 2.39 2.53
ENRU 2.82 2.72 2.96 2.87 2.79
Table 1: Average quality scores per annotator and
per system.
Inter-annotator agreement was computed using
the Fleiss? kappa coefficient (Fleiss, 1971), and re-
sulted in 22.70% for ENZH, 5.24% for ENAR, and
21.80% for ENRU. While for ENZH and ENRU
the results fall in the range of ?fair? agreement
(Landis and Koch, 1977), for ENAR only ?slight?
agreement is reached, reflecting the higher anno-
tators? variability evidenced in Table 1.
A more fine-grained agreement analysis is pre-
sented in Figure 2, where the kappa values are
given for each score class. In general we no-
tice a lower agreement on the intermediate quality
scores, while annotators tend to agree on very bad
and, even more, on good translations. In partic-
ular, we see that the agreement for ENAR is sys-
tematically lower than the values measured for the
other languages on all the score classes.
-??0.1	 ?
0	 ?
0.1	 ?
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
0.7	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
FLE
ISS
'	 ?KA
PPA
	 ?
QUALITY	 ?SCORES	 ?
ENZH	 ?
ENAR	 ?
ENRU	 ?
Figure 2: Class specific inter-annotator agreement.
4.2 Error Annotation
This evaluation task was carried out by one ex-
pert for each language direction, who was asked to
identify the type of errors present in the MT output
and to mark their position in the text. Since the fo-
cus of our work is the analysis method rather than
the definition of an ideal error taxonomy, for the
difficult language directions addressed we opted
for the following general error classes, partially
overlapping with (Vilar et al., 2006): i) reordering
errors, ii) lexicon errors (including wrong lexical
choices and extra words), iii) missing words, iv)
morphology errors.
Figure 3 shows the distribution of the errors in
terms of affected tokens (words) for each error
type. Since token counts for Chinese are not word-
based but character-based, for readability purposes
the number of errors counted for Chinese trans-
lations have been divided by 2.5. Note also that
morphological errors annotated for ENZH involve
only 13 characters and thus are not visible in the
plot. The total number of errors amounts to 16,320
characters for ENZH, 4,926 words for ENAR, and
5,965 words for ENRU.
This distribution highlights some differences
between languages directions. For example, trans-
lations into Arabic and Russian present several
morphology errors, while word reordering is the
most frequent issue for translations into Chinese.
As we will see in ?5.1, error frequency does not
give a direct indication of their impact on trasla-
tion quality judgements.
4.3 Automatic Metrics
In our investigation we consider three popular au-
tomatic metrics: sentence-level BLEU (Lin and
Och, 2004), TER (Snover et al., 2006), and GTM
(Turian et al., 2003). We compute all automatic
scores by relying on a single reference and by
1647
0	 ?
500	 ?
1000	 ?
1500	 ?
2000	 ?
2500	 ?
3000	 ?
3500	 ?
4000	 ?
ENZH	 ? ENAR	 ? ENRU	 ?
LEX	 ?
MISS	 ?
MORPH	 ?
REO	 ?
Figure 3: Distribution of error types.
means of standard packages. In particular, auto-
matic scores on Chinese are computed at the char-
acter level. Moreover, as we use metrics as re-
sponse variables for our regression models, we
compute all metrics at the sentence level. The
overall mean scores for all systems and languages
are reported in Table 2. Differences in systems?
performance can be observed for all language
pairs; as we will observe in ?5.2 such variability
explains the effectiveness of considering the MT
systems as a random effect.
BLEU TER GTM
Sys1 Sys2 Sys1 Sys2 Sy1 Sys2
ENZH 27.95 44.11 64.52 48.13 62.15 72.30
ENAR 19.63 25.25 68.83 63.99 47.20 52.33
ENRU 27.10 31.07 60.89 54.41 53.74 56.41
Table 2: Overall automatic scores per system.
5 Experiments
To assess the impact of translation errors on MT
quality we perform two sets of experiments. The
first set (?5.1) addresses the relation between er-
rors and human quality judgements. The sec-
ond set (?5.2) focuses on the relation between er-
rors and automatic metrics. In both cases, be-
fore measuring the impact of different errors on
the response variable (respectively quality judge-
ments and metrics), we validate the effectiveness
of mixed linear models by comparing their predic-
tion capability with other methods.
In all experiments, error counts of each category
were normalized into percentages with respect to
the sentence length and mapped in a logarithmic
scale. In this way, we basically assume that the
impact of errors tends to saturate above a given
threshold, hypothesis that also results in better fits
by our models.
3
Notice that while the chosen log-
3
In other words, we assume that human sensitivity to er-
10 base is easy to interpret, linear models can im-
plicitly adjust it. Our analysis makes use of mixed
linear models incorporating, as fixed effects, the
four types of errors (lex, miss, morph and reo) and
their pairwise interactions (the product of the sin-
gle error log counts), while their random struc-
ture depends on each specific experiment. For
the experiments we rely on the R language (R
Core Team, 2013) implementation of linear mixed
model in the lme4 library (Bates et al., 2014).
We assess the quality of our mixed linear mod-
els (MLM) by comparing their prediction capabil-
ity with a sequence of simpler linear models in-
cluding only fixed effects. In particular, we built
five univariate models and two multivariate mod-
els. The univariate models use as covariates, re-
spectively, the sum of all error types (baseline),
and each of the four types of errors (lex, miss,
morph and reo). The two multivariate models in-
clude all the four error types, considering them
without interactions (FLM w/o Interact.) and with
interactions (FLM).
Prediction performance is computed in terms of
Mean Absolute Error (MAE),
4
which we estimate
by averaging over 1,000 random splits of the data
in 90% training and 10% test. In particular, for the
human quality classes we pick the integer between
1-5 that is closest to the predicted value.
5.1 Errors vs. Quality Judgements
The response variable we target in this experiment
is the quality score produced by human annotators.
Our measurements follow a typical within-subject
design in which all the 3 annotators are exposed
to the same conditions (levels of the independent
variables), corresponding in our case to perfectly
balanced observations from 2 MT systems and N
sentences. This setting results in repeated or clus-
tered observations (thus violating independence)
corresponding to groups which naturally identify
possible random effects,
5
namely the annotators
(3 levels with 2xN observations each), the systems
(2 levels and 3xN observations each), and the sen-
rors follows a log-scale law: e.g. more sensitive to variations
in the interval [1-10] that in the interval [30-40].
4
MAE is calculated as the average of the absolute errors
|f
i
? y
i
|, where f
i
is the prediction of the model and y
i
the
true value for the i
th
instance. As it is a measure of error,
lower MAE scores indicate that our predictions are closer to
the true values of each test instance.
5
In all our experiments, random effects are limited to ran-
dom shifts since preliminary experiments also including ran-
dom slopes did not provide consistent results.
1648
Model ENZH ENAR ENRU
baseline 0.58 0.73 0.67
lex 0.67 0.78 0.72
miss 0.72 0.89 0.74
morph 0.72 0.89 0.74
reo 0.70 0.82 0.76
FLM w/o Interact. 0.59 0.77 0.65
FLM 0.57 0.72 0.63
MLM 0.53 0.61 0.61
Table 3: Prediction capability of human judge-
ments (MAE).
tences (N levels with 6 observations each). In prin-
ciple, such random effects permit to remove sys-
tematic biases of individual annotators, single sys-
tems and even single sentences, which are mod-
elled as random variables sampled from distinct
populations.
Table 3 shows a comparison of the prediction
capability of the mixed model
6
with simpler ap-
proaches. While the good performance achieved
by our strong baseline cannot be outperformed
by separately counting the number of errors of a
single type, lower MAE results are obtained by
methods based on multivariate analysis. Among
them, FLM brings the first consistent improve-
ments over the baseline by considering error in-
teractions, while MLM leads to the lowest MAE
due to the addition of random effects. The impor-
tance of random effects is particularly evidenced
by ENAR (12 points below the baseline). Indeed,
as discussed in ?4.1, for this language combina-
tion human annotators show the lowest agreement
score. This variability, which hides the smaller
differences in systems? behaviour, demonstrates
the importance of accounting for the erratic fac-
tors that might influence empirical observations in
a given setting. The good performance achieved
by MLM, combined with their high descriptive
power,
7
motivates their adoption in our study.
Concerning the analysis of error impact, Ta-
ble 4 shows the statistically significant coefficients
for the full-fledged MLM models for each trans-
lation direction. By default, all reported coeffi-
cients have p-values ? 10
?4
, while those marked
with ? and ? have respectively p-values ? 10
?3
and ? 10
?2
. Slope coefficients basically show
6
Note that the mixed model used in prediction does not in-
clude the random effect on sentences since the training sam-
ples do not guarantee sufficient observations for each test sen-
tence.
7
Note that the strong baseline used for comparison is not
capable to describe the contribution of the different error
types.
Error ENZH ENAR ENRU
Intercept 4.29 3.79
?
4.21
lex -1.27 -0.96 -1.12
miss -1.76 -0.90 -1.30
morph -0.48
?
-0.83 -0.51
reo -1.01 -0.75 -0.18
lex:miss 1.00 0.39 0.68
lex:morph - 0.29 0.32
lex:reo 0.50 0.21 -
miss:morph - 0.35 -
miss:reo 0.54 0.33 -
morph:reo - 0.37 -
Table 4: Effect of translation errors on MT qual-
ity perception on all judged sentences. Reported
coefficients (?) are all statistically significant with
p ? 10
?4
, except those marked with
?
(p ? 10
?3
),
and
?
(p ? 10
?2
).
the impact of different error types (alone and in
combination) on human quality scores. Those that
are not statistically significant are omitted as they
do not increase the fitting capability of our model.
As can be seen from the table, such impact varies
across the different language combinations. While
for ENZH and ENRU miss is the error having
the highest impact (highest decrement with respect
to the intercept), the most problematic error for
ENAR is lex. It is interesting to observe that pos-
itive values for error combinations indicate that
their combined impact is lower that the sum of the
impact of the single errors. For instance, while for
ENZH a one-step increment in lex and miss errors
would respectively cause a reduction in the human
judgement of 1.27 and 1.76, their occurrence in
the same sentence would be discounted by 1.00.
This would result in a global judgement of 2.26
(4.29 -1.27 -1.76 +1.00) instead of 1.26. While
for ENAR this phenomenon can be observed for
all error combinations, such discount effects are
not always significant for the other two language
pairs. The existence of discount effects of various
magnitude associated to the different error com-
binations is a novel finding made possible by the
adoption of mixed-effect models.
Another interesting observation is that, in con-
trast with the common belief that the most fre-
quent errors have the highest impact on human
quality judgements, our experiments do not re-
veal such strict correlation (at least for the exam-
ined language pairs). For instance, for ENZH and
ENRU the impact of miss errors is higher than the
impact of other more frequent issues.
1649
BLEU score TER GTM
Model ENZH ENAR ENRU ENZH ENAR ENRU ENZH ENAR ENRU
baseline 12.4 9.8 12.2 15.7 13.4 14.4 9.8 10.6 11.5
lex 12.9 10.4 13.0 16.3 13.8 14.9 9.7 10.9 12.1
miss 13.8 10.5 14.1 17.3 14.2 16.4 10.5 11.1 13.2
morph 13.9 10.3 13.6 17.5 13.8 16.3 10.5 10.9 13.1
reo 13.7 10.5 14.0 17.4 14.1 16.3 10.4 11.1 13.1
FLM w/o Interact. 12.9 9.9 12.2 16.3 13.5 14.4 9.7 10.7 11.7
FLM 12.3 9.7 12.1 15.6 13.4 14.3 9.4 10.6 11.6
MLM 10.8 9.5 12.0 14.7 13.0 14.2 8.9 10.5 11.6
Table 5: Prediction capability of BLEU score, TER and GTM (MAE).
5.2 Errors vs. Automatic Metrics
In this experiment, the response variable is an au-
tomatic metric which is computed on a sample of
MT outputs (which are again perfectly balanced
over systems and sentences) and a set of reference
translations. As no subjects are involved in the ex-
periment, random variability is assumed to come
from the involved systems, the tested sentences,
and the unknown missing link between the covari-
ates (error types) and the response variable which
is modelled by the residual noise. Notice that,
in this case, the random effect on the sentences
also incorporates in some sense the randomness
of the corresponding reference translations, which
are themselves representatives of larger samples.
The prediction capability of the mixed model,
in comparison with the simpler ones, is reported
in Table 5. Also in this case, the low MAE
achieved by the baseline is out of the reach of uni-
variate methods. Again, small improvements are
brought by FLM when considering error interac-
tions, whereas the most visible gains are achieved
by MLM due to their control of random effects.
This is more evident for some language combina-
tions and can be explained by the differences in
systems? performance, a variability factor easily
absorbed by random effects. Indeed, the largest
MAE decrements over the baseline are always ob-
served for ENZH (for which the overall mean re-
sults reported in Table 2 show the largest dif-
ferences) and the smallest decrements relate to
language/metric combinations where systems? be-
haviour is more similar (e.g. ENRU/GTM).
Concerning the analysis of error impact, Table
6 shows how different error types (alone and in
combination) influence performance results mea-
sured with automatic metrics. To ease interpre-
tation of the reported figures we also show Pear-
son and Spearman correlations of each set of coef-
ficients (excluding intercept estimates) with their
corresponding coefficients reported in Table 4. In
fact, our primary interest in this experiment is to
see which metrics show a sensitivity to specific er-
ror types similar to human perception. As we can
see, the coefficients for each metric significantly
vary depending on the language, for the simple
reason that also the distribution and co-occurrence
of errors vary significantly across the different lan-
guages and MT systems. Remarkably, for some
translation directions, some of the metrics show
a sensitivity to errors that is very similar to that
of human judges. In particular, BLEU for ENZH
and ENAR, and GTM for ENZH show a very high
correlation with the human sensitivity to transla-
tion errors, with Pearson correlation coefficient ?
0.97. For ENRU, the best Pearson correlation is
instead achieved by TER (-0.78).
Besides these general observations, a closer
look at the reported scores brings additional find-
ings. In three cases (BLEU for ENZH, GTM for
ENZH and ENAR) the analysed metrics are most
sensitive to the same error type that has the high-
est influence on human judgements (according to
Table 4, these are miss for ENZH and ENRU, lex
for ENAR). On the contrary, in one case (TER for
ENZH) the analysed metric is insensitive to the er-
ror type (miss) which has the highest impact on hu-
man quality scores. From a practical point of view,
these remarks provide useful indications about the
appropriateness of each metric to highlight the de-
ficiencies of a specific system and to measure im-
provements targeting specific issues. As a rule of
thumb, for instance, to measure improvements of
an ENZH system with respect to missing words,
it would be more advisable to use BLEU or GTM
instead of TER.
8
8
Note that this conclusion holds for our data sample, in
which different types of errors co-occur and only one refer-
ence translation is available. In such conditions, our regres-
sion model shows that TER is not influenced by miss errors in
a statistically significant way. This does not mean that TER
is insensitive to missing words when occurring in isolation,
1650
BLEU score TER GTM
Error ENZH ENAR ENRU ENZH ENAR ENRU ENZH ENAR ENRU
Intercept 60.55
2
38.45
?
51.73 32.41
2
52.25
?
33.4
?
83.57
?
60.11
?
75.38
lex -18.78 -9.25 -16.57 16.87 9.66 18.45 -13.63 -7.60 -16.13
miss -23.20 -10.41 -6.75 - - 8.24 -14.87 - -5.98
morph - -9.97 -12.65 - 8.90 11.41 - -6.60 -10.42
reo -13.27 -7.62 -10.57 14.44 9.81 6.39 -7.29 -5.50 -7.03
lex:miss 14.37 4.97
?
- - - - 8.24
?
- -
lex:morph - - 5.27
?
- - -5.22
?
- - 4.92
lex:reo 8.57 3.57
?
5.40
?
-7.24
?
-4.35
?
- 5.46 3.22
?
3.65
2
miss:morph - 4.44
?
- - - - - - -
miss:reo 6.74
?
- 4.30 - - -6.38
?
5.07
?
- 4.71
?
morph:reo - 3.81
?
- - -4.97
?
- - 2.57
?
-
Pearson 0.98 0.97 0.70 -0.58 -0.78 -0.78 0.98 0.78 0.74
Spearman 0.97 0.91 0.73 -0.57 -0.59 -0.80 0.97 0.59 0.76
Table 6: Effect of translation errors on BLEU score, TER and GTM on all judged sentences and correla-
tion with their corresponding effects on human quality scores (from Table 4). Reported coefficients (?)
are statistically significant with p ? 10
?4
, except those marked with
?
(p ? 10
?3
),
?
(p ? 10
?2
) and
2
(p ? 10
?1
).
Similar considerations also apply to the analysis
of the impact of error combinations. The same dis-
count effects that we noticed when analysing the
impact of errors? co-occurrence on human percep-
tion (?5.1) are evidenced, with different degrees of
sensitivity, by the automatic metrics. While some
of them substantially reflect human response (e.g.
BLEU and GTM for ENZH), in some cases we
observe either the insensitivity to specific combi-
nations (mostly for ENAR), or a higher sensitivity
compared to the values measured for human as-
sessors (mostly for ENRU, where the impact of
miss:reo combinations is discounted - hence un-
derestimated - by all the metrics).
Despite such small differences, the coherence of
our results with previous findings (?5.1) suggests
the reliability of the applied method. Complet-
ing the picture along the side of the MT evalua-
tion triangle which connects error annotations and
automatic metrics, our findings contribute to shed
light on the existing relationships between transla-
tion errors, their interaction, and the sensitivity of
widely used automatic metrics.
6 Conclusion
We investigated the MT evaluation triangle (hav-
ing as corners automatic metrics, human quality
judgements and error annotations) along the two
less explored sides, namely: i) the relation be-
tween MT errors and human quality judgements
but that TER becomes less sensitive to such errors when they
co-occur with other types of errors. Overall, our experiments
show that when MT outputs contain more than one error type,
automatic metrics show different levels of sensitivity to each
specific error type.
and ii) the relation between MT errors and auto-
matic metrics. To this aim we employed a ro-
bust statistical analysis framework based on lin-
ear mixed-effects models (the first contribution of
the paper), which have a higher descriptive power
than simpler methods based on the raw count of
translation errors and are less artificial compared
to previous statistically-grounded approaches.
Working on three translation directions having
Chinese, Arabic and Russian as target (our second
contribution), we analysed error-annotated trans-
lations considering the impact of specific errors
(alone and in combination) and accounting for the
variability of the experimental set-up that origi-
nated our empirical observations. This led us to
interesting findings specific to each language pair
(third contribution). Concerning the relation be-
tween MT errors and quality judgements, we have
shown that: i) the frequency of errors of a given
type does not correlate with human preferences,
ii) errors having the highest impact can be pre-
cisely isolated and iii) the impact of error inter-
actions is often subject to measurable and previ-
ously unknown ?discount? effects. Concerning the
relation between MT errors and automatic met-
rics (BLEU, TER and GTM), our analysis evi-
denced significant differences in the sensitivity of
each metric to different error types. Such differ-
ences provide useful indications about the most
appropriate metric to assess system improvements
with respect to specific weaknesses. If learning
from errors is a crucial aspect of improving exper-
tise, our method and the resulting empirical find-
ings represent a significant contribution towards a
1651
more informed approach to system development,
improvement and evaluation.
Acknowledgements
This work has been partially supported by the EC-
funded project MateCat (ICT-2011.4.2-287688).
References
Harald R. Baayen, Douglas J. Davidson, and Dou-
glas M. Bates. 2008. Mixed-effects modeling with
crossed random effects for subjects and items. Jour-
nal of memory and language, 59(4):390?412.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Douglas Bates, Martin Maechler, Ben Bolker, and
Steven Walker, 2014. lme4: Linear mixed-effects
models using Eigen and S4. R package version 1.1-
6.
Fr?ed?eric Blain, Jean Senellart, Holger Schwenk, Mirko
Plitt, and Johann Roturier. 2011. Qualitative analy-
sis of post-editing for high quality machine transla-
tion. In Asia-Pacific Association for Machine Trans-
lation (AAMT), editor, Machine Translation Summit
XIII, Xiamen (China), 19-23 sept.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Herbert H. Clark. 1973. The language-as-fixed-effect
fallacy: A critique of language statistics in psycho-
logical research. Journal of verbal learning and ver-
bal behavior, 12(4):335?359.
Mireia Farr?us, Marta R. Costa-juss`a, and Maja
Popovi?c. 2012. Study and correlation analysis of
linguistic, perceptual, and automatic machine trans-
lation evaluations. J. Am. Soc. Inf. Sci. Technol.,
63(1):174?184, January.
Mireia Farr?us Cabeceran, Marta Ruiz Costa-Juss`a,
Jos?e Bernardo Mari?no Acebal, Jos?e Adri?an
Rodr??guez Fonollosa, et al. 2010. Linguistic-based
evaluation criteria to identify statistical machine
translation errors. In Proceedings of the 14th
Annual Conference of the European Association for
Machine Translation (EAMT).
Mary Flanagan. 1994. Error classification for mt eval-
uation. In Technology Partnerships for Crossing the
Language Barrier: Proceedings of the First Confer-
ence of the Association for Machine Translation in
the Americas, pages 65?72.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5).
Christian Girardi, Luisa Bentivogli, Mohammad Amin
Farajian, and Marcello Federico. 2014. Mt-equal:
a toolkit for human assessment of machine trans-
lation output. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics: System Demonstrations, pages 120?
123, Dublin, Ireland, August. Dublin City Univer-
sity and Association for Computational Linguistics.
Sharon Goldwater, Daniel Jurafsky, and Christopher D.
Manning. 2010. Which words are hard to rec-
ognize? prosodic, lexical, and disfluency factors
that increase speech recognition error rates. Speech
Communication, 52(3):181?200.
Spence Green, Jeffrey Heer, and Christopher D. Man-
ning. 2013. The efficacy of human post-editing for
language translation. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems, pages 439?448. ACM.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL ?09, pages 503?511, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Katrin Kirchhoff, Daniel Capurro, and Anne M. Turner.
2013. A conjoint analysis framework for evaluating
user preferences in machine translation. Machine
Translation, pages 1?17.
Richard J. Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33 (1):159?174.
Samuel L?aubli, Mark Fishel, Gary Massey, Maureen
Ehrensberger-Dow, and Martin Volk. 2013. Assess-
ing Post-Editing Efficiency in a Realistic Translation
Environment. In Michel Simard Sharon O?Brien
and Lucia Specia (eds.), editors, Proceedings of MT
Summit XIV Workshop on Post-editing Technology
and Practice, pages 83?91, Nice, France.
Chin-Yew Lin and Franz Josef Och. 2004. Orange:
a method for evaluating automatic evaluation met-
rics for machine translation. In Proceedings of Col-
ing 2004, pages 501?507, Geneva, Switzerland, Aug
23?Aug 27. COLING.
Arle Lommel, Aljoscha Burchardt, Maja Popovi?c, Kim
Harris, Eleftherios Avramidis, and Hans Uszkoreit.
1652
2014. Using a new analytic measure for the anno-
tation and analysis of mt errors on real data. In
Proceedings of the 17th Conference of the Euro-
pean Association for Machine Translation (EAMT),
Dubrovnik, Croatia, June.
Sharon O?Brien. 2011. Cognitive Explorations of
Translation. Bloomsbury Studies in Translation.
Bloomsbury Academic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Research Report
RC22176, IBM Research Division, Thomas J. Wat-
son Research Center.
Maja Popovi?c and Hermann Ney. 2011. Towards au-
tomatic error analysis of machine translation output.
Comput. Linguist., 37(4):657?688, December.
Maja Popovic, Eleftherios Avramidis, Aljoscha Bur-
chardt, Sabine Hunsicker, Sven Schmeier, Cindy
Tscherwinka, David Vilar, and Hans Uszkoreit.
2013. Learning from human judgments of machine
translation output. In Proceedings of the MT Summit
XIV. Proceedings of MT Summit XIV.
Maja Popovi?c, Arle Lommel, Aljoscha Burchardt,
Eleftherios Avramidis, and Hans Uszkoreit. 2014.
Relations between different types of post-editing op-
erations, cognitive effort and temporal effort. In
Proceedings of the 17th Conference of the Euro-
pean Association for Machine Translation (EAMT),
Dubrovnik, Croatia, June.
Maja Popovic. 2012. Class error rates for evaluation
of machine translation output. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 71?75, Montr?eal, Canada, June. Associ-
ation for Computational Linguistics.
R Core Team, 2013. R: A Language and Environment
for Statistical Computing. R Foundation for Statis-
tical Computing, Vienna, Austria.
Nick Ruiz and Marcello Federico. 2014. Assessing the
Impact of Speech Recognition Errors on Machine
Translation Quality. In 11th Conference of the As-
sociation for Machine Translation in the Americas
(AMTA), Vancouver, BC, Canada.
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In 5th Conference of the Association for Machine
Translation in the Americas (AMTA), Boston, Mas-
sachusetts, August.
Sara Stymne and Lars Ahrenberg. 2012. On
the practice of error analysis for machine trans-
lation evaluation. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Uur Doan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey, may. European Language Resources
Association (ELRA).
Irina Temnikova. 2010. Cognitive evaluation approach
for a controlled language post-editing experiment.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan
Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC?10), Valletta, Malta, may. European
Language Resources Association (ELRA).
Joseph P. Turian, I. Dan Melamed, and Luke Shen.
2003. Evaluation of machine translation and its
evaluation. In Proceedings of the MT Summit IX.
Gerhard Tutz and Wolfgang Hennevogl. 1996. Ran-
dom effects in ordinal regression models. Computa-
tional Statistics & Data Analysis, 22(5):537?557.
David Vilar, Jia Xu, Luis Fernando dHaro, and Her-
mann Ney. 2006. Error analysis of statistical ma-
chine translation output. In Proceedings of the Fifth
International Conference on Language Resources
and Evaluation (LREC?06), pages 697?702.
1653
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 321?324,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Towards Cross-Lingual Textual Entailment
Yashar Mehdad1,2, Matteo Negri1, Marcello Federico1
FBK-Irst1, University of Trento2
Trento, Italy
{mehdad,negri,federico}@fbk.eu
Abstract
This paper investigates cross-lingual textual
entailment as a semantic relation between two
text portions in different languages, and pro-
poses a prospective research direction. We
argue that cross-lingual textual entailment
(CLTE) can be a core technology for sev-
eral cross-lingual NLP applications and tasks.
Through preliminary experiments, we aim at
proving the feasibility of the task, and provid-
ing a reliable baseline. We also introduce new
applications for CLTE that will be explored in
future work.
1 Introduction
Textual Entailment (TE) (Dagan and Glickman,
2004) has been proposed as a generic framework for
modeling language variability. Given two texts T
and H, the task consists in deciding if the meaning
of H can be inferred from the meaning of T. So far,
TE has been only applied in a monolingual setting,
where both texts are assumed to be written in the
same language. In this work, we propose and inves-
tigate a cross-lingual extension of TE, where we as-
sume that T and H are written in different languages.
The great potential of integrating (monolingual)
TE recognition components into NLP architectures
has been reported in several works, such as ques-
tion answering (Harabagiu and Hickl, 2006), infor-
mation retrieval (Clinchant et al, 2006), informa-
tion extraction (Romano et al, 2006), and document
summarization (Lloret et al, 2008).
To the best of our knowledge, mainly due to
the absence of cross-lingual TE (CLTE) recognition
components, similar improvements have not been
achieved yet in any cross-lingual application. As
a matter of fact, despite the great deal of attention
that TE has received in recent years (also witnessed
by five editions of the Recognizing Textual Entail-
ment Challenge1), interest for cross-lingual exten-
sions has not been in the mainstream of TE research,
which until now has been mainly focused on the En-
glish language.
Nevertheless, the strong interest towards cross-
lingual NLP applications (both from the market and
research perspectives, as demonstrated by success-
ful evaluation campaigns such as CLEF2) is, to our
view, a good reason to start investigating CLTE, as
well. Along such direction, research can now ben-
efit from recent advances in other fields, especially
machine translation (MT), and the availability of: i)
large amounts of parallel and comparable corpora in
many languages, ii) open source software to com-
pute word-alignments from parallel corpora, and iii)
open source software to set-up strong MT baseline
systems. We strongly believe that all these resources
can potentially help in developing inference mecha-
nisms on multilingual data.
Building on these considerations, this paper aims
to put the basis for future research on the cross-
lingual Textual Entailment task, in order to allow
for semantic inference across languages in different
NLP tasks. Among these, as a long-term goal, we
plan to adopt CLTE to support the alignment of text
portions that express the same meaning in different
languages. As a possible application scenario, CLTE
1http://pascallin.ecs.soton.ac.uk/Challenges/RTE/
2www.clef-campaign.org/
321
can be used to address content merging tasks in tidy
multilingual environments, such as commercial Web
sites, digital libraries, or user generated content col-
lections. Within such framework, as it will be dis-
cussed in the last section of this paper, CLTE com-
ponents can be used for automatic content synchro-
nization in a concurrent, collaborative, and multilin-
gual editing setting, e.g. Wikipedia.
2 Cross Lingual Textual Entailment
Adapting the definition of TE we define CLTE as
a relation between two natural language portions in
different languages, namely a text T (e.g. in En-
glish), and a hypothesis H (e.g. in French), that
holds if a human after reading T would infer that H
is most likely true, or otherwise stated, the meaning
of H can be entailed (inferred) from T .
We can see two main orthogonal directions for ap-
proaching CLTE: i) simply bring CLTE back to the
monolingual case by translating H into the language
of T, or vice-versa; ii) try to embed cross-lingual
processing techniques inside the TE recognition pro-
cess. In the following, we briefly overview and mo-
tivate each approach.
Basic approaches. The simplest approach is to
add a MT component to the front-end of an existing
TE engine. For instance, let the French hypothesis
H be translated into English and then run the TE en-
gine on T and the translation of H. There are sev-
eral good reasons to follow this divide-and-conquer
approach, as well as some drawbacks. Decoupling
the cross-lingual and the entailment components re-
sults in a simple and modular architecture that, ac-
cording to well known software engineering princi-
ples, results easier to develop, debug, and maintain.
Moreover, a decoupled CLTE architecture would al-
low for easy extensions to other languages as it just
requires extra MT systems. Along the same idea of
pivoting through English, in fact, the same TE sys-
tem can be employed to perform CLTE between any
language pair, once MT is available from each lan-
guage into English. A drawback of the decoupled
approach is that as MT is still far from being perfect,
translation errors are propagated to the TE engine
and might likely affect performance. To cope with
this issue, we explored the alternative approach of
applying TE on a list of n-best translations provided
by the MT engine, and take a final decision based on
some system combination criterion. This latter ap-
proach potentially reduces the impact of translation
errors, but might significantly increase the computa-
tional requirements of CLTE.
Advanced approaches. The idea is to move to-
wards a cross-lingual TE approach that takes advan-
tage of a tighter integration of MT and TE algo-
rithms and techniques. This could result in methods
for recognizing TE across languages without trans-
lating the texts and, in principle, with a lower com-
plexity. When dealing with phrase-based statistical
MT (Koehn et al, 2007), a possible approach is to
extract information from the phrase-table to enrich
the inference and entailment rules which could be
used in a distance based entailment system. As an
example the entailment relations between the French
phrase ?ordinateur portable? and the English phrase
?laptop?, or between the German phrase ?europaeis-
chen union? and the English word ?Europe? could
be captured from parallel corpora through statistical
phrase-based MT approaches.
There are several implications that make this ap-
proach interesting. First of all, we believe that re-
search on CLTE can employ inference mechanisms
and semantic knowledge sources to augment exist-
ing MT methods, leading to improvements in the
translation quality (e.g. (Pado? et al, 2009)). In
addition, the acquired rules could as well enrich
the available multilingual resources and dictionaries
such as MultiWordNet3.
3 Feasibility studies
The main purpose of our preliminary experiments is
to verify the feasibility of CLTE, as well as setting
baseline results to be further improved over time. To
this aim, we started by adopting the basic approach
previously discussed. In particular, starting from an
English/French corpus of T-H pairs, we automati-
cally translated each H fragment from French into
English.
Our decisions build on several motivations. First
of all, the reason for setting English and French
as a first language pair for experiments is to rely
on higher quality translation models, and larger
amounts of parallel data for future improvements.
3http://multiwordnet.fbk.eu/
322
Second, the reason for translating the hypotheses is
that, according to the notion of TE, they are usually
shorter, less detailed, and barely complex in terms of
syntax and concepts with respect to the texts. This
makes them easier to translate preserving the origi-
nal meaning. Finally, from an application-oriented
perspective, working with English Ts seems more
promising due the richness of English data available
(e.g. in terms of language variability, and more de-
tailed elaboration of concepts). This increases the
probability to discover entailment relations with Hs
in other languages.
In order to create a realistic and standard setting,
we took advantage of the available RTE data, select-
ing the RTE3 development set and manually trans-
lating the hypotheses into French. Since the man-
ual translation requires trained translators, and due
to time and logistics constraints, we obtained 520
translated hypotheses (randomly selected from the
entire RTE3 development set) which built our bi-
lingual entailment corpus for evaluation.
In the initial step, following our basic approach,
we translated the French hypotheses to English us-
ing Google4 and Moses5. We trained a phrase-
base translation model using Europarl6 and News
Commentary parallel corpora in Moses, applying a
6-gram language model trained on the New York
Times portion of the English Gigaword corpus7.
As a TE engine , we used the EDITS8 package
(Edit Distance Textual Entailment Suite). This sys-
tem is an open source software package based on
edit distance algorithms, which computes the T-H
distance as the cost of the edit operations (i.e. in-
sertion, deletion and substitution) that are necessary
to transform T into H. By defining the edit distance
algorithm and a cost scheme (i.e. which defines the
costs of each edit operation), this package is able to
learn a distance model over a set of training pairs,
which is used to decide if an entailment relation
holds over each test pair.
In order to obtain a monolingual TE model, we
trained and tuned (Mehdad, 2009) our model on the
RTE3 test set, to reduce the overfitting bias, since
4http://translate.google.com
5http://www.statmt.org/moses/
6http://www.statmt.org/europarl/
7http://www.ldc.upenn.edu
8http://edits.fbk.eu/
our original data was created over the RTE3 devel-
opment set. Moreover, we used a set of lexical en-
tailment rules extracted from Wikipedia and Word-
Net, as described in (Mehdad et al, 2009). To be-
gin with, we used this model to classify the cre-
ated cross-lingual entailment corpus in three differ-
ent settings: 1) hypotheses translated by Google, 2)
hypotheses translated by Moses (1st best), and 3) the
original RTE3 monolingual English pairs.
Results reported in Table 1 show that using
Google as a translator, in comparison with the orig-
inal manually-created data, does not cause any drop
in performance. This confirms that merely trans-
lating the hypothesis using a very good translation
model (Google) is a feasible and promising direc-
tion for CLTE. Knowing that Google has one of the
best French-English translation models, the down-
trend of results using Moses translator, in contrast
with Google, is not out of our expectation. Trying
to bridge this gap brings us to the next round of
experiments, where we extracted the n-best trans-
Orig. Google Moses Moses Moses
1st best 30 best > 0.4
Acc. 63.48 63.48 61.37 62.90 62.90
Table 1: Results comparison over 520 test pairs.
lations produced by Moses, to have a richer lexical
variability, beneficial for improving the TE recogni-
tion. The graph in Figure 1 shows an incremental
improvement when the n-best translated hypotheses
are used. Besides that, trying to reach a more mono-
tonic distribution of the results, we normalized the
ranking score (from 0 to 1) given by Moses, and in
each step we chose the first n results over a normal-
ized score. In this way, having the hypotheses with
the score of above 0.4, we achieved the highest accu-
racy of 62.9%. This is exactly equal to adopting the
30-best hypotheses translated by Moses. Using this
method, we could improve the performance up to
1.5% above the 1st best results, achieving almost the
same level of performance obtained with Google.
4 A possible application scenario
Among the many possible applications, the task of
managing textual information in multiple languages
represents an ideal application scenario for CLTE.
Along such direction, our long-term goal is to use
323
Figure 1: Accuracy gained by n-best Moses translations.
CLTE components in the task of synchronizing the
content of documents about the same topic (e.g.
Wikipedia articles), written in different languages.
Currently, multilingual Wikis rely on users to manu-
ally translate different Wiki pages on the same sub-
ject. This is not only a time-consuming procedure
but also the source of many inconsistencies, as users
update the different language versions separately,
and every update would require translators to com-
pare the different language versions and synchronize
the updates. Our goal is to automate this process
by integrating MT and CLTE in a two-step process
where: i) CLTE is used to identify text portions that
should ?migrate? from one page to the other, and ii)
MT is used to actually translate these portions in the
appropriate target language.
The adoption of entailment-based techniques to
address the multilingual content synchronization
task looks promising, as several issues inherent to
such task can be formalized as TE-related problems.
Given two pages (P1 and P2), these issues include
identifying (and then properly managing):
1. Text portions in P1 and P2 that express exactly
the same meaning (bi-directional entailment, or se-
mantic equivalence) and which should not migrate
across pages;
2. Text portions in P1 that are more specific than
portions of P2 (unidirectional entailment between
P2 and P1 or vice-versa) and should replace them;
3. Text portions in P1 describing facts that are not
present in P2, and which should be added in P2 or
vice-versa (the ?unknown? cases in RTE parlance);
4. Meaning discrepancies between text portions
in P1 and text portions in P2 (?contradictions? in
RTE parlance).
5 Conclusion
This paper presented a preliminary investigation to-
wards cross-lingual Textual Entailment, focusing on
possible research directions and alternative method-
ologies. Baseline results have been provided to
demonstrate the potentialities of a simple approach
that integrates MT and monolingual TE compo-
nents. Overall, our work sets a novel framework
for further studies and experiments to improve cross-
lingual NLP tasks. In particular, CLTE can be scaled
to more complex problems, such as cross-lingual
content merging and synchronization.
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853)
References
S. Clinchant, C. Goutte, and E. Gaussier. 2006. Lex-
ical entailment for information retrieval. In Proc.
ECIR?06.
I. Dagan and O. Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. Proc. of the PASCAL Workshop of Learn-
ing Methods for Text Understanding and Mining.
S. Harabagiu and A. Hickl. 2006. Methods for using tex-
tual entailment in open-domain question answering.
In Proc. COLING/ACL 2006.
P. Koehn et al 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. ACL07 Demo
and Poster Sessions.
E. Lloret, O?. Ferra?ndez, R. Mun?oz, and M. Palomar.
2008. A text summarization approach under the in-
fluence of textual entailment. In Proc. NLPCS 2008.
Y. Mehdad, M. Negri, E. Cabrio, M. Kouylekov, and
B. Magnini. 2009. Edits: An open source framework
for recognizing textual entailment. In Proc. TAC 2009.
To appear.
Yashar Mehdad. 2009. Automatic cost estimation for
tree edit distance using particle swarm optimization.
In Proc. ACL ?09.
S. Pado?, M. Galley, D. Jurafsky, and C. D. Manning.
2009. Textual entailment features for machine trans-
lation evaluation. In Proc. StatMT ?09.
L. Romano, M. Kouylekov, I. Szpektor, I. Dagan, and
A. Lavelli. 2006. Investigating a generic paraphrase-
based approach for relation extraction. In Proc. EACL
2006.
324
Proceedings of the ACL 2010 System Demonstrations, pages 42?47,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
An Open-Source Package for Recognizing Textual Entailment
Milen Kouylekov and Matteo Negri
FBK - Fondazione Bruno Kessler
Via Sommarive 18, 38100 Povo (TN), Italy
[kouylekov,negri]@fbk.eu
Abstract
This paper presents a general-purpose
open source package for recognizing Tex-
tual Entailment. The system implements a
collection of algorithms, providing a con-
figurable framework to quickly set up a
working environment to experiment with
the RTE task. Fast prototyping of new
solutions is also allowed by the possibil-
ity to extend its modular architecture. We
present the tool as a useful resource to ap-
proach the Textual Entailment problem, as
an instrument for didactic purposes, and as
an opportunity to create a collaborative en-
vironment to promote research in the field.
1 Introduction
Textual Entailment (TE) has been proposed as
a unifying generic framework for modeling lan-
guage variability and semantic inference in dif-
ferent Natural Language Processing (NLP) tasks.
The Recognizing Textual Entailment (RTE) task
(Dagan and Glickman, 2007) consists in deciding,
given two text fragments (respectively called Text
- T, and Hypothesis - H), whether the meaning of
H can be inferred from the meaning of T, as in:
T: ?Yahoo acquired Overture?
H: ?Yahoo owns Overture?
The RTE problem is relevant for many different
areas of text processing research, since it repre-
sents the core of the semantic-oriented inferences
involved in a variety of practical NLP applications
including Question Answering, Information Re-
trieval, Information Extraction, Document Sum-
marization, and Machine Translation. However, in
spite of the great potential of integrating RTE into
complex NLP architectures, little has been done
to actually move from the controlled scenario pro-
posed by the RTE evaluation campaigns1 to more
practical applications. On one side, current RTE
technology might not be mature enough to provide
reliable components for such integration. Due to
the intrinsic complexity of the problem, in fact,
state of the art results still show large room for im-
provement. On the other side, the lack of available
tools makes experimentation with the task, and the
fast prototyping of new solutions, particularly dif-
ficult. To the best of our knowledge, the broad
literature describing RTE systems is not accompa-
nied with a corresponding effort on making these
systems open-source, or at least freely available.
We believe that RTE research would significantly
benefit from such availability, since it would allow
to quickly set up a working environment for ex-
periments, encourage participation of newcomers,
and eventually promote state of the art advances.
The main contribution of this paper is to present
the latest release of EDITS (Edit Distance Textual
Entailment Suite), a freely available, open source
software package for recognizing Textual Entail-
ment. The system has been designed following
three basic requirements:
Modularity. System architecture is such that the
overall processing task is broken up into major
modules. Modules can be composed through a
configuration file, and extended as plug-ins ac-
cording to individual requirements. System?s
workflow, the behavior of the basic components,
and their IO formats are described in a compre-
hensive documentation available upon download.
Flexibility. The system is general-purpose, and
suited for any TE corpus provided in a simple
XML format. In addition, both language depen-
dent and language independent configurations are
allowed by algorithms that manipulate different
representations of the input data.
1TAC RTE Challenge: http://www.nist.gov/tac
EVALITA TE task: http://evalita.itc.it
42
Figure 1: Entailment Engine, main components
and workflow
Adaptability. Modules can be tuned over train-
ing data to optimize performance along several di-
mensions (e.g. overall Accuracy, Precision/Recall
trade-off on YES and NO entailment judgements).
In addition, an optimization component based on
genetic algorithms is available to automatically set
parameters starting from a basic configuration.
EDITS is open source, and available under
GNU Lesser General Public Licence (LGPL). The
tool is implemented in Java, it runs on Unix-based
Operating Systems, and has been tested on MAC
OSX, Linux, and Sun Solaris. The latest release
of the package can be downloaded from http:
//edits.fbk.eu.
2 System Overview
The EDITS package allows to:
? Create an Entailment Engine (Figure 1) by
defining its basic components (i.e. algo-
rithms, cost schemes, rules, and optimizers);
? Train such Entailment Engine over an anno-
tated RTE corpus (containing T-H pairs anno-
tated in terms of entailment) to learn aModel;
? Use the Entailment Engine and the Model to
assign an entailment judgement and a confi-
dence score to each pair of an un-annotated
test corpus.
EDITS implements a distance-based framework
which assumes that the probability of an entail-
ment relation between a given T-H pair is inversely
proportional to the distance between T and H (i.e.
the higher the distance, the lower is the probability
of entailment). Within this framework the system
implements and harmonizes different approaches
to distance computation, providing both edit dis-
tance algorithms, and similarity algorithms (see
Section 3.1). Each algorithm returns a normalized
distance score (a number between 0 and 1). At a
training stage, distance scores calculated over an-
notated T-H pairs are used to estimate a threshold
that best separates positive from negative exam-
ples. The threshold, which is stored in a Model, is
used at a test stage to assign an entailment judge-
ment and a confidence score to each test pair.
In the creation of a distance Entailment Engine,
algorithms are combined with cost schemes (see
Section 3.2) that can be optimized to determine
their behaviour (see Section 3.3), and optional ex-
ternal knowledge represented as rules (see Section
3.4). Besides the definition of a single Entailment
Engine, a unique feature of EDITS is that it al-
lows for the combination of multiple Entailment
Engines in different ways (see Section 4.4).
Pre-defined basic components are already pro-
vided with EDITS, allowing to create a variety of
entailment engines. Fast prototyping of new solu-
tions is also allowed by the possibility to extend
the modular architecture of the system with new
algorithms, cost schemes, rules, or plug-ins to new
language processing components.
3 Basic Components
This section overviews the main components of
a distance Entailment Engine, namely: i) algo-
rithms, iii) cost schemes, iii) the cost optimizer,
and iv) entailment/contradiction rules.
3.1 Algorithms
Algorithms are used to compute a distance score
between T-H pairs.
EDITS provides a set of predefined algorithms,
including edit distance algorithms, and similar-
ity algorithms adapted to the proposed distance
framework. The choice of the available algorithms
is motivated by their large use documented in RTE
literature2.
Edit distance algorithms cast the RTE task as
the problem of mapping the whole content of H
into the content of T. Mappings are performed
as sequences of editing operations (i.e. insertion,
deletion, substitution of text portions) needed to
transform T into H, where each edit operation has
a cost associated with it. The distance algorithms
available in the current release of the system are:
2Detailed descriptions of all the systems participating in
the TAC RTE Challenge are available at http://www.
nist.gov/tac/publications
43
? Token Edit Distance: a token-based version
of the Levenshtein distance algorithm, with
edit operations defined over sequences of to-
kens of T and H;
? Tree Edit Distance: an implementation of the
algorithm described in (Zhang and Shasha,
1990), with edit operations defined over sin-
gle nodes of a syntactic representation of T
and H.
Similarity algorithms are adapted to the ED-
ITS distance framework by transforming measures
of the lexical/semantic similarity between T and H
into distance measures. These algorithms are also
adapted to use the three edit operations to support
overlap calculation, and define term weights. For
instance, substitutable terms in T and H can be
treated as equal, and non-overlapping terms can be
weighted proportionally to their insertion/deletion
costs. Five similarity algorithms are available,
namely:
? Word Overlap: computes an overall (dis-
tance) score as the proportion of common
words in T and H;
? Jaro-Winkler distance: a similarity algorithm
between strings, adapted to similarity on
words;
? Cosine Similarity: a common vector-based
similarity measure;
? Longest Common Subsequence: searches the
longest possible sequence of words appearing
both in T and H in the same order, normaliz-
ing its length by the length of H;
? Jaccard Coefficient: confronts the intersec-
tion of words in T and H to their union.
3.2 Cost Schemes
Cost schemes are used to define the cost of each
edit operation.
Cost schemes are defined as XML files that ex-
plicitly associate a cost (a positive real number) to
each edit operation applied to elements of T and
H. Elements, referred to as A and B, can be of dif-
ferent types, depending on the algorithm used. For
instance, Tree Edit Distance will manipulate nodes
in a dependency tree representation, whereas To-
ken Edit Distance and similarity algorithms will
manipulate words. Figure 2 shows an example of
<scheme>
<insertion><cost>10</cost></insertion>
<deletion><cost>10</cost></deletion>
<substitution>
<condition>(equals A B)</condition>
<cost>0</cost>
</substitution>
<substitution>
<condition>(not (equals A B))</condition>
<cost>20</cost>
</substitution>
</scheme>
Figure 2: Example of XML Cost Scheme
cost scheme, where edit operation costs are de-
fined as follows:
Insertion(B)=10 - inserting an element B from H
to T, no matter what B is, always costs 10;
Deletion(A)=10 - deleting an element A from T,
no matter what A is, always costs 10;
substitution(A,B)=0 if A=B - substituting A with
B costs 0 if A and B are equal;
substitution(A,B)=20 if A !=B - substituting A
with B costs 20 if A and B are different.
In the distance-based framework adopted by
EDITS, the interaction between algorithms and
cost schemes plays a central role. Given a T-H
pair, in fact, the distance score returned by an al-
gorithm directly depends on the cost of the opera-
tions applied to transform T into H (edit distance
algorithms), or on the cost of mapping words in
H with words in T (similarity algorithms). Such
interaction determines the overall behaviour of an
Entailment Engine, since distance scores returned
by the same algorithm with different cost schemes
can be considerably different. This allows users to
define (and optimize, as explained in Section 3.3)
the cost schemes that best suit the RTE data they
want to model3.
EDITS provides two predefined cost schemes:
? Simple Cost Scheme - the one shown in Fig-
ure 2, setting fixed costs for each edit opera-
tion.
? IDF Cost Scheme - insertion and deletion
costs for a word w are set to the inverse doc-
ument frequency of w (IDF(w)). The sub-
stitution cost is set to 0 if a word w1 from
T and a word w2 from H are the same, and
IDF(w1)+IDF(w2) otherwise.
3For instance, when dealing with T-H pairs composed by
texts that are much longer than the hypotheses (as in the RTE5
Campaign), setting low deletion costs avoids penalization to
short Hs fully contained in the Ts.
44
In the creation of new cost schemes, users can
express edit operation costs, and conditions over
the A and B elements, using a meta-language
based on a lisp-like syntax (e.g. (+ (IDF A) (IDF
B)), (not (equals A B))). The system also provides
functions to access data stored in hash files. For
example, the IDF Cost Scheme accesses the IDF
values of the most frequent 100K English words
(calculated on the Brown Corpus) stored in a file
distributed with the system. Users can create new
hash files to collect statistics about words in other
languages, or other information to be used inside
the cost scheme.
3.3 Cost Optimizer
A cost optimizer is used to adapt cost schemes (ei-
ther those provided with the system, or new ones
defined by the user) to specific datasets.
The optimizer is based on cost adaptation
through genetic algorithms, as proposed in
(Mehdad, 2009). To this aim, cost schemes can
be parametrized by externalizing as parameters the
edit operations costs. The optimizer iterates over
training data using different values of these param-
eters until on optimal set is found (i.e. the one that
best performs on the training set).
3.4 Rules
Rules are used to provide the Entailment Engine
with knowledge (e.g. lexical, syntactic, semantic)
about the probability of entailment or contradic-
tion between elements of T and H. Rules are in-
voked by cost schemes to influence the cost of sub-
stitutions between elements of T and H. Typically,
the cost of the substitution between two elements
A and B is inversely proportional to the probability
that A entails B.
Rules are stored in XML files called Rule
Repositories, with the format shown in Figure 3.
Each rule consists of three parts: i) a left-hand
side, ii) a right-hand side, iii) a probability that
the left-hand side entails (or contradicts) the right-
hand side.
EDITS provides three predefined sets of lexical
entailment rules acquired from lexical resources
widely used in RTE: WordNet4, Lin?s word sim-
ilarity dictionaries5, and VerbOcean6.
4http://wordnet.princeton.edu
5http://webdocs.cs.ualberta.ca/ lindek/downloads.htm
6http://demo.patrickpantel.com/Content/verbocean
<rule entailment="ENTAILMENT">
<t>acquire</t>
<h>own</h>
<probability>0.95</probability>
</rule>
<rule entailment="CONTRADICTION">
<t>beautiful</t>
<h>ugly</h>
<probability>0.88</probability>
</rule>
Figure 3: Example of XML Rule Repository
4 Using the System
This section provides basic information about the
use of EDITS, which can be run with commands
in a Unix Shell. A complete guide to all the pa-
rameters of the main script is available as HTML
documentation downloadable with the package.
4.1 Input
The input of the system is an entailment corpus
represented in the EDITS Text Annotation Format
(ETAF), a simple XML internal annotation for-
mat. ETAF is used to represent both the input T-H
pairs, and the entailment and contradiction rules.
ETAF allows to represent texts at two different
levels: i) as sequences of tokens with their asso-
ciated morpho-syntactic properties, or ii) as syn-
tactic trees with structural relations among nodes.
Plug-ins for several widely used annotation
tools (including TreeTagger, Stanford Parser, and
OpenNLP) can be downloaded from the system?s
website. Users can also extend EDITS by imple-
menting plug-ins to convert the output of other an-
notation tools in ETAF.
Publicly available RTE corpora (RTE 1-3, and
EVALITA 2009), annotated in ETAF at both the
annotation levels, are delivered together with the
system to be used as first experimental datasets.
4.2 Configuration
The creation of an Entailment Engine is done by
defining its basic components (algorithms, cost
schemes, optimizer, and rules) through an XML
configuration file. The configuration file is divided
in modules, each having a set of options. The fol-
lowing XML fragment represents a simple exam-
ple of configuration file:
<module alias="distance">
<module alias="tree"/>
<module alias="xml">
<option name="scheme-file"
45
value="IDF_Scheme.xml"/>
</module>
<module alias="pso"/>
</module>
This configuration defines a distance Entailment
Engine that combines Tree Edit Distance as a core
distance algorithm, and the predefined IDF Cost
Scheme that will be optimized on training data
with the Particle Swarm Optimization algorithm
(?pso?) as in (Mehdad, 2009). Adding external
knowledge to an entailment engine can be done by
extending the configuration file with a reference to
a rules file (e.g. ?rules.xml?) as follows:
<module alias="rules">
<option name="rules-file"
value="rules.xml"/>
</module>
4.3 Training and Test
Given a configuration file and an RTE corpus an-
notated in ETAF, the user can run the training
procedure to learn a model. At this stage, ED-
ITS allows to tune performance along several di-
mensions (e.g. overall Accuracy, Precision/Recall
trade-off on YES and/or NO entailment judge-
ments). By default the system maximizes the over-
all accuracy (distinction between YES and NO
pairs). The output of the training phase is a model:
a zip file that contains the learned threshold, the
configuration file, the cost scheme, and the en-
tailment/contradiction rules used to calculate the
threshold. The explicit availability of all this in-
formation in the model allows users to share, repli-
cate and modify experiments7.
Given a model and an un-annotated RTE corpus
as input, the test procedure produces a file con-
taining for each pair: i) the decision of the system
(YES, NO), ii) the confidence of the decision, iii)
the entailment score, iv) the sequence of edit oper-
ations made to calculate the entailment score.
4.4 Combining Engines
A relevant feature of EDITS is the possibility to
combine multiple Entailment Engines into a sin-
gle one. This can be done by grouping their def-
initions as sub-modules in the configuration file.
EDITS allows users to define customized combi-
nation strategies, or to use two predefined com-
bination modalities provided with the package,
7Our policy is to publish online the models we use for par-
ticipation in the RTE Challenges. We encourage other users
of EDITS to do the same, thus creating a collaborative envi-
ronment, allow new users to quickly modify working config-
urations, and replicate results.
Figure 4: Combined Entailment Engines
namely: i) Linear Combination, and ii) Classi-
fier Combination. The two modalities combine in
different ways the entailment scores produced by
multiple independent engines, and return a final
decision for each T-H pair.
Linear Combination returns an overall entail-
ment score as the weighted sum of the entailment
scores returned by each engine:
scorecombination =
n?
i=0
scorei ? weighti (1)
In this formula, weighti is an ad-hoc weight
parameter for each entailment engine. Optimal
weight parameters can be determined using the
same optimization strategy used to optimize the
cost schemes, as described in Section 3.3.
Classifier Combination is similar to the ap-
proach proposed in (Malakasiotis and Androut-
sopoulos, 2007), and is based on using the entail-
ment scores returned by each engine as features to
train a classifier (see Figure 4). To this aim, ED-
ITS provides a plug-in that uses the Weka8 ma-
chine learning workbench as a core. By default
the plug-in uses an SVM classifier, but other Weka
algorithms can be specified as options in the con-
figuration file.
The following configuration file describes a
combination of two engines (i.e. one based on
Tree Edit Distance, the other based on Cosine
Similarity), used to train a classifier with Weka9.
<module alias="weka">
<module alias="distance">
<module alias="tree"/>
<module alias="xml">
<option name="scheme-file"
value="IDF_Scheme.xml"/>
</module>
</module>
8http://www.cs.waikato.ac.nz/ml/weka
9A linear combination can be easily obtained by changing
the alias of the highest-level module (?weka?) into ?linear?.
46
<module alias="distance">
<module alias="cosine"/>
<module alias="IDF_Scheme.xml"/>
</module>
</module>
5 Experiments with EDITS
To give an idea of the potentialities of the ED-
ITS package in terms of flexibility and adaptabil-
ity, this section reports some results achieved in
RTE-related tasks by previous versions of the tool.
The system has been tested in different scenarios,
ranging from the evaluation of standalone systems
within task-specific RTE Challenges, to their inte-
gration in more complex architectures.
As regards the RTE Challenges, in the last
years EDITS has been used to participate both in
the PASCAL/TAC RTE Campaigns for the En-
glish language (Mehdad et al, 2009), and in the
EVALITA RTE task for Italian (Cabrio et al,
2009). In the last RTE-5 Campaign the result
achieved in the traditional ?2-way Main task?
(60.17% Accuracy) roughly corresponds to the
performance of the average participating systems
(60.36%). In the ?Search? task (which consists in
finding all the sentences that entail a given H in
a given set of documents about a topic) the same
configuration achieved an F1 of 33.44%, rank-
ing 3rd out of eight participants (average score
29.17% F1). In the EVALITA 2009 RTE task,
EDITS ranked first with an overall 71.0% Accu-
racy. To promote the use of EDITS and ease ex-
perimentation, the complete models used to pro-
duce each submitted run can be downloaded with
the system. An improved model obtained with the
current release of EDITS, and trained over RTE-5
data (61.83% Accuracy on the ?2-way Main task?
test set), is also available upon download.
As regards application-oriented integrations,
EDITS has been successfully used as a core com-
ponent in a Restricted-Domain Question Answer-
ing system within the EU-Funded QALL-ME
Project10. Within this project, an entailment-based
approach to Relation Extraction has been defined
as the task of checking for the existence of en-
tailment relations between an input question (the
text in RTE parlance), and a set of textual realiza-
tions of domain-specific binary relations (the hy-
potheses in RTE parlance). In recognizing 14 re-
lations relevant in the CINEMA domain present in
a collection of spoken English requests, the system
10http://qallme.fbk.eu
achieved an F1 of 72.9%, allowing to return cor-
rect answers to 83% of 400 test questions (Negri
and Kouylekov, 2009).
6 Conclusion
We have presented the first open source package
for recognizing Textual Entailment. The system
offers a modular, flexible, and adaptable working
environment to experiment with the task. In addi-
tion, the availability of pre-defined system config-
urations, tested in the past Evaluation Campaigns,
represents a first contribution to set up a collabo-
rative environment, and promote advances in RTE
research. Current activities are focusing on the de-
velopment of a Graphical User Interface, to further
simplify the use of the system.
Acknowledgments
The research leading to these results has received
funding from the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der Grant Agreement n. 248531 (CoSyne project).
References
Prodromos Malakasiotis and Ion Androutsopoulos
2007. Learning Textual Entailment using SVMs and
String Similarity Measures. Proc. of the ACL ?07
Workshop on Textual Entailment and Paraphrasing.
Ido Dagan and 0ren Glickman 2004. Probabilistic
Textual Entailment: Generic Applied Modeling of
Language Variability. Proc. of the PASCAL Work-
shop on Learning Methods for Text Understanding
and Mining.
Kaizhong Zhang and Dennis Shasha 1990. Fast Al-
gorithm for the Unit Cost Editing Distance Between
Trees. Journal of Algorithms. vol.11.
Yashar Mehdad 2009. Automatic Cost Estimation for
Tree Edit Distance Using Particle Swarm Optimiza-
tion. Proc. of ACL-IJCNLP 2009.
Matteo Negri and Milen Kouylekov 2009. Question
Answering over Structured Data: an Entailment-
Based Approach to Question Analysis. Proc. of
RANLP-2009.
Elena Cabrio, Yashar Mehdad, Matteo Negri, Milen
Kouylekov, and Bernardo Magnini 2009. Rec-
ognizing Textual Entailment for Italian EDITS @
EVALITA 2009 Proc. of EVALITA 2009.
Yashar Mehdad, Matteo Negri, Elena Cabrio, Milen
Kouylekov, and Bernardo Magnini 2009. Recogniz-
ing Textual Entailment for English EDITS @ TAC
2009 To appear in Proceedings of TAC 2009.
47
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1336?1345,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Using Bilingual Parallel Corpora
for Cross-Lingual Textual Entailment
Yashar Mehdad
FBK - irst and Uni. of Trento
Povo (Trento), Italy
mehdad@fbk.eu
Matteo Negri
FBK - irst
Povo (Trento), Italy
negri@fbk.eu
Marcello Federico
FBK - irst
Povo (Trento), Italy
federico@fbk.eu
Abstract
This paper explores the use of bilingual par-
allel corpora as a source of lexical knowl-
edge for cross-lingual textual entailment. We
claim that, in spite of the inherent difficul-
ties of the task, phrase tables extracted from
parallel data allow to capture both lexical re-
lations between single words, and contextual
information useful for inference. We experi-
ment with a phrasal matching method in or-
der to: i) build a system portable across lan-
guages, and ii) evaluate the contribution of
lexical knowledge in isolation, without inter-
action with other inference mechanisms. Re-
sults achieved on an English-Spanish corpus
obtained from the RTE3 dataset support our
claim, with an overall accuracy above average
scores reported by RTE participants on mono-
lingual data. Finally, we show that using par-
allel corpora to extract paraphrase tables re-
veals their potential also in the monolingual
setting, improving the results achieved with
other sources of lexical knowledge.
1 Introduction
Cross-lingual Textual Entailment (CLTE) has been
proposed by (Mehdad et al, 2010) as an extension
of Textual Entailment (Dagan and Glickman, 2004)
that consists in deciding, given two texts T and H in
different languages, if the meaning of H can be in-
ferred from the meaning of T. The task is inherently
difficult, as it adds issues related to the multilingual
dimension to the complexity of semantic inference
at the textual level. For instance, the reliance of cur-
rent monolingual TE systems on lexical resources
(e.g. WordNet, VerbOcean, FrameNet) and deep
processing components (e.g. syntactic and semantic
parsers, co-reference resolution tools, temporal ex-
pressions recognizers and normalizers) has to con-
front, at the cross-lingual level, with the limited
availability of lexical/semantic resources covering
multiple languages, the limited coverage of the ex-
isting ones, and the burden of integrating language-
specific components into the same cross-lingual ar-
chitecture.
As a first step to overcome these problems,
(Mehdad et al, 2010) proposes a ?basic solution?,
that brings CLTE back to the monolingual scenario
by translating H into the language of T. Despite the
advantages in terms of modularity and portability of
the architecture, and the promising experimental re-
sults, this approach suffers from one main limitation
which motivates the investigation on alternative so-
lutions. Decoupling machine translation (MT) and
TE, in fact, ties CLTE performance to the availabil-
ity of MT components, and to the quality of the
translations. As a consequence, on one side trans-
lation errors propagate to the TE engine hampering
the entailment decision process. On the other side
such unpredictable errors reduce the possibility to
control the behaviour of the engine, and devise ad-
hoc solutions to specific entailment problems.
This paper investigates the idea, still unexplored,
of a tighter integration of MT and TE algorithms and
techniques. Our aim is to embed cross-lingual pro-
cessing techniques inside the TE recognition pro-
cess in order to avoid any dependency on external
MT components, and eventually gain full control of
the system?s behaviour. Along this direction, we
1336
start from the acquisition and use of lexical knowl-
edge, which represents the basic building block of
any TE system. Using the basic solution proposed
by (Mehdad et al, 2010) as a term of comparison,
we experiment with different sources of multilingual
lexical knowledge to address the following ques-
tions:
(1) What is the potential of the existing mul-
tilingual lexical resources to approach CLTE?
To answer this question we experiment with lex-
ical knowledge extracted from bilingual dictionar-
ies, and from a multilingual lexical database. Such
experiments show two main limitations of these re-
sources, namely: i) their limited coverage, and ii)
the difficulty to capture contextual information when
only associations between single words (or at most
named entities and multiword expressions) are used
to support inference.
(2) Does MT provide useful resources or tech-
niques to overcome the limitations of existing re-
sources? We envisage several directions in which
inputs from MT research may enable or improve
CLTE. As regards the resources, phrase and para-
phrase tables extracted from bilingual parallel cor-
pora can be exploited as an effective way to cap-
ture both lexical relations between single words, and
contextual information useful for inference. As re-
gards the algorithms, statistical models based on co-
occurrence observations, similar to those used inMT
to estimate translation probabilities, may contribute
to estimate entailment probabilities in CLTE. Focus-
ing on the resources direction, the main contribu-
tion of this paper is to show that the lexical knowl-
edge extracted from parallel corpora allows to sig-
nificantly improve the results achieved with other
multilingual resources.
(3) In the cross-lingual scenario, can we achieve
results comparable to those obtained in mono-
lingual TE? Our experiments show that, although
CLTE seems intrinsically more difficult, the results
obtained using phrase and paraphrase tables are bet-
ter than those achieved by average systems on mono-
lingual datasets. We argue that this is due to the
fact that parallel corpora are a rich source of cross-
lingual paraphrases with no equivalents in monolin-
gual TE.
(4) Can parallel corpora be useful also for mono-
lingual TE? To answer this question, we experiment
on monolingual RTE datasets using paraphrase ta-
bles extracted from bilingual parallel corpora. Our
results improve those achieved with the most widely
used resources in monolingual TE, namely Word-
Net, Verbocean, and Wikipedia.
The remainder of this paper is structured as fol-
lows. Section 2 shortly overviews the role of lexical
knowledge in textual entailment, highlighting a gap
between TE and CLTE in terms of available knowl-
edge sources. Sections 3 and 4 address the first three
questions, giving motivations for the use of bilingual
parallel corpora in CLTE, and showing the results of
our experiments. Section 5 addresses the last ques-
tion, reporting on our experiments with paraphrase
tables extracted from phrase tables on the monolin-
gual RTE datasets. Section 6 concludes the paper,
and outlines the directions of our future research.
2 Lexical resources for TE and CLTE
All current approaches to monolingual TE, ei-
ther syntactically oriented (Rus et al, 2005), or
applying logical inference (Tatu and Moldovan,
2005), or adopting transformation-based techniques
(Kouleykov and Magnini, 2005; Bar-Haim et al,
2008), incorporate different types of lexical knowl-
edge to support textual inference. Such information
ranges from i) lexical paraphrases (textual equiva-
lences between terms) to ii) lexical relations pre-
serving entailment between words, and iii) word-
level similarity/relatedness scores. WordNet, the
most widely used resource in TE, provides all the
three types of information. Synonymy relations
can be used to extract lexical paraphrases indicat-
ing that words from the text and the hypothesis en-
tail each other, thus being interchangeable. Hy-
pernymy/hyponymy chains can provide entailment-
preserving relations between concepts, indicating
that a word in the hypothesis can be replaced
by a word from the text. Paths between con-
cepts and glosses can be used to calculate simi-
larity/relatedness scores between single words, that
contribute to the computation of the overall similar-
ity between the text and the hypothesis.
Besides WordNet, the RTE literature documents
the use of a variety of lexical information sources
(Bentivogli et al, 2010; Dagan et al, 2009).
These include, just to mention the most popular
1337
ones, DIRT (Lin and Pantel, 2001), VerbOcean
(Chklovski and Pantel, 2004), FrameNet (Baker et
al., 1998), and Wikipedia (Mehdad et al, 2010;
Kouylekov et al, 2009). DIRT is a collection of sta-
tistically learned inference rules, that is often inte-
grated as a source of lexical paraphrases and entail-
ment rules. VerbOcean is a graph of fine-grained
semantic relations between verbs, which are fre-
quently used as a source of precise entailment rules
between predicates. FrameNet is a knowledge-base
of frames describing prototypical situations, and the
role of the participants they involve. It can be
used as an alternative source of entailment rules,
or to determine the semantic overlap between texts
and hypotheses. Wikipedia is often used to extract
probabilistic entailment rules based word similar-
ity/relatedness scores.
Despite the consensus on the usefulness of lexi-
cal knowledge for textual inference, determining the
actual impact of these resources is not straightfor-
ward, as they always represent one component in
complex architectures that may use them in differ-
ent ways. As emerges from the ablation tests re-
ported in (Bentivogli et al, 2010), even the most
common resources proved to have a positive impact
on some systems and a negative impact on others.
Some previous works (Bannard and Callison-Burch,
2005; Zhao et al, 2009; Kouylekov et al, 2009)
indicate, as main limitations of the mentioned re-
sources, their limited coverage, their low precision,
and the fact that they are mostly suitable to capture
relations mainly between single words.
Addressing CLTE we have to face additional and
more problematic issues related to: i) the stronger
need of lexical knowledge, and ii) the limited avail-
ability of multilingual lexical resources. As regards
the first issue, it?s worth noting that in the monolin-
gual scenario simple ?bag of words? (or ?bag of n-
grams?) approaches are per se sufficient to achieve
results above baseline. In contrast, their applica-
tion in the cross-lingual setting is not a viable so-
lution due to the impossibility to perform direct lex-
ical matches between texts and hypotheses in differ-
ent languages. This situation makes the availability
of multilingual lexical knowledge a necessary con-
dition to bridge the language gap. However, with
the only exceptions represented by WordNet and
Wikipedia, most of the aforementioned resources
are available only for English. Multilingual lexi-
cal databases aligned with the EnglishWordNet (e.g.
MultiWordNet (Pianta et al, 2002)) have been cre-
ated for several languages, with different degrees of
coverage. As an example, the 57,424 synsets of the
Spanish section of MultiWordNet algned to English
cover just around 50% of the WordNet?s synsets,
thus making the coverage issue even more problem-
atic than for TE. As regards Wikipedia, the cross-
lingual links between pages in different languages
offer a possibility to extract lexical knowledge use-
ful for CLTE. However, due to their relatively small
number (especially for some languages), bilingual
lexicons extracted from Wikipedia are still inade-
quate to provide acceptable coverage. In addition,
featuring a bias towards named entities, the infor-
mation acquired through cross-lingual links can at
most complement the lexical knowledge extracted
from more generic multilingual resources (e.g bilin-
gual dictionaries).
3 Using Parallel Corpora for CLTE
Bilingual parallel corpora represent a possible solu-
tion to overcome the inadequacy of the existing re-
sources, and to implement a portable approach for
CLTE. To this aim, we exploit parallel data to: i)
learn alignment criteria between phrasal elements
in different languages, ii) use them to automatically
extract lexical knowledge in the form of phrase ta-
bles, and iii) use the obtained phrase tables to create
monolingual paraphrase tables.
Given a cross-lingual T/H pair (with the text in
l1 and the hypothesis in l2), our approach leverages
the vast amount of lexical knowledge provided by
phrase and paraphrase tables to map H into T. We
perform such mapping with two different methods.
The first method uses a single phrase table to di-
rectly map phrases extracted from the hypothesis to
phrases in the text. In order to improve our system?s
generalization capabilities and increase the cover-
age, the second method combines the phrase table
with two monolingual paraphrase tables (one in l1,
and one in l2). This allows to:
1. use the paraphrase table in l2 to find para-
phrases of phrases extracted from H;
2. map them to entries in the phrase table, and ex-
tract their equivalents in l1;
1338
3. use the paraphrase table in l1 to find para-
phrases of the extracted fragments in l1;
4. map such paraphrases to phrases in T.
With the second method, phrasal matches between
the text and the hypothesis are indirectly performed
through paraphrases of the phrase table entries.
The final entailment decision for a T/H pair is as-
signed considering a model learned from the similar-
ity scores based on the identified phrasal matches.
In particular, ?YES? and ?NO? judgements are as-
signed considering the proportion of words in the
hypothesis that are found also in the text. This way
to approximate entailment reflects the intuition that,
as a directional relation between the text and the hy-
pothesis, the full content of H has to be found in T.
3.1 Extracting Phrase and Paraphrase Tables
Phrase tables (PHT) contain pairs of correspond-
ing phrases in two languages, together with associa-
tion probabilities. They are widely used in MT as a
way to figure out how to translate input in one lan-
guage into output in another language (Koehn et al,
2003). There are several methods to build phrase ta-
bles. The one adopted in this work consists in learn-
ing phrase alignments from a word-aligned bilingual
corpus. In order to build English-Spanish phrase ta-
bles for our experiments, we used the freely avail-
able Europarl V.4, News Commentary and United
Nations Spanish-English parallel corpora released
for the WMT101. We run TreeTagger (Schmid,
1994) for tokenization, and used the Giza++ (Och
and Ney, 2003) to align the tokenized corpora at
the word level. Subsequently, we extracted the bi-
lingual phrase table from the aligned corpora using
the Moses toolkit (Koehn et al, 2007). Since the re-
sulting phrase table was very large, we eliminated
all the entries with identical content in the two lan-
guages, and the ones containing phrases longer than
5 words in one of the two sides. In addition, in or-
der to experiment with different phrase tables pro-
viding different degrees of coverage and precision,
we extracted 7 phrase tables by pruning the initial
one on the direct phrase translation probabilities of
0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting
1http://www.statmt.org/wmt10/
phrase tables range from 76 to 48 million entries,
with an average of 3.9 words per phrase.
Paraphrase tables (PPHT) contain pairs of corre-
sponding phrases in the same language, possibly as-
sociated with probabilities. They proved to be use-
ful in a number of NLP applications such as natural
language generation (Iordanskaja et al, 1991), mul-
tidocument summarization (McKeown et al, 2002),
automatic evaluation of MT (Denkowski and Lavie,
2010), and TE (Dinu and Wang, 2009).
One of the proposed methods to extract para-
phrases relies on a pivot-based approach using
phrase alignments in a bilingual parallel corpus
(Bannard and Callison-Burch, 2005). With this
method, all the different phrases in one language that
are aligned with the same phrase in the other lan-
guage are extracted as paraphrases. After the extrac-
tion, pruning techniques (Snover et al, 2009) can
be applied to increase the precision of the extracted
paraphrases.
In our work we used available2 paraphrase
databases for English and Spanish which have been
extracted using the method previously outlined.
Moreover, in order to experiment with different
paraphrase sets providing different degrees of cov-
erage and precision, we pruned the main paraphrase
table based on the probabilities, associated to its en-
tries, of 0.1, 0.2 and 0.3. The number of phrase pairs
extracted varies from 6 million to about 80000, with
an average of 3.2 words per phrase.
3.2 Phrasal Matching Method
In order to maximize the usage of lexical knowledge,
our entailment decision criterion is based on similar-
ity scores calculated with a phrase-to-phrase match-
ing process.
A phrase in our approach is an n-gram composed
of up to 5 consecutive words, excluding punctua-
tion. Entailment decisions are estimated by com-
bining phrasal matching scores (Scoren) calculated
for each level of n-grams , which is the number
of 1-grams, 2-grams,..., 5-grams extracted from H
that match with n-grams in T. Phrasal matches are
performed either at the level of tokens, lemmas, or
stems, can be of two types:
2http://www.cs.cmu.edu/ alavie/METEOR
1339
1. Exact: in the case that two phrases are identical
at one of the three levels (token, lemma, stem);
2. Lexical: in the case that two different phrases
can be mapped through entries of the resources
used to bridge T and H (i.e. phrase tables, para-
phrases tables, dictionaries or any other source
of lexical knowledge).
For each phrase in H, we first search for exact
matches at the level of token with phrases in T. If
no match is found at a token level, the other levels
(lemma and stem) are attempted. Then, in case of
failure with exact matching, lexical matching is per-
formed at the same three levels. To reduce redun-
dant matches, the lexical matches between pairs of
phrases which have already been identified as exact
matches are not considered.
Once matching for each n-gram level has been
concluded, the number of matches (Mn) and the
number of phrases in the hypothesis (Nn) are used
to estimate the portion of phrases in H that are
matched at each level (n). The phrasal matching
score for each n-gram level is calculated as follows:
Scoren =
Mn
Nn
To combine the phrasal matching scores obtained
at each n-gram level, and optimize their relative
weights, we trained a Support Vector Machine clas-
sifier, SVMlight (Joachims, 1999), using each score
as a feature.
4 Experiments on CLTE
To address the first two questions outlined in Sec-
tion 1, we experimented with the phrase matching
method previously described, contrasting the effec-
tiveness of lexical information extracted from par-
allel corpora with the knowledge provided by other
resources used in the same way.
4.1 Dataset
The dataset used for our experiments is an English-
Spanish entailment corpus obtained from the orig-
inal RTE3 dataset by translating the English hy-
pothesis into Spanish. It consists of 1600 pairs
derived from the RTE3 development and test sets
(800+800). Translations have been generated by
the CrowdFlower3 channel to Amazon Mechanical
Turk4 (MTurk), adopting the methodology proposed
by (Negri and Mehdad, 2010). The method relies
on translation-validation cycles, defined as separate
jobs routed to MTurk?s workforce. Translation jobs
return one Spanish version for each hypothesis. Val-
idation jobs ask multiple workers to check the cor-
rectness of each translation using the original En-
glish sentence as reference. At each cycle, the trans-
lated hypothesis accepted by the majority of trust-
ful validators5 are stored in the CLTE corpus, while
wrong translations are sent back to workers in a
new translation job. Although the quality of the re-
sults is enhanced by the possibility to automatically
weed out untrusted workers using gold units, we per-
formed a manual quality check on a subset of the ac-
quired CLTE corpus. The validation, carried out by
a Spanish native speaker on 100 randomly selected
pairs after two translation-validation cycles, showed
the good quality of the collected material, with only
3 minor ?errors? consisting in controversial but sub-
stantially acceptable translations reflecting regional
Spanish variations.
The T-H pairs in the collected English-Spanish
entailment corpus were annotated using TreeTagger
(Schmid, 1994) and the Snowball stemmer6 with to-
ken, lemma, and stem information.
4.2 Knowledge sources
For comparison with the extracted phrase and para-
phrase tables, we use a large bilingual dictionary
and MultiWordNet as alternative sources of lexical
knowledge.
Bilingual dictionaries (DIC) allow for precise
mappings between words in H and T. To create
a large bilingual English-Spanish dictionary we
processed and combined the following dictionaries
and bilingual resources:
- XDXF Dictionaries7: 22,486 entries.
3http://crowdflower.com/
4https://www.mturk.com/mturk/
5Workers? trustworthiness can be automatically determined
by means of hidden gold units randomly inserted into jobs.
6http://snowball.tartarus.org/
7http://xdxf.revdanica.com/
1340
Figure 1: Accuracy on CLTE by pruning the phrase table
with different thresholds.
- Universal dictionary database8: 9,944 entries.
- Wiktionary database9: 5,866 entries.
- Omegawiki database10: 8,237 entries.
- Wikipedia interlanguage links11: 7,425 entries.
The resulting dictionary features 53,958 entries,
with an average length of 1.2 words.
MultiWordNet (MWN) allows to extract mappings
between English and Spanish words connected by
entailment-preserving semantic relations. The ex-
traction process is dataset-dependent, as it checks
for synonymy and hyponymy relations only between
terms found in the dataset. The resulting collection
of cross-lingual words associations contains 36,794
pairs of lemmas.
4.3 Results and Discussion
Our results are calculated over 800 test pairs of our
CLTE corpus, after training the SVM classifier over
800 development pairs. This section reports the
percentage of correct entailment assignments (accu-
racy), comparing the use of different sources of lex-
ical knowledge.
Initially, in order to find a reasonable trade-off be-
tween precision and coverage, we used the 7 phrase
tables extracted with different pruning thresholds
8http://www.dicts.info/
9http://en.wiktionary.org/
10http://www.omegawiki.org/
11http://www.wikipedia.org/
MWN DIC PHT PPHT Acc. ?
x 55.00 0.00
x 59.88 +4.88
x 62.62 +7.62
x x 62.88 +7.88
Table 1: Accuracy results on CLTE using different lexical
resources.
(see Section 3.1). Figure 1 shows that with the prun-
ing threshold set to 0.05, we obtain the highest re-
sult of 62.62% on the test set. The curve demon-
strates that, although with higher pruning thresholds
we retain more reliable phrase pairs, their smaller
number provides limited coverage leading to lower
results. In contrast, the large coverage obtained with
the pruning threshold set to 0.01 leads to a slight
performance decrease due to probably less precise
phrase pairs.
Once the threshold has been set, in order to
prove the effectiveness of information extracted
from bilingual corpora, we conducted a series of ex-
periments using the different resources mentioned in
Section 4.2.
As it can be observed in Table 1, the highest
results are achieved using the phrase table, both
alone and in combination with paraphrase tables
(62.62% and 62.88% respectively). These results
suggest that, with appropriate pruning thresholds,
the large number and the longer entries contained
in the phrase and paraphrase tables represent an ef-
fective way to: i) obtain high coverage, and ii) cap-
ture cross-lingual associations between multiple lex-
ical elements. This allows to overcome the bias to-
wards single words featured by dictionaries and lex-
ical databases.
As regards the other resources used for compari-
son, the results show that dictionaries substantially
outperform MWN. This can be explained by the
low coverage of MWN, whose entries also repre-
sent weaker semantic relations (preserving entail-
ment, but with a lower probability to be applied)
than the direct translations between terms contained
in the dictionary.
Overall, our results suggest that the lexical knowl-
edge extracted from parallel data can be successfully
used to approach the CLTE task.
1341
Dataset WN VO WIKI PPHT PPHT 0.1 PPHT 0.2 PPHT 0.3 AVG
RTE3 61.88 62.00 61.75 62.88 63.38 63.50 63.00 62.37
RTE5 62.17 61.67 60.00 61.33 62.50 62.67 62.33 61.41
RTE3-G 62.62 61.5 60.5 62.88 63.50 62.00 61.5 -
Table 2: Accuracy results on monolingual RTE using different lexical resources.
5 Using parallel corpora for TE
This section addresses the third and the fourth re-
search questions outlined in Section 1. Building
on the positive results achieved on the cross-lingual
scenario, we investigate the possibility to exploit
bilingual parallel corpora in the traditional monolin-
gual scenario. Using the same approach discussed
in Section 4, we compare the results achieved with
English paraphrase tables with those obtained with
other widely used monolingual knowledge resources
over two RTE datasets.
For the sake of completeness, we report in this
section also the results obtained adopting the ?basic
solution? proposed by (Mehdad et al, 2010). Al-
though it was presented as an approach to CLTE,
the proposed method brings the problem back to the
monolingual case by translating H into the language
of T. The comparison with this method aims at ver-
ifying the real potential of parallel corpora against
the use of a competitive MT system (Google Trans-
late) in the same scenario.
5.1 Dataset
We experiment with the original RTE3 and RTE5
datasets, annotated with token, lemma, and stem in-
formation using the TreeTagger and the Snowball
stemmer.
In addition to confront our method with the solu-
tion proposed by (Mehdad et al, 2010) we translated
the Spanish hypotheses of our CLTE dataset into En-
glish using Google Translate. The resulting dataset
was annotated in the same way.
5.2 Knowledge sources
We compared the results achieved with paraphrase
tables (extracted with different pruning thresh-
olds12) with those obtained using the three most
12We pruned the paraphrase table (PPHT), with probabilities
set to 0.1 (PPHT 0.1), 0.2 (PPHT 0.2), and 0.3 (PPHT 0.3)
widely used English resources for Textual Entail-
ment (Bentivogli et al, 2010), namely:
WordNet (WN). WordNet 3.0 has been used
to extract a set of 5396 pairs of words connected by
the hyponymy and synonymy relations.
VerbOcean (VO). VerbOcean has been used
to extract 18232 pairs of verbs connected by the
?stronger-than? relation (e.g. ?kill? stronger-than
?injure?).
Wikipedia (WIKI). We performed Latent Se-
mantic Analysis (LSA) over Wikipedia using the
jLSI tool (Giuliano, 2007) to measure the relat-
edness between words in the dataset. Then, we
filtered all the pairs with similarity lower than 0.7 as
proposed by (Kouylekov et al, 2009). In this way
we obtained 13760 word pairs.
5.3 Results and Discussion
Table 2 shows the accuracy results calculated over
the original RTE3 and RTE5 test sets, training our
classifier over the corresponding development sets.
The first two rows of the table show that pruned
paraphrase tables always outperform the other lexi-
cal resources used for comparison, with an accuracy
increase up to 3%. In particular, we observe that us-
ing 0.2 as a pruning threshold provides a good trade-
off between coverage and precision, leading to our
best results on both datasets (63.50% for RTE3, and
62.67% for RTE5). It?s worth noting that these re-
sults, compared with the average scores reported by
participants in the two editions of the RTE Challenge
(AVG column), represent an accuracy improvement
of more than 1%. Overall, these results confirm our
claim that increasing the coverage using context sen-
sitive phrase pairs obtained from large parallel cor-
pora, results in better performance not only in CLTE,
1342
but also in the monolingual scenario.
The comparison with the results achieved on
monolingual data obtained by automatically trans-
lating the Spanish hypotheses (RTE3-G row in Ta-
ble 2) leads to four main observations. First, we no-
tice that dealing with MT-derived inputs, the optimal
pruning threshold changes from 0.2 to 0.1, leading
to the highest accuracy of 63.50%. This suggests
that the noise introduced by incorrect translations
can be tackled by increasing the coverage of the
paraphrase table. Second, in line with the findings
of (Mehdad et al, 2010), the results obtained over
the MT-derived corpus are equal to those we achieve
over the original RTE3 dataset (i.e. 63.50%). Third,
the accuracy obtained over the CLTE corpus using
combined phrase and paraphrase tables (62.88%, as
reported in Table 1) is comparable to the best re-
sult gained over the automatically translated dataset
(63.50%). In all the other cases, the use of phrase
and paraphrase tables on CLTE data outperforms
the results achieved on the same data after transla-
tion. Finally, it?s worth remarking that applying our
phrase matching method on the translated dataset
without any additional source of knowledge would
result in an overall accuracy of 62.12%, which is
lower than the result obtained using only phrase ta-
bles on cross-lingual data (62.62%). This demon-
strates that phrase tables can successfully replace
MT systems in the CLTE task.
In light of this, we suggest that extracting lexi-
cal knowledge from parallel corpora is a preferable
solution to approach CLTE. One of the main rea-
sons is that placing a black-box MT system at the
front-end of the entailment process reduces the pos-
sibility to cope with wrong translations. Further-
more, the access to MT components is not easy (e.g.
Google Translate limits the number and the size of
queries, while open source MT tools cover few lan-
guage pairs). Moreover, the task of developing a
full-fledged MT system often requires the availabil-
ity of parallel corpora, and is much more complex
than extracting lexical knowledge from them.
6 Conclusion and Future Work
In this paper we approached the cross-lingual Tex-
tual Entailment task focusing on the role of lexi-
cal knowledge extracted from bilingual parallel cor-
pora. One of the main difficulties in CLTE raises
from the lack of adequate knowledge resources to
bridge the lexical gap between texts and hypothe-
ses in different languages. Our approach builds on
the intuition that the vast amount of knowledge that
can be extracted from parallel data (in the form of
phrase and paraphrase tables) offers a possible so-
lution to the problem. To check the validity of our
assumptions we carried out several experiments on
an English-Spanish corpus derived from the RTE3
dataset, using phrasal matches as a criterion to ap-
proximate entailment. Our results show that phrase
and paraphrase tables allow to: i) outperform the re-
sults achieved with the few multilingual lexical re-
sources available, and ii) reach performance levels
above the average scores obtained by participants in
the monolingual RTE3 challenge. These improve-
ments can be explained by the fact that the lexi-
cal knowledge extracted from parallel data provides
good coverage both at the level of single words, and
at the level of phrases.
As a further contribution, we explored the appli-
cation of paraphrase tables extracted from parallel
data in the traditional monolingual scenario. Con-
trasting results with those obtained with the most
widely used resources in TE, we demonstrated the
effectiveness of paraphrase tables as a mean to over-
come the bias towards single words featured by the
existing resources.
Our future work will address both the extraction
of lexical information from bilingual parallel cor-
pora, and its use for TE and CLTE. On one side,
we plan to explore alternative ways to build phrase
and paraphrase tables. One possible direction is to
consider linguistically motivated approaches, such
as the extraction of syntactic phrase tables as pro-
posed by (Yamada and Knight, 2001). Another in-
teresting direction is to investigate the potential of
paraphrase patterns (i.e. patterns including part-
of-speech slots), extracted from bilingual parallel
corpora with the method proposed by (Zhao et al,
2009). On the other side we will investigate more
sophisticated methods to exploit the acquired lexi-
cal knowledge. As a first step, the probability scores
assigned to phrasal entries will be considered to per-
form weighted phrase matching as an improved cri-
terion to approximate entailment.
1343
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853).
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. Proceedings
of COLING-ACL.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL 2005).
Roy Bar-haim , Jonathan Berant , Ido Dagan , Iddo
Greental , Shachar Mirkin , Eyal Shnarch , and Idan
Szpektor. 2008. Efficient semantic deduction and ap-
proximate matching over compact parse forests. Pro-
ceedings of the TAC 2008 Workshop on Textual Entail-
ment.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The Sixth
PASCAL Recognizing Textual Entailment Challenge.
Proceedings of the the Text Analysis Conference (TAC
2010).
Timothy Chklovski and Patrick Pantel. 2004. Verbocean:
Mining the web for fine-grained semantic verb rela-
tions. Proceedings of Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-04).
Ido Dagan and Oren Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. Proceedings of the PASCAL Workshop of
Learning Methods for Text Understanding and Min-
ing.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Journal of Natural Language
Engineering , Volume 15, Special Issue 04, pp i-xvii.
Michael Denkowski and Alon Lavie. 2010. Extending
the METEOR Machine Translation Evaluation Metric
to the Phrase Level. Proceedings of Human Language
Technologies (HLT-NAACL 2010).
Georgiana Dinu and Rui Wang. 2009. Inference Rules
and their Application to Recognizing Textual Entail-
ment. Proceedings of the 12th Conference of the Eu-
ropean Chapter of the ACL (EACL 2009).
Claudio Giuliano. 2007. jLSI a tool for la-
tent semantic indexing. Software avail-
able at http://tcc.itc.it/research/textec/tools-
resources/jLSI.html.
Lidija Iordanskaja, Richard Kittredge, and Alain Polg re..
1991. Lexical selection and paraphrase in a meaning
text generation model. Natural Language Generation
in Articial Intelligence and Computational Linguistics.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical.
Philipp Koehn, Franz Josef Och, and Daniel Marcu 2003.
Statistical Phrase-Based Translation. Proceedings of
HLT/NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. Proceed-
ings of the Conference of the Association for Compu-
tational Linguistics (ACL).
Milen Kouleykov and Bernardo Magnini. 2005. Tree
edit distance for textual entailment. Proceedings of
RALNP-2005, International Conference on Recent Ad-
vances in Natural Language Processing.
Milen Kouylekov, Yashar Mehdad, and Matteo Negri.
2010. Mining Wikipedia for Large-Scale Repositories
of Context-Sensitive Entailment Rules. Proceedings
of the Language Resources and Evaluation Conference
(LREC 2010).
Yashar Mehdad, Alessandro Moschitti and Fabio Mas-
simo Zanzotto. 2010. Syntactic/semantic structures
for textual entailment recognition. Proceedings of the
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT 2010).
Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery
of Inference Rules from Text.. Proceedings of ACM
Conference on Knowledge Discovery and Data Mining
(KDD-01).
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news on
a daily basis with Columbias Newsblaster. Proceed-
ings of the Human Language Technology Conference..
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment.
Proceedings of the 11th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL HLT 2010).
Dan Moldovan and Adrian Novischi. 2002. Lexical
chains for question answering. Proceedings of COL-
ING.
Matteo Negri and Yashar Mehdad. 2010. Creating a Bi-
lingual Entailment Corpus through Translations with
Mechanical Turk: $100 for a 10-day Rush. Proceed-
ings of the NAACL 2010 Workshop on Creating Speech
and Language Data With Amazons Mechanical Turk .
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):1951.
1344
Emanuele Pianta, Luisa Bentivogli, and Christian Gi-
rardi. 2002. MultiWordNet: Developing and Aligned
Multilingual Database. Proceedings of the First Inter-
national Conference on Global WordNet.
Vasile Rus, Art Graesser, and Kirtan Desai 2005.
Lexico-Syntactic Subsumption for Textual Entailment.
Proceedings of RANLP 2005.
Helmut Schmid 2005. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. Proceedings of the In-
ternational Conference on New Methods in Language
Processing.
Marta Tatu andDan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. Proceed-
ings of the Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP 2005).
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy, or
HTER? Exploring Different Human Judgments with
a Tunable MT Metric. Proceedings of WMT09.
Rui Wang and Yi Zhang,. 2009. Recognizing Tex-
tual Relatedness with Predicate-Argument Structures.
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP 2009).
Kenji Yamada and Kevin Knight 2001. A Syntax-Based
Statistical Translation Model. Proceedings of the Con-
ference of the Association for Computational Linguis-
tics (ACL).
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2009. Extracting Paraphrase Patterns from Bilingual
Parallel Corpora. Journal of Natural Language Engi-
neering , Volume 15, Special Issue 04, pp 503-526.
1345
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 120?124,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Detecting Semantic Equivalence and Information Disparity
in Cross-lingual Documents
Yashar Mehdad Matteo Negri Marcello Federico
Fondazione Bruno Kessler, FBK-irst
Trento , Italy
{mehdad|negri|federico}@fbk.eu
Abstract
We address a core aspect of the multilingual
content synchronization task: the identifica-
tion of novel, more informative or semanti-
cally equivalent pieces of information in two
documents about the same topic. This can be
seen as an application-oriented variant of tex-
tual entailment recognition where: i) T and
H are in different languages, and ii) entail-
ment relations between T and H have to be
checked in both directions. Using a combi-
nation of lexical, syntactic, and semantic fea-
tures to train a cross-lingual textual entailment
system, we report promising results on differ-
ent datasets.
1 Introduction
Given two documents about the same topic writ-
ten in different languages (e.g. Wiki pages), con-
tent synchronization deals with the problem of au-
tomatically detecting and resolving differences in
the information they provide, in order to produce
aligned, mutually enriched versions. A roadmap to-
wards the solution of this problem has to take into
account, among the many sub-tasks, the identifica-
tion of information in one page that is semantically
equivalent, novel, or more informative with respect
to the content of the other page. In this paper we
set such problem as an application-oriented, cross-
lingual variant of the Textual Entailment (TE) recog-
nition task (Dagan and Glickman, 2004). Along this
direction, we make two main contributions:
(a) Experiments with multi-directional cross-
lingual textual entailment. So far, cross-lingual
textual entailment (CLTE) has been only applied
to: i) available TE datasets (uni-directional rela-
tions between monolingual pairs) transformed into
their cross-lingual counterpart by translating the hy-
potheses into other languages (Negri and Mehdad,
2010), and ii) machine translation (MT) evaluation
datasets (Mehdad et al, 2012). Instead, we ex-
periment with the only corpus representative of the
multilingual content synchronization scenario, and
the richer inventory of phenomena arising from it
(multi-directional entailment relations).
(b) Improvement of current CLTE methods. The
CLTE methods proposed so far adopt either a ?piv-
oting approach? based on the translation of the two
input texts into the same language (Mehdad et al,
2010), or an ?integrated solution? that exploits bilin-
gual phrase tables to capture lexical relations and
contextual information (Mehdad et al, 2011). The
promising results achieved with the integrated ap-
proach, however, still rely on phrasal matching tech-
niques that disregard relevant semantic aspects of
the problem. By filling this gap integrating linguis-
tically motivated features, we propose a novel ap-
proach that improves the state-of-the-art in CLTE.
2 CLTE-based content synchronization
CLTE has been proposed by (Mehdad et al, 2010) as
an extension of textual entailment which consists of
deciding, given a text T and an hypothesis H in dif-
ferent languages, if the meaning of H can be inferred
from the meaning of T. The adoption of entailment-
based techniques to address content synchronization
looks promising, as several issues inherent to such
task can be formalized as entailment-related prob-
120
lems. Given two pages (P1 and P2), these issues
include identifying, and properly managing:
(1) Text portions in P1 and P2 that express the same
meaning (bi-directional entailment). In such cases
no information has to migrate across P1 and P2, and
the two text portions will remain the same;
(2) Text portions in P1 that are more informa-
tive than portions in P2 (forward entailment). In
such cases, the entailing (more informative) portions
from P1 have to be translated and migrated to P2 in
order to replace or complement the entailed (less in-
formative) fragments;
(3) Text portions in P2 that are more informa-
tive than portions in P1 (backward entailment), and
should be translated to replace or complement them;
(4) Text portions in P1 describing facts that are not
present in P2, and vice-versa (the ?unknown? cases
in RTE parlance). In such cases, the novel infor-
mation from both sides has to be translated and mi-
grated in order to mutually enrich the two pages;
(5) Meaning discrepancies between text portions in
the two pages (?contradictions? in RTE parlance).
CLTE has been previously modeled as a phrase
matching problem that exploits dictionaries and
phrase tables extracted from bilingual parallel cor-
pora to determine the number of word sequences in
H that can be mapped to word sequences in T. In
this way a semantic judgement about entailment is
made exclusively on the basis of lexical evidence.
When only unidirectional entailment relations from
T to H have to be determined (RTE-like setting), the
full mapping of the hypothesis into the text usually
provides enough evidence for a positive entailment
judgement. Unfortunately, when dealing with multi-
directional entailment, the correlation between the
proportion of matching terms and the correct entail-
ment decisions is less strong. In such framework, for
instance, the full mapping of the hypothesis into the
text is per se not sufficient to discriminate between
forward entailment and semantic equivalence. To
cope with these issues, we explore the contribution
of syntactic and semantic features as a complement
to lexical ones in a supervised learning framework.
3 Beyond lexical CLTE
In order to enrich the feature space beyond pure lex-
ical match through phrase table entries, our model
builds on two additional feature sets, derived from i)
semantic phrase tables, and ii) dependency relations.
Semantic Phrase Table (SPT) matching repre-
sents a novel way to leverage the integration of se-
mantics and MT-derived techniques. SPT matching
extends CLTE methods based on pure lexical match
by means of ?generalized? phrase tables annotated
with shallow semantic labels. SPTs, with entries in
the form ?[LABEL] word1...wordn [LABEL]?, are
used as a recall-oriented complement to the phrase
tables used in MT. A motivation for this augmenta-
tion is that semantic tags allow to match tokens that
do not occur in the original bilingual parallel cor-
pora used for phrase table extraction. Our hypothe-
sis is that the increase in recall obtained from relaxed
matches through semantic tags in place of ?out of
vocabulary? terms (e.g. unseen person names) is an
effective way to improve CLTE performance, even
at the cost of some loss in precision.
Like lexical phrase tables, SPTs are extracted
from parallel corpora. As a first step we annotate
the parallel corpora with named-entity taggers for
the source and target languages, replacing named
entities with general semantic labels chosen from
a coarse-grained taxonomy (person, location, orga-
nization, date and numeric expression). Then, we
combine the sequences of unique labels into one sin-
gle token of the same label, and we run Giza++ (Och
and Ney, 2000) to align the resulting semantically
augmented corpora. Finally, we extract the seman-
tic phrase table from the augmented aligned corpora
using the Moses toolkit (Koehn et al, 2007). For
the matching phase, we first annotate T and H in the
same way we labeled our parallel corpora. Then, for
each n-gram order (n=1 to 5) we use the SPT to cal-
culate a matching score as the number of n-grams in
H that match with phrases in T divided by the num-
ber of n-grams in H.1
Dependency Relation (DR) matching targets the
increase of CLTE precision. Adding syntactic con-
straints to the matching process, DR features aim to
reduce the amount of wrong matches often occur-
ring with bag-of-words methods (both at the lexi-
cal level and with recall-oriented SPTs). For in-
stance, the contradiction between ?Yahoo acquired
1When checking for entailment from H to T, the normaliza-
tion is carried out dividing by the number of n-grams in T.
121
Overture? and ?Overture compro? Yahoo?, which is
evident when syntax is taken into account, can not
be caught by shallow methods. We define a de-
pendency relation as a triple that connects pairs of
words through a grammatical relation. DR matching
captures similarities between dependency relations,
combining the syntactic and lexical level. In a valid
match, while the relation has to be the same, the con-
nected words can be either the same, or semantically
equivalent terms in the two languages (e.g. accord-
ing to a bilingual dictionary). Given the dependency
tree representations of T and H, for each grammati-
cal relation (r) we calculate a DR matching score as
the number of matching occurrences of r in T and
H, divided by the number of occurrences of r in H.
Separate DR matching scores are calculated for each
relation r appearing both in T and H.
4 Experiments and results
4.1 Content synchronization scenario
In our first experiment we used the English-German
portion of the CLTE corpus described in (Negri et
al., 2011), consisting of 500 multi-directional entail-
ment pairs which we equally divided into training
and test sets. Each pair in the dataset is annotated
with ?Bidirectional?, ?Forward?, or ?Backward? en-
tailment judgements. Although highly relevant for
the content synchronization task, ?Contradiction?
and ?Unknown? cases (i.e. ?NO? entailment in both
directions) are not present in the annotation. How-
ever, this is the only available dataset suitable to
gather insights about the viability of our approach to
multi-directional CLTE recognition.2 We chose the
ENG-GER portion of the dataset since for such lan-
guage pair MT systems performance is often lower,
making the adoption of simpler solutions based on
pivoting more vulnerable.
To build the English-German phrase tables we
combined the Europarl, News Commentary and ?de-
news?3 parallel corpora. After tokenization, Giza++
and Moses were respectively used to align the cor-
pora and extract a lexical phrase table (PT). Simi-
larly, the semantic phrase table (SPT) has been ex-
2Recently, a new dataset including ?Unknown? pairs has
been used in the ?Cross-Lingual Textual Entailment for Content
Synchronization? task at SemEval-2012 (Negri et al, 2012).
3http://homepages.inf.ed.ac.uk/pkoehn/
tracted from the same corpora annotated with the
Stanford NE tagger (Faruqui and Pado?, 2010; Finkel
et al, 2005). Dependency relations (DR) have been
extracted running the Stanford parser (Rafferty and
Manning, 2008; De Marneffe et al, 2006). The dic-
tionary created during the alignment of the parallel
corpora provided the lexical knowledge to perform
matches when the connected words are different, but
semantically equivalent in the two languages. To
combine and weight features at different levels we
used SVMlight (Joachims, 1999) with default pa-
rameters.
In order to experiment under testing conditions
of increasing complexity, we set the CLTE problem
both as a two-way and as a three-way classification
task. Two-way classification casts multi-directional
entailment as a unidirectional problem, where each
pair is analyzed checking for entailment both from
left to right and from right to left. In this condi-
tion, each original test example is correctly clas-
sified if both pairs originated from it are correctly
judged (?YES-YES? for bidirectional, ?YES-NO?
for forward, and ?NO-YES? for backward entail-
ment). Two-way classification represents an intu-
itive solution to capture multidirectional entailment
relations but, at the same time, a suboptimal ap-
proach in terms of efficiency since two checks are
performed for each pair. Three-way classification is
more efficient, but at the same time more challeng-
ing due to the higher difficulty of multiclass learn-
ing, especially with small datasets.
Results are compared with two pivoting ap-
proaches, checking for entailment between the orig-
inal English texts and the translated German hy-
potheses.4 The first (Pivot-EDITS), uses an op-
timized distance-based model implemented in the
open source RTE system EDITS (Kouylekov and
Negri, 2010; Kouylekov et al, 2011). The second
(Pivot-PPT) exploits paraphrase tables for phrase
matching, and represents the best monolingual
model presented in (Mehdad et al, 2011). Table
1 demonstrates the success of our results in prov-
ing the two main claims of this paper. (a) In both
settings all the feature sets used outperform the ap-
proaches taken as terms of comparison. The 61.6%
accuracy achieved in the most challenging setting
4Using Google Translate.
122
PT PT+DR PT+SPT PT+SPT+DR Pivot-EDITS Pivot-PPT
Cont. Synch. (2-way) 57.8 58.6 62.4 63.3 27.4 57.0
Cont. Synch. (3-way) 57.4 57.8 58.7 61.6 25.3 56.1
RTE-3 AVG Pivot PPT
RTE3-derived 62.6 63.6 63.5 64.5 62.4 63.5
Table 1: CLTE accuracy results over content synchronization and RTE3-derived datasets.
(3-way) demonstrates the effectiveness of our ap-
proach to capture meaning equivalence and informa-
tion disparity in cross-lingual texts.
(b) In both settings the combination of lexical, syn-
tactic and semantic features (PT+SPT+DR) signif-
icantly improves5 the state-of-the-art CLTE model
(PT). Such improvement is motivated by the joint
contribution of SPTs (matching more and longer n-
grams, with a consequent recall improvement), and
DR matching (adding constraints, with a consequent
gain in precision). However, the performance in-
crease brought by DR features over PT is mini-
mal. This might be due to the fact that both PT and
DR features are precision-oriented, and their effec-
tiveness becomes evident only in combination with
recall-oriented features (SPT).
Cross-lingual models also significantly outper-
form pivoting methods. This suggests that the noise
introduced by incorrect translations makes the pivot-
ing approach less attractive in comparison with the
more robust cross-lingual models.
4.2 RTE-like CLTE scenario
Our second experiment aims at verifying the effec-
tiveness of the improved model over RTE-derived
CLTE data. To this aim, we compare the results ob-
tained by the new CLTE model with those reported
in (Mehdad et al, 2011), calculated over an English-
Spanish entailment corpus derived from the RTE-3
dataset (Negri and Mehdad, 2010).
In order to build the English-Spanish lexical
phrase table (PT), we used the Europarl, News Com-
mentary and United Nations parallel corpora. The
semantic phrase table (SPT) was extracted from the
same corpora annotated with FreeLing (Carreras et
al., 2004). Dependency relations (DR) have been ex-
tracted parsing English texts and Spanish hypotheses
with DepPattern (Gamallo and Gonzalez, 2011).
5p < 0.05, calculated using the approximate randomization
test implemented in (Pado?, 2006).
Accuracy results have been calculated over 800
test pairs of the CLTE corpus, after training the SVM
binary classifier over the 800 development pairs.
Our new features have been compared with: i) the
state-of-the-art CLTE model (PT), ii) the best mono-
lingual model (Pivot-PPT) presented in (Mehdad et
al., 2011), and iii) the average result achieved by
participants in the monolingual English RTE-3 eval-
uation campaign (RTE-3 AVG). As shown in Ta-
ble 1, the combined feature set (PT+SPT+DR) sig-
nificantly5 outperforms the lexical model (64.5%
vs 62.6%), while SPT and DR features separately
added to PT (PT+SPT, and PT+DR) lead to marginal
improvements over the results achieved by the PT
model alone (about 1%). This confirms the con-
clusions drawn from the previous experiment, that
precision-oriented and recall-oriented features lead
to a larger improvement when they are used in com-
bination.
5 Conclusion
We addressed the identification of semantic equiv-
alence and information disparity in two documents
about the same topic, written in different languages.
This is a core aspect of the multilingual content syn-
chronization task, which represents a challenging
application scenario for a variety of NLP technolo-
gies, and a shared research framework for the inte-
gration of semantics and MT technology. Casting
the problem as a CLTE task, we extended previous
lexical models with syntactic and semantic features.
Our results in different cross-lingual settings prove
the feasibility of the approach, with significant state-
of-the-art improvements also on RTE-derived data.
Acknowledgments
This work has been partially supported by the EU-
funded project CoSyne (FP7-ICT-4-248531).
123
References
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
FreeLing: An Open-Source Suite of Language Ana-
lyzers. In Proceedings of the 4th Language Resources
and Evaluation Conference (LREC 2004), volume 4.
I. Dagan and O. Glickman. 2004. Probabilistic Textual
Entailment: Generic Applied Modeling of Language
Variability. In Proceedings of the PASCAL Workshop
of Learning Methods for Text Understanding and Min-
ing.
M.C. De Marneffe, B. MacCartney, and C.D. Man-
ning. 2006. Generating Typed Dependency Parses
from Phrase Structure Parses. In Proceedings of the
5th Language Resources and Evaluation Conference
(LREC 2006), volume 6, pages 449?454.
M. Faruqui and S. Pado?. 2010. Training and Evaluat-
ing a German Named Entity Recognizer with Seman-
tic Generalization. In Proceedings of the 10th Con-
ference on Natural Language Processing (KONVENS
2010), Saarbru?cken, Germany.
J.R. Finkel, T. Grenager, and C. Manning. 2005. Incor-
porating Non-local Information into Information Ex-
traction Systems by Gibbs Sampling. In Proceedings
of the 43rd Annual Meeting on Association for Com-
putational Linguistics (ACL 2005).
P. Gamallo and I. Gonzalez. 2011. A grammatical for-
malism based on patterns of part of speech tags. Inter-
national Journal of Corpus Linguistics, 16(1):45?71.
T. Joachims. 1999. Advances in kernel methods. chap-
ter Making large-scale support vector machine learn-
ing practical, pages 169?184. MIT Press, Cambridge,
MA, USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings of
the 45th Annual Meeting on Association for Computa-
tional Linguistics, Demonstration Session (ACL 2007).
M. Kouylekov and M. Negri. 2010. An Open-Source
Package for Recognizing Textual Entailment. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, system demonstrations
(ACL 2010).
M. Kouylekov, Y. Mehdad, and M. Negri. 2011. Is it
Worth Submitting this Run? Assess your RTE Sys-
tem with a Good Sparring Partner. Proceedings of the
EMNLP TextInfer 2011 Workshop on Textual Entail-
ment.
Y. Mehdad, M. Negri, and M. Federico. 2010. Towards
Cross-Lingual Textual Entailment. In Proceedings of
the 11th Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL HLT 2010).
Y. Mehdad, M. Negri, and M. Federico. 2011. Using
Bilingual Parallel Corpora for Cross-Lingual Textual
Entailment. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies (ACL HLT 2011).
Y. Mehdad, M. Negri, and M. Federico. 2012. Match
without a Referee: Evaluating MT Adequacy without
Reference Translations. In Proceedings of the Ma-
chine Translation Workshop (WMT2012).
M. Negri and Y. Mehdad. 2010. Creating a Bi-lingual
Entailment Corpus through Translations with Mechan-
ical Turk: $100 for a 10-day Rush. In Proceedings of
the NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazons? Mechanical Turk.
M. Negri, L. Bentivogli, Y. Mehdad, D. Giampiccolo, and
A. Marchetti. 2011. Divide and Conquer: Crowd-
sourcing the Creation of Cross-Lingual Textual Entail-
ment Corpora. Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2011).
M. Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, and
D. Giampiccolo. 2012. Semeval-2012 Task 8: Cross-
lingual Textual Entailment for Content Synchroniza-
tion. In Proceedings of the 6th International Workshop
on Semantic Evaluation (SemEval 2012).
F.J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proceedings of the 38th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2000).
S. Pado?, 2006. User?s guide to sigf: Significance test-
ing by approximate randomisation.
A.N. Rafferty and C.D. Manning. 2008. Parsing Three
German Treebanks: Lexicalized and Unlexicalized
Baselines. In In Proceedings of the ACL 2008 Work-
shop on Parsing German.
124
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 771?776,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Exploiting Qualitative Information from Automatic Word Alignment
for Cross-lingual NLP Tasks
Jose? G.C. de Souza
FBK-irst,
University of Trento
Trento, Italy
desouza@fbk.eu
Miquel Espla`-Gomis
Universitat d?Alacant
Alacant, Spain
mespla@dlsi.ua.es
Marco Turchi
FBK-irst
Trento, Italy
turchi@fbk.eu
Matteo Negri
FBK-irst
Trento, Italy
negri@fbk.eu
Abstract
The use of automatic word alignment to
capture sentence-level semantic relations
is common to a number of cross-lingual
NLP applications. Despite its proved
usefulness, however, word alignment in-
formation is typically considered from a
quantitative point of view (e.g. the number
of alignments), disregarding qualitative
aspects (the importance of aligned terms).
In this paper we demonstrate that integrat-
ing qualitative information can bring sig-
nificant performance improvements with
negligible impact on system complexity.
Focusing on the cross-lingual textual en-
tailment task, we contribute with a novel
method that: i) significantly outperforms
the state of the art, and ii) is portable, with
limited loss in performance, to language
pairs where training data are not available.
1 Introduction
Meaning representation, comparison and projec-
tion across sentences are major challenges for a
variety of cross-lingual applications. So far, de-
spite the relevance of the problem, research on
multilingual applications has either circumvented
the issue, or proposed partial solutions.
When possible, the typical approach builds on
the reduction to a monolingual task, burdening the
process with dependencies from machine transla-
tion (MT) components. For instance, in cross-
lingual question answering and cross-lingual tex-
tual entailment (CLTE), intermediate MT steps
are respectively performed to ease answer re-
trieval/presentation (Parton, 2012; Tanev et al,
2006) and semantic inference (Mehdad et al,
2010). Direct solutions that avoid such pivot-
ing strategies typically exploit similarity measures
that rely on bag-of-words representations. As an
example, most supervised approaches to MT qual-
ity estimation (Blatz et al, 2003; Callison-Burch
et al, 2012) and CLTE (Wa?schle and Fendrich,
2012) include features that consider the amount of
equivalent terms that are found in the input sen-
tence pairs. Such simplification, however, disre-
gards the fact that semantic equivalence is not only
proportional to the number of equivalent terms,
but also to their importance. In other words, in-
stead of checking what of a given sentence can be
found in the other, current approaches limit the
analysis to the amount of lexical elements they
share, under the rough assumption that the more
the better.
In this paper we argue that:
(1) Considering qualitative aspects of word align-
ments to identify sentence-level semantic relations
can bring significant performance improvements
in cross-lingual NLP tasks.
(2) Shallow linguistic processing techniques (of-
ten a constraint in real cross-lingual scenarios due
to limited resources availability) can be leveraged
to set up portable solutions that still outperform
current bag-of-words methods.
To support our claims we experiment with the
CLTE task, which allows us to perform exhaus-
tive comparative experiments due to the availabil-
ity of comparable benchmarks for different lan-
guage pairs. In the remainder of the paper, we:
(1) Prove the effectiveness of our method over
datasets for four language combinations;
(2) Assess the portability of our models across lan-
guages in different testing conditions.
2 Objectives and Method
We propose a supervised learning approach for
identifying and classifying semantic relations be-
tween two sentences T1 and T2 written in different
languages. Beyond semantic equivalence, which
is relevant to applications such as MT quality es-
771
(a) (c)(b)
Word alignment 
model for L1-L2
Parallel data 
for L1-L2
Unlabeled 
CLTE data 
for L1-L2
Word alignment 
algorithm
CLTE 
annotation 
Learning 
algorithm
CLTE model 
for L1-L2
Labeled 
CLTE data 
for L1-L2
Word alignment 
model for L3-L4
Parallel data 
for L3-L4
Unlabeled 
CLTE data 
for L3-L4
Word alignment 
algorithm
CLTE 
annotation 
CLTE model 
for L1-L2
Word alignment 
model for L3-L4
Parallel data 
for L3-L4
Unlabeled 
CLTE data 
for L3-L4
Word alignment 
algorithm
CLTE 
annotation 
CLTE model 
for L1-L2
CLTE model 
for L5-L6
CLTE model 
for L7-L8
Combination
Figure 1: System architecture in different training/evaluation conditions. (a): parallel data and CLTE
labeled data are available for language pair L1-L2. (b): the L1-L2 CLTE model is used to cope with the
unavailability of labeled data for L3-L4. (c): the same problem is tackled by combining multiple models.
timation (Mehdad et al, 2012b),1 we aim to cap-
ture a richer set of relations potentially relevant to
other tasks. For instance, recognizing unrelated-
ness, forward and backward entailment relations,
represents a core problem in cross-lingual docu-
ment summarization (Lenci et al, 2002) and con-
tent synchronization (Monz et al, 2011; Mehdad
et al, 2012a). CLTE, as proposed within the Se-
mEval evaluation exercises (Negri et al, 2012;
Negri et al, 2013), represents an ideal framework
to evaluate such capabilities. Within this frame-
work, our goal is to automatically identify the fol-
lowing entailment relations between T1 and T2:
forward (T1 ? T2), backward (T1 ? T2), bidi-
rectional (T1 ? T2) and no entailment.
Our approach (see Figure 1) involves two core
components: i) a word alignment model, and ii) a
CLTE classifier. The former is trained on a par-
allel corpus, and associates equivalent terms in T1
and T2. The information about word alignments
is used to extract quantitative (amount and dis-
tribution of the alignments) and qualitative fea-
tures (importance of the aligned terms) to train the
CLTE classifier. Although in principle both com-
ponents need training data (respectively a paral-
lel corpus and labeled CLTE data), our goal is to
develop a method that is also portable across lan-
guages. To this aim, while the parallel corpus is
necessary to train the word aligner for any lan-
guage pair we want to deal with, the CLTE clas-
1A translation has to be semantically equivalent to the
source sentence.
sifier can be designed to learn from features that
capture language independent knowledge.2 This
allows us to experiment in different testing con-
ditions, namely: i) when CLTE training data are
available for a given language pair (Figure 1a),
and ii) when CLTE training data are missing, and
a model trained on other language pairs has to be
reused (Figure 1b-c).
Features. Considering word alignment informa-
tion, we extract three different groups of features:
AL, POS, and IDF.
The AL group provides quantitative informa-
tion about the aligned/unaligned words in each
sentence T? of the pair. These features are:
1. proportion of aligned words in T?. We use
this indicator as our baseline (B henceforth);
2. number of sequences of unaligned words,
normalized by the length of T?;
3. length of the longest a) sequence of aligned
words, and b) sequence of unaligned words,
both normalized by the length of T?;
4. average length of a) the aligned word se-
quences, and b) the unaligned word se-
quences;
5. position of a) the first unaligned word, and
b) the last unaligned word, both normalized
by the lenght of T?;
6. proportion of word n-grams in T? contain-
ing only aligned words (the feature was com-
2For instance, the fact that aligning all nouns and the most
relevant terms in T1 and T2 is a good indicator of semantic
equivalence.
772
puted separately for values of n = 1 . . . 5).
The POS group considers the part of speech
(PoS) of the words in T? as a source of qualitative
information about their importance. To compute
these features we use the TreeTagger (Schmid,
1995), manually mapping the fine-grained set of
assigned PoS labels into a more general set of tags
(P ) based on the universal PoS tag set by Petrov
et al (2012). POS features differentiate between
aligned words (words in T1 that are aligned to one
or more words in T2) and alignments (the edges
connecting words in T1 and T2). Features consid-
ering the aligned words in T? are:
7. for each PoS tag p ? P , proportion of aligned
words in T? tagged with p;
8. proportion of words in T1 aligned with words
with the same PoS tag in T2 (and vice-versa);
9. for each PoS tag p ? P , proportion of words
in T1 tagged as p which are aligned to words
with the same tag in T2 (and vice-versa).
Features considering the alignments are:
10. proportion of alignments connecting words
with the same PoS tag p;
11. for each PoS tag p ? P , proportion of align-
ments connecting two words tagged as p.
IDF, the last feature, uses the inverse docu-
ment frequency (Salton and Buckley, 1988) as an-
other source of qualitative information under the
assumption that rare words (and, therefore, with
higher IDF) are more informative:
12. summation of all the IDF scores of the
aligned words in T? over the summation of
the IDF scores of all words in T?.
3 Experiments
Our experiments cover two different scenarios.
First, the typical one, in which the CLTE model
is trained on labeled data for the same pair of lan-
guages L1?L2 of the test set. Then, simulating
the less favorable situation in which labeled train-
ing data for L1?L2 are missing, we investigate the
possibility to use existing CLTE models trained on
labeled data for a different language pair L3?L4.
The SemEval 2012 CLTE datasets used in our
experiments are available for four language pairs:
Es?En, De?En, Fr?En, and It?En. Each dataset
was created with the crowdsourcing-based method
described in Negri et al (2011), and consists of
1000 T1?T2 pairs (500 for training, 500 for test).
To train the word alignment models we used
the Europarl parallel corpus (Koehn, 2005), con-
catenated with the News Commentary corpus3
for three language pairs: De?En (2,079,049
sentences), Es?En (2,123,036 sentences), Fr?En
(2,144,820 sentences). For It?En we only used
the parallel data available in Europarl (1,909,115
sentences) since this language pair is not covered
by the News Commentary corpus. IDF values for
the words in each language were calculated on the
monolingual part of these corpora, using the aver-
age IDF value of each language for unseen terms.
To build the word alignment models we used the
MGIZA++ package (Gao and Vogel, 2008). Ex-
periments have been carried out with the hidden
Markov model (HMM) (Vogel et al, 1996) and
IBM models 3 and 4 (Brown et al, 1993).4 We also
explored three symmetrization techniques (Koehn
et al, 2005): union, intersection, and grow-diag-
final-and. A greedy feature selection process on
training data, with different combinations of word
alignment models and symmetrization methods,
indicated HMM/intersection as the best perform-
ing combination. For this reason, all our experi-
ments use this setting.
The SVM implementation of Weka (Hall et
al., 2009) was used to build the CLTE model.5
Two binary classifiers were trained to separately
check T1 ? T2 and T1 ? T2, merging
their output to obtain the 4-class judgments (e.g.
yes/yes=bidirectional, yes/no=forward).
3.1 Evaluation with CLTE training data
Figure 2 shows the accuracy obtained by the dif-
ferent feature groups.6 For the sake of compari-
son, state-of-the-art results achieved for each lan-
guage combination at SemEval 2012 are also re-
ported. As regards Es?En (63.2% accuracy) and
De?En (55.8%), the top scores were obtained by
the system described in (Wa?schle and Fendrich,
2012), where a combination of binary classifiers
for each entailment direction is trained with a mix-
3http://www.statmt.org/wmt11/
translation-task.html#download
4Five iterations of HMM, and three iterations of IBM
models 3 and 4 have been performed on the training corpora.
5The polynomial kernel was used with parameters empir-
ically estimated on the training set (C = 2.0, and d = 1)
6In Figures 2 and 3, the ?*? indicates statistically signif-
icant improvements over the state of the art at p ? 0.05,
calculated with approximate randomization (Pado?, 2006).
773
ture of monolingual (i.e. with the input sentences
translated in the same language using Google
Translate7) and cross-lingual features. Although
such system exploits word-alignment information
to some extent, this is only done at quantitative
level (e.g. number of unaligned words, percentage
of aligned words, length of the longest unaligned
subsequence). As regards It?En, the state of the
art (56.6%) is represented by the system described
in (Jimenez et al, 2012), which uses a pure pivot-
ing method (using Google Translate) and adaptive
similarity functions based on ?soft? cardinality for
flexible term comparisons. The two systems ob-
tained the same result on Fr?En (57.0%).
 50
 55
 60
 65
 70
 75
Es-En De-En Fr-En It-En
Ac
cu
rac
y (
%) *
* * * * * *
*
state-of-the-art
BB+ALB+AL+IDFB+AL+POSB+AL+IDF+POS
Figure 2: Accuracy obtained by each feature
group on four language combinations.
As can be seen in Figure 2, the combination of
all our features outperforms the state of the art
for each language pair. The accuracy improve-
ment ranges from 6.6% for Es?En (from 63.2% to
67.4%) to 14.6% for De?En (from 55.8% to 64%).
Except for Es?En, that has very competitive state-
of-the-art results, the combination of AL with POS
or IDF feature groups always outperforms the best
systems. Furthermore, the performance increase
with qualitative features (POS and IDF) shows co-
herent trends across all language pairs. It is worth
noting that, while we rely on a pure cross-lingual
approach, both the state-of-the-art CLTE systems
include features from the translation of T1 into the
language of T2. For De?En, quantitative features
alone achieve lower results compared to the other
languages. This can be motivated by the higher
difficulty in aligning De?En pairs (this hypothesis
is supported by the fact that the average number
of alignments per sentence pair is 18 for De?En,
and >22 for the other combinations). Neverthe-
less, qualitative features lead to results comparable
7http://translate.google.com/
with the other language pairs.
The selection of the best performing features
for each language pair produces further improve-
ments of varying degrees in Es?En (from 67.4%
to 68%), De?En (64% ? 64.8%) and It?En (63.4%
? 66.8%), while performance remains stable for
Fr?En (63%). All these configurations include
the IDF feature (12) and the proportion of aligned
words for each PoS category (7), proving the ef-
fectiveness of qualitative word alignment features.
The fact that HMM/intersection is the best com-
bination of alignment model and symmetrization
method is interesting, since it contradicts the gen-
eral notion that IBM models 3 and 4 perform bet-
ter than HMM (Och and Ney, 2003). A possible
explanation is that, while word alignment models
are usually trained on parallel corpora, the major-
ity of CLTE sentence pairs are not parallel. In
this setting, where producing reliable alignments
is more difficult, IBM models are less effective for
at least two reasons. First, including a word fertil-
ity model, IBM 3 and 4 limit (typically to the half
of the source sentence length) the number of tar-
get words that can be aligned with the null word.
Therefore, when such limit is reached, these mod-
els tend to force low probability, hence less reli-
able, word alignments. Second, in IBM model 4,
the larger distortion limit makes it possible to align
distant words. In the case of non-parallel sen-
tences, this often results in wrong or noisy align-
ments that affect final results. For these reasons,
CLTE data seem more suitable for the simpler and
more conservative HMM model, and a precision-
oriented symmetrization method like intersection.
3.2 Evaluation without CLTE training data
The goal of our second round of experiments is to
investigate if, and to what extent, our approach can
be considered as language-independent. Confirm-
ing this would allow to reuse models trained for
a given language pair in situations where CLTE
training data is missing. This is a rather realistic
situation since, while bitexts to train word aligners
are easier to find, the availability of labeled CLTE
data is far from being guaranteed.
Our experiments have been carried out, over the
same SemEval datasets, with two methods that do
not use labeled data for the target language com-
bination. The first one (method b in Figure 1)
uses a CLTE model trained for a language pair
L1?L2 for which labeled training data are avail-
774
able, and applies this model to a language pair
L3?L4 for which only parallel corpora are avail-
able. The second method (c in Figure 1) addresses
the same problem, but exploits a combination of
CLTE models trained for different language pairs.
For each test set, the models trained for the other
three language pairs are used in a voting scheme,
in order to check whether they can complement
each other to increase final results.
All the experiments have been performed using
the best CLTE model for each language pair, com-
paring results with those presented in Section 3.1.
 50
 55
 60
 65
 70
 75
 80
 85
Es-En De-En Fr-En It-En
Ac
cu
rac
y (
%) full s
ys.
full 
sys
.
full 
sys
. full 
sys
.
*
**
*
* * *
state-of-the-art
Es-EnDe-EnFr-EnIt-EnVoting
Figure 3: Accuracy obtained by reusing CLTE
models (alone and in a voting scheme).
As shown in Figure 3, reusing models for a new
language pair leads to results that still outperform
the state of the art.6 Remarkably, when used for
other language combinations, the Es?En, It?En,
and Fr?En models always lead to results above,
or equal to the state of the art. For similar lan-
guages such as Spanish, French, and Italian, the
accuracy increase over the state of the art is up to
14.8% (from 56.6% to 65.0%) and 13.4% (from
56.6% to 64.2%) when the Fr?En and Es?En mod-
els are respectively used to label the It?En dataset.
Although not always statistically significant and
below the performance obtained in the ideal sce-
nario where CLTE training data are available (full
sys.), such improvements suggest that our features
can be re-used, at least to some extent, across dif-
ferent language settings. As expected, the major
incompatibilities arise between German and the
other languages due to the linguistic differences
between this language and the others. However, it
is interesting to note that: i) at least in one case
(i.e. when tested on It?En) the De?En model still
achieves results above the state of the art, and ii)
on the De?En evaluation setting the worst model
(Fr?En) still achieves state of the art results.
The results obtained with the voting scheme
suggest that our models can complement each
other when used on a new language pair. Although
statistically significant only over It?En data, vot-
ing results both outperform the state of the art and
the results achieved by single models.
4 Conclusion
We investigated the usefulness of qualitative infor-
mation from automatic word alignment to iden-
tify semantic relations between sentences in dif-
ferent languages. With coherent results in CLTE,
we demonstrated that features considering the im-
portance of aligned terms can successfully inte-
grate the quantitative evidence (number and pro-
portion of aligned terms) used by previous su-
pervised learning approaches. A study on the
portability across languages of the learned mod-
els demonstrated that word alignment information
can be exploited to train reusable models for new
language combinations where bitexts are available
but CLTE labeled data are not.
Acknowledgments
This work has been partially supported by the EC-
funded projects CoSyne (FP7-ICT-4-248531) and
MateCat (ICT-2011.4.2?287688), and by Span-
ish Government through projects TIN2009-14009-
C02-01 and TIN2012-32615.
References
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence Es-
timation for Machine Translation. Summer work-
shop final report, JHU/CLSP.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation
(WMT?12), pages 10?51, Montre?al, Canada.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Software En-
gineering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49?57, Columbus,
Ohio, USA.
775
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: an Up-
date. SIGKDD Explorations, 11(1):10?18.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft Cardinality + ML: Learning Adap-
tive Similarity Functions for Cross-lingual Textual
Entailment. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval 2012),
pages 684?688, Montre?al, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Descrip-
tion for the 2005 IWSLT Speech Translation Evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, Pittsburgh, Penn-
sylvania, USA.
Philip Koehn. 2005. Europarl: a Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit X, pages 79?86, Phuket, Thailand.
Alessandro Lenci, Roberto Bartolini, Nicoletta Cal-
zolari, Ana Agua, Stephan Busemann, Emmanuel
Cartier, Karine Chevreau, and Jose? Coch. 2002.
Multilingual summarization by integrating linguistic
resources in the MLIS-MUSI Project. In Proceed-
ings of the Third International Conference on Lan-
guage Resources and Evaluation (LREC?02), pages
1464?1471, Las Palmas de Gran Canaria, Spain.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment.
In Proceedings of the Eleventh Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL HLT 2010),
pages 321?324, Los Angeles, California, USA.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012a. Detecting Semantic Equivalence and Infor-
mation Disparity in Cross?lingual Documents. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?12),
pages 120?124, Jeju Island, Korea.
Yashar Mehdad, Matteo Negri, and Marcello Fed-
erico. 2012b. Match without a Referee: Evaluating
MT Adequacy without Reference Translations. In
Proceedings of the Machine Translation Workshop
(WMT2012), Montre?al, Canada.
Christoph Monz, Vivi Nastase, Matteo Negri, Angela
Fahrni, Yashar Mehdad, and Michael Strube. 2011.
CoSyne: a Framework for Multilingual Content
Synchronization of Wikis. In Proceedings of Wik-
iSym 2011, the International Symposium on Wikis
and Open Collaboration, pages 217?218, Mountain
View, California, USA.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad,
Danilo Giampiccolo, and Alessandro Marchetti.
2011. Divide and Conquer: Crowdsourcing the Cre-
ation of Cross-Lingual Textual Entailment Corpora.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2011), Edinburgh, Scotland.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 Task 8: Cross-Lingual Textual En-
tailment for Content Synchronization. In Proceed-
ings of the 6th International Workshop on Seman-
tic Evaluation (SemEval 2012), pages 399?407,
Montre?al, Canada.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2013.
Semeval-2013 Task 8: Cross-Lingual Textual En-
tailment for Content Synchronization. In Proceed-
ings of the 7th International Workshop on Semantic
Evaluation (SemEval 2013), Atlanta, GA.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics, 29(1):19?51.
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
Kristen Parton. 2012. Lost and Found in Transla-
tion: Cross-Lingual Question Answering with Result
Translation. Ph.D. thesis, Columbia University.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings
of the Eight International Conference on Language
Resources and Evaluation (LREC?12), pages 2089?
2096, Istanbul, Turkey.
Gerard Salton and Christopher Buckley. 1988.
Term-weighting Approaches in Automatic Text Re-
trieval. Information Processing and Management,
24(5):513?523.
Helmut Schmid. 1995. Improvements in Part-of-
Speech Tagging with an Application to German. In
Proceedings of the ACL SIGDAT-Workshop, pages
47?50, Dublin, Ireland.
Hristo Tanev, Milen Kouylekov, Bernardo Magnini,
Matteo Negri, and Kiril Simov. 2006. Exploit-
ing Linguistic Indices and Syntactic Structures for
Multilingual Question Answering: ITC-irst at CLEF
2005. Accessing Multilingual Information Reposito-
ries, pages 390?399.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based Word Alignment in Statisti-
cal Translation. In Proceedings of the 16th Inter-
national Conference on Computational Linguistics
(ACL?96), pages 836?841, Copenhagen, Denmark.
Katharina Wa?schle and Sascha Fendrich. 2012. HDU:
Cross-lingual Textual Entailment with SMT Fea-
tures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), pages
467?471, Montre?al, Canada.
776
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 710?720,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Adaptive Quality Estimation for Machine Translation
Marco Turchi
(1)
Antonios Anastasopoulos
(3)
Jos
?
e G. C. de Souza
(1,2)
Matteo Negri
(1)
(1)
FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy
(2)
University of Trento, Italy
(3)
National Technical University of Athens, Greece
{turchi,desouza,negri}@fbk.eu
anastasopoulos.ant@gmail.com
Abstract
The automatic estimation of machine
translation (MT) output quality is a hard
task in which the selection of the appro-
priate algorithm and the most predictive
features over reasonably sized training sets
plays a crucial role. When moving from
controlled lab evaluations to real-life sce-
narios the task becomes even harder. For
current MT quality estimation (QE) sys-
tems, additional complexity comes from
the difficulty to model user and domain
changes. Indeed, the instability of the sys-
tems with respect to data coming from dif-
ferent distributions calls for adaptive so-
lutions that react to new operating con-
ditions. To tackle this issue we propose
an online framework for adaptive QE that
targets reactivity and robustness to user
and domain changes. Contrastive exper-
iments in different testing conditions in-
volving user and domain changes demon-
strate the effectiveness of our approach.
1 Introduction
After two decades of steady progress, research
in statistical machine translation (SMT) started to
cross its path with translation industry with tan-
gible mutual benefit. On one side, SMT research
brings to the industry improved output quality and
a number of appealing solutions useful to increase
translators? productivity. On the other side, the
market needs suggest concrete problems to solve,
providing real-life scenarios to develop and eval-
uate new ideas with rapid turnaround. The evolu-
tion of computer-assisted translation (CAT) envi-
ronments is an evidence of this trend, shown by
the increasing interest towards the integration of
suggestions obtained from MT engines with those
derived from translation memories (TMs).
The possibility to speed up the translation pro-
cess and reduce its costs by post-editing good-
quality MT output raises interesting research chal-
lenges. Among others, these include deciding
what to present as a suggestion, and how to do it
in the most effective way.
In recent years, these issues motivated research
on automatic QE, which addresses the problem
of estimating the quality of a translated sentence
given the source and without access to reference
translations (Blatz et al, 2003; Specia et al, 2009;
Mehdad et al, 2012). Despite the substantial
progress done so far in the field and in success-
ful evaluation campaigns (Callison-Burch et al,
2012; Bojar et al, 2013), focusing on concrete
market needs makes possible to further define the
scope of research on QE. For instance, moving
from controlled lab testing scenarios to real work-
ing environments poses additional constraints in
terms of adaptability of the QE models to the vari-
able conditions of a translation job. Such variabil-
ity is due to two main reasons:
1. The notion of MT output quality is highly
subjective (Koponen, 2012; Turchi et al,
2013; Turchi and Negri, 2014). Since the
quality standards of individual users may
vary considerably (e.g. according to their
knowledge of the source and target lan-
guages), the estimates of a static QE model
trained with data collected from a group of
post-editors might not fit with the actual
judgements of a new user;
2. Each translation job has its own specifici-
ties (domain, complexity of the source text,
average target quality). Since data from a
new job may differ from those used to train
the QE model, its estimates on the new in-
stances might result to be biased or uninfor-
mative.
The ability of a system to self-adapt to the be-
710
haviour of specific users and domain changes is
a facet of the QE problem that so far has been
disregarded. To cope with these issues and deal
with the erratic conditions of real-world trans-
lation workflows, we propose an adaptive ap-
proach to QE that is sensitive and robust to dif-
ferences between training and test data. Along this
direction, our main contribution is a framework in
which QE models can be trained and can continu-
ously evolve over time accounting for knowledge
acquired from post editors? work.
Our approach is based on the online learning
paradigm and exploits a key difference between
such framework and the batch learning methods
currently used. On one side, the QE models ob-
tained with batch methods are learned exclusively
from a predefined set of training examples under
the assumption that they have similar characteris-
tics with respect to the test data. This makes them
suitable for controlled evaluation scenarios where
such condition holds. On the other side, online
learning techniques are designed to learn in a step-
wise manner (either from scratch, or by refining an
existing model) from new, unseen test instances
by taking advantage of external feedback. This
makes them suitable for real-life scenarios where
the new instances to be labelled can considerably
differ from the data used to train the QE model.
To develop our approach, different online algo-
rithms have been embedded in the backbone of
a QE system. This required the adaptation of its
standard batch learning workflow to:
1. Perform online feature extraction from a
source?target pair (i.e. one instance at a time
instead of processing an entire training set);
2. Emit a prediction for the input instance;
3. Gather user feedback for the instance (i.e.
calculating a ?true label? based on the
amount of user post-editions);
4. Send the true label back to the model to up-
date its predictions for future instances.
Focusing on the adaptability to user and domain
changes, we report the results of comparative ex-
periments with two online algorithms and the stan-
dard batch approach. The evaluation is carried out
by measuring the global error of each algorithm
on test sets featuring different degrees of similar-
ity with the data used for training. Our results
show that the sensitivity of online QE models to
different distributions of training and test instances
makes them more suitable than batch methods for
integration in a CAT framework.
Our adaptive QE infrastructure has been re-
leased as open source. Its C++ implementation is
available at http://hlt.fbk.eu/technologies/
aqet.
2 Related work
QE is generally cast as a supervised machine
learning task, where a model trained from a col-
lection of (source, target, label) instances is used
to predict labels
1
for new, unseen test items (Spe-
cia et al, 2010).
In the last couple of years, research in the field
received a strong boost by the shared tasks orga-
nized within the WMT workshop on SMT,
2
which
is also the framework of our first experiment in
?5. Current approaches to the tasks proposed at
WMT have mainly focused on three main direc-
tions, namely: i) feature engineering, as in (Hard-
meier et al, 2012; de Souza et al, 2013a; de Souza
et al, 2013b; Rubino et al, 2013b), ii) model
learning with a variety of classification and regres-
sion algorithms, as in (Bicici, 2013; Beck et al,
2013; Soricut et al, 2012), and iii) feature selec-
tion as a way to overcome sparsity and overfitting
issues, as in (Soricut et al, 2012).
Being optimized to perform well on specific
WMT sub-tasks and datasets, current systems re-
flect variations along these directions but leave im-
portant aspects of the QE problem still partially
investigated or totally unexplored.
3
Among these,
the necessity to model the diversity of human qual-
ity judgements and correction strategies (Kopo-
nen, 2012; Koponen et al, 2012) calls for solu-
tions that: i) account for annotator-specific be-
haviour, thus being capable of learning from inher-
ently noisy datasets produced by multiple annota-
tors, and ii) self-adapt to changes in data distribu-
tion, learning from user feedback on new, unseen
test items.
1
Possible label types include post-editing effort scores
(e.g. 1-5 Likert scores indicating the estimated percentage
of MT output that has to be corrected), HTER values (Snover
et al, 2006), and post-editing time (e.g. seconds per word).
2
http://www.statmt.org/wmt13/
3
For a comprehensive overview of the QE approaches
proposed so far we refer the reader to the WMT12 and
WMT13 QE shared task reports (Callison-Burch et al, 2012;
Bojar et al, 2013).
711
These interconnected issues are particularly rel-
evant in the CAT framework, where translation
jobs from different domains are routed to pro-
fessional translators with different idiolect, back-
ground and quality standards.
The first aspect, modelling annotators? individ-
ual behaviour and interdependences, has been ad-
dressed by Cohn and Specia (2013), who explored
multi-task Gaussian Processes as a way to jointly
learn from the output of multiple annotations. This
technique is suitable to cope with the unbalanced
distribution of training instances and yields better
models when heterogeneous training datasets are
available.
The second problem, the adaptability of QE
models, has not been explored yet. A common
trait of all current approaches, in fact, is the re-
liance on batch learning techniques, which assume
a ?static? nature of the world where new unseen
instances that will be encountered will be similar
to the training data.
4
However, similarly to trans-
lation memories that incrementally store translated
segments and evolve over time incorporating users
style and terminology, all components of a CAT
tool (the MT engine and the mechanisms to assign
quality scores to the suggested translations) should
take advantage of translators feedback.
On the MT system side, research on adaptive
approaches tailored to interactive SMT and CAT
scenarios explored the online learning protocol
(Littlestone, 1988) to improve various aspects of
the decoding process (Cesa-Bianchi et al, 2008;
Ortiz-Mart??nez et al, 2010; Mart??nez-G?omez et
al., 2011; Mart??nez-G?omez et al, 2012; Mathur
et al, 2013; Bertoldi et al, 2013).
As regards QE models, our work represents the
first investigation on incremental adaptation by ex-
ploiting users feedback to provide targeted (sys-
tem, user, or project specific) quality judgements.
3 Online QE for CAT environments
When operating with advanced CAT tools, transla-
tors are presented with suggestions (either match-
ing fragments from a translation memory or auto-
matic translations produced by an MT system) for
each sentence of a source document. Before being
approved and published, translation suggestions
may require different amounts of post-editing op-
erations depending on their quality.
4
This assumption holds in the WMT evaluation scenario,
but it is not necessarily valid in real operating conditions.
Each post-edition brings a wealth of dynamic
knowledge about the whole translation process
and the involved actors. For instance, adaptive QE
components could exploit information about the
distance between automatically assigned scores
and the quality standards of individual translators
(inferred from the amount of their corrections) to
?profile? their behaviour.
The online learning paradigm fits well with this
research objective. In the online framework, dif-
ferently from the batch mode, the learning al-
gorithm sequentially processes an unknown se-
quence of instances X = x
1
, x
2
, ..., x
n
, returning
a prediction p(x
i
) as output at each step. Differ-
ences between p(x
i
) and the true label p?(x
i
) ob-
tained as feedback are used by the learner to refine
the next prediction p(x
i+1
).
In our experiments on adaptive QE we aim to
predict the quality of the suggested translations
in terms of HTER, which measures the minimum
edit distance between the MT output and its man-
ually post-edited version in the [0,1] interval.
5
In
this scenario:
? The set of instances X is represented by
(source, target) pairs;
? The prediction p(x
i
) is the automatically es-
timated HTER score;
? The true label p?(x
i
) is the actual HTER score
calculated over the target and its post-edition.
At each step of the process, the goal of the learner
is to exploit user post-editions to reduce the differ-
ence between the predicted HTER values and the
true labels for the following (source, target) pairs.
As depicted in Figure 1, this is done as follows:
1. At step i, an unlabelled (source, target) pair
x
i
is sent to a feature extraction component.
To this aim, we used an adapted version
(Shah et al, 2014) of the open-source QuEst
6
tool (Specia et al, 2013). The tool, which im-
plements a large number of features proposed
by participants in the WMT QE shared tasks,
has been modified to process one sentence at
a time as requested for integration in a CAT
environment;
5
Edit distance is calculated as the number of edits (word
insertions, deletions, substitutions, and shifts) divided by the
number of words in the reference. Lower HTER values indi-
cate better translations.
6
http://www.quest.dcs.shef.ac.uk/
712
Figure 1: Online QE workflow. <src>, <trg> and <pe> respectively stand for the source sentence, the
target translation and the post-edited target.
2. The extracted features are sent to an on-
line regressor, which returns a QE prediction
score p(x
i
) in the [0,1] interval (set to 0 at the
first round of the iteration);
3. Based on the post-edition done by the user,
the true HTER label p?(x
i
) is calculated by
means of the TERCpp
7
open source tool;
4. The true label is sent back to the online al-
gorithm for a stepwise model improvement.
The updated model is then ready to process
the following instance x
i+1
.
This new paradigm for QE makes it possible
to: i) let the QE system learn from one point at
a time without complete re-training from scratch,
ii) customize the predictions of an existing QE
model with respect to a specific situation (post-
editor or domain), or even iii) build a QE model
from scratch when training data is not available.
For the sake of clarity it is worth observing that,
at least in principle, a model built in a batch fash-
ion could also be adapted to new test data. For in-
stance, this could be done by running periodic re-
training routines once a certain amount of new la-
belled instances has been collected (de facto mim-
icking an online process). Such periodic updates,
however, would not represent a viable solution in
the CAT framework where post-editors? work can-
not be slowed by time-consuming procedures to
re-train core system components from scratch.
7
goo.gl/nkh2rE
4 Evaluation framework
To measure the adaptation capability of different
QE models, we experiment with a range of condi-
tions defined by variable degrees of similarity be-
tween training and test data.
The degree of similarity depends on several fac-
tors: the MT engine used, the domain of the docu-
ments to be translated, and the post-editing style of
individual translators. In our experiments, the de-
gree of similarity is measured in terms of ?HTER,
which is computed as the absolute value of the dif-
ference between the average HTER of the training
and test sets. Large values indicate a low simi-
larity between training and test data and a more
challenging scenario for the learning algorithms.
4.1 Experimental setup
In the range of possible evaluation scenarios, our
experiments cover:
? One artificial setting (?5) obtained from the
WMT12 QE shared task data, in which train-
ing/test instances are arranged to reflect ho-
mogeneous distributions of the HTER labels.
? Two settings obtained from data collected
with a CAT tool in real working condi-
tions, in which different facets of the adap-
tive QE problem interact with each other.
In the first (user change, ?6.1), train-
ing and test data from the same domain are
obtained from different users. In the sec-
713
ond (user+domain change, ?6.2), train-
ing and test data are obtained from different
users and domains.
For each setting, we compare an adaptive and
an empty model against a system trained in batch
mode. The adaptive model is built on top of an
existing model created from the training data and
exploits the new test instances to refine its predic-
tions in a stepwise manner. The empty model only
learns from the test set, simulating the worst con-
dition where training data is not available. The
batch model is built by learning only from the
training data and is evaluated on the test set with-
out exploiting information from the test instances.
Each model is also compared against a common
baseline for regression tasks, which is particularly
relevant in settings featuring different data distri-
butions between training and test sets. This base-
line (? henceforth) is calculated by labelling each
instance of the test set with the mean HTER score
of the training set. Previous works (Rubino et al,
2013a) demonstrated that its results can be partic-
ularly hard to beat.
4.2 Performance indicator and feature set
To measure the adaptability of our model to a
given test set we compute the Mean Absolute Er-
ror (MAE), a metric for regression problems also
used in the WMT QE shared tasks. The MAE is
the average of the absolute errors e
i
= |f
i
? y
i
|,
where f
i
is the prediction of the model and y
i
is
the true value for the i
th
instance.
As our focus is on the algorithmic aspect, in all
experiments we use the same feature set, which
consists of the seventeen features proposed in
(Specia et al, 2009). This feature set, fully de-
scribed in (Callison-Burch et al, 2012), takes into
account the complexity of the source sentence
(e.g. number of tokens, number of translations per
source word) and the fluency of the target trans-
lation (e.g. language model probabilities). The
results of previous WMT QE shared tasks have
shown that these baseline features are particularly
competitive in the regression task (with only few
systems able to beat them at WMT12).
4.3 Online algorithms
In our experiments we evaluate two online algo-
rithms, OnlineSVR (Parrella, 2007)
8
and Passive-
8
http://www2.imperial.ac.uk/
?
gmontana/
onlinesvr.htm
Aggressive Perceptron (Crammer et al, 2006),
9
by
comparing their performance with a batch learning
strategy based on the Scikit-learn implementation
of Support Vector Regression (SVR).
10
The choice of the OnlineSVR and Passive-
Aggressive (OSVR and PA henceforth) is moti-
vated by different considerations. From a perfor-
mance point of view, as an adaptation of -SVR
which proved to be one of the top performing algo-
rithms in the regression QE tasks at WMT, OSVR
seems to be the best candidate. For this reason,
we use the online adaptation of -SVR proposed
by (Ma et al, 2003). The goal of OnlineSVR is to
find a way to add each new sample to one of three
sets (support, empty, error) maintaining the con-
sistency of a set of conditions known as Karush-
Kuhn Tucker (KKT) conditions. For each new
point, OSVR starts a cycle where the samples are
moved across the three sets until the KKT condi-
tions are verified and the new point is assigned to
one of the sets. If the point is identified as a sup-
port vector, the parameters of the model are up-
dated. This allows OSVR to benefit from the pre-
diction capability of -SVR in an online setting.
From a practical point of view, providing the
best trade off between accuracy and computational
time (He and Wang, 2012), PA represents a good
solution to meet the demand of efficiency posed
by the CAT framework. For each instance i, after
emitting a prediction and receiving the true label,
PA computes the -insensitive hinge loss function.
If its value is larger than the tolerance parameter
(), the weights of the model are updated as much
as the aggressiveness parameter C allows. In con-
trast with OSVR, which keeps track of the most
important points seen in the past (support vectors),
the update of the weights is done without consid-
ering the previously processed i-1 instances. Al-
though it makes PA faster than OSVR, this is a
riskier strategy because it may lead the algorithm
to change the model to adapt to outlier points.
5 Experiments with WMT12 data
The motivations for experiments with training and
test data featuring homogeneous label distribu-
tions are twofold. First, since in this artificial sce-
nario adaptation capabilities are not required for
the QE component, batch methods operate in the
ideal conditions (as training and test are indepen-
9
https://code.google.com/p/sofia-ml/
10
http://scikit-learn.org/
714
WMT Dataset
Train Test
? ? Batch Adaptive Empty
HTER MAE MAE MAE Alg. MAE Alg.
200 754 0.39 13.7 13.2 13.2
?
OSVR 13.5
?
OSVR
600 754 1.32 13.8 12.7 12.9
?
OSVR 13.5
?
OSVR
1500 754 1.22 13.8 12.7 12.8
?
OSVR 13.5
?
OSVR
Table 1: MAE of the best performing batch, adaptive and empty models on WMT12 data. Training sets
of different size and the test set have been arranged to reflect homogeneous label distributions.
dent and identically distributed). This makes pos-
sible to obtain from batch models the best possible
performance to compare with. Second, this sce-
nario provides the fairest conditions for such com-
parison because, in principle, online algorithms
are not favoured by the possibility to learn from
the diversity of the test instances.
For our controlled experiments we use the
WMT12 English-Spanish corpus, which consists
of 2,254 source-target pairs (1,832 for training,
422 for test). The HTER labels for our regression
task are calculated from the post-edited version
and the target sentences provided in the dataset.
To avoid biases in the label distribution, the
WMT12 training and test data have been merged,
shuffled, and eventually separated to generate
three training sets of different size (200, 600, and
1500 instances), and one test set with 754 in-
stances. For each algorithm, the training sets are
used for learning the QE models, optimizing pa-
rameters (i.e. C, , the kernel and its parame-
ters for SVR and OSVR; tolerance and aggressive-
ness for PA) through grid search in 10-fold cross-
validation.
Evaluation is carried out by measuring the per-
formance of the batch (learning only from the
training set), the adaptive (learning from the train-
ing set and adapting to the test set), and the empty
(learning from scratch from the test set) models in
terms of global MAE scores on the test set.
Table 1 reports the results achieved by the
best performing algorithm for each type of model
(batch, adaptive, empty). As can be seen, close
MAE values show a similar behaviour for the three
types of models.
11
With the same amount of train-
ing data, the performance of the batch and the
adaptive models (in this case always obtained with
OSVR) is almost identical. This demonstrates
that, as expected, the online algorithms do not take
11
Results marked with the ?
?
? symbol are NOT statisti-
cally significant compared to the corresponding batch model.
The others are always statistically significant at p?0.005, cal-
culated with approximate randomization (Yeh, 2000).
advantage of test data with a label distribution sim-
ilar to the training set. All the models outper-
form the baseline, even if the minimal differences
confirm the competitiveness of such a simple ap-
proach.
Overall, these results bring some interesting in-
dications about the behaviour of the different on-
line algorithms. First, the good results achieved
by the empty models (less than one MAE point
separates them from the best ones built on the
largest training set) suggest their high potential
when training data are not available. Second,
our results show that OSVR is always the best
performing algorithm for the adaptive and empty
models. This suggests a lower capability of PA to
learn from instances similar to the training data.
6 Experiments with CAT data
To experiment with adaptive QE in more realis-
tic conditions we used a CAT tool
12
to collect
two datasets of (source, target, post edited tar-
get) English-Italian tuples.The source sentences in
the datasets come from two documents from dif-
ferent domains, respectively legal (L) and infor-
mation technology (IT). The L document, which
was extracted from a European Parliament resolu-
tion published on the EUR-Lex platform,
13
con-
tains 164 sentences. The IT document, which was
taken from a software user manual, contains 280
sentences. The source sentences were translated
with two SMT systems built by training the Moses
toolkit (Koehn et al, 2007) on parallel data from
the two domains (about 2M sentences for IT and
1.5M for L). Post-editions were collected from
eight professional translators (four for each docu-
ment) operating with the CAT tool in real working
conditions.
According to the way they are created, the two
datasets allow us to evaluate the adaptability of
different QE models with respect to user changes
12
MateCat ? http://www.matecat.com/
13
http://eur-lex.europa.eu/
715
user change
Legal Domain
Train Test
? ? Batch Adaptive Empty
HTER MAE MAE MAE Alg. MAE Alg.
rad cons 20.5 21.4 20.6 14.5 PA 12.5 OSVR
cons rad 19.4 21.2 21.3 16.1 PA 11.3 OSVR
sim1 sim2 3.3 14.7 12.2 12.6
?
OSVR 12.9
?
OSVR
sim2 sim1 3.2 13.4 13.3 13.9
?
OSVR 15.2
?
OSVR
IT Domain
Train Test
? ? Batch Adaptive Empty
HTER MAE MAE MAE Alg MAE Alg
cons rad 12.8 19.2 19.8 17.5
?
OSVR 16.6 OSVR
rad cons 9.6 16.8 16.6 15.6 PA 15.5 OSVR
sim2 sim1 3.3 14.7 14.4 15
?
OSVR 15.5
?
OSVR
sim1 sim2 1.1 15 13.9 14.4
?
OSVR 16.1
?
OSVR
Table 2: MAE of the best performing batch, adaptive and empty models on CAT data collected from
different users in the same domain.
within the same domain (?6.1), as well as user and
domain changes at the same time (?6.2).
For each document D (L or IT), these two sce-
narios are obtained by dividing D into two parts
of equal size (80 instances for L and 140 for IT).
The result is one training set and one test set for
each post-editor within the same domain. For the
user change experiments, training and test sets
are selected from different post-editors within the
same domain. For the user+domain change
experiments, training and test sets are selected
from different post-editors in different domains.
On each combination of training and test sets,
the batch, adaptive, and empty models are trained
and evaluated in terms of global MAE scores on
the test set.
6.1 Dealing with user changes
Among the possible combinations of training and
test data from different post-editors in the same
domain, Table 2 refers to two opposite scenarios.
For each domain, these respectively involve the
most dissimilar and the most similar post-editors
according to the ?HTER. Also in this case, for
each model (batch, adaptive and empty) we only
report the MAE of the best performing algorithm.
The first scenario defines a challenging situation
where two post-editors (rad and cons) are charac-
terized by opposite behaviour. As evidenced by
the high ?HTER values, one of them (rad) is the
most ?radical? post-editor (performing more cor-
rections) while the other (cons) is the most ?con-
servative? one. As shown in Table 2, global MAE
scores for the online algorithms (both adaptive and
empty) indicate their good adaptation capabilities.
This is evident from the significant improvements
both over the baseline (?) and the batch models.
Interestingly, the best results are always achieved
by the empty models (with MAE reductions up to
10 points when tested on rad in the L domain,
and 3.2 points when tested on rad in the IT do-
main). These results (MAE reductions are always
statistically significant) suggest that, when deal-
ing with datasets with very different label distri-
butions, the evident limitations of batch methods
are more easily overcome by learning from scratch
from the feedback of a new post-editor. This also
holds when the amount of test points to learn from
is limited, as in the L domain where the test set
contains only 80 instances. From the application-
oriented perspective that motivates our work, con-
sidering the high costs of acquiring large and rep-
resentative QE training data, this is an important
finding.
The second scenario defines a less challeng-
ing situation where the two post-editors (sim1 and
sim2) are characterized by the most similar be-
haviour (small ?HTER). This scenario is closer to
the situation described in Section ?5. Also in this
case MAE results for the adaptive and empty mod-
els are slightly worse, but not significantly, than
those of the batch models and the baseline. How-
ever, considering the very small amount of ?unin-
formative? instances to learn from (especially for
the empty models), these lower results are not sur-
prising.
A closer look at the behaviour of the online al-
gorithms in the two domains leads to other obser-
vations. First, OSVR always outperforms PA for
the empty models and when post-editors have sim-
716
user+domain change
Train Test
? ? Batch Adaptive Empty
HTER MAE MAE MAE Alg MAE Alg
L cons IT rad 24.5 26.4 27 18.2 OSVR 16.6 OSVR
IT rad L cons 24.0 24.9 25.4 19.7 OSVR 12.5 OSVR
L rad L cons 20.5 21.4 20.6 14.5 PA 12.5 OSVR
L cons L rad 19.4 21.2 21.3 16.1 PA 11.3 OSVR
IT cons L cons 13.5 17.3 17.5 15.7 OSVR 12.5 OSVR
IT cons IT rad 12.8 19.2 19.8 17.5 OSVR 16.6 OSVR
L cons IT cons 12.7 17.6 17.6 15.1 OSVR 15.5 OSVR
IT rad IT cons 9.6 16.8 16.6 15.6 PA 15.5 OSVR
IT cons L rad 8.3 12.3 13 10.7 OSVR 11.3 OSVR
L rad IT rad 6.8 17 16.9 16.2 OSVR 16.6 OSVR
L rad IT cons 5.0 15.4 16.2 14.7 OSVR 15.5 OSVR
IT rad L rad 2.2 10.6 10.8 10.5 OSVR 11.3 OSVR
Table 3: MAE of the best performing batch, adaptive and empty models on CAT data collected from
different users and domains.
ilar behaviour, which are situations where the al-
gorithm does not have to quickly adapt or react to
sudden changes.
Second, PA seems to perform better for the
adaptive models when the post-editors have sig-
nificantly different behaviour and a quick adapta-
tion to the incoming points is required. This can
be motivated by the fact that PA relies on a simpler
and less robust learning strategy that does not keep
track of all the information coming from the previ-
ously processed instances, and can easily modify
its weights taking into consideration the last seen
point (see Section ?3). For OSVR the addition of
new points to the support set may have a limited
effect on the whole model, in particular if the num-
ber of points in the set is large. This also results
in a different processing time for the two algo-
rithms.
14
For instance, in the empty configurations
on IT data, OSVR devotes 6.0 ms per instance to
update the model, while PA devotes 4.8ms, which
comes at the cost of lower performance.
6.2 Dealing with user and domain changes
In the last round of experiments we evaluate the
reactivity of different online models to simultane-
ous user and domain changes. To this aim, our
QE models are created using a training set coming
from one domain (L or IT), and then used to pre-
dict the HTER labels for the test instances coming
from the other domain (e.g. training on L, testing
on IT).
Among the possible combinations of training
14
Their complexity depends on the number of features (f )
and the number of previously seen instances (n). While for
PA it is linear in f, i.e. O(f), for OSVR it is quadratic in n, i.e.
O(n
2
*f).
and test data, Table 3 refers to scenarios involv-
ing the most conservative and radical post-editors
in each domain (previously identified with cons
and rad)
15
. In the table, results are ordered ac-
cording to the ?HTER computed between the se-
lected post-editor in the training domain (e.g. L
cons) and the selected post-editor in the test do-
main (e.g. IT rad). For the sake of comparison,
we also report (grey rows) the results of the ex-
periments within the same domain presented in
?6.1. For each type of model (batch, adaptive and
empty) we only show the MAE obtained by the
best performing algorithm.
Intuitively, dealing with simultaneous user and
domain changes represents a more challenging
problem compared to the previous setting where
only post-editors changes were considered. Such
intuition is confirmed by the results of the adaptive
models that outperform both the baseline (?) and
the batch models even for low ?HTER values. Al-
though in these cases the distance between train-
ing and test data is comparable to the experiments
with similar post-editors working in the same do-
main (sim1 and sim2), here the predictive power
of the batch models seems in fact to be lower. The
same holds also for the empty models except in
two cases where the ?HTER is the smallest (2.2
and 5.0). This is a strong evidence of the fact that,
in case of domain changes, online models can still
learn from new test instances even if they have a
label distribution similar to the training set.
When the distance between training and test in-
creases, our results confirm our previous findings
15
For brevity, we omit the results for the other post-editors
which, however, show similar trends with respect to the pre-
vious experiments.
717
about the potential of the empty models. The ob-
served MAE reductions range in fact from 10.4
to 12.9 points for the two combinations with the
highest ?HTER.
From the algorithmic point of view, our results
indicate that OSVR achieves the best performance
for all the combinations involving user and domain
changes. This contrasts with the results of most of
the combinations involving only user changes with
post-editors characterized by opposite behaviour
(grey rows in Table 3). However, it has to be re-
marked that in the case of heterogeneous datasets
the difference between the two algorithms is al-
ways very high. In our experiments, when PA out-
performs OSVR, its MAE results are significantly
lower and vice-versa (respectively up to 1.5 and
1.7 MAE points). This suggests that, although PA
is potentially capable of achieving higher results
and better adapt to the new test points, its instabil-
ity makes it less reliable for practical use.
As a final analysis of our results, we investi-
gated how the performance of the different types
of models (batch, adaptive, empty) relates to the
distance between training and test sets. To this
aim, we computed the Pearson correlation be-
tween the ?HTER (column 3 in Table 3) and the
MAE of each model (columns 5, 6 and 8), which
respectively resulted in 0.9 for the batch, 0.63 for
the adaptive and -0.07 for the empty model. These
values confirm that batch models are heavily af-
fected by the dissimilarity between training and
test data: large differences in the label distribution
imply higher MAE results and vice-versa. This
is in line with our previous findings about batch
models that, learning only from the training set,
cannot leverage possible dissimilarities of the test
set. The lower correlation observed for the adap-
tive models also confirms our intuitions: adapting
to the new test points, these models are in fact
more robust to differences with the training data.
As expected, the results of the empty models are
completely uncorrelated with the ?HTER since
they only use the test set.
This analysis confirms that, even when dealing
with different domains, the similarity between the
training and test data is one of the main factors that
should drive the choice of the QE model. When
this distance is minimal, batch models can be a
reasonable option, but when the gap between train-
ing and test data increases, adaptive or empty mod-
els are a preferable choice to achieve good results.
7 Conclusion
In the CAT scenario, each translation job can be
seen as a complex situation where the user (his
personal style and background), the source doc-
ument (the language and the domain) and the un-
derlying technology (the translation memory and
the MT engine that generate translation sugges-
tions) contribute to make the task unique. So far,
the adaptability to such specificities (a major chal-
lenge for CAT technology) has been mainly sup-
ported by the evolution of translation memories,
which incrementally store translated segments in-
corporating the user style. The wide adoption of
translation memories demonstrates the importance
of capitalizing on such information to increase
translators productivity.
While this lesson recently motivated research
on adaptive MT decoders that learn from user cor-
rections, nothing has been done to develop adap-
tive QE components. In the first attempt to ad-
dress this problem, we proposed the application
of the online learning protocol to leverage users
feedback and to tailor QE predictions to their qual-
ity standards. Besides highlighting the limitations
of current batch methods to adapt to user and
domain changes, we performed an application-
oriented analysis of different online algorithms fo-
cusing on specific aspects relevant to the CAT sce-
nario. Our results show that the wealth of dynamic
knowledge brought by user corrections can be ex-
ploited to refine in a stepwise fashion the qual-
ity judgements in different testing conditions (user
changes as well as simultaneous user and domain
changes).
As an additional contribution, to spark further
research on this facet of the QE problem, our adap-
tive QE infrastructure (integrating all the compo-
nents and the algorithms described in this paper)
has been released as open source. Its C++ im-
plementation is available at http://hlt.fbk.eu/
technologies/aqet.
Acknowledgements
This work has been partially supported by the EC-
funded project MateCat (ICT-2011.4.2-287688).
References
Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia
Specia. 2013. SHEF-Lite: When less is more for
translation quality estimation. In Proceedings of the
718
8th
Workshop on Statistical Machine Translation,
Sofia, Bulgaria, August.
Nicola Bertoldi, Mauro Cettolo, and Federico Mar-
cello. 2013. Cache-based Online Adaptation
for Machine Translation Enhanced Computer As-
sisted Translation. In Proceedings of the XIV Ma-
chine Translation Summit, pages 1147?1162, Nice,
France.
Ergun Bicici. 2013. Feature decay algorithms for fast
deployment of accurate statistical machine transla-
tion systems. In Proceedings of the 8
th
Workshop
on Statistical Machine Translation, Sofia, Bulgaria,
August.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence Es-
timation for Machine Translation. Summer work-
shop final report, JHU/CLSP.
Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Workshop
on Statistical Machine Translation. In Proceedings
of the 8
th
Workshop on Statistical Machine Transla-
tion, WMT-2013, pages 1?44, Sofia, Bulgaria.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the 7
th
Work-
shop on Statistical Machine Translation (WMT?12),
pages 10?51, Montr?eal, Canada.
Nicol`o Cesa-Bianchi, Gabriel Reverberi, and Sandor
Szedmak. 2008. Online Learning Algorithms for
Computer-Assisted Translation. Deliverable D4.2,
SMART: Statistical Multilingual Analysis for Re-
trieval and Translation.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of the 51
st
Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL-2013, pages 32?42, Sofia, Bulgaria.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
Passive-Aggressive Algorithms. J. Mach. Learn.
Res., 7:551?585, December.
Jos?e G.C. de Souza, Christian Buck, Marco Turchi, and
Matteo Negri. 2013a. FBK-UEdin participation to
the WMT13 quality estimation shared task. In Pro-
ceedings of the 8
th
Workshop on Statistical Machine
Translation, Sofia, Bulgaria, August.
Jos?e G.C. de Souza, Miquel Espl`a-Gomis, Marco
Turchi, and Matteo Negri. 2013b. Exploiting Quali-
tative Information from Automatic Word Alignment
for Cross-lingual NLP Tasks. In Proceedings of the
51
st
Annual Meeting of the Association for Compu-
tational Linguistics - Short Papers, pages 771?776,
Sofia, Bulgaria.
Christian Hardmeier, Joakim Nivre, and J?org Tiede-
mann. 2012. Tree Kernels for Machine Transla-
tion Quality Estimation. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation
(WMT?12), pages 109?113, Montr?eal, Canada.
Zhengyan He and Houfeng Wang. 2012. A Com-
parison and Improvement of Online Learning Al-
gorithms for Sequence Labeling. In Proceedings
of the 24th International Conference on Compu-
tational Linguistics (COLING 2012), pages 1147?
1162, Mumbai, India.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45
th
Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180.
Maarit Koponen, Wilker Aziz, Luciana Ramos, and
Lucia Specia. 2012. Post-editing Time as a Mea-
sure of Cognitive Effort. In Proceedings of the
AMTA 2012 Workshop on Post-editing Technology
and Practice (WPTP 2012), San Diego, California.
Maarit Koponen. 2012. Comparing Human Percep-
tions of Post-editing Effort with Post-editing Op-
erations. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 181?190,
Montr?eal, Canada.
Nick Littlestone. 1988. Learning Quickly when Irrel-
evant Attributes Abound: A New Linear-Threshold
Algorithm. In Machine Learning, pages 285?318.
Junshui Ma, James Theiler, and Simon Perkins. 2003.
Accurate Online Support Vector Regression. Neural
Computation, 15:2683?2703.
Pascual Mart??nez-G?omez, Germ?an Sanchis-Trilles, and
Francisco Casacuberta. 2011. Online Learning via
Dynamic Reranking for Computer Assisted Transla-
tion. In Proceedings of the 12th international con-
ference on Computational linguistics and intelligent
text processing - Volume Part II, CICLing?11.
Pascual Mart??nez-G?omez, Germ?an Sanchis-Trilles, and
Francisco Casacuberta. 2012. Online adaptation
strategies for statistical machine translation in post-
editing scenarios. Pattern Recognition, 45(9):3193?
3203, September.
Prashant Mathur, Mauro Cettolo, and Marcello Fed-
erico. 2013. Online Learning Approaches in Com-
puter Assisted Translation. In Proceedings of the
8
th
Workshop on Statistical Machine Translation,
Sofia, Bulgaria.
719
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Match without a Referee: Evaluating MT
Adequacy without Reference Translations. In Pro-
ceedings of the 7
th
Workshop on Statistical Machine
Translation, pages 171?180, Montr?eal, Canada.
Daniel Ortiz-Mart??nez, Ismael Garc??a-Varea, and Fran-
cisco Casacuberta. 2010. Online learning for in-
teractive statistical machine translation. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, HLT ?10, pages
546?554, Stroudsburg, PA, USA.
Francesco Parrella. 2007. Online support vector re-
gression. Master?s Thesis, Department of Informa-
tion Science, University of Genoa, Italy.
Raphael Rubino, Jos?e G.C. de Souza, Jennifer Fos-
ter, and Lucia Specia. 2013a. Topic Models for
Translation Quality Estimation for Gisting Purposes.
In Proceedings of the Machine Translation Summit
XIV, Nice, France.
Raphael Rubino, Antonio Toral, S Cort?es Va??llo, Jun
Xie, Xiaofeng Wu, Stephen Doherty, and Qun Liu.
2013b. The CNGL-DCU-Prompsit translation sys-
tems for WMT13. In Proceedings of the 8
th
Work-
shop on Statistical Machine Translation, pages 211?
216, Sofia, Bulgaria.
Kashif Shah, Marco Turchi, and Lucia Specia. 2014.
An Efficient and User-friendly Tool for Machine
Translation Quality Estimation. In Proceedings of
the 9
t
h International Conference on Language Re-
sources and Evaluation, Reykjavik, Iceland.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Trans-
lation in the Americas, pages 223?231, Cambridge,
Massachusetts, USA.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceedings of
the 7
th
Workshop on Statistical Machine Translation
(WMT?12), pages 145?151, Montr?eal, Canada.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proceedings of the 13
th
Annual Con-
ference of the European Association for Machine
Translation (EAMT?09), pages 28?35, Barcelona,
Spain.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine Translation Evaluation versus Quality Es-
timation. Machine translation, 24(1):39?50.
Lucia Specia, Kashif Shah, Jos?e G.C. de Souza, and
Trevor Cohn. 2013. QuEst - A Translation Qual-
ity Estimation Framework. In Proceedings of the
51
st
Annual Meeting of the Association for Compu-
tational Linguistics: System Demonstrations, ACL-
2013, pages 79?84, Sofia, Bulgaria.
Marco Turchi and Matteo Negri. 2014. Automatic An-
notation of Machine Translation Datasets with Bi-
nary Quality Judgements. In Proceedings of the 9
th
International Conference on Language Resources
and Evaluation, Reykjavik, Iceland.
Marco Turchi, Matteo Negri, and Marcello Federico.
2013. Coping with the Subjectivity of Human
Judgements in MT Quality Estimation. In Proceed-
ings of the 8
th
Workshop on Statistical Machine
Translation, pages 240?251, Sofia, Bulgaria.
Alexander Yeh. 2000. More Accurate Tests for the
Statistical Significance of Result Differences. In
Proceedings of the 18th conference on Computa-
tional linguistics (COLING 2000) - Volume 2, pages
947?953, Saarbrucken, Germany.
720
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 202?205,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
FBK NK: a WordNet-based System
for Multi-Way Classification of Semantic Relations
Matteo Negri and Milen Kouylekov
FBK-Irst
Trento, Italy
{negri,kouylekov}@fbk.eu
Abstract
We describe a WordNet-based system for
the extraction of semantic relations be-
tween pairs of nominals appearing in
English texts. The system adopts a
lightweight approach, based on training
a Bayesian Network classifier using large
sets of binary features. Our features con-
sider: i) the context surrounding the an-
notated nominals, and ii) different types
of knowledge extracted from WordNet, in-
cluding direct and explicit relations be-
tween the annotated nominals, and more
general and implicit evidence (e.g. seman-
tic boundary collocations). The system
achieved a Macro-averaged F1 of 68.02%
on the ?Multi-Way Classification of Se-
mantic Relations Between Pairs of Nom-
inals? task (Task #8) at SemEval-2010.
1 Introduction
The ?Multi-Way Classification of Semantic Re-
lations Between Pairs of Nominals? task at
SemEval-2010 (Hendrickx et al, 2010) consists
in: i) selecting from an inventory of nine possi-
ble relations the one that most likely holds be-
tween two annotated nominals appearing in the in-
put sentence, and ii) specifying the order of the
nominals as the arguments of the relation. In con-
trast with the semantic relations classification task
(Task #4) at SemEval-2007 (Girju et al, 2007),
which treated each semantic relation separately as
a single two-class (positive vs. negative) classifi-
cation task, this year?s edition of the challenge pre-
sented participating systems with a more difficult
and realistic multi-way setup, where the relation
Other can also be assigned if none of the nine re-
lations is suitable for a given sentence. Examples
of the possible markable relations are reported in
Table 1
1
.
The objective of our experiments with the pro-
posed task is to develop a Relation Extraction sys-
tem based on shallow linguistic processing, taking
the most from available thesauri and ontologies.
As a first step in this direction, our submitted runs
have been obtained by processing the input sen-
tences only to lemmatize their terms, and by using
WordNet as the sole source of knowledge.
Similar to other approaches (Moldovan and
Badulescu, 2009; Beamer et al, 2009), our sys-
tem makes use of semantic boundaries extracted
from the WordNet IS-A backbone. Such bound-
aries (i.e. divisions in the WordNet hierarchy
that best generalize over the training examples)
are used to define pairs of high-level synsets with
high correlation with specific relations. For in-
stance, <microorganism#1, happening#1> and
<writing#1, consequence#1> are extracted from
the training data as valid high-level collocations
respectively for the relations Cause-Effect and
Message-Topic. Besides exploiting the Word-
Net IS-A hierarchy, the system also uses the
holo-/meronymy relations, and information de-
rived from the WordNet glosses to capture specific
relations such as Member-Collection and Product-
Producer. In addition, the context surrounding
the annotated nominals is represented as a bag-of-
words/synonyms to enhance the relation extraction
process. Several experiments have been carried
out encoding all the information as large sets of
binary features (up to ?6200) to train a Bayesian
Network classifier available in the Weka
2
toolkit.
To capture both the relations and the order of
1
In the first example the order of the nominals is
(<e2>,<e1>), while in the others is (<e1>,<e2>)
2
http://www.cs.waikato.ac.nz/ml/weka/
202
1 Cause-Effect(e2,e1) A person infected with a particular <e1>flu</e1> <e2>virus</e2> strain develops an
antibody against that virus.
2 Instrument-Agency(e1,e2) The <e1>river</e1> once powered a <e2>grist mill</e2>.
3 Product-Producer(e1,e2) The <e1>honey</e1><e2>bee</e2> is the third insect genome published by scientists,
after a lab workhorse, the fruit fly, and a health menace, the mosquito.
4 Content-Container(e1,e2) I emptied the <e1>wine</e1> <e2>bottle</e2> into my glass and toasted my friends.
5 Entity-Origin(e1,e2) <e1>This book</e1>is from the 17th <e2>century</e2>.
6 Entity-Destination(e1,e2) <e1>Suspects</e1> were handed over to the <e2>police station</e2>.
7 Component-Whole(e1,e2) <e1>Headlights</e1> are considered as the eyes of the <e2>vehicle</e2>.
8 Member-
Collection(e1,e2)
Mary looked back and whispered: ?I know every <e1>tree</e1> in this
<e2>forest</e2>, every scent?.
9 Message-Topic(e1,e2) Here we offer a selection of our favourite <e1>books</e1> on military
<e2>history</e2>.
Table 1: SemEval-2010 Task #8 semantic relations.
their arguments, training sentences having oppo-
site argument directions for the same relation have
been handled separately, and assigned to different
classes (thus obtaining 18 classes for the nine tar-
get relations, plus one for the Other relation).
The following sections overview our experi-
ments, describing the features used by the sys-
tem (Section 2), and the submitted runs with the
achieved results (Section 3). A concluding discus-
sion on the results is provided in Section 4.
2 Features used
The system uses two types of boolean features:
WordNet features, and context features.
2.1 WordNet features
WordNet features consider different types of
knowledge extracted from WordNet 3.0.
Semantic boundary collocations. Collocations
of high-level synsets featuring a high correlation
with specific relations are acquired from the train-
ing set using a bottom-up approach. Starting from
the nominals annotated in the training sentences
(<e1> and<e2>), the WordNet IS-A backbone is
climbed to collect all their ancestors. Then, all the
ancestors? collocations occurring at least n times
for at most m relations are retained, and treated
as boolean features (set to 1 for a given sentence
if its annotated nominals appear among their hy-
ponyms). The n and m parameters are optimized
on the training set.
Holo-/meronymy relations. These boolean fea-
tures are set to 1 every time a pair of annotated
nominals in a sentence is directly connected by
holo-/meronyny relations. They are particularly
appropriate to capture the Component-Whole and
Member-Collection relations, as in the 8th exam-
ple in Table 1 (where tree#1 is an holonym of
forest#1). Due to time constraints, we did not
explore the possibility to generalize these fea-
tures considering transitive closures of the nomi-
nals? hypo-/hypernyms. This possibility could al-
low to handle sentences like ?A <e1>herd</e1>
is a large group of <e2>animals</e2>.? Here,
though herd#1 and animal#1 are not directly con-
nected by the meronymy relation, all the herd#1
meronyms have animal#1 as a common ancestor.
Glosses. Given a pair of annotated nominals
<e1>,<e2>, these features are set to 1 every time
either <e1> appears in the gloss of <e2>, or
vice-versa. They are intended to support the dis-
covery of relations in the case of consecutive nom-
inals (e.g. honey#1 and bee#1 in the 3rd example
in Table 1), where contextual information does not
provide sufficient clues to make a choice. In our
experiments we extracted features from both tok-
enized and lemmatized words (both nominals, and
gloss words). Also in this case, due to time con-
straints we did not explore the possibility to gener-
alize the feature considering the nominals? hypo-
/hypernyms. This possibility could allow to handle
sentences like examples 1 and 4 in Table 1. For
instance in example 4, the gloss of ?bottle? con-
tains two hypernyms of wine#1, namely drink#3
and liquid#1, that could successfully trigger the
Content-Container relation.
Synonyms. While the previous features operate
with the annotated nominals, WordNet synonyms
are used to generalize the other terms in the sen-
tence, allowing to extract different types of con-
textual features (see the next Section).
2.2 Context features
Besides the annotated nominals, also specific
words (and word combinations) appearing in the
surrounding context often contribute to trigger the
203
target relations. Distributional evidence is cap-
tured by considering word contexts before, be-
tween, and after the annotated nominals. To this
aim, we experimented with windows of different
size, containing words that occur in the training
set a variable number of times. Both the parame-
ters (i.e. the size of the windows, and the number
of occurrences) are optimized on training data. In
our experiments we extracted contextual features
from lemmatized sentences.
3 Submitted runs and results
Our participation to the SemEval-2010 Task
#8 consisted in four runs, with the best one
(FBK NK-RES1) achieving a Macro-averaged F1
of 68.02% on the test data. For this submis-
sion, the overall training and test running times are
about 12?30? and 1?30? respectively, on an Intel
Core2 Quad 2.66GHz with 4GB RAM.
FBK NK-RES1. This run has been obtained
adopting a conservative approach, trying to min-
imize the risk of overfitting the training data. The
features used can be summarized as follows:
? Semantic boundary collocations: all the col-
locations of <e1> and <e2> ancestors oc-
curring at least 10 times in the training set (m
param.), for at most 3 relations (n param.);
? Holo-/meronymy relations between the anno-
tated nominals;
? Glosses: handled at the level of tokens;
? Context features: left, between, and right
context windows of size 3-ALL-3 words re-
spectively. Number of occurrences: 25 (left),
10 (between), 25 (right).
On the training set, the Bayesian Network classi-
fier (trained with 2239 features, and evaluated with
10-fold cross-validation) achieves an Accuracy of
65.62% (5249 correctly classified instances out of
8000), and a Macro F1 of 78.15%.
FBK NK-RES2. Similar to the first run, but:
? Semantic boundary collocations: m=9, n=3;
? Glosses: handled at the level of lemmas;
? Context features: left, between, and right
context windows of size 4-ALL-1 words re-
spectively (occurrences: 25-10-25).
Run 1000 2000 4000 8000
FBK NK-RES1 55.71 64.06 67.80 68.02
FBK NK-RES2 54.27 63.68 67.08 67.48
FBK NK-RES3 54.25 62.73 66.11 66.90
FBK NK-RES4 44.11 58.85 63.06 65.84
Table 2: Test results (Macro-averaged F1) using
different amounts of training sentences.
Based on the observation of system?s behaviour on
the training data, the objectives of this run were to:
i) add more collocations as features, ii) increase
the importance of terms appearing in the left con-
text, iii) reduce the importance of terms appearing
in the right context, and iv) increase the possibil-
ity of matching the nominals with gloss terms by
considering their respective lemmas. On the train-
ing set, the classifier (trained with 2998 features)
achieves 66.92% Accuracy (5353 correctly classi-
fied instances), and a Macro F1 of 79.56%.
FBK NK-RES3. Similar to the second run, but
considering the synonyms of the most frequent
sense of the words between <e1> and <e2>.
The goal of this run was to generalize the con-
text between nominals, by considering word lem-
mas. On the training set, the classifier (trained
with 2998 features) achieves an Accuracy of
64.94% (5195 correctly classified instances), and
a Macro F1 of 77.38%.
FBK NK-RES4. Similar to the second run, but
considering semantic boundary collocations oc-
curring at least 7 times in the training set (m
param.), for at most 3 relations (n param.).
The goal of this run was to further increase the
number of collocations used as features. On the
training set, the classifier (trained with 6233 fea-
tures) achieves achieves 68.12% Accuracy (5449
correct classifications), and 82.24% Macro F1.
As regards the results on the test set, Table 2 re-
ports the scores achieved by each run using differ-
ent portions of the training set (1000, 2000, 4000,
8000 examples), while Figure 1 shows the learn-
ing curves for each relation of our best run.
4 Discussion and conclusion
As can be seen from Table 2, the results contra-
dict our expectations about the effectiveness of our
less conservative configurations and, in particular,
about the utility of using larger amounts of se-
mantic boundary collocations. The performance
204
Figure 1: Learning curves on the test set
(FBK NK-RES1).
decrease from Run2 to Run4
3
clearly indicates an
overfitting problem. Though suitable to model the
training data, the additional collocations were not
encountered in the test set. This caused a bias to-
wards the Other relation, which reduced the over-
all performance of the system.
Regarding our best run, Figure 1 shows dif-
ferent system?s behaviours with the different tar-
get relations. For some of them (e.g. Entity-
Destination, Cause-Effect) better results are mo-
tivated by the fact that they are often triggered
by frequent unambiguous word patterns (e.g.
?<e1>has been moved to a <e2>?, ?<e1>
causes <e2>?). Such relations are effectively
handled by the context features which, in contrast,
are inadequate for those expressed with high lex-
ical variability. This is particularly evident with
the Other relation, for which the acquired context
features poorly discriminate positive from nega-
tive examples even on the training set.
For some relations additional evidence is suc-
cessfully brought by the WordNet features. For
instance, the good results for Member-Collection
demonstrate the usefulness of the holo-/meronymy
features.
As regards semantic boundary collocations, to
check their effectiveness we performed a post-hoc
analysis of those used in our best run. Such anal-
ysis was done in two ways: i) by counting the
number of collocations acquired on the training
set for each relation R
i
, and ii) by calculating the
ambiguity of each R
i
?s collocation on the train-
3
The only difference between Run2 and Run4 is the addi-
tion of around 4000 semantic boundary collocations, which
lead to an overall 2.4% F1 performance decrease. The de-
crease mainly comes in terms of Recall (from 65.91% in
Run2 to 63.35% in Run4).
ing set (i.e. the average number of other relations
activated by the collocation). The analysis re-
vealed that the top performing relations (Member-
Collection, Entity-Destination, Cause-Effect, and
Content-Container) are those for which we ac-
quired lots of unambiguous collocations. These
findings also explain the poor performance on the
Instrument-Agency and the Other relation. For
Instrument-Agency we extracted the lowest num-
ber of collocations, which were also the most am-
biguous ones. For the Other relation the high am-
biguity of the collocations extracted is not com-
pensated by their huge number (around 50% of the
total collocations acquired).
In conclusion, considering i) the level of pro-
cessing required (only lemmatization), ii) the fact
that WordNet is used as the sole source of knowl-
edge, and iii) the many possible solutions left
unexplored due to time constraints, our results
demonstrate the validity of our approach, de-
spite its simplicity. Future research will focus
on a better use of semantic boundary colloca-
tions, on more refined ways to extract knowledge
from WordNet, and on integrating other knowl-
edge sources (e.g. SUMO, YAGO, Cyc).
Acknowledgments
The research leading to these results has received
funding from the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der Grant Agreement n. 248531 (CoSyne project).
References
B. Beamer, A. Rozovskaya, and R. Girju 2008. Au-
tomatic Semantic Relation Extraction with Multiple
Boundary Generation. Proceedings of The National
Conference on Artificial Intelligence (AAAI).
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-
ney, and D. Yuret 2007. SemEval-2007 task 04:
Classification of semantic relations between nomi-
nals. Proceedings of the 4th Semantic Evaluation
Workshop (SemEval-2007).
I. Hendrickx et al 2010. SemEval-2010 Task 8: Multi-
Way Classification of Semantic Relations Between
Pairs of Nominals Proceedings of the 5th SIGLEX
Workshop on Semantic Evaluation.
D. Moldovan, A. Badulescu 2005. A Semantic Scatter-
ing Model for the Automatic Interpretation of Gen-
itives. Proceedings of The Human Language Tech-
nology Conference (HLT).
205
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 399?407,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Semeval-2012 Task 8:
Cross-lingual Textual Entailment for Content Synchronization
Matteo Negri
FBK-irst
Trento, Italy
negri@fbk.eu
Alessandro Marchetti
CELCT
Trento, Italy
amarchetti@celct.it
Yashar Mehdad
FBK-irst
Trento, Italy
mehdad@fbk.eu
Luisa Bentivogli
FBK-irst
Trento, Italy
bentivo@fbk.eu
Danilo Giampiccolo
CELCT
Trento, Italy
giampiccolo@celct.it
Abstract
This paper presents the first round of the
task on Cross-lingual Textual Entailment for
Content Synchronization, organized within
SemEval-2012. The task was designed to pro-
mote research on semantic inference over texts
written in different languages, targeting at the
same time a real application scenario. Par-
ticipants were presented with datasets for dif-
ferent language pairs, where multi-directional
entailment relations (?forward?, ?backward?,
?bidirectional?, ?no entailment?) had to be
identified. We report on the training and test
data used for evaluation, the process of their
creation, the participating systems (10 teams,
92 runs), the approaches adopted and the re-
sults achieved.
1 Introduction
The cross-lingual textual entailment task (Mehdad et
al., 2010) addresses textual entailment (TE) recog-
nition (Dagan and Glickman, 2004) under the new
dimension of cross-linguality, and within the new
challenging application scenario of content synchro-
nization.
Cross-linguality represents a dimension of the TE
recognition problem that has been so far only par-
tially investigated. The great potential for integrat-
ing monolingual TE recognition components into
NLP architectures has been reported in several ar-
eas, including question answering, information re-
trieval, information extraction, and document sum-
marization. However, mainly due to the absence of
cross-lingual textual entailment (CLTE) recognition
components, similar improvements have not been
achieved yet in any cross-lingual application. The
CLTE task aims at prompting research to fill this
gap. Along such direction, research can now ben-
efit from recent advances in other fields, especially
machine translation (MT), and the availability of: i)
large amounts of parallel and comparable corpora in
many languages, ii) open source software to com-
pute word-alignments from parallel corpora, and iii)
open source software to set up MT systems. We
believe that all these resources can positively con-
tribute to develop inference mechanisms for multi-
lingual data.
Content synchronization represents a challenging
application scenario to test the capabilities of ad-
vanced NLP systems. Given two documents about
the same topic written in different languages (e.g.
Wiki pages), the task consists of automatically de-
tecting and resolving differences in the information
they provide, in order to produce aligned, mutually
enriched versions of the two documents. Towards
this objective, a crucial requirement is to identify the
information in one page that is either equivalent or
novel (more informative) with respect to the content
of the other. The task can be naturally cast as an
entailment recognition problem, where bidirectional
and unidirectional entailment judgments for two text
fragments are respectively mapped into judgments
about semantic equivalence and novelty. Alterna-
tively, the task can be seen as a machine translation
evaluation problem, where judgments about seman-
tic equivalence and novelty depend on the possibility
to fully or partially translate a text fragment into the
other.
399
Figure 1: ?bidirectional?, ?forward?, ?backward? and
?no entailment? judgments for SP/EN CLTE pairs.
The recent advances on monolingual TE on the
one hand, and the methodologies used in Statisti-
cal Machine Translation (SMT) on the other, offer
promising solutions to approach the CLTE task. In
line with a number of systems that model the RTE
task as a similarity problem (i.e. handling similar-
ity scores between T and H as useful evidence to
draw entailment decisions), the standard sentence
and word alignment programs used in SMT offer a
strong baseline for CLTE. However, although repre-
senting a solid starting point to approach the prob-
lem, similarity-based techniques are just approx-
imations, open to significant improvements com-
ing from semantic inference at the multilingual
level (e.g. cross-lingual entailment rules such as
?perro???animal?). Taken in isolation, similarity-
based techniques clearly fall short of providing an
effective solution to the problem of assigning direc-
tions to the entailment relations (especially in the
complex CLTE scenario, where entailment relations
are multi-directional). Thanks to the contiguity be-
tween CLTE, TE and SMT, the proposed task pro-
vides an interesting scenario to approach the issues
outlined above from different perspectives, and large
room for mutual improvement.
2 The task
Given a pair of topically related text fragments (T1
and T2) in different languages, the CLTE task con-
sists of automatically annotating it with one of the
following entailment judgments (see Figure 1 for
Spanish/English examples of each judgment):
? bidirectional (T1?T2 & T1?T2): the two
fragments entail each other (semantic equiva-
lence);
? forward (T1?T2 & T16?T2): unidirectional
entailment from T1 to T2;
? backward (T16?T2 & T1?T2): unidirectional
entailment from T2 to T1;
? no entailment (T16?T2 & T16?T2): there is
no entailment between T1 and T2 in both direc-
tions;
In this task, both T1 and T2 are assumed to be
true statements. Although contradiction is relevant
from an application-oriented perspective, contradic-
tory pairs are not present in the dataset created for
the first round of the task.
3 Dataset description
Four CLTE corpora have been created for the fol-
lowing language combinations: Spanish/English
(SP-EN), Italian/English (IT-EN), French/English
(FR-EN), German/English (DE-EN). The datasets
are released in the XML format shown in Figure 1.
3.1 Data collection and annotation
The dataset was created following the crowdsourc-
ing methodology proposed in (Negri et al, 2011),
which consists of the following steps:
1. First, English sentences were manually ex-
tracted from copyright-free sources (Wikipedia
and Wikinews). The selected sentences repre-
sent one of the elements (T1) of each entail-
ment pair;
2. Next, each T1 was modified through crowd-
sourcing in various ways in order to ob-
tain a corresponding T2 (e.g. introduc-
ing meaning-preserving lexical and syntactic
changes, adding and removing portions of
text);
3. Each T2 was then paired to the original T1,
and the resulting pairs were annotated with one
of the four entailment judgments. In order to
reduce the correlation between the difference
in sentences? length and entailment judgments,
400
only the pairs where the difference between the
number of words in T1 and T2 (length diff ) was
below a fixed threshold (10 words) were re-
tained.1 The final result is a monolingual En-
glish dataset annotated with multi-directional
entailment judgments, which are well dis-
tributed over length diff values ranging from 0
to 9;
4. In order to create the cross-lingual datasets,
each English T1 was manually translated into
four different languages (i.e. Spanish, German,
Italian and French) by expert translators;
5. By pairing the translated T1 with the cor-
responding T2 in English, four cross-lingual
datasets were obtained.
To ensure the good quality of the datasets, all the
collected pairs were manually checked and corrected
when necessary. Only pairs with agreement between
two expert annotators were retained. The final result
is a multilingual parallel entailment corpus, where
T1s are in 5 different languages (i.e. English, Span-
ish, German, Italian, and French), and T2s are in En-
glish. It?s worth mentioning that the monolingual
English corpus, a by-product of our data collection
methodology, will be publicly released as a further
contribution to the research community.2
3.2 Dataset statistics
Each dataset consists of 1,000 pairs (500 for training
and 500 for test), balanced across the four entail-
ment judgments (bidirectional, forward, backward,
and no entailment).
For each language combination, the distribu-
tion of the four entailment judgments according to
length diff is shown in Figure 2. Vertical bars rep-
resent, for each length diff value, the proportion
of pairs belonging to the four entailment classes.
As can be seen, the length diff constraint applied
to the length difference in the monolingual English
1Such constraint has been applied in order to focus as much
as possible on semantic aspects of the problem, by reduc-
ing the applicability of simple association rules such as IF
length(T1)>length(T2) THEN T1?T2.
2The cross-lingual datasets are already available for research
purposes at http://www.celct.it/resourcesList.
php. The monolingual English dataset will be publicly released
to non participants in July 2012.
pairs (step 3 of the creation process) is substantially
reflected in the cross-lingual datasets for all lan-
guage combinations. In fact, as shown in Table 1,
the majority of the pairs is always included in the
same length diff range (approximately [-5,+5]) and,
within this range, the distribution of the four classes
is substantially uniform. Our assumption is that such
data distribution makes entailment judgments based
on mere surface features such as sentence length in-
effective, thus encouraging the development of alter-
native, deeper processing strategies.
SP-EN IT-EN FR-EN DE-EN
Forward 104 132 121 179
Backward 202 182 191 123
No entailment 163 173 169 174
Bidirectional 175 199 193 209
ALL 644 686 674 685
Table 1: CLTE pairs distribution within the -5/+5
length diff range.
4 Evaluation metrics and baselines
Evaluation results have been automatically com-
puted by comparing the entailment judgments re-
turned by each system with those manually assigned
by human annotators. The metric used for systems?
ranking is accuracy over the whole test set, i.e. the
number of correct judgments out of the total number
of judgments in the test set. Additionally, we calcu-
lated precision, recall, and F1 measures for each of
the four entailment judgment categories taken sep-
arately. These scores aim at giving participants the
possibility to gain clearer insights into their system?s
behavior on the entailment phenomena relevant to
the task.
For each language combination, two baselines
considering the length difference between T1 and T2
have been calculated (besides the trivial 0.25 accu-
racy score obtained by assigning each test pair in the
balanced dataset to one of the four classes):
? Composition of binary judgments (Bi-
nary). To calculate this baseline an SVM
classifier is trained to take binary entailment
decisions (?YES?, ?NO?). The classifier uses
length(T1)/length(T2) as a single feature to
check for entailment from T1 to T2, and
length(T2)/length(T1) for the opposite direc-
tion. For each test pair, the unidirectional
401
010
20
30
40
50
60
70
80
-21 -18 -15 -12 -9 -6 -3 0 3 6 9
no_entailmentforwardbidirectionalbackward
(a) SP-EN
0
10
20
30
40
50
60
70
80
-17 -14 -11 -8 -5 -2 1 4 7 10
no_entailmentforwardbidirectionalbackward
(b) IT-EN
0
10
20
30
40
50
60
70
80
90
-21 -18 -15 -12 -9 -6 -3 0 3 6 9 12
no_entailmentforwardbidirectionalbackward
(c) FR-EN
0
10
20
30
40
50
60
70
80
90
-14 -10 -7 -4 -1 2 5 8 11 14 17
no_entailmentforwardbidirectionalbackward
(d) DE-EN
Figure 2: CLTE pairs distribution for different length diff values across all datasets.
judgments returned by the two classifiers are
composed into a single multi-directional judg-
ment (?YES-YES?=?bidirectional?, ?YES-
NO?=?forward?, ?NO-YES?=?backward?,
?NO-NO?=?no entailment?);
? Multi-class classification (Multi-class). A
single SVM classifier is trained with the same
features to directly assign to each pair one of
the four entailment judgments.
Both the baselines have been calculated with the
LIBSVM package (Chang and Lin, 2011), using a
linear kernel with default parameters. Baseline re-
sults are reported in Table 2.
Although the four CLTE datasets are derived from
the same monolingual EN-EN corpus, baseline re-
sults present slight differences due to the effect of
translation into different languages.
SP-EN IT-EN FR-EN DE-EN
1-class 0.25 0.25 0.25 0.25
Binary 0.34 0.39 0.39 0.40
Multi-class 0.43 0.44 0.42 0.42
Table 2: Baseline accuracy results.
5 Submitted runs and results
Participants were allowed to submit up to five runs
for each language combination. A total of 17 teams
registered to participate in the task and downloaded
the training set. Out of them, 12 downloaded the
test set and 10 (including one of the task organizers)
submitted valid runs. Eight teams produced submis-
sions for all the language combinations, while two
teams participated only in the SP-EN task. In total,
92 runs have been submitted and evaluated (29 for
SP-EN, and 21 for each of the other language pairs).
402
Despite the novelty and the difficulty of the problem,
these numbers demonstrate the interest raised by the
task, and the overall success of the initiative.
System name SP-EN IT-EN FR-EN DE-EN
BUAP run1 0.350 0.336 0.334 0.330
BUAP run2 0.366 0.344 0.342 0.268
celi run1 0.276 0.278 0.278 0.280
celi run2 0.336 0.338 0.300 0.352
celi run3 0.322 0.334 0.298 0.350
celi run4 0.268 0.280 0.280 0.274
DirRelCond3 run1 0.300 0.280 0.362 0.336
DirRelCond3 run2 0.300 0.284 0.360 0.336
DirRelCond3 run3 0.300 0.338 0.384 0.364
DirRelCond3 run4 0.344 0.316 0.384 0.374
FBK run1* 0.502 - - -
FBK run2* 0.490 - - -
FBK run3* 0.504 - - -
FBK run4* 0.500 - - -
HDU run1 0.630 0.554 0.564 0.558
HDU run2 0.632 0.562 0.570 0.552
ICT run1 0.448 0.454 0.456 0.460
JU-CSE-NLP run1 0.274 0.316 0.288 0.262
JU-CSE-NLP run2 0.266 0.326 0.294 0.296
JU-CSE-NLP run3 0.272 0.314 0.296 0.264
Sagan run1 0.342 0.352 0.346 0.342
Sagan run2 0.328 0.352 0.336 0.310
Sagan run3 0.346 0.356 0.330 0.332
Sagan run4 0.340 0.330 0.310 0.310
SoftCard run1 0.552 0.566 0.570 0.550
UAlacant run1 LATE 0.598 - - -
UAlacant run2 0.582 - - -
UAlacant run3 LATE 0.510 - - -
UAlacant run4 0.514 - - -
Highest 0.632 0.566 0.570 0.558
Average 0.440 0.411 0.408 0.408
Median 0.407 0.350 0.365 0.363
Lowest 0.274 0.326 0.296 0.296
Table 3: Accuracy results (92 runs) over the 4 lan-
guage combinations. Highest, average, median and low-
est scores are calculated considering the best run for each
team (*task organizers? system).
Accuracy results are reported in Table 3. As can
be seen from the table, overall accuracy scores are
quite different across language pairs, with the high-
est result on SP-EN (0.632), which is considerably
higher than the highest score on DE-EN (0.558).
This might be due to the fact that most of the partic-
ipating systems rely on a ?pivoting? approach that
addresses CLTE by automatically translating T1 in
the same language of T2 (see Section 6). Regard-
ing the DE-EN dataset, pivoting methods might be
penalized by the lower quality of MT output when
German T1s are translated into English.
The comparison with baselines results leads to in-
teresting observations. First of all, while all systems
significantly outperform the lowest 1-class baseline
(0.25), both other baselines are surprisingly hard to
beat. This shows that, despite the effort in keep-
ing the distribution of the entailment classes uni-
form across different length diff values, eliminating
the correlation between sentences? length and cor-
rect entailment decisions is difficult. As a conse-
quence, although disregarding semantic aspects of
the problem, features considering such information
are quite effective.
In general, systems performed better on the SP-
EN dataset, with most results above the binary base-
line (8 out of 10), and half of the systems above the
multi-class baseline. For the other language pairs
the results are lower, with only 3 out of 8 partici-
pants above the two baselines in all datasets. Aver-
age results reflect this situation: the average scores
are always above the binary baseline, whereas only
the SP-EN average result is higher than the multi-
class baseline(0.44 vs. 0.43).
To better understand the behaviour of each sys-
tem (also in relation to the different language com-
binations), Table 4 provides separate precision, re-
call, and F1 scores for each entailment judgment,
calculated over the best runs of each participating
team. Overall, the results suggest that the ?bidi-
rectional? and ?no entailment? categories are more
problematic than ?forward? and ?backward? judg-
ments. For most datasets, in fact, systems? perfor-
mance on ?bidirectional? and ?no entailment? is sig-
nificantly lower, typically on recall. Except for the
DE-EN dataset (more problematic on ?forward?),
also average F1 results on these judgments are lower.
This might be due to the fact that, for all datasets, the
vast majority of ?bidirectional? and ?no entailment?
judgments falls in a length diff range where the dis-
tribution of the four classes is more uniform (see
Figure 2).
Similar reasons can justify the fact that ?back-
ward? entailment results are consistently higher on
all datasets. Compared with ?forward? entailment,
these judgments are in fact less scattered across the
entire length diff range (i.e. less intermingled with
the other classes).
403
6 Approaches
A rough classification of the approaches adopted by
participants can be made along two orthogonal di-
mensions, namely:
? Pivoting vs. Cross-lingual. Pivoting meth-
ods rely on the automatic translation of one of
the two texts (either single words or the en-
tire sentence) into the language of the other
(typically English) in order perform monolin-
gual TE recognition. Cross-lingual methods as-
sign entailment judgments without preliminary
translation.
? Composition of binary judgments vs. Multi-
class classification. Compositional approaches
map unidirectional entailment decisions taken
separately into single judgments (similar to the
Binary baseline in Section 4). Methods based
on multi-class classification directly assign one
of the four entailment judgments to each test
pair (similar to our Multi-class baseline).
Concerning the former dimension, most of the
systems (6 out of 10) adopted a pivoting approach,
relying on Google Translate (4 systems), Microsoft
Bing Translator (1), or a combination of Google,
Bing, and other MT systems (1) to produce English
T2s. Regarding the latter dimension, the composi-
tional approach was preferred to multi-class classi-
fication (6 out of 10). The best performing system
relies on a ?hybrid? approach (combining monolin-
gual and cross-lingual alignments) and a compo-
sitional strategy. Besides the frequent recourse to
MT tools, other resources used by participants in-
clude: on-line dictionaries for the translation of sin-
gle words, word alignment tools, part-of-speech tag-
gers, NP chunkers, named entity recognizers, stem-
mers, stopwords lists, and Wikipedia as an external
multilingual corpus. More in detail:
BUAP [pivoting, compositional] (Vilarin?o et al,
2012) adopts a pivoting method based on translating
T1 into the language of T2 and vice versa (Google
Translate3 and the OpenOffice Thesaurus4). Simi-
larity measures (e.g. Jaccard index) and rules are
3http://translate.google.com/
4http://extensions.services.openoffice.
org/en/taxonomy/term/233
respectively used to annotate the two resulting sen-
tence pairs with entailment judgments and combine
them in a single decision.
CELI [cross lingual, compositional & multi-
class] (Kouylekov, 2012) uses dictionaries for word
matching, and a multilingual corpus extracted from
Wikipedia for term weighting. Word overlap and
similarity measures are then used in different ap-
proaches to the task. In one run (Run 1), they are
used to train a classifier that assigns separate en-
tailment judgments for each direction. Such judg-
ments are finally composed into a single one for each
pair. In the other runs, the same features are used for
multi-class classification.
DirRelCond3 [cross lingual, compositional]
(Perini, 2012) uses bilingual dictionaries (Freedict5
and WordReference6) to translate content words into
English. Then, entailment decisions are taken com-
bining directional relatedness scores between words
in both directions (Perini, 2011).
FBK [cross lingual, compositional & multi-
class] (Mehdad et al, 2012a) uses cross-lingual
matching features extracted from lexical phrase ta-
bles, semantic phrase tables, and dependency rela-
tions (Mehdad et al, 2011; Mehdad et al, 2012b;
Mehdad et al, 2012c). The features are used for
multi-class and binary classification using SVMs.
HDU [hybrid, compositional] (Wa?schle and
Fendrich, 2012) uses a combination of binary clas-
sifiers for each entailment direction. The classifiers
use both monolingual alignment features based on
METEOR (Banerjee and Lavie, 2005) alignments
(translations obtained from Google Translate), and
cross-lingual alignment features based on GIZA++
(Och and Ney, 2000) (word alignments learned on
Europarl).
ICT [pivoting, compositional] (Meng et al,
2012) adopts a pivoting method (using Google
Translate and an in-house hierarchical MT system),
and the open source EDITS system (Kouylekov and
Negri, 2010) to calculate similarity scores between
monolingual English pairs. Separate unidirectional
entailment judgments obtained from binary classi-
fier are combined to return one of the four valid
CLTE judgments.
5http://www.freedict.com/
6http://www.wordreference.com/
404
SP-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
BUAP spa-eng run2 0,337 0,664 0,447 0,406 0,568 0,473 0,333 0,088 0,139 0,391 0,144 0,211
celi spa-eng run2 0,324 0,368 0,345 0,411 0,368 0,388 0,306 0,296 0,301 0,312 0,312 0,312
DirRelCond3 spa-eng run4 0,358 0,608 0,451 0,444 0,448 0,446 0,286 0,032 0,058 0,243 0,288 0,264
FBK spa-eng run3 0,515 0,704 0,595 0,546 0,568 0,557 0,447 0,304 0,362 0,482 0,440 0,460
HDU spa-eng run2 0,607 0,656 0,631 0,677 0,704 0,690 0,602 0,592 0,597 0,643 0,576 0,608
ICT spa-eng run1 0,750 0,240 0,364 0,440 0,472 0,456 0,395 0,560 0,464 0,436 0,520 0,474
JU-CSE-NLP spa-eng run1 0,211 0,288 0,243 0,272 0,296 0,284 0,354 0,232 0,280 0,315 0,280 0,297
Sagan spa-eng run3 0,225 0,200 0,212 0,269 0,224 0,245 0,418 0,448 0,432 0,424 0,512 0,464
SoftCard spa-eng run1 0,602 0,616 0,609 0,650 0,624 0,637 0,471 0,448 0,459 0,489 0,520 0,504
UAlacant spa-eng run1 LATE 0,689 0,568 0,623 0,645 0,728 0,684 0,507 0,544 0,525 0,566 0,552 0,559
AVG. 0,462 0,491 0,452 0,476 0,5 0,486 0,412 0,354 0,362 0,43 0,414 0,415
IT-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
BUAP ita-eng run2 0,324 0,456 0,379 0,327 0,672 0,440 0,538 0,056 0,101 0,444 0,192 0,268
celi ita-eng run2 0,349 0,360 0,354 0,455 0,36 0,402 0,294 0,320 0,307 0,287 0,312 0,299
DirRelCond3 ita-eng run3 0,323 0,488 0,389 0,480 0,288 0,360 0,331 0,368 0,348 0,268 0,208 0,234
HDU ita-eng run2 0,564 0,600 0,581 0,628 0,648 0,638 0,551 0,520 0,535 0,500 0,480 0,490
ICT ita-eng run1 0,661 0,296 0,409 0,554 0,368 0,442 0,427 0,448 0,438 0,383 0,704 0,496
JU-CSE-NLP ita-eng run2 0,240 0,280 0,258 0,339 0,480 0,397 0,412 0,280 0,333 0,359 0,264 0,304
Sagan ita-eng run3 0,306 0,296 0,301 0,252 0,216 0,233 0,395 0,512 0,446 0,455 0,400 0,426
SoftCard ita-eng run1 0,602 0,616 0,609 0,617 0,696 0,654 0,560 0,448 0,498 0,481 0,504 0,492
AVG. 0,421 0,424 0,410 0,457 0,466 0,446 0,439 0,369 0,376 0,397 0,383 0,376
FR-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
BUAP fra-eng run2 0,447 0,272 0,338 0,291 0,760 0,420 0,250 0,016 0,030 0,449 0,320 0,374
celi fra-eng run2 0,316 0,296 0,306 0,378 0,360 0,369 0,270 0,296 0,282 0,244 0,248 0,246
DirRelCond3 fra-eng run3 0,393 0,576 0,468 0,441 0,512 0,474 0,387 0,232 0,290 0,278 0,216 0,243
HDU fra-eng run2 0,564 0,672 0,613 0,582 0,736 0,650 0,676 0,384 0,490 0,500 0,488 0,494
ICT fra-eng run1 0,750 0,192 0,306 0,517 0,496 0,506 0,385 0,656 0,485 0,444 0,480 0,462
JU-CSE-NLP fra-eng run3 0,215 0,208 0,211 0,289 0,296 0,292 0,341 0,496 0,404 0,333 0,184 0,237
Sagan fra-eng run1 0,244 0,168 0,199 0,297 0,344 0,319 0,394 0,568 0,466 0,427 0,304 0,355
SoftCard fra-eng run1 0,551 0,608 0,578 0,649 0,696 0,672 0,560 0,488 0,521 0,513 0,488 0,500
AVG. 0,435 0,374 0,377 0,431 0,525 0,463 0,408 0,392 0,371 0,399 0,341 0,364
DE-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
BUAP deu-eng run1 0,395 0,120 0,184 0,248 0,224 0,235 0,344 0,688 0,459 0,364 0,288 0,321
celi deu-eng run2 0,347 0,416 0,378 0,402 0,392 0,397 0,339 0,312 0,325 0,319 0,288 0,303
DirRelCond3 deu-eng run4 0,429 0,312 0,361 0,408 0,552 0,469 0,367 0,320 0,342 0,298 0,312 0,305
HDU deu-eng run1 0,559 0,528 0,543 0,600 0,696 0,644 0,540 0,488 0,513 0,524 0,520 0,522
ICT deu-eng run1 0,718 0,224 0,341 0,493 0,552 0,521 0,390 0,512 0,443 0,439 0,552 0,489
JU-CSE-NLP deu-eng run2 0,182 0,048 0,076 0,307 0,496 0,379 0,315 0,560 0,403 0,233 0,080 0,119
Sagan deu-eng run1 0,250 0,168 0,201 0,239 0,256 0,247 0,405 0,600 0,484 0,443 0,344 0,387
SoftCard deu-eng run1 0,568 0,568 0,568 0,611 0,640 0,625 0,521 0,488 0,504 0,496 0,504 0,500
AVG. 0,431 0,298 0,332 0,414 0,476 0,440 0,403 0,496 0,434 0,390 0,361 0,368
Table 4: precision, recall and F1 scores, calculated for each team?s best run for all the language combinations.
JU-CSE-NLP [pivoting, compositional] (Neogi
et al, 2012) uses Microsoft Bing translator7 to pro-
duce monolingual English pairs. Separate lexical
mapping scores are calculated (from T1 to T2 and
vice-versa) considering different types of informa-
tion and similarity metrics. Binary entailment de-
7http://www.microsofttranslator.com/
cisions are then heuristically combined into single
decisions.
Sagan [pivoting, multi-class] (Castillo and Car-
denas, 2012) adopts a pivoting method using Google
Translate, and trains a monolingual system based on
a SVM multi-class classifier. A CLTE corpus de-
rived from the RTE-3 dataset is also used as a source
of additional training material.
405
SoftCard [pivoting, multi-class] (Jimenez et al,
2012) after automatic translation with Google Trans-
late, uses SVMs to learn entailment decisions based
on information about the cardinality of: T1, T2, their
intersection and their union. Cardinalities are com-
puted in different ways, considering tokens in T1 and
T2, their IDF, and their similarity (computed with
edit-distance)
UAlacant [pivoting, multi-class] (Espla`-Gomis
et al, 2012) exploits translations obtained from
Google Translate, Microsoft Bing translator, and the
Apertium open-source MT platform (Forcada et al,
2011).8 Then, a multi-class SVM classifier is used
to take entailment decisions using information about
overlapping sub-segments as features.
7 Conclusion
Despite the novelty of the problem and the diffi-
culty to capture multi-directional entailment rela-
tions across languages, the first round of the Cross-
lingual Textual Entailment for Content Synchroniza-
tion task organized within SemEval-2012 was a suc-
cessful experience. This year a new interesting chal-
lenge has been proposed, a benchmark for four lan-
guage combinations has been released, baseline re-
sults have been proposed for comparison, and a
monolingual English dataset has been produced as
a by-product which can be useful for monolingual
TE research. The interest shown by participants
was encouraging: 10 teams submitted a total of 92
runs for all the language pairs proposed. Overall,
the results achieved on all datasets are encourag-
ing, with best systems significantly outperforming
the proposed baselines. It is worth observing that the
nature of the task, which lies between semantics and
machine translation, led to the participation of teams
coming from both these communities, showing in-
teresting opportunities for integration and mutual
improvement. The proposed approaches reflect this
situation, with teams traditionally working on MT
now dealing with entailment, and teams tradition-
ally participating in the RTE challenges now dealing
with cross-lingual alignment techniques. Our ambi-
tion, for the future editions of the CLTE task, is to
further consolidate the bridge between the semantics
and MT communities.
8http://www.apertium.org/
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853). The
authors would also like to acknowledge Giovanni
Moretti from CELCT for evaluation scripts and
technical assistance, and the volunteer translators
that contributed to the creation of the dataset:
Mar??a Sol Accossato, Laura Barthe?le?my, Clau-
dia Biacchi, Jane Brendler, Amandine Chantrel,
Hanna Cheda Patete, Ellen Clancy, Rodrigo Damian
Tejeda, Daniela Dold, Valentina Frattini, Debora
Hedy Amato, Geniz Hernandez, Be?ne?dicte Jean-
nequin, Beate Jones, Anne Kauffman, Marcia Laura
Zanoli, Jasmin Lewis, Alicia Lo?pez, Domenico Los-
eto, Sabrina Luja?n Sa?nchez, Julie Mailfait, Gabriele
Mark, Nunzio Pruiti, Lourdes Rey Cascallar, Sylvie
Martlew, Aleane Salas Velez, Monica Scalici, An-
dreas Schwab, Marianna Sicuranza, Chiara Sisler,
Stefano Tordazzi, Yvonne.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 65?72.
Julio Castillo and Marina Cardenas. 2012. Sagan: A
Cross Lingual Textual Entailment system based on
Machine Traslation. In Proceedings of the 6th Inter-
national Workshop on Semantic Evaluation (SemEval
2012).
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
Ido Dagan and Oren Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic Applied Modeling of Lan-
guage Variability. In Proceedings of the PASCAL
Workshop of Learning Methods for Text Understand-
ing and Mining.
Miquel Espla`-Gomis, Felipe Sa?nchez-Mart??nez, and
Mikel L. Forcada. 2012. UAlacant: Using Online
Machine Translation for Cross-Lingual Textual Entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012).
Mikel L. Forcada, Ginest??-Rosell Mireia, Nordfalk Jacob,
O?Regan Jim, Ortiz-Rojas Sergio, Pe?rez-Ortiz Juan A.,
Sa?nchez-Mart??nez Felipe, Ram??rez-Sa?nchez Gema,
406
and Tyers Francis M. 2011. Apertium: a Free/Open-
Source Platform for Rule-Based Machine Translation.
Machine Translation, 25(2):127?144. Special Issue:
Free/Open-Source Machine Translation.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft Cardinality + ML: Learning Adap-
tive Similarity Functions for Cross-lingual Textual En-
tailment. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval 2012).
Milen Kouylekov and Matteo Negri. 2010. An open-
source package for recognizing textual entailment. In
Proceedings of the ACL 2010 System Demonstrations.
Milen Kouylekov. 2012. CELI: An Experiment with
Cross Language Textual Entailment. In Proceedings
of the 6th International Workshop on Semantic Evalu-
ation (SemEval 2012).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment. In
Proceedings of the 11th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL HLT 2010).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using Bilingual Parallel Corpora for Cross-
Lingual Textual Entailment. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL HLT 2011).
Yashar Mehdad, Matteo Negri, and Jose? G. C. de Souza.
2012a. FBK: Cross-Lingual Textual Entailment With-
out Translation. In Proceedings of the 6th Interna-
tional Workshop on Semantic Evaluation (SemEval
2012).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012b. Detecting Semantic Equivalence and Informa-
tion Disparity in Cross-lingual Documents. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012c. Match without a Referee: Evaluating MT Ade-
quacy without Reference Translations. In Proceedings
of the 7th Workshop on Statistical Machine Translation
(WMT 2012).
Fandong Meng, Hao Xiong, and Qun Liu. 2012. ICT:
A Translation based Cross-lingual Textual Entailment.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012).
Matto Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and Conquer: Crowdsourcing the Creation of
Cross-Lingual Textual Entailment Corpora. Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2011).
Snehasis Neogi, Partha Pakray, Sivaji Bandyopadhyay,
and Alexander Gelbukh. 2012. JU-CSE-NLP: Lan-
guage Independent Cross-lingual Textual Entailment
System. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012).
Franz J. Och and Hermann Ney. 2000. Improved Sta-
tistical Alignment Models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL 2000).
Alpa?r Perini. 2011. Detecting textual entailment
with conditions on directional text relatedness scores.
Studia Universitatis Babes-Bolyai Series Informatica,
LVI(2):13?18.
Alpa?r Perini. 2012. DirRelCond3: Detecting Textual
Entailment Across Languages With Conditions On Di-
rectional Text Relatedness Scores. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012).
Darnes Vilarin?o, David Pinto, Mireya Tovar, Saul Leo?n,
and Esteban Castillo. 2012. BUAP: Lexical and
Semantic Similarity for Cross-lingual Textual Entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012).
Katharina Wa?schle and Sascha Fendrich. 2012. HDU:
Cross-lingual Textual Entailment with SMT Features.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012).
407
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 624?630,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
FBK: Machine Translation Evaluation and Word Similarity metrics
for Semantic Textual Similarity
Jose? Guilherme C. de Souza
Fondazione Bruno Kessler
University of Trento
Povo, Trento, Italy
desouza@fbk.eu
Matteo Negri
Fondazione Bruno Kessler
Povo, Trento
Italy
negri@fbk.eu
Yashar Mehdad
Fondazione Bruno Kessler
Povo, Trento
Italy
mehdad@fbk.eu
Abstract
This paper describes the participation of FBK
in the Semantic Textual Similarity (STS) task
organized within Semeval 2012. Our ap-
proach explores lexical, syntactic and se-
mantic machine translation evaluation metrics
combined with distributional and knowledge-
based word similarity metrics. Our best
model achieves 60.77% correlation with hu-
man judgements (Mean score) and ranked 20
out of 88 submitted runs in the Mean rank-
ing, where the average correlation across all
the sub-portions of the test set is considered.
1 Introduction
The Semantic Textual Similarity (STS) task pro-
posed at SemEval 2012 consists of examining the
degree of semantic equivalence between two sen-
tences and assigning a score to quantify such sim-
ilarity ranging from 0 (the two texts are about dif-
ferent topics) to 5 (the two texts are semantically
equivalent). The complete description of the task,
the datasets and the evaluation methodology adopted
can be found in (Agirre et al, 2012).
Typical approaches to measure semantic textual
similarity exploit information at the lexical level.
The proposed solutions range from calculating the
overlap of common words between the two text seg-
ments (Salton et al, 1997) to the application of
knowledge-based and corpus-based word similarity
metrics to cope with the low recall achieved by on
simple lexical matching (Mihalcea et al, 2006).
Our participation in the STS task is inspired by
previous work on paraphrase recognition, in which
machine translation (MT) evaluation metrics are
used to identify whether a pair of sentences are
semantically equivalent or not (Finch and Hwang,
2005; Wan et al, 2006). Our approach to semantic
textual similarity makes use of not only lexical in-
formation but also syntactic and semantic informa-
tion. To this aim, our metrics are based on different
natural language processing tools that provide syn-
tactic and semantic annotation. These include shal-
low parsing, constituency parsing, dependency pars-
ing, semantic roles labeling, discourse representa-
tion analyzer, and named entities recognition. In ad-
dition, we employed distributional and knowledge-
based word similarity metrics in an attempt to im-
prove the results given by the MT metrics. The com-
puted scores are used as features to train a regression
model in a supervised learning framework.
Our best run model achieves 60.77% correlation
with human judgements when evaluating the seman-
tic similarity of texts from the entire test set and
was ranked in the 20th position (out of 88 submit-
ted runs) in the Mean ranking.
2 System Description
The system has been designed following a ma-
chine learning based approach in which a regres-
sion model is induced using different shallow and
deep linguistic features extracted from the datasets.
The STS training corpora are first preprocessed us-
ing different tools that annotate the texts at different
levels. Using the preprocessed data, the features are
extracted for each pair and used to train a model that
will be applied to unseen test pairs. The training
set is composed by three datasets (MSRpar, MSRvid
and SMTeuroparl) which combined contain a total
of 2234 instances. The test data is composed by a
different sample of the same three datasets plus in-
stances derived from two additional corpora (OnWN
624
and SMTnews). The datasets construction and anno-
tation are described in (Agirre et al, 2012).
Our system exploits two sets of features which re-
spectively build on MT evaluation metrics (2.1) and
word similarity metrics (2.2). The whole feature set
is summarized in figure 1.
2.1 Machine Translation Evaluation Metrics
MT evaluation metrics are designed to assess
whether the output of a MT system is semantically
equivalent to a set of reference translations. The
MT evaluation metrics described in this section, im-
plemented in the Asiya Open Toolkit for Automatic
Machine Translation (Meta-) Evaluation1 (Gime?nez
and Ma`rquez, 2010) are used to extract features at
different linguistic levels: lexical, syntactic and se-
mantic. For the syntactic and semantic levels, Asiya
calculates similarity measures based on the linguis-
tic elements provided by each kind of annotation.
Linguistic elements are defined as ?the linguistic
units, structures, or relationships? (Gime?nez, 2008)
(e.g. dependency relations, discourse relations,
named entities, part-of-speech tags, among others).
(Gime?nez, 2008) defines two simple measures us-
ing the linguistic elements of a given linguistic level:
overlapping and matching. Overlapping is a
measure of the proportion of items inside the lin-
guistic elements of a certain type shared by both
texts. Matching is defined in the same way with
the difference that the order between the items inside
a linguistic element is taken into consideration. That
is, the items of a linguistic element are concatenated
in a single unit from left to right.
2.1.1 Lexical Level
At the lexical level we explored different n-gram
and edit distance based metrics. The difference
among them is in the way each algorithm calcu-
lates the lexical similarity, which yields to differ-
ent results. We used the following n-gram-based
metrics: BLEU (Papineni et al, 2002), NIST (Dod-
dington, 2002), ROUGE (Lin and Och, 2004), GTM
(Melamed et al, 2003), METEOR (Banerjee and
Lavie, 2005). Besides those, we also used metrics
based on edit distance. Such metrics calculate the
number of edit operations (e.g. insertions, deletions,
and substitutions) necessary to transform one text
1http://nlp.lsi.upc.edu/asiya/
into the other (the lower the number of edit oper-
ations, the higher the similarity score). The edit-
distance-based metrics used were: WER (Nie? en et
al., 2000), PER (Tillmann et al, 1997), TER (Snover
et al, 2006) and TER-Plus (Snover et al, 2009). The
lexical metrics form a group of metrics that we here-
after call lex.
2.1.2 Syntactic Level
The syntactic level was explored by running con-
stituency parsing (cp), dependency parsing (dp),
and shallow parsing (sp). Constituency trees were
produced by the Max-Ent reranking parser (Char-
niak, 2005). The constituency parse trees were
exploited by using three different classes of met-
rics that were designed to calculate the similarities
between the trees of two texts: overlapping in
function of a given part-of-speech; matching in
function of a given constituency type; and syntactic
tree matching (STM) metric proposed by (Liu and
Gildea, 2005).
Dependency trees were obtained using MINI-
PAR (Lin, 2003). Two types of metrics were used
to calculate the similarity between two texts using
dependency trees. In the first, different similarity
measures were calculated taking into consideration
three different perspectives: overlap of words that
hang in the same level or in a deeper level of the
dependency tree; overlap between words that hang
directly from terminal nodes given a specified part-
of-speech; and overlap between words that are ruled
by non-terminal nodes given a specified grammat-
ical relation (subject, object, relative clause, among
others). The second type is an implementation of the
head-word chain matching introduced in (Liu and
Gildea, 2005).
The shallow syntax approach proposed by
(Gime?nez, 2008) uses three different tools to ex-
plore the parts-of-speech, word lemmas and base
phrases chunks, respectively: SVMTool (Gime?nez
and Ma`rquez, 2004), Freeling (Carreras et al, 2004)
and Phreco (Carreras et al, 2005). In this type of
metrics the idea is to measure the similarity between
the two texts using parts-of-speech and chunk types.
The following metrics were used: overlapping
according to the part-of-speech; overlapping ac-
cording to the chunk type; the accumulated NIST
metric (Doddington, 2002) scores over different
625
Figure 1: A summary of the class of features explored.
sequences (lemmas, parts-of-speech, base phrase
chunks and chunk IOB labels).
2.1.3 Semantic Level
At the semantic level we aplored three different
types of information, namely: discourse represen-
tations, named entities and semantic roles. Here-
after they are respectively referred to as dr, ne, and
sr features. The discourse relations are automat-
ically annotated using the C&C Tools (Clark and
Curran, 2004). The following metrics using seman-
tic tree representations were proposed by (Gime?nez,
2008). A metric similar to the STM in which se-
mantic trees are used instead of constituency trees;
the overlapping between discourse representa-
tion structures according to their type; and the mor-
phosyntactic overlapping of discourse represen-
tation structures that share the same type.
Named entities metrics are calculated by com-
paring the entities that appear in each text. The
named entities were annotated using the BIOS pack-
age (Surdeanu et al, 2005). Two types of metrics
were used: the overlapping between the named
entities in each sentence according to their type and
the matching between the named entities in func-
tion of their type.
Semantic roles were automatically annotated us-
ing the SwiRL package (Surdeanu and Turmo,
2005). The arguments and adjuncts annotated in
each sentence are compared according to three dif-
ferent metrics: overlapping between the seman-
tic roles according to their type; the matching be-
tween the semantic roles according to their type; and
the overlapping of the roles without taking into
consideration their lexical realization.
2.2 Word Similarity Metrics
Besides the MT evaluation metrics, we experi-
mented with lexical semantics by calculating word
similarity metrics. For that, we followed a distri-
butional and a knowledge-based word similarity ap-
proach.
2.2.1 Distributional Word Similarity
As some previous work on semantic textual tex-
tual similarity (Mihalcea et al, 2006) and textual
entailment (Kouylekov et al, 2010; Mehdad et al,
2010) have shown, distributional word similarity
measures can improve the performance of both tasks
by allowing matches between terms that are lexically
different. We measure the word similarity comput-
ing a set of Latent Semantic Analysis (LSA) metrics
over Wikipedia. The 200,000 most visited articles
of Wikipedia were extracted and cleaned to build the
626
term-by-document matrix using the jLSI tool2.
Using this model we designed three different sim-
ilarity metrics that compute the similarity between
all elements in one text with all elements in the other
text. For two metrics we calculate the similarities
between different parts-of-speech: (i) similarity over
nouns and adjectives, and (ii) similarity over verbs.
The third metric computes the similarity between
all words in the two sentences. The similarity is
computed by averaging the pairwise similarity using
the LSA model between the elements of each text.
These metrics are hereafter called lsa.
2.2.2 Knowledge-based Word Similarity
In order to incorporate world knowledge informa-
tion about entities (persons, organizations, locations,
among others) into our model we experimented with
knowledge-based (thesaurus-based) word similarity
metrics. Usually such approaches have a very lim-
ited coverage of concepts due to the reduced size of
the available thesauri. In order to increase the cov-
erage we extracted concepts from the YAGO2 se-
mantic knowledge base (Hoffart et al, 2011) derived
from Wikipedia, Wordnet (Miller, 1995) and Geon-
ames3. YAGO2 contains knowledge about 10 mil-
lion entities and more than 120 million facts about
these entities.
In order to link the entities in the text to the enti-
ties in YAGO2 we have used ?The Wiki Machine?
(TWM) tool4. The tool solves the linking problem
by disambiguating each entity mention in the text
(excluding pronouns) using Wikipedia to provide the
sense inventory and the training data (Giuliano et
al., 2009). After preprocessing the datasets with
TWM the entities are annotated with their respective
Wikipedia entries represented by their URLs. Using
the entity?s URL it is possible to retrieve the Word-
net synsets related to the entity?s entry in YAGO2
and explore different knowledge-based metrics to
compute word similarity between entities.
In our experiments we selected three differ-
ent algorithms to calculate word similarity using
YAGO2: Wu-Palmer (Zhibiao and Palmer, 1994),
the Leacock-Chodorow (Leacock et al, 1998) and
2http://hlt.fbk.eu/en/technology/jlsi
3http://www.geonames.org/
4http://thewikimachine.fbk.eu/html/
index.html
the path distance (score based on the shortest path
that connects the senses in the Wordnet hyper-
nym/hyponym taxonomy). Two classes of metrics
were designed: (i) the average of the similarity be-
tween all the entities in each sentence and (ii) the
similarity of the pair of elements which have the
shortest path in the Wordnet taxonomy among all
possible pairs. There are six different metrics using
the three algorithms in total. An extra metric was
designed using only TWM. The metric is calculated
by taking the number of common entities in the two
sentences divided by the total number of entities an-
notated in the two sentences. The metrics described
in this section are part of the yago group.
3 Experiments and Discussion
In this section we present our experiments settings,
the configuration of the runs submitted and discuss
the results obtained. All our experiments were made
using half of the training set for training and half
for testing (development). Ten different random-
izations were run over the training data in order
to obtain ten different pairs of train/development
sets and reduce overfitting. We tried several differ-
ent regression algorithms and the best performance
was achieved with the implementation of Support
Vector Machines (SVM) of the SVMLight package
(Joachims, 1998). We used the radial basis function
kernel with default parameters without any special
tuning for the different datasets.
3.1 Submitted Runs and Results
Based on the results achieved with different feature
sets over training data we have selected the best
combinations for our submission. The feature sets
for each run are:
Run 1: lex, lsa, yago, and a selection of
features in the cp, dp, sp, dr, ne and sr
groups, forming a total of 286 features.
Run 2: lex, lsa, and yago, in a total of 50
features.
Run 3: lex and lsa, forming a total of 43
features.
The results obtained by our three submitted runs
are summarized in table 1. The table reports the
627
Runs submitted
Run 1 Run 2 Run 3 Base PE
Development 0.885 0.863 0.859 - -
Test
MSp 0.249 0.512 0.516 0.433 0.577
MSv 0.611 0.780 0.777 0.299 0.818
SMTe 0.149 0.379 0.441 0.454 0.450
Wn 0.421 0.622 0.629 0.586 0.629
SMTn 0.243 0.547 0.608 0.390 0.608
All 0.563 0.643 0.651 0.310 0.789
Allnrm 0.712 0.808 0.810 0.673 0.633
Mean 0.362 0.588 0.607 0.435 0.829
Table 1: Results of each run for each dataset (MSRpar,
MSRvid, SMTeuroparl, OnWn, SMTnews) calculated
with the Pearson correlation between the system?s out-
puts and the gold standard annotation. Official scores ob-
tained using the three evaluation scores All, Allnrm and
Mean. Development row presents the average results for
each run in the whole training dataset. Base is the of-
ficial baseline system. Post Evaluation is the experiment
ran after the evaluation period with models trained for the
specific datasets.
Pearson correlation between the system output and
the gold standard annotation provided by the task or-
ganizers. The table also presents the official scores
used to rank the systems and described in (Agirre et
al., 2012). Our best model, Run 3, was ranked 20th
according to the Mean score, 25th according to the
RankNrm score and 32th according to the All score
among 88 submitted runs.
The ?Development? row reports the results of our
three best models in the development phase. The
results obtained for the three training datasets are
higher than the results obtained for the testing. One
hypothesis that might explain this behavior is over-
fitting during the training phase due to the way we
divided the training set and carried out the experi-
ments. A different experiment setting to carry out
the development should be tried to evaluate this hy-
pothesis.
To our surprise, in the test datasets the results of
Run 1 and Run 3 swapped positions: in the train-
ing setting Run 1 was the best model and Run 3 the
third best. The performance of Run 3 was relatively
stable across the five datasets ranging from about
the 30th to the 48th position the exception being
the SMTnews dataset. In this dataset Run 3 was the
best performing run of the evaluation exercise (and
Run 2 the second). One possible explanation for this
behavior is the fact that Run 3 is based on lexical
features that do not take into consideration the syn-
tactic structure of the two texts and therefore is not
penalized by the noise introduced by the texts gen-
erated by MT systems. This hypothesis, however,
does not explain why Run 3 score for the SMTeu-
roparl dataset was below the baseline score. Error
analysis of the effects of different group of features
in the test datasets is required to better understand
such behaviors.
3.2 Post-evaluation Experiments
After the evaluation period, as a first step towards
the required error analysis and a better comprehen-
sion of the potential of our approach, we performed
an experiment to assess the impact of having mod-
els trained for specific datasets. In this experiment,
each training dataset (MSRpar, MSRvid and SMTeu-
roparl) was used to train a model. Each dataset?s
model was tested on its respective test dataset. The
model for the surprise datasets (OnWn and SMT-
news) were trained using the whole training dataset.
We used the Run 3 feature set (the best run in the
official evaluation). The results of the experiment
are reported in the column ?Exp? of table 1. The
impact of having specific models for each dataset
is high. The Mean score goes from .607 to .829
and improvements are also observed in the All score
(0.789). These scores would rank our system at the
7th position in the Mean rank. However, it is impor-
tant to notice that in a real-world setting, knowledge
about the source of data is not always available. We
consider that having a general model that does not
rely on this kind of information represents a more re-
alistic way to confront with real-world applications.
4 Final Remarks
In this paper we described FBK?s participation in
the STS Semeval 2012 task. Our approach is based
on a combination of MT evaluation metrics, distri-
butional, and knowledge-based word similarity met-
rics. Our best run achieved the 20th position among
88 runs in the Mean overall ranking. An error analy-
sis of the problematic test pairs is required to under-
stand the potential of our feature sets and improve
the overall performance of our approach. Along this
direction, a first experiment with our best features
and a different strategy already led to significant im-
provements in the Mean and All scores (from .651 to
628
.789 and from .607 to .829, respectively).
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853).
The authors would like to thank Claudio Giuliano
for kindly helping us to preprocess the datasets with
the Wiki Machine.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonza-
lez. 2012. SemEval-2012 Task 6: A Pilot on Semantic
Textual Similarity. In 6th International Workshop on
Semantic Evaluation (SemEval 2012), in conjunction
with the First Joint Conference on Lexical and Com-
putational Semantics (*SEM 2012).
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In ACL 2005
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for MT and/or Summarization.
Xavier Carreras, Isaac Chao, Llu??s Padro, and Muntsa
Padro?. 2004. Freeling: An open-source suite of lan-
guage analyzers. In 4th International Conference on
Language Resources and Evaluation (LREC), pages
239?242.
Xavier Carreras, Llu??s Ma`rquez, and Jorge Catro. 2005.
Filtering-Ranking Perceptron Learning. Machine
Learning, 60:41?75.
Eugene Charniak. 2005. Coarse-to-fine n-best parsing
and MaxEnt discriminative reranking. In Proceedings
of the 43rd Annual Meeting on, volume 1, pages 173?
180.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In ACL ?04
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the Second Interna-
tional Conference on Human Language Technology
Research, pages 138?145. Morgan Kaufmann Publish-
ers Inc.
Andrew Finch and YS Hwang. 2005. Using ma-
chine translation evaluation techniques to determine
sentence-level semantic equivalence. In Third Inter-
national Workshop on Paraphrasing, pages 17?24.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A
general POS tagger generator based on Support Vector
Machines. In 4th International Conference on Lan-
guage Resources and Evaluation (LREC), pages 43?
46.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-) Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, (94):77?86.
J. Gime?nez. 2008. Empirical Machine Translation and
its Evaluation. Ph.D. thesis.
Claudio Giuliano, Alfio Massimiliano Gliozzo, and Carlo
Strapparava. 2009. Kernel methods for minimally su-
pervised wsd. Computational Linguistics, 35(4):513?
528.
Johannes Hoffart, Fabian M. FM Suchanek, Klaus
Berberich, Edwin Lewis Kelham, Gerard de Melo, and
Gerhard Weikum. 2011. YAGO2: Exploring and
Querying World Knowledge in Time, Space, Context,
and Many Languages. In 20th International World
Wide Web Conference (WWW 2011), pages 229?232.
Thorsten Joachims. 1998. Making Large-Scale SVM
Learning Practical. In Bernhard Scholkopf, Christo-
pher J. C. Burges, and Alexander J. Smola, editors,
Advances in Kernel Methods - Support Vector Learn-
ing, pages 41?56. MIT Press, Cambridge, USA.
Milen Kouylekov, Yashar Mehdad, and Matteo Negri.
2010. Mining Wikipedia for Large-Scale Reposito-
ries of Context-Sensitive Entailment Rules. In Seventh
international conference on Language Resources and
Evaluation (LREC 2010), pages 3550?3553, La Val-
letta, Malta.
Claudia Leacock, George A. Miller, and Martin
Chodorow. 1998. Using corpus statistics and Word-
Net relations for sense identification. Computational
Linguistics, 24(1):147?166.
C.Y. Lin and F.J. Och. 2004. Automatic evaluation
of machine translation quality using longest common
subsequence and skip-bigram statistics. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, page 605. Association for
Computational Linguistics.
Dekang Lin. 2003. Dependency-Based Evaluation of
Minipar. Text, Speech and Language Technology,
20:317?329.
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In ACL Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization, num-
ber June, pages 25?32.
Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-
simo Zanzotto. 2010. Syntactic/semantic structures
for textual entailment recognition. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the ACL, number June,
pages 1020?1028.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation. In
629
Proceedings of the Joint Conference on Human Lan-
guage Technology and the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL).
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of the
American Association for Artificial Intelligence, pages
775?780.
George A. Miller. 1995. WordNet: A Lexical Database
for English. Communications of the ACM, 38(11):39?
41.
Sonja Nie? en, Franz Josef Och, Gregor Leusch, and Her-
mann Ney. 2000. An evaluation tool for machine
translation: Fast evaluation for MT research. In Lan-
guage Resources and Evaluation, pages 0?6.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), number July, pages 311?318.
Gerard Salton, Amit Singhal, and Mandar Mitra. 1997.
Automatic text structuring and summarization. Infor-
mation Processing &amp;, 33(2):193?207.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Association for Machine Translation in the
Americas.
Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. TER-Plus: paraphrase, se-
mantic, and alignment enhancements to Translation
Edit Rate. Machine Translation, 23(2-3):117?127,
December.
Mihai Surdeanu and Jordi Turmo. 2005. Semantic
role labeling using complete syntactic analysis. In
9th Conference on Computational Natural Language
Learning (CoNLL), number June, pages 221?224.
Mihai Surdeanu, Jordi Turmo, and Eli Comelles. 2005.
Named Entity Recognition from Spontaneous Open-
domain Speech. In 9th International Conference on
Speech Communication and Technology (Interspeech),
pages 3433?3436.
C Tillmann, S Vogel, H Ney, A. Zubiaga, and H. Sawaf.
1997. Accelerated DP Based Search for Statistical
Translation. In Fifth European Conference on Speech
Communication and Technology, pages 2667?2670.
Stephen Wan, Mark Dras, Robert Dale, and Ce?cile Paris.
2006. Using Dependency-Based Features to Take the
?Para-farce? out of Paraphrase. In 2006 Australasian
Language Technology Workshop (ALTW2006), num-
ber 2005, pages 131?138.
Wu Zhibiao and Martha Palmer. 1994. Verb Seman-
tics and Lexical Selection. In ACL ?94 Proceedings
of the 32nd annual meeting on Association for Com-
putational Linguistics, pages 133?138.
630
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 701?705,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
FBK: Cross-Lingual Textual Entailment Without Translation
Yashar Mehdad
FBK-irst
Trento , Italy
mehdad@fbk.eu
Matteo Negri
FBK-irst
Trento , Italy
negri@fbk.eu
Jose? Guilherme C. de Souza
FBK-irst & University of Trento
Trento, Italy
desouza@fbk.eu
Abstract
This paper overviews FBK?s participation
in the Cross-Lingual Textual Entailment
for Content Synchronization task organized
within SemEval-2012. Our participation is
characterized by using cross-lingual matching
features extracted from lexical and semantic
phrase tables and dependency relations. The
features are used for multi-class and binary
classification using SVMs. Using a combi-
nation of lexical, syntactic, and semantic fea-
tures to create a cross-lingual textual entail-
ment system, we report on experiments over
the provided dataset. Our best run achieved
an accuracy of 50.4% on the Spanish-English
dataset (with the average score and the me-
dian system respectively achieving 40.7% and
34.6%), demonstrating the effectiveness of a
?pure? cross-lingual approach that avoids in-
termediate translations.
1 Introduction
So far, cross-lingual textual entailment (CLTE)
(Mehdad et al, 2010) has been applied to: i)
available TE datasets (?YES?/?NO? uni-directional
relations between monolingual pairs) transformed
into their cross-lingual counterpart by translating
the hypotheses into other languages (Negri and
Mehdad, 2010), and ii) machine translation evalu-
ation datasets (Mehdad et al, 2012b). The content
synchronization task represents a challenging appli-
cation scenario to test the capabilities of CLTE sys-
tems, by proposing a richer inventory of phenomena
(i.e. ?Bidirectional?/?Forward?/?Backward?/?No
entailment? multi-directional entailment relations).
Multi-directional CLTE recognition can be seen
as the identification of semantic equivalence and in-
formation disparity between two topically related
sentences, at the cross-lingual level. This is a core
aspect of the multilingual content synchronization
task, which represents a challenging application sce-
nario for a variety of NLP technologies, and a shared
research framework for the integration of semantics
and MT technology.
The CLTE methods proposed so far adopt either
a ?pivoting approach? (translation of the two in-
put texts into the same language, as in (Mehdad et
al., 2010)), or an ?integrated solution? that exploits
bilingual phrase tables to capture lexical relations
and contextual information (Mehdad et al, 2011).
The promising results achieved with the integrated
approach still rely on phrasal matching techniques
that disregard relevant semantic aspects of the prob-
lem. By filling this gap integrating linguistically
motivated features, in our participation, we propose
an approach that combines lexical, syntactic and se-
mantic features within a machine learning frame-
work (Mehdad et al, 2012a).
Our submitted runs have been produced by train-
ing and optimizing multiclass and binary SVM clas-
sifiers, over the Spanish-English (Spa-Eng) devel-
opment set. In both cases, our results were posi-
tive, showing significant improvements over the me-
dian systems and average scores obtained by partic-
ipants. The overall results confirm the difficulty of
the task, and the potential of our approach in com-
bining linguistically motivated features in a ?pure?
cross-lingual approach that avoids the recourse to
external MT components.
701
2 Experiments
In our experiment we used the Spa-Eng portion of
the dataset described in (Negri et al, 2012; Negri
et al, 2011), consisting of 500 multi-directional en-
tailment pairs which was provided to train the sys-
tems and 500 pairs for the submission. Each pair in
the dataset is annotated with ?Bidirectional?, ?For-
ward?, ?Backward? or ?No entailment? judgements.
2.1 Approach
Our system builds on the integration of lexical,
syntactic and semantic features in a supervised
learning framework. Our model builds on three
main feature sets, respectively derived from: i)
phrase tables, ii) dependency relations, and iii)
semantic phrase tables.
1. Phrase Table (PT) matching: through
these features, a semantic judgement about entail-
ment is made exclusively on the basis of lexical
evidence. The matching features are calculated
with a phrase-to-phrase matching process. A phrase
in our approach is an n-gram composed of one
or more (up to 5) consecutive words, excluding
punctuation. Entailment decisions are assigned
combining phrasal matching scores calculated for
each level of n-grams (i.e. considering the number
of 1-grams, 2-grams,..., 5-grams extracted from H
that match with n-grams in T). Phrasal matches,
performed either at the level of tokens, lemmas, or
stems, can be of two types:
1. Exact: in the case that two phrases are identical
at one of the three levels (token, lemma, stem).
2. Lexical: in the case that two different phrases
can be mapped through entries of the resources
used to bridge T and H (i.e. phrase tables).
For each phrase in H, we first search for exact
matches at the level of token with phrases in T. If
no match is found at a token level, the other levels
(lemma and stem) are attempted. Then, in case of
failure with exact matching, lexical matching is per-
formed at the same three levels. To reduce redun-
dant matches, the lexical matches between pairs of
phrases which have already been identified as exact
matches are not considered.
Once the matching phase for each n-gram
level has been concluded, the number of matches
Matchn and the number of phrases in the hypoth-
esis H(n) is used to estimate the portion of phrases
in H that are matched at each level n (Equation 1).1
Since languages can express the same meaning with
different amounts of words, a phrase with length n
in H can match a phrase with any length in T.
Matchn =
Matchn
|H(n)|
(1)
In order to build English-Spanish phrase tables
for our experiments, we used the freely available
Europarl V.4, News Commentary and United
Nations Spanish-English parallel corpora released
for the WMT10 Shared Translation Task.2 We
run the TreeTagger (Schmid, 1995) and Snowball
stemmer (Porter, 2001) for preprocessing, and used
the Giza++ (Och and Ney, 2000) toolkit to align the
tokenized corpora at the word level. Subsequently,
we extracted the bi-lingual phrase table from the
aligned corpora using the Moses toolkit (Koehn et
al., 2007).
2. Dependency Relation (DR) matching tar-
gets the increase of CLTE precision. By adding
syntactic constraints to the matching process,
DR features aim to reduce wrong matches often
occurring at the lexical level. For instance, the con-
tradiction between ?Yahoo acquired Overture? and
?Overture compro? Yahoo? is evident when syntax
(in this case subject-object inversion) is taken into
account, but can not be caught by bag-of-words
methods.
We define a dependency relation as a triple that
connects pairs of words through a grammatical rela-
tion. For example, ?nsubj (loves, John)? is a depen-
dency relation with head loves and dependent John
connected by the relation nsubj, which means that
?John? is the subject of ?loves?. DR matching cap-
tures similarities between dependency relations, by
combining the syntactic and lexical level. In a valid
match, while the relation has to be the same (?exact?
1When checking for entailment from H to T, the normaliza-
tion is carried out dividing the number of n-grams in H by the
number of n-grams in T. The same holds for dependency rela-
tion and semantic phrase table matching.
2http://www.statmt.org/wmt10/
702
match), the connected words must be either the same
or semantically equivalent in the two languages. For
example, ?nsubj (loves, John)? can match ?nsubj
(ama, John)? and ?nsubj (quiere, John)? but not
?dobj (quiere, John)?.
Given the dependency tree representations of T
and H, for each grammatical relation (r) we calcu-
late a DR matching score (Matchr, see Equation 2)
as the number of matching occurrences of r in T and
H (respectively DRr(T ) and DRr(H)), divided by
the number of occurrences of r in H.
matchr =
|match(DRr(T ), DRr(H))|
|DRr(H)|
(2)
In our experiments, in order to extract de-
pendency relation (DR) matching features, the
dependency tree representations of English and
Spanish texts have been produced with DepPattern
(Otero and Lopez, 2011). We then mapped the
sets of dependency relation labels for the English-
Spanish parser output into: Adjunct, Determiner,
Object, Subject and Preposition. The dictionary,
containing about 9M bilingual word pairs, created
during the alignment of the English-Spanish parallel
corpora provided the lexical knowledge to perform
matches when the connected words are different.
3. Semantic Phrase Table (SPT) matching:
represents a novel way to leverage the integration
of semantics and MT-derived techniques. To this
aim, SPT improves CLTE methods relying on pure
lexical match, by means of ?generalized? phrase
tables annotated with shallow semantic labels.
Semantically enhanced phrase tables, with entries in
the form ?[LABEL] word1...wordn [LABEL]? (e.g.
?[ORG] acquired [ORG]?), are used as a recall-
oriented complement to the lexical phrase tables
used in machine translation (token-based entries like
?Yahoo acquired Overture?). The main motivation
for this augmentation is that word replacement with
semantic tags allows to match T-H tokens that do
not occur in the original bilingual parallel corpora
used for phrase table extraction. Our hypothesis
is that the increase in recall obtained from relaxed
matches through semantic tags in place of ?out of
vocabulary? terms (e.g. unseen person, location, or
organization names) is an effective way to improve
CLTE performance, even at the cost of some loss in
precision. Semantic phrase tables, however, have
two additional advantages. The first is related to
their smaller size and, in turn, its positive impact
on system?s efficiency, due to the considerable
search space reduction. Semantic tags allow to
merge different sequences of tokens into a single tag
and, consequently, different phrase entries can be
unified to one semantic phrase entry. As a result, for
instance, the SPT used in our experiments is more
than 30% smaller than the original token-based one.
The second advantage relates to their potential im-
pact on the confidence of CLTE judgements. Since
a semantic tag might cover more than one token
in the original entry phrase, SPT entries are often
short generalizations of longer original phrases.
Consequently, the matching process can benefit
from the increased probability of mapping higher
order n-grams (i.e. those providing more contextual
information) from H into T and vice-versa.
Like lexical phrase tables, SPTs are extracted
from parallel corpora. As a first step, we annotate
the corpora with named-entity taggers (FreeLing in
our case (Carreras et al, 2004)) for the source and
target languages, replacing named entities with gen-
eral semantic labels chosen from a coarse-grained
taxonomy including the categories: person, location,
organization, date and numeric expression. Then,
we combine the sequences of unique labels into one
single token of the same label, and we run Giza++
(Och and Ney, 2000) to align the resulting seman-
tically augmented corpora. Finally, we extract the
semantic phrase table from the augmented aligned
corpora using the Moses toolkit (Koehn et al, 2007).
For the matching phase, we first annotate T and
H in the same way we labeled our parallel corpora.
Then, for each n-gram order (n=1 to 5, excluding
punctuation), we use the SPT to calculate a matching
score (SPT matchn, see Equation 3), as the num-
ber of n-grams in H that match with phrases in T
divided by the number of n-grams in H. The match-
ing algorithm is same as the phrase table matching
one.
SPT matchn =
|SPTn(H) ? SPT (T )|
|SPTn(H)|
(3)
703
Run Features Classification Parameter selection Result
1 PT+SPT+DR Multiclass Entire training set 0.502
2 PT+SPT+DR Multiclass 2-fold cross validation 0.490
3 PT+SPT+DR Binary Entire training set 0.504
4 PT+SPT+DR Binary 2-fold cross validation 0.500
Table 1: Summary of the submitted runs and results for Spa-Eng dataset.
Forward Backward No entailment Bidirectional
P R F1 P R F1 P R F1 P R F1
0.515 0.704 0.595 0.546 0.568 0.557 0.447 0.304 0.362 0.482 0.440 0.460
Table 2: Best run?s Precision/Recall/F1 scores.
In our supervised learning framework, the com-
puted PT, SPT and DR scores are used as sepa-
rate features, giving to an SVM classifier, LIBSVM
(Chang and Lin, 2011), the possibility to learn opti-
mal feature weights from training data.
2.2 Submitted runs
In order to test our models under different condi-
tions, we set the CLTE problem both as two-way and
multiclass classification tasks.
Two-way classification casts multidirectional en-
tailment as a unidirectional problem, where each
pair is analyzed checking for entailment both from
left to right and from right to left. In this condi-
tion, each original test example is correctly clas-
sified if both pairs originated from it are correctly
judged (?YES-YES? for bidirectional, ?YES-NO?
for forward, ?NO-YES? for backward entailment,
and ?NO-NO? for no entailment). Two-way clas-
sification represents an intuitive solution to capture
multidirectional entailment relations but, at the same
time, a suboptimal approach in terms of efficiency
since two checks are performed for each pair.
Multiclass classification is more efficient, but at
the same time more challenging due to the higher
difficulty of multiclass learning, especially with
small datasets. We also tried to use the parameter se-
lection tool for C-SVM classification using the RBF
(radial basis function) kernel, available in LIBSVM
package. Our submitted runs and results have been
obtained with the settings summarized in table 1.
As can be seen from the table, our best result has
been achieved by Run 3 (50.4% accuracy), which
is significantly higher than the average and median
score over the best runs obtained by participants
(44.0% and 40.7% respectively). The detailed re-
sults achieved by the best run are reported in Table
2. We can observe that our system is performing
well for recognizing the unidirectional entailment
(i.e. forward and backward), while the performance
drops over no entailment pairs. The low results for
bidirectional cases also reflect the difficulty of dis-
criminating the no entailment pairs from the bidi-
rectional ones. Looking at the detailed results, we
can observe a high recall in the forward and back-
ward entailment cases, which could be explained by
the effectiveness of the semantic phrase table match-
ing features aiming at coverage increase over lexi-
cal methods. Adding more linguistically motivated
features and weighting the non-matched phrases can
be a starting point to improve the overall results for
other cases (bidirectional and no entailment).
3 Conclusion
In this paper we described our participation to the
cross-lingual textual entailment for content synchro-
nization task at SemEval-2012. We approached this
task by combining lexical, syntactic and semantic
features, at the cross-lingual level without recourse
to intermediate translation steps. In spite of the
difficulty and novelty of the task, our results on
the Spanish-English dataset (0.504) prove the effec-
tiveness of the approach with significant improve-
ments over the reported average and median accu-
racy scores for the 29 submitted runs (respectively
40.7% and 34.6%).
Acknowledgments
This work has been partially supported by the EU-
funded project CoSyne (FP7-ICT-4-248531).
704
References
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
FreeLing: An Open-Source Suite of Language An-
alyzers. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC?04).
C.C. Chang and C.J. Lin. 2011. LIBSVM: A Library
for Support Vector Machines. ACM Transactions on
Intelligent Systems and Technology (TIST), 2(3).
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions (ACL 2007).
Y. Mehdad, M. Negri, and M. Federico. 2010. Towards
Cross-Lingual Textual Entailment. In Proceedings of
the 11th Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL HLT 2010).
Y. Mehdad, M. Negri, and M. Federico. 2011. Using
Bilingual Parallel Corpora for Cross-Lingual Textual
Entailment. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies (ACL HLT 2011).
Y. Mehdad, M. Negri, and M. Federico. 2012a. Detect-
ing Semantic Equivalence and Information Disparity
in Cross-lingual Documents. In Proceedings of the
ACL?12.
Y. Mehdad, M. Negri, and M. Federico. 2012b. Match
without a Referee: Evaluating MT Adequacy without
Reference Translations. In Proceedings of the Ma-
chine Translation Workshop (WMT2012).
M. Negri and Y. Mehdad. 2010. Creating a Bi-lingual
Entailment Corpus through Translations with Mechan-
ical Turk: $100 for a 10-day rush. In Proceedings of
the NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk,
pages 212?216. Association for Computational Lin-
guistics.
M. Negri, L. Bentivogli, Y. Mehdad, D. Giampiccolo, and
A. Marchetti. 2011. Divide and conquer: Crowd-
sourcing the creation of cross-lingual textual entail-
ment corpora. In Proceedings of EMNLP 2011.
M. Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, and
D. Giampiccolo. 2012. Semeval-2012 Task 8: Cross-
lingual Textual Entailment for Content Synchroniza-
tion. In Proceedings of the 6th International Workshop
on Semantic Evaluation (SemEval 2012).
F.J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proceedings of the 38th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2000).
P.G. Otero and I.G. Lopez. 2011. A Grammatical For-
malism Based on Patterns of Part-of-Speech Tags. In-
ternational journal of corpus linguistics, 16(1).
M. Porter. 2001. Snowball: A language for stemming
algorithms.
H. Schmid. 1995. Treetaggera language indepen-
dent part-of-speech tagger. Institut fu?r Maschinelle
Sprachverarbeitung, Universita?t Stuttgart.
705
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 25?33, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
Semeval-2013 Task 8:
Cross-lingual Textual Entailment for Content Synchronization
Matteo Negri
FBK-irst
Trento, Italy
negri@fbk.eu
Alessandro Marchetti
CELCT
Trento, Italy
amarchetti@celct.it
Yashar Mehdad
UBC
Vancouver, Canada
mehdad@cs.ubc.ca
Luisa Bentivogli
FBK-irst
Trento, Italy
bentivo@fbk.eu
Danilo Giampiccolo
CELCT
Trento, Italy
giampiccolo@celct.it
Abstract
This paper presents the second round of the
task on Cross-lingual Textual Entailment for
Content Synchronization, organized within
SemEval-2013. The task was designed to pro-
mote research on semantic inference over texts
written in different languages, targeting at the
same time a real application scenario. Par-
ticipants were presented with datasets for dif-
ferent language pairs, where multi-directional
entailment relations (?forward?, ?backward?,
?bidirectional?, ?no entailment?) had to be
identified. We report on the training and test
data used for evaluation, the process of their
creation, the participating systems (six teams,
61 runs), the approaches adopted and the re-
sults achieved.
1 Introduction
The cross-lingual textual entailment task (Mehdad et
al., 2010) addresses textual entailment (TE) recog-
nition (Dagan and Glickman, 2004) under the new
dimension of cross-linguality, and within the new
challenging application scenario of content synchro-
nization. Given two texts in different languages, the
cross-lingual textual entailment (CLTE) task con-
sists of deciding if the meaning of one text can be
inferred from the meaning of the other text. Cross-
linguality represents an interesting direction for re-
search on recognizing textual entailment (RTE), es-
pecially due to its possible application in a vari-
ety of tasks. Among others (e.g. question answer-
ing, information retrieval, information extraction,
and document summarization), multilingual content
synchronization represents a challenging application
scenario to evaluate CLTE recognition components
geared to the identification of sentence-level seman-
tic relations.
Given two documents about the same topic writ-
ten in different languages (e.g. Wikipedia pages),
the content synchronization task consists of au-
tomatically detecting and resolving differences in
the information they provide, in order to produce
aligned, mutually enriched versions of the two docu-
ments (Monz et al, 2011; Bronner et al, 2012). To-
wards this objective, a crucial requirement is to iden-
tify the information in one page that is either equiv-
alent or novel (more informative) with respect to the
content of the other. The task can be naturally cast
as an entailment recognition problem, where bidi-
rectional and unidirectional entailment judgements
for two text fragments are respectively mapped into
judgements about semantic equivalence and novelty.
The task can also be seen as a machine translation
evaluation problem, where judgements about se-
mantic equivalence and novelty depend on the pos-
sibility to fully or partially translate a text fragment
into the other.
The recent advances on monolingual TE on the
one hand, and the methodologies used in Statisti-
cal Machine Translation (SMT) on the other, offer
promising solutions to approach the CLTE task. In
line with a number of systems that model the RTE
task as a similarity problem (i.e. handling similar-
ity scores between T and H as features contributing
to the entailment decision), the standard sentence
and word alignment programs used in SMT offer
a strong baseline for CLTE (Mehdad et al, 2011;
25
Figure 1: Example of SP-EN CLTE pairs.
Mehdad et al, 2012). However, although repre-
senting a solid starting point to approach the prob-
lem, similarity-based techniques are just approx-
imations, open to significant improvements com-
ing from semantic inference at the multilingual
level (e.g. cross-lingual entailment rules such as
?perro???animal?). Taken in isolation, similarity-
based techniques clearly fall short of providing an
effective solution to the problem of assigning direc-
tions to the entailment relations (especially in the
complex CLTE scenario, where entailment relations
are multi-directional). Thanks to the contiguity be-
tween CLTE, TE and SMT, the proposed task pro-
vides an interesting scenario to approach the issues
outlined above from different perspectives, and of-
fers large room for mutual improvement.
Building on the success of the first CLTE evalua-
tion organized within SemEval-2012 (Negri et al,
2012a), the remainder of this paper describes the
second evaluation round organized within SemEval-
2013. The following sections provide an overview
of the datasets used, the participating systems, the
approaches adopted, the achieved results, and the
lessons learned.
2 The task
Given a pair of topically related text fragments (T1
and T2) in different languages, the CLTE task con-
sists of automatically annotating it with one of the
following entailment judgements (see Figure 1 for
Spanish/English examples of each judgement):
? bidirectional (T1?T2 & T1?T2): the two
fragments entail each other (semantic equiva-
lence);
? forward (T1?T2 & T16?T2): unidirectional
entailment from T1 to T2;
? backward (T16?T2 & T1?T2): unidirectional
entailment from T2 to T1;
? no entailment (T16?T2 & T16?T2): there is
no entailment between T1 and T2 in either di-
rection;
In this task, both T1 and T2 are assumed to be
true statements. Although contradiction is relevant
from an application-oriented perspective, contradic-
tory pairs are not present in the dataset.
3 Dataset description
The CLTE-2013 dataset is composed of four CLTE
corpora created for the following language combi-
nations: Spanish/English (SP-EN), Italian/English
(IT-EN), French/English (FR-EN), German/English
(DE-EN). Each corpus consists of 1,500 sentence
pairs (1,000 for training and 500 for test), balanced
across the four entailment judgements.
In this year?s evaluation, as training set we used
the CLTE-2012 corpus1 that was created for the
SemEval-2012 evaluation exercise2 (including both
training and test sets). The CLTE-2013 test set was
created from scratch, following the methodology de-
scribed in the next section.
3.1 Data collection and annotation
To collect the entailment pairs for the 2013 test set
we adopted a slightly modified version of the crowd-
sourcing methodology followed to create the CLTE-
2012 corpus (Negri et al, 2011). The main differ-
ence with last year?s procedure is that we did not
take advantage of crowdsourcing for the whole data
collection process, but only for part of it.
As for CLTE-2012, the collection and annotation
process consists of the following steps:
1. First, English sentences were manually ex-
tracted from Wikipedia and Wikinews. The se-
lected sentences represent one of the elements
(T1) of each entailment pair;
1http://www.celct.it/resources.php?id page=CLTE
2http://www.cs.york.ac.uk/semeval-2012/task8/
26
2. Next, each T1 was modified in various ways
in order to obtain a corresponding T2. While
in the CLTE-2012 dataset the whole T2 cre-
ation process was carried out through crowd-
sourcing, for the CLTE-2013 test set we crowd-
sourced only the first phase of T1 modification,
namely the creation of paraphrases. Focusing
on the creation of high quality paraphrases, we
followed the crowdsourcing methodology ex-
perimented in Negri et al (2012b), in which
a paraphrase is obtained through an itera-
tive modification process of an original sen-
tence, by asking workers to introduce meaning-
preserving lexical and syntactic changes. At
each round of the iteration, new workers are
presented with the output of the previous iter-
ation in order to increase divergence from the
original sentence. At the end of the process,
only the more divergent paraphrases according
to the Lesk score (Lesk, 1986) are selected. As
for the second phase of T2 creation process,
this year it was carried out by expert annota-
tors, who followed the same criteria used last
year for the crowdsourced tasks, i.e. i) remove
information from the input (paraphrased) sen-
tence and ii) add information from sentences
surrounding T1 in the source article;
3. Each T2 was then paired to the original T1, and
the resulting pairs were annotated with one of
the four entailment judgements. In order to re-
duce the correlation between the difference in
sentences? length and entailment judgements,
only the pairs where the difference between the
number of words in T1 and T2 (length diff ) was
below a fixed threshold (10 words) were re-
tained.3 The final result is a monolingual En-
glish dataset annotated with multi-directional
entailment judgements, which are well dis-
tributed over length diff values ranging from 0
to 9;
4. In order to create the cross-lingual datasets,
each English T1 was manually translated into
3Such constraint has been applied in order to focus as much
as possible on semantic aspects of the problem, by reduc-
ing the applicability of simple association rules such as IF
length(T1)>length(T2) THEN T1?T2.
four different languages (i.e. Spanish, German,
Italian and French) by expert translators;
5. By pairing the translated T1 with the cor-
responding T2 in English, four cross-lingual
datasets were obtained.
To ensure the good quality of the datasets, all the
collected pairs were cross-annotated and filtered to
retain only those pairs with full agreement in the
entailment judgement between two expert annota-
tors. The final result is a multilingual parallel en-
tailment corpus, where T1s are in 5 different lan-
guages (i.e. English, Spanish, German, Italian, and
French), and T2s are in English. It is worth men-
tioning that the monolingual English corpus, a by-
product of our data collection methodology, will be
publicly released as a further contribution to the re-
search community.
3.2 Dataset statistics
As described in section 3.1, the methodology fol-
lowed to create the training and test sets was the
same except for the crowdsourced tasks. This al-
lowed us to obtain two datasets with the same bal-
ance across the entailment judgements, and to keep
under control the distribution of the pairs for differ-
ent length diff values in each language combination.
Training Set. The training set is composed of
1,000 CLTE pairs for each language combina-
tion, balanced across the four entailment judge-
ments (bidirectional, forward, backward, and
no entailment). As shown in Table 1, our data col-
lection procedure led to a dataset where the major-
ity of the pairs falls in the +5 -5 length diff range
for each language pair (67.2% on average across the
four language pairs). This characteristic is partic-
ularly relevant as our assumption is that such data
distribution makes entailment judgements based on
mere surface features such as sentence length inef-
fective, thus encouraging the development of alter-
native, deeper processing strategies.
Test Set. The test set is composed of 500 entail-
ment pairs for each language combination, balanced
across the four entailment judgements. As shown
in Table 2, also in this dataset the majority of the
collected entailment pairs is uniformly distributed
27
(a) SP-EN (b) IT-EN
(c) FR-EN (d) DE-EN
Figure 2: Pair distribution in the 2013 test set: total number of pairs (y-axis) for different length diff values (x-axis).
SP-EN IT-EN FR-EN DE-EN
Forward 104 132 121 179
Backward 202 182 191 123
No entailment 163 173 169 174
Bidirectional 175 199 193 209
ALL 644 686 674 685
% (out of 1,000) 64.4 68.6 67.4 68.5
Table 1: Training set pair distribution within the -5/+5
length diff range.
in the [-5,+5] length diff range (68.1% on average
across the four language pairs).
However, comparing training and test set for
each language pair, it can be seen that while the
Spanish-English and Italian-English datasets are ho-
mogeneous with respect to the length diff feature,
the French-English and German-English datasets
present noticeable differences between training and
test set. These figures show that, despite the consid-
erable effort spent to produce comparable training
SP-EN IT-EN FR-EN DE-EN
backward 82 89 82 102
bidirectional 89 92 90 106
forward 69 78 76 98
no entailment 71 80 59 100
ALL 311 339 307 406
% (out of 500) 62.2 67.8 61.4 81.2
Table 2: Test set pair distribution within the -5/+5
length diff range.
and test sets, the ideal objective of a full homogene-
ity between the datasets for these two languages was
difficult to reach.
Complete details about the distribution of the
pairs in terms of length diff for the four cross-
lingual corpora in the test set are provided in Figure
2. Vertical bars represent, for each length diff value,
the proportion of pairs belonging to the four entail-
ment classes.
28
4 Evaluation metrics and baselines
Evaluation results have been automatically com-
puted by comparing the entailment judgements re-
turned by each system with those manually assigned
by human annotators in the gold standard. The met-
rics used for systems? ranking is accuracy over the
whole test set, i.e. the number of correct judge-
ments out of the total number of judgements in the
test set. Additionally, we calculated precision, re-
call, and F1 measures for each of the four entail-
ment judgement categories taken separately. These
scores aim at giving participants the possibility to
gain clearer insights into their system?s behaviour on
the entailment phenomena relevant to the task.
To allow comparison with the CLTE-2012 re-
sults, the same three baselines were calculated on the
CLTE-2013 test set for each language combination.
The first one is the 0.25 accuracy score obtained by
assigning each test pair in the balanced dataset to
one of the four classes. The other two baselines con-
sider the length difference between T1 and T2:
? Composition of binary judgements (Bi-
nary). To calculate this baseline an SVM
classifier is trained to take binary en-
tailment decisions (?YES?, ?NO?). The
classifier uses length(T1)/length(T2) and
length(T2)/length(T1) as features respectively
to check for entailment from T1 to T2 and vice-
versa. For each test pair, the unidirectional
judgements returned by the two classifiers are
composed into a single multi-directional judge-
ment (?YES-YES?=?bidirectional?, ?YES-
NO?=?forward?, ?NO-YES?=?backward?,
?NO-NO?=?no entailment?);
? Multi-class classification (Multi-class). A
single SVM classifier is trained with the same
features to directly assign to each pair one of
the four entailment judgements.
Both the baselines have been calculated with the
LIBSVM package (Chang and Lin, 2011), using de-
fault parameters. Baseline results are reported in Ta-
ble 3.
Although the four CLTE datasets are derived from
the same monolingual EN-EN corpus, baseline re-
sults present slight differences due to the effect of
translation into different languages. With respect to
last year?s evaluation, we can observe a slight drop
in the binary classification baseline results. This
might be due to the fact that the length distribution
of examples is slightly different this year. How-
ever, there are no significant differences between the
multi-class baseline results of this year in compar-
ison with the previous round results. This might
suggest that multi-class classification is a more ro-
bust approach for recognizing multi-directional en-
tailment relations. Moreover, both baselines failed
in capturing the ?no-entailment? examples in all
datasets (F1no?entailment = 0).
SP-EN IT-EN FR-EN DE-EN
1-class 0.25 0.25 0.25 0.25
Binary 0.35 0.39 0.37 0.39
Multi-class 0.43 0.44 0.42 0.42
Table 3: Baseline accuracy results.
5 Submitted runs and results
Like in the 2012 round of the CLTE task, partici-
pants were allowed to submit up to five runs for each
language combination. A total of twelve teams reg-
istered for participation and downloaded the train-
ing set. Out of them, six4 submitted valid runs.
Five teams produced submissions for all the four
language combinations, while one team participated
only in the DE-EN task. In total, 61 runs have been
submitted and evaluated (16 for DE-EN, and 15 for
each of the other language pairs).
Accuracy results are reported in Table 4. As can
be seen from the table, the performance of the best
systems is quite similar across the four language
combinations, with the best submissions achieving
results in the 43.4-45.8% accuracy interval. Simi-
larly, also average and median results are close to
each other, with a small drop on DE-EN. This drop
might be explained by the difference between the
training and test set with respect to the length diff
feature. Moreover, the performance of DE-EN auto-
matic translation might affect approaches based on
?pivoting?, (i.e. addressing CLTE by automatically
translating T1 in the same language of T2, as de-
scribed in Section 6).
4Including the task organizers.
29
System name SP-EN IT-EN FR-EN DE-EN
altn run1* 0.428 0.432 0.420 0.388
BUAP run1 0.364 0.358 0.368 0.322
BUAP run2 0.374 0.358 0.364 0.318
BUAP run3 0.380 0.358 0.362 0.316
BUAP run4 0.364 0.388 0.392 0.350
BUAP run5 0.386 0.360 0.372 0.318
celi run1 0.340 0.324 0.334 0.342
celi run2 0.342 0.324 0.340 0.342
ECNUCS run1 0.428 0.426 0.438 0.422
ECNUCS run2 0.404 0.420 0.450 0.436
ECNUCS run3 0.408 0.426 0.458 0.432
ECNUCS run4 0.422 0.416 0.436 0.452
ECNUCS run5 0.392 0.402 0.442 0.426
SoftCard run1 0.434 0.454 0.416 0.414
SoftCard run2 0.432 0.448 0.426 0.402
umelb run1 ? ? ? 0.324
Highest 0.434 0.454 0.458 0.452
Average 0.404 0.404 0.401 0.378
Median 0.428 0.426 0.420 0.369
Lowest 0.342 0.324 0.340 0.324
Table 4: CLTE-2013 accuracy results (61 runs) over the
4 language combinations. Highest, average, median and
lowest scores are calculated considering only the best run
for each team (*task organizers? system).
Compared to the results achieved last year, shown
in Table 5, a sensible decrease in the highest scores
can be observed. While in CLTE-2012 the top sys-
tems achieved an accuracy well above 0.5 (with a
maximum of 0.632 in SP-EN), the results for this
year are far below such level (the peak is now at
45,8% for FR-EN). A slight decrease with respect
to 2012 can also be noted for average performances.
However, it?s worth remarking the general increase
of the lowest and median scores, which are less sen-
sitive to isolate outstanding results achieved by sin-
gle teams. This indicates that a progress in CLTE
research has been made building on the lessons
learned after the first round of the initiative.
To better understand the behaviour of each sys-
tem, Table 6 provides separate precision, recall, and
F1 scores for each entailment judgement, calculated
over the best runs of each participating team. In
contrast to CLTE-2012, where the ?bidirectional?
and ?no entailment? categories consistently proved
to be more problematic than ?forward? and ?back-
ward? judgements, this year?s results are more ho-
mogeneous across the different classes. Neverthe-
less, on average, the classification of ?bidirectional?
pairs is still worse for three language pairs (SP-EN,
IT-EN and FR-EN), and results for ?no entailment?
are lower for two of them (SP-EN and DE-EN).
SP-EN IT-EN FR-EN DE-EN
Highest 0.632 0.566 0.570 0.558
Average 0.440 0.411 0.408 0.408
Median 0.407 0.350 0.365 0.363
Lowest 0.274 0.326 0.296 0.296
Table 5: CLTE-2012 accuracy results. Highest, average,
median and lowest scores are calculated considering only
the best run for each team.
As regards the comparison with the baselines,
this year?s results confirm that the length diff -based
baselines are hard to beat. More specifically, most
of the systems are slightly above the binary classi-
fication baseline (with the exception of the DE-EN
dataset where only two systems out of six outper-
formed it), whereas for all the language combina-
tions the multi-class baseline was beaten only by the
best participating system.
This shows that, despite the effort in keeping the
distribution of the entailment classes uniform across
different length diff values, eliminating the correla-
tion between sentence length and correct entailment
decisions is difficult. As a consequence, although
disregarding semantic aspects of the problem, fea-
tures considering length information are quite ef-
fective in terms of overall accuracy. Such features,
however, perform rather poorly when dealing with
challenging cases (e.g. ?no-entailment?), which are
better handled by participating systems.
6 Approaches
A rough classification of the approaches adopted by
participants can be made along two orthogonal di-
mensions, namely:
? Pivoting vs. Cross-lingual. Pivoting meth-
ods rely on the automatic translation of one of
the two texts (either single words or the en-
tire sentence) into the language of the other
(typically English) in order perform monolin-
gual TE recognition. Cross-lingual methods
assign entailment judgements without prelim-
inary translation.
? Composition of binary judgements vs.
Multi-class classification. Compositional ap-
proaches map unidirectional (?YES?/?NO?)
30
SP-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full spa-eng 0.509 0.464 0.485 0.440 0.264 0.330 0.464 0.416 0.439 0.357 0.568 0.438
BUAP spa-eng run5 0.446 0.360 0.398 0.521 0.296 0.378 0.385 0.456 0.418 0.300 0.432 0.354
celi spa-eng run2 0.396 0.352 0.373 0.431 0.400 0.415 0.325 0.328 0.327 0.245 0.288 0.265
ECNUCS spa-eng run1 0.458 0.432 0.444 0.533 0.320 0.400 0.406 0.416 0.411 0.380 0.544 0.447
SoftCard spa-eng run1 0.462 0.344 0.394 0.619 0.480 0.541 0.418 0.472 0.444 0.325 0.440 0.374
AVG. 0.454 0.390 0.419 0.509 0.352 0.413 0.400 0.418 0.408 0.321 0.454 0.376
IT-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full ita-eng 0.448 0.376 0.409 0.417 0.344 0.377 0.512 0.496 0.504 0.374 0.512 0.432
BUAP ita-eng run4 0.418 0.328 0.368 0.462 0.384 0.419 0.379 0.440 0.407 0.327 0.400 0.360
celi ita-eng run1 0.288 0.256 0.271 0.395 0.408 0.402 0.336 0.304 0.319 0.279 0.328 0.301
ECNUCS ita-eng run1 0.422 0.456 0.438 0.592 0.336 0.429 0.440 0.440 0.440 0.349 0.472 0.401
SoftCard ita-eng run1 0.514 0.456 0.483 0.612 0.480 0.538 0.392 0.464 0.425 0.364 0.416 0.388
AVG. 0.418 0.374 0.394 0.496 0.390 0.433 0.412 0.429 0.419 0.339 0.426 0.376
FR-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full fra-eng 0.405 0.392 0.398 0.420 0.296 0.347 0.500 0.440 0.468 0.381 0.552 0.451
BUAP fra-eng run4 0.407 0.472 0.437 0.431 0.376 0.402 0.379 0.376 0.378 0.352 0.344 0.348
celi fra-eng run2 0.394 0.344 0.368 0.364 0.376 0.370 0.352 0.352 0.352 0.263 0.288 0.275
ECNUCS fra-eng run3 0.422 0.432 0.427 0.667 0.352 0.461 0.514 0.432 0.470 0.383 0.616 0.472
SoftCard fra-eng run2 0.477 0.416 0.444 0.556 0.400 0.465 0.412 0.432 0.422 0.335 0.456 0.386
AVG. 0.421 0.411 0.415 0.488 0.360 0.409 0.431 0.406 0.418 0.343 0.451 0.386
DE-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full deu-eng 0.432 0.408 0.420 0.378 0.272 0.316 0.445 0.392 0.417 0.330 0.480 0.391
BUAP deu-eng run4 0.364 0.344 0.354 0.389 0.280 0.326 0.352 0.352 0.352 0.317 0.424 0.363
celi deu-eng run1 0.346 0.352 0.349 0.414 0.424 0.419 0.351 0.264 0.301 0.272 0.328 0.297
ECNUCS deu-eng run4 0.429 0.432 0.430 0.611 0.352 0.447 0.415 0.392 0.403 0.429 0.632 0.511
SoftCard deu-eng run1 0.511 0.368 0.428 0.527 0.384 0.444 0.417 0.400 0.408 0.317 0.504 0.389
umelb deu-eng run1 0.323 0.320 0.321 0.240 0.184 0.208 0.362 0.376 0.369 0.347 0.416 0.378
AVG. 0.401 0.371 0.384 0.426 0.316 0.360 0.390 0.363 0.375 0.335 0.464 0.389
Table 6: Precision, recall and F1 scores, calculated for each team?s best run for all the language combinations.
entailment decisions taken separately into sin-
gle judgements (similar to the Binary baseline
in Section 4). Methods based on multi-class
classification directly assign one of the four en-
tailment judgements to each test pair (similar to
our Multi-class baseline).
In contrast with CLTE-2012, where the combina-
tion of pivoting and compositional methods was the
option adopted by the majority of the approaches,
this year?s solutions do not show a clear trend. Con-
cerning the former dimension, participating systems
are equally distributed in cross-lingual and pivoting
methods relying on external automatic translation
tools. Regarding the latter dimension, in addition
to compositional and multi-class strategies, also al-
ternative solutions that leverage more sophisticated
meta-classification strategies have been proposed.
Besides the recourse to MT tools (e.g. Google
Translate), other tools and resources used by partic-
ipants include: WordNet, word alignment tools (e.g.
Giza++), part-of-speech taggers (e.g. Stanford POS
Tagger), stemmers (e.g. Snowball), machine learn-
ing libraries (e.g. Weka, SVMlight), parallel corpora
(e.g. Europarl), and stopword lists. More in detail:
ALTN [cross-lingual, compositional] (Turchi
and Negri, 2013) adopts a supervised learning
method based on features that consider word align-
ments between the two sentences obtained with
GIZA++ (Och et al, 2003). Binary entailment
judgements are taken separately, and combined into
final CLTE decisions.
BUAP [pivoting, multi-class and meta-
classifier] (Vilarin?o et al, 2013) adopts a pivoting
method based on translating T1 into the language of
31
T2 and vice versa (using Google Translate5). Sim-
ilarity measures (e.g. Jaccard index) and features
based on n-gram overlap, computed at the level of
words and part of speech categories, are used (either
alone or in combination) by different classification
strategies including: multi-class, a meta-classifier
(i.e. combining the output of 2/3/4-class classifiers),
and majority voting.
CELI [cross-lingual, meta-classifier]
(Kouylekov, 2013) uses dictionaries for word
matching, and a multilingual corpus extracted
from Wikipedia for term weighting. A variety of
distance measures implemented in the RTE system
EDITS (Kouylekov and Negri, 2010; Negri et
al., 2009) are used to extract features to train a
meta-classifier. Such classifier combines binary
decisions (?YES?/?NO?) taken separately for each
of the four CLTE judgements.
ECNUCS [pivoting, multi-class] (Jiang and
Man, 2013) uses Google Translate to obtain the En-
glish translation of each T1. After a pre-processing
step aimed at maximizing the commonalities be-
tween the two sentences (e.g. abbreviation replace-
ment), a number of features is extracted to train
a multi-class SVM classifier. Such features con-
sider information about sentence length, text sim-
ilarity/difference measures, and syntactic informa-
tion.
SoftCard [pivoting, multi-class] (Jimenez et al,
2013) after automatic translation with Google Trans-
late, uses SVMs to learn entailment decisions based
on information about the cardinality of: T1, T2, their
intersection and their union. Cardinalities are com-
puted in different ways, considering tokens in T1 and
T2, their IDF, and their similarity.
Umelb [cross-lingual, pivoting, compositional]
(Graham et al, 2013) adopts both pivoting and
cross-lingual approaches. For the latter, GIZA++
was used to compute word alignments between the
input sentences. Word alignment features are used
to train binary SVM classifiers whose decisions are
eventually composed into CLTE judgements.
7 Conclusion
Following the success of the first round of the Cross-
lingual Textual Entailment for Content Synchroniza-
5http://translate.google.com/
tion task organized within SemEval-2012, a second
evaluation task has been organized within SemEval-
2013. Despite the decrease in the number of partic-
ipants (six teams - four less than in the first round
- submitted a total of 61 runs) the new experience
is still positive. In terms of data, a new test set
has been released, extending the old one with 500
new CLTE pairs. The resulting 1,500 cross-lingual
pairs, aligned over four language combinations (in
addition to the monolingual English version), and
annotated with multiple entailment relations, repre-
sent a significant contribution to the research com-
munity and a solid starting point for further develop-
ments.6 In terms of results, in spite of a significant
decrease of the top scores, the increase of both me-
dian and lower results demonstrates some encour-
aging progress in CLTE research. Such progress is
also demonstrated by the variety of the approaches
proposed. While in the first round most of the
teams adopted more intuitive and ?simpler? solu-
tions based on pivoting (i.e. translation of T1 and
T2 in the same language) and compositional entail-
ment decision strategies, this year new ideas and
more complex solutions have emerged. Pivoting and
cross-lingual approaches are equally distributed, and
new classification methods have been proposed. Our
hope is that the large room for improvement, the in-
crease of available data, and the potential of CLTE
as a way to address complex NLP tasks and applica-
tions will motivate further research on the proposed
problem.
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-248531). The
authors would also like to acknowledge Pamela
Forner and Giovanni Moretti from CELCT, and the
volunteer translators that contributed to the creation
of the dataset: Giusi Calo, Victoria D??az, Bianca
Jeremias, Anne Kauffman, Laura Lo?pez Ortiz, Julie
Mailfait, Laura Mora?n Iglesias, Andreas Schwab.
6Together with the datasets derived from translation of the
RTE data (Negri and Mehdad, 2010), this is the only material
currently available to train and evaluate CLTE systems.
32
References
Amit Bronner, Matteo Negri, Yashar Mehdad, Angela
Fahrni, and Christof Monz. 2012. Cosyne: Synchro-
nizing multilingual wiki content. In Proceedings of
WikiSym 2012.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
Ido Dagan and Oren Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic Applied Modeling of Lan-
guage Variability. In Proceedings of the PASCAL
Workshop of Learning Methods for Text Understand-
ing and Mining.
Yvette Graham, Bahar Salehi, and Tim Baldwin. 2013.
Unimelb: Cross-lingual Textual Entailment with Word
Alignment and String Similarity Features. In Proceed-
ings of the 7th International Workshop on Semantic
Evaluation (SemEval 2013).
Zhao Jiang and Lan Man. 2013. ECNUCS: Recognizing
Cross-lingual Textual Entailment Using Multiple Fea-
ture Types. . In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013).
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013. Soft Cardinality-CLTE: Learning to Iden-
tify Directional Cross-Lingual Entailmens from Car-
dinalities and SMT. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
Milen Kouylekov and Matteo Negri. 2010. An open-
source package for recognizing textual entailment. In
Proceedings of the ACL 2010 System Demonstrations.
Milen Kouylekov. 2013. Celi: EDITS and Generic Text
Pair Classification. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
Michael Lesk. 1986. Automated Sense Disambiguation
Using Machine-readable Dictionaries: How to Tell a
Pine Cone from an Ice Cream Cone. In Proceedings
of the 5th annual international conference on Systems
documentation (SIGDOC86).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment. In
Proceedings of the 11th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL HLT 2010).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using Bilingual Parallel Corpora for Cross-
Lingual Textual Entailment. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL HLT 2011).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Detecting Semantic Equivalence and Informa-
tion Disparity in Cross-lingual Documents. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012).
Christof Monz, Vivi Nastase, Matteo Negri, Angela
Fahrni, Yashar Mehdad, and Michael Strube. 2011.
Cosyne: a framework for multilingual content syn-
chronization of wikis. In Proceedings of WikiSym
2011.
Matteo Negri and Yashar Mehdad. 2010. Creating a Bi-
lingual Entailment Corpus through Translations with
Mechanical Turk: $100 for a 10-day Rush. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Cre-
ating Speech and Language Data with Amazons? Me-
chanical Turk.
Matteo Negri, Milen Kouylekov, Bernardo Magnini,
Yashar Mehdad, and Elena Cabrio. 2009. Towards ex-
tensible textual entailment engines: the edits package.
In AI* IA 2009: Emergent Perspectives in Artificial In-
telligence, pages 314?323. Springer.
Matto Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and Conquer: Crowdsourcing the Creation of
Cross-Lingual Textual Entailment Corpora. Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2011).
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012a.
Semeval-2012 Task 8: Cross-lingual Textual Entail-
ment for Content Synchronization. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012).
Matteo Negri, Yashar Mehdad, Alessandro Marchetti,
Danilo Giampiccolo, and Luisa Bentivogli. 2012b.
Chinese Whispers: Cooperative Paraphrase Acqui-
sition. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC12), volume 2, pages 2659?2665.
F. Och, H. Ney, F. Josef, and O. H. Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics.
Marco Turchi and Matteo Negri. 2013. ALTN: Word
Alignment Features for Cross-Lingual Textual Entail-
ment. In Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013).
Darnes Vilarin?o, David Pinto, Saul Leo?n, Yuridiana
Alema?n, and Helena Go?mez-Adorno. 2013. BUAP:
N -gram based Feature Evaluation for the Cross-
Lingual Textual Entailment Task. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013).
33
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 128?132, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
ALTN: Word Alignment Features for Cross-lingual Textual Entailment
Marco Turchi and Matteo Negri
Fondazione Bruno Kessler
Trento, Italy
{turchi,negri}@fbk.eu
Abstract
We present a supervised learning approach to
cross-lingual textual entailment that explores
statistical word alignment models to predict
entailment relations between sentences writ-
ten in different languages. Our approach
is language independent, and was used to
participate in the CLTE task (Task#8) or-
ganized within Semeval 2013 (Negri et al,
2013). The four runs submitted, one for
each language combination covered by the test
data (i.e. Spanish/English, German/English,
French/English and Italian/English), achieved
encouraging results. In terms of accuracy,
performance ranges from 38.8% (for Ger-
man/English) to 43.2% (for Italian/English).
On the Italian/English and Spanish/English
test sets our systems ranked second among
five participants, close to the top results (re-
spectively 43.4% and 45.4%).
1 Introduction
Cross-lingual textual entailment (CLTE) is an ex-
tension of the Textual Entailment task (Dagan and
Glickman, 2004) that consists in deciding, given
two texts T and H written in different languages
(respectively called text and hypothesis), if H can
be inferred from T (Mehdad et al, 2010). In the
case of SemEval 2013, the task is formulated as
a multi-class classification problem in which there
are four possible relations between T and H: for-
ward (T ? H), backward (T ? H), bidirectional
(T ? H) and ?no entailment?.
Targeting the identification of semantic equiva-
lence and information disparity between topically
related sentences, CLTE recognition can be seen as a
core task for a number of cross-lingual applications.
Among others, multilingual content synchronization
has been recently proposed as an ideal framework
for the exploitation of CLTE components and the in-
tegration of semantics and machine translation (MT)
technology (Mehdad et al, 2011; Mehdad et al,
2012b; Bronner et al, 2012; Monz et al, 2011).
In the last few years, several methods have been
proposed for CLTE. These can be roughly divided
in two main groups (Negri et al, 2012): i) those us-
ing a pivoting strategy by translating H into the lan-
guage of T and then using monolingual TE compo-
nents1, and those directly using cross-lingual strate-
gies. Among this second group, several sources of
cross-lingual knowledge have been used, such as
dictionaries (Kouylekov et al, 2012; Perini, 2012),
phrase and paraphrase tables (Mehdad et al, 2012a),
GIZA++ (Och and Ney, 2003) word alignment mod-
els (Wa?schle and Fendrich, 2012), MT of sub-
segments (Espla`-Gomis et al, 2012), or semantic
Wordnets (Castillo, 2011).
In this work we propose a CLTE detection method
based on a new set of features using word align-
ment as a source of cross-lingual knowledge. This
set, which is richer than the one by (Wa?schle and
Fendrich, 2012), is aimed not only at grasping infor-
mation about the proportion of aligned words, but
also about the distribution of the alignments in both
1In the first CLTE evaluation round at Semeval 2012, for
instance, the system described in (Meng et al, 2012) used the
open source EDITS system (Kouylekov and Negri, 2010; Negri
et al, 2009) to calculate similarity scores between monolingual
English pairs.
128
H and T . This set of features is later used by two
support vector machine (SVM) classifiers for detect-
ing CLTE separately in both directions (T ? H and
T ? H). We use the combined output of both clas-
sifiers for performing the CLTE detection.
The paper is organized as follows: Section 2
describes the features used and the classification
method; Section 3 explains the experimental frame-
work and the results obtained for the different
language-pair sets; finally, the conclusions obtained
from the results are summarised in Section 4.
2 ALTN System
In our approach we have implemented a system
based on supervised learning. It takes an unlabeled
sentence pair as input (T and H) and labels it au-
tomatically with one of the possible four valid en-
tailment relations. The architecture is depicted in
Figure 1.
A key component to our approach is the word
alignment model. In a preprocessing step it is
trained on a set of parallel texts for the target lan-
guage pair. Next, different features based on the
word alignment are extracted. Taking the features
and the target language pair labels as input, a su-
pervised learning algorithm is run to fit a model to
the data. The last step is to use the model to au-
tomatically label unseen instances with entailment
relations.
2.1 Features
What characterizes our submission is the use of
word alignment features to capture entailment rela-
tions. We extract the following features from a word
alignment model for a given sentence pair (all fea-
tures are calculated for both T and H):
? proportion of aligned words in the sentence
(baseline);
? number of unaligned sequences of words nor-
malized by the length of the sentence;
? length of the longest sequence of aligned words
normalized by the length of the sentence;
? length of the longest sequence of unaligned
words normalized by the length of the sentence;
Figure 1: System architecture
? average length of the aligned word sequences;
? average length of the unaligned word se-
quences;
? position of the first unaligned word normalized
by the length of the sentence;
? position of the last unaligned word normalized
by the lenght of the sentence;
? proportion of aligned n-grams in the sentence
(n varying from 1 to 5).
These features are language independent as they
are obtained from statistical models that take as in-
put a parallel corpus. Provided that there exist paral-
lel data for a given language pair, the only constraint
in terms of resources, the adoption of these features
makes our approach virtually portable across lan-
guages with limited effort.
2.2 CLTE Model
Our CLTE model is composed by two supervised bi-
nary classifiers that predict whether there is entail-
ment between the T and H . One classifier checks
129
for forward entailment (T ? H) and the other
checks for backward entailment (T ? H). The out-
put of both classifiers is combined to form the four
valid entailment decisions:
? forward and backward classifier output true:
?bidirectional? entailment;
? forward is true and backward is false:
?forward? entailment;
? forward is false and backward is true:
?backward? entailment;
? both forward and backward output false: ?no
entailment? relation.
Both binary classifiers were implemented using
the SVM implementation of Weka (Hall et al,
2009).
3 Experiments
In our submission we experimented with three stan-
dard word alignment algorithms: the hidden Markov
model (HMM) (Vogel et al, 1996) and IBM models
3 and 4 (Brown et al, 1993). They are implemented
in the MGIZA++ package (Gao and Vogel, 2008).
Building on a probabilistic lexical model to establish
mappings between words in two languages, these
models compute alignments between the word po-
sitions in two input sentences S1 and S2. The mod-
els are trained incrementally: HMM is the base for
IBM model 3, which is the base for IBM model 4.
To train our models, we used 5 iterations of HMM,
and 3 iterations of IBM models 3 and 4.
Word alignments produced by these models are
asymmetric (S1 ? S2 6= S2 ? S1). To cope
with this, different heuristics (Koehn et al, 2005)
have been proposed to obtain symmetric alignments
from two asymmetric sets (S1 ? S2). We ex-
perimented with three symmetrization heuristics,
namely: union, intersection, and grow-diag-final-
and, a more complex symmetrization method which
combines intersection with some alignments from
the union.
To train the word alignment models we used
the Europarl parallel corpus (Koehn, 2005) con-
catenated with the News Commentary corpus2 for
2http://www.statmt.org/wmt11/
translation-task.html#download
three language pairs: English-German (2,079,049
sentences), English-Spanish (2,123,036 sentences),
English-French (2,144,820 sentences). For English-
Italian we only used the parallel data available in Eu-
roparl (1,909,115 sentences) since this language pair
is not covered by the News Commentary corpus.
For our submitted run the SVM classifiers were
trained using the whole training set. Such dataset
consists of 1,000 pairs for each of the four language
combinations, resulting from a concatenation of the
training and test sets used for the first round of eval-
uation at SemEval 2012 (Negri et al, 2012; Negri et
al., 2011). We have set a polynomial kernel with pa-
rameters empirically estimated on the training set:
C = 2.0, and d = 1. After some preliminary ex-
periments we have concluded that the HMM model
in conjunction with the intersection symmetrization
provides the best results.
Our results, calculated over the 500 test pairs pro-
vided for each language combination, are presented
in Table 3. As can be seen from the table, our system
consistently outperforms the best average run of all
participants and is the second best system for Span-
ish/English and Italian/English. For the other two
languages, French/English and German/English, it
is the 3rd best system with a larger distance from top
results. The motivations for such lower results, cur-
rently under investigation, might be related to lower
performance in terms of word alignment, the core
of our approach. The first step of our analysis will
hence address, and in case try to cope with, signifi-
cant differences in word alignment performance af-
fecting results.
Overall, considering the small distance from top
results, and the fact that our approach does not re-
quire deep linguistic processing to be reasonably ef-
fective for any language pair for which parallel cor-
pora are available, our results are encouraging and
motivate further research along such direction.
4 Conclusion
In this paper we presented the participation of the
Fondazione Bruno Kessler in the Semeval 2013
Task#8 on Cross-lingual Textual Entailment for
Content Synchronization. To identify entailment re-
lations between texts in different languages, our sys-
tem explores the use of word alignment features
130
Features / Language pair German/English Spanish/English French/English Italian/English
Avg best runs 0.378 0.404 0.407 0.405
ALTN 0.388 0.428 0.420 0.432
Best system 0.452 0.434 0.458 0.454
Table 1: Accuracy results for the language pairs evaluated for the average of the best runs of the participating systems,
our submission and the best systems.
within a supervised learning setting. In our ap-
proach, word alignment models obtained by statis-
tical methods from parallel corpora leverage infor-
mation about the number, the proportion, and the
distribution of aligned terms in the input sentences.
In terms of accuracy results over the SemEval 2013
CLTE test data, performance ranges from 38.8%
(for German/English) to 43.2% (for Italian/English).
On the Italian/English and Spanish/English test sets
our systems ranked second among five participants,
close to the top results (respectively 43.4% and
45.4%). Such results suggest that the use of word
alignment models to capture sentence-level seman-
tic relations in different language settings represents
a promising research direction.
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-248531).
References
Amit Bronner, Matteo Negri, Yashar Mehdad, Angela
Fahrni, and Christof Monz. 2012. CoSyne: Synchro-
nizing Multilingual Wiki Content. In Proceedings of
the Eighth Annual International Symposium on Wikis
and Open Collaboration, WikiSym ?12, pages 33:1?
33:4, New York, NY, USA. ACM.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263?311.
Julio J. Castillo. 2011. A WordNet-based Semantic Ap-
proach to Textual Entailment and Cross-lingual Tex-
tual Entailment. International Journal of Machine
Learning and Cybernetics, 2(3):177?189.
Ido Dagan and Oren Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic Applied Modeling of Lan-
guage Variability. In Proceedings of the PASCAL
Workshop of Learning Methods for Text Understand-
ing and Mining, Grenoble, France.
Miquel Espla`-Gomis, Felipe Sa?nchez-Mart??nez, and
Mikel L. Forcada. 2012. UAlacant: Using Online
Machine Translation for Cross-Lingual Textual Entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), pages
472?476, Montre?al, Canada.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
USA.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: an Update.
SIGKDD Explorations, 11(1):10?18.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proceedings of the International Workshop on Spo-
ken Language Translation, Pittsburgh, Pennsylvania,
USA.
Philip Koehn. 2005. Europarl: a Parallel Corpus for
Statistical Machine Translation. In Proceedings of
Machine Translation Summit X, pages 79?86, Phuket,
Thailand.
Milen Kouylekov and Matteo Negri. 2010. An Open-
source Package for Recognizing Textual Entailment.
In Proceedings of the ACL 2010 System Demonstra-
tions.
Milen Kouylekov, Luca Dini, Alessio Bosca, and Marco
Trevisan. 2012. CELI: an Experiment with Cross
Language Textual Entailment. In Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), pages 696?700, Montre?al, Canada.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment. In
Proceedings of the 11th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL HLT 2010).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using Bilingual Parallel Corpora for Cross-
Lingual Textual Entailment. In Proceedings of the
49th Annual Meeting of the Association for Compu-
131
tational Linguistics: Human Language Technologies
(ACL HLT 2011).
Yashar Mehdad, Matteo Negri, and Jose? Guilherme C.
de Souza. 2012a. FBK: cross-lingual textual entail-
ment without translation. In Proceedings of the 6th
International Workshop on Semantic Evaluation (Se-
mEval 2012), pages 701?705, Montre?al, Canada.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012b. Detecting Semantic Equivalence and Informa-
tion Disparity in Cross-lingual Documents. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012).
Fandong Meng, Hao Xiong, and Qun Liu. 2012. ICT:
A Translation based Cross-lingual Textual Entailment.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012).
Christoph Monz, Vivi Nastase, Matteo Negri, Angela
Fahrni, Yashar Mehdad, and Michael Strube. 2011.
CoSyne: a Framework for Multilingual Content Syn-
chronization of Wikis. In Proceedings of Wikisym
2011, the International Symposium on Wikis and Open
Collaboration, pages 217?218, Mountain View, Cali-
fornia, USA.
Matteo Negri, Milen Ognianov Kouylekov, Bernardo
Magnini, Yashar Mehdad, and Elena Cabrio. 2009.
Towards Extensible Textual Entailment Engines: the
EDITS Package. In AI*IA 2009: XIth International
Conference of the Italian Association for Artificial In-
telligence.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and Conquer: Crowdsourcing the Creation of
Cross-Lingual Textual Entailment Corpora. Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2011).
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 Task 8: Cross-Lingual Textual Entail-
ment for Content Synchronization. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation (SemEval 2012), pages 399?407, Montre?al,
Canada.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2013.
Semeval-2013 Task 8: Cross-lingual Textual Entail-
ment for Content Synchronization. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013).
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Alpa?r Perini. 2012. DirRelCond3: detecting textual en-
tailment across languages with conditions on direc-
tional text relatedness scores. In Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), pages 710?714, Montre?al, Canada.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th International Con-
ference on Computational Linguistics (ACL?96), pages
836?841, Copenhagen, Denmark.
Katharina Wa?schle and Sascha Fendrich. 2012. HDU:
Cross-lingual Textual Entailment with SMT Features.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012), pages 467?471,
Montre?al, Canada.
132
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 212?216,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Creating a Bi-lingual Entailment Corpus through Translations with
Mechanical Turk: $100 for a 10-day Rush
Matteo Negri1 and Yashar Mehdad1,2
FBK-Irst1, University of Trento2
Trento, Italy
{negri,mehdad}@fbk.eu
Abstract
This paper reports on experiments in the cre-
ation of a bi-lingual Textual Entailment cor-
pus, using non-experts? workforce under strict
cost and time limitations ($100, 10 days). To
this aim workers have been hired for transla-
tion and validation tasks, through the Crowd-
Flower channel to Amazon Mechanical Turk.
As a result, an accurate and reliable corpus of
426 English/Spanish entailment pairs has been
produced in a more cost-effective way com-
pared to other methods for the acquisition of
translations based on crowdsourcing. Focus-
ing on two orthogonal dimensions (i.e. relia-
bility of annotations made by non experts, and
overall corpus creation costs), we summarize
the methodology we adopted, the achieved re-
sults, the main problems encountered, and the
lessons learned.
1 Introduction
Textual Entailment (TE) (Dagan and Glickman,
2004) has been proposed as a generic framework for
modelling language variability. Given a text T and
an hypothesis H, the task consists in deciding if the
meaning of H can be inferred from the meaning of
T. At the monolingual level, the great potential of
integrating TE recognition (RTE) components into
NLP architectures has been demonstrated in several
areas, including question answering, information re-
trieval, information extraction, and document sum-
marization. In contrast, mainly due to the absence of
cross-lingual TE (CLTE) recognition components,
similar improvements have not been achieved yet
in any cross-lingual application. Along such di-
rection, focusing on feasibility and architectural is-
sues, (Mehdad et al, 2010) recently proposed base-
line results demonstrating the potential of a simple
approach that integrates Machine Translation and
monolingual TE components.
As a complementary research problem, this paper
addresses the data collection issue, focusing on the
definition of a fast, cheap, and reliable methodology
to create CLTE corpora. The main motivation is that,
as in many other NLP areas, the availability of large
quantities of annotated data represents a critical bot-
tleneck in the systems? development/evaluation cy-
cle. Our first step in this direction takes advantage
of an already available monolingual corpus, casting
the problem as a translation one. The challenge con-
sists in taking a publicly available RTE dataset of
English T-H pairs (i.e. the PASCAL-RTE3 dataset1),
and create its English-Spanish CLTE equivalent by
translating the hypotheses into Spanish. To this
aim non-expert workers have been hired through
the CrowdFlower2 channel to Amazon Mechanical
Turk3 (MTurk), a crowdsourcing marketplace re-
cently used with success for a variety of NLP tasks
(Snow et al, 2008; Callison-Burch, 2009; Mihalcea
and Strapparava, 2009; Marge et al, 2010; Ambati
et al, 2010).
The following sections overview our experiments,
carried out under strict time (10 days) and cost
($100) limitations. In particular, Section 2 describes
our data acquisition process; Section 3 summarizes
1Available at: http://www.nist.gov/tac/data/RTE/index.html
2http://crowdflower.com/
3https://www.mturk.com/mturk/
212
the successive approximations that led to the defini-
tion of our methodology, and the lessons learned at
each step; Section 4 concludes the paper and pro-
vides directions for future work.
2 Corpus creation cycles
Starting from the RTE3 Development set (800 En-
glish T-H pairs), our corpus creation process has
been organized in sentence translation-validation
cycles, defined as separate ?jobs? routed to Crowd-
Fower?s workforce. At the first stage of each cycle,
the original English hypotheses are used to create a
translation job for collecting their Spanish equiva-
lents. At the second stage, the collected translations
are used to create a validation job, where multiple
judges are asked to check the correctness of each
translation, given the English source. Translated hy-
potheses that are positively evaluated by the major-
ity of trustful validators (i.e. those judged correct
with a confidence above 0.8) are retained, and di-
rectly stored in our CLTE corpus together with the
corresponding English texts. The remaining ones
are used to create a new translation job. The proce-
dure is iterated until substantial agreement for each
translated hypothesis is reached.
As regards the first phase of the cycle, we defined
our translation HIT as follows:
In this task you are asked to:
? First, judge if the Spanish sentence is a correct
translation of the English sentence. If the En-
glish sentence and its Spanish translation are blank
(marked as -), you can skip this step.
? Then, translate the English sentence above the text
box into Spanish.
Please make sure that your translation is:
1. Faithful to the original phrase in both meaning and
style.
2. Grammatically correct.
3. Free of spelling errors and typos.
Don?t use any automatic (machine) translation tool! You
can have a look at any on-line dictionary or reference
for the meaning of a word.
This HIT asks workers to first check the qual-
ity of an English-Spanish translation (used as a gold
unit), and then write the Spanish translation of a
new English sentence. The quality check allows
to collect accurate translations, by filtering out
judgments made by workers missing more than
20% of the gold units.
As regards the second phase of the cycle, our
validation HIT has been defined as follows:
Su tarea es verificar si la traduccio?n dada de una
frase del Ingle?s al espaol es correcta o no. La traduccio?n
es correcta si:
1. El estilo y sentido de la frase son fieles a los de la
original.
2. Es gramaticalmente correcta.
3. Carece de errores ortogra?ficos y tipogra?ficos.
Nota: el uso de herramientas de traduccio?n automa?tica
(ma?quina) no esta? permitido!
This HIT asks workers to take binary decisions
(Yes/No) for a set of English-Spanish translations
including gold units. The title and the description
are written in Spanish in order to weed out untrusted
workers (i.e. those speaking only English), and
attract the attention of Spanish speakers.
In our experiments, both the translation and vali-
dation jobs have been defined in several ways, trying
to explore different strategies to quickly collect reli-
able data in a cost effective way. Such cost reduction
effort led to the following differences between our
work and similar related approaches documented in
literature (Callison-Burch, 2009; Snow et al, 2008):
? Previous works built on redundancy of the col-
lected translations (up to 5 for each source
sentence), thus resulting in more costly jobs.
For instance, adopting a redundancy-based ap-
proach to collect 5 translations per sentence at
the cost of $0.01 each, and 5 validations per
translation at the cost of $0.002 each, would re-
sult in $80 for 800 sentences.
Assuming that the translation process is com-
plex and expensive, our cycle-based technique
builds on simple and cheap validation mech-
anisms that drastically reduce the amount of
translations required. In our case, 1 translation
per sentence at the cost of $0.01, and 5 valida-
tions per translation at the cost of $0.002 each,
213
would result in $32 for 800 sentences, making
a conservative assumption of up to 8 iterations
with 50% wrong translations at each cycle (i.e.
800 sentences in the first cycle, 400 in the sec-
ond, 200 in the third, etc.).
? Previous works involving validation of the col-
lected data are based on ranking/voting mecha-
nisms, where workers are asked to order a num-
ber of translations, or select the best one given
the source. Our approach to validation is based
on asking workers to take binary decisions over
source-target pairs. This results in an easier,
faster, and eventually cheaper task.
? Previous works did not use any specific method
to qualify the workers? knowledge, apart from
post-hoc agreement computation. Our ap-
proach systematically includes gold units to fil-
ter out untrusted workers during the process.
As a result we pay only for qualified judgments.
3 Experiments and lessons learned
The overall methodology, and the definition of the
HITs described in Section 2, are the result of suc-
cessive approximations that took into account two
correlated aspects: the quality of the collected trans-
lations, and the current limitations of the Crowd-
Flower service. On one side, simpler, cheaper, and
faster jobs launched in the beginning of our experi-
ments had to be refined to improve the quality of the
retained translations. On the other side, ad-hoc solu-
tions had to be found to cope with the limited quality
control functionalities provided by CrowdFlower. In
particular, the lack of regional qualifications of the
workers, and of any qualification tests mechanism
(useful features of MTurk) raised the need of defin-
ing more controlled, but also more expensive jobs.
Table 1 and the rest of this section summarize the
progress of our work in defining the methodology
adopted, the main improvements experimented at
each step, the overall costs, and the lessons learned.
Step 1: a na??ve approach. Initially, transla-
tion/validation jobs were defined without using qual-
ification mechanisms, giving permission to any
worker to complete our HITs. In this phase, our goal
was to estimate the trade-off between the required
development time, the overall costs, and the qual-
ity of translations collected in the most na??ve condi-
tions.
As expected, the job accomplishment time was
negligible, and the overall cost very low. More
specifically, it took about 1 hour for translating the
800 hypotheses at the cost of $12, and less than 6
hours to obtain 5 validations per each translation at
the same cost of $12.
Nevertheless, as revealed by further experiments
with the introduction of gold units, the quality of the
collected translations was poor. In particular, 61% of
them should have been rejected, often due to gross
mistakes. As an example, among the collected mate-
rial several translations in languages other than En-
glish revealed a massive and defective use of on-line
translation tools by untrusted workers, as also ob-
served by (Callison-Burch, 2009).
Step 2: reducing validation errors. A first im-
provement addressed the validation phase, where
we introduced gold units as a mechanism to qual-
ify the workers, and consequently prune the un-
trusted ones. To this aim, we launched the valida-
tion HIT described in Section 2, adding around 50
English-Spanish control pairs. The pairs (equally
distributed into positive and negative samples) have
been extracted from the collected data, and manually
checked by a Spanish native speaker.
The positive effect of using gold units has been
verified in two ways. First, we checked the quality
of the translations collected in the first na??ve transla-
tion job, by counting the number of rejections (61%)
after running the improved validation job. Then, we
manually checked the quality of the translations re-
tained with the new job. A manual check on 20% of
the retained translations was carried out by a Span-
ish native speaker, resulting in 97% Accuracy. The
3% errors encountered are equally divided into mi-
nor translation errors, and controversial (but sub-
stantially acceptable) cases due to regional Spanish
variations.
The considerable quality improvement observed
has been obtained with a small increase of 25% in
the cost (less than $3). However, as regards the ac-
complishment time, adding the gold units to qualify
workers led to a considerable increase in duration
(about 4 days for the first iteration). This is mainly
214
due to the high number of automatically rejected
judgments, obtained from untrusted workers miss-
ing the gold units. Because of the discrepancy be-
tween trusted and untrusted judgments, we faced an-
other limitation of the CrowdFlower service, which
further delayed our experiments. Often, in fact, the
rapid growth of untrusted judgments activates auto-
matic pausing mechanisms, based on the assumption
that gold units are not accurate. This, however, is a
strong assumption which does not take into account
the huge amount of non-qualified workers accepting
(or even just playing with) the HITs. For instance,
in our case the vast majority of errors came from
workers located in specific regions where the native
language is not Spanish nor English.
Step 3: reducing translation errors. The ob-
served improvement obtained by introducing gold
units in the validation phase, led us to the definition
of a new translation task, also involving a similar
qualification mechanism. To this aim, due to lan-
guage variability, it was clearly impossible to use
reference translations as gold units. Taking into ac-
count the limitations of the CrowdFlower interface,
which does not allow to set qualification tests or
split the jobs into sequential subtasks (other effec-
tive and widely used features of MTurk), we solved
the problem by defining the translation HITs as de-
scribed in Section 2. This solution combines a va-
lidity check and a translation task, and proved to be
effective with a decrease in the translations eventu-
ally rejected (45%).
Step 4: reducing time. Considering the extra time
required by using gold units, we decided to spend
more money on each HIT to boost the speed of our
jobs. In addition, to overcome the delays caused by
the automatic pausing mechanism, we obtained from
CrowdFlower the possibility to pose regional quali-
fication, as commonly used in MTurk.
As expected, both solutions proved to be effective,
and contributed to the final definition of our method-
ology. On one side, doubling the payment for each
task (from $0.01 to $0.02 for each translation and
from from $0.002 to $0.005 for each validation), we
halved the required time to finish each job. On the
other side, by imposing the regional qualification,
we eventually avoided unexpected automatic pauses.
4 Conclusion and future work
We presented a set of experiments targeting the cre-
ation of bi-lingual Textual Entailment corpora by
means of non experts? workforce (i.e. the Crowd-
Flower channel to Amazon Mechanical Turk).
As a first step in this direction, we took advantage
of an already existing monolingual English RTE cor-
pus, casting the problem as a translation task where
Spanish translations of the hypotheses are collected
and validated by the workers. Strict time and cost
limitations on one side, and the current limitations
of the CrowdFlower service on the other side, led
us to the definition of an effective corpus creation
methodology. As a result, less than $100 were spent
in 10 days to define such methodology, leading to
collect 426 pairs as a by-product. However, it?s
worth remarking that applying this technique to cre-
ate the full corpus would cost about $30.
The limited costs, together with the short time re-
quired to acquire reliable results, demonstrate the
effectiveness of crowdsourcing services for simple
sentence translation tasks. However, while MTurk is
already a well tested, stable, and rich of functional-
ities platform, some limitations emerged during our
experience with the more recent CrowdFlower ser-
vice (currently the only one accessible to non-US
citizens). Some of these limitations, such as the
regional qualification mechanism, have been over-
come right after the end of our experimentation with
the introduction of new functionalities provided as
?Advanced Options?. Others (such as the lack of
other qualification mechanisms, and the automatic
pausing of the HITs in case of high workers? error
rates on the gold units) at the moment still represent
a possible complication, and have to be carefully
considered when designing experiments and inter-
preting the results4.
In light of this positive experience, next steps
in our research will further explore crowdsourcing-
based data acquisition methods to address the com-
plementary problem of collecting new entailment
pairs from scratch. This will allow to drastically re-
duce data collection bottlenecks, and boost research
both on cross-lingual and mono-lingual Textual En-
4However, when asked through the provided support ser-
vice, the CrowdFlower team proved to be quite reactive in pro-
viding ad-hoc solutions to specific problems.
215
Elapsed time Running cost Focus Lessons learned
1 day $24 Approaching CrowdFlower,
defining a na??ve methodology
Need of qualification mechanism,
task definition in Spanish.
7 days $58 Improving validation Qualification mechanisms (gold units
and regional) are effective, need of
payment increase to boost speed.
9 days $99.75 Improving translation Combined HIT for qualification, pay-
ment increase worked!
10 days $99.75 Obtaining bi-lingual RTE corpus Fast, cheap, and reliable method.
Table 1: $100 for a 10-day rush (summary and lessons learned)
tailment.
Acknowledgments
We would like to thank MTurk and CrowdFlower
for providing the $100 credit used for the experi-
ments, and our colleague Silvana Bernaola Biggio,
who kindly accepted to validate our results.
The research leading to these results has re-
ceived funding from the European Community?s
Seventh Framework Programme (FP7/2007-2013)
under Grant Agreement n. 248531.
References
V. Ambati, S. Vogel and J. Carbonell 2010. Active
Learning and Crowd-Sourcing for Machine Transla-
tion. To appear in Proceedings of LREC 2010.
C. Callison-Burch 2009. Fast, Cheap, and Creative:
Evaluating Translation Quality Using Amazon?s Me-
chanical Turk. In Proceedings of EMNLP 2009.
I. Dagan and O. Glickman 2004. Probabilistic Textual
Entailment: Generic Applied Modeling of Language
Variability. In Proceedings of the PASCAL Workshop
of Learning Methods for Text Understanding and Min-
ing.
M. Marge, S. Banerjee and A. Rudnicky 2010. Using the
Amazon Mechanical Turk for Transcription of Spoken
Language. In Proceedings of the 2010 IEEE Interna-
tional Conference on Acoustics, Speech and Spoken
Language (ICASSP 2010).
Y. Mehdad, M. Negri, and M. Federico 2010. Towards
Cross-Lingual Textual Entailment. To appear in Pro-
ceedings of NAACL HLT 2010.
R. Mihalcea and C. Strapparava 2009. The Lie Detector:
Explorations in the Automatic Recognition of Decep-
tive Language. In Proceedings of ACL 2009.
R. Snow, B. O?Connor, D. Jurafsky, and A. Y. Ng 2008.
Cheap and Fast - but is it Good? Evaluating Non-
expert Annotations for Natural Language Tasks. In
Proceedings of EMNLP 2008.
216
Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 30?34,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Is it Worth Submitting this Run?
Assess your RTE System with a Good Sparring Partner
Milen Kouylekov
CELI s.r.l.
Turin, Italy
kouylekov@celi.it
Yashar Mehdad
FBK-irst and University of Trento
Trento, Italy
mehdad@fbk.eu
Matteo Negri
FBK-irst
Trento, Italy
negri@fbk.eu
Abstract
We address two issues related to the devel-
opment of systems for Recognizing Textual
Entailment. The first is the impossibility to
capitalize on lessons learned over the different
datasets available, due to the changing nature
of traditional RTE evaluation settings. The
second is the lack of simple ways to assess
the results achieved by our system on a given
training corpus, and figure out its real potential
on unseen test data. Our contribution is the ex-
tension of an open-source RTE package with
an automatic way to explore the large search
space of possible configurations, in order to
select the most promising one over a given
dataset. From the developers? point of view,
the efficiency and ease of use of the system,
together with the good results achieved on all
previous RTE datasets, represent a useful sup-
port, providing an immediate term of compar-
ison to position the results of their approach.
1 Introduction
Research on textual entailment (TE) has received a
strong boost by the Recognizing Textual Entailment
(RTE) Challenges, organized yearly to gather the
community around a shared evaluation framework.
Within such framework, besides the intrinsic diffi-
culties of the task (i.e. deciding, given a set of Text-
Hypothesis pairs, if the hypotheses can be inferred
from the meaning of the texts), the development of
RTE systems has to confront with a number of ad-
ditional problems and uncertainty factors. First of
all, since RTE systems are usually based on com-
plex architectures that integrate a variety of tools and
resources, it is per se very difficult to tune them and
define the optimal configuration given a new dataset.
In general, when participating to the evaluation chal-
lenges there?s no warranty that the submitted runs
are those obtained with the best possible configura-
tion allowed by the system. Second, the evaluation
settings change along the years. Variations in the
length of the texts, the origin of the pairs, the bal-
ance between positive and negative examples, and
the type of entailment decisions allowed, reflect the
need to move from easier and more artificial settings
to more complex and natural ones. However, in con-
trast with other more stable tasks in terms of eval-
uation settings and metrics (e.g. machine transla-
tion), such changes make it difficult to capitalize on
the experience obtained by participants throughout
the years. Third, looking at RTE-related literature
and the outcomes of the six campaigns organised so
far, the conclusions that can be drawn are often con-
troversial. For instance, it is not clear whether the
availability of larger amounts of training data corre-
lates with better performance (Hickl et al, 2006) or
not (Zanzotto et al, 2007; Hickl and Bensley, 2007),
even within the same evaluation setting. In addi-
tion, ablation tests carried out in recent editions of
the challenge do not allow for definite conclusions
about the actual usefulness of tools and resources,
even the most popular ones (Bentivogli et al, 2009).
Finally, the best performing systems often have dif-
ferent natures from one year to another, showing al-
ternations of deep (Hickl and Bensley, 2007; Tatu
and Moldovan, 2007) and shallow approaches (Jia
et al, 2010) ranked at the top positions. In light
of these considerations, it would be useful for sys-
30
tems developers to have: i) automatic ways to sup-
port systems? tuning at a training stage, and ii) reli-
able terms of comparison to validate their hypothe-
ses, and position the results of their work before sub-
mitting runs for evaluation. In this paper we address
these needs by extending an open-source RTE pack-
age (EDITS1) with a mechanism that automatizes
the selection of the most promising configuration
over a training dataset. We prove the effectiveness
of such extension showing that it allows not only to
achieve good performance on all the available RTE
Challenge datasets, but also to improve the official
results, achieved with the same system, through ad
hoc configurations manually defined by the devel-
opers team. Our contribution is twofold. On one
side, in the spirit of the collaborative nature of open
source projects, we extend an existing tool with a
useful functionality that was still missing. On the
other side, we provide a good ?sparring partner? for
system developers, to be used as a fast and free term
of comparison to position the results of their work.
2 ?Coping? with configurability
EDITS (Kouylekov and Negri, 2010) is an open
source RTE package, which offers a modular, flex-
ible, and adaptable working environment to experi-
ment with the RTE task over different datasets. The
package allows to: i) create an entailment engine
by defining its basic components (i.e. algorithms,
cost schemes, rules, and optimizers); ii) train such
entailment engine over an annotated RTE corpus to
learn a model; and iii) use the entailment engine and
the model to assign an entailment judgement and a
confidence score to each pair of an un-annotated test
corpus. A key feature of EDITS is represented by its
high configurability, allowed by the availability of
different algorithms, the possibility to integrate dif-
ferent sets of lexical entailment/contradiction rules,
and the variety of parameters for performance opti-
mization (see also Mehdad, 2009). Although config-
urability is per se an important aspect (especially for
an open-source and general purpose system), there
is another side of the coin. In principle, in order to
select the most promising configuration over a given
development set, one should exhaustively run a huge
number of training/evaluation routines. Such num-
1http://edits.fbk.eu/
ber corresponds to the total number of configura-
tions allowed by the system, which result from the
possible combinations of parameter settings. When
dealing with enlarging dataset sizes, and the tight
time constraints usually posed by the evaluation
campaigns, this problem becomes particularly chal-
lenging, as developers are hardly able to run exhaus-
tive training/evaluation routines. As recently shown
by the EDITS developers team, such situation re-
sults in running a limited number of experiments
with the most ?reasonable? configurations, which
consequently might not lead to the optimal solution
(Kouylekov et al, 2010).
The need of a mechanism to automatically ob-
tain the most promising solution on one side, and
the constraints posed by the evaluation campaigns
on the other side, arise the necessity to optimize
this procedure. Along this direction, the objective
is good a trade-off between exhaustive experimen-
tation with all possible configurations (unfeasible),
and educated guessing (unreliable). The remainder
of this section tackles this issue introducing an op-
timization strategy based on genetic algorithms, and
describing its adaptation to extend EDITS with the
new functionality.
2.1 Genetic algorithm
Genetic algorithms (GA) are well suited to effi-
ciently deal with large search spaces, and have been
recently applied with success to a variety of opti-
mization problems and specific NLP tasks (Figueroa
and Neumann, 2008; Otto and Riff, 2004; Aycinena
et al, 2003). GA are a direct stochastic method for
global search and optimization, which mimics natu-
ral evolution. To this aim, they work with a popu-
lation of individuals, representing possible solutions
to the given task. Traditionally, solutions are rep-
resented in binary as strings of 0s and 1s, but other
encodings (e.g. sequences of real values) are possi-
ble. The evolution usually starts from a population
of randomly generated individuals, and at each gen-
eration selects the best-suited individuals based on
a fitness function (which measures the optimality of
the solution obtained by the individual). Such selec-
tion is then followed by modifications of the selected
individuals obtained by recombining (crossover) and
performing random changes (mutation) to form a
new population, which will be used in the next iter-
31
ation. Finally, the algorithm is terminated when the
maximum number of generations, or a satisfactory
fitness level has been reached for the population.
2.2 EDITS-GA
Our extension to the EDITS package, EDITS-GA,
consists in an iterative process that starts with an
initial population of randomly generated configura-
tions. After a training phase with the generated con-
figurations, the process is evaluated by means of the
fitness function, which is manually defined by the
user2. This measure is used by the genetic algo-
rithm to iteratively build new populations of config-
urations, which are trained and evaluated. This pro-
cess can be seen as the combination of: i) a micro
training/evaluation routine for each generated con-
figuration of the entailment engine; and ii) a macro
evolutionary cycle, as illustrated in Figure 1. The
fitness function is an important factor for the evalu-
ation and the evolution of the generated configura-
tions, as it drives the evolutionary process by deter-
mining the best-suited individuals used to generate
new populations. The procedure to estimate and op-
timize the best configuration applying the GA, can
be summarized as follows.
(1) Initialization: generate a random initial popula-
tion (i.e. a set of configurations).
(2) Selection:
2a. The fitness function (accuracy, or F-measure)
is evaluated for each individual in the population.
2b. The individuals are selected according to their
fitness function value.
(3) Reproduction: generate a new population of
configurations from the selected one, through ge-
netic operators (cross-over and mutation).
(4) Iteration: repeat the Selection and Reproduction
until Termination.
(5) Termination: end if the maximum number of
iterations has been reached, or the population has
converged towards a particular solution.
In order to extend EDITS with genetic algo-
rithms, we used a GA implementation available in
the JGAP tool3. In our settings, each individual con-
tains a sequence of boolean parameters correspond-
2For instance, working on the RTE Challenge ?Main? task
data, the fitness function would be the accuracy for RTE1 to
RTE5, and the F-measure for RTE6.
3http://jgap.sourceforge.net/
Figure 1: EDITS-GA framework.
ing to the activation/de-activation of the system?s
basic components (algorithms, cost schemes, rules,
and optimizers). The configurations corresponding
to such individuals constitute the populations itera-
tively evaluated by EDITS-GA on a given dataset.
3 Experiments
Our experiments were carried out over the datasets
used in the six editions of the RTE Challenge
(?Main? task data from RTE1 to RTE6). For each
dataset we obtained the best model by training
EDITS-GA over the development set, and evaluat-
ing the resulting model on the test pairs. To this
aim, the optimization process is iterated over all
the available algorithms in order to select the best
combination of parameters. As termination crite-
rion, we set to 20 the maximum number of itera-
tions. To increase efficiency, we extended EDITS
to pre-process each dataset using the tokenizer and
stemmer available in Lucene4. This pre-processing
phase is automatically activated when the EDITS-
GA has to process non-annotated datasets. How-
ever, we also annotated the RTE corpora with the
Stanford parser plugin (downloadable from the ED-
ITS websitein order to run the syntax-based algo-
rithms available (e.g. tree edit distance). The num-
ber of boolean parameters used to generate the con-
figurations is 18. In light of this figure, it becomes
evident that the number of possible configurations
is too large (218=262,144) for an exhaustive train-
ing/evaluation routine over each dataset5. However,
4http://lucene.apache.org/
5In an exploratory experiment we measured in around 4
days the time required to train EDITS, with all possible con-
32
# Systems Best Lowest Average EDITS (rank) EDITS-GA (rank) % Impr. Comp. Time
RTE1 15 0.586 0.495 0.544 0.559 (8) 0.5787 (3) +3.52% 8m 24s
RTE2 23 0.7538 0.5288 0.5977 0.605 (6) 0.6225 (5) +2.89% 9m 8s
RTE3 26 0.8 0.4963 0.6237 - 0.6875 (4) - 9m
RTE4 26 0.746 0.516 0.5935 0.57 (17) 0.595 (10) +4.38% 30m 54s
RTE5 20 0.735 0.5 0.6141 0.6017 (14) 0.6233 (9) +3.58% 8m 23s
RTE6 18 0.4801 0.116 0.323 0.4471 (4) 0.4673 (3) +4.51% 1h 54m 20s
Table 1: RTE results (acc. for RTE1-RTE5, F-meas. for RTE6). For each participant, only the best run is considered.
with an average of 5 reproductions on each iteration,
EDITS-GA makes an average of 100 configurations
for each algorithm. Thanks to EDITS-GA, the aver-
age number of evaluated configurations for a single
dataset is reduced to around 4006.
Our results are summarized in Table 1, showing
the total number of participating systems in each
RTE Challenge, together with the highest, lowest,
and average scores they achieved. Moreover, the of-
ficial results obtained by EDITS are compared with
the performance achieved with EDITS-GA on the
same data. We can observe that, for all datasets,
the results achieved by EDITS-GA significantly im-
prove (up to 4.51%) the official EDITS results. It?s
also worth mentioning that such scores are always
higher than the average ones obtained by partici-
pants. This confirms that EDITS-GA can be poten-
tially used by RTE systems developers as a strong
term of comparison to assess the capabilities of
their own system. Since time is a crucial factor for
RTE systems, it is important to remark that EDITS-
GA allows to converge on a promising configura-
tion quite efficiently. As can be seen in Table 1,
the whole process takes around 9 minutes7 for the
smaller datasets (RTE1 to RTE5), and less than 2
hours for a very large dataset (RTE6). Such time
analysis further proves the effectiveness of the ex-
tended EDITS-GA framework. For the sake of com-
pleteness we gave a look at the differences between
the ?educated guessing? done by the EDITS de-
velopers for the official RTE submissions, and the
?optimal? configuration automatically selected by
EDITS-GA. Surprisingly, in some cases, even a mi-
nor difference in the selected parameters leads to
figurations, over small datasets (RTE1 to RTE5).
6With these settings, training EDITS-GA over small datasets
(RTE1 to RTE5) takes about 9 minutes each.
7All time figures are calculated on an Intel(R) Xeon(R),
CPU X3440 @ 2.53GHz, 8 cores with 8 GB RAM.
significant gaps in the results. For instance, in RTE6
dataset, the ?guessed? configuration (Kouylekov et
al., 2010) was based on the lexical overlap algo-
rithm, setting the cost of replacing H terms with-
out an equivalent in T to the minimal Levenshtein
distance between such words and any word in T.
EDITS-GA estimated, as a more promising solution,
a combination of lexical overlap with a different cost
scheme (based on the IDF of the terms in T). In ad-
dition, in contrast with the ?guessed? configuration,
stop-words filtering was selected as an option, even-
tually leading to a 4.51% improvement over the of-
ficial RTE6 result.
4 Conclusion
?Is it worth submitting this run??,?How good is my
system??. These are the typical concerns of system
developers approaching the submission deadline of
an RTE evaluation campaign. We addressed these is-
sues by extending an open-source RTE system with
a functionality that allows to select the most promis-
ing configuration over an annotated training set. Our
contribution provides developers with a good ?spar-
ring partner? (a free and immediate term of compar-
ison) to position the results of their approach. Ex-
perimental results prove the effectiveness of the pro-
posed extension, showing that it allows to: i) achieve
good performance on all the available RTE datasets,
and ii) improve the official results, achieved with the
same system, through ad hoc configurations manu-
ally defined by the developers team.
Acknowledgments
This work has been partially supported by the EC-
funded projects CoSyne (FP7-ICT-4-24853), and
Galateas (CIP-ICT PSP-2009-3-250430).
33
References
Margaret Aycinena, Mykel J. Kochenderfer, and David
Carl Mulford. 2003. An Evolutionary Approach to
Natural Language Grammar Induction. Stanford CS
224N Natural Language Processing.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, Bernardo Magnini. 2009. The Fifth
PASCAL Recognizing Textual Entailment Challenge.
Proceedings of the TAC 2009 Workshop.
Ido Dagan and Oren Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. Proceedings of the PASCAL Workshop of
Learning Methods for Text Understanding and Min-
ing.
Alejandro G. Figueroa and Gu?nter Neumann. 2008. Ge-
netic Algorithms for Data-driven Web Question An-
swering. Evolutionary Computation 16(1) (2008) pp.
89-125.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing Textual Entailment with LCCs Groundhog Sys-
tem. Proceedings of the Second PASCAL Challenges
Workshop.
Andrew Hickl and Jeremy Bensley. 2007. A discourse
commitment-based framework for recognizing textual
entailment. Proceedings of the ACL-PASCAL Work-
shop on Textual Entailment and Paraphrasing.
Houping Jia, Xiaojiang Huang, Tengfei Ma, Xiaojun
Wan, and Jianguo Xiao. 2010. PKUTM Participation
at TAC 2010 RTE and Summarization Track. Proceed-
ings of the Sixth Recognizing Textual Entailment Chal-
lenge.
Milen Kouylekov and Matteo Negri. 2010. An Open-
source Package for Recognizing Textual Entailment.
Proceedings of ACL 2010 Demo session.
Milen Kouylekov, Yashar Mehdad, Matteo Negri, and
Elena Cabrio. 2010. FBK Participation in RTE6:
Main and KBP Validation Task. Proceedings of the
Sixth Recognizing Textual Entailment Challenge.
Yashar Mehdad 2009. Automatic Cost Estimation for
Tree Edit Distance Using Particle Swarm Optimiza-
tion. Proceedings of ACL-IJCNLP 2009.
Eridan Otto and Mar??a Cristina Riff 2004. Towards an
efficient evolutionary decoding algorithm for statisti-
cal machine translation. LNAI, 2972:438447..
Marta Tatu and Dan Moldovan. 2007. COGEX at RTE3.
Proceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing.
Fabio Massimo Zanzotto, Marco Pennacchiotti and
Alessandro Moschitti. 2007. Shallow Semantics in
Fast Textual Entailment Rule Learners. Proceedings
of the Third Recognizing Textual Entailment Chal-
lenge.
34
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 171?180,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Match without a Referee:
Evaluating MT Adequacy without Reference Translations
Yashar Mehdad Matteo Negri Marcello Federico
Fondazione Bruno Kessler, FBK-irst
Trento , Italy
{mehdad|negri|federico}@fbk.eu
Abstract
We address two challenges for automatic ma-
chine translation evaluation: a) avoiding the
use of reference translations, and b) focusing
on adequacy estimation. From an economic
perspective, getting rid of costly hand-crafted
reference translations (a) permits to alleviate
the main bottleneck in MT evaluation. From
a system evaluation perspective, pushing se-
mantics into MT (b) is a necessity in order
to complement the shallow methods currently
used overcoming their limitations. Casting
the problem as a cross-lingual textual entail-
ment application, we experiment with differ-
ent benchmarks and evaluation settings. Our
method shows high correlation with human
judgements and good results on all datasets
without relying on reference translations.
1 Introduction
While syntactically informed modelling for statis-
tical MT is an active field of research that has re-
cently gained major attention from the MT commu-
nity, work on integrating semantic models of ade-
quacy into MT is still at preliminary stages. This sit-
uation holds not only for system development (most
current methods disregard semantic information, in
favour of statistical models of words distribution),
but also for system evaluation. To realize its full po-
tential, however, MT is now in the need of semantic-
aware techniques, capable of complementing fre-
quency counts with meaning representations.
In order to integrate semantics more deeply into
MT technology, in this paper we focus on the eval-
uation dimension. Restricting our investigation to
some of the more pressing issues emerging from this
area of research, we provide two main contributions.
1. An automatic evaluation method that avoids
the use of reference translations. Most current
metrics are based on comparisons between auto-
matic translations and human references, and reward
lexical similarity at the n-gram level (e.g. BLEU
(Papineni et al, 2002), NIST (Doddington, 2002),
METEOR (Banerjee and Lavie, 2005), TER (Snover
et al, 2006)). Due to the variability of natural lan-
guages in terms of possible ways to express the same
meaning, reliable lexical similarity metrics depend
on the availability of multiple hand-crafted (costly)
realizations of the same source sentence in the tar-
get language. Our approach aims to avoid this bot-
tleneck by adapting cross-lingual semantic inference
capabilities and judging a translation only given the
source sentence.
2. A method for evaluating translation adequacy.
Most current solutions do not consistently reward
translation adequacy (semantic equivalence between
source sentence and target translation). The scarce
integration of semantic information in MT, specif-
ically at the multilingual level, led to MT systems
that are ?illiterate? in terms of semantics and mean-
ing. Moreover, current metrics are often difficult to
interpret. In contrast, our method targets the ade-
quacy dimension, producing easily interpretable re-
sults (e.g. judgements in a 4-point scale).
Our approach builds on recent advances in
cross-lingual textual entailment (CLTE) recognition,
which provides a natural framework to address MT
adequacy evaluation. In particular, we approach
the problem as an application of CLTE where bi-
171
directional entailment between source and target is
considered as evidence of translation adequacy. Be-
sides avoiding the use of references, the proposed
solution differs from most previous methods which
typically rely on surface-level features, often ex-
tracted from the source or the target sentence taken
in isolation. Although some of these features might
correlate well with adequacy, they capture seman-
tic equivalence only indirectly, and at the level of
a probabilistic prediction. Focusing on a combina-
tion of surface, syntactic and semantic features, ex-
tracted from both source and target (e.g. ?source-
target length ratio?, ?dependency relations in com-
mon?), our approach leads to informed adequacy
judgements derived from the actual observation of
a translation given the source sentence.
2 Background
Some recent works proposed metrics able to approx-
imately assess meaning equivalence between can-
didate and reference translations. Among these,
(Gime?nez and Ma`rquez, 2007) proposed a hetero-
geneous set comprising overlapping and matching
metrics, compiled from a rich set of variants at five
different linguistic levels: lexical, shallow-syntactic,
syntactic, shallow-semantic and semantic. More
similar to our approach, (Pado? et al, 2009) proposed
semantic adequacy metrics that exploit feature rep-
resentations motivated by Textual Entailment (TE).
Both metrics, however, highly depend on the avail-
ability of multiple reference translations.
Early attempts to avoid reference translations ad-
dressed quality estimation (QE) by means of large
numbers of source, target, and system-dependent
features to discriminate between ?good? and ?bad?
translations (Blatz et al, 2004; Quirk, 2004). More
recently (Specia et al, 2010b; Specia and Farzindar,
2010; Specia, 2011) conducted a series of experi-
ments using features designed to estimate translation
post-editing effort (in terms of volume and time) as
an indicator of MT output quality. Good results in
QE have been achieved by adding linguistic infor-
mation such as shallow parsing, POS tags (Xiong
et al, 2010), or dependency relations (Bach et al,
2011; Avramidis et al, 2011) as features. However,
in general these approaches do not distinguish be-
tween fluency (i.e. syntactic correctness of the out-
put translation) and adequacy, and mostly rely on
fluency-oriented features (e.g. ?number of punctu-
ation marks?). As a result, a simple surface form
variation is given the same importance of a content
word variation that changes the meaning of the sen-
tence. To the best of our knowledge, only (Specia et
al., 2011) proposed an approach to frame MT evalu-
ation as an adequacy estimation problem. However,
their method still includes many features which are
not focused on adequacy, and often look either at the
source or at the target in isolation (see for instance
?source complexity? and ?target fluency? features).
Moreover, the actual contribution of the adequacy
features used is not always evident and, for some
testing conditions, marginal.
Our approach to adequacy evaluation builds on
and extends the above mentioned works. Similarly
to (Pado? et al, 2009) we rely on the notion of textual
entailment, but we cast it as a cross-lingual problem
in order to bypass the need of reference translations.
Similarly to (Blatz et al, 2004; Quirk, 2004), we try
to discriminate between ?good? and ?bad? transla-
tions, but we focus on adequacy. To this aim, like
(Xiong et al, 2010; Bach et al, 2011; Avramidis et
al., 2011; Specia et al, 2010b; Specia et al, 2011)
we rely on a large number of features, but focusing
on source-target dependent ones, aiming at informed
adequacy evaluation of a translation given the source
instead of a more generic quality assessment based
on surface features.
3 CLTE for adequacy evaluation
We address adequacy evaluation by adapting cross-
lingual textual entailment recognition as a way to
measure to what extent a source sentence and its au-
tomatic translation are semantically similar. CLTE
has been proposed by (Mehdad et al, 2010) as an ex-
tension of textual entailment (Dagan and Glickman,
2004) that consists in deciding, given a text T and a
hypothesis H in different languages, if the meaning
of H can be inferred from the meaning of T.
The main motivation in approaching adequacy
evaluation using CLTE is that an adequate trans-
lation and the source text should convey the same
meaning. In terms of entailment, this means that an
adequate MT output and the source sentence should
entail each other (bi-directional entailment). Los-
172
ing or altering part of the meaning conveyed by the
source sentence (i.e. having more, or different infor-
mation in one of the two sides) will change the en-
tailment direction and, consequently, the adequacy
judgement. Framed in this way, CLTE-based ade-
quacy evaluation methods can be designed to dis-
tinguish meaning-preserving variations from true di-
vergence, regardless of reference translations.
Similarly to many monolingual TE approaches,
CLTE solutions proposed so far adopt supervised
learning methods, with features that measure to what
extent the hypotheses can be mapped into the texts.
The underlying assumption is that the probability of
entailment is proportional to the number of words in
H that can be mapped to words in T (Mehdad et al,
2011). Such mapping can be carried out at differ-
ent word representation levels (e.g. tokens, lemmas,
stems), possibly with the support of lexical knowl-
edge in order to cross the language barrier between
T and H (e.g. dictionaries, phrase tables).
Under the same assumption, since in the adequacy
evaluation framework the entailment relation should
hold in both directions, the mapping is performed
both from the source to the target and vice-versa,
building on features extracted from both sentences.
Moreover, to improve over previous CLTE methods
and boost MT adequacy evaluation performance, we
explore the joint contribution of a number of lexi-
cal, syntactic and semantic features (Mehdad et al,
2012).
Concerning the features used, it?s worth observ-
ing that the cost of implementing our approach (in
terms of required resources and linguistic proces-
sors), and the need of reference translations are in-
trinsically different bottlenecks for MT. While the
limited availability of processing tools for some lan-
guage pairs is a ?temporary? bottleneck, the acqui-
sition of multiple references is a ?permanent? one.
The former cost is reducing over time due to the
progress in NLP research; the latter represents a
fixed cost that has to be eliminated. Similar consid-
erations hold regarding the need of annotated data to
develop our supervised learning approach. Concern-
ing this, the cost of labelling source-target pairs with
adequacy judgments is significantly lower compared
to the creation of multiple references.
3.1 Features
In order to learn models for classification and regres-
sion we used the Support Vector Machine (SVM)
algorithms implemented in the LIBSVM package
(Chang and Lin, 2011) with a linear kernel and de-
fault parameters setting. Aiming at objective ade-
quacy evaluation, our method limits the recourse to
MT system-dependent features to reduce the bias
of evaluating MT technology with its own core
methods. The experiments described in the follow-
ing sections are carried out on publicly available
English-Spanish datasets, exploring the potential of
a combination of surface, syntactic and semantic
features. Language-dependent ones are extracted
by exploiting processing tools for the two lan-
guages (part-of-speech taggers, dependency parsers
and named entity recognizers), most of which are
available for many languages.
Our feature set can be described as follows:
Surface Form (F) features consider the num-
ber of words, punctuation marks and non-word
markers (e.g. quotations and brackets) in source
and target, as well as their ratios (source/target and
target/source), and the number of out of vocabulary
terms encountered.
Shallow Syntactic (SSyn) features consider
the number and ratios of common part-of-speech
(POS) tags in source and target. Since the list of
valid POS tags varies for different languages, we
mapped English and Spanish tags into a common
list using the FreeLing tagger (Carreras et al, 2004).
Syntactic (Syn) features consider the number
and ratios of dependency roles common to source
and target. To create a unique list of roles, we used
the DepPattern (Otero and Lopez, 2011) package,
which provides English and Spanish dependency
parsers.
Phrase Table (PT) matching features are cal-
culated as in (Mehdad et al, 2011), with a phrasal
matching algorithm that takes advantage of a lexical
phrase table extracted from a bilingual parallel
corpus. The algorithm determines the number of
phrases in the source (1 to 5-grams, at the level of
173
tokens, lemmas and stems) that can be mapped into
target word sequences, and vice-versa. To build our
English-Spanish phrase table, we used the Europarl,
News Commentary and United Nations Spanish-
English parallel corpora. After tokenization, the
Giza++ (Och and Ney, 2000) and the Moses toolkit
(Koehn et al, 2007) were respectively used to
align the corpora and extract the phrase table.
Although the phrase table was generated using MT
technology, its use to compute our features is still
compatible with a system-independent approach
since the extraction is carried out without tuning the
process towards any particular task. Moreover, our
phrase matching algorithm integrates matches from
overlapping n-grams of different size and nature
(tokens, lemmas and stems) which current MT
decoding algorithms cannot explore for complexity
reasons.
Dependency Relation (DR) matching fea-
tures target the increase of CLTE precision by
adding syntactic constraints to the matching pro-
cess. These features capture similarities between
dependency relations, combining syntactic and
lexical levels. We define a dependency relation
as a triple that connects pairs of words through a
grammatical relation. In a valid match, while the
relation has to be the same, the connected words
can be either the same, or semantically equivalent
terms in the two languages. For example, ?nsubj
(loves, John)? can match ?nsubj (ama, John)?
and ?nsubj (quiere, John)? but not ?dobj (quiere,
John)?. Term matching is carried out by means
of a bilingual dictionary extracted from parallel
corpora during PT creation. Given the dependency
tree representations of source and target produced
with DepPattern, for each grammatical relation r we
calculate two DR matching scores as the number
of matching occurrences of r in both source and
target, respectively normalized by: i) the number of
occurrences of r in the source, and ii) the number of
occurrences of r in the target.
Semantic Phrase Table (SPT) matching features
represent a novel way to leverage the integration of
semantics and MT-derived techniques. Semantically
enhanced phrase tables are used as a recall-oriented
complement to the lexical PT matching features.
SPTs are extracted from the same parallel corpora
used to build lexical PTs, augmented with shallow
semantic labels. To this aim, we first annotate the
corpora with the FreeLing named-entity tagger,
replacing named entities with general semantic
labels chosen from a coarse-grained taxonomy
(person, location, organization, date and numeric
expression). Then, we combine the sequences of
unique labels into one single token of the same
label. Finally, we extract the semantic phrase
table from the augmented corpora in the same way
mentioned above. The resulting SPTs are used to
map phrases between NE-annotated source-target
pairs, similar to PT matching. SPTs offer three
main advantages: i) semantic tags allow to match
tokens that do not occur in the original parallel
corpora used to extract the phrase table, ii) SPT
entries are often short generalizations of longer
original phrases, so the matching process can
benefit from the increased probability of mapping
higher order n-grams (i.e. those providing more
contextual information), and iii) their smaller size
has positive impact on system?s efficiency, due to
the considerable search space reduction.
4 Experiments and results
4.1 Datasets
Datasets with manual evaluation of MT output have
been made available through a number of shared
evaluation tasks. However, most of these datasets
are not specifically annotated for adequacy measure-
ment purposes, and the available adequacy judge-
ments are limited to few hundred sentences for some
language pairs. Moreover, most datasets are created
by comparing reference translations with MT sys-
tems? output, disregarding the input sentences. Such
judgements are hence biased towards the reference.
Furthermore, the inter-annotator agreement is often
low (Callison-Burch et al, 2007). In light of these
limitations, most of the available datasets are per se
not fully suitable for adequacy evaluation methods
based on supervised learning, nor to provide sta-
ble and meaningful results. To partially cope with
these problems, our experiments have been carried
out over two different datasets:
? 16K: 16.000 English-Spanish pairs, with
Spanish translations produced by multiple MT
174
systems, annotated by professional translators
with quality scores in a 4-point scale (Specia et
al., 2010a).
? WMT07: 703 English-Spanish pairs derived
from MT systems? output, with explicit ade-
quacy judgements on a 5-point scale.
The two datasets present complementary advan-
tages and disadvantages. On the one hand, al-
though it is not annotated to explicitly capture
meaning-related aspects of MT output, the quality
oriented dataset has the main advantage of being
large enough for supervised approaches. Moreover,
it should allow to check the effectiveness of our fea-
ture set in estimating adequacy as a latent aspect of
the more general notion of MT output quality. On
the other hand, the smaller dataset is less suitable
for supervised learning, but represents an appropri-
ate benchmark for MT adequacy evaluation.
4.2 Adequacy and quality prediction
To experiment with our CLTE-based evaluation
method minimizing overfitting, we randomized each
dataset 5 times (D1 to D5), and split them into 80%
for training and 20% for testing. Using different
feature sets, we then trained and tested various re-
gression models over each of the five splits, and
computed correlation coefficients between the CLTE
model predictions and the human gold standard an-
notations ([1-4] for quality, and [1-5] for adequacy).
16K quality-based dataset
In Table 1 we compare the Pearson?s correlation
coefficient of our SVM regression models against
the results reported in (Specia et al, 2010b), calcu-
lated with the same three common MT evaluation
metrics with a single reference: BLEU, TER and
Meteor. For the sake of comparison, we also re-
port the average quality correlation (QE) obtained
by (Specia et al, 2010b) over the same dataset.1
The results show that the integration of syntac-
tic and semantic information allows our adequacy-
oriented model to achieve a correlation with hu-
man quality judgements that is always significantly
1We only show the average results reported in (Specia et al,
2010b), since the distributions of the 16K dataset is different
from our randomized distribution.
higher2 than the correlation obtained by the MT
evaluation metrics used for comparison. As ex-
pected a considerable improvement over surface fea-
tures is achieved by the integration of syntactic in-
formation. A further increase, however, is brought
by the complementary contribution of SPT (recall-
oriented, due to the higher coverage of semantics-
aware phrase tables with respect to lexical PTs), and
DR matching features (precision-oriented, due to
the syntactic constraints posed to matching text por-
tions). Although they are meant to capture meaning-
related aspects of MT output, our features allow
to outperform the results obtained by the generic
quality-oriented features used by (Specia et al,
2010b), which do not discriminate between ade-
quacy and fluency.3 When dependency relations and
phrase tables (both lexical and semantics-aware) are
used in combination, our scores also outperform the
average QE score. Finally, looking at the different
random splits of the same dataset (D1 to D5), our
correlation scores remain substantially stable, prov-
ing the robustness of our approach not only for ade-
quacy, but also for quality estimation.
WMT07 adequacy-based dataset
In Table 2 we compare our regression model,
obtained in the same way previously described,
against three commonly used MT evaluation metrics
(Callison-Burch et al, 2007). In this case, the re-
ported results do not show the same consistency over
the 5 randomized datasets (D1 to D5). However, it is
worth pointing out that: i) the small dataset is partic-
ularly challenging to train models with higher corre-
lation with humans, ii) our aim is checking how far
we get using only adequacy-oriented features rather
than outperforming BLEU/TER/Meteor at any cost,
and iii) our results are not far from those achieved
by metrics that rely on reference translations. Com-
pared with Meteor, the correlation is even higher
proving the effectiveness of the proposed method.
2p < 0.05, calculated using the approximate randomization
test implemented in (Pado?, 2006).
3As reported in (Specia et al, 2010b), more than 50% (39
out of 74) of the features used is translation-independent (only
source-derived features).
175
Features D1 D2 D3 D4 D5 AVG
F 0.2506 0.2578 0.2436 0.2527 0.2443 0.25
SSyn+Syn 0.4387 0.4114 0.3994 0.4114 0.3793 0.41
F+SSyn+Syn 0.4215 0.4398 0.4059 0.4464 0.4255 0.428
F+SSyn+Syn+DR 0.4668 0.4602 0.4386 0.4437 0.4454 0.451
F+SSyn+Syn+DR+PT 0.4724 0.4715 0.4852 0.5028 0.4653 0.48
F+SSyn+Syn+DR+PT+SPT 0.4967 0.4802 0.4688 0.4894 0.4887 0.485
BLEU 0.2268
TER 0.1938
METEOR 0.2713
QE (Specia et al, 2010b) 0.4792
Table 1: Pearson?s correlation between SVM regression and human quality annotation over 16K dataset.
Features D1 D2 D3 D4 D5 AVG
F 0.10 0.03 0.04 0.10 0.14 0.083
SSyn+Syn 0.299 0.351 0.1834 0.2962 0.2417 0.274
F+SSyn+Syn 0.2648 0.2870 0.4061 0.3601 0.1327 0.29
F+SSyn+Syn+DR 0.3196 0.4568 0.2860 0.5057 0.4066 0.395
F+SSyn+Syn+DR+PT 0.3254 0.4710 0.3921 0.4599 0.3501 0.40
F+SSyn+Syn+DR+PT+SPT 0.3487 0.4032 0.4803 0.4380 0.3929 0.413
BLEU 0.466
TER 0.437
METEOR 0.357
Table 2: Pearson?s correlation between SVM regression and human adequacy annotation over WMT07.
4.3 Multi-class classification
To further explore the potential of our CLTE-based
MT evaluation method, we trained an SVM multi-
class classifier to predict the exact adequacy and
quality scores assigned by human judges. The eval-
uation was carried out measuring the accuracy of our
models with 10-fold cross validation to minimize
overfitting. As a baseline, we calculated the per-
formance of the Majority Class (MjC) classifier pro-
posed in (Specia et al, 2011), which labels all exam-
ples with the most frequent class among all classes.
The performance improvement over the result ob-
tained by the MjC baseline (?) has been calculated
to assess the contribution of different feature sets.
16K quality-based dataset
The accuracy results reported in Table 3a show
that also in this testing condition, syntactic and se-
mantic features improve over surface form ones. Be-
sides that, we observe a steady improvement over
the MjC baseline (from 5% to 12%). This demon-
strates the effectiveness of our adequacy-based fea-
tures to predict exact quality scores in a 4-point
scale, although this is a more challenging and dif-
ficult task than regression and binary classification.
Such improvement is even more interesting consid-
ering that (Specia et al, 2010b) reported discour-
aging results with multi-class classification to pre-
dict quality scores. Moreover, while they claimed
that removing target-independent features (i.e. those
only looking at the source text) significantly de-
grades their QE performance, we achieved good re-
sults without using any of these features.
WMT07 adequacy-based dataset
As we can observe in Table 3b, all variations
of adequacy estimation models significantly outper-
form the MjC baseline, with improvements rang-
176
Features 10-fold acc. ?
F 42.16% 5.16
Syn+SSyn 46.61% 9.61
F+Syn+SSyn 47.10% 10.10
F+Syn+SSyn+DR 47.26% 10.26
F+Syn+SSyn+DR+PT 48.15% 11.15
F+Syn+SSyn+DR+PT+SPT 48.74% 11.74
MjC 37% -
(a) 16K dataset.
Features 10-fold acc. ?
F 50.07% 14.07
Syn+SSyn 54.19% 18.19
F+Syn+SSyn 54.34% 18.34
F+Syn+SSyn+DR 56.47% 20.47
F+Syn+SSyn+DR+PT 56.61% 20.61
F+Syn+SSyn+DR+PT+SPT 56.75% 20.75
MjC 36% -
(b) WMT07 dataset
Table 3: Multi-class classification accuracy of the quality/adequacy scores.
Features 10-fold acc. ?
F 65.85% 11.85
Syn+SSyn 69.59% 15.59
F+Syn+SSyn 70.89% 16.89
F+Syn+SSyn+DR 71.39% 17.39
F+Syn+SSyn+DR+PT 71.92% 17.92
F+Syn+SSyn+DR+PT+SPT 72.21% 18.21
MjC 54% -
(a) 16k dataset.
Features 10-fold acc. ?
F 83.24% 12.84
Syn+SSyn 83.67% 13.27
F+Syn+SSyn 84.31% 13.91
F+Syn+SSyn+DR 84.86% 14.46
F+Syn+SSyn+DR+PT 84.96% 14.56
F+Syn+SSyn+DR+PT+SPT 85.20% 14.80
MjC 70.4% -
(b) WMT07 dataset.
Table 4: Accuracy of the binary classification into ?good? or ?adequate?, and ?bad? or ?inadequate?.
ing from 14% to 20%. Interestingly, although the
dataset is small and the number of classes is higher
(5-point scale), the improvement and overall results
are better than those obtained on the 16K dataset.
Such result confirms our hypothesis that adequacy-
based features extracted from both source and target
perform better on a dataset explicitly annotated with
adequacy judgements. In addition, the improvement
over the MjC baseline (?) of our best model is much
higher (20%) than the one reported in (Specia et al,
2011) on adequacy estimation (6%). We are aware
that their results are calculated over a dataset for a
different language pair (i.e. English-Arabic) which
brings up more challenges. However, our smaller
dataset (700 vs 2580 pairs) and the higher number
of classes (5 vs 4) compensate to some extent the
difficulty of dealing with English-Arabic pairs.
4.4 Recognizing ?good? vs ?bad? translations
Last but not least, we considered the traditional sce-
nario for quality and confidence estimation, which
is a binary classification of translations into ?good?
and ?bad? or, from the meaning point of view, ?ade-
quate? and ?inadequate?. Adequacy-oriented binary
classification has many potential applications in the
translation industry, ranging from the design of con-
fidence estimation methods that reward meaning-
preserving translations, to the optimization of the
translation workflow. For instance, an ?adequate?
translation can be just post-edited in terms of fluency
by a target language native speaker, without having
any knowledge of the source language. On the other
hand, an ?inadequate? translation should be sent to a
human translator or to another MT system, in order
to reach acceptable adequacy. Effective automatic
binary classification has an evident positive impact
on such workflow.
16K quality-based dataset
We grouped the quality scores in the 4-point scale
into two classes, where scores {1,2} are considered
as ?bad? or ?inadequate?, while {3,4} are taken as
?good? or ?adequate?. We carried out learning and
177
classification using different sets of features with 10-
fold cross validation. We also compared our accu-
racy with the MjC baseline, and calculated the im-
provement of each model (?) against it.
The results reported in Table 4a demonstrate that
the accuracy of our models is always significantly
superior to the MjC baseline. Moreover, also in this
case there is a steady improvement using syntactic
and semantic features over the results obtained by
surface form features. Additionally, it is worth men-
tioning that the best model improvement over the
baseline (?) is much higher (about 18%) than the
improvement reported in (Specia et al, 2010b) over
the same dataset (about 8%), considering the aver-
age score obtained with their data distribution. This
confirms the effectiveness of our CLTE approach
also in classifying ?good? and ?bad? translations.
WMT07 adequacy-based dataset
We mapped the 5-point scale adequacy scores into
two classes, with {1,2,3} judgements assigned to the
?inadequate? class, and {4,5} judgements assigned
to the ?adequate? class. The main motivation for this
distribution was to separate the examples in a way
that adequate translations are substantially accept-
able, while inadequate translations present evident
meaning discrepancies with the source.
The results reported in Table 4b show that the
accuracy of the binary classifiers to distinguish be-
tween ?adequate? and ?inadequate? classes was sig-
nificantly superior (up to about 15%) to the MjC
baseline. We also notice that surface form fea-
tures have a significant contribution to deal with the
adequacy-oriented dataset, while the gain obtained
using syntactic and semantic features (2%) is lower
than the improvement observed in the 16K dataset.
This might be due to the more unbalanced distribu-
tion of the classes which: i) leads to a high baseline,
and ii) together with the small size of the WMT07
dataset, makes supervised learning more challeng-
ing. Finally, the improvement of all models (?) over
the MjC baseline is much higher than the gain re-
ported in (Specia et al, 2011) over their adequacy-
oriented dataset (around 2%).
5 Conclusions
In the effort of integrating semantics into MT tech-
nology, we focused on automatic MT evaluation, in-
vestigating the potential of applying cross-lingual
textual entailment techniques for adequacy assess-
ment. The underlying assumption is that MT output
adequacy can be determined by verifying that an en-
tailment relation holds from the source to the target,
and vice-versa. Within such framework, this paper
makes two main contributions.
First, in contrast with most current metrics based
on the comparison between automatic translations
and multiple references, we avoid the bottleneck
represented by the manual creation of such refer-
ences.
Second, beyond current approaches biased to-
wards fluency or general quality judgements, we
tried to isolate the adequacy dimension of the prob-
lem, exploring the potential of adequacy-oriented
features extracted from the observation of source
and target.
To achieve our objectives, we successfully ex-
tended previous CLTE methods with a variety of lin-
guistically motivated features. Altogether, such fea-
tures led to reliable judgements that show high cor-
relation with human evaluation. Coherent results on
different datasets and classification schemes demon-
strate the effectiveness of the approach and its poten-
tial for different applications.
Future works will address both the improvement
of our adequacy evaluation method and its integra-
tion in SMT for optimization purposes. On one
hand, we plan to explore new features capturing
other semantic dimensions. A possible direction is
to consider topic modelling techniques to measure
the relatedness of source and target. Another inter-
esting direction is to investigate the use of Wikipedia
entity linking tools to support the mapping between
source and target terms. On the other hand, we plan
to explore the integration of our model as an error
criterion in SMT system training.
Acknowledgments
This work has been partially supported by the
CoSyne project (FP7-ICT-4-24853) and T4ME net-
work of excellence (FP7-IST-249119), funded by
the European Commission under the 7th Frame-
work Programme. The authors would like to thank
Hanna Bechara, Antonio Valerio Miceli Barone and
Daniele Pighin for their contributions during the MT
Marathon 2011.
178
References
E. Avramidis, M. Popovic, V. Vilar Torres, and A. Bur-
chardt. 2011. Evaluate with Confidence Estimation:
Machine Ranking of Translation Outputs using Gram-
matical Features. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation (WMT ?11).
N. Bach, F. Huang, and Y. Al-Onaizan. 2011. Good-
ness: A Method for Measuring Machine Translation
Confidence. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2011).
S. Banerjee and A. Lavie. 2005. METEOR: An Auto-
matic Metric for MT Evaluation with Improved Corre-
lation with Human Judgments. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,
A. Kulesza, A. Sanchis, and N. Ueffing. 2004. Con-
fidence Estimation for Machine Translation. In Pro-
ceedings of the 20th international conference on Com-
putational Linguistics (COLING ?04). Association for
Computational Linguistics.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) Evaluation of Machine
Translation. In Proceedings of the Second Workshop
on Statistical Machine Translation (WMT ?07).
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
FreeLing: An Open-Source Suite of Language An-
alyzers. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC?04).
C.C. Chang and C.J. Lin. 2011. LIBSVM: A Library
for Support Vector Machines. ACM Transactions on
Intelligent Systems and Technology (TIST), 2(3).
I. Dagan and O. Glickman. 2004. Probabilistic Textual
Entailment: Generic Applied Modeling of Language
Variability. In Proceedings of the PASCAL Workshop
of Learning Methods for Text Understanding and Min-
ing.
G. Doddington. 2002. Automatic Evaluation of Machine
Translation Quality Using N-gram Co-Occurrence
Statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, HLT ?02.
J. Gime?nez and L. Ma`rquez. 2007. Linguistic Features
for Automatic Evaluation of Heterogenous MT Sys-
tems. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation (StatMT ?07).
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions (ACL 2007).
Y. Mehdad, M. Negri, and M. Federico. 2010. Towards
Cross-Lingual Textual Entailment. In Proceedings of
the 11th Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL HLT 2010).
Y. Mehdad, M. Negri, and M. Federico. 2011. Using
Bilingual Parallel Corpora for Cross-Lingual Textual
Entailment. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies (ACL HLT 2011).
Y. Mehdad, M. Negri, and M. Federico. 2012. Detect-
ing Semantic Equivalence and Information Disparity
in Cross-lingual Documents. In Proceedings of the
ACL?12.
F.J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proceedings of the 38th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2000).
P.G. Otero and I.G. Lopez. 2011. A Grammatical For-
malism Based on Patterns of Part-of-Speech Tags. In-
ternational journal of corpus linguistics, 16(1).
S. Pado?, M. Galley, D. Jurafsky, and C. D. Manning.
2009. Textual Entailment Features for Machine Trans-
lation Evaluation. In Proceedings of the Fourth Work-
shop on Statistical Machine Translation (StatMT ?09).
S. Pado?, 2006. User?s guide to sigf: Significance test-
ing by approximate randomisation.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation (ACL 2002. In Proceedings of the
40th annual meeting on association for computational
linguistics.
C.B. Quirk. 2004. Training a Sentence-Level Machine
Translation Confidence Measure. In Proceedings of
LREC 2004.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In Proceedings of
Association for Machine Translation in the Americas
(AMTA 2006).
L. Specia and A. Farzindar. 2010. Estimating Machine
Translation Post-Editing Effort with HTER. In Pro-
ceedings of the AMTA-2010 Workshop, Bringing MT
to the User: MT Research and the Translation Indus-
try.
L. Specia, N. Cancedda, and M. Dymetman. 2010a.
A Dataset for Assessing Machine Translation Eval-
uation Metrics. In Proceedings of the 7th interna-
tional conference on Language Resources and Eval-
uation (LREC10).
179
L. Specia, D. Raj, and M. Turchi. 2010b. Machine Trans-
lation Evaluation Versus Quality Estimation. Machine
translation, 24(1).
L. Specia, N. Hajlaoui, C. Hallett, and W. Aziz. 2011.
Predicting Machine Translation Adequacy. In Pro-
ceedings of the 13th Machine Translation Summit (MT-
Summit 2011).
L. Specia. 2011. Exploiting Objective Annotations for
Minimising Translation Post-editing Effort. In Pro-
ceedings of the 15th Conference of the European As-
sociation for Machine Translation (EAMT 2011).
D. Xiong, M. Zhang, and H. Li. 2010. Error Detection
for Statistical Machine Translation Using Linguistic
Features. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics (ACL
2010). Association for Computational Linguistics.
180
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 240?251,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Coping with the Subjectivity of Human Judgements
in MT Quality Estimation
Marco Turchi Matteo Negri Marcello Federico
Fondazione Bruno Kessler, FBK-irst
Trento , Italy
{turchi|negri|federico}@fbk.eu
Abstract
Supervised approaches to NLP tasks rely
on high-quality data annotations, which
typically result from expensive manual la-
belling procedures. For some tasks, how-
ever, the subjectivity of human judgements
might reduce the usefulness of the an-
notation for real-world applications. In
Machine Translation (MT) Quality Esti-
mation (QE), for instance, using human-
annotated data to train a binary classifier
that discriminates between good (useful
for a post-editor) and bad translations is
not trivial. Focusing on this binary task,
we show that subjective human judge-
ments can be effectively replaced with an
automatic annotation procedure. To this
aim, we compare binary classifiers trained
on different data: the human-annotated
dataset from the 7th Workshop on Statis-
tical Machine Translation (WMT-12), and
an automatically labelled version of the
same corpus. Our results show that human
labels are less suitable for the task.
1 Introduction
With the steady progress in the field of Statistical
Machine Translation (SMT), the translation indus-
try is now faced with the possibility of significant
productivity increases (i.e. amount of publishable
output per unit of time). One way to achieve this
goal, in Computer Assisted Translation (CAT) en-
vironments, is the integration of (precise, but of-
ten partial) suggestions obtained through ?fuzzy
matches? from a Translation Memory (TM), with
(complete, but potentially less precise) translations
produced by an MT system. Such integration can
loosely consist in presenting translators with un-
ranked suggestions obtained from the MT and the
TM, or rely on tighter combination strategies. For
instance, MT and TM translations can be automat-
ically ranked to ease the selection of the most suit-
able one for post-editing (He et al, 2010), or the
TM can be used to constrain and improve MT sug-
gestions (Ma et al, 2011). In all cases, the ef-
fectiveness of the integration is conditioned by:
i) the quality of MT, and ii) the accuracy in au-
tomatically predicting such quality. Higher pro-
ductivity increases depend on the capability of the
MT system to output useful material that is close
to be publishable ?as is? (Denkowski and Lavie,
2012), and the capability to automatically identify
and present to human translators only such sug-
gestions.
Recognizing good translations falls in the scope
of research on automatic MT Quality Estimation
(QE), which addresses the problem of estimating
the quality of a translated sentence at run-time,
without access to reference translations (Specia et
al., 2009; Soricut and Echihabi, 2010; Bach et al,
2011; Specia, 2011; Mehdad et al, 2012b). In
recent years QE gained increasing interest in the
MT community, resulting in several datasets avail-
able for training and evaluation (Callison-Burch et
al., 2012), the definition of features showing good
correlation with human judgements (Soricut et al,
2012), and the release of open-source software.1
The proposed solutions to the QE problem rely
on supervised methods that strongly depend on the
availability of labelled data. While early works
(Blatz et al, 2003) exploited annotations obtained
with automatic MT evaluation metrics like BLEU
(Papineni et al, 2002), the current trend is to
rely on human annotations, which seem to lead
to more accurate models (Quirk, 2004; Specia et
al., 2009). Along this direction, the QE task con-
sists in predicting scores that reflect human quality
judgements, by learning from manually annotated
datasets (e.g. collections of source-target pairs la-
1http://www.quest.dcs.shef.ac.uk/
240
belled according to an n-point Likert scale or with
real numbers in a given interval). Within this dom-
inant supervised framework, we explore different
ways to obtain labelled data for training a bi-
nary QE classifier suitable for integration in a
CAT tool. Since, to the best of our knowledge,
labelled data with binary judgements are currently
not available, we consider two alternative options.
The first option is to adapt an existing dataset,
checking whether it can be partitioned in a way
that reflects the distinction between good (use-
ful for the translator, suitable for post editing)
and bad translations (that need complete rewrit-
ing).2 To this aim we experiment with the QE
data released within the 7th Workshop on Ma-
chine Translation (WMT-12). The corpus con-
sists of source-target pairs annotated with manual
QE labels (1-5 scores) indicating the post-editing
needed to correct the translations. Besides explicit
human judgements, the availability of post-edited
translations makes also possible to calculate the
actual HTER values (Snover et al, 2009), indicat-
ing the minimum edit distance between the ma-
chine translation and its manually post-edited ver-
sion in the [0,1] interval.
The second option is to automatically re-
annotate the same dataset, trying to produce labels
that reflect an objective and more reliable binary
distinction based on empirical observations.
Our analysis aims to answer the following ques-
tions:
1. Are human labels reliable and coherent
enough to train accurate binary models?
2. Are arbitrarily-set thresholds useful to parti-
tion QE data for this task?
3. Is it possible to obtain reliable binary annota-
tions from an automatic procedure?
Negative answers to the first two questions would
respectively call into question: i) the intuitive idea
that human labels are the most reliable for a super-
vised approach to binary QE, and ii) the possibility
that thresholds on a single metric (e.g. the HTER)
can be set to capture the subtle differences separat-
ing useful from useless translations. A positive an-
swer to the third question would open to the possi-
bility to create training datasets in a more coherent
2In the remainder of the paper we will consider as ?good?
translations those for which post-editing requires a smaller
effort than translation from scratch. Conversely, we will label
as ?bad? the translations that need complete rewriting.
and replicable way compared to current data anno-
tation methods. By answering these questions, this
paper provides the following main contributions:
? We show that training a binary classifier on
arbitrary partitions of an existing dataset is
difficult. Our experiments with the WMT-
12 corpus demonstrate that neither following
standard indications (e.g. ?if more than 70%
of the MT output needs to be edited, a trans-
lation from scratch is necessary?)3, nor con-
sidering arbitrary HTER thresholds, it is pos-
sible to obtain accurate binary classifiers suit-
able for integration in a CAT environment;
? We propose a replicable automatic (hence
non subjective) method to re-annotate an ex-
isting dataset in a way that the resulting bi-
nary classifier outperforms those trained with
human labels.
? We show that, with our method, a smaller
amount of training data is sufficient to ob-
tain similar or better performance compared
to that of the human-annotated dataset used
for comparison.
2 Binary QE for CAT environments
QE has been mainly addressed as a classification
or regression task, where a quality score (respec-
tively an integer or a real value) has to be automat-
ically assigned to MT output sentences given their
source (Specia et al, 2010). Casting the problem
in this way, the integration of a QE component
in a CAT environment makes possible to present
translators with estimates of the expected quality
of each MT suggestion. Such intuitive solution,
however, disregards the fact that even precise QE
scores would not alleviate translators from the ef-
fort of reading useless MT output (or at least the
associated score).
A more effective alternative is to use the esti-
mated QE scores to filter out poor MT suggestions,
presenting only those worth for post-editing. Bi-
nary classification, however, has to confront with
the problem of setting reasonable cut-off criteria.
The arbitrary thresholds, used in several previous
works (Quirk, 2004; Specia et al, 2010; Specia
et al, 2011) are in fact hard to justify, and even
harder to learn from human-labelled training data.
3This was a guideline for the professional trans-
lators involved in the annotation of a previous ver-
sion of the dataset used for the WMT-12 evalua-
tion (see http://www.statmt.org/wmt12/
quality-estimation-task.html).
241
On one side, for instance, there is no evi-
dence that the 70% HTER threshold used in some
datasets yields the optimal separation between ac-
ceptable and totally useless suggestions. Such ar-
bitrary criterion, based on the raw count of post-
editing operations, is likely to reflect a partial view
on a complex problem, disregarding important as-
pects such as the distribution of the corrections in
the MT output. However, in some cases, having
the first 30% of words correctly translated might
take less post-editing effort than having 50% of
correctly translated terms scattered throughout the
whole sentence. In these cases, a 70% HTER
threshold would wrongly consider useless trans-
lations as positive instances and vice-versa.
On the other side, when arbitrary thresholds are
used as annotation guidelines (Callison-Burch et
al., 2012), the moderate agreement between hu-
man judges might make manual labels ill-suited to
learn accurate models.
Under the constraints posed by a CAT envi-
ronment, where only useful suggestions can lead
to a significant productivity increase, the ideal
model should maximize the number of true posi-
tives (useful translations recognized as good) min-
imizing, at the same time, the number of false pos-
itives (useless translations recognized as good). To
this aim, the more the training data are partitioned
according to objective criteria, the higher the ex-
pected reliability of the corresponding cut-off and,
in turn, the higher the expected performance of the
binary classifier.
Focusing on these issues, the following sections
discuss various methods to obtain training data for
binary QE geared to the integration in a CAT en-
vironment. Partitions based on human judgements
from the WMT-12 dataset will be compared with
an automatic method to re-annotate the same cor-
pus. The suitability of the resulting training sets
for binary classification will be assessed by mea-
suring the performance of classifiers built from
each training set. Metrics sensitive to the number
of false positives will be used for this purpose.
3 Partitioning the WMT-12 dataset
Due to the lack of datasets annotated with ex-
plicit binary (good, bad) judgements about transla-
tion quality, the most intuitive way to obtain train-
ing data for our QE classifier is to adapt exist-
ing manually-labelled data. The reasonable size
of the WMT-12 dataset makes it a good candidate
for our purposes. The corpus consists of 2,254
English-Spanish news sentences (1,832 for train-
ing, 422 for test) produced by the Moses phrase-
based SMT system (Koehn et al, 2007) trained
on Europarl (Koehn, 2005) and News Commen-
taries corpora,4 along with their source sentences,
reference translations and post-edited translations.
Training and test instances have been annotated by
professional translators with scores (1 to 5) indi-
cating the estimated post-editing effort (percent-
age of MT output that has to be corrected). Ac-
cording to the proposed scheme, the highest score
indicates lowest effort (MT output requires little or
no editing), while the lowest score indicates that
the MT output needs to be translated from scratch.
To cope with systematic biases among the anno-
tators,5 the judgements were combined in a final
score obtained from their weighted average, re-
sulting in a labelled dataset with real numbers in
the [1, 5] interval as effort scores.
In order to obtain suitable data for binary QE,
the WMT-12 training set (1,832 instances) has
been partitioned in different ways, leaving the test
set for evaluation (see Section 5). The goal, for
each partition strategy, was to label as bad (the as-
signed label is -1) only the translations that need
complete rewriting, keeping all the other transla-
tions as good instances (labelled with +1). Consid-
ering the averaged effort scores, the actual human
judgements, and the HTER values calculated be-
tween the translations and the corresponding post-
edited version, we experimented with the follow-
ing three partition criteria.
Average effort scores (AES). Three partitions
have been generated based on the effort scores
of 2, 2.5, and 3, labelling the WMT-12 train-
ing instances with scores below or equal to each
threshold as negative examples (-1), and the in-
stances with scores above the threshold as posi-
tive examples (+1). Partitions with thresholds be-
low 2 were also considered, including the most
intuitive partition with cut-off set to 1. However,
the resulting number of negative instances, if any,
was too scarce, and the overall dataset too unbal-
anced, to make standard supervised learning meth-
ods effective The creation of highly unbalanced
data is a recurring issue for all the partition meth-
4http://www.statmt.org/wmt11/
translation-task.html#download
5Such biases support the idea that labelling translations
with quality scores is per se a highly subjective task.
242
ods we applied to the WMT-12 corpus. Together
with the low homogeneity of human labels (even
for very poor translations the three judges do not
agree in assigning the lowest score), in most of
the cases the small number of low-quality transla-
tions in the dataset makes the negative class con-
siderably smaller than the positive one. This can
be observed in Table 1, which provides the to-
tal number of positive and negative instances for
each partition method. For instance, with our low-
est AES threshold (2) the total number of nega-
tive instances is 113, while the positive ones are
1,719. Although considering different cut-off cri-
teria aims to make our investigation more com-
plete, it?s also worth remarking that the higher the
threshold, the higher the distance of the result-
ing experimental setting from our target scenario.
While 2, as an effort score threshold, is likely
to reflect a reasonable separation between useless
and post-editable translations, higher values are in
principle more appropriate for ?soft? separations
into worse versus better translations.
Human scores (HS). Five partitions have been
generated using the actual labels assigned by the
three annotators to each translation instead of the
average effort scores. In particular, we considered
the following score combinations (?X? stands for
any integer between 1 and 5): 1-X-X, 2-2-2, 2-
2-X, 2-3-3, 3-3-3. Also in this case, as shown
in Table 1, partitions based on lower scores lead
to highly unbalanced datasets of limited usability,
while those based on higher scores are increas-
ingly more distant to our application scenario.6
HTER scores (HTER). Seven partitions have
been generated considering the following HTER
thresholds: 0.75, 0.7, 0.65, 0.6, 0.55, 0.5, 0.45.
In this case, being the HTER an error measure,
training instances with scores above or equal to
the threshold were labelled as negative examples
(-1), while instances with lower scores were la-
belled as positive examples (+1). Similar to the
other partition criteria, some of our threshold val-
ues reflect our task more closely than others, but
result in more unbalanced datasets. In particular,
thresholds around 0.7 substantially adhere to the
WMT-12 annotation guidelines (as far as transla-
tions that need complete rewriting are concerned)
6The partition most closely related to our task (i.e. 1-1-1)
was impossible to produce since none of the examples was
labelled with 1 by all the annotators. Even for 1-1-X, the
negative class contains only one example.
and produce training data with fewer negative in-
stances. Other thresholds, which is still worth ex-
ploring since we do not know the optimal cut-off
value, are in principle less suitable to our task but
produce more balanced training data.
Training instances
Average effort scores (AES) Positive Negative
2 1,719 113
2.5 1,475 357
3 1,194 638
Human scores (HS) Positive Negative
1-X-X 1,736 96
2-2-2 1,719 113
2-2-X 1,612 220
2-3-3 1,457 375
3-3-3 1,360 472
HTER scores (HTER) Positive Negative
0.75 1,798 34
0.7 1,786 46
0.65 1,756 76
0.6 1,708 124
0.55 1,653 179
0.5 1,531 301
0.45 1,420 412
Table 1: Number of positive/negative instances for
each partition of the WMT-12 training set.
4 Re-annotating the WMT-12 dataset
As an alternative to partitioning methods, we in-
vestigated the possibility to re-annotate the WMT-
12 training set with an automatic procedure.
4.1 Approach
Our approach, which does not involve subjec-
tive human judgements, is based on the observa-
tion of similarities and dissimilarities between an
automatic translation (TGT), its post-edited ver-
sion (PE) and the corresponding reference trans-
lation (RT). Such comparisons provide useful in-
dications about the behaviour of a post-editor
when correcting automatic translations and, in
turn, about MT output quality.
Typically, the PE version of a good-quality TGT
preserves some characteristics (e.g. lexical, struc-
tural) that indicate a moderate correction activity
by the post editor. Conversely, in the PE ver-
sion of a low-quality TGT, such characteristics
are more difficult to observe, indicating an in-
tense correction activity. At the two extremes, the
PE of a perfect TGT preserves all its characteris-
tics, while the PE of a useless TGT looses most
of them. In the first case TGT and PE are iden-
243
tical, and their similarity is the highest possible
(i.e. sim(TGT, PE) = 1). In the second case,
TGT and PE show a degree of similarity close to
that of TGT and a completely rewritten transla-
tion featuring different lexical choices and struc-
ture. This is where reference translations come
into play: considering RT as a good example of
rewritten sentence,7 for low-quality TGT we will
have sim(TGT, PE) ? sim(TGT,RT ).
In light of these considerations, we hypothe-
size that the automatic re-annotation of WMT-12
training data can take advantage of a classifier that
learns a similarity threshold T such that:
? a PE sentence with sim(TGT, PE) ? T
will be considered as a rewritten translation
(hence TGT is useless, and the correspond-
ing source-TGT pair a negative example to
be labelled as ?-1?);
? a PE sentence with sim(TGT, PE) > T
will be considered as a real post-edition
(hence TGT is useful for the post-editor, and
the corresponding source-TGT pair a positive
example to be labelled as ?+1?).
Based on this hypothesis, to perform our au-
tomatic re-annotation procedure we: 1) create a
training set Z of positive and negative examples
(i.e. [TGT, correct translation] pairs, where cor-
rect translation is either a post-editing or a rewrit-
ten translation); 2) design a feature set capable
to capture different aspects of the similarity be-
tween TGT and correct translation; 3) build a bi-
nary classifier using Z; 4) use the classifier to label
the [TGT, PE] pairs as instances of post-editings
or rewritings; 5) assess the quality of the resulting
annotation.
4.2 Building the classifier
Training corpus. To build a classifier capable
of labelling PE sentences as rewritten/post-edited
material, we first created a set of positive and neg-
ative instances from the WMT-12 training set. For
each tuple [source, TGT, PE, RT] of the dataset,
one positive and one negative instance have been
respectively obtained as the combination of [TGT,
PE] and [TGT, RT]. Figure 1, which plots the dis-
tribution of positive and negative instances against
HTER, shows a fairly good separation between the
7Such assumption is supported by the fact that reference
sentences are, by definition, free translations manually pro-
duced without any influence from the target.
0 500 1000 1500 2000 2500 3000 3500 40000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Sentences
HTE
R
 
 TGT?PE sentencesTGT?RT sentences
Figure 1: Distribution of [TGT, PE] and [TGT,
RT] pairs plotted against the HTER.
two classes. This indicates that our use of the
references as examples of rewritten translations
builds on a reasonable assumption.
Features. Crucial to our classification task, a
number of features can be used to estimate sen-
tence similarity. Differently from the binary QE
task, where the possibility to catch common char-
acteristics between two sentences is limited by
language barriers, in our re-annotation task all the
features are extracted by comparing two monolin-
gual sentences (i.e. TGT and a correct translation,
either a PE or a RT). Although the problem of
measuring sentence similarity can be addressed
in many ways, the solutions should not overlook
the specificities of the task. In our case, for in-
stance, the scarce importance of the semantic as-
pect (TGT, PE and RT typically show a high se-
mantic similarity) makes features used for other
tasks (e.g. based on distributional similarity) less
effective than shallow features looking at the sur-
face form of the input sentences. Our problem
presents some similarities with the plagiarism de-
tection task, where subtle lexical and structural
similarities have to be identified to spot suspicious
plagiarized texts (Potthast et al, 2010). For this
reason, part of our features (e.g. ROUGE scores)
are inspired by research in such field (Chen et al,
2010), while others have been designed ad-hoc,
based on the specific requirements of our task. The
resulting feature set aims to capture text similar-
ity by measuring word/n-gram matches, as well as
the level of sparsity and density of the common
words as a shallow indicator of structural similar-
ity. In total, from each [TGT, correct translation]
244
pair, the following 22 features are extracted:
? Human-targeted Translation Error Rate ?
HTER. The editing operations considered
are: shift, insertion, substitution and deletion.
? Number of words in common.
? Number of words in common, normalized by
TGT length and correct translation length (2
features).
? Number of words in TGT and in the cor-
rect translation (2 features).
? Size of the longest common subsequence.
? Size of the longest common subsequence,
normalized by TGT length.
? Aligned word density: total number of
aligned words,8 divided by the number of
aligned blocks (more than 1 aligned word).
? Unaligned word density: total number of un-
aligned words, divided by the number of un-
aligned blocks (more than 1 unaligned word).
? Normalized number of aligned blocks: total
number of aligned blocks, divided by TGT
length.
? Normalized number of unaligned blocks: to-
tal number of unaligned blocks, divided by
TGT length.
? Normalized density difference: difference
between aligned word density and unaligned
word density, divided by TGT length.
? Modified Lesk score (Lesk, 1986): sum of
the squares of the length of n-gram matches,
normalized by the product of the sentence
lengths.
? ROUGE-1/2/3/4: n-gram recall with n=1,...,4
(4 features).9
? ROUGE-L: size of longest common
subsequence, normalized by the cor-
rect translation length.
? ROUGE-W: the ROUGE-L using different
weights for consecutive matches of length L
(default weight = 1.2).
? ROUGE-S: the ROUGE-L allowing for the
presence of skip-bigrams (pairs of words,
even not adjacent, in their sentence order).
? ROUGE-SU: the extension of ROUGE-S
adding unigrams as counting unit.
8Monolingual stem-to-stem exact matches between TGT
and correct translation are inferred by computing the HTER,
as in (Blain et al, 2012).
9All ROUGE scores, described in (Lin, 2004), have been
calculated using the software available at http://www.
berouge.com.
To increase the capability of identifying simi-
lar sentences, all sentences are tokenized, lower-
cased and stemmed using the Snowball algorithm
(Porter, 2001).
Classifier. On the resulting corpus, an SVM
classifier has been trained using the LIBSVM tool-
box (Chang and Lin, 2011). The selection of the
kernel (linear) and the optimization of the param-
eters (C=0.8) were carried out through grid search
in 5-fold cross-validation.
Labelling the dataset. Using the best parameter
setting obtained, [TGT, PE] and [TGT, RT] pairs
have been re-labelled as post-editings or rewrit-
ings through 5 rounds of cross-validation. The fi-
nal label of each instance was set to the mode of
the predictions produced by each cross-validation
round. Since we assume that the quality of the tar-
get sentence can be inferred from the amount of
correction activity done by the post-editor, the la-
bels assigned to the [TGT, PE] pairs represent the
result of our re-annotation of the corpus into posi-
tive and negative instances.
At the end of the process, of the 1,832 [TGT,
PE] pairs of the WMT 2012 training set, 1.394 are
labelled as examples of post-editing (TGT is use-
ful), and 438 as examples of complete rewriting
(TGT is useless). Compared to the distribution
of positive and negative instances obtained with
most of the partition methods described in Section
3, our automatic annotation produces a fairly bal-
anced dataset. The resulting proportion of nega-
tive examples (?1:3) is similar to what could be
reached only by partitions reflecting a ?soft? sep-
aration into worse versus better translations rather
than a strict separation into useless versus useful
translations.10 In Figure 2, the labelling results
plotted against the HTER show that there is a quite
clear separation between [TGT, PE] pairs marked
as post-editings (lower HTER values) and pairs
marked as rewritings (higher HTER values). Such
separation corresponds to an HTER value around
0.4, which is significantly lower than the thresh-
old of 0.7 proposed by the WMT-12 guidelines as
a criterion to label sentences for which ?a trans-
lation from scratch is necessary?. This confirms
that our separation differs from those produced by
partition methods based on human annotations or
arbitrary HTER thresholds. Furthermore, our au-
10Such partitions are: average effort scores = 3, human
scores = 3-3-3, HTER score = 0.45.
245
0 200 400 600 800 1000 1200 1400 1600 1800 20000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Sentences
HTE
R
 
 TGT?PE labelled as PETGT?PE labelled as RT
Figure 2: TGT-PE classification in post-editings
and rewritings.
tomatic annotation procedure relies on the contri-
bution of features designed to capture different as-
pects of the similarity between the TGT and a cor-
rect translation, while some of the partition meth-
ods discussed in Section 3 rely on thresholds set on
a single score (e.g. HTER). Considering the many
facets of the binary QE problem, we expect that
our features are more effective to deal with latent
aspects disregarded by such thresholds.
5 Experiments and results
At this point, the question is: are the automatically
labelled data more suitable than partitions based
on human labels to train a binary QE classifier?
To answer this question, all the proposed separa-
tions of the WMT-12 training set have been eval-
uated on different test sets. For each separation
we trained a binary classifier able to assign a label
(good or bad) to unseen source-target pairs. Since
the classifiers use the same algorithm and feature
set, differences in performance will mainly depend
on the quality of the training data on which they
are built. Using task-oriented metrics sensitive to
the number of false positives, results highlighting
such differences will indicate the best separation.
5.1 Experimental Setting
Binary QE classifier. Each separation of the
WMT-12 training data was used to train a binary
SVM classifier. Different kernels and parameters
were optimized through a grid search in 5-fold
cross-validation on each training set. Being the
number of positive and negative training instances
highly unbalanced, the best models were selected
optimizing a metric that takes into account the
number of true and false positives (see below).
Seventeen features proposed in (Specia et al,
2009) were extracted from each source-target pair.
This feature set, fully described in (Callison-
Burch et al, 2012), mainly takes into account the
complexity of the source sentence (e.g. number
of tokens, number of translations per source word)
and the fluency of the target translation (e.g. lan-
guage model probabilities). Results of the WMT
2012 QE task shown that these ?baseline? features
are particularly competitive in the regression task,
with only few systems able to beat them. All the
features are extracted using the Quest software11
and the model files released by the organizers of
the WMT 2013 workshop.
Test sets. To obtain different separations be-
tween good and bad translations, artificial test sets
have been created using arbitrary thresholds on
the HTER (the same used to partition the train-
ing set on a HTER basis) and the post-editing time
(PET).12 Two different datasets were split: i) the
WMT-12 test (422 source, target, post-edited and
reference sentences); ii) the WMT-13 training set
for Task 1.3 (800 source, target and post-edited
sentences labelled with PET). The first dataset, the
most similar to the WMT-12 training set, should
better reflect (and reward) the HTER-based parti-
tions proposed in Section 3. The WMT-13 dataset
contains sentences translated with a different con-
figuration (data and parameters) of the SMT en-
gine. This can result in different HTER-based par-
titions in good and bad, useful to test the portabil-
ity of our automatic re-annotation method across
different datasets. Finally, testing on data parti-
tions based on PET allows us to check the stability
of the automatic re-annotation method when eval-
uated on a test set divided according to a different
concept of translation quality. In the end, the com-
bination of different partition methods, thresholds
and datasets results in 21 different test sets (see
Table 2).
Evaluation metrics. F-score and accuracy are
the classic evaluation metrics used in classifica-
tion. In our evaluation, however, they would al-
ways result in high uninformative values due to
the unbalanced nature of the test sets (positive in-
stances  negative instances). In order to bet-
11http://www.quest.dcs.shef.ac.uk/
12PET is the time spent by a post-editor to transform the
target into a publishable sentence.
246
Test instances
WMT-12 HTER Positive Negative
0.45 289 133
0.5 319 103
0.55 352 70
0.6 371 51
0.65 386 36
0.70 398 24
0.75 406 16
WMT-13 Task 1.3 HTER Positive Negative
0.45 582 218
0.5 622 178
0.55 695 105
0.6 724 76
0.65 748 52
0.70 763 37
0.75 773 27
WMT-13 Task 1.3 PET Positive Negative
4 499 301
4.16? 517 283
4.50 554 246
5 594 206
6 659 141
7 698 102
8 727 73
Table 2: Number of positive and negative in-
stances for each partition of the WMT-12 test set
and WMT-13 training set. ?*?: Average PET com-
puted on all the instances in the WMT-13 dataset.
ter understand the real quality of the classifica-
tion, we hence opted for two task-oriented evalua-
tion metrics sensitive to the number of false posi-
tives (the main issue in a CAT environment, where
false positives and true positives should be re-
spectively minimized and maximized). These are:
i) the weighted combination of the false positive
rate (FPR) and false discovery rate (FDR) (Ben-
jamini and Hochberg, 1995), and ii) the weighed
average of sensitivity and specificity (also called
balanced/weighted accuracy). FPR measures the
level of false positives, but does not provide infor-
mation about the number of true positives. For this
reason, we combined it with FDR (1-precision),
which indirectly controls the level of true posi-
tives. FPR and FDR were equally weighted in
the average; lower values indicate good perfor-
mance. Furthermore, in our scenario it is desir-
able to have a classifier with high prediction ac-
curacy over the minority class (specificity), while
maintaining reasonable accuracy for the majority
class (sensitivity). Weighted accuracy is useful in
such situations. To better asses the performance on
the minority (negative) class, we hence gave more
importance to specificity (0.7 vs 0.3). As regards
weighted accuracy higher values in indicate bet-
ter performance. Penalizing majority voting clas-
sifiers, both metrics are particularly appropriate in
our framework. Besides evaluation, the weighted
average of FPR and FDR was also used to tune the
parameters of the SVM classifier.
5.2 Results
Table 3 presents the results achieved by classifiers
trained on different datasets, on the 21 splits pro-
duced from the test sets used for evaluation.
Although the total number of classifiers tested
is 16 (15 resulting from partitions based on human
labels, and 1 obtained with our automatic annota-
tion method), most of them are not present in the
table since they predict the majority class for all
the test points. These are, in general, trained on
highly unbalanced training sets where the number
of negative samples is really small. However, it
is interesting to note that increasing the number
of instances in the negative class does not always
result in a better classifier. For instance, the classi-
fier built on an HTER separation with threshold at
0.55 performs majority voting even if it is built on
a more balanced (but probably more noisy) train-
ing set than the classifier obtained with threshold
at 0.6. This suggests that the quality of the sep-
aration is as important as the actual proportion of
positive and negative instances.
On all test sets, and for both the evaluation met-
rics used, the results achieved by the classifier built
from the automatically annotated training set (AA)
produces lower error rates (Weighted FPR-FDR)
and higher accuracy (Weighted Accuracy), outper-
forming all the other classifiers. The effective-
ness of the automatic annotation is confirmed by
the fact that classifiers 3 (based on the average
of effort scores - AES) and 3-3-3 (based on the
actual human scores - HS), which are trained on
more balanced training sets, achieve worse perfor-
mances than the AA classifier.13
Results on the WMT-13 PET test set are not as
good as in the other two test sets. This shows that
test data labelled in terms of time are more dif-
ficult to be correctly classified compared to those
based on the HTER. This can be explained consid-
ering the intrinsic differences between the HTER
and the PET as approximations of the post-editing
13The distribution of positive/negative instances in the
training sets is: 1194/638 for classifier 3, 1360/472 for clas-
sifier 3-3-3, 1394/438 for classifier AA.
247
Weighted Training: WMT-12 Separations
FPR-FDR 3 2-2-X 2-3-3 3-3-3 0.5 0.6 AA
AES HS HS HS HTER HTER
Tes
t:W
MT
-12
HT
ER 0.45 0.61 0.66 0.66 0.66 0.66 0.66 0.550.5 0.57 0.62 0.62 0.62 0.62 0.62 0.49
0.55 0.52 0.58 0.58 0.58 0.58 0.58 0.42
0.6 0.5 0.56 0.56 0.56 0.56 0.56 0.4
0.65 0.5 0.54 0.54 0.54 0.54 0.54 0.39
0.7 0.49 0.53 0.53 0.53 0.53 0.53 0.39
0.75 0.49 0.52 0.52 0.52 0.52 0.52 0.35
Tes
t:W
MT
-13
HT
ER 0.45 0.59 0.63 0.63 0.64 0.64 0.63 0.540.5 0.57 0.6 0.6 0.61 0.61 0.6 0.5
0.55 0.51 0.56 0.56 0.57 0.57 0.56 0.41
0.6 0.49 0.54 0.54 0.55 0.55 0.54 0.37
0.65 0.47 0.53 0.53 0.53 0.53 0.53 0.33
0.7 0.44 0.52 0.52 0.52 0.52 0.52 0.29
0.75 0.44 0.52 0.52 0.52 0.52 0.52 0.28
Tes
t:W
MT
-13
PET
4 0.61 0.68 0.68 0.69 0.69 0.68 0.58
4.16 0.61 0.67 0.67 0.67 0.67 0.67 0.56
4.5 0.58 0.65 0.64 0.65 0.65 0.65 0.54
5 0.55 0.63 0.62 0.63 0.63 0.62 0.51
6 0.49 0.58 0.58 0.58 0.58 0.58 0.45
7 0.45 0.55 0.55 0.56 0.56 0.55 0.43
8 0.45 0.54 0.54 0.54 0.54 0.54 0.41
Weighted Training: WMT-12 Separations
Accuracy 3 2-2-X 2-3-3 3-3-3 0.5 0.6 AA
AES HS HS HS HTER HTER
Tes
t:W
MT
-12
HT
ER 0.45 0.35 0.3 0.3 0.3 0.3 0.3 0.410.5 0.35 0.3 0.3 0.3 0.3 0.3 0.44
0.55 0.37 0.3 0.3 0.3 0.3 0.3 0.48
0.6 0.37 0.3 0.3 0.3 0.3 0.3 0.49
0.65 0.35 0.3 0.3 0.3 0.3 0.3 0.47
0.7 0.35 0.3 0.3 0.3 0.3 0.3 0.45
0.75 0.33 0.3 0.3 0.3 0.3 0.3 0.49
Tes
t:W
MT
-13
HT
ER 0.45 0.33 0.31 0.31 0.3 0.3 0.31 0.40.5 0.34 0.31 0.31 0.3 0.3 0.31 0.42
0.55 0.35 0.31 0.31 0.3 0.3 0.31 0.48
0.6 0.35 0.31 0.31 0.3 0.3 0.31 0.51
0.65 0.36 0.3 0.3 0.3 0.3 0.3 0.54
0.7 0.39 0.3 0.3 0.3 0.3 0.3 0.56
0.75 0.38 0.3 0.3 0.3 0.3 0.3 0.59
Tes
t:W
MT
-13
PET
4 0.37 0.3 0.31 0.3 0.3 0.3 0.4
4.16 0.37 0.3 0.31 0.3 0.3 0.3 0.4
4.5 0.37 0.3 0.31 0.3 0.3 0.3 0.4
5 0.38 0.31 0.31 0.3 0.3 0.31 0.41
6 0.41 0.31 0.31 0.3 0.3 0.31 0.43
7 0.42 0.31 0.31 0.3 0.3 0.31 0.44
8 0.4 0.31 0.31 0.3 0.3 0.31 0.43
Table 3: Weighted FPR-FDR (left table) and weighted Accuracy (right table) obtained by the binary QE
classifiers trained on different separations of the WMT-12 training set. Several arbitrary partitions of the
WMT-12 Test set and WMT-13 Training set are considered.
effort, as pointed out by several recent works (Spe-
cia, 2011; Koponen, 2012).
Comparing the results calculated with the two
metrics, we note that weighted accuracy seems to
be less sensible to small variations in terms of true
and false negatives returned by the classifier, even
if the specificity (accuracy on our minority class)
is weighted more than sensitivity (accuracy on our
majority class). This often results in scores very
close (differences ? 10?3) to the accuracy ob-
tained by majority voting classification (0.3).
Overall, our experiments demonstrate that the
proposed automatic separation method is more ef-
fective than arbitrary partitions of datasets anno-
tated with subjective human judgements.
5.3 Learning Curve
Our automatic re-annotation approach requires
post-edited and reference sentences. Although all
the datasets annotated for QE include post-edited
sentences, this is not always true for the refer-
ences. The cost of having both resources is in
fact not negligible. For this reason, we investi-
gated the minimal number of training data needed
to re-annotate the WMT-12 training set without
altering performance on binary classification. To
this aim, we selected two of the test sets on which
our re-annotation method produces classifiers with
high performance results (WMT-13 HTER 0.6 and
0.75), and measured score variations with increas-
ing amounts of data.
Nine subsets of the WMT-12 training set cor-
pus were created (with 10%, 20%,..., 100% of the
dataset) by sub-sampling sentences from a uni-
form distribution. The process was iterated 10
times. Then, for each subset, a new re-annotation
process was run, the resulting training set was used
to build the relative binary QE classifier, which
was eventually evaluated on the test set in terms of
weighted FPR-FDR. Figures 3 and 4 show the ob-
tained learning curves. Each point is the average
result of the 10 runs; the error bars show ?1std.
As can be seen from both curves, performance
results with 60% of the training data are already
comparable with those obtained using the whole
training data. Similar trends have been observed
for several learning curves created with different
test sets. This shows that, besides avoiding the
use of human labelled data, our approach allows
to drastically reduce the amount of training in-
stances. Considering the high costs of collecting
post-editions, and the fact that reference transla-
tions can be taken from parallel corpora, our solu-
tion represents a viable way to overcome the lack
of training data for binary QE geared towards in-
tegration in a CAT environment.
248
0 0.2 0.4 0.6 0.8 10.36
0.38
0.4
0.42
0.44
0.46
0.48
0.5
Training Set Size
Wei
ghte
d FP
R?F
DR
Figure 3: Learning curve for WMT-13 HTER 0.60.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.32
0.37
0.42
0.47
0.52
Training Set Size
Wei
ghte
d FP
R?F
DR
Figure 4: Learning curve for WMT-13 HTER 0.75.
6 Conclusion
We presented a task-oriented analysis of the use-
fulness of human-labelled data for binary qual-
ity estimation. Our target scenario is computer-
assisted translation, which calls for solutions to
present human translators with useful MT sugges-
tions (i.e. easier to correct than to rewrite from
scratch). Within this framework, the integration
of binary classifiers capable to distinguish ?good?
(useful) from ?bad? (useless) suggestions would
make possible to significantly increase translators?
productivity. Such binary classifiers, however,
need labelled training data (possibly of good qual-
ity) that are currently not available.
An intuitive solution to fill this gap is to take
advantage of an existing dataset, adapting its man-
ual annotations to our task. Exploring this solu-
tion (the first contribution of this paper) has to
face problems related to the subjectivity of human
judgements about translation quality, and the re-
sulting variability in the annotation. In particular,
our experiments with the WMT-12 dataset show
that any adaptation (either based on human judge-
ments or arbitrarily-set HTER thresholds) collides
with the problem of setting reasonable partition
criteria. Our results suggest that the subtle dif-
ferences between useful and useless translations
make subjective human judgements inadequate to
learn effective models.
Instead of relying on manually-assigned qual-
ity labels, an alternative solution to the problem
is to re-annotate an existing dataset. Proposing
an automatic way to do that (the second contri-
bution of this paper), we argue that reliable data
separations into positive and negative examples
can be obtained by measuring the similarities be-
tween: i) automatic translations and post-editings,
and ii) automatic translations and their references.
Our results demonstrate that binary classifiers built
from training data produced with our supervised
method are less prone to the misclassification of
bad suggestions.
As in any supervised learning framework, the
amount of data needed to obtain good results is of
crucial importance. By analysing the demand of
our automatic annotation method in terms of train-
ing data (the third contribution of this paper), we
show that competitive results can be obtained with
a fraction of the data needed by methods based on
human labels. Our results indicate that a good-
quality training set for binary classification can
be obtained with 40% less instances of [training,
post edited sentence, reference sentence], totally
avoiding manually-assigned quality judgements.
Our future works will address the improvement
of the automatic annotation procedure using super-
vised methods suitable to learn from unbalanced
training sets (e.g. one-class SVM, weighted ran-
dom forests), and the integration of new features
(e.g. GTM, meteor) to refine our classification of a
correct sentence into rewritten/post-edited. Then,
to boost binary QE results on the resulting corpora,
the ?baseline? features used for experiments in this
paper will be extended with new features explored
in recent works (Mehdad et al, 2012a; de Souza
et al, 2013; Turchi and Negri, 2013).
Acknowledgments
This work has been partially supported by the EC-
funded project MateCat (ICT-2011.4.2-287688).
249
References
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011. Goodness: a Method for Measuring Ma-
chine Translation Confidence. In The 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, Proceed-
ings of the Conference, 19-24 June, 2011, Portland,
Oregon, USA, pages 211?219. The Association for
Computer Linguistics.
Yoav Benjamini and Yosef Hochberg. 1995. Control-
ling the False Discovery Rate: a Practical and Pow-
erful Approach to Multiple Testing. Journal of the
Royal Statistical Society. Series B (Methodological),
pages 289?300.
Fre?de?ric Blain, Holger Schwenk, and Jean Senellart.
2012. Incremental Adaptation Using Translation In-
formation and Post-Editing Analysis. In Interna-
tional Workshop on Spoken Language Translation,
pages 234?241, Hong-Kong (China).
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence Es-
timation for Machine Translation. Summer work-
shop final report, JHU/CLSP.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation
(WMT?12), pages 10?51, Montre?al, Canada.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: a Library for Support Vector Machines. ACM
Trans. Intell. Syst. Technol., 2(3):27:1?27:27, May.
Chien-Ying Chen, Jen-Yuan Yeh, and Hao-Ren Ke.
2010. Plagiarism Detection using ROUGE and
WordNet. Journal of Computing, 2(3).
Jose? G. C. de Souza, Miquel Espla`-Gomis, Marco
Turchi, and Matteo Negri. 2013. Exploiting Quali-
tative Information from Automatic Word Alignment
for Cross-lingual NLP Tasks. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2013).
Michael Denkowski and Alon Lavie. 2012. Chal-
lenges in Predicting Machine Translation Utility
for Human Post-Editors. In Proceedings of AMTA
2012.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging SMT and TM with Translation
Recommendation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Philip Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of Machine Translation Summit X, pages 79?86,
Phuket, Thailand.
Maarit Koponen. 2012. Comparing Human Percep-
tions of Post-editing Effort with Post-editing Oper-
ations. In Proceedings of the Seventh Workshop on
Statistical Machine Translation, pages 181?190. As-
sociation for Computational Linguistics.
Michael Lesk. 1986. Automated Sense Disambigua-
tion Using Machine-readable Dictionaries: How to
Tell a Pine Cone from an Ice Cream Cone. In Pro-
ceedings of the 5th annual international conference
on Systems documentation (SIGDOC86).
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Proceedings of
the ACL workshop on Text Summarization Branches
Out., pages 74?81, Barcelona, Spain.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. 2011. Consistent Translation using Discrim-
inative Learning: a Translation Memory-inspired
Approach. Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1239?
1248.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012a. Detecting semantic equivalence and infor-
mation disparity in cross?lingual documents. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?12),
pages 120?124, Jeju Island, Korea.
Yashar Mehdad, Matteo Negri, and Marcello Fed-
erico. 2012b. Match without a Referee: Evaluat-
ing MT Adequacy without Reference Translations.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, WMT ?12, pages 171?
180, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
M. Porter. 2001. Snowball: A language for stemming
algorithms.
Martin Potthast, Alberto Barro?n-Ceden?o, Andreas
Eiselt, Benno Stein, and Paolo Rosso. 2010.
Overview of the 2nd International Competition on
Plagiarism Detection. Notebook Papers of CLEF,
10.
250
Christopher B. Quirk. 2004. Training a Sentence-
Level Machine Translation Confidence Measure. In
In Proceedings of LREC.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy,
or HTER?: Exploring Different Human Judgments
with a Tunable MT Metric. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, StatMT ?09, pages 259?268, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Radu Soricut and Abdessamad Echihabi. 2010.
TrustRank: Inducing Trust in Automatic Transla-
tions via Ranking. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 612?621, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL language weaver systems in the WMT12
quality estimation shared task. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion (WMT?12), pages 145?151, Montre?al, Canada.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Es-
timating the Sentence-Level Quality of Machine
Translation Systems. In Proceedings of the 13th
Annual Conference of the European Association
for Machine Translation (EAMT?09), pages 28?35,
Barcelona, Spain.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine Translation Evaluation versus Quality Es-
timation. Machine translation, 24(1):39?50.
Lucia Specia, Najeh Hajlaoui, Catalina Hallett, and
Wilker Aziz. 2011. Predicting machine transla-
tion adequacy. In Proceedings of the 13th Ma-
chine Translation Summit, pages 513?520, Xiamen,
China, September.
Lucia Specia. 2011. Exploiting Objective Annota-
tions for Measuring Translation Post-editing Effort.
pages 73?80.
Marco Turchi and Matteo Negri. 2013. ALTN: Word
Alignment Features for Cross-Lingual Textual En-
tailment. Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013).
251
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 352?358,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
FBK-UEdin participation to the WMT13 Quality Estimation shared-task
Jose? G. C. de Souza
FBK-irst
University of Trento
Trento, Italy
desouza@fbk.eu
Christian Buck
School of Informatics
University of Edinburgh
Edinburgh, UK
christian.buck@ed.ac.uk
Marco Turchi, Matteo Negri
FBK-irst
Trento, Italy
{turchi,negri}@fbk.eu
Abstract
In this paper we present the approach and
system setup of the joint participation of
Fondazione Bruno Kessler and University
of Edinburgh in the WMT 2013 Quality
Estimation shared-task. Our submissions
were focused on tasks whose aim was pre-
dicting sentence-level Human-mediated
Translation Edit Rate and sentence-level
post-editing time (Task 1.1 and 1.3, re-
spectively). We designed features that
are built on resources such as automatic
word alignment, n-best candidate transla-
tion lists, back-translations and word pos-
terior probabilities. Our models consis-
tently overcome the baselines for both
tasks and performed particularly well for
Task 1.3, ranking first among seven parti-
cipants.
1 Introduction
Quality Estimation (QE) for Machine Transla-
tion (MT) is the task of evaluating the quality
of the output of an MT system without relying
on reference translations. The WMT 2013 QE
Shared Task defined four different tasks covering
both word and sentence level QE. In this work
we describe the Fondazione Bruno Kessler (FBK)
and University of Edinburgh approach and system
setup of our participation to the shared task. We
developed models for two sentence-level tasks:
Task 1.1: Scoring and ranking for post-editing ef-
fort, and Task 1.3: Predicting post-editing time.
The first task aims at predicting the Human-
mediated Translation Edit Rate (HTER) (Snover
et al, 2006) between a suggestion generated by
a machine translation system and its manually
post-edited version. The data set contains 2,754
English-Spanish sentence pairs post-edited by one
translator (2,254 for training and 500 for test). We
participated only in the scoring mode of this task.
The second task requires to predict the time, in
seconds, that was required to post edit a transla-
tion given by a machine translation system. Par-
ticipants are provided with 1,087 English-Spanish
sentence pairs, source and suggestion, along with
their respective post-edited sentence and post-
editing time in seconds (803 data points for train-
ing and 284 for test).
For both tasks we applied supervised learning
methods and made use of information about word
alignments, n-best diversity scores, word posterior
probabilities, pseudo-references, and back trans-
lation to train our models. In the remainder of
this paper we describe the features designed for
our participation (Section 2), the learning methods
used to build our models (Section 3), the experi-
ments that led to our submitted systems (Section
4), and we briefly conclude our experience in this
evaluation task (Section 5).
2 Features
2.1 Word Alignment
Information about word alignments is used to ex-
tract quantitative (amount and distribution of the
alignments) and qualitative features (importance
of the aligned terms) under the assumption that
features that explore what is aligned can bring im-
provements to tasks where sentence-level seman-
tic relations need to be identified. Among the pos-
sible applications, Souza et al (2013) recently in-
vestigated with success their application in Cross-
lingual Textual Entailment for content synchro-
nization (Mehdad et al, 2012; Negri et al, 2013).
For our experiments in both tasks we built word
alignment models using the resources made avail-
able for the evaluation campaign. To train the
word alignment models we used the MGIZA++
implementation (Gao and Vogel, 2008) of the IBM
models (Brown et al, 1993) and the concatenation
of Europarl, News Commentary, MultiUN, paral-
352
lel corpora made available for task 1.3. The train-
ing data comprises about 12.8 million sentence
pairs.
The word alignment features are divided into
three main groups: AL, POS and IDF. The
AL group regards quantitative information about
aligned and unaligned words between source
sentence (src) and machine translation output
(tgt). The features of this group are computed
for both src and tgt:
? proportion of aligned words;
? number of contiguous unaligned words nor-
malized by the length of the sentence;
? length of the longest sequence of
aligned/unaligned words normalized by
the length of the sentence;
? average length of aligned/unaligned se-
quences of words;
? position of the first/last unaligned word nor-
malized by the length of the sentence;
? proportion of aligned n-grams in the sen-
tence.
To compute the features of the POS group
we use part-of-speech (PoS) information for each
word in src and tgt. Training and test data for
both tasks were preprocessed with the TreeTag-
ger (Schmid, 1995) and mapped to a more coarse-
grained set of part-of-speech tags (P ) based on the
universal PoS tag set by Petrov et al (2012). In
this group there are two different types of features:
one is computed for the alignments (the mapping
between a word in src and a word in tgt) and
the other is computed for aligned words (words in
src that are aligned to one or more words in tgt
and vice-versa). The features computed over the
alignments are:
? proportion of alignments connecting words
with the same PoS tag;
? proportion of alignments connecting words
with the same PoS tag for each tag p ? P .
The features implemented for aligned words
are:
? proportion of aligned words tagged with p in
the sentence (p ? P ). This feature is pro-
cessed for both src and tgt;
? proportion of words in src aligned with
words in tgt that share the same PoS tag
(and vice-versa);
? proportion of words tagged with p in src and
that are aligned to words with the same tag
p in tgt (and vice-versa). This is done for
every p ? P .
The last group, IDF, is composed by one fea-
ture that explores the notion of inverse document
frequency as another source of qualitative infor-
mation. The idea is that rare words (with higher
IDF) are more informative than frequent words.
The IDF scores for each word are calculated for
English and Spanish on each side of the parallel
corpora used to build the alignment models. This
feature is calculated for both src and tgt (at test
stage, the average IDF value of each language is
assigned to unseen terms):
? summation of the IDF scores of aligned
words in src divided by the sum of IDF
scores of the aligned words in tgt (and vice-
versa).
Preliminary experiments have been executed to
find the best word alignment algorithm for each
task. We explored three different word alignment
algorithms: the hidden Markov model (HMM)
(Vogel et al, 1996) and IBM models 3 and 4
(Brown et al, 1993). We also tried three sym-
metrization models (Koehn et al, 2005): union,
intersection, and grow-diag-final-and, a more
complex symmetrization method which combines
intersection with some alignments from the union.
The best alignment and symmetrization combina-
tion found for Task 1.1 was IBM4 with intersec-
tion and for task 1.3 was HMM with intersec-
tion. These experiments were carried out in 10-
fold cross-validation on the training set and used
only the alignment features.
2.2 N-best Diversity scores
Our n-best diversity features are based on the intu-
ition that a large number of possible choices gen-
erally leads to more errors. While a similar notion
can be expressed locally by counting the transla-
tion options for each word or phrase, we consider
n-best lists as a good approximation of the search
space. This allows us to circumvent problems as-
sociated with the local measures, such as ambigu-
ous alignment and segmentation, and limitations
353
of using the search graph directly such as the in-
ability compute edit distance between hypotheses.
Thus, to quantify the coherence of translation
options we compute a (symmetrical) matrix of
pairwise Levenshtein distances, either on token or
character level, for n-best lists of size up to 100k1
using the baseline system and the systems we de-
scribe in Section 2.4. For this matrix the following
features are produced:
1. The index of the central hypothesis, i.e. the
translation with the minimum average dis-
tance to all other entries.
2. The average edit distance between the cen-
tral hypothesis and all other entries normal-
ized by the length of top scoring hypothesis.
3. Edit distance between top scoring and central
hypothesis
4. Number of hypotheses with an edit distance
to the top-scoring hypothesis below a set
threshold.
2.3 Word Posterior Probabilities
Following previous work on word posterior prob-
abilities (WPPs) (Ueffing et al, 2003) we com-
puted the sequence of edit operations needed to
transform the MT suggestion into all entries of an
n-best list in which we normalized the logarithmic
model scores to resemble probabilities. Tokens are
considered incorrect is the operation is either in-
sert or substitute, otherwise the probability of the
hypothesis counts towards the correctness of the
word. These word-level features were then nor-
malized by taking the geometric mean of the in-
dividual probabilities. We did this for all systems
described in Section 2.4 and varying sizes of n be-
tween 10 and 100k.
2.4 Pseudo-references and back-translation
Motivated by the success of pseudo-reference fea-
tures (Soricut et al, 2012) we employed three ad-
ditional MT systems: one similar to the original
system but trained on more data, a hierarchical
phrase-based system, and a Spanish-English sys-
tem to translate back into English. All models
1Computing the pair-wise edit-distances between all 100k
entries is computationally expensive. However, we found the
n-best lists to be highly repetitive, so that on average only
3.7% of the values had to be computed. The computation is
also trivially parallel.
have been estimated using publicly available soft-
ware (SRILM (Stolcke, 2002), Moses (Koehn et
al., 2007)), and corpora (Europarl, News Com-
mentary, MultiUN, Gigaword). Using the predic-
tions of the English-Spanish systems as pseudo-
references and likewise the original source as ref-
erence for the back-translation system we com-
puted a number of automatic metrics including
BLEU (Papineni et al, 2002), GTM (Turian et al,
2003), PER (Tillmann et al, 1997), TER (Snover
et al, 2006) and Meteor (Denkowski and Lavie,
2011).
3 Learning algorithms
To build our models using the features presented
in Section 2 we tried different learning algorithms.
After some preliminary experiments for both tasks
we decided to use mainly two: support vector
machines (SVM) and extremely randomized trees
(Geurts et al, 2006). For all experiments pre-
sented in this paper we use the Scikit-learn (Pe-
dregosa et al, 2011) implementations of the above
algorithms.
In preliminary experiments we noticed that the
number of features that we were using for both
tasks was leading to poor results when using the
SVM regression (SVR) models. In order to cope
with this problem we performed feature selection
prior to the SVM regression training. For that
we used Randomized Lasso, or stability selec-
tion (Meinshausen and Bu?hlmann, 2010). It re-
samples the training data several times and fits a
Lasso regression model on each sample. Features
that appear in a given number of samples are re-
tained. Both the fraction of the data to be sam-
pled and the threshold to select the features can be
configured. In our experiments we set the sam-
pling fraction to 75%, the selection threshold to
25% and the number of re-samples to 200.
To optimize the SVR with radial basis function
(RBF) kernel hyper-parameters we used random
search (Bergstra and Bengio, 2012) instead of the
traditional grid search procedure. We found ran-
dom search to be as efficient or better than grid
search and it drastically reduced the time required
to compute the best parameter combination.
Finally, we trained an extremely randomized
forest, i.e. an ensemble of extremely randomized
trees. Each tree can be parameterized differently.
The results of the individual trees are combined by
averaging their predictions. When a tree is built,
354
System Features MAE RMSE Predict. Interval Parameters
SVR Base 0.127 0.163 [0.046, 0.671] 347.5918, 0.001, 0.0001
SVR Base + All 0.121 0.155 [0.090, 0.714] 0.4052, 0.0753, 0.0010
RL + SVR Sel(Base + All) 0.119 0.1534 [0.084, 0.745] 40.5873, 0.0484, 0.0002
ET Base + All 0.123 0.156 [0.142, 0.708] 100
ET Base + All 0.122 0.155 [0.164, 0.712] 1000
Table 1: Experiments results for Task 1.1 on 10-fold cross-validation. ?Base? are the 17 baseline features.
?All? corresponds to all the features described in Section 2 in a total of 141 features. ?SVR? is support
vector regression, ?RL? is randomized Lasso and ?ET? is extremely randomized trees. MAE stands for
the average mean absolute error and RMSE is the root mean squared error. Parameters for SVR are C, ,
? and for ET is the number of estimators.
the node splitting step is done at random by pick-
ing the best split among a random subset of the
input features.
4 Experiments
For both tasks we set up a baseline system that
uses the same 17 black box ?baseline? features
provided for the WMT 2012 QE shared task
(Callison-Burch et al, 2012). The baseline model
is trained with an SVM regression with RBF ker-
nel and optimized parameters. Parameter opti-
mization for SVM regression models was per-
formed with 1000 iterations of random search for
which the process was set to minimize the mean
absolute error (MAE)2. The parameters of SVR
with RBF kernel (the penalty parameter C, the
width of the insensitivity zone , and the RBF pa-
rameter ?) are sampled from an exponential distri-
bution.
Experiments for both tasks were run using
10-fold cross-validation on the training set. In
Task 1.3 some data points were annotated by
2 or more post-editors and, in a normal cross-
validation scheme, the same data point might ap-
pear in the training and test set but annotated by
different post-editors. To address this characteris-
tic we implemented a cross-validation that divides
along source sentences, so that all translations of a
source segment end up in either the training or test
portion of a split. The number of features available
for both tasks is not the same (112 for Task 1.1
and 141 for Task 1.3) because there were fewer n-
best diversity, pseudo-references and word poste-
rior probability based features developed with dif-
ferent parameters due to time constraints.
2Given by MAE =
?N
i=1
|H(si)?V (si)|
N , where H(si)is the hypothesis score for the entry si and V (si) is the gold
standard value for si in a dataset with N entries.
During our experiments with the training set,
the best model for Task 1.1 was the combination
of randomized Lasso feature selection with SVR
(0.119 MAE). The extremely randomized trees
presented results around 0.12 MAE worse than the
figures obtained by the SVR models. Results ob-
tained for Task 1.1 are summarized in Table 1.
As for Task 1.3, training results are presented in
Table 2. The best model combines feature selec-
tion (randomized Lasso) with SVR. During train-
ing it obtained the lowest average MAE (38.6).
Compared to the models built with extremely ran-
domized trees, the prediction interval of this sys-
tem is narrower. This indicates that the tree-based
models cover a wider range of data points than the
SVR-based models.
In the official results released by the organiz-
ers our submissions had close performances for
Task 1.1. The difference between the SVR and the
extremely randomized tree models is very small
(around 0.0012 MAE points). For Task 1.3 our
best submission is the one based on ensembles of
trees, a trend that was not observed during train-
ing. Our hypothesis is that the tree-based ensem-
ble model was capable of generalizing the train-
ing data better than the SVR-based ones and that
despite the low number of employed features the
latter was prone to overfitting.
Table 3 presents the official evaluation numbers
for both tasks.
4.1 Feature analysis
To gain some insight about the relevance of the
features we explored in our submissions, we com-
pared the output of the randomized Lasso with
the most important features computed by the ex-
tremely randomized tree algorithm. Below we
present the features that appear in the intersection
355
System Features MAE RMSE Predict. Interval Parameters
SVR Base 41.3 69.2 [5.6, 315.7] 138.7359, 2.3331, 0.0185
SVR Base + All 40.2 70.6 [8.6, 335.6] 308.3817, 0.2194, 0.0009
RL + SVR Sel(Base + All) 38.6 69.1 [11.5, 332.0] 161.5705, 7.3370, 0.0460
ET Base + All 44.1 72.2 [11.9, 446.2] 100
ET Base + All 43.7 72.0 [12.6, 446.2] 1000
Table 2: Experiments results for Task 1.3 on 10-fold cross-validation. ?Base? are the 17 baseline features.
?All? corresponds to all the features described in Section 2 in a total of 141 features. ?SVR? is support
vector regression, ?RL? is randomized Lasso and ?ET? is extremely randomized trees. MAE stands for
the average mean absolute error and RMSE is the root mean squared error. Parameters for SVR are C, ,
? and for ET is the number of estimators.
System MAE RMSE
Task 1.1
Official Baseline 0.1491 0.1822
RL + SVR 0.1450 0.1773
ET 0.1438 0.1768
Task 1.3
Official Baseline 51.93 93.35
RL + SVR 47.92 86.66
ET 47.52 82.60
Table 3: Official results for tasks 1.1 and 1.3 on
the test set.
of these two sets for each task.
In Task 1.1, the feature selection algorithm re-
tained 29 out of 112 features. We take the intersec-
tion of this set with the 29 most relevant features
computed by the ensemble tree-based method.
This selection comes from features based on dif-
ferent resources:
? proportion of words in src aligned with
words in tgt that share the same PoS tag;
? average number of translations per source
word according to IBM Model 1 thresholded
P (t|s) > 0.01;
? average number of translations per source
word according to IBM Model 1 thresholded
P (t|s) > 0.2;
? average source sentence token length;
? number of times the top-scoring hypothesis is
repeated in an 10k-best list;
? position of the first unaligned word normal-
ized by the length of the sentence for src
and tgt;
? position of the last unaligned word normal-
ized by the length of the sentence for src
and tgt;
? summation of the IDF scores of aligned
words in tgt divided by the summation of
IDF scores of the aligned words in src;
? length of the longest sequence of unaligned
words normalized by the length of the src;
? percentage of bigrams in the 4th quartile of
frequency of the source language corpus;
? percentage of trigrams in the 4th quartile of
frequency of the source language corpus;
? proportion of alignments connecting words
with the same PoS tag;
? proportion of aligned words in src.
For Task 1.3, the randomized Lasso selection
reduced the input feature vector from 141 fea-
tures to 19. We compared these features with the
19 most important features computed by the ex-
tremely randomized tree algorithm. As above the
intersection of both sets utilizes many resources:
? proportion of aligned words in src with the
adjective PoS tag.
? rank of central hypothesis (see Section 2.2)
and average edit distance to all other entries
in 10k-best list of Spanish-English backtrans-
lation system;
? language model probability for tgt;
? length of the longest sequence of aligned
words in tgt;
356
? number of occurrences of the target word
within the target hypothesis averaged for all
words in the hypothesis;
? percentage of bigrams in the 4th quartile of
frequency of the source language corpus;
? percentage of trigrams in the 4th quartile of
frequency of the source language corpus;
? number of contiguous unaligned words in
tgt normalized by the length of tgt.
5 Conclusion
This paper presented the participation of FBK
and University of Edinburgh to the WMT 2013
Quality Estimation shared task. Our approach
explored features based on word alignment, n-
best diversity scores, pseudo-references and back-
translations, and word posterior probabilities. We
experimented with two different learning methods,
SVR and extremely randomized trees for predict-
ing sentence-level post-editing time and HTER.
Our submitted systems were particularly suc-
cessful for predicting sentence-level post-editing
time, ranking 1st among seven participants. The
submitted models for predicting HTER consis-
tently overcome the baseline for the task. In addi-
tion to the description of our approach and system
setup, we presented a first analysis of the features
used in our models with the objective of assess-
ing the importance of the features used either for
predicting time or HTER.
6 Acknowledgments
This work was partially funded by the European
Commission under the project MateCat, Grant
287688. The authors want to thank Philipp Koehn
for training two of the models used in Section 2.2.
References
James Bergstra and Yoshua Bengio. 2012. Random
Search for Hyper-Parameter Optimization. Journal
of Machine Learning Research, 13:281?305, March.
Peter F. E Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation :
Parameter Estimation. Computational Linguistics,
19(2):263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the 7th Work-
shop on Statistical Machine Translation, pages 10?
51, Montreal, Canada, June. Association for Com-
putational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance, SETQA-
NLP ?08, pages 49?57, Stroudsburg, PA, USA.
Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006. Extremely randomized trees. Machine Learn-
ing, 63(1):3?42, March.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zenz, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL 2007 Demo and Poster Sessions, pages 177?
180, June.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Detecting Semantic Equivalence and Infor-
mation Disparity in Cross?lingual Documents. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?12),
pages 120?124, Jeju Island, Korea.
Nicolai Meinshausen and Peter Bu?hlmann. 2010.
Stability selection. Journal of the Royal Statis-
tical Society: Series B (Statistical Methodology),
72(4):417?473, July.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2013.
Semeval-2013 Task 8: Cross-lingual Textual Entail-
ment for Content Synchronization. In Proceedings
of the 7th International Workshop on Semantic Eval-
uation (SemEval 2013).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?318,
July.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
357
E. Duchesnay. 2011. Scikit-learn : Machine Learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A Universal Part-of-Speech Tagset. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12), Istan-
bul, Turkey, May. European Language Resources
Association (ELRA).
Helmut Schmid. 1995. Improvements in Part-of-
Speech Tagging with an Application to German. In
Proceedings of the ACL SIGDAT-Workshop.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Association for Machine Translation in
the Americas, pages 223?231.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceedings of
the 7th Workshop on Statistical Machine Transla-
tion, pages 145?151.
Jose? G. C. de Souza, Miquel Espla`-Gomis, Marco
Turchi, and Matteo Negri. 2013. Exploiting qualita-
tive information from automatic word alignment for
cross-lingual nlp tasks. In The 51st Annual Meeting
of the Association for Computational Linguistics -
Short Papers (ACL Short Papers 2013).
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
pages 901?904, Denver, Colorado.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Alex Zubiaga, and Hassan Sawaf. 1997. Accel-
erated DP Based Search for Statistical Translation.
In Proceedings of the Fifth European Conference
on Speech Communication and Technology, pages
2667?2670, Rhodos, Greece.
Joseph P. Turian, Luke Shen, and I. Dan Melamed.
2003. Evaluation of machine translation and its
evaluation. In In Proceedings of MT Summit IX,
pages 386?393, New Orleans, LA, USA.
Nicola Ueffing, Klaus Macherey, and Hermann Ney.
2003. Confidence measures for statistical machine
translation. In In Procedings of Machine Transla-
tion Summit IX.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics-Volume 2, pages 836?
841, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
358
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 322?328,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
FBK-UPV-UEdin participation in the WMT14 Quality Estimation
shared-task
Jos
?
e G. C. de Souza
?
University of Trento
Fondazione Bruno Kessler
Trento, Italy
desouza@fbk.eu
Jes?us Gonz
?
alez-Rubio
?
PRHLT Group
U. Polit`ecnica de Val`encia
Valencia, Spain
jegonzalez@prhlt.upv.es
Christian Buck
?
University of Edinburgh
School of Informatics
Edinburgh, Scotland, UK
cbuck@lantis.de
Marco Turchi, Matteo Negri
Fondazione Bruno Kessler
turchi,negri@fbk.eu
Abstract
This paper describes the joint submission
of Fondazione Bruno Kessler, Universitat
Polit`ecnica de Val`encia and University of
Edinburgh to the Quality Estimation tasks
of the Workshop on Statistical Machine
Translation 2014. We present our submis-
sions for Task 1.2, 1.3 and 2. Our systems
ranked first for Task 1.2 and for the Binary
and Level1 settings in Task 2.
1 Introduction
Quality Estimation (QE) for Machine Translation
(MT) is the task of evaluating the quality of the
output of an MT system without reference transla-
tions. Within the WMT 2014 QE Shared Task four
evaluation tasks were proposed, covering both
word and sentence level QE. In this work we de-
scribe the Fondazione Bruno Kessler (FBK), Uni-
versitat Polit`ecnica de Val`encia (UPV) and Uni-
versity of Edinburgh (UEdin) approach and sys-
tem setup for the shared task.
We developed models for two sentence-level
tasks: Task 1.2, scoring for post-editing effort,
and Task 1.3, predicting post-editing time, and
for all word-level variants of Task 2, binary and
multiclass classification. As opposed to previous
editions of the shared task, this year the partici-
pants were not supplied with the MT system that
was used to produce the translation. Furthermore
no system-internal features were provided. Thus,
while the trained models are tuned to detect the
errors of a specific system the features have to be
generated independently (black-box).
2 Sentence Level QE
We submitted runs to two sentence-level tasks:
Task 1.2 and Task 1.3. The first task aims at
?
Contributed equally to this work.
predicting the Human mediated Translation Edit
Rate (HTER) (Snover et al., 2006) between a sug-
gestion generated by a machine translation sys-
tem and its manually post-edited version. The
data set contains 1,104 English-Spanish sentence
pairs post-edited by one translator (896 for train-
ing and 208 for test). The second task requires
to predict the time, in milliseconds, that was re-
quired to post edit a translation given by a ma-
chine translation system. Participants are provided
with 858 English-Spanish sentence pairs, source
and suggestion, along with their respective post-
edited sentence and post-editing time in seconds
(650 data points for training and 208 for test). We
participated in the scoring mode of both tasks.
2.1 Features
For our sentence-level submissions we compute
features using different resources that do not use
the MT system internals. We use the same set of
features for both Task 1.2 and 1.3.
QuEst Black-box features (quest79). We ex-
tract 79 black-box features that capture the com-
plexity, fluency and adequacy aspects of the QE
problem. These features are extracted using the
implementation provided by the QuEst framework
(Specia et al., 2013). Among them are the 17 base-
line features provided by the task organizers.
The complexity features are computed on the
source sentence and indicate the complexity of
translating the segment. Examples of these fea-
tures are the language model (LM) probabilities
of the source sentence computed in a corpus of the
source language, different surface counts like the
number of punctuation marks and the number of
tokens in the source sentence, among others.
The fluency features are computed over the
translation generated by the MT system and in-
dicate how fluent the translation is in the target
322
language. One example would again be the LM
probability of the translation given by a LM model
trained on a corpus of the target language. Another
example is the average number of occurrences of
the target word within the target segment.
The third aspect covered by the QuEst features
is the adequacy of the translation with respect to
the source sentence, i.e., how the meaning of the
source is preserved in the translation. Examples of
features are the ratio of nouns, verbs and adjectives
in the source and in the translation. For a more
detailed description of the features in this group
please refer to (Specia et al., 2013).
Word alignment (wla). Following our last
year?s submission (de Souza et al., 2013a) we ex-
plore information about word alignments to ex-
tract quantitative (amount and distribution of the
alignments) and qualitative features (importance
of the aligned terms). Our assumption is that
features that explore what is aligned can bring
improvements to tasks where sentence-level se-
mantic relations need to be identified. We train
the word alignment models with the MGIZA++
toolkit (Gao and Vogel, 2008) implementation of
the IBM models (Brown et al., 1993). The models
are built on the concatenation of Europarl, News
Commentary, and MultiUN parallel corpora made
available in the QE shared task of 2013, compris-
ing about 12.8 million sentence pairs. A more de-
tailed description of the 89 features extracted can
be found in (de Souza et al., 2013a; de Souza et
al., 2013b).
Word Posterior Probabilities (wpp). Using an
external SMT system we produce 100k-best lists
from which we derive Word Posterior Probabili-
ties as detailed in Subsection 3.1.
We use the geometric mean of these probabili-
ties to derive a sentence-level score.
Because the system that we use to produce the
N-best list is not the same that generated the sug-
gestions some suggested words never appear in the
N-best list and thus receive zero probability. To
overcome this issue we first clip the WPPs to a
minimum probability. Using a small sample of the
data to estimate this number we arrive at:
log(p)
min
= ?2.
N-best diversity (div). Using the same 100k-
best list as above we extract a number of measures
that grasp the spatial distribution of hypotheses in
the search space as described in (de Souza et al.,
2013a).
Word Prediction (wpred). We introduce the
use of the predictions provided by the word-level
QE system described in Section 3 to leverage in-
formation for the sentence-level tasks. We com-
bine the binary word-level predictions in different
ways, with the objective of measuring the fluency
of the translation in a more fine-grained way. We
target a quantitative aspect of the words by com-
puting ratios of OK or BAD predictions. Further-
more, we also explore a qualitative aspect by cal-
culating ratios of different classes of words given
by their part-of-speech tags, indicating the qual-
ity of distinct meaningful regions that compose the
translation sentence. In total, we compute 18 fea-
tures:
? number of OK predictions divided by the no.
of words in the translation sentence (1 fea-
ture);
? number of OK function/content words predic-
tions divided by the no. of function/content
words in the translation (2 features);
? number of OK nouns, verbs, proper-nouns,
adjective, pronouns predictions divided by
the total nouns, verbs, proper-nouns, adjec-
tive, pronouns (5 features);
? size of the longest sequence of OK/BAD word
predictions divided by the total number of
OK/BAD predictions in the translation (2 fea-
tures);
? number of OK predicted n-grams divided by
the total number of n-grams in the transla-
tion. We vary n from 2 to 5 (4 features);
? number of words predicted as OK in the
first/second half of the translation divided by
the total number of words in the first/second
half of the translation (2 features).
? number of words predicted as OK in the
first/second quarter of the translation di-
vided by the total number of words in the
first/second quarter of the translation (2 fea-
tures).
For some instances of the sentence-level tasks
we were not able to produce word-level predic-
tions due to an incomplete overlap between the
word-level and sentence-level tasks datasets. For
such data points we use the median of the feature
column for Task 1.2 and the mean for Task 1.3.
323
Method Features Train T1.2 Train T1.3 Test T1.2 Test T1.3
SVR baseline 16.90 16864 15.23 21490
ET baseline 16.25 17888 17.73 19400
ET quest79 + wla + wpp 15.62 17474 14.44 18658
ET quest79 + wla + wpp + div
2
15.57 17471 14.38 18693
ET quest79 + wla + wpp + div + wpred
1
15.05 16392 12.89 17477
Table 1: Training and test results for Task 1.2 and 1.3. Scores are the MAE on a development set
randomly sampled from the training data (20%). Baseline features were provided by the shared task
organizers. We used Support Vector Machines (SVM) regression to train the baseline models (first row).
Submissions are marked with
1
and
2
for primary and secondary, respectively.
2.2 Experimental Setup
We build the sentence-level models for both tasks
(T1.2 and T1.3) with the features described in Sec-
tion 2.1 using one learning algorithm: extremely
randomized trees (ET) (Geurts et al., 2006). ET is
an ensemble of randomized trees in which each
decision tree can be parameterized differently.
When a tree is built, the node splitting step is done
at random by picking the best split among a ran-
dom subset of the input features. All the trees
are grown on the whole training set and the re-
sults of the individual trees are combined by aver-
aging their predictions. The models produced by
this method demonstrated to be robust to a large
number of input features. For our experiments and
submissions we used the ET implementation in-
cluded in the Scikit-learn library (Pedregosa et al.,
2011).
During training we evaluate the models on a
development set. The development set was ob-
tained by randomly sampling 20% of the training
data. The remaining 80% were used for training.
The training process was carried out by optimiz-
ing the ET hyper-parameters with 100 iterations
of random search optimization (Bergstra and Ben-
gio, 2012) set to minimize the mean absolute er-
ror (MAE)
1
on 10-fold cross-validation over the
training data. The ET hyper-parameters optimized
are: the number of decision trees in the ensemble,
the maximum number of features to consider when
looking for the best split, the maximum depth of
the trees used in the ensembles, the minimal num-
ber of samples required to split a node of the tree,
and the minimum number of samples in newly cre-
ated leaves. For the final submissions we run the
random search with 1000 iterations over the whole
training dataset.
1
Given by MAE =
?
N
i=1
|H(s
i
)?V (s
i
)|
N
, where H(s
i
) is
the hypothesis score for the entry s
i
and V (s
i
) is the gold
standard value for s
i
in a dataset with N entries.
2.3 Results
We train models on different combinations of fea-
ture groups (described in Section 2.1). Experi-
ments results are summarized in Table 1. We have
results with baseline features for both SVR and the
ET models. For Task 1.2, adding features from dif-
ferent groups leads to increasing improvements.
The combination of the quest79, wla and wpp
groups outperforms the SVR baseline for Task 1.2
but not for Task 1.3. However, when compared
to the ET model trained with the baseline fea-
tures, it is possible to observe improvements with
this group of features. In addition, adding the
div group on top of the previous three leads to
marginal improvements for both tasks. The best
feature combination is given when adding the fea-
tures based on the word-level predictions, config-
uring the combination of all the feature groups to-
gether (a total of 221 features). For both tasks
this is our primary submission. The contrastive
run for both tasks is the best feature group com-
bination without the word-prediction-based fea-
tures, quest79, wla, wpp and div for Task 1.2 and
quest79, wla, wpp for Task 1.3.
Results on the test set can be found in the two
last columns of Table 1 and are in line with what
we found in the training phase. The rows that do
not correspond to the official submissions and that
are reported on the test set are experiments done
after the evaluation phase. For both tasks the im-
provements increase as we add features on top of
the baseline feature set and the best performance
is reached when using the word prediction fea-
tures with all the other features. The SVR base-
lines performance are the official numbers pro-
vided by the organizers. For Task 1.2 our primary
submission achieves a MAE score lower than the
score achieved during the training phase, show-
ing that the model is robust. For Task 1.3, how-
ever, we do not observe such trend. Even though
324
the primary submission for this task consistently
improves over the other feature combinations, it
does not outperform the score obtained during the
training phase. This might be explained due to
the difference in the distribution between train-
ing and test labels. In Task 1.2 the two distri-
butions are more similar than in Task 1.3, which
presents slightly different distributions between
training and test data.
3 Word-Level QE
Task 2 is the word-level quality estimation of auto-
matically translated news sentences without given
reference translations. Participants are required to
produce a label for each word in one or more of
the following settings:
Binary classification: a OK/BAD label, where
BAD indicates the need for editing the word.
Level1 classification: OK, Accuracy, or
Fluency label specifying a coarser level of
errors for each word, or OK for words with
no error.
Multi-Class classification: one of the 20 error la-
bels described in the shared-task description
or OK for words with no error.
We submit word-level quality estimations for
the English-Spanish translation direction. The cor-
pus contains 1957 training sentences for a total of
47411 Spanish words, and 382 test sentences for a
total of 9613 words.
3.1 Features
Word Posterior Probabilities (WPP) In order
to generate an approximation of the decoder?s
search space as well as an N-best list of possi-
ble translations we re-translate the source using
the system that is available for the 2013 WMT QE
Shared Task (Bojar et al., 2013).
Certainly, there is a mismatch between the orig-
inal system and the one that we used but, since our
system was trained using the same news domain
as the QE data, we assume that both face similar
ambiguous words or possible reorderings. Using
this system we generate a 100k-best list which is
the foundation of several features.
We extract a set of word-level features based on
posterior probabilities computed over N-best lists
as proposed by previous works (Blatz et al., 2004;
Ueffing and Ney, 2007; Sanchis et al., 2007).
Consider a target word e
i
belonging to a transla-
tion e = e
1
. . . e
i
. . . e
|e|
generated from a source
sentence f . Let N (f) be the list of N-best trans-
lations for f . We compute features as the nor-
malized sum of probabilities of those translations
S(e
i
) ? N (f) that ?contain? word e
i
:
1
?
e
??
?N (f)
P(e
??
| f)
?
e
?
?S(e
i
)
P(e
?
| f) (1)
where P(e | f) is the probability translation e given
source sentence f according to the SMT model.
We follow (Zens and Ney, 2006) and extract
three different WPP features depending on the
criteria chosen to compute S(e
i
):
S(e
i
) = {e
?
? N (f) | a=Le(e
?
, e)?e
?
a
i
= e
i
}
S(e
i
) contain those translations e
?
for which the
word Levenshtein-aligned (Levenshtein, 1966) to
position i in e is equal to e
i
.
S(e
i
) = {e
?
? N (f) | e
?
i
= e
i
}
A second option is to select those translations
e
?
that contain the word e
i
at position i.
S(e
i
) = {e
?
? N (f) | ?i
?
: e
?
i
?
= e
i
}
As third option, we select those translations e
?
that contain the word e
i
, disregarding its position.
Confusion Networks (CN) We use the same N-
best list used to compute the WPP features in the
previous section to compute features based on the
graph topology of confusion networks (Luong et
al., 2014). First, we Levenshtein-align all trans-
lations in the N-best list using e as skeleton, and
merge all of them into a confusion network. In this
network, each word-edge is labelled with the pos-
terior probability of the word. The output edges of
each node define different confusion sets of words,
each word belonging to one single confusion set.
Each complete path passing through all nodes in
the network represents one sentence in the N-best
list, and must contain exactly one link from each
confusion set. Looking to the confusion set which
the hypothesis word belongs to, we extract four
different features: maximum and minimum proba-
bility in the set (2 features), number of alternatives
in the set (1 feature) and entropy of the alternatives
in the set (1 feature).
Language Models (LM) As language model
features we produced n-gram length/backoff be-
haviour and conditional probabilities for every
word in the sentence. We employed both an inter-
polated LM taken from the MT system discussed
325
in Section 3 as well as a very large LM which we
built on 62 billion tokens of monolingual data ex-
tracted from Common Crawl, a public web crawl.
While generally following the procedure of Buck
et al. (2014) we apply an additional lowercasing
step before training the model.
Word Lexicons (WL) We compute two dif-
ferent features based on statistical word lexi-
cons (Blatz et al., 2004):
Avg. probability:
1
|f |+1
?
|f |
j=0
P(e
i
| f
j
)
Max. probability: max
0?j?|f |
P(e
i
| f
j
)
where P(e | f) is a probabilistic lexicon, and f
0
is
the source ?NULL? word (Brown et al., 1993).
POS tags (POS) We extract the part-of-speech
(POS) tags for both source and translation sen-
tences using TreeTagger (Schmid, 1994). We use
the actual POS tag of the target word as a feature.
Specifically, we represent it as a one-hot indicator
vector where all values are equal to zero except
the one representing the current tag of the word,
which is set to one. Regarding the source POS
tags, we first compute the lexical probability of
each target word given each source word. Then,
we compute two different feature vectors for each
target word. On the one hand, we use an indica-
tor vector to represent the POS tag of the maxi-
mum probability source word. On the other hand,
we sum up the indicator vectors for all the source
words each one weighted by the lexical probability
of the corresponding word. As a result, we obtain
a vector that represents the probability distribution
of source POS tags for each target word. Addi-
tionally, we extract a binary feature that indicates
whether the word is a stop word or not.
2
Stacking (S) Finally, we also exploit the diverse
granularity of the word labels. The word classes
for the Level1 and Multi-class conditions are fine
grained versions of the Binary annotation, i.e. the
OK examples are the same for all cases.
We re-use our binary predictions as an addi-
tional feature for the finer-grained classes. How-
ever, due to time constrains, we were not able to
run the proper nested cross-validation but used a
model trained on all available data, which there-
fore over-fits on the training data. Cross-validation
results using the stacking approach are thus very
optimistic.
2
https://code.google.com/p/stop-words/
3.2 Classifiers
We use bidirectional long short-term memory
recurrent neural networks (BLSTM-RNNs) as
implemented in the RNNLib package (Graves,
2008). Recurrent neural networks are a connec-
tionist model containing a self-connected hidden
layer. The recurrent connection provides informa-
tion of previous inputs, hence, the network can
benefit from past contextual information. Long
short-term memory is an advanced RNN archi-
tecture that allows context information over long
periods of time. Finally, BLSTM-RNNs com-
bine bidirectional recurrent neural networks and
the long short-term memory architecture allowing
forward and backward context information. Us-
ing such context modelling classifier we can avoid
the use of context-based features that have been
shown to lead to only slight improvements in QE
accuracy (Gonz?alez-Rubio et al., 2013).
As a secondary binary model we train a CRF.
Our choice of implementation is Pocket CRF
3
which, while currently unmaintained, implements
continuous valued features. We use a history of
size 2 for all features and perform 10-fold cross-
validation, training on 9 folds each time.
3.3 Experimental Setup
The free parameters of the BLSTM-RNNs are op-
timized by 10-fold cross-validation on the train-
ing set. Each cross-validation experiment con-
sider eight folds for training, one held-out fold
for development, and a final held-out fold for test-
ing. We estimate the neural network with the eight
training folds using the prediction performance in
the validation fold as stopping criterion. The re-
sult of each complete cross-validation experiment
is the average of the results for the predictions of
the ten held-out test folds. Additionally, to avoid
noise due to the random initialization of the net-
work, we repeat each cross-validation experiment
ten times and average the results. Once the opti-
mal values of the free parameters are established,
we estimate a new BLSTM-RNN using the full
training corpus and we use it as the final model
to predict the class labels of the test words.
Since our objective is to detect words that need
to be edited, we use the weighted averaged F
1
score over the different class labels that denote an
error as our main performance metric (wF1
err
).
We also report the weighted averaged F
1
scores
3
http://pocket-crf-1.sourceforge.net/
326
Binary Level1 MultiClass
Method Features wF1
err
wF1
all
wF1
err
wF1
all
wF1
err
wF1
all
BLSTM-RNNs LM+WPP+CN+WL 35.9 63.0 23.7 59.4 10.7 55.5
+POS 38.5
1
62.7 26.7
1
59.5 12.7
1
55.5
+Stacking ? ? 82.9
2
93.9 64.7
2
88.0
CRF LM+WPP+CN+WL+POS 39.5
2
62.4 0 ? ? ? ?
Table 2: Cross-validation results for the different setups tested for Task 2. Our two submissions are
marked as (
1
) and (
2
) respectively.
over all the classes (wF1
all
).
3.4 Results
Table 2 presents the wF1
err
and wF1
all
scores
for different sets of features. Our initial experi-
ment includes language model (LM), word poste-
rior probability (WPP), confusion network (CN),
and word lexicon (WL) features for a total of 11
features. We extend this basic feature set with the
indicator features based on POS tags for a total of
163 features. We further extend the feature vectors
by adding the stacking feature in a total of 164 fea-
tures.
Analyzing the results we observe that prediction
accuracy is quite low. Our hypothesis is that this is
due to the skewed class distribution. Even for the
binary classification scenario (the most balanced
of the three conditions), OK labels account for two
thirds of the samples. This effect worsens with in-
creasing number of error classes and the resulting
sparsity of observations. As a result, the system
tends to classify all samples as OK which leads to
the low F
1
scores presented in Table 2.
We can observe that the use of POS tags indica-
tor features clearly improved the prediction accu-
racy of the systems in the three conditions. This
setup is our primary submission for the three con-
ditions of task 2.
In addition, we observe that the use of the stack-
ing feature provides a considerable improvement
in prediction accuracy for Level1 and MultiClass.
As discussed above the cross-validation results for
the stacking features are very optimistic. Test pre-
dictions using this setup are our contrastive sub-
mission for Level1 and MultiClass conditions.
Results achieved on the official test set can be
found in Table 3. Much in line with our cross-
validation results the stacking-features prove help-
ful, albeit by a much lower margin. For the bi-
nary task the RNN model strongly outperforms the
CRF.
Setup Binary Level1 MultiClass
BLSTM-RNN 48.7 37.2 17.1
+ Stacking ? 38.5 23.1
CRF 42.6 ? ?
Table 3: Test results for Task 2. Numbers are
weighted averaged F
1
scores (%) for all but the
OK class.
4 Conclusion
This paper describes the approaches and system
setups of FBK, UPV and UEdin in the WMT14
Quality Estimation shared-task. In the sentence-
level QE tasks 1.2 (predicting post-edition effort)
and 1.3 (predicting post-editing time, in ms) we
explored different features and predicted with a
supervised tree-based ensemble learning method.
We were able to improve our results by explor-
ing features based on the word-level predictions
made by the system developed for Task 2. Our best
system for Task 1.2 ranked first among all partici-
pants.
In the word-level QE task (Task 2), we explored
different sets of features using a BLSTM-RNN as
our classification model. Cross-validation results
show that POS indicator features, despite sparse,
were able to improve the results of the baseline
features. Also, the use of the stacking feature pro-
vided a big leap in prediction accuracy. With this
model, we ranked first in the Binary and Level1
settings of Task 2 in the evaluation campaign.
Acknowledgments
This work was supported by the MateCat and Cas-
macat projects, which are funded by the EC un-
der the 7
th
Framework Programme. The authors
would like to thank Francisco
?
Alvaro Mu?noz for
providing the RNN classification software.
327
References
James Bergstra and Yoshua Bengio. 2012. Random
Search for Hyper-Parameter Optimization. Journal
of Machine Learning Research, 13:281?305.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. In Proceedings of
the international conference on Computational Lin-
guistics, pages 315?321.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19:263?311.
Christian Buck, Kenneth Heafield, and Bas van Ooyen.
2014. N-gram Counts and Language Models from
the Common Crawl. In Proceedings of the Lan-
guage Resources and Evaluation Conference.
Jos?e G. C. de Souza, Christian Buck, Marco Turchi,
and Matteo Negri. 2013a. FBK-UEdin participation
to the WMT13 Quality Estimation shared-task. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, pages 352?358.
Jos?e G. C. de Souza, Miquel Espl?a-Gomis, Marco
Turchi, and Matteo Negri. 2013b. Exploiting qual-
itative information from automatic word alignment
for cross-lingual nlp tasks. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 771?776.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57.
Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006. Extremely randomized trees. Machine Learn-
ing, 63(1):3?42.
Jes?us Gonz?alez-Rubio, Jos?e R. Navarro-Cerdan, and
Francisco Casacuberta. 2013. Partial least squares
for word confidence estimation in machine transla-
tion. In 6th Iberian Conference on Pattern Recog-
nition and Image Analysis, (IbPRIA) LNCS 7887,
pages 500?508. Springer.
Alex Graves. 2008. Rnnlib: A recurrent neural
network library for sequence learning problems.
http://sourceforge.net/projects/
rnnl/.
Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707?710.
Ngoc-Quang Luong, Laurent Besacier, and Benjamin
Lecouteux. 2014. Word confidence estimation and
its integration in sentence quality estimation for ma-
chine translation. In Knowledge and Systems Engi-
neering, volume 244, pages 85?98. Springer.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn : Machine Learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Alberto Sanchis, Alfons Juan, and Enrique Vidal.
2007. Estimation of confidence measures for ma-
chine translation. In Proceedings of the Machine
Translation Summit XI, pages 407?412.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, volume 12, pages 44?49.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Association for Machine Translation in
the Americas.
Lucia Specia, Kashif Shah, Jos?e G. C. de Souza, and
Trevor Cohn. 2013. QuEst?a translation quality es-
timation framework. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics, pages 79?84.
Nicola Ueffing and Hermann Ney. 2007. Word-
level confidence estimation for machine translation.
Computational Linguistics, 33:9?40.
Richard Zens and Hermann Ney. 2006. N-gram poste-
rior probabilities for statistical machine translation.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 72?77.
328
