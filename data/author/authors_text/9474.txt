Proceedings of the 12th Conference of the European Chapter of the ACL, pages 345?353,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
End-to-End Evaluation in Simultaneous Translation
Olivier Hamon1,2, Christian F?gen3, Djamel Mostefa1, Victoria Arranz1,
Muntsin Kolss3, Alex Waibel3,4 and Khalid Choukri1
1Evaluations and Language Resources Distribution Agency (ELDA), Paris, France
2 LIPN (UMR 7030) ? Universit? Paris 13 & CNRS, Villetaneuse, France
3 Univerit?t Karlsruhe (TH), Germany
4 Carnegie Mellon University, Pittsburgh, USA
{hamon|mostefa|arranz|choukri}@elda.org,
{fuegen|kolss|waibel}@ira.uka.de
Abstract
This paper presents the end-to-end evalu-
ation of an automatic simultaneous trans-
lation system, built with state-of-the-art
components. It shows whether, and for
which situations, such a system might be
advantageous when compared to a human
interpreter. Using speeches in English
translated into Spanish, we present the
evaluation procedure and we discuss the
results both for the recognition and trans-
lation components as well as for the over-
all system. Even if the translation process
remains the Achilles? heel of the system,
the results show that the system can keep
at least half of the information, becoming
potentially useful for final users.
1 Introduction
Anyone speaking at least two different languages
knows that translation and especially simultaneous
interpretation are very challenging tasks. A human
translator has to cope with the special nature of
different languages, comprising phenomena like
terminology, compound words, idioms, dialect
terms or neologisms, unexplained acronyms or ab-
breviations, proper names, as well as stylistic and
punctuation differences. Further, translation or in-
terpretation are not a word-by-word rendition of
what was said or written in a source language. In-
stead, the meaning and intention of a given sen-
tence have to be reexpressed in a natural and fluent
way in another language.
Most professional full-time conference inter-
preters work for international organizations like
the United Nations, the European Union, or the
African Union, whereas the world?s largest em-
ployer of translators and interpreters is currently
the European Commission. In 2006, the European
Parliament spent about 300 million Euros, 30% of
its budget, on the interpretation and translation of
the parliament speeches and EU documents. Gen-
erally, about 1.1 billion Euros are spent per year
on the translating and interpreting services within
the European Union, which is around 1% of the
total EU-Budget (Volker Steinbiss, 2006).
This paper presents the end-to-end evaluation
of an automatic simultaneous translation system,
built with state-of-the-art components. It shows
whether, and in which cases, such a system might
be advantageous compared to human interpreters.
2 Challenges in Human Interpretation
According to Al-Khanji et al (2000), researchers
in the field of psychology, linguistics and interpre-
tation seem to agree that simultaneous interpre-
tation (SI) is a highly demanding cognitive task
involving a basic psycholinguistic process. This
process requires the interpreter to monitor, store
and retrieve the input of the source language in
a continuous manner in order to produce the oral
rendition of this input in the target language. It is
clear that this type of difficult linguistic and cog-
nitive operation will force even professional in-
terpreters to elaborate lexical or synthetic search
strategies.
Fatigue and stress have a negative effect on the
interpreter, leading to a decrease in simultaneous
interpretation quality. In a study by Moser-Mercer
et al (1998), in which professional speakers were
asked to work until they could no longer provide
acceptable quality, it was shown that (1) during
the first 20 minutes the frequency of errors rose
steadily, (2) the interpreters, however, seemed to
be unaware of this decline in quality, (3) after 60
minutes, all subjects made a total of 32.5 mean-
ing errors, and (4) in the category of nonsense the
number of errors almost doubled after 30 minutes
on the task.
Since the audience is only able to evaluate the
simultaneously interpreted discourse by its form,
345
the fluency of an interpretation is of utmost im-
portance. According to a study by Kopczynski
(1994), fluency and style were third on a list of
priorities (after content and terminology) of el-
ements rated by speakers and attendees as con-
tributing to quality. Following the overview in
(Yagi, 2000), an interpretation should be as natu-
ral and as authentic as possible, which means that
artificial pauses in the middle of a sentence, hes-
itations, and false-starts should be avoided, and
tempo and intensity of the speaker?s voice should
be imitated.
Another point to mention is the time span be-
tween a source language chunk and its target lan-
guage chunk, which is often referred to as ear-
voice-span. Following the summary in (Yagi,
2000), the ear-voice-span is variable in duration
depending on some source and target language
variables, like speech delivery rate, information
density, redundancy, word order, syntactic charac-
teristics, etc. Short delays are usually preferred for
several reasons. For example, the audience is irri-
tated when the delay is too large and is soon asking
whether there is a problem with the interpretation.
3 Automatic Simultaneous Translation
Given the explanations above on human interpre-
tation, one has to weigh two factors when consid-
ering the use of simultaneous translation systems:
translation quality and cost.
The major disadvantage of an automatic system
compared to human interpretation is its translation
quality, as we will see in the following sections.
Current state-of-the-art systems may reach satis-
factory quality for people not understanding the
lecturer at all, but are still worse than human inter-
pretation. Nevertheless, an automatic system may
have considerable advantages.
One such advantage is its considerable short-
term memory: storing long sequences of words is
not a problem for a computer system. Therefore,
compensatory strategies are not necessary, regard-
less of the speaking rate of the speaker. However,
depending on the system?s translation speed, la-
tency may increase. While it is possible for hu-
mans to compress the length of an utterance with-
out changing its meaning (summarization), it is
still a challenging task for automatic systems.
Human simultaneous interpretation is quite ex-
pensive, especially due to the fact that usually two
interpreters are necessary. In addition, human in-
terpreters require preparation time to become fa-
miliar with the topic. Moreover, simultaneous in-
terpretation requires a soundproof booth with au-
dio equipment, which adds an overall cost that is
unacceptable for all but the most elaborate multi-
lingual events. On the other hand, a simultaneous
translation system also needs time and effort for
preparation and adaptation towards the target ap-
plication, language and domain. However, once
adapted, it can be easily re-used in the same do-
main, language, etc. Another advantage is that the
transcript of a speech or lecture is produced for
free by using an automatic system in the source
and target languages.
3.1 The Simultaneous Translation System
Figure 1 shows a schematic overview of the si-
multaneous translation system developed at Uni-
versit?t Karlsruhe (TH) (F?gen et al, 2006b). The
speech of the lecturer is recorded with the help
of a close-talk microphone and processed by the
speech recognition component (ASR). The par-
tial hypotheses produced by the ASR module are
collected in the resegmentation component, for
merging and re-splitting at appropriate ?seman-
tic? boundaries. The resegmented hypotheses are
then transferred to one or more machine transla-
tion components (MT), at least one per language
pair. Different output technologies may be used
for presenting the translations to the audience. For
a detailed description of the components as well
as the client-server framework used for connect-
ing the components please refer to (F?gen et al,
2006b; F?gen et al, 2006a; Kolss et al, 2006; F?-
gen and Kolss, 2007; F?gen et al, 2001).
3.2 End-to-End Evaluation
The evaluation in speech-to-speech translation
jeopardises many concepts and implies a lot of
subjectivity. Three components are involved and
an overall system may grow the difficulty of esti-
mating the output quality. However, two criteria
are mainly accepted in the community: measuring
the information preservation and determining how
much of the translation is understandable.
Several end-to-end evaluations in speech-to-
speech translation have been carried out in the last
few years, in projects such as JANUS (Gates et
al., 1996), Verbmobil (N?bel, 1997) or TC-STAR
(Hamon et al, 2007). Those projects use the
main criteria depicted above, and protocols differ
in terms of data preparation, rating, procedure, etc.
346
Dictionary
Source
Hypothesis Translatable
Segment
Model
Source Boundary
Resegmen?
tationRecognition
Speech
Translation
Model Model
Target Language
Machine
Translation
Model
Source Acoustic
Model
Source Language
Output
Translated
Translation
Vocabulary
Audio Stream
Text
Output
(Subtitles)(Synthesis)
Spoken
Figure 1: Schematic overview and information flow of the simultaneous translation system. The main
components of the system are represented by cornered boxes and the models used for theses components
by ellipses. The different output forms are represented by rounded boxes.
To our opinion, to evaluate the performance of a
complete speech-to-speech translation system, we
need to compare the source speech used as input to
the translated output speech in the target language.
To that aim, we reused a large part of the evalua-
tion protocol from the TC-STAR project(Hamon
et al, 2007).
4 Evaluation Tasks
The evaluation is carried out on the simultaneously
translated speech of a single speaker?s talks and
lectures in the field of speech processing, given in
English, and translated into Spanish.
4.1 Data used
Two data sets were selected from the talks and
lectures. Each set contained three excerpts, no
longer than 6 minutes each and focusing on dif-
ferent topics. The former set deals with speech
recognition and the latter with the descriptions of
European speech research projects, both from the
same speaker. This represents around 7,200 En-
glish words. The excerpts were manually tran-
scribed to produce the reference for the ASR eval-
uation. Then, these transcriptions were manually
translated into Spanish by two different transla-
tors. Two reference translations were thus avail-
able for the spoken language translation (SLT)
evaluation. Finally, one human interpretation was
produced from the excerpts as reference for the
end-to-end evaluation. It should be noted that for
the translation system, speech synthesis was used
to produce the spoken output.
4.2 Evaluation Protocol
The system is evaluated as a whole (black box
evaluation) and component by component (glass
box evaluation):
ASR evaluation. The ASR module is evaluated
by computing the Word Error Rate (WER) in case
insensitive mode.
SLT evaluation. For the SLT evaluation, the au-
tomatically translated text from the ASR output is
compared with two manual reference translations
by means of automatic and human metrics. Two
automatic metrics are used: BLEU (Papineni et
al., 2001) and mWER (Niessen et al, 2000).
For the human evaluation, each segment is eval-
uated in relation to adequacy and fluency (White
and O?Connell, 1994). For the evaluation of ad-
equacy, the target segment is compared to a ref-
erence segment. For the evaluation of fluency,
the quality of the language is evaluated. The two
types of evaluation are done independently, but
each evaluator did both evaluations (first that of
fluency, then that of adequacy) for a certain num-
ber of segments. For the evaluation of fluency,
evaluators had to answer the question: ?Is the text
written in good Spanish??. For the evaluation of
adequacy, evaluators had to answer the question:
?How much of the meaning expressed in the ref-
erence translation is also expressed in the target
translation??.
For both evaluations, a five-point scale is pro-
posed to the evaluators, where only extreme val-
ues are explicitly defined. Three evaluations are
carried out per segment, done by three different
evaluators, and segments are divided randomly,
because evaluators must not recreate a ?story?
347
and thus be influenced by the context. The total
number of judges was 10, with around 100 seg-
ments per judge. Furthermore, the same number
of judges was recruited for both categories: ex-
perts, from the domain with a knowledge of the
technology, and non-experts, without that knowl-
edge.
End-to-End evaluation. The End-to-End eval-
uation consists in comparing the speech in the
source language to the output speech in the tar-
get language. Two important aspects should be
taken into account when assessing the quality of
a speech-to-speech system.
First, the information preservation is measured
by using ?comprehension questionnaires?. Ques-
tions are created from the source texts (the En-
glish excerpts), then questions and answers are
translated into Spanish by professional translators.
These questions are asked to human judges after
they have listened to the output speech in the tar-
get language (Spanish). At a second stage, the an-
swers are analysed: for each answer a Spanish val-
idator gives a score according to a binary scale (the
information is either correct or incorrect). This al-
lows us to measure the information preservation.
Three types of questions are used in order to di-
versify the difficulty of the questions and test the
system at different levels: simple Factual (70%),
yes/no (20%) and list (10%) questions. For in-
stance, questions were: What is the larynx respon-
sible for?, Have all sites participating in CHIL
built a CHIL room?, Which types of knowledge
sources are used by the decoder?, respectively.
The second important aspect of a speech-to-
speech system is the quality of the speech output
(hereafter quality evaluation). For assessing the
quality of the speech output one question is asked
to the judges at the end of each comprehension
questionnaire: ?Rate the overall quality of this au-
dio sample?, and values go from 1 (?1: Very bad,
unusable?) to 5 (?It is very useful?). Both auto-
matic system and interpreter outputs were evalu-
ated with the same methodology.
Human judges are real users and native Span-
ish speakers, experts and non-experts, but different
from those of the SLT evaluation. Twenty judges
were involved (12 excerpts, 10 evaluations per ex-
cerpt and 6 evaluations per judge) and each judge
evaluated both automatic and human excerpts on a
50/50 percent basis.
5 Components Results
5.1 Automatic Speech Recognition
The ASR output has been evaluated using the
manual transcriptions of the excerpts. The overall
Word Error Rate (WER) is 11.9%. Table 1 shows
the WER level for each excerpt.
Excerpts WER [%]
L043-1 14.5
L043-2 14.5
L043-3 9.6
T036-1 11.3
T036-2 11.7
T036-3 9.2
Overall 11.9
Table 1: Evaluation results for ASR.
T036 excerpts seem to be easier to recognize au-
tomatically than L043 ones, probably due to the
more general language of the former.
5.2 Machine Translation
5.2.1 Human Evaluation
Each segment within the human evaluation is eval-
uated 4 times, each by a different judge. This aims
at having a significant number of judgments and
measuring the consistency of the human evalua-
tions. The consistency is measured by computing
the Cohen?s Kappa coefficient (Cohen, 1960).
Results show a substantial agreement for flu-
ency (kappa of 0.64) and a moderate agreement
for adequacy (0.52).The overall results of the hu-
man evaluation are presented in Table 2. Regard-
ing both experts? and non-experts? details, agree-
ment is very similar (0.30 and 0.28, respectively).
All judges Experts Non experts
Fluency 3.13 2.84 3.42
Adequacy 3.26 3.21 3.31
Table 2: Average rating of human evalua-
tions [1<5].
Both fluency and adequacy results are over the
mean. They are lower for experts than for non-
experts. This may be due to the fact that experts
are more familiar with the domain and therefore
more demanding than non experts. Regarding the
detailed evaluation per judge, scores are generally
lower for non-experts than for experts.
348
5.2.2 Automatic Evaluation
Scores are computed using case-sensitive metrics.
Table 3 shows the detailed results per excerpt.
Excerpts BLEU [%] mWER [%]
L043-1 25.62 58.46
L043-2 22.60 62.47
L043-3 28.73 62.64
T036-1 34.46 55.13
T036-2 29.41 59.91
T036-3 35.17 50.77
Overall 28.94 58.66
Table 3: Automatic Evaluation results for SLT.
Scores are rather low, with a mWER of 58.66%,
meaning that more than half of the translation is
correct. According to the scoring, the T036 ex-
cerpts seem to be easier to translate than the L043
ones, the latter being of a more technical nature.
6 End-to-End Results
6.1 Evaluators Agreement
In this study, ten judges carried out the evaluation
for each excerpt. In order to observe the inter-
judges agreement, the global Fleiss?s Kappa co-
efficient was computed, which allows to measure
the agreement between m judges with r criteria of
judgment. This coefficient shows a global agree-
ment between all the judges, which goes beyond
Cohen?s Kappa coefficient. However, a low co-
efficient requires a more detailed analysis, for in-
stance, by using Kappa for each pair of judges.
Indeed, this allows to see how deviant judges are
from the typical judge behaviour. For m judges,
n evaluations and r criteria, the global Kappa is
defined as follows:
? = 1 ?
nm2 ?
?n
i=1
?r
j=1 X
2
ij
nm(m? 1) ?rj=1 Pj(1 ? Pj)
where:
Pj =
?n
i=1 Xij
nm
and: Xij is the number of judgments for the ith
evaluation and the jth criteria.
Regarding quality evaluation (n = 6, m = 10,
r = 5), Kappa values are low for both human in-
terpreters (? = 0.07) and the automatic system
(? = 0.01), meaning that judges agree poorly
(Landis and Koch, 1977). This is explained by
the extreme subjectivity of the evaluation and the
small number of evaluated excerpts. Looking at
each pair of judges and the Kappa coefficients
themselves, there is no real agreement, since most
of the Kappa values are around zero. However,
some judge pairs show fair agreement, and some
others show moderate or substantial agreement. It
is observed, though, that some criteria are not fre-
quently selected by the judges, which limits the
statistical significance of the Kappa coefficient.
The limitations are not the same for the com-
prehension evaluation (n = 60, m = 10, r = 2),
since the criteria are binary (i.e. true or false). Re-
garding the evaluated excerpts, Kappa values are
0.28 for the automatic system and 0.30 for the in-
terpreter. According to Landis and Koch (1977),
those values mean that judges agree fairly. In
order to go further, the Kappa coefficients were
computed for each pair of judges. Results were
slightly better for the interpreter than for the au-
tomatic system. Most of them were between 0.20
and 0.40, implying a fair agreement. Some judges
agreed moderately.
Furthermore, it was also observed that for the
120 available questions, 20 had been answered
correctly by all the judges (16 for the interpreter
evaluation and 4 for the automatic system one)
and 6 had been answered wrongly by all judges (1
for the former and 5 for the latter). That shows a
trend where the interpreter comprehension would
be easier than that of the automatic system, or at
least where the judgements are less questionable.
6.2 Quality Evaluation
Table 4 compares the quality evaluation results of
the interpreter to those of the automatic system.
Samples Interpreter Automatic system
L043-1 3.1 1.6
L043-2 2.9 2.3
L043-3 2.4 2.1
T036-1 3.6 3.1
T036-2 2.7 2.5
T036-3 3.5 2.5
Mean 3.03 2.35
Table 4: Quality evaluation results for the inter-
preter and the automatic system [1<5].
As can be seen, with a mean score of 3.03 even
for the interpreter, the excerpts were difficult to
interpret and translate. This is particularly so for
349
L043, which is more technical than T036. The
L043-3 excerpt is particularly technical, with for-
mulae and algorithm descriptions, and even a com-
plex description of the human articulatory system.
In fact, L043 provides a typical presentation with
an introduction, followed by a deeper description
of the topic. This increasing complexity is re-
flected on the quality scores of the three excerpts,
going from 3.1 to 2.4.
T036 is more fluent due to the less technical na-
ture of the speech and the more general vocabu-
lary used. However, the T036-2 and T036-3 ex-
cerpts get a lower quality score, due to the descrip-
tion of data collections or institutions, and thus the
use of named entities. The interpreter does not
seem to be at ease with them and is mispronounc-
ing some of them, such as ?Grenoble? pronounced
like in English instead of in Spanish. The inter-
preter seems to be influenced by the speaker, as
can also be seen in his use of the neologism ?el ce-
nario? (?the scenario?) instead of ?el escenario?.
Likewise, ?Karlsruhe? is pronounced three times
differently, showing some inconsistency of the in-
terpreter.
The general trend in quality errors is similar to
those of previous evaluations: lengthening words
(?seeee?ales?), hesitations, pauses between syl-
lables and catching breath (?caracter?s...ticas?),
careless mistakes (?probibilidad? instead of ?prob-
abilidad?), self-correction of wrong interpreting
(?reconocien-/reconocimiento?), etc.
An important issue concerns gender and num-
ber agreement. Those errors are explained by
the presence of morphological gender in Spanish,
like in ?estos se?ales? instead of ?estas se?ales?
(?these signals?) together with the speaker?s speed
of speech. The speaker seems to start by default
with a masculine determiner (which has no gen-
der in English), adjusting the gender afterward de-
pending on the noun following. A quick transla-
tion may also be the cause for this kind of errors,
like ?del se?al acustico? (?of the acoustic signal?)
with a masculine determiner, a feminine substan-
tive and ending in a masculine adjective. Some
translation errors are also present, for instance
?computerizar? instead of ?calcular? (?compute?).
The errors made by the interpreter help to un-
derstand how difficult oral translation is. This
should be taken into account for the evaluation of
the automatic system.
The automatic system results, like those of
the interpreter, are higher for T036 than for L043.
However, scores are lower, especially for the
L043-1 excerpt. This seems to be due to the
type of lexicon used by the speaker for this ex-
cerpt, more medical, since the speaker describes
the articulatory system. Moreover, his description
is sometimes metaphorical and uses a rather col-
loquial register. Therefore, while the interpreter
finds it easier to deal with these excerpts (known
vocabulary among others) and L043-3 seems to be
more complicated (domain-specific, technical as-
pect), the automatic system finds it more compli-
cated with the former and less with the latter. In
other words, the interpreter has to ?understand?
what is said in L043-3, contrary to the automatic
system, in order to translate.
Scores are higher for the T036 excerpts. In-
deed, there is a high lexical repetition, a large
number of named entities, and the quality of the
excerpt is very training-dependant. However, the
system runs into trouble to process foreign names,
which are very often not understandable. Differ-
ences between T036-1 and the other T036 excerpts
are mainly due to the change in topic. While the
former deals with a general vocabulary (i.e. de-
scription of projects), the other two excerpts de-
scribe the data collection, the evaluation metrics,
etc., thus increasing the complexity of translation.
Generally speaking, quality scores of the au-
tomatic system are mainly due to the transla-
tion component, and to a lesser extent to the
recognition component. Many English words are
not translated (?bush?, ?keyboards?, ?squeaking?,
etc.), and word ordering is not always correct.
This is the case for the sentence ?how we solve
it?, translated into ?c?mo nos resolvers lo? instead
of ?c?mo lo resolvemos?. Funnily enough, the
problems of gender (?maravillosos aplicaciones?
- masc. vs fem.) and number (?pueden real-
mente ser aplicado? - plu. vs sing.) the in-
terpreter has, are also found for the automatic
system. Moreover, the translation of compound
nouns often shows wrong word ordering, in partic-
ular when they are long, i.e. up to three words (e.g.
?reconocimiento de habla sistemas? for ?speech
recognition system? instead of ?sistemas de re-
conocimiento de habla?).
Finally, some error combinations result in fully
non-understandable sentences, such as:
?usted tramo se en emacs es squeaking
ruido y dries todos demencial?
350
where the following errors take place:
? tramo: this translation of ?stretch? results
from the choice of a substantive instead of a
verb, giving rise to two choices due to the lex-
ical ambiguity: ?estiramiento? and ?tramo?,
which is more a linear distance than a stretch
in that context;
? se: the pronoun ?it? becomes the reflexive
?se? instead of the personal pronoun ?lo?;
? emacs: the recognition module transcribed
the couple of words ?it makes? into ?emacs?,
not translated by the translation module;
? squeaking: the word is not translated by the
translation module;
? dries: again, two successive errors are made:
the word ?drives? is transcribed into ?dries?
by the recognition module, which is then left
untranslated.
The TTS component also contributes to decreas-
ing the output quality. The prosody module finds it
hard to make the sentences sound natural. Pauses
between words are not very frequent, but they do
not sound natural (i.e. like catching breath) and
they are not placed at specific points, as it would
be done by a human. For instance, the prosody
module does not link the noun and its determiner
(e.g. ?otros aplicaciones?). Finally, a not user-
friendly aspect of the TTS component is the rep-
etition of the same words always pronounced in
the same manner, what is quite disturbing for the
listener.
6.3 Comprehension Evaluation
Tables 5 and 6 present the results of the compre-
hension evaluation, for the interpreter and for the
automatic system, respectively. They provide the
following information:
identifiers of the excerpt: Source data are the
same for the interpreter and the automatic
system, namely the English speech;
subj. E2E: The subjective results of the end-to-
end evaluation are done by the same assessors
who did the quality evaluation. This shows
the percentage of good answers;
fair E2E: The objective verification of the an-
swers. The audio files are validated to check
whether they contain the answers to the ques-
tions or not (as the questions were created
from the English source). This shows the
maximum percentage of answers an evalua-
tor managed to find from either the interpreter
(speaker audio) or the automatic system out-
put (TTS) in Spanish. For instance, informa-
tion in English could have been missed by
the interpreter because he/she felt that this in-
formation was meaningless and could be dis-
carded. We consider those results as an ob-
jective evaluation.
SLT, ASR: Verification of the answers in each
component of the end-to-end process. In or-
der to determine where the information for
the automatic system is lost, files from each
component (recognised files for ASR, trans-
lated files for SLT, and synthesised files for
TTS in the ?fair E2E? column) are checked.
Excerpts subj. E2E fair E2E
L043-1 69 90
L043-2 75 80
L043-3 72 60
T036-1 80 100
T036-2 73 80
T036-3 76 100
Mean 74 85
Table 5: Comprehension evaluation results for the
interpreter [%].
Regarding Table 5, the interpreter loses 15%
of the information (i.e. 15% of the answers were
incorrect or not present in the interpreter?s trans-
lation) and judges correctly answered 74% of the
questions. Five documents get above 80% of cor-
rect results, while judges find almost above 70%
of the answers for the six documents.
Regarding the automatic system results (Table
6), the information rate found by judges is just
above 50% since, by extension, more than half the
questions were correctly answered. The lowest
excerpt, L043-1, gets a rate of 25%, the highest,
T036-1, a rate of 76%, which is in agreement with
the observation for the quality evaluation. Infor-
mation loss can be found in each component, es-
pecially for the SLT module (35% of the informa-
tion is lost here). It should be noticed that the TTS
module made also errors which prevented judges
351
Excerpts subj. E2E fair E2E SLT ASR
L043-1 25 30 30 70
L043-2 62 70 80 70
L043-3 43 40 60 100
T036-1 76 80 90 100
T036-2 61 70 60 80
T036-3 47 60 70 80
Mean 52 58 65 83
Table 6: Comprehension evaluation results for the
automatic system [%].
from answering related questions. Moreover, the
ASR module loses 17% of the information. Those
results are certainly due to the specific vocabulary
used in this experiment.
So as to objectively compare the interpreter with
the automatic system, we selected the questions
for which the answers were included in the inter-
preter files (i.e. those in the ?fair E2E? column
of Table 5). The goal was to compare the overall
quality of the speech-to-speech translation to in-
terpreters? quality, without the noise factor of the
information missing. The assumption is that the
interpreter translates the ?important information?
and skips the useless parts of the original speech.
This experiment is to measure the level of this in-
formation that is preserved by the automatic sys-
tem. So a new subset of results was obtained, on
the information kept by the interpreter. The same
study was repeated for the three components and
the results are shown in Tables 7 and 8.
Excerpts subj. E2E fair E2E SLT ASR
L043-1 27 33 33 78
L043-2 65 75 88 75
L043-3 37 67 83 100
T036-1 76 80 90 100
T036-2 69 88 75 100
T036-3 47 60 70 80
Mean 53 60 70 80
Table 7: Evaluation results for the automatic sys-
tem restricted to the questions for which answers
can be found in the interpreter speech [%].
Comparing the automatic system to the inter-
preter, the automatic system keeps 40% of the in-
formation where the interpreter translates the doc-
uments correctly. Those results confirm that ASR
loses a lot of information (20%), while SLT loses
10% further, and so does the TTS. Judges are quite
close to the objective validation and found most of
the answers they could possibly do.
Excerpts subj. E2E
L043-1 66
L043-2 90
L043-3 88
T036-1 80
T036-2 81
T036-3 76
Mean 80
Table 8: Evaluation results for interpreter, re-
stricted to the questions for which answers can be
found in the interpreter speech [%].
Subjective results for the restricted evaluation
are similar to the previous results, on the full data
(80% vs 74% of the information found by the
judges). Performance is good for the interpreter:
98% of the information correctly translated by the
automatic system is also correctly interpreted by
the human. Although we can not compare the
performance of the restricted automatic system to
that of the restricted interpreter (since data sets of
questions are different), it seems that of the inter-
preter is better. However, the loss due to subjective
evaluation seems to be higher for the interpreter
than for the automatic system.
7 Conclusions
Regarding the SLT evaluation, the results achieved
with the simultaneous translation system are still
rather low compared to the results achieved with
offline systems for translating European parlia-
ment speeches in TC-STAR. However, the offline
systems had almost no latency constraints, and
parliament speeches are much easier to recognize
and translate when compared to the more spon-
taneous talks and lectures focused in this paper.
This clearly shows the difficulty of the whole task.
However, the human end-to-end evaluation of the
system in which the system is compared with hu-
man interpretation shows that the current transla-
tion quality allows for understanding of at least
half of the content, and therefore, may be already
quite helpful for people not understanding the lan-
guage of the lecturer at all.
352
References
Rajai Al-Khanji, Said El-Shiyab, and Riyadh Hussein.
2000. On the Use of Compensatory Strategies in Si-
multaneous Interpretation. Meta : Journal des tra-
ducteurs, 45(3):544?557.
Jacob Cohen. 1960. A coefficient of agreement for
nominal scales. In Educational and Psychological
Measurement, volume 20, pages 37?46.
Christian F?gen and Muntsin Kolss. 2007. The influ-
ence of utterance chunking on machine translation
performance. In Proc. of the European Conference
on Speech Communication and Technology (INTER-
SPEECH), Antwerp, Belgium, August. ISCA.
Christian F?gen, Martin Westphal, Mike Schneider,
Tanja Schultz, and Alex Waibel. 2001. LingWear:
A Mobile Tourist Information System. In Proc. of
the Human Language Technology Conf. (HLT), San
Diego, California, March. NIST.
Christian F?gen, Shajith Ikbal, Florian Kraft, Kenichi
Kumatani, Kornel Laskowski, John W. McDonough,
Mari Ostendorf, Sebastian St?ker, and Matthias
W?lfel. 2006a. The isl rt-06s speech-to-text system.
In Steve Renals, Samy Bengio, and Jonathan Fiskus,
editors, Machine Learning for Multimodal Interac-
tion: Third International Workshop, MLMI 2006,
Bethesda, MD, USA, volume 4299 of Lecture Notes
in Computer Science, pages 407?418. Springer Ver-
lag Berlin/ Heidelberg.
Christian F?gen, Muntsin Kolss, Matthias Paulik, and
Alex Waibel. 2006b. Open Domain Speech Trans-
lation: From Seminars and Speeches to Lectures.
In TC-Star Speech to Speech Translation Workshop,
Barcelona, Spain, June.
Donna Gates, Alon Lavie, Lori Levin, Alex. Waibel,
Marsal Gavalda, Laura Mayfield, and Monika Wosz-
cyna. 1996. End-to-end evaluation in janus: A
speech-to-speech translation system. In Proceed-
ings of the 6th ECAI, Budapest.
Olivier Hamon, Djamel Mostefa, and Khalid Choukri.
2007. End-to-end evaluation of a speech-to-speech
translation system in tc-star. In Proceedings of the
MT Summit XI, Copenhagen, Denmark, September.
Muntsin Kolss, Bing Zhao, Stephan Vogel, Ashish
Venugopal, and Ying Zhang. 2006. The ISL Statis-
tical Machine Translation System for the TC-STAR
Spring 2006 Evaluations. In TC-Star Workshop
on Speech-to-Speech Translation, Barcelona, Spain,
December.
Andrzej Kopczynski, 1994. Bridging the Gap: Empiri-
cal Research in Simultaneous Interpretation, chapter
Quality in Conference Interpreting: Some Pragmatic
Problems, pages 87?100. John Benjamins, Amster-
dam/ Philadelphia.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
In Biometrics, Vol. 33, No. 1 (Mar., 1977), pp. 159-
174.
Barbara Moser-Mercer, Alexander Kunzli, and Ma-
rina Korac. 1998. Prolonged turns in interpreting:
Effects on quality, physiological and psychological
stress (pilot study). Interpreting: International jour-
nal of research and practice in interpreting, 3(1):47?
64.
Sonja Niessen, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for mt research.
In Proceedings of the 2nd International Conference
on Language Resources and Evaluation, Athens,
Greece.
Rita N?bel. 1997. End-to-end Evaluation in Verb-
mobil I. In Proceedings of the MT Summit VI, San
Diego.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical Report
RC22176 (W0109-022), Research Report, Com-
puter Science IBM Research Division, T.J.Watson
Research Center.
Accipio Consulting Volker Steinbiss. 2006.
Sprachtechnologien f?r Europa. www.tc-star.
org/pubblicazioni/D17_HLT_DE.pdf.
John S. White and Theresa A. O?Connell. 1994.
Evaluation in the arpa machine translation program:
1993 methodology. In HLT ?94: Proceedings of the
workshop on Human Language Technology, pages
135?140, Morristown, NJ, USA. Association for
Computational Linguistics.
Sane M. Yagi. 2000. Studying Style in Simultane-
ous Interpretation. Meta : Journal des traducteurs,
45(3):520?547.
353
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 333?338,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Document-level Automatic MT Evaluation
based on Discourse Representations
Jesu?s Gime?nez and
Llu??s Ma`rquez
TALP UPC
Barcelona, Spain
{jgimenez, lluism}
@lsi.upc.edu
Elisabet Comelles and
Irene Castello?n
Universitat de Barcelona
Barcelona, Spain
{elicomelles,
icastellon} @ub.edu
Victoria Arranz
ELDA/ELRA
Paris, France
arranz@elda.org
Abstract
This paper describes the joint submission of
Universitat Polite`cnica de Catalunya and Uni-
versitat de Barcelona to the Metrics MaTr
2010 evaluation challenge, in collaboration with
ELDA/ELRA. Our work is aimed at widening the
scope of current automatic evaluation measures
from sentence to document level. Preliminary ex-
periments, based on an extension of the metrics by
Gime?nez and Ma`rquez (2009) operating over dis-
course representations, are presented.
1 Introduction
Current automatic similarity measures for Ma-
chine Translation (MT) evaluation operate all,
without exception, at the segment level. Trans-
lations are analyzed on a segment-by-segment1
fashion, ignoring the text structure. Document
and system scores are obtained using aggregate
statistics over individual segments. This strategy
presents the main disadvantage of ignoring cross-
sentential/discursive phenomena.
In this work we suggest widening the scope
of evaluation methods. We have defined genuine
document-level measures which are able to ex-
ploit the structure of text to provide more informed
evaluation scores. For that purpose we take advan-
tage of two coincidental facts. First, test beds em-
ployed in recent MT evaluation campaigns include
a document structure grouping sentences related
to the same event, story or topic (Przybocki et al,
2008; Przybocki et al, 2009; Callison-Burch et al,
2009). Second, we count on automatic linguistic
processors which provide very detailed discourse-
level representations of text (Curran et al, 2007).
Discourse representations allow us to focus on
relevant pieces of information, such as the agent
1A segment typically consists of one or two sentences.
(who), location (where), time (when), and theme
(what), which may be spread all over the text.
Counting on a means of discerning the events, the
individuals taking part in each of them, and their
role, is crucial to determine the semantic equiva-
lence between a reference document and a candi-
date translation.
Moreover, the discourse analysis of a document
is not a mere concatenation of the analyses of its
individual sentences. There are some phenom-
ena which may go beyond the scope of a sen-
tence and can only be explained within the con-
text of the whole document. For instance, in a
newspaper article, facts and entities are progres-
sively added to the discourse and then referred
to anaphorically later on. The following extract
from the development set illustrates the impor-
tance of such a phenomenon in the discourse anal-
ysis: ?Among the current or underlying crises in
the Middle East, Rod Larsen mentioned the Arab-
Israeli conflict and the Iranian nuclear portfolio,
as well as the crisis between Lebanon and Syria.
He stated: ?All this leads us back to crucial val-
ues and opinions, which render the situation prone
at any moment to getting out of control, more so
than it was in past days.??. The subject pronoun
?he? works as an anaphoric pronoun whose an-
tecedent is the proper noun ?Rod Larson?. The
anaphoric relation established between these two
elements can only be identified by analyzing the
text as a whole, thus considering the gender agree-
ment between the third person singular masculine
subject pronoun ?he? and the masculine proper
noun ?Rod Larson?. However, if the two sen-
tences were analyzed separately, the identification
of this anaphoric relation would not be feasible
due to the lack of connection between the two ele-
ments. Discourse representations allow us to trace
links across sentences between the different facts
and entities appearing in them. Therefore, provid-
ing an approach to the text more similar to that of
333
a human, which implies taking into account the
whole text structure instead of considering each
sentence separately.
The rest of the paper is organized as follows.
Section 2 describes our evaluation methods and
the linguistic theory upon which they are based.
Experimental results are reported and discussed in
Section 3. Section 4 presents the metric submitted
to the evaluation challenge. Future work is out-
lined in Section 5.
As an additional result, document-level metrics
generated in this study have been incorporated to
the IQMT package for automatic MT evaluation2.
2 Metric Description
This section provides a brief description of our ap-
proach. First, in Section 2.1, we describe the un-
derlying theory and give examples on its capabili-
ties. Then, in Section 2.2, we describe the associ-
ated similarity measures.
2.1 Discourse Representations
As previously mentioned in Section 1, a document
has some features which need to be analyzed con-
sidering it as a whole instead of dividing it up
into sentences. The anaphoric relation between
a subject pronoun and a proper noun has already
been exemplified. However, this is not the only
anaphoric relation which can be found inside a
text, there are some others which are worth men-
tioning:
? the connection between a possessive adjec-
tive and a proper noun or a subject pro-
noun, as exemplified in the sentences ?Maria
bought a new sweater. Her new sweater is
blue.?, where the possessive feminine adjec-
tive ?her? refers to the proper noun ?Maria?.
? the link between a demonstrative pronoun
and its referent, which is exemplified in the
sentences ?He developed a new theory on
grammar. However, this is not the only the-
ory he developed?. In the second sentence,
the demonstrative pronoun ?this? refers back
to the noun phrase ?new theory on grammar?
which occurs in the previous sentence.
? the relation between a main verb and an aux-
iliary verb in certain contexts, as illustrated in
the following pair of sentences ?Would you
2http://www.lsi.upc.edu/
?
nlp/IQMT
like more sugar? Yes, I would?. In this ex-
ample, the auxiliary verb ?would? used in
the short answer substitutes the verb phrase
?would like?.
In addition to anaphoric relations, other features
need to be highlighted, such as the use of discourse
markers which help to give cohesion to the text,
link parts of a discourse and show the relations es-
tablished between them. Below, some examples
are given:
? ?Moreover?, ?Furthermore?, ?In addition?
indicate that the upcoming sentence adds
more information.
? ?However?, ?Nonetheless?, ?Nevertheless?
show contrast with previous ideas.
? ?Therefore?, ?As a result?, ?Consequently?
show a cause and effect relation.
? ?For instance?, ?For example? clarify or il-
lustrate the previous idea.
It is worth noticing that anaphora, as well as dis-
course markers, are key features in the interface
between syntax, semantics and pragmatics. Thus,
when dealing with these phenomena at a text level
we are not just looking separately at the different
language levels, but we are trying to give a com-
plete representation of both the surface and the
deep structures of a text.
2.2 Definition of Similarity Measures
In this work, as a first proposal, instead of elabo-
rating on novel similarity measures, we have bor-
rowed and extended the Discourse Representation
(DR) metrics defined by Gime?nez and Ma`rquez
(2009). These metrics analyze similarities be-
tween automatic and reference translations by
comparing their respective discourse representa-
tions over individual sentences.
For the discursive analysis of texts, DR met-
rics rely on the C&C Tools (Curran et al, 2007),
specifically on the Boxer component (Bos, 2008).
This software is based on the Discourse Represen-
tation Theory (DRT) by Kamp and Reyle (1993).
DRT is a theoretical framework offering a rep-
resentation language for the examination of con-
textually dependent meaning in discourse. A dis-
course is represented in a discourse representation
structure (DRS), which is essentially a variation of
first-order predicate calculus ?its forms are pairs
334
of first-order formulae and the free variables that
occur in them.
DRSs are viewed as semantic trees, built
through the application of two types of DRS con-
ditions:
basic conditions: one-place properties (pred-
icates), two-place properties (relations),
named entities, time-expressions, cardinal
expressions and equalities.
complex conditions: disjunction, implication,
negation, question, and propositional attitude
operations.
For instance, the DRS representation for the
sentence ?Every man loves Mary.? is as follows:
?y named(y,mary, per) ? (?x man(x) ?
?z love(z) ? event(z) ? agent(z, x) ?
patient(z, y)). DR integrates three different
kinds of metrics:
DR-STM These metrics are similar to the Syntac-
tic Tree Matching metric defined by Liu and
Gildea (2005), in this case applied to DRSs
instead of constituent trees. All semantic sub-
paths in the candidate and reference trees are
retrieved. The fraction of matching subpaths
of a given length (l=4 in our experiments) is
computed.
DR-Or(?) Average lexical overlap between dis-
course representation structures of the same
type. Overlap is measured according to the
formulae and definitions by Gime?nez and
Ma`rquez (2007).
DR-Orp(?) Average morphosyntactic overlap,
i.e., between grammatical categories ?parts-
of-speech? associated to lexical items, be-
tween discourse representation structures of
the same type.
We have extended these metrics to operate at
document level. For that purpose, instead of run-
ning the C&C Tools in a sentence-by-sentence
fashion, we run them document by document.
This is as simple as introducing a ?<META>? tag
at the beginning of each document to denote doc-
ument boundaries3 .
3Details on the advanced use of Boxer are avail-
able at http://svn.ask.it.usyd.edu.au/trac/
candc/wiki/BoxerComplex.
3 Experimental Work
In this section, we analyze the behavior of the new
DR metrics operating at document level with re-
spect to their sentence-level counterparts.
3.1 Settings
We have used the ?mt06? part of the development
set provided by the Metrics MaTr 2010 organiza-
tion, which corresponds to a subset of 25 docu-
ments from the NIST 2006 Open MT Evaluation
Campaign Arabic-to-English translation. The to-
tal number of segments is 249. The average num-
ber of segments per document is, thus, 9.96. The
number of segments per document varies between
2 and 30. For the purpose of automatic evaluation,
4 human reference translations and automatic out-
puts by 8 different MT systems are available. In
addition, we count on the results of a process of
manual evaluation. Each translation segment was
assessed by two judges. After independently and
completely assessing the entire set, the judges re-
viewed their individual assessments together and
settled on a single final score. Average system ad-
equacy is 5.38.
In our experiments, metrics are evaluated in
terms of their correlation with human assess-
ments. We have computed Pearson, Spearman
and Kendall correlation coefficients between met-
ric scores and adequacy assessments. Document-
level and system-level assessments have been ob-
tained by averaging over segment-level assess-
ments. We have computed correlation coefficients
and confidence intervals applying bootstrap re-
sampling at a 99% statistical significance (Efron
and Tibshirani, 1986; Koehn, 2004). Since the
cost of exhaustive resampling was prohibitive, we
have limited to 1,000 resamplings. Confidence in-
tervals, not shown in the tables, are in all cases
lower than 10?3.
3.2 Metric Performance
Table 1 shows correlation coefficients at the docu-
ment level for several DR metric representatives,
and their document-level counterparts (DRdoc).
For the sake of comparison, the performance of
the METEOR metric is also reported4.
Contrary to our expectations, DRdoc variants
obtain lower levels of correlation than their DR
4We have used METEOR version 1.0 with default param-
eters optimized by its developers over adequacy and fluency
assessments. The METEOR metric is publicly available at
http://www.cs.cmu.edu/
?
alavie/METEOR/
335
Metric Pearson? Spearman? Kendall?
METEOR 0.9182 0.8478 0.6728
DR-Or(?) 0.8567 0.8061 0.6193
DR-Orp(?) 0.8286 0.7790 0.5875
DR-STM 0.7880 0.7468 0.5554
DRdoc-Or(?) 0.7936 0.7784 0.5875
DRdoc-Orp(?) 0.7219 0.6737 0.4929
DRdoc-STM 0.7553 0.7421 0.5458
Table 1: Meta-evaluation results at document level
Metric Pearson? Spearman? Kendall?
METEOR 0.9669 0.9151 0.8533
DR-Or(?) 0.9100 0.6549 0.5764
DR-Orp(?) 0.9471 0.7918 0.7261
DR-STM 0.9295 0.7676 0.7165
DRdoc-Or(?) 0.9534 0.8434 0.7828
DRdoc-Orp(?) 0.9595 0.9101 0.8518
DRdoc-STM 0.9676 0.9655 0.9272
DR-Or(?)? 0.9836 0.9594 0.9296
DR-Orp(?)? 0.9959 1.0000 1.0000
DR-STM? 0.9933 0.9634 0.9307
Table 2: Meta-evaluation results at system level
counterparts. There are three different factors
which could provide a possible explanation for
this negative result. First, the C&C Tools, like any
other automatic linguistic processor are not per-
fect. Parsing errors could be causing the metric
to confer less informed scores. This is especially
relevant taking into account that candidate transla-
tions are not always well-formed. Secondly, we
argue that the way in which we have obtained
document-level quality assessments, as an average
of segment-level assessments, may be biasing the
correlation. Thirdly, perhaps the similarity mea-
sures employed are not able to take advantage of
the document-level features provided by the dis-
course analysis. In the following subsection we
show some error analysis we have conducted by
inspecting particular cases.
Table 2 shows correlation coefficients at system
level. In the case of DR and DRdoc metrics, sys-
tem scores are computed by simple average over
individual documents. Interestingly, in this case
DRdoc variants seem to obtain higher correlation
than their DR counterparts. The improvement is
especially substantial in terms of Spearman and
Kendall coefficients, which do not consider ab-
solute values but ranking positions. However, it
could be the case that it was just an average ef-
fect. While DR metrics compute system scores as
an average of segment scores, DRdoc metrics av-
erage directly document scores. In order to clarify
this result, we have modified DR metrics so as to
compute system scores as an average of document
scores (DR? variants, the last three rows in the ta-
ble). It can be observed that DR? variants out-
perform their DRdoc counterparts, thus confirming
our suspicion about the averaging effect.
3.3 Analysis
It is worth noting that DRdoc metrics are able to
detect and deal with several linguistic phenomena
related to both syntax and semantics at sentence
and document level. Below, several examples il-
lustrating the potential of this metric are presented.
Control structures. Control structures (either
subject or object control) are always a
difficult issue as they mix both syntactic and
semantic knowledge. In Example 1 a couple
of control structures must be identified
and DRdoc metrics deal correctly with the
argument structure of all the verbs involved.
Thus, in the first part of the sentence, a
subject control verb can be identified being
?the minister? the agent of both verb forms
?go? and ?say?. On the other hand, in the
336
quoted question, the verb ?invite? works as
an object control verb because its patient
?Chechen representatives? is also the agent
of the verb visit.
Example 1: The minister went on to say,
?What would Moscow say if we were to invite
Chechen representatives to visit Jerusalem??
Anaphora and pronoun resolution. Whenever
there is a pronoun whose antecedent is a
named entity (NE), the metric identifies
correctly its antecedent. This feature is
highly valuable because a relationship be-
tween syntax and semantics is established.
Moreover, when dealing with Semantic
Roles the roles of Agent or Patient are given
to the antecedents instead of the pronouns.
Thus, in Example 2 the antecedent of the
relative pronoun ?who? is the NE ?Putin?
and the patient of the verb ?classified? is
also the NE ?Putin? instead of the relative
pronoun ?who?.
Example 2: Putin, who was not classified
as his country Hamas as ?terrorist organiza-
tions?, recently said that the European Union
is ?a big mistake? if it decided to suspend fi-
nancial aid to the Palestinians.
Nevertheless, although Boxer was expected
to deal with long-distance anaphoric relations
beyond the sentence, after analyzing several
cases, results show that it did not succeed in
capturing this type of relations as shown in
Example 3. In this example, the antecedent
of the pronoun ?he? in the second sentence
is the NE ?Roberto Calderoli? which ap-
pears in the first sentence. DRdoc metrics
should be capable of showing this connec-
tion. However, although the proper noun
?Roberto Calderoli? is identified as a NE, it
does not share the same reference as the third
person singular pronoun ?he?.
Example 3: Roberto Calderoli does not in-
tend to apologize. The newspaper Corriere
Della Sera reported today, Saturday, that
he said ?I don?t feel responsible for those
deaths.?
4 Our Submission
Instead of participating with individual metrics,
we have combined them by averaging their scores
as described in (Gime?nez and Ma`rquez, 2008).
This strategy has proven as an effective means of
combining the scores conferred by different met-
rics (Callison-Burch et al, 2008; Callison-Burch
et al, 2009). Metrics submitted are:
DRdoc an arithmetic mean over a heuristically-
defined set of DRdoc metric variants, respec-
tively computing lexical overlap, morphosyn-
tactic overlap, and semantic tree match-
ing (M = {?DRdoc-Or(?)?, ?DRdoc-Orp(?)?, ?DRdoc-
STM4?}). Since DRdoc metrics do not operate
over individual segments, we have assigned
each segment the score of the document in
which it is contained.
DR a measure analog to DRdoc but using the de-
fault version of DR metrics operating at the
segment level (M = {?DR-Or(?)?, ?DR-Orp(?)?,
?DR-STM4?}).
ULCh an arithmetic mean over a heuristically-
defined set of metrics operating at differ-
ent linguistic levels, including lexical met-
rics, and measures of overlap between con-
stituent parses, dependency parses, seman-
tic roles, and discourse representations (M =
{?ROUGEW ?, ?METEOR?, ?DP-HWCr?, ?DP-Oc(?)?,
?DP-Ol(?)?, ?DP-Or(?)?, ?CP-STM4?, ?SR-Or(?)?,
?SR-Orv?, ?DR-Orp(?)?}). This metric corre-
sponds exactly to the metric submitted in our
previous participation.
The performance of these metrics at the docu-
ment and system levels is shown in Table 3.
5 Conclusions and Future Work
We have presented a modified version of the DR
metrics by Gime?nez and Ma`rquez (2009) which,
instead of limiting their scope to the segment level,
are able to capture and exploit document-level fea-
tures. However, results in terms of correlation
with human assessments have not reported any im-
provement of these metrics over their sentence-
level counterparts as document and system quality
predictors. It must be clarified whether the prob-
lem is on the side of the linguistic tools, in the
similarity measure, or in the way in which we have
built document-level human assessments.
For future work, we plan to continue the er-
ror analysis to clarify why DRdoc metrics do not
outperform their DR counterparts at the document
level, and how to improve their behavior. This
337
Document level System level
Metric Pearson? Spearman? Kendall? Pearson? Spearman? Kendall?
ULCDR 0.8418 0.8066 0.6135 0.9349 0.7936 0.7145
ULCDRdoc 0.7739 0.7358 0.5474 0.9655 0.9062 0.8435
ULCh 0.8963 0.8614 0.6848 0.9842 0.9088 0.8638
Table 3: Meta-evaluation results at document and system level for submitted metrics
may imply defining new metrics possibly using
alternative linguistic processors. In addition, we
plan to work on the identification and analysis
of discourse markers. Finally, we plan to repeat
this experiment over other test beds with docu-
ment structure, such as those from the 2009 Work-
shop on Statistical Machine Translation shared
task (Callison-Burch et al, 2009) and the 2009
NIST MT Evaluation Campaign (Przybocki et al,
2009). In the case that document-level assess-
ments are not provided, we will also explore the
possibility of producing them ourselves.
Acknowledgments
This work has been partially funded by the
Spanish Government (projects OpenMT-2,
TIN2009-14675-C03, and KNOW, TIN-2009-
14715-C0403) and the European Community?s
Seventh Framework Programme (FP7/2007-2013)
under grant agreement numbers 247762 (FAUST
project, FP7-ICT-2009-4-247762) and 247914
(MOLTO project, FP7-ICT-2009-4-247914). We
are also thankful to anonymous reviewers for their
comments and suggestions.
References
Johan Bos. 2008. Wide-coverage semantic analy-
sis with boxer. In Johan Bos and Rodolfo Del-
monte, editors, Semantics in Text Processing. STEP
2008 Conference Proceedings, Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation, pages 70?106.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale nlp with c&c
and boxer. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 33?36.
Bradley Efron and Robert Tibshirani. 1986. Bootstrap
Methods for Standard Errors, Confidence Intervals,
and Other Measures of Statistical Accuracy. Statis-
tical Science, 1(1):54?77.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguis-
tic Features for Automatic Evaluation of Heteroge-
neous MT Systems. In Proceedings of the ACL
Workshop on Statistical Machine Translation, pages
256?264.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. A Smor-
gasbord of Features for Automatic MT Evaluation.
In Proceedings of the Third Workshop on Statistical
Machine Translation, pages 195?198.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2009. On the Ro-
bustness of Syntactic and Semantic Features for Au-
tomatic MT Evaluation. In Proceedings of the 4th
Workshop on Statistical Machine Translation (EACL
2009).
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic: An Introduction to Modeltheoretic Semantics
of Natural Language, Formal Logic and Discourse
Representation Theory. Dordrecht: Kluwer.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 388?395.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization,
pages 25?32.
Mark Przybocki, Kay Peterson, and Se?bastien Bron-
sart. 2008. NIST Metrics for Machine Translation
2008 Evaluation (MetricsMATR08). Technical re-
port, National Institute of Standards and Technol-
ogy.
Mark Przybocki, Kay Peterson, and Se?bastien Bron-
sart. 2009. NIST Open Machine Translation 2009
Evaluation (MT09). Technical report, National In-
stitute of Standards and Technology.
338
