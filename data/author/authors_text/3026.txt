Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 793?800
Manchester, August 2008
Metric Learning for Synonym Acquisition
Nobuyuki Shimizu
Information Technology Center
University of Tokyo
shimizu@r.dl.itc.u-tokyo.ac.jp
Masato Hagiwara
Graduate School of Information Science
Nagoya University
hagiwara@kl.i.is.nagoya-u.ac.jp
Yasuhiro Ogawa and Katsuhiko Toyama
Graduate School of Information Science
Nagoya University
{yasuhiro,toyama}@kl.i.is.nagoya-u.ac.jp
Hiroshi Nakagawa
Information Technology Center
University of Tokyo
n3@dl.itc.u-tokyo.ac.jp
Abstract
The distance or similarity metric plays an
important role in many natural language
processing (NLP) tasks. Previous stud-
ies have demonstrated the effectiveness of
a number of metrics such as the Jaccard
coefficient, especially in synonym acqui-
sition. While the existing metrics per-
form quite well, to further improve perfor-
mance, we propose the use of a supervised
machine learning algorithm that fine-tunes
them. Given the known instances of sim-
ilar or dissimilar words, we estimated the
parameters of the Mahalanobis distance.
We compared a number of metrics in our
experiments, and the results show that the
proposed metric has a higher mean average
precision than other metrics.
1 Introduction
Accurately estimating the semantic distance be-
tween words in context has applications for
machine translation, information retrieval (IR),
speech recognition, and text categorization (Bu-
danitsky and Hirst, 2006), and it is becoming
clear that a combination of corpus statistics can be
used with a dictionary, thesaurus, or other knowl-
edge source such as WordNet or Wikipedia, to in-
crease the accuracy of semantic distance estima-
tion (Mohammad and Hirst, 2006). Although com-
piling such resources is labor intensive and achiev-
ing wide coverage is difficult, these resources to
some extent explicitly capture semantic structures
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
of concepts and words. In contrast, corpus statis-
tics achieve wide coverage, but the semantic struc-
ture of a concept is only implicitly represented in
the context. Assuming that two words are semanti-
cally closer if they occur in similar contexts, statis-
tics on the contexts of words can be gathered and
compared for similarity, by using a metric such as
the Jaccard coefficient.
Our proposal is to extend and fine-tune the latter
approach with the training data obtained from the
former. We apply metric learning to this task. Al-
though still in their infancy, distance metric learn-
ing methods have undergone rapid development in
the field of machine learning. In a setting simi-
lar to semi-supervised clustering, where known in-
stances of similar or dissimilar objects are given,
a metric such as the Mahalanobis distance can be
learned from a few data points and tailored to fit a
particular purpose. Although classification meth-
ods such as logistic regression now play impor-
tant roles in natural language processing, the use
of metric learning has yet to be explored.
Since popular current methods for synonym ac-
quisition require no statistical learning, it seems
that supervised machine learning should easily
outperform them. Unfortunately, there are obsta-
cles to overcome. Since metric learning algorithms
usually learn the parameters of a Mahalanobis dis-
tance, the number of parameters is quadratic to the
number of features. They learn how two features
should interact to produce the final metric. While
traditional metrics forgo examining of the interac-
tions entirely, in applying metrics such as Jaccard
coefficient, it is not uncommon nowadays to use
more than 10,000 features, a number that a typical
metric learner is incapable of processing. Thus we
have two options: one is to find the most impor-
tant features and model the interactions between
793
them, and the other is simply to use a large number
of features. We experimentally examined the two
options and found that metric learning is useful in
synonym acquisition, despite it utilizing fewer fea-
tures than traditional methods.
The remainder of this paper is organized as fol-
lows: in section 2, we review prior work on syn-
onym acquisition and metric learning. In section
3, we introduce the Mahalanobis distance metric
and a learning algorithm based on this metric. In
section 4 and 5, we explain the experimental set-
tings and propose the use of normalization to make
the Mahalanobis distances work in practice, and
then in section 6, we discuss issues we encountered
when applying this metric to synonym acquisition.
We conclude in section 7.
2 Prior Work
As this paper is based on two different lines of re-
search, we first review the work in synonym acqui-
sition, and then review the work in generic metric
learning. To the best of the authors? knowledge,
none of the metric learning algorithms have been
applied to automatic synonym acquisition.
Synonym relation is important lexical knowl-
edge for many natural language processing
tasks including automatic thesaurus construction
(Croach and Yang, 1992; Grefenstette, 1994) and
IR (Jing and Croft, 1994). Various methods (Hin-
dle, 1990; Lin, 1998) of automatically acquiring
synonyms have been proposed. They are usu-
ally based on the distributional hypothesis (Har-
ris, 1985), which states that semantically simi-
lar words share similar contexts, and they can be
roughly viewed as the combinations of two steps:
context extraction and similarity calculation. The
former extracts useful features from the contexts of
words, such as surrounding words or dependency
structure. The latter calculates how semantically
similar two given words are based on similarity or
distance metrics.
Many studies (Lee, 1999; Curran and Moens,
2002; Weeds et al, 2004) have investigated
similarity calculation, and a variety of dis-
tance/similarity measures have already been com-
pared and discussed. Weeds et al?s work is espe-
cially useful because it investigated the character-
istics of metrics based on a few criteria such as
the relative frequency of acquired synonyms and
clarified the correlation between word frequency,
distributional generality, and semantic generality.
However, all of the existing research conducted
only a posteriori comparison, and as Weeds et al
pointed out, there is no one best measure for all ap-
plications. Therefore, the metrics must be tailored
to applications, even to corpora and other settings.
We next review the prior work in generic metric
learning. Most previous metric learning methods
learn the parameters of the Mahalanobis distance.
Although the algorithms proposed in earlier work
(Xing et al, 2002; Weinberger et al, 2005; Glober-
son and Roweis, 2005) were shown to yield excel-
lent classification performance, these algorithms
all have worse than cubic computational complex-
ity in the dimensionality of the data. Because of
the high dimensionality of our objects, we opted
for information-theoretic metric learning proposed
by (Davis et al, 2007). This algorithm only uses
an operation quadratic in the dimensionality of the
data.
Other work on learning Mahalanobis metrics in-
cludes online metric learning (Shalev-Shwartz et
al., 2004), locally-adaptive discriminative methods
(Hastie and Tibshirani, 1996), and learning from
relative comparisons (Schutz and Joahims, 2003).
Non-Mahalanobis-based metric learning methods
have also been proposed, though they seem to suf-
fer from suboptimal performance, non-convexity,
or computational complexity. Examples include
neighborhood component analysis (Goldberger et
al., 2004).
3 Metric Learning
3.1 Problem Formulation
To set the context for metric learning, we first de-
scribe the objects whose distances from one an-
other we would like to know. As noted above re-
garding the distributional hypothesis, our object is
the context of a target word. To represent the con-
text, we use a sparse vector in Rd. Each dimension
of an input vector represents a feature of the con-
text, and its value corresponds to the strength of
the association. The vectors of two target words
represent their contexts as points in multidimen-
sional feature-space. A suitable metric (for exam-
ple, Euclidean) defines the distance between the
two points, thereby estimating the semantic dis-
tance between the target words.
Given points x
i
, x
j
? R
d
, the (squared) Ma-
halanobis distance between them is parameter-
ized by a positive definite matrix A as follows
d
A
(x
i
, x
j
) = (x
i
? x
j
)
?
A(x
i
? x
j
). The Ma-
794
halanobis distance is a straightforward extension
of the standard Euclidean distance. If we let A
be the identity matrix, the Mahalanobis distance
reduces to the Euclidean distance. Our objective
is to obtain the positive definite matrix A that pa-
rameterizes the Mahalanobis distance, so that the
distance between the vectors of two synonymous
words is small, and the distance between the vec-
tors of two dissimilar words is large. Stated more
formally, the Mahalanobis distance between two
similar points must be smaller than a given upper
bound, i.e., d
A
(x
i
, x
j
) ? u for a relatively small
value of u. Similarly, two points are dissimilar if
d
A
(x
i
, x
j
) ? l for sufficiently large l.
As we discuss below, we were able to use
the Euclidean distance to acquire synonyms quite
well. Therefore, we would like the positive definite
matrix A of the Mahalanobis distance to be close to
the identity matrix I . This keeps the Mahalanobis
distance similar to the Euclidean distance, which
would help to prevent overfitting the data. To op-
timize the matrix, we follow the information theo-
retic metric learning approach described in (Davis
et al, 2007). We summarize the problem formula-
tion advocated by this approach in this section and
the learning algorithm in the next section.
To define the closeness between A and I , we
use a simple bijection (up to a scaling function)
from the set of Mahalanobis distances to the set
of equal mean multivariate Gaussian distributions.
Without loss of generalization, let the equal mean
be ?. Then given a Mahalanobis distance pa-
rameterized by A, the corresponding Gaussian is
p(x;A) =
1
Z
exp(?
1
2
d
A
(x, ?)) where Z is the
normalizing factor. This enables us to measure
the distance between two Mahalanobis distances
with the Kullback-Leibler (KL) divergence of two
Gaussians:
KL(p(x; I)||p(x;A)) =
?
p(x, I) log
(
p(x; I)
p(x;A)
)
dx.
Given pairs of similar points S and pairs of dis-
similar points D, the optimization problem is:
min
A
KL(p(x; I)||p(x;A))
subject to d
A
(x
i
, x
j
) ? u (i, j) ? S
d
A
(x
i
, x
j
) ? l (i, j) ? D
3.2 Learning Algorithm
(Davis and Dhillon, 2006) has shown that the
KL divergence between two multivariate Gaus-
sians can be expressed as the convex combination
of a Mahalanobis distance between mean vectors
and the LogDet divergence between the covariance
matrices. The LogDet divergence equals
D
ld
(A,A
0
) = tr(AA
?1
0
)? log det(AA
?1
0
)? n
for n by n matrices A,A
0
. If we assume the means
of the Gaussians to be the same, we have
KL(p(x;A
0
||p(x,A)) =
1
2
D
ld
(A,A
0
)
The optimization problem can be restated as
min
A0
D
ld
(A, I)
s.t. tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? u (i, j) ? S
tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? l (i, j) ? D
We then incorporate slack variables into the for-
mulation to guarantee the existence of a feasible
solution for A. The optimization problem be-
comes:
min
A0
D
ld
(A, I) + ?D
ld
(diag(?), diag(?
0
))
s.t. tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? ?
c(i,j)
(i, j) ? S
tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? ?
c(i,j)
(i, j) ? D
where c(i, j) is the index of the (i, j)-th constraint
and ? is a vector of slack variables whose compo-
nents are initialized to u for similarity constraints
and l for dissimilarity constraints. The tradeoff
between satisfying the constraints and minimiz-
ing D
ld
(A, I) is controlled by the parameter ?.
To solve this optimization problem, the algorithm
shown in Algorithm 3.1 repeatedly projects the
current solution onto a single constraint.
This completes the summary of (Davis et al,
2007).
4 Experimental Settings
In this section, we describe the experimental set-
tings including the preprocessing of data and fea-
tures, creation of the query word sets, and settings
of the cross validation.
4.1 Features
We used a dependency structure as the context for
words because it is the most widely used and one
of the best performing contextual information in
the past studies (Ruge, 1997; Lin, 1998). As the
extraction of an accurate and comprehensive de-
pendency structure is in itself a complicated task,
the sophisticated parser RASP Toolkit 2 (Briscoe
et al, 2006) was utilized to extract this kind of
word relation.
Let N(w, c) be the raw cooccurrence count of
word w and context c, the grammatical relation
795
Algorithm
3.1: INFORMATION THEORETIC METRIC LEARNING
Input :
X(d by n matrix), I(identity matrix)
S(set of similar pairs),D(set of dissimilar pairs)
?(slack parameter), c(constraint index function)
u, l(distance thresholds)
Output :
A(Mahalanobis matrix)
A := I
?
ij
:= 0
?
c(i,j)
:= u for (i, j) ? S; otherwise, ?
c(i,j)
:= l
repeat
Pick a constraint (i, j) ? S or (i, j) ? D
p := (x
i
? x
j
)
?
A(x
i
? x
j
)
? := 1 if (i, j) ? S,?1 otherwise.
? := min(?
ij
,
?
2
(
1
p
?
?
?
c(i,j)
))
? := ??/(1? ???
c(i,j)
)
?
c(i,j)
:= ??
c(i,j)
/(? + ???
c(i,j)
)
?
ij
:= ?
ij
? ?
A := A + ?A(x
i
? x
j
)(x
i
? x
j
)
?
A
until convergence
return (A)
in which w occurs. These raw counts were ob-
tained from New York Times articles (July 1994)
extracted from English Gigaword 1. The section
consists of 7,593 documents and approx. 5 million
words. As discussed below, we limited the vocab-
ulary to the nouns in the Longman Defining Vo-
cabulary (LDV) 2. The features were constructed
by weighting them using pointwise mutual infor-
mation: wgt(w, c) = PMI(w, c) = log P (w,c)
P (w)P (c)
.
Co-occurrence data constructed this way can
yield more than 10,000 context types, rendering
metric learning impractical. As the applications
of feature selection reduce the performance of the
baseline metrics, we tested them in two different
settings: with and without feature selection. To
mitigate this problem, we applied a feature selec-
tion technique to reduce the feature dimensional-
ity. We selected features using two approaches.
The first approach is a simple frequency cutoff, ap-
plied as a pre-processing to filter out words and
contexts with low frequency and to reduce com-
putational cost. Specifically, all words w such
that
?
c
N(w, c) < ?
f
and contexts c such that
?
w
N(w, c) < ?
f
, with ?
f
= 5, are removed from
the co-occurrence data.
The second approach is feature selection by con-
1http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2003T05
2http://www.cs.utexas.edu/users/kbarker/working notes/
ldoce-vocab.html
text importance (Hagiwara et al, 2008). First, the
context importance score for each context type is
calculated, and then the least important context
types are eliminated, until a desired numbers of
them remains. To measure the context importance
score, we used the number of unique words the
context co-occurs with: df(c) = |{w|N(w, c) >
0}|. We adopted this context selection criterion
on the assumption that the contexts shared by
many words should be informative, and the syn-
onym acquisition performance based on normal
distributional similarity calculation retains its orig-
inal level of performance until up to almost 90%
of context types are eliminated (Hagiwara et al,
2008). In our experiment, we selected features
rather aggressively, finally using only 10% of the
original contexts. These feature reduction oper-
ations reduced the dimensionality to a figure as
small as 1,281, while keeping the performance loss
at a minimum.
4.2 Similarity and Distance Functions
We compared seven similarity/distance functions
in our experiments: cosine similarity, Euclidean
distance, Manhattan distance, Jaccard coeffi-
cient, vector-based Jaccard coefficient (Jaccardv),
Jensen-Shannon Divergence (JS) and skew diver-
gence (SD99). We first define some notations. Let
C(w) be the set of context types that co-occur with
word w, i.e., C(w) = {c|N(w, c) > 0}, and w
i
be
the feature vector corresponding to word w, i.e.,
w
i
= [wgt(w
i
, c
1
) ... wgt(w
i
, c
M
)]
?
. The first
three, the cosine, Euclidean and Manhattan dis-
tance, are vector-based metrics.
cosine similarity
w
1
?w
2
||w
1
|| ? ||w
2
||
Euclidean distance
?
?
c?C(w
1
)?C(w
2
)
(wgt(w
1
, c)? wgt(w
2
, c))
2
Manhattan distance
?
c?C(w
1
)?C(w
2
)
|wgt(w
1
, c)? wgt(w
2
, c)|
Jaccard coefficient
?
c?C(w
1
)?C(w
2
)
min(wgt(w
1
, c),wgt(w
2
, c))
?
c?C(w
1
)?C(w
2
)
max(wgt(w
1
, c),wgt(w
2
, c))
,
796
vector-based Jaccard coefficient (Jaccardv)
w
i
?w
j
||w
i
|| + ||w
j
|| ?w
i
?w
j
.
Jensen-Shannon divergence (JS)
1
2
{KL(p
1
||m) + KL(p
2
||m)}, m = p
1
+ p
2
.
JS and SD99 are based on the KL divergence, so
the vectors must be normalized to form a probabil-
ity distribution. For notational convenience, we let
p
i
be the probability distribution representation of
feature vector w
i
, i.e., p
i
(c) = N(w
i
, c)/N(w
i
).
While the KL divergence suffers from the so-called
zero-frequency problem, a symmetric version of
the KL divergence called the Jensen-Shannon di-
vergence naturally avoids it.
skew divergence (SD99)
KL(p
1
||?p
2
+ (1? ?)p
1
).
As proposed by (Lee, 2001), the skew diver-
gence also avoids the zero-frequency problem by
mixing the original distribution with the target dis-
tribution. Parameter ? is set to 0.99.
4.3 Query Word Set and Cross Validation
To formalize the experiments, we must prepare a
set of query words for which synonyms are known
in advance. We chose the Longman Defining
Vocabulary (LDV) as the candidate set of query
words. For each word in the LDV, we consulted
three existing thesauri: Roget?s Thesaurus (Ro-
get, 1995), Collins COBUILD Thesaurus (Collins,
2002), and WordNet (Fellbaum, 1998). Each LDV
word was looked up as a noun to obtain the union
of synonyms. After removing words marked ?id-
iom?, ?informal? or ?slang? and phrases com-
prised of two or more words, this union was used
as the reference set of query words. LDV words for
which no noun synonyms were found in any of the
reference thesauri were omitted. From the remain-
ing 771 LDV words, there were 231 words that had
five or more synonyms in the combined thesaurus.
We selected these 231 words to be the query words
and distributed them into five partitions so as to
conduct five-fold cross validation. Four partitions
were used in training, and the remaining partition
was used in testing. For each fold, we created
the training set from four partitions as follows; for
each query word in the partitions, we randomly se-
lected five synonymous words and added the pairs
of query words and synonymous words to S, the
set of similar pairs. Similarly, five pairs of query
words and dissimilar words were randomly added
to D, the set of dissimilar pairs. The training set
for each fold consisted of S and D. Since a learner
trained on an imbalanced dataset may not learn
to discriminate enough between classes, we sam-
pled dissimilar pairs to create an evenly distributed
training dataset.
To make the evaluation realistic, we used a dif-
ferent method to create the test set: we paired each
query word with each of the 771 remaining words
to form the test set. Thus, in each fold, the training
set had an equal number of positive and negative
pairs, while in the test set, negative pairs outnum-
bered the positive pairs. While this is not a typical
setting for cross validation, it renders the evalua-
tion more realistic since an automatic synonym ac-
quisition system in operation must be able to pick
a few synonyms from a large number of dissimilar
words.
The meta-parameters of the metric learning
model were simply set u = 1, l = 2 and ? = 1.
Each training set consisted of 1,850 pairs, and the
test set consisted of 34,684 pairs. Since we con-
ducted five-fold cross validation, the reported per-
formance in this paper is actually a summary over
different folds.
4.4 Evaluation Measures
We used an evaluation program for KDD Cup
2004 (Caruana et al, 2004) called Perf to measure
the effectiveness of the metrics in acquiring syn-
onyms. To use the program, we used the following
formula to convert each distance metric to a simi-
larity metric. s(x
i
, x
j
) = 1/(1 + exp(d(x
i
, x
j
))).
Below, we summarize the three measures we
used: Mean Average Precision, TOP1, and Aver-
age Rank of Last Synonym.
Mean Average Precision (APR)
Perf implements a definition of average preci-
sion sometimes called ?expected precision?. Perf
calculates the precision at every recall where it is
defined. For each of these recall values, Perf finds
the threshold that produces the maximum preci-
sion, and takes the average over all of the recall
values greater than 0. Average precision is mea-
sured on each query, and then the mean of each
query?s average precision is used as the final met-
ric. A mean average precision of 1.0 indicates per-
fect prediction. The lowest possible mean average
797
precision is 0.0.
Average Rank of Last Synonym (RKL)
As in other evaluation measures, synonym can-
didates are sorted by predicted similarity, and this
metric measures how far down the sorted cases we
must go to find the last true synonym. A rank of
1 indicates that the last synonym is placed in the
top position. Given a query word, the highest ob-
tainable rank is N if there are N synonyms in the
corpus. The lower this measure is the better. Aver-
age ranks near 771 indicate poor performance.
TOP1
In each query, synonym candidates are sorted by
predicted similarity. If the word that ranks at the
top (highest similarity to the query word) is a true
synonym of the query word, Perf scores a 1 for
that query, and 0 otherwise. If there are ties, Perf
scores 0 unless all of the tied cases are synonyms.
TOP1 score ranges from 1.0 to 0.0. To achieve 1.0,
perfect TOP1 prediction, a similarity metric must
place a true synonym at the top of the sorted list
in every query. In the next section, we report the
mean of each query?s TOP1.
5 Results
The evaluations of the metrics are listed in Table
1. The figure on the left side of ? represents the
performance with 1,281 features, and that on the
right side with 12,812 features. Of all the met-
rics in Table 1, only the Mahalanobis L2 is trained
with the previously presented metric learning al-
gorithm. Thus, the values for the Mahalanobis
L2 are produced by the five-fold cross validation,
while the rest are given by the straight application
of the metrics discussed in Section 4.2 to the same
dataset. Strictly speaking, this is not a fair com-
parison, since we ought to compare a supervised
learning with a supervised learning. However, our
baseline is not the simple Euclidean distance; it
is the Jaccard coefficient and cosine similarity, a
handcrafted, best performing metric for synonym
acquisition, with 10 times as many features.
The computational resources required to obtain
the Mahalanobis L2 results were as follows: in the
training phase, each fold of cross validation took
about 80 iterations (less than one week) to con-
verge on a Xeon 5160 3.0GHz. The time required
to use the learned distance was a few hours at most.
At first, we were unable to perform competi-
tively with the Euclidean distance. As seen in Ta-
ble 1, the TOP1 measure of the Euclidean distance
is only 1.732%. This indicates that the likelihood
of finding the first item on the ranked list to be a
true synonym is 1.732%. The vector-based Jac-
card coefficient performs much better than the Eu-
clidean distance, placing a true synonym at the top
of the list 30.736% of the time.
Table 2 shows the Top 10 Words for Query
?branch?. The results for the Euclidean distance
rank ?hut? and other dissimilar words highly. This
is because the norm of such vectors is small, and in
a high dimensional space, the sparse vectors near
the origin are relatively close to many other sparse
vectors. To overcome this problem, we normal-
ized the input vectors by the L2 norm x? = x/||x||
This normalization enables the Euclidean distance
to perform very much like the cosine similarity,
since the Euclidean distance between points on a
sphere acts like the angle between the vectors. Sur-
prisingly, normalization by L2 did not affect other
metrics all that much; while the performances of
some metrics improved slightly, the L2 normaliza-
tion lowered that of the Jaccardv metric.
Once we learned the normalization trick, the
learned Mahalanobis distance consistently outper-
formed all other metrics, including the ones with
10 times more features, in all three evaluation
measures, achieving an APR of 18.66%, RKL of
545.09 and TOP1 of 45.455%.
6 Discussion
Examining the learned Mahalanobis matrix re-
vealed interesting features. The matrix essentially
shows the covariance between features. While it
was not as heavily weighted as the diagonal ele-
ments, we found that its positive non-diagonal el-
ements were quite interesting. They indicate that
some of the useful features for finding synonyms
are correlated and somewhat interchangeable. The
example includes a pair of features, (dobj begin
*) and (dobj end *). It was a pleasant surprise to
see that one implies the other. Among the diag-
onal elements of the matrix, one of the heaviest
features was being the direct object of ?by?. This
indicates that being the object of the preposition
?by? is a good indicator that two words are simi-
lar. A closer inspection of the NYT corpus showed
that this preposition overwhelmingly takes a per-
son or organization as its object, indicating that
words with this feature belong to the same class
of a person or organization. Similarly, the class
798
Metric APR RKL TOP1
Cosine 0.1184 ? 0.1324 580.27 ? 579.00 0.2987 ? 0.3160
Euclidean 0.0229 ? 0.0173 662.74 ? 695.71 0.0173 ? 0.0000
Euclidean L2 0.1182 ? 0.1324 580.30 ? 578.99 0.2943 ? 0.3160
Jaccard 0.1120 ? 0.1264 580.76 ? 579.51 0.2684 ? 0.2943
Jaccard L2 0.1113 ? 0.1324 580.29 ? 570.88 0.2640 ? 0.2987
Jaccardv 0.1189 ? 0.1318 580.50 ? 580.19 0.3073 ? 0.3030
Jaccardv L2 0.1184 ? 0.1254 580.27 ? 570.00 0.2987 ? 0.3160
JS 0.0199 ? 0.0170 681.97 ? 700.53 0.0129 ? 0.0000
JS L2 0.0229 ? 0.0173 679.21 ? 699.00 0.0303 ? 0.0086
Manhattan 0.0181 ? 0.0168 687.73 ? 701.47 0.0043 ? 0.0000
Manhattan L2 0.0185 ? 0.0170 686.56 ? 701.11 0.0043 ? 0.0086
SD99 0.0324 ? 0.1039 640.71 ? 588.16 0.0173 ? 0.2640
SD99 L2 0.0334 ? 0.1117 633.32 ? 586.78 0.0216 ? 0.2900
Mahalanobis L2 0.1866 545.09 0.4545
Table 1: Evaluation of Various Metrics, as Number of Features Increase from 1,281 to 12,812
Cosine Euclidean Euclidean L2 Jaccard Jaccardv Mahalanobis L2
1 (*) office hut (*) office (*) office (*) office (*) division
2 area wild area border area group
3 (*) division polish (*) division area (*) division (*) office
4 border thirst border plant border line
5 group hollow group (*) division group period
6 organization shout organization mouth organization organization
7 store fold store store store (*) department
8 mouth dear mouth circle mouth charge
9 plant hate plant stop plant world
10 home wake home track home body
(*) = a true synonym
Table 2: Top 10 Words for Query ?branch?
of words that ?to? and ?within?, take as an objects
were clear from the corpus: ?to? takes a person
or place, ?within? takes duration of time 3. Other
heavy features includes being the object of ?write?
or ?about?. While not obvious, we postulate that
having these words as a part of the context indi-
cates that a word is an event of some type.
7 Conclusion
We applied metric learning to automatic synonym
acquisition for the first time, and our experiments
showed that the learned metric significantly out-
performs existing similarity metrics. This outcome
indicates that while we must resort to feature se-
lection to apply metric learning, the performance
gain from the supervised learning is enough to off-
set the disadvantage and justify its usage in some
applications. This leads us to think that a com-
bination of the learned metric with unsupervised
metrics with even more features may produces the
best results. We also discussed interesting features
found in the learned Mahalanobis matrix. Since
3Interestingly, we note that not all prepositions were as
heavy: ?beyond? and ?without? were relatively light among
the diagonal elements. In the NYT corpus, the class of words
they take was not as clear as, for example, ?by?.
metric learning is known to boost clustering per-
formance in a semi-supervised clustering setting,
we believe these automatically identified features
would be helpful in assigning a target word to a
word class.
References
T. Briscoe, J. Carroll and R. Watson. 2006. The Sec-
ond Release of the RASP System. Proc. of the COL-
ING/ACL 2006 Interactive Presentation Sessions,
77?80.
T. Briscoe, J. Carroll, J. Graham and A. Copestake,
2002. Relational evaluation schemes. Proc. of the
Beyond PARSEVAL Workshop at the Third Interna-
tional Conference on Language Resources and Eval-
uation, 4?8.
A. Budanitsky and G. Hirst. 2006. Evaluat-
ing WordNet-based measures of semantic distance.
Computational Linguistics, 32(1):13?47.
R. Caruana, T. Jachims and L. Backstrom. 2004. KDD-
Cup 2004: results and analysis ACM SIGKDD Ex-
plorations Newslatter, 6(2):95?108.
C. J. Croach and B. Yang. 1992. Experiments in au-
tomatic statistical thesaurus construction. the 15th
799
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
77?88.
J. R. Curran and M. Moens. 2002. Improvements in
automatic thesaurus extraction. In Workshop on Un-
supervised Lexical Acquisition. Proc. of the ACL
SIGLEX, 231?238.
J. V. Davis and I. S. Dhillon. 2006. Differential En-
tropic Clustering of Multivariate Gaussians. Ad-
vances in Neural Information Processing Systems
(NIPS).
J. V. Davis, B. Kulis, P. Jain, S. Sra and I. S. Dhillon.
2007. Information Theoretic Metric Learning. Proc.
of the International Conference on Machine Learn-
ing (ICML).
A. Globerson and S. Roweis. 2005. Metric Learning by
Collapsing Classes. Advances in Neural Information
Processing Systems (NIPS).
J. Goldberger, S. Roweis, G. Hinton and R. Salakhut-
dinov. 2004. Neighbourhood Component Analysis.
Advances in Neural Information Processing Systems
(NIPS).
G. Grefenstette. 1994. Explorations in Automatic The-
suarus Discovery. Kluwer Academic Publisher.
M. Hagiwara, Y. Ogawa, and K. Toyama. 2008. Con-
text Feature Selection for Distributional Similarity.
Proc. of IJCNLP-08, 553?560.
Z. Harris. 1985. Distributional Structure. Jerrold
J. Katz (ed.) The Philosophy of Linguistics. Oxford
University Press. 26?47.
T. Hastie and R. Tibshirani. 1996. Discriminant adap-
tive nearest neighbor classification. Pattern Analysis
and Machine Intelligence, 18, 607?616.
D. Hindle. 1990. Noun classification from predicate-
argument structures. Proc. of the ACL, 268?275.
J. J. Jiang and D. W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxon-
omy. Proceedings of International Conference on
Research on Computational Linguistics (ROCLING
X), Taiwan.
Y. Jing and B. Croft. 1994. An Association The-
saurus for Information Retrieval. Proc. of Recherche
d?Informations Assiste?e par Ordinateur (RIAO),
146?160.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. Proc. of COLING/ACL 1998, 786?
774.
L. Lee. 1999. Measures of distributional similarity.
Proc. of the ACL, 23?32
L. Lee. 2001. On the Effectiveness of the Skew Diver-
gence for Statistical Language Analysis. Artificial
Intelligence and Statistics 2001, 65?72.
S. Mohammad and G. Hirst. 2006. Distributional mea-
sures of concept-distance: A task-oriented evalua-
tion. Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP),
Sydney, Australia.
P. Resnik. 1995. Using information content to evaluate
semantic similarity. Proceedings of the 14th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-95), 448?453, Montreal, Canada.
G. Ruge. 1997. Automatic detection of thesaurus re-
lations for information retrieval applications. Foun-
dations of Computer Science: Potential - Theory -
Cognition, LNCS, Volume 1337, 499?506, Springer
Verlag, Berlin, Germany.
S. Shalev-Shwartz, Y. Singer and A. Y. Ng. 2004. On-
line and Batch Learning of Pseudo-Metrics. Proc. of
the International Conference on Machine Learning
(ICML).
M. Schutz and T. Joachims. 2003. Learning a Dis-
tance Metric from Relative Comparisons. Advances
in Neural Information Processing Systems (NIPS)..
J. Weeds, D. Weir and D. McCarthy. 2004. Character-
ising Measures of Lexical Distributional Similarity.
Proc. of COLING 2004, 1015?1021.
K. Q. Weinberger, J. Blitzer and L. K. Saul. 2005.
Distance Metric Learning for Large Margin Nearest
Neighbor Classification. Advances in Neural Infor-
mation Processing Systems (NIPS).
E. P. Xing, A. Y. Ng, M. Jordan and S. Russell 2002.
Distance metric learning with application to cluster-
ing with sideinformation. Advances in Neural Infor-
mation Processing Systems (NIPS).
Y. Yang and J. O. Pedersen. 1997. A Comparative
Study on Feature Selection in Text Categorization.
Proc. of the International Conference on Machine
Learning (ICML), 412?420.
800
Context Feature Selection for Distributional Similarity
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko Toyama
Graduate School of Information Science,
Nagoya University
Furo-cho, Chikusa-ku, Nagoya, JAPAN 464-8603
{hagiwara, yasuhiro, toyama}@kl.i.is.nagoya-u.ac.jp
Abstract
Distributional similarity is a widely used
concept to capture the semantic relatedness
of words in various NLP tasks. However, ac-
curate similarity calculation requires a large
number of contexts, which leads to imprac-
tically high computational complexity. To
alleviate the problem, we have investigated
the effectiveness of automatic context selec-
tion by applying feature selection methods
explored mainly for text categorization. Our
experiments on synonym acquisition have
shown that while keeping or sometimes in-
creasing the performance, we can drastically
reduce the unique contexts up to 10% of the
original size. We have also extended the
measures so that they cover context cate-
gories. The result shows a considerable cor-
relation between the measures and the per-
formance, enabling the automatic selection
of effective context categories for distribu-
tional similarity.
1 Introduction
Semantic similarity of words is one of the most im-
portant lexical knowledge for NLP tasks including
word sense disambiguation and synonym acquisi-
tion. To measure the semantic relatedness of words,
a concept called distributional similarity has been
widely used. Distributional similarity represents the
relatedness of two words by the commonality of
contexts the words share, based on the distributional
hypothesis (Harris, 1985), which states that seman-
tically similar words share similar contexts.
A wide range of contextual information, such
as surrounding words (Lowe and McDonald, 2000;
Curran and Moens, 2002a), dependency or case
structure (Hindle, 1990; Ruge, 1997; Lin, 1998),
and dependency path (Lin and Pantel, 2001; Pado
and Lapata, 2007), has been utilized for similar-
ity calculation, and achieved considerable success.
However, a major problem which arises when adopt-
ing distributional similarity is that it easily yields a
huge amount of unique contexts. This can lead to
high dimensionality of context space, often up to the
order of tens or hundreds of thousands, which makes
the calculation computationally impractical. Be-
cause not all of the contexts are useful, it is strongly
required for the efficiency to eliminate the unwanted
contexts to ease the expensive cost.
To tackle this issue, Curran and Moens (2002b)
suggest assigning an index vector of canonical at-
tributes, i.e., a small number of representative el-
ements extracted from the original vector, to each
word. When the comparison is performed, canonical
attributes of two target words are firstly consulted,
and the original vectors are referred to only if the
attributes have a match between them. However, it
is not clear whether the condition for canonical at-
tributes they adopted, i.e., that the attributes must be
the most weighted subject, direct object, or indirect
object, is optimal in terms of the performance.
There are also some existing studies which paid
attention to the comparison of context categories
for synonym acquisition (Curran and Moens, 2002a;
Hagiwara et al, 2006). However, they have con-
ducted only a posteriori comparison based on perfor-
mance evaluation, and we are afraid that these find-
553
ings are somewhat limited to their own experimental
settings which may not be applicable to completely
new settings, e.g., one with a new set of contexts
extracted from different sources. Therefore, general
quantitative measures which can be used for reduc-
tion and selection of any kind of contexts and con-
text categories are strongly required.
Shifting our attention from word similarity to
other areas, a great deal of studies on feature selec-
tion has been conducted in the literature, especially
for text categorization (Yang and Pedersen, 1997)
and gene expression classification (Ding and Peng,
2003). Whereas these methods have been successful
in reducing feature size while keeping classification
performance, the problem of distributional similar-
ity is radically different from that of classification,
and whether the same methods are applicable and
effective for automatic context selection in the simi-
larity problem is yet to be investigated.
In this paper, we firstly introduce existing quan-
titative methods for feature selection, namely, DF,
TS, MI, IG, CHI2, and show how to apply them to
the distributional similarity problem to measure the
context importance. We then extracted dependency
relations as context from the corpus, and conducted
automatic synonym acquisition experiments to eval-
uate the context selection performance, reducing the
unimportant contexts based on the feature selection
methods. Finally we extend the context importance
to cover context categories (RASP2 grammatical re-
lations), and show that the above methods are also
effective in selecting categories.
This paper is organized as follows: in Section
2, five existing context selection methods are in-
troduced, and how to apply classification-based se-
lection methods to distributional similarity is de-
scribed. In Section 3 and 4, the synonym acquisition
method and evaluation measures, AP and CC, em-
ployed in the evaluation experiments are detailed.
Section 5 includes two main experiments and their
results: context reduction and context category se-
lection, along with experimental settings and discus-
sions. Section 6 concludes this paper.
2 Context Selection Methods
In this section, context selection methods proposed
for text categorization or information retrieval are
introduced. In the following, n and m represent
the number of unique words and unique contexts,
respectively, and N(w, c) denotes the number of co-
occurrence of word w and context c.
2.1 Document Frequency (DF)
Document frequency (DF), commonly used for
weighting in information retrieval, is the number of
documents a term co-occur with. However, in the
distributional similarity settings, DF corresponds to
word frequency, i.e., the number of unique words the
context co-occurs with:
df(c) = |{w|N(w, c) > 0}|.
The motivation of adopting DF as a context selection
criterion is the assumption that the contexts shared
by many words should be informative. It is to note,
however, that the contexts with too high DF are not
always useful, since there are some exceptions in-
cluding so-called stopwords.
2.2 Term Strength (TS)
Term strength (TS), proposed by Wilbur and
Sirotkin (1992) and applied to text categorization
by Yang and Wilbur (1996), measures how likely a
term is to appear in ?similar documents,? and it is
shown to achieve a successful outcome in reducing
the amount of vocabulary for text retrieval. For dis-
tributional similarity, TS is defined as:
s(c) = P (c ? C(w2)|c ? C(w1)),
where (w1, w2) is a related word pair and C(w) is
a set of contexts co-occurring with the word w, i.e.,
C(w) = {c|N(w, c) > 0}. s(c) is calculated, let-
ting PH be a set of related word pairs, as
s(c) = |{(w1, w2) ? PH |c ? C(w1) ? C(w2)}|
|{(w1, w2) ? PH |c ? C(w1)}|
.
What makes TS different from DF is that it re-
quires a training set PH consisting of related word
pairs. We used the test set for class s = 1 as PH
described in the next section.
2.3 Formalization of Distributional Similarity
The following methods, MI, IG, and CHI2, are rad-
ically different from the above ones, in that they are
554
designed essentially for ?class classification? prob-
lems. Thus we formalize distributional similarity as
a classification problem as described below.
First of all, we deal with word pairs, instead of
words, as the targets of classification, and define fea-
tures f1, ..., fm corresponding to contexts c1, ..., cm,
for each pair. The feature fj = 1 if the two words of
the pair has the context cj in common, and fj = 0
otherwise. Then, we define target class s, so that
s = 1 when the pair is semantically related, and
s = 0 if not. These defined, distributional similar-
ity is formalized as a binary classification problem
which assigns the word pairs to the class s ? {0, 1}
based on the features c1, ..., cm. Finally, to calcu-
late the specific values of the following feature im-
portance measures, we prepare two test sets of re-
lated word pairs for class s = 1 and unrelated ones
for class s = 0. This enables us to apply existing
feature selection methods designed for classification
problems to the automatic context selection.
The two test sets, related and unrelated one, are
prepared using the reference sets described in Sec-
tion 4. More specifically, we created 5,000 related
word pairs by extracting from synonym pairs in the
reference set, and 5,000 unrelated ones by firstly cre-
ating random pairs of LDV, whose detail is described
later, and then manually making sure that no related
pairs are included in these random pairs.
2.4 Mutual Information (MI)
Mutual information (MI), commonly used for word
association and co-occurrence weighing in statisti-
cal NLP, is the measure of the degree of dependence
between two events. The pointwise MI value of fea-
ture f and class s is calculated as:
I(f, s) = log P (f, s)
P (f)P (s)
.
To obtain the final context importance, we combine
the MI value over both of the classes as Imax(cj) =
maxs?{0,1} I(fj , s). Note that, here we employed
the maximum value of pointwise MI values since
it is claimed to be the best in (Yang and Peder-
sen, 1997), although there can be other combination
ways such as weighted average.
2.5 Information Gain (IG)
Information gain (IG), often employed in the ma-
chine learning field as a criterion for feature impor-
tance, is the amount of gained information of an
event by knowing the outcome of the other event,
and is calculated as the weighted sum of the point-
wise MI values over all the event combinations:
G(cj) =
?
fj?{0,1}
?
s?{0,1}
P (fj , s) log
P (fj , s)
P (fj)P (s)
.
2.6 ?2 Statistic (CHI2)
?2 statistic (CHI2) estimates the lack of indepen-
dence between classes and features, which is equal
to the summed difference of observed and expected
frequency over the contingency table cells. More
specifically, letting F jnm(n,m ? {0, 1}) be the num-
ber of word pairs with fj = n and s = m, and the
number of all pairs be N , ?2 statistic is defined as:
?2(cj)
= N(F11F00 ? F01F10)
(F11 + F01)(F10 + F00)(F11 + F10)(F01 + F00)
.
3 Synonym Acquisition Method
This section describes the synonym acquisition
method, a major and important application of distri-
butional similarity, which we employed for the eval-
uation of automatic context selection. Here we men-
tion how to extract the original contexts from cor-
pora in detail, as well as the calculation of weight
and similarity between words.
3.1 Context Extraction
We adopted dependency structure as the context of
words since it is the most widely used and well-
performing contextual information in the past stud-
ies (Ruge, 1997; Lin, 1998). As the extraction of ac-
curate and comprehensive dependency structure is in
itself a difficult task, the sophisticated parser RASP
Toolkit 2 (Briscoe et al, 2006) was utilized to ex-
tract this kind of word relations. Take the following
sentence for example:
Shipments have been relatively level since January,
the Commerce Department noted.
555
RASP outputs the extracted dependency structure
as n-ary relations as follows, which are called gram-
matical relations. Annotations regarding suffix, part
of speech tags, offsets for individual words are omit-
ted for simplicity.
(ncsubj be Shipment _)
(aux be have)
(xcomp _ be level)
(ncmod _ be relatively)
(ccomp _ level note)
(ncmod _ note since)
(ncsubj note Department _)
(det Department the)
(ncmod _ Department Commerce)
(dobj since January)
While the RASP outputs are n-ary relations in
general, what we need here is co-occurrences of
words and contexts, so we extract the set of co-
occurrences of stemmed words and contexts by tak-
ing out the target word from the relation and replac-
ing the slot by an asterisk ?*?:
(words) - (contexts)
Shipment - ncsubj:be:*_
have - aux:be:*
be - ncsubj:*:Shipment:_
be - aux:*:have
be - xcomp:_:*:level
be - ncmod:_:*:relatively
relatively - ncmod:_:be:*
level - xcomp:_:be:*
level - ccomp:_:*:note
...
Summing all these up produces the raw co-
occurrence count N(w, c) of word w and context c.
3.2 Similarity Calculation
Although it is possible to use the raw count acquired
above for the similarity calculation, directly using
the raw count may cause performance degradation,
thus we need an appropriate weighting measure. In
response to the preliminary experiment results, we
employed pointwise mutual information as weight:
wgt(w, c) = log P (w, c)
P (w)P (c)
Here we made a small modification to bind the
weight to non-negative such that wgt(w, c) ? 0,
because negative weight values sometimes worsen
the performance (Curran and Moens, 2002b). The
weighting by PMI is applied after the pre-processing
including frequency cutoff and context selection.
As for the similarity measure, we used Jaccard co-
efficient, which is widely adopted to capture overlap
proportion of two sets:
?
c?C(w1)?C(w2) min(wgt(w1, c),wgt(w2, c))
?
c?C(w1)?C(w2) max(wgt(w1, c),wgt(w2, c))
.
4 Evaluation Measures
This section describes the two evaluation methods
we employed ? average precision (AP) and corre-
lation coefficient (CC).
4.1 Average Precision (AP)
The first evaluation measure, average precision
(AP), is a common evaluation scheme for informa-
tion retrieval, which evaluates how accurately the
methods are able to extract synonyms. We first pre-
pare a set of query words, for which synonyms are
obtained to evaluate the precision. We adopted the
Longman Defining Vocabulary (LDV) 1 as the can-
didate set of query words. For each word in LDV,
three existing thesauri are consulted: Roget?s The-
saurus (Roget, 1995), Collins COBUILD Thesaurus
(Collins, 2002), and WordNet (Fellbaum, 1998).
The union of synonyms obtained when the LDV
word is looked up as a noun is used as the refer-
ence set, except for words marked as ?idiom,? ?in-
formal,? ?slang? and phrases comprised of two or
more words. The LDV words for which no noun
synonyms are found in any of the reference thesauri
are omitted. From the remaining 771 LDV words,
100 query words are randomly extracted, and for
each of them the eleven precision values at 0%, 10%,
..., and 100% recall levels are averaged to calculate
the final AP value.
4.2 Correlation Coefficient (CC)
The second evaluation measure is correlation coef-
ficient (CC) between the target similarity and the
reference similarity, i.e., the answer value of sim-
ilarity for word pairs. The reference similarity is
calculated based on the closeness of two words in
the tree structure of WordNet. More specifically, the
similarity between word w with senses w1, ..., wm1
and word v with senses v1, ..., vm2 is obtained as fol-
lows. Let the depth of node wi and vj be di and dj ,
1http://www.cs.utexas.edu/users/kbarker/working notes/
ldoce-vocab.html
556
and the depth of the deepest common ancestors of
both nodes be ddca. The similarity is then
sim(w, v) = max
i,j
sim(wi, vj) = maxi,j
2 ? ddca
di + dj
,
which takes the value between 0.0 and 1.0. Then,
the value of CC is calculated as the correlation co-
efficient of reference similarities r = (r1, r2, ..., rn)
and target similarities s = (s1, s2, ..., sn) over the
word pairs in sample set Ps, which is created by
choosing the most similar 2,000 word pairs from
4,000 randomly created pairs from LDV. To avoid
test-set dependency, all the CC values presented in
this paper are the average values of three trials using
different test sets.
5 Experiments
Now we describe the experimental settings and the
evaluation results of context selection methods.
5.1 Experimental Settings
As for the corpus, New York Times section of En-
glish Gigaword 2, consisting of around 914 million
words and 1.3 million documents was analyzed to
obtain word-context co-occurrences. Frequency cut-
off was applied as a pre-processing in order to filter
out any words and contexts with low frequency and
to reduce computational cost. More specifically, any
words w such that
?
c tf(w, c) < ?f and any con-
texts c such that
?
w tf(w, c) < ?f , with ?f = 40,
were removed from the co-occurrence data.
Since we set our purpose here to the automatic
acquisition of synonymous nouns, only the nouns
except for proper nouns were selected. To distin-
guish nouns, using POS tags annotated by RASP2,
any words with POS tags APP, ND, NN, NP, PN, PP
were labeled as nouns. This left a total of 40,461
unique words and 139,618 unique context, which
corresponds to the number of vectors and the dimen-
sionality of semantic space, respectively.
5.2 Context Reduction
In the first experiment, we show the effectiveness of
the five contextual selection methods introduced in
Section 2 for context reduction problem. The five
2http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2003T05
measures were calculated for each context, and con-
texts were sorted by their importance. The change of
performance, AP and CC, was calculated on elimi-
nating the low-ranked contexts and varying the pro-
portion of remaining ones, until only 0.2% (279 in
number) of the unique contexts are left.
The result is displayed in Figure 1. The overall
observation is that the performance not only kept the
original level but also slightly improved even during
the ?aggressive? reduction when more than 80% of
the original contexts were eliminated and less than
20,000 contexts were left. It was not until 90% (ap-
prox. 10,000 remaining) elimination that the AP
values began to fall. The tendency of performance
change was almost the same for AP and CC, but
we observe a slight difference regarding which of
the five measures were effective. More specifically,
TS, IG and CHI2 worked well for AP, and DF, TS,
while CHI2 did for CC. On the whole, TS and CHI2
were performing the best, whereas the performance
of MI quickly worsened. Although the task is dif-
ferent, this experiment showed a very consistent re-
sult compared with the one of Yang and Pedersen?s
(1997). This means that feature selection methods
are also effective for context selection in distribu-
tional similarity, and our formalization of the prob-
lem described in Section 2 turned out to be appro-
priate for the purpose.
5.3 Context Category Selection
We are then naturally interested in what kinds of
contexts are included in these top-ranked effective
ones and how much they affect the overall perfor-
mance. To investigate this, we firstly built a set of
elite contexts, by gathering each top 10% (13,961
in number) contexts chosen by DF, TS, IG, and
CHI2, and obtaining the intersection of these four
top-ranked contexts. It was found that these four had
a great deal of overlap among them, the number of
which turned out to be 6,440.
Secondly, to measure the degree of effect a con-
text category has, we defined category importance
as the sum of all IG values of the contexts which
belong to the category. The reason is that, (a) IG
was one of the best-performing criteria as the previ-
ous experiment showed, and (b) IG value for a set of
contexts can be calculated as the sum of IG values of
individual elements, assuming that all the contexts
557
0.10
0.15
0.20
0.25
020000400006000080000100000120000
Number of Unique Context
Cor
rela
tion
 Co
effi
cien
t (C
C)
DF
TS
MI
IG
CHI2
`
(c)
6.0%
8.0%
10.0%
12.0%
14.0%
020000400006000080000100000120000
Number of Unique Context
Ave
rage
 Pre
cisi
on (
AP)
DF
TS
MI
IG
CHI2 (a)
0.10
0.15
0.20
0.25
05000100001500020000
Number of Unique Context
Cor
rela
tion
 Co
effi
cien
t (C
C)
DF
TS
IG
CHI2
`
(d)
6.0%
8.0%
10.0%
12.0%
14.0%
05000100001500020000
Number of Unique Context
Ave
rage
 Pre
cisi
on (
AP)
DF
TS
IG
CHI2 (b)
Figure 1: Performance of synonym acquisition on automatic context reduction
(a) The overall view and (b) the close-up of 0 to 20,000 unique contexts for AP,
and (c) the overall view and (b) the close-up for CC
are mutually independent, which is a naive but prac-
tical assumption because of the high independence
of acquired contexts from corpora.
For the categories: ncsubj, dobj, obj, obj2,
ncmod, xmod, cmod, ccomp, det, ta, based on the
RASP2 grammatical relations which occur fre-
quently (more than 1.0%) in the corpus, their cat-
egory importance within the elite context set was
computed and showed in Figure 2. The graph also
shows the performance of individual context cat-
egories, calculated when each category was sepa-
rately extracted from the entire corpus. The re-
sult indicates that there is a considerable correlation
(r = 0.760) between category importance and per-
formance, which means it is possible to predict the
final performance of any context categories by cal-
culating their category importance values in the lim-
ited size of selected context set.
As for the qualitative difference of category types,
the result also shows the effectiveness of modifica-
tion (ncmod) category, which is consistent with the
result (Hagiwara et al, 2006) that mod is more con-
tributing than subj and obj, which have been ex-
tensively used in the past. However, it can be seen
that the reason why the ncmod performs well may be
only because it is the largest category in size (2,515
558
0% 2% 4% 6% 8% 10% 12% 14%
ncsubj
dobj 
obj 
obj2 
ncmod 
xmod
cmod
ccomp
det
ta 
Average Precision (AP)
0 2 4 6 8 10
Category Importance (CI)
AP
CI
Figure 2: Performance of synonym acquisition vs
context category importance
in the elite contexts). The investigation of the rela-
tions between context size and performance should
be conducted in the future.
6 Conclusion
In this study, we firstly introduced feature selec-
tion methods, previously proposed for text catego-
rization, and showed how to apply them for auto-
matic context selection for distributional similarity
by formalizing the similarity problem as classifica-
tion. We then extracted dependency-based context
from the corpus, and conducted evaluation experi-
ments on automatic synonym acquisition.
The experimental results showed that while keep-
ing or even improving the original performance, it
is possible to eliminate a large proportion of con-
texts (almost up to 90%). We also extended the con-
text importance to cover context categories based on
RASP2 grammatical relations, and showed a consid-
erable correlation between the importance and the
actual performance, suggesting the possibility of au-
tomatic context category selection.
As the future works, we should further discuss
other kinds of formalization of distributional simi-
larity and their impact, because we introduced and
only briefly described a quite simple formalization
model in Section 2.3. More detailed investigations
on the contributions of sub-categories of contexts,
and other contexts than dependency structure, such
as surrounding words and dependency path, is also
the future work.
References
Ted Briscoe, John Carroll and Rebecca Watson. 2006.
The Second Release of the RASP System. Proc. of the
COLING/ACL 2006 Interactive Presentation Sessions,
77?80.
Collins. 2002. Collins Cobuild Major New Edition CD-
ROM. HarperCollins Publishers.
James R. Curran and Marc Moens. 2002. Scaling Con-
text Space. Proc. of ACL 2002, 231?238.
James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Workshop
on Unsupervised Lexical Acquisition. Proc. of ACL
SIGLEX, 231?238.
Chris Ding and Hanchuan Peng. 2003. Minimum Re-
dundancy Feature Selection from Microarray Gene
Expression Data. Proc. of the IEEE Computer Soci-
ety Conference on Bioinformatics, 523?528.
Editors of the American Heritage Dictionary. 1995. Ro-
get?s II: The New Thesaurus, 3rd ed. Houghton Mif-
flin.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database, MIT Press.
Masato Hagiwara, Yasuhiro Ogawa, Katsuhiko Toyama.
2006. Selection of Effective Contextual Information
for Automatic Synonym Acquisition. Proc. of COL-
ING/ACL 2006, 353?360.
Zellig Harris. 1985. Distributional Structure. Jerrold J.
Katz (ed.) The Philosophy of Linguistics. Oxford Uni-
versity Press. 26?47
Donald Hindle. 1990. Noun classification from
predicate-argument structures. Proc. of the 28th An-
nual Meeting of the ACL, 268?275.
Will Lowe and Scott McDonald. 2000. The direct route:
Mediated priming in semantic space. Proc. of the 22nd
Annual Conference of the Cognitive Science Society,
675?680.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. Proc. of COLING/ACL 1998, 786?774.
559
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, Volume 7, Issue 4, 343?360.
Seastian Pado and Mirella Lapata. 2007. Dependency-
Based Construction of Semantic Space Models Com-
putational Linguistics, Volume 33, Issue 2, 161?199.
Gerda Ruge. 1997. Automatic detection of thesaurus re-
lations for information retrieval applications. Founda-
tions of Computer Science: Potential - Theory - Cogni-
tion, LNCS, Volume 1337, 499?506, Springer Verlag,
Berlin, Germany.
Yiming Yang and John Wilbur. 1996. Using corpus
statistics to remove redundant words in text categoriza-
tion. Journal of the American Society for Information
Science, Volume 47, Issue 5, 357?369.
Yiming Yang and Jan O. Pedersen. 1997. A Compara-
tive Study on Feature Selection in Text Categorization.
Proc. of ICML 97, 412?420.
John Wilbur and Karl Sirotkin. 1992. The automatic
identification of stop words. Journal of Information
Science, 45?55.
560
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 191?199,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Japanese Query Alteration Based on Semantic Similarity
Masato Hagiwara
Nagoya University
Furo-cho, Chikusa-ku
Nagoya 464-8603, Japan
hagiwara@kl.i.is.nagoya-u.ac.jp
Hisami Suzuki
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
hisamis@microsoft.com
Abstract
We propose a unified approach to web search
query alterations in Japanese that is not lim-
ited to particular character types or ortho-
graphic similarity between a query and its al-
teration candidate. Our model is based on pre-
vious work on English query correction, but
makes some crucial improvements: (1) we
augment the query-candidate list to include
orthographically dissimilar but semantically
similar pairs; and (2) we use kernel-based
lexical semantic similarity to avoid the prob-
lem of data sparseness in computing query-
candidate similarity. We also propose an ef-
ficient method for generating query-candidate
pairs for model training and testing. We show
that the proposed method achieves about 80%
accuracy on the query alteration task, improv-
ing over previously proposed methods that use
semantic similarity.
1 Introduction
Web search query correction is an important prob-
lem to solve for robust information retrieval given
how pervasive errors are in search queries: it is said
that more than 10% of web search queries contain
errors (Cucerzan and Brill, 2004). English query
correction has been an area of active research in re-
cent years, building on previous work on general-
purpose spelling correction. However, there has
been little investigation of query correction in lan-
guages other than English.
In this paper, we address the issue of query cor-
rection, and more generally, query alteration in
Japanese. Japanese poses particular challenges to
the query correction task due to its complex writ-
ing system, summarized in Fig. 11. There are four
1The figure is somewhat over-simplified as it does not in-
clude any word consisting of multiple character types. It also
does not include examples of spelling mistakes and variants in
word segmentation.
Kanji
Sp: ?????
Abbr: ???????
Abbr: ?????????
Hiragana
Sp: ?????????
Roman
Alphabet
Sp: Ohno~Oono
Sp: center~centre
Katakana
Sp: ????????????
Abbr: ??????????
Abbr: ?????????????
Sp: Fedex???????
Abbr: MS????????
Sp: ?????
Syn: ?????????
Sp: ???????
Sp: ??????
Sp:???????
Syn: ??????
Syn: ????ANA
Figure 1: Japanese character types and spelling variants
main character types, including two types of kana
(phonetic alphabet - hiragana and katakana), kanji
(ideographic - characters represent meaning) and
Roman alphabet; a word can be legitimately spelled
in multiple ways, combining any of these character
sets. For example, the word for ?protein? can be
spelled as ?????? (all in hiragana), ????
? (katakana+kanji), ??? (all in kanji) or ???
? (hiragana+kanji), all pronounced in the same way
(tanpakushitsu). Some examples of these spelling
variants are shown in Fig. 1 with the prefix Sp: as is
observed from the figure, spelling variation occurs
within and across different character types. Resolv-
ing these variants will be essential not only for in-
formation retrieval but practically for all NLP tasks.
A particularly prolific source of spelling varia-
tions in Japanese is katakana. Katakana charac-
ters are used to transliterate words from English and
other foreign languages, and as such, the variations
in the source language pronunciation as well as the
ambiguity in sound adaptation are reflected in the
katakana spelling. For example, Masuyama et al
(2004) report that at least six distinct translitera-
tions of the word ?spaghetti? (??????, ???
???, etc.) are attested in the newspaper corpus
they studied. Normalizing katakana spelling varia-
tions has been the subject of research by itself (Ara-
maki et al, 2008; Masuyama et al, 2004). Similarly,
English-to-katakana transliteration (e.g., ?fedex? as
?????? fedekkusu in Fig. 1) and katakana-to-
191
English back-transliteration (e.g.,?????? back
into ?fedex?) have also been studied extensively (Bi-
lac and Tanaka, 2004; Brill et al, 2001; Knight and
Graehl, 1998), as it is an essential component for
machine translation. To our knowledge, however,
there has been no work that addresses spelling vari-
ation in Japanese generally.
In this paper, we propose a general approach to
query correction/alteration in Japanese. Our goal is
to find precise re-write candidates for a query, be
it a correction of a spelling error, normalization of
a spelling variant, or finding a strict synonym in-
cluding abbreviations (e.g., MS ???????
?Microsoft?, prefixed by Abbr in Fig. 1) and true
synonyms (e.g., ?? (translation of ?seat?) ???
(transliteration of ?seat?, indicated by Syn in Fig. 1)2.
Our method is based on previous work on English
query correction in that we use both spelling and se-
mantic similarity between a query and its alteration
candidate, but is more general in that we include al-
teration candidates that are not similar to the original
query in spelling. In computing semantic similar-
ity, we adopt a kernel-based method (Kandola et al,
2002), which improves the accuracy of the query al-
teration results over previously proposed methods.
We also introduce a novel approach to creating a
dataset of query and alteration candidate pairs effi-
ciently and reliably from query session logs.
2 Related Work
The key difference between traditional general-
purpose spelling correction and search query cor-
rection lies in the fact that the latter cannot rely on
a lexicon: web queries are replete with valid out-
of-dictionary words which are not mis-spellings of
in-vocabulary words. Cucerzan and Brill (2004) pi-
oneered the research of query spelling correction,
with an excellent description of how a traditional
dictionary-based speller had to be adapted to solve
the realistic query correction problem. The model
they proposed is a source-channel model, where the
source model is a word bigram model trained on
query logs, and the channel model is based on a
weighted Damerau-Levenshtein edit distance. Brill
2Our goal is to harvest alternation candidates; therefore, ex-
actly how they are used in the search task (whether it is used to
substitute the original query, to expand it, or simply to suggest
an alternative) is not a concern to us here.
and Moore (2000) proposed a general, improved
source model for general spelling correction, while
Ahmad and Kondrak (2005) learned a spelling error
model from search query logs using the Expectation
Maximization algorithm, without relying on a train-
ing set of misspelled words and their corrections.
Extending the work of Cucerzan and Brill (2004),
Li et al (2006) proposed to include semantic sim-
ilarity between the query and its correction candi-
date. They point out that adventura is a common
misspelling of aventura, not adventure, and this can-
not be captured by a simple string edit distance, but
requires some knowledge of distributional similar-
ity. Distributional similarity is measured by the sim-
ilarity of the context shared by two terms, and has
been successfully applied to many natural language
processing tasks, including semantic knowledge ac-
quisition (Lin, 1998).
Though the use of distributional similarity im-
proved the query correction results in Li et al?s
work, one problem is that it is sparse and is not avail-
able for many rarer query strings. Chen et al (2007)
addressed this problem by using external informa-
tion (i.e., web search results); we take a different ap-
proach to solve the sparseness problem, namely by
using semantic kernels.
Jones et al (2006a) generated Japanese query al-
teration pairs from by mining query logs and built a
regression model which predicts the quality of query
rewriting pairs. Their model includes a wide variety
of orthographical features, but not semantic similar-
ity features.
3 Query Alteration Model
3.1 Problem Formulation
We employ a formulation of query alteration model
that is similar to conventional query correction mod-
els. Given a query string q as input, a query correc-
tion model finds a correct alteration c? within the
confusion set of q, so that it maximizes the posterior
probability:
c? = arg max
c?CF(q)?C
P (c|q) (1)
whereC is the set of all white-space separated words
and their bigrams in query logs in our case3, and
3In regular text, Japanese uses no white spaces to separate
words; however, white spaces are often (but not consistently)
192
CF(q) ? C is the confusion set of q, consisting of
the candidates within a certain edit distance from q,
i.e., CF(q) = {c ? C|ED(q, c) < ?}. We set ? =
24 using an unnormalized edit distance. The detail
of the edit distance ED(q, c) is described in Section
3.2. The query string q itself is contained in CF(q),
and if the model output is different from q, it means
the model suggests a query alteration. Formulated
in this way, both query error detection and alteration
are performed in a unified way.
After computing the posterior probability of each
candidate in CF(q) by the source channel model
(Section 3.2), an N-best list is obtained as the ini-
tial candidate set C0, which is then augmented by
the bootstrapping method Tchai (Section 3.4) to cre-
ate the final candidate list C(q). The candidates in
C(q) are re-ranked by a maximum entropy model
(Section 3.5) and the candidate with the highest pos-
terior probability is selected as the output.
3.2 Source Channel Model
Source channel models are widely used for spelling
and query correction (Brill and Moore, 2000;
Cucerzan and Brill, 2004). Instead of directly com-
puting Eq. (1), we can decompose the posterior
probability using Bayes? rule as:
c? = arg max
c?CF(q)?C
P (c)P (q|c), (2)
where the source model P (c) measures how proba-
ble the candidate c is, while the error model P (q|c)
measures how similar q and c are.
For the source model, an n-gram based statisti-
cal language model is the standard in previous work
(Ahmad and Kondrak, 2005; Li et al, 2006). Word
n-gram models are simple to create for English,
which is easy to tokenize and to obtain word-based
statistics, but this is not the case with Japanese.
Therefore, we simply considered the whole input
string as a candidate to be altered, and used the rel-
ative frequency of candidates in the query logs to
build the language model:
P (c) = Freq(c)?
c??C Freq(c?) . (3)
For the error model, we used an improved chan-
nel model described in (Brill and Moore, 2000),
used to separate words in Japanese search queries, due to their
keyword-based nature.
which we call the alpha-beta model in this paper.
The model is a weighted extension of the normal
Damerau-Levenshtein edit distance which equally
penalizes single character insertion, substitution, or
deletion operations (Damerau, 1964; Levenshtein,
1966), and considers generic edit operations of the
form ? ? ?, where ? and ? are any (possibly
null) strings. From misspelled/correct word pairs,
alpha-beta trains the probability P (? ? ?|PSN),
conditioned by the position PSN of ? in a word,
where PSN ? {start of word, middle of word, end of
word}. Under this model, the probability of rewrit-
ing a string w to a string s is calculated as:
P??(s|w) = max
R?Part(w),T?Part(s)
|R|?
i=1
P (Ri ? Ti|PSN(Ri)),
which corresponds to finding best partitionsR and T
in all possible partitions Part(w) and Part(s). Brill
and Moore (2000) reported that this model gave a
significant improvement over conventional edit dis-
tance methods.
Brill et al (2001) applied this model for ex-
tracting katakana-English transliteration pairs from
query logs. They trained the edit distance between
character chunks of katakana and Roman alphabets,
after converting katakana strings to Roman script.
We also trained this model using 59,754 katakana-
English pairs extracted from aligned Japanese and
English Wikipedia article titles. In this paper we al-
lowed |?|, |?| ? 3. The resulting edit distance is
obtained as the negative logarithm of the alpha-beta
probability, i.e., ED??(q|c) = ? logP??(q|c).
Since the edit operations are directional and c and
q can be any string consisting of katakana and En-
glish, distance in both directions were considered.
We also included a modified edit distance EDhd for
simple kana-kana variations after converting them
into Roman script. The distance EDhd is essen-
tially the same as the normal Damerau-Levenshtein
edit distance, with the modification that it does not
penalize character halving (aa ? a) and doubling
(a ? aa), because a large part of katakana vari-
ants only differ in halving/doubling (e.g. ?????
(supageti) vs?????? (supagetii)4.
The final error probability is obtained from the
minimum of these three distances:
4However, character length can be distinctive in katakana,
as in?? biru ?building? vs.??? biiru ?beer?.
193
ED(q, c) = min[ED??(q|c),ED??(c|q),EDhd(q, c)],(4)
P (q|c) = exp[?ED(q, c)] (5)
where every edit distance is normalized to [0, 1] by
multiplying by a factor of 2/(|q||c|) so that it does
not depend on the length of the input strings5.
3.3 Kernel-based Lexical Semantic Similarity
3.3.1 Distributional Similarity
The source channel model described in Sec-
tion 3.2 only considers language and error models
and cannot capture semantic similarity between the
query and its correction candidate. To address this
issue, we use distributional similarity (Lin, 1998) es-
timated from query logs as additional evidence for
query alteration, following Li et al (2006).
For English, it is relatively easy to define the con-
text of a word based on the bag-of-words model. As
this is not expected to work on Japanese, we de-
fine context as everything but the query string in a
query log, as Pas?ca et al (2006) and Komachi and
Suzuki (2008) did for their information extraction
tasks. This formulation does not involve any seg-
mentation or boundary detection, which makes this
method fast and robust. On the other hand, this may
cause additional sparseness in the vector representa-
tion; we address this issue in the next two sections.
Once the context of a candidate ci is de-
fined as the patterns that the candidate co-occurs
with, it can be represented as a vector ci =
[pmi(ci, p1), . . . ,pmi(ci, pM )]?, where M denotes
the number of context patterns and x? is the trans-
position of a vector (or possibly a matrix) x. The el-
ements of the vector are given by pointwise mutual
information between the candidate ci and the pattern
pj , computed as:
pmi(ci, pj) = log |ci, pj ||ci, ?||?, pj | , (6)
where |ci, pj | is the frequency of the pattern pj in-
stantiated with the candidate ci, and ?*? denotes a
5We did not include kanji variants here, because disam-
biguating kanji readings is a very difficult task, and the ma-
jority of the variations in queries are in katakana and Roman
alphabet. The framework proposed in this paper, however, can
incorporate kanji variants straightforwardly into ED(q, c) once
we have reasonable edit distance functions for kanji variations.
wildcard, i.e., |ci, ?| = ?p |ci, p| and |?, pj | =?
c |c, pj |. With these defined, the distributional
similarity can be calculated as cosine similarity. Let
c?i be the L2-normalized pattern vector of the candi-
date ci, and X = {c?i} be the candidate-pattern co-
occurrence matrix. The candidate similarity matrix
K can then be obtained asK = X ?X . In the follow-
ing, the (i, j)-element of the matrix K is denoted as
Kij , which corresponds to the cosine similarity be-
tween candidates ci and cj .
3.3.2 Semantic Kernels
Although distributional similarity serves as strong
evidence for semantically relevant candidates, di-
rectly applying the technique to query logs faces the
sparseness problem. Because context patterns are
drawn from query logs and can also contain spelling
errors, alterations, and word permutations as much
as queries do, context differs so greatly in represen-
tations that even related candidates might not have
sufficient contextual overlap between them. For
example, a candidate ?YouTube? matched against
the patterns ?YouTube+movie?, ?movie+YouTube?
and ?You-Tube+movii? (with a minor spelling er-
ror) will yield three distinct patterns ?#+movie?,
?movie+#? and ?#+movii?6, which will be treated as
three separate dimensions in the vector space model.
This sparseness problem can be partially ad-
dressed by considering the correlation between pat-
terns. Kandola et al (2002) proposed new kernel-
based similarity methods which incorporate indirect
similarity between terms for a text retrieval task. Al-
though their kernels are built on a document-term
co-occurrence model, they can also be applied to our
candidate-pattern co-occurrence model. The pro-
posed kernel is recursively defined as:
K? = ?X ?G?X + K, G? = ?XK?X ? + G, (7)
where G = XX ? is the correlation matrix between
patterns and ? is the factor to ensure that longer
range effects decay exponentially. This can be in-
terpreted as augmenting the similarity matrix K
through indirect similarities of patterns G? and vice
versa. Semantically related pairs of patterns are ex-
pected to be given high correlation in the matrix G?
and this will alleviate the sparseness problem. By
6?+? denotes a white space, and ?#? indicates where the word
of interest is found in a context pattern.
194
?YouTube?
?#+movie?
?stage6?
?You+Tube?
?movie+#? ?#+anime?
c
1
c
2
c
3
p
1
p
2
p
3
(a)
?YouTube?
?#+movie?
?stage6?
?You+Tube?
?movie+#? ?#+anime?
c
1
c
2
c
3
p
1
p
2
p
3
(b)
Figure 2: Orthographically Augmented Graph
solving the above recursive definition, one obtains
the von Neumann kernel:
K?(?) = K(I ? ?K)?1 =
??
t=1
?t?1Kt. (8)
This can also be interpreted in terms of a random
walk in a graph where the nodes correspond to all the
candidates and the weight of an edge (i, j) is given
by Kij . A simple calculation shows that Kij equals
the sum of the products of the edge weights over all
possible paths between the nodes corresponding ci
and cj in the graph. Also, Ktij corresponds to the
probability that a random walk beginning at node ci
ends up at node cj after t steps, assuming that the en-
tries are all positive and the sum of the connections
is 1 at each node. Following this notion, Kandola
et al (2002) proposed another kernel called expo-
nential kernel, with alternative faster decay factors:
K?(?) = K
??
t=1
?tKt
t! = K exp(?K). (9)
They showed that this alternative kernel achieved a
better performance for their text retrieval task. We
employed these two kernels to compute distribu-
tional similarity for our query correction task.
3.3.3 Orthographically Augmented Kernels
Although semantic relatedness can be partially
captured by the semantic kernels introduced in the
previous section, they may still have difficulties
computing correlations between candidates and pat-
terns especially for only sparsely connected graphs.
Take the graph (a) in Fig. 2 for example, which is
a simplified yet representative graph topology for
candidate-pattern co-occurrence we often encounter.
In this case K = X ?X equals I , meaning that the
connections between candidates and patterns are too
sparse to obtain sufficient correlation even when se-
mantic kernels are used.
Input
query q
0
C
Pattern
induction
Source channel
model
0
P
1
C
1
P
Instance
induction
Pattern
induction
10
)( CCqC ?=
1
P
Distributional
similarity
Figure 3: Bootstrapping Additional Candidates
In order to address this issue, we propose to aug-
ment the graph by weakly connecting the candidate
and pattern nodes as shown in the graph (b) of Fig. 2
based on prior knowledge of orthographic similarity
about candidates and patterns. This can be achieved
using the following candidate similarity matrix K+
instead of K:
K+ = ?SC + (1? ?)X ? [?SP + (1? ?)I]X (10)
where SC = {sc(i, j)} is the orthographical similar-
ity matrix of candidates in which the (i, j)-element
is given by the edit distance based similarity, i.e.,
sc(i, j) = exp [?ED(ci, cj)]. The orthographical
similarity matrix of patterns SP = {sP (i, j)} is de-
fined similarly, i.e., sP (i, j) = exp[?ED(pi, pj)].
Note that using this similarity matrix K+ can be
interpreted as a random walk process on a bipar-
tite graph as follows. Let C and P as the sets of
candidates and patterns. K+ corresponds to a sin-
gle walking step from C to C, by either remaining
within C with a probability of ? or moving to ?the
other side? P of the graph with a probability of 1??.
When the walking remains in C, it is allowed to
move to another candidate node following the candi-
date orthographic similarity SC . Otherwise it moves
to P by the matrix X , chooses either to move within
P with a probability ?SP or to stay with a probabil-
ity 1? ?, and finally comes back to C by the matrix
X ?. Multiplication (K+)t corresponds to repeating
this process t times. Using this similarity, we can de-
fine two orthographically augmented semantic ker-
nels which differ in the decaying factors, augmented
von Neumann kernel and exponential kernel:
K?+(?) = K+(I ? ?K+)?1 (11)
K?+(?) = K+ exp(?K+). (12)
3.4 Bootstrapping Additional Candidates
Now that we have a semantic model, our query
correction model can cover query-candidate pairs
195
which are only semantically related. However, pre-
vious work on query correction all used a string dis-
tance function and a threshold to restrict the space of
potential candidates, allowing only the orthographi-
cally similar candidates.
To collect additional candidates, the use of
context-based semantic extraction methods would
be effective because semantically related candidates
are likely to share context with the initial query
q, or at least with the initial candidate set C0.
Here we used the Tchai algorithm (Komachi and
Suzuki, 2008), a modified version of Espresso (Pan-
tel and Pennacchiotti, 2006) to collect such candi-
dates. This algorithm starts with initial seed in-
stances, then induces reliable context patterns co-
occurring with the seeds, induces instances from
the patterns, and iterates this process to obtain cat-
egories of semantically related words. Using the
candidates in C0 as the seed instances, one boot-
strapping iteration of the Tchai algorithm is executed
to obtain the semantically related set of instances
C1. The seed instance reliabilities are given by the
source channel probabilities P (c)P (q|c). Finally we
take the union C0 ? C1 to obtain the candidate set
C(q). This process is outlined in Fig. 3.
3.5 Maximum Entropy Model
In order to build a unified probabilistic query al-
teration model, we used the maximum entropy ap-
proach of (Beger et al, 1996), which Li et al (2006)
also employed for their query correction task and
showed its effectiveness. It defines a conditional
probabilistic distribution P (c|q) based on a set of
feature functions f1, . . . , fK :
P (c|q) = exp
?K
i=1 ?ifi(c, q)?
c exp?Ki=1 ?ifi(c, q)
, (13)
where ?1, . . . , ?K are the feature weights. The op-
timal set of feature weights ?? can be computed by
maximizing the log-likelihood of the training set.
We used the Generalized Iterative Scaling (GIS)
algorithm (Darroch and Ratcliff, 1972) to optimize
the feature weights. GIS trains conditional proba-
bility in Eq. (13), which requires the normalization
over all possible candidates. However, the number
of all possible candidates C obtained from a query
log can be very large, so we only calculated the sum
over the candidates in C(q). This is the same ap-
proach that Och and Ney (2002) took for statistical
machine translation, and Li et al (2006) for query
spelling correction.
We used the following four categories of func-
tions as the features:
1. Language model feature, given by the logarithm
of the source model probability: logP (c).
2. Error model features, which are composed of
three edit distance functions: ?ED??(q|c),
?ED??(c|q), and ?EDhd(q, c).
3. Similarity based feature, computed as the loga-
rithm of distributional similarity between q and c:
log sim(q, c), which is calcualted using one of the
following kernels (Section 3.3): K, K?, K?, K?+,
and K?+. The similarity values were normalized
to [0, 1] after adding a small discounting factor
? = 1.0? 10?5.
4. Similarity based correction candidate features,
which are binary features with a value of 1 if and
only if the frequency of c is higher than that of
q, and distributional similarity between them is
higher than a certain threshold. Li et al (2006)
used this set of features, and suggested that these
features give the evidence that q may be a com-
mon misspelling of c. The thresholds on the nor-
malized distributional similarity are enumerated
from 0.5 to 0.9 with the interval 0.1.
4 Experiment
4.1 Dataset Creation
For all the experiments conducted in this paper, we
used a subset of the Japanese search query logs sub-
mitted to Live Search (www.live.com) in November
and December of 2007. Queries submitted less than
eight times were deleted. The query log we used
contained 83,080,257 tokens and 1,038,499 unique
queries.
Models of query correction in previous work were
trained and evaluated using manually created query-
candidate pairs. That is, human annotators were
given a set of queries and were asked to provide a
correction for each query when it needed to be re-
written. As Cucerzan and Brill (2004) point out,
however, this method is seriously flawed in that the
intention of the original query is completely lost to
the annotator, without which the correction is often
impossible: it is not clear if gogle should be cor-
rected to google or goggle, or neither ? gogle may
be a brand new product name. Cucerzan and Brill
196
therefore performed a second evaluation, where the
test data was drawn by sampling the query logs for
successive queries (q1, q2) by the same user where
the edit distance between q1 and q2 are within a cer-
tain threshold, which are then submitted to annota-
tors for generating the correction. While this method
makes the annotation more reliable by relying on
user (rather than annotator) reformulation, the task
is still overly difficult: going back to the example
in Section 1, it is unclear which spelling of ?protein?
produces the best search results? it can only be em-
pirically determined. Their method also eliminates
all pairs of candidates that are not orthographically
similar. We have therefore improved their method
in the following manner, making the process more
automated and thus more reliable.
We first collected a subset of the query log that
contains only those pairs (q1, q2) that are issued suc-
cessively by the same user, q2 is issued within 3 min-
utes of q1, and q2 resulted in a click of the resulting
page while q1 did not. The last condition adds the
evidence that q2 was a better formulation than q1.
We then ranked the collected query pairs using log-
likelihood ratio (LLR) (Dunning, 1993), which mea-
sures the dependence between q1 and q2 within the
context of web queries (Jones et al, 2006b). We ran-
domly sampled 10,000 query pairs with LLR? 200,
and submitted them to annotators, who only confirm
or reject a query pair as being synonymous. For ex-
ample, q1 = nikon and q2 = canon are related but
not synonymous, while we are reasonably sure q1 =
ipot and q2 = ipod are synonymous, given that this
pair has a high LLR value. This verification process
is extremely fast and consistent across annotators:
it takes less than 1 hour to go through 1,000 query
pairs, and the inter-annotator agreement rate of two
annotators on 2,000 query pairs was 95.7%. We
annotated 10,000 query pairs consisting of alpha-
numerical and kana characters in this manner. After
rejecting non-synonymous pairs and those which do
not co-occur with any context patterns, 6,489 pairs
remained, and we used 1,243 pairs for testing, 628
as a development set, and 4,618 for training the max-
imum entropy model.
4.2 Experimental Settings
The performance of query alteration was evaluated
based on the following measures (Li et al, 2006).
Table 1: Performance results (%)
Model Accuracy Recall Precision
SC 71.12 39.29 45.09
ME-NoSim 74.58 44.58 52.52
ME-Cos 74.18 45.84 50.70
ME-vN 74.34 45.59 52.16
ME-Exp 73.61 44.84 50.57
ME-vN+ 75.06 44.33 53.01
ME-Exp+ 75.14 44.08 53.52
The input queries, correct suggestions, and outputs
were matched in a case-insensitive manner.
? Accuracy: The number of correct outputs gener-
ated by the system divided by the total number of
queries in the test set;
? Recall: The number of correct suggestions for al-
tered queries divided by the total number of al-
tered queries in the test set;
? Precision: The number of correct suggestions for
altered queries divided by the total number of al-
terations made by the system.
The parameters for the kernels, namely, ?, ?, and
?, are tuned using the development set. The finally
employed values are: ? = 0.3 for K?, K?, and K?+,
? = 0.2 for K?+, ? = 0.2 and ? = 0.4 for K?+, and
? = 0.35 and ? = 0.7 for K?+. In the source channel
model, we manually scaled the language probability
by a factor of 0.1 to alleviate the bias toward highly
frequent candidates.
As the initial candidate set C0, top-50 instances
were selected by the source channel model, and 100
patterns were extracted as P0 by the Tchai iteration
after removing generic patterns, which we detected
simply by rejecting those which induced more than
200 unique instances. Finally top-30 instances were
induced using P0 to create C1. Generic instances
were not removed in this process because they may
still be alterations of input query q. The maximum
size of P1 was set to 2,000, after removing unreliable
patterns with reliability smaller than 0.0001.
4.3 Results
Table 1 shows the evaluation results. SC is the
source channel model, while the others are maxi-
mum entropy (ME) models with different features.
ME-NoSim uses the same features as SC, but con-
siderably outperforms SC in all three measures, con-
firming the superiority of the ME approach. Decom-
posing the three edit distance functions into three
197
separate features in the ME model may also explain
the better result. All the ME approaches outper-
formed SC in accuracy with a statistically significant
difference (p < 0.0001 on McNemar?s test).
The model with the cosine similarity (ME-Cos)
in addition to the basic set of features yielded higher
recall compared to ME-NoSim, but decreased accu-
racy and precision, which are more important than
recall for our purposes because a false alteration
does more damage than no alteration. This is also
the case when the kernel-based methods, ME-vN
(the von Neumann kernel) and ME-Exp (the expo-
nential kernel), are used in place of the cosine sim-
ilarity. This shows that using semantic similarity
does not always help, which we believe is due to
the sparseness of the contextual information used in
computing semantic similarity.
On the other hand, ME-vN+ (with augmented von
Neumann kernel) and ME-Exp+ (with augmented
exponential kernel) increased both accuracy and pre-
cision with a slight decrease of recall, compared to
the distributional similarity baseline and the non-
augmented kernel-based methods. ME-Exp+ was
significantly better than ME-Exp (p < 0.01).
Note that the accuracy values appear lower than
some of the previous results on English (e.g., more
than 80% in (Li et al, 2006)), but this is because
the dataset creation method we employed tends to
over-represent the pairs that lead to alteration: the
simplest baseline (= always propose no alteration)
performs 67.3% accuracy on our data, in contrast to
83.4% on the data used in (Li et al, 2006).
Manually examining the suggestions made by the
system also confirms the effectiveness of our model.
For example, the similarity-based models altered the
query ipot to ipod, while the simple ME-NoSim
model failed, because it depends too much on the
edit distance-based features. We also observed that
many of the suggestions made by the system were
actually reasonable, even though they were differ-
ent from the annotated gold standard. For example,
ME-vN+ suggests a re-write of the query 2tyann as
2????? (?2-channel?), while the gold standard
was an abbreviated form 2??? (?2-chan?).
To incorporate such possibly correct candidates
into account, we conducted a follow-up experiment
where we considered multiple reference alterations,
created automatically from our data set in the fol-
Table 2: Performance with the multiple reference model
Model Accuracy Recall Precision
SC 75.30 48.61 55.78
ME-NoSim 79.49 56.17 66.17
ME-Cos 79.32 58.19 64.35
ME-vN 79.24 57.18 65.42
ME-Exp 78.52 56.42 63.64
ME-vN+ 79.89 55.67 66.57
ME-Exp+ 79.81 54.91 66.67
lowing manner. Suppose that a query q1 is corrected
as q2, and q2 is corrected as q3 in our annotated data.
If this is the case, we considered q1 ? q3 as a valid
alteration as well. By applying this chaining oper-
ation up to 5 degrees of separation, we re-created a
set of valid alterations for each input query. Note
that directionality is important ? in the above ex-
ample, q1 ? q3 is valid, while q3 ? q1 is not. Table
2 shows the results of evaluation with multiple refer-
ences. The numbers substantially improved over the
single reference cases, as expected, but did not af-
fect the relative performance of each model. Again,
the differences in accuracy between the SC and ME
models, and ME-Exp and ME-Exp+ were statisti-
cally significant (p < 0.01).
5 Conclusion and future work
In this paper we have presented a unified approach
to Japanese query alteration. Our approach draws
on previous research in English spelling and query
correction, Japanese katakana variation and translit-
eration, and semantic similarity, and builds a model
that makes improvements over previously proposed
query correction methods. In particular, the use of
orthographically augmented semantic kernels pro-
posed in this paper is general and applicable to other
languages, including English, for query alteration,
especially when the data sparseness is an issue. In
the future, we also plan to investigate other meth-
ods, such as PLSI (Hofmann, 1999), to deal with
data sparseness in computing semantic similarity.
Acknowledgments
This research was conducted during the first au-
thor?s internship at Micorosoft Research. We thank
the colleagues, especially Dmitriy Belenko, Chris
Brockett, Jianfeng Gao, Christian Ko?nig, and Chris
Quirk for their help in conducting this research.
198
References
Farooq Ahmad and Grzegorz Kondrak. 2005. Learning
a spelling error model from search query logs. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP-2005), pages
955?962.
Eiji Aramaki, Takeshi Imai, Kengo Miyo, and Kazuhiko
Ohe. 2008. Orthographic disambiguation incorporat-
ing transliterated probability. In Proceedings in the
third International Joint Conference on Natural Lan-
guage Processing (IJCNLP-2008), pages 48?55.
Adam L. Beger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?72.
Slaven Bilac and Hozumi Tanaka. 2004. A hybrid back-
transliteration system for japanese. In Proceedings of
the 20th international conference on Computational
Linguistics (COLING-2004), pages 597?603.
Eric Brill and Robert C. Moore. 2000. An improved er-
ror model for noisy channel spelling. In Proceedings
of the 38th Annual Meeting on Association for Com-
putational Linguistics (ACL-2000), pages 286?293.
Eric Brill, Gary Kacmarcik, and Chris Brockett. 2001.
Automatically harvesting katakana-english term pairs
from search engine query logs. In Proceedings of the
Sixth Natural Language Processing Pacific Rim Sym-
posium (NLPRS-2001), pages 393?399.
Qing Chen, Mu Li, , and Ming Zhou. 2007. Improv-
ing query spelling correction using web search results.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 181?189.
Silviu Cucerzan and Eric Brill. 2004. Spelling correc-
tion as an iterative process that exploits the collective
knowledge of web users. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2004), pages 293?300.
Fred Damerau. 1964. A technique for computer detec-
tion and correction of spelling errors. Communication
of the ACM, 7(3):659?664.
J.N. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. Annuals of Mathemati-
cal Statistics, 43:1470?1480.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Research and Development in Informa-
tion Retrieval, pages 50?57.
Rosie Jones, Kevin Bartz, Pero Subasic, and Benjamin
Rey. 2006a. Automatically generating related queries
in japanese. Language Resources and Evaluation
(LRE), 40(3-4):219?232.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006b. Generating query substitutions. In
Proceedings of the 15th international World Wide Web
conference (WWW-06), pages 387?396.
Jaz Kandola, John Shawe-Taylor, and Nello Cristianini.
2002. Learning semantic similarity. In Neural Infor-
mation Processing Systems (NIPS 15), pages 657?664.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599?
612.
Mamoru Komachi and Hisami Suzuki. 2008. Mini-
mally supervised learning of semantic knowledge from
query logs. In Proceedings of the 3rd International
Joint Conference on Natural Language Processing
(IJCNLP-2008), pages 358?365.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physice - Doklady, 10:707?710.
Mu Li, Muhua Zhu, Yang Zhang, and Ming Zhou.
2006. Exploring distributional similarity based mod-
els for query spelling correction. In Proceedings of
COLING/ACL-2006, pages 1025?1032.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING/ACL-1998,
pages 786?774.
Takeshi Masuyama, Satoshi Sekine, and Hiroshi Nak-
agawa. 2004. Automatic construction of japanese
katakana variant list from large corpus. In Proceed-
ings of Proceedings of the 20th international confer-
ence on Computational Linguistics (COLING-2004),
pages 1214?1219.
Franz Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of the 40th an-
nual meeting of ACL, pages 295?302.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Organizing and searching
the world wide web of facts - step one: the one-million
fact extraction challenge. In Proceedings of the 21st
National Conference on Artificial Intelligence (AAAI-
06), pages 1400?1405.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically har-
vesting semantic relations. In Proceedings of
COLING/ACL-2006, pages 113?120.
199
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 353?360,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Selection of Effective Contextual Information
for Automatic Synonym Acquisition
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko Toyama
Graduate School of Information Science,
Nagoya University
Furo-cho, Chikusa-ku, Nagoya, JAPAN 464-8603
{hagiwara, yasuhiro, toyama}@kl.i.is.nagoya-u.ac.jp
Abstract
Various methods have been proposed for
automatic synonym acquisition, as syn-
onyms are one of the most fundamen-
tal lexical knowledge. Whereas many
methods are based on contextual clues
of words, little attention has been paid
to what kind of categories of contex-
tual information are useful for the pur-
pose. This study has experimentally inves-
tigated the impact of contextual informa-
tion selection, by extracting three kinds of
word relationships from corpora: depen-
dency, sentence co-occurrence, and prox-
imity. The evaluation result shows that
while dependency and proximity perform
relatively well by themselves, combina-
tion of two or more kinds of contextual in-
formation gives more stable performance.
We?ve further investigated useful selection
of dependency relations and modification
categories, and it is found that modifi-
cation has the greatest contribution, even
greater than the widely adopted subject-
object combination.
1 Introduction
Lexical knowledge is one of the most important re-
sources in natural language applications, making it
almost indispensable for higher levels of syntacti-
cal and semantic processing. Among many kinds
of lexical relations, synonyms are especially use-
ful ones, having broad range of applications such
as query expansion technique in information re-
trieval and automatic thesaurus construction.
Various methods (Hindle, 1990; Lin, 1998;
Hagiwara et al, 2005) have been proposed for syn-
onym acquisition. Most of the acquisition meth-
ods are based on distributional hypothesis (Har-
ris, 1985), which states that semantically similar
words share similar contexts, and it has been ex-
perimentally shown considerably plausible.
However, whereas many methods which adopt
the hypothesis are based on contextual clues con-
cerning words, and there has been much consid-
eration on the language models such as Latent
Semantic Indexing (Deerwester et al, 1990) and
Probabilistic LSI (Hofmann, 1999) and synonym
acquisition method, almost no attention has been
paid to what kind of categories of contextual infor-
mation, or their combinations, are useful for word
featuring in terms of synonym acquisition.
For example, Hindle (1990) used co-
occurrences between verbs and their subjects
and objects, and proposed a similarity metric
based on mutual information, but no exploration
concerning the effectiveness of other kinds of
word relationship is provided, although it is
extendable to any kinds of contextual information.
Lin (1998) also proposed an information theory-
based similarity metric, using a broad-coverage
parser and extracting wider range of grammatical
relationship including modifications, but he didn?t
further investigate what kind of relationships
actually had important contributions to acquisi-
tion, either. The selection of useful contextual
information is considered to have a critical impact
on the performance of synonym acquisition. This
is an independent problem from the choice of
language model or acquisition method, and should
therefore be examined by itself.
The purpose of this study is to experimen-
tally investigate the impact of contextual infor-
mation selection for automatic synonym acqui-
sition. Because nouns are the main target of
353
synonym acquisition, here we limit the target of
acquisition to nouns, and firstly extract the co-
occurrences between nouns and three categories of
contextual information ? dependency, sentence
co-occurrence, and proximity ? from each of
three different corpora, and the performance of
individual categories and their combinations are
evaluated. Since dependency and modification re-
lations are considered to have greater contribu-
tions in contextual information and in the depen-
dency category, respectively, these categories are
then broken down into smaller categories to ex-
amine the individual significance.
Because the consideration on the language
model and acquisition methods is not the scope of
the current study, widely used vector space model
(VSM), tf?idf weighting scheme, and cosine mea-
sure are adopted for similarity calculation. The re-
sult is evaluated using two automatic evaluation
methods we proposed and implemented: discrimi-
nation rate and correlation coefficient based on the
existing thesaurus WordNet.
This paper is organized as follows: in Section
2, three kinds of contextual information we use
are described, and the following Section 3 explains
the synonym acquisition method. In Section 4 the
evaluation method we employed is detailed, which
consists of the calculation methods of reference
similarity, discrimination rate, and correlation co-
efficient. Section 5 provides the experimental con-
ditions and results of contextual information se-
lection, followed by dependency and modification
selection. Section 6 concludes this paper.
2 Contextual Information
In this study, we focused on three kinds of con-
textual information: dependency between words,
sentence co-occurrence, and proximity, that is, co-
occurrence with other words in a window, details
of which are provided the following sections.
2.1 Dependency
The first category of the contextual information we
employed is the dependency between words in a
sentence, which we suppose is most commonly
used for synonym acquisition as the context of
words. The dependency here includes predicate-
argument structure such as subjects and objects
of verbs, and modifications of nouns. As the ex-
traction of accurate and comprehensive grammat-
ical relations is in itself a difficult task, the so-
dependent
mod
ncmod xmod cmod detmod
arg_mod arg aux conj
subj_or_dobj
subj
ncsubj xsubj csubj
comp
obj clausal
obj2dobj iobjxcomp ccomp
mod
subj
obj
Figure 1: Hierarchy of grammatical relations and
groups
phisticated parser RASP Toolkit (Briscoe and Car-
roll, 2002) was utilized to extract this kind of
word relations. RASP analyzes input sentences
and provides wide variety of grammatical infor-
mation such as POS tags, dependency structure,
and parsed trees as output, among which we paid
attention to dependency structure called grammat-
ical relations (GRs) (Briscoe et al, 2002).
GRs represent relationship among two or more
words and are specified by the labels, which con-
struct the hierarchy shown in Figure 1. In this hier-
archy, the upper levels correspond to more general
relations whereas the lower levels to more specific
ones. Although the most general relationship in
GRs is ?dependent?, more specific labels are as-
signed whenever possible. The representation of
the contextual information using GRs is as fol-
lows. Take the following sentence for example:
Shipments have been relatively level
since January, the Commerce Depart-
ment noted.
RASP outputs the extracted GRs as n-ary rela-
tions as follows:
(ncsubj note Department obj)
(ncsubj be Shipment _)
(xcomp _ be level)
(mod _ level relatively)
(aux _ be have)
(ncmod since be January)
(mod _ Department note)
(ncmod _ Department Commerce)
354
(detmod _ Department the)
(ncmod _ be Department)
While most of GRs extracted by RASP are bi-
nary relations of head and dependent, there are
some relations that contain additional slot or ex-
tra information regarding the relations, as shown
?ncsubj? and ?ncmod? in the above example. To
obtain the final representation that we require for
synonym acquisition, that is, the co-occurrence
between words and their contexts, these relation-
ships must be converted to binary relations, i.e.,
co-occurrence. We consider the concatenation of
all the rest of the target word as context:
Department ncsubj:note:*:obj
shipment ncsubj:be:*:_
January ncmod:since:be:*
Department mod:_:*:note
Department ncmod:_:*:Commerce
Commerce ncmod:_:Department:*
Department detmod:_:*:the
Department ncmod:_:be:*
The slot for the target word is replaced by ?*? in
the context. Note that only the contexts for nouns
are extracted because our purpose here is the auto-
matic extraction of synonymous nouns.
2.2 Sentence Co-occurrence
As the second category of contextual information,
we used the sentence co-occurrence, i.e., which
sentence words appear in. Using this context is,
in other words, essentially the same as featuring
words with the sentences in which they occur.
Treating single sentences as documents, this fea-
turing corresponds to exploiting transposed term-
document matrix in the information retrieval con-
text, and the underlying assumption is that words
that commonly appear in the similar documents or
sentences are considered semantically similar.
2.3 Proximity
The third category of contextual information,
proximity, utilizes tokens that appear in the vicin-
ity of the target word in a sentence. The basic as-
sumption here is that the more similar the distri-
bution of proceeding and succeeding words of the
target words are, the more similar meaning these
two words possess, and its effectiveness has been
previously shown (Macro Baroni and Sabrina Bisi,
2004). To capture the word proximity, we consider
a window with a certain radius, and treat the la-
bel of the word and its position within the window
as context. The contexts for the previous example
sentence, when the window radius is 3, are then:
shipment R1:have
shipment R2:be
shipment R3:relatively
January L1:since
January L2:level
January L3:relatively
January R1:,
January R2:the
January R3:Commerce
Commerce L1:the
Commerce L2:,
Commerce L3:January
Commerce R1:Department
...
Note that the proximity includes tokens such as
punctuation marks as context, because we suppose
they offer useful contextual information as well.
3 Synonym Acquisition Method
The purpose of the current study is to investigate
the impact of the contextual information selection,
not the language model itself, we employed one
of the most commonly used method: vector space
model (VSM) and tf?idf weighting scheme. In this
framework, each word is represented as a vector
in a vector space, whose dimensions correspond
to contexts. The elements of the vectors given by
tf?idf are the co-occurrence frequencies of words
and contexts, weighted by normalized idf. That
is, denoting the number of distinct words and con-
texts as N and M , respectively,
wi = t[tf(wi, c1) ? idf(c1) ... tf(wi, cM ) ? idf(cM )],
(1)
where tf(wi, cj) is the co-occurrence frequency of
word wi and context cj . idf(cj) is given by
idf(cj) = log(N/df(cj))maxk log(N/df(vk)) , (2)
where df(cj) is the number of distinct words that
co-occur with context cj .
Although VSM and tf?idf are naive and simple
compared to other language models like LSI and
PLSI, they have been shown effective enough for
the purpose (Hagiwara et al, 2005). The similar-
ity between two words are then calculated as the
cosine value of two corresponding vectors.
4 Evaluation
This section describes the evaluation methods we
employed for automatic synonym acquisition. The
evaluation is to measure how similar the obtained
similarities are to the ?true? similarities. We firstly
prepared the reference similarities from the exist-
ing thesaurus WordNet as described in Section 4.1,
355
and by comparing the reference and obtained sim-
ilarities, two evaluation measures, discrimination
rate and correlation coefficient, are calculated au-
tomatically as described in Sections 4.2 and 4.3.
4.1 Reference similarity calculation using
WordNet
As the basis for automatic evaluation methods, the
reference similarity, which is the answer value that
similarity of a certain pair of words ?should take,?
is required. We obtained the reference similarity
using the calculation based on thesaurus tree struc-
ture (Nagao, 1996). This calculation method re-
quires no other resources such as corpus, thus it is
simple to implement and widely used.
The similarity between word sense wi and word
sense vj is obtained using tree structure as follows.
Let the depth1 of node wi be di, the depth of node
vj be dj , and the maximum depth of the common
ancestors of both nodes be ddca. The similarity
between wi and vj is then calculated as
sim(wi, vj) = 2 ? ddcadi + dj , (3)
which takes the value between 0.0 and 1.0.
Figure 2 shows the example of calculating the
similarity between the word senses ?hill? and
?coast.? The number on the side of each word
sense represents the word?s depth. From this tree
structure, the similarity is obtained:
sim(?hill?, ?coast?) = 2 ? 35 + 5 = 0.6. (4)
The similarity between word w with senses
w1, ..., wn and word v with senses v1, ..., vm is de-
fined as the maximum similarity between all the
pairs of word senses:
sim(w, v) = max
i,j
sim(wi, vj), (5)
whose idea came from Lin?s method (Lin, 1998).
4.2 Discrimination Rate
The following two sections describe two evalua-
tion measures based on the reference similarity.
The first one is discrimination rate (DR). DR, orig-
inally proposed by Kojima et al (2004), is the rate
1To be precise, the structure of WordNet, where some
word senses have more than one parent, isn?t a tree but a
DAG. The depth of a node is, therefore, defined here as the
?maximum distance? from the root node.
entity     0
inanimate-object     1
natural-object     2
geological-formation     3
4 natural-elevation
5 hill
shore     4
coast     5
Figure 2: Example of automatic similarity calcu-
lation based on tree structure
(answer, reply)(phone, telephone)(sign, signal)(concern, worry)
(animal, coffee)(him, technology)(track, vote)(path, youth)
? ?
highly related unrelated
Figure 3: Test-sets for discrimination rate calcula-
tion.
(percentage) of pairs (w1, w2) whose degree of as-
sociation between two words w1, w2 is success-
fully discriminated by the similarity derived by
the method under evaluation. Kojima et al dealt
with three-level discrimination of a pair of words,
that is, highly related (synonyms or nearly syn-
onymous), moderately related (a certain degree of
association), and unrelated (irrelevant). However,
we omitted the moderately related level and lim-
ited the discrimination to two-level: high or none,
because of the difficulty of preparing a test set that
consists of moderately related pairs.
The calculation of DR follows these steps: first,
two test sets, one of which consists of highly re-
lated word pairs and the other of unrelated ones,
are prepared, as shown in Figure 3. The similar-
ity between w1 and w2 is then calculated for each
pair (w1, w2) in both test sets via the method un-
der evaluation, and the pair is labeled highly re-
lated when similarity exceeds a given threshold t
and unrelated when the similarity is lower than t.
The number of pairs labeled highly related in the
highly related test set and unrelated in the unre-
lated test set are denoted na and nb, respectively.
356
DR is then given by:
1
2
( na
Na +
nb
Nb
)
, (6)
where Na and Nb are the numbers of pairs in
highly related and unrelated test sets, respectively.
Since DR changes depending on threshold t, max-
imum value is adopted by varying t.
We used the reference similarity to create these
two test sets. Firstly, Np = 100, 000 pairs of
words are randomly created using the target vo-
cabulary set for synonym acquisition. Proper
nouns are omitted from the choice here because
of their high ambiguity. The two testsets are then
created extracting n = 2, 000 most related (with
high reference similarity) and unrelated (with low
reference similarity) pairs.
4.3 Correlation coefficient
The second evaluation measure is correlation co-
efficient (CC) between the obtained similarity and
the reference similarity. The higher CC value is,
the more similar the obtained similarities are to
WordNet, thus more accurate the synonym acqui-
sition result is.
The value of CC is calculated as follows. Let
the set of the sample pairs be Ps, the sequence of
the reference similarities calculated for the pairs
in Ps be r = (r1, r2, ..., rn), the corresponding
sequence of the target similarity to be evaluated
be r = (s1, s2, ..., sn), respectively. Correlation
coefficient ? is then defined by:
? =
1
n
?n
i=1(ri ? r?)(si ? s?)
?r?s , (7)
where r?, s?, ?r, and ?s represent the average of r
and s and the standard deviation of r and s, re-
spectively. The set of the sample pairs Ps is cre-
ated in a similar way to the preparation of highly
related test set used in DR calculation, except that
we employed Np = 4, 000, n = 2, 000 to avoid
extreme nonuniformity.
5 Experiments
Now we desribe the experimental conditions and
results of contextual information selection.
5.1 Condition
We used the following three corpora for the ex-
periment: (1) Wall Street Journal (WSJ) corpus
(approx. 68,000 sentences, 1.4 million tokens),
(2) Brown Corpus (BROWN) (approx. 60,000
sentences, 1.3 million tokens), both of which are
contained in Treebank 3 (Marcus, 1994), and (3)
written sentences in WordBank (WB) (approx.
190,000 sentences, 3.5 million words) (Hyper-
Collins, 2002). No additional annotation such as
POS tags provided for Treebank was used, which
means that we gave the plain texts stripped off any
additional information to RASP as input.
To distinguish nouns, using POS tags annotated
by RASP, any words with POS tags APP, ND, NN,
NP, PN, PP were labeled as nouns. The window
radius for proximity is set to 3. We also set a
threshold tf on occurrence frequency in order to
filter out any words or contexts with low frequency
and to reduce computational cost. More specifi-
cally, any words w such that ?c tf(w, c) < tf and
any contexts c such that ?w tf(w, c) < tf were
removed from the co-occurrence data. tf was set
to tf = 5 for WSJ and BROWN, and tf = 10 for
WB in Sections 5.2 and 5.3, and tf = 2 for WSJ
and BROWN and tf = 5 for WB in Section 5.4.
5.2 Contextual Information Selection
In this section, we experimented to discover what
kind of contextual information extracted in Sec-
tion 2 is useful for synonym extraction. The per-
formances, i.e. DR and CC are evaluated for each
of the three categories and their combinations.
The evaluation result for three corpora is shown
in Figure 4. Notice that the range and scale of the
vertical axes of the graphs vary according to cor-
pus. The result shows that dependency and prox-
imity perform relatively well alone, while sen-
tence co-occurrence has almost no contributions
to performance. However, when combined with
other kinds of context information, every category,
even sentence co-occurrence, serves to ?stabilize?
the overall performance, although in some cases
combination itself decreases individual measures
slightly. It is no surprise that the combination of all
categories achieves the best performance. There-
fore, in choosing combination of different kinds of
context information, one should take into consid-
eration the economical efficiency and trade-off be-
tween computational complexity and overall per-
formance stability.
5.3 Dependency Selection
We then focused on the contribution of individual
categories of dependency relation, i.e. groups of
grammatical relations. The following four groups
357
65.0%
65.5%
66.0%
66.5%
67.0%
67.5%
68.0%
68.5%
dis
cr
im
ina
tio
n 
ra
te
 
(DR
)a
0.09
0.10
0.11
0.12
0.13
correlation
 coefficient
 (CC))
DR
CC
dep sent prox dep
sent
dep
prox
sent
prox
all
(1) WSJ
DR
 = 52.8%
CC
 = -0.0029
sent:
65.0%
65.5%
66.0%
66.5%
67.0%
67.5%
68.0%
68.5%
69.0%
dis
cr
im
ina
tio
n 
ra
te
 
(DR
)a
0.13
0.14
0.15
correlation
 coefficient
 (CC))
DR
CC
dep sent prox dep
sent
dep
prox
sent
prox
all
(2) BROWN
DR
 = 53.8%
CC
 = 0.060
sent:
66.0%
66.5%
67.0%
67.5%
68.0%
68.5%
69.0%
dis
cr
im
ina
tio
n 
ra
te
 
(DR
)a
0.16
0.17
0.18
0.19
correlation
 coefficient
 (CC))
DR
CC
dep sent prox dep
sent
dep
prox
sent
prox
all
(3) WB
DR
 = 52.2%
CC
 = 0.0066
sent:
Figure 4: Contextual information selection perfor-
mances
Discrimination rate (DR) and correlation coefficient (CC)
for (1) Wall Street Journal corpus, (2) Brown Corpus, and
(3) WordBank.
of GRs are considered for comparison conve-
nience: (1) subj group (?subj?, ?ncsubj?, ?xsubj?,
and ?csubj?), (2) obj group (?obj?, ?dobj?, ?obj2?,
and ?iobj?), (3) mod group (?mod?, ?ncmod?,
?xmod?, ?cmod?, and ?detmod?), and (4) etc
group (others), as shown in the circles in Figure
1. This is because distinction between relations
in a group is sometimes unclear, and is consid-
ered to strongly depend on the parser implemen-
tation. The final target is seven kinds of combina-
tions of the above four groups: subj, obj, mod, etc,
subj+obj, subj+obj+mod, and all.
The two evaluation measures are similarly cal-
culated for each group and combination, and
shown in Figure 5. Although subjects, objects,
and their combination are widely used contextual
information, the performances for subj and obj
categories, as well as their combination subj+obj,
were relatively poor. On the contrary, the re-
sult clearly shows the importance of modification,
which alone is even better than widely adopted
subj+obj. The ?stabilization effect? of combina-
tions observed in the previous experiment is also
confirmed here as well.
Because the size of the co-occurrence data
varies from one category to another, we conducted
another experiment to verify that the superiority
of the modification category is simply due to the
difference in the quality (content) of the group,
not the quantity (size). We randomly extracted
100,000 pairs from each of mod and subj+obj cat-
egories to cancel out the quantity difference and
compared the performance by calculating aver-
aged DR and CC of ten trials. The result showed
that, while the overall performances substantially
decreased due to the size reduction, the relation
between groups was preserved before and after the
extraction throughout all of the three corpora, al-
though the detailed result is not shown due to the
space limitation. This means that what essentially
contributes to the performance is not the size of
the modification category but its content.
5.4 Modification Selection
As the previous experiment shows that modifica-
tions have the biggest significance of all the depen-
dency relationship, we further investigated what
kind of modifications is useful for the purpose. To
do this, we broke down the mod group into these
five categories according to modifying word?s cat-
egory: (1) detmod, when the GR label is ?det-
358
54.0%
56.0%
58.0%
60.0%
62.0%
64.0%
66.0%
68.0%
dis
cr
im
ina
tio
n 
ra
te
 
(DR
)a
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
correlation
 coefficient
 (CC))
DR
CC
subj obj mod etc subj
obj
subj
obj
mod
all
(1) WSJ
54.0%
56.0%
58.0%
60.0%
62.0%
64.0%
66.0%
68.0%
dis
cr
im
ina
tio
n 
ra
te
 
(DR
)a
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
correlation
 coefficient
 (CC))
DR
CC
subj obj mod etc subj
obj
subj
obj
mod
all
(2) BROWN
54.0%
56.0%
58.0%
60.0%
62.0%
64.0%
66.0%
68.0%
70.0%
dis
cr
im
in
at
ion
 
ra
te
 
(DR
)a
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
0.18
0.20
correlation
 coefficient
 (CC))
DR
CC
subj obj mod etc subj
obj
subj
obj
mod
all
(3) WB
Figure 5: Dependency selection performances
Discrimination rate (DR) and correlation coefficient (CC)
for (1) Wall Street Journal corpus, (2) Brown Corpus, and
(3) WordBank.
50.0%
52.0%
54.0%
56.0%
58.0%
60.0%
62.0%
64.0%
66.0%
dis
cr
im
ina
tio
n 
ra
te
 
(DR
)a
0.00
0.02
0.04
0.06
0.08
0.10
0.12
correlation
 coefficient
 (CC))
DR
CC
detmod
ncmod-n
ncmod-j
ncmod-p
etc all
(1) WSJ
50.0%
52.0%
54.0%
56.0%
58.0%
60.0%
62.0%
64.0%
66.0%
dis
cr
im
ina
tio
n 
ra
te
 
(DR
)a
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
correlation
 coefficient
 (CC))
DR
CC
detmod
ncmod-n
ncmod-j
ncmod-p
etc all
(2) BROWN
CC
 = -0.018
57.0%
59.0%
61.0%
63.0%
65.0%
67.0%
dis
cr
im
in
at
ion
 
ra
te
 
(DR
)a
0.04
0.06
0.08
0.10
0.12
0.14
0.16
0.18
correlation
 coefficient
 (CC))
DR
CC
detmod
ncmod-n
ncmod-j
ncmod-p
etc all
(3) WB
Figure 6: Modification selection performances
Discrimination rate (DR) and correlation coefficient (CC)
for (1) Wall Street Journal corpus, (2) Brown Corpus, and
(3) WordBank.
359
mod?, i.e., the modifying word is a determiner, (2)
ncmod-n, when the GR label is ?ncmod? and the
modifying word is a noun, (3) ncmod-j, when the
GR label is ?ncmod? and the modifying word is an
adjective or number, (4) ncmod-p, when the GR
label is ?ncmod? and the modification is through a
preposition (e.g. ?state? and ?affairs? in ?state of
affairs?), and (5) etc (others).
The performances for each modification cate-
gory are evaluated and shown in Figure 6. Al-
though some individual modification categories
such as detmod and ncmod-j outperform other cat-
egories in some cases, the overall observation is
that all the modification categories contribute to
synonym acquisition to some extent, and the ef-
fect of individual categories are accumulative. We
therefore conclude that the main contributing fac-
tor on utilizing modification relationship in syn-
onym acquisition isn?t the type of modification,
but the diversity of the relations.
6 Conclusion
In this study, we experimentally investigated the
impact of contextual information selection, by ex-
tracting three kinds of contextual information ?
dependency, sentence co-occurrence, and proxim-
ity ? from three different corpora. The acqui-
sition result was evaluated using two evaluation
measures, DR and CC using the existing thesaurus
WordNet. We showed that while dependency and
proximity perform relatively well by themselves,
combination of two or more kinds of contextual
information, even with the poorly performing sen-
tence co-occurrence, gives more stable result. The
selection should be chosen considering the trade-
off between computational complexity and overall
performance stability. We also showed that modi-
fication has the greatest contribution to the acqui-
sition of all the dependency relations, even greater
than the widely adopted subject-object combina-
tion. It is also shown that all the modification cate-
gories contribute to the acquisition to some extent.
Because we limited the target to nouns, the re-
sult might be specific to nouns, but the same exper-
imental framework is applicable to any other cate-
gories of words. Although the result also shows
the possibility that the bigger the corpus is, the
better the performance will be, the contents and
size of the corpora we used are diverse, so their
relationship, including the effect of the window ra-
dius, should be examined as the future work.
References
Marco Baroni and Sabrina Bisi 2004. Using cooccur-
rence statistics and the web to discover synonyms
in a technical language. Proc. of the Fourth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2004).
Ted Briscoe and John Carroll. 2002. Robust Accu-
rate Statistical Annotation of General Text. Proc. of
the Third International Conference on Language Re-
sources and Evaluation (LREC 2002), 1499?1504.
Ted Briscoe, John Carroll, Jonathan Graham and Ann
Copestake 2002. Relational evaluation schemes.
Proc. of the Beyond PARSEVAL Workshop at the
Third International Conference on Language Re-
sources and Evaluation, 4?8.
Scott Deerwester, et al 1990. Indexing by Latent Se-
mantic Analysis. Journal of the American Society
for Information Science, 41(6):391?407.
Christiane Fellbaum. 1998. WordNet: an electronic
lexical database. MIT Press.
Masato Hagiwara, Yasuhiro Ogawa, Katsuhiko
Toyama. 2005. PLSI Utilization for Automatic
Thesaurus Construction. Proc. of The Second In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-05), 334?345.
Zellig Harris. 1985. Distributional Structure. Jerrold
J. Katz (ed.) The Philosophy of Linguistics. Oxford
University Press. 26?47.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. Proc. of the 28th An-
nual Meeting of the ACL, 268?275.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Indexing. Proc. of the 22nd International Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR ?99), 50?57.
Kazuhide Kojima, Hirokazu Watabe, and Tsukasa
Kawaoka. 2004. Existence and Application of
Common Threshold of the Degree of Association.
Proc. of the Forum on Information Technology
(FIT2004) F-003.
Collins. 2002. Collins Cobuild Mld Major New Edi-
tion CD-ROM. HarperCollins Publishers.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. Proc. of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Compu-
tational linguistics (COLING-ACL ?98), 786?774.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2):313?330.
Makoto Nagao (ed.). 1996. Shizengengoshori.
The Iwanami Software Science Series 15, Iwanami
Shoten Publishers.
360
Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 1?6,
Columbus, June 2008. c?2008 Association for Computational Linguistics
A Supervised Learning Approach to Automatic Synonym Identification
based on Distributional Features
Masato Hagiwara
Graduate School of Information Science
Nagoya University
Furo-cho, Chikusa-ku, Nagoya 464-8603, JAPAN
hagiwara@kl.i.is.nagoya-u.ac.jp
Abstract
Distributional similarity has been widely used
to capture the semantic relatedness of words in
many NLP tasks. However, various parame-
ters such as similarity measures must be hand-
tuned to make it work effectively. Instead, we
propose a novel approach to synonym iden-
tification based on supervised learning and
distributional features, which correspond to
the commonality of individual context types
shared by word pairs. Considering the inte-
gration with pattern-based features, we have
built and compared five synonym classifiers.
The evaluation experiment has shown a dra-
matic performance increase of over 120% on
the F-1 measure basis, compared to the con-
ventional similarity-based classification. On
the other hand, the pattern-based features have
appeared almost redundant.
1 Introduction
Semantic similarity of words is one of the most im-
portant lexical knowledge for NLP tasks including
word sense disambiguation and automatic thesaurus
construction. To measure the semantic relatedness
of words, a concept called distributional similarity
has been widely used. Distributional similarity rep-
resents the relatedness of two words by the common-
ality of contexts the words share, based on the distri-
butional hypothesis (Harris, 1985), which states that
semantically similar words share similar contexts.
A number of researches which utilized distri-
butional similarity have been conducted, including
(Hindle, 1990; Lin, 1998; Geffet and Dagan, 2004)
and many others. Although they have been success-
ful in acquiring related words, various parameters
such as similarity measures and weighting are in-
volved. As Weeds et al (2004) pointed out, ?it is
not at all obvious that one universally best measure
exists for all application,? thus they must be tuned by
hand in an ad-hoc manner. The fact that no theoretic
basis is given is making the matter more difficult.
On the other hand, if we pay attention to lexical
knowledge acquisition in general, a variety of sys-
tems which utilized syntactic patterns are found in
the literature. In her landmark paper in the field,
Hearst (1992) utilized syntactic patterns such as
?such X as Y? and ?Y and other X,? and extracted
hypernym/hyponym relation of X and Y. Roark and
Charniak (1998) applied this idea to extraction of
words which belong to the same categories, utiliz-
ing syntactic relations such as conjunctions and ap-
positives. What is worth attention here is that super-
vised machine learning is easily incorporated with
syntactic patterns. For example, Snow et al (2004)
further extended Hearst?s idea and built hypernym
classifiers based on machine learning and syntactic
pattern-based features, with a considerable success.
These two independent approaches, distributional
similarity and syntactic patterns, were finally inte-
grated by Mirkin et al (2006). Although they re-
ported that their system successfully improved the
performance, it did not achieve a complete integra-
tion and was still relying on an independent mod-
ule to compute the similarity. This configuration in-
herits a large portion of drawbacks of the similarity-
based approach mentioned above. To achieve a full
integration of both approaches, we suppose that re-
1
formalization of similarity-based approach would
be essential, as pattern-based approach is enhanced
with the supervised machine learning.
In this paper, we propose a novel approach to
automatic synonym identification based on super-
vised learning technique. Firstly, we re-formalize
synonym acquisition as a classification problem:
one which classifies word pairs into synonym/non-
synonym classes, without depending on a single
value of distributional similarity. Instead, classi-
fication is done using a set of distributional fea-
tures, which correspond to the degree of common-
ality of individual context types shared by word
pairs. This formalization also enables to incorporate
pattern-based features, and we finally build five clas-
sifiers based on distributional and/or pattern-based
features. In the experiment, their performances are
compared in terms of synonym acquisition precision
and recall, and the differences of actually acquired
synonyms are to be clarified.
The rest of this paper is organized as follows: in
Sections 2 and 3, distributional and pattern-based
features are defined, along with the extraction meth-
ods. Using the features, in Section 4 we build five
types of synonym classifiers, and compare their per-
formances in Section 5. Section 6 concludes this
paper, mentioning the future direction of this study.
2 Distributional Features
In this section, we firstly describe how we extract
contexts from corpora and then how distributional
features are constructed for word pairs.
2.1 Context Extraction
We adopted dependency structure as the context of
words since it is the most widely used and well-
performing contextual information in the past stud-
ies (Ruge, 1997; Lin, 1998). In this paper the sophis-
ticated parser RASP Toolkit 2 (Briscoe et al, 2006)
was utilized to extract this kind of word relations.
We use the following example for illustration pur-
poses: The library has a large collection of classic
books by such authors as Herrick and Shakespeare.
RASP outputs the extracted dependency structure as
n-ary relations as follows:
(ncsubj have library _)
(dobj have collection)
(det collection a)
(ncmod _ collection large)
(iobj collection of)
(dobj of book)
(ncmod _ book by)
(dobj by author)
(det author such)
(ncmod _ author as)
... ,
whose graphical representation is shown in Figure 1.
While the RASP outputs are n-ary relations in
general, what we need here is co-occurrences of
words and contexts, so we extract the set of co-
occurrences of stemmed words and contexts by tak-
ing out the target word from the relation and replac-
ing the slot by an asterisk ?*?:
library - (ncsubj have * _)
library - (det * The)
collection - (dobj have *)
collection - (det * a)
collection - (ncmod _ * large)
collection - (iobj * of)
book - (dobj of *)
book - (ncmod _ * by)
book - (ncmod _ * classic)
author - (dobj by *)
author - (det * such)
...
Summing all these up produces the raw co-
occurrence count N(w, c) of the word w and the
context c. In the following, the set of context types
co-occurring with the word w is denoted as C(w),
i.e., C(w) = {c|N(w, c) > 0}.
2.2 Feature Construction
Using the co-occurrences extracted above, we define
distributional features fDj (w1, w2) for the word pair
(w1, w2). The feature value fDj is determined so that
it represents the degree of commonality of the con-
text cj shared by the word pair. We adopted point-
wise total correlation, one of the generalizations of
pointwise mutual information, as the feature value:
fDj (w1, w2) = log
P (w1, w2, cj)
P (w1)P (w2)P (cj)
. (1)
The advantage of this feature construction is that,
given the independence assumption between the
words w1 and w2, the feature value is easily calcu-
lated as the simple sum of two corresponding point-
wise mutual information weights as:
fDj (w1, w2) = PMI(w1, cj) + PMI(w2, cj), (2)
2
The library has a large collection of classic books by such authors as Herrick and Shakespeare. 
ncsubj
dobj
det 
ncmod iobj
dobj ncmod dobj
det 
ncmod
dobj
conjconjncmod 
det 
(dobj)
(dobj)
Figure 1: Dependency structure of the example sentence, along with conjunction shortcuts (dotted lines).
where the value of PMI, which is also the weights
wgt(wi, cj) assigned for distributional similarity, is
calculated as:
wgt(wi, cj) = PMI(wi, cj) = log
P (wi, cj)
P (wi)P (cj)
. (3)
There are three things to note here: when
N(wi, cj) = 0 and PMI cannot be defined, then
we define wgt(wi, cj) = 0. Also, because it has
been shown (Curran and Moens, 2002) that negative
PMI values worsen the distributional similarity per-
formance, we bound PMI so that wgt(wi, cj) = 0
if PMI(wi, cj) < 0. Finally, the feature value
fDj (w1, w2) is defined as shown in Equation (2) only
when the context cj co-occurs with both w1 and w2.
In other words, fDj (w1, w2) = 0 if PMI(w1, cj) =
0 and/or PMI(w2, cj) = 0.
3 Pattern-based Features
This section describes the other type of features, ex-
tracted from syntactic patterns in sentences.
3.1 Syntactic Pattern Extraction
We define syntactic patterns based on dependency
structure of sentences. Following Snow et al
(2004)?s definition, the syntactic pattern of words
w1, w2 is defined as the concatenation of the words
and relations which are on the dependency path from
w1 to w2, not including w1 and w2 themselves.
The syntactic pattern of word authors
and books in Figure 1 is, for example,
dobj:by:ncmod, while that of authors and Her-
rick is ncmod-of:as:dobj-of:and:conj-of.
Notice that, although not shown in the figure,
every relation has a reverse edge as its counterpart,
with the direction opposite and the postfix ?-of?
attached to the label. This allows to follow the
relations in reverse, increasing the flexibility and
expressive power of patterns.
In the experiment, we limited the maximum
length of syntactic path to five, meaning that word
pairs having six or more relations in between were
disregarded. Also, we considered conjunction short-
cuts to capture the lexical relations more precisely,
following Snow et al (2004). This modification cuts
short the conj edges when nouns are connected by
conjunctions such as and and or. After this shortcut,
the syntactic pattern between authors and Herrick is
ncmod-of:as:dobj-of, and that of Herrick and
Shakespeare is conj-and, which is a newly intro-
duced special symmetric relation, indicating that the
nouns are mutually conjunctional.
3.2 Feature Construction
After the corpus is analyzed and patterns are ex-
tracted, the pattern based feature fPk (w1, w2), which
corresponds to the syntactic pattern pk, is defined
as the conditional probability of observing pk given
that the pair (w1, w2) is observed. This definition is
similar to (Mirkin et al, 2006) and is calculated as:
fPk (w1, w2) = P (pk|w1, w2) =
N(w1, w2, pk)
N(w1, w2)
. (4)
4 Synonym Classifiers
Now that we have all the features to consider, we
construct the following five classifiers. This section
gives the construction detail of the classifiers and
corresponding feature vectors.
Distributional Similarity (DSIM) DSIM classi-
fier is simple acquisition relying only on distribu-
tional similarity, not on supervised learning. Simi-
lar to conventional methods, distributional similar-
ity between words w1 and w2, sim(w1, w2), is cal-
culated for each word pair using Jaccard coefficient:
?
c?C(w1)?C(w2) min(wgt(w1, c),wgt(w2, c))
?
c?C(w1)?C(w2) max(wgt(w1, c),wgt(w2, c))
,
3
considering the preliminary experimental result. A
threshold is set on the similarity and classification is
performed based on whether the similarity is above
or below of the given threshold. How to optimally
set this threshold is described later in Section 5.1.
Distributional Features (DFEAT) DFEAT clas-
sifier does not rely on the conventional distributional
similarity and instead uses the distributional features
described in Section 2. The feature vector ~v of a
word pair (w1, w2) is constructed as:
~v = (fD1 , ..., fDM ). (5)
Pattern-based Features (PAT) This classifier
PAT uses only pattern-based features, essentially the
same as the classifier of Snow et al (2004). The
feature vector is:
~v = (fP1 , ..., fPK). (6)
Distributional Similarity and Pattern-based Fea-
tures (DSIM-PAT) DSIM-PAT uses the distribu-
tional similarity of pairs as a feature, in addition
to pattern-based features. This classifier is essen-
tially the same as the integration method proposed
by Mirkin et al (2006). Letting fS = sim(w1, w2),
the feature vector is:
~v = (fS , fP1 , ..., fPK). (7)
Distributional and Pattern-based Features
(DFEAT-PAT) The last classifier, DFEAT-PAT,
truly integrates both distributional and pattern-based
features. The feature vector is constructed by
replacing the fS component of DSIM-PAT with
distributional features fD1 , ..., fDM as:
~v = (fD1 , ..., fDM , fP1 , ..., fPK). (8)
5 Experiments
Finally, this section describes the experimental set-
ting and the comparison of synonym classifiers.
5.1 Experimental Settings
Corpus and Preprocessing As for the corpus,
New York Times section (1994) of English Giga-
word 1, consisting of approx. 46,000 documents,
1http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2003T05
922,000 sentences, and 30 million words, was an-
alyzed to obtain word-context co-occurrences.
This can yield 10,000 or more context types, thus
we applied feature selection and reduced the dimen-
sionality. Firstly, we simply applied frequency cut-
off to filter out any words and contexts with low
frequency. More specifically, any words w such
that
?
c N(w, c) < ?f and any contexts c such that
?
w N(w, c) < ?f , with ?f = 5, were removed. DF
(document frequency) thresholding is then applied,
and context types with the lowest values of DF were
removed until 10% of the original contexts were left.
We verified through a preliminary experiment that
this feature selection keeps the performance loss at
minimum. As a result, this process left a total of
8,558 context types, or feature dimensionality.
The feature selection was also applied to pattern-
based features to avoid high sparseness ? only syn-
tactic patterns which occurred more than or equal to
7 times were used. The number of syntactic pattern
types left after this process is 17,964.
Supervised Learning Training and test sets were
created as follows: firstly, the nouns listed in the
Longman Defining Vocabulary (LDV) 2 were cho-
sen as the target words of classification. Then, all
the LDV pairs which co-occur more than or equal
to 3 times with any of the syntactic patterns, i.e.,
{(w1, w2)|w1, w2 ? LDV,
?
p N(w1, w2, p) ? 3}
were classified into synonym/non-synonym classes
as mentioned in Section 5.2. All the positive-marked
pair, as well as randomly chosen 1 out of 5 negative-
marked pairs, were collected as the example set E.
This random selection is to avoid extreme bias to-
ward the negative examples. The example set E
ended up with 2,148 positive and 13,855 negative
examples, with their ratio being approx. 6.45.
The example set E was then divided into five par-
titions to conduct five-fold cross validation, of which
four partitions were used for learning and the one for
testing. SVMlight was adopted for machine learn-
ing, and RBF as the kernel. The parameters, i.e.,
the similarity threshold of DSIM classifier, gamma
parameter of RBF kernel, and the cost-factor j of
SVM, i.e., the ratio by which training errors on pos-
itive examples outweight errors on negative ones,
2http://www.cs.utexas.edu/users/kbarker/working notes/
ldoce-vocab.html
4
Table 1: Performance comparison of synonym classifiers
Classifier Precision Recall F-1
DSIM 33.13% 49.71% 39.76%
DFEAT 95.25% 82.31% 88.30%
PAT 23.86% 45.17% 31.22%
DSIM-PAT 30.62% 51.34% 38.36%
DFEAT-PAT 95.37% 82.31% 88.36%
were optimized using one of the 5-fold cross valida-
tion train-test pair on the basis of F-1 measure. The
performance was evaluated for the other four train-
test pairs and the average values were recorded.
5.2 Evaluation
To test whether or not a given word pair (w1, w2)
is a synonym pair, three existing thesauri were con-
sulted: Roget?s Thesaurus (Roget, 1995), Collins
COBUILD Thesaurus (Collins, 2002), and WordNet
(Fellbaum, 1998). The union of synonyms obtained
when the head word is looked up as a noun is used
as the answer set, except for words marked as ?id-
iom,? ?informal,? ?slang? and phrases comprised of
two or more words. The pair (w1, w2) is marked as
synonyms if and only if w2 is contained in the an-
swer set of w1, or w1 is contained in that of w2.
5.3 Classifier Performance
The performances, i.e., precision, recall, and F-1
measure, of the five classifiers were evaluated and
shown in Table 1. First of all, we observed a drastic
improvement of DFEAT over DSIM ? over 120%
increase of F-1 measure. When combined with
pattern-based features, DSIM-PAT showed a slight
recall increase compared to DSIM, partially recon-
firming the favorable integration result of (Mirkin et
al., 2006). However, the combination DFEAT-PAT
showed little change, meaning that the discrimina-
tive ability of DFEAT was so high that pattern-based
features were almost redundant. To note, the perfor-
mance of PAT was the lowest, reflecting the fact that
synonym pairs rarely occur in the same sentence,
making the identification using only syntactic pat-
tern clues even more difficult.
The reason of the drastic improvement is that, as
far as we speculate, the supervised learning may
have favorably worked to cause the same effect as
automatic feature selection technique. Features with
high discriminative power may have been automat-
ically promoted. In the distributional similarity set-
ting, in contrast, the contributions of context types
are uniformly fixed. In order to elucidate what is
happening in this situation, the investigations on ma-
chine learning settings, as well as algorithms other
than SVM should be conducted as the future work.
5.4 Acquired Synonyms
In the second part of this experiment, we further in-
vestigated what kind of synonyms were actually ac-
quired by the classifiers. The targets are not LDV,
but all of 27,501 unique nouns appeared in the cor-
pus, because we cannot rule out the possibility that
the high performance seen in the previous exper-
iment was simply due to the rather limited target
word settings. The rest of the experimental setting
was almost the same as the previous one, except that
the construction of training set is rather artificial ?
we used all of the 18,102 positive LDV pairs and
randomly chosen 20,000 negative LDV pairs.
Table 2 lists the acquired synonyms of video and
program. The results of DSIM and DFEAT are or-
dered by distributional similarity and the value of
decision function of SVM, respectively. Notice that
because neither word is included in LDV, all the
pairs of the query and the words listed in the table
are guaranteed to be excluded from the training set.
The result shows the superiority of DFEAT over
DSIM. The irrelevant words (marked by ?*? by
human judgement) seen in the DSIM list are de-
moted and replaced with more relevant words in the
DFEAT list. We observed the same trend for lower
ranked words and other query words.
6 Conclusion and Future Direction
In this paper, we proposed a novel approach to au-
tomatic synonym identification based on supervised
machine learning and distributional features. For
this purpose, we re-formalized synonym acquisition
as a classification problem, and constructed the fea-
tures as the total correlation of pairs and contexts.
Since this formalization allows to integrate pattern-
based features in a seamless way, we built five clas-
sifiers based on distributional and/or pattern-based
features. The result was promising, achieving more
than 120% increase over conventional DSIM classi-
5
Table 2: Acquired synonyms of video and program
For query word: video
Rank DSIM DFEAT
1 computer computer
2 television television
3 movie multimedia
4 film communication
5 food* entertainment
6 multimedia advertisement
7 drug* food*
8 entertainment recording
9 music portrait
10 radio movie
For query word: program
Rank DSIM DFEAT
1 system project
2 plan system
3 project unit
4 service status
5 policy schedule
6 effort* organization*
7 bill* activity*
8 company* plan
9 operation scheme
10 organization* policy
fier. Pattern-based features were partially effective
when combined with DSIM whereas with DFEAT
they were simply redundant.
The impact of this study is that it makes unneces-
sary to carefully choose similarity measures such as
Jaccard?s ? instead, features can be directly input
to supervised learning right after their construction.
There are still a great deal of issues to address as the
current approach is only in its infancy. For example,
the formalization of distributional features requires
further investigation. Although we adopted total
correlation this time, there can be some other con-
struction methods which show higher performance.
Still, we believe that this is one of the best ac-
quisition performances achieved ever and will be an
important step to truly practical lexical knowledge
acquisition. Setting our future direction on the com-
pletely automatic construction of reliable thesaurus
or ontology, the approach proposed here is to be ap-
plied to and integrated with various kinds of lexical
knowledge acquisition methods in the future.
Acknowledgments
The author would like to thank Assoc. Prof. Katsu-
hiko Toyama and Assis. Prof. Yasuhiro Ogawa for
their kind supervision and advice.
References
Ted Briscoe, John Carroll and Rebecca Watson. 2006.
The Second Release of the RASP System. Proc. of the
COLING/ACL 06 Interactive Presentation Sessions,
77?80.
Collins. 2002. Collins Cobuild Major New Edition CD-
ROM. HarperCollins Publishers.
James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Workshop
on Unsupervised Lexical Acquisition. Proc. of ACL
SIGLEX, 231?238.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database, MIT Press.
Maayan Geffet and Ido Dagan. 2004. Feature Vector
Quality and Distributional Similarity. Proc. of COL-
ING 04, 247?253.
Zellig Harris. 1985. Distributional Structure. Jerrold J.
Katz (ed.) The Philosophy of Linguistics. Oxford Uni-
versity Press, 26?47.
Marti A. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. Proc. of COLING
92, 539?545.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. Proc. of ACL 90, 268?
275.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. Proc. of COLING/ACL 98, 786?774.
Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006.
Integrating pattern-based and distributional similarity
methods for lexical entailment acquisition. Proc. of
COLING/ACL 06, 579?586.
Brian Roark and Eugene Charniak. 1998. Noun
phrase cooccurrence statistics for semi-automatic lex-
icon construction. Proc. of COLING/ACL 98, 1110?
1116.
Roget. 1995. Roget?s II: The New Thesaurus, 3rd ed.
Houghton Mifflin.
Gerda Ruge. 1997. Automatic detection of thesaurus re-
lations for information retrieval applications. Founda-
tions of Computer Science: Potential - Theory - Cogni-
tion, LNCS, Volume 1337, 499?506, Springer Verlag.
Rion Snow, Daniel Jurafsly, and Andrew Y. Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. Advances in Neural Information Process-
ing Systems (NIPS) 17.
Julie Weeds, David Weir and Diana McCarthy. 2004.
Characterising Measures of Lexical Distributional
Similarity. Proc. of COLING 04, 1015?1021.
6
PLSI Utilization for Automatic Thesaurus Construction
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko Toyama
Graduate School of Information Science, Nagoya University,
Furo-cho, Chikusa-ku, Nagoya, JAPAN 464-8603
{hagiwara, yasuhiro, toyama}@kl.i.is.nagoya-u.ac.jp
Abstract. When acquiring synonyms from large corpora, it is important to deal
not only with such surface information as the context of the words but also their
latent semantics. This paper describes how to utilize a latent semantic model PLSI
to acquire synonyms automatically from large corpora. PLSI has been shown
to achieve a better performance than conventional methods such as tf?idf and
LSI, making it applicable to automatic thesaurus construction. Also, various PLSI
techniques have been shown to be effective including: (1) use of Skew Divergence
as a distance/similarity measure; (2) removal of words with low frequencies, and
(3) multiple executions of PLSI and integration of the results.
1 Introduction
Thesauri, dictionaries in which words are arranged according to meaning, are one of the
most useful linguistic sources, having a broad range of applications, such as information
retrieval and natural language understanding. Various thesauri have been constructed
so far, including WordNet [6] and Bunruigoihyo [14]. Conventional thesauri, however,
have largely been compiled by groups of language experts, making the construction
and maintenance cost very high. It is also difficult to build a domain-specific thesaurus
flexibly. Thus it is necessary to construct thesauri automatically using computers.
Many studies have been done for automatic thesaurus construction. In doing so,
synonym acquisition is one of the most important techniques, although a thesaurus gen-
erally includes other relationships than synonyms (e.g., hypernyms and hyponyms). To
acquire synonyms automatically, contextual features of words, such as co-occurrence
and modification are extracted from large corpora and often used. Hindle [7], for ex-
ample, extracted verb-noun relationships of subjects/objects and their predicates from a
corpus and proposed a method to calculate similarity of two words based on their mu-
tual information. Although methods based on such raw co-occurrences are simple yet
effective, in a naive implementation some problems arise: namely, noises and sparse-
ness. Being a collection of raw linguistic data, a corpus generally contains meaningless
information, i.e., noises. Also, co-occurrence data extracted from corpora are often very
sparse, making them inappropriate for similarity calculation, which is also known as the
?zero frequency problem.? Therefore, not only surface information but also latent se-
mantics should be considered when acquiring synonyms from large corpora.
Several latent semantic models have been proposed so far, mainly for information
retrieval and document indexing. The most commonly used and prominent ones are La-
tent Semantic Indexing (LSI) [5] and Probabilistic LSI (PLSI) [8]. LSI is a geometric
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 334?345, 2005.
c? Springer-Verlag Berlin Heidelberg 2005
PLSI Utilization for Automatic Thesaurus Construction 335
model based on the vector space model. It utilizes singular value decomposition of the
co-occurrence matrix, an operation similar to principal component analysis, to auto-
matically extract major components that contribute to the indexing of documents. It can
alleviate the noise and sparseness problems by a dimensionality reduction operation,
that is, by removing components with low contributions to the indexing. However, the
model lacks firm, theoretical basis [9] and the optimality of inverse document frequency
(idf) metric, which is commonly used to weight elements, has yet to be shown [13].
On the contrary, PLSI, proposed by Hofmann [8], is a probabilistic version of LSI,
where it is formalized that documents and terms co-occur through a latent variable.
PLSI puts no assumptions on distributions of documents or terms, while LSI performs
optimal model fitting, assuming that documents and terms are under Gaussian distribu-
tion [9]. Moreover, ad hoc weighting such as idf is not necessary for PLSI, although it
is for LSI, and it is shown experimentally to outperform the former model [8].
This study applies the PLSI model to the automatic acquisition of synonyms by es-
timating each word?s latent meanings. First, a number of verb-noun pairs were collected
from a large corpus using heuristic rules. This operation is based on the assumption that
semantically similar words share similar contexts, which was also employed in Hindle?s
work [7] and has been shown to be considerably plausible. Secondly, the co-occurrences
obtained in this way were fit into the PLSI model, and the probability distribution of
latent classes was calculated for each noun. Finally, similarity for each pair of nouns
can be calculated by measuring the distances or the similarity between two probability
distributions using an appropriate distance/similarity measure. We then evaluated and
discussed the results using two evaluation criteria, discrimination rates and scores.
This paper also discusses basic techniques when applying PLSI to the automatic
acquisition of synonyms. In particular, the following are discussed from methodological
and experimental views: (1) choice of distance/similarity measures between probability
distributions; (2) filtering words according to their frequencies of occurrence; and (3)
multiple executions of PLSI and integration of the results.
This paper is organized as follows: in Sect. 2 a brief explanation of the PLSI model
and calculation is provided, and Sect. 3 outlines our approach. Sect. 4 shows the results
of comparative experiments and basic techniques. Sect. 5 concludes this paper.
2 The PLSI Model
This section provides a brief explanation of the PLSI model in information retrieval
settings. The PLSI model, which is based on the aspect model, assumes that document
d and term w co-occur through latent class z, as shown in Fig. 1 (a).
The co-occurrence probability of documents and terms is given by:
P (d, w) = P (d)
?
z
P (z|d)P (w|z). (1)
Note that this model can be equivalently rewritten as
P (d, w) =
?
z
P (z)P (d|z)P (w|z), (2)
336 M. Hagiwara, Y. Ogawa, and K. Toyama
d z w
P(d) P(z|d) P(w|z)
d z w
P(z)
P(d|z) P(w|z)
(a) (b)
Fig. 1. PLSI model asymmetric (a) and symmetric (b) parameterization
corpus
co-occurrence
(v, c, n)
(eat, obj, lunch)
(eat, obj, hamburger)
(have, obj, breakfast)
???
PLSI model
z n
P(v) P(z|v) P(n|z)
latent class
noun
(v,c)
verb+case
0.0
0.1
0.2
0.3
0.4
0.5
0.6
1 2 3 4 5 6 7 8 9 10
lunch
0 .0
0 .1
0 .2
0 .3
0 .4
0 .5
0 .6
1 2 3 4 5 6 7 8 9 10
breakfast
),( 21 wwsim
latent class distribution
similarity calculation
P(z|n)
P(z|n)
Fig. 2. Outline of our approach
whose graphical model representation is shown in Fig. 1 (b). This is a symmetric pa-
rameterization with respect to documents and terms. The latter parameterization is used
in the experiment section because of its simple implementation.
Theoretically, probabilities P (d), P (z|d), P (w|z) are determined by maximum
likelihood estimation, that is, by maximizing the likelihood of document term
co-occurrence:
L =
?
d,w
N(d, w) log P (d, w), (3)
where N(d, w) is the frequency document d and term w co-occur.
While the co-occurrence of document d and term w in the corpora can be observed
directly, the contribution of latent class z cannot be directly seen in this model. For
the maximum likelihood estimation of this model, the EM algorithm [1], which is used
for the estimation of systems with unobserved (latent) data, is used. The EM algorithm
performs the estimation iteratively, similar to the steepest descent method.
3 Approach
The original PLSI model, as described above, deals with co-occurrences of documents
and terms, but it can also be applied to verbs and nouns in the corpora. In this way, latent
PLSI Utilization for Automatic Thesaurus Construction 337
John gave presents to his colleagues.
John    gave    presents    to    his    colleagues.   
NNP
NP
VBD
VP
NNS TO NNS
NP
PRP$
PPNP
S
[John]    gave    [presents]    to    [his    colleagues]
NP S VP
NPVPVBD
PPVPVBD NPTO PP
(a) Original sentence
(b) Parsing result
(c) Dependency structure
(d) Co-occurrence extraction from dependencies
John    gave
NP S VP
(?give?, subj, ?John?)
gave   presents
VBDVP NP
(?give?, obj, ?present?)
gave   to   his   colleagues
TO PP NP
(?give?, ?to?, ?colleague?)
PPVPVBD
n
NP S VP
v (v, subj, n)
(v, obj, n)
(v, prep, n)
but (v, obj, n) when the verb is ?be? + past participle.
n
NP VP baseVP
v
n
NP PP
prep
PP* VP baseVP
v
Rule 1?
Rule 2?
Rule 3?
(e) Rules for co-occurrence identification
Fig. 3. Co-occurrence extraction
class distribution, which can be interpreted as latent ?meaning? corresponding to each
noun, is obtained. Semantically similar words are then obtained accordingly, because
words with similar meaning have similar distributions. Fig. 2 outlines our approach,
and the following subsections provide the details.
3.1 Extraction of Co-occurrence
We adopt triples (v, c, n) extracted from the corpora as co-occurrences fit into the PLSI
model, where v, c, and n represent a verb, case/preposition, and a noun, respectively.
The relationships between nouns and verbs, expressed by c, include case relation (sub-
ject and object) as well as what we call here ?prepositional relation,? that is, a co-
occurrence through a preposition. Take the following sentence for example:
John gave presents to his colleagues.
First, the phrase structure (Fig. 3(b)) is obtained by parsing the original sentence
(Fig. 3(a)). The resulting tree is then used to derive the dependency structure (Fig. 3(c)),
using Collins? method [4]. Note that dependencies in baseNPs (i.e., noun phrases that
do not contain NPs as their child constituents, shown as the groups of words enclosed
by square brackets in Fig. 3(c)), are ignored. Also, we introduced baseVPs, that is,
sequences of verbs 1, modals (MD), or adverbs (RB), of which the last word must be
a verb. BaseVPs simplify the handling of sequences of verbs such as ?might not be?
1 Ones expressed as VB, VBD, VBG, VBN, VBP, and VBZ by the Penn Treebank POS tag set
[15].
338 M. Hagiwara, Y. Ogawa, and K. Toyama
and ?is always complaining.? The last word of a baseVP represents the entire baseVP
to which it belongs. That is, all the dependencies directed to words in a baseVP are
redirected to the last verb of the baseVP.
Finally, co-occurrences are extracted and identified by matching the dependency
patterns and the heuristic rules for extraction, which are all listed in Fig. 3 (e). For
example, since the label of the dependency ?John? ??gave? is ?NP S VP?, the noun
?John? is identified as the subject of the verb ?gave? (Fig. 3(d)). Likewise, the de-
pendencies ?presents???gave? and ?his colleagues???to???gave? are identified as a
verb-object relation and prepositional relation through ?to?.
A simple experiment was conducted to test the effectiveness of this extraction
method, using the corpus and the parser mentioned in the experiment section. Co-
occurrence extraction was performed for the 50 sentences randomly extracted from
the corpus, and precision and recall turned out to be 88.6% and 78.1%, respectively. In
this context, precision is more important than recall because of the substantial size of
the corpus, and some of the extraction errors result from parsing error caused by the
parser, whose precision is claimed to be around 90% [2]. Therefore, we conclude that
this method and its performance are sufficient for our purpose.
3.2 Applying PLSI to Extracted Co-occurence Data
While the PLSI model deals with dyadic data (d, w) of document d and term w, the co-
occurrences obtained by our method are triples (v, c, n) of a verb v, a case/preposition
c, and a noun n. To convert these triples into dyadic data (pairs), verb v and case/
preposition c are paired as (v, c) and considered a new ?virtual? verb v. This enables it
to handle the triples as the co-occurrence (v, n) of verb v and noun n to which the PLSI
model becomes applicable. Pairing verb v and case/preposition c also has a benefit that
such phrasal verbs as ?look for? or ?get to? can be naturally treated as a single verb.
After the application of PLSI, we obtain probabilities P (z), P (v|z), and P (n|z).
Using Bayes theorem, we then obtain P (z|n), which corresponds to the latent class
distribution for each noun. In other words, distribution P (z|n) represents the features
of meaning possessed by noun n. Therefore, we can calculate the similarity between
nouns n1 and n2 by measuring the distance or similarity between the two correspond-
ing distribution, P (z|n1) and P (z|n2), using an appropriate measure. The choice of
measure affects the synonym acquisition results and experiments on comparison of dis-
tance/similarity measures are detailed in Sect. 4.3.
4 Experiments
This section includes the results of comparison experiments and those on the basic PLSI
techniques.
4.1 Conditions
The automatic acquisition of synonyms was conducted according to the method de-
scribed in Sect. 3, using WordBank (190,000 sentences, 5 million words) [3] as a cor-
PLSI Utilization for Automatic Thesaurus Construction 339
pus. Charniak?s parser [2] was used for parsing and TreeTagger [16] for stemming. A
total of 702,879 co-occurrences was extracted by the method described in Sect. 3.1.
When using EM algorithm to implement PLSI, overfitting, which aggravates the
performance of the resultant language model, occasionally occurs. We employed the
tempered EM (TEM) [8] algorithm, instead of a naive one, to avoid this problem. TEM
algorithm is closely related to the deterministic annealing EM (DAEM) algorithm [17],
and helps avoid local extrema by introducing inverse temperature ?. The parameter was
set to ? = 0.86, considering the results of the preliminary experiments.
As the similarity/distance measure and frequency threshold tf , Skew Divergence
(? = 0.99) and tf = 15 were employed in the following experiments in response to the
results from the experiments described in Sects. 4.3 and 4.5. Also, because estimation
by EM algorithm is started from the random parameters and consequently the PLSI
results change every time it is executed, the average performance of the three executions
was recorded, except in Sect. 4.6.
4.2 Measures for Performance
The following two measures, discrimination rate and scores, were employed for the
evaluation of automated synonym acquisition.
Discrimination rate Discrimination rate, originally proposed by Kojima et al [10], is
the rate (percentage) of pairs (w1, w2) whose degree of association between two words
w1, w2 is successfully discriminated by the similarity derived by a method. Kojima
et al dealt with three-level discrimination of a pair of words, that is, highly related
(synonyms or nearly synonymous), moderately related (a certain degree of association),
and unrelated (irrelevant). However, we omitted the moderately related level and limited
the discrimination to two-level: high or none, because of the high cost of preparing a
test set that consists of moderately related pairs.
The calculation of discrimination rate follows these steps: first, two test sets, one
of which consists of highly related word pairs and the other of unrelated ones, were
prepared, as shown in Fig. 4. The similarity between w1 and w2 is then calculated for
each pair (w1, w2) in both test sets via the method under evaluation, and the pair is
labeled highly related when similarity exceeds a given threshold t and unrelated when
the similarity is lower than t. The number of pairs labeled highly related in the highly
related test set and unrelated in the unrelated test set are denoted na and nb, respectively.
The discrimination rate is then given by:
1
2
(
na
Na
+
nb
Nb
)
, (4)
where Na and Nb are the numbers of pairs in highly related and unrelated test sets,
respectively. Since the discrimination rate changes depending on threshold t, maximum
value is adopted by varying t.
We created a highly related test set using the synonyms in WordNet [6]. Pairs in a
unrelated test set were prepared by first choosing two words randomly and then con-
firmed by hand whether the consisting two words are truly irrelevant. The numbers of
pairs in the highly and unrelated test sets are 383 and 1,124, respectively.
340 M. Hagiwara, Y. Ogawa, and K. Toyama
(answer, reply)
(phone, telephone)
(sign, signal)
(concern, worry)
(animal, coffee)
(him, technology)
(track, vote)
(path, youth)
?
?
highly related unrelated
Fig. 4. Test-sets for discrimination rate calcula-
tion
base word: computer
rank synonym sim sim? rel.(p) p ? sim?
1 equipment 0.6 0.3 B(0.5) 0.15
2 machine 0.4 0.2 A(1.0) 0.20
3 Internet 0.4 0.2 B(0.5) 0.10
4 spray 0.4 0.2 C(0.0) 0.00
5 PC 0.2 0.1 A(1.0) 0.10
total 2.0 1.0 0.55
Table 5. Procedure for score calculation
Scores We propose a score which is similar to precision used for information retrieval
evaluation, but different in that it considers the similarity of words. This extension is
based on the notion that the more accurately the degrees of similarity are assigned to
the results of synonym acquisition, the higher the score values should be.
Described in the following, along with Table 5, is the procedure for score calcula-
tion. Table 5 shows the obtained synonyms and their similarity with respect to the base
word ?computer.? Results are obtained by calculating the similarity between the base
word and each noun, and ranking all the nouns in descending order of similarity sim.
The highest five are used for calculations in this example.
The range of similarity varies based on such factors as the employed distance/
similarity measure, which unfavorably affects the score value. To avoid this, the val-
ues of similarity are normalized such that their sum equals one, as shown in the column
sim? in Fig. 5. Next, the relevance of each synonym to the base word is checked and
evaluated manually, giving them three-level grades: highly related (A), moderately re-
lated (B), and unrelated (C), and relevance scores p = 1.0, 0.5, 0.0 are assigned for
each grade, respectively (?rel.(p)? column in Fig. 5). Finally, each relevance score p is
multiplied by corresponding similarity sim? , and the products (the p ? sim? column
in Fig. 5) are totaled and then multiplied by 100 to obtain a score, which is 55 in this
case. In actual experiments, thirty words chosen randomly were adopted as base words,
and the average of the scores of all base words was employed. Although this example
considers only the top five words for simplicity, the top twenty words were used for
evaluation in the following experiments.
4.3 Distance/Similarity Measures of Probability Distribution
The choice of distance measure between two latent class distributions P (z|ni), P (z|nj)
affects the performance of synonym acquisition. Here we focus on the following seven
distance/similarity measures and compare their performance.
? Kullback-Leibler (KL) divergence [12]: KL(p || q) = ?x p(x) log(p(x)/q(x))
? Jensen-Shannon (JS) divergence [12]: JS(p, q) = {KL(p || m)+KL(q || m)}/2,
m = (p + q)/2
? Skew Divergence [11]: s?(p || q) = KL(p || ?q + (1 ? ?)p)
? Euclidean distance: euc(p, q) = ||p ? q||
? L1 distance: L1(p, q) =
?
x |p(x) ? q(x)|
PLSI Utilization for Automatic Thesaurus Construction 341
? Inner product: p ? q =
?
x p(x)q(x)
? Cosine: cos(p, q) = (p ? q)/||p|| ? ||q||
KL divergence is widely used for measuring the distance between two probabil-
ity distributions. However, it has such disadvantages as asymmetricity and zero fre-
quency problem, that is, if there exists x such that p(x) = 0, q(x) = 0, the distance is
not defined. JS divergence, in contrast, is considered the symmetrized KL divergence
and has some favorable properties: it is bounded [12] and does not cause the zero fre-
quency problem. Skew Divergence, which has recently been receiving attention, has
also solved the zero frequency problem by introducing parameter ? and mixing the
two distributions. It has shown that Skew Divergence achieves better performance than
the other measures [11]. The other measures commonly used for calculation of the
similarity/distance of two vectors, namely Euclidean distance, L1 distance (also called
Manhattan Distance), inner product, and cosine, are also included for comparison.
Notice that the first five measures are of distance (the more similar p and q, the lower
value), whereas the others, inner product and cosine, are of similarity (the more similar
p and q, the higher value). We converted distance measure D to a similarity measure
sim by the following expression:
sim(p, q) = exp{??D(p, q)}, (5)
inspired by Mochihashi and Matsumoto [13]. Parameter ? was determined in such a
way that the average of sim doesn?t change with respect to D. Because KL divergence
and Skew Divergence are asymmetric, the average of both directions (e.g. for KL diver-
gence, 12 (KL(p||q) + KL(q||p))) is employed for the evaluation.
Figure 6 shows the performance (discrimination rate and score) for each measure. It
can be seen that Skew Divergence with parameter ? = 0.99 shows the highest perfor-
mance of the seven, with a slight difference to JS divergence. These results, along with
several studies, also show the superiority of Skew Divergence. In contrast, measures for
vectors such as Euclidean distance achieved relatively poor performance compared to
those for probability distributions.
4.4 Word Filtering by Frequencies
It may be difficult to estimate the latent class distributions for words with low frequen-
cies because of a lack of sufficient data. These words can be noises that may degregate
the results of synonym acquisition. Therefore, we consider removing such words with
low frequencies before the execution of PLSI improves the performance. More specif-
ically, we introduced threshold tf on the frequency, and removed nouns ni such that
?
j tf
i
j < tf and verbs vj such that
?
i tf
i
j < tf from the extracted co-occurrences.
The discrimination rate change on varying threshold tf was measured and shown
in Fig. 7 for d = 100, 200, and 300. In every case, the rate increases with a moderate
increase of tf , which shows the effectiveness of the removal of low frequency words.
We consequently fixed tf = 15 in other experiments, although this value may depend
on the corpus size in use.
342 M. Hagiwara, Y. Ogawa, and K. Toyama
50.0%
55.0%
60.0%
65.0%
70.0%
75.0%
80.0%
KL
JS s(0
.99)
s(0
.95)
s(0
.90)
E
uc
.
 dist
.
L1
 dist
.
cosine
inner
 prod
.distance/similarity measure
di
s c
rim
in
a t
io
n
 
ra
te
 
(%
)
5.0
7.0
9.0
11.0
13.0
15.0
17.0
19.0
21.0
23.0
25.0
sco
re
disc. rate
score
Fig. 6. Performances of distance/similarity
measures
70.0%
71.0%
72.0%
73.0%
74.0%
75.0%
76.0%
77.0%
78.0%
0 5 10 15 20 25 30
threshold
di
sc
rim
in
at
io
n
 r
at
e 
(%
)
d=100
d=200
d=300
Fig. 7. Discrimination rate measured by varying
threshold tf
4.5 Comparison Experiments with Conventional Methods
Here the performances of PLSI and the following conventional methods are compared.
In the following, N and M denote the numbers of nouns and verbs, respectively.
? tf: The number of co-occurrence tf ij of noun ni and verb vj is used directly for
similarity calculation. The corresponding vector ni to noun ni is given by:
ni = t[tf i1 tf
i
2 ... tf
i
M ]. (6)
? tf?idf: The vectors given by tf method are weighted by idf. That is,
n?i =
t[tf i1 ? idf1 tf i2 ? idf2 ... tf iM ? idfM ], (7)
where idfj is given by
idfj =
log(N/dfj)
maxk log(N/dfk)
, (8)
using dfj , the number of distinct nouns that co-occur with verb vj .
? tf+LSI: A co-occurrence matrix X is created using vectors ni defined by tf:
X = [n1 n2 ... nN ], (9)
to which LSI is applied.
? tf?idf+LSI : A co-occurrence matrix X? is created using vectors n?i defined by
tf?idf:
X? = [n?1 n
?
2 ... n
?
N ], (10)
to which LSI is applied.
? Hindle?s method: The method described in [7] is used. Whereas he deals only
with subjects and objects as verb-noun co-occurrence, we used all the kinds of
co-occurrence mentioned in Sect. 3.1, including prepositional relations.
PLSI Utilization for Automatic Thesaurus Construction 343
60.0%
62.0%
64.0%
66.0%
68.0%
70.0%
72.0%
74.0%
76.0%
78.0%
number of latent classes
di
s c
rim
in
a t
io
n
 r
a t
e  
(%
)
PLSI
tf
?
idf+LSI
tf+LSI
tf
?
idf
tf
Hindletf
tf
?
idf
Hindle
0 500 1000
5.0
7.0
9.0
11.0
13.0
15.0
17.0
19.0
21.0
23.0
number of latent classes
sc
o
re
Hindle
tf
?
idf
tf
0 500 1000
Fig. 8. Performances of PLSI and conventional methods
The values of discrimination rate and scores are calculated for PLSI as well as the
methods described above, and the results are shown in Fig. 8. Because the number of
latent classes d must be given beforehand for PLSI and LSI, the performances of the
latent semantic models are measured varying d from 50 to 1,000 with a step of 50. The
cosine measure is used for the similarity calculation of tf, tf?idf, tf+LSI, and tf?idf+LSI.
The results reveal that the highest discrimination rate is achieved by PLSI, with the
latent class number of approximately 100, although LSI overtakes with an increase of
d. As for the scores, the performance of PLSI stays on top for almost all the values of
d, strongly suggesting the superiority of PLSI over the conventional method, especially
when d is small, which is often.
The performances of tf and tf+LSI, which are not weighted by idf, are consistently
low regardless of the value of d. PLSI and LSI distinctly behave with respect to d,
especially in the discrimination rate, whose cause require examination and discussion.
4.6 Integration of PLSI Results
In maximum likelihood estimation by EM algorithm, the initial parameters are set to
values chosen randomly, and likelihood is increased by an iterative process. Therefore,
the results are generally local extrema, not global, and they vary every execution, which
is unfavorable. To solve this problem, we propose to execute PLSI several times and
integrate the results to obtain a single one.
To achieve this, PLSI is executed several times for the same co-occurrence data
obtained via the method described in Sect. 3.1. This yields N values of similarity
sim1(ni, nj), ..., simN (ni, nj) for each noun pair (ni, nj). These values are integrated
using one of the following four schemes to obtain a single value of similarity sim(ni, nj).
? arithmetic mean: sim(ni, nj) = 1N
?N
k=1 simk(ni, nj)
? geometric mean:sim(ni, nj) = N
?
?N
k=1 simk(ni, nj)
? maximum: sim(ni, nj) = maxk simk(ni, nj)
? minimum: sim(ni, nj) = mink simk(ni, nj)
344 M. Hagiwara, Y. Ogawa, and K. Toyama
70.0%
71.0%
72.0%
73.0%
74.0%
75.0%
76.0%
77.0%
78.0%
di
s c
rim
in
a t
io
n
 
ra
te
 
(%
)
15.0
17.0
19.0
21.0
23.0
25.0
sco
re
disc . rate
score
1 2 3 arith.
mean
geo.
mean
max min
before integration after integration
Fig. 9. Integration result for N = 3
71.0%
72.0%
73.0%
74.0%
75.0%
76.0%
77.0%
1 2 3 4 5 6 7 8 9 10
N
di
sc
rim
in
at
io
n
 r
at
e 
(%
)
15.0
17.0
19.0
21.0
23.0
25.0
27.0
29.0
31.0
sc
o
re
integrated (disc. score)
maximum (disc. score)
average (disc. rate)
integrated (score)
maximum (score)
average (score)
Fig. 10. Integration results varying N
Integration results are shown in Fig. 9, where the three sets of performance on the
left are the results of single PLSI executions, i.e., before integration. On the right are
the results after integration by the four schemes. It can be observed that integration
improves the performance. More specifically, the results after integration are as good or
better than any of the previous ones, except when using the minimum as a scheme.
An additional experiment was conducted that varied N from 1 to 10 to confirm that
such performance improvement is always achieved by integration. Results are shown in
Fig. 10, which includes the average and maximum of the N PLSI results (unintegrated)
as well as the performance after integration using arithmetic average as the scheme.
The results show that the integration consistently improves the performance for all 2 ?
N ? 10. An increase of the integration performance was observed for N ? 5, whereas
increases in the average and maximum of the unintegrated results were relatively low.
It is also seen that using N > 5 has less effect for integration.
5 Conclusion
In this study, automatic synonym acquisition was performed using a latent semantic
model PLSI by estimating the latent class distribution for each noun. For this purpose,
co-occurrences of verbs and nouns extracted from a large corpus were utilized. Discrim-
ination rates and scores were used to evaluate the current method, and it was found that
PLSI outperformed such conventional methods as tf?idf and LSI. These results make
PLSI applicable for automatic thesaurus construction. Moreover, the following tech-
niques were found effective: (1) employing Skew Divergence as the distance/similarity
measure between probability distributions; (2) removal of words with low frequencies,
and (3) multiple executions of PLSI and integration of the results.
As future work, the automatic extraction of the hierarchical relationship of words
also plays an important role in constructing thesauri, although only synonym relation-
ships were extracted this time. Many studies have been conducted for this purpose, but
extracted hyponymy/hypernymy relations must be integrated in the synonym relations
to construct a single thesaurus based on tree structure. The characteristics of the latent
class distributions obtained by the current method may also be used for this purpose.
PLSI Utilization for Automatic Thesaurus Construction 345
In this study, similarity was calculated only for nouns, but one for verbs can be
obtained using an identical method. This can be achieved by pairing noun n and case /
preposition c of co-occurrence (v, c, n), not v and c as previously done, and executing
PLSI for the dyadic data (v, (c, n)). By doing this, the latent class distributions for each
verb v, and consequently the similarity between them, are obtained.
Moreover, although this study only deals with verb-noun co-occurrences, other in-
formation such as adjective-noun modifications or descriptions in dictionaries may be
used and integrated. This will be an effective way to improve the performance of auto-
matically constructed thesauri.
References
1. Bilmes, J. 1997. A gentle tutorial on the EM algorithm and its application to parameter
estimation for gaussian mixture and hidden markov models. Technical Report ICSI-TR-97-
021, International Computer Science Institute (ICSI), Berkeley, CA.
2. Charniak, E. 2000. A maximum-entropy-inspired parser. NAACL 1, 132?139.
3. Collins. 2002. Collins Cobuild Major New Edition CD-ROM. HarperCollins Publishers.
4. Collins, M. 1996. A new statistical parser based on bigram lexical dependencies. Proc. of
34th ACL, 184?191.
5. Deerwester, S., et al 1990. Indexing by Latent Semantic Analysis. Journal of the American
Society for Information Science, 41(6):391?407.
6. Fellbaum, C. 1998. WordNet: an electronic lexical database. MIT Press.
7. Hindle, D. 1990. Noun classification from predicate-argument structures. Proc. of the 28th
Annual Meeting of the ACL, 268?275.
8. Hofmann, T. 1999. Probabilistic Latent Semantic Indexing. Proc. of the 22nd International
Conference on Research and Development in Information Retrieval (SIGIR ?99), 50?57.
9. Hofmann, T. 2001. Unsupervised Learning by Probabilistic Latent Semantic Analysis. Ma-
chine Learning, 42:177?196.
10. Kojima, K., et. al. 2004. Existence and Application of Common Threshold of the Degree of
Association. Proc. of the Forum on Information Technology (FIT2004) F-003.
11. Lee, L. 2001. On the Effectiveness of the Skew Divergence for Statistical Language Analysis.
Artificial Intelligence and Statistics 2001, 65?72.
12. Lin, J. 1991. Divergence measures based on the shannon entropy. IEEE Transactions on
Information Theory, 37(1):140?151.
13. Mochihashi, D., Matsumoto, Y. 2002. Probabilistic Representation of Meanings. IPSJ SIG-
Notes Natural Language, 2002-NL-147:77?84.
14. The National Institute of Japanese Language. 2004. Bunruigoihyo. Dainippontosho.
15. Santorini, B. 1990. Part-of-Speech Tagging Guidelines for the Penn Treebank Project.
ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz
16. Schmid, H. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. Proc. of the
First International Conference on New Methods in Natural Language Processing (NemLap-
94), 44?49.
17. Ueda, N., Nakano, R. 1998. Deterministic annealing EM algorithm. Neural Networks,
11:271?282.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 39?43, Dublin, Ireland, August 23-29 2014.
Lightweight Client-Side Chinese/Japanese
Morphological Analyzer Based on Online Learning
Masato Hagiwara Satoshi Sekine
Rakuten Institute of Technology, New York
215 Park Avenue South, New York, NY
{masato.hagiwara, satoshi.b.sekine}@mail.rakuten.com
Abstract
As mobile devices and Web applications become popular, lightweight, client-side language
analysis is more important than ever. We propose Rakuten MA, a Chinese/Japanese
morphological analyzer written in JavaScript. It employs an online learning algorithm
SCW, which enables client-side model update and domain adaptation. We have achieved
a compact model size (5MB) while maintaining the state-of-the-art performance, via
techniques such as feature hashing, FOBOS, and feature quantization.
1 Introduction
Word segmentation (WS) and part-of-speech (PoS) tagging, often jointly called morphological
analysis (MA), are the essential component for processing Chinese and Japanese, where words
are not explicitly separated by whitespaces. There have been many word segmentater and PoS
taggers proposed in both Chinese and Japanese, such as Stanford Segmenter (Tseng et al.,
2005), zpar (Zhang and Clark, 2011), MeCab (Kudo et al., 2004), JUMAN (Kurohashi and
Nagao, 1994), to name a few. Most of them are intended for server-side use and provide limited
capability to extend or re-train models. However, as mobile devices such as smartphones and
tablets become popular, there is a growing need for client based, lightweight language analysis,
and a growing number of applications are built upon lightweight languages such as HTML, CSS,
and JavaScript. Techniques such as domain adaptation and model extension are also becoming
more important than ever.
In this paper, we present Rakuten MA, a morphological analyzer entirely written in JavaScript
based on online learning. We will be releasing the software as open source before the COLING
2014 conference at https://github.com/rakuten-nlp/rakutenma, under Apache License, ver-
sion 2.0. It relies on general, character-based sequential tagging, which is applicable to any
languages and tasks which can be processed in a character-by-character basis, including WS and
PoS tagging for Chinese and Japanese. Notable features include:
1. JavaScript based ? Rakuten MA works as a JavaScript library, the de facto ?lingua franca?
of the Web. It works on popular Web browsers as well as node.js, which enables a wide range
of adoption such as smartphones and Web browser extensions. Note that TinySegmenter1
is also entirely written in JavaScript, but it does not support model re-training or any
languages other than Japanese. It doesn?t output PoS tags, either.
2. Compact ? JavaScript-based analyzers pose a difficult technical challenge, that is, the
compactness of the model. Modern language analyzers often rely on a large number of
features and/or dictionary entries, which is impractical on JavaScript runtime environments.
In order to address this issue, Rakuten MA implements some notable techniques. First, the
features are character-based and don?t rely on dictionaries. Therefore, while it is inherently
incapable of dealing with words which are longer than features can capture, it may be robust
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and
proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1http://chasen.org/~taku/software/TinySegmenter/
39
S-N-nc S-P-k 
!! "!
B-V-c E-V-c 
#! $!
S-P-sj 
%!
tags 
characters x1:! c1:C x2:" c2:H x3:# c3:C x4:$ c4:H x5:% c5:H 
? 
Figure 1: Character-based tagging model
Feature Description
x
i?2
, x
i?1
, x
i
, x
i+1
, x
i+2
char. unigrams
x
i?2
x
i?1
, x
i?1
x
i
, x
i
x
i+1
, x
i+1
x
i+2
char. bigrams
c
i?2
, c
i?1
, c
i
, c
i+1
, c
i+2
type unigrams
c
i?2
c
i?1
, c
i?1
c
i
, c
i
c
i+1
, c
i+1
c
i+2
type bigrams
Table 1: Feature Templates Used for Tagging
x
i
and c
i
are the character and the character type at ia.
aEach feature template is instantiated and concate-
nated with possible tags. Character type bigram features
were only used for JA. In CN, we built a character type
dictionary, where character types are simply all the possi-
ble tags in the training corpus for a particular character.
to unknown words compared with purely dictionary-based systems. Second, it employs
techniques such as feature hashing (Weinberger et al., 2009), FOBOS (Duchi and Singer,
2009), and feature quantization, to make the model compact while maintaining the same
level of analysis performance.
3. Online Learning ? it employs a modern online learning algorithm called SCW (Wang et al.,
2012), and the model can be incrementally updated by feeding new instances. This enables
users to update the model if errors are found, without even leaving the Web browser or
node.js. Domain adaptation is also straightforward. Note that MeCab (Kudo et al., 2004),
also supports model re-training using a small re-training corpus. However, the training is
inherently a batch, iterative algorithm (CRF) thus it is hard to predict when it finishes.
2 Analysis Model and Compact Model Representation
Base Model Rakuten MA employs the standard character-based sequential tagging model.
It assigns combination of position tags2 and PoS tags to each character (Figure 1). The optimal
tag sequence y? for an input string x is inferred based on the features ?(y) and the weight vector
w as y? = arg max
y?Y (x)
w ? ?(y), where Y (x) denotes all the possible tag sequences for x, via
standard Viterbi decoding. Table 1 shows the feature template sets.
For training, we used soft confidence weighted (SCW) (Wang et al., 2012). SCW is an online
learning scheme based on Confidence Weighted (CW), which maintains ?confidence? of each
parameter as variance ? in order to better control the updates. Since SCW itself is a general
classification model, we employed the structured prediction model (Collins, 2002) for WS.
The code snippet in Figure 2 shows typical usage of Rakuten MA in an interactive way. Lines
starting with ?//? and ?>? are comments and user input, and the next lines are returned results.
Notice that the analysis of????????? ?President Barak Obama? get better as the model
observes more instances. The analyzer can only segment it into individual characters when the
model is empty ((1) in the code), whereas WS is partially correct after observing the first 10
sentences of the corpus ((2) in the code). After directly providing the gold standard, the result
(3) becomes perfect.
We used and compared the following three techniques for compact model representations:
Feature Hashing (Weinberger et al., 2009) applies hashing functions h which turn an arbi-
trary feature ?
i
(y) into a bounded integer value v, i.e., v = h(?
i
(y)) ? R, where 0 ? v < 2N ,
(N = hash size). This technique is especially useful for online learning, where a large, growing
number of features such as character/word n-grams could be observed on the fly, which the
model would otherwise need to keep track of using flexible data structures such as trie, which
could make training slower as the model observes more training instances. The negative effect of
hash collisions to the performance is negligible because most collisions are between rare features.
2As for the position tags, we employed the SBIEO scheme, where S stands for a single character word, BIE
for beginning, middle, and end of a word, respectively, and O for other positions.
40
// initialize with empty model
> var r = new RakutenMA({});
// (1) first attempt, failed with separate chars.
> r.tokenize("?????????").toString()
"?,,?,,?,,?,,?,,?,,?,,?,,?,"
// train with first 10 sentences in a corpus
> for (var i = 0; i < 10; i ++)
> r.train_one( rcorpus[i] );
// the model is no longer empty
> r.model
Object {mu: Object, sigma: Object}
// (2) second attempt -> getting closer
> r.tokenize("?????????").toString()
"???,N-nc,???,N-pn,?,,?,,?,Q-n"
// retrain with an answer
// return object suggests there was an update
> r.train_one([["???","N-np"],
... ["???","N-np"],["???","N-nc"]]);
Object {ans: Array[3], sys: Array[5], updated: true}
// (3) third attempt
> r.tokenize("?????????").toString()
"???,N-np,???,N-np,???,N-nc"
Figure 2: Rakuten MA usage example
Chinese Prec. Rec. F Japanese Prec. Rec. F
Stanford Parser 97.37 93.54 95.42 MeCab+UniDic 99.15 99.61 99.38
zpar 91.18 92.36 91.77 JUMAN **88.55 **83.06 **85.72
Rakuten MA 92.61 92.64 92.62 KyTea *80.57 *85.02 *82.73
TinySegmenter *86.93 *85.19 *86.05
Rakuten MA 96.76 97.30 97.03
Table 2: Segmentation Performance Comparison with Different Systems
* These systems use different WS criteria and their performance is shown simply for reference. ** We
postprocessed JUMAN?s WS result so that the WS criteria are closer to Rakuten MA?s.
Forward-Backward Splitting (FOBOS) (Duchi and Singer, 2009) is a framework to intro-
duce regularization to online learning algorithms. For each training instance, it runs uncon-
strained parameter update of the original algorithm as the first phase, then solves an instan-
taneous optimization problem to minimize a regularization term while keeping the parameter
close to the first phrase. Specifically, letting w
t+
1
2
,j
the j-th parameter after the first phrase of
iteration t and ? the regularization coefficient, parameter update of FOBOS with L1 regulariza-
tion is done by: w
t+1,j
= sign
(
w
t+
1
2
,j
) [
?
?
?
w
t+
1
2
,j
?
?
?
? ?
]
+
. The strength of regularization can be
adjusted by the coefficient ?. In combining SCW and FOBOS, we retained the confidence value
? of SCW unchanged.
Feature Quantization simply multiplies float numbers (e.g., 0.0165725659236262) by M
(e.g., 1,000) and round it to obtain a short integer (e.g., 16). The multiple M determines the
strength of quantization, i.e., the larger the finer grained, but the larger model size.
3 Experiments
We used CTB 7.0 (Xue et al., 2005) for Chinese (CN), and BCCWJ (Maekawa, 2008) for
Japanese (JA), with 50,805 and 60,374 sentences, respectively. We used the top two levels of
BCCWJ?s PoS tag hierarchy (38 unique tags) and all the CTB PoS tags (38 unique tags). The
average decoding time was 250 millisecond per sentence on Intel Xeon 2.13GHz, measured on
node.js. We used precision (Prec.), recall (Rec.), and F-value (F) of WS as the evaluation metrics,
averaged over 5-fold cross validation. We ignored the PoS tags in the evaluation because they
are especially difficult to compare across different systems with different PoS tag hierarchies.
Comparison with Other Analyzers First, we compare the performance of Rakuten MA
with other word segmenters. In CN, we compared with Stanford Segmenter (Tseng et al., 2005)
and zpar (Zhang and Clark, 2011). In JA, we compared with MeCab (Kudo et al., 2004),
JUMAN (Kurohashi and Nagao, 1994), KyTea (Neubig et al., 2011), and TinySegmenter.
Table 2 shows the result. Note that, some of the systems (e.g., Stanford Parser for CN and
MeCab+UniDic for JA) use the same corpus as the training data and their performance is
unfairly high. Also, other systems such as JUMAN and KyTea employ different WS criteria,
and their performance is unfairly low, although JUMAN?s WS result was postprocessed so that
41
!"#$
%"#$
&"#$
'"#$
($ )$ *$ +$ ,$ !$ %$ &$ '$ ("$
!"#
$%#
&'
()"
*
+',)-*./&0"#*
-./01$
2/01$
3$
Figure 3: Domain Adaptation Result for CN
!"#$
%"#$
&"#$
'""#$
'$ ($ )$ *$ +$ ,$ !$ %$ &$ '"$
!"#
$%#
&'
()"
*
+',)-*./&0"#*
-./01$
2/01$
3$
Figure 4: Domain Adaptation Result for JA
!"#"""$
%&'($
%&')$%&'*$%&'+$
,&'-"./($
,&*-"./($,&'-"./)$
,&*-"./)$
,&'-"./*$
012$0'2$
"-(*$
"-3"$
"-3*$
"-4"$
"-4*$
*""$ *#"""$
!"
#$%&'"()*&"+),"-./"
56789:;8$<86=-$>67?:;@$<A5AB$<86=-$>67?:;@C<A5AB$DE6;=-$<86=-$>67?:;@CDE6;=-$<A5ABCDE6;=-$F99$
Figure 5: Model Comparison
it gives a better idea how it compares with Rakuten MA. We can see that Rakuten MA can
achieve WS performance comparable with the state-of-the-art even without using dictionaries.
Domain Adaptation Second, we tested Rakuten MA?s domain adaptation ability. We chose
e-commerce as the target domain, since it is a rich source of out-of-vocabulary words and poses
a challenge to analyzers trained on newswire text. We sampled product titles and descriptions
from Rakuten Taiwan3 (for CN, 2,786 sentences) and Rakuten Ichiba4 (for JA, 13,268 sentences).
These collections were then annotated by human native speakers in each language, following the
tagging guidelines of CTB (for CN) and BCCWJ (for JA).
We divided the corpus into 5 folds, then used four of them for re-training and one for testing.
The re-training data is divided into ?batches,? consisting of mutually exclusive 50 sentences,
which were fed to the pre-trained model on CTB (for CN) and BCCWJ (for JA) one by one.
Figure 3 and 4 show the results. The performance quickly levels off after five batches for JA,
which gives an approximated number of re-training instances needed (200 to 300) for adaptation.
Note that adaptation took longer on CN, which may be attributed to the fact that Chinese WS
itself is a harder problem, and to the disparity between CTB (mainly news articles in mainland
China) and the adaptation corpus (e-commerce text in Taiwan).
3http://www.rakuten.com.tw/. Note that Rakuten Taiwan is written in Taiwanese traditional Chinese. It
was converted to simplified Chinese by using Wikipedia?s traditional-simplified conversion table http://svn.
wikimedia.org/viewvc/mediawiki/trunk/phase3/includes/ZhConversion.php. Still, having large Taiwan spe-
cific vocabulary poses additional challenges for domain adaptation.
4http://www.rakuten.co.jp/
42
Compact Model Representation Third, we consider the three techniques, and investigate
how these techniques affect the trade-off between WS performance and the model size, which is
measured by the feature trie byte size in raw JSON format5.
Figure 5 shows the scatter plot of F-value vs model size in KB, since we are rather interested
in the trade-off between the model size and the performance. The baseline is the raw model
without any techniques mentioned above. Notice that the figure?s x-axis is in log scale, and the
upper left corner in the figure corresponds to better trade-off (higher performance with smaller
model size). We can observe that all the three techniques can reduce the model size to some
extent while keeping the performance at the same level. In fact, these three techniques are
independent from each other and can be freely combined to achieve better trade-off. If we limit
strictly the same level of performance compared to the baseline, feature hashing (17bit hash
size) with quantization (Point (1) in the figure) seems to achieve the best trade-off, slightly
outperforming the baseline (F = 0.9457 vs F = 0.9455 of the baseline) with the model size of
as little as one fourth (5.2MB vs 20.6MB of the baseline). It is somewhat surprising to see that
feature quantization, which is a very simple method, achieves relatively good performance-size
trade-off (Point (2) in the figure).
4 Conclusion
In this paper, we proposed Rakuten MA, a lightweight, client-side morphological analyzer
entirely written in JavaScript. It supports online learning based on the SCW algorithm, which
enables quick domain adaptation, as shown in the experiments. We successfully achieved a
compact model size of as little as 5MB while maintaining the state-of-the-art performance, using
feature hashing, FOBOS, and feature quantization. We are planning to achieve even smaller
model size by adopting succinct data structure such as wavelet tree (Grossi et al., 2003).
Acknowledgements
The authors thank Satoko Marumoto, Keiji Shinzato, Keita Yaegashi, and Soh Masuko for
their contribution to this project.
References
Michael Collins. 2002. Discriminative training methods for hidden Markov models: theory and experiments with
perceptron algorithms. In Proc. of the EMNLP 2002, pages 1?8.
John Duchi and Yoram Singer. 2009. Efficient online and batch learning using forward backward splitting.
Journal of Machine Learning Research, 10:2899?2934.
Roberto Grossi, Ankur Gupta, and Jeffrey Scott Vitter. 2003. High-order entropy-compressed text indexes. In
Prof. of SODA 2003, pages 841?850.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese
morphological analysis. In Proceedings of EMNLP, pages 230?237.
Sadao Kurohashi and Makoto Nagao. 1994. Improvements of Japanese morphological analyzer JUMAN. In
Proceedings of the International Workshop on Sharable Natural Language Resources, pages 22?38.
Kikuo Maekawa. 2008. Compilation of the Kotonoha-BCCWJ corpus (in Japanese). Nihongo no kenkyu (Studies
in Japanese), 4(1):82?95.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori. 2011. Pointwise prediction for robust, adaptable japanese
morphological analysis. In Proceedings of ACL-HLT, pages 529?533.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional
random field word segmenter. In Fourth SIGHAN Workshop on Chinese Language Processing.
Jialei Wang, Peilin Zhao, and Steven C. Hoi. 2012. Exact soft confidence-weighted learning. In Proc. of ICML
2012, pages 121?128.
Kilian Weinberger, Anirban Dasgupta, Josh Attenberg, John Langford, and Alex Smola. 2009. Feature hashing
for large scale multitask learning. In Proc. of ICML 2009, pages 1113?1120.
Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer. 2005. The penn Chinese treebank: Phrase structure
annotation of a large corpus. Natural Language Engineering, 11(2):207?238.
Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search.
Computational Linguistics, 37(1):105?151.
5We used one fifth of the Japanese corpus BCCWJ for this experiment. Parameter ? of FOBOS was varied
over {1.0? 10?7, 5.0? 10?7, 1.0? 10?6, 5.0? 10?6, 1.0? 10?5}. The hash size of feature hashing was varied over
14, 15, 16, 17
6. The multiple of feature quantization M is set to M = 1000.
43
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 24?27,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
KooSHO: Japanese Text Input Environment
based on Aerial Hand Writing
Masato Hagiwara
Rakuten Institute of Technology
215 Park Avenue South,
New York, NY, USA 10003
masato.hagiwara@mail.rakuten.com
Soh Masuko
Rakuten Institute of Technology
4-13-9 Higashi-shinagawa
Shinagawa-ku, Tokyo, JAPAN 140-0002
so.masuko@mail.rakuten.com
Abstract
Hand gesture-based input systems have
been in active research, yet most of them
focus only on single character recognition.
We propose KooSHO: an environment for
Japanese input based on aerial hand ges-
tures. The system provides an integrated
experience of character input, Kana-Kanji
conversion, and search result visualization.
To achieve faster input, users only have to
input consonant, which is then converted
directly to Kanji sequences by direct conso-
nant decoding. The system also shows sug-
gestions to complete the user input. The
comparison with voice recognition and a
screen keyboard showed that KooSHO can
be a more practical solution compared to
the existing system.
1 Introduction
In mobile computing, intuitive and natural
text input is crucial for successful user experi-
ence, and there have been many methods and
systems proposed as the alternatives to tradi-
tional keyboard-and-mouse input devices. One
of the most widely used input technologies is
voice recognition such as Apple Inc.?s Siri. How-
ever, it has some drawbacks such as being vul-
nerable to ambient noise and privacy issues
when being overheared. Virtual keyboards1
require extensive practice and could be error-
prone compared to physical keyboards.
1http://www.youtube.com/watch?v=h9htRy0-sUw
(a) (b) (c)
Figure 1: Overall text input procedure using
KooSHO ? (a) Character recognition (b) Kana-
Kanji conversion results (c) Search results
In order to address these issues, many gesture-
based text input interfaces have been proposed,
including a magnet-based hand writing device
(Han et al, 2007). Because these systems re-
quire users to wear or hold special devices, hand
gesture recognition systems based on video cam-
eras are proposed, such as Yoon et al (1999)
and Sonoda and Muraoka (2003). However, a
large portion of the literature only focuses on
single character input. One must consider over-
all text input experience when users are typing
words and phrases. This problem is pronounced
for languages which require explicit conversion
from Romanized forms to ideographic writing
systems such as Japanese.
In this paper, we propose KooSHO: an in-
tegrated environment for Japanese text input
based on aerial hand gestures. It provides an
integrated experience of character input, Kana-
Kanji conversion, i.e., conversion from Roman-
ized forms to ideographic (Kanji) ones, and
search result visualization. Figure 1 shows the
overall procedure using KooSHO. First, (a) a
user draws alphabetical shapes in the air, whose
hand position is captured by Microsoft Kinect.
KooSHO then recognizes characters, and after
24
KooSHO Engine
Kinect
The Internet
Screen
Front-EndCharacter Recognition& User Interface
Back-EndKana-Kanji Conversion& Suggestion
Figure 2: Confiugration of the KooSHO system
Kana-Kanji conversion, the results are shown in
a circle centered at the user?s shoulder (b). Fi-
nally, the user can choose one of the candidates
by ?touching? it, and (c) the search result using
the chosen word as the query is shown in circle
again for the user to choose.
KooSHO has several novelties to achieve
seamless yet robust text input, including:
Non-restrictive character forms ? the
system does not restrict on the input character
forms, unlike Graffiti 22.
Robust continuous recognition and con-
version ? Aerial handwriting poses special dif-
ficulties since the system cannot obtain individ-
ual strokes. We solve this problem by employing
a discriminative Kana-Kanji conversion model
trained on the specific domain.
Faster input by suggestions and con-
sonant input ? KooSHO shows suggestions
to predict the words the user is about to in-
put, while it allows users to type only conso-
nants, similar to Tanaka-Ishii et al (2001).
We propose direct consonant decoding, which
runs Viterbi search directly on the input con-
sonant sequence without converting them back
into Kana candidates.
We conducted the evaluations on character
recognition and Kana-Kanji conversion accu-
racy to measure KooSHO?s performance. We
also ran an overall user experience test, compar-
ing its performance with the voice recognition
software Siri and a screen keyboard.
2 Character Recognition
Figure 2 describes the overall configuration. A
user draws alphabetical shapes in the air, which
is captured by Kinect and sent to KooSHO. We
2http://en.wikipedia.org/wiki/Graffiti_2
?? (hokubu)
BOS EOS
H K B K R?? (kore)
? (huku) ? (bukuro)
? (huku) ?? (buki) ? (ro)
? (hu) ???? (kobukuro)
Figure 3: Lattice search based on consonants
used the skeleton recognition functionalities in-
cluded in Kinect for Windows SDK v1.5.1. The
system consists of the front-end and back-end
parts, which are responsible for character recog-
nition and user interface, and Kana-Kanji con-
version and suggestion, respectively.
We continuously match the drawn trajectory
to templates (training examples) using dynamic
programming. The trajectory and the templates
are both represented by 8 direction features to
facilitate the match, and the distance is cal-
culated based on how apart the directions are.
This coding system is robust to scaling of char-
acters and a slight variation of writing speed,
while not robust to stroke order. This is re-
peated every frame to produce the distance be-
tween the trajectory ending at the current frame
and each template. If the distance is below a cer-
tain threshold, the character is considered to be
the one the user has just drawn.
If more than one characters are detected and
their periods overlap, they are both sent as al-
ternative. The result is represented as a lattice,
with alternation and concatenation. To each let-
ter a confidence score (the inverse of the mini-
mum distance from the template) is assigned.
3 Kana-Kanji Conversion
In this section, we describe the Kana-Kanji
conversion model we employed to achieve the
consonant-to-Kanji conversion. As we men-
tioned, the input to the back-end part passed
from the front-end part is a lattice of possi-
ble consonant sequences. We therefore have to
?guess? the possibly omitted vowels somehow
and convert the sequences back into intended
Kanji sequences. However, it would be an ex-
ponentially large number if we expand the in-
put consonant sequence to all possible Kana se-
25
quences. Therefore, instead of attempting to re-
store all possible Kana sequences, we directly
?decode? the input consonant sequence to ob-
tain the Kanji sequence. We call this process
direct consonant decoding, shown in Figure 3. It
is basically the same as the vanilla Viterbi search
often used for Japanese morphological analysis
(Kudo et al, 2004), except that it runs on a con-
sonant sequence. The key change to this Viterbi
search is to make it possible to look up the dic-
tionary directly by consonant substrings. To do
this, we convert dictionary entries to possible
consonant sequences referring to Microsoft IME
Kana Table3 when the dictionary structure is
loaded onto the memory. For example, for a dic-
tionary entry??/????? hukubukuro, possi-
ble consonant sequences such as ?hkbkr,? ?huk-
bkr,? ?hkubkr,? ?hukubkr,? ?hkbukr,?... are
stored in the index structure.
As for the conversion model, we employed the
discriminative Kana-Kanji conversion model by
Tokunaga (2011). The basic algorithm is the
same except that the Viterbi search also runs on
consonant sequences rather than Kana ones. We
used surface unigram, surface + class (PoS tags)
unigram, surface + reading unigram, class bi-
gram, surface bigram as features. The red lines
in Figure 3 illustrate the finally chosen path.
The suggestion candidates, which is to show
candidates such as hukubukuro (lucky bag) and
hontai (body) for an input ?H,? are chosen from
2000 most frequent query fragments issued in
2011 at Rakuten Ichiba4. We annotate each
query with Kana pronunciation, which is con-
verted into possible consonant sequence as in
the previous section. At run-time, prefix search
is perfomed on this consonant trie to obtain the
candidate list. The candidate list is sorted by
the frequency, and shown to the user supple-
menting the Kana-Kanji conversion results.
4 Experiments
In this section, we compare KooSHO with
Siri and a software keyboard system. We
used the following three training corpora: 1)
3http://support.microsoft.com/kb/883232/ja
4http://www.rakuten.co.jp/
BCCWJ-CORE (60,374 sentences and 1,286,899
tokens)5, 2) EC corpus, consists of 1,230 product
titles and descriptions randomly sampled from
Rakuten Ichiba (118,355 tokens). 3) EC query
log (2000 most frequent query fragments issued
in 2011 at Rakuten Ichiba) As the dictionary,
we used UniDic6.
Character Recognition Firstly, we evaluate
the accuracy of the character recognition model.
For each letter from ?A? to ?Z,? two subjects
attempted to type the letter for three times, and
the accuracy how many times the character was
correctly recognized was measured.
We observed recognition accuracy varies from
letter to letter. Letters which have similar
forms, such as ?F? and ?T? can be easily mis-
recognized, leading lower accuracy. For some
of the cases where the letter shape completely
contains a shape of the other, e.g., ?F? and ?E,?
recognition error is inevitable. The overall char-
acter recognition accuracy was 0.76.
Kana-Kanji Conversion Secondly, we eval-
uate the accuracy of the Kana-Kanji conversion
algorithm. We used ACC (averaged Top-1 ac-
curacy), MFS (mean F score), and MRR (mean
reciprocal rank) as evaluation measures (Li et
al., 2009). We used a test set consisting of
100 words and phrases which are randomly ex-
tracted from Rakuten Ichiba, popular products
and query logs. The result was ACC = 0.24,
MFS = 0.50, and MRR = 0.30, which suggests
the right choice comes at the top 24 percent of
the time, about half (50%) the characters of the
top candidate match the answer, and the aver-
age position of the answer is 1 / MRR = 3.3. No-
tice that this is a very strict evaluation since it
does not allow partial input. For example, even
if ???????????? fittonesushu-zu (fitness
shoes) does not come up at the top, one could
obtain the same result by inputting ??????
?? (fitness) and ?????? (shoes) separately.
Also, we found that some spelling variations
such as ??? and ??? (both meaning eye-
lashes) lower the evaluation result, even though
5http://www.ninjal.ac.jp/corpus_center/bccwj/
6http://www.tokuteicorpus.jp/dist/
26
they are not a problem in practice.
Overall Evaluation Lastly, we evaluate the
overall input accuracy, speed, and user experi-
ence comparing Siri, a screen keyboard (Tablet
PC Input Panel) controlled by Kinect using
KinEmote7, and KooSHO.
First, we measured the recognition accuracy
of Siri based on the 100 test queries. The accu-
racy turned out to be 85%, and the queries were
recognized within three to four seconds. How-
ever, about 14% of the queries cannot be recog-
nized even after many attempts. There are es-
pecially two types of queries where voice recog-
nition performed poorly ? the first one is rel-
atively new, unknown words such as ????
?? (ogaland), which obviously depends on the
recognition system?s language models and the
vocabulary set. The second the is homonyms,
i.e., voice recognition is, in principle, unable to
discern multiple words with the same pronun-
ciation, such as ???? (package) and ????
(broadcast) housou, and ??????? (alum) and
???? (tomorrow evening) myouban. This is
where KooSHO-like visual feedback on the con-
version results has a clear advantage.
Second, we tried the screen keyboard con-
trolled by Kinect. Using a screen keyboard was
extremely difficult, almost impossible, since it
requires fine movement of hands in order to
place the cursor over the desired keys. There-
fore, only the time required to place the cursor
on the desired keys in order was measured. The
fact that users have to type out all the characters
including vowels is making the matter worse.
This is also where KooSHO excels.
Finally, we measured the time taken for
KooSHO to complete each query. The result
varied depending on query, but the ones which
contain characters with low recognition accu-
racy such as ?C? (e.g., ????? (cheese)) took
longer. The average was 35 seconds.
Conclusion and Future Works
In this paper, we proposed a novel environ-
ment for Japanese text input based on aerial
hand gestures called KooSHO, which provides
7http://www.kinemote.net/
an integrated experience of character input,
Kana-Kanji conversion, and search result vi-
sualization. This is the first to propose a
Japanese text input system beyond single char-
acters based on hand gestures. The system has
several novelties, including 1) non-restrictive
character forms, 2) robust continuous recogni-
tion and Kana-Kanji conversion, and 3) faster
input by suggestions and consonant input. The
comparison with voice recognition and a screen
keyboard showed KooSHO can be a more prac-
tical solution compared to the screen keyboard.
Since KooSHO is an integrated Japanese in-
put environment, not just a character recog-
nition software, many features implemented in
modern input methods, such as fuzzy match-
ing and user personalization, can also be im-
plemented. In particular, how to let the user
modify the mistaken input is a great challenge.
References
Xinying Han, Hiroaki Seki, Yoshitsugu kamiya, and
Masatoshi Hikizu. 2007. Wearable handwriting
input device using magnetic field. In Proc. of
SICE, pages 365?368.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proc. of
EMNLP, pages 230?237.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009. Report of news 2009 machine
transliteration shared task. In Proc. of NEWS,
pages 1?18.
Tomonari Sonoda and Yoishic Muraoka. 2003. A
letter input system of handwriting gesture (in
Japanese). The Transactions of the Institute of
Electronics, Information and Communication En-
gineers D-II, J86-D-II:1015?1025.
Kumiko Tanaka-Ishii, Yusuke Inutsuka, and Masato
Takeichi. 2001. Japanese text input system with
digits. In Proc. of HLT, pages 1?8.
Hiroyuki Tokunaga, Daisuke Okanohara, and Shin-
suke Mori. 2011. Discriminative method for
Japanese kana-kanji input method. In Proc. of
WTIM.
Ho-Sub Yoon, Jung Soh, Byung-Woo Min, and
Hyun Seung Yang. 1999. Recognition of al-
phabetical hand gestures using hidden markov
model. IEICE Trans. Fundamentals, E82-
A(7):1358?1366.
27
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 53?57,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Latent Class Transliteration based on Source Language Origin
Masato Hagiwara
Rakuten Institute of Technology, New York
215 Park Avenue South, New York, NY
masato.hagiwara@mail.rakuten.com
Satoshi Sekine
Rakuten Institute of Technology, New York
215 Park Avenue South, New York, NY
satoshi.b.sekine@mail.rakuten.com
Abstract
Transliteration, a rich source of proper noun
spelling variations, is usually recognized by
phonetic- or spelling-based models. How-
ever, a single model cannot deal with dif-
ferent words from different language origins,
e.g., ?get? in ?piaget? and ?target.? Li et
al. (2007) propose a method which explicitly
models and classifies the source language ori-
gins and switches transliteration models ac-
cordingly. This model, however, requires an
explicitly tagged training set with language
origins. We propose a novel method which
models language origins as latent classes. The
parameters are learned from a set of translit-
erated word pairs via the EM algorithm. The
experimental results of the transliteration task
of Western names to Japanese show that the
proposed model can achieve higher accuracy
compared to the conventional models without
latent classes.
1 Introduction
Transliteration (e.g., ??????? baraku obama /
Barak Obama?) is phonetic translation between lan-
guages with different writing systems. Words are
often transliterated when imported into differet lan-
guages, which is a major cause of spelling variations
of proper nouns in Japanese and many other lan-
guages. Accurate transliteration is also the key to
robust machine translation systems.
Phonetic-based rewriting models (Knight and
Jonathan, 1998) and spelling-based supervised mod-
els (Brill and Moore, 2000) have been proposed for
recognizing word-to-word transliteration correspon-
dence. These methods usually learn a single model
given a training set. However, single models cannot
deal with words from multiple language origins. For
example, the ?get? parts in ?piaget /???? piaje?
(French origin) and ?target / ????? ta?getto?
(English origin) may differ in how they are translit-
erated depending on their origins.
Li et al (2007) tackled this issue by proposing a
class transliteration model, which explicitly models
and classifies origins such as language and genders,
and switches corresponding transliteration model.
This method requires training sets of transliterated
word pairs with language origin. However, it is diffi-
cult to obtain such tagged data, especially for proper
nouns, a rich source of transliterated words. In ad-
dition, the explicitly tagged language origins are not
necessarily helpful for loanwords. For example, the
word ?spaghetti? (Italian origin) can also be found
in an English dictionary, but applying an English
model can lead to unwanted results.
In this paper, we propose a latent class transliter-
ation model, which models the source language ori-
gin as unobservable latent classes and applies appro-
priate transliteration models to given transliteration
pairs. The model parameters are learned via the EM
algorithm from training sets of transliterated pairs.
We expect that, for example, a latent class which is
mostly occupied by Italian words would be assigned
to ?spaghetti /?????supageti? and the pair will
be correctly recognized.
In the evaluation experiments, we evaluated the
accuracy in estimating a corresponding Japanese
transliteration given an unknown foreign word,
53
flextime
furekkusutaimu
s:
t:
?i
?i
Figure 1: Minimum edit operation sequence in the alpha-
beta model (Underlined letters are match operations)
using lists of Western names with mixed lan-
guages. The results showed that the proposed model
achieves higher accuracy than conventional models
without latent classes.
Related researches include Llitjos and Black
(2001), where it is shown that source language ori-
gins may improve the pronunciation of proper nouns
in text-to-speech systems. Another one by Ahmad
and Kondrak (2005) estimates character-based error
probabilities from query logs via the EM algorithm.
This model is less general than ours because it only
deals with character-based error probability.
2 Alpha-Beta Model
We adopted the alpha-beta model (Brill and Moore,
2000), which directly models the string substitu-
tion probabilities of transliterated pairs, as the base
model in this paper. This model is an extension to
the conventional edit distance, and gives probabil-
ities to general string substitutions in the form of
? ? ? (?, ? are strings of any length). The whole
probability of rewriting word s with t is given by:
PAB(t|s) = max
T?Part(t),S?Part(s)
|S|
?
i=1
P (?i ? ?i), (1)
where Part(x) is all the possible partitions of word
x. Taking logarithm and regarding ? logP (? ? ?)
as the substitution cost of ? ? ?, this maximiza-
tion is equivalent to finding a minimum of total sub-
stitution costs, which can be solved by normal dy-
namic programming (DP). In practice, we condi-
tioned P (? ? ?) by the position of ? in words,
i.e., at the beginning, in the middle, or at the end of
the word. This conditioning is simply omitted in the
equations in this paper.
The substitution probabilities P (? ? ?) are
learned from transliterated pairs. Firstly, we obtain
an edit operation sequence using the normal DP for
edit distance computation. In Figure 1 the sequence
is f?f, ? ?u, l?r, e?e,??k, x?k, ... and so on.
Secondly, non-match operations are merged with ad-
jacent edit operations, with the maximum length of
substitution pairs limited to W . When W = 2,
for example, the first non-match operation ? ?u is
merged with one operation on the left and right, pro-
ducing f?fu and l?ur. Finally, substitution prob-
abilities are calculated as relative frequencies of all
substitution operations created in this way. Note that
the minimum edit operation sequence is not unique,
so we take the averaged frequencies of all the possi-
ble minimum sequences.
3 Class Transliteration Model
The alpha-beta model showed better performance in
tasks such as spelling correction (Brill and Moore,
2000), transliteration (Brill et al, 2001), and query
alteration (Hagiwara and Suzuki, 2009). However,
the substitution probabilities learned by this model
are simply the monolithic average of training set
statistics, and cannot be switched depending on the
source language origin of given pairs, as explained
in Section 1.
Li et al (2007) pointed out that similar problems
arise in Chinese. Transliteration of Indo-European
names such as ????? / Alexandra? can be ad-
dressed by Mandarin pronunciation (Pinyin) ?Ya-Li-
Shan-Da,? while Japanese names such as ??? /
Yamamoto? can only be addressed by considering
the Japanese pronunciation, not the Chinese pro-
nunciation ?Shan-Ben.? Therefore, Li et al took
into consideration two additional factors, i.e., source
language origin l and gender / first / last names g,
and proposed a model which linearly combines the
conditioned probabilities P (t|s, l, g) to obtain the
transliteration probability of s ? t as:
P (t|s)soft =
?
l,g
P (t, l, g|s)
=
?
l,g
P (t|s, l, g)P (l, g|s) (2)
We call the factors c = (l, g) as classes in this paper.
This model can be interpreted as firstly computing
54
the class probability distribution given P (c|s) then
taking a weighted sum of P (t|s, c) with regard to
the estimated class c and the target t.
Note that this weighted sum can be regarded
as doing soft-clustering of the input s into classes
with probabilities. Alternatively, we can employ
hard-clustering by taking one class such that c? =
argmaxl,g P (l, g|s) and compute the transliteration
probability by:
P (t|s)hard ? P (t|s, c?). (3)
4 Latent Class Transliteration Model
The model explained in the previous section inte-
grates different transliteration models for words with
different language origins, but it requires us to build
class detection model c from training pairs explicitly
tagged with language origins.
Instead of assigning an explicit class c to each
transliterated pair, we can introduce a random vari-
able z and consider a conditioned string substitution
probability P (? ? ?|z). This latent class z cor-
responds to the classes of transliterated pairs which
share the same transliteration characteristics, such as
language origins and genders. Although z is not di-
rectly observable from sets of transliterated words,
we can compute it via EM algorithm so that it max-
imizes the training set likelihood as shown below.
Due to the space limitation, we only show the up-
date equations. Xtrain is the training set consisting
of transliterated pairs {(sn, tn)|1 ? n ? N}, N is
the number of training pairs, and K is the number of
latent classes.
Parameters: P (z = k) = pik, P (? ? ?|z)
(4)
E-Step: ?nk =
pikP (tn|sn, z = k)
?K
k=1 pikP (tn|sn, z = k)
, (5)
P (tn|sn, z) = max
T?Part(tn),S?Part(sn)
|S|
?
i=1
P (?i ? ?i|z)
M-Step: pi?k =
Nk
N
, Nk =
N
?
n=1
?nk (6)
P (? ? ?|z = k)? = 1
Nk
N
?
n=1
?nk
fn(? ? ?)
?
??? fn(? ? ?)
Here, fn(? ? ?) is the frequency of substitution
pair ? ? ? in the n-th transliterated pair, whose
calculation method is explained in Section 2. The
final transliteration probability is given by:
Platent(t|s) =
?
z
P (t, z|s) =
?
z
P (z|s)P (t|s, z)
?
?
z
pikP (s|z)P (t|s, z) (7)
The proposed model cannot explicitly model
P (s|z), which is in practice approximated by
P (t|s, z). Even omitting this factor only has a
marginal effect on the performance (within 1.1%).
5 Experiments
Here we evaluate the performance of the transliter-
ation models as an information retrieval task, where
the model ranks target t? for a given source s?, based
on the model P (t?|s?). We used all the t?n in the
test set Xtest = {(s?n, t?n)|1 ? n ? M} as target
candidates and s?n for queries. Five-fold cross vali-
dation was adopted when learning the models, that
is, the datasets described in the next subsections are
equally splitted into five folds, of which four were
used for training and one for testing. The mean re-
ciprocal rank (MRR) of top 10 ranked candidates
was used as a performance measure.
5.1 Experimental Settings
Dataset 1: Western Person Name List This
dataset contains 6,717 Western person names and
their Katakana readings taken from an European
name website ?????? 1, consisting of Ger-
man (de), English (en), and French (fr) person name
pairs. The numbers of pairs for these languages are
2,470, 2,492, and 1,747, respectively. Accent marks
for non-English languages were left untouched. Up-
percase was normalized to lowercase.
Dataset 2: Western Proper Noun List This
dataset contains 11,323 proper nouns and their
Japanese counterparts extracted from Wikipedia in-
terwiki. The languages and numbers of pairs con-
tained are: German (de): 2,003, English (en): 5,530,
Spanish (es): 781, French (fr): 1,918, Italian (it):
1http://www.worldsys.org/europe/
55
Language de en fr
Precision(%) 80.4 77.1 74.7
Table 1: Language Class Detection Result (Dataset 1)
1,091. Linked English and Japanese titles are ex-
tracted, unless the Japanese title contains any other
characters than Katakana, hyphen, or middle dot.
The language origin of titles were detected
whether appropriate country names are included in
the first sentence of Japanese articles. If they con-
tain ????? (of Germany),? ?????? (of
France),? ?????? (of Italy),? they are marked
as German, French, and Italian origin, respectively.
If the sentence contains any of Spain, Argentina,
Mexico, Peru, or Chile plus ???(of), it is marked
as Spanish origin. If they contain any of Amer-
ica, England, Australia or Canada plus ???(of), it
is marked as English origin. The latter parts of
Japanese/foreign titles starting from ?,? or ?(? were
removed. Japanese and foreign titles were split into
chunks by middle dots and ? ?, respectively, and re-
sulting chunks were aligned. Titles pairs with differ-
ent numbers of chunks, or ones with foreign char-
acter length less than 3 were excluded. All accent
marks were normalized (German ??? was converted
to ?ss?).
Implementation Details P (c|s) of the class
transliteration model was calculated by a charac-
ter 3-gram language model with Witten-Bell dis-
counting. Japanese Katakanas were all converted
to Hepburn-style Roman characters, with minor
changes so as to incorporate foreign pronunciations
such as ?wi / ??? and ?we / ??.? The hyphens
??? were replaced by the previous vowels (e.g., ??
??????? is converted to ?supagettii.?)
The maximum length of substitution pairs W de-
scribed in Section 2 was set W = 2. The EM al-
gorithm parameters P (? ? ?|z) were initialized to
the probability P (? ? ?) of the alpha-beta model
plus Gaussian noise, and pik were uniformly initial-
ized to 1/K. Based on the preliminary results, we
repeated EM iterations for 40 times.
5.2 Results
Language Class Detection We firstly show the
precision of language detection using the class
Language de en es fr it
Precision(%) 65.4 83.3 48.2 57.7 66.1
Table 2: Language Class Detection Result (Dataset 2)
Model Dataset 1 Dataset 2
AB 94.8 90.9
HARD 90.3 89.8
SOFT 95.7 92.4
LATENT 95.8 92.4
Table 3: Model Performance Comparison (MRR; %)
transliteration model P (c|s) and Equation (3) (Table
5.2, 5.2). The overall precision is relatively lower
than, e.g., Li et al (2007), which is attributed to the
fact that European names can be quite ambiguous
(e.g., ?Charles? can read ?????? cha?ruzu? or
????? sharuru?) The precision of Dataset 2 is
even worse because it has more classes. We can also
use the result of the latent class transliteration for
clustering by regarding k? = argmaxk ?nk as the
class of the pair. The resulting cluster purity way
was 0.74.
Transliteration Model Comparison We show
the evaluation results of transliteration candidate re-
trieval task using each of PAB(t|s) (AB), Phard(t|s)
(HARD), Psoft(t|s) (SOFT), and Platent(t|s) (LA-
TENT) (Table 5.2). The number of latent classes
was K = 3 for Dataset 1 and K = 5 for Dataset 2,
which are the same as the numbers of language ori-
gins. LATENT shows comparable performance ver-
sus SOFT, although it can be higher depending on
the value of K, as stated below. HARD, on the other
hand, shows lower performance, which is mainly
due to the low precision of class detection. The de-
tection errors are alleviated in SOFT by considering
the weighted sum of transliteration probabilities.
We also conducted the evaluation based on the
top-1 accuracy of transliteration candidates. Be-
cause we found out that the tendency of the results
is the same as MRR, we simply omitted the result in
this paper.
The simplest model AB incorrectly reads ?Felix
/ ??????,? ?Read / ???? as ?????
Firisu? and ????? Rea?do.? This may be because
English pronunciation ?x / ??? kkusu? and ?ea /
56
?? ?i? are influenced by other languages. SOFT
and LATENT can find correct candidates for these
pairs. Irregular pronunciation pairs such as ?Caen
/ ??? ka?n? (French; misread ????? sha?n?)
and ?Laemmle /??? Remuri? (English; misread
???? Riamu?) were misread by SOFT but not by
LATENT. For more irregular cases such as ?Hilda?
??? Iruda?(English), it is difficult to find correct
counterparts even by LATENT.
Finally, we investigated the effect of the number
of latent classes K. The performance is higher when
K is slightly smaller than the number of language
origins in the dataset (e.g., K = 4 for Dataset 2) but
the performance gets unstable for larger values of K
due to the EM algorithm initial values.
6 Conclusion
In this paper, we proposed a latent class translitera-
tion method which models source language origins
as latent classes. The model parameters are learned
from sets of transliterated words with different ori-
gins via the EM algorithm. The experimental re-
sult of Western person / proper name transliteration
task shows that, even though the proposed model
does not rely on explicit language origins, it achieves
higher accuracy versus conventional methods using
explicit language origins. Considering sources other
than Western languages as well as targets other than
Japanese is the future work.
References
Farooq Ahmad and Grzegorz Kondrak. 2005. Learning a
spelling error model from search query logs. In Proc.
of EMNLP-2005, pages 955?962.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling. In Proc. ACL-
2000, pages 286?293.
Eric Brill, Gary Kacmarcik, and Chris Brockett. 2001.
Automatically harvesting katakana-english term pairs
from search engine query logs. In Proc. NLPRS-2001,
pages 393?399.
Masato Hagiwara and Hisami Suzuki. 2009. Japanese
query alteration based on semantic similarity. In Proc.
of NAACL-2009, page 191.
Kevin Knight and Graehl Jonathan. 1998. Machine
transliteration. Computational Linguistics, 24:599?
612.
Haizhou Li, Khe Chai Sum, Jin-Shea Kuo, and Minghui
Dong. 2007. Semantic transliteration of personal
names. In Proc. of ACL 2007, pages 120?127.
Ariadna Font Llitjos and Alan W. Black. 2001. Knowl-
edge of language origin improves pronunciation accu-
racy. In Proc. of Eurospeech, pages 1919?1922.
57
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 183?189,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Accurate Word Segmentation using Transliteration
and Language Model Projection
Masato Hagiwara Satoshi Sekine
Rakuten Institute of Technology, New York
215 Park Avenue South, New York, NY
{masato.hagiwara, satoshi.b.sekine}@mail.rakuten.com
Abstract
Transliterated compound nouns not
separated by whitespaces pose diffi-
culty on word segmentation (WS). Of-
fline approaches have been proposed to
split them using word statistics, but
they rely on static lexicon, limiting
their use. We propose an online ap-
proach, integrating source LM, and/or,
back-transliteration and English LM.
The experiments on Japanese and Chi-
nese WS have shown that the pro-
posed models achieve significant im-
provement over state-of-the-art, reduc-
ing 16% errors in Japanese.
1 Introduction
Accurate word segmentation (WS) is the
key components in successful language pro-
cessing. The problem is pronounced in lan-
guages such as Japanese and Chinese, where
words are not separated by whitespaces. In
particular, compound nouns pose difficulties
to WS since they are productive, and often
consist of unknown words.
In Japanese, transliterated foreign com-
pound words written in Katakana are ex-
tremely difficult to split up into components
without proper lexical knowledge. For ex-
ample, when splitting a compound noun ?
???????? burakisshureddo, a traditional
word segmenter can easily segment this as ?
???/????? ?*blacki shred? since ????
? shureddo ?shred? is a known, frequent word.
It is only the knowledge that ????buraki
(*?blacki?) is not a valid word which prevents
this. Knowing that the back-transliterated un-
igram ?blacki? and bigram ?blacki shred? are
unlikely in English can promote the correct
WS, ??????/??? ?blackish red?. In Chi-
nese, the problem can be more severe since
the language does not have a separate script
to represent transliterated words.
Kaji and Kitsuregawa (2011) tackled
Katakana compound splitting using back-
transliteration and paraphrasing. Their ap-
proach falls into an offline approach, which
focuses on creating dictionaries by extract-
ing new words from large corpora separately
before WS. However, offline approaches have
limitation unless the lexicon is constantly
updated. Moreover, they only deal with
Katakana, but their method is not directly ap-
plicable to Chinese since the language lacks a
separate script for transliterated words.
Instead, we adopt an online approach, which
deals with unknown words simultaneously as
the model analyzes the input. Our ap-
proach is based on semi-Markov discrimina-
tive structure prediction, and it incorporates
English back-transliteration and English lan-
guage models (LMs) into WS in a seamless
way. We refer to this process of transliterat-
ing unknown words into another language and
using the target LM as LM projection. Since
the model employs a general transliteration
model and a general English LM, it achieves
robust WS for unknown words. To the best
of our knowledge, this paper is the first to use
transliteration and projected LMs in an online,
seamlessly integrated fashion for WS.
To show the effectiveness of our approach,
we test our models on a Japanese balanced cor-
pus and an electronic commerce domain cor-
pus, and a balanced Chinese corpus. The re-
sults show that we achieved a significant im-
provement in WS accuracy in both languages.
2 Related Work
In Japanese WS, unknown words are usu-
ally dealt with in an online manner with the
unknown word model, which uses heuristics
183
depending on character types (Kudo et al,
2004). Nagata (1999) proposed a Japanese un-
known word model which considers PoS (part
of speech), word length model and orthog-
raphy. Uchimoto et al (2001) proposed a
maximum entropy morphological analyzer ro-
bust to unknown words. In Chinese, Peng et
al. (2004) used CRF confidence to detect new
words.
For offline approaches, Mori and Nagao
(1996) extracted unknown word and estimated
their PoS from a corpus through distributional
analysis. Asahara and Matsumoto (2004)
built a character-based chunking model using
SVM for Japanese unknown word detection.
Kaji and Kitsuregawa (2011)?s approach is
the closest to ours. They built a model
to split Katakana compounds using back-
transliteration and paraphrasing mined from
large corpora. Nakazawa et al (2005) is
a similar approach, using a Ja-En dictionary
to translate compound components and check
their occurrence in an English corpus. Sim-
ilar approaches are proposed for other lan-
guages, such as German (Koehn and Knight,
2003) and Urdu-Hindi (Lehal, 2010). Correct
splitting of compound nouns has a positive ef-
fect on MT (Koehn and Knight, 2003) and IR
(Braschler and Ripplinger, 2004).
A similar problem can be seen in Korean,
German etc. where compounds may not be
explicitly split by whitespaces. Koehn and
Knight (2003) tackled the splitting problem in
German, by using word statistics in a mono-
lingual corpus. They also used the informa-
tion whether translations of compound parts
appear in a German-English bilingual corpus.
Lehal (2010) used Urdu-Devnagri translitera-
tion and a Hindi corpus for handling the space
omission problem in Urdu compound words.
3 Word Segmentation Model
Out baseline model is a semi-Markov struc-
ture prediction model which estimates WS and
the PoS sequence simultaneously (Kudo et al,
2004; Zhang and Clark, 2008). This model
finds the best output y? from the input sen-
tence string x as: y? = argmaxy?Y (x) w ??(y).
Here, Y (x) denotes all the possible sequences
of words derived from x. The best analysis is
determined by the feature function ?(y) the
ID Feature ID Feature
1 wi 13 w1i?1w1i
2 t1i 14 t1i?1t1i
3* t1i t2i 15* t1i?1t2i?1t1i t2i
4* t1i t2i t3i 16* t1i?1t2i?1t3i?1t1i t2i t3i
5* t1i t2i t5i t6i 17* t1i?1t2i?1t5i?1t6i?1t1i t2i t5i t6i
6* t1i t2i t6i 18* t1i?1t2i?1t6i?1t1i t2i t6i
7 wit1i 19 ?LMS1 (wi)
8* wit1i t2i 20 ?LMS2 (wi?1, wi)
9* wit1i t2i t3i 21 ?LMP1 (wi)
10* wit1i t2i t5i t6i 22 ?LMP2 (wi?1, wi)
11* wit1i t2i t6i
12 c(wi)l(wi)
Table 1: Features for WS & PoS tagging
weight vector w. WS is conducted by stan-
dard Viterbi search based on lattice, which
is illustrated in Figure 1. We limit the fea-
tures to word unigram and bigram features,
i.e., ?(y) = ?i[?1(wi) + ?2(wi?1, wi)] for y =
w1...wn. By factoring the feature function into
these two subsets, argmax can be efficiently
searched by the Viterbi algorithm, with its
computational complexity proportional to the
input length. We list all the baseline features
in Table 11. The asterisks (*) indicate the fea-
ture is used for Japanese (JA) but not for Chi-
nese (ZH) WS. Here, wi and wi?1 denote the
current and previous word in question, and tji
and tji?1 are level-j PoS tags assigned to them.
l(w) and c(w) are the length and the set of
character types of word w.
If there is a substring for which no dic-
tionary entries are found, the unknown word
model is invoked. In Japanese, our unknown
word model relies on heuristics based on char-
acter types and word length to generate word
nodes, similar to that of MeCab (Kudo et
al., 2004). In Chinese, we aggregated con-
secutive 1 to 4 characters add them as ?n
(common noun)?, ?ns (place name)?, ?nr (per-
sonal name)?, and ?nz (other proper nouns),?
since most of the unknown words in Chinese
are proper nouns. Also, we aggregated up to
20 consecutive numerical characters, making
them a single node, and assign ?m? (number).
For other character types, a single node with
PoS ?w (others)? is created.
1The Japanese dictionary and the corpus we used
have 6 levels of PoS tag hierarchy, while the Chinese
ones have only one level, which is why some of the
PoS features are not included in Chinese. As character
type, Hiragana (JA), Katakana (JA), Latin alphabet,
Number, Chinese characters, and Others, are distin-
guished. Word length is in Unicode.
184
Input: ? ? ? ? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ?
?
? ?
? ?
? ? ? EOS? ? ? ?
? ? ? ? ?? ? ? ?
? ? ?
. . . 
. . . 
? ? ?
? ? ?
? ? ? ?
?BOS
very  popular       color                     blackish                            red
bu ki
bla bra kish
brakiblaki
brakiblaki
brackishblackish
ledreadred
shreadshred
node (a)
node (b)
edge (c)
TransliterationModel
English
LM
Figure 1: Example lattice with LM projection
4 Use of Language Model
Language Model Augmentation Analo-
gous to Koehn and Knight (2003), we can ex-
ploit the fact that ??? reddo (red) in the
example ????????? is such a common
word that one can expect it appears frequently
in the training corpus. To incorporate this
intuition, we used log probability of n-gram
as features, which are included in Table 1
(ID 19 and 20): ?LMS1 (wi) = log p(wi) and
?LMS2 (wi?1, wi) = log p(wi?1, wi). Here the
empirical probability p(wi) and p(wi?1, wi) are
computed from the source language corpus. In
Japanese, we applied this source language aug-
mentation only to Katakana words. In Chi-
nese, we did not limit the target.
4.1 Language Model Projection
As we mentioned in Section 2, English
LM knowledge helps split transliterated com-
pounds. We use (LM) projection, which is
a combination of back-transliteration and an
English model, by extending the normal lat-
tice building process as follows:
Firstly, when the lattice is being built, each
node is back-transliterated and the resulting
nodes are associated with it, as shown in
Figure 1 as the shaded nodes. Then, edges
are spanned between these extended English
nodes, instead of between the original nodes,
by additionally taking into consideration En-
glish LM features (ID 21 and 22 in Table 1):
?LMP1 (wi) = log p(wi) and ?LMP2 (wi?1, wi) =
log p(wi?1, wi). Here the empirical probabil-
ity p(wi) and p(wi?1, wi) are computed from
the English corpus. For example, Feature 21
is set to ?LMP1 (?blackish?) for node (a), to
?LMP1 (?red?) for node (b), and Feature 22 is
set to ?LMP2 (?blackish?, ?red?) for edge (c) in
Figure 1. If no transliterations were generated,
or the n-grams do not appear in the English
corpus, a small frequency ? is assumed.
Finally, the created edges are traversed from
EOS, and associated original nodes are chosen
as the WS result. In Figure 1, the bold edges
are traversed at the final step, and the corre-
sponding nodes ?? - ?? - ? - ??????-
???? are chosen as the final WS result.
For Japanese, we only expand and project
Katakana noun nodes (whether they are
known or unknown words) since transliterated
words are almost always written in Katakana.
For Chinese, only ?ns (place name)?, ?nr (per-
sonal name)?, and ?nz (other proper noun)?
nodes whose surface form is more than 1-
character long are transliterated. As the En-
glish LM, we used Google Web 1T 5-gram Ver-
sion 1 (Brants and Franz, 2006), limiting it to
unigrams occurring more than 2000 times and
bigrams occurring more than 500 times.
5 Transliteration
For transliterating Japanese/Chinese words
back to English, we adopted the Joint Source
Channel (JSC) Model (Li et al, 2004), a gen-
erative model widely used as a simple yet pow-
erful baseline in previous research e.g., (Hagi-
wara and Sekine, 2012; Finch and Sumita,
2010).2 The JSC model, given an input
of source word s and target word t, de-
fines the transliteration probability based on
transliteration units (TUs) ui = ?si, ti? as:
PJSC(?s, t?) =
?f
i=1 P (ui|ui?n+1, ..., ui?1),
where f is the number of TUs in a given source
/ target word pair. TUs are atomic pair units
of source / target words, such as ?la/?? and
?ish/????. The TU n-gram probabilities are
learned from a training corpus by following it-
erative updates similar to the EM algorithm3.
In order to generate transliteration candidates,
we used a stack decoder described in (Hagi-
wara and Sekine, 2012). We used the training
data of the NEWS 2009 workshop (Li et al,
2009a; Li et al, 2009b).
As reference, we measured the performance
on its own, using NEWS 2009 (Li et al, 2009b)
data. The percentage of correctly transliter-
ated words are 37.9% for Japanese and 25.6%
2Note that one could also adopt other generative /
discriminative transliteration models, such as (Jiampo-
jamarn et al, 2007; Jiampojamarn et al, 2008).
3We only allow TUs whose length is shorter than or
equal to 3, both in the source and target side.
185
for Chinese. Although the numbers seem low
at a first glance, Chinese back-transliteration
itself is a very hard task, mostly because
Chinese phonology is so different from En-
glish that some sounds may be dropped when
transliterated. Therefore, we can regard this
performance as a lower bound of the translit-
eration module performance we used for WS.
6 Experiments
6.1 Experimental Settings
Corpora For Japanese, we used (1) EC
corpus, consists of 1,230 product titles and
descriptions randomly sampled from Rakuten
(Rakuten-Inc., 2012). The corpus is manually
annotated with the BCCWJ style WS (Ogura
et al, 2011). It consists of 118,355 tokens, and
has a relatively high percentage of Katakana
words (11.2%). (2) BCCWJ (Maekawa, 2008)
CORE (60,374 sentences, 1,286,899 tokens,
out of which approx. 3.58% are Katakana
words). As the dictionary, we used UniDic
(Den et al, 2007). For Chinese, we used
LCMC (McEnery and Xiao, 2004) (45,697 sen-
tences and 1,001,549 tokens). As the dictio-
nary, we used CC-CEDICT (MDGB, 2011)4.
Training and Evaluation We used Aver-
aged Perceptron (Collins, 2002) (3 iterations)
for training, with five-fold cross-validation. As
for the evaluation metrics, we used Precision
(Prec.), Recall (Rec.), and F-measure (F). We
additionally evaluated the performance lim-
ited to Katakana (JA) or proper nouns (ZH)
in order to see the impact of compound split-
ting. We also used word error rate (WER) to
see the relative change of errors.
6.2 Japanese WS Results
We compared the baseline model, the
augmented model with the source language
(+LM-S) and the projected model (+LM-P).
Table 3 shows the result of the proposed mod-
els and major open-source Japanese WS sys-
tems, namely, MeCab 0.98 (Kudo et al, 2004),
JUMAN 7.0 (Kurohashi and Nagao, 1994),
4Since the dictionary is not explicitly annotated
with PoS tags, we firstly took the intersection of the
training corpus and the dictionary words, and assigned
all the possible PoS tags to the words which appeared
in the corpus. All the other words which do not appear
in the training corpus are discarded.
and KyTea 0.4.2 (Neubig et al, 2011) 5. We
observed slight improvement by incorporat-
ing the source LM, and observed a 0.48 point
F-value increase over baseline, which trans-
lates to 4.65 point Katakana F-value change
and 16.0% (3.56% to 2.99 %) WER reduc-
tion, mainly due to its higher Katakana word
rate (11.2%). Here, MeCab+UniDic achieved
slightly better Katakana WS than the pro-
posed models. This may be because it is
trained on a much larger training corpus (the
whole BCCWJ). The same trend is observed
for BCCWJ corpus (Table 2), where we gained
statistically significant 1 point F-measure in-
crease on Katakana word.
Many of the improvements of +LM-S over
Baseline come from finer grained splitting,
for example, * ?????? reinsuutsu ?rain
suits? to ???/???, while there is wrong
over-splitting, e.g., ???????terekyasutaa
?Telecaster? to * ??/?????. This type of
error is reduced by +LM-P, e.g., * ???/??
? purasu chikku ?*plus tick? to ??????
purasuchikku ?plastic? due to LM projection.
+LM-P also improved compounds whose com-
ponents do not appear in the training data,
such as * ???????? ruukasufirumu to
????/???? ?Lucus Film.? Indeed, we
randomly extracted 30 Katakana differences
between +LM-S and +LM-P, and found out
that 25 out of 30 (83%) are true improvement.
One of the proposed method?s advantages is
that it is very robust to variations, such as
?????????? akutibeitiddo ?activated,?
even though only the original form, ?????
?? akutibeito ?activate? is in the dictionary.
One type of errors can be attributed
to non-English words such as ??????
sunokobeddo, which is a compound of Japanese
word ??? sunoko ?duckboard? and an En-
glish word ??? beddo ?bed.?
6.3 Chinese WS Results
We compare the results on Chinese WS,
with Stanford Segmenter (Tseng et al, 2005)
(Table 4) 6. Including +LM-S decreased the
5Because MeCab+UniDic and KyTea models are
actually trained on BCCWJ itself, this evaluation is
not meaningful but just for reference. The WS granu-
larity of IPADic, JUMAN, and KyTea is also different
from the BCCWJ style.
6Note that the comparison might not be fair since
(1) Stanford segmenter?s criteria are different from
186
Model Prec. (O) Rec. (O) F (O) Prec. (K) Rec. (K) F (K) WER
MeCab+IPADic 91.28 89.87 90.57 88.74 82.32 85.41 12.87
MeCab+UniDic* (98.84) (99.33) (99.08) (96.51) (97.34) (96.92) (1.31)
JUMAN 85.66 78.15 81.73 91.68 88.41 90.01 23.49
KyTea* (81.84) (90.12) (85.78) (99.57) (99.73) (99.65) (20.02)
Baseline 96.36 96.57 96.47 84.83 84.36 84.59 4.54
+LM-S 96.36 96.57 96.47 84.81 84.36 84.59 4.54
+LM-S+LM-P 96.39 96.61 96.50 85.59 85.40 85.50 4.50
Table 2: Japanese WS Performance (%) on BCCWJ ? Overall (O) and Katakana (K)
Model Prec. (O) Rec. (O) F (O) Prec. (K) Rec. (K) F (K) WER
MeCab+IPADic 84.36 87.31 85.81 86.65 73.47 79.52 20.34
MeCab+UniDic 95.14 97.55 96.33 93.88 93.22 93.55 5.46
JUMAN 90.99 87.13 89.2 92.37 88.02 90.14 14.56
KyTea 82.00 86.53 84.21 93.47 90.32 91.87 21.90
Baseline 97.50 97.00 97.25 89.61 85.40 87.45 3.56
+LM-S 97.79 97.37 97.58 92.58 88.99 90.75 3.17
+LM-S+LM-P 97.90 97.55 97.73 93.62 90.64 92.10 2.99
Table 3: Japanese WS Performance (%) on the EC domain corpus
Model Prec. (O) Rec. (O) F (O) Prec. (P) Rec. (P) F (P) WER
Stanford Segmenter 87.06 86.38 86.72 ? ? ? 17.45
Baseline 90.65 90.87 90.76 83.29 51.45 63.61 12.21
+LM-S 90.54 90.78 90.66 72.69 43.28 54.25 12.32
+LM-P 90.90 91.48 91.19 75.04 52.11 61.51 11.90
Table 4: Chinese WS Performance (%) ? Overall (O) and Proper Nouns (P)
performance, which may be because one can-
not limit where the source LM features are
applied. This is why the result of +LM-
S+LM-P is not shown for Chinese. On the
other hand, replacing LM-S with LM-P im-
proved the performance significantly. We
found positive changes such as * ??/?
??? oumai/ersalihe to ???/???
oumaier/salihe ?Umar Saleh? and * ??/?
??? lingdao/renmandela to ???/???
lingdaoren/mandela?Leader Mandela?. How-
ever, considering the overall F-measure in-
crease and proper noun F-measure decrease
suggests that the effect of LM projection is
not limited to proper nouns but also promoted
finer granularity because we observed proper
noun recall increase.
One of the reasons which make Chinese LM
projection difficult is the corpus allows sin-
gle tokens with a transliterated part and Chi-
nese affices, e.g., ?????? makesizhuy-
izhe ?Marxists? (??? makesi ?Marx? + ?
?? zhuyizhe ?-ist (believers)?) and ???
niluohe ?Nile River? ( ?? niluo ?Nile? +
? he ?-river?). Another source of errors is
transliteration accuracy. For example, no ap-
ours, and (2) our model only uses the intersection of
the training set and the dictionary. Proper noun per-
formance for the Stanford segmenter is not shown since
it does not assign PoS tags.
propriate transliterations were generated for
??? weinasi ?Venus,? which is commonly
spelled ??? weinasi. Improving the JSC
model could improve the LM projection per-
formance.
7 Conclusion and Future Works
In this paper, we proposed a novel, on-
line WS model for the Japanese/Chinese
compound word splitting problem, by seam-
lessly incorporating the knowledge that back-
transliteration of properly segmented words
also appear in an English LM. The experi-
mental results show that the model achieves
a significant improvement over the baseline
and LM augmentation, achieving 16% WER
reduction in the EC domain.
The concept of LM projection is general
enough to be used for splitting other com-
pound nouns. For example, for Japanese per-
sonal names such as ???? Naka Riisa, if
we could successfully estimate the pronuncia-
tion Nakariisa and look up possible splits in
an English LM, one is expected to find a cor-
rect WS Naka Riisa because the first and/or
the last name are mentioned in the LM. Seek-
ing broader application of LM projection is a
future work.
187
References
Masayuki Asahara and Yuji Matsumoto. 2004.
Japanese unknown word identification by
character-based chunking. In Proceedings of
COLING 2004, pages 459?465.
Thorsten Brants and Alex Franz. 2006. Web 1T
5-gram Version 1. Linguistic Data Consortium.
Martin Braschler and B?rbel Ripplinger. 2004.
How effective is stemming and decompounding
for german text retrieval? Information Re-
trieval, pages 291?316.
Michael Collins. 2002. Discriminative training
methods for hidden markov models: theory and
experiments with perceptron algorithms. In
Proceedings of EMNLP 2012, pages 1?8.
Yasuharu Den, Toshinobu Ogiso, Hideki Ogura,
Atsushi Yamada, Nobuaki Minematsu, Kiy-
otaka Uchimoto, and Hanae Koiso. 2007.
The development of an electronic dictionary
for morphological analysis and its application
to Japanese corpus linguistics (in Japanese).
Japanese linguistics, 22:101?122.
Andrew Finch and Eiichiro Sumita. 2010. A
bayesian model of bilingual segmentation for
transliteration. In Proceedings of IWSLT 2010,
pages 259?266.
Masato Hagiwara and Satoshi Sekine. 2012. La-
tent class transliteration based on source lan-
guage origin. In Proceedings of NEWS 2012,
pages 30?37.
Sittichai Jiampojamarn, Grzegorz Kondrak, and
Tarek Sherif. 2007. Applying many-to-many
alignments and hidden markov models to letter-
to-phoneme conversion. In Proceedings of
NAACL-HLT 2007, pages 372?379.
Sittichai Jiampojamarn, Colin Cherry, and Grze-
gorz Kondrak. 2008. Joint processing and dis-
criminative training for letter-to-phoneme con-
version. In Proceedings of ACL 2008, pages
905?913.
Nobuhiro Kaji and Masaru Kitsuregawa. 2011.
Splitting noun compounds via monolingual and
bilingual paraphrasing: A study on japanese
katakana words. In Proceedings of the EMNLP
2011, pages 959?969.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings
of EACL 2003, pages 187?193.
Taku Kudo, Kaoru Yamamoto, and Yuji Mat-
sumoto. 2004. Applying conditional random
fields to Japanese morphological analysis. In
Proceedings of EMNLP 2004, pages 230?237.
Sadao Kurohashi and Makoto Nagao. 1994. Im-
provements of Japanese morphological analyzer
juman. In Proceedings of the International
Workshop on Sharable Natural Language Re-
sources, pages 22?38.
Gurpreet Singh Lehal. 2010. A word segmentation
system for handling space omission problem in
urdu script. In Proceedings of the 1st Workshop
on South and Southeast Asian Natural Language
Processing (WSSANLP), pages 43?50.
Haizhou Li, Zhang Min, and Su Jian. 2004. A
joint source-channel model for machine translit-
eration. In Proceedings of ACL 2004, pages
159?166.
Haizhou Li, A Kumaran, Vladimir Pervouchine,
and Min Zhang. 2009a. Report of news 2009
machine transliteration shared task. In Proceed-
ings of NEWS 2009, pages 1?18.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009b. Whitepaper of news 2009
machine transliteration shared task. In Proceed-
ings of NEWS 2009, pages 19?26.
Kikuo Maekawa. 2008. Compilation of
the Kotonoha-BCCWJ corpus (in Japanese).
Nihongo no kenkyu (Studies in Japanese),
4(1):82?95.
Anthony McEnery and Zhonghua Xiao. 2004. The
lancaster corpus of mandarin chinese: A corpus
for monolingual and contrastive language study.
In Proceedings of LREC 2004, pages 1175?1178.
MDGB. 2011. CC-CEDICT,
Retreived August, 2012 from
http://www.mdbg.net/chindict/chindict.php?
page=cedict.
Shinsuke Mori and Makoto Nagao. 1996. Word
extraction from corpora and its part-of-speech
estimation using distributional analysis. In Pro-
ceedings of COLING 2006, pages 1119?1122.
Masaaki Nagata. 1999. A part of speech estima-
tion method for Japanese unknown words using
a statistical model of morphology and context.
In Proceedings of ACL 1999, pages 277?284.
Toshiaki Nakazawa, Daisuke Kawahara, and Sadao
Kurohashi. 2005. Automatic acquisition of ba-
sic katakana lexicon from a given corpus. In
Proceedings of IJCNLP 2005, pages 682?693.
Graham Neubig, Yosuke Nakata, and Shinsuke
Mori. 2011. Pointwise prediction for robust,
adaptable Japanese morphological analysis. In
Proceedings of ACL-HLT 2011, pages 529?533.
Hideki Ogura, Hanae Koiso, Yumi Fujike, Sayaka
Miyauchi, and Yutaka Hara. 2011. Mor-
phological Information Guildeline for BCCWJ:
Balanced Corpus of Contemporary Written
188
Japanese, 4th Edition. National Institute for
Japanese Language and Linguistics.
Fuchun Peng, Fangfang Feng, and Andrew McCal-
lum. 2004. Chinese segmentation and new word
detection using conditional random fields. In
Proceedings COLING 2004.
Rakuten-Inc. 2012. Rakuten Ichiba
http://www.rakuten.co.jp/.
Huihsin Tseng, Pichuan Chang, Galen Andrew,
Daniel Jurafsky, and Christopher Manning.
2005. A conditional random field word seg-
menter. In Fourth SIGHAN Workshop on Chi-
nese Language Processing.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi
Isahara. 2001. Morphological analysis based
on a maximum entropy model ? an ap-
proach to the unknown word problem ? (in
Japanese). Journal of Natural Language Pro-
cessing, 8:127?141.
Yue Zhang and Stephen Clark. 2008. Joint word
segmentation and pos tagging using a single per-
ceptron. In Proceedings of ACL 2008, pages
888?896.
189
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 30?37,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Latent Semantic Transliteration using Dirichlet Mixture
Masato Hagiwara Satoshi Sekine
Rakuten Institute of Technology, New York
215 Park Avenue South, New York, NY
{masato.hagiwara, satoshi.b.sekine}@mail.rakuten.com
Abstract
Transliteration has been usually recog-
nized by spelling-based supervised models.
However, a single model cannot deal with
mixture of words with different origins,
such as ?get? in ?piaget? and ?target?.
Li et al (2007) propose a class translit-
eration method, which explicitly models
the source language origins and switches
them to address this issue. In contrast
to their model which requires an explic-
itly tagged training corpus with language
origins, Hagiwara and Sekine (2011) have
proposed the latent class transliteration
model, which models language origins as
latent classes and train the transliteration
table via the EM algorithm. However, this
model, which can be formulated as uni-
gram mixture, is prone to overfitting since
it is based on maximum likelihood estima-
tion. We propose a novel latent seman-
tic transliteration model based on Dirichlet
mixture, where a Dirichlet mixture prior
is introduced to mitigate the overfitting
problem. We have shown that the pro-
posed method considerably outperform the
conventional transliteration models.
1 Introduction
Transliteration (e.g., ?????? baraku
obama ?Barak Obama?) is phonetic transla-
tion between languages with different writing
systems, which is a major way of importing
foreign words into different languages. Su-
pervised, spelling-based grapheme-to-grapheme
models such as (Brill and Moore, 2000; Li et
al., 2004), which directly align characters in the
training corpus without depending on phonetic
information, and statistically computing their
correspondence, have been a popular method to
detect and/or generate transliterations, in con-
trast to phonetic-based methods such as (Knight
and Jonathan, 1998). However, single, mono-
lithic models fail to deal with sets of foreign
words with multiple language origins mixed to-
gether. For example, the ?get? part of ?pi-
aget / ????piaje? and ?target / ????
? t?getto? differ in pronunciation and spelling
correspondence depending on their source lan-
guages, which are French and English in this
case.
To address this issue, Li et al (2007) have
proposed class transliteration model, which ex-
plicitly models and classifies classes of languages
(such as Chinese Hanzi, Japanese Katakana,
and so on) and genders, and switches corre-
sponding transliteration models based on the
input. This model requires training sets of
transliterated word pairs tagged with language
origin, which is difficult to obtain. Hagiwara
and Sekine proposed the latent class translitera-
tion (LCT) model (Hagiwara and Sekine, 2011),
which models source language origins as directly
unobservable latent classes and applies appro-
priate transliteration models to given transliter-
ation pairs. The model parameters are learned
from corpora without language origins in an un-
supervised manner. This enables us to correctly
assign latent classes for English and French to
?piaget / ????piaje? and ?target / ????
30
? t?getto? and to identify their transliteration
correspondence correctly. However, this model
is based on maximum likelihood estimation on
multinomials and thus sensitive to noise in the
training data such as transliteration pairs with
irregular pronunciation, and tends to overfit the
data.
Considering the atomic re-writing unit
(transliteration unit, or TU, e.g., ?get / ??
? getto?) as a word, and a transliteration pair
as a document consisting of a word sequence,
class-based transliteration can be modeled by
the perfect analogy to document topic models
proposed in tha past. In fact, the LCT model,
where the transliteration probability is defined
by a mixture of multinomials, can be regarded
as a variant of a topic model, namely Unigram
Mixuture (UM) (Nigam et al, 2000). There
has been an extension of unigram mixture pro-
posed (Sj?lander et al, 1996; Yamamoto and
Sadamitsu, 2005) which introduces a Dirichlet
mixture distribution as a prior and alleviates the
overfitting problem. We can expect to improve
the transliteration accuracy by formulating the
transliteration problem using a similar frame-
work to these topic models.
In this paper, we formalize class-based
transliteration based on language origins in the
framework of topic models. We then propose the
latent semantic transliteration model based on
Dirichlet mixture (DM-LST). We show through
experiments that it can significantly improve the
transliteration performance by alleviating the
overfitting issue.
Note that we tackle the task of transliteration
generation in this paper, in contrast to translit-
eration recognition. A transliteration generation
task is, given an input word s (such as ?piaget?),
the system is asked to generate from scratch
the most probable transliterated word t (e.g.,
?????piaje?). The transliteration recogni-
tion task, on the other hand, is to induce the
most probable transliteration t? ? T such that
t? = arg maxt?T P (?s, t?) given the input word
s and a pool of transliteration candidates T . We
call P (?s, t?) transliteration model in this paper.
This model can be regarded as the hy-
brid of an unsupervised alignment technique
for transliteration and class-based translitera-
tion. Related researches for the former in-
clude (Ahmad and Kondrak, 2005), who esti-
mate character-based error probabilities from
query logs via the EM algorithm. For the lat-
ter, Llitjos and Black (2001) showed that source
language origins may improve the pronunciation
of proper nouns in text-to-speech systems.
The structure of this paper is as follows:
we introduce the alpha-beta model(Brill and
Moore, 2000) in Section 2, which is the most ba-
sic spelling-based transliteration model on which
other models are based. In the following Section
3, we introduce and relate the joint source chan-
nel (JSC) model (Li et al, 2004) to the alpha-
beta model. We describe the LCT model as an
extension to the JSC model in Section 4. In
Section 5, we propose the DM-LST model, and
show the experimental results on transliteration
generation in Section 6.
2 Alpha-Beta Model
In this section, we describe the alpha-beta
model, which is one of the simplest spelling-
based transliteration models. Though simple,
the model has been shown to achieve better
performance in tasks such as spelling correction
(Brill and Moore, 2000), transliteration (Brill et
al., 2001), and query alteration (Hagiwara and
Suzuki, 2009).
The method directly models spelling-based
re-writing probabilities of transliteration pairs.
It is an extension to the normal edit distance,
where the cost of operations (substitution, in-
sertion, and deletion) is fixed to 1, and assigns a
probability to a string edit operation of the form
si ? ti (si and ti are any substrings of length 0
to w). We call the unit operation of string re-
writing ui = ?si, ti? as transliteration unit (TU)
as in (Li et al, 2004). The total transliteration
probability of re-writing a word s to t is given
by
PAB(?s, t?) = maxu1...uf
f
?
i=1
P (ui), (1)
where f is the number of TUs and u1...uf is any
sequence of TUs (e.g., ?pi ?? a ?? get ?
31
???) created by splitting up the input/output
transliteration pair ?s, t?. The above equa-
tion can be interpreted as a problem of find-
ing a TU sequence u1...uf which maximizes the
probability defined by the product of individ-
ual probabilities of independent TUs. After tak-
ing the logarithm of the both sides, and regard-
ing ? logP (ui) as the cost of string substitution
si ? ti, the problem is equivalent to minimizing
the sum of re-writing costs, and therefore can
be efficiently solved by dynamic programming
as done in the normal edit distance.
TU probabilities P (ui) are calculated from a
training set of transliteration pairs. However,
training sets usually lack alignment information
specifying which characters in s corresponding
which characters in t. Brill and Moore (2000)
resorted to heuristics to align same characters
and to induce the alignment of string chunks.
Hagiwara and Sekine (2011) converted Japanese
Katakana sequences into Roman alphabets be-
cause their model also assumed that the strings
si and ti are expressed in the same alphabet sys-
tem. Our method on the contrary, does not pose
such assumption so that strings in different writ-
ing systems (such as Japanese Katakana and
English alphabets, and Chinese characters and
English alphabets, etc.) can be aligned without
being converted to phonetic representation. For
this reason, we cannot adopt algorithms (such
as the one described in (Brill and Moore, 2000))
which heuristically infer alignment based on the
correspondence of the same characters.
When applying this alpha-beta model, we
computed TU probabilities by counting relative
frequencies of all the alignment possibilities for a
transliteration pair. For example, all the align-
ment possibilities for a pair of strings ?abc? and
?xy? are (a-x b-y c-?), (a-x b-? c-y), and (a-? b-x
c-y). By considering merging up to two adjacent
aligned characters in the first alignment, one ob-
tains the following five aligned string pairs: a-x,
b-y, c-?, ab-xy bc-y. Note that all the translit-
eration models described in this paper implic-
itly depend on the parameter w indicating the
maximum length of character n-grams. We fixed
w = 3 throughout this paper.
3 Joint Source Channel Model
The alpha-beta model described above has
shortcomings that the character alignment is
fixed based on heuristics, and it cannot cap-
ture the dependencies between TUs. One ex-
ample of such dependencies is the phenomenon
that the suffix ?-ed? in English verbs following
a voiced consonant is pronounced /d/, whereas
the one followed by an unvoiced consonant is
/t/. This section describes the JSC model(Li
et al, 2004), which was independently proposed
from the alpha-beta model. The JSC model is
essentially equivalent to the alpha-beta model
except: 1) it can also incorporate higher order
of n-grams of TUs and 2) the TU statistics is
taken not by fixing the heuristic initial align-
ment but by iteratively updating via an EM-like
algorithm.
In the JSC model, the transliteration proba-
bility is defined by the n-gram probabilities of
TUs ui = ?si, ti? as follows:
PJSC(?s, t?) =
f
?
i=1
P (ui|ui?n+1, ..., ui?1). (2)
Again, f is the number of TUs. The TU n-gram
probabilities P (ui|ui?n+1, ..., ui?1) can be calcu-
lated by the following iterative updates similar
to the EM algorithm:
1. Set the initial alignment randomly.
2. E-step: Take the TU n-gram statistics fix-
ing the current alignment, and update the
transliteration model.
3. M-step: Compute the alignment based on
the current transliteration model. The
alignment is inferred by dynamic program-
ming similar to the alpha-beta model.
4. Iterate the E- and M- step until conver-
gence.
Notice the alpha-beta model and the JSC
model are both transliteration recognition mod-
els. In order to output a transliterated word t
for a given input s, we generated transliteration
candidates with high probability using a stack
32
s
?s?
append reduce
(s,?)
(s,?)shift
s
?m?
(s,?)
(s,?)
sm
m
m
ap.
ap.
ap.
(s,?) (m,?)
(s,?) (m,?)
r.
?
reduce (sm,??)
smshift
s:? 5.22 s:? 6.69m: ? 6.14m: ? 8.47th: ? 6.72?
Figure 1: Overview of the stack decoder (generation
of ???? sumisu? from the input ?smith?)
decoder, whose overview is shown in Figure 1.
One character in the input string s (which is
?smith? in the figure) is given at a time, which
is appended at the end of the last TUs for each
candidate. (the append operation in the fig-
ure). Next, the last TU of each candidate is
either reduced or shifted. When it is reduced,
top R TUs with highest probabilities are gener-
ated and fixed referring to the TU table (shown
in the bottom-left of the figure). In Figure 1,
two candidates, namely (?s?, ?? su?) and (?s?,
?? zu?) are generated after the character ?s? is
given. When the last TU is shifted, it remains
unchanged and unfixed for further updates. Ev-
ery time a single character is given, the translit-
eration probability is computed using Eq. 2 for
each candidate, and all but the top-B candidates
with highest probabilities are discarded. The re-
duce width R and the beam width B were deter-
mined using the determined using development
sets, as mentioned in Section 6.
4 Latent Class Transliteration Model
As mentioned in Section 1, the alpha-beta
model and the JSC model build a single translit-
eration model which is simply the monolithic
average of training set statistics, failing to cap-
ture the difference in the source language ori-
gins. Li et al (2004) address this issue by defin-
ing classes c, i.e., the factors such as source lan-
guage origins, gender, and first/last names, etc.
which affect the transliteration probability. The
authors then propose the class transliteration
model which gives the probability of s ? t as
follows:
PLI(t|s) =
?
c
P (t, c|s) =
?
c
P (c|s)P (t|c, s) (3)
However, this model requires a training set
explicitly tagged with the classes. Instead of
assigning an explicit class c to each transliter-
ated pair, Hagiwara and Sekine (2011) introduce
a random variable z which indicates implicit
classes and conditional TU probability P (ui|z).
The latent class transliteration (LCT) model is
then defined as1:
PLCT(?s, t?) =
K
?
z=1
P (z)
f
?
i=1
P (ui|z) (4)
where K is the number of the latent classes.
The latent classes z correspond to classes such
as the language origins and genders mentioned
above, shared by sets of transliterated pairs with
similar re-writing characteristics. The classes z
are not directly observable from the training set,
but can be induced by maximizing the training
set likelihood via the EM algorithm as follows.
Parameters: P (z = k) = pik, P (ui|z) (5)
E-Step: ?nk =
pikP (?sn, tn?|z = k)
?K
k?=1 pik?P (?sn, tn?|z = k?)
, (6)
P (?sn, tn?|z) = maxu1..uf
fn
?
i=1
P (ui|z) (7)
M-Step: pinewk ?
N
?
n=1
?nk, (8)
P (ui|z = k)new =
1
Nk
N
?
n=1
?nk
fn(ui)
fn
(9)
where Nk =
?
n ?nk. Here, ?sn, tn? is the n-
th transliterated pair in the training set, and fn
and fn(ui) indicate how many TUs there are in
total in the n-th transliterated pair, and how
many times the TU ui appeared in it, respec-
tively. As done in the JSC model, we update the
alignment in the training set before the E-Step
for each iteration. Thus fn takes different values
1Note that this LCT model is formalized by intro-
ducing a latent variable to the transliteration generative
probability P (?s, t?) as in the JSC model, not to P (t|s).
33
from iteration to iteration in general. Further-
more, since the alignment is updated based on
P (ui|z) for each z = k, M different alignment
candidates are retained for each transliterated
pairs, which makes the value of fn dependent
on k, i.e., fkn . We initialize P (z = k) = 1/M
to and P (ui|z) = PAB(u) + ?, that is, the TU
probability induced by the alpha-beta algorithm
plus some random noise ?.
Considering a TU as a word, and a translit-
eration pair as a document consisting of a word
sequence, this LCT model defines the transliter-
ation probability as the mixture of multinomi-
als defined over TUs. This can be formulated
by unigram mixture (Nigam et al, 2000), which
is a topic model over documents. This follows a
generation story where documents (i.e., translit-
erated pairs) are generated firstly by choosing a
class z by P (z) and then by generating a word
(i.e., TU) by P (ui|z). Nevertheless, as men-
tioned in Section 1, since this model trains the
parameters based on the maximum likelihood
estimation over multinomials, it is vulnerable to
noise in the training set, thus prone to overfit
the data.
5 Latent Semantic Transliteration
Model based on Dirichlet Mixture
We propose the latent semantic translitera-
tion model based on Dirichlet mixture (DM-
LST), which is an extension to the LCT model
based on unigram mixture. This model enables
to prevent multinomials from being exceedingly
biased towards the given data, still being able to
model the transliteration generation by a mix-
ture of multiple latent classes, by introducing
Dirichlet mixture as a prior to TU multinomi-
als. The compound distribution of multinomi-
als when their parameters are given by Dirichlet
mixtures is given by the Polya mixture distribu-
tion(Yamamoto and Sadamitsu, 2005):
PDM (?s, t?) (10)
=
?
PMul(?s, t?;p)PDM (p;?,?K1 )dp
?
K
?
k=1
?kPPolya(?s, t?;?K1 ) (11)
=
K
?
k=1
?k
?(?k)
?(?k + f)
f
?
i=1
?(f(ui) + ?kui)
?(?kui)
where PMul(?;p) is multinomial with the pa-
rameter p. PDM is Dirichlet mixture, which
is a mixture (with co-efficients ?1, ..., ?K) of K
Dirichlet distributions with parameters ?K1 =
(?1,?2, ...,?K).
The model parameters can be induced by the
following EM algorithm. Notice that we adopted
a fast induction algorithm which extends an in-
duction method using leaving-one-out to mix-
ture distributions(Yamamoto et al, 2003).
Parameters: ? = (?1, ..., ?K),
(12)
?K1 = (?1,?2, ...,?K) (13)
E-Step: ?nk =
?kPPolya(?sn, tn?;?k)
?
k? ?k?PPolya(?sn, tn?;?k?)
(14)
M-Step: ?newk ?
N
?
n=1
?nk (15)
?newku = ?ku
?
n ?nk{fn(u)/(fn(u) ? 1 + ?ku)}
?
n ?nk{fn/(fn ? 1 + ?k)}
(16)
The prediction distribution when a sin-
gle TU u is the input is given PDM (u) =
?K
k=1 ?k?ku/?k. We therefore updated the
alignment in the training corpus, as done in the
JSC model updates, based on the probability
proportional to ?ku/?k for each k before ev-
ery M-Step. The parameters are initially set to
?k = 1/K, ?ku = PAB(u) + ?, as explained in
the previous section.
Since neither LCT nor DM-LST is a translit-
eration generation model, we firstly generated
transliteration candidates T by using the JSC
model and the stack decoder (Section 3) as a
34
baseline, then re-ranked the candidates using
the probabilities given by LCT (Eq. 4 or DM-
LST (Eq. 11), generating the re-ranked list
of transliterated outputs. Because the parame-
ters trained by the EM algorithm differ depend-
ing on the initial values, we trained 10 models
P 1DM , ..., P 10DM using the same training data and
random initial values and computed the aver-
age 110
?10
j=1 P
j
DM (?s, t?) to be used as the final
transliteration model.
It is worth mentioning that another topic
model, namely latent Dirichlet alocation (LDA)
(Blei et al, 2003), assumes that words in a doc-
ument can be generated from different topics
from each other. This assumption corresponds
to the notion that TUs in a single transliter-
ated pairs can be generated from different source
languages, which is presumably a wrong as-
sumption for transliteration tasks, probably ex-
cept for compound-like words with mixed ori-
gins such as ?na?veness?. In fact, we con-
firmed through a preliminary experiment that
LDA does not improve the transliteration per-
formance over the baseline.
6 Experiments
6.1 Evaluation
In this section, we compare the following
models: alpha-beta (AB), joint source channel
(JSC), latent class transliteration (LCT), and
latent semantic transliteration based on Dirich-
let mixture (DM-LST).
For the performance evaluation, we used three
language pairs, namely, English-Japanese (En-
Ja), English-Chinese (En-Ch), and English-
Korean (En-Ko), from the transliteration shared
task at NEWS 2009 (Li et al, 2009a; Li et al,
2009b). The size of each training/test set is
shown in the first column of Table 1. In general,
rn, a set of one or more reference transliterated
words, is associated with the n-th input sn in the
training/test corpus. Let cn,i, cn,2, ... be the out-
put of the transliteration system, i.e., the candi-
dates with highest probabilities assigned by the
transliteration model being evaluated. We used
the following three performance measures:
? ACC (averaged Top-1 accuracy): For ev-
ery ?sn, rn?, let an be an = 1 if the can-
didate with the highest probability cn,1 is
contained in the reference set rn and an =
0 otherwise. ACC is then calculated as
ACC 1N
?N
i=1 sn.
? MFS (mean F score): Let the
reference transliterated word clos-
est to the top-1 candidate cn, 1 be
r?n = arg minrn,j?rn ED(cn,1, rn,j), where
ED is the edit distance. The F-score of the
top candidate cn,1 for the n-th input sn is
then given by:
Pn = LSC(cn,1, r?n)/|cn,1| (17)
Rn = LCS(cn,1, r?n)/|r?n| (18)
Fn = 2RiPi/(Ri + Pi), (19)
where |x| is the length of string x, and
LCS(x, y) is the length of the longest com-
mon subsequence of x and y. Edit distance,
lengths of strings, and LCS are measured
in Unicode characters. Finally, MFS is de-
fined as MFS = 1N
?N
i=1 Fn.
? MRR (mean reciprocal rank): Of the
ranked candidates cn,1, cn,2, ..., let the high-
est ranked one which is also included in
the reference set rn be cn,j . We then
define reciprocal rank RRn = 1/j. If
none of the candidates are in the refer-
ence, RRn = 0. MRR is then defined by
MRR = 1N
?N
n=1RRn.
We used Kneser-Nay smoothing to smooth the
TU probabilities for LCT. The number of EM
iterations is fixed to 15 for all the models, based
on the result of preliminary experiments.
The reduce width R and the beam width B
for the stack decoder are fixed to R = 8 and
B = 32, because the transliteration generation
performance increased very little beyond these
widths based on the experiment using the de-
velopment set. We also optimized M , i.e., the
number of latent classes for LCT and DM-LST,
for each language pair and model in the same
way based on the development set.
35
Table 1: Performance comparison of transliteration
models
Language pair Model ACC MFS MRR
En-Ja AB 0.293 0.755 0.378
Train: 23,225 JSC 0.326 0.770 0.428
Test: 1,489 LCT 0.345 0.768 0.437
DM-LST 0.349 0.776 0.444
En-Ch AB 0.358 0.741 0.471
Train: 31,961 JSC 0.417 0.761 0.527
Test: 2,896 LCT 0.430 0.764 0.532
DM-LST 0.445 0.770 0.546
En-Ko AB 0.145 0.537 0.211
Train: 4,785 JSC 0.151 0.543 0.221
Test: 989 LCT 0.079 0.483 0.167
DM-LST 0.174 0.556 0.237
6.2 Results
We compared the performance of each
transliteration model in Table 1. For the lan-
guage pairs En-Ja and En-Ch, all the perfor-
mance increase in the order of AB < JSC <
LCT < DM-LST, showing the superiority our
proposed method. For the language pair En-
Ko, the performance for LCT re-ranking con-
siderably decreases compared to JSC. We sus-
pect this is due to the relatively small number
of training set, which caused the excessive fitting
to the data. We also found out that the optimal
value of M which maximizes the performance of
DM-LST is equal to or smaller than that of LCT.
This goes along with the findings (Yamamoto
and Sadamitsu, 2005) that Dirichlet mixture of-
ten achieves better language model perplexity
with smaller dimensionality compared to other
models.
Specific examples in the En-Ja test set whose
transliteration is improved by the proposed
methods include ?dijon ?????? dijon? and
?goldenberg ????????? g?rudenb?gu?.
Conventional methods, including LCT, sug-
gested ????? diyon? and ?????????
g?rudenberugu?, meaning that the translitera-
tion model is affected and biased towards non-
English pronunciation. The proposed method
can retain the major class of transliteration char-
acteristics (which is English in this case) and can
deal with multiple language origins depending
on transliteration pairs at the same time.
This trend can be also confirmed in other
language pairs, En-Ch and En-Ko. In En-Ch,
the transliterated words of ?covell? and ?nether-
wood? are improved ? ???? kefuer ???
? keweier? and ?????? neitehewude ??
??? neisewude?, respectively. in En-Ko, the
transliterated word of ?darling? is improved ??
?? dareuling? ? ??? dalling?.
We also observed that ?gutheim ?????
gutehaimu in En-Ch and martina ????
? mareutina in En-Ko are correctly translated
by the proposed method, even though they do
not have the English origin. Generally speak-
ing, however, how these non-English words are
pronounced depend on the context, as ?charles?
has different pronunciation in English and in
French, with the soft ?sh? sound at the begin-
ning. We need external clues to disambiguate
such transliteration, such as context information
and/or Web statistics.
7 Conclusion
In this paper, we proposed the latent seman-
tic transliteration model based on Dirichlet mix-
ture (DM-LST) as the extension to the latent
class transliteration model. The experimental
results showed the superior transliteration per-
formance over the conventional methods, since
DM-LST can alleviate the overfitting problem
and can capture multiple language origins. One
drawback is that it cannot deal with dependen-
cies of higher order of TU n-grams than bigrams.
How to incorporate these dependencies into the
latent transliteration models is the future work.
References
Farooq Ahmad and Grzegorz Kondrak. 2005. Learn-
ing a spelling error model from search query logs.
In Proc. of EMNLP-2005, pages 955?962.
David M. Blei, Andrew Y. Ng, and Michael I. Jor-
dan. 2003. Latent dirichlet alocation. Journal of
Machine Learning Research, 3:993?1022.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling. In Proc.
ACL-2000, pages 286?293.
36
Eric Brill, Gary Kacmarcik, and Chris Brockett.
2001. Automatically harvesting katakana-english
term pairs from search engine query logs. In Proc.
NLPRS-2001, pages 393?399.
Masato Hagiwara and Satoshi Sekine. 2011. Latent
class transliteration based on source language ori-
gin. In Proc. of ACL-HLT 2011, pages 53?57.
Masato Hagiwara and Hisami Suzuki. 2009.
Japanese query alteration based on semantic sim-
ilarity. In Proc. of NAACL-2009, page 191.
Kevin Knight and Graehl Jonathan. 1998. Ma-
chine transliteration. Computational Linguistics,
24:599?612.
Haizhou Li, Zhang Min, and Su Jian. 2004. A joint
source-channel model for machine transliteration.
In Proc. of ACL 2004, pages 159?166.
Haizhou Li, Khe Chai Sum, Jin-Shea Kuo, and
Minghui Dong. 2007. Semantic transliteration
of personal names. In Proc. of ACL 2007, pages
120?127.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009a. Report of news 2009 machine
transliteration shared task. In Proc. of the 2009
Named Entities Workshop, pages 1?18.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009b. Whitepaper of news 2009
machine transliteration shared task. In Proc. of
the 2009 Named Entities Workshop, pages 19?26.
Ariadna Font Llitjos and Alan W. Black. 2001.
Knowledge of language origin improves pronun-
ciation accuracy. In Proc. of Eurospeech, pages
1919?1922.
Kamal Nigam, Andrew Kachites McCallum, Sebas-
tian Thrun, and Tom Mitchell. 2000. Text clas-
sification from labeled and unlabeled documents
using em. Machine Learning, 39(2):103?134.
K. Sj?lander, K. Karplus, M. Brown, R. Hunghey,
A. Krogh, I.S. Mian, and D. Haussler. 1996.
Dirichlet mixtures:a method for improved detec-
tion of weak but significant protein sequence
homology. Computer Applications in the Bio-
sciences, 12(4):327?345.
Mikio Yamamoto and Kugatsu Sadamitsu. 2005.
Dirichlet mixtures in text modeling. CS Technical
Report, CS-TR-05-1.
Mikio Yamamoto, Kugatsu Sadamitsu, and Takuya
Mishina. 2003. Context modeling using dirichlet
mixtures and its applications to language models
(in japnaese). IPSJ, 2003-SLP-48:29?34.
37
