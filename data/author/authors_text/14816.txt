Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 239?247,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Facilitating Mental Modeling in Collaborative Human-Robot Interaction
through Adverbial Cues
Gordon Briggs and Matthias Scheutz
Human-Robot Interaction Laboratory
Tufts University, Medford, MA 02155, USA
{gbriggs,mscheutz}@cs.tufts.edu
Abstract
Mental modeling is crucial for natural human-
robot interactions (HRI). Yet, effective mech-
anisms that enable reasoning about and com-
munication of mental states are not available.
We propose to utilize adverbial cues, routinely
employed by humans, for this goal and present
a novel algorithm that integrates adverbial
modifiers with belief revision and expression,
phrasing utterances based on Gricean conver-
sational maxims. The algorithm is demon-
strated in a simple HRI scenario.
1 Introduction
Advances in robotics and autonomous systems are
paving the way for the development of robots that
can take on increasingly complex tasks without the
need of minute human supervision. As a result
of this greater autonomy, the interaction styles be-
tween humans and robots are slowly shifting from
those of humans micromanaging robot behaviors
(e.g., via remote controls) to more higher-level in-
teractions (e.g., verbal commands) which are re-
quired for many mixed initiative tasks where hu-
mans and robots work together in teams (e.g., in
search and rescue missions). In order for these joint
human-robot interactions to be productive and effi-
cient, robots must have the ability to communicate in
natural and human-like ways (Scheutz et al, 2007).
Natural human-like communication in robots, how-
ever, requires us to tackle several challenges, includ-
ing the development of robust natural language (NL)
competencies and the ability to understand and uti-
lize a variety of affective, gestural, and other non-
linguistic cues that are indicative of the interlocu-
tor?s mental states. Hence, natural human-like in-
teraction also requires the construction and mainte-
nance of mental models of other agents, especially
in the context of collaborative team tasks where ac-
tions among multiple agents must be coordinated,
often through natural language dialogues.
Several recent efforts are aimed at endowing
robots with natural language processing capabilities
to allow for verbal instructions as a first step (e.g.,
(Brenner, 2007; Dzifcak et al, 2009; Kress-Gazit et
al., 2008; Rybski et al, 2007; Kollar et al, 2010)).
Independently, user modeling has been extensively
explored in order to generate more natural and pro-
ductive human-machine interactions (Kobsa, 2001),
including adapting the natural language output of di-
alogue systems based on mental models of human-
users (Wahlster and Kobsa, 1989). However, there
is currently no integrated robotic architecture that
includes explicit mechanisms for efficiently convey-
ing natural language information about the robot?s
?mental states? (i.e., beliefs, goals, intentions) to a
human teammate. Yet, such mechanisms are not
only desirable to make the robot?s behavior more in-
tuitive and predictable to humans, but can also be
crucial for team success (e.g., quick updates on goal
achievement or early corrections of wrong human
assumptions).
We propose a novel integrated belief revision and
expression algorithm that allows robots to track and
update the beliefs of their interlocutors in a way
that respects Gricean maxims about language usage.
The algorithm explicitly models and updates task-
relevant beliefs and intentions of all participating
239
agents. Whenever a discrepancy is detected between
a human belief (as implied in a natural language ex-
pression uttered by the human) and the robot?s men-
tal model of the human, the robot generates a natural
language response that corrects the discrepancy in
the most effective way. To achieve effectiveness, the
robot uses linguistic rules about the pragmatic im-
plications of adverbial modifiers like ?yet?, ?still?,
?already?, and others that are used by humans to ef-
fectively communicate their beliefs and intentions.
The rest of the paper is organized as follows. We
start with a motivation of our approach based on
Gricean maxims. Then, we introduce formalizations
of linguistic devices that humans use to generate ef-
fective task-based dialogue interactions and present
our algorithm for generating appropriate utterances
in response to human queries. Next we use a simple
remote human-robot interaction scenario to demon-
strate the operation of the algorithm, followed by a
discussion and summary of our contributions.
2 Motivation
Joint activity often requires agents to monitor and
keep track of each others? mental states to ensure ef-
fective team performance. For example, searchers
during rescue operations in disaster zones typically
coordinate their (distributed and remote) activities
through spoken natural language interactions via
wireless audio links to keep team members informed
of discoveries and plans of other team members. Co-
ordination as part of joint activities requires two im-
portant processes in an agent: (1) building and main-
taining a mental model of the other agents? beliefs
and intentions (based on perceived, communicated,
and inferred information), which is critical for sit-
uational awareness (Lison et al, 2010); and (2) ac-
tively supporting the maintenance of others? mental
models of oneself (e.g., by proactively communicat-
ing new information to the other agents in ways that
will allow them to update their mental models).
Cohen et al (1990), for example, discuss the
necessity of various communicative acts that serve
to synchronize agent belief-models. These com-
municative acts include both linguistic and non-
linguistic cues, such as utterances of confirmation
(?okay.?) or signals that indicate intention (putting
on a turn-signal). In addition to utilizing explicit
cues to synchronize belief-models, humans employ
various other mechanisms to convey information
about one?s own belief-state, in particular, various
linguistic devices. A simple, but very powerful lin-
guistic mechanism is the use of adverbial cues.
Consider a scenario where one agent wants to
know the location of another agent, e.g., whether
the agent is at home. A straightforward way to ob-
tain this information is to simply ask ?Are you at
home?? The other agent can then answer ?yes? or
?no? accordingly. Now, suppose the first agent knew
that the second agent was planning to be at home at
some point. In that case, the agent might ask ?Are
you at home yet?? Note that semantically both ques-
tions have the same meaning, but their pragmatic
implications are different as the second implies that
that agent 1 knows that agent 2 was planning to be
at home, while no such implication can be inferred
from the first query. Conversely, suppose that agent
2 responded ?not yet? in the first example (instead
of ?no?). While the semantic meaning is the same as
?no?, ?not yet? communicates to agent 1 that agent
2 has the goal to be home. In general, adverbs like
?yet? can be used to convey information about one?s
(or somebody else?s) beliefs concerning mutually-
recognized goals and intentions. Not surprisingly,
humans use them regularly and with ease to aid their
interlocutors with maintaining an accurate model of
their beliefs and goals.
The challenges that need to be addressed to al-
low robots to have the above kinds of linguistic ex-
changes are: (1) how to formalize the functional
roles of adverbial modifiers in different sentence
types, and how to use the formalized principles to
(2) perform belief updates and (3) generate effective
natural language responses that are natural, succinct,
and complete. To tackle these three challenges, we
turn to Gricean principles that have long be used in
pragmatics as guiding principles of human commu-
nicative exchanges.
3 NL Understanding and Generation
Grice (1975) proposed four general principles to aid
in the pragmatic analysis of utterances. Phrased as
rules, it is unsurprising that they have been used
as an inspiration for NL generation systems before.
Dale and Reiter (1995) have enlisted the maxims in
240
their design of an algorithm to generate referring ex-
pressions, while others have cited Gricean influence
in utterance selection for intelligent tutor systems
(Eugenio et al, 2008). The particular maxims we
considered are the maxims of quality (G1), quantity
(G2), and relevance (G3): (G1) requires one to not
say what one believes is false or for which one lacks
adequate evidence; (G2) requires one to make con-
tributions as informative as necessary for the current
purposes of the exchange, but not more informative;
and (G3) tersely states ?be relevant.?
Our approach to belief-model synchronization
and utterance selection is based on the above max-
ims and attempts to select the most appropriate re-
sponse to another agent?s query based on relevance
of semantic content. It uses speech pragmatic mean-
ing postulates for linguistic devices such as adver-
bial modifiers to search for a succinct and natural
linguistic representation that captures the intended
updates. Rather than explicitly communicating each
and every proposition that needs to be communi-
cated to a human to allow the person to update
their mental model of the robot, the algorithm makes
heavy use of ?implied meanings?, i.e., propositions
that humans will infer from the way the informa-
tion is phrased linguistically. This allows for much
shorter messages to be communicated than other-
wise possible and addresses the second maxim of
quantity.
3.1 Formalizing pragmatic implications
We start by introducing four types of sentences
as they are found in typical dialogue interac-
tions: statements (expressed through declarative
sentences), questions (expressed through interrog-
ative sentences), commands (expressed through
imperative sentences) and acknowledgments (ex-
pressed through words like ?okay?, ?yes?, ?no?,
etc.). For simplicity, we restrict the discussion to
one predicate at(?, ?) which states that agent ? is
in location ?.
3.1.1 Statements
We will use the form Stmt(?, ?, ?, ?) to ex-
press that agent ? communicates ? to agent ? us-
ing adverbial modifiers in a set ?. For exam-
ple, Stmt(A2, A1,?at(A2, home), yet) means that
agent A2 tells A1 that it is not at home yet. Note
that we are indifferent about the exact linguistic rep-
resentation of ? here as the goal is to capture the
pragmatic implications.
If ? informs ? that it is at ? without any adverbial
modifiers or additional contextual information, then
we can assume using (G1) that ? is indeed at that
location:
[[Stmt(?, ?, at(?, ?), {})]]c := at(?, ?) (1)
Here we use [[..]]c to denote the ?pragmatic mean-
ing? of an expression in context c, which includes
task, goal, belief and discourse aspects. Next, we
inductively define the pragmatic meanings for sev-
eral adverbial modifiers ?still?, ?already?, ?now?,
and ?not yet? (the meanings of compound expres-
sions such as at(?, ?1) ? ?at(?, ?2) are defined re-
cursively in the usual way).
If ? states that it is ?still? at ?, one can infer that
? is at ? and that ? will not be at ? at some point in
the future:
[[Stmt(?, ?, at(?, ?), {still})]]c := (2)
[[Stmt(?, ?, at(?, ?)), {}]]c ? Future(?at(?, ?))
If ? states that it is ?already? at ?, one can infer that
? is at ? and that ? had a goal (expressed via the
?G? operator) to be at ? at some point in the past:
[[Stmt(?, at(?, ?), {already})]]c := (3)
[[Stmt(?, ?, at(?, ?), {}]]c ? Past(G(?, at(?, ?)))
If ? states that it is ?now? at ?, one can infer that ?
is at ? and that ? had not been at ? at some point in
the past:
[[Stmt(?, ?, at(?, ?), {now})]]c := (4)
[[Stmt(?, ?, at(?, ?)), {}]]c ? Past(?at(?, ?))
If ? states that it is ?not...yet? at ?, one can infer
that ? is not at ?, but has an intention to be at ?.
[[Stmt(?, ?,?at(?, ?), {yet})]]c := (5)
?at(?, ?) ?G(?, at(?, ?))
Even in our limited domain, one must be cog-
nizant of the ambiguities that arise from how ad-
verbial cues are deployed. In addition to the simple
presence of an adverbial cue, the location of the ad-
verb in a sentence and prosodic factors may affect
the intended meaning of the utterance. For instance,
consider the statements: (a) I am now at ?; (b) I am
241
at ? now; (c) I am still at ?; and (d) I am still at ?.
Statement (a) is a simple situational update utterance
as described above, while (b) could be construed as
a statement akin to ?I am already at ?. Statement
(d) could be interpreted as additionally signaling the
frustration of the agent, beyond conveying the infor-
mation from (c).
It should also be noted that our analysis of these
adverbial cues is to be understood in the limited
context of these simple task-related predicates (e.g.
at(?, ?)). Formal definition of these adverbial cues
in general cases is beyond the scope of this paper.
For instance, ?yet? could be used in a context when
the predicate is not intended by the agent to which it
applies (e.g. ?Has Bill been fired yet??). In this case,
it would probably be incorrect to infer that the agent
Bill had a goal to be fired. Instead an inference could
be made regarding the probabilistic judgments of the
interlocutors regarding the topic agent?s future state.
However, in the context of this paper, it is assumed
that ?yet? is used in the context of goals intended by
agents.
3.1.2 Questions
Here we will limit the discussion to two question
types, the ?where? question (regarding locations)
and simple ?Yes-No? questions.
If ? asks ? about its location in the general sense
(?where are you??), then one can infer that ? has an
intention to know (expressed via the ?IK? operator,
see (Perrault and Allen, 1980)) where ? is located:
[[Askloc(?, ?, {})]]c := IK(?, at(?, ?)) (6)
for some ?.
If ? asks ? whether it is at ?, then one can infer
that ? has an intention to know whether ? is at ?:
[[Askyn(?, ?, at(?, ?), {})]]c := IK(?, at(?, ?)) (7)
If ? is asked by ? whether it is ?still? at ?, ? can in-
fer that ? believes (expressed via the ?B? operator)
that ? is currently at ?:
[[Askyn(?, ?, at(?, ?), {still})]]c := (8)
[[Askyn(?, ?, at(?, ?), {})]]c ?B(?, at(?, ?))
If ? is asked by ? whether it is at ? ?yet?, ? can
infer that ? believes that ? has a goal to be at ?:
[[Askyn(?, ?, at(?, ?), {yet})]]c := (9)
[[Askyn(?, ?, at(?, ?), {})]]c ?B(?,G(?, at(?, ?)))
3.1.3 Question-Answer Pairs
Next, we consider how discourse context as pro-
vided by question-answer pairs can further specify
the pragmatic implications.
If ? asks ? whether it is at ? with
any set of adverbial modifiers ? (i.e.,
Prior(Askyn(?, ?, at(?, ?), ?)) ? c), and ?
responds by stating that it is ?still? at ?, then one
can infer that ? has the belief that ? was at ? in the
recent past:
[[Stmt(?, ?, at(?, ?), {still})]]c := (10)
[[Stmt(?, ?, at(?, ?), {})]]c
?B(?,RecPast(at(?, ?)))
where Prior(Askyn(?, ?, at(?, ?), ?)) ? c. Also,
RecPast(?) denotes that ? was true in the recent
past, as distinct from ? holding at some arbitrary
point in the past (i.e. Past(?)). This distinction is
necessary as it only makes sense to use the adverbial
cue at this point if agent ? believed at(?, ?) at some
relative and recent point in the past. Formalizing this
would require keeping track of the points in time at
which certain propositions are believed. To avoid
committing to a particular temporal modeling sys-
tem, we make the simplifying assumption that the
RecPast operator is not applied in rules (10) and
(11), which is sufficient for the very simple interac-
tions examined in this paper.
If ? asks ? whether it is at ? with any set of ad-
verbial modifiers ?, and ? responds by stating that it
is ?now? at ?, then one can infer that ? has the belief
that ? is was not at ? in the recent past:
[[Stmt(?, ?, at(?, ?), {now})]]c := (11)
[[Stmt(?, ?, at(?, ?), {})]]c
?B(?,RecPast(?at(?, ?)))
where Prior(Askyn(?, ?, at(?, ?), ?)) ? c.
3.1.4 Commands
We also briefly describe how command process-
ing (which we have studied elsewhere in much
greater detail (Dzifcak et al, 2009)) can be aug-
mented with the inclusion of pragmatic meanings.
If ? orders ? to travel to ?, then one can infer that ?
has a goal for ? to be at ? and that ? intends to know
whether ? has received its new goal:
[[Cmd(?, ?, at(?, ?), {})]]c := (12)
G(?, at(?, ?))
?IK(?,G(?, at(?, ?)))
242
It would be an oversimplification to assume that
the proposition G(?, at(?, ?)) is immediately un-
derstood by all listening agents. In order to generate
the appropriate goal belief in the target agent, ad-
ditional inference rules need to be considered. The
following rule states that ? will instantiate the goal
G(?, at(?, ?)) when it believes ? has the same goal
and it believes authority(?, ?), which denotes that
? has command authority over ?:
G(?, at(?, ?)) ? authority(?) ?
G(?, at(?, ?))
Other agents would have to wait for an acknowledg-
ment that this inference has indeed taken place (as ?
could have not heard the initial command utterance).
These acknowledgment utterances are described in
the subsequent section.
3.1.5 Acknowledgments
Finally, we consider typical forms of acknowledg-
ment. If ? utters an acknowledgment (e.g., ?OK.?)
when the previous utterance was a positive statement
of location by ?, then one can infer ? no longer has
the intention to know ??s location:
[[Ack(?, ?, {})]]c := ?IK(?, at(?, ?)) (13)
for some ? where for any M
Prior(Stmt(?, ?, at(?, ?), {M})) ? c.
If ? utters an acknowledgment (e.g., ?OK.?) when
the previous utterance was a command by ? to be at
?, then one can infer that
[[Ack(?, ?, {})]]c := (14)
G(?, at(?, ?)) ?G(?, at(?, ?))
??IK(?,G(?, at(?, ?)
where Prior(Cmd(?, ?, at(?, ?), {M})) ? c for
any M .
We should note here that the distinction between
explicitly not intending-to-know and the lack of an
intention-to-know has been blurred in the above
rules for the sake of simplicity. As described
in the subsequent section, agent beliefs are re-
moved when contradicted in the current system (i.e.
Remove(?,B?) ? (??) ? B?). A more com-
prehensive belief update system should allow for a
mechanism to remove beliefs without the need for
explicit contradiction.
3.2 Agent Modeling and Belief Updates
Belief updates occur whenever an agent ? receives
an utterance Utt from another agent ? in context
c. First, [[Utt]]c is computed using the pragmatic
principles and definitions developed in Section 3.1.
For simplicity, we assume that agents adhere to
the Gricean maxim of quality and, therefore, do
not communicate information they do not believe.
Hence, all propositions ? ? [[Utt]]c are assumed
to be true and to the extent that they are inconsis-
tent with existing beliefs of ? as determined by ??s
inference algorithm ?b?, the conflicting beliefs are
removed from the agent?s sets of beliefs Belself (b
here denotes some finite bound on the inference al-
gorithm, e.g., resources, computation time, etc.).1
To model other agents hearing the utterance, agent
? derives the set B?B? = {?|B(?, ?) ? Belself}
for all other agents ? 6= ?. The agent updates these
belief sets by applying the same rules as it does to
Belself .
It should be noted that these belief update rules
are indeed simplifications designed to avoid the is-
sue of resolving conflicting information from dif-
ferent sources. These belief update rules would be
problematic, for instance, when agents have incor-
rect beliefs (and proceed to communicate them), as
no method for belief disputation exists. For the pur-
pose of illustrating the implementation and utility of
adverbial cues, however, they should suffice. We
set up our environment and rule sets such that the
autonomous agent has perfect information about it-
self (specifically location), and no utterances exists
to communicate propositions that are not about one-
self.
3.3 Sentence Generation
Depending on the sentence type ? received (and the
extent to which meanings can be resolved, an issue
we will not address in this paper), different response
sentence types are appropriate (e.g., a yes-no ques-
1Note that we are not making any assumption about a partic-
ular inference algorithm or its (as it will, in general, depend on
the expressive power of the employed logic to represent mean-
ings), only that if a contradiction can be reached using the in-
ference algorithm, the existing belief needs to be removed (oth-
erwise existing beliefs are taken to be consistent with the impli-
cations of the utterance). In our implemented system, we use a
simplified version of the resolution inference principle.
243
tion requires a statement answering the question).
The generation of an appropriate response proceeds
in two steps. First, based on the agent?s current set
of beliefs Belself , we determine the set of proposi-
tions ?comm that the agent has an interest in con-
veying. Second, we attempt to find the smallest ut-
terance Utt given a set of pragmatic principles (as
specified in Section 3.1) that communicates one or
more of these propositions and implies the rest for
recipient ?.
3.3.1 What to say
In obtaining a set ?comm of propositions to com-
municate, ? may obey the Gricean maxim of qual-
ity by adding a proposition ? to ?comm only if
? ? Belself . The maxims of relevance and quan-
tity are heeded by restricting believed propositions
to be conveyed solely to those that either correct a
false belief of ? or provide ? some piece of infor-
mation it wants to know. Specifically, we find the
set of all propositions used to correct false beliefs
?rev, defined as:
? ? ?rev ? ??, ? :
B(?, ?) ? ? ? Belself ? (? ?
b
? ??)
The set of all propositions other agents want to
know, ?IK , can be defined as:
? ? ?IK ? ??, ? : ? ? Belself?
IK(?, ? ? Belself ) ? (? ?
b
? ? ? ? ?
b
? ??)
The final set of propositions to convey is obtained
by merging these two sets, ?comm = ?rev ? ?IK .
Note that this set is always consistent because propo-
sitions are added to ?rev and ?IK if and only if they
exist in Belself , which is maintained to be consis-
tent.
3.3.2 How to say it
Once ?comm has been obtained, ?must select po-
tential utterances to produce. It starts by generating
an initial set Utt0 of utterances that in the present
context c imply some subset of ?comm:
(u ? Utt0) ? ?? ? ?comm?? ? ? : ([[u]]c ?
b
? ?)
Currently, this is achieved by searching through
the set of all utterances defined by rules such as those
found in Section 3.1. Note that while this approach
is feasible for our quite limited domain, more effi-
cient methods for identifying candidate utterances
must be developed as the number of understood ut-
terances grows.
Applying the maxim of quality, this set can be
pruned of all utterances that are defined by addi-
tional propositions that we either have no evidence
for (?unsupported?) or explicitly believe to be false:
False(?) ? ?? : ? ? Belself ? (? ?
b
? ??)
NoSupp(?) ? ??? : ? ? Belself ? (? ?
b
? ?)
Using these conditions, we can generate a new sub-
set of utterance candidates Utt1:
(u ? Utt1) ? ??? : ([[u]]c ?
b
? ?)
?(False(?) ?NoSupp(?)))
Applying the maxim of quantity, utterances that
revise or add the most beliefs to other agent belief-
spaces ought to be favored:
RevBel(?, ?) ?
?? : B(?, ?) ? Bself ? (? ?
b
? ??)
AddBel(?, ?) ? B(?, ?) 6? Belself
Using these definitions, we can derive the
?correction-score? of an utterance by counting
the number of propositions ? ? [[u]]c that revise or
add a belief for ?.
If multiple candidate utterances still exist at this
point, we can again apply the maxim of quantity to
favor utterances that convey the most (true) informa-
tion. Because all definitions with false propositions
have been eliminated, we can simply count the num-
ber of true propositions derived from the utterance,
thereby favoring semantically richer utterances. At
this point, if multiple candidate utterances are still
available, the difference is of stylistic nature only
and we may choose an arbitrary one. Note that the
correct usage of adverbial modifiers emerges natu-
rally from these rules as utterances that include in-
appropriate adverbs are removed in Utt1, while ut-
terances that include appropriate adverbial cues are
subsequently favored.
244
4 Case Study
We now demonstrate the operation of the proposed
algorithm in a simple joint activity scenario where
a robot (R) is located at nav-point 1 and correctly
knows its location, having the initial belief-space
BR = {at(R,N1)}. The remote human operator
starts by asking:
O: R, where are you?
R updates its beliefs based on this question:
u := parse(?O: R, where are you??)
? u := Askloc(O,R, {})
[[u]]c := {IK(O, at(R,N1)), IK(O, at(R,N2)),
IK(O, at(R,N3))}
Pcontra := contradictedTerms([[u]]c, Bself )
BR := (BR ? Pcontra) + [[u]]c
BRBO := (BRBO ? Pcontra) + [[u]]c
which yields a new belief-space:
BR := {at(R,N1), IK(O, at(R,N1)),
IK(O, at(R,N2)), IK(O, at(R,N3)),
B(O, IK(O, at(R,N1))), B(O, IK(O, at(R,N2))),
B(O, IK(O, at(R,N3)))}
Next, R proceeds to respond. For compactness, we
refer below to utterance candidates according to the
index of the applicable rules from Section 3.1, so
that u13 denotes Ack(?, ?, {}).
BRBO := {IK(O, at(R,N1)), IK(O, at(R,N2))
IK(O, at(R,N3))}
?rev := {}; ?IK := {at(R,N1)}
?comm := {at(R,N1)};
? Utt0 := {u1, u2, u3, u4}
R now has an initial set of candidate utterances,
which it prunes using the rules from Section 3.3.2.
[[u1]]c := at(R,N1)
[[u2]]c := at(R,N1) ? Future(?at(R,N1))
[[u3]]c := at(R,N1) ? Past(G(R,N1))
[[u4]]c := at(R,N1) ? Past(?at(R,N1))
? Utt1 := {u1}
Thus, R chooses the utterance of the form,
Stmt(R,O, at(R,N1), {}), and responds:
R: I am at N1.
Finally, R processes its own utterance so that it can
update its beliefs according to rule (1):
BR := {at(R,N1), IK(O, at(R,N1)),
IK(O, at(R,N2)), IK(O, at(R,N3)),
B(O, IK(O, at(R,N1))), B(O, IK(O, at(R,N2))),
B(O, IK(O, at(R,N3))), B(O, at(R,N1)}
When the operator responds:
O: Okay.
R also processes this acknowledgment to update its
beliefs according to rule (13):
BR := {at(R,N1), B(O, at(R,N1)}
R proceeds to respond, but finds that it has nothing
to convey.
BRBO := {at(R,N1)}
; ?rev := {}; ?IK := {}
; ?comm := {};
? Utt0 := {}
Thus, R generates no utterance. Now let us suppose
that R moves to N2, and enough time elapses such
that the operator forfeits his/her conversational turn.
R then proceeds to generate an utterance.
BR := {at(R,N2), Past(at(R,N1))}
BRBO := {at(R,N1)}
?rev := {at(R,N2)}; ?IK := {}
?comm := {at(R,N2)}
? Utt0 := {u1, u2, u3, u4}
[[u1]]c := at(R,N2)
[[u2]]c := at(R,N2) ? Future(?at(R,N2))
[[u3]]c := at(R,N2) ?G(R,N2)
[[u4]]c := at(R,N2) ? Past(?at(R,N2))
? Utt1 := {u1, u4}
So, R must now resolve which of these candidate
utterances to select by choosing the one that revises
the most beliefs of O, or failing that, the one that has
the most true propositions.
at(R,N2)? ?at(R,N1)
? NumRev([[u1]]c) := 1;NumRev([[u4]]c) := 1;
NumTrue([[u1]]c) := 1;NumTrue([[u4]]c) := 2;
? Uttfinal := u4
Thus, R chooses the utterance of the form,
Stmt(R,O, at(R,N2), {now}), and responds:
R: I am now at N2.
R again processes its own utterance to update its be-
liefs according to rule (4). If O then asks:
O: R, are you still at N2?
R updates its beliefs according to rule (10):
BR := {at(R,N2), B(O, at(R,N2)),
Past(at(R,N1)), B(O,Past(at(R,N1))),
IK(O, at(R,N2)), B(O, IK(O, at(R,N2)))}
Next, R generates a response:
?rev := {}; ?IK := {at(R,N2)}
?comm := {at(R,N2)};
? Utt0 := {u1, u2, u10, u11}
[[u1]]c := at(R,N2)
[[u2]]c := at(R,N2) ? Future(?at(R,N2))
[[u10]]c := at(R,N2) ?B(O, at(R,N2))
[[u11]]c := at(R,N2) ?B(O,?at(R,N2))
? Utt1 := {u1, u10}
? NumRev([[u1]]c) := 0;NumRev([[u10]]c) := 0;
NumTrue([[u1]]c) := 1;NumTrue([[u10]]c) := 2;
? Uttfinal := u10
replying with the utterance:
R: I am still at N2.
245
and processes its own utterance to updates its beliefs
according to rule (10). O?s acknowledgment:
O: Okay.
causes R to update its beliefs according to rule (13):
BR := {at(R,N2), B(O, at(R,N2)),
Past(at(R,N1)), B(O,Past(at(R,N1)))}
R does not generate a response as there are no be-
liefs to revise or intentions to know. Now suppose
R moves back to N1, without O?s knowledge, after
which O commands:
O: R, go to N1.
R, updates its belief according to rule (12):
BR := {at(R,N1), B(O, at(R,N2)),
Past(at(R,N2)), B(O,Past(at(R,N2))),
G(R, at(R,N1)), G(O, at(R,N1)),
IK(O,G(R, at(R,N1))),
B(O,G(R, at(R,N1))),
B(O, IK(O,G(R, at(R,N1)))),
B(O,G(O, at(R,N1)))}
and proceeds to generate a response:
?rev := {at(R,N1)}
?IK := {G(R, at(R,N1))}
?comm := {at(R,N1), G(R, at(R,N1))}
? Utt0 := {u1, u2, u3, u4}
[[u1]]c := at(R,N1)
[[u2]]c := at(R,N1) ? Future(?at(R,N1))
[[u3]]c := at(R,N1) ?G(R, at(R,N1))
[[u4]]c := at(R,N1) ? Past(?at(R,N1))
? Utt1 := {u1, u3, u4}
? NumRev([[u1]]c) := 1;NumRev([[u3]]c) := 2;
NumRev([[u4]]c) := 1
? Uttfinal := u3
Thus, R responds:
R: I am already at N1.
5 Discussion and Related Work
While the above case study was kept simple due to
space restrictions, it demonstrates the utility of our
utterance generation method in adapting NL output
at the sentence-level based on a mental-model of an
interlocutor. In particular, we adapted utterances by
employing adverbial modifiers, which serve to make
the speaker?s belief-space more transparent and nat-
ural, which was the main motivation for the devel-
opment of the formal framework with rules for ad-
verbial modifiers in the first place. Other examples
of adaptations that are intended to make an auto-
mated system?s reasoning and internal state repre-
sentations more open and clear to human-users in-
clude the sentence-level adaptation of restaurant rec-
ommendations (Walker et al, 2007) and the adapta-
tion of query-phrasing in a robotic context (Kruijff
and Brenner, 2009). In addition to conveying in-
formation about one?s own mental state, pragmatic
principles and rules, such as those we have pre-
sented, may be deployed to reason about the in-
tentions and beliefs of others (Perrault and Allen,
1980).
The current system, while a promising step to-
wards more natural task-based dialogue interactions,
has several limitations. Aside from lexical and se-
mantic limitations, the currently implemented ad-
verbial modifiers are restricted to very simple pred-
icates. Clearly, these restrictions will have to be
addressed and the formal definitions will have to
be widened. Moreover, the system currently does
not handle situations where a human?s mental state
changes without the robot?s knowledge, which can
cause misunderstandings that need to be detected
and corrected effectively. Additionally, agents can
be mistaken about their beliefs. Real-world com-
plexities such as these suggest the inclusion of han-
dling uncertainty in a belief modeling system (Lison
et al, 2010), potentially by assigning beliefs confi-
dence values. This is clearly an important topic for
future work.
User-model based adaptation of NL output at the
sentence level that includes multi-modal compo-
nents (Walker et al, 2004) has also not been ad-
dressed. Further study is required to determine
whether our Gricean-inspired utterance selection
method can also be applied to non-linguistic com-
munication modalities. Finally, the current sys-
tem can only handle simple perceptual updates and
has limitations when handling multi-robot dialogues
(neither of which are discussed here for space rea-
sons). The challenges of perceptual updates that will
have to be addressed are investigated in the con-
text of a plan-based situated dialogue system for
robots in (Brenner, 2007) and extensions to multi-
robot scenarios are explored in (Brenner and Kruijff-
Korbayova, 2008).
6 Conclusion
Competency in mental modeling is a crucial com-
ponent in the development of natural, human-like
interaction capabilities for robots in mixed initia-
tive settings. We showed that the ability to under-
246
stand and employ adverbial modifiers can help both
in constructing mental models of human operators
and conveying one?s own mental state to others.
To this end, we made three contributions. First,
we introduced a framework for formalizing different
sentence types and the pragmatic meanings of ad-
verbial modifiers. Second, we showed how one can
perform belief updates based on implied meanings
of adverbial modifiers. And third, we introduced
a novel algorithm for generating effective responses
that obey three Gricean maxims and aid the listener
in appropriate belief updates. The core properties of
the algorithm are that it corrects false or missing be-
liefs in other agents, that it provides an agent with
information that is wanted, that it never generates an
utterance that implies false propositions, and that it
first favors utterances that convey more (true) propo-
sitions after favoring utterances that revise or add
more beliefs to the listener?s belief-space. Finally,
we demonstrated our algorithm responding to basic
operator queries in a simple case study, correctly us-
ing adverbial cues to sound more natural and convey
more information regarding its beliefs.
There are extensive avenues to pursue future
work. For instance, we plan to extend the algo-
rithm to include multi-modal perceptual integration
as well as multi-agent multi-dialogue capabilities.
A variety of empirical evaluations would be desir-
able to evaluate the efficacy and naturalness of the
proposed adverbial cues in simulated and real HRI
tasks. Additionally, empirical evaluations could also
be performed to observe additional cues to incorpo-
rate into the system.
7 Acknowledgments
This work was supported by an ONR MURI grant
#N00014-07-1-1049 to the second author. We wish
to extend our thanks to Paul Schermerhorn and the
anonymous reviewers for providing valuable feed-
back.
References
A. Kobsa. 2001. Generic User Modeling Systems. User
Modeling and User-Adapted Interaction 11: 49?63.
B. Di Eugenio, et al 2008. Be Brief, And They Shall
Learn: Generating Concise Language Feedback for a
Computer Tutor. International Journal of Artificial In-
telligence in Education 18(4).
C. R. Perrault and J. F. Allen. 1980. A Plan-Based Anal-
ysis of Indirect Speech Acts. American Journal of
Computational Linguistics, 6(3-4):167?182.
G. M. Kruijff and M. Brenner. 2009. Phrasing Questions.
AAAI 2009 Spring Symposium.
H. Kress-Gazit and G. E. Fainekos and G. J. Pappas 2008
Translating Structured English to Robot Controllers
Advanced Robotics 22, 12, 1343?1359
H. P. Grice. 1975. Logic and conversation. Syntax and
Semantics, 3(1):43?58.
J. Dzifcak, M. Scheutz, C. Baral, and P. Schermerhorn.
2009. What to do and how to do it: Translating Nat-
ural Language Directives into Temporal and Dynamic
Logic Representation for Goal Management and Ac-
tion Execution. ICRA.
M. A. Walker, et al 2004. Generation and evaluation of
user tailored responses in multimodal dialogue. Cog-
nitive Science 28: 811?840.
M. Walker, et al 2007. Individual and Domain Adap-
tation in Sentence Planning for Dialogue. Journal of
Artificial Intelligence Research. 30: 413?456.
M. Scheutz, et al 2007. First Steps toward Natural
Human-Like HRI. Autonomous Robots 22(4):411?
423.
M. Brenner. 2007. Situation-Aware Interpretation,
Planning and Execution of User Commands by Au-
tonomous Robots. RO-MAN 2007.
M. Brenner and I. Kruijff-Korbayova. 2008. Continual
planning and acting in dynamic multiagent environ-
ments. 12th SEMDIAL Workshop.
P. R. Cohen, et al 1990. Task-Oriented Dialogue as a
Consequence of Joint Activity. Pacific Rim Interna-
tional Conference on Artificial Intelligence.
P. Lison, C. Ehrler, and G. M. Kruijff. 2010. Belief Mod-
elling for Situation Awareness in Human-Robot Inter-
action. 19th IEEE International Symposium.
R. Dale and E. Reiter. 1995. Computational Interpre-
tations of the Gricean Maxims in the Generation of
Referring Expressions. Cognitive Science, 18(1):233?
263.
P. Rybski, K. Yoon, J. Stolarz, and M. Veloso. 2007
Interactive robot task training through dialog and
demonstration HRI, 49?56
T. Kollar and S. Tellex and D. Roy and N. Roy 2010
Toward Understanding Natural Language Directions
HRI, 259-266
W. Wahlster and A. Kobsa. 1989. User Models in Dia-
log Systems. User Models in Dialog Systems, 4?34.
Springer-Verlag, Berlin.
247
Proceedings of the INLG and SIGDIAL 2014 Joint Session, pages 1?5,
Philadelphia, Pennsylvania, 19 June 2014. c 2014 Association for Computational Linguistics
Modeling Blame to Avoid Positive Face Threats in Natural Language
Generation
Gordon Briggs
Human-Robot Interaction Laboratory
Tufts University
Medford, MA USAgbriggs@cs.tufts.edu Matthias ScheutzHuman-Robot Interaction LaboratoryTufts UniversityMedford, MA USAmscheutz@cs.tufts.edu
Abstract
Prior approaches to politeness modulation
in natural language generation (NLG) of-
ten focus on manipulating factors such as
the directness of requests that pertain to
preserving the autonomy of the addressee
(negative face threats), but do not have a
systematic way of understanding potential
impoliteness from inadvertently critical or
blame-oriented communications (positive
face threats). In this paper, we discuss on-
going work to integrate a computational
model of blame to prevent inappropriate
threats to positive face.
1 Introduction
When communicating with one another, people
often modulate their language based on a variety
of social factors. Enabling natural and human-
like interactions with virtual and robotic agents
may require engineering these agents to be able
to demonstrate appropriate social behaviors. For
instance, increasing attention is being paid to the
effects of utilizing politeness strategies in both
human-computer and human-robot dialogue inter-
actions (Cassell and Bickmore, 2003; Torrey et
al., 2013; Strait et al., 2014). This work has
shown that, depending on context, the deployment
of politeness strategies by artificial agents can in-
crease human interactants? positive assessments of
an agent along multiple dimensions (e.g. likeabil-
ity).
However, while these studies investigated the
human factors aspects of utilizing politeness
strategies, they were not concerned with the nat-
ural language generation (NLG) mechanisms nec-
essary to appropriately realize and deploy these
strategies. Instead, there is a small, but grow-
ing, body of work on natural language genera-
tion architectures that seek to address this chal-
lenge (Gupta et al., 2007; Miller et al., 2008;
Briggs and Scheutz, 2013). The common ap-
proach taken by these architectures is the opera-
tionalization of key factors in Brown and Levin-
son?s seminal work on politeness theory, in partic-
ular, the degree to which an utterance can be con-
sidered a face-threatening act (FTA) (Brown and
Levinson, 1987).
While this prior work demonstrates the abilities
of these NLG architectures to successfully pro-
duce polite language, there remain some key chal-
lenges. Perhaps the most crucial question is: how
does one calculate the degree to which an utter-
ance is a FTA1? This is a complex issue, as not
only is this value modulated by factors such as so-
cial distance, power, and context, but also the mul-
tifaceted nature of ?face.? An utterance may be
polite in relation to negative face (i.e. the agent?s
autonomy), but may be quite impolite with regard
to positive face (i.e. the agent?s image and per-
ceived character).
In this paper, we investigate the problem of
modeling threats to positive face. First we discuss
how prior work that has focused primarily on miti-
gating threats to negative face, and examine a spe-
cific example, taken from the human subject data
of (Gupta et al., 2007), to show why accounting
for positive face is necessary. Next, we discuss
our proposed solution to begin to model threats to
positive face? specifically, integrating a computa-
tional model of blame. Finally, we discuss the jus-
tification behind and limitations of this proposed
approach.
2 Motivation
Brown and Levinson (1987) articulated a tax-
onomy of politeness strategies, distinguishing
broadly between the notion of positive and neg-
ative politeness (with many distinct strategies for
each). These categories of politeness correspond
1Less crucially, what is the appropriate notation for this
value? It is denoted differently in each paper: ?,W , and ?.
1
to the concepts of positive and negative face, re-
spectively. An example of a positive politeness
strategy is the use of praise (?Great!?), whereas
a common negative politeness strategy is the use
of an indirect speech act (ISA), in particular, an
indirect request. An example of an indirect re-
quest is the question, ?Could you get me a cof-
fee??, which avoids the autonomy-threatening di-
rect imperative, while still potentially being con-
strued as a request. This is an example of a con-
ventionalized form, in which the implied request
is more directly associated with the implicit form.
Often considered even less of a threat to negative
face are unconventionalized ISAs, which often re-
quire a deeper chain of inference to derive their
implied meaning. It is primarily the modulation of
the level of request indirectness that is the focus of
(Gupta et al., 2007; Briggs and Scheutz, 2013).
To provide an empirical evaluation of their sys-
tem, Gupta et al. (2007) asked human subjects
to rate the politeness of generated requests on a
five-point Likert scale in order of most rude (1)
to to most polite (5). The results from (Gupta et
al., 2007) for each of their politeness strategy cat-
egories are below:
1. Autonomy [3.4] (e.g. ?Could you possibly do
X for me??)
2. Approval [3.0] (e.g. ?Could you please do X
mate??)
3. Direct [2.0] (e.g. ?Do X .?)
4. Indirect [1.8] (e.g. ?X is not done yet.?)
This finding is, in some sense, counterintuitive,
as unconventionalized request forms should be
the least face-threatening. However, Gupta et al.
(2007) briefly often an explanation, saying that the
utterances generated in the indirect category sound
a bit like a ?complaint or sarcasm.? We agree with
this assessment. More precisely, while negative
face is protected by the use of their unconvention-
alized ISAs, positive face was not.
To model whether or not utterances may be in-
terpreted as being complaints or criticisms, we
seek to determine whether or not they can be in-
terpreted as an act of blame2.
2What the precise ontological relationship is between
concepts such as complaining, criticizing, and blaming is be-
yond the scope of this paper.
3 Approach
Like praise, blame (its negative counterpart) is
both a cognitive and social phenomenon (Malle et
al., 2012). The cognitive component pertains to
the internal attitudes of an agent regarding another
agent and their actions, while the social compo-
nent involves the expression of these internal at-
titudes through communicative acts. To achieve
blame-sensitivity in NLG, we need to model both
these aspects. In the following sections, we briefly
discuss how this could be accomplished.
3.1 Pragmatic and Belief Reasoning
Before a speaker S can determine the high-level
perlocutionary effects of an utterance on an ad-
dressee (H) vis-a?-vis whether or not they feel crit-
icized or blamed, it is first necessary to determine
the precise set of beliefs and intentions of the ad-
dressee upon hearing an utterance u in context c.
We denote this updated set of beliefs and inten-
tions  H(u, c). Note that this set is a model of
agent H?s beliefs and intentions from the speaker
S?s perspective, and not necessarily equivalent to
the actual belief state of agent H . In order to per-
form this mental modeling, we utilize a reason-
ing system similar to that in (Briggs and Scheutz,
2011). This pragmatic reasoning architecture uti-
lizes a set of rules of the form:
[[U ]]C :=  1 ^ ... ^  n
where U denotes an utterance form, C
denotes a set of contextual constraints that
must hold, and   denotes a belief update
predicate. An utterance form is specified
by u = UtteranceType(?, , X,M), where
UtteranceType denotes the dialogue turn type
(e.g. statement, y/n-question), ? denotes the
speaker of the utterance u,   denotes the addressee
of the utterance, X denotes the surface semantics
of the utterance, and M denotes a set of sentential
modifiers. An example of such a pragmatic rule is
found below:
[[Stmt(S,H,X, {})]]; := want(S, bel(H,X))
which denotes that a statement by the speaker
S to an addressee H that X holds should in-
dicate that, ?S wants H to believe X ,? in all
contexts (given the empty set of contextual con-
straints). If this rule matches a recognized ut-
terance (and the contextual constraints are satis-
2
fied, which is trivial in this case), then the men-
tal model of the addressee is updated such that:
want(S, bel(H,X)) 2  H(u, c).
Of particular interest with regard to the Gupta
et al. (2007) results, Briggs and Scheutz (2011)
describe how they can use their system to un-
derstand the semantics of the adverbial modifier
?yet,? which they describe as being indicative of
mutually understood intentionality. More accu-
rately, ?yet,? is likely indicative of a belief regard-
ing expectation of an action being performed or
state being achieved. Therefore, a plausible prag-
matic rule to interpret, ?X is not done yet,? could
be:
[[Stmt(S,H,?done(X), {yet})]]; :=
want(S, bel(H,?done(X))) ^
expects(S, done(X))
Furthermore, in a cooperative, task-driven con-
text, such as that described in (Gupta et al., 2007),
it would not be surprising for an interactant to infer
that this expectation is further indicative of a belief
in a particular intention or a task-based obligation
to achieve X .3
As such, if we consider an utterance ud as being
a standard direct request form (strategy 3), and an
utterance uy as being an indirect construction with
a yet modifier (strategy 4), the following facts may
hold:
bel(S, promised(H,S,X, tp)) 62  H(ud, c)
bel(S, promised(H,S,X, tp)) 2  H(uy, c)
If S is making a request to H , there is no be-
lieved agreement to achieve X . However, if ?yet,?
is utilized, this may indicate to H a belief that S
thinks there is such an agreement.
Having calculated an updated mental model of
the addressee?s beliefs after hearing a candidate ut-
terance u, we now can attempt to infer the degree
to which u is interpreted as an act of criticism or
blame.
3.2 Blame Modeling
Attributions of blame are influenced by several
factors including, but not limited to, beliefs about
an agent?s intentionality, capacity, foreknowledge,
obligations, and possible justifications (Malle et
3How precisely this reasoning is and/or ought to be per-
formed is an important question, but is outside the scope of
this paper.
al., 2012). Given the centrality of intentionality
in blame attribution, it is unsurprising that current
computational models involve reasoning within a
symbolic BDI (belief, desire, intention) frame-
work, utilizing rules to infer an ordinal degree of
blame based on the precise set of facts regarding
these factors (Mao and Gratch, 2012; Tomai and
Forbus, 2007). A rule that is similar to those found
in these systems is:
bel(S, promised(H,S,X, tp)) ^ bel(S,?X) ^
bel(S, (t > tp)) ^ bel(S, capable of(H,X))) blames(S,H, high)
that is to say, if agent S believes agent H
promised to him or her to achieve X by time
tp, and S believes X has not been achieved and
the current time t is past tp, and S believes H
is capable of fulfilling this promise, then S will
blame H to a high degree. Continuing our discus-
sion regarding the perlocutionary effects of ud and
uy, it is likely then that: blames(S,H, high) 62
 H(ud, c) and blames(S,H, high) 2  H(uy, c).
3.3 FTA Modeling
Having determined whether or not an addressee
would feel criticized or blamed by a particu-
lar candidate utterance, it is then necessary to
translate this assessment back into the terms of
FTA-degree (the currency of the NLG system).
This requires a function  ( ) that maps the or-
dinal blame assessment of the speaker toward
the hearer based on a set of beliefs  , de-
scribed in the previous section, to a numerical
value than can be utilized to calculate the sever-
ity of the FTA (e.g. blames(S,H, high) = 9.0,
blames(S,H,medium) = 4.5). For the purposes
of this paper we adopt the theta-notation of Gupta
et al. (2007) to denote the degree to which an ut-
terance is a FTA. With the   function, we can then
express the blame-related FTA severity of an utter-
ance as:
?blame(u, c) =  H( H(u, c))  ?(c) ?  S( S)
where  H denotes the level of blame the speaker
believes the hearer has inferred based on the ad-
dressee?s belief state after hearing utterance u with
context c ( H(u, c))).  S denotes the level of
blame the speaker believes is appropriate given his
or her current belief state. Finally, ?(c) denotes a
3
multiplicative factor that models the appropriate-
ness of blame given the current social context. For
instance, independent of the objective blamewor-
thiness of a superior, it may be inappropriate for a
subordinate to criticize his or her superior in cer-
tain contexts.
Finally, then, the degree to which an utterance is
a FTA is the sum of all the contributions of evalu-
ations of possible threats to positive face and pos-
sible threats to negative face:
?(u, c) =
Xp2P ?p(u, c) +Xn2N ?n(u, c)
where P denotes the set of all possible threats
to positive face (e.g. blame) and N denotes the set
of all possible threats to negative face (e.g. direct-
ness).
We can see how this would account for the
human-subject results from (Gupta et al., 2007), as
conventionally indirect requests (strategies 1 and
2) would not produce large threat-value contri-
butions from either the positive or negative FTA
components. Direct requests (strategy 3) would,
however, potentially produce a large?N contribu-
tion, while their set of indirect requests (strategy
4) would trigger a large ?P contribution.
4 Discussion
Having presented an approach to avoid certain
types of positive-FTAs through reasoning about
blame, one may be inclined to ask some questions
regarding the justification behind this approach.
Why should we want to better model one highly
complex social phenomenon (politeness) through
the inclusion of a model of another highly complex
social phenomenon (blame)? Does the integration
of a computational model of blame actually add
anything that would justify the effort?
At a superficial level, it does not. The
criticism/blame-related threat of a specific speech
act can be implicitly factored into the base FTA-
degree evaluation function supplied to the sys-
tem, determined by empirical data or designer-
consensus as is the case of (Miller et al., 2008).
However, this approach is limited in a couple
ways. First, this does not account for the fact that,
in addition to the set of social factors Brown and
Levinson articulated, the appropriateness of an act
of criticism or blame is also dependent on whether
or not it is justified. Reasoning about whether or
not an act of blame is justified requires: a compu-
tational model of blame.
Second, the inclusion of blame-reasoning
within the larger scope of the entire agent ar-
chitecture may enable useful behaviors both in-
side and outside the natural language system.
There is a growing community of researchers in-
terested in developing ethical-reasoning capabili-
ties for autonomous agents (Wallach and Allen,
2008), and the ability to reason about blame has
been proposed as one key competency for such
an ethically-sensitive agent (Bello and Bringsjord,
2013). Not only is there interest in utilizing such
mechanisms to influence general action-selection
in autonomous agents, but there is also interest in
the ability to understand and generate valid expla-
nations and justifications for adopted courses of
action in ethically-charged scenarios, which is of
direct relevance to the design of NLG architec-
tures.
While our proposed solution tackles threats
to positive face that arise due to unduly
critical/blame-oriented utterances, there are many
different ways of threatening positive face aside
from criticism/blame. These include phenomena
such as the discussion of inappropriate/sensitive
topics or non-cooperative behavior (e.g. purpose-
fully ignoring an interlocutor?s dialogue contribu-
tion). Indeed, empirical results show that referring
to an interlocutor in a dyadic interaction using an
impersonal pronoun (e.g. ?someone?) may consti-
tute another such positive face threat (De Jong et
al., 2008). Future work will need to be done to de-
velop mechanisms to address these other possible
threats to positive face.
5 Conclusion
Enabling politeness in NLG is a challenging prob-
lem that requires the modeling of a host of com-
plex, social psychological factors. In this paper,
we discuss ongoing work to integrate a compu-
tational model of blame to prevent inappropriate
threats to positive face that can account for prior
human-subject data. As an ongoing project, future
work is needed to further test and evaluate this pro-
posed approach.
Acknowledgments
We would like to thank the reviewers for their
helpful feedback. This work was supported by
NSF grant #111323.
4
References
Paul Bello and Selmer Bringsjord. 2013. On how to
build a moral machine. Topoi, 32(2):251?266.
Gordon Briggs and Matthias Scheutz. 2011. Facilitat-
ing mental modeling in collaborative human-robot
interaction through adverbial cues. In Proceedings
of the SIGDIAL 2011 Conference, pages 239?247,
Portland, Oregon, June. Association for Computa-
tional Linguistics.
Gordon Briggs and Matthias Scheutz. 2013. A hybrid
architectural approach to understanding and appro-
priately generating indirect speech acts. In Proceed-
ings of the 27th AAAI Conference on Artificial Intel-
ligence.
Penelope Brown and Stephen C. Levinson. 1987. Po-
liteness: Some universals in language usage. Cam-
bridge University Press.
Justine Cassell and Timothy Bickmore. 2003. Negoti-
ated collusion: Modeling social language and its re-
lationship effects in intelligent agents. User Model-
ing and User-Adapted Interaction, 13(1-2):89?132.
Markus De Jong, Marie?t Theune, and Dennis Hofs.
2008. Politeness and alignment in dialogues with
a virtual guide. In Proceedings of the 7th interna-
tional joint conference on Autonomous agents and
multiagent systems-Volume 1, pages 207?214. In-
ternational Foundation for Autonomous Agents and
Multiagent Systems.
Swati Gupta, Marilyn A Walker, and Daniela M Ro-
mano. 2007. How rude are you?: Evaluating po-
liteness and affect in interaction. In Affective Com-
puting and Intelligent Interaction, pages 203?217.
Springer.
Bertram F Malle, Steve Guglielmo, and Andrew E
Monroe. 2012. Moral, cognitive, and social: The
nature of blame. Social thinking and interpersonal
behavior, 14:313.
Wenji Mao and Jonathan Gratch. 2012. Modeling so-
cial causality and responsibility judgment in multi-
agent interactions. Journal of Artificial Intelligence
Research, 44(1):223?273.
Christopher A Miller, Peggy Wu, and Harry B Funk.
2008. A computational approach to etiquette: Oper-
ationalizing brown and levinson?s politeness model.
Intelligent Systems, IEEE, 23(4):28?35.
Megan Strait, Cody Canning, and Matthias Scheutz.
2014. Let me tell you! investigating the ef-
fects of robot communication strategies in advice-
giving situations based on robot appearance, inter-
action modality and distance. In Proceedings of
the 2014 ACM/IEEE international conference on
Human-robot interaction, pages 479?486. ACM.
Emmett Tomai and Ken Forbus. 2007. Plenty of blame
to go around: a qualitative approach to attribution of
moral responsibility. Technical report, DTIC Docu-
ment.
Cristen Torrey, Susan R Fussell, and Sara Kiesler.
2013. How a robot should give advice. In Human-
Robot Interaction (HRI), 2013 8th ACM/IEEE Inter-
national Conference on, pages 275?282. IEEE.
Wendell Wallach and Colin Allen. 2008. Moral ma-
chines: Teaching robots right from wrong. Oxford
University Press.
5
