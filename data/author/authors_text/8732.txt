The Automat ic  Translation of Discourse Structures 
Danie l  Marcu  
Information Sciences Institute and 
Department of Computer Science 
University of Southern California 
4676 Admiralty Way, Suite 1001 
Marina del Rey, CA 90292 
marcu@isi.edu 
Lynn Car l son  
U.S. Department of Defense 
Ft. Meade, MD 20755 
Imcarls@afterlife. ncsc. mil 
Maki  Watanabe 
Department of Linguistics 
University of Southern California 
Los Angeles, CA 90089 
mwatanab@usc.edu 
Abst rac t  
We empirically show that there are significant differ- 
ences between the discourse structure of Japanese 
texts and the discourse structure of their corre- 
sponding English translations. To improve trans- 
lation quality, we propose a computational model 
for rewriting discourse structures. When we train 
our model on a parallel corpus of manually built 
Japanese and English discourse structure trees, we 
learn to rewrite Japanese trees as trees that are 
closer to the natural English rendering than the orig- 
inal ones. 
1 Mot ivat ion  
Almost all current MT systems process text one sen- 
tence at a time. Because of this limited focus, MT 
systems cannot re-group and re-order the clauses 
and sentences of an input text to achieve the most 
natural rendering in a target language. Yet, even 
between languages as close as English and French, 
there is a 10% mismatch in number of sentences 
- -  what is said in two sentences in one language 
is said in only one, or in three, in the other (Gale 
and Church, 1993). For distant language pairs, such 
as Japanese and English, the differences are more 
significant. 
Consider, for example, Japanese sentence (1), a 
word-by-word "gloss" of it (2), and a two-sentence 
translation of it that was produced by a professional 
translator (3). 
(1) 
\[The Ministry of Health and Welfare last year 
revealed I \] \[population of future estimate ac- 
cording to 2\] \[in future 1.499 persons as the 
lowest s\] \[that after *SAB* rising to turn that 4\] 
\[*they* estimated but s \] \[already the estimate 
misses a point ~\] \[prediction became. 7\] 
(2) 
\[In its future population estimates'\] \[made (3) 
public last year, 2\] \[the Ministry of Health and 
Welfare predicted that the SAB would drop to 
a new low of 1.499 in the future, s) \[but would 
make a comeback after that, 4\] \[increasing once 
again, s\] \[However, it looks as if that prediction 
will be quickly shattered. 6\] 
The labeled spans of text represent elementary 
discourse units (edus), i.e., minimal text spans that 
have an unambiguous discourse function (Mann and 
Thompson, 1988). If we analyze the text frag- 
ments closely, we will notice that in translating sen- 
tence (1), a professional translator chose to realize 
the information in Japanese unit 2 first (unit 2 in 
text (1) corresponds roughly to unit 1 in text (3)); 
to realize then some of the information in Japanese 
unit 1 (part of unit 1 in text (1) corresponds to unit 
2 in text (3)); to fuse then information given in units 
1, 3, and 5 in text (1) and realize it in English as 
unit 3; and so on. Also, the translator chose to re- 
package the information in the original Japanese sen- 
tence into two English sentences. 
At the elementary unit level, the correspondence 
between Japanese sentence (1) and its English trans- 
lation (3) can be represented as in (4}, where j C e 
denotes the fact that the semantic ontent of unit 
j is realized fully in unit e; j D e denotes the fact 
that the semantic ontent of unit e is realized fully 
in unit j; j = e denotes the fact that units j and e 
are semantically equivalent; and j ~ e denotes the 
fact that there is a semantic overlap between units j 
and e, but neither proper inclusion nor proper equiv- 
alence. 
.!1 D e2;jt  -~ e3; 
.12 ---- el; 
33 C e3; 
.14 ~ e4;j4 ~ es; 
.15 ~ e3; 
.16 C e6; 
.17 C e6 
(4) 
9
C :.(~(.(:S'.~0r I t 
J 
e 3bota'lc.r- o31.~ c; - 3ttlio~te- e 
.~- - ' - -~ I  
:~) (~) 
l~ - IP .~ra~2,~ ,~,Ooll l t , \] iLtt ~: 
I..,? (The Ministry dL '(popu ~t ,m - 
ofHgMth .~nd ~f Ntu le  esti~n.~te 
reve~ie,:~) 
............... -,,.,~__~ ._~--- ~ ...-.<_-7_"" ___~__--,~ 
(5) (6) (7) 
~.. .~ ---....~ hev\] estin'at~d ~ ~alr~acy the redicti0n 
6?,) (~) but) estlnate miss - a becar.e) 
e 1 499 persons \[SAI~ rising - \[o 
a: the lowest) turn th~1) 
' i :  .................................................................................................................... ==:~::::?2' ~2~?:::-~.:.,~a ............... 
Is) 
. . . . . . .  _~_ .b~ ~, ,~d i Ho,e~v~, 
looks as if 
, ~ , !  r .~  ,~. - pNxil ~i~',~ will 
(1) 12) O) . ,,- quickly 
in its future made public the MiniMry ~i~.~ , .~-  dn iota| ihatN~ CNI. 
popukltion kin yem', o~ I'~slth and ~ '  "~ 
?-~imm?~ Welfare (4) (5) 
the SAB make II onto again. 
wotdd drop to 
afte~ that, 
a r, ew I~ ol 
1.499 In the 
fulute. 
Figure 1: The discourse structures of texts (1) and (3). 
Hence. the mappings in (4) provide all explicit 
representation of the way information is re-ordered 
and re-packaged when translated from Japanese into 
English. However, when translat ing text, it is also 
the case that t he rhetorical rendering changes. What  
is realized ill Japanese using an CONTRAST relation 
can be realized in English using, for example, a COXl- 
PARISON or  a CONCESSION relation. 
Figure I presents in the style of Mann and Thomp-  
son (1988) the discourse structures of text frag- 
ments (1) and (3), Each discourse structure is a 
tree whose leaves correspond to the edus and whose 
internal nodes correspond to contiguous text spans. 
Each node is characterized by a s tatus  (NUCLEUS or 
SATELLITE) and a rhetor ica l  re lat ion,  which is a re- 
lation that holds between two non-overlapping text 
spans. The distinction between nuclei and satellites 
comes from the empirical observation that the nu- 
cleus expresses what is more essential to the writer's 
intention than the satellite: and that the nucleus of 
a rhetorical relation is comprehensible independent 
of tile satellite, but not vice versa. When spans are 
equally important ,  the relation is nmltinuclear: for 
example,  the CONTRAST relation that holds between 
unit \[3\] and span \[4.5\] in the rhetorical structure of 
the English text in figure 1 is nmhinuclear.  Rhetor- 
ical relations that end in the suffix "'-e'" denote re- 
lations that correspond to embedded syntactic con- 
stituents. For example, the ELABORATION-OBJECT- 
ATTRIBUTE-E relation that holds between units 2 
and 1 in the English discourse structure corresponds 
to a restrictive relative. 
If one knows the mappings at the edu level, 
one can determine the mappings at the span (dis- 
course constituent) level as well. For example, us- 
ing the elementary mappings in (4), one call deter- 
mine that Japanese span \[1,2\] corresponds to English 
span \[I,2\], Japanese unit \[4\] to English span \[4,5\], 
Japanese span \[6.7\] to English unit \[6\], Japanese 
span \[1.5\] to English span \[1.5\], and so on. As Fig- 
ure 1 shows, the CONCESSION relation that holds be- 
tween spans \[1,5\] and \[6,7\] in the Japanese tree corre- 
sponds to a similar relation that. holds between span 
\[1,5\] and unit \[6\] in the English tree (modulo the fact 
that,  in Japanese, the relation holds between sen- 
t ence fragments, while in English it holds between 
full sentences). However, the TEMPORAL-AFTER re- 
lation that holds between units \[:3\] and \[4\] ill the 
Japanese tree is realized as a CONTRAST relation 
between unit \[3\] and span \[4.5\] in the English tree. 
And because Japanese units \[6\] and \[7\] are fused 
into unit \[6\] in English, the relation ELABORATION- 
OBJECT-ATTRIBUTE-E is 11o longer made explicit in 
the English text. 
Some of the differences between the two discourse 
trees in Figure 1 have been tradit ionally addressed 
10
Corpus k~ (#) k, (#) k,~ (#) k~ (#) 
Japanese 0.856 (80) 0.785 (3377) 0.724 (3377) 0.650 (3377) 
English 0.925 (60) 0.866 (1826) 0.839 (1826) 0.748 (1826) 
Table 1: Tagging reliability 
in MT systems at the syntactic level. For exam- 
ple, the re-ordering of units 1 and 2, can be dealt 
with using only syntactic models. However, as we 
will see in Section 2, there are significant differences 
between Japanese and English with respect to the 
way information is packaged and organized rhetori- 
cally not only at the sentence level, but also, at the 
paragraph and text levels. More specifically, as hu- 
mans translate Japanese into English, they re-order 
the clauses, sentences, and paragraphs of Japanese 
texts, they re-package the information into clauses, 
sentences, and paragraphs that are not a one-to-one 
mapping of the original Japanese units, and they 
rhetorically re-organize the structure of the trans- 
lated text so as to reflect rhetorical constraints pe- 
cific to English. If a translation system is to produce 
text that is not only grammatical but also coherent, 
it will have to ensure that the discourse structure of 
the target text reflects the natural renderings of the 
target language, and not that of the source language. 
In Section 2, we empirically show that there are 
significant differences between the rhetorical struc- 
ture of Japanese texts and their corresponding En- 
glish translations. These differences justify our in- 
vestigation into developing computational models 
for discourse structure rewriting. In Section 3, we 
present such a rewriting model, which re-orders the 
edus of the original text, determines English-specific 
clause, sentence, and paragraph boundaries, and re- 
builds the Japanese discourse structure of a text us- 
ing English-specific rhetorical renderings. In Sec- 
tion 4, we evaluate the performance of an imple- 
mentation of this model. We end with a discussion. 
2 Experiment 
In order to assess the role of discourse structure in 
MT, we built manually a corpus of discourse trees 
for 40 Japanese texts and their corresponding trans- 
lations. The texts were selected randomly from the 
ARPA corpus (White and O'Connell, 1994). On av- 
erage, each text had about 460 words. The Japanese 
texts had a total of 335 paragraphs and 773 sen- 
tences. The English texts had a total of 337 para- 
graphs and 827 sentences. 
We developed a discourse annotation protocol for 
Japanese and English along the lines followed by 
Marcu et al (1999). We used Marcu's discourse an- 
notation tool (1999) in order to manually construct 
the discourse structure of all Japanese and English 
texts in the corpus. 10% of the Japanese and En- 
glish texts were rhetorically labeled by two of us. 
The tool and the annotation protocol are available 
at http://www.isi.edu/~marcu/software/. The an- 
notation procedure yielded over the entire corpus 
2641 Japanese edus and 2363 English edus. 
We computed the reliability of the annotation us- 
ing Marcu et al (1999)'s method for computing 
kappa statistics (Siegel and Castellan, 1988) over hi- 
erarchical structures. Table 1 displays average kappa 
statistics that reflect the reliability of the annota- 
tion of elementary discourse units, k~,, hierarchical 
discourse spans, ks, hierarchical nuclearity assign- 
ments, k,~, and hierarchical rhetorical relation as- 
signments, k~. Kappa figures higher than 0.8 corre- 
spond to good agreement; kappa figures higher than 
0.6 correspond to acceptable agreement. All kappa 
statistics were statistically significant at levels higher 
than a = 0.01. In addition to the kappa statis- 
tics, table 1 also displays in parentheses the average 
number of data points per document, over which the 
kappa statistics were computed. 
For each pair of Japanese-English discourse struc- 
tures, we also built manually an alignment file, 
which specified in the notation discussed on page 1 
the correspondence b tween the edus of the Japanese 
text and the edus of its English translation. 
We computed the similarity between English and 
Japanese discourse trees using labeled recall and pre- 
cision figures that reflected the resemblance of tile 
Japanese and English discourse structures with re- 
spect to their assignment of edu boundaries, hierar- 
chical spans, nuclearity, and rhetorical relations. 
Because the trees we compared differ from one 
language to the other in the number of elementary 
units, the order of these units, and the way the units 
are grouped recursively into discourse spans, we 
computed two types of recall and precision figures. 
In computing Position-Dependent (P-D) recall and 
precision figures, a Japanese span was considered to 
match an English span when the Japanese span con- 
tained all the Japanese dus that corresponded to tile 
edus in the English span, and when the Japanese and 
English spans appeared in tile same position with 
respect to the overall structure. For example, the 
English tree in figure 1 is characterized by 10 sub- 
sentential spans: \[1\], \[2\], \[3\], \[4\], \[5\], \[6\], \[1,2\], \[4,5\], 
\[3,5\], and \[1,5\]. (Span \[1,6\] subsumes 2 sentences, 
so it is not sub-sentential.) The Japanese discourse 
tree has only 4 spans that could be matched in the 
same positions with English spans, namely spans 
\[1,2\], \[4\], \[5\], and \[1,5\]. Hence the similarity between 
the Japanese tree and the English tree with respect 
11 11
Level Units Spans Status/Nuclearity Relations 
P-D P P-D R P-D P P -DR P -DP  P -DR P-I) P P -DR 
Sentence 29.1 25.0 27.2 22.7 21.3 17.7 14.9 12.4 
Paragraph 53.9 53.4 46.8 47.3 38.6 39.0 31.9 32.3 
Text 41.3 42.6 31.5 32.6 28.8 29.9 26.1 27.1 
Weighted Average 36.0 32.5 31.8 28.4 26.0 23.1 20.1 17.9 
All 8.2 7.4 5.9 5.3 4.4 3.9 3.3 3.0 
P-I R P-I P P-I R P-I P P-I R P-I P P-I R P-I P 
Sentence 71.0 61.0 56.0 46.6 44.3 36.9 30.5 25.4 
Paragraph 62.1 61.6 53.2 53.8 43.3 43.8 35.1 35.5 
Text 74.1 76.5 54.4 56.5 48.5 50.4 41.1 42.7 
Weighted Average 55.2 49.2 44.8 39.9 33.1 29.5 
26.8 24.3 All 
69.6 63.0 
74.5 66.8 50.6 45.8 39.4 35.7 
Table 2: Similarity of the Japanese 
to their discourse structure below the sentence level 
has a recall of 4/10 and a precision of 4/11 (in Fig- 
ure 1, there are 11 sub-sentential Japanese spans). 
In computing Position-Independent (P-I) recall 
and precision figures, even when a Japanese span 
"floated" during the translation to a position in the 
English tree that was different from the position in 
the initial tree, the P-I recall and precision figures 
were not affected. The Position-Independent figures 
reflect the intuition that if two trees tl and t2 both 
have a subtree t, tl and t2 are more similar than 
if they were if they didn't share any tree. At the 
sentence level, we hence assume that if, for exam- 
ple, the syntactic structure of a relative clause is 
translated appropriately (even though it is not ap- 
propriately attached), this is better than translating 
wrongly that clause. The Position-Independent fig- 
ures offer a more optimistic metric for comparing 
discourse trees. They span a wider range of values 
than the Position-Dependent figures, which enable 
a finer grained comparison, which in turn enables 
a better characterization of the differences between 
Japanese and English discourse structures. When 
one takes an optimistic stance, for the spans at the 
sub-sentential level in the trees in Table 1 the recall 
is 6/10 and the precision is 6/11 because in addition 
to spans \[1,2\], \[4\], \[5\], and \[1,5\], one can also match 
Japanese span \[1\] to English span \[2\] and Japanese 
span \[2\] to Japanese span \[1\]. 
In order to provide a better estimate of how close 
two discourse trees were, we computed Position- 
Dependent and -Independent recall and precision fig- 
ures for the sentential level (where units are given by 
edus and spans are given by sets of edus or single sen- 
tences); paragraph level (where units are given by 
sentences and spans are given by sets of sentences 
or single paragraphs); and text level (where units 
are given by paragraphs and spans are given by sets 
of paragraphs). These figures offer a detailed pic- 
ture of how discourse structures and relations are 
mapped from one language to the other across all 
and English discourse structures 
discourse levels, from sentence to text. The differ- 
ences at the sentence level can be explained by differ- 
ences between the syntactic structures of Japanese 
and English. The differences at the paragraph and 
text levels have a purely rhetorical explanation. 
As expected, when we computed the recall and 
precision figures with respect to the nuclearity and 
relation assignments, we also factored in the statuses 
and the rhetorical relations that labeled each pair of 
spans. 
Table 2 smnmarizes the results (P-D and P- 
I (R)ecall and (P)recision figures) for each level 
(Sentence, Paragraph, and Text). The numbers 
in the "Weighted Average" line report averages of 
the Sentence-, Paragraph-, and Text-specific figures, 
weighted according to the number of units at each 
level. The numbers in the "All" line reflect recall and 
precision figures computed across the entire trees, 
with no attention paid to sentence and paragraph 
boundaries. 
Given the significantly different syntactic struc- 
tures of Japanese and English, we were not surprised 
by the low recall and precision results that reflect 
the similarity between discourse trees built below 
the sentence level. However, as Table 2 shows, there 
are significant differences between discourse trees at 
the paragraph and text levels as well. For exam- 
pie, the Position-Independent figures show that only 
about 62% of the sentences and only about 53% of 
the hierarchical spans built across sentences could 
be matched between the two corpora. When one 
looks at the status and rhetorical relations associ- 
ated with the spans built across sentences at the 
paragraph level, the P-I recall and precision figures 
drop to about 43% and 35% respectively. 
The differences in recall and precision are ex- 
plained both by differences in the way information is 
packaged into paragraphs in the two languages and 
the way it is structured rhetorically both within and 
above the paragraph level. 
These results strongly suggest hat if one attempts 
12
to translate Japanese into English on a sentence-by- 
sentence basis, it is likely that the resulting text will 
be unnatural from a discourse perspective. For ex- 
ample, if some information rendered using a CON- 
TRAST relation in Japanese is rendered using an 
ELABORATION relation in English, it would be in- 
appropriate to use a discourse marker like "but" in 
the English translation, although that would be con- 
sistent with the Japanese discourse structure. 
An inspection of the rhetorical mappings between 
Japanese and English revealed that some Japanese 
rhetorical renderings are consistently mapped into 
one or a few preferred renderings in English. For ex- 
ample, 34 of 115 CONTRAST relations in the Japanese 
texts are mapped into CONTRAST relations in En- 
glish; 27 become nuclei of relations uch as ANTITHE- 
SIS and CONCESSION, 14 are translated as COMPAR- 
ISON relations, 6 as satellites of CONCESSION rela- 
tions, 5 as LIST relations, etc. Our goal is to learn 
these systematic discourse mapping rules and exploit 
them in a machine translation context. 
3 Towards  a d i scourse -based  
mach ine  t rans la t ion  sys tem 
3.1 Overa l l  a rch i tec ture  
We are currently working towards building the mod- 
ules of a Discourse-Based Machine Translation sys- 
tem that works along the following lines. 
1. A discourse parser, such as those described by 
Sumita et al (1992), Kurohashi (1994), and 
MarcH (1999), initially derives the discourse 
structure of the text given as input. 
2. A discourse-structure transfer module 
rewrites the discourse structure of the input 
text so as to reflect a discourse rendering 
that is natural to the target language. 
3. A statistical module maps the input text 
into the target language using translation and 
language models that incorporate discourse- 
specific features, which are extracted from the 
outputs of the discourse parser and discourse 
transfer modules. 
In this paper, we focus only on the discourse- 
structure transfer module. That is, we investigate 
the feasibility of building such a module. 
3.2 The  d i scourse -based  t rans fer  mode l  
In order to learn to rewrite discourse structure trees, 
we first address a related problem, which we define 
below: 
Def in i t ion  3.1 Given two trees Ts and Tt and a 
correspondence Table C defined between Ts and Tt 
at the leaf level in terms of-----, C, D, and ~ relations, 
find a sequence of actions that rewrites the tree T~ 
into Tt. 
If for any tuple (Ts, Tt, C> such a sequence of actions 
can be derived, it is then possible to use a corpus 
of (Ts, Tt, C) tuples in order to automatically learn 
to derive from an unseen tree Ts,, which has the 
same structural properties as the trees Ts, a tree 
Ttj, which has structural properties imilar to those 
of the trees Tt. 
In order to solve the problem in definition 3.1, we 
extend the shift-reduce parsing paradigm applied by 
Magerman (1995), Hermjakob and Mooney (1997), 
and MarcH (1999). In this extended paradigm, the 
transfer process starts with an empty Stack and an 
Input List that contains a sequence of elementary 
discourse trees edts, one edt for each edu in the tree 
Ts given as input. The status and rhetorical rela- 
tion associated with each edt is undefined. At each 
step, the transfer module applies an operation that is 
aimed at building from the units in T, the discourse 
tree Tt. In the context of our discourse-transfer mod- 
ule, we need 7 types of operations: 
? SHIFT operations transfer the first edt from 
the input list into the stack; 
? REDUCE operations pop the two discourse 
trees located at the top of the stack; combine 
them into a new tree updating the statuses 
and rhetorical relation names of the trees in- 
volved in the operation; and push the new 
tree on the top of the stack. These opera- 
tions are used to build the structure of the 
discourse tree in the target language. 
? BREAK operations are used in order to break 
the edt at the beginning of the input list into 
a predetermined number of units. These op- 
erations are used to ensure that the result- 
ing tree has the same number of edts as Tt. 
A BREAK operation is necessary whenever a 
Japanese edu is mapped into nmltiple English 
units. 
? CREATE-NEXT operations are used in order 
to create English discourse constituents that 
have no correspondent in the Japanese tree. 
? FUSE operations are used in order to fuse the 
edt at the top of the stack into the tree that 
immediately precedes it. These operations 
are used whenever multiple Japanese edus are 
mapped into one English edu. 
? SWAP operations wap the edt at the begin- 
ning of the input list with an edt found one 
or more positions to the right. These oper- 
ations are necessary for re-ordering discourse 
constituents. 
? ASSIGNTYPE operations assign one or more of 
the following types to the tree at the top of 
the stack: Unit, MultiUnit, Sentence, Para- 
graph, MultiParagraph, and Text. These op- 
13
erations are necessary in order to ensure sen- 
tence and paragraph boundaries that are spe- 
cific to the target language. 
For example, the first sentence of the English tree in 
Figure 1 can be obtained from the original Japanese 
sequence by following the sequence of actions (5), 
whose effects are shown in Figure 2. For the purpose 
of compactness, the figure does not illustrate the ef- 
fect of ASSIGNTYPE actions. For the same purpose, 
some lines correspond to more than one action, 
BREAK 2; SWAP 2; SHIFT; ASSIGNTYPE 
UNIT; SHIFT; REDUCE-NS-ELABORATION- 
OBJECT-ATTRIBUTE-E; ASSIGNTYPE 
MULTIUNIT; SHIFT; ASSIGNTYPE UNIT; 
SHIFT; ASSIGNTYPE UNIT; FUSE; 
ASSIGNTYPE UNIT; SWAP 2; SHIFT; 
ASSIGNTYPE UNIT; FUSE; BREAK 2; (5) 
SHIFT; ASSIGNTYPE UNIT; SHIFT; 
ASSIGNTYPE UNIT; REDUCE-NS- 
ELABORATION-ADDITIONAL; ASSIGNTYPE 
MULTIUNIT; REDUCE-NS-CONTRAST; 
ASSIGNTYPE MULTIUNIT; REDUCE-SN- 
BACKGROUND; ASSIGNTYPE SENTENCE. 
For our corpus, in order to enable a discourse- 
based transfer module to derive any English dis- 
course tree starting from any Japanese discourse 
tree, it is sufficient o implement: 
* one SHIFT operation; 
? 3 x 2 ? 85 REDUCE operations; (For each 
of the three possible pairs of nuclear- 
ity assignments NUCLEUS-SATELLITE (NS), 
SATELLITE-NUCLEUS (SN), AND NUCLEUS- 
NUCLEUS (NN), there are two possible ways 
to reduce two adjacent rees (one results 
in a binary tree, the other in a non-binary 
tree (Marcu, 1999)), and 85 relation names.) 
? three types of BREAK operations; (In our cor- 
pus, a Japanese unit is broken into two, three, 
or at most four units.) 
? one type of CREATE-NEXT operation; 
? one type of FUSE operation; 
? eleven types of SWAP operations; (In our 
corpus, Japanese units are at most l l posi- 
tions away from their location in an English- 
specific rendering.) 
? seven types of ASSIGN~\]~YPE operations: Unit, 
MultiUnit, Sentence, MultiSentence, Para- 
graph, MultiParagraph, and Text. 
These actions are sufficient for rewriting any tree 
Ts into any tree Tt, where Tt may have a different 
number of edus, where the edus of Tt may have a 
different ordering than the edus of Ts, and where 
the hierarchical structures of the two trees may be 
different as well. 
3.3 Learn ing  the  parameters  o f  the  
d i scourse - t rans fer  mode l  
We associate with each configuration of our trans- 
fer model a learning case. The cases were gener- 
ated by a program that automatically derived the 
sequence of actions that mapped the Japanese trees 
in our corpus into the sibling English trees, using the 
correspondences at the elementary unit level that 
were constructed manually. Overall, the 40 pairs of 
Japanese and English discourse trees yielded 14108 
cases. 
To each learning example, we associated a set of 
features from the following classes: 
Operat iona l  and  d iscourse  features  reflect the 
number of trees in the stack, the input list, 
and the types of the last five operations. 
They encode information pertaining to the 
types of the partial trees built up to a certain 
t ime and the rhetorical relations that hold be- 
tween these trees. 
Cor respondence-based  features  reflect the nu- 
clearity, rhetorical relations, and types of 
the Japanese trees that correspond to the 
English-like partial trees derived up to a given 
time. 
Lex ica l features  specify whether the Japanese 
spans that correspond to the structures de- 
rived up to a given time use potential dis- 
course markers, such as dakara (because) and 
no ni (although). 
The discourse transfer module uses the C4.5 pro- 
gram (Quinlan, 1993) in order to learn decision trees 
and rules that specify how Japanese discourse trees 
should be mapped into English-like trees. A ten-fold 
cross-validation evaluation of the classifier yielded an 
accuracy of 70.2% (+ 0.21). 
In order to better understand the strengths and 
weaknesses of the classifier, we also attempted to 
break the problem into smaller components. Hence, 
instead of learning all actions at once, we attempted 
to learn first whether the rewriting procedure should 
choose a SHIFT, REDUCE, BREAK, FUSE, SWAP, or 
ASSIGNTYPE operation (the "Main Action Type" 
classifier in table 3), and only then to refine this 
decision by determining what type of reduce opera- 
tion to perform, how many units to break a Japanese 
units into, how big the distance to the SWAP-ed unit 
should be, and what type of ASSIGNTYPE operation 
one should perform. Table 3 shows the sizes of each 
14
STACK 
2 
2 1" 
\[~A BORATION_(IB~TI- 
2 l "  
}~1 ABORATION_O~E_E - -  
2 I "  1' 3 
. . . . . . . . . . . . . . .  
2 1'* 1",3 
EI.ABOP, AT ION~(~E_E  
2 1" I', 3,5 
IiLA B( )RA T ION_(~T 'E_ I~ 
2 1" I', 3, 5 4' 4'" 
HI ABOl,b%TIONIOBIE~:'r A'rrRIBUTE\],3,~ .LAB(~T ON-ADDIT 
2 1"" 4" 4" 
2 1" 1 ", 3, 5 ~ A ~  \[: 
BACKGROUND 
2 1"" . - FJ.ABOIIATION-ADD\[1 r. 3 ,y -~ 
4" 4" 
INPUT LIST 
1 2 3 4 5 6 7 
1" 1'" 2 3 4 5 6 7 
2 I"  1' 3 4 5 
I"  I' 3 4 5 6 
1" 3 4 5 6 7 
1' 3 4 5 6 7 
4 5 6 7 
4 5 6 7 
4 6 7 
6 7 
c)N~ 
6 
"IONAL 
6 
)N~J. 
6 
7 
7 
BREAK 2 
SWAP 2 
SHIFT 
SHIFT 
REDUCE-NS-ELABORATION-OBJECT-ATTRIBUTE-E 
SHIFT; SHIFT 
FUSE 
SWAP 2; SHIFT; FUSE 
BREAK 2; SHIFT; SHIFT 
REDUCE-NS-ELABORATION-ADDITIONAL 
REDUCE-NN-CONTRAST 
REDUCE-SN-BACKGROUND 
ASSIGNTYPE SENTENCE 
Figure 2: Example of incremental tree reconstruction. 
data set and the performance of each of these classi- 
tiers, as determined using a ten-fold cross-validation 
procedure. For the purpose of comparison, each clas- 
sifier is paired with a majority baseline. 
The results in Table 3 show that the most diffi- 
cult subtasks to learn are that of determining the 
number of units a Japanese unit should be broken 
into and that of determining the distance to the unit 
that is to be swapped. The features we used are 
not able to refine the baseline classifiers for these 
action types. The confusion matrix for the "Main 
Action Type" classifier (see Table 5) shows that the 
system has trouble mostly identifying BREAK and 
CREATE-NEXT actions. The system has difficulty 
learning what type of nuclearity ordering to pre- 
fer (the "Nuclearity-Reduce" classifier) and what re- 
lation to choose for the English-like structure (the 
"Relation-Reduce" classifier). 
Figure 3 shows a typical learning curve, the one 
that corresponds to the "Reduce Relation" classifier. 
Our learning curves suggest hat more training data 
may improve performance. However, they also sug- 
gest that better features may be needed in order to 
improve performance significantly. 
Table 4 displays ome learned rules. The first rule 
accounts for rhetorical mappings in which the or- 
der of the nucleus and satellite of an ATTRIBUTION 
relation is changed when translated from Japanese 
into English. The second rule was learned in order 
to map EXAMPLE Japanese satellites into EVIDENCE 
English satellites. 
1R 15
Classifier @ cases 
General 
(Learns all classes at once) 
Main Action Type 
AssignType 
Break 
Nuclearity-Reduce 
Relation-Reduce 
Swap 
14108 
14108 
6416 
394 
2388 
2388 
842 
Accuracy (10-fold cross validation) 
70.20% (+0.21) 
82.53% (?0.25) 
90.46% (?0.39) 
82.91% (?1.40) 
67.43% (?1.03) 
48.20% (?i.01) 
62.98% (?1.62) 
Majority baseline accuracy 
22.05% (on ASSIGNTYPE UNIT) 
45.47% (on ASSIGNTYPE) 
57.30% (on ASSIGNTYPE Unit) 
82.91% (on BREAK 2) 
50.92% (on KS) 
17.18% (on ELABORATION- 
OBJECT-ATTRIBUTE-E)  
62.98% (on SWAP 1) 
Table 3: Performance of the classifiers 
~oo 
440o 
~oo 
38 oo 
~oa 
I I I I 
RtlauoaRcaa? e 
tC~ xlO 3 
Figure 3: Learning curve for the Relation-Reduce 
classifier. 
if rhetRelOfStack-llnJapTree = ATTRIBUTION 
then rhetRelOffFopStacklnEngTree ~ ATTRIBUTION 
if rhetRelOffFopStacklnJapTree ---- EXAMPLE A 
isSentenceTheLastUnitlnJapTreeOfropStack = f lse
then rhetRelOfI'opStackInEngTree ~ EVIDENCE 
Table 4: Rule examples for the Relation-Reduce 
classifier. 
4 Eva luat ion  o f  the  d i scourse -based  
t rans fer  modu le  
By applying the General classifier or the other six 
classifiers successively, one can map any Japanese 
discourse tree into a tree whose structure comes 
closer to the natural rendering of English. To evalu- 
ate the discourse-based transfer module, we carried 
out a ten-fold cross-validation experiment. That is, 
we trained the classifiers on 36 pairs of manually 
built and aligned discourse structures, and we then 
used the learned classifiers in order to map 4 un- 
seen Japanese discourse trees into English-like trees. 
We measured the similarity of the derived trees with 
the English trees built manually, using the metrics 
discussed in Section 2. We repeated the procedure 
ten times, each time training and testing on different 
subsets of tree pairs. 
Act ion  (a) (b) (c) (d) (e) (f) (g) 
ASSIGNTYPE (a) 660 
BREAK (b) 1 2 28 1 
CREATE-NEXT (C) I S 
FUSE (d) 69 8 3 
REDUCE (e) 4 18 193 30 3 
SHIFT (f) 1 4 15 44 243 25 
.SWAP (g) 3 4 14 43 25 
Table 5: Confusion matrix for the Main Action Type 
classifier. 
We take the results reported in Table 2 as a base- 
line for our model. The baseline corresponds to ap- 
plying no knowledge of discourse. Table 6 displays 
the absolute improvement (in percentage points) in 
recall and precision figures obtained when the Gen- 
eral classifier was used to map Japanese trees into 
English-looking trees. The General classifier yielded 
the best results. The results in Table 6 are averaged 
over a ten-fold cross-validation experiment. 
The results in Table 6 show that our model 
outperforms the baseline with respect to building 
English-like discourse structures for sentences, but 
it under-performs the baseline with respect o build- 
ing English-like structures at the paragraph and text 
levels. The main shortcoming of our model seems to 
come from its low performance in assigning para- 
graph boundaries. Because our classifier does not 
learn correctly which spans to consider paragraphs 
and which spans not, the recall and precision results 
at the paragraph and text levels are negatively af- 
fected. The poorer esults at the paragraph and text 
levels can be also explained by errors whose effect cu- 
mulates during the step-by-step tree-reconstruction 
procedure; and by the fact that, for these levels, 
there is less data to learn from. 
However, if one ignores the sentence and para- 
graph boundaries and evaluates the discourse struc- 
tures overall, one can see that our model outper- 
forms the baseline on all accounts according to 
the Position-Dependent evaluation; outperforms the 
baseline with respect to the assignment of elemen- 
tary units, hierarchical spans, and nuclearity sta- 
tuses according to the Position-Independent evalu- 
ation and under-performs the baseline only slightly 
16 16
Level Units 
P-D R P-D P 
Spans 
P-D R P-D P 
Status/Nuclearity 
P-D R P-D P 
Relations 
P-D R P-D P 
Sentence +9.1 +25.5 +2.0 +19.9 +0.4 +13.4 -0.01 +8.4 
Paragraph -14.7 +1.4 -12.5 -1.7 -11.0 -2.4 -9.9 -3.3 
Text -9.6 -13.5 -7.1 -11.1 -6.3 -10.0 -5.2 -8.8 
Weighted Average +1.5 +14.1 -2.1 +9.9 -3.1 +6.4 -3.0 +3.9 
All -1.2 +2.5 -0.1 +2.9 +0.6 +3.5 +0.7 +2.6 
P-I R P-I P P-I R P-I P P-I R P-I P P-I R P-I P 
Sentence +13.4 +30.4 +3.1 +36.1 -6.3 +18.6 -10.1 +3.9 
Paragraph -15.6 +0.6 -13.5 -0.8 -11.7 -1.8 -10.3 -2.8 
Text -15.4 -23.3 -13.0 -20.4 -13.2 -19.5 -11.5 -17.0 
Weighted Average +3.6 +15.5 -2.7 +17.1 -8.5 +7.3 -10.5 -0.4 
All +12.7 +29.6 +2.0 +28.8 -5.1 +13.0 -7.9 +2.2 
Table 6: Relative evaluation of the discourse-based transfer module with respect o the figures in Table 2. 
with respect o the rhetorical relation assignment 
according to the Position-Independent evaluation. 
More sophisticated discourse features, such as those 
discussed by Maynard (1998), for example, and a 
tighter integration with the lexicogrammar of the 
two languages may yield better cues for learning 
discourse-based translation models. 
5 Conc lus ion  
We presented a systematic empirical study of the 
role of discourse structure in MT. Our study strongly 
supports the need for enriching MT systems with 
a discourse module, capable of re-ordering and re- 
packaging the information i  a source text in a way 
that is consistent with the discourse rendering of a 
target language. We presented an extended shift- 
reduce parsing model that can be used to map dis- 
course trees specific to a source language into dis- 
course trees specific to a target language. Our model 
outperforms a baseline with respect o its ability to 
predict the discourse structure of sentences. Our 
model also outperforms the baseline with respect 
to its ability to derive discourse structures that are 
closer to the natural, rhetorical rendering in a tar- 
get language than the original discourse structures 
in the source language. Our model is still unable to 
determine correctly how to re-package sentences into 
paragraphs; a better understanding of the notion of 
"paragraph" is required in order to improve this. 
Re ferences  
William A. Gale and Kenneth W. Church. 1993. A 
program for aligning sentences in bilingual cor- 
pora. Computational Linguistics, 19(1):75-102. 
Ulf Hermjakob and Raymond J. Mooney. 1997. 
Learning parse and translation decisions from ex- 
amples with rich context. In Proc. of ACL'97, 
pages 482-489, Madrid, Spain.. 
Sadao Kurohashi and Makoto Nagao. 1994. Auto- 
matic detection of discourse structure by check- 
ing surface information in sentences. In Proc. of 
COLING'94, volume 2, pages 1123-1127, Kyoto, 
Japan. 
David M. Magerman. 1995. Statistical decision-tree 
models for parsing. In Proc. of A CL '95, pages 
276-283, Cambridge, Massachusetts. 
William C. Mann and Sandra A. Thompson. 1988. 
Rhetorical structure theory: Toward a functional 
theory of text organization. Text, 8(3):243-281. 
Daniel Marcu. 1999. A decision-based approach to 
rhetorical parsing. In Proc. of A CL'99, pages 365- 
372, Maryland. 
Daniel Marcu, Estibaliz Amorrortu, and Magdalena 
Romera. 1999. Experiments inconstructing a cor- 
pus of discourse trees. In Proc. of the A CL'99 
Workshop on Standards and Tools for Discourse 
Tagging, pages 48-57, Maryland. 
Senko K. Maynard. 1998. Principles of Japanese 
Discourse: A Handbook. Cambridge Univ. Press. 
J. Ross Quinlan. 1993. C4.5: Programs for Machine 
Learning. Morgan Kaufmann Publishers. 
Sidney Siegel and N.J. Castellan. 1988. Non- 
parametric Statistics for the Behavioral Sciences. 
McGraw-Hill, Second edition. 
Kazuo Sumita, Kenji Ono, T. Chino, Teruhiko 
Ukita, and Shin'ya Amano. 1992. A discourse 
structure analyzer for Japanese text. In Proceed- 
ings of the International Conference on Fifth Gen- 
eration Computer Systems, v 2, pages 1133-1140. 
J. White and T. O'Connell. 1994. Evaluation in 
the ARPA machine-translation program: 1993 
methodology. In Proceedings ofthe ARPA Human 
Language Technology Workshop, pages 135-140, 
Washington, D.C. 
17 17
An Empirical Study in Multilingual Natural Language 
Generation: What Should A Text Planner Do? 
Danie l  Marcu  
Information Sciences Institute and 
Department of Computer Science 
University of Southern California 
4676 Admiralty Way, Suite 1001 
Marina del Rey, CA 90292 
marcu@isi, edu 
Lynn Carlson 
U.S. Department of Defense 
, _Et .  Meade,_ MD~.20755 
lmcarls@afterlife, ncsc. rail 
Maki  Watanabe 
Department of Linguistics 
........ ? , ~n i .v ,  e rs i ty .o f  Southern~California 
Los Angeles, CA 90089 
m watanab@usc, edu 
Abstract 
We present discourse annotation work aimed at con- 
structing a parallel corpus of Rhetorical Structure 
trees for a collection of Japanese texts and their cor- 
responding English translations. We discuss impli- 
cations of our empirical findings for the task of text 
planning in the context of implementing multilingual 
natural anguage generation systems. 
1 I n t roduct ion  
The natural anguage generation community has em- 
phasized for a number of years the strengths of mul- 
tilingual generation (MGEN) systems (Iordanskaja 
et al, 1992; RTsner and Stede, 1992; Reiter and Mel- 
lish, 1993; Goldberg et al, 1994; Paris et al, 1995; 
Power and Scott, 1998). These strengths concern the 
reuse of knowledge, the support for early drafts in 
several anguages, the support for maintaining con- 
sistency when making changes, the support for pro- 
ducing alternative formulations, and the potential 
for producing higher quality outputs than machine 
translation. (The weaknesses concern the high-cost 
of building large, language-independent k owledge 
bases, and the dilficulty of producing high-quality. 
broad-coverage neration algorithms.) 
From an economic perspective, the more a sys- 
tem can rely on language independent modules for 
the purpose of multilingual generatiom the better. 
If an MGEN system needs to develop language de- 
pendent knowledge bases, and language dependent 
algorithms for content selection, text planning, and 
sentence planning, it-is difficult to justify its eco- 
nomic viability. However, if most of these compo- 
nents are language independent and/or much of the 
code can be re-used, an MGEN system becomes a
viable option. 
.Many of the earl3 implementations of MGEN sys- 
tems have adopted the perspective that text plan- 
ners can be implemented as language-independent 
modules (lordanskaja el, ;11., 1992: Goldberg et el., 
1994), possibly followed by a hm:aricatwn stage, 
in which discourse l.rees are re-written to refleet~ 
language-specific constraints (R6sner and Stede. 
1992; St,ede, 1999). Although such an approach may 
17 
be adequate for highly restricted text genres, such 
as weather forecasts, it usually poses problems for 
less restricted genres. Studies of instruction man- 
uals (RTsner and Stede, 1992; Delin et al, 1994: 
Delin et al, 1996) suggest hat there are variations 
with respect to the way high-level communicative 
goals  are realized across languages. For example, 
Delin et al (1994) noticed that sentences (1), (2), 
and (3), which were taken from a trilingual instruc- 
tion manual for a step-aerobics machine, yield non- 
isomorphic Rhetorical Structure (Mann and Thomp- 
son, 1988) analyses in English, French, and German 
respectively (see Figure 1). 
English: \[The stepping load can be altered I \] 
\[by loosening the locking lever 2\] \[and changing 
the position of the cylinder foota\]. 
(1) 
French: \[Pour modifier la charge d'appui, l\] 
\[desserrer l s levieres 2\] \[puis d6placer le pied 
des v6rins a\] (\[To modify the load stepping ~\] 
\[loosen the levers 2\] \[then change .the foot of 
the cylinder foot.el) 
(2) 
German: \[,Nach Lockern der Klelnmhebel 2\] (3) 
\[kann t \] \[durch Verschieben des Zylinderfudes 3 \]
\[die Tretbelastung verS.ndert werden.~ \] (\[After 
loosening of the levers 2\] \[can'\] \[by puslfing of 
the cylinder foot 3\] \[the load changed be, ~\]) 
Hmvever, previous.discourse ,studies do .not es- 
timate how ubiquitous uch non-isomorphic analy- 
ses are. Are the examples above an exception or 
the norm? Are non-isomorphic analyses specific 
to discourse structures built, across elementary dis- 
course units of single sentences, or do they also 
occur across sentences and paragraphs? If non- 
isomorphism is ubiquitous, how should an MGEN 
system be designed in order to effectively deal wit h 
non-isomorphic discourse structures when mapping 
knowledge bases into multiple languages? 
In this paper, we describe an experiment that was 
designed to answer these questions. To investigate 
English French German 
Circumstance 
_ .. - -.' ! : ,  !'{Modifier)- ~ ~ r - '  O-.?ckem ) 
2 3 2 3 3 1 
Loosen Change Loosen Change Change Alter 
(Desserrer) (Deplacer) (Verschieben) (Verandert) 
Figure 1: Contrasting multi l ingual discourse structure representations (Delin et al, 1994, p. 63) 
how discourse structures differ across languages, we 
manual ly built a parallel corpus of discourse trees of 
newspaper Japanese texts and their corresponding 
English translations. In section 2, we present some 
of the problems pecific to the construction of such 
a corpus. In section 3, we present our experiment 
and discuss our empirical findings. In Section 4, we 
discuss the implications of our work for the task of 
text planning, in the context of multi l ingual natural 
language generation. 
2 Towards bu i ld ing a paral le l  corpus 
of d iscourse trees: an example  
Consider, for example, Japanese sentence (4), a 
word-by-word "gloss" of it (5), and a two-sentence 
translation of it that was produced by a professional 
translator (6). 
~/rf~ ~ 4\] \ [~afL~, t~,  5\] \ [~< g-~.~oq~t-? 
{4) 
Thompson,  1988). If we analyze the text frag- 
ments closely, we will notice that in translat ing sen- 
tence (4), a professional translator chose to realize 
the information in Japanese unit 2 first (unit 2 in 
text (4) corresponds roughly to unit I in text (6)); 
to realize then some of the information in Japanese 
unit 1 (part of unit 1 in text (4) corresponds to unit 
2 in text (6)); to fuse then information given in units 
1, 3, and 5 in text (4) and realize it in English as 
unit 3; and so on. Also, the translator chose to re- 
package the information in the original Japanese sen- 
tence into two English sentences. 
At the elementary unit level, the correspondence 
between Japanese sentence (4) and its English trans- 
lation (6) can be represented as in (7), where j C e 
denotes the fact that the semantic content of unit 
j is realized fully in unit e; j D e denotes the fact 
that the semantic content of unit e is realized fully 
in unit. j; j = e denotes the fact that units j and e 
are semantical ly equivalent; and j ~ e denotes the 
fact that  there is a semantic overlap between units j 
and e, but neither proper inclusion nor proper equiv- 
alence. 
\[The Ministry of Health and Welfare last year 
revealed 1\] \[population of future estimate ac- 
cording to 2\] \[m future 1.499 persons as the 
lowest a\] \[that after *SAB* rising to turn that 4\] 
\[*they* estimated but 5\] \[already the estimate 
misses a point G\] \[prediction became, r\] 
\[In its future population estimateQ\] \[made 
public last yearf\] \[the Ministry of Health and 
Welfare predicted that the SAB would drop to 
a new low of 1.-199 in the future, a\] \[but would 
make a comeback after that .4\] \[increasing once 
again/;\] \[However. it looks as if that prediction 
will be quickly shattered, t; \] 
(.~) 
(6) 
The labeled spans o f  lext represent elementary 
discourse units (~dus). i.e.. minimal text spans that 
have an unambiguous discourse function (Mann and 
18 
J l  
.12 
33 
../4 
3s 
./6 
Jr 
e2; j l  ~ ca :  
----eel; 
C e3; 
e4; j4  ~ e5 ;  
e3; 
C e6; 
C e6 
(7) 
Hence, tile mappings in (7) provide an explicit, rep- 
resentation of the way information is re-ordered and 
re-packaged when translated from Japanese into En- 
glish. However, when translating text, it is not only 
that information is re-packaged and re-ordered; it 
is also that the rhetorical rendering changes. What  
is realized in Japanese using an ELABORATION rela- 
tion can be realized in English using, for example, a 
CON' I 'RAST  or  a CONCESSION relation. 
Figure 2 presents in the style of Mann and Thomp-  
C oncess~or ._~ " 
i _ . .  , . .  ? 
attriiout+or, a:tr~bmi:~n e aioorahor - oaject- ~.It,lolute- e
I 
e alaorzm~.-mje+t e : : .  . . . . .  " . . . . . . . .  .temporal- a l te r . . .+ .  +,~P~a:+m~+ ~~.+.~i=m +~+ +-++++ia ; i+- '> +": 
:1 ) (Z)  (~ (a)  " + . . . . . . . . . . . .  +~ . . . . . . . . . . . .  + . . . .  S + ' . . . . .  + + +" " 
b.?  (The Ministry ~ '(popu alien - X -E~.  'tfutur @g (that - a~er 
of Health and of future estimate e 1.499 persons \[SAB I rLqng - to 
Weffare last yeal - a:ccrcling - to) as the lowest) turn lhal) 
revealed) 
t - .=  
I-2 3-3 
e l a b o r ~ r  |buto--? 
I11 12) (3) ~.3 
In Its fulure made pobll? the Mlnl~ry el:l ~ ~t~,n_.~ d.d ~ i on.~ i 
I~Jl~liOn ~"  year, ~ Heolth and 
LIv.~r "~,.. 
estimates WeHare . (4) (5) but would increasing 
the SAB rmk~a c~nce again. 
comet~sc~r 
wo~rld rop to after that. 
a ni~a. low o~ 
1,4~0 In the 
future, 
(~ 
,,,~ie+gai 
Figure 2: The discourse structures of texts (4) and (6). 
son (1988) the discourse structures of text frag- 
ments (4) and (6). Each discourse structure is a 
tree whose leaves correspond to the edus and whose 
internal nodes correspond t.o contiguous text spans. 
Each node is characterized by a status (NUCLEUS or 
SATELLITE) and a rhetorical relation, which is a re- 
lation that holds between two non-overlapping text 
spans. (There are a few exceptions to this rule: some 
relations, such as the CONTRAST relation that holds 
between unit \[3\] and span \[4,.5\] in the structure of the 
English text are multinuclear.) The distinction be- 
tween nuclei and satellites comes from the empirical 
observation that the nucleus expresses what is more 
essential to the writer's intention than the satellite: 
and that the nucleus of a rhetorical relation is com- 
prehensible independent of the satellite, but not vice 
versa. Rhetorical relations'that end.in the.suffix :'-e". 
denote relations that correspond to embedded syn- 
tactic constituents. For example, the ELABORATION- 
OBJEC'T-ATTRIBUTE-E relation that holds between 
units 9. and 1 in the English discourse structure cor- 
responds to a restrictive relative. We chose to label 
these relations because we have noticed that they 
often dominate complex discourse trees, whose ele- 
nlentary units are fully fleshed clauses. 
If one knows the mappings at the ed~l level, 
one can determine the mappings at. the span (dis- 
course constituent) level as well. For example, us- 
19 
ing the elementary mappings in (7), one can deter- 
mine that Japanese span \[1,2\] corresponds to English 
span \[1,2\], Japanese unit \[4\] to English span \[4,5\]. 
,Japanese span \[6,7\] to English unit \[6\], Japanese 
span \[1,5\] to English span \[1,5\], and so on. As Fig- 
ure 2 shows, the CONCESSION relation that holds be- 
tween spans \[1,5\] and \[6,7\] in the Japanese tree corre- 
sponds to a similar relation that holds between span 
\[I.5\] and unit \[6\] in the English tree (modulo the fact 
that, in Japanese, the relation holds between sen- 
tence fragments, while in English it holds between 
fldl sentences). However, the TEMPORAL-AFTER re- 
lation that holds between units \[3\] and \[4\] in the 
Japanese tree is realized as a CONTRAST relation 
between unit  \[3\] and span \[4,5\] in the English tree: 
And because Japanese units \[6\] and \[7\] are fused 
into: unit \[6\] ing:nglish, the  rel,ation .ELA-BORAT.ION- 
OBJECT-ATTRIBUTE-E iS no longer made explicit in 
the English text. 
Assume now that it is the task of an MGEN sys- 
tem to produce from a knowledge base texts (4) 
and (6). The system will have to select, the ap- 
propriate information, generate text plans for the 
two texts, generate sentence plans, and realize them. 
Should the syst.en~ generate a text plan having a 
structure similar to the PtST analysis at. the top or 
the bottom of Figure 2"? Or something in between'? 
As one can see, the discourse trees in Figure 2 are 
quite different: they suggest hat depending on the 
output language, text plans should use different re- 
lations, different orderings of elementary units, dif- 
ferent aggregations across semantic units, etc. 
Some researchers may argue that the two RST 
analyses in Figure 2 are too specific. That  they, 
figures. 
In computing Position-Dependent (P-D) recall 
and precision figures, a Japanese span was consid- 
ered to match an English span when the Japanese 
span contained all the Japanese edus that cor- 
responded to the edus in the English span, and 
in fact, correspond t0.:text.,~plaz!s .,tha-t .l!.ave bee.n 
already refined by an aggregation module and ar- 
guably, even by a sentence planner. After all, the 
re-ordering of units 1 and 2 can be explained only 
in terms of different syntactic ontraints in Japanese 
and English. We agree with such a concern. Never- 
theless, as our experiment shows, significant differ- 
ences across discourse trees are found not only for 
trees built at the sentence level, but also for trees 
built at the paragraph and text levels. For these lev- 
els, it is difficult to explain the differences in terms 
of language-specific syntactic onstraints. Rather, it 
seems more adequate to assume that there are sig- 
nificant differences with respect o the way informa- 
tion is organized rhetorically across languages. The 
experiment described in the next section estimates 
quantitatively this difference. 
3 Exper iment  
In order to assess how similar discourse structures 
are across languages, we built manually a cor- 
pus of discourse trees for 40 Japanese texts and 
their corresponding translations. The texts, se- 
lected randomly from the ARPA corpus (White 
and O'Connell, 1994), contained on average about 
460 words. We developed a discourse annota- 
tion protocol for ,Japanese and English along the 
lines followed by Marcu et al (1999). We used 
Marcu's discourse annotation tool (1999) in order 
to manually construct he discourse structure of all 
Japanese and English texts it, the corpus. 10~. of 
the Japanese and English texts were rhetorically 
labeled by two of us. The agreement was sta- 
tistically significant (Kappa = 0.65.0 > 0.01 for 
Japanese and Kappa = 0.748,0 > 0.01 for En- 
glish (Carletta, 1996; Siegel-and Castellan, 1988)). 
The tool and the annotation protocol are available 
at. http://www, isi.edt,/~r, zarcu/softwa,-e/. For each 
pair of Japanese-English discourse, structures, we 
also built, manually an alignment file, which specified 
the correspondence b tween the edus of the Japanese 
and English texts. 
Using labeled recall and precision figures, we com- 
puted the similarity between English and Japanese 
discourse trees with respect t,o their assignment of 
edu boundaries, hierarchical spans, nuclearity, and 
rhetorical relations, Because the trees we comparod 
differ from one language to the other ill the ntnnber 
of elernent ary units, the order, of these units, and the 
way the units are grouped rectirsively into discourse 
spans, we comptlted two types of recall and precision 
, :+when :~e.J~laa~tese:~-and.~En~lish::spans -ap eared-in 
the same position linearly. For example, the En- 
glish tree in Figure 2 is characterized by 10 sub- 
sentential spans, which span across positions \[1,1\], 
\[2,2\], \[3,3\], \[4,4\], \[5,5\], \[6,6\], \[1,2\], \[4,5\], \[3,5\], and 
\[1,5\]. (Span \[1,6\] subsumes 2 sentences, so it is 
not sub-sentential.) The Japanese discourse tree has 
only 4 spans that could be matched in the same po- 
sitions with English spans, namely spans \[1,2\]. \[4,4\], 
\[5,5\], and \[1,5\]. Hence the similarity between the 
Japanese tree and the English tree with respect to 
their discourse structure below the sentence level has 
a recall of 4/10 and a precision of 4 / l l  (in Figure 2, 
there are 11 sub-sentential Japanese spans). 
In computing Position-Independent (P-I) recall 
and precision figures, even when a Japanese span 
"floated" during the translation to a position in the 
English tree that was different from the position 
in the initial tree, the P-I recall and precision fig- 
ures are affected less than when computing Position- 
Dependent figures. The position-independent fig- 
ures reflect the intuition that if two trees tl and t2 
both have a subtree t, tl and 12 are more similar 
than if they were if they didn't share ally subtree. 
For instance, for the spans at the sub-sentential level 
in the trees in Figure 2 the position-independent 
recall is 6/10 and the position-independent preci- 
sion is 6/11 because in addition to spans \[1,2\], \[4,4\], 
\[5,5\], and \[1,5\], one can also match Japanese spat, 
\[1,1\] to English spa,, \[2,2\] and Japanese spa,, \[2,2\] 
to Japanese span \[1,1\]. The Position-Independent 
figures offer a more optimistic metric for comparing 
discourse trees. They span a wider range of values 
than the Position-Dependent figures, which enables 
a finer grained comparison, which in turn enables 
a better characterization of the differ.ences between 
Japanese and English discourse structures. 
In order to provide a better estimate of how close 
two  discourse trees were, we computed Position- 
Dependent and -Independent recall and precision fig- 
ures for the sentential evel (where units are given 
by edus and spans are given by sets of edus or single 
sentences); paragraph level (where units are given by 
sentences and spans are given by sets of sentences or 
single paragraphs): and text level (where units are 
given by paragraphs and spans are given by sets of 
paragraphs). These figures offer a detailed picture of 
how discourse structures and relations are mapped 
-from one languageto the other. Some of the differ- 
ences at the sentence level can be explained by differ- 
ences between the syntactic structures of Japanese 
20 
Level 
Sentence 
Paragraph 
Text 
Weighted Average 
All .- 
Sentence 
Paragraph 
Text 
Weighted Average 
All 
Units 
P-D R P-D P 
29.1 25.0 
53.9 53.4 
41.3 42.6 
36.0 32.5 
Spans 
P-D R P-D P 
27.2 22.7 
46.8 47.3 
31.5 32.6 
31.8 28.4 
? 8 .2  ..... : 7,4 .. . . .  - .5 .9  . ...... 5.3.: 
P- IR  P - IP  
71.0 61.0 
62.1 61.6 
74.1 76.5 
69.6 63.0 
74.5 66.8 
P- IR  P - IP  
56.0 46.6 
53.2 53.8 
54.4 56.5 
55.2 49.2 
50.6 45.8 
Nuclei 
P-D R P-D P 
21.3 17.7 
38.6 39.0 
28.8 29.9 
26.0 23.1 
..... -.4A ............ 3~9:_ 
P- IR  P - IP  
44.3 36.9 
43.3 43.8 
48.5 50.4 
44.8 39.9 
39.4 35.7 
Relations 
P-D R P-D P 
14.9 12.4 
31.9 32.3 
26.1 27.1 
20.1 17.9 
. . . .  ..3.3 . . . . . . .  3 .0 .  
P - IR  P - IP  
30.5 25.4 
35.1 35.5 
41.1 42.7 
33.1 29.5 
26.8 24.3 
Table 1: Similarity of the Japanese and English discourse structures 
and English. The differences at the paragraph and 
text levels have a purely rhetorical explanation. 
As expected, when one computes the recall and 
precision figures with respect to the nuclearity and 
relation assignments, one also factors in the nucle- 
arity status and the rhetorical relation that is asso- 
ciated with each span. 
Table 1 summarizes the results (P-D and P-I 
(R)ecall and (P)recision figures) for each level (Sen- 
tence, Paragraph, and Text). It presents Recall and 
Precision figures with respect to span assignment, 
nuclearity status, and rhetorical relation labeling of 
discourse spans. The numbers in the "Weighted 
Average" line report averages of the Sentence-, 
Paragraph-, and Text-specific figures, weighted ac- 
cording to the number of units at each level. The 
numbers in the "All" line reflect recall and precision 
figures computed across the entire trees, with no at- 
tention paid t.o sentence and paragraph boundaries. 
Given the significantly different syntactic struc- 
tures of Japanese and English. we were not surprised 
by tile low recall and precision results that reflect 
the similarity between discourse trees built below 
the sentence level. However, as Table 1 shows, there 
are astonishing differences between discourse trees 
at the paragraph and text. levels as well. For exam- 
pie, the Position-Independent figures show that only 
about 62% of the sentences: and only :about 53% of 
the hierarchical spans built across sentences could be 
matched between the two corpora. When one looks 
at the nuclearity status and rhetorical relations as- 
sociated with the spans built across sentences, the 
P-I recall and precision figures drop to about 43c2~ 
and :/5~ respectively. 
The differences in recall and precision are ex- 
l)lained both by differen,-es in the way information is 
packaged rote paragraphs in the-two languages arid 
the way it is structured rhetorically both within and 
above the paragraph level. 
4 How shou ld  a mul t i l i ngua l  text  
p lanner  work?  
The results in Section 3 strongly suggest hat if one 
is to build text plans in the context of a Japanese- 
English multilingual generation system, a language- 
independent text planning module whose output is 
mapped straightforwardly into sentence plans (Ior- 
danskaja et al, 1992; Goldberg et al, 1994) will not 
do. The differences between the rhetorical structures 
of Japanese and English texts are simply too big to 
support the derivation of a unique text plan, which 
would subsume both the Japanese- and English- 
specific realizations. If we are to build MGEN sys- 
tems capable of generating rich texts in languages 
as distant as English and Japanese, we would need 
to use more sophisticated techniques. In the rest of 
this section, we discuss a set of possible approaches, 
which are consistent with work that has been carried 
out to date in the NLG field. 
4.1 Use text  p lan representat ions  that  are 
more  abst rac t  than  d iscourse trees 
Delin et al (1994) have shown that although tile 
rhetorical renderings in Figure 1 are non-isomorphic. 
dmy are alt subsumed by one .commol~, more.ab- 
stract t.ext.-plan representation language that for- 
realizes the procedural relations of Generation and 
Enablement (Goldman, 1970). One caa~ conceive of. 
text plans being represented as sequences of actions 
or hierarchies of actions and goals over which one can 
identify Generation and Enablement relations that 
hold between them. In such a framework, text plan- 
ning is carried out ill a language-independent man- 
ner. which is then followed by a rhetorical "'fleshing 
out". (Delin et al (1994) have shown how Gener- 
ation and Enablenlent relations are realized rhetor- 
ically in various languages using relations such as 
PURPOSE, 'SEQUENCE, CONDITION, and MEANS.) 
Bateman and Rondhuis (1997) suggest that the 
variability present in Delin et al's Rhetorical Struc- 
21 
ture analyses in Figure 1 can be explained by the 
inadequate mixture of intentional and semantic re- 
lations, at different levels of granularity. They pro- 
pose that discourse phenomena should be accounted 
for at a more abstract level than RST relations 
and they present a classification system in terms 
discourse-tree r writing module capable of rewriting 
P-specific discourse structures into O-specific dis- 
course structures. When generating texts in lan- 
guage P, the MGEN system works as a monolin- 
gum generator. When generating texts in language 
O, the MGEN system generates a text plan in lan- 
of "stratification", ..'!me?afunction?., ,,and .::p~radig,: ........ guage.-~, xnapsitdr~to.=taaag,uageO.,~ anti then ~proceeds - .. 
matic/syntagmatic axiality" that enables one to rep- 
resent discourse structures at multiple levels of ab- 
straction. 
Adopting such an approach could be an extremely 
rewarding enterprise. Unfortunately, the research 
of Delin et al (1994) and Bateman and Rond- 
huis (1997) cannot be applied yet to unrestricted o- 
mains. Generation and Enablement are only two of 
the abstract relations that can hold between actions 
and goals. And some texts, such as descriptions, are 
difficult to characterize only in terms of actions and 
goals. Building a "complete" taxonomy of such ab- 
stract relations and identifying adequate mappings 
between there relations and rhetorical relations are 
still open problems. 
4.2 Derive a language- independent 
discourse structure, and then l inearize 
it 
RSsner and Stede (1992) and Stede (1999) assume 
that a discourse representation g la Mann and 
Thompson imposes no contraints on the linear order 
of the leaves. For tile purpose of multilingual text 
planning, one can, hence, assume that a language- 
independent text planner derives first a language- 
independent rhetorical structure and then linearizes 
it, i.e., transforms it to make it language specific. 
The transformations that RSsner and Stede have ap- 
plied concern primarily re-orderings of the children 
of some nodes and re-assignment of rhetorical rela- 
tion labels. But given, for example, tile significant 
differences between the discourse structures in Fig- 
ure 2, it is difficult to envision what the language- 
independent text plan might look like. It is deft- 
nitely possible to conceive of such a text plan rep- 
resentation. However, the linearization module will 
need then to be much more sophisticated: it will 
need to be able to rewrite full structures, re-order 
constituents, aggregate_across possibly non-adjacent 
units, etc. 
4.3 hnp lement  a text  p lann ing  a lgor i thm 
for one language only. For all o ther  
languages,  dev ise d i scourse - t ree  
rewr i t ing  modu les  
In this approach, the system developer assigns a pre- 
ferrential status to one of the languages that are 
to be handled I) 3 ' the MGEN system. Lot's call 
this language P. The system developer implenlents 
text planning algorithms only for this language. For 
any other language O, the developer itnplements a 
22 
further with the sentence planning and realization 
stages. Marcu et al (2000) present and evaluate a 
discourse-tree r writing algorithm that exploits ma- 
chine learning methods in order to map Japanese 
discourse trees into discourse trees that resemble 
English-specific renderings. 
The advantage of such an approach is that the 
tree-rewriting modules can be also used in the con- 
text of machine translation systems in order to re- 
package and re-organize the input text rhetorically, 
to reflect constraints pecific to the target language. 
The disadvantage is that, from an NLG perspective, 
there is no guarantee that such a system could pro- 
duce better results than a system that implements 
language-dependent text planning modules. 
4.4 Derive language-dependent  text plans 
Another viable approach is to acknowledge that 
text plans vary significantly across languages and, 
therefore, should be derived by language-dependent 
planners. To this end, one could use both top- 
down (How, 1993; Moore and Paris, 1993) and 
bottom-up (Marcu, t997; Mellish et al, 1998) text 
planning algorithms. The advantage of this ap- 
proach is that it has the potential of producing trees 
that reflect tile peculiarities specific to any language. 
The disadvantage is that only the text planning al- 
gorithms are general: the plan operators and the 
rhetorical relations they operate with are language- 
dependent, and hence, more expensive to develop 
and maintain. 
4.5 Discuss ion 
Depending oil tile languages and text genres it op- 
erates with, all MGEN system may get away with 
a language-independent text planner. However, 
for sophisticated genres and distant languages, im- 
plementing a language-independent planner that is 
straightforwardly'mapped i:nto sentence, plans does 
not appear to be a felicitous solution. We enu- 
merated four possible alternatives for addressing the 
text planning problem in an MGEN system. Each 
of tile approaches has its own pluses and minuses. 
Which will eventually win in large-scale deployable 
MGEN systems remains an open question. 
Re ferences  
John A. Bateman and Klaas Jan Flondliuis. 1997. 
Coherence relations: Towards a general specifica- 
tion. Dtsco~u'se Processes, 24:3-49. 
Jean Carletta. 1996. Assessing agreement on clas- 
sification tasks: The kappa statistic. Computa- 
tional Linguistics, 22(2):249-254, June. 
Judy L. Delin, Anthony Hartley, C@ile L. Paris, Do- 
nia R. Scott, and Keith Vander Linden. 1994. Ex- 
pressing procedural relationships in multilingual 
-. instructions. In -P. roeeedirtgs.,of 4he .,geventh:-,tnter~ 
national Workshop on Natural Language Genera- 
tion, pages 61-70, Kennebunkport, Maine, June. 
J. Delin, D. Scott, and A. Hartley. 1996. Prag- 
matic congruence through language-specific map- 
pings from semantics to syntax. Technical report, 
ITRI Research Report ITRI-96-12, University of 
Brighton. 
E. Goldberg, N. Driedger, and R. Kittredge. 1994. 
Using natural-language processing to produce 
weather forecasts. IEEE Expert, 9(2):45-53. 
A.I. Goldman. 1970. A Theory of Human Action. 
Prentice Hall, Englewood Cliffs, NJ. 
Eduard H. Hovy. 1993. Automated iscourse gen- 
eration using discourse structure relations. Artifi- 
cial Intelligence, 63(1-2):341-386, October. 
L. Iordanskaja, M. Kim, R. Kittredge, B. Lavoie, 
and A. Polguere. 1992. Generation of extended 
bilingual statistical reports. In Proceedings of 
the 14th International Conference on Compu- 
tational Linguistics (COLING'92), pages 1019- 
1023, Nantes, France. 
William C. Mann and Sandra A. Thompson. 1988. 
Rhetorical structure theory: Toward a functional 
theory of text organization. Text, 8(3):243-281. 
Daniel Marcu. 1997. From local to global coherence: 
A bottom-up approach to text planning. In Pro- 
ceedings of the Fourteenth National Conference on 
Artificial Intelligence (AAAI-97), pages 629-635, 
Providence, Rhode Island, July 28-31. 
Daniel Marcu, Estibaliz Amorrortu, and Magdalena 
Romera. 1999. Experiments in constructing a
corpus of discourse trees, tn Proceedings of the 
ACL'99 Workshop on Standards and Tools for 
Discourse Tagging, pages 48-.57, University of 
Maryland. June 22. 
Daniel Marcu, Lynn Carlson, and Maki Watan- 
abe. 2000. The automatic translation of discourse 
structures. In Proceedings of the First Annual 
Meeting of the A'orth American Chapter of the As- 
sociation for Computational Linguistics NAACL- 
2000, Seattle, Washington. April 29 - May 3. 
Chris Mellish. Alistair Knott. Jon Oberlander, 
and Mick O'Donnell. 1998. Experiments using 
stochastic search for text planning. In Proceed- 
}ngs of lbc 9lh International H'oHcstwp on .Vatvral 
Language. (;era'cation. pages 98 107. Niagara-on- 
the-Lake, Canada. August 5-7. 
,lohanna D. Moore and Cdcile L. Paris. 1993. Plan- 
ning text \['or advisory dialogues: Capturing inten- 
23 
tional and rhetorical information. Computational _ 
Linguistics, 19(4):651-694. 
C. Paris, K. Vander Linden, M. Fischer, A. Hart- 
Icy, L. Pemberton, R. Power, and D. Scott. 1995. 
A support tool for writing multilingual instruc- 
tions. In Proceedings. of the 14th International 
Joint. ,Gonfer~nee. ~on.:Artificial ~tnt~ttigenee (IJ- 
CA I'95), pages 1398-1404, Montreal, Canada. 
Richar Power and Donia Scott. 1998. Multilingual 
authoring using feedback texts. In Proceedings of 
the 36th Annual Meeting of the Association for 
Computational Linguistics (ACL'98), Montreal, 
Canada, August. 
Ehud Reiter and Chris Mellish. 1993. Optimizing 
the costs and benefits of natural anguage gen- 
eration. In Proceedings of the 131h International 
Joint Conference on Artificial Intelligence, pages 
1164-1169. 
Dietmar R6sner and Manfred Stede. 1992. Cus- 
tomizing RST for the automatic production 
of technical manuals. In R. Dale, E. How, 
D. R6sner, and O. Stock, editors, Aspects of Au- 
tomated Natural Language Generation; 6th Inter- 
national Workshop on Natural Language Gener- 
ation, number 587 in Lecture Notes in Artificial 
Intelligence, pages 199-214, Trento, Italy, April. 
Springer-Verlag. 
Sidney Siegel and N.J. Castellan. 1988. Non- 
parametric Statistics for the Behavioral Sciences. 
McGraw-Hill, second edition. 
Manfred Stede. 1999. Rhetorical structure and the- 
matic structure in text generation. In Working 
Notes of the Workshop on Levels of Represen- 
tation m Discourse, pages 117-123, Edinburgh, 
Scotland, July 7-9. 
J. White and T. O'Connell. 1994. Evalua- 
tion in the ARPA machine-translation pro- 
gram: 1993 methodology. In Proceedings of 
the .4RPA Human language Technology Work- 
shop, pages 135-140, Washington, D.C. See also 
h ttp :/ /  ursula, geowetown, edu/. 
