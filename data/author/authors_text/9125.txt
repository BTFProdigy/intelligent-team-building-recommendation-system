Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 46?53,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Towards the Interpretation of Utterance Sequences in a Dialogue System
Ingrid Zukerman and Patrick Ye and Kapil Kumar Gupta and Enes Makalic
Faculty of Information Technology
Monash University
Clayton, VICTORIA 3800, Australia
ingrid@infotech.monash.edu.au, {ye.patrick,kapil.k.gupta,emakalic}@gmail.com
Abstract
This paper describes a probabilistic mech-
anism for the interpretation of sentence se-
quences developed for a spoken dialogue
system mounted on a robotic agent. The
mechanism receives as input a sequence of
sentences, and produces an interpretation
which integrates the interpretations of in-
dividual sentences. For our evaluation, we
collected a corpus of hypothetical requests
to a robot. Our mechanism exhibits good
performance for sentence pairs, but re-
quires further improvements for sentence
sequences.
1 Introduction
DORIS (Dialogue Oriented Roaming Interactive
System) is a spoken dialogue system under devel-
opment, which will eventually be mounted on a
household robot. The focus of our current work is
on DORIS?s language interpretation module called
Scusi?. In this paper, we consider the interpreta-
tion of a sequence of sentences.
People often utter several separate sentences to
convey their wishes, rather than producing a sin-
gle sentence that contains all the relevant informa-
tion (Zweig et al, 2008). For instance, people are
likely to say ?Go to my office. Get my mug. It is
on the table.?, instead of ?Get my mug on the table
in my office?. This observation, which was val-
idated in our corpus study (Section 4), motivates
the mechanism for the interpretation of a sequence
of sentences presented in this paper. Our mecha-
nism extends our probabilistic process for inter-
preting single spoken utterances (Zukerman et al,
2008) in that (1) it determines which sentences in
a sequence are related, and if so, combines them
into an integrated interpretation; and (2) it pro-
vides a formulation for estimating the probability
of an interpretation of a sentence sequence, which
supports the selection of the most probable inter-
pretation. Our evaluation demonstrates that our
mechanism performs well in understanding textual
sentence pairs of different length and level of com-
plexity, and highlights particular aspects of our al-
gorithms that require further improvements (Sec-
tion 4).
In the next section, we describe our mechanism
for interpreting a sentence sequence. In Section 3,
we present our formalism for assessing the prob-
ability of an interpretation. The performance of
our system is evaluated in Section 4, followed by
related research and concluding remarks.
2 Interpreting a Sequence of Utterances
Scusi? employs an anytime algorithm to interpret
a sequence of sentences (Algorithm 1). The algo-
rithm generates interpretations until time runs out
(in our case, until a certain number of iterations
has been executed). In Steps 1?5, Algorithm 1
processes each sentence separately according to
the interpretation process for single sentences de-
scribed in (Zukerman et al, 2008).1 Charniak?s
probabilistic parser2 is applied to generate parse
trees for each sentence in the sequence. The parser
produces up to N (= 50) parse trees for each sen-
tence, associating each parse tree with a probabil-
ity. The parse trees for each sentence are then it-
eratively considered in descending order of proba-
bility, and algorithmically mapped into Uninstan-
tiated Concept Graphs (UCGs) ? a representa-
1Although DORIS is a spoken dialogue system, our cur-
rent results pertain to textual input only. Hence, we omit the
aspects of our work pertaining to spoken input.
2ftp://ftp.cs.brown.edu/pub/nlparser/
46
Algorithm 1 Interpret a sentence sequence
Require: Sentences T1, . . . , Tn
{ Interpret Sentences }
1: for all sentences Ti do
2: Generate parse trees {Pi}, and UCGs {Ui}
3: Generate candidate modes {Mi}
4: For each identifier j in Ti, generate candi-
date referents {Rij}
5: end for
{ Combine UCGs }
6: while there is time do
7: Get {(U1,M1, R1), . . . , (Un,Mn, Rn)} ?
a sequence of tuples (one tuple per sen-
tence)
8: Generate {UD}, a sequence of declara-
tive UCGs, by merging the declarative
UCGs in {(Ui,Mi, Ri)} as specified by
their identifier-referent pairs and modes
9: Generate {U I}, a sequence of imperative
UCGs, by merging each imperative UCG
in {(Ui,Mi, Ri)} with declarative UCGs
as specified by their identifier-referent pairs
and modes
10: Generate candidate ICG sequences {IIj } for
the sequence {U I}
11: Select the best sequence of ICGs {II?}
12: end while
tion based on Concept Graphs (Sowa, 1984) ?
one parse tree yielding one UCG (but several parse
trees may produce the same UCG). UCGs rep-
resent syntactic information, where the concepts
correspond to the words in the parent parse tree,
and the relations are derived from syntactic in-
formation in the parse tree and prepositions (Fig-
ure 1(a) illustrates UCGs UD and U I generated
from the sentences ?The mug is on the table. Clean
it.?).
Our algorithm requires sentence mode (declar-
ative, imperative or interrogative3), and resolved
references to determine how to combine the sen-
tences in a sequence. Sentence mode is obtained
using a classifier trained on part of our corpus
(Section 2.2). The probability distribution for the
referents of each identifier is obtained from the
corpus and from rules derived from (Lappin and
Leass, 1994; Ng et al, 2005) (Section 2.3).
At this point, for each sentence Ti in a sequence,
we have a list of UCGs, a list of modes, and lists
3Interrogatives are treated as imperatives at present, so in
the remainder of the paper we do not mention interrogatives.
clean0
mug03
table01
On
Patient
on
mug
DEF
DEF
table
I
1
{U       , R="the table"}1
clean0
table02
Patient
object
clean
it
IU
object
clean
DEFtable
clean
object
on
mug DEF
table DEF
DU
I
1
{U       , R="the mug"}2
I
1}1I{ I1}2I{
DECLARATIVE
   
   
The mug is on the table. Clean it.
IMPERATIVE
       (b) Merged UCGs       (c) Candidate ICGs(a) Declarative and
      imperative UCGs
Figure 1: Combining two sentences
of referents (one list for each identifier in the sen-
tence). In Step 7, Algorithm 1 generates a tu-
ple (Ui,Mi, Ri) for each sentence Ti by selecting
from these lists a UCG, a mode and a referent for
each identifier (yielding a list of identifier-referent
pairs). Each element in each (U,M,R) tuple is it-
eratively selected by traversing the appropriate list
in descending order of probability. For instance,
given sentences T1, T2, T3, the top UCG for T1 is
picked first, together with the top mode and the
top identifier-referent pairs for that sentence (like-
wise for T2 and T3); next the second-top UCG is
chosen for T1, but the other elements remain the
same; and so on.
Once the (U,M,R) tuples have been deter-
mined, the UCGs for the declarative sentences
are merged in the order they were given (Step 8).
This is done by first merging a pair of declara-
tive UCGs, then merging the resultant UCG with
the next declarative UCG, and so on. The idea is
that if the declarative sentences have co-referents,
then the information about these co-referents can
be combined into one representation. For exam-
ple, consider the sequence ?The mug is on the ta-
ble. It is blue. Find it. The mug is near the phone.
Bring it to me.? Some of the UCG sequences ob-
tained from the declarative sentences (first, second
and fourth) are:
{UD1 }1={mug(CLR blue)-
(on-table & near-phone)}
{UD1 }2={mug-(on-table(CLR blue) &
near-phone)}
{UD1 , UD2 }3={mug(CLR blue)-on-table,
mug-near-phone}.4
4The different notations are because colour (and size) are
properties of objects, while prepositions indicate relations.
47
The first two sequences contain one declarative
merged UCG, and the third contains two UCGs.
In Step 9, Algorithm 1 considers a UCG for
each imperative sentence in turn, and merges it
with declarative UCGs (which may have resulted
from a merger), as specified by the modes and
identifier-referent pairs of the sentences in ques-
tion. For example, consider the sentence sequence
?Find my mug. It is in my office. Bring it.? One of
the (U,M,R)-tuple sequences for this instruction
set is
{(find-obj-mug-owner-me, imperative, NIL),
(it1-in-office-owner-me, declarative, it1-mug),
(bring-obj-it2, imperative, it2-mug)}.
After merging the first two UCGs (imperative-
declarative), and then the second and third UCGs
(declarative-imperative), we obtain the imperative
UCG sequence{U I1,U I2 }:
U I1=find-obj-mug-(owner-me &
in-office-owner-me)
U I2=bring-obj-mug-(in-office-owner-me).
This process enables Scusi? to iteratively merge
ever-expanding UCGs with subsequent UCGs,
eventually yielding UCG sequences which contain
detailed UCGs that specify an action or object. A
limitation of this merging process is that the infor-
mation about the objects specified in an impera-
tive UCG is not aggregated with the information
about these objects in other imperative UCGs, and
this sometimes can cause the merged imperative
UCGs to be under-specified. This limitation will
be addressed in the immediate future.
After a sequence of imperative UCGs has been
generated, candidate Instantiated Concept Graphs
(ICGs) are proposed for each imperative UCG,
and the most probable ICG sequence is selected
(Steps 10?11 of Algorithm 1). We focus on im-
perative UCGs because they contain the actions
that the robot is required to perform; these actions
incorporate relevant information from declarative
UCGs. ICGs are generated by nominating dif-
ferent instantiated concepts and relations from the
system?s knowledge base as potential realizations
for each concept and relation in a UCG (Zukerman
et al, 2008); each UCG can generate many ICGs.
Since this paper focuses on the generation of UCG
sequences, the generation of ICGs will not be dis-
cussed further.
2.1 Merging UCGs
Given tuples (Ui,Mi, Ri) and (Uj ,Mj , Rj) where
j > i, pronouns and one-anaphora in Uj are re-
placed with their referent in Ui on the basis of the
set of identifier-referent pairs in Rj (if there is no
referent in Ui for an identifier in Uj , the identifier
is left untouched). Ui and Uj are then merged into
a UCG Um by first finding a node n that is com-
mon to Ui and Uj , and then copying the sub-tree of
Uj whose root is n into a copy of Ui. If more than
one node can be merged, the node (head noun) that
is highest in the Uj structure is used. If one UCG
is declarative and the other imperative, we swap
them if necessary, so that Ui is imperative and Uj
declarative.
For instance, given the sentences ?The mug is
on the table. Clean it.? in Figure 1, Step 4 of
Algorithm 1 produces the identifier-referent pairs
{(it, mug), (it, table)}, yielding two intermedi-
ate UCGs for the imperative sentence: (1) clean-
object-mug, and (2) clean-object-table. The first
UCG is merged with a UCG for the declarative
sentence using mug as root node, and the second
UCG is merged using table as root node. This
results in merged UCG sequences (of length 1)
corresponding to ?Clean the table? and ?Clean the
mug on the table? ({U I1 }1 and {U I1 }2 respectively
in Figure 1(b), which in turn produce ICG se-
quences {II1}1 and {II1}2 in Figure 1(c), among
others).
2.2 Determining modes
We use the MaxEnt classifier5 to determine the
mode of a sentence. The input features to the clas-
sifier (obtained from the highest probability parse
tree for this sentence) are: (1) top parse-tree node;
(2) position and type of the top level phrases under
the top parse-tree node, e.g., (0, NP), (1, VP), (2,
PP); (3) top phrases under the top parse-tree node
reduced to a regular expression, e.g., VP-NP+ to
represent, say, VP NP NP; (4) top VP head ? the
head word of the first top level VP; (5) top NP head
? the head word of the first top level NP; (6) first
three tokens in the sentence; and (7) last token in
the sentence. Using leave-one-out cross valida-
tion, this classifier has an accuracy of 97.8% on
the test data ? a 30% improvement over the ma-
jority class (imperative) baseline.
2.3 Resolving references
Scusi? handles pronouns, one-anaphora and NP
identifiers (e.g., ?the book?). At present, we con-
sider only precise matches between NP identifiers
5http://homepages.inf.ed.ac.uk/
s0450736/maxent_toolkit.html
48
and referents, e.g., ?the cup? does not match ?the
dish?. In the future, we will incorporate similar-
ity scores based on WordNet, e.g., Leacock and
Chodorow?s (1998) scores for approximate lexical
matches; such matches occurred in 4% of our cor-
pus (Section 4).
To reduce the complexity of reference reso-
lution across a sequence of sentences, and the
amount of data required to reliably estimate prob-
abilities (Section 3), we separate our problem into
two parts: (1) identifying the sentence being re-
ferred to, and (2) determining the referent within
that sentence.
Identifying a sentence. Most referents in our
corpus appear in the current, previous or first sen-
tence in a sequence, with a few referents appear-
ing in other sentences (Section 4). Hence, we
have chosen the sentence classes {current, previ-
ous, first, other}. The probability of referring to
a sentence of a particular class from a sentence
in position i is estimated from our corpus, where
i = 1, . . . , 5, > 5 (there are only 13 sequences
with more than 5 sentences). We estimate this dis-
tribution for each leave-one-out cross-validation
fold in our evaluation (Section 4).
Determining a referent. We use heuristics
based on those described in (Lappin and Leass,
1994) to classify pronouns (an example of a non-
pronoun usage is ?It is ModalAdjective that S?),
and heuristics based on the results obtained in (Ng
et al, 2005) to classify one-anaphora (an exam-
ple of a high-performing feature pattern is ?one as
head-noun with NN or CD as Part-of-speech and
no attached of PP?). If a term is classified as a pro-
noun or one-anaphor, then a list of potential ref-
erents is constructed using the head nouns in the
target sentence. We use the values in (Lappin and
Leass, 1994) to assign a score to each anaphor-
referent pair according to the grammatical role of
the referent in the target UCG (obtained from the
highest probability parse tree that is a parent of this
UCG). These scores are then converted to proba-
bilities using a linear mapping function.
3 Estimating the Probability of a Merged
Interpretation
We now present our formulation for estimating the
probability of a sequence of UCGs, which sup-
ports the selection of the most probable sequence.
One sentence. The probability of a UCG gener-
ated from a sentence T is estimated as described
in (Zukerman et al, 2008), resulting in
Pr(U |T ) ? ?P Pr(P |T )?Pr(U |P ) (1)
where T , P and U denote text, parse tree and UCG
respectively. The summation is taken over all pos-
sible parse trees from the text to the UCG, be-
cause a UCG can have more than one ancestor. As
mentioned above, the parser returns an estimate of
Pr(P |T ); and Pr(U |P ) = 1, since the process of
generating a UCG from a parse tree is determinis-
tic.
A sentence sequence. The probability of an in-
terpretation of a sequence of sentences T1, . . . , Tn
is
Pr(U1, . . . , Um|T1, . . . , Tn) =
Pr(U1, . . .,Un,M1, . . .,Mn,R1, . . .,Rn|T1, . . .,Tn)
where m is the number of UCGs in a merged se-
quence.
By making judicious conditional independence
assumptions, and incorporating parse trees into the
formulation, we obtain
Pr(U1, . . . , Um|T1, . . . , Tn) =
n
?
i=1
Pr(Ui|Ti)?Pr(Mi|Pi, Ti)?Pr(Ri|P1, . . . , Pi)
This formulation is independent of the num-
ber of UCGs in a merged sequence generated
by Algorithm 1, thereby supporting the compari-
son of UCG sequences of different lengths (pro-
duced when different numbers of mergers are per-
formed).
Pr(Ui|Ti) is calculated using Equation 1, and
Pr(Mi|Pi, Ti) is obtained as described in Sec-
tion 2.2 (recall that the input features to the clas-
sifier depend on the parse tree and the sentence).
In principle, Pr(Mi|Pi, Ti) and Pr(Ri|P1, . . . , Pi)
could be obtained by summing over all parse trees,
as done in Equation 1. However, at present we use
the highest-probability parse tree to simplify our
calculations.
To estimate Pr(Ri|P1, . . . , Pi) we assume con-
ditional independence between the identifiers in a
sentence, yielding
Pr(Ri|P1, . . . , Pi) =
ki
?
j=1
Pr(Rij |P1, . . . , Pi)
where ki is the number of identifiers in sentence
i, and Rij is the referent for identifier j in sen-
tence i. As mentioned in Section 2.3, this factor is
49
separated into determining a sentence, and deter-
mining a referent in that sentence. We also include
in our formulation the Type of the identifier (pro-
noun, one-anaphor or NP) and sentence position i,
yielding
Pr(Rij |P1, . . . , Pi) =
Pr(Rij ref NPa in sent b, Type(Rij)|i, P1, . . . , Pi)
After additional conditionalization we obtain
Pr(Rij |P1, . . . , Pi) =
Pr(Rij ref NPa|Rij ref sent b,Type(Rij),Pi,Pb)?
Pr(Rij ref sent b|Type(Rij), i)?Pr(Type(Rij)|Pi)
As seen in Section 2.3, Pr(Type(Rij)|Pi) and
Pr(Rij ref NPa|Rij ref sent b,Type(Rij),Pi,Pb)
are estimated in a rule-based manner, and
Pr(Rij ref sent b|Type(Rij), i) is estimated from
the corpus (recall that we distinguish between
sentence classes, rather than specific sentences).
4 Evaluation
We first describe our experimental set-up, fol-
lowed by our results.
4.1 Experimental set-up
We conducted a web-based survey to collect a cor-
pus comprising multi-sentence requests. To this
effect, we presented participants with a scenario
where they are in a meeting room, and they ask
a robot to fetch something from their office. The
idea is that if people cannot see a scene, their in-
structions will be more segmented than if they can
view the scene. The participants were free to de-
cide which object to fetch, and what was in the
office. There were no restrictions on vocabulary
or grammatical form for the requests.
We collected 115 sets of instructions mostly
from different participants (a few people did the
survey more than once).6 The sentence sequences
in our corpus contain between 1 and 9 sentences,
with 74% of the sequences comprising 1 to 3 sen-
tences. Many of the sentences had grammatical
requirements which exceeded the capabilities of
our system. To be able to use these instruction
sets in our evaluation, we made systematic manual
changes to produce sentences that meet our sys-
tem?s grammatical restrictions (in the future, we
6We acknowledge the modest size of our corpus compared
to that of some publicly available corpora, e.g., ATIS. How-
ever, we must generate our own corpus since our task differs
in nature from the tasks where these large corpora are used.
SMALL OFFICEMAIN OFFICE
PRINTER TABLE
CHAIR
BO
O
K
C
A
SE
WINDOW
SIDE
DESK
FILING
CABINET
GLASS
MAIN DESK
CABINET
BO
O
K
C
A
SE
JOE?S DESK
Figure 2: Our virtual environment (top view)
will relax these restrictions, as required by a de-
ployable system). Below are the main types of
changes we made.
? Indirect Speech Acts in the form of questions
were changed to imperatives. For instance,
?Can you get my tea?? was changed to ?Get
my tea?.
? Conjoined verb phrases or sentences were sep-
arated into individual sentences.
? Composite verbs were simplified, e.g., ?I think
I left it on? was changed to ?it is on?, and out-
of-vocabulary composite nouns were replaced
by simple nouns or adjectives, e.g., ?the diary
is A4 size? to ?the diary is big?.
? Conditional sentences were removed.
Table 1 shows two original texts compared with
the corresponding modified texts (the changed
portions in the originals have been italicized).
Our evaluation consists of two experiments:
(1) ICGs for sentence pairs, and (2) UCGs for sen-
tence sequences.
Experiment 1. We extracted 106 sentence pairs
from our corpus ? each pair containing one
declarative and one imperative sentence. To eval-
uate the ICGs, we constructed a virtual environ-
ment comprising a main office and a small office
(Figure 2). Furniture and objects were placed in
a manner compatible with what was mentioned in
the requests in our corpus; distractors were also
placed in the virtual space. In total, our environ-
ment contains 183 instantiated concepts (109 of-
fice and household objects, 43 actions and 31 re-
lations). The (x, y, z) coordinates, colour and di-
mensions of these objects were stored in a knowl-
edge base. Since we have two sentences and their
mode is known, no corpus-based information is
used for this experiment, and hence no training is
required.
50
Original Get my book ?The Wizard of Oz? from my office. It?s green and yellow. It has a picture
of a dog and a girl on it. It?s in my desk drawer on the right side of my desk, the second
drawer down. If it?s not there, it?s somewhere on my shelves that are on the left side of my
office as you face the window.
Modified Get my book from my office. It?s green. It?s in my drawer on the right of my desk.
Original DORIS, I left my mug in my office and I want a coffee. Can you go into my office and get
my mug. It is on top of the cabinet that is on the left side of my desk.
Modified My mug is in my office. Go into my office. Get my mug. It is on top of the cabinet on the
left of my desk.
Table 1: Original and modified text
Experiment 2. Since UCGs contain only syn-
tactic information, no additional setup was re-
quired. However, for this experiment we need to
train our mode classifier (Section 2.2), and esti-
mate the probability distribution of referring to a
particular sentence in a sequence (Section 2.3).
Owing to the small size of our corpus, we use
leave-one-out cross validation.
For both experiments, Scusi? was set to gener-
ate up to 300 sub-interpretations (including parse
trees, UCGs and ICGs) for each sentence in the
test-set; on average, it took less than 1 second
to go from a text to a UCG. An interpretation
was deemed successful if it correctly represented
the speaker?s intention, which was represented by
an imperative Gold ICG for the first experiment,
and a sequence of imperative Gold UCGs for the
second experiment. These Gold interpretations
were manually constructed by the authors through
consensus-based annotation (Ang et al, 2002). As
mentioned in Section 2, we evaluated only imper-
ative ICGs and UCGs, as they contain the actions
the robot is expected to perform.
4.2 Results
Table 2 summarizes our results. Column 1 shows
the type of outcome being evaluated (ICGs in Ex-
periment 1, and UCG sequences and individual
UCGs in Experiment 2). The next two columns
display how many sentences had Gold interpreta-
tions whose probability was among the top-1 and
top-3 probabilities. The average rank of the Gold
interpretation appears in Column 4 (?not found?
Gold interpretations are excluded from this rank).
The rank of an interpretation is its position in a
list sorted in descending order of probability (start-
ing from position 0), such that all equiprobable in-
terpretations have the same position. Columns 5
and 6 respectively show the median and 75%-ile
rank of the Gold interpretation. The number of
Gold interpretations that were not found appears in
Column 7, and the total number of requests/UCGs
is shown in the last column.
Experiment 1. As seen in the first row of Ta-
ble 2, the Gold ICG was top ranked in 75.5% of
the cases, and top-3 ranked in 85.8%. The aver-
age rank of 2.17 is mainly due to 7 outliers, which
together with the ?not-found? Gold ICG, are due
to PP-attachment issues, e.g., for the sentence pair
?Fetch my phone from my desk. It is near the key-
board.?, the top parses and resultant UCGs have
?near the keyboard? attached to ?the desk? (in-
stead of ?the phone?). Nonetheless, the top-ranked
interpretation correctly identified the intended ob-
ject and action in 5 of these 7 cases. Median
and 75%-ile results confirm that most of the Gold
ICGs are top ranked.
Experiment 2. As seen in the second row of Ta-
ble 2, the Gold UCG sequence was top ranked for
51.3% of the requests, and top-3 ranked for 53.0%
of the requests. The third row shows that 62.4%
of the individual Gold UCGs were top-ranked,
and 65.4% were top-3 ranked. This indicates that
when Scusi? cannot fully interpret a request, it
can often generate a partially correct interpreta-
tion. As for Experiment 1, the average rank of
3.14 for the Gold UCG sequences is due to out-
liers, several of which were ranked above 30. The
median and 75%-ile results show that when Scusi?
generates the correct interpretation, it tends to be
highly ranked.
Unlike Experiment 1, in Experiment 2 there is
little difference between the top-1 and top-3 re-
sults. A possible explanation is that in Experi-
ment 1, the top-ranked UCG may yield several
probable ICGs, such that the Gold ICG is not top
ranked ? a phenomenon that is not observable at
the UCG stage.
Even though Experiment 2 reaches only the
51
Table 2: Scusi??s interpretation performance
# Gold interps. with prob. in Average Median 75%-ile Not Total
top 1 top 3 rank rank rank found #
ICGs 80 (75.5%) 91 (85.8%) 2.17 0 0 1 (0.9%) 106 reqs.
UCG seqs. 59 (51.3%) 61 (53.0%) 3.14 0 1 36 (31.3%) 115 reqs.
UCGs 146 (62.4%) 153 (65.4%) NA NA NA 55 (23.5%) 234 UCGs
UCG stage, Scusi??s performance for this exper-
iment is worse than for Experiment 1, as there
are more grounds for uncertainty. Table 2 shows
that 31.3% of Gold UCG sequences and 23.5% of
Gold UCGs were not found. Most of these cases
(as well as the poorly ranked UCG sequences
and UCGs) were due to (1) imperatives with
object specifications (19 sequences), (2) wrong
anaphora resolution (6 sequences), and (3) wrong
PP-attachment (6 sequences). In the near future,
we will refine the merging process to address the
first problem. The second problem occurs mainly
when there are multiple anaphoric references in a
sequence. We propose to include this factor in our
estimation of the probability of referring to a sen-
tence. We intend to alleviate the PP-attachment
problem, which also occurred in Experiment 1,
by interleaving semantic and pragmatic interpreta-
tion of prepositional phrases as done in (Brick and
Scheutz, 2007). The expectation is that this will
improve the rank of candidates which are pragmat-
ically more plausible.
5 Related Research
This research extends our mechanism for inter-
preting stand-alone utterances (Zukerman et al,
2008) to the interpretation of sentence sequences.
Our approach may be viewed as an information
state approach (Larsson and Traum, 2000; Becker
et al, 2006), in the sense that sentences may up-
date different informational aspects of other sen-
tences, without requiring a particular ?legal? set of
dialogue acts. However, unlike these information
state approaches, ours is probabilistic.
Several researchers have investigated proba-
bilistic approaches to the interpretation of spo-
ken utterances in dialogue systems, e.g., (Pfleger
et al, 2003; Higashinaka et al, 2003; He and
Young, 2003; Gorniak and Roy, 2005; Hu?wel and
Wrede, 2006). Pfleger et al (2003) and Hu?wel
and Wrede (2006) employ modality fusion to com-
bine hypotheses from different analyzers (linguis-
tic, visual and gesture), and apply a scoring mech-
anism to rank the resultant hypotheses. They dis-
ambiguate referring expressions by choosing the
first object that satisfies a ?differentiation crite-
rion?, hence their system does not handle situa-
tions where more than one object satisfies this cri-
terion. He and Young (2003) and Gorniak and
Roy (2005) use Hidden Markov Models for the
ASR stage. However, these systems do not han-
dle utterance sequences. Like Scusi?, the system
developed by Higashinaka et al (2003) maintains
multiple interpretations, but with respect to dia-
logue acts, rather than the propositional content of
sentences. All the above systems employ seman-
tic grammars, while Scusi? uses generic, statisti-
cal tools, and incorporates semantic- and domain-
related information only in the final stage of the
interpretation process. This approach is supported
by the findings reported in (Knight et al, 2001) for
relatively unconstrained utterances by users unfa-
miliar with the system, such as those expected by
DORIS.
Our mechanism is also well suited for process-
ing replies to clarification questions (Horvitz and
Paek, 2000; Bohus and Rudnicky, 2005), since a
reply can be considered an additional sentence to
be incorporated into top-ranked UCG sequences.
Further, our probabilistic output can be used by a
utility-based dialogue manager (Horvitz and Paek,
2000).
6 Conclusion
We have extended Scusi?, our spoken language
interpretation system, to interpret sentence se-
quences. Specifically, we have offered a procedure
that combines the interpretations of the sentences
in a sequence, and presented a formalism for es-
timating the probability of the merged interpre-
tation. This formalism supports the comparison
of interpretations comprising different numbers of
UCGs obtained from different mergers.
Our empirical evaluation shows that Scusi? per-
forms well for textual input corresponding to
(modified) sentence pairs. However, we still need
52
to address some issues pertaining to the integra-
tion of UCGs for sentence sequences of arbitrary
length. Thereafter, we propose to investigate the
influence of speech recognition performance on
Scusi??s performance. In the future, we intend to
expand Scusi??s grammatical capabilities.
Acknowledgments
This research was supported in part by grant
DP0878195 from the Australian Research Coun-
cil.
References
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and
A. Stolcke. 2002. Prosody-based automatic de-
tection of annoyance and frustration in human-
computer dialog. In ICSLP?2002 ? Proceedings of
the 7th International Conference on Spoken Lan-
guage Processing, pages 2037?2040, Denver, Col-
orado.
T. Becker, P. Poller, J. Schehl, N. Blaylock, C. Ger-
stenberger, and I. Kruijff-Korbayova?. 2006. The
SAMMIE system: Multimodal in-car dialogue. In
Proceedings of the COLING/ACL 2006 Interactive
Presentation Sessions, pages 57?60, Sydney, Aus-
tralia.
D. Bohus and A. Rudnicky. 2005. Constructing accu-
rate beliefs in spoken dialog systems. In ASRU?05
? Proceedings of the IEEE Workshop on Automatic
Speech Recognition and Understanding, pages 272?
277, San Juan, Puerto Rico.
T. Brick and M. Scheutz. 2007. Incremental natural
language processing for HRI. In HRI 2007 ? Pro-
ceedings of the 2nd ACM/IEEE International Con-
ference on Human-Robot Interaction, pages 263?
270, Washington, D.C.
P. Gorniak and D. Roy. 2005. Probabilistic grounding
of situated speech using plan recognition and refer-
ence resolution. In ICMI?05 ? Proceedings of the
7th International Conference on Multimodal Inter-
faces, pages 138?143, Trento, Italy.
Y. He and S. Young. 2003. A data-driven spo-
ken language understanding system. In ASRU?03
? Proceedings of the IEEE Workshop on Automatic
Speech Recognition and Understanding, pages 583?
588, St. Thomas, US Virgin Islands.
R. Higashinaka, M. Nakano, and K. Aikawa. 2003.
Corpus-Based discourse understanding in spoken di-
alogue systems. In ACL-2003 ? Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 240?247, Sapporo,
Japan.
E. Horvitz and T. Paek. 2000. DeepListener: Har-
nessing expected utility to guide clarification dialog
in spoken language systems. In ICSLP?2000 ? Pro-
ceedings of the 6th International Conference on Spo-
ken Language Processing, pages 229?229, Beijing,
China.
S. Hu?wel and B. Wrede. 2006. Spontaneous speech
understanding for robust multi-modal human-robot
communication. In Proceedings of the COL-
ING/ACL Main Conference Poster Sessions, pages
391?398, Sydney, Australia.
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, and I. Lewin. 2001. Comparing grammar-based
and robust approaches to speech understanding: A
case study. In Proceedings of Eurospeech 2001,
pages 1779?1782, Aalborg, Denmark.
S. Lappin and H.J. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational
Linguistics, 20:535?561.
S. Larsson and D. Traum. 2000. Information state
and dialogue management in the TRINDI dialogue
move engine toolkit. Natural Language Engineer-
ing, 6:323?340.
C. Leacock and M. Chodorow. 1998. Combining lo-
cal context and WordNet similarity for word sense
identification. In C. Fellbaum, editor, WordNet: An
Electronic Lexical Database, pages 265?285. MIT
Press.
H.T. Ng, Y. Zhou, R. Dale, and M. Gardiner. 2005.
A machine learning approach to identification and
resolution of one-anaphora. In IJCAI-05 ? Proceed-
ings of the 19th International Joint Conference on
Artificial Intelligence, pages 1105?1110, Edinburgh,
Scotland.
N. Pfleger, R. Engel, and J. Alexandersson. 2003. Ro-
bust multimodal discourse processing. In Proceed-
ings of the 7th Workshop on the Semantics and Prag-
matics of Dialogue, pages 107?114, Saarbru?cken,
Germany.
J.F. Sowa. 1984. Conceptual Structures: Information
Processing in Mind and Machine. Addison-Wesley,
Reading, MA.
I. Zukerman, E. Makalic, M. Niemann, and S. George.
2008. A probabilistic approach to the interpreta-
tion of spoken utterances. In PRICAI 2008 ? Pro-
ceedings of the 10th Pacific Rim International Con-
ference on Artificial Intelligence, pages 581?592,
Hanoi, Vietnam.
G. Zweig, D. Bohus, X. Li, and P. Nguyen. 2008.
Structured models for joint decoding of repeated ut-
terances. In Proceedings of Interspeech 2008, pages
1157?1160, Brisbane, Australia.
53
Semantic Role Labelling of Prepositional Phrases
Patrick Ye1 and Timothy Baldwin1,2
1 Department of Computer Science and Software Engineering,
University of Melbourne, VIC 3010, Australia
2 NICTA Victoria Laboratories,
University of Melbourne, VIC 3010, Australia
{jingy, tim}@cs.mu.oz.au
Abstract. We propose a method for labelling prepositional phrases ac-
cording to two different semantic role classifications, as contained in the
Penn treebank and the CoNLL 2004 Semantic Role Labelling data set.
Our results illustrate the difficulties in determining preposition seman-
tics, but also demonstrate the potential for PP semantic role labelling to
improve the performance of a holistic semantic role labelling system.
1 Introduction
Prepositional phrases (PPs) are both common and semantically varied in open
English text. Learning the semantics of prepositions is not a trivial task in gen-
eral. It may seem that the semantics of a given PP can be predicted with rea-
sonable reliability independent of its context. However, it is actually common for
prepositions or even identical PPs to exhibit a wide range of semantic fuctions
in different open English contexts. For example, consider the PP to the car : this
PP will generally occur as a directional adjunct (e.g. walk to the car), but it can
also occur as an object to the verb (e.g. refer to the car) or contrastive argu-
ment (e.g. the default mode of transport has shifted from the train to the car); to
further complicate the situation, in key to the car it functions as a complement
to the N-bar key. Based on this observation, we may consider the possibility of
constructing a semantic tagger specifically for PPs, which uses the surrounding
context of the PP to arrive at a semantic analysis. It is this task of PP semantic
role labelling that we target in this paper.
A PP semantic role labeller would allow us to take a document and identify
all adjunct PPs with their semantics. We would expect this to include a large
portion of locative and temporal expressions, e.g., in the document, providing
valuable data for tasks such as information extraction and question answering.
Indeed our initial foray into PP semantic role labelling relates to an interest in
geospatial and temporal analysis, and the realisation of the importance of PPs
in identifying and classifying spatial and temporal references.
The contributions of this paper are to propose a method for PP semantic role
labelling, and evaluate its performance over both the Penn treebank (including
comparative evaluation with previous work) and also the data from the CoNLL
Semantic Role Labelling shared task. As part of this process, we identify the
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 779?791, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
780 P. Ye and T. Baldwin
Fig. 1. An example of the preposition semantic roles in Penn Teebank
level of complementarity of a dedicated PP semantic role labeller with a conven-
tional holistic semantic role labeller, suggesting PP semantic role labelling as a
potential avenue for boosting the performance of existing systems.
2 Preposition Semantic Role Disambiguation in Penn
Treebank
Significant numbers of prepositional phrases (PPs) in the Penn treebank [1] are
tagged with their semantic role relative to the governing verb. For example,
Figure 1, shows a fragment of the parse tree for the sentence [Japan?s reserves
of gold, convertible foreign currencies, and special drawing rights] fell by a hefty
$1.82 billion in October to $84.29 billion [the Finance Ministry said], in which
the three PPs governed by the verb fell are tagged as, respectively: PP-EXT
(?extend?), meaning how much of the reserve fell; PP-TMP (?temporal?), meaning
when the reserve fell; and PP-DIR (?direction?), meaning the direction of the fall.
According to our analysis, there are 143 preposition semantic roles in the tree-
bank. However, many of these semantic roles are very similar to one another;
for example, the following semantic roles were found in the treebank: PP-LOC,
PP-LOC-1, PP-LOC-2, PP-LOC-3, PP-LOC-4, PP-LOC-5, PP-LOC-CLR, PP-
LOC-CLR-2, PP-LOC-CLR-TPC-1. Inspection of the data revealed no systematic
semantic differences between these PP types. Indeed, for most PPs, it was im-
possible to distinguish the subtypes of a given superclass (e.g. PP-LOC in our
example). We therefore decided to collapse the PP semantic roles based on their
first semantic feature. For example, all semantic roles that start with PP-LOC
are collapsed to the single class PP-LOC. Table 1 shows the distribution of the
collapsed preposition semantic roles.
[2] describe a system1 for disambiguating the semantic roles of prepositions in
the Penn treebank according to 7 basic semantic classes. In their system, O?Hara
and Weibe used a decision tree classifier, and the following types of features:
? POS tags of surrounding tokens: The POS tags of the tokens before and
after the target preposition within a predefined window size. In O?Hara and
Wiebe?s work, this window size is 2.
1 This system was trained with WEKA?s J48 decision tree implementation.
Semantic Role Labelling of Prepositional Phrases 781
Table 1. Penn treebank semantic role distribution (top-9 roles)
Semantic Role Count Frequency Meaning
PP-LOC 21106 38.2 Locative
PP-TMP 12561 22.7 Temporal
?Closely related? (somewhere between
PP-CLR 11729 21.2
an argument and an adjunct)
PP-DIR 3546 6.4 Direction (from/to X)
PP-MNR 1839 3.3 Manner (incl. instrumentals)
PP-PRD 1819 3.3 Predicate (non-VP)
PP-PRP 1182 2.1 Purpose or reason
PP-CD 654 1.2 Cardinal (numeric adjunct)
PP-PUT 296 0.5 Locative complement of put
? POS tag of the target preposition
? The target preposition
? Word collocation: All the words in the same sentence as the target prepo-
sition; each word is treated as a binary feature.
? Hypernym collocation: The WordNet hypernyms [3] of the open class
words before and after the target preposition within a predefined window
size (set to 5 words); each hypernym is treated as a binary feature.
O?Hara and Wiebe?s system also performs the following pre-classification
filtering on the collocation features:
? Frequency constraint: f(coll) > 1, where coll is either a word from the
word collocation or a hypernym from the hypernym collocation
? Conditional independence threshold: p(c|coll)?p(c)p(c) >= 0.2, where c is a
particular semantic role and coll is from the word collocation or a hypernym
from the hypernym collocation
We began our research by replicating O?Hara and Wiebe?s method and seek-
ing ways to improve it. Our initial investigation revealed that there were around
44000 word and hypernym collocation features even after the frequency con-
straint filter and the conditional independence filter have been applied. We did
not believe all these collocation features were necessary, and we deployed an ad-
ditional ranking-based filtering mechanism over the collocation features to only
select collocation features which occur in the top N frequency bins. Algorithm 1
shows the details of this filtering mechanism.
This ranking-based filtering mechanism allows us to select collocation feature
sets of differing size, and in doing so not only improve the training and tagging
Algorithm 1. Ranking based filtering algorithm
1. Let s be the list that contains the frequency of all the collocation features
2. Sort s in descending order
3. minFrequency = s[N ]
4. Discard all features whose frequency is less than minFrequency
782 P. Ye and T. Baldwin
Table 2. Penn treebank preposition semantic role disambiguation results
Accuracy (%)
Ranking Classifier 1 Classifier 2
10 74.75 81.28
20 76.53 83.52
50 79.21 86.34
100 80.13 87.02
300 81.32 87.62
1000 82.34 87.71
all 82.76 87.45
O?Hara & Wiebe N/A 85.8
speed of the preposition semantic role labelling, but also observe how the number
of collocation features affects the performance of the PP semantic role labeller
and which collocation features are more important.
2.1 Results
Since some of the preposition semantic roles in the treebank have extremely low
frequencies, we decided to build our first classifier using only the top 9 seman-
tic roles, as detailed in Table 1. We also noticed that the semantic roles PP-CLR,
PP-CD and PP-PUT were excluded from O?Hara?s system which only used PP-BNF,
PP-EXT, PP-MNR, PP-TMP, PP-DIR, PP-LOC and PP-PRP, therefore we built a sec-
ond classifier using only the semantic roles used by O?Hara?s system2. The two
classifiers were trained with a maximum entropy [4] learner3.
Table 2 shows the results of our classifier under stratified 10-fold cross val-
idation4 using different parameters for the rank-based filter. We also list the
accuracy reported by O?Hara and Wiebe for comparison.
The results show that the performance of the classifier increases as we add
more collocation features. However, this increase is not linear, and the improve-
ment of performance is only marginal when the number collocation features is
greater than 100. It also can be observed that there is a consistent performance
difference between classifiers 1 and 2, which may suggest that PP-CLR may be
harder to distinguish from other semantic roles. This is not totally surprising
given the relatively vague definition of the semantics of PP-CLR. We return to
analyse these results in greater depth in Section 4.
3 Preposition Semantic Role Labelling over the CoNLL
2004 Dataset
Having built a classifier which has reasonable performance on the task of tree-
bank preposition semantic role disambiguation, we decided to investigate
2 PP-BNF with only 47 counts was not used by the second classifier.
3 http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
4 O?Hara?s system was also evaluated using stratified 10-fold cross validation.
Semantic Role Labelling of Prepositional Phrases 783
whether we could use the same feature set to perform PP semantic role labelling
over alternate systems of PP classification. We chose the 2004 CoNLL Semantic
Role Labelling (SRL) dataset [5] because it contained a wide range of semantic
classes of PPs, in part analogous to the Penn treebank data, and also because
we wished to couple our method with a holistic SRL system to demonstrate the
ability of PP semantic role labelling to enhance overall system performance.
Since the focus of the CoNLL data is on SRL relative to a set of pre-
determined verbs for each sentence input,5 our primary objective is to inves-
tigate whether the performance of SRL systems in general can be improved in
any way by an independent preposition SRL system. We achieve this by embed-
ding our PP classification method within an existing holistic SRL system?that
is a system which attempts to tag all semantic role types in the CoNLL 2004
data?through the following three steps:
1. Perform SRL on each preposition in the CoNLL dataset;
2. Merge the output of the preposition SRL with the output of a given verb
SRL system over the same dataset;
3. Perform standard CoNLL SRL evaluation over the merged output.
The details of preposition SRL and combination with the output of a holistic
SRL system are discussed below.
3.1 Breakdown of the Preposition Semantic Role Labelling Problem
Preposition semantic role labelling over the CoNLL dataset is considerably more
complicated than the task of disambiguating preposition semantic roles in the
Penn treebank. There are three separate subtasks which are required to perform
preposition SRL:
1. PP Attachment: determining which verb to attach each preposition to.
2. Preposition Semantic Role Disambiguation
3. Argument Segmentation: determining the boundaries of the semantic
roles.
The three subtasks are not totally independent of each other, as we demon-
strate in the results section, and improved performance over one of the subtasks
does not necessarily correlate with an improvement in the final results.
3.2 PP Attachment Classification
PP attachment (PPA) classification is the first step of preposition semantic role
labelling and involves determining the verb attachment site for a given prepo-
sition, i.e. which of the pre-identified verbs in the sentence the preposition is
5 Note that the CoNLL 2004 data identifies certain verbs as having argument struc-
ture, and that the semantic role annotation is relative to these verbs only. This is
often not the sum total of all verbs in a given sentence: the verbs in relative clauses,
e.g., tend not to be identified as having argument structure.
784 P. Ye and T. Baldwin
governed by. Normally, this task would be performed by a parser. However, since
the CoNLL dataset contains no parsing information6 and we did not want to use
any resources not explicitly provided in the CoNLL data, we had to construct a
PPA classifier to specifically perform this task.
This classifier uses the following features, all of which are derived from infor-
mation provided in the CoNLL data:
? POS tags of surrounding tokens: The POS tags of the tokens before and
after the target preposition within a window size of 2 tokens ([?2, 2]).
? POS tag of the target preposition
? The target preposition
? Verbs and their relative position (VerbRelPos): All the (pre-
identified) verbs in the same sentence as the target preposition and their
relative positions to the preposition are extracted as features. Each (verb,
relative position) tuple is treated as a binary feature. The relative positions
are determined in a way such that the 1st verb before the preposition will
be given the position ?1, the 2nd verb before the preposition will be given
the position ?2, and so on.
? The type of the clause containing the target preposition
? Neighbouring chunk type: The types (NP, PP, VP, etc.) of chunks before
and after the target preposition within a window of 3 chunks.
? Word collocation (WordColl): All the open class words in the phrases
before and after the target preposition within a predefined window of 3
chunks.
? Hypernym collocation (HyperColl): All the hypernyms from the open
class words in the phrases before and after the target preposition within a
predefined window of 3 chunks.
? Named Entity collocation NEColl: All the named entity information
from the phrases before and after the target preposition within a predefined
window of 3 chunks.
The PPA classifier outputs the relative position of the governing verb to the
target preposition, or None if the preposition does not have a semantic role.
We trained the PPA classifier over the CoNLL 2004 training set, and tested it
on the testing set. Table 3 shows the distribution of the classes in the testing set.
The same maximum entropy learner used in the treebank SRL task was used
to train the PPA classifier. The accuracy of this classifier on the CoNLL 2004
testing set is 78.99%.
3.3 Preposition Semantic Role Disambiguation
For the task of preposition semantic role disambiguation (SRD), we constructed
a classifier using the same features as the PPA classifier, with the following
differences:
6 The CoNLL 2005 SRL data does contain parse trees for the sentences, possibly
obviating the need for independent verb attachment classification.
Semantic Role Labelling of Prepositional Phrases 785
Table 3. PPA class distribution
PPA Count Frequency
None 3005 60.71
-1 1454 29.37
1 411 8.30
-2 40 0.81
2 29 0.59
3 8 0.16
-3 2 0.04
-6 1 0.02
Table 4. CoNLL 2004 semantic role distribution in the CoNLL 2004 test dataset(top-
14 roles)
Semantic Role Count Frequency Meaning
A1 424 21.79 Argument 1
A2 355 18.24 Argument 2
AM-TMP 299 15.36 Temporal adjunct
AM-LOC 188 9.66 Locative adjunct
A0 183 9.40 Argument 0
AM-MNR 125 6.42 Manner adjunct
A3 106 5.45 Argument 3
AM-ADV 71 3.65 General-purpose adjunct
A4 44 2.26 Argument 4
AM-CAU 40 2.06 Causal adjunct
AM-PNC 32 1.64 Purpose adjunct
AM-DIS 32 1.64 Discourse marker
AM-DIR 19 0.97 Directional adjunct
AM-EXT 7 0.36 Extent adjunct
1. The window size for the POS tags of surrounding tokens is 5 tokens.
2. The window sizes for the WordColl, the HyperColl and the NeColl fea-
tures are set to include the entire sentence.
We trained the SRD classifier once again on the CoNLL 2004 training set,
and tested it on the testing set. Table 4 shows the distribution of the classes in
the testing set.
We used the same maximum entropy leaner as for the PPA classifier to train
the SRD classifier. The accuracy of the SRD classifier on the CoNLL 2004 testing
set is 58.68%.
3.4 Argument Segmentation
In order to determine the extent of each NP selected for by a given preposition
(i.e. the span of words contained in the NP), we use a simple regular expression
over the chunk parser analysis of the sentence provided in the CoNLL 2004 data,
786 P. Ye and T. Baldwin
namely: PP NP+. We additionally experimented with a robust statistical parser
[6] to determine PP extent, but found that the regular expression-based method
performed equally well or marginally better, without requiring any resources
external to the original task data.
We make no attempt to perform separate evaluation of this particular subtask
because without the semantic role information, no direct comparison can be
made with the CoNLL data.
3.5 Combining the Output of the Subtasks
Once we have identified the association between verbs and prepositions, and dis-
ambiguated the semantic roles of the prepositions, we can begin the process of cre-
ating the final output of the preposition semantic role labelling system. This takes
place by identifying the data column corresponding to the verb governing each
classified PP in the CoNLL data format (as determined by the PPA classifier),
and recording the semantic role of that PP (as determined by the SRD classifier)
over the full extent of the PP (as determined by the segmentation classifier).
3.6 Merging the Output of Preposition SRL and Verb SRL
Once we have generated the output of the preposition SRL system, we can
proceed to the final stage where the semantic roles of the prepositions are merged
with the semantic roles of an existing holistic SRL system.
It is possible, and indeed likely, that the semantic roles produced by the two
systems will conflict in terms of overlap in the extent of labelled constituents
and/or the semantic role labelling of constituents. To address any such conflicts,
we designed three merging strategies to identify the right balance between the
outputs of the two component systems:
S1 When a conflict is encountered, only use the semantic role information from
the holistic SRL system.
S2 When a conflict is encountered, if the start positions of the semantic role
are the same for both SRL systems, then replace the semantic role of the
holistic SRL system with that of the preposition SRL system, but keep the
holistic SRL system?s boundary end.
S3 When a conflict is encountered, only use the semantic role information from
the preposition SRL system.
3.7 Results
To evaluate the performance of our preposition SRL system, we combined its
outputs with the 3 top-performing holistic SRL systems from the CoNLL 2004
SRL shared task.7 The three systems are [7], [8] and [9]. Furthermore, in order
to establish the upper bound of the improvement of preposition SRL on verb
7 Using the test data outputs of the three systems made available at
http://www.lsi.upc.edu/?srlconll/st04/st04.html.
Semantic Role Labelling of Prepositional Phrases 787
Table 5. Preposition SRL results before merging with the holistic SRL systems, (P =
precision, R = recall, F = F-score; above-baseline results in boldface)
SRDAUTO SRDORACLE
SEGNP SEGORACLE SEGNP SEGORACLE
P R F P R F P R F P R F
VAAUTO 38.77 4.58 8.2 55.12 6.96 12.36 62.68 7.42 13.27 91.41 11.53 20.48
VAORACLE 42.2 6.96 11.95 56.64 10.36 17.51 71.64 11.81 20.28 99.37 18.15 30.69
Table 6. Preposition SRL combined with [7] (P = precision, R = recall, F = F-score;
above-baseline results in boldface)
SRDAUTO SRDORACLE
SEGNP SEGORACLE SEGNP SEGORACLE
P R F P R F P R F P R F
ORIG 72.43 66.77 69.49 72.43 66.77 69.49 72.43 66.77 69.49 72.43 66.77 69.49
VAAUTO 72.00 66.84 69.32 72.08 66.91 69.40 72.13 66.95 69.44 72.31 67.11 69.61S1
VAORACLE 71.92 67.02 69.38 71.97 67.30 69.55 72.29 67.39 69.75 72.81 68.12 70.39
VAAUTO 71.34 66.22 68.68 70.66 65.60 68.04 73.12 67.89 70.41 73.42 68.16 70.69S2
VAORACLE 71.01 66.16 68.50 69.78 65.21 67.42 73.68 68.67 71.08 74.35 69.55 71.87
VAAUTO 70.10 65.00 67.46 72.25 66.83 69.43 73.12 67.84 70.38 77.16 71.39 74.16S3
VAORACLE 70.38 65.91 68.07 73.10 68.67 70.81 75.58 70.82 73.12 81.42 76.55 78.91
Table 7. Preposition SRL combined with [8] (P = precision, R = recall, F = F-score;
above-baseline results in boldface)
SRDAUTO SRDORACLE
SEGNP SEGORACLE SEGNP SEGORACLE
P R F P R F P R F P R F
ORIG 70.07 63.07 66.39 70.07 63.07 66.39 70.07 63.07 66.39 70.07 63.07 66.39
VAAUTO 68.50 63.79 66.06 69.17 64.44 66.72 69.37 64.60 66.90 70.58 65.73 68.07S1
VAORACLE 68.18 64.59 66.33 68.93 65.57 67.21 69.75 66.09 67.87 71.65 68.18 69.87
VAAUTO 68.21 63.52 65.79 68.31 63.64 65.89 70.53 65.68 68.02 71.87 66.94 69.32S2
VAORACLE 67.77 64.19 65.93 67.50 64.19 65.81 71.43 67.68 69.51 73.51 69.95 71.69
VAAUTO 67.14 62.30 64.63 69.39 64.23 66.71 70.19 65.14 67.57 74.34 68.81 71.47S3
VAORACLE 66.79 63.22 64.96 69.58 66.05 67.76 71.98 68.14 70.01 77.87 73.93 75.85
SRL, and investigate how the three subtasks interact with each other and what
their respective limits are, we also used oracled outputs from each subtask in
combining the final outputs of the preposition SRL system. The oracled outputs
are what would be produced by perfect classifiers, and are emulated by inspection
of the gold-standard annotations for the testing data.
Table 5 shows the results of the preposition SRL systems before they are
merged with the verb SRL systems. These results show that the coverage of our
preposition SRL system is quite low relative to the total number of arguments
788 P. Ye and T. Baldwin
Table 8. Preposition SRL combined with [9] (P = precision, R = recall, F = F-score;
above-baseline results in boldface)
SRDAUTO SRDORACLE
SEGNP SEGORACLE SEGNP SEGORACLE
P R F P R F P R F P R F
ORIG 71.81 61.11 66.03 71.81 61.11 66.03 71.81 61.11 66.03 71.81 61.11 66.03
VAAUTO 70.23 61.87 65.78 70.74 62.43 66.32 71.13 62.65 66.62 72.34 63.83 67.82S1
VAORACLE 69.61 62.63 65.94 70.20 63.60 66.74 71.57 64.38 67.79 73.49 66.60 69.87
VAAUTO 69.92 61.60 65.50 69.91 61.69 65.54 72.10 63.50 67.53 73.39 64.75 68.80S2
VAORACLE 69.14 62.19 65.48 68.84 62.35 65.43 72.79 65.47 68.94 74.83 67.82 71.15
VAAUTO 69.01 60.66 64.57 71.31 62.57 66.65 72.24 63.49 67.58 76.54 67.15 71.54S3
VAORACLE 68.77 61.86 65.13 71.59 64.81 68.03 74.19 66.74 70.27 80.25 72.67 76.27
in the testing data, even when oracled outputs from all three subsystems are
used (recall = 18.15%). However, this is not surprising because we expected the
majority of semantic roles to be noun phrases.
In Tables 6, 7 and 8, we show how our preposition SRL system performs
when merged with the top 3 systems under the 3 merging strategies introduced
in Section 3.6. In each table, ORIG refers to the base system without preposition
SRL merging.
We can make a few observations from the results of the merged systems.
First, out of verb attachment, SRD and segmentation, the SRD module is both:
(a) the component with the greatest impact on overall performance, and (b)
the component with the greatest differential between the oracle performance
and classifier (AUTO) performance. This would thus appear to be the area in
which future efforts should be concentrated in order to boost the performance
of holistic SRLs through preposition SRL.
Second, the results show that in most cases, the recall of the merged system is
higher than that of the original SRL system. This is not surprising given that we
are generally relabelling or adding information to the argument structure of each
verb, although with the more aggressive merging strategies (namely S2 and S3)
it sometimes happens that recall drops, by virtue of the extent of an argument
being aversely affected by relabelling. It does seem to point to a complementarity
between verb-driven SRL and preposition-specific SRL, however.
Finally, it was somewhat disappointing to see that in no instance did a fully-
automated method surpass the base system in precision or F-score. Having said
this, we were encouraged by the size of the margin between the base systems and
the fully oracle-based systems, as it supports our base hypothesis that preposi-
tion SRL has the potential to boost the performance of holistic SRL systems,
up to a margin of 10% in F-score for S3.
4 Analysis and Discussion
In the previous 2 sections, we presented the methodologies and results of two
systems that perform statistical analysis on the semantics of prepositions, each
Semantic Role Labelling of Prepositional Phrases 789
using a different data set. The performance of the 2 systems was very differ-
ent. The SRD system trained on the treebank produced highly credible results,
whereas the SRL system trained on CoNLL 2004 SRL data set produced some-
what negative results. In the remainder of this section, we will analyze these
results and discuss their significance.
There is a significant difference between the results obtained by the tree-
bank classifier and that obtained by the CoNLL SRL classifier. In fact, even
with a very small number of collocation features, the treebank classifier still
outperformed the CoNLL SRL classifier. This suggests that the semantic tag-
ging of prepositions is somewhat artificial. This is evident in three ways. First,
the proportion of prepositional phrases tagged with semantic roles is small ?
around 57,000 PPs out of the million-word Treebank corpus. This small pro-
portion suggests that the preposition semantic roles were tagged only in cer-
tain prototypical situations. Second, we were able to achieve reasonably high
results even when we used a collocation feature set with fewer than 200 fea-
tures. This further suggests that the semantic roles were tagged for only a small
number of verbs in relatively fixed situations. Third, the preposition SRD sys-
tem for the CoNLL data set used a very similar feature set to the treebank
system, but was not able to produce anywhere near comparable results. Since
the CoNLL dataset is aimed at holistic SRL across all argument types, it in-
corporates a much larger set of verbs and tagging scenarios; as a result, the
semantic role labelling of PPs is far more heterogeneous and realistic than is
the case in the treebank. Therefore, we conclude that the results of our tree-
bank preposition SRD system are not very meaningful in terms of predict-
ing the success of the method at identifying and semantically labelling PPs
in open text.
A few interesting facts came out of the results over the CoNLL dataset. The
most important one is that by using an independent preposition SRL system,
the results of a general verb SRL system can be significantly boosted. This
is evident because when the oracled results of all three subtasks were used, the
merged results were around 10% higher than those for the original systems, in all
three cases. Unfortunately, it was also evident from the results that we were not
successful in automating preposition SRL. Due to the strictness of the CoNLL
evaluation, it was not always possible to achieve a better overall performance
by improving just one of the three subsystems. For example, in some cases,
worse results were achieved by using the oracled results for PPA, and the results
produced by SRD classifier than using the PPA classifier and the SRD classifiers
in conjunction. The reason for the worse results is that in our experiments, the
oracled PPA always identifies more prepositions attached to verbs than the PPA
classifier, therefore more prepositions will be given semantic roles by the SRD
classifier. However, since the performance of the SRD classifier is not high, and
the segmentation subsystem does not always produce the same semantic role
boundaries as the CoNLL data set, most of these additional prepositions would
either be given a wrong semantic role or wrong phrasal extent (or both), thereby
causing the overall performance to fall.
790 P. Ye and T. Baldwin
Finally, it is evident that the merging strategy also plays an important role
in determining the performance of the merged preposition SRL and verb SRL
systems: when the performance of the preposition SRL system is high, a more
preposition-oriented merging scheme would produce better overall results, and
vice versa.
5 Conclusion and Future Work
In this paper, we have proposed a method for labelling preposition semantics and
deployed the method over two different data sets involving preposition semantics.
We have shown that preposition semantics is not a trivial problem in general,
and also that has the potential to complement other semantic analysis tasks,
such as semantic role labelling.
Our analysis of the results of the preposition SRL system shows that sig-
nificant improvement in all three stages of preposition semantic role labelling?
namely verb attachment, preposition semantic role disambiguation and argu-
ment segmentation?must be achieved before preposition SRL can make a sig-
nificant contribution to holistic SRL. The unsatisfactory results of our CoNLL
preposition SRL system show that the relatively simplistic feature sets used in
our research are far from sufficient. Therefore, we will direct our future work
towards using additional NLP tools, information repositories and feature engi-
neering to improve all three stages of preposition semantic role labelling.
Acknowledgements
We would like to thank Phil Blunsom and Steven Bird for their suggestions and
encouragement, Tom O?Hara for providing insight into the inner workings of
his semantic role disambiguation system, and the anonymous reviewers for their
comments.
References
1. Marcus, M.P., Marcinkiewicz, M.A., Santorini, B.: Building a large annotated corpus
of English: the Penn treebank. Computational Linguistics 19 (1993) 313?330
2. O?Hara, T., Wiebe, J.: Preposition semantic classification via treebank and
FrameNet. In: Proc. of the 7th Conference on Natural Language Learning (CoNLL-
2003), Edmonton, Canada (2003)
3. Miller, G.A.: WordNet: a lexical database for English. Communications of the ACM
38 (1995) 39?41
4. Berger, A.L., Pietra, V.J.D., Pietra, S.A.D.: A maximum entropy approach to
natural language processing. Computational Linguistics 22 (1996) 39?71
5. Carreras, X., Ma`rquez, L.: Introduction to the CoNLL-2004 shared task: Seman-
tic role labeling. In: Proc. of the 8th Conference on Natural Language Learning
(CoNLL-2004), Boston, USA (2004) 89?97
Semantic Role Labelling of Prepositional Phrases 791
6. Briscoe, T., Carroll, J.: Robust accurate statistical annotation of general text. In:
Proc. of the 3rd International Conference on Language Resources and Evaluation
(LREC 2002), Las Palmas, Canary Islands (2002) 1499?1504
7. Hacioglu, K., Pradhan, S., Ward, W., Martin, J.H., Jurafsky, D.: Semantic role
labeling by tagging syntactic chunks. In: Proc. of the 8th Conference on Natural
Language Learning (CoNLL-2004), Boston, USA (2004)
8. Punyakanok, V., Roth, D., Yih, W.T., Zimak, D., Tu, Y.: Semantic role labeling
via generalized inference over classifiers. In: Proc. of the 8th Conference on Natural
Language Learning (CoNLL-2004), Boston, USA (2004)
9. Carreras, X., Ma`rquez, L., Chrupa, G.: Hierarchical recognition of propositional
arguments with perceptrons. In: Proc. of the 8th Conference on Natural Language
Learning (CoNLL-2004), Boston, USA (2004)
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 241?244,
Prague, June 2007. c?2007 Association for Computational Linguistics
MELB-YB: Preposition Sense Disambiguation Using Rich Semantic
Features
Patrick Ye and Timothy Baldwin
Computer Science and Software Engineering
University of Melbourne, Australia
{jingy,tim}@csse.unimelb.edu.au
Abstract
This paper describes a maxent-based prepo-
sition sense disambiguation system entry to
the preposition sense disambiguation task
of the SemEval 2007. This system uses a
wide variety of semantic and syntactic fea-
tures to perform the disambiguation task and
achieves a precision of 69.3% over the test
data.
1 Introduction
Prepositional phrases (PPs) are both common and
semantically varied in open English text. While the
conventional view on prepositions from the com-
putational linguistics community has been that they
are semantically transient at best, and semantically-
vacuous at worst, a robust account of the semantics
of prepositions and disambiguation method can be
helpful in a range of NLP tasks including machine
translation, parsing (prepositional phrase attach-
ment) and semantic role labelling (Durand, 1993;
O?Hara and Wiebe, 2003; Ye and Baldwin, 2006a).
The SemEval 2007 preposition sense disambigua-
tion task provides a common test bed for the evalua-
tion of preposition sense disambiguation systems.
Our proposed method is maximum entropy based,
and combines features developed in the context of
preposition sense disambiguation for semantic role
labelling (Ye and Baldwin, 2006a), and verb sense
disambiguation (Ye and Baldwin, 2006b).
The remainder of this paper is structured as fol-
lows. We first discuss the pre-processing steps
used in our system (Section 2), and outline the fea-
tures our preposition disambiguation method uses
(Section 3) and our parameter tuning method (Sec-
tion 4). We then discuss and analyse the results of
our method (Section 5) and conclude the paper (Sec-
tion 6).
2 Pre-processing
The following list shows the pre-processing steps
that our system goes through and the tools used:
Part of speech tagging SVMTool version 1.2
(Gime?nez and Ma`rquez, 2004).
Chunking An in-house chunker implemented
with fnTBL, a transformation based learner (Ngai
and Florian, 2001), and trained on the British Na-
tional Corpus (BNC).1
Parsing Charniak?s re-ranking parser, version Au-
gust, 2006 (Charniak and Johnson, 2005).
Named entity extraction A statistical NER sys-
tem described in Cohn et al (2005).
Supersense tagging A WordNet-based super-
sense tagger (Ciaramita and Altun, 2006).
Semantic role labeling ASSERT version 1.4
(Pradhan et al, 2004).
3 Features
The disambiguation features used by our system can
be divided into three categories: collocation fea-
tures, syntactic features and semantic-role based fea-
tures. We discuss each in turn below.
3.1 Collocation Features
The collocation features were inspired by the
one-sense-per-collocation heuristic proposed by
Yarowsky (1995). These features were designed to
capture open class words that exhibit strong colloca-
tion properties with respect to the different senses of
the target preposition. Details of the features in this
category are listed below.
1This chunker is not exactly the same as Ngai and Florian?s
system, however it does use the default transformation tem-
plates supplied by fnTBL.
241
Bag of open class words The part-of-speech
(POS) tags and lemmas of all the open class words
that occur in the same sentence as the target prepo-
sition.
Bag of WordNet synsets The WordNet (Miller,
1993) synonym sets and their hypernyms of all the
open class words that occur in the same sentence as
the target preposition.
Bag of named entities Each named entity in the
same sentence as the target preposition is treated as
a separate feature.
Surrounding words These features are the com-
binations of the lemma, POS tag and relative posi-
tion of the words surrounding the target preposition
within a window of 7 words.
Surrounding super senses These features are the
combinations of super-sense tag, POS tag and rel-
ative position of the words surrounding the target
preposition within a window of 7 words.
3.2 Syntactic Features
The syntactic features were designed to capture both
the flat and recursive syntactic properties of the tar-
get preposition. The flat syntactic features were de-
rived from the surrounding POS tags and chunk tags
of the target preposition; the recursive syntactic fea-
tures were derived from the parse trees. The details
of these feature are given below.
Surrounding POS tags These features are the
combination of POS tag and relative position of the
words surrounding the target preposition within a
window of 7 words.
Surrounding chunk tags These features are the
combination of IOB style chunk tag and relative po-
sition of the words surrounding the target preposi-
tion within a window of 5 words.
Surrounding chunk types Instead of using only
the chunk tags themselves, we also extracted the ac-
tual chunk types (NP, VP, ADJP, etc) of the words
surrounding the target preposition within a window
of 5 words. Each chunk type is also combined with
its relative position to the target preposition as a sep-
arate feature.
S
I
NP VP
live
in 
PP
Melbourne
S_NP S_VP
live VP_PP
in PP_NP
Melbourne
I
S
NP
Figure 1: Parse tree examples
Parse tree features Given the position of the tar-
get preposition p in the parse tree, the basic form of
the corresponding parse tree feature is just the list of
nodes of p?s siblings in the tree (the POS tags are
treated as part of the terminal). For example, sup-
pose the original parse tree for the sentence I live in
Melbourne is the left tree in Figure 1, for the target
preposition in, the basic form of the parse tree fea-
ture would be (1, NP). In order to gain more syn-
tactic information, we further annotated each non-
terminal of the parse tree with its parent node, and
used the new non-terminals as our features. The
right tree in Figure 1 shows the result of applying
this annotation once to the original parse tree. Two
levels of additional annotation were performed on
the original parse trees in our feature extraction.
3.3 Semantic-Role Based Features
Finally, since prepositional phrases can often func-
tion as the temporal, location, and manner modifiers
for verbs, we designed semantic-role-based features
to specifically capture this type of verb-preposition
semantic information. The details of these features
are as follows:
Surrounding semantic role tags The semantic
role tags of the words surrounding the target preposi-
tion within a window of 5 words are combined with
their relative positions to the target preposition and
treated as separate features. For example, consider
the preposition on in the sentence The man who
stole my car on Sunday has apologised to me, the
semantic roles for the two verbs (stole and apolo-
gised ) are shown in Table 1. The semantic roles for
stole would generate the following features: (-5, I-
A0), (-4, R-A0), (-3, TARGET), (-2, B-A1), (-1,
I-A1), (0, B-AM-TMP), (1, I-AM-TMP), (2, O), (3,
O), (4, O and (5, O).
Attached verbs This feature was designed to
capture the verb-particle and verb-preposition-
242
The man who stole my car on Sunday has apologised to me
stole B-A0 I-A0 R-A0 TARGET B-A1 I-A1 B-AM-TMP I-AM-TMP O O O O
apologised B-A0 I-A0 I-A0 I-A0 I-A0 I-A0 I-A0 I-A0 O TARGET B-A2 I-A2
Table 1: Example semantic-role-labelled sentence
attachment relationships between verbs and prepo-
sitions. There are two situations in which a preposi-
tion p is deemed to be attached to a verb v: (1) p has
a semantic role tag relative to v and this tag is a ?B?
tag, (2) p has no semantic role tag relative to v, but
the first token to the right of p has a ?B? tag relative
to v. In the sentence shown in Table 1, stole would
be considered as the governor of on.
Verb?s relative position The lemma of each verb
in the same sentence as the target preposition is com-
bined with its relative position to the target preposi-
tion and treated as a separate feature. For example,
the sentence shown in Table 1 would generate the
two features: (-1, steal) and (1, apologize).
More detailed descriptions and examples for these
features may be found in Ye and Baldwin (2006b).
4 Parameter Tuning
We used the ranking-based feature selection method
from Ye and Baldwin (2006b) to select the most rele-
vant feature based on our training data. This method
works in two steps. Firstly, we calculated the infor-
mation gain, gain ratio and Chi-squared statistics for
each feature, and used these values to generate 3 sets
of rankings for the features. We then summed up the
individual ranks, and used the sums to create a set of
final rankings for the features.
The feature selection process is based on 10-fold
cross validation: we divided our training data into
10 pairs of training-test datasets; then for each fold,
we extracted the top N% ranked features using our
feature selection heuristic from the cv-training set
(where N was set to values 5, 10, .., 100), and used
these features to test the held-out test set. The best
N as determined by the cross validation was then
applied to the entire training data set.
Additionally, since we used a maximum entropy-
based machine learning package,2 it was important
to determine the best Gaussian smoothing parameter
g for the probability distribution. The tuning of g
2http://homepages.inf.ed.ac.uk/s0450736/
maxent_toolkit.html
was incorporated into the cross validation process of
feature selection.
Given the possible combinations of parameter
tuning, we trained the following three classifiers for
the preposition sense disambiguation task:
Non-tuned Using all the original features and
10.0 for the Gaussian smoothing parameter.
Smoothing-tuned Using all the original features
but automatically tuned Gaussian smoothing param-
eter.
Fully-tuned Using both automatically tuned fea-
tures and Gaussian smoothing parameter.
5 Results and Analysis
The overall precision (%) obtained by the three clas-
sifiers for the fine-grained senses are as follows:
Non-tuned Smoothing-tuned Fully tuned
67.9 68.0 69.3
The best overall results were achieved when both
the features and the Gaussian smoothing parameters
were automatically tuned, achieving a 1.4% absolute
precision gain over the non-tuned system. However,
such parameter tuning may not always be useful: the
same tuning process was found to be detrimental in a
Senseval-2 verb sense disambiguation task (Ye and
Baldwin, 2006b). Consistent with the findings of
Ye and Baldwin (2006b), the improvement caused
by the tuning of the Gaussian smoothing parame-
ter is only marginal compared with the improvement
caused by the tuning of the features.
We also evaluated our features based on their cate-
gories and types. Collocation features performed the
best among the three feature categories. Without any
parameter tuning, the collocation-feature-only clas-
sifier achieved an overall precision of 67.4% on the
test set; the semantic-role-feature-only classifier and
the syntactic-feature-only classifier achieved preci-
sion of 46.9% and 50.5% respectively.
The best-performing individual features are the
bag-of-words features and bag-of-synsets features.
243
Feature type % in Overall
Feature type top N% features % of the
10 20 30 feature type
Bag of Words 13.46 13.43 12.94 13.37
Bag of Synsets 57.83 58.38 59.53 58.29
Verb?s rel. positions 3.97 3.95 3.76 4.02
Surrounding POS tags 1.36 1.33 1.43 1.27
Table 2: Percentages of top-performing feature
types in the top N% ranked features
On the test set, the bag-of-words-only classifier and
the bag-of-synsets-only classifier achieved overall
precision of 63.2% and 61.9% respectively.
We also analysed the top ranking features as cal-
culated by our feature selection algorithm, as pre-
sented in Table 2. The results show the percentages
of the top-performing feature types of each feature
category in the top N% ranked features. It can be
observed that none of the top-performing features
seem to have a significantly disproportional repre-
sentation in the top-ranked features. This indicates
that the disambiguation power of a particular type
of features is determined mostly by the number of
features of that type.
On the other hand, the bag-of-words features ap-
pear to be the most effective, considering that they
account for only 13.4% of the total features, but
out-performed the bag-of-synsets features which ac-
count for nearly 60% of the total features.
It is also disappointing to see that the syntactic
and semantic-role based features had little positive
influence in the disambiguation process. However,
this is perhaps caused by the sparseness of these fea-
tures since they together only account for less than
10% of all the extracted features.
The overall finding from all this is that, similar
to nouns and verbs, preposition sense is determined
primarily by word context, and that syntactic and se-
mantic role-based features play only a minor role.
6 Conclusions
In this paper, we have described a maximum entropy
based preposition sense disambiguation system that
uses a rich set of features. We have shown that
this system performed well above the majority class
baseline of 39.6% precision. Our analysis showed
that the most important disambiguation features are
collocation-based features. This indicates that the
semantics of prepositions can be learnt mostly from
their surrounding context, and not syntactic proper-
ties or verb-preposition semantics.
Acknowledgements
The research in this paper has been supported by the Aus-
tralian Research Council through Discovery Project grant num-
ber DP0663879.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In Pro-
ceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages 173?180, Ann
Arbor, USA.
Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-
coverage sense disambiguation and information extraction
with a supersense sequence tagger. In Proceedings of the
2006 Conference on Empirical Methods in Natural Lan-
guage Processing, pages 594?602, Sydney, Australia.
Trevor Cohn, Andrew Smith, and Miles Osborne. 2005. Scal-
ing conditional random fields using error-correcting codes.
In Proceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages 10?17,
Ann Arbor, USA.
Jacques Durand. 1993. On the translation of prepositions in
multilingual MT. In Frank Van Eynde, editor, Linguistic Is-
sues in Machine Translation, pages 138?159. Pinter Publish-
ers, London, UK.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. Svmtool: A gen-
eral pos tagger generator based on support vector machines.
In Proceedings of the 4th International Conference on Lan-
guage Resources and Evaluation, pages 43?46, Lisbon, Por-
tugal.
George A. Miller. 1993. Wordnet: a lexical database for en-
glish. In HLT ?93: Proceedings of the workshop on Human
Language Technology, pages 409?409, Princeton, USA.
Grace Ngai and Radu Florian. 2001. Transformation-based
learning in the fast lane. In Proc. of the 2nd Annual Meeting
of the North American Chapter of Association for Compu-
tational Linguistics (NAACL2001), pages 40?7, Pittsburgh,
USA.
Tom O?Hara and Janyce Wiebe. 2003. Preposition semantic
classification via Treebank and FrameNet. In Proc. of the 7th
Conference on Natural Language Learning (CoNLL-2003),
pages 79?86, Edmonton, Canada.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2004. Support
vector learning for semantic argument classification. Ma-
chine Learning, 60(1?3):11?39.
David Yarowsky. 1995. Unsupervised word sense disambigua-
tion rivaling supervised methods. In Meeting of the Associ-
ation for Computational Linguistics, pages 189?196, Cam-
bridge, USA.
Patrick Ye and Timothy Baldwin. 2006a. Semantic role label-
ing of prepositional phrases. ACM Transactions on Asian
Language Information Processing (TALIP), 5(3):228?244.
Patrick Ye and Timothy Baldwin. 2006b. Verb sense dis-
ambiguation using selectional preferences extracted with a
state-of-the-art semantic role labeler. In Proceedings of the
Australasian Language Technology Workshop, pages 141?
148, Sydney, Australia.
244
Coling 2010: Poster Volume, pages 1558?1566,
Beijing, August 2010
Interpreting Pointing Gestures and Spoken Requests ? A Probabilistic,
Salience-based Approach
Ingrid Zukerman and Gideon Kowadlo and Patrick Ye
Faculty of Information Technology
Monash University
Ingrid.Zukerman@monash.edu,gkowadlo@gmail.com,ye.patrick@gmail.com
Abstract
We present a probabilistic, salience-based
approach to the interpretation of pointing
gestures together with spoken utterances.
Our mechanism models dependencies be-
tween spatial and temporal aspects of ges-
tures and features of utterances. For our
evaluation, we collected a corpus of re-
quests which optionally included point-
ing. Our results show that pointing infor-
mation improves interpretation accuracy.
1 Introduction
DORIS (Dialogue Oriented Roaming Interactive
System) is a spoken dialogue system designed for
a household robot. In (Zukerman et al, 2008),
we described Scusi? ? a spoken language in-
terpretation module which considers multiple sub-
interpretations at different levels of the interpreta-
tion process, and estimates the probability of each
sub-interpretation at each level (Section 2). This
formalism is required for requests such as ?Get
me the blue cup? in the context of the scene de-
picted in Figure 1, where possible candidates are
the three white cups, and the blue and purple tum-
blers, but it is unclear which is the intended object,
as none of the alternatives match the request per-
fectly.
In this paper, we integrate pointing gestures
into Scusi??s probabilistic formalism. We adopt
a salience-based approach, where we take into ac-
count spatial and temporal information to estimate
the probability that a pointing gesture refers to an
Figure 1: Experimental Setup
object. To evaluate our formalism, we collected a
corpus of requests where people were allowed to
point (Section 4). Our results show that when peo-
ple point, our mechanism yields significant im-
provements in interpretation accuracy; and when
pointing was artificially added to utterances where
the people did not point, its effect on interpretation
accuracy was reduced.
This paper is organized as follows. Section 2
outlines the interpretation of a spoken request and
the estimation of the probability of an interpre-
tation. Section 3 describes how pointing affects
this probability. Our evaluation is detailed in Sec-
tion 4. Related research is discussed in Section 5,
followed by concluding remarks.
2 Interpreting Spoken Requests
Here we summarize our previous work on the in-
terpretation of single-sentence requests (Makalic
et al, 2008; Zukerman et al, 2008).
1558
Scusi? processes spoken input in three stages:
speech recognition, parsing and semantic inter-
pretation. First, Automatic Speech Recognition
(ASR) software (Microsoft Speech SDK 5.3) gen-
erates candidate hypotheses (texts) from a speech
signal. The ASR produces up to 50 texts for a
spoken utterance, where each text is associated
with a probability. In the parsing stage, the texts
are considered in descending order of probability.
Charniak?s probabilistic parser (ftp://ftp.cs.
brown.edu/pub/nlparser/) is applied to each
text, yielding up to 50 parse trees ? each asso-
ciated with a probability.
During semantic interpretation, parse trees are
successively mapped into two representations
based on Concept Graphs (Sowa, 1984). First
Uninstantiated Concept Graphs (UCGs), and then
Instantiated Concept Graphs (ICGs). UCGs are
obtained from parse trees deterministically ? one
parse tree generates one UCG. A UCG represents
syntactic information, where the concepts corre-
spond to the words in the parent parse tree, and
the relations are derived from syntactic informa-
tion in the parse tree and prepositions. Each UCG
can generate many ICGs. This is done by nomi-
nating different instantiated concepts and relations
from the system?s knowledge base as potential re-
alizations for each concept and relation in a UCG.
Instantiated concepts are objects and actions in the
domain (e.g., mug01, mug02 and cup01 are pos-
sible instantiations of the uninstantiated concept
?mug?), and instantiated relations are similar to
semantic role labels (Gildea and Jurafsky, 2002).
The interpretation process continues until a pre-
set number of sub-interpretations (including texts,
parse trees, UCGs and ICGs) has been generated
or all options have been exhausted.
Figure 2 illustrates a UCG and an ICG for the
request ?get the large red folder on the table?. The
intrinsic features of an object (lexical item, colour
and size) are stored in the UCG node for this ob-
ject. Structural features, which involve two ob-
jects (e.g., ?folder-on-table?), are represented as
sub-graphs of the UCG (and the ICG).
2.1 Estimating the probability of an ICG
Scusi? ranks candidate ICGs according to their
probability of being the intended meaning of a
Patient
get01
On
folder02
table01
get
object
SIZE          LARGE
LEX folder
 COLOUR  RED
on
LEX table
Utterance:
UCG
Get the large red folder on the table
ICG
Figure 2: UCG and ICG for a sample utterance
spoken utterance. Given a speech signal W and a
context C, the probability of an ICG I , Pr(I|W, C),
is proportional to
?
?
Pr(T |W )?Pr(P |T )?Pr(U |P )?Pr(I|U, C) (1)
where T , P and U denote text, parse tree and
UCG respectively. The summation is taken over
all possible paths ? = {P,U} from the speech
wave to the ICG, because a UCG and an ICG
can have more than one ancestor. As mentioned
above, the ASR and the parser return an esti-
mate of Pr(T |W ) and Pr(P |T ) respectively; and
Pr(U |P ) = 1, since the process of generating a
UCG from a parse tree is deterministic. The es-
timation of Pr(I|U, C) is described in (Zukerman
et al, 2008). Here we present the final equation
obtained for Pr(I|U, C), and outline the ideas in-
volved in its calculation.
Pr(I|U, C)?
?
k?I
Pr(u|k, C) Pr(k|kp, kgp) Pr(k|C)
(2)
where u is a node in UCG U , k is the correspond-
ing instantiated node in ICG I , kp is k?s parent
node, and kgp is k?s grandparent node. For exam-
ple, On is the parent of table01, and folder02
the grandparent in the ICG in Figure 2.
? Pr(u|k) is the ?match probability? between the
specifications for node u in UCG U and the in-
trinsic features of the corresponding node k in
ICG I , i.e., the probability that a speaker who
intended a particular object k gave the specifi-
cations in u.
1559
? Pr(k|kp, kgp) represents the structural proba-
bility of ICG I , where structural information
is simplified to node trigrams, e.g., whether
folder02 is On table01.
? Pr(k|C) is the probability of a concept in light
of the context, which includes information
about domain objects, actions and relations.
Scusi? handles three intrinsic features: lexical
item, colour and size; and two structural features:
ownership and several locative relations (e.g., on,
under, near). The match probability Pr(u|k) and
the structural probability Pr(k|kp, kgp) are esti-
mated using a distance function between the re-
quirements specified by the user and what is found
in reality ? the closer the distance between the
specifications and reality, the higher the probabil-
ity (for details see (Makalic et al, 2008)).
3 Incorporating Pointing Gestures
Pointing affects the salience of objects and the
language used to refer to objects: objects in the
temporal and spatial vicinity of a pointing gesture
are more salient than objects that are farther away,
and pointing is often associated with demonstra-
tive determiners. Thus, the incorporation of point-
ing into Scusi? affects the following elements of
Equation 2 (Section 2.1).
? Pr(k|C) ? the context-based probability of an
object (i.e., its salience) is affected by the time
of a pointing gesture and the space it encom-
passes. For instance, if the user says ?Get the
cup? in the context of the scene in Figure 1,
pointing around the time s/he said ?cup?, the
gesture most likely refers to an object that may
be called ?cup?. Further, among the candidate
cups in Figure 1, those closer to the ?pointing
vector? have a higher probability.1
? Pr(u|k, C) ? when pointing, people often use
demonstrative determiners, e.g., ?get me that
cup?. Also, people often use generic identifiers
in conjunction with demonstrative determiners
1At present, we assume that an utterance is associated
with at most one pointing gesture, and that pointing pertains
to objects. This assumption is supported by our user study
(Section 4.1).
to refer to unfamiliar objects, e.g., ?that thing?
to refer to a vacuum tube (Figure 1).
These probabilities are estimated in Sec-
tions 3.1 and 3.2. Our calculations are based on
information returned by the gesture recognition
system described in (Li and Jarvis, 2009): gesture
type, time, probability and relevant parameters
(e.g., a vector for a pointing gesture). Since we fo-
cus on pointing gestures, we convert the probabil-
ities expected from Li and Jarvis?s system into the
probability of Pointing and that of Not Pointing,
which comprises all other gestures and no gesture
(these hypotheses are returned at the same time).2
3.1 Calculating salience from pointing
When pointing is taken into account, the probabil-
ity of object k is expressed as follows.
Pr(k|C) = Pr(k|P, C) ? Pr(P|C) + (3)
Pr(k|?P, C) ? Pr(?P|C)
where P designates Pointing, Pr(P|C) and its
complement are returned by the gesture recog-
nition system, and Pr(k|?P, C) = 1N (N is thenumber of objects in the room, i.e., in the absence
of pointing, we assume that all the objects in the
room are equiprobable3).
As indicated above, we posit that pointing is
spatially correlated with an intended object, and
temporally correlated with a word referring to the
intended object. Hence, we separate Pointing into
two components: spatial (s) and temporal (t), ob-
taining ?Ps,Pt?. Thus
Pr(k|P, C) = Pr(k,Pt,Ps, C)Pr(P, C) (4)
= Pr(Pt|k,Ps, C) ? Pr(k|Ps, C) ? Pr(Ps|C)Pr(P|C)
We assume that given k, Pt is conditionally in-
dependent fromPs; and that Pr(Ps|C) = Pr(P|C),
i.e., the spatial probability of a pointing gesture is
the probability returned by the gesture system for
the entire pointing hypothesis (time and space).
This yields
Pr(k|P, C) = Pr(Pt|k, C) ? Pr(k|Ps, C) (5)
2Owing to timing limitations of the gesture recognition
system (Section 4), we simulate its output.
3At present, we do not consider dialogue salience.
1560
ve
cto
r
po
int
ing
kj
pd(k,PLine) d(k,PLine)
PLineuse
r
(a) Pointing
po
int
ing
ve
cto
r
OLinek
j
pd(j,OLine)d(j,OLine)
us
er
(b) Occlusion
Figure 3: Spatial pointing and occlusion
where Pr(k|Ps, C) and Pr(Pt|k, C) are estimated
as described in Section 3.1.1 and 3.1.2 respec-
tively. This equation is smoothed as follows (and
incorporated into Equation 3) to take into account
objects that are (spatially or temporally) excluded
from the pointing gesture.
Pr?(k|P, C) = Pr(k|P, C) +
1
N
1 +
?N
j=1 Pr(kj |P, C)
(6)
3.1.1 Estimating Pr(k|Ps, C)
Pr(k|Ps, C), the probability that the user in-
tended object k when pointing to a location, is es-
timated using a conic Gaussian density function
around PLine, the Pointing Line created by ex-
tending the pointing vector returned by the gesture
identification system (Figure 3(a)).4
Pr(k|Ps, C) = ??k?2pi?Ps(pd)
e
? d(k,PLine)
2
2?2Ps(pd) (7)
where ? is a normalizing constant; ?Ps(pd) is the
standard deviation of the Gaussian cone as a func-
tion of pd(k,PLine), the projected distance be-
tween the user?s pointing hand and the projection
of object k on PLine; d(k,PLine) is the shortest
distance between the center of object k and PLine;
and ?k is a factor that reduces the probability of
object k if it is (partially) occluded (Figure 3(b)).
The projected distance pd takes into account
the imprecision of pointing actions ? a problem
that is exacerbated by the uncertainty associated
with sensing a pointing vector. A small angular
4Since this is a continuous density function, it does not
directly yield a point probability. Hence, it is normalized on
the basis of the largest possible returned value.
error in the detected pointing vector yields a dis-
crepancy in the distance between the pointing line
and candidate objects. This discrepancy increases
as pd(k,PLine) increases. To compensate for this
situation, we increase the variance of the Gaussian
distribution linearly with the projected distance
from the user?s hand (we start with a small stan-
dard deviation of ?0 = 5 mm at the user?s fingers,
attributed to sensor error). This allows farther ob-
jects with a relatively high displacement from the
pointing vector to be encompassed in a pointing
gesture (e.g., the larger mug in Figure 3(a)), while
closer objects with the same displacement are ex-
cluded (e.g., the smaller mug). This yields the fol-
lowing equation for the variance.
?2Ps(pd) = ?20 +K ? pd(k,PLine)
where K = 2.5 mm is an empirically determined
increase rate.
The occlusion factor ?k reduces the probability
of objects as they become more occluded. We ap-
proximate ?k by considering the objects that are
closer to the user than k, and estimating the extent
to which these objects occlude k (Figure 3(b)).
This estimate is a function of the position of these
objects and their size ? the larger an intervening
object, the lower the probability that the user is
pointing at k. These factors are taken into account
as follows.
Pr(j occl k)= ??
2pi??(pd)e
? (d(j,OLine)?
1
2 dimmin(j))2
2?2?(pd)
(8)
where ? is a normalizing constant; the numera-
tor of the exponent represents the maximum dis-
tance from the edge of object j to the line between
the user?s hand and object k, denoted Object Line
(OLine); and
?2?(pd) = 12
(?20 +K ? pd(j,OLine)
)
represents the variance of a cone from the user?s
hand to object k as a function of distance. In order
to represent the idea that object j must be close
to the Object Line to occlude object k, we use
half the variance of that used for the ?pointing
cone?, which yields a thinner ?occlusion cone?
(Figure 3(b)). ?k is then estimated as 1 minus the
1561
maximum occlusion caused by the objects that are
closer to the user than k.
?k=1? max?j d(j,hand)<d(k,hand) {Pr(j occl k)} (9)
3.1.2 Estimating Pr(Pt|k, C)
Pr(Pt|k, C) is obtained as follows.
Pr(Pt|k, C) =
n?
i=1
Pr(Pt, k,Wi, C)
Pr(k, C) (10)
=
n?
i=1
Pr(k|Pt, wi, C)?Pr(T (wi)|Pt, C)?Pr(Pt|C)
Pr(k|C)
where n is the number of nouns in the user?s utter-
ance, and Wi = ?wi, T (wi)? is a tuple comprising
the ith noun and the mid point of the time when it
was uttered.
We make the following assumptions.
? Pr(Pt|C) = 1, as all the gesture hypotheses
are returned at the same time;
? given Pt, the timing of a word T (wi) is condi-
tionally independent of C; and
? given wi, k is conditionally independent of
the timing of the pointing gesture Pt, i.e.,
Pr(k|Pt, wi, C) = Pr(k|wi, C).
This probability is represented as
Pr(k|wi, C) = Pr(wi|k) ? Pr(k|C)?N
j=1 {Pr(wi|kj) ? Pr(kj |C)}
where N is the number of objects.
These assumptions yield
Pr(Pt|k, C)=
n?
i=1
Pr(wi|k) ? Pr(T (wi)|Pt)?N
j=1 {Pr(wi|kj)?Pr(kj |C)}
(11)
where Pr(T (wi)|Pt), the probability of the time
of word wi given the time of the pointing gesture,
is obtained from the following Gaussian time dis-
tribution for pointing.
Pr(T (wi)|Pt) = ??2pi?Pt
e
? (T(wi)?PTime)
2
2?2Pt (12)
where ? is a normalizing constant, PTime is the
time of the gesture, and ?Pt is the standard de-
viation of the Gaussian density function, which is
currently set to 650 msec (based on our corpus).
As in our previous work (Makalic et al,
2008), we estimate Pr(wi|k) using the Leacock
and Chodorow (1998) WordNet similarity metric.
This metric also yields a match probability be-
tween most objects and generic words like ?ob-
ject, thing, here, there?, enabling us to handle re-
quests such as ?Get that thing over there?.
3.2 Calculating the probability of a referring
expression
As mentioned in Section 2, the intrinsic features
previously considered in Scusi? are lexical item,
colour and size (Makalic et al, 2008). Pointing
affects referring expressions in that people may
point instead of generating complex descriptions,
they may employ demonstrative determiners to-
gether with generic terms such as ?thing? (espe-
cially when they are unfamiliar with the name of
an object), and they may use demonstrative pro-
nouns. The first two behaviours were exhibited in
our user study (Section 4), but none of our trial
participants used demonstrative pronouns.
To incorporate pointing into the calculation of
Pr(u|k, C), we add determiners to Scusi??s for-
malism for intrinsic features, which yields
Pr(u|k, C) = Pr(ulex, udet, ucolr, usize|k, C)
After adding weights for the intrinsic features
(inspired by (Dale and Reiter, 1995)), and making
some simplifying assumptions, we obtain
Pr(u|k, C) = (13)
Pr(ulex|k, C)wlex ? Pr(udet|k, C)wdet ?
Pr(ucolr|k)wcolr ? Pr(usize|ulex, k)wsize
The estimation of Pr(ulex|k, C), Pr(ucolr|k) and
Pr(usize|ulex, k) is described in (Makalic et al,
2008). Here we focus on Pr(udet|k, C).
3.2.1 Estimating Pr(udet|k, C)
Pr(udet|k, C) is estimated as follows.
Pr(udet|k, C) = Pr(k|udet, C) ? Pr(udet|C)Pr(k|C) (14)
= Pr(k|udet, C)Pr(k|C)
[ Pr(udet|P, C) ? Pr(P|C)+
Pr(udet|?P, C) ? Pr(?P|C)
]
1562
where det = {def article, indef article, de-
monstr this, demonstr that}; Pr(P|C) and
Pr(?P|C) are returned by the gesture system;
Pr(udet|P, C) and Pr(udet|?P, C) are obtained
from our corpus; and for now we assume that
Pr(k|udet, C) = Pr(k|C).5 This yields
Pr(udet|k, C) = Pr(udet|P, C) ? Pr(P|C) + (15)
Pr(udet|?P, C) ? Pr(?P|C)
4 Evaluation
To obtain a corpus, we conducted a user study
whereby we set up a room with labeled objects
(Figure 1), and asked trial participants to request
12 selected items from DORIS (the room included
33 items in total, including distractors, and one of
the authors pretended to be DORIS). The objects
were selected and laid out in the room to reflect
a variety of conditions, e.g., common and rare
objects (e.g., vacuum tube); unique, non-unique
and similar objects (e.g., white cups); and objects
placed near each other and far from each other.
We divided our corpus of requests into two
parts: with and without pointing. Scusi??s perfor-
mance was tested on input obtained from the ASR
and on textual input (perfect ASR). We consid-
ered two scenarios for each sub-corpus: Pointing,
where our pointing mechanism was activated on
the basis of a simulated pointing gesture,6 and No-
Pointing, where no pointing gesture was detected.
This was done in order to test two hypotheses:
(1) when people point, pointing information im-
proves interpretation performance; and (2) when
they do not point, even perfect pointing has little
effect on interpretation performance.
Scusi? was set to generate at most 300 sub-
interpretations in total (including texts, parse
trees, UCGs and ICGs) for each spoken request,
and at most 200 sub-interpretations for each tex-
tual request. On average, Scusi? takes 10 seconds
to go from texts to ICGs. An interpretation was
5In the future, we will incorporate distance from the user
to refine the probabilities of determiners.
6At present, we assume accurate pointing and gesture de-
tection, and precise information regarding the position of ob-
jects. In the near future, we will study the sensitivity of our
mechanism to pointing inaccuracies, and to errors in gesture
detection and scene analysis.
deemed successful if it correctly represented the
speaker?s intention, which was encoded in one or
more Gold ICGs. These ICGs were manually con-
structed on the basis of the requested objects and
the participants? utterances. Multiple Gold ICGs
were allowed if there were several suitable actions
and objects.
4.1 The Corpus
19 people participated in the trial, generating a to-
tal of 276 requests, of which 136 involved point-
ing gestures (3 participants were asked to repeat
the experiment after it became clear that they were
refraining from pointing, as they erroneously as-
sumed they were not allowed to gesture). We fil-
tered out 64 requests, which included concepts
our system cannot yet handle, specifically ?the
end of the table?, projective modifiers (e.g., ?be-
hind/left?), ordinals (?first/second?), references to
groups of things (e.g., ?six blue pens?), and zero-
and one-anaphora. This yielded 212 requests, of
which 105 involved pointing gestures.
In addition, the software we used has the fol-
lowing limitations: the gesture recognition sys-
tem (Li and Jarvis, 2009) requires users to hold
a gesture for 2 seconds, and the ASR system is
speaker dependent and cannot recognize certain
words (e.g., ?mug?, ?bowl? and ?pen?). To cir-
cumvent these problems, each pointing gesture
was manually encoded into a time-stamped vec-
tor; and one of the authors read slightly sanitized
versions of participants? utterances into the ASR:
?can you?, ?please? and ?DORIS? were omitted;
long prepositional phrases were shortened (e.g.,
?the thing with wires sticking out of it?); and
words that were problematic for the ASR were re-
placed (e.g., ?pencil? was used instead of ?pen?).
There was some difference in the length of re-
quests with and without pointing, but it wasn?t as
pronounced as reported in (Johnston et al, 2002):
requests with/without pointing had 5.84/6.27
words on average. ASR performance was worse
for the requests that had pointing, with the top
ASR interpretation being correct for only 46%
of these requests, compared to 57.5% for the re-
quests without pointing. This difference may be
attributed to the ASR having trouble with sen-
tence constructs associated with pointing. Overall
1563
% Gold ICGs in Avg adj rank % Not Avg adj rank % Not
top 1 top 3 (rank) found (rank) 20 found 20
Sub-corpus without pointing
Text, Scusi?-NoPointing 89.7 93.5 4.39 (0.78) 0.9 1.18 (0.13) 4.7
Text, Scusi?-Pointing 86.9 87.9 3.28 (1.89) 0.9 0.39 (0.35) 4.7
ASR, Scusi?-NoPointing 81.3 85.0 4.67 (0.83) 7.5 1.24 (0.17) 12.1
ASR, Scusi?-Pointing 79.4 81.3 5.00 (2.62) 5.6 0.46 (0.40) 12.1
Sub-corpus with pointing
Text, Scusi?-NoPointing 84.8 89.5 3.54 (0.59) 4.8 1.48 (0.20) 9.5
Text, Scusi?-Pointing 82.9 86.7 4.19 (1.63) 1.9 0.41 (0.29) 7.6
ASR, Scusi?-NoPointing 76.2 82.9 7.93 (0.95) 10.5 1.79 (0.27) 15.2
ASR, Scusi?-Pointing 73.3 81.0 8.65 (2.76) 8.6 0.68 (0.40) 14.3
Table 1: Scusi??s interpretation performance
the ASR returned the correct interpretation, at any
rank, for 88% of the requests.
4.2 Results
Table 1 summarizes our results. Column 1 dis-
plays the test condition (sub-corpus with/without
pointing, text/ASR, and with/without Scusi??s
pointing mechanism). Columns 2-3 show the per-
centage of utterances that had Gold ICGs whose
probability was among the top 1 and top 3, e.g.,
in the sub-corpus with pointing, when Scusi?-
Pointing was run on text, 82.9% of the utter-
ances had Gold ICGs with the highest probability
(top 1). The average adjusted rank (AR) and av-
erage rank of the Gold ICG appear in Column 4.
The rank of an ICG I is its position in a list sorted
in descending order of probability (starting from
position 0), such that all equiprobable ICGs are
deemed to have the same position. The adjusted
rank of an ICG I is the mean of the positions of
all ICGs that have the same probability as I . For
example, if we have 4 equiprobable ICGs in po-
sitions 0-3, each has a rank of 0, but an adjusted
rank of rbest+rworst2 = 1.5. Column 5 shows thepercentage of utterances that didn?t yield a Gold
ICG. Column 6 shows the average AR for inter-
pretations with AR < 20 (and their average rank),
and Column 7 shows the percentage of utterances
that had AR ? 20 or were not found. We dis-
tinguish between Gold ICGs with ARs 0 to 19
and total Gold ICGs that were found, because a
dialogue manager is likely to inspect the promis-
ing options, i.e., those with AR < K (we assume
K = 20). In addition, there is normally a trade-
off between the number of Not Found Gold ICGs
and average AR. ICGs that are not found by one
approach but are found by another approach typi-
cally have a high (bad) rank when they are even-
tually found (Zukerman et al, 2008). Thus, an ap-
proach that fails to find such ?difficult? ICGs usu-
ally yields a lower average AR than an approach
that finds these ICGs. Capping the ARs of the
found Gold ICGs at 20 clarifies the trade-off be-
tween average AR and Not Found.
Our results show that, as expected, the main
role of pointing is in referent disambiguation.
This is evident from the significant reduction in
average AR-20 (Column 6) for the pointing and
no-pointing sub-corpora, under the text/ASR in-
put conditions. All the differences are statistically
significant with p < 0.01.7 Nonetheless, the im-
provements in average AR-20 obtained by artifi-
cially introduced pointing in the no-pointing sub-
corpus are smaller for both text and ASR than the
improvements obtained with actual pointing. We
posit that this smaller impact is due to the fact that
utterances without pointing are more descriptive
than those with pointing, hence benefitting less
from the disambiguating effect of pointing.
The Pointing condition has a seemingly adverse
effect on the number of interpretations with top
ranks (Columns 2-3). This is explained by the fact
7The differences were calculated using a paired t-test for
all the Gold ICGs that were found in both configurations.
1564
that all equiprobable interpretations have the same
rank, which happens more often under the No-
Pointing condition than under the Pointing con-
dition (as pointing has a disambiguating effect).
Finally, under all conditions, the rank of the re-
quest at the 75%-ile is 0, which indicates cred-
itable performance. The larger number of Not
Found Gold ICGs for the ASR condition is ex-
pected, as the ASR failed to find 12% of the cor-
rect texts on average, performing worse for the
pointing sub-corpus. The other Not Found Gold
ICGs were mainly due to parsing preferences, and
multiple parses for some utterances that had the
word ?thing? (which matched all objects).
5 Related Research
Gesture recognition systems endeavour to detect
the gesture being made. Common approaches in-
clude Hidden Markov Models, e.g., (Nickel and
Stiefelhagen, 2003), and Finite State Machines,
e.g., (Li and Jarvis, 2009). Systems that focus
on pointing also identify the target object, with-
out recognizing the type of this object (Nickel and
Stiefelhagen, 2003; Li and Jarvis, 2009).
Most of the research in gesture and speech in-
tegration focuses on pointing gestures, employ-
ing speech as the main input modality, and us-
ing semantic fusion to combine spoken input with
gesture. Different approaches are used for ges-
ture detection, e.g., vision (Stiefelhagen et al,
2004; Brooks and Breazeal, 2006) and sensor
glove (Corradini et al, 2002); and for language
interpretation, e.g., dedicated grammars (Stiefel-
hagen et al, 2004; Brooks and Breazeal, 2006)
and keywords (Einstein and Christoudias, 2004).
Fusion is variously implemented using heuristics
based on temporal overlap (Bolt, 1980; Johnston
et al, 2002), querying a gesture-sensing module
when ambiguous referents are identified (Fransen
et al, 2007), or unification to determine which
elements can be merged (Corradini et al, 2002;
Stiefelhagen et al, 2004). These are some-
times combined with search techniques coupled
with penalties (Einstein and Christoudias, 2004;
Brooks and Breazeal, 2006). With the exception
of Bolt?s system, these systems were tested on ut-
terances that were quite short and constrained.
Our approach integrates spatial and temporal
aspects of gesture into our probabilistic formal-
ism (Zukerman et al, 2008), focusing on the ef-
fect of pointing on object salience. Other salience-
based approaches are described in (Einstein and
Christoudias, 2004; Huls et al, 1995). How-
ever, they are not directly comparable with our
approach, as they use salience to weigh the im-
portance of factors pertaining to gesture-speech
alignment, but there is no uncertainty associated
with the visual salience resulting from pointing.
Our use of a probabilistic parser enables us
to handle more complex utterances than those
considered by most speech-gesture systems (Sec-
tion 2). At the same time, we do not yet handle
speech disfluencies, which are currently handled
by (Einstein and Christoudias, 2004; Stiefelhagen
et al, 2004). Also, at present we do not consider
the challenges pertaining to the real-time synchro-
nization of the output of a gesture-sensing and
a speech-recognition system (Stiefelhagen et al,
2004; Brooks and Breazeal, 2006).
6 Conclusion and Future Work
We have extended Scusi?, our spoken language in-
terpretation system, to incorporate pointing ges-
tures. Specifically, we have offered a formalism
that takes into account relationships between as-
pects of gesture and spoken language to integrate
information about pointing gestures into the es-
timation of the probability of candidate interpre-
tations of an utterance. Our empirical evaluation
shows that our formalism significantly improves
interpretation accuracy.
In the future, we propose to refine our model
of demonstrative determiners. We also intend to
perform sensitivity analysis regarding the accu-
racy of the vision system, and that of the gesture
recognition system. In addition, we will conduct
user studies to gain insights with respect to con-
ditions that influence the probability of pointing,
e.g., type of object and its position relative to the
speaker.
Acknowledgments
This research was supported in part by ARC grant
DP0878195. The authors thank R. Jarvis and D.
Li for their help with the gesture system.
1565
References
Bolt, R.A. 1980. ?Put-that-there?: voice and ges-
ture at the graphics interface. In Proceedings of
the 7th Annual Conference on Computer Graphics
and Interactive Techniques, pages 262?270, Seattle,
Washington.
Brooks, A.G. and C. Breazeal. 2006. Working with
robots and objects: Revisiting deictic reference for
achieving spatial common ground. In Proceedings
of the 1st ACM SIGCHI/SIGART Conference on
Human-robot Interaction, pages 297?304, Salt Lake
City, Utah.
Corradini, A., R.M. Wesson, and P.R. Cohen. 2002.
A Map-Based system using speech and 3D gestures
for pervasive computing. In ICMI?02 ? Proceedings
of the 4th International Conference on Multimodal
Interfaces, pages 191?196, Pittsburgh, Pennsylva-
nia.
Dale, R. and E. Reiter. 1995. Computational in-
terpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
18(2):233?263.
Einstein, J. and C.M. Christoudias. 2004. A salience-
based approach to gesture-speech alignment. In
Proceedings of the Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 25?32, Boston, Mas-
sachusetts.
Fransen, B., V. Morariu, E. Martinson, S. Blis-
ard, M. Marge, S. Thomas, A. Schultz, and
D. Perzanowski. 2007. Using vision, acoustics, and
natural language for disambiguation. In Proceed-
ings of the ACM/IEEE International Conference on
Human-robot Interaction, pages 73?80, Washing-
ton, DC.
Gildea, D. and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
Huls, C., W. Claassen, and E. Bos. 1995. Automatic
referent resolution of deictic and anaphoric expres-
sions. Computational Linguistics, 21(1):59?79.
Johnston, M., S. Bangalore, G. Vasireddy, A. Stent,
P. Ehlen, M. Walker, S. Whittaker, and P. Maloor.
2002. MATCH: an architecture for multimodal di-
alogue systems. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 376?383, Philadelphia, Pennsylvania.
Leacock, C. and M. Chodorow. 1998. Combining lo-
cal context and WordNet similarity forword sense
identification. In Fellbaum, C., editor, WordNet: An
Electronic Lexical Database, pages 265?285. MIT
Press.
Li, Z. and R. Jarvis. 2009. Real time hand gesture
recognition using a range camera. In Proceedings
of the Australasian Conference on Robotics and Au-
tomation, Sydney, Australia.
Makalic, E., I. Zukerman, M. Niemann, and
D. Schmidt. 2008. A probabilistic model for under-
standing composite spoken descriptions. In PRICAI
2008 ? Proceedings of the 10th Pacific Rim Interna-
tional Conference on Artificial Intelligence, pages
750?759, Hanoi, Vietnam.
Nickel, K. and R. Stiefelhagen. 2003. Pointing
gesture recognition based on 3D-tracking of face,
hands and head orientation. In ICMI?03 ? Pro-
ceedings of the 5th International Conference on
Multimodal Interfaces, pages 140?146, Vancouver,
British Columbia.
Sowa, J.F. 1984. Conceptual Structures: Information
Processing in Mind and Machine. Addison-Wesley,
Reading, MA.
Stiefelhagen, R., C. Fugen, R. Gieselmann,
H. Holzapfel, K. Nickel, and A. Waibel. 2004.
Natural human-robot interaction using speech, head
pose and gestures. In IROS 2004 ? Proceedings
of the IEEE/RSJ International Conference on
Intelligent Robots and Systems, volume 3, pages
2422?2427, Sendai, Japan.
Zukerman, I., E. Makalic, M. Niemann, and S. George.
2008. A probabilistic approach to the interpreta-
tion of spoken utterances. In PRICAI 2008 ? Pro-
ceedings of the 10th Pacific Rim International Con-
ference on Artificial Intelligence, pages 581?592,
Hanoi, Vietnam.
1566
