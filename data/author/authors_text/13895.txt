		

	
	

			
	

	

ISYS Group
AI Department
Technical University of
Madrid
Boadilla del Monte, 28660
Madrid, Spain
agarcia@ dia.fi.upm.es
	
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 100?101,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A preliminary approach to recognize generic drug names by combining 
UMLS resources and USAN naming conventions 
 
Isabel Segura-Bedmar Paloma Mart?nez Doaa Samy 
Computer Sciences Department Computer Sciences Department Linguistic Department 
Carlos III University of Madrid Carlos III University of Madrid Cairo University 
Avd. Universidad, 30, Legan?s, 
28911, Madrid, Spain 
Avd. Universidad, 30, Legan?s, 
28911, Madrid, Spain 
Egypt 
isegura@inf.uc3m.es pmf@inf.uc3m.es dsamy@cu.edu.eg 
  
 
Abstract 
This paper presents a system1 for drug name 
identification and classification in biomedical 
texts.  
1 Introduction 
Numerous studies have tackled gene and protein 
names recognition (Collier et al 2002), (Tanabe 
and Wilbur, 2002). Nevertheless, drug names have 
not been widely addressed (Rindflesch et al, 
2000). 
Automating the process of new drugs recognition 
and classification is a challenging task. With the 
rapidly changing vocabulary, new drugs are 
introduced while old ones are made obsolete. 
Though the terminological resources are frequently 
updated, they can not follow the accelerated pace 
of the changing terminology. 
Drug receives three distinct names: the chemical 
name, the generic (or nonproprietary) name, and 
the brand (or trademark) name. The U.S. Adopted 
Name (USAN) Council establishes specific 
nomenclature rules for naming generic drugs. 
These rules rely on the use of affixes that classify 
drugs according to their chemical structure, 
indication or mechanism of action. For example, 
analgesics substances can receive affixes such as  
-adol-, -butazone, -fenine, -eridine and ?fentanil. 
In the present work, we focus, particulary, on the 
implementation of a set of 531 affixes approved by 
                                                           
1 This work has been partially supported by the projects: FIT-
350300-2007-75 (Semantic Interoperability in Electronic 
Health Care) and TIN2007-67407-C03-01 (BRAVO: 
Advanced Multimodal and Multilingual Question Answering). 
the USAN Council and published in 20072. The 
affixes allow a specific classification of drugs on 
pharmacological families, which ULMS Semantic 
NetWork is unable to provide. 
2 The System 
The system consists of four main modules: a basic 
text processing module, WordNet look-up module, 
UMLS look-up module and the USAN rules 
module, as shown in Figure 1.  
A corpus of 90 medical abstracts was compiled for 
the experiment. For the basic processing of the 
abstracts, GATE3 architecture is used. This text 
processing provides sentence segmentation, 
tokenization and POS tagging. Tokens which 
receive a noun or proper noun POS tag are 
extracted. 
The nouns found on WordNet are discarded and 
those which are not found in WordNet are looked 
up in the UMLS Metathesaurus. If a noun is found 
in UMLS, it is tagged with its corresponding 
semantic types as assigned by UMLS. A subset of 
these nouns is tagged as ?drug? if their semantic 
types are ?Pharmacological Substance? or 
?Antibiotic?. Finally, nouns which have not been 
found in UMLS are tagged as ?unknown?. 
The list of nouns tagged as ?drug? is passed to the 
rule module to detect their pharmacological 
families according to the affixes. In addition, the 
rule module processes the list of ?unknown? nouns 
which are not found in UMLS to check the 
presence of affixes, and thereby, of possible drugs. 
3 Preliminary results 
                                                           
2 http://www.ama-
assn.org/ama1/pub/upload/mm/365/usan_stem_list.pdf 
Accessed January 2008 
3 http://www.gate.ac.uk/ 
100
A manual evaluation by a domain4 expert was 
carried out. The list of nouns not found in 
WordNet contained 1885 initial candidates. This 
initial list is looked up in UMLS and 93.4% of 
them (1761) is linked with some concepts of 
UMLS. The UMLS module recognized 1400 
nouns as pharmacological substances or 
antibiotics. The rest of nouns, 361, are detected by 
UMLS but neither as pharmacological substance 
nor as antibiotics.  
The expert manually evaluated the set of nouns 
detected by UMLS as pharmacological substances 
or antibiotics (1400). Evaluation showed that only 
1100 were valid drugs.  
 
Figure 1 System Architecture 
The list of nouns (124) which have not been found 
in UMLS are processed by the rule module to 
detect new candidate drugs not included in UMLS. 
This module only detects 17 candidate drugs. The 
manual evaluation showed that 7 of them were 
valid drugs and the rest of nouns are biomedical 
concepts not included in UMLS. Some of these 
drugs are Mideplanin, Tomopenem, Elvitegravir, 
and so on. The rest of nouns neither detected by 
the UMLS module nor by the rules module, 106, 
were also validated by the expert in order to 
estimate the overall coverage of our approach. The 
evaluation of these nouns shows that only 7 of 
them are valid drugs, however, the rest of the 
nouns are named entities of the general domain 
(organization, person names or cities) or 
biomedical concepts. Introducing a module of 
generic NER should decrease the noise caused by 
such entities.  
                                                           
4 The authors are grateful to Maria Bedmar Segura, Manager 
of the Drug Information Center, Mostoles University Hospital, 
for her valuable assistance in the evaluation of the system. 
Finally, precision and recall of the overall system 
combining UMLS and rules were calculated. The 
system achieved 78% of precision and 99.3% of 
recall  
3.1 The classification in pharmacological 
families 
Once processed by the rule module, 73.8% of the 
candidate drugs recognised by UMLS were also 
classified in pharmacological families by the 
USAN naming rules. Expert?s evaluation of the 
rule-based classification showed that rules 
achieved 89% precision. Short affixes such as ?ol, 
?pin and -ox are responsible of the wrong 
classifications. Thus, additional clues are necessary 
to detect these drug families. 
4 Some Conclusions  
As a preliminary approach, it is a first step towards 
a useful Information Extraction System in the field 
of Pharmacology. Though evaluation reveals that 
rules alone are not feasible enough in detecting 
drugs, but they help to improve the coverage. In 
addition, rules provide a drug classification in 
pharmacological families. Such classification is an 
added value in the development of NLP 
applications within the pharmacological domain.  
For future work, the approach will be extended to 
address additional information about 
pharmacologic classes included in many 
biomedical terminologies integrated in the UMLS 
such as MeSH or SNOMED. 
Future work will also target a wider coverage and a 
bigger set of drug types through including more 
affixes, detecting complex entities (multi-words), 
detecting synonyms, resolving acronyms and 
ambiguities as well as using contextual information 
to disambiguate the correct semantic type of each 
term occurring in the texts.  
References  
Collier N, Takeuchi K. 2004. Comparison of characterlevel 
and part of speech features for name recognition in bio-
medical texts:423? 35. 
Rindflesch, T.C., Tanabe,L., Weinstein,J.N. and Hunter,L. 
2000. EDGAR: extraction of drugs, genes and relations 
from the biomedical literature. Pac. Symp. Biocomput. 5, 
517?528 
Tanabe, L. y Wilbur, W.J. 2002. Tagging gene and protein 
names in biomedical text. Bioinformatics 18, 1124?1132 
101
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 329?332,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
UC3M system: Determining the Extent, Type and Value of Time 
Expressions in TempEval-2 
 
 
Mar?a Teresa Vicente-D?ez, Juli?n Moreno Schneider, Paloma Mart?nez 
Department of Computer Science 
Universidad Carlos III de Madrid 
Avda. Universidad, 30 
Legan?s, 28911, Madrid, Spain. 
{tvicente, jmschnei, pmf}@inf.uc3m.es 
 
  
 
Abstract 
This paper describes the participation of 
Universidad Carlos III de Madrid in Task A of 
the TempEval-2 evaluation. The UC3M 
system was originally developed for the 
temporal expressions recognition and 
normalization (TERN task) in Spanish texts, 
according to the TIDES standard. Current 
version supposes an almost-total refactoring of 
the earliest system. Additionally, it has been 
adapted to the TimeML annotation schema 
and a considerable effort has been done with 
the aim of increasing its coverage. It takes a 
rule-based design both in the identification and 
the resolution phases. It adopts an inductive 
approach based on the empirical study of 
frequency of temporal expressions in Spanish 
corpora. Detecting the extent of the temporal 
expressions the system achieved a 
Precision/Recall of 0.90/0.87 whereas, in 
determining the TYPE and VALUE of those 
expressions, system results were 0.91 and 
0.83, respectively.  
1 Introduction 
The study of temporality in NLP is not a new 
task. However, in the last years it has witnessed a 
huge interest. Initiatives like TempEval task or 
the Automatic Context Extraction1 (ACE) TERN 
competitions have boosted research on the field 
and have promoted the development of new 
resources to the scientific community. 
There are two main advantages in 
participating in these evaluations. On the one 
                                                 
1
 Automatic Content Extraction Evaluation. National 
Institute of Standards and Technology (NIST) 
http://www.itl.nist.gov/iad/mig//tests/ace/ 
hand it is possible to measure the systems? 
performance under standardized metrics, sharing 
datasets and other resources. On the other hand, 
it is possible to make comparative evaluations 
among distinct participants looking forward the 
same objectives but using different approaches. 
Until recently, most of temporally annotated 
corpora, as well as temporal taggers, were 
available in English. Since languages as Spanish 
start to become prominent in the field it seems 
interesting the development of specific resources. 
Tempeval-2 has contributed to this target in a 
significant way thanks to the release of annotated 
corpora and the publication of specific guidelines 
(Sauri et al, 2009), (Saur? et al, 2010). 
This paper resumes the participation of the 
UC3M system in the task of determining the 
extent and resolving the value of time 
expressions in texts (Task A). This system was 
originally developed for the Spanish TERN task 
proposed in ACE 2007 evaluation (Vicente-D?ez 
et al, 2007), achieving encouraging results 
although it was in a early stage of development. 
The system follows a ruled-based approach 
whose knowledge base has been inducted from 
the study of annotated temporal corpora 
(Vicente-D?ez et al, 2008). A machine learning 
approach was initially discarded due to the 
limitation of annotated Spanish corpora. 
The aims of this work were to improve the 
coverage of the original system and test its 
performance against new available datasets with 
a view to its integration in future domains of 
application. Main challenges were to move to a 
new temporal model where interval is considered 
as the basic time unit as well as the isolation of 
the internal representation of temporal 
information from the annotation schema.  
329
This paper is organized as follows: Section 2 
describes the system operation; Section 3 
presents experimentation and results; conclusions 
and future work are discussed in Section 4. 
2 System Description 
The UC3M system recognizes and annotates 
temporal expressions in texts based on a 
linguistic rules engine for Spanish language. 
Our system is divided into three different 
parts: recognition of temporal expressions, 
normalization of the detections, and annotation 
of the temporal expressions according to the 
TimeML schema.  
Following the definition of the Task A, the 
system is able to determine not only the extent of 
the temporal expressions but also the value of the 
features TYPE and VAL. It differentiates among 
the four TYPE values (dates, durations, sets and 
times) thanks to the classification of the 
recognition rules. The system straightforwardly 
provides a VAL attribute that accomplishes the 
format defined by TIMEX2 and TIMEX3 
standards through its internal model for 
representing time.  
2.1 Recognition 
The recognizer detects temporal expressions by 
means of a set of linguistic rules, focusing on 
those which are most frequent in Spanish.  
We adopted an empirical inductive approach 
through the analysis of the different types of 
temporal expressions in news corpora, and we 
could outline a typology of most common time 
expressions in the language. The typology 
together with the patterns that define these 
expressions form up the knowledge base for a 
successful automatic identification and resolution 
of temporal expressions. 
The rule engine allows managing different sets 
of rules independently of the target. In this case, 
the rules have been created attending to each 
pattern that is likely to match a temporal 
expression. Each rule determines the set of 
tokens that form an expression, the normalization 
type to be applied and the expression type. 
In Table 1 an example of a rule to identify 
dates is shown. The first line represents the name 
of the rule. The second line specifies the 
normalization method that will be used once the 
expression is recognized. The third line specifies 
the type of the temporal expression and the 
annotation pattern. Finally, the fourth line shows 
the tokens that trigger the rule.  
1. TEMPORAL_RULE(r1.3) 
2. TEMPORAL_ANALYSIS_NORMALIZATION_ 
TYPE=(abs_dia_mes_anio_3) 
3. TEMPORAL_ANALYSIS_TYPE= 
(date:init:YYYY-MM-DD) 
4. RULE= 
[[el/_] [DIC(DIASEMANA)/_] [dia/_] DIC(DIA) de 
DIC(MES) DIC(PREP) METHOD(year)] 
Table 1 Rule definition example 
The operation of the system is described as 
follows: first, the text is parsed token by token. 
Then, for each token, every rule is checked to 
find out if it triggers through a given token and 
the following ones. 
This operation implies that the higher the 
number of rules, the slower the text processing. 
The disadvantage of the processing speed has 
been accepted as a design criterion for the sake 
of the simplicity of creating new rules.  
2.2 Normalization 
The temporal expression normalization is done 
as an intermediate step between recognition and 
annotation, isolating the extraction of semantics 
from the annotation schema while trying to 
facilitate the second step. 
Normalization is important since recognized 
time expressions are managed and returned in a 
standard format that avoids semantic 
ambiguities.  
UC3M system applies an interval-based 
temporal normalization. It means that every 
temporal expression is represented as an interval 
with two boundaries: an initial and a final date 
(including time). This approach is motivated by 
the belief that the use of intervals as a basic time 
unit leads to a lower loss of semantics. For 
instance, when an expression like ?en enero? (?in 
January?) is detected, current task proposes the 
annotation ?2010-01?. However, we think that 
for many applications that are likely to use this 
system it would be more useful to have the 
complete interval that the expression refers 
(?2010-01-01 - 2010-01-31?). Through a set of 
procedures (as getting the length of a given 
month), our system tries to define the interval 
boundaries as much as possible. Every 
normalized expression is made up of two dates 
although it refers to a concrete date or time. 
In the internal representation model 
normalized dates and times adopts the ISO-8601 
form, durations are captured as a length related 
to the unit of measure, and sets are managed in a 
similar way to durations, adding quantity and 
frequency modifiers. 
330
The normalization process is dependent on the 
rule used to recognize each expression. For each 
new rule added to the engine a new 
normalization clause is needed. 
In Table 2 some temporal expression 
normalization examples are presented: 
Expression Init Date Final Date 
18 de abril de 2005 
18th of April of 2005 20050418 20050418 
mayo de 1999 
May of 1999 19990501 19990531 
en 1975 
in 1975 19750101 19751231 
el pr?ximo mes 
next month 20100501 20100531 
Table 2 Interval-based normalization sample 
2.3 Annotation 
The annotation process starts from the 
normalized form of the temporal expression. The 
system implements a transformation procedure 
based on patterns. This transformation is 
dependent on the temporal expression type. 
Dates: when dealing with dates, the VAL 
value is extracted from the initial boundary of the 
interval in accordance with the annotation pattern 
defined in the corresponding rule (see Table 1). 
Some examples are shown in Table 3. 
Expression Norm. Init Date Pattern VAL 
mayo de 1999 
May of 1999 19990501 YYYY-MM 1999-05 
la semana  
pasada 
last week 
20100405 YYYY-WXX 2010-W14 
los a?os 80 
the 80?s 19800101 YYY 198 
Table 3 Annotation patterns for dates 
Durations: the model represents durations by 
capturing the length of action as a quantity. This 
quantity is stored in the position of the initial 
boundary whose granularity corresponds with the 
unit of measure. The annotation patterns indicate 
the granularity to be considered (Table 4). 
Expression Norm. Init Date Pattern VAL 
4 a?os 
4 a?os 00040000 PXY P4Y 
4 meses, 3 
d?as y 2 
horas 
4 moths,3 
days and 2 
hours 
00040003- 
02:00:00 COMBINED P4M3DT2H 
Table 4 Annotation patterns for durations 
Sets are managed similarly to durations. In this 
case also frequency and quantity modifiers are 
captured internally together with the interval 
representation, so that the transformation is 
immediate. 
Expression Norm. Init Date Pattern VAL FREQ QUANT 
cada 2 a?os 
each 2 
years 
00020000 
F1QEv PXY P2Y 1x EVERY 
2 veces al 
d?a 
twice a day 
00000001 
F2QEv PXD P1D 2x EVERY 
Table 5 Annotation patterns for sets 
Times: the representation model allows 
capturing hours, minutes, seconds and 
milliseconds if they are specified. Similarly to 
the annotation of dates, VAL value is obtained of 
the information in the initial boundary in the way 
the pattern determines (Table 6). 
Expression Norm. Init Date Pattern VAL 
a las 12:30 PM 
at 12:30 PM 
20100405 
12:30:00 THXMX 
2010-04-
05T12H30M 
por la tarde 
in the evening 
20100405 
12:00:00 TDP 2010-04-05TAF 
Table 6 Annotation patterns for times 
3 Experiments and Results 
Precision and recall and f-measure are used as 
evaluation metrics according to the evaluation 
methodology (Pustejovsky et al, 2009). To 
determine the quality of annotation, results are 
completed with figures concerning to the 
resolution of TYPE and VAL attributes. 
Before evaluation, the system was tested on 
the training corpus and, once the test datasets 
were released, it was tested on the corpus for 
relations detection (tasks C-F) since it contained 
both files "timex-extents.tab" and "timex-
attributes.tab". The results are shown in Table 7. 
Timex Extent Timex Attbs. Corpus P R F TYPE VAL 
Training 0.93 0.67 0.78 0.87 0.82 
Relation-Test 0.89 0.63 0.74 0.86 0.83 
Table 7 Results on training corpus 
In Table 8 results of final evaluation are 
presented and compared with the other 
participants? figures for the same task and 
language. Since the test corpora were not 
aligned, further comparisons for different 
languages have not been proposed. 
Our system achieved a precision rate of 90% 
and a recall of 87%, being the f-measure of 88%. 
Thus, it supposes a significant improvement over 
our earlier work. In more, determining the value 
of TIMEX3 attributes the system raises good 
331
figures, obtaining the best VAL score, what 
means that normalization is working well. 
Timex Extent Timex Attrbs. Team P R F TYPE VAL 
UC3M 0.90 0.87 0.88 0.91 0.83 
TIPSem 0.95 0.87 0.91 0.91 0.78 
TIPSem-B 0.97 0.81 0.88 0.99 0.75 
Table 8 Results on test corpus 
Analyzing the experimental errors several 
facts can be highlighted: 
The percentage of expressions completely and 
correctly recognized and normalized is good but 
there are some missing expressions, mainly due 
to their complexity (or fuzziness) and to the 
absence of a rule to manage them, i.e.: ?durante 
un largo periodo? (during a long period).  
Errors in determining the extent of the 
temporal expressions were mainly due to the 
inclusion of prepositions or articles that precede 
to the kernel of the expression, i.e.: ?a corto 
plazo? vs. ?corto plazo? (in short term). 
A number of false positives were due to some 
inconsistencies in the annotation of the corpus. 
An example has been observed in fuzzy time 
expressions that denotes a future reference: ?el 
pr?ximo t?cnico? (the next trainer) (not 
annotated) vs. ?el pr?ximo preparador? (the next 
coach) (FUTURE_REF)  
Although normalization figures are good, 
some annotations are incorrect if their resolution 
implies context-aware mechanisms. 
4 Conclusions and Future Work 
In this paper a rule based approach for 
automatically detecting and annotating temporal 
expressions according to TimeML TIMEX3 tag 
has been presented. It is based on an empirical 
study of temporal expressions frequencies in 
Spanish that provides the main recognition rules 
of the knowledge base. At the normalization 
stage, a representation model based on intervals 
has been adopted with the aim of capturing most 
semantics. The annotation process relies on 
patterns that distinguish among different types 
and granularities of the expressions to be tagged. 
Obtained results suppose a significant 
improvement over our previous work. Part of this 
success is due to the specific annotation 
guidelines for Spanish that have been released 
with occasion of the TempEval-2. It is a helpful 
tool to optimize the system performance, since 
each language has its own peculiarities that 
should be taken into account. The promotion of a 
common framework and the development of 
resources like specific corpora are also very 
interesting topics to boost research in the field, 
since both comparative and standardized 
evaluation of the systems are needed. 
Several aspects should be taken into account 
in future versions of the system. In order to 
improve the recall new knowledge must be 
incorporated to the rule engine. That supposes 
the addition of new rules and annotation patterns. 
This objective includes the implementation of 
dictionaries with a broader coverage of 
translatable temporal expressions, such as 
holidays, festivities, etc. 
We will also explore context extraction 
techniques that facilitate the resolution of 
context-aware temporal expressions. 
Another pending issue is the enlargement of 
the system to span the detection of events and the 
relations among events and time expressions. 
Finally, the system will be integrated into a 
NLP application that benefits from the temporal 
information management. We want to check the 
improvement that the extraction of temporal 
entities supposes on a traditional approach. 
 
Acknowledgments 
This work has been partially supported by the 
Research Network MAVIR (S-0505/TIC-0267), 
and project BRAVO (TIN2007-67407-C03-01). 
References  
James Pustejovsky, Marc Verhagen, Xue Nianwen, 
Robert Gaizauskas, Mark Hepple, Frank Schilder, 
Graham Katz, Roser Saur?, Estela Saquete, 
Tommaso Caselli, Nicoletta Calzolari, Kiyong Lee, 
and Seohyun Im. 2009. TempEval2: Evaluating 
Events, Time Expressions and Temporal Relations. 
SemEval Task Proposal. 
Mar?a Teresa Vicente-D?ez, Doaa Samy and Paloma 
Mart?nez. 2008. An empirical approach to a 
preliminary successful identification and resolution 
of temporal expressions in Spanish news corpora. 
In Proceedings of the LREC'08. 
Mar?a Teresa Vicente-D?ez, C?sar de Pablo-S?nchez 
and Paloma Mart?nez. Evaluaci?n de un Sistema de 
Reconocimiento y Normalizaci?n de Expresiones 
Temporales en Espa?ol. Procesamiento del 
lenguaje natural. N. 39 pp. 113-120, Sept. 2007. 
Roser Saur?, Estela Saquete and James Pustejovsky. 
2010. Annotating Time Expressions in Spanish. 
TimeML Annotation Guidelines. Version 
TempEval-2010.  
Roser Saur?, Olga Batiukova, James Pustejovsky. 
2009. Annotating Events in Spanish. TimeML 
Annotation Guidelines. Version TempEval-2010. 
332
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 341?350, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 9 : Extraction of Drug-Drug Interactions from
Biomedical Texts (DDIExtraction 2013)
Isabel Segura-Bedmar, Paloma Mart??nez, Mar??a Herrero-Zazo
Universidad Carlos III de Madrid
Av. Universidad, 30, Legane?s 28911, Spain
{isegura,pmf}@inf.uc3m.es, mhzazo@pa.uc3m.es
Abstract
The DDIExtraction 2013 task concerns the
recognition of drugs and extraction of drug-
drug interactions that appear in biomedical
literature. We propose two subtasks for the
DDIExtraction 2013 Shared Task challenge:
1) the recognition and classification of drug
names and 2) the extraction and classification
of their interactions. Both subtasks have been
very successful in participation and results.
There were 14 teams who submitted a total of
38 runs. The best result reported for the first
subtask was F1 of 71.5% and 65.1% for the
second one.
1 Introduction
The definition of drug-drug interaction (DDI) is
broadly described as a change in the effects of one
drug by the presence of another drug (Baxter and
Stockely, 2010). The detection of DDIs is an im-
portant research area in patient safety since these in-
teractions can become very dangerous and increase
health care costs. Drug interactions are frequently
reported in journals, making medical literature the
most effective source for their detection (Aronson,
2007). Therefore, Information Extraction (IE) can
be of great benefit in the pharmaceutical industry al-
lowing identification and extraction of relevant in-
formation on DDIs and providing an interesting way
of reducing the time spent by health care profession-
als on reviewing the literature.
The DDIExtraction 2013 follows up on a
first event organized in 2011, DDIExtraction
2011 (Segura-Bedmar et al, 2011b) whose main
goal was the detection of drug-drug interactions
from biomedical texts. The new edition includes in
addition to DDI extraction also a supporting task,
the recognition and classification of pharmacologi-
cal substances. DDIExtraction 2013 is designed to
address the extraction of DDIs as a whole, but di-
vided into two subtasks to allow separate evaluation
of the performance for different aspects of the prob-
lem. The shared task includes two challenges:
? Task 9.1: Recognition and classification of
pharmacological substances.
? Task 9.2: Extraction of drug-drug interactions.
Additionally, while the datasets used for
the DDIExtraction 2011 task were composed
by texts describing DDIs from the DrugBank
database(Wishart et al, 2006), the new datasets for
DDIExtraction 2013 also include MedLine abstracts
in order to deal with different types of texts and
language styles.
This shared task has been conceived with a dual
objective: advancing the state-of-the-art of text-
mining techniques applied to the pharmacological
domain, and providing a common framework for
evaluation of the participating systems and other re-
searchers interested in the task.
In the next section we describe the DDI corpus
used in this task. Sections 3 and 4 focus on the de-
scription of the task 9.1 and 9.2 respectively. Finally,
Section 5 draws the conclusions and future work.
2 The DDI Corpus
The DDIExtraction 2013 task relies on the DDI cor-
pus, which is a semantically annotated corpus of
341
documents describing drug-drug interactions from
the DrugBank database and MedLine abstracts on
the subject of drug-drug interactions.
The DDI corpus consists of 1,017 texts (784
DrugBank texts and 233 MedLine abstracts) and
was manually annotated with a total of 18,491 phar-
macological substances and 5,021 drug-drug inter-
actions (see Table 1). A detailed description of the
method used to collect and process documents can
be found in (Segura-Bedmar et al, 2011a). The cor-
pus is distributed in XML documents following the
unified format for PPI corpora proposed by Pyysalo
et al, (2008) (see Figure 1). A detailed description
and analysis of the DDI corpus and its methodology
are included in an article currently under review by
BioInformatics journal.1
The corpus was split in order to build the datasets
for the training and evaluation of the different par-
ticipating systems. Approximately 77% of the DDI
corpus documents were randomly selected for the
training dataset and the remaining (142 DrugBank
texts and 91MedLine abstracts) was used for the test
dataset. The training dataset is the same for both
subtasks since it contains entity and DDI annota-
tions. The test dataset for the task 9.1 was formed by
discarding documents which contained DDI annota-
tions. Entity annotations were removed from this
dataset to be used by participants. The remaining
documents (that is, those containing some interac-
tion) were used to create the test dataset for task 9.2.
Since entity annotations are not removed from these
documents, the test dataset for the task 9.2 can also
be used as additional training data for the task 9.1.
3 Task 9.1: Recognition and classification
of pharmacological substances.
This task concerns the named entity extraction of
pharmacological substances in text. This named en-
tity task is a crucial first step for information ex-
traction of drug-drug interactions. In this task, four
types of pharmacological substances are defined:
drug (generic drug names), brand (branded drug
names), group (drug group names) and drug-n (ac-
tive substances not approved for human use). For a
1M. Herrero-Zazo, I. Segura-Bedmar, P. Mart??nez. 2013.
The DDI Corpus: an annotated corpus with pharmacological
substances and drug-drug interactions, submitted to BioInfor-
matics
Training Test for task 9.1 Test for task 9.2
D
D
I-
D
ru
gB
an
k
documents 572 54 158
sentences 5675 145 973
drug 8197 180 1518
group 3206 65 626
brand 1423 53 347
drug n 103 5 21
mechanism 1260 0 279
effect 1548 0 301
advice 819 0 215
int 178 0 94
D
D
I-
M
ed
L
in
e
documents 142 58 33
sentences 1301 520 326
drug 1228 171 346
group 193 90 41
brand 14 6 22
drug n 401 115 119
mechanism 62 0 24
effect 152 0 62
advice 8 0 7
int 10 0 2
Table 1: Basic statistics on the DDI corpus.
more detailed description, the reader is directed to
our annotation guidelines.2
For evaluation, a part of the DDI corpus consist-
ing of 52 documents from DrugBank and 58 Med-
Line abstracts, is provided with the gold annota-
tion hidden. The goal for participating systems is to
recreate the gold annotation. Each participant sys-
tem must output an ASCII list of reported entities,
one per line, and formatted as:
IdSentence|startOffset-endOffset|text|type
Thus, for each recognized entity, each line must
contain the id of the sentence where this entity ap-
pears, the position of the first character and the one
of the last character of the entity in the sentence, the
text of the entity, and its type. When the entity is a
discontinuous name (eg. aluminum and magnesium
hydroxide), this second field must contain the start
and end positions of all parts of the entity separated
by semicolon. Multiple mentions from the same sen-
tence should appear on separate lines.
3.1 Evaluation Metrics
This section describes the methodology that is used
to evaluate the performance of the participating sys-
tems in task 9.1.
The major forums of the Named Entity Recogni-
tion and Classification (NERC) research community
(such as MUC-7 (Chinchor and Robinson, 1997),
CoNLL 2003 (Tjong Kim Sang and De Meulder,
2003) or ACE07 have proposed several techniques
to assess the performance of NERC systems. While
2http://www.cs.york.ac.uk/semeval-2013/task9/
342
Figure 1: Example of an annotated document of the DDI corpus.
Team Affiliation Description
Ta
sk
9.
1
LASIGE(Grego et al, 2013) University of Lisbon, Portugal Conditional random fields
NLM LHC National Library of Medicine, USA Dictionary-based approach
UEM UC3M(Sanchez-Cisneros and Aparicio, 2013) European U. of Madrid, Carlos III University of Madrid, Spain Ontology-based approach
UMCC DLSI(Collazo et al, 2013) Matanzas University, Cuba j48 classifier
UTurku(Bjo?rne et al, 2013) University of Turku, Finland SVM classifier (TEES system)
WBI NER(Rockta?schel et al, 2013) Humboldt University of Berlin, Germany Conditional random fields
Ta
sk
9.
2
FBK-irst (Chowdhury and Lavelli, 2013c) FBK-irst, Italy hybrid kernel + scope of negations and semantic roles
NIL UCM(Bokharaeian, 2013) Complutense University of Madrid, Spain SVM classifier (Weka SMO)
SCAI(Bobic? et al, 2013) Fraunhofer SCAI, Germany SVM classifier (LibLINEAR)
UC3M(Sanchez-Cisneros, 2013) Carlos III University of Madrid, Spain Shallow Linguistic Kernel
UCOLORADO SOM(Hailu et al, 2013) University of Colorado School of Medicine, USA SVM classifier (LIBSVM)
UTurku(Bjo?rne et al, 2013) University of Turku, Finland SVM classifier (TEES system)
UWM-TRIADS(Rastegar-Mojarad et al, 2013) University of Wisconsin-Milwaukee, USA Two-stage SVM
WBI DDI(Thomas et al, 2013) Humboldt University of Berlin, Germany Ensemble of SVMs
Table 2: Short description of the teams.
ACE evaluation is very complex because its scores
are not intuitive, MUC and CoNLL 2003 used the
standard precision/recall/f-score metrics to compare
their participating systems. The main shared tasks in
the biomedical domain have continued using these
metrics to evaluate the outputs of their participant
teams.
System performance should be scored automat-
ically by how well the generated pharmacological
substance list corresponds to the gold-standard an-
notations. In our task, we evaluate the results of
the participating systems according to several evalu-
ation criteria. Firstly, we propose a strict evaluation,
which does not only demand exact boundary match,
but also requires that both mentions have the same
entity type. We are aware that this strict criterion
may be too restrictive for our overall goal (extrac-
tion of drug interactions) because it misses partial
matches, which can provide useful information for
a DDI extraction system. Our evaluation metrics
should score if a system is able to identify the ex-
act span of an entity (regardless of the type) and if
it is able to assign the correct entity type (regardless
of the boundaries). Thus, our evaluation script will
output four sets of scores according to:
1. Strict evaluation (exact-boundary and type
matching).
2. Exact boundary matching (regardless to the
type).
3. Partial boundary matching (regardless to the
type).
4. Type matching (some overlap between the
tagged entity and the gold entitity is required).
Evaluation results are reported using the standard
precision/recall/f-score metrics. We refer the reader
to (Chinchor and Sundheim, 1993) for a more de-
tailed description of these metrics.
These metrics are calculated over all entities and
on both axes (type and span) in order to evaluate
the performance of each axe separately. The final
score is the micro-averaged F-measure, which is cal-
culated over all entity types without distinction. The
main advantage of the micro-average F1 is that it
343
takes into account all possible types of errors made
by a NERC system.
Additionally, we calculate precision, recall and f-
measure for each entity type and then their macro-
average measures are provided. Calculating these
metrics for each entity type allows us to evalu-
ate the level of difficulty of recognizing each en-
tity type. In addition to this, since not all entity
types have the same frequency, we can better as-
sess the performance of the algorithms proposed by
the participating systems. This is mainly because
the results achieved on the most frequent entity type
have a much greater impact on overall performance
than those obtained on the entity types with few in-
stances.
3.2 Results and Discussion
Participants could send a maximum of three system
runs. After downloading the test datasets, they had
a maximum of two weeks to upload the results. A
total of 6 teams participated, submitting 16 system
runs. Table 2 lists the teams, their affiliations and
a brief description of their approaches. Due to the
lack of space we cannot describe them in this paper.
Tables 3, 4 and 5 show the F1 scores for each run in
alphabetic order. The reader can find the full ranking
information on the SemEval-2013 Task 9 website3.
The best results were achieved by the WBI
team with a conditional random field. They em-
ployed a domain-independent feature set alng
with features generated from the output of
ChemSpot (Rockta?schel et al, 2012), an existing
chemical named entity recognition tool, as well as
a collection of domain-specific resources. Its model
was trained on the training dataset as well as on en-
tities of the test dataset for task 9.2. The second
top best performing team developed a dictionary-
based approach combining biomedical resources
such as DrugBank, the ATC classification system,4
or MeSH,5 among others. Regarding the classifi-
cation of each entity type, we observed that brand
drugs were easier to recognize than the other types.
This could be due to the fact that when a drug is mar-
keted by a pharmaceutical company, its brand name
is carefully selected to be short, unique and easy to
3http://www.cs.york.ac.uk/semeval-2013/task9/
4http://www.whocc.no/atc ddd index/
5http://www.ncbi.nlm.nih.gov/mesh
remember (Boring, 1997). On the other hand, sub-
stances not approved for human use (drug-n) were
more difficult, due to the greater variation and com-
plexity in their naming. In fact, the UEM UC3M
team was the only team who obtained an F1 measure
greater than 0 on the DDI-DrugBank dataset. Also,
this may indicate that this type is less clearly defined
than the others in the annotation guidelines. Another
possible reason is that the presence of such sub-
stances in this dataset is very scarce (less than 1%).
It is interesting that almost every participating sys-
temwas better in detecting and classifying entities of
a particular class compared to all other systems. For
instance, on the whole dataset the dictionary-based
system from NLM LHC had it strengths at drug en-
tities, UEM UC3M at drug N entities, UTurku at
brand entities and WBI NER at group entities.
Finally, the results on the DDI-DrugBank dataset
are much better than those obtained on the DDI-
MedLine dataset. While DDI-DrugBank texts focus
on the description of drugs and their interactions, the
main topic of DDI-MedLine texts would not neces-
sarily be on DDIs. Coupled with this, it is not al-
ways trivial to distinguish between substances that
should be classified as pharmacological substances
and those who should not. This is due to the ambi-
guity of some pharmacological terms. For example,
insulin is a hormone produced by the pancreas, but
can also be synthesized in the laboratory and used
as drug to treat insulin-dependent diabetes mellitus.
The participating systems should be able to deter-
mine if the text is describing a substance originated
within the organism or, on the contrary, it describes a
process in which the substance is used for a specific
purpose and thus should be identified as pharmaco-
logical substance.
4 Task 9.2: Extraction of drug-drug
interactions.
The goal of this subtask is the extraction of drug-
drug interactions from biomedical texts. However,
while the previous DDIExtraction 2011 task focused
on the identification of all possible pairs of inter-
acting drugs, DDIExtraction 2013 also pursues the
classification of each drug-drug interaction accord-
ing to one of the following four types: advice, ef-
fect, mechanism, int. A detailed description of these
344
Team Run Rank STRICT EXACT PARTIAL TYPE DRUG BRAND GROUP DRUG N MAVG
LASIGE
1 6 0,656 0,781 0,808 0,69 0,741 0,581 0,712 0,171 0,577
2 9 0,639 0,775 0,801 0,672 0,716 0,541 0,696 0,182 0,571
3 10 0,612 0,715 0,741 0,647 0,728 0,354 0,647 0,16 0,498
NLM LHC
1 4 0,698 0,784 0,801 0,722 0,803 0,809 0,646 0 0,57
2 3 0,704 0,792 0,807 0,726 0,81 0,846 0,643 0 0,581
UMCC DLSI 1,2,3 14,15,16 0,275 0,3049 0,367 0,334 0,297 0,313 0,257 0,124 0,311
UEM UC3M
1 13 0,458 0,528 0,585 0,51 0,718 0,075 0,291 0,185 0,351
2 12 0,529 0,609 0,669 0,589 0,752 0,094 0,291 0,264 0,38
UTurku
1 11 0,579 0,639 0,719 0,701 0,721 0,603 0,478 0,016 0,468
2 8 0,641 0,659 0,731 0,766 0,784 0,901 0,495 0,015 0,557
3 7 0,648 0,666 0,743 0,777 0,783 0,912 0,485 0,076 0,604
WBI
1 5 0,692 0,772 0,807 0,729 0,768 0,787 0,761 0,071 0,615
2 2 0,708 0,831 0,855 0,741 0,786 0,803 0,757 0,134 0,643
3 1 0,715 0,833 0,856 0,748 0,79 0,836 0,776 0,141 0,652
Table 3: F1 scores for task 9.1 on the whole test dataset (DDI-MedLine + DDI-DrugBank). (MAVG for macro-
average). Each run is ranked by STRICT performance.
Team Run Rank STRICT EXACT PARTIAL TYPE DRUG BRAND GROUP DRUG N MAVG
LASIGE
1 8 0,771 0,834 0,855 0,799 0,817 0,571 0,833 0 0,563
2 9 0,771 0,831 0,852 0,799 0,823 0,553 0,824 0 0,568
3 11 0,682 0,744 0,764 0,713 0,757 0,314 0,756 0 0,47
NLM LHC
1 2 0,869 0,902 0,922 0,902 0,909 0,907 0,766 0 0,646
2 3 0,869 0,903 0,919 0,896 0,911 0,907 0,754 0 0,644
UMCC DLSI 1,2,3 14,15,16 0,424 0,4447 0,504 0,487 0,456 0,429 0,371 0 0,351
UEM UC3M
1 13 0,561 0,632 0,69 0,632 0,827 0,056 0,362 0,022 0,354
2 12 0,595 0,667 0,721 0,667 0,842 0,063 0,366 0,028 0,37
UTurku
1 10 0,739 0,753 0,827 0,864 0,829 0,735 0,553 0 0,531
2 6 0,785 0,795 0,863 0,908 0,858 0,898 0,559 0 0,581
3 7 0,781 0,787 0,858 0,905 0,847 0,911 0,551 0 0,578
WBI
1 5 0,86 0,877 0,9 0,89 0,905 0,857 0,782 0 0,636
2 4 0,868 0,894 0,914 0,897 0,909 0,865 0,794 0 0,642
3 1 0,878 0,901 0,917 0,908 0,912 0,904 0,806 0 0,656
Table 4: F1 scores for task 9.1 on the DDI-DrugBank test data. (MAVG for macro-average). Each run is ranked by
STRICT performance.
Team Run Rank STRICT EXACT PARTIAL TYPE DRUG BRAND GROUP DRUG N MAVG
LASIGE
1 4 0,567 0,74 0,772 0,605 0,678 0,667 0,612 0,183 0,577
2 8 0,54 0,733 0,763 0,576 0,631 0,444 0,595 0,196 0,512
3 6 0,557 0,693 0,723 0,596 0,702 0,667 0,56 0,171 0,554
NLM LHC
1 5 0,559 0,688 0,702 0,575 0,717 0,429 0,548 0 0,462
2 3 0,569 0,702 0,715 0,586 0,726 0,545 0,555 0 0,486
UMCC DLSI 1,2,3 14,15,16 0,187 0,2228 0,287 0,245 0,2 0,091 0,191 0,13 0,23
UEM UC3M
1 13 0,39 0,461 0,516 0,431 0,618 0,111 0,238 0,222 0,341
2 11 0,479 0,564 0,628 0,529 0,665 0,182 0,233 0,329 0,387
UTurku
1 12 0,435 0,538 0,623 0,556 0,614 0,143 0,413 0,016 0,328
2 10 0,502 0,528 0,604 0,628 0,703 0,923 0,436 0,016 0,533
3 9 0,522 0,551 0,634 0,656 0,716 0,923 0,426 0,08 0,582
WBI
1 7 0,545 0,681 0,726 0,589 0,634 0,353 0,744 0,074 0,479
2 2 0,576 0,779 0,807 0,612 0,673 0,444 0,729 0,14 0,534
3 1 0,581 0,778 0,805 0,617 0,678 0,444 0,753 0,147 0,537
Table 5: F1 scores for task 9.1 on the DDI-MedLine test data. (MAVG for macro-average). Each run is ranked by
STRICT performance.
345
types can be found in our annotation guidelines6.
Gold standard annotations (correct, human-
created annotations) of pharmacological substances
are provided to participants both for training and test
data. The test data for this subtask consists of 158
DrugBank documents and 33 MedLine abstracts.
Each participant system must output an ASCII list
including all pairs of drugs in each sentence, one per
line (multiple DDIs from the same sentence should
appear on separate lines), its prediction (1 if the pair
is a DDI and 0 otherwise) and its type (label null
when the prediction value is 0), and formatted as:
IdSentence|IdDrug1|IdDrug2|prediction|type
4.1 Evaluation Metrics
Evaluation is relation-oriented and based on the
standard precision, recall and F-score metrics. A
DDI is correctly detected only if the system is able
to assign the correct prediction label and the correct
type to it. In other words, a pair is correct only if
both prediction and type are correct. The perfor-
mance of systems to identify those pairs of drugs
interacting (regardless of the type) is also evaluated.
This allows us to assess the progress made with re-
gard to the previous edition, which only dealt with
the detection of DDIs.
Additionally, we are interested in assessing which
drug interaction types are most difficult to detect.
Thus, we calculate precision, recall and F1 for each
DDI type and then their macro-average measures are
provided. While micro-averaged F1 is calculated
by constructing a global contingency table and then
calculating precision and recall, macro-averaged F-
score is calculated by first calculating precision and
recall for each type and then taking the average of
these results.
Evaluating each DDI type separately allows us to
assess the level of difficulty of detecting and classi-
fying each type of interaction. Additionally, it is im-
portant to note that the scores achieved on the most
frequent DDI type have a much greater impact on
overall performance than those achieved on the DDI
types with few instances. Therefore, by calculating
scores for each type of DDI, we can better assess
the performance of the algorithms proposed by the
6http://www.cs.york.ac.uk/semeval-2013/task9/
participating systems.
4.2 Results and Discussion
The task of extracting drug-drug interactions from
biomedical texts has attracted the participation of 8
teams (see Table 2) who submitted 22 runs. Tables 6,
7 and 8 show the results for each run in alphabetic
order. Due to the lack of space, the performance
information is only shown in terms of F1 score. The
reader can find the full ranking information on the
SemEval-2013 Task 9 website7.
Most of the participating systems were built on
support vector machines. In general, approaches
based on non-linear kernels methods achieved better
results than linear SVMs. As in the previous edition
of DDIExtraction, most systems have used primarily
syntactic information. However, semantic informa-
tion has been poorly used.
The best results were submitted by the team from
FBK-irst. They applied a novel hybrid kernel based
RE approach described in Chowdhury (2013a).
They also exploited the scope of negations and
semantic roles for negative instance filtering as
proposed in (Chowdhury and Lavelli, 2013b) and
(Chowdhury and Lavelli, 2012). The second best
results were obtained by the WBI team from the
Humboldt University of Berlin. Its system com-
bines several kernel methods (APG (Airola et al,
2008) and Shallow Linguistic Kernel (SL) (Giuliano
et al, 2006) among others), the Turku Event Ex-
traction system (TEES) (Bjo?rne et al, 2011)8 and
the Moara system (Neves et al, 2009). These two
teams were also the top two ranked teams in DDIEx-
traction 2011. For a more detailed description, the
reader is encouraged to read the papers of the partic-
ipants in the proceedings book.
While the DDIExtraction 2011 shared task con-
centrated efforts on the detection of DDIs, this new
DDIExtraction 2013 task involved not only the de-
tection of DDIs, but also their classification. Al-
though the results of DDIExtraction 2011 are not di-
rectly comparable with the ones reported in DDIEx-
traction 2013 due to the use of different training and
test datasets in each edition, it should be noted that
there has been a significant improvement in the de-
7http://www.cs.york.ac.uk/semeval-2013/task9/
8http://jbjorne.github.io/TEES/
346
Team Run Rank CLA DEC MEC EFF ADV INT MAVG
FBK-irst
1 3 0.638 0.8 0.679 0.662 0.692 0.363 0.602
2 1 0.651 0.8 0.679 0.628 0.692 0.547 0.648
3 2 0.648 0.8 0.627 0.662 0.692 0.547 0.644
NIL UCM
1 12 0.517 0.588 0.515 0.489 0.613 0.427 0.535
2 10 0.548 0.656 0.531 0.556 0.61 0.393 0.526
SCAI
1 14 0.46 0.69 0.446 0.459 0.562 0.02 0.423
2 16 0.452 0.683 0.441 0.44 0.559 0.021 0.448
3 15 0.458 0.704 0.45 0.462 0.54 0.02 0.411
UC3M
1 11 0.529 0.676 0.48 0.547 0.575 0.5 0.534
2 21 0.294 0.537 0.268 0.286 0.325 0.402 0.335
UCOLORADO SOM
1 22 0.214 0.492 0.109 0.25 0.219 0.097 0.215
2 20 0.334 0.504 0.361 0.311 0.381 0.333 0.407
3 19 0.336 0.491 0.335 0.313 0.42 0.329 0.38
UTurku
1 9 0.581 0.684 0.578 0.585 0.606 0.503 0.572
2 7 0.594 0.696 0.582 0.6 0.63 0.507 0.587
3 8 0.582 0.699 0.569 0.593 0.608 0.511 0.577
UWM-TRIADS
1 17 0.449 0.581 0.413 0.446 0.502 0.397 0.451
2 13 0.47 0.599 0.446 0.449 0.532 0.421 0.472
3 18 0.432 0.564 0.442 0.383 0.537 0.292 0.444
WBI
1 6 0.599 0.736 0.602 0.604 0.618 0.516 0.588
2 5 0.601 0.745 0.616 0.595 0.637 0.49 0.588
3 4 0.609 0.759 0.618 0.61 0.632 0.51 0.597
Table 6: F1 scores for Task 9.2 on the whole test dataset (DDI-MedLine + DDI-DrugBank). DEC for Detection, CLA
for detection and classification, MEC for mechanism type, EFF for effect type, ADV for advice type, INT for int type
and MAVG for macro-average. Each run is ranked by CLA performance.
Team Run Rank CLA DEC MEC EFF ADV INT MAVG
FBK-irst
1 3 0.663 0.827 0.705 0.699 0.705 0.376 0.624
2 1 0.676 0.827 0.705 0.664 0.705 0.545 0.672
3 2 0.673 0.827 0.655 0.699 0.705 0.545 0.667
NIL UCM
1 12 0.54 0.615 0.527 0.525 0.625 0.444 0.565
2 10 0.573 0.68 0.552 0.597 0.619 0.408 0.55
SCAI
1 15 0.464 0.711 0.449 0.459 0.57 0.021 0.461
2 16 0.463 0.71 0.445 0.458 0.569 0.021 0.46
3 14 0.473 0.734 0.468 0.482 0.551 0.021 0.439
UC3M
1 11 0.555 0.703 0.493 0.593 0.59 0.51 0.561
2 21 0.306 0.549 0.274 0.302 0.334 0.426 0.352
UCOLORADO SOM
1 22 0.218 0.508 0.115 0.251 0.24 0.098 0.228
2 20 0.341 0.518 0.373 0.313 0.398 0.344 0.425
3 19 0.349 0.511 0.353 0.324 0.429 0.327 0.394
UTurku
1 8 0.608 0.712 0.6 0.63 0.617 0.522 0.6
2 7 0.62 0.724 0.605 0.644 0.638 0.522 0.614
3 9 0.608 0.726 0.591 0.635 0.617 0.522 0.601
UWM-TRIADS
1 17 0.462 0.596 0.43 0.459 0.509 0.405 0.463
2 13 0.485 0.616 0.467 0.466 0.536 0.425 0.486
3 18 0.445 0.573 0.469 0.39 0.544 0.29 0.46
WBI
1 6 0.624 0.762 0.621 0.645 0.634 0.52 0.61
2 5 0.627 0.775 0.636 0.636 0.652 0.5 0.611
3 4 0.632 0.783 0.629 0.652 0.65 0.513 0.617
Table 7: F1 scores for task 9.2 on the DDI-DrugBank test dataset. Each run is ranked by CLA performance.
Team Run Rank CLA DEC MEC EFF ADV INT MAVG
FBK-irst
1 4 0.387 0.53 0.383 0.436 0.286 0.211 0.406
2 3 0.398 0.53 0.383 0.407 0.286 0.571 0.436
3 2 0.398 0.53 0.339 0.436 0.286 0.571 0.44
NIL UCM
1 20 0.19 0.206 0.286 0.186 0 0 0.121
2 19 0.219 0.336 0.143 0.271 0 0 0.11
SCAI
1 1 0.42 0.462 0.412 0.458 0.2 0 0.269
2 8 0.323 0.369 0.389 0.333 0 0 0.182
3 6 0.341 0.474 0.31 0.379 0.222 0 0.229
UC3M
1 15 0.274 0.406 0.333 0.267 0 0.364 0.268
2 22 0.186 0.421 0.222 0.171 0.143 0 0.149
UCOLORADO SOM
1 21 0.188 0.37 0.042 0.241 0 0 0.073
2 14 0.275 0.394 0.258 0.302 0.138 0 0.177
3 17 0.244 0.356 0.194 0.255 0.222 0.4 0.272
UTurku
1 18 0.242 0.339 0.258 0.256 0.2 0 0.18
2 16 0.262 0.344 0.214 0.278 0.364 0 0.224
3 13 0.286 0.376 0.286 0.289 0.333 0 0.232
UWM-TRIADS
1 10 0.312 0.419 0.233 0.36 0.267 0 0.219
2 9 0.319 0.436 0.233 0.34 0.421 0.333 0.345
3 11 0.306 0.479 0.247 0.326 0.381 0.333 0.33
WBI
1 7 0.336 0.456 0.368 0.344 0.154 0.4 0.334
2 12 0.304 0.406 0.343 0.318 0.167 0 0.209
3 5 0.365 0.503 0.476 0.347 0.143 0.4 0.353
Table 8: F1 scores for task 9.2 on the DDI-MedLine test dataset. Each run is ranked by CLA performance.
347
tection of DDIs: F1 has a remarkable increase from
65.74% (the best F1-score in DDIExtraction 2011)
to 80% (see DEC column of Table 6). The increase
of the size of the corpus made for DDIExtraction
2013 and of the quality of their annotations may
have contributed significantly to this improvement.
However, the results for the detection and classifi-
cation for DDIs did not exceed an F1 of 65.1%. Ta-
ble 6 suggests that some type of DDIs are more diffi-
cult to classify than others. The best F1 ranges from
69.2% for advice to 54.7% for int. One possible ex-
planation for this could be that recommendations or
advice regarding a drug interaction are typically de-
scribed by very similar text patterns such as DRUG
should not be used in combination with DRUG or
Caution should be observed when DRUG is admin-
istered with DRUG.
Regarding results for the int relationship, it should
be noted that the proportion of instances of this re-
lationship (5.6%) in the DDI corpus is much smaller
than those of the rest of the relations (41.1% for ef-
fect, 32.3% for mechanism and 20.9% for advice).
As stated earlier, one of the differences from
the previous edition is that the corpus developed
for DDIExtraction 2013 is made up of texts from
two different sources: MedLine and the DrugBank
database. Thus, the different approaches can be
evaluated on two different styles of biomedical texts.
While MedLine abstracts are usually written in ex-
tremely scientific language, texts from DrugBank
are written in a less technical form of the language
(similar to the language used in package inserts). In-
deed, this may be the reason why the results on the
DDI-DrugBank dataset are much better than those
obtained on the DDI-MedLine dataset (see Tables 7
and 8).
5 Conclusions
The DDIExtraction 2011 task concentrated efforts
on the novel aspects of the DDI extraction task, the
drug recognition was assumed and the annotations
for drugs were provided to the participants. This
new DDIExtraction 2013 task pursues the detec-
tion and classification of drug interactions as well
as the recognition and classification of pharmaco-
logical substances. The task attracted broad interest
from the community. A total of 14 teams from 7 dif-
ferent countries participated, submitted a total of 38
runs, exceeding the participation of DDIExtraction
2011 (10 teams). The participating systems demon-
strated substantial progress at the established DDI
extraction task on DrugBank texts and showed that
their methods also obtain good results for MedLine
abstracts.
The results that the participating systems have re-
ported show successful approaches to this difficult
task, and the advantages of non-linear kernel-based
methods over linear SVMs for extraction of DDIs.
In the named entity task, the participating systems
perform well in recognizing generic drugs, brand
drugs and groups of drugs, but they fail in recogniz-
ing active substances not approved for human use.
Although the results are positive, there is still much
room to improve in both subtasks. We have ac-
complished our goal of providing a framework and
a benchmark data set to allow for comparisons of
methods for the recognition of pharmacological sub-
stances and detection and classification of drug-drug
interactions from biomedical texts.
We would like that our test dataset can still serve
as the basis for fair and stable evaluation after the
task. Thus, we have decided that the full gold an-
notations for the test data are not available for the
moment. We plan to make available a web service
where researchers can test their methods on the test
dataset and compare their results with the DDIEx-
traction 2013 task participants.
Acknowledgments
This research work has been supported by the Re-
gional Government of Madrid under the Research
Network MA2VICMR (S2009/TIC-1542), by the
Spanish Ministry of Education under the project
MULTIMEDICA (TIN2010-20644-C03-01). Addi-
tionally, we would like to thank all participants for
their efforts and to congratulate them to their inter-
esting work.
References
A. Airola, S. Pyysalo, J. Bjorne, T. Pahikkala, F. Gin-
ter, and T. Salakoski. 2008. All-paths graph kernel
for protein-protein interaction extraction with evalu-
ation of cross-corpus learning. BMC bioinformatics,
9(Suppl 11):S2.
348
JK. Aronson. 2007. Communicating information about
drug interactions. British Journal of Clinical Pharma-
cology, 63(6):637?639, June.
K. Baxter and I.H. Stockely. 2010. Stockley?s drug inter-
actions.8th ed. London:Pharmaceutical Press.
J. Bjo?rne, J. Heimonen, F. Ginter, A. Airola, T. Pahikkala,
and T. Salakoski. 2011. Extracting contextualized
complex biological events with graph-based feature
sets. Computational Intelligence, 27(4):541?557.
J. Bjo?rne, S. Kaewphan, and T. Salakoski. 2013.
UTurku: Drug Named Entity Detection and Drug-drug
Interaction Extraction Using SVM Classification and
Domain Knowledge. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
T. Bobic?, J. Fluck, and M. Hofmann-Apitius. 2013.
SCAI: Extracting drug-drug interactions using a rich
feature vector. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013).
A. Bokharaeian, B.and D??az. 2013. NIL UCM: Extract-
ing Drug-Drug interactions from text through combi-
nation of sequence and tree kernels. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013).
D. Boring. 1997. The development and adop-
tion of nonproprietary, established, and proprietary
names for pharmaceuticals. Drug information journal,
31(3):621?634.
N. Chinchor and P. Robinson. 1997. Muc-7 named entity
task definition. In Proceedings of the 7th Conference
on Message Understanding.
N. Chinchor and B. Sundheim. 1993. Muc-5 evalua-
tion metrics. In Proceedings of the 5th conference on
Message understanding, pages 69?78. Association for
Computational Linguistics.
MFM. Chowdhury and A. Lavelli. 2012. Impact of
less skewed distributions on efficiency and effective-
ness of biomedical relation extraction. In Proceedings
of COLING 2012.
MFM. Chowdhury and A. Lavelli. 2013b. Exploiting
the scope of negations and heterogeneous features for
relation extraction: Case study drug-drug interaction
extraction. In Proceedings of NAACL 2013.
M.F.M. Chowdhury and A. Lavelli. 2013c. FBK-irst
: A Multi-Phase Kernel Based Approach for Drug-
Drug Interaction Detection and Classification that Ex-
ploits Linguistic Information. In Proceedings of the
7th International Workshop on Semantic Evaluation
(SemEval 2013).
MFM. Chowdhury. 2013a. Improving the Effectiveness
of Information Extraction from Biomedical Text. Ph.d.
dissertation, University of Trento.
A. Collazo, A. Ceballo, D Puig, Y. Gutie?rrez, J. Abreu,
J Pe?rez, A. Ferna?ndez-Orqu??n, A. Montoyo, R. Mun?oz,
and F. Camara. 2013. UMCC DLSI-(DDI): Seman-
tic and Lexical features for detection and classification
Drugs in biomedical texts. In Proceedings of the 7th
International Workshop on Semantic Evaluation (Se-
mEval 2013).
C. Giuliano, A. Lavelli, and L. Romano. 2006. Ex-
ploiting shallow linguistic information for relation ex-
traction from biomedical literature. In Proceedings of
the Eleventh Conference of the European Chapter of
the Association for Computational Linguistics (EACL-
2006), pages 401?408.
T. Grego, F. Pinto, and F.M. Couto. 2013. LASIGE: us-
ing Conditional Random Fields and ChEBI ontology.
In Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013).
N.D. Hailu, L.E. Hunter, and K.B. Cohen. 2013.
UColorado SOM: Extraction of Drug-Drug Interac-
tions from Biomedical Text using Knowledge-rich and
Knowledge-poor Features. In Proceedings of the 7th
International Workshop on Semantic Evaluation (Se-
mEval 2013).
ML. Neves, JM. Carazo, and A. Pascual-Montano. 2009.
Extraction of biomedical events using case-based rea-
soning. In Proceedings of the Workshop on BioNLP:
Shared Task, pages 68?76. Association for Computa-
tional Linguistics.
S. Pyysalo, A. Airola, J. Heimonen, J. Bjorne, F. Gin-
ter, and T. Salakoski. 2008. Comparative analysis of
five protein-protein interaction corpora. BMC bioin-
formatics, 9(Suppl 3):S6.
M. Rastegar-Mojarad, R. D. Boyce, and R. Prasad. 2013.
UWM-TRIADS: Classifying Drug-Drug Interactions
with Two-Stage SVM and Post-Processing. In Pro-
ceedings of the 7th International Workshop on Seman-
tic Evaluation (SemEval 2013).
T. Rockta?schel, M. Weidlich, and U. Leser. 2012.
Chemspot: a hybrid system for chemical named entity
recognition. Bioinformatics, 28(12):1633?1640.
T. Rockta?schel, T. Huber, M. Weidlich, and U. Leser.
2013. WBI-NER: The impact of domain-specific fea-
tures on the performance of identifying and classifying
mentions of drugs. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
D. Sanchez-Cisneros and F. Aparicio. 2013. UEM-
UC3M: An Ontology-based named entity recognition
system for biomedical texts. In Proceedings of the 7th
International Workshop on Semantic Evaluation (Se-
mEval 2013).
D. Sanchez-Cisneros. 2013. UC3M: A kernel-based ap-
proach for identify and classify DDIs in biomedical
349
texts. In Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013).
I. Segura-Bedmar, P. Mart??nez, and C. de Pablo-Sa?nchez.
2011a. Using a shallow linguistic kernel for drug-drug
interaction extraction. Journal of Biomedical Infor-
matics, 44(5):789 ? 804.
I. Segura-Bedmar, P. Mart?nez, and D. Sa?nchez-Cisneros.
2011b. The 1st ddiextraction-2011 challenge task:
Extraction of drug-drug interactions from biomedical
texts. In Proceedings of DDIExtraction-2011 chal-
lenge task, pages 1?9.
P. Thomas, M. Neves, T. Rockta?schel, and U. Leser.
2013. WBI-DDI: Drug-Drug Interaction Extraction
usingMajority Voting. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
E.F. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL 2003-Volume 4, pages 142?147.
Association for Computational Linguistics.
D.S. Wishart, C. Knox, A.C. Guo, S. Shrivastava,
M. Hassanali, P. Stothard, Z. Chang, and J. Woolsey.
2006. Drugbank: a comprehensive resource for in sil-
ico drug discovery and exploration. Nucleic acids re-
search, 34(suppl 1):D668?D672.
350
