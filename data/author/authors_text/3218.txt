Proceedings of NAACL HLT 2007, pages 460?467,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Combining Lexical and Grammatical Features to Improve Readability 
Measures for First and Second Language Texts 
Michael J. Heilman 
 
Kevyn Collins-
Thompson 
Jamie Callan 
 
Maxine Eskenazi 
 
Language Technologies Institute 
School of Computer Science 
Carnegie Mellon University 
4502 Newell Simon Hall 
Pittsburgh, PA 15213-8213 
 {mheilman,kct,callan,max}@cs.cmu.edu 
 
 
Abstract 
This work evaluates a system that uses in-
terpolated predictions of reading difficulty 
that are based on both vocabulary and 
grammatical features.  The combined ap-
proach is compared to individual gram-
mar- and language modeling-based 
approaches.  While the vocabulary-based 
language modeling approach outper-
formed the grammar-based approach, 
grammar-based predictions can be com-
bined using confidence scores with the 
vocabulary-based predictions to produce 
more accurate predictions of reading dif-
ficulty for both first and second language 
texts.  The results also indicate that gram-
matical features may play a more impor-
tant role in second language readability 
than in first language readability. 
1 Introduction 
The REAP tutoring system (Heilman, et al 2006), 
aims to provide authentic reading materials of the 
appropriate difficulty level, in terms of both vo-
cabulary and grammar, for English as a Second 
Language students.  An automatic measure of read-
ability that incorporated both lexical and gram-
matical features was thus needed. 
For first language (L1) learners (i.e., children 
learning their native tongue), reading level has 
been predicted using a variety of techniques, based 
on models of a student?s lexicon, grammatical sur-
face features such as sentence length (Flesch, 
1948), or combinations of such features (Schwarm 
and Ostendorf, 2005).  It was shown by Collins-
Thompson and Callan (2004) that a vocabulary-
based language modeling approach was effective at 
predicting the readability of grades 1 to 12 of Web 
documents of varying length, even with high levels 
of noise.   
Prior work on first language readability by 
Schwarm and Ostendorf (2005) incorporated 
grammatical surface features such as parse tree 
depth and average number of verb phrases.  This 
work combining grammatical and lexical features 
was promising, but it was not clear to what extent 
the grammatical features improved predictions.   
Also, discussions with L2 instructors suggest 
that a more detailed grammatical analysis of texts 
that examines features such as passive voice and 
various verb tenses can provide better features with 
which to predict reading difficulty.  One goal of 
this work is to show that the use of pedagogically 
motivated grammatical features (e.g., passive 
voice, rather than the number of words per sen-
tence) can improve readability measures based on 
lexical features alone. 
One of the differences between L1 and L2 read-
ability is the timeline and processes by which first 
and second languages are acquired.  First language 
acquisition begins at infancy, and the primary 
grammatical structures of the target language are 
acquired by age four in typically developing chil-
460
dren (Bates, 2003).  That is, most grammar is ac-
quired prior to the beginning of a child?s formal 
education.  Therefore, most grammatical features 
seen at high reading levels such as high school are 
present with similar frequencies at low reading 
levels such as grades 1-3 that correspond to ele-
mentary school-age children.  It should be noted 
that sentence length is one grammar-related differ-
ence that can be observed as L1 reading level in-
creases.  Sentences are kept short in texts for low 
L1 reading levels in order to reduce the cognitive 
load on child readers.  The average sentence length 
of texts increases with the age and reading level of 
the intended audience.  This phenomenon has been 
utilized in early readability measures (Flesch, 
1948).  Vocabulary change, however, continues 
even into adulthood, and has been shown to be a 
more effective predictor of L1 readability than 
simpler measures such as sentence length (Collins-
Thompson and Callan, 2005). 
Second language learners, unlike their L1 coun-
terparts, are still very much in the process of ac-
quiring the grammar of their target language.  In 
fact, even intermediate and advanced students of 
second languages, who correspond to higher L2 
reading levels, often struggle with the grammatical 
structures of their target language.  This phenome-
non suggests that grammatical features may play a 
more important role in predicting and measuring 
L2 readability.  That is not to say, however, that 
vocabulary cannot be used to predict L2 reading 
levels.  Second language learners are learning both 
vocabulary and grammar concurrently, and reading 
materials for this population are chosen or au-
thored according to both lexical and grammatical 
complexity.  Therefore, the authors predict that a 
readability measure for texts intended for second 
language learners that incorporates both grammati-
cal and lexical features could clearly outperform a 
measure based on only one of these two types of 
features. 
This paper begins with descriptions of the lan-
guage modeling and grammar-based prediction 
systems.  A description of the experiments follows 
that covers both the evaluation metrics and corpora 
used.  Experimental results are presented, followed 
by a discussion of these results, and a summary of 
the conclusions of this work.  
2 Language Model Readability Prediction 
for First Language Texts 
Statistical language modeling exploits patterns of 
use in language.  To build a statistical model of 
text, training examples are used to collect statistics 
such as word frequency and order.  Each training 
example has a label that tells the model the ?true? 
category of the example.  In this approach, one 
statistical model is built for each grade level to be 
predicted. 
The statistical language modeling approach has 
several advantages over traditional readability 
formulas, which are usually based on linear regres-
sion with two or three variables.  First, a language 
modeling approach generally gives much better 
accuracy for Web documents and short passages 
(Collins-Thompson and Callan, 2004).  Second, 
language modeling provides a probability distribu-
tion across all grade models, not just a single pre-
diction.  Third, language modeling provides more 
data on the relative difficulty of each word in the 
document.  This might allow an application, for 
example, to provide more accurate vocabulary as-
sistance. 
The statistical model used for this study is 
based on a variation of the multinomial Na?ve 
Bayes classifier.  For a given text passage T, the 
semantic difficulty of T relative to a specific grade 
level Gi is predicted by calculating the likelihood 
that the words of T were generated from a repre-
sentative language model of Gi.  This likelihood is 
calculated for each of a number of language mod-
els, corresponding to reading difficulty levels.  The 
reading difficulty of the passage is then estimated 
as the grade level of the language model most 
likely to have generated the passage T. 
The language models employed in this work are 
simple: they are based on unigrams and assume 
that the probability of a token is independent of the 
surrounding tokens.  A unigram language model is 
simply defined by a list of types (words) and their 
individual probabilities.  Although this is a weak 
model, it can be effectively trained from less la-
beled data than more complex models, such as bi-
gram or trigram models.  Additionally, higher 
order n-gram models might capture grammatical as 
well as lexical differences.  The relative contribu-
tions of grammatical and lexical features were thus 
better distinguished by using unigram language 
461
models that more exclusively focus on lexical dif-
ferences. 
In this language modeling approach, a genera-
tive model is assumed for a passage T, in which a 
hypothetical author generates the tokens of T by: 
1. Choosing a grade language model, Gi, 
from the set G = {Gi} of 12 unigram language 
models, according to a prior probability distri-
bution P(Gi). 
2. Choosing a passage length |T| in tokens ac-
cording to a probability distribution P(|T|). 
3. Sampling |T| tokens from Gi?s multinomial 
word distribution according to the ?na?ve? as-
sumption that each token is independent of all 
other tokens in the passage, given the language 
model Gi. 
These assumptions lead to the following expres-
sion for the probability of T being generated by 
language model Gi according to a multinomial dis-
tribution: 
 
?
?
=
Vw
wC
i
i
wC
GwPTTPGTP )!(
)|(|!||)(|)|(
)(
 
 
Next, according to Bayes? Theorem: 
  
)(
)|()()|(
TP
GTPGPTGP iii = . 
 
Substituting (1) into (2), taking logarithms, and 
simplifying produces: 
 
SRwC
GwPwCTGP
Vw
i
Vw
i
loglog)!(log
)|(log)()|(log
++?
=


?
?
, 
 
where V is the list of all types in the passage T, w is 
a type in V, and C(w) is the number of tokens with 
type w in T.  For simplicity, the factor R represents 
the contribution of the prior P(Gi), and S represents 
the contribution of the passage length |T|, given the 
grade level.   
Two further assumptions are made to simplify 
the illustration: 
1. That all grades are equally likely a priori.   
That is, 
G
i N
GP 1)( =  where NG is the number 
of grade levels.  For example, if there are 12 
grade levels, then NG = 12.  This allows log R to 
be ignored. 
2. That all passage lengths (up to a maximum 
length M) are equally likely.  This allows log S 
to be ignored. 
These may be poor assumptions in a real appli-
cation, but they can be easily included or excluded 
in the model as desired.  The log C(w)! term can 
also be ignored because it is constant across levels.  
Under these conditions, an extremely simple form 
for the grade likelihood remains.  In order to find 
which model Gi maximizes Equation (3), the 
model which Gi that maximizes the following 
equation must be found: 
 
)|(log)()|( i
Vw
i GwPwCGTL 
?
=  
 
This is straightforward to compute: for each token 
in the passage T, the log probability of the token 
according to the language model of Gi is calcu-
lated.  Summing the log probabilities of all tokens 
produces the overall likelihood of the passage, 
given the grade.  The grade level with the maxi-
mum likelihood is then chosen as the final read-
ability level prediction. 
This study employs a slightly more sophisti-
cated extension of this model, in which a sliding 
window is moved across the text, with a grade pre-
diction being made for each window.  This results 
in a distribution of grade predictions.  The grade 
level corresponding to a given percentile of this 
distribution is chosen as the prediction for the en-
tire document.  The values used in these experi-
ments for the percentile thresholds for L1 and L2 
were chosen by accuracy on held-out data. 
3 Grammatical Construction Readability 
Prediction for Second Language Texts 
The following sections describe the approach to 
predicting readability based on grammatical fea-
tures.  As with any classifier, two components are 
required to classify texts by their reading level: 
first, a definition for and method of identifying 
features; second, an algorithm for using these fea-
tures to classify a given text.  A third component, 
training data, is also necessary in this classification 
462
task.  The corpus of materials used for training and 
testing is discussed in a subsequent section. 
3.1 Features for Grammar-based Prediction 
L2 learners usually learn grammatical patterns ex-
plicitly from grammar explanations in L2 text-
books, unlike their L1 counterparts who learn them 
implicitly through natural interactions.  Grammati-
cal features would therefore seem to be an essential 
component of an automatic readability measure for 
L2 learners, who must actively acquire both the 
lexicon and grammar of their target language. 
The grammar-based readability measure relies 
on being able to automatically identify grammati-
cal constructions in text.  Doing so is a multi-step 
process that begins by syntactically parsing the 
document.  The Stanford Parser (Klein and Man-
ning, 2002) was used to produce constituent struc-
ture trees.  The choice of parser is not essential to 
the approach, although the accuracy of parsing 
does play a role in successful identification of cer-
tain grammatical patterns. PCFG scores from the 
parser were also used to filter out some of the ill-
formed text present in the test corpora.  The default 
training set of Penn Treebank (Marcus et al 1993) 
was used for the parser because the domain and 
style of those texts actually matches fairly well 
with the domain and style of the texts on which a 
reading level predictor for second language learn-
ers might be used. 
Once a document is parsed, the predictor uses 
Tgrep2 (Rohde, 2005), a tree structure searching 
tool, to identify instances of the target patterns.  A 
Tgrep2 pattern defines dominance, sisterhood, 
precedence, and other relationships between nodes 
in the parse tree for a sentence.  A pattern can also 
place constraints on the terminal symbols (e.g., 
words and punctuation), such that a pattern might 
require a form of the copula ?be? to exist in a cer-
tain position in the construction.  An example of a 
TGrep2 search pattern for the progressive verb 
tense is the following: 
 
?VP < /^VB/ < (VP < VBG)? 
 
Searching for this pattern returns sentences in 
which a verb phrase (VP) dominates an auxiliary 
verb (whose symbol begins with VB) as well as 
another verb phrase, which in turn dominates a 
verb in gerund form (VBG).  An example of a 
matching sentence is, ?The student was reading a 
book,? shown in Figure 2. 
 
Figure 2: The parse tree for an example sentence 
that matches a pattern for progressive verb tense. 
 
A set of 22 relevant grammatical constructions 
were identified from grammar textbooks for three 
different ESL levels (Fuchs et al, 2005).  These 
grammar textbooks had different authors and pub-
lishers than the ones used in the evaluation corpora 
in order to minimize the chance of experimental 
results not generalizing beyond the specific materi-
als employed in this study.  The ESL levels corre-
spond to the low-intermediate (hereafter, level 3), 
high-intermediate (level 4), and advanced (level 5) 
courses at the University of Pittsburgh?s English 
Language Institute.  The constructions identified in 
these grammar textbooks were then implemented 
in the form of Tgrep2 patterns.   
 
Feature  Lowest Level Highest Level 
Passive Voice 0.11 0.71 
Past Participle 0.28 1.63 
Perfect Tense 0.01 0.33 
Relative Clause 0.54 0.60 
Continuous 
Tense 
0.19 0.27 
Modal 0.80 1.44 
Table 1: The rates of occurrence per 100 words of 
a few of the features used by the grammar-based 
predictor.  Rates are shown for the lowest (2) and 
highest (5) levels in the L2 corpus. 
 
The rate of occurrence of constructions was 
calculated on a per word basis.  A per-word rather 
a book 
The student 
S 
VP 
VBD VP 
VBG 
NP 
was 
reading 
NP 
463
than a per-sentence measure was chosen because a 
per-sentence measure would depend too greatly on 
sentence length, which also varies by level.  It was 
also desirable to avoid having sentence length con-
founded with other features.  Table 1 shows that 
the rates of occurrence of certain constructions be-
come more frequent as level increases.  This sys-
tematic variation across levels is the basis for the 
grammar-based readability predictions. 
A second feature set was defined that consisted 
of 12 grammatical features that could easily be 
identified without computationally intensive syn-
tactic parsing.  These features included sentence 
length, the various verb forms in English, includ-
ing the present, progressive, past, perfect, continu-
ous tenses, as well as part of speech labels for 
words.  The goal of using a second feature set was 
to examine how dependent prediction quality was 
on a specific set of features, as well as to test the 
extent to which the output of syntactic parsing 
might improve prediction accuracy. 
3.2 Algorithm for Grammatical Feature-
based Classification 
A k-Nearest Neighbor (kNN) algorithm is used for 
classification based on the grammatical features 
described above.  The kNN algorithm is an in-
stance-based learning technique originally devel-
oped by Cover and Hart (1967) by which a test 
instance is classified according to the classifica-
tions of a given number (k) of training instances 
closest to it.  Distance is defined in this work as the 
Euclidean distance of feature vectors.  Mitchell 
(1997) provides more details on the kNN algo-
rithm.  This algorithm was chosen because it has 
been shown to be effective in text classification 
tasks when compared to other popular methods 
(Yang 1999).  A k value of 12 was chosen because 
it provided the best performance on held-out data. 
Additionally, it is straightforward to calculate 
a confidence measure with which kNN predictions 
can be combined with predictions from other clas-
sifiers?in this case with predictions from the uni-
gram language modeling-based approach described 
above.  A confidence measure was important in 
this task because it provided a means with which to 
combine the grammar-based predictions with the 
predictions from the language modeling-based 
predictor while maintaining separate models for 
each type of feature.  These separate models were 
maintained to better determine the relative contri-
butions of grammatical and lexical features. 
A static linear interpolation of predictions us-
ing the two approaches led to only minimal reduc-
tions of prediction error, likely because predictions 
from the poorer performing grammar-based classi-
fier were always given the same weight.  However, 
with the confidence measures, predictions from the 
grammar-based classifier could be given more 
weight when the confidence measure was high, and 
less weight when the measure was low and the 
predictions were likely to be inaccurate.  The case-
dependent interpolation of prediction values al-
lowed for the effective combination of language 
modeling- and grammar-based predictions.  
The confidence measure employed is the pro-
portion of the k most similar training examples, or 
nearest neighbors, that agree with the final label 
chosen for a given test document.  For example, if 
seven of ten neighbors have the same label, then 
the confidence score will be 0.6.  The interpolated 
readability prediction value is calculated as fol-
lows: 
 
LI = LLM + CkNN * LGR, 
 
where LLM is the language model-based prediction, 
LGR is the grammar-based prediction from the kNN 
algorithm, and CkNN is the confidence value for the 
kNN prediction.  The language modeling approach 
is treated as a black box, but it would likely be 
beneficial to have confidence measures for it as 
well. 
4 Descriptions of Experiments 
This section describes the experiments used to test 
the hypothesis that grammar-based features can 
improve readability measures for English, espe-
cially for second language texts.  The measures 
and cross-validation setup are described.  A de-
scription of the evaluation corpora of labeled first 
and second language texts follows. 
4.1 Experimental Setup 
Two measurements were used in evaluating the 
effectiveness of the reading level predictions.  
First, the correlation coefficient evaluated whether 
the trends of prediction values matched the trends 
for human-labeled texts.  Second, the mean 
squared error of prediction values provided a 
464
measure of how correct each of the predictors was 
on average,  penalizing more severe errors more 
heavily.  Mean square error was used rather than 
simple accuracy (i.e., number correct divided by 
sample size) because the task of readability predic-
tion is more akin to regression than classification.  
Evaluation measures such as accuracy, precision, 
and recall are thus less meaningful for readability 
prediction tasks because they do not capture the 
fact that an error of 4 levels is more costly than an 
error of a single level. 
A nine-fold cross-validation was employed.  
The data was first split into ten sets.  One set was 
used as held-out data for selecting the parameter k 
for the kNN algorithm and the percentile value for 
the language modeling predictor, and then the re-
maining nine were used to evaluate the quality of 
predictions.  Each of these nine was in turn se-
lected as the test set, and the other eight were used 
as training data. 
4.2 Corpora of Labeled Texts 
Two corpora of labeled texts were used in the 
evaluation.  The first corpus was from a set of texts 
gathered from the Web for a prior evaluation of the 
language modeling approach.  The 362 texts had 
been assigned L1 levels (1-12) by grade school 
teachers, and consisted of approximately 250,000 
words.  For more details on the L1 corpus, see 
(Collins-Thompson and Callan, 2005). 
The second corpora consisted of textbook mate-
rials (Adelson-Goldstein and Howard, 2004, for 
level 2; Ediger and Pavlik, 2000, for levels 3 and 4; 
Silberstein, 2002, for level 5) from a series of Eng-
lish as a Second Language reading courses at the 
English Language Institute at the University of 
Pittsburgh.  The four reading practice textbooks 
that constitute this corpus were from separate au-
thors and publishers than the grammar textbooks 
used to select and define grammatical features.  
The reading textbooks in the corpus are used in 
courses intended for beginning (level 2) through 
advanced (level 5) students.  The textbooks were 
scanned into electronic format, and divided into 
fifty roughly equally sized files.  This second lan-
guage corpus consisted of approximately 200,000 
words. 
Although the sources and formats of the two 
corpora were different, they share a number of 
characteristics.  Their size was roughly equal. The 
documents in both were also fairly but not per-
fectly evenly distributed across the levels.  Both 
corpora also contained a significant amount of 
noise which made accurate prediction of reading 
level more challenging.  The L1 corpus was from 
the Web, and therefore contained navigation 
menus, links, and the like.  The texts in the L2 cor-
pus also contained significant levels of noise due to 
the inclusion of directions preceding readings, ex-
ercises and questions following readings, as well as 
labels on figures and charts.  The scanned files 
were not hand-corrected in this study, in part to test 
that the measures are robust to noise, which is pre-
sent in the Web documents for which the readabil-
ity measures are employed in the REAP tutoring 
system.  
The grammar-based prediction seems to be 
more significantly negatively affected by the noise 
in the two corpora because the features rely more 
on dependencies between different words in the 
text.  For example, if a word happened to be part of 
an image caption rather than a well-formed sen-
tence, the unigram language modeling approach 
would only be affected for that word, but the 
grammar-based approach might be affected for 
features spanning an entire clause or sentence. 
5 Results of Experiments 
The results show that for both the first and sec-
ond language corpora, the language modeling 
(LM) approach alone produced more accurate pre-
dictions than the grammar-based approach alone.  
The mean squared error values (Table 2) were 
lower, and the correlation coefficients (Table 3) 
were higher for the LM predictor than the gram-
mar-based predictor.   
The results also indicate that while grammar-
based predictions are not as accurate as the vo-
cabulary-based scores, they can be combined with 
vocabulary-based scores to produce more accurate 
interpolated scores.  The interpolated predictions 
combined by using the kNN confidence measure 
were slightly and in most tests significantly more 
accurate in terms of mean squared error than the 
predictions from either single measure.   Interpola-
tion using the first set of grammatical features led 
to 7% and 22% reductions in mean squared error 
on the L1 and L2 corpora, respectively.  These re-
sults were verified using a one-tailed paired t-test 
465
of the squared error values of the predictions, and 
significance levels are indicated in Table 2. 
 
Mean Squared Error Values 
Test Set (Num. Levels) L1(12) L2(4) 
Language Modeling 5.02 0.51 
Grammar 10.27 1.08 
Interpolation 4.65* 0.40** 
Grammar2 (feature set #2) 12.77 1.26 
Interp2. (feature set #2) 4.73 0.43* 
Table 2.  Comparison of Mean Squared Error of 
predictions compared to human labels for different 
methods.  Interpolated values are significantly bet-
ter compared to language modeling predictions 
where indicated (* = p<0.05, ** = p<0.01). 
 
Correlation Coefficients 
Test Set (Num. Levels) L1(12) L2(4) 
Language Modeling 0.71 0.80 
Grammar 0.46 0.55 
Interpolation 0.72 0.83 
Grammar2 (feature set #2) 0.34 0.48 
Interp2. (feature set #2) 0.72 0.81 
Table 3.  Comparison of Correlation Coefficients 
of prediction values to human labels for different 
prediction methods. 
 
The trends were similar for both sets of gram-
matical features.  However, the first set of features 
that included complex syntactic constructs led to 
better performance than the second set, which in-
cluded only verb tenses, part of speech labels, and 
sentence length.  Therefore, when syntactic parsing 
is not feasible because of corpora size, it seems 
that grammatical features requiring only part-of-
speech tagging and word counts may still improve 
readability predictions.  This is practically impor-
tant because parsing can be too computationally 
intensive for large corpora. 
All prediction methods performed better, in 
terms of correlations, on the L2 corpus than on the 
L1 corpus.  The L2 corpus is somewhat smaller in 
size and should, if only on the basis of training ma-
terial available to the prediction algorithms, actu-
ally be more difficult to predict than the L1 corpus.  
To ensure that the range of levels was not causing 
the four-level L2 corpus to have higher predictions 
than the twelve-level L1 corpus, the L1 corpus was 
also divided into four bins (grades 1-3, 4-6, 7-9, 
10-12).  The accuracy of predictions for the binned 
version of the L1 corpus was not substantially dif-
ferent than for the 12-level version. 
6 Discussion 
In the experimental tests, the LM approach was 
more effective for measuring both L1 and L2 read-
ability.  There are several potential causes of this 
effect.  First, the language modeling approach can 
utilize all the words as they appear in the text as 
features, while the grammatical features were cho-
sen and defined manually.  As a result, the LM 
approach can make measurements on a text for as 
many features as there are words in its lexicon.  
Additionally, the noise present in the corpora likely 
affected the grammar-based approach dispropor-
tionately more because that method relies on accu-
rate parsing of relationships between words. 
Additionally, English is a morphologically im-
poverished language compared to most languages.  
Text classification, information retrieval, and many 
other human language technology tasks can be ac-
complished for English without accounting for 
grammatical features such as morphological inflec-
tions.  For example, an information retrieval sys-
tem can perform reasonably well in English 
without performing stemming, which does not 
greatly increase performance except when queries 
and documents are short (Krovetz, 1993). 
However, most languages have a rich morphol-
ogy by which a single root form may have thou-
sands or perhaps millions of inflected or derived 
forms.  Language technologies must account for 
morphological features in such languages or the 
vocabulary grows so large that it becomes unman-
ageable.  Lee (2004), for example, showed that 
morphological analysis can improve the quality of 
statistical machine translation for Arabic.  Thus it 
seems that grammatical features could contribute 
even more to measures of readability for texts in 
other languages. 
That said, the use of grammatical features ap-
pears to play a more important role in readability 
measures for L2 than for L1.  When interpolated 
with grammar-based scores, the reduction of mean 
squared error over the language modeling approach 
for L1 was only 7%, while for L2 the reduction or 
squared error was 22%.  An evaluation on corpora 
with less noise would likely bring out these differ-
466
ences further and show grammar to be an even 
more important factor in second language readabil-
ity.  This result is consistent with the fact that sec-
ond language learners are still in the process of 
acquiring the basic grammatical constructs of their 
target language. 
7 Conclusion 
The results of this work suggest that grammatical 
features can play a role in predicting reading diffi-
culty levels for both first and second language texts 
in English.  Although a vocabulary-based language 
modeling approach outperformed the grammar-
based predictor, an interpolated measure using 
confidence scores for the grammar-based predic-
tions showed improvement over both individual 
measures.  Also, grammar appears to play a more 
important role in second language readability than 
in first language readability.  Ongoing work aims 
to improve grammar-based readability by reducing 
noise in training data, automatically creating larger 
grammar feature sets, and applying more sophisti-
cated modeling techniques. 
8 Acknowledgements 
We would like to acknowledge Lori Levin for use-
ful advice regarding grammatical constructions, as 
well as the anonymous reviewers for their sugges-
tions.  
This material is based on work supported by 
NSF grant IIS-0096139 and Dept. of Education 
grant R305G03123. Any opinions, findings, con-
clusions or recommendations expressed in this ma-
terial are the authors', and do not necessarily reflect 
those of the sponsors. 
References 
J. Adelson-Goldstein and L. Howard. 2004.  Read and 
Reflect 1.  Oxford University Press, USA. 
E. Bates. 2003. On the nature and nurture of language. 
In R. Levi-Montalcini, D. Baltimore, R. Dulbecco, F. 
Jacob, E. Bizzi, P. Calissano, & V. Volterra (Eds.), 
Frontiers of biology: The brain of Homo sapiens (pp. 
241?265). Rome: Istituto della Enciclopedia Italiana 
fondata da Giovanni Trecanni. 
M. Fuchs, M. Bonner, M. Westheimer. 2005.  Focus on 
Grammar, 3rd Edition.  Pearson ESL. 
K. Collins-Thompson and J. Callan. 2004.  A language 
modeling approach to predicting reading difficulty.  
Proceedings of the HLT/NAACL Annual Conference. 
T. Cover and P. Hart. 1967.  Nearest neighbor pattern 
classification.  IEEE Transactions on Information 
Theory, 13, 21-27. 
A. Ediger and C. Pavlik.  2000.  Reading Connections 
Intermediate.  Oxford University Press, USA. 
A. Ediger and C. Pavlik.  2000.  Reading Connections 
High Intermediate.  Oxford University Press, USA. 
M. Heilman, K. Collins-Thompson, J. Callan & M. Es-
kenazi. 2006. Classroom success of an Intelligent Tu-
toring System for lexical practice and reading 
comprehension. Proceedings of the Ninth Interna-
tional Conference on Spoken Language Processing. 
D. Klein and C. D. Manning. 2002. Fast Exact Inference 
with a Factored Model for Natural Language Parsing. 
Advances in Neural Information Processing Systems 
15 (NIPS 2002), December 2002. 
R. Krovetz. 1993. Viewing morphology as an inference 
process. SIGIR-93, 191?202. 
Y. Lee. 2004.  Morphological Analysis for Statistical 
Machine Translation.  Proceedings of the 
HLT/NAACL Annual Conference. 
M. Marcus, B. Santorini and M. Marcinkiewicz. 1993. 
"Building a large annotated corpus of English: the 
Penn Treebank." Computational Linguistics, 19(2). 
T. Mitchell. 1997. Machine Learning.  The McGraw-
Hill Companies, Inc.  pp. 231-236. 
D. Rohde. 2005. Tgrep2 User Manual. 
http://tedlab.mit.edu/~dr/Tgrep2/tgrep2.pdf. 
S. Schwarm, and M. Ostendorf. 2005.  Reading Level 
Assessment Using Support Vector Machines and Sta-
tistical Language Models.  Proceedings of the Annual 
Meeting of the Association for Computational Lin-
guistics. 
S. Silberstein, B. K. Dobson, and M. A. Clarke. 2002. 
Reader's Choice, 4th edition.  University of Michigan 
Press/ESL. 
Y. Yang. 1999.  A re-examination of text categorization 
methods.  Proceedings of ACM SIGIR Conference on 
Research and Development in Information Retrieval 
(SIGIR'99, pp 42--49). 
467
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 71?79,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
An Analysis of Statistical Models and Features for Reading Difficulty
Prediction
Michael Heilman, Kevyn Collins-Thompson and Maxine Eskenazi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{mheilman,kct,max}@cs.cmu.edu
Abstract
A reading difficulty measure can be described
as a function or model that maps a text to a
numerical value corresponding to a difficulty
or grade level. We describe a measure of read-
ability that uses a combination of lexical fea-
tures and grammatical features that are derived
from subtrees of syntactic parses. We also
tested statistical models for nominal, ordinal,
and interval scales of measurement. The re-
sults indicate that a model for ordinal regres-
sion, such as the proportional odds model, us-
ing a combination of grammatical and lexical
features is most effective at predicting reading
difficulty.
1 Introduction
A reading difficulty, or readability, measure can be
described as a function or model that maps a text
to a numerical value corresponding to a difficulty or
grade level. Inputs to this function are usually statis-
tics for various lexical and grammatical features of
the text. The output is one of a set of ordered dif-
ficulty levels, usually corresponding to grade levels
for elementary school through high school. As such,
reading difficulty prediction can be viewed as a re-
gression of grade level on a set of textual features.
Early work on readability measures employed
simple proxies for grammatical and lexical complex-
ity, including sentence length and the number of syl-
lables in a word. Fairly simple features were often
employed because of a lack of computational power.
Such features exhibit high bias because they rely on
strong assumptions about what makes a text difficult
to read. For example, the use of sentence length as a
measure of grammatical complexity assumes that a
longer sentence is more grammatically complex than
a shorter one, which is often but not always the case.
In one early model, the Dale-Chall model (Dale and
Chall, 1948; Chall and Dale, 1995), reading diffi-
culty is a linear function of the mean sentence length
and the percentage of rare words, as defined by a list
of 3,000 words commonly known by 4th grade. In
this paper, sentence length is defined as the mean
number of words in the sentences of a text.
Many early measures did not employ direct esti-
mates of word frequency due to computational lim-
itations (e.g., (Gunning, 1952; McLaughlin, 1969;
Kincaid et al, 1975)). Instead, these measures relied
on the strong relationship between the frequency of
and the number of syllables in a word. More fre-
quent words are more likely to have fewer syllables
(e.g., ?the?) than less frequent words (e.g., ?vocab-
ulary?), an association that is related to Zipf?s Law
(Zipf, 1935). The Flesch-Kincaid measure (Kincaid
et al, 1975) is probably the most common reading
difficulty measure in use. It is implemented in com-
mon word processing programs. This measure is a
linear function of the mean number of syllables per
word and the mean number of words per sentence.
Klare (1974) provides a summary of other early
work on readability.
More recent approaches to reading difficulty em-
ploy more sophisticated models that make use of the
growth in computational power. The Lexile Frame-
work (e.g., (Stenner, 1996)) uses individual word
frequency estimates as a measure of lexical diffi-
culty. The word frequency estimates are derived
71
from a large, varied corpus of text. Lexile uses a
Rasch model (Rasch, 1980) with the mean log word
frequency as a lexical feature and the log of the mean
sentence length as a grammatical feature. The Rasch
model, related to logistic regression, is used to esti-
mate the level of a student that would comprehend
75% of a given text. The converted log odds ratio
called a ?Lexile? that is used as part of this measure
can be easily mapped to grade school levels.
A reading difficulty measure developed by
Collins-Thompson and Callan (2005) uses
smoothed unigram language modeling to capture
the predictive ability of individual words based
on their frequency at each reading difficulty level.
Collins-Thompson and Callan found that certain
words were very predictive of certain levels. For
example, ?grownup? was very predictive of grade
1, and ?essay? was very predictive of grade 12. For
a given text, this measure estimates the likelihood
that the text was generated by each level?s language
model. The prediction is the level of the model with
the highest likelihood of generating the text. There
are no grammatical features.
Natural language processing techniques enable
more sophisticated grammatical analysis for read-
ing difficulty measures. Rather than using sentence
length as a proxy, measures can employ tools for au-
tomatic analysis of the syntactic structure of texts
(e.g., (Charniak, 2000)). A measure by Schwarm
and Ostendorf (2005) incorporates syntactic analy-
ses, among a variety of other types of features. It in-
cludes four grammatical features derived from syn-
tactic parses of text: the mean parse tree height, the
mean number of noun phrases, mean number of verb
phrases, and mean number of ?SBARs.? ?SBARs?
are non-terminal nodes that are associated with sub-
ordinate clauses. Their system led to better pre-
dictions than the Flesch-Kincaid and Lexile mea-
sures, but the predictive value of the grammatical
features is not entirely clear. In initial experiments
using such course-grain grammatical features alone,
rather than in conjunction with language modeling
and other features as in Schwarm and Ostendorf?s
system, we found relatively poor prediction perfor-
mance. Our final approach using subtrees of syn-
tactic parses allows for a finer level of discrimina-
tion that may support the detection of differences in
grade levels between texts that exhibit the same high
level features.
A reading difficulty measure developed by Heil-
man, Collins-Thompson, Callan, and Eskenazi
(2007) uses the frequency of grammatical construc-
tions as a measure of grammatical difficulty. A set
of approximately twenty constructions were selected
from English as a Second Language grammar text-
books. This set includes grammatical constructions
such as the passive voice, relative clauses, and vari-
ous verb tenses. The frequencies are used as features
for a nearest neighbor classification algorithm. The
unigram language modeling approach of Collins-
Thompson and Callan (2005) is used to estimate
lexical difficulty in this measure. The final predic-
tion is a linear function of the lexical and grammat-
ical components. That model assumes that gram-
matical difficulty is adequately captured by a small
number of constructions chosen according to de-
tailed knowledge of English grammar. In that work,
the constructions were selected from an English as
a Second Language grammar textbook, a labor- and
knowledge-intensive task that may be less practical
for other languages.
We aim to identify the appropriate scale of mea-
surement for reading difficulty?nominal, ordinal, or
interval?by comparing the effectiveness of statistical
models for each type of data. We also extend pre-
vious work combining lexical and grammatical fea-
tures (Heilman et al, 2007) by making it possible
to include a large number of grammatical features
derived from syntactic structures without requiring
significant linguistic or pedagogical content knowl-
edge, such as a reference guide for the grammar of
the language of interest.
2 Types of Features
2.1 Lexical Features
This section and the following section describe the
lexical and grammatical features used in our read-
ing difficulty models. The lexical features are the
relative frequencies of word unigrams. The use of
word unigrams is a standard approach in text clas-
sification (Yang and Pedersen, 1997), and has also
been successfully used to predict reading difficulty
(Collins-Thompson and Callan, 2005). Higher order
n-grams such as bigrams and trigrams were not used
as features because they did not improve predictions
72
in preliminary tests. The specific set of lexical fea-
tures was chosen based on the frequencies of words
in the training corpus. The system performs mor-
phological stemming and stopword removal. The
remaining 5000 most common words comprised the
lexical feature set.
2.2 Grammatical Features
Grammatical features are extracted from automatic
context-free grammar parses of sentences. The sys-
tem computes relative frequencies of partial syn-
tactic derivations, which will be called ?subtrees?
hereafter. The approach extends (Heilman et al,
2007), where frequencies of manually defined syn-
tactic patterns were extracted from syntactic struc-
tures. In that approach, the features are defined man-
ually using linguistic knowledge of the target lan-
guage to implement tree search patterns, a labor- and
knowledge-intensive process. The approach advo-
cated in this paper, however, extracts frequencies for
an automatically defined set of subtree patterns. The
system considers all subtrees up to a given depth that
occur in the training corpus. Examples of grammati-
cal features at levels 0 through 2 are shown in Figure
1. The sentence for the parse tree shown was taken
from a third grade text.
For depth 0, the system includes all subtrees con-
sisting of just nonterminal nodes. This includes all
parts of speech, as well as non-terminal nodes for
noun phrases, adjective phrases, clauses, etc. For
depth 1, the system includes subtrees corresponding
to the application of a single context free grammar
rule in the derivation of the tree. An example of a
feature at this level would be a sentence node that
dominates nodes for noun phrases and verb phrases.
For deeper levels, the system includes subtrees cor-
responding to the successive application of rules on
non-terminals symbols until either a terminal sym-
bol is reached or the given depth is reached. An
example feature for level 2 is a subtree in which
a prepositional phrase node dominates a preposi-
tion node and noun phrase node, and the preposition
node in turn dominates a preposition, and the noun
phrase dominates determiner, adjective, and noun
nodes.
We used a maximum depth of 3 in our exper-
iments. Features of deeper levels occur less fre-
quently in general, and deeper levels were avoided
due to data sparseness. A depth first search algo-
rithm extracts candidate grammatical features from
the training corpus. First, a context-free grammar
parser (Klein and Manning, 2003) derives parse
trees for all texts in the training corpus. The algo-
rithm traverses these parses, at each node counting
all subtree features up to the given depth that are
rooted at that node. The subtree features are sorted
by their overall counts in the corpus. In our ex-
periments, frequencies of the most common 1000
subtrees were chosen as the final features. These
included 64 level 0 features corresponding to non-
terminal symbols, 334 level 1 features, 461 level 2
features, and 141 level 3 features. Deeper levels
have more possible features, but sparsity at level 3
resulted in fewer level 3 features being selected.
In our experiments, the subtrees included terminal
symbols for stopwords. However, the system effec-
tively removed content word terminals from parses
before extracting features. The system could be
modified to include terminal symbols for content
words, or even to ignore all nodes for terminal sym-
bols. Subtree features including terminal symbols
for content words would, of course, occur with low
frequency and not likely be included in the final fea-
ture set. Terminal symbols for content words were
omitted so that lexical information was not included
in the set of grammatical features. Similar to leaving
higher order n-grams out of the lexical feature set,
omitting terminal symbols for content words avoids
confounding grammatical and lexical information in
the grammatical feature set. Subtree counts are nor-
malized by the number of words in a text to compute
the relative frequencies. Normalization by the num-
ber of sentences in a text is also possible, but did not
perform as well in preliminary tests. The Stanford
Parser (Klein andManning, 2003) version 1.5.1 was
used to derive tree structures for sentences. We used
the unlexicalized model included in the distribution
which was trained on Wall Street Journal texts.
3 Statistical Models
3.1 Scales of Measurement for Reading
Difficulty
Several statistical models were tested for effective-
ness at predicting reading difficulty. The appropri-
ateness of these models depends on the nature of
73
Figure 1: Parse Tree for Sentence from Third Grade Text with Example Subtree Features.
reading difficulty data, particularly the scale of mea-
surement. The standard unit for reading difficulty is
the grade level. First through twelfth grade levels in
American schools have been used in previous work
(e.g., (Heilman et al, 2007; Collins-Thompson and
Callan, 2005)). English as a Second Language lev-
els have also been used (Heilman et al, 2007),
as well as grade levels for other languages such
as French (Collins-Thompson and Callan, 2005).
While these grades are assigned evenly spaced inte-
gers, the ranges of reading difficulty corresponding
to these grades are not necessarily evenly spaced. It
is possible, of course, that assuming even spacing
between levels might produce more parsimonious
and accurate statistical models. A more reasonable
assumption is that the grade numbers assigned to
each difficulty level denote an ordering: for exam-
ple, that grade 1 is in some sense less than grade 2,
which is less than grade 3, etc. Different statistical
models handle this assumption more or less well.
Statistics generally distinguish four scales of mea-
surement, which are, ordered by increasing assump-
tions about the relationships between values: nomi-
nal, ordinal, interval, and ratio (Stevens, 1946; Co-
hen et al, 2003). Nominal data involve no relation-
ships between the labels or classes of the data. An
example would be types of fruits, where a model
might be used to make decisions between apples and
oranges. This type of prediction is generally called
classification in machine learning and related fields.
Ordinal data have a natural ordering, but the val-
ues are not necessarily evenly spaced. For exam-
ple, data about the severity of illnesses might have
labels such as mild, moderate, severe, deceased, in
which the transitions between consecutive classes
all have the same direction but not the same mag-
nitude. Making predictions about such data is gen-
erally called ordinal regression (McCullagh, 1980).
Interval data, however, are both ordered and evenly
spaced. An example would be temperature as mea-
sured in Fahrenheit degrees. Such data have an ar-
bitrary zero point, and negative values may occur.
Ratio data, of which annual income is an example,
do have a meaningful zero point. We will not dis-
cuss ratio data further since its distinction from in-
terval data is not relevant to this paper. It is not clear
to which scale reading difficulty corresponds. The
assumption of an interval scale allows for simpler
models with fewer parameters. However, models for
ordinal or even nominal data might be more appro-
priate if the strong assumption of an interval scale
does not hold.
We experimented with three linear and log-linear
models corresponding to interval, ordinal, and nom-
inal data. Parameters were estimated using L2 reg-
ularization, which corresponds to a Gaussian prior
distribution with zero mean and a user-specified
variance over the parameters. We chose these mod-
els because they are commonly used in the statis-
tics, machine learning, and behavioral science com-
munities, and aimed to set up meaningful compar-
isons among the scales of measurement. Other ma-
chine learning algorithms might also be employed.
In fact, we briefly tested the maximummargin (Vap-
nik, 1995) approach, which led to comparable re-
sults and might be worth exploring in future work.
74
3.2 Linear Regression
Linear Regression (LIN) produces a linear model in
which the dependent, or outcome, variable is a lin-
ear function of the values for predictor variables,
or features. A prediction for a given text is the
inner product of a vector of feature values for the
text and a vector of regression coefficients estimated
from training data. For the case of reading difficulty,
the grade level is a linear combination of the lexi-
cal and/or grammatical feature values. LIN provides
continuous estimates of reading difficulty, such that
a prediction might fall between grade levels. The
estimates were not rounded to whole numbers in the
experiments. For rare cases of an LIN prediction
falling outside the appropriate range of grade levels,
the value was set to the maximum or minimum grade
level. LIN implicitly assumes that the data fall on
an interval scale, meaning that the levels are evenly
spaced. The LIN model has relatively few parame-
ters but makes strong assumptions about the scale of
measurement. For details, see (Hastie et al, 2001).
3.3 Proportional Odds Model
The Proportional Odds (PO) model, also called the
parallel regression model and the cumulative logit
model, is a form of log-linear, or exponential, model
for ordinal data (McCullagh, 1980). Given a new
unlabeled instance as input, the model provides es-
timates of the probability that the instance belongs
to a class at or above a particular level. In Equation
(1), P (y ? j) is this estimated probability, ?j is an
intercept parameter for the given level j, ? is vector
of regression coefficients, Xi is the vector of feature
values for instance i, and yi is the predicted reading
difficulty level.
P (yi ? j) =
exp(?j + ?TXi)
1 + exp(?j + ?TXi)
(1)
ln
P (yi ? j)
1? P (yi ? j)
= ?j + ?TXi (2)
The PO model has a parameter ?j for the thresh-
old, or intercept, at each level j, but only a single set
? of parameters for the features. These two types of
parameters correspond to an implicit assumption of
ordinality. Having a single set of parameters for fea-
tures across the levels means that changes in feature
values proportionally affect the odds of transitioning
from any one class to another.
The estimated probability of an instance belong-
ing to a particular class is the difference between es-
timates for that class and the next highest class. For
example, the estimated probability of a text being
at the eighth grade level would be the estimate for
being at or above eighth grade minus the estimate
for being at or above ninth grade. As in binary lo-
gistic regression, the PO model estimates log odds
ratios based on the values of features or predictor
variables. The numerator of the odds ratio is the
probability of being at or above a level, and the de-
nominator is the probability of being below a level.
Equation (2) shows the form of the model that is
linear in the parameters.
3.4 Multi-class Logistic Regression
Multi-class Logistic Regression (LOG), or multino-
mial logit regression, is a log-linear model for nom-
inal data. In contrast to the simpler PO model, the
model maintains parameters for all of the features
for every class except one category, which is used
for comparison. Thus, for reading difficulty, there
are about 11 times as many parameters to estimate
compared to LIN and PO. The increased difficulty
of parameter estimation for this model is offset for
domains in which assumptions of ordinality or lin-
earity do not hold. For more details, see (Hastie et
al., 2001).
4 Evaluation
4.1 Web Corpus
The corpus of materials used for training and test-
ing the models consists of the content text extracted
from Web pages with reading difficulty level labels.
Web pages were used because the system for pre-
dicting reading difficulty is being used as part of the
REAP tutoring system, which finds authentic and
appropriate Web pages for English vocabulary prac-
tice (Brown and Eskenazi, 2004; Heilman et al,
2006). Approximately half of these texts were au-
thored by students at the particular grade level, and
half were authored by teachers or writers and aimed
at readers at a particular grade level. Texts were
found for grade levels 1 through 12. The twelfth
grade level also included some post-secondary level
75
texts. Various genres and subjects were represented.
In all cases, either the text itself or a link to it iden-
tified it as having a certain level. The content text
was manually extracted from these Web pages so
that noisy information such as navigation menus and
advertisements were not included. Automatic con-
tent extraction may, however, be able to remove such
noisy information without human intervention (e.g.,
(Gupta et al, 2003)). This Web corpus is adapted
from the corpora used in prior work on reading dif-
ficulty predication (Collins-Thompson and Callan,
2005; Heilman et al, 2007). We modified that cor-
pus because it contained a number of documents
pertaining to mathematics and vocabulary practice.
The majority of tokens in these texts were not part
of well-formed, grammatical sentences suitable for
reading practice. Since our goal is to measure the
difficulty of reading passages, we removed these
documents and added additional texts consisting of
more suitable reading material. The corpus con-
sisted of approximately 150,000 words, distributed
among 289 texts. The number of texts for each grade
level was approximately the same, with at least 28
texts at each level. The mean length in words of
the texts was approximately 500 words, which corre-
sponds to about a page. Texts for lower grades were
necessarily shorter. We extracted excerpts for higher
level texts so that texts were otherwise roughly equal
in length across levels. For these excerpts, the first
500 or so words of text were extracted, while re-
specting sentence and paragraph boundaries.
4.2 Evaluation Metrics
Root mean square error (RMSE), Pearson?s correla-
tion coefficient, and accuracy within 1 grade level
served as metrics for evaluating the performance
of reading difficulty predictions. Multiple statistics
were used because it is not entirely clear what the
best measure of prediction quality is for reading dif-
ficulty. RMSE is the square root of the empirical
mean of the squared error of predictions. It more
strongly penalizes those errors that are further away
from the true value. It can be interpreted as the aver-
age number of grade levels that predictions measure
deviate from human-assigned labels.
Pearson?s correlation coefficient measures the
strength of the linear relationship, or similarity of
trends, between two random variables. A high corre-
lation would indicate that difficult texts would more
likely receive high predicted difficulty values, and
easier texts would be more likely to receive low pre-
dicted difficulty values. Correlations do not, how-
ever, measure the degree to which values match in
absolute terms.
Adjacent accuracy is the proportion of predic-
tions that were within one grade level of the human-
assigned label for the given text. Exact accuracy is
too stringent a measure because the human-assigned
reading levels are not always perfect and consis-
tent. For example, one school might read ?Romeo
and Juliet? in 9th grade while another school might
read it in 10th grade. The drawback of this accuracy
metric is that predictions that are two levels off are
treated the same as predictions that are ten levels off.
4.3 Baselines
The performance of other algorithms for estimat-
ing reading difficulty was estimated using the same
data. These comparison include Collins-Thompson
and Callan?s implementation of their language mod-
eling approach (2005), an implementation of the
Flesch-Kincaid reading level measure (Kincaid et
al., 1975), and a measure using word frequency and
sentence length similar to Lexile (Stenner et al,
1983). We did not directly test the approach de-
scribed by (Heilman et al, 2007). We observe
that its reported results for first language texts were
not significantly different in terms of correlation and
only slightly better in terms of mean squared er-
ror than the language modeling approach. Finally,
a simple uniform baseline, which always chose the
middle value of 6.5, was tested.
The Lexile-like measure (LX) used the same two
features as the Lexile measure: mean log frequency
or words and log mean sentence length. Instead of
using a Rasch model and converting scores to ?Lex-
iles,? however, the PO model was used to directly
predict grade levels. The log frequency values for
words were estimated from the second release of the
American National Corpus (Reppen et al, 2005),
a 20 million word corpus with texts in American
English from different genres on a variety of sub-
jects. Using the proportional odds models is effec-
tively equivalent to using Lexile?s Rasch model and
mapping its output to grade levels. The major differ-
ence between the Lexile measure and the implemen-
76
tation used in these experiments is the training data
sets used to estimated word frequencies and model
parameters.
4.4 Procedure
The Web Corpus was randomly split into training
and test sets. The test set consisted of 25% of the
individual texts at each level, a total of 84 texts.
Ten-fold stratified cross-validation on the training
set was employed to estimate the prediction per-
formance according to the evaluation metrics. In
cross-validation, data are partitioned randomly into
a given number of folds, and each fold is used for
testing while all others are used for training. For
more details and a discussion of validation meth-
ods, see (Hastie et al, 2001). The regularization
hyper-parameters were tuned on the training set dur-
ing cross-validation by a simple grid search. After
cross-validation, models were trained on the entire
training set, and then evaluated using the held-out
test data.
We tested whether each feature-set, algorithm pair
or baseline performed significantly differently than
our hypothesized best model, the PO model with
the combined feature set. We employed the bias-
corrected and accelerated (BCa) Bootstrap (Efron
and Tibshirani, 1993) with 50,000 replications of the
held-out test data to generate confidence intervals
for differences in evaluation results. If the (1??)%
confidence intervals for the difference do not con-
tain zero, which is the value corresponding to the
null hypothesis, then that difference is significant at
the ? level. For example, the 99% confidence inter-
val for the difference in adjacent accuracy between
the language modeling baseline and the PO model
with the combined feature set was (-1.86, -0.336),
indicating that this difference is significant at the .01
level since it does not contain zero.
5 Results
Table 1 presents correlation coefficients, RMSE val-
ues, and accuracy values for cross-validation and
held-out test data. Statistical significance was tested
only for the held-out test data since the hyper-
parameters were tuned during cross-validation. Our
discussion of the results pertains mostly to the eval-
uation on the test-set.
Of the various statistical models, the PO model
for ordinal data appears to provide superior perfor-
mance over the LIN and LOG models. Compared
to the LOG model, the PO model performs sig-
nificantly better in terms of correlation and RMSE
and comparably well in terms of adjacent accuracy.
Compared to the LINmodel, the POmodel performs
almost as well in terms of correlation, comparably
well in terms of RMSE, and far better in terms of
accuracy.
The performance of the methods when using dif-
ferent feature sets does not clearly indicate a best set
of features to use for predicting reading difficulty.
For the PO model, none of the feature sets lead to
significant gains over the others in terms of any of
the metrics. However, the combined feature set led
to the best performance in terms of correlation and
adjacent accuracy during cross-validation as well as
RMSE on the test set, suggesting at the very least
that including the extra features does not degrade
performance.
The PO model with the combined feature set out-
performed most of the baseline measures. LX had
the same accuracy value on the test set. The LX
method appears to perform the best in general of
the baselines models. Interestingly, LX uses pro-
portional odds logistic regression like PO, and thus
assumes an ordinal but not interval scale of measure-
ment. RMSE values were significantly lower for the
PO model than for LX and the language modeling
approach.
No statistically significant advantages are seen
for PO model when compared to Flesch-Kincaid.
We observe however, that for the sample of web
pages which constitutes the evaluation corpus the
PO model produced superior results across evalua-
tion metrics. That is, PO performed better in terms
of adjacent accuracy, RMSE, and correlation coeffi-
cients, both in cross-validation and testing with held-
out data.
6 Discussion
In our tests, the PO model, which assumes ordinal
data, lead to the most effective predictions of read-
ing difficulty in general. This result indicates that the
reading difficulty of texts, according to grade level,
lies on an ordinal scale of measurement. That is,
77
Method Features Cross-Validation Held-Out Test Set
Correl. RMSE Adj. Acc. Correl. RMSE Adj. Acc.
LIN Lexical .629 2.73 .242 .779 2.42 .167**
Grammatical .767 2.26 .294 .753 2.33 .274*
Combined .679 2.57 .284 .819** 2.21 .226**
PO Lexical .713 2.57 .498 .780 2.29 .464
Grammatical .762 2.22 .505 .734 2.42 .560
Combined .773 2.24 .519 .767 2.23 .440
LOG Lexical .517 3.24 .443 .619* 2.83* .548
Grammatical .632 2.87 .443 .506** 3.38** .464
Combined .582 2.94 .446 .652* 2.71* .556
LX - .659 2.77 .467 .731 2.67* .464
Lang. Modeling - .590 2.74 .370 .630 2.70** .381
Flesch-Kincaid - .697 2.66 .388 .718 2.54 .369
Uniform - .000 3.39 .170 .000** 3.45** .167**
Table 1: Results from Cross-Validation and Test Set Evaluations, as measured by Correlation Coefficients (Correl.),
Root Mean Square Error (RMSE), and Adjacent Accuracy. The best result for each metric for each evaluation is
given in bold. Asterisks indicate significant differences compared to the PO model with a Combined Feature Set. * =
p < .05, ** = p < .01.
reading difficulty appears to increase steadily but not
linearly with grade level. As such, the LIN approach
that produces linear models was less effective, par-
ticularly in terms of adjacent accuracy. The LOG
model, for nominal data, also led to inferior perfor-
mance compared to the PO model, which can be at-
tributed to the difficulty of accurately estimating a
more complex model with many parameters for each
level.
Our tests found that grammatical features alone
can be effective predictors of readability. This find-
ing disagrees with a previous result that found that a
model using a combination of lexical and manually
defined grammatical features (Heilman et al, 2007)
outperformed a model using grammatical features
alone. The superior predictive ability of the mod-
els we describe that use grammatical features can be
attributed to the automatic derivation of a grammat-
ical feature set that is more than an order of magni-
tude larger than in the previous approach. Our ap-
proach enables the use of much larger grammatical
feature sets because it does not require the extensive
linguistic knowledge and effort to manually define
the grammatical features. The automatic approach
also enables an easier transition to other languages,
assuming a parser is available. Using the combined
feature set did not hurt performance, however, and
since regularized statistical models can avoid over-
fitting large numbers of parameters, a combined fea-
ture set still seems appropriate.
Acknowledgments
We thank Jamie Callan for his comments and sug-
gestions. This research was supported in part by
the Institute of Education Sciences, U.S. Depart-
ment of Education, through Grant R305B040063 to
Carnegie Mellon University; Dept. of Education
grant R305G03123; the Pittsburgh Science of Learn-
ing Center which is funded by the National Sci-
ence Foundation, award number SBE-0354420; and
a National Science Foundation Graduate Research
Fellowship awarded to the first author. Any opin-
ions, findings, conclusions, or recommendations ex-
pressed in this material are the authors, and do not
necessarily reflect those of the sponsors.
References
Jon Brown and Maxine Eskenazi. 2004. Retrieval of au-
thentic documents for reader-specific lexical practice.
Proceedings of InSTIL/ICALL Symposium 2004.
78
J. S. Chall and E. Dale. 1995. Readability Revisited:
The New Dale-Chall Readability Formula. Brookline
Books. Cambridge, MA.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. Proceedings of the NAACL.
J. Cohen, P. Cohen, S. G. West, and L. S. Aiken. 2003.
Applied Multiple Regression/Correlation Analysis for
the Behavioral Sciences, 3rd Edition. Lawrence Erl-
baum Associates, Inc.
Michael Collins and Nigel Duffy. 2002. Convolution
Kernels for Natural Language. Advances in Neural In-
formation Processing Systems..
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical language
models. Journal of the American Society for Informa-
tion Science and Technology, 56(13). pp. 1448-1462..
E. Dale and J. S. Chall. 1948. A Formula for Predicting
Readability. Educational Research Bulletin Vol. 27,
No. 1.
Bradley Efron and Robert J. Tibshirani. 1993. An In-
troduction to the Bootstrap. Chapman and Hall, New
York.
R. Gunning. 1952. The technique of clear writing..
McGraw-Hill, New York.
S. Gupta, G. Kaiser, D. Neistadt, and P. Grimm. 2003.
DOM-based content extraction of HTML documents.
ACM Press, New York.
Trevor Hastie, Robert Tibshirani, Jerome Friedman.
2003. The Elements of Statistical Learning:Data Min-
ing, Inference, and Prediction. Springer.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2007. Combining Lex-
ical and Grammatical Features to Improve Readability
Measures for First and Second Language Texts. Pro-
ceedings of the Human Language Technology Confer-
ence. Rochester, NY.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2006. Classroom suc-
cess of an Intelligent Tutoring System for lexical prac-
tice and reading comprehension. Proceedings of the
Ninth International Conference on Spoken Language
Processing. Pittsburgh, PA.
J. Kincaid, R. Fishburne, R. Rodgers, and B. Chissom.
1975. Derivation of new readability formulas for navy
enlisted personnel. Branch Report 8-75. Chief of
Naval Training, Millington, TN.
G. R. Klare. 1974. Assessing Readability. Reading Re-
search Quarterly, Vol. 10, No. 1. pp. 62-102..
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. Proceedings of the 41st Meet-
ing of the Association for Computational Linguistics,
pp. 423-430.
G. H. McLaughlin. 1969. SMOG grading: A new read-
ability formula. Journal of Reading.
P. McCullagh. 1980. Regression Models for Ordinal
Data. Journal of the Royal Statistical Society. Series
B (Methodological), Vol. 42, No. 2. pp. 109-142.
G. Rasch. 1980. Probabilistic Models for Some Intelli-
gence and Attainment Tests. MESA Press, Chicago,
IL.
G. Rasch. 2005. American National Corpus (ANC) Sec-
ond Release.. Linguistic Data Consortium. Philadel-
phia, PA.
Sarah Schwarm and Mari Ostendorf. 2005. Read-
ing level assessment using support vector machines
and statistical language models. Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics.
A. J. Stenner, M. Smith, and D. S. Burdick. 1983. To-
ward a Theory of Construct Definition. Journal of Ed-
ucational Measurement, Vol. 20, No. 4. pp. 305-316.
A. J. Stenner. 1996. Measuring reading comprehension
with the Lexile framework. Fourth North American
Conference on Adolescent/Adult Literacy.
S. S. Stevens. 1946. On the theory of scales of measure-
ment. Science, 103, pp. 677-680.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
Y. Yang and J. P. Pedersen. 1997. A Comparative Study
on Feature Selection in Text Categorization. Proceed-
ings of the Fourteenth International Conference on
Machine Learning (ICML?97), pp. 412-420.
G. K. Zipf. 1935. The Psychobiology off Language.
Houghton Mifflin, Boston, MA.
79
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 80?88,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Retrieval of Reading Materials for Vocabulary and Reading Practice
Michael Heilman, Le Zhao, Juan Pino and Maxine Eskenazi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{mheilman,lezhao,jmpino,max}@cs.cmu.edu
Abstract
Finding appropriate, authentic reading mate-
rials is a challenge for language instructors.
The Web is a vast resource of texts, but most
pages are not suitable for reading practice, and
commercial search engines are not well suited
to finding texts that satisfy pedagogical con-
straints such as reading level, length, text qual-
ity, and presence of target vocabulary. We
present a system that uses various language
technologies to facilitate the retrieval and pre-
sentation of authentic reading materials gath-
ered from the Web. It is currently deployed in
two English as a Second Language courses at
the University of Pittsburgh.
1 Introduction
Reading practice is an important component of first
and second language learning, especially with re-
gards to vocabulary learning (Hafiz and Tudor,
1989). Appropriating suitable reading material for
the needs of a particular curriculum or particular stu-
dent, however, is a challenging process. Manually
authoring or editing readings is time-consuming and
raises issues of authenticity, which are particularly
significant in second language learning (Peacock,
1997). On the other hand, the Web is a vast resource
of authentic reading material, but commercial search
engines which are designed for a wide variety of in-
formation needs may not effectively facilitate the re-
trieval of appropriate readings for language learners.
In order to demonstrate the problem of finding ap-
propriate reading materials, here is a typical exam-
ple of an information need from a teacher of an En-
glish as a Second Language (ESL) course focused
on reading skills. This example was encountered
during the development of the system. It should
be noted that while we describe the system in the
context of ESL, we claim that the approach is gen-
eral enough to be applied to first language reading
practice and to languages other than English. To
fit within his existing curriculum, the ESL teacher
wanted to find texts on the specific topic of ?interna-
tional travel.? He sought texts that contained at least
a few words from the list of target vocabulary that
his student were learning that week. In addition, he
needed the texts to be within a particular range of
reading difficulty, fifth to eighth grade in an Ameri-
can school, and shorter than a thousand words.
Sending the query ?international travel? to a pop-
ular search engine did not produce a useful list of re-
sults1. The first result was a travel warning from the
Department of State2, which was at a high reading
level (grade 10 according to the approach described
by (Heilman et al, 2008)) and not likely to be of
interest to ESL students because of legal and techni-
cal details. Most of the subsequent results were for
commercial web sites and travel agencies. A query
for a subset of the target vocabulary words for the
course also produced poor results. Since the search
engine used strict boolean retrieval methods, the top
results for the query ?deduce deviate hierarchy im-
plicit undertake? were all long lists of ESL vocabu-
lary words3.
We describe a search system, called REAP
Search, that is tailored to the needs of language
1www.google.com, March 5, 2008
2http://travel.state.gov/travel/cis pa tw/cis pa tw 1168.html
3e.g., www.espindle.org/university word list uwl.html
80
teachers and learners. The system facilitates the re-
trieval of texts satisfying particular pedagogical con-
straints such as reading level and text length, and al-
lows the user to constrain results so that they con-
tain at least some, but not necessarily all, of the
words from a user-specified target vocabulary list.
It also filters out inappropriate material as well as
pages that do not contain significant amounts of text
in well-formed sentences. The system provides sup-
port for learners including an interface for reading
texts, easy access to dictionary definitions, and vo-
cabulary exercises for practice and review.
The educational application employs multiple
language technologies to achieve its various goals.
Information retrieval and web search technologies
provide the core components. Automated text clas-
sifiers organize potential readings by general topic
area and reading difficulty. We are also developing
an approach to measuring reading difficulty that uses
a parser to extract grammatical structures. Part of
Speech (POS) tagging is used to filter web pages to
maintain text quality.
2 Path of a Reading
In the REAP Search system, reading materials take a
path from the Web to students through various inter-
mediate steps as depicted in Figure 1. First, a crawl-
ing program issues queries to large-scale commer-
cial search engines to retrieve candidate documents.
These documents are annotated, filtered, and stored
in a digital library, or corpus. This digital library cre-
ation process is done offline. A customized search
interface facilitates the retrieval of useful reading
materials by teachers, who have particular curricu-
lar goals and constraints as part of their information
needs. The teachers organize their selected readings
through a curriculum manager. The reading inter-
face for students accesses the curriculum manager?s
database and provides the texts along with support
in the form of dictionary definitions and practice ex-
ercises.
3 Creating a Digital Library of Readings
The foundation of the system is a digital library of
potential reading material. The customized search
component does not search the Web directly, but
rather accesses this filtered and annotated database
of Web pages. The current library consists of ap-
proximately five million documents. Construction
of the digital library begins with a set of target vo-
cabulary words that might be covered by a course or
set of courses (typically 100-1,500 words), and a set
of constraints on text characteristics. The constraints
can be divided into three sets: those that can be ex-
pressed in a search engine query (e.g., target words,
number of target words per text, date, Web domain),
those that can be applied using just information in
the Web search result list (e.g., document size), and
those that require local annotation and filtering (e.g.,
reading level, text quality, profanity).
The system obtains candidate documents by
query-based crawling, as opposed to following
chains of links. The query-based document crawl-
ing approach is designed to download documents
for particular target words. Queries are submitted
to a commercial Web search engine4, result links are
downloaded, and then the corresponding documents
are downloaded. A commercial web search engine
is used to avoid the cost of maintaining a massive,
overly general web corpus.
Queries consist of combinations of multiple tar-
get words. The system generates 30 queries for each
target word (30 is a manageable and sufficient num-
ber in practice). These are spread across 2-, 3-,
and 4-word combinations with other target words.
Queries to search engines can often specify a date
range. We employ ranges to find more recent mate-
rial, which students prefer. The tasks of submitting
queries, downloading the result pages, and extract-
ing document links are distributed among a dozen
or so clients running on desktop machines, to run as
background tasks. The clients periodically upload
their results to a server, and request a new batch of
queries.
Once the server has a list of candidate pages, it
downloads them and applies various filters. The fi-
nal yield of texts is typically approximately one per-
cent of the originally downloaded results. Many web
pages are too long, contain too little well-formed
text, or are far above the appropriate reading level
for language learners. After downloading docu-
ments, the system annotates them as described in
the next section. It then stores the pages in a full-
4www.altavista.com
81
Figure 1: Path of Reading Materials from the Web to a Student.
text search engine called Indri, which is part of
the Lemur Toolkit5. This index provides a consis-
tent and efficient interface to the documents. Using
Lemur and the Indri Query Language allows for the
retrieval of annotated documents according to user-
specified constraints.
4 Annotations and Filters
Annotators automatically tag the documents in the
corpus to enable the filtering and retrieval of read-
ing material that matches user-specified pedagogical
constraints. Annotations include reading difficulty,
general topic area, text quality, and text length. Text
length is simply the number of word tokens appear-
ing in the document.
4.1 Reading Level
The system employs a language modeling ap-
proach developed by Collins-Thompson and Callan
(Collins-Thompson and Callan, 2005) that creates a
model of the lexicon for each grade level and pre-
dicts reading level, or readability, of given docu-
ments according to those models. The readabil-
ity predictor is a specialized Naive Bayes classi-
fier with lexical unigram features. For web docu-
ments in particular, Collins-Thompson and Callan
report that this language modeling-based prediction
has a stronger correlation with human-assigned lev-
els than other commonly used readability measures.
This automatic readability measure allows the sys-
tem to satisfy user-specified constraints on reading
difficulty.
We are also experimenting with using syntac-
tic features to predict reading difficulty. Heilman,
Collins-Thompson, and Eskenazi (Heilman et al,
2008) describe an approach that combines predic-
tions based on lexical and grammatical features. The
5www.lemurproject.org
grammatical features are frequencies of occurrence
of grammatical constructions, which are computed
from automatic parses of input texts. Using multiple
measures of reading difficulty that focus on different
aspects of language may allow users more freedom
to find texts that match their needs. For example,
a teacher may want to find grammatically simpler
texts for use in a lesson focused on introducing dif-
ficult vocabulary.
4.2 General Topic Area
A set of binary topic classifiers automatically clas-
sifies each potential reading by its general topic, as
described by Heilman, Juffs, and Eskenazi (2007).
This component allows users to search for readings
on their general interests without specifying a par-
ticular query (e.g., ?international travel?) that might
unnecessarily constrain the results to a very narrow
topic.
A Linear Support Vector Machine text classifier
(Joachims, 1999) was trained on Web pages from
the Open Directory Project (ODP)6. These pages ef-
fectively have human-assigned topic labels because
they are organized into a multi-level hierarchy of
topics. The following general topics were manually
selected from categories in the ODP: Movies and
Theater; Music; Visual Arts; Computers and Tech-
nology; Business; Math, Physics and Chemistry; Bi-
ology and Environment; Social Sciences; Health and
Medicine; Fitness and Nutrition; Religion; Politics;
Law and Crime; History; American Sports; and Out-
door Recreation.
Web pages from the ODP were used as gold-
standard labels in the training data for the classi-
fiers. SVM-Light (Joachims, 1999) was used as an
implementation of the Support Vector Machines. In
preliminary tests, the linear kernel produced slightly
6dmoz.org
82
better performance than a radial basis function ker-
nel. The values of the decision functions of the clas-
sifiers for each topic are used to annotate readings
with their likely topics.
The binary classifiers for each topic category were
evaluated according to the F1 measure, the harmonic
mean of precision and recall, using leave-one-out
cross-validation. Values for the F1 statistic range
from .68 to .86, with a mean value of .76 across
topics. For comparison, random guessing would be
expected to correctly choose the gold-standard label
only ten percent of the time. During an error analy-
sis, we observed that many of the erroneous classifi-
cations were, in fact, plausible for a human to make
as well. Many readings span multiple topics. For
example, a document on a hospital merger might be
classified as ?Health and Medicine? when the cor-
rect label is ?Business.? In the evaluation, the gold
standard included only the single topic specified by
the ODP. The final system, however, assigns multi-
ple topic labels when appropriate.
4.3 Text Quality
A major challenge of using Web documents for ed-
ucational applications is that many web pages con-
tain little or no text in well-formed sentences and
paragraphs. We refer to this problem as ?Text Qual-
ity.? Many pages consist of lists of links, navigation
menus, multimedia, tables of numerical data, etc. A
special annotation tool filters out such pages so that
they do not clutter up search results and make it dif-
ficult for users to find suitable reading materials.
The text quality filter estimates the proportion of
the word tokens in a page that are contained in well-
formed sentences. To do this it parses the Document
Object Model structure of the web page, and orga-
nizes it into text units delineated by the markup tags
in the document. Each new paragraph, table ele-
ment, span, or divider markup tag corresponds to the
beginning of a new text unit. The system then runs
a POS tagger7 over each text unit. We have found
that a simple check for whether the text unit con-
tains both a noun and a verb can effectively distin-
guish between content text units and those text units
that are just part of links, menus, etc. The proportion
7The OpenNLP toolkit?s tagger was used
(opennlp.sourceforge.net).
of the total tokens that are part of content text units
serves as a useful measure of text quality. We have
found that a threshold of about 85% content text is
appropriate, since most web pages contain at least
some non-content text in links, menus, etc. This ap-
proach to content extraction is related to previous
work on increasing the accessibility of web pages
(Gupta et al, 2003).
5 Constructing Queries
Users search for readings in the annotated corpus
through a simple interface that appears similar to,
but extends the functionality of, the interfaces for
commercial web search engines. Figure 2 shows
a screenshot of the interface. Users have the op-
tion to specify ad hoc queries in a text field. They
can also use drop down menus to specify optional
minimum and/or maximum reading levels and text
lengths. Another optional drop-down menu allows
users to constrain the general topic area of results. A
separate screen allows users to specify a list of tar-
get vocabulary words, some but not all of which are
required to appear in the search results. For ease of
use, the target word list is stored for an entire session
(i.e., until the web browser application is closed)
rather than specified with each query. After the user
submits a query, the system displays multiple results
per screen with titles and snippets.
5.1 Ranked versus Boolean Retrieval
In a standard boolean retrieval model, with AND as
the default operator, the results list consists of doc-
uments that contain all query terms. In conjunc-
tion with relevance ranking techniques, commercial
search engines typically use this model, a great ad-
vantage of which is speed. Boolean retrieval can en-
counter problems when queries have many terms be-
cause every one of the terms must appear in a doc-
ument for it to be selected. In such cases, few or
no satisfactory results may be retrieved. This issue
is relevant because a teacher might want to search
for texts that contain some, but not necessarily all,
of a list of target vocabulary words. For example,
a teacher might have a list of ten words, and any
text with five of those words would be useful to give
as vocabulary and reading practice. In such cases,
ranked retrieval models are more appropriate be-
83
Figure 2: Screenshot of Search Interface for Finding Appropriate Readings.
cause they do not require that all of the query terms
appear. Instead, these models prefer multiple occur-
rences of different word types as opposed to multiple
occurrences of the same word tokens, allowing them
to rank documents with more distinct query terms
higher than those with distinct query terms. Docu-
ments that contain only some of the query terms are
thus assigned nonzero weights, allowing the user to
find useful texts that contain only some of the target
vocabulary. The REAP search system uses the Indri
Query Language?s ?combine? and ?weight? opera-
tors to implement a ranked retrieval model for target
vocabulary. For more information on text retrieval
models, see (Manning et al, 2008).
5.2 Example Query
Figure 3 shows an example of a structured query
produced by the system from a teacher?s original
query and constraints. This example was slightly
altered from its original form for clarity of presen-
tation. The first line with the filrej operator filters
and rejects any documents that contain any of a long
list of words considered to be profanity, which are
omitted in the illustration for brevity and posterity.
The filreq operator in line 2 requires that all of the
constraints on reading level, text length and quality
in lines 2-4 are met. The weight operator at the start
of line 5 balances between the ad hoc query terms in
line 5 and the user-specific target vocabulary terms
in lines 6-8. The uw10 operator on line 5 tells the
system to prefer texts where the query terms appear
together in an unordered window of size 10. Such
proximity operators cause search engines to prefer
documents in which query terms appear near each
other. The implicit assumption is that the terms in
queries such as ?coal miners safety? are more likely
to appear in the same sentence or paragraph in rele-
vant documents than irrelevant ones, even if they do
not appear consecutively. Importantly, query terms
are separated from target words because there are
usually a much greater number of target words, and
thus combining the two sets would often result in
the query terms being ignored. The higher weight
assigned to the set of target words ensures they are
not ignored.
6 Learner and Teacher Support
In addition to search facilities, the system provides
extensive support for students to read and learn from
texts as well as support for teachers to track stu-
dents? progress. All interfaces are web-based for
easy access and portability. Teachers use the search
system to find readings, which are stored in a cur-
riculum manager that allows them to organize their
selected texts. The manager interface allows teach-
ers to perform tasks such as specifying the order
of presentation of their selected readings, choosing
target words to be highlighted in the texts to focus
learner attention, and specifying time limits for each
text.
The list of available readings are shown to stu-
dents when they log in during class time or for
homework. Students select a text to read and move
on to the reading interface, which is illustrated in
Figure 4. The chosen web page is displayed in its
original format except that the original hyperlinks
and pop-ups are disabled. Target words that were
84
Figure 3: Example Structured Query. The line numbers on the left are for reference only.
chosen by the teacher are highlighted and linked to
definitions. Students may also click on any other
unknown words to access definitions. The dictio-
nary definitions are provided from the Cambridge
Advanced Learner?s Dictionary8, which is authored
specifically for ESL learners. All dictionary access
is logged, and teachers can easily see which words
students look up.
The system also provides vocabulary exercises af-
ter each reading for additional practice and review
of target words. Currently, students complete cloze,
or fill-in-the-blank, exercises for each target word in
the readings. Other types of exercises are certainly
possible. For extra review, students also complete
exercises for target words from previous readings.
Students receive immediate feedback on the prac-
tice and review exercises. Currently, sets of the ex-
ercises are manually authored for each target word
and stored in a database, but we are exploring auto-
mated question generation techniques (Brown et al,
2005; Liu et al, 2005). At runtime, the system se-
lects practice and review exercises from this reposi-
tory.
7 Related Work
A number of recent projects have taken similar ap-
proaches to providing authentic texts for language
learners. WERTi (Amaral et al, 2006) is an in-
telligent automatic workbook that uses texts from
the Web to increase knowledge of English gram-
matical forms and functions. READ-X (Miltsakaki
and Troutt, 2007) is a tool for finding texts at spec-
ified reading levels. SourceFinder (Sheehan et al,
2007) is an authoring tool for finding suitable texts
for standardized test items on verbal reasoning and
8dictionary.cambridge.org
reading comprehension.
The REAP Tutor (Brown and Eskenazi, 2004;
Heilman et al, 2006) for ESL vocabulary takes a
slightly different approach. Rather than teachers
choosing texts as in the REAP Search system, the
REAP Tutor itself selects individualized practice
readings from a digital library. The readings contain
target vocabulary words that a given student needs
to learn based on a student model. While the in-
dividualized REAP Tutor has the potential to better
match the needs of each student since each student
can work with different texts, a drawback of its ap-
proach is that instructors may have difficulty coor-
dinating group discussion about readings and inte-
grating the Tutor into their curriculum. In the REAP
Search system, however, teachers can find texts that
match the needs and interests of the class as a whole.
While some degree of individualization is lost, the
advantages of better coordinated support from teach-
ers and classroom integration are gained.
8 Pilot Study
8.1 Description
Two teachers and over fifty students in two ESL
courses at the University of Pittsburgh used the sys-
tem as part of a pilot study in the Spring of 2008.
The courses focus on developing the reading skills
of high-intermediate ESL learners. The target vo-
cabulary words covered in the courses come from
the Academic Word List (Coxhead, 2000), a list
of broad-coverage, general purpose English words
that frequently appear in academic writing. Students
used the system once per week in a fifty-minute class
for eight weeks. For approximately half of a ses-
sion, students read the teacher-selected readings and
worked through individualized practice exercises.
85
Figure 4: Screenshot of Student Interface Displaying a Reading and Dictionary Definition.
For the other half of each session, the teacher pro-
vided direct instruction on and facilitated discussion
about the texts and target words, making connec-
tions to the rest of the curriculum when possible.
For each session, the teachers found three to five
readings. Students read through at least two of the
readings, which were discussed in class. The extra
readings allowed faster readers to progress at their
own pace if they complete the first two. Teachers
learned to use the system in a training session that
lasted about 30 minutes.
8.2 Usage Analysis
To better understand the two teachers? interactions
with the search system, we analyzed query log data
from a four week period. In total, the teachers used
the system to select 23 readings for their students.
In the process, they issued 47 unique queries to the
system. Thus, on average they issued 2.04 queries
per chosen text. Ideally, a user would only have to
issue a single query to find useful texts, but from
the teachers? comments it appears that the system?s
usability is sufficiently good in general. Most of
the time, they specified 20 target words, only some
of which appeared in their selected readings. The
teachers included ad hoc queries only some of the
time. These were informational in nature and ad-
dressed a variety of topics. Example queries in-
clude the following: ?surviving winter?, ?coal min-
ers safety?, ?gender roles?, and ?unidentified flying
objects?. The teachers chose these topics because
they matched up with topics discussed in other parts
of their courses? curricula. In other cases, it was
more important for them to search for texts with tar-
get vocabulary rather than those on specific topics,
so they only specified target words and pedagogical
constraints.
8.3 Post-test and Survey Results
At the end of the semester, students took an exit sur-
vey followed by a post-test consisting of cloze vo-
cabulary questions for the target words they prac-
ticed with the system. In previous semesters, the
REAP Tutor has been used in one of the two courses
that were part of the pilot study. For comparison
with those results, we focus our analysis on the sub-
set of data for the 20 students in that course. The
exit survey results, shown in 5, indicate that stu-
dents felt it was easy-to-use and should be used in
future classes. These survey results are actually very
similar to previous results from a Spring 2006 study
with the REAP Tutor (Heilman et al, 2006). How-
ever, responses to the prompt ?My teacher helped
me to learn by discussing the readings after I read
86
Figure 5: The results from the pilot study exit survey, which used a Likert response format from 1-5 with 1=Strongly
Disagree, 3=Neither Agree nor Disagree, and 5=Strongly Agree. Error bars indicate standard deviations.
them? suggest that the tight integration of an edu-
cational system with other classroom activities, in-
cluding teacher-led discussions, can be beneficial.
Learning of target words was directly measured
by the post-test. On average, students answered
89% of cloze exercises correctly, compared to less
than 50% in previous studies with the REAP Tutor.
A direct comparison to those studies is challenging
since the system in this study provided instruction
on words that students were also studying as part of
their regular coursework, whereas systems in previ-
ous studies did not.
9 Discussion and Future Work
We have described a system that enables teachers
to find appropriate, authentic texts from the Web
for vocabulary and reading practice. A variety of
language technologies ranging from text retrieval to
POS tagging perform essential functions in the sys-
tem. The system has been used in two courses by
over fifty ESL students.
A number of questions remain. Can language
learners effectively and efficiently use such a system
to search for reading materials directly, rather than
reading what a teacher selects? Students could use
the system, but a more polished user interface and
further progress on filtering out readings of low text
quality is necessary. Is such an approach adaptable
to other languages, especially less commonly taught
languages for which there are fewer available Web
pages? Certainly there are sufficient resources avail-
able on the Web in commonly taught languages such
as French or Japanese, but extending to other lan-
guages with fewer resources might be significantly
more challenging. How effective would such a tool
be in a first language classroom? Such an approach
should be suitable for use in first language class-
rooms, especially by teachers who need to find sup-
plemental materials for struggling readers. Are there
enough high-quality, low-reading level texts for very
young readers? From observations made while de-
veloping REAP, the proportion of Web pages below
fourth grade reading level is small. Finding appro-
priate materials for beginning readers is a challenge
that the REAP developers are actively addressing.
Issues of speed and scale are also important to
consider. Complex queries such as the one shown
in Figure 3 are not as efficient as boolean queries.
The current system takes a few seconds to return re-
sults from its database of several million readings.
Scaling up to a much larger digital library may re-
quire sophisticated distributed processing of queries
across multiple disks or multiple servers. However,
we maintain that this is an effective approach for
providing texts within a particular grade level range
or known target word list.
Acknowledgments
This research was supported in part by the Insti-
tute of Education Sciences, U.S. Department of Ed-
ucation, through Grant R305B040063 to Carnegie
Mellon University; Dept. of Education grant
R305G03123; the Pittsburgh Science of Learning
Center which is funded by the National Science
Foundation, award number SBE-0354420; and a Na-
tional Science Foundation Graduate Research Fel-
lowship awarded to the first author. Any opin-
ions, findings, conclusions, or recommendations ex-
pressed in this material are the authors, and do not
necessarily reflect those of the sponsors.
References
Luiz Amaral, Vanessa Metcalf and Detmar Meurers.
87
2006. Language Awareness through Re-use of NLP
Technology. Pre-conference Workshop on NLP in
CALL ? Computational and Linguistic Challenges.
CALICO 2006.
Jon Brown and Maxine Eskenazi. 2004. Retrieval of
authentic documents for reader-specific lexical prac-
tice. Proceedings of InSTIL/ICALL Symposium 2004.
Venice, Italy.
Jon Brown, Gwen Frishkoff, and Maxine Eskenazi.
2005. Automatic question generation for vocabulary
assessment. Proceedings of HLT/EMNLP 2005. Van-
couver, B.C.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical language
models. Journal of the American Society for Informa-
tion Science and Technology, 56(13). pp. 1448-1462.
Averil Coxhead. 2000. A New Academic Word List.
TESOL Quarterly, 34(2). pp. 213-238.
S. Gupta, G. Kaiser, D. Neistadt, and P. Grimm. 2003.
DOM-based content extraction of HTML documents.
ACM Press, New York.
F. M. Hafiz and Ian Tudor. 1989. Extensive reading
and the development of language skills. ELT Journal
43(1):4-13. Oxford University Press.
Michael Heilman, Kevyn Collins-Thompson, Maxine Es-
kenazi. 2008. An Analysis of Statistical Models and
Features for Reading Difficulty Prediction. The 3rd
Workshop on Innovative Use of NLP for Building Edu-
cational Applications. Association for Computational
Linguistics.
Michael Heilman, Alan Juffs, Maxine Eskenazi. 2007.
Choosing Reading Passages for Vocabulary Learning
by Topic to Increase Intrinsic Motivation. Proceedings
of the 13th International Conferenced on Artificial In-
telligence in Education. Marina del Rey, CA.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2006. Classroom suc-
cess of an Intelligent Tutoring System for lexical prac-
tice and reading comprehension. Proceedings of the
Ninth International Conference on Spoken Language
Processing. Pittsburgh, PA.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. Advances in Kernel Methods -
Support Vector Learning, B. Schlkopf and C. Burges
and A. Smola (ed.) MIT-Press.
Chao-Lin Liu, Chun-Hung Wang, Zhao-Ming Gao, and
Shang-Ming Huang. 2005. Applications of Lexical
Information for Algorithmically Composing Multiple-
Choice Cloze Items Proceedings of the Second
Workshop on Building Educational Applications Us-
ing NLP. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press. Draft available
at http://www-csli.stanford.edu/?hinrich/information-
retrieval-book.html.
Eleni Miltsakaki and Audrey Troutt. 2007. Read-X: Au-
tomatic Evaluation of Reading Difficulty of Web Text.
Proceedings of E-Learn 2007, sponsored by the Asso-
ciation for the Advancement of Computing in Educa-
tion. Quebec, Canada.
Matthew Peacock. 1997. The effect of authentic mate-
rials on the motivation of EFL learners. ELT Journal
51(2):144-156. Oxford University Press.
Kathleen M. Sheehan, Irene Kostin, Yoko Futagi. 2007.
SourceFinder: A Construct-Driven Approach for Lo-
cating Appropriately Targeted Reading Comprehen-
sion Source Texts. Proceedings of the SLaTE Work-
shop on Speech and Language Technology in Educa-
tion. Carnegie Mellon University and International
Speech Communication Association (ISCA).
88
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 594?604,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Predicting a Scientific Community?s Response to an Article
Dani Yogatama Michael Heilman Brendan O?Connor Chris Dyer
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dyogatama,mheilman,brenocon,cdyer}@cs.cmu.edu
Bryan R. Routledge
Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
routledge@cmu.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We consider the problem of predicting mea-
surable responses to scientific articles based
primarily on their text content. Specif-
ically, we consider papers in two fields
(economics and computational linguistics)
and make predictions about downloads and
within-community citations. Our approach is
based on generalized linear models, allowing
interpretability; a novel extension that cap-
tures first-order temporal effects is also pre-
sented. We demonstrate that text features
significantly improve accuracy of predictions
over metadata features like authors, topical
categories, and publication venues.
1 Introduction
Written communication is an essential component
of the complex social phenomenon of science. As
such, natural language processing is well-positioned
to provide tools for understanding the scientific pro-
cess, by analyzing the textual artifacts (papers, pro-
ceedings, etc.) that it produces. This paper is about
modeling collections of scientific documents to un-
derstand how their textual content relates to how a
scientific community responds to them. While past
work has often focused on citation structure (Borner
et al, 2003; Qazvinian and Radev, 2008), our em-
phasis is on the text content, following Ramage et
al. (2010) and Gerrish and Blei (2010).
Instead of task-independent exploratory data anal-
ysis (e.g., topic modeling) or multi-document sum-
marization, we consider supervised models of the
collective response of a scientific community to a
published article. There are many measures of im-
pact of a scientific paper; ours come from direct
measurements of the number of downloads (from
an established website where prominent economists
post papers before formal publication) and citations
(within a fixed scientific community). We adopt a
discriminative approach based on generalized lin-
ear models that can make use of any text or meta-
data features, and show that simple lexical fea-
tures offer substantial power in modeling out-of-
sample response and in forecasting response for fu-
ture articles. Realistic forecasting evaluations re-
quire methodological care beyond the usual best
practices of train/test separation, and we elucidate
these issues.
In addition, we introduce a new regularization
technique that leverages the intuition that the rela-
tionship between observable features and response
should evolve smoothly over time. This regularizer
allows the learner to rely more strongly on more re-
cent evidence, while taking into account a long his-
tory of training data. Our time series-inspired regu-
larizer is computationally efficient in learning and is
a significant advance over earlier text-driven fore-
casting models that ignore the time variable alto-
gether (Kogan et al, 2009; Joshi et al, 2010).
We evaluate our approaches in two novel experi-
mental settings: predicting downloads of economics
articles and predicting citation of papers at ACL
conferences. Our approaches substantially outper-
594
0
15
00
4 9
log(# downloads)
# d
oc
s.
0
25
00
0 18
# citations
# d
oc
s.
Figure 1: Left: the distribution of log download counts
for papers in the NBER dataset one year after post-
ing. Right: the distribution of within-dataset citations of
ACL papers within three years of publication (outliers ex-
cluded for readability).
form text-ignorant baselines on ground-truth predic-
tions. Our time series models permit flexibility in
features and offer a novel and perhaps more inter-
pretable view of the data than summary statistics.
2 Data
We make use of two collections of scientific litera-
ture, one from the economics domain, and the other
from computational linguistics and natural language
processing. Statistics are summarized in Table 1.
2.1 NBER
Our first dataset consists of research papers in eco-
nomics from the National Bureau of Economic
Research (NBER) from 1999 to 2009 (http://
www.nber.org). Approximately 1,000 research
economists are affiliated with the NBER. New
NBER working papers are posted to the website
weekly. The papers are not yet peer-reviewed, but
given the prominence of many economists affiliated
with the NBER, many of the papers are widely read.
Text from the abstracts of the papers and related
metadata are publicly available. Full text is available
to subscribers (universities typically have access).
The NBER provided us with download statistics
for these papers. For each paper, we computed
the total number of downloads in the first year af-
ter each paper?s posting.1 The download counts are
log-normally distributed, as shown in Figure 1, and
so our regression models (?3) minimize squared er-
rors in the log space. Our download logs begin in
1For the vast majority of papers, most of the downloads oc-
cur soon after the paper?s posting. We explored different mea-
sures with different download windows (two years, for exam-
ple) with broadly similar results. We leave a more detailed anal-
ysis of the time series patterns of downloads to future work.
Dataset # Docs. Avg. #
Words
Response
NBER 8,814 155 # downloads in first
year (mean 761)
ACL 4,026 3,966 at least 1 citation in
first 3 years? (54% no)
Table 1: Descriptive statistics about the datasets.
1999. We use the 8,814 papers from 1999?2009 pe-
riod (there are 16,334 papers in the full dataset dat-
ing back to 1985). We only use text from the ab-
stracts, since we were able to obtain full texts for
just a portion of the papers, and since the OCR of
the full texts we do have is very noisy.
2.2 ACL
Our second dataset consists of research papers
from the Association for Computational Linguis-
tics (ACL) from 1980 to 2006 (Radev et al, 2009a;
Radev et al, 2009b). We have the full texts for pa-
pers (OCR output) as well as structured citation data.
There are 15,689 papers in the whole dataset. For
the citation prediction task, we include conference
papers from ACL, EACL, HLT, and NAACL.2 We
remove journal papers, since they are characteristi-
cally different from conference papers, as well as
workshop papers. We do include short papers, in-
teractive demo session papers, and student research
papers that are included in the companion volumes
for these conferences (such papers are cited less than
full papers, but many are still cited). The resulting
dataset contains 4,026 papers. The number of pa-
pers in each year varies because not all conferences
are annual.
We look at citations in the three-year window fol-
lowing publication, excluding self-citations and only
considering citations from papers within these con-
ferences. Figure 1 shows a histogram; note that
many papers (54%) are not cited at all, and the dis-
tribution of citations per paper is neither normal nor
log-normal. We organize the papers into two classes:
those with zero citations and those with non-zero ci-
tations in the three-year window.
2EMNLP is a relatively recent conference, and, in this col-
lection, complete data for its papers postdate the end of the last
training period, so we chose to exclude it from our dataset.
595
3 Model
Our forecasting approach is based on generalized
linear models for regression and classification. The
models are trained with an `2-penalty, often called
a ?ridge? model (Hoerl and Kennard, 1970).3 For
the NBER data, where (log) number of downloads is
nearly a continuous measure, we use linear regres-
sion. For the ACL data, where response is the bi-
nary cited-or-not variable we use logistic regression,
often referred to as a ?maximum entropy? model
(Berger et al, 1996) or a log-linear model. We
briefly review the class of models. Then, we de-
scribe a time series model appropriate for time series
data.
3.1 Generalized Linear Models
Consider a model that predicts a response y given a
vector input x = ?x1, . . . , xd? ? Rd. Our models
are linear functions of x and parameterized by the
vector ?. Given a corpus of M document features,
X , and responses Y , we estimate:
?? = argmin? R(?) + L(?,X, Y ) (1)
where L is a model-dependent loss function and R
is a regularization penalty to encourage models with
small weight vectors. We describe models and loss
functions first and then turn to regularization.
For the NBER data, the (log) number of down-
loads is continuous, and so we use least-squares
linear regression model. The loss function is the
sum of the squared errors for the M documents in
our training data: L(?,X, Y ) = ?Mi=1(yi ? y?i)2,
where the prediction rule for new documents is:
y? =
?d
j=0 ?jxj . Probabilistically, this equates to an
assumption that ?>x is the mean of a normal (i.e.,
Gaussian) distribution from which random variable
y is drawn.
For the ACL data, we predict y from a discrete
set C (specifically, the binary set of zero citations or
more than zero citations), and we use logistic regres-
sion. This model assumes that for the ith training
input xi, the output yi is drawn according to:
p(yi | xi) =
(
exp?>c xi
) /(?
c??C exp?>c?xi
)
3Preliminary experiments found no consistent benefit from
`1 (?lasso?) models, though we note that `1-regularization leads
to sparse, compact models that may be more interpretable.
where there is a feature vector ?c for each class
c ? C. Under this interpretation, parameter esti-
mation is maximum a posteriori inference for ?,
and R(?) is a log-prior for the weights. The loss
function is the negative log likelihood for the M
documents: L(?,X, Y ) = ??Mi=1 log p(yi | xi).
The prediction rule for a new document is: y? =
argmaxc?C
?d
j=0 ?c,jxj . Generalized linear mod-
els and penalized regression are well-studied with
an extensive literature (Mccullagh and Nelder, 1989;
Hastie et al, 2009). We leave other types of mod-
els, such as Poisson (Cameron and Trivedi, 1998)
or ordinal (McCullagh, 1980) regression models, to
future work.
3.2 Ridge Regression
With large numbers of features, regularization is
crucial to avoid overfitting. In ridge regression (Ho-
erl and Kennard, 1970), a standard method to which
we compare the time series regularization discussed
in ?3.3, the penalty R(?) is proportional to the `2-
norm of ?:
R(?) = ????2 = ?
?
j ?2j
where ? is a regularization hyperparameter that is
tuned on development data or by cross-validation.4
This penalty pushes many ?j close (but not com-
pletely) to zero. In practice, we multiply the penalty
by the number of examples M to facilitate tuning of
?.
The ridge linear regression model can be inter-
preted probabilistically as each coefficient ?j is
drawn i.i.d. from a normal distribution with mean
0 and variance 2??1.
3.3 Time Series Regularization
A simple way to capture temporal variation is to con-
join traditional features with a time variable. Here,
we divide the dataset into T time steps (years). In the
new representation, the feature space expands from
Rd to RT?d. For a document published at year t, the
elements of x are non-zero only for those features
that correspond to year-t; that is xt?,j = 0 for all
t? 6= t.
4The linear regression has a bias ?0 that is always active.
The logistic regression also has an unpenalized bias ?c,0 for
each class c. This weight is not regularized.
596
Estimating this model with the new features using
the `2-penalty would be effectively estimating sepa-
rate models for each year under the assumption that
each ?t,j is independent; even for features that dif-
fered only temporally (e.g., ?t,j and ?t+1,j).
In this work, we apply time series regularization
to GLMs, enabling models that have coefficients that
change over time but prefer gradual changes across
time steps. Boyd and Vandenberghe (2004, ?6.3) de-
scribe a general version of this sort of regularizer.
To our knowledge, such regularizers have not previ-
ously been applied to temporal modeling of text.
The time series regularization penalty becomes:
R(?) = ?
T?
t=1
d?
j=1
?2t,j+??
T?
t=2
d?
j=0
(?t,j ? ?t?1,j)2
It includes a standard `2-penalty on the coefficients,
and a penalty for differences between coefficients
for adjacent time steps to induce smooth changes.5
Similar to the previous model, in practice, we mul-
tiply the regularization constant ? by MT to facili-tate tuning of ? for datasets with different numbers
of examples M and numbers of time steps T . The
new parameter, ?, controls the smoothness of the es-
timated coefficients. Setting ? to zero imposes no
penalty for time-variation in the coefficients and re-
sults in independent ridge regressions at each time
step. Also, when the number of examples is con-
stant across time steps, setting a large ? parameter
(? ? ?) results in a single ridge regression over all
years since it imposes ?t,j = ?t+1,j for all t ? T .
The partial derivative is:
?R/??t,j = 2??t,j
+ 1{t > 1}2??(?t,j ? ?t?1,j)
+ 1{t < T}2??(?t,j ? ?t+1,j)
This time series regularization can be applied more
generally, not just to linear and logistic regression.
With either ridge regularization or this time se-
ries regularization scheme, Eq. 1 is an unconstrained
convex optimization problem for the linear models
5Our implementation of the time series regularizer does not
penalize the magnitude of the weight for the bias feature (as in
ridge regression). It does, however, penalize the difference in
the bias weight between time steps (as with other features).
?
1
?
2
?
3
?
T
Y
1
Y
2
Y
3
Y
T
...
X
1
X
2
X
3
X
T
?,?
Figure 2: Time series regression as a graphical model;
the variables Xt and Yt are the sets of feature vectors
and response variables from documents dated t.
we describe here. There exist a number of optimiza-
tion procedures for it; we use the L-BFGS quasi-
Newton algorithm (Liu and Nocedal, 1989).
Probabilistic Interpretation
We can interpret the time series regularization prob-
abilistically as follows. Let the coefficient for the
jth feature over time be ?j = ??1,j , ?2,j , ..., ?T,j?.
?j are draws from a multivariate normal distribu-
tion with a tridiagonal precision matrix ??1 = ? ?
RT?T :
? = ?
?
??????
1 + ? ?? 0 0 . . .
?? 1 + 2? ?? 0 . . .
0 ?? 1 + 2? ?? . . .
0 0 ?? 1 + 2? . . .
... ... ... ... . . .
?
??????
The form of R(?) follows from noting:
?2 log p(?j ;?, ?) = ?>j ??j + constant
The squared difference between adjacent time steps
comes from the off-diagonal entries in the preci-
sion matrix.6 Figure 2 shows a graphical represen-
tation of the time series regularization in our model.
Its Markov chain structure corresponds to the off-
diagonals.
There is a rich literature on time series analysis
(Box et al, 2008; Hamilton, 1994). The prior dis-
tribution over the sequence ??1,j , . . . , ?T,j? that our
regularizer posits is closely linked to a first-order au-
toregressive process, AR(1).
6Consistent with the previous section, we assume that pa-
rameters for different features, ?j and ?k, are independent.
597
NBER ACL
Response log(#downloads+1) 1{#citations > 0}
GLM type normal / squared-loss logistic / log-loss
Metric 1 mean absolute error accuracy
Metric 2 Kendall?s ? Kendall?s ?
Table 2: Summary of the setup for the NBER download
and ACL citation prediction experiments.
4 Features
NBER metadata features
? Authors? last names. We treat each name as a bi-
nary feature. If a paper has multiple authors, all
authors are used and they have equal weights re-
gardless of their ordering.
? NBER program(s).7 There are 19 major re-
search programs at the NBER (e.g., Monetary
Economics, Health Economics, etc.).
ACL metadata features
? Authors? last names as binary features.
? Conference venues. We use first letter of the ACL
anthology paper ID, which correlates with its con-
ference venue (e.g., P for the ACL main confer-
ence, H for the HLT conference, etc.).8
Text features
? Binary indicator features for the presence of each
unigram, bigram, and trigram. For the NBER
data, we have separate features for titles and ab-
stracts. For the ACL data, we have separate fea-
tures for titles and full texts. We pruned text fea-
tures by document frequency (details in ?5).
? Log transformed word counts. We include fea-
tures for the numbers of words in the title and the
abstract (NBER) or the full text (ACL).
7Almost all NBER papers are tagged with one or more pro-
grams (we assign untagged papers a ?null? tag). The complete
list of NBER programs can be found at http://www.nber.
org/programs
8Papers in the ACL dataset have a tag which shows which
workshop, conference, or journal they appeared in. However,
sometimes a conference is jointly held with another confer-
ence, such that meta information in the dataset is different even
though the conference is the same. For this reason, we simply
use the first letter of the paper ID.
5 Experiments
For each of the datasets in ?2, we test our models
for two tasks: forecasting about future papers (i.e.,
making predictions about papers that appeared af-
ter a training dataset) and modeling held-out papers
from the past (i.e., making predictions within the
same time period as the training dataset, on held-out
examples).
For the NBER dataset, the task is to predict the
number of downloads a paper will receive in its first
year after publication. For the ACL dataset, the task
is to predict whether a paper will be cited at all (by
another ACL paper in our dataset) within the first
three years after its publication. To our knowledge,
clean, reliable citation counts are not available for
the NBER dataset; nor are download statistics avail-
able for the ACL dataset. Table 2 summarizes the
variables of interest, model types, and evaluation
metrics for the tasks.
5.1 Extrapolation
The lag between a paper?s publication and when its
outcome (download or citation count) can be ob-
served poses a unique methodological challenge.
Consider predicting the number of downloads over
g future time steps. If t is the time of forecasting,
we can observe the texts of all articles published be-
fore t. However, any article published in the interval
[t ? g, t] is too recent for the outcome measurement
of y to be taken. We refer to the interval [t? g, t] as
the ?forecast gap?. Since recent articles are some-
times the most relevant predictions at t, we do not
want to ignore them. Consider a paper at time step
t?, t?g < t? < t. To extrapolate its number of down-
loads, we consider the observed number in [t?, t], and
then estimate the ratio r of downloads that occur in
the first t?t? time steps, against the first g time steps,
using the fully observed portion of the training data.
We then scale the observed downloads during [t?, t]
by r?1 to extrapolate. The same method is used to
extrapolate citation counts.
In preliminary experiments, we observed that ex-
trapolating responses for papers in the forecast gap
led to better performance in general. For example,
for the ridge regressions trained on all past years
with the full feature set, the error dropped from 262
to 259 when using extrapolation compared to with-
598
out extrapolation. Also, the extrapolated download
counts were quite close to the true values (which we
have but do not use because of the forecast gap): for
example, the mean absolute error of the extrapolated
responses was 99 when extrapolated based on the
median of the fully observed portion of the training
data (measured monthly).
5.2 Forecasting NBER Downloads
In our first set of experiments, we predict the number
of downloads of an NBER paper within one year of
its publication.
We compare four approaches for predicting
downloads. The first is a baseline that simply uses
the median of the log of the training and develop-
ment data as the prediction. The second and third
use GLMs with ridge regression-style regularization
(?3.2), trained on all past years (?all years?) and on
the single most recent past year (?one year?), respec-
tively. The last model (?time series?) is a GLM with
time series regularization (?3.3).
We divided papers by year. Figure 3 illustrates the
experimental setup. We held out a random 20% of
papers for each year from 1999?2007 as a test set for
the task of modeling the past. To define the feature
set and tune hyperparameters, we used the remain-
ing 80% of papers from 1999?2005 as our training
data and the remaining papers in 2006 as our devel-
opment data. After pruning,9 we have 37,251 to-
tal features, of which 2,549 are metadata features.
When tuning hyperparameters, we simulated the ex-
istence of a forecast gap by using extrapolated re-
sponses for papers in the last year of the training
data instead of their true responses. We considered
? ? 5{2,1,...,?5,?6}, and ? ? 5{3,2,...,?1,?2} and se-
lected those that led to the best performance on the
development set.
We then used the selected feature set and hyperpa-
rameters to test the forecasting and modeling capa-
bilities of each model. For forecasting, we predicted
numbers of downloads of papers in 2008 and 2009.
We used the baseline median, ridge regression, and
time series regularization models trained on papers
in 1999?2007 and 1999?2008, respectively. We
treated the last year of the training data (2007 and
9For NBER, text features appearing in less than 0.1% or
more than 99.9% of the training documents were removed. For
ACL, the thresholds were 2% and 98%.
training
modeling test (unused)
gap dev.
trr taitan tag
training gap test
trr tai tam tao
ddd
ddd
modeling test (unused)
oae
lae
oae
lae
training gap test
trr tamddd
modeling test
oae
lae
tao tar
NBER Experiments
ACL Experiments
training
modeling test (unused)
dev.
toa tro ta ddd
oae
lae
s(u)p)v.(uu)p)vHuman Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 609?617,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Good Question! Statistical Ranking for Question Generation
Michael Heilman Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{mheilman,nasmith}@cs.cmu.edu
Abstract
We address the challenge of automatically
generating questions from reading materials
for educational practice and assessment. Our
approach is to overgenerate questions, then
rank them. We use manually written rules to
perform a sequence of general purpose syn-
tactic transformations (e.g., subject-auxiliary
inversion) to turn declarative sentences into
questions. These questions are then ranked by
a logistic regression model trained on a small,
tailored dataset consisting of labeled output
from our system. Experimental results show
that ranking nearly doubles the percentage of
questions rated as acceptable by annotators,
from 27% of all questions to 52% of the top
ranked 20% of questions.
1 Introduction
In this paper, we focus on question generation (QG)
for the creation of educational materials for read-
ing practice and assessment. Our goal is to gener-
ate fact-based questions about the content of a given
article. The top-ranked questions could be filtered
and revised by educators, or given directly to stu-
dents for practice. Here we restrict our investigation
to questions about factual information in texts.
We begin with a motivating example. Consider
the following sentence from the Wikipedia article on
the history of Los Angeles:1 During the Gold Rush
years in northern California, Los Angeles became
known as the ?Queen of the Cow Counties? for its
role in supplying beef and other foodstuffs to hungry
miners in the north.
Consider generating the following question from
that sentence: What did Los Angeles become known
1?History of Los Angeles.? Wikipedia. 2009. Wikimedia
Foundation, Inc. Retrieved Nov. 17, 2009 from: http://en.
wikipedia.org/wiki/History_of_Los_Angeles.
as the ?Queen of the Cow Counties? for?
We observe that the QG process can be viewed
as a two-step process that essentially ?factors? the
problem into simpler components.2 Rather than si-
multaneously trying to remove extraneous informa-
tion and transform a declarative sentence into an in-
terrogative one, we first transform the input sentence
into a simpler sentence such as Los Angeles become
known as the ?Queen of the Cow Counties? for its
role in supplying beef and other foodstuffs to hungry
miners in the north, which we then can then trans-
form into a more succinct question.
Question transformation involves complex long
distance dependencies. For example, in the ques-
tion about Los Angeles, the word what at the begin-
ning of the sentence is a semantic argument of the
verb phrase known as . . . at the end of the ques-
tion. The characteristics of such phenomena are (ar-
guably) difficult to learn from corpora, but they have
been studied extensively in linguistics (Ross, 1967;
Chomsky, 1973). We take a rule-based approach in
order to leverage this linguistic knowledge.
However, since many phenomena pertaining to
question generation are not so easily encoded with
rules, we include statistical ranking as an integral
component. Thus, we employ an overgenerate-and-
rank approach, which has been applied successfully
in areas such as generation (Walker et al, 2001;
Langkilde and Knight, 1998) and syntactic parsing
(Collins, 2000). Since large datasets of the appro-
priate domain, style, and form of questions are not
available to train our ranking model, we learn to rank
from a relatively small, tailored dataset of human-
labeled output from our rule-based system.
The remainder of the paper is organized as fol-
2The motivating example does not exhibit lexical semantic
variations such as synonymy. In this work, we do not model
complex paraphrasing, but believe that paraphrase generation
techniques could be incorporated into our approach.
609
lows. ?2 clarifies connections to prior work and enu-
merates our contributions. ?3 discusses particular
terms and conventions we will employ. ?4 discusses
rule-based question transformation. ?5 describes the
data used to learn and to evaluate our question rank-
ing model, and ?6 then follows with details on the
ranking approach itself. We then present and dis-
cuss results from an evaluation of ranked question
output in ?7 and conclude in ?8.
2 Connections with Prior Work
The generation of questions by humans has long mo-
tivated theoretical work in linguistics (e.g., Ross,
1967), particularly work that portrays questions as
transformations of canonical declarative sentences
(Chomsky, 1973).
Questions have also been a major topic of study
in computational linguistics, but primarily with the
goal of answering questions (Dang et al, 2008).
While much of the question answering research has
focused on retrieval or extraction (e.g., Ravichan-
dran and Hovy, 2001; Hovy et al, 2001), mod-
els of the transformation from answers to questions
have also been developed (Echihabi and Marcu,
2003) with the goal of finding correct answers given
a question (e.g., in a source-channel framework).
Also, Harabagiu et al (2005) present a system that
automatically generates questions from texts to pre-
dict which user-generated questions the text might
answer. In such work on question answering, ques-
tion generation models are typically not evaluated
for their intrinsic quality, but rather with respect to
their utility as an intermediate step in the question
answering process.
QG is very different from many natural language
generation problems because the input is natural lan-
guage rather than a formal representation (cf. Reiter
and Dale, 1997). It is also different from some other
tasks related to generation: unlike machine transla-
tion (e.g., Brown et al, 1990), the input and output
for QG are in the same language, and their length
ratio is often far from one to one; and unlike sen-
tence compression (e.g., Knight and Marcu, 2000),
QG may involve substantial changes to words and
their ordering, beyond simple removal of words.
Some previous research has directly approached
the topic of generating questions for educational
purposes (Mitkov and Ha, 2003; Kunichika et al,
2004; Gates, 2008; Rus and Graessar, 2009; Rus and
Lester, 2009), but to our knowledge, none has in-
volved statistical models for choosing among output
candidates. Mitkov et al (2006) demonstrated that
automatic generation and manual correction of ques-
tions can be more time-efficient than manual author-
ing alone. Much of the prior QG research has evalu-
ated systems in specific domains (e.g., introductory
linguistics, English as a Second Language), and thus
we do not attempt empirical comparisons. Exist-
ing QG systems model their transformations from
source text to questions with many complex rules
for specific question types (e.g., a rule for creating
a question Who did the Subject Verb? from a
sentence with SVO word order and an object refer-
ring to a person), rather than with sets of general
rules.
This paper?s contributions are as follows:
? We apply statistical ranking to the task of gen-
erating natural language questions. In doing so,
we show that question rankings are improved by
considering features beyond surface characteris-
tics such as sentence lengths.
? We model QG as a two-step process of first
simplifying declarative input sentences and then
transforming them into questions, the latter step
being achieved by a sequence of general rules.
? We incorporate linguistic knowledge to explic-
itly model well-studied phenomena related to long
distance dependencies in WH questions, such as
noun phrase island constraints.
? We develop a QG evaluation methodology, in-
cluding the use of broad-domain corpora.
3 Definitions and Conventions
The term ?source sentence? refers to a sentence
taken directly from the input document, from which
a question will be generated (e.g., Kenya is located
in Africa.). The term ?answer phrase? refers to
phrases in declarative sentences which may serve
as targets for WH-movement, and therefore as possi-
ble answers to generated questions (e.g., in Africa).
The term ?question phrase? refers to the phrase con-
taining the WH word that replaces an answer phrase
(e.g., Where in Where is Kenya located?).
610
To represent the syntactic structure of sentences,
we use simplified Penn Treebank-style phrase struc-
ture trees, including POS and category labels, as
produced by the Stanford Parser (Klein and Man-
ning, 2003). Noun phrase heads are selected using
Collins? rules (Collins, 1999).
To implement the rules for transforming source
sentences into questions, we use Tregex, a tree
query language, and Tsurgeon, a tree manipula-
tion language built on top of Tregex (Levy and An-
drew, 2006). The Tregex language includes vari-
ous relational operators based on the primitive re-
lations of immediate dominance (denoted ?<?) and
immediate precedence (denoted ?.?). Tsurgeon
adds the ability to modify trees by relabeling, delet-
ing, moving, and inserting nodes.
4 Rule-based Overgeneration
Many useful questions can be viewed as lexical, syn-
tactic, or semantic transformations of the declarative
sentences in a text. We describe how to model this
process in two steps, as proposed in ?1.3
4.1 Sentence Simplification
In the first step for transforming sentences into ques-
tions, each of the sentences from the source text is
expanded into a set of derived declarative sentences
(which also includes the original sentence) by al-
tering lexical items, syntactic structure, and seman-
tics. Many existing NLP transformations could po-
tentially be exploited in this step, including sentence
compression, paraphrase generation, or lexical se-
mantics for word substitution.
In our implementation, a set of transformations
derive a simpler form of the source sentence by
removing phrase types such as leading conjunc-
tions, sentence-level modifying phrases, and apposi-
tives. Tregex expressions identify the constituents
to move, alter, or delete. Similar transformations
have been utilized in previous work on headline gen-
eration (Dorr and Zajic, 2003) and summarization
(Toutanova et al, 2007).
To enable questions about syntactically embedded
content, our implementation also extracts a set of
declarative sentences from any finite clauses, rela-
3See Heilman and Smith (2009) for details on the rule-based
component.
tive clauses, appositives, and participial phrases that
appear in the source sentence. For example, it trans-
forms the sentence Selling snowballed because of
waves of automatic stop-loss orders, which are trig-
gered by computer when prices fall to certain lev-
els into Automatic stop-loss orders are triggered by
computer when prices fall to certain levels, from
which the next step will produce What are triggered
by computer when prices fall to certain levels?.
4.2 Question Transformation
In the second step, the declarative sentences de-
rived in step 1 are transformed into sets of ques-
tions by a sequence of well-defined syntactic and
lexical transformations (subject-auxiliary inversion,
WH-movement, etc.). It identifies the answer phrases
which may be targets for WH-movement and con-
verts them into question phrases.4
In the current implementation, answer phrases can
be noun phrases or prepositional phrases, which en-
ables who, what, where, when, and how much ques-
tions. The system could be extended to transform
other types of phrases into other types of questions
(e.g., how, why, and what kind of ). It should be
noted that the transformation from answer to ques-
tion is achieved by applying a series of general-
purpose rules. This would allow, for example, the
addition of a rule to generate why questions that
builds off of the existing rules for subject-auxiliary
inversion, verb decomposition, etc. In contrast, pre-
vious QG approaches have employed separate rules
for specific sentence types (e.g., Mitkov and Ha,
2003; Gates, 2008).
For each sentence, many questions may be pro-
duced: there are often multiple possible answer
phrases, and multiple question phrases for each an-
swer phrase. Hence many candidates may result
from the transformations.
These rules encode a substantial amount of lin-
guistic knowledge about the long distance depen-
dencies prevalent in questions, which would be chal-
lenging to learn from existing corpora of questions
and answers consisting typically of only thousands
of examples (e.g., Voorhees, 2003).
Specifically, the following sequence of transfor-
4We leave the generation of correct answers and distractors
to future work.
611
During the Gold Rush years in northern California, 
Los Angeles became known as the "Queen of the 
Cow Counties" for its role in supplying beef and 
other foodstuffs to hungry miners in the north.
Los Angeles became known as the "Queen of the 
Cow Counties" for its role in supplying beef and 
other foodstuffs to hungry miners in the north.
Los Angeles became known as the "QotCC" for 
Los Angeles did become known as the "QotCC" for 
did Los Angeles become known as the "QotCC" for
What did Los Angeles become known as the  "QotCC" for?
Source Sentence
Answer Phrase: its role...
(other possibilities)
(other possibilities)
(other possibilities)
Sentence Simplification
Answer Phrase Selection
Subject-Auxiliary
Inversion
Main Verb Decomposition
Movement and Insertion of Question Phrase
Statistical Ranking
(other possibilities)
1. What became known as ...?
2. What did Los Angeles become known...for?
3. What did Los Angeles become known...as?
4. During the Gold Rush years... ?
5. Whose role in supplying beef...?
...
NP
S
VBD
VP
PP
INVBN
VP
PP
IN NP
VB
VP
PP
INVBN
VP
PP
IN NPNPVBDWP
WHNP
SQ
SBARQ
Figure 1: An illustration of the sequence of steps for generating questions. For clarity, trees are not shown for all steps.
Also, while many questions may be generated from a single source sentence, only one path is shown.
mations is performed, as illustrated in Figure 1:
mark phrases that cannot be answer phrases due to
constraints on WH movement (?4.2.1, not in figure);
select an answer phrase, remove it, and generate pos-
sible question phrases for it (?4.2.2); decompose the
main verb; invert the subject and auxiliary verb; and
insert one of the possible question phrases.
Some of these steps do not apply in all cases. For
example, no answer phrases are removed when gen-
erating yes-no questions.
4.2.1 Marking Unmovable Phrases
In English, various constraints determine whether
phrases can be involved in WH-movement and other
phenomena involving long distance dependencies.
In a seminal dissertation, Ross (1967) described
many of these phenomena. Goldberg (2006) pro-
vides a concise summary of them.
For example, noun phrases are ?islands? to
movement, meaning that constituents dominated
by a noun phrase typically cannot undergo WH-
movement. Thus, from John liked the book that I
gave him, we generate What did John like? but not
*Who did John like the book that gave him?.
We operationalize this linguistic knowledge to ap-
propriately restrict the set of questions produced.
Eight Tregex expressions mark phrases that cannot
be answer phrases due to WH-movement constraints.
For example, the following expression encodes
the noun phrase island constraint described above,
where unmv indicates unmovable noun phrases:
NP << NP=unmv.
4.2.2 Generating Possible Question Phrases
After marking unmovable phrases, we iteratively
remove each possible answer phrase and generate
possible question phrases from it. The system an-
notates the source sentence with a set of entity types
taken from the BBN Identifinder Text Suite (Bikel
et al, 1999) and then uses these entity labels along
with the syntactic structure of a given answer phrase
to generate zero or more question phrases, each of
which is used to generate a final question. (This step
is skipped for yes-no questions.)
5 Rating Questions for Evaluation and
Learning to Rank
Since different sentences from the input text, as well
as different transformations of those sentences, may
be more or less likely to lead to high-quality ques-
tions, each question is scored according to features
of the source sentence, the input sentence, the ques-
tion, and the transformations used in its generation.
The scores are used to rank the questions. This is
612
an example of an ?overgenerate-and-rank? strategy
(Walker et al, 2001; Langkilde and Knight, 1998).
This section describes the acquisition of a set
of rated questions produced by the steps described
above. Separate portions of these labeled data will
be used to develop a discriminative question ranker
(?6), and to evaluate ranked lists of questions (?7).
Fifteen native English-speaking university stu-
dents rated a set of questions produced from steps
1 and 2, indicating whether each question exhibited
any of the deficiencies listed in Table 1.5 If a ques-
tion exhibited no deficiencies, raters were asked to
label it ?acceptable.? Annotators were asked to read
the text of a newswire or encyclopedia article (?5.1
describes the corpora used), and then rate approxi-
mately 100 questions generated from that text. They
were asked to consider each question independently,
such that similar questions about the same informa-
tion would receive similar ratings.
For a predefined training set, each question was
rated by a single annotator (not the same for each
question), leading to a large number of diverse ex-
amples. For the test set, each question was rated by
three people (again, not the same for each question)
to provide a more reliable gold standard. To assign
final labels to the test data, a question was labeled as
acceptable only if a majority of the three raters rated
it as acceptable (i.e., without deficiencies).6
An inter-rater agreement of Fleiss?s ? = 0.42
was computed from the test set?s acceptability rat-
ings. This value corresponds to ?moderate agree-
ment? (Landis and Koch, 1977) and is somewhat
lower than for other rating schemes.7
5.1 Corpora
The training and test datasets consisted of 2,807
and 428 questions, respectively. The questions were
5The ratings from one person were excluded due to an ex-
tremely high rate of accepting questions as error-free and other
irregularities.
6The percentages in Table 1 do not add up to 100% for two
reasons: first, questions are labeled acceptable in the test set
only if the majority of raters labeled them as having no defi-
ciencies, rather than the less strict criterion of requiring no de-
ficiencies to be identified by a majority of raters; second, the
categories are not mutually exclusive.
7E.g., Dolan and Brockett (2005) and Glickman et al (2005)
report ? values around 0.6 for paraphrase identification and tex-
tual entailment, respectively.
generated from three corpora.
The first corpus was a random sample from the
featured articles in the English Wikipedia8 with be-
tween 250 and 2,000 word tokens. This English
Wikipedia corpus provides expository texts written
at an adult reading level from a variety of domains,
which roughly approximates the prose that a sec-
ondary or post-secondary student would encounter.
By choosing from the featured articles, we intended
to select well-edited articles on topics of general in-
terest. The training set included 1,328 questions
about 12 articles, and the test set included 120 ques-
tions about 2 articles from this corpus.
The second corpus was a random sample from the
articles in the Simple English Wikipedia of simi-
lar length. This corpus provides similar text but at
a reading level corresponding to elementary educa-
tion or intermediate second language learning.9 The
training set included 1,195 questions about 16 arti-
cles, and the test set included 118 questions about 2
articles from this corpus.
The third corpus was Section 23 of the Wall Street
Journal data in the Penn Treebank (Marcus et al,
1993).10 The training set included 284 questions
about 8 articles, and the test set included 190 ques-
tions about 2 articles from this corpus.
6 Ranking
We use a discriminative ranker to rank questions,
similar to the approach described by Collins (2000)
for ranking syntactic parses. Questions are ranked
by the predictions of a logistic regression model of
question acceptability. Given the question q and
source text t, the model defines a binomial distribu-
tion p(R | q, t), with binary random variableR rang-
ing over a (?acceptable?) and u (?unacceptable?).
We estimate the parameters by optimizing the reg-
ularized log-likelihood of the training data (cf. ?5.1)
with a variant of Newton?s method (le Cessie and
8The English and Simple English Wikipedia data were
downloaded on December 16, 2008 from http://en.
wikipedia.org and http://simple.wikipedia.
org, respectively.
9The subject matter of the articles in the two Wikipedia cor-
pora was not matched.
10In separate experiments with the Penn Treebank, gold-
standard parses led to an absolute increase of 15% in the per-
centage of acceptable questions (Heilman and Smith, 2009).
613
Question Deficiency Description %
Ungrammatical The question is not a valid English sentence. (e.g., In what were nests excavated exposed to the
sun? from . . . eggs are usually laid . . . , in nests excavated in pockets of earth exposed to the
sun.. This error results from the incorrect attachment by the parser of exposed to the sun to the
verb phrase headed by excavated)
14.0
Does not make sense The question is grammatical but indecipherable. (e.g., Who was the investment?) 20.6
Vague The question is too vague to know exactly what it is asking about, even after reading the article
(e.g., What do modern cities also have? from . . . , but modern cities also have many problems).
19.6
Obvious answer The correct answer would be obvious even to someone who has not read the article (e.g., a
question where the answer is obviously the subject of the article).
0.9
Missing answer The answer to the question is not in the article. 1.4
Wrong WH word The question would be acceptable if the WH phrase were different (e.g., a what question with a
person?s name as the answer).
4.9
Formatting There are minor formatting errors (e.g., with respect to capitalization, punctuation). 8.9
Other The question was unacceptable for other reasons. 1.2
None The question exhibits none of the above deficiencies and is thus acceptable. 27.3
Table 1: Deficiencies a question may exhibit, and the percentages of test set questions labeled with them.
van Houwelingen, 1997). In our experiments, the
regularization constant was selected through cross-
validation on the training data.
The features used by the ranker can be organized
into several groups described in this section. This
feature set was developed by an analysis of ques-
tions generated from the training set. The num-
bers of distinct features for each type are denoted in
parentheses, with the second number, after the ad-
dition symbol, indicating the number of histogram
features (explained below) for that type.
Length Features (3 + 24) The set includes integer
features for the numbers of tokens in the question,
the source sentence, and the answer phrase from
which the WH phrase was generated. These num-
bers of tokens will also be used for computing the
histogram features discussed below.
WH Words (9 + 0) The set includes boolean fea-
tures for the presence of each possible WH word in
the question.
Negation (1 + 0) This is a boolean feature for the
presence of not, never, or no in the question.
N -Gram Language Model Features (6 + 0) The
set includes real valued features for the log like-
lihoods and length-normalized log likelihoods of
the question, the source sentence, and the answer
phrase. Separate likelihood features are included for
unigram and trigram language models. These lan-
guage models were estimated from the written por-
tion of the American National Corpus Second Re-
lease (Ide and Suderman, 2004), which consists of
approximately 20 million tokens, using Kneser and
Ney (1995) smoothing.
Grammatical Features (23 + 95) The set includes
integer features for the numbers of proper nouns,
pronouns, adjectives, adverbs, conjunctions, num-
bers, noun phrases, prepositional phrases, and sub-
ordinate clauses in the phrase structure parse trees
for the question and answer phrase. It also includes
one integer feature for the number of modifying
phrases at the start of the question (e.g., as in At
the end of the Civil War, who led the Union Army?);
three boolean features for whether the main verb is
in past, present, or future tense; and one boolean fea-
ture for whether the main verb is a form of be.
Transformations (8 + 0) The set includes bi-
nary features for the possible syntactic transforma-
tions (e.g., removal of appositives and parentheti-
cals, choosing the subject of source sentence as the
answer phrase).
Vagueness (3 + 15) The set includes integer fea-
tures for the numbers of noun phrases in the ques-
tion, source sentence, and answer phrase that are
potentially vague. We define this set to include pro-
nouns as well as common nouns that are not speci-
fied by a subordinate clause, prepositional phrase, or
possessive. In the training data, we observed many
vague questions resulting from such noun phrases
(e.g., What is the bridge named for?).
614
Histograms In addition to the integer features for
lengths, counts of grammatical types, and counts of
vague noun phrases, the set includes binary ?his-
togram? features for each length or count. These
features indicate whether a count or length exceeds
various thresholds: 0, 1, 2, 3, and 4 for counts; 0,
4, 8, 12, 16, 20, 24, and 28 for lengths. We aim to
account for potentially non-linear relationships be-
tween question quality and these values (e.g., most
good questions are neither very long nor very short).
7 Evaluation and Discussion
This section describes the results of experiments to
evaluate the quality of generated questions before
and after ranking. Results are aggregated across the
3 corpora (?5.1). The evaluation metric we employ
is the percentage of test set questions labeled as ac-
ceptable. For rankings, our metric is the percentage
of the top N% labeled as acceptable, for various N .
7.1 Results for Unranked Questions
First, we present results for the unranked questions
produced by the rule-based overgenerator. As shown
in Table 1, 27.3% of test set questions were labeled
acceptable (i.e., having no deficiencies) by a major-
ity of raters.11
The most frequent deficiencies were ungrammati-
cality (14.0%), vagueness (19.6%), and semantic er-
rors labeled with the ?Does not make sense? cate-
gory (20.6%). Formatting errors (8.9%) were due
to both straightforward issues with pre-processing
and more challenging issues such as failing to iden-
tifying named entities (e.g., Who was nixon?s second
vice president?).
While Table 1 provides data on how often bad
questions were generated, a measure of how often
good questions were not generated would require
knowing the number of possible valid questions. In-
stead, we provide a measure of productivity: the sys-
tem produced an average of 6.0 acceptable questions
per 250 words (i.e., the approximate average number
of words on a single page in a printed book).
7.2 Configurations and Baselines
For ranking experiments, we present results for the
following configurations of features:
1112.1% of test set questions were unanimously acceptable.
All This configuration includes the entire set of
features described in ?6.
Surface Features This configuration includes
only features that can be computed from the sur-
face form of the question, source sentence, and
answer phrase?that is, without hidden linguistic
structures such as parts of speech or syntactic struc-
tures. Specifically, it includes features for lengths,
length histograms, WH words, negation, and lan-
guage model likelihoods.
Question Only This configuration includes all
features of questions, but no features involving the
source sentence or answer phrase (e.g., it does not
include source sentence part of speech counts). It
does not include transformation features.
We also present two baselines for comparison:
Random The expectation of the performance if
questions were ranked randomly.
Oracle The expected performance if all questions
that were labeled acceptable were ranked higher
than all questions that were labeled unacceptable.
7.3 Ranking Results
Figure 2 shows that the percentage of questions
rated as acceptable generally increases as the set
of questions is restricted from the full 428 ques-
tions in the test set to only the top ranked questions.
While 27.3% of all test set questions were accept-
able, 52.3% of the top 20% of ranked questions were
acceptable. Thus, the quality of the top fifth was
nearly doubled by ranking with all the features.
Ranking with surface features also improved
question quality, but to a lesser extent. Thus, unob-
served linguistic features such as parts of speech and
syntax appear to add value for ranking questions.12
The ranker seems to have focused on the ?Does
not make sense? and ?Vague? categories. The
percentage of nonsensical questions dropped from
20.6% to 4.7%, and vagueness dropped from 19.6%
12Ranking with all features was statistically significantly bet-
ter (p < .05) in terms of the percentage of acceptable questions
in the top ranked 20% than ranking with the ?question only?
or ?surface? configurations, or the random baseline, as verified
by computing 95% confidence intervals with the BCa Bootstrap
(Efron and Tibshirani, 1993).
615
50%60%70%
Pct. Rated Acceptable
Oracle All Fe
atures Questio
n Only
Surfac
e Feat
ures
Rando
m
20%30%40%50%60%70% 0
100
200
300
400
Numb
er of T
op-Ra
nked Q
uestio
ns
Oracle All Fe
atures Questio
n Only
Surfac
e Feat
ures
Rando
m
20%30%40%50%60%70% 0
100
200
300
400
Numb
er of T
op-Ra
nked Q
uestio
ns
Oracle All Fe
atures Questio
n Only
Surfac
e Feat
ures
Rando
m
Figure 2: A graph of the percentage of acceptable ques-
tions in the top-N questions in the test set, using various
rankings, for N varying from 0 to the size of the test set.
The percentages become increasingly unstable when re-
stricted to very few questions (e.g., < 50).
to 7.0%, while ungrammaticality dropped from
14.0% to 10.5%, and the other, less prevalent, cat-
egories changed very little.13
7.4 Ablation Study
Ablation experiments were also conducted to study
the effects of removing each of the different types of
features. Table 2 presents the percentages of accept-
able test set questions in the top 20% and top 40%
when they are scored by rankers trained with vari-
ous feature sets that are defined by removing various
feature types from the set of all possible features.
Grammatical features appear to be the most im-
portant: removing them from the feature set resulted
in a 9.0% absolute drop in acceptability in the top
20% of questions, from 52.3% to 43.3%.
Some of the features did not appear to be partic-
ularly helpful, notably the N -gram language model
features. We speculate that they might improve re-
sults when used with a larger, less noisy training set.
Performance did not drop precipitously upon the
removal of any particular feature type, indicating a
high amount of shared variance among the features.
However, removing several types of features at once
led to somewhat larger drops in performance. For
example, using only surface features led to a 12.8%
13We speculate that improvements in syntactic parsing and
entity recognition would reduce the proportion of ungrammati-
cal questions and incorrect WH words, respectively.
Features # Top 20% Top 40%
All 187 52.3 40.8
All ? Length 160 52.3 42.1
All ? WH 178 50.6 39.8
All ? Negation 186 51.7 39.3
All ? Lang. Model 181 51.2 39.9
All ? Grammatical 69 43.2 38.7
All ? Transforms 179 46.5 39.0
All ? Vagueness 169 48.3 41.5
All ? Histograms 53 49.4 39.8
Surface 43 39.5 37.6
Question Only 91 41.9 39.5
Random - 27.3 27.3
Oracle - 100.0 87.3
Table 2: The total numbers of features (#) and the per-
centages of the top 20% and 40% of ranked test set ques-
tions labeled acceptable, for rankers built from variations
of the complete set of features (?All?). E.g., ?All ? WH?
is the set of all features except WH word features.
drop in acceptability in the top 20%, and using only
features of questions led to a 10.4% drop.
8 Conclusion
By ranking the output of rule-based natural lan-
guage generation system, existing knowledge about
WH-movement from linguistics can be leveraged to
model the complex transformations and long dis-
tance dependencies present in questions. Also, in
this overgenerate-and-rank framework, a statistical
ranker trained from a small set of annotated ques-
tions can capture trends related to question quality
that are not easily encoded with rules. In our exper-
iments, we found that ranking approximately dou-
bled the acceptability of the top-ranked questions
generated by our approach.
Acknowledgments
We acknowledge partial support from the Institute
of Education Sciences, U.S. Department of Educa-
tion, through Grant R305B040063 to Carnegie Mel-
lon University; and from the National Science Foun-
dation through a Graduate Research Fellowship for
the first author and grant IIS-0915187 to the second
author. We thank the anonymous reviewers for their
comments.
616
References
D. M. Bikel, R. Schwartz, and R. M. Weischedel. 1999.
An algorithm that learns what?s in a name. Machine
Learning, 34(1-3).
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2).
N. Chomsky. 1973. Conditions on transformations. A
Festschrift for Morris Halle.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. of ICML.
H. T. Dang, D. Kelly, and J. Lin. 2008. Overview of
the TREC 2007 question answering track. In Proc. of
TREC.
W. B. Dolan and C. Brockett. 2005. Automatically con-
structing a corpus of sentential paraphrases. In Proc.
of IWP.
B. Dorr and D. Zajic. 2003. Hedge Trimmer: A parse-
and-trim approach to headline generation. In Proc. of
Workshop on Automatic Summarization.
A. Echihabi and D. Marcu. 2003. A noisy-channel ap-
proach to question answering. In Proc. of ACL.
B. Efron and R. Tibshirani. 1993. An Introduction to the
Bootstrap. Chapman & Hall/CRC.
D. M. Gates. 2008. Generating reading comprehension
look-back strategy questions from expository texts.
Master?s thesis, Carnegie Mellon University.
O. Glickman, I. Dagan, and M. Koppel. 2005. A prob-
abilistic classification approach for lexical textual en-
tailment. In Proc. of AAAI.
A. Goldberg. 2006. Constructions at Work: The Na-
ture of Generalization in Language. Oxford Univer-
sity Press, New York.
S. Harabagiu, A. Hickl, J. Lehmann, and D. Moldovan.
2005. Experiments with interactive question-
answering. In Proc. of ACL.
Michael Heilman and Noah A. Smith. 2009. Ques-
tion generation via overgenerating transformations and
ranking. Technical Report CMU-LTI-09-013, Lan-
guage Technologies Institute, Carnegie Mellon Uni-
versity.
E. Hovy, U. Hermjakob, and C. Lin. 2001. The use of
external knowledge in factoid QA. In Proc. of TREC.
N. Ide and K. Suderman. 2004. The american national
corpus first release. In Proc. of LREC.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
Advances in NIPS 15.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proc. of IEEE Int.
Conf. Acoustics, Speech and Signal Processing.
K. Knight and D. Marcu. 2000. Statistics-based summa-
rization - step one: Sentence compression. In Proc. of
the Seventeenth National Conference on Artificial In-
telligence and Twelfth Conference on Innovative Ap-
plications of Artificial Intelligence.
H. Kunichika, T. Katayama, T. Hirashima, and
A. Takeuchi. 2004. Automated question generation
methods for intelligent English learning systems and
its evaluation. In Proc. of ICCE.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33.
I. Langkilde and Kevin Knight. 1998. Generation that
exploits corpus-based statistical knowledge. In Proc.
of ACL.
S. le Cessie and J. C. van Houwelingen. 1997. Ridge es-
timators in logistic regression. Applied Statistics, 41.
R. Levy and G. Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data struc-
tures. In Proc. of LREC.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19.
R. Mitkov and L. A. Ha. 2003. Computer-aided gen-
eration of multiple-choice tests. In Proc. of the HLT-
NAACL 03 workshop on Building educational appli-
cations using natural language processing.
R. Mitkov, L. A. Ha, and N. Karamanis. 2006. A
computer-aided environment for generating multiple-
choice test items. Natural Language Engineering,
12(2).
D. Ravichandran and E. Hovy. 2001. Learning surface
text patterns for a question answering system. In Proc.
of ACL.
E. Reiter and R. Dale. 1997. Building applied natural
language generation systems. Nat. Lang. Eng., 3(1).
J. R. Ross. 1967. Constraints on Variables in Syntax.
Phd dissertation, MIT, Cambridge, MA.
V. Rus and A. Graessar, editors. 2009. The Question
Generation Shared Task and Evaluation Challenge.
http://www.questiongeneration.org.
V. Rus and J. Lester, editors. 2009. Proc. of the 2nd
Workshop on Question Generation. IOS Press.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,
H. Suzuki, and L. Vanderwende. 2007. The PYTHY
summarization system: Microsoft research at duc
2007. In Proc. of DUC.
E. M. Voorhees. 2004. Overview of the TREC 2003
question answering track. In Proc. of TREC 2003.
M. A. Walker, O. Rambow, and M. Rogati. 2001. Spot:
a trainable sentence planner. In Proc. of NAACL.
617
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1011?1019,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Tree Edit Models for Recognizing Textual Entailments, Paraphrases,
and Answers to Questions
Michael Heilman Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{mheilman,nasmith}@cs.cmu.edu
Abstract
We describe tree edit models for representing
sequences of tree transformations involving
complex reordering phenomena and demon-
strate that they offer a simple, intuitive, and
effective method for modeling pairs of seman-
tically related sentences. To efficiently extract
sequences of edits, we employ a tree kernel
as a heuristic in a greedy search routine. We
describe a logistic regression model that uses
33 syntactic features of edit sequences to clas-
sify the sentence pairs. The approach leads to
competitive performance in recognizing tex-
tual entailment, paraphrase identification, and
answer selection for question answering.
1 Introduction
Many NLP tasks involve modeling relations be-
tween pairs of sentences or short texts in the same
language. Examples include recognizing textual en-
tailment, paraphrase identification, and question an-
swering. Generic approaches are, of course, desir-
able; we believe such approaches are also feasible
because these tasks exhibit some similar semantic
relationships between sentences.
A popular method for such tasks is Tree Edit Dis-
tance (TED), which models sentence pairs by find-
ing a low or minimal cost sequence of editing oper-
ations to transform a tree representation of one sen-
tence (e.g., a dependency or phrase structure parse
tree) into a tree for the other. Unlike grammar-
based models and shallow-feature discriminative ap-
proaches, TED provides an intuitive story for tree
pairs where one tree is derived from the other by a
sequence of simple transformations.
The available operations in standard TED are the
following: insertion of a node, relabeling (i.e., re-
naming) of a node, and deletion (i.e., removal) of a
node. While the restriction to these three operations
permits efficient dynamic programming solutions
for finding a minimum-cost edit sequence (Klein,
1989; Zhang and Shasha, 1989), certain interesting
and prevalent phenomena involving reordering and
movement cannot be elegantly captured. For exam-
ple, consider the following sentence pair, which is
a simplified version of a true entailment (i.e., the
premise entails the hypothesis) in the development
data for the RTE-3 task.
Premise: Pierce built the home for his daughter off
Rossville Blvd, as he lives nearby.
Hypothesis: Pierce lives near Rossville Blvd.
In a plausible dependency tree representation of
the premise, live and Rossville Blvd would be in sep-
arate subtrees under built. In the hypothesis tree,
however, the corresponding nodes would be in a
grandparent-child relationship as part of the same
phrase, lives near Rossville Blvd. In general, one
would expect that short transformation sequences to
provide good evidence of true entailments. How-
ever, to account for the grandparent-child relation-
ship in the hypothesis, TED would produce a fairly
long sequence, relabeling nearby to be near, delet-
ing the two nodes for Rossville Blvd, and then re-
inserting those nodes under near.
We describe a tree edit approach that allows for
more effective modeling of such complex reordering
phenomena. Our approach can find a shorter and
more intuitive edit sequence, relabeling nearby to be
near, and then moving the whole subtree Rossville
Blvd to be a child of near, as shown in Figure 1.
A model should also be able to consider character-
istics of the tree edit sequence other than its overall
length (e.g., how many proper nouns were deleted).
Using a classifier with a small number of syntactic
1011
Pierce lives near Rossville Blvd.
Pierce built the home for his daughter off Rossville Blvd, as he lives nearby.
Pierce built the home for his daughter off Rossville Blvd, as he lives near.
built the home for his daughter off, as Pierce he lives near Rossville Blvd.
Pierce built the home for his daughter off, as he lives near Rossville Blvd.
Piercie lvsinaRoBd.b
uvti hmcfPiinPg,,y2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 20?28,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Identifying High-Level Organizational Elements
in Argumentative Discourse
Nitin Madnani Michael Heilman Joel Tetreault
Educational Testing Service
Princeton, NJ, USA
{nmadnani,mheilman,jtetreault}@ets.org
Martin Chodorow
Hunter College of CUNY
New York, NY, USA
martin.chodorow@hunter.cuny.edu
Abstract
Argumentative discourse contains not only
language expressing claims and evidence, but
also language used to organize these claims
and pieces of evidence. Differentiating be-
tween the two may be useful for many appli-
cations, such as those that focus on the content
(e.g., relation extraction) of arguments and
those that focus on the structure of arguments
(e.g., automated essay scoring). We propose
an automated approach to detecting high-level
organizational elements in argumentative dis-
course that combines a rule-based system and
a probabilistic sequence model in a principled
manner. We present quantitative results on a
dataset of human-annotated persuasive essays,
and qualitative analyses of performance on es-
says and on political debates.
1 Introduction
When presenting an argument, a writer or speaker
usually cannot simply state a list of claims and
pieces of evidence. Instead, the arguer must explic-
itly structure those claims and pieces of evidence, as
well as explain how they relate to an opponent?s ar-
gument. Consider example 1 below, adapted from
an essay rebutting an opponent?s argument that griz-
zly bears lived in a specific region of Canada.
The argument states that based on the
result of the recent research, there proba-
bly were grizzly bears in Labrador. It may
seem reasonable at first glance, but ac-
tually, there are some logical mistakes
in it. . . . There is a possibility that they
were a third kind of bear apart from black
and grizzly bears. Also, the explorer ac-
counts were recorded in the nineteenth
century, which was more than 100 years
ago. . . . In sum, the conclusion of this
argument is not reasonable since the ac-
count and the research are not convinc-
ing enough. . . .
The argument begins by explicitly restating the
opponent?s claim, prefacing the claim with the
phrase ?The argument states that.? Then, the sec-
ond sentence explicitly marks the opponent?s argu-
ment as flawed. Later on, the phrase ?There is a
possibility that? indicates the subsequent clause in-
troduces evidence contrary to the opponent?s claim.
Finally, the sentence ?In sum, . . .? sums up the ar-
guer?s stance in relation to the opponent?s claim.1
As illustrated in the above example, argumenta-
tive discourse can be viewed as consisting of lan-
guage used to express claims and evidence, and
language used to organize them. We believe that
differentiating organizational elements from content
would be useful for analyzing persuasive discourse.
1The word Also signals that additional evidence is about to
be presented and should also be marked as shell. However, it
was not marked in this specific case by our human annotator
(?3.2).
20
We refer to such organizational elements as shell, in-
dicating that they differ from the specific claims and
evidence, or ?meat,? of an argument. In this work,
we develop techniques for detecting shell in texts.
We envision potential applications in political sci-
ence (e.g., to better understand political debates), in-
formation extraction or retrieval (e.g., to help a sys-
tem focus on content rather than organization), and
automated essay scoring (e.g., to analyze the quality
of a test-taker?s argument), though additional work
is needed to determine exactly how to integrate our
approach into such applications.
Detecting organizational elements could also be a
first step in parsing an argument to infer its structure.
We focus on this initial step, leaving the other steps
of categorization of spans (as to whether they evalu-
ate the opponent?s claims, connect one?s own claims,
etc.), and the inference of argumentation structure to
future work.
Before describing our approach to identifying
shell, we begin by defining it. Shell refers to se-
quences of words used to refer to claims and evi-
dence in persuasive writing or speaking, providing
an organizational framework for an argument. It
may be used by the writer or the speaker in the fol-
lowing ways:
? to declare one?s own claims (e.g., ?There is the
possibility that?)
? to restate an opponent?s claims (e.g., ?The argu-
ment states that?)
? to evaluate an opponent?s claims (e.g., ?It may
seem reasonable at first glance, but actually, there
are some logical mistakes in it?)
? to present evidence and relate it to specific claims
(e.g., ?To illustrate my point, I will now give the
example of?)
There are many ways of analyzing discourse. The
most relevant is perhaps rhetorical structure theory
(RST) (Mann and Thompson, 1988). To our knowl-
edge, the RST parser from Marcu (2000) is the only
RST parser readily available for experimentation.
The parser is trained to model the RST corpus (Carl-
son et al, 2001), which treats complete clauses (i.e.,
clauses with their obligatory complements) as the el-
ementary units of analysis. Thus, the parser treats
the first sentence in example 1 as a single unit and
does not differentiate between the main and subordi-
nate clauses. In contrast, our approach distinguishes
the sequence ?The argument states that . . . ? as shell
(which is used here to restate the external claim).
Furthermore, we identify the entire second sentence
as shell (here, used to evaluate the external claim),
whereas the RST parser splits the sentence into two
clauses, ?It may seem . . .? and ?but actually . . .?,
linked by a ?contrast? relationship.2 Finally, our
approach focuses on explicit markers of organiza-
tional structure in arguments, whereas RST covers a
broader range of discourse connections (e.g., elabo-
ration, background information, etc.), including im-
plicit ones. (Note that additional related work is de-
scribed in ?6.)
This work makes the following contributions:
? We describe a principled approach to the task
of detecting high-level organizational elements in
argumentative discourse, combining rules and a
probabilistic sequence model (?2).
? We conduct experiments to validate the approach
on an annotated sample of essays (?3, ?4).
? We qualitatively explore how the approach per-
forms in a new domain: political debate (?5).
2 Detection Methods
In this section, we describe three approaches to the
problem of shell detection: a rule-based system
(?2.1), a supervised probabilistic sequence model
(?2.2), and a simple lexical baseline (?2.3).
2.1 Rule-based system
We begin by describing a knowledge-based ap-
proach to detecting organizational elements in argu-
mentative discourse. This approach uses a set of 25
hand-written regular expression patterns.3
In order to develop these patterns, we created a
sample of 170 annotated essays across 57 distinct
prompts.4 The essays were written by test-takers of
a standardized test for graduate admissions. This
sample of essays was similar in nature to but did
not overlap with those discussed in other sections
2We used the RST parser of Marcu (2000) to analyze the
original essay from which the example was adapted.
3We use the PyParsing toolkit to parse sentences with the
grammar for the rule system.
4Prompts are short texts that present an argument or issue
and ask test takers to respond to it, either by analyzing the given
argument or taking a stance on the given issue.
21
MODAL? do | don?t | can | cannot | will | would | . . .
ADVERB? strongly | totally | fundamentally | vehemently | . . .
AGREEVERB? disagree | agree | concur | . . .
AUTHORNOUN? writer | author | speaker | . . .
SHELL? I [MODAL] [ADVERB] AGREEVERB with the AUTHORNOUN
Figure 1: An example pattern that recognizes shell language describing the author?s position with respect to an oppo-
nent?s, e.g., I totally agree with the author or I will strongly disagree with the speaker.
of the paper (?2.2, ?3.2). The annotations were car-
ried out by individuals experienced in scoring per-
suasive writing. No formal annotation guidelines
were provided. Besides shell language, there were
other annotations relevant to essay scoring. How-
ever, we ignored them for this study because they
are not directly relevant to the task of shell language
detection.
From this sample, we computed lists of n-grams
(n = 1, 2, . . . , 9) that occurred more than once in
essays from at least half of the 57 distinct essay
prompts. We then wrote rules to recognize the shell
language present in the n-gram lists. Additional
rules were added to cover instances of shell that we
observed in the annotated essays but that were not
frequent enough to appear in the n-gram analysis.
We use ?Rules? to refer to this method.
2.2 Supervised Sequence Model
The next approach we describe is a supervised, prob-
abilistic sequence model based on conditional ran-
dom fields (CRFs) (Lafferty et al, 2001), using a
small number of general features based on lexical
frequencies. We assume access to a labeled dataset
of N examples (w,y) indexed by i, containing se-
quences of words w(i) and sequences of labels y(i),
with individual words and labels indexed by j (?3
describes our development and testing sets). y(i) is a
sequence of binary values, indicating whether each
word w(i)j in the sequence is shell (y
(i)
j = 1) or not
(y(i)j = 0). Following Lafferty et al (2001), we find
a parameter vector ? that maximizes the following
log-likelihood objective function:
L(?|w,y) =
N?
i=1
log p
(
y(i) | w(i), ?
)
(1)
=
N?
i=1
(
?>f(w(i), y(i))? logZ(i)
)
The normalization constant Zi is a sum over all
possible label sequences for the ith example, and f
is a feature function that takes pairs of word and la-
bel sequences and returns a vector of feature values,
equal in dimensions to the number of parameters in
?.5
The feature values for the jth word and label pair
are as follows (these are summed over all elements
to compute the values of f for the entire sequence):
? The relative frequency of w(i)j in the British Na-
tional Corpus.
? The relative frequency of w(i)j in a set of 100,000
essays (see below).
? Eight binary features for whether the above fre-
quencies meet or exceed the following thresholds:
10{?6,?5,?4,?3}.
? The proportion of prompts for which w(i)j ap-
peared in at least one essay about that prompt in
the set of 100,000.
? Three binary features for whether the above pro-
portion of prompts meets or exceeds the following
thresholds: {0.25, 0.50, 0.75}.
? A binary feature with value 1 if w(i)j consists only
of letters a-z, and 0 otherwise. This feature dis-
tinguishes punctuation and numbers from other to-
kens.
5We used CRFsuite 0.12 (Okazaki, 2007) to implement the
CRF model.
22
? A binary feature with value 1 if the rule-based sys-
tem predicts that w(i)j is shell, and 0 otherwise.
? A binary feature with value 1 if the rule-based sys-
tem predicts that w(i)j?1 is shell, and 0 otherwise.
? Two binary features for whether or not the current
token was the first or last in the sentence, respec-
tively.
? Four binary features for the possible transitions
between previous and current labels (y(i)j and y
(i)
j?1,
respectively).
To define the features related to essay prompts
and lexical frequencies in essays, we created a set
of 100,000 essays from a larger set of essays written
by test-takers of a standardized test for graduate ad-
missions (the same domain as in ?2.1). The essays
were written in response to 228 different prompts
that asked students to analyze various issues or ar-
guments. We use additional essays sampled from
this source later to acquire annotated training and
test data (?3.2).
We developed the above feature set using cross-
validation on our development set (?3). The intu-
ition behind developing the word frequency features
is that shell language generally consists of chunks of
words that occur frequently in persuasive language
(e.g., ?claims,? ?conclude?) but not necessarily as
frequently in general text (e.g., the BNC). The se-
quence model can also learn to disprefer changes of
state, such that multi-word subsequences are labeled
as shell even though some of the individual words in
the subsequence are stop words, punctuation, etc.
Note there are a relatively small number of pa-
rameters in the model,6 which allows us to estimate
parameters on a relatively small set of labeled data.
We briefly experimented with adding an `2 penalty
on the magnitude of ? in Equation 2, but this did not
seem to improve performance.
When making predictions y?(i) about the label se-
quence for a new sentence, the most common ap-
proach is to find the most likely sequence of labels y
given the words w(i), found with Viterbi decoding:
6There were 42 parameters in our implementation of the full
CRF model. Excluding the four transition features, each of the
19 features had two parameters, one for the positive class and
one for the negative class. Having two parameters for each is
unnecessary, but we are not aware of how to have the crfsuite
toolkit avoid these extra features.
y?(i) = argmax
y
p?(y | w
(i)) (2)
We use ?CRFv? to refer to this approach. We use
the suffix ?+R? to denote models that include the
two rule-based system prediction features, and we
use ?-R? to denote models that exclude these two
features.
In development, we observed that this decoding
approach seemed to very strongly prefer labeling an
entire sentence as shell or not, which is often not
desirable since shell often appears at just the begin-
nings of sentences (e.g., ?The argument states that?).
We therefore test an alternative prediction rule
that works at the word-level, rather than sequence-
level. This approach labels each word as shell if
the sum of the probabilities of all paths in which
the word was labeled as shell?that is, the marginal
probability?exceeds some threshold ?. Words are
labeled as non-shell otherwise. Specifically, an indi-
vidual word w(i)j is labeled as shell (i.e., y?
(i)
j = 1)
according to the following equation, where 1(q) is
an indicator function that returns 1 if its argument q
is true, and 0 otherwise.
y?(i)j = 1
((
?
y
p?(y | w
(i)) yj
)
? ?
)
(3)
We tune ? using the development set, as discussed
in ?3.
We use ?CRFm? to refer to this approach.
2.3 Lexical Baseline
As a simple baseline, we also evaluated a method
that labels words as shell if they appear frequently
in persuasive writing?specifically, in the set of
100,000 unannotated essays described in ?2.2. In
this approach, word tokens are marked as shell
if they belonged to the set of k most frequent
words from the essays. Using the development
set discussed in ?3.2, we tested values of k in
{100, 200, . . . , 1000}. Setting k = 700 led to the
highest F1.
We use ?TopWords? to refer to this method.
23
3 Experiments
In this section, we discuss the design of our exper-
imental evaluation and present results on our devel-
opment set, which we used to select the final meth-
ods to evaluate on the held-out test set.
3.1 Metrics
In our experiments, we evaluated the performance
of the shell detection methods by comparing token-
level system predictions to human labels. Shell lan-
guage typically occurs as fairly long sequences of
words, but identifying the exact span of a sequence
of shell seems less important than in related tag-
ging tasks, such as named entity recognition. There-
fore, rather than evaluating based on spans (either
with exact or a partial credit system), we measured
performance at the word token-level using standard
metrics: precision, recall, and the F1 measure. For
example, for precision, we computed the propor-
tion of tokens predicted as shell by a system that
were also labeled as shell in our human-annotated
datasets.
3.2 Annotated Data
To evaluate the methods described in ?2, we gath-
ered annotations for 200 essays that were not in the
larger, unannotated set discussed in ?2.2. We split
this set of essays into a development set of 150 es-
says (68,601 word tokens) and a held-out test set of
50 essays (21,277 word tokens). An individual with
extensive experience at scoring persuasive writing
and familiarity with shell language annotated all to-
kens in the essays with judgments of whether they
were shell or not (in contrast to ?2.1, this annotation
only involved labeling shell language).
From the first annotator?s judgments on the devel-
opment set, we created a set of annotation guidelines
and trained a second annotator. The second anno-
tator marked the held-out test set so that we could
measure human agreement. Comparing the two an-
notators? test set annotations, we observed agree-
ment of F1 = 0.736 and Cohen?s ? = 0.699 (we
do not use ? in our experiments but report it here
since it is a common measure of human agreement).
Except for measuring agreement, we did not use the
second annotator?s judgments in our experiments.7
7In the version of this paper submitted for review, we mea-
recall
pre
cisi
on
0.0
0.2
0.4
0.6
0.8
1.0
l
0.0 0.2 0.4 0.6 0.8 1.0
linesCRFm?RCRFm+R
points
l CRFm?RCRFm+RCRFv?RCRFv+RRulesTopWords
Figure 2: Precision and recall of the detection methods at
various thresholds, computed through cross-validation on
the development set. Points indicate performance for the
rule-based and baseline system as well as points where
F1 is highest.
3.3 Cross-validation Results
To develop the CRF?s feature set, to tune hyperpa-
rameters, and to select the most promising systems
to evaluate on the test set, we randomly split the sen-
tences from the development set into two halves and
conducted tests with two-fold cross-validation.
We tested thresholds for the CRF at ? =
{0.01, 0.02, . . . , 1.00}.
Figure 2 shows the results on the development set.
For the rule-based system, which did not require la-
beled data, performance is computed on the entire
development set. For the CRF approaches, the pre-
cision and recall were computed after concatenating
predictions on each of the cross-validation folds.
The TopWords baseline performed quite poorly,
with F1 = 0.205. The rule-based system performed
much better, with F1 = 0.382, but still not as well
as the CRF systems. The CRF systems that pre-
dict maximum sequences had F1 = 0.382 without
the rule-based system features (CRFv?R), and F1 =
0.467 with the rule-based features (CRFv+R). The
CRF systems that made predictions from marginal
scores performed best, with F1 = 0.516 without
the rule-based features, and F1 = 0.551 with the
rule-based features. Thus, both the rule-based sys-
sured test set agreement with judgments from a third individ-
ual, who was informally trained by the first, without the formal
guidelines. Agreement was somewhat lower: F1 = 0.668 and
? = 0.613.
24
Method P R F1 Len
TopWords 0.125 0.759 0.214 ? 2.80
Rules 0.561 0.360 0.439 ? 4.99
CRFv?R 0.729 0.268 0.392 ? 15.67
CRFv+R 0.763 0.369 0.498 ? 13.30
CRFm?R 0.586 0.574 0.580 9.00
CRFm+R 0.556 0.670 0.607 9.96
Human 0.685 0.796 0.736 ? 7.91
Table 1: Performance on the held-out test set, in terms of
precision (P), recall (R), F1 measure, and average length
in tokens of sequences of one or more words labeled as
shell (Len). ? indicates F1 scores that are statistically
reliably different from CRFm+R at the p < 0.01 level.
tem features and the marginal prediction approach
led to gains in performance.
From an examination of the predictions from the
CRFm+R and CRFm?R systems, it appears that a
major contribution of the features derived from the
rule-based system is to help the hybrid CRFm+R
system avoid tagging entire sentences as shell when
only parts of them are actually shell. For exam-
ple, consider the sentence ?According to this state-
ment, the speaker asserts that technology can not
only influence but also determine social customs and
ethics? (typographical errors included). CRFm?R
tags everything up to ?determine? as shell, whereas
the rule-based system and CRFm+R correctly stop
after ?asserts that.?
4 Test Set Results
Next, we present results on the held-out test set.
For the CRFm systems, we used the thresholds that
led to the highest F1 scores on the development
set (? = 0.26 for CRFm+R and ? = 0.32 for
CRFm?R). Table 1 presents the results for all sys-
tems, along with results comparing the second anno-
tator?s labels (?Human?) to the gold standard labels
from the first annotator.
The same pattern emerged as on the development
set, with CRFm+R performing the best. The F1
score of 0.607 for the CRFm+R system was rel-
atively close to the F1 score of 0.736 for agree-
ment between human annotators. To test whether
CRFm+R?s relatively high performance was due to
chance, we computed 99% confidence intervals for
the differences in F1 score between CRFm+R and
each of the other methods. We used the bias-
corrected and accelerated (BCa) Bootstrap (Efron
and Tibshirani, 1993) with 10,000 rounds of resam-
pling at the sentence level for each comparison. A
difference is statistically reliable at the ? level (i.e.,
p < ?) if the (1 ? ?)% confidence interval for the
difference does not contain zero, which corresponds
to the null hypothesis. Statistically reliable differ-
ences are indicated in Table 1. The only system that
did not have a reliably lower F1 score than CRFm+R
was CRFm?R, though due to the relatively small
size of our test set, we do not take this as strong ev-
idence against using the rule-based system features
in the CRF.
We note that while the CRFm+R system had lower
precision (0.556) than the CRFv+R system (0.763),
its threshold ? could be tuned to prefer high preci-
sion rather than the best development set F1. Such
tuning could be very important depending on the rel-
ative costs of false positives and false negatives for
a particular application.
We also computed the mean length of sequences
of one or more contiguous words labeled as shell.
Here also, we observed that the CRFm+R approach
provided a close match to human performance. The
mean lengths of shell for the first and second anno-
tators were 8.49 and 7.91 tokens, respectively. For
the CRFm+R approach, the mean length was slightly
higher at 9.96 tokens, but this was much closer to the
means of the human annotators than the mean for
the CRFv+R system, which was 13.30 tokens. For
the rule-based system, the mean length was 4.99 to-
kens, indicating that it captures short sequences such
as ?In addition,? more often than the other systems.
5 Observations about a New Domain
In this section, we apply our system to a corpus of
transcripts of political debates8 in order to under-
stand whether the system can generalize to a new
domain with a somewhat different style of argu-
mentation. Our analyses are primarily qualitative
in nature due to the lack of gold-standard annota-
tions. We chose two historically well-known debates
8The Lincoln?Douglas debates were downloaded from
http://www.bartleby.com/251/. The other debates
were downloaded from http://debates.org/.
25
(Lincoln?Douglas from 1858 and Kennedy?Nixon
from 1960) and two debates that occurred more re-
cently (Gore?Bush from 2000 and Obama?McCain
from 2008). These debates range in length from
38,000 word tokens to 65,000 word tokens.
Political debates are similar to the persuasive es-
says we used above in that debate participants state
their own claims and evidence as well as evaluate
their opponents? claims. They are different from es-
says in that they are spoken rather than written?
meaning that they contain more disfluencies, collo-
quial language, etc.?and that they cover different
social and economic issues. Also, the debates are in
some sense a dialogue between two people.
We tagged all the debates using the CRFm+R sys-
tem, using the same parameters as for the test set
experiments (?4).
First, we observed that a smaller percentage of
tokens were tagged as shell in the debates than in
the essays. For the annotated essay test set (?3.2),
the percentage of tokens tagged as shell was 14.0%
(11.6% were labeled as shell by the first annota-
tor). In contrast, the percentage of tokens tagged
as shell was 4.2% for Lincoln?Douglas, 5.4% for
Kennedy?Nixon, 4.6% for Gore?Bush, and 4.8% for
Obama?McCain. It is not completely clear whether
the smaller percentages tagged as shell are due to a
lack of coverage by the shell detector or more sub-
stantial differences in the domain.
However, it seems that these debates genuinely in-
clude less shell. One potential reason is that many of
the essay prompts asked test-takers to respond to a
particular argument, leading to responses containing
many phrases such as ?The speaker claims that? and
?However, the argument lacks specificity . . . ?.
We analyzed the system?s predictions and ex-
tracted a set of examples, some of which appear in
Table 2, showing true positives, where most of the
tokens appear to be labeled correctly as shell; false
positives, where tokens were incorrectly labeled as
shell; and false negatives, where the system missed
tokens that should have been marked.
Table 2 also provides some examples from our de-
velopment set, for comparison.
We observed many instances of correctly marked
shell, including many that appeared very different
in style than the language used in essays. For ex-
ample, Lincoln demonstrates an aggressive style in
the following: ?Now, I say that there is no charitable
way to look at that statement, except to conclude that
he is actually crazy.? Also, Bush employs a some-
what atypical sentence structure here: ?It?s not what
I think and its not my intentions and not my plan.?
However, the system also incorrectly tagged se-
quences as shell, particularly in short sentences (e.g.,
?Are we as strong as we should be??). It also missed
shell, partially or entirely, such as in the following
example: ?But let?s get back to the core issue here.?
These results suggest that although there is poten-
tial for improvement in adapting to new domains,
our approach to shell detection at least partially gen-
eralizes beyond our initial domain of persuasive es-
say writing.
6 Related Work
There has been much previous work on analyzing
discourse. In this section, we describe similarities
and differences between that work and ours.
Rhetorical structure theory (Mann and Thomp-
son, 1988) is perhaps the most relevant area of work.
See ?1 for a discussion.
In research on intentional structure, Grosz and
Sidner (1986) propose that any discourse is com-
posed of three interacting components: the linguistic
structure defined by the actual utterances, the inten-
tional structure defined by the purposes underlying
the discourse, and an attentional structure defined by
the discourse participants? focus of attention. De-
tecting shell may also be seen as trying to identify
explicit cues of intentional structure in a discourse.
Additionally, the categorization of shell spans as to
whether they evaluate the opponents claims, connect
ones own claims, etc., may be seen as determining
what Grosz and Sidener call ?discourse segment pur-
poses? (i.e., the intentions underlying the segments
containing the shell spans).
We can also view shell detection as the task of
identifying phrases that indicate certain types of
speech acts (Searle, 1975). In particular, we aim to
identify markers of assertive speech acts, which de-
clare that the speaker believes a certain proposition,
and expressive speech acts, which express attitudes
toward propositions.
Shell also overlaps with the concept of discourse
markers (Hutchinson, 2004), such as ?however? or
26
LINCOLN (L) ? DOUGLAS (D) DEBATES
TP L: Now, I say that there is no charitable way to look at that statement, except to conclude that he is
actually crazy.
L: The first thing I see fit to notice is the fact that . . .
FP D: He became noted as the author of the scheme to . . .
D: . . . such amendments were to be made to it as would render it useless and inefficient . . .
FN D: I wish to impress it upon you, that every man who voted for those resolutions . . .
L: That statement he makes, too, in the teeth of the knowledge that I had made the stipulation to
come down here . . .
KENNEDY (K) ? NIXON (N) DEBATES
TP N: I favor that because I believe that?s the best way to aid our schools . . .
N: And in our case, I do believe that our programs will stimulate the creative energies of . . .
FP N: We are for programs, in addition, which will see that our medical care for the aged . . .
K: Are we as strong as we should be?
FN K: I should make it clear that I do not think we?re doing enough . . .
N: Why did Senator Kennedy take that position then? Why do I take it now?
BUSH (B) ? GORE (G) DEBATES
TP B: It?s not what I think and its not my intentions and not my plan.
G: And FEMA has been a major flagship project of our reinventing government efforts. And I agree, it
works extremely well now.
FP B: First of all, most of this is at the state level.
G: And it focuses not only on increasing the supply, which I agree we have to do, but also on . . .
FN B: My opponent thinks the government?the surplus is the government?s money. That?s not what I
think
G: I strongly support local control, so does Governor Bush.
OBAMA (O) ? MCCAIN (M) DEBATES
TP M: But the point is?the point is, we have finally seen Republicans and Democrats sitting down and
negotiating together . . .
O: And one of the things I think we have to do is make sure that college is affordable . . .
FP O: . . . but in the short term there?s an outlay and we may not see that money for a while.
O: We have to do that now, because it will actually make our businesses and our families better off.
FN O: So I think the lesson to be drawn is that we should never hesitate to use military force . . . to keep the
American people safe.
O: But let?s get back to the core issue here.
PERSUASIVE ESSAYS (DEVELOPMENT SET, SPELLING ERRORS INCLUDED)
TP However, the argument lacks specificity and relies on too many questionable assumptions to make a
strong case for adopting an expensive and logistically complicated program.
I believe that both of these claims have been made in hase and other factors need to be considered.
FP Since they are all far from now, the prove is not strong enough to support the conclusion.
As we know that one mind can not think as the other does.
FN History has proven that . . .
The given issue which states that in any field of inquiry . . . is a controversional one.
Table 2: Examples of CRFm+R performance. Underlining marks tokens predicted to be shell, and bold font indicates
shell according to human judgments (our judgments for the debate transcripts, and the annotator?s judgments for the
development set). Examples include true positives (TP), false positives (FP), and false negatives (FN). Note that some
FP and FN examples include partially accurate predictions.
27
?therefore.? Discourse markers, however, are typ-
ically only single words or short phrases that ex-
press a limited number of relationships. On the other
hand, shell can capture longer sequences that ex-
press more complex relationships between the com-
ponents of an argumentative discourse (e.g., ?But
let?s get back to the core issue here? signals that the
following point is more important than the previous
one).
There are also various other approaches to ana-
lyzing arguments. Notably, much recent theoreti-
cal research on argumentation has focused on ar-
gumentation schemes (Walton et al, 2008), which
are high-level strategies for constructing arguments
(e.g., argument from consequences). Recently, Feng
and Hirst (2011) developed automated methods for
classifying texts by argumentation scheme. In sim-
ilar work, Anand et al (2011) use argumentation
schemes to identify tactics in blog posts (e.g., moral
appeal, social generalization, appeals to external au-
thorities etc.). Although shell language can certainly
be found in persuasive writing, it is used to orga-
nize the persuader?s tactics and claims rather than
to express them. For example, consider the follow-
ing sentence: ?It must be the case that this diet
works since it was recommended by someone who
lost 20 pounds on it.? In shell detection, we focus
on the lexico-syntactic level, aiming to identify the
bold words as shell. In contrast, work on argumenta-
tion schemes focuses at a higher level of abstraction,
aiming to classify the sentence as an attempt to per-
suade by appealing to an external authority.
7 Conclusions
In this paper, we described our approach to detect-
ing language used to explicitly structure an arguer?s
claims and pieces of evidence as well as explain
how they relate to an opponent?s argument. We im-
plemented a rule-based system, a supervised proba-
bilistic sequence model, and a principled hybrid ver-
sion of the two. We presented evaluations of these
systems using human-annotated essays, and we ob-
served that the hybrid sequence model system per-
formed the best. We also applied our system to po-
litical debates and found evidence of the potential to
generalize to new domains.
Acknowledgments
We would like to thank the annotators for helping
us create the essay data sets. We would also like
to thank James Carlson, Paul Deane, Yoko Futagi,
Beata Beigman Klebanov, Melissa Lopez, and the
anonymous reviewers for their useful comments on
the paper and annotation scheme.
References
P. Anand, J. King, J. Boyd-Graber, E. Wagner, C. Martell,
D. Oard, and P. Resnik. 2011. Believe me?we can
do this! annotating persuasive acts in blog text. In
Proc. of AAAI Workshop on Computational Models of
Natural Argument.
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the framework
of rhetorical structure theory. In Proc. of the Second
SIGdial Workshop on Discourse and Dialogue.
B. Efron and R. Tibshirani. 1993. An Introduction to the
Bootstrap. Chapman and Hall/CRC.
V. W. Feng and G. Hirst. 2011. Classifying arguments
by scheme. In Proc. of ACL.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, Intentions, and the Structure of Discourse. Com-
put. Linguist., 12(3):175?204.
B. Hutchinson. 2004. Acquiring the meaning of dis-
course markers. In Proc. of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
W. C. Mann and S. A. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3).
D. Marcu. 2000. The Theory and Practice of Discourse
Parsing and Summarization. MIT Press.
N. Okazaki. 2007. CRFsuite: a fast implementation of
conditional random fields (CRFs).
J. R. Searle. 1975. A classification of illocutionary acts.
Language in Society, 5(1).
D. Walton, C. Reed, and F. Macagno. 2008. Argumenta-
tion Schemes. Cambridge University Press.
28
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 42?47,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills,
Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith
School of Computer Science, Carnegie Mellon Univeristy, Pittsburgh, PA 15213, USA
{kgimpel,nschneid,brenocon,dipanjan,dpmills,
jacobeis,mheilman,dyogatama,jflanigan,nasmith}@cs.cmu.edu
Abstract
We address the problem of part-of-speech tag-
ging for English data from the popular micro-
blogging service Twitter. We develop a tagset,
annotate data, develop features, and report
tagging results nearing 90% accuracy. The
data and tools have been made available to the
research community with the goal of enabling
richer text analysis of Twitter and related so-
cial media data sets.
1 Introduction
The growing popularity of social media and user-
created web content is producing enormous quanti-
ties of text in electronic form. The popular micro-
blogging service Twitter (twitter.com) is one
particularly fruitful source of user-created content,
and a flurry of recent research has aimed to under-
stand and exploit these data (Ritter et al, 2010; Shar-
ifi et al, 2010; Barbosa and Feng, 2010; Asur and
Huberman, 2010; O?Connor et al, 2010a; Thelwall
et al, 2011). However, the bulk of this work eschews
the standard pipeline of tools which might enable
a richer linguistic analysis; such tools are typically
trained on newstext and have been shown to perform
poorly on Twitter (Finin et al, 2010).
One of the most fundamental parts of the linguis-
tic pipeline is part-of-speech (POS) tagging, a basic
form of syntactic analysis which has countless appli-
cations in NLP. Most POS taggers are trained from
treebanks in the newswire domain, such as the Wall
Street Journal corpus of the Penn Treebank (PTB;
Marcus et al, 1993). Tagging performance degrades
on out-of-domain data, and Twitter poses additional
challenges due to the conversational nature of the
text, the lack of conventional orthography, and 140-
character limit of each message (?tweet?). Figure 1
shows three tweets which illustrate these challenges.
(a) @Gunservatively@ obozo? willV goV nutsA
whenR PA? electsV aD RepublicanA GovernorN
nextP Tue? ., CanV youO sayV redistrictingV ?,
(b) SpendingV theD dayN withhhP mommmaN !,
(c) lmao! ..., s/oV toP theD coolA assN asianA
officerN 4P #1$ notR runninV myD licenseN and&
#2$ notR takinV druN booN toP jailN ., ThankV
uO God? ., #amen#
Figure 1: Example tweets with gold annotations. Under-
lined tokens show tagger improvements due to features
detailed in Section 3 (respectively: TAGDICT, METAPH,
and DISTSIM).
In this paper, we produce an English POS tagger
that is designed especially for Twitter data. Our con-
tributions are as follows:
? we developed a POS tagset for Twitter,
? we manually tagged 1,827 tweets,
? we developed features for Twitter POS tagging
and conducted experiments to evaluate them, and
? we provide our annotated corpus and trained POS
tagger to the research community.
Beyond these specific contributions, we see this
work as a case study in how to rapidly engi-
neer a core NLP system for a new and idiosyn-
cratic dataset. This project was accomplished in
200 person-hours spread across 17 people and two
months. This was made possible by two things:
(1) an annotation scheme that fits the unique char-
acteristics of our data and provides an appropriate
level of linguistic detail, and (2) a feature set that
captures Twitter-specific properties and exploits ex-
isting resources such as tag dictionaries and phonetic
normalization. The success of this approach demon-
strates that with careful design, supervised machine
learning can be applied to rapidly produce effective
language technology in new domains.
42
Tag Description Examples %
Nominal, Nominal + Verbal
N common noun (NN, NNS) books someone 13.7
O pronoun (personal/WH; not
possessive; PRP, WP)
it you u meeee 6.8
S nominal + possessive books? someone?s 0.1
? proper noun (NNP, NNPS) lebron usa iPad 6.4
Z proper noun + possessive America?s 0.2
L nominal + verbal he?s book?ll iono
(= I don?t know)
1.6
M proper noun + verbal Mark?ll 0.0
Other open-class words
V verb incl. copula,
auxiliaries (V*, MD)
might gonna
ought couldn?t is
eats
15.1
A adjective (J*) good fav lil 5.1
R adverb (R*, WRB) 2 (i.e., too) 4.6
! interjection (UH) lol haha FTW yea
right
2.6
Other closed-class words
D determiner (WDT, DT,
WP$, PRP$)
the teh its it?s 6.5
P pre- or postposition, or
subordinating conjunction
(IN, TO)
while to for 2 (i.e.,
to) 4 (i.e., for)
8.7
& coordinating conjunction
(CC)
and n & + BUT 1.7
T verb particle (RP) out off Up UP 0.6
X existential there,
predeterminers (EX, PDT)
both 0.1
Y X + verbal there?s all?s 0.0
Twitter/online-specific
# hashtag (indicates
topic/category for tweet)
#acl 1.0
@ at-mention (indicates
another user as a recipient
of a tweet)
@BarackObama 4.9
~ discourse marker,
indications of continuation
of a message across
multiple tweets
RT and : in retweet
construction RT
@user : hello
3.4
U URL or email address http://bit.ly/xyz 1.6
E emoticon :-) :b (: <3 o O 1.0
Miscellaneous
$ numeral (CD) 2010 four 9:30 1.5
, punctuation (#, $, '', (,
), ,, ., :, ``)
!!! .... ?!? 11.6
G other abbreviations, foreign
words, possessive endings,
symbols, garbage (FW,
POS, SYM, LS)
ily (I love you) wby
(what about you) ?s
 -->
awesome...I?m
1.1
Table 1: The set of tags used to annotate tweets. The
last column indicates each tag?s relative frequency in the
full annotated data (26,435 tokens). (The rates for M and
Y are both < 0.0005.)
2 Annotation
Annotation proceeded in three stages. For Stage 0,
we developed a set of 20 coarse-grained tags based
on several treebanks but with some additional cate-
gories specific to Twitter, including URLs and hash-
tags. Next, we obtained a random sample of mostly
American English1 tweets from October 27, 2010,
automatically tokenized them using a Twitter tok-
enizer (O?Connor et al, 2010b),2 and pre-tagged
them using the WSJ-trained Stanford POS Tagger
(Toutanova et al, 2003) in order to speed up man-
ual annotation. Heuristics were used to mark tokens
belonging to special Twitter categories, which took
precedence over the Stanford tags.
Stage 1 was a round of manual annotation: 17 re-
searchers corrected the automatic predictions from
Stage 0 via a custom Web interface. A total of
2,217 tweets were distributed to the annotators in
this stage; 390 were identified as non-English and
removed, leaving 1,827 annotated tweets (26,436 to-
kens).
The annotation process uncovered several situa-
tions for which our tagset, annotation guidelines,
and tokenization rules were deficient or ambiguous.
Based on these considerations we revised the tok-
enization and tagging guidelines, and for Stage 2,
two annotators reviewed and corrected all of the
English tweets tagged in Stage 1. A third anno-
tator read the annotation guidelines and annotated
72 tweets from scratch, for purposes of estimating
inter-annotator agreement. The 72 tweets comprised
1,021 tagged tokens, of which 80 differed from the
Stage 2 annotations, resulting in an agreement rate
of 92.2% and Cohen?s ? value of 0.914. A final
sweep was made by a single annotator to correct er-
rors and improve consistency of tagging decisions
across the corpus. The released data and tools use
the output of this final stage.
2.1 Tagset
We set out to develop a POS inventory for Twitter
that would be intuitive and informative?while at
the same time simple to learn and apply?so as to
maximize tagging consistency within and across an-
1We filtered to tweets sent via an English-localized user in-
terface set to a United States timezone.
2http://github.com/brendano/tweetmotif
43
notators. Thus, we sought to design a coarse tagset
that would capture standard parts of speech3 (noun,
verb, etc.) as well as categories for token varieties
seen mainly in social media: URLs and email ad-
dresses; emoticons; Twitter hashtags, of the form
#tagname, which the author may supply to catego-
rize a tweet; and Twitter at-mentions, of the form
@user, which link to other Twitter users from within
a tweet.
Hashtags and at-mentions can also serve as words
or phrases within a tweet; e.g. Is #qadaffi going down?.
When used in this way, we tag hashtags with their
appropriate part of speech, i.e., as if they did not start
with #. Of the 418 hashtags in our data, 148 (35%)
were given a tag other than #: 14% are proper nouns,
9% are common nouns, 5% are multi-word express-
sions (tagged as G), 3% are verbs, and 4% are some-
thing else. We do not apply this procedure to at-
mentions, as they are nearly always proper nouns.
Another tag, ~, is used for tokens marking spe-
cific Twitter discourse functions. The most popular
of these is the RT (?retweet?) construction to publish
a message with attribution. For example,
RT @USER1 : LMBO ! This man filed an
EMERGENCY Motion for Continuance on
account of the Rangers game tonight ! 
Wow lmao
indicates that the user @USER1 was originally the
source of the message following the colon. We ap-
ply ~ to the RT and : (which are standard), and
also, which separates the author?s comment from
the retweeted material.4 Another common discourse
marker is ellipsis dots (. . . ) at the end of a tweet,
indicating a message has been truncated to fit the
140-character limit, and will be continued in a sub-
sequent tweet or at a specified URL.
Our first round of annotation revealed that, due to
nonstandard spelling conventions, tokenizing under
a traditional scheme would be much more difficult
3Our starting point was the cross-lingual tagset presented by
Petrov et al (2011). Most of our tags are refinements of those
categories, which in turn are groupings of PTB WSJ tags (see
column 2 of Table 1). When faced with difficult tagging deci-
sions, we consulted the PTB and tried to emulate its conventions
as much as possible.
4These ?iconic deictics? have been studied in other online
communities as well (Collister, 2010).
than for Standard English text. For example, apos-
trophes are often omitted, and there are frequently
words like ima (short for I?m gonna) that cut across
traditional POS categories. Therefore, we opted not
to split contractions or possessives, as is common
in English corpus preprocessing; rather, we intro-
duced four new tags for combined forms: {nominal,
proper noun} ? {verb, possessive}.5
The final tagging scheme (Table 1) encompasses
25 tags. For simplicity, each tag is denoted with a
single ASCII character. The miscellaneous category
G includes multiword abbreviations that do not fit
in any of the other categories, like ily (I love you), as
well as partial words, artifacts of tokenization errors,
miscellaneous symbols, possessive endings,6 and ar-
rows that are not used as discourse markers.
Figure 2 shows where tags in our data tend to oc-
cur relative to the middle word of the tweet. We
see that Twitter-specific tags have strong positional
preferences: at-mentions (@) and Twitter discourse
markers (~) tend to occur towards the beginning of
messages, whereas URLs (U), emoticons (E), and
categorizing hashtags (#) tend to occur near the end.
3 System
Our tagger is a conditional random field (CRF; Laf-
ferty et al, 2001), enabling the incorporation of ar-
bitrary local features in a log-linear model. Our
base features include: a feature for each word type,
a set of features that check whether the word con-
tains digits or hyphens, suffix features up to length 3,
and features looking at capitalization patterns in the
word. We then added features that leverage domain-
specific properties of our data, unlabeled in-domain
data, and external linguistic resources.
TWORTH: Twitter orthography. We have features
for several regular expression-style rules that detect
at-mentions, hashtags, and URLs.
NAMES: Frequently-capitalized tokens. Micro-
bloggers are inconsistent in their use of capitaliza-
tion, so we compiled gazetteers of tokens which are
frequently capitalized. The likelihood of capital-
ization for a token is computed as Ncap+?CN+C , where
5The modified tokenizer is packaged with our tagger.
6Possessive endings only appear when a user or the tok-
enizer has separated the possessive ending from a possessor; the
tokenizer only does this when the possessor is an at-mention.
44
Figure 2: Average position, relative to the middle word in the tweet, of tokens labeled with each tag. Most tags fall
between ?1 and 1 on this scale; these are not shown.
N is the token count, Ncap is the capitalized to-
ken count, and ? and C are the prior probability
and its prior weight.7 We compute features for
membership in the top N items by this metric, for
N ? {1000, 2000, 3000, 5000, 10000, 20000}.
TAGDICT: Traditional tag dictionary. We add
features for all coarse-grained tags that each word
occurs with in the PTB8 (conjoined with their fre-
quency rank). Unlike previous work that uses tag
dictionaries as hard constraints, we use them as soft
constraints since we expect lexical coverage to be
poor and the Twitter dialect of English to vary sig-
nificantly from the PTB domains. This feature may
be seen as a form of type-level domain adaptation.
DISTSIM: Distributional similarity. When train-
ing data is limited, distributional features from un-
labeled text can improve performance (Schu?tze and
Pedersen, 1993). We used 1.9 million tokens from
134,000 unlabeled tweets to construct distributional
features from the successor and predecessor proba-
bilities for the 10,000 most common terms. The suc-
cessor and predecessor transition matrices are hori-
zontally concatenated into a sparse matrixM, which
we approximate using a truncated singular value de-
composition: M ? USVT, where U is limited to
50 columns. Each term?s feature vector is its row
in U; following Turian et al (2010), we standardize
and scale the standard deviation to 0.1.
METAPH: Phonetic normalization. Since Twitter
includes many alternate spellings of words, we used
the Metaphone algorithm (Philips, 1990)9 to create
a coarse phonetic normalization of words to simpler
keys. Metaphone consists of 19 rules that rewrite
consonants and delete vowels. For example, in our
7? = 1100 , C = 10; this score is equivalent to the posterior
probability of capitalization with a Beta(0.1, 9.9) prior.
8Both WSJ and Brown corpora, no case normalization. We
also tried adding the WordNet (Fellbaum, 1998) and Moby
(Ward, 1996) lexicons, which increased lexical coverage but did
not seem to help performance.
9Via the Apache Commons implementation: http://
commons.apache.org/codec/
data, {thangs thanks thanksss thanx thinks thnx}
are mapped to 0NKS, and {lmao lmaoo lmaooooo}
map to LM. But it is often too coarse; e.g. {war we?re
wear were where worry} map to WR.
We include two types of features. First, we use
the Metaphone key for the current token, comple-
menting the base model?s word features. Second,
we use a feature indicating whether a tag is the most
frequent tag for PTB words having the same Meta-
phone key as the current token. (The second feature
was disabled in both ?TAGDICT and ?METAPH ab-
lation experiments.)
4 Experiments
Our evaluation was designed to test the efficacy of
this feature set for part-of-speech tagging given lim-
ited training data. We randomly divided the set of
1,827 annotated tweets into a training set of 1,000
(14,542 tokens), a development set of 327 (4,770 to-
kens), and a test set of 500 (7,124 tokens). We com-
pare our system against the Stanford tagger. Due
to the different tagsets, we could not apply the pre-
trained Stanford tagger to our data. Instead, we re-
trained it on our labeled data, using a standard set
of features: words within a 5-word window, word
shapes in a 3-word window, and up to length-3
prefixes, length-3 suffixes, and prefix/suffix pairs.10
The Stanford system was regularized using a Gaus-
sian prior of ?2 = 0.5 and our system with a Gaus-
sian prior of ?2 = 5.0, tuned on development data.
The results are shown in Table 2. Our tagger with
the full feature set achieves a relative error reduction
of 25% compared to the Stanford tagger. We also
show feature ablation experiments, each of which
corresponds to removing one category of features
from the full set. In Figure 1, we show examples
that certain features help solve. Underlined tokens
10We used the following feature modules in the Stanford tag-
ger: bidirectional5words, naacl2003unknowns,
wordshapes(-3,3), prefix(3), suffix(3),
prefixsuffix(3).
45
Dev. Test
Our tagger, all features 88.67 89.37
independent ablations:
?DISTSIM 87.88 88.31 (?1.06)
?TAGDICT 88.28 88.31 (?1.06)
?TWORTH 87.51 88.37 (?1.00)
?METAPH 88.18 88.95 (?0.42)
?NAMES 88.66 89.39 (+0.02)
Our tagger, base features 82.72 83.38
Stanford tagger 85.56 85.85
Annotator agreement 92.2
Table 2: Tagging accuracies on development and test
data, including ablation experiments. Features are or-
dered by importance: test accuracy decrease due to ab-
lation (final column).
Tag Acc. Confused Tag Acc. Confused
V 91 N ! 82 N
N 85 ? L 93 V
, 98 ~ & 98 ?
P 95 R U 97 ,
? 71 N $ 89 P
D 95 ? # 89 ?
O 97 ? G 26 ,
A 79 N E 88 ,
R 83 A T 72 P
@ 99 V Z 45 ?
~ 91 ,
Table 3: Accuracy (recall) rates per class, in the test set
with the full model. (Omitting tags that occur less than
10 times in the test set.) For each gold category, the most
common confusion is shown.
are incorrect in a specific ablation, but are corrected
in the full system (i.e. when the feature is added).
The ?TAGDICT ablation gets elects, Governor,
and next wrong in tweet (a). These words appear
in the PTB tag dictionary with the correct tags, and
thus are fixed by that feature. In (b), withhh is ini-
tially misclassified an interjection (likely caused by
interjections with the same suffix, like ohhh), but is
corrected by METAPH, because it is normalized to the
same equivalence class as with. Finally, s/o in tweet
(c) means ?shoutout?, which appears only once in
the training data; adding DISTSIM causes it to be cor-
rectly identified as a verb.
Substantial challenges remain; for example, de-
spite the NAMES feature, the system struggles to
identify proper nouns with nonstandard capitaliza-
tion. This can be observed from Table 3, which
shows the recall of each tag type: the recall of proper
nouns (?) is only 71%. The system also struggles
with the miscellaneous category (G), which covers
many rare tokens, including obscure symbols and ar-
tifacts of tokenization errors. Nonetheless, we are
encouraged by the success of our system on the
whole, leveraging out-of-domain lexical resources
(TAGDICT), in-domain lexical resources (DISTSIM),
and sublexical analysis (METAPH).
Finally, we note that, even though 1,000 train-
ing examples may seem small, the test set accuracy
when training on only 500 tweets drops to 87.66%,
a decrease of only 1.7% absolute.
5 Conclusion
We have developed a part-of-speech tagger for Twit-
ter and have made our data and tools available to the
research community at http://www.ark.cs.
cmu.edu/TweetNLP. More generally, we be-
lieve that our approach can be applied to address
other linguistic analysis needs as they continue to
arise in the era of social media and its rapidly chang-
ing linguistic conventions. We also believe that the
annotated data can be useful for research into do-
main adaptation and semi-supervised learning.
Acknowledgments
We thank Desai Chen, Chris Dyer, Lori Levin, Behrang
Mohit, Bryan Routledge, Naomi Saphra, and Tae Yano
for assistance in annotating data. This research was sup-
ported in part by: the NSF through CAREER grant IIS-
1054319, the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant num-
ber W911NF-10-1-0533, Sandia National Laboratories
(fellowship to K. Gimpel), and the U. S. Department of
Education under IES grant R305B040063 (fellowship to
M. Heilman).
References
Sitaram Asur and Bernardo A. Huberman. 2010. Pre-
dicting the future with social media. In Proc. of WI-
IAT.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proc. of COLING.
Lauren Collister. 2010. Meaning variation of the iconic
deictics ? and <? in an online community. In New
Ways of Analyzing Variation.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
46
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010. An-
notating named entities in Twitter data with crowd-
sourcing. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313?330.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010a.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proc. of ICWSM.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010b. TweetMotif: Exploratory search and topic
summarization for Twitter. In Proc. of ICWSM (demo
track).
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. ArXiv:1104.2086.
Lawrence Philips. 1990. Hanging on the Metaphone.
Computer Language, 7(12).
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of Twitter conversations. In Proc.
of NAACL.
Hinrich Schu?tze and Jan Pedersen. 1993. A vector model
for syntagmatic and paradigmatic relatedness. In Pro-
ceedings of the 9th Annual Conference of the UW Cen-
tre for the New OED and Text Research.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing microblogs automatically. In
Proc. of NAACL.
Mike Thelwall, Kevan Buckley, and Georgios Paltoglou.
2011. Sentiment in Twitter events. Journal of the
American Society for Information Science and Tech-
nology, 62(2):406?418.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc. of
HLT-NAACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proc. of ACL.
Grady Ward. 1996. Moby lexicon. http://icon.
shef.ac.uk/Moby.
47
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 174?180,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Predicting Grammaticality on an Ordinal Scale
Michael Heilman Aoife Cahill Nitin Madnani Melissa Lopez Matthew Mulholland
Educational Testing Service
Princeton, NJ, USA
{mheilman,acahill,nmadnani,mlopez002,mmulholland}@ets.org
Joel Tetreault
Yahoo! Research
New York, NY, USA
tetreaul@yahoo-inc.com
Abstract
Automated methods for identifying
whether sentences are grammatical
have various potential applications (e.g.,
machine translation, automated essay
scoring, computer-assisted language
learning). In this work, we construct a
statistical model of grammaticality using
various linguistic features (e.g., mis-
spelling counts, parser outputs, n-gram
language model scores). We also present
a new publicly available dataset of learner
sentences judged for grammaticality on
an ordinal scale. In evaluations, we
compare our system to the one from Post
(2011) and find that our approach yields
state-of-the-art performance.
1 Introduction
In this paper, we develop a system for the task
of predicting the grammaticality of sentences, and
present a dataset of learner sentences rated for
grammaticality. Such a system could be used, for
example, to check or to rank outputs from systems
for text summarization, natural language genera-
tion, or machine translation. It could also be used
in educational applications such as essay scoring.
Much of the previous research on predicting
grammaticality has focused on identifying (and
possibly correcting) specific types of grammati-
cal errors that are typically made by English lan-
guage learners, such as prepositions (Tetreault and
Chodorow, 2008), articles (Han et al, 2006), and
collocations (Dahlmeier and Ng, 2011). While
some applications (e.g., grammar checking) rely
on such fine-grained predictions, others might be
better addressed by sentence-level grammaticality
judgments (e.g., machine translation evaluation).
Regarding sentence-level grammaticality, there
has been much work on rating the grammatical-
ity of machine translation outputs (Gamon et al,
2005; Parton et al, 2011), such as the MT Quality
Estimation Shared Tasks (Bojar et al, 2013, ?6),
but relatively little on evaluating the grammatical-
ity of naturally occurring text. Also, most other re-
search on evaluating grammaticality involves arti-
ficial tasks or datasets (Sun et al, 2007; Lee et al,
2007; Wong and Dras, 2010; Post, 2011).
Here, we make the following contributions.
? We develop a state-of-the-art approach for
predicting the grammaticality of sentences on
an ordinal scale, adapting various techniques
from the previous work described above.
? We create a dataset of grammatical and un-
grammatical sentences written by English
language learners, labeled on an ordinal scale
for grammaticality. With this unique data set,
which we will release to the research com-
munity, it is now possible to conduct realis-
tic evaluations for predicting sentence-level
grammaticality.
2 Dataset Description
We created a dataset consisting of 3,129 sentences
randomly selected from essays written by non-
native speakers of English as part of a test of
English language proficiency. We oversampled
lower-scoring essays to increase the chances of
finding ungrammatical sentences. Two of the au-
thors of this paper, both native speakers of English
with linguistic training, annotated the data. We
refer to these annotators as expert judges. When
making judgments of the sentences, they saw the
previous sentence from the same essay as context.
These two authors were not directly involved in
development of the system in ?3.
Each sentence was annotated on a scale from
1 to 4 as described below, with 4 being the most
174
grammatical. We use an ordinal rather than bi-
nary scale, following previous work such as that of
Clark et al (2013) and Crocker and Keller (2005)
who argue that the distinction between grammati-
cal and ungrammatical is not simply binary. Also,
for practical applications, we believe that it is use-
ful to distinguish sentences with minor errors from
those with major errors that may disrupt communi-
cation. Our annotation scheme was influenced by
a translation rating scheme by Coughlin (2003).
Every sentence judged on the 1?4 scale must be
a clause. There is an extra category (?Other?) for
sentences that do not fit this criterion. We exclude
instances of ?Other? in our experiments (see ?4).
4. Perfect The sentence is native-sounding. It has
no grammatical errors, but may contain very mi-
nor typographical and/or collocation errors, as in
Example (1).
(1) For instance, i stayed in a dorm when i
went to collge.
3. Comprehensible The sentence may contain
one or more minor grammatical errors, includ-
ing subject-verb agreement, determiner, and mi-
nor preposition errors that do not make the mean-
ing unclear, as in Example (2).
(2) We know during Spring Festival, Chinese
family will have a abundand family banquet
with family memebers.
?Chinese family?, which could be corrected to
?Chinese families?, ?each Chinese family?, etc.,
would be an example of a minor grammatical er-
ror involving determiners.
2. Somewhat Comprehensible The sentence
may contain one or more serious grammatical
errors, including missing subject, verb, object,
etc., verb tense errors, and serious preposition
errors. Due to these errors, the sentence may
have multiple plausible interpretations, as in
Example (3).
(3) I can gain the transportations such as buses
and trains.
1. Incomprehensible The sentence contains so
many errors that it would be difficult to correct,
as in Example (4).
(4) Or you want to say he is only a little boy do
not everything clearly?
The phrase ?do not everything? makes the sen-
tence practically incomprehensible since the sub-
ject of ?do? is not clear.
O. Other/Incomplete This sentence is incom-
plete. These sentences, such as Example (5), ap-
pear in our corpus due to the nature of timed tests.
(5) The police officer handed the
This sentence is cut off and does not at least in-
clude one clause.
We measured interannotator agreement on a
subset of 442 sentences that were independently
annotated by both expert annotators. Exact agree-
ment was 71.3%, unweighted ? = 0.574, and
Pearson?s r = 0.759.
1
For our experiments, one
expert annotator was arbitrarily selected, and for
the doubly-annotated sentences, only the judg-
ments from that annotator were retained.
The labels from the expert annotators are dis-
tributed as follows: 72 sentences are labeled 1;
538 are 2; 1,431 are 3; 978 are 4; and 110 are ?O?.
We also gathered 5 additional judgments using
Crowdflower.
2
For this, we excluded the ?Other?
category and any sentences that had been marked
as such by the expert annotators. We used 100
(3.2%) of the judged sentences as ?gold? data in
Crowdflower to block contributors who were not
following the annotation guidelines. For those
sentences, only disagreements within 1 point of
the expert annotator judgment were accepted. In
preliminary experiments, averaging the six judg-
ments (1 expert, 5 crowdsourced) for each item
led to higher human-machine agreement. For all
experiments reported later, we used this average
of six judgments as our gold standard.
For our experiments (?4), we randomly split the
data into training (50%), development (25%), and
testing (25%) sets. We also excluded all instances
labeled ?Other?. These are relatively uncommon
and less interesting to this study. Also, we believe
that simpler, heuristic approaches could be used to
identify such sentences.
We use ?GUG? (?Grammatical? versus ?Un-
Grammatical?) to refer to this dataset. The dataset
is available for research at https://github.
com/EducationalTestingService/
gug-data.
1
The reported agreement values assume that ?Other?
maps to 0. For the sentences where both labels were in the
1?4 range (n = 424), Pearson?s r = 0.767.
2
http://www.crowdflower.com
175
3 System Description
This section describes the statistical model (?3.1)
and features (?3.2) used by our system.
3.1 Statistical Model
We use `
2
-regularized linear regression (i.e., ridge
regression) to learn a model of sentence grammat-
icality from a variety of linguistic features.
34
To tune the `
2
-regularization hyperparameter ?,
the system performs 5-fold cross-validation on the
data used for training. The system evaluates ? ?
10
{?4,...,4}
and selects the one that achieves the
highest cross-validation correlation r.
3.2 Features
Next, we describe the four types of features.
3.2.1 Spelling Features
Given a sentence with with n word tokens, the
model filters out tokens containing nonalpha-
betic characters and then computes the num-
ber of misspelled words n
miss
(later referred to
as num misspelled), the proportion of mis-
spelled words
n
miss
n
, and log(n
miss
+ 1) as fea-
tures. To identify misspellings, we use a freely
available spelling dictionary for U.S. English.
5
3.2.2 n-gram Count and Language Model
Features
Given each sentence, the model obtains the counts
of n-grams (n = 1 . . . 3) from English Gigaword
and computes the following features:
6
?
?
s?S
n
log(count(s) + 1)
?S
n
?
3
We use ridge regression from the scikit-learn
toolkit (Pedregosa et al, 2011) v0.23.1 and the
SciKit-Learn Laboratory (http://github.com/
EducationalTestingService/skll).
4
Regression models typically produce conservative pre-
dictions with lower variance than the original training data.
So that predictions better match the distribution of labels in
the training data, the system rescales its predictions. It saves
the mean and standard deviation of the training data gold
standard (M
gold
and SD
gold
, respectively) and of its own
predictions on the training data (M
pred
and SD
pred
, respec-
tively). During cross-validation, this is done for each fold.
From an initial prediction y?, it produces the final prediction:
y?
?
=
y??M
pred
SD
pred
? SD
gold
+M
gold
. This transformation does
not affect Pearson?s r correlations or rankings, but it would
affect binarized predictions.
5
http://pythonhosted.org/pyenchant/
6
We use the New York Times (nyt), the Los Ange-
les Times-Washington Post (ltw), and the Washington Post-
Bloomberg News (wpb) sections from the fifth edition of En-
glish Gigaword (LDC2011T07).
? max
s?S
n
log(count(s) + 1)
? min
s?S
n
log(count(s) + 1)
where S
n
represents the n-grams of order n from
the given sentence. The model computes the fol-
lowing features from a 5-gram language model
trained on the same three sections of English Gi-
gaword using the SRILM toolkit (Stolcke, 2002):
? the average log-probability of the
given sentence (referred to as
gigaword avglogprob later)
? the number of out-of-vocabulary words in the
sentence
Finally, the system computes the average
log-probability and number of out-of-vocabulary
words from a language model trained on a col-
lection of essays written by non-native English
speakers
7
(?non-native LM?).
3.2.3 Precision Grammar Features
Following Wagner et al (2007) and Wagner et
al. (2009), we use features extracted from preci-
sion grammar parsers. These grammars have been
hand-crafted and designed to only provide com-
plete syntactic analyses for grammatically cor-
rect sentences. This is in contrast to treebank-
trained grammars, which will generally provide
some analysis regardless of grammaticality. Here,
we use (1) the Link Grammar Parser
8
and (2)
the HPSG English Resource Grammar (Copestake
and Flickinger, 2000) and PET parser.
9
We use a binary feature, complete link,
from the Link grammar that indicates whether at
least one complete linkage can be found for a sen-
tence. We also extract several features from the
HPSG analyses.
10
They mostly reflect information
about unification success or failure and the associ-
ated costs. In each instance, we use the logarithm
of one plus the frequency.
7
This did not overlap with the data described in ?2 and
was a subset of the data released by Blanchard et al (2013).
8
http://www.link.cs.cmu.edu/link/
9
http://moin.delph-in.net/PetTop
10
The complete list of relevant statistics used as features
is: trees, unify cost succ, unify cost fail,
unifications succ, unifications fail,
subsumptions succ, subsumptions fail,
words, words pruned, aedges, pedges,
upedges, raedges, rpedges, medges. During
development, we observed that some of these features vary
for some inputs, probably due to parsing search timeouts. On
10 preliminary runs with the development set, this variance
had minimal effects on correlations with human judgments
(less than 0.00001 in terms of r).
176
rour system 0.668
? non-native LM (?3.2.2) 0.665
? HPSG parse (?3.2.3) 0.664
? PCFG parse (?3.2.4) 0.662
? spelling (?3.2.1) 0.643
? gigaword LM (?3.2.2) 0.638
? link parse (?3.2.3) 0.632
? gigaword count (?3.2.2) 0.630
Table 1: Pearson?s r on the development set, for
our full system and variations excluding each fea-
ture type. ?? X? indicates the full model without
the ?X? features.
3.2.4 PCFG Parsing Features
We find phrase structure trees and basic depen-
dencies with the Stanford Parser?s English PCFG
model (Klein and Manning, 2003; de Marneffe et
al., 2006).
11
We then compute the following:
? the parse score as provided by the Stan-
ford PCFG Parser, normalized for sentence
length, later referred to as parse prob
? a binary feature that captures whether the top
node of the tree is sentential or not (i.e. the
assumption is that if the top node is non-
sentential, then the sentence is a fragment)
? features binning the number of dep rela-
tions returned by the dependency conversion.
These dep relations are underspecified for
function and indicate that the parser was un-
able to find a standard relation such as subj,
possibly indicating a grammatical error.
4 Experiments
Next, we present evaluations on the GUG dataset.
4.1 Feature Ablation
We conducted a feature ablation study to iden-
tify the contributions of the different types of fea-
tures described in ?3.2. We compared the perfor-
mance of the full model with all of the features
to models with all but one type of feature. For
this experiment, all models were estimated from
the training set and evaluated on the development
set. We report performance in terms of Pearson?s
r between the averaged 1?4 human labels and un-
rounded system predictions.
The results are shown in Table 1. From these
results, the most useful features appear to be the
n-gram frequencies from Gigaword and whether
the link parser can fully parse the sentence.
4.2 Test Set Results
In this section, we present results on the held-out
test set for the full model and various baselines,
summarized in Table 2. For test set evaluations,
we trained on the combination of the training and
development sets (?2), to maximize the amount of
training data for the final experiments.
We also trained and evaluated on binarized ver-
sions of the ordinal GUG labels: a sentence was
labeled 1 if the average judgment was at least 3.5
(i.e., would round to 4), and 0 otherwise. Evaluat-
ing on a binary scale allows us to measure how
well the system distinguishes grammatical sen-
tences from ungrammatical ones. For some ap-
plications, this two-way distinction may be more
relevant than the more fine-grained 1?4 scale. To
train our system on binarized data, we replaced the
`
2
-regularized linear regression model with an `
2
-
regularized logistic regression and used Kendall?s
? rank correlation between the predicted probabil-
ities of the positive class and the binary gold stan-
dard labels as the grid search metric (?3.1) instead
of Pearson?s r.
For the ordinal task, we report Pearson?s r be-
tween the averaged human judgments and each
system. For the binary task, we report percentage
accuracy. Since the predictions from the binary
and ordinal systems are on different scales, we in-
clude the nonparametric statistic Kendall?s ? as a
secondary evaluation metric for both tasks.
We also evaluated the binary system for the or-
dinal task by computing correlations between its
estimated probabilities and the averaged human
scores, and we evaluated the ordinal system for the
binary task by binarizing its predictions.
12
We compare our work to a modified version of
the publicly available
13
system from Post (2011),
which performed very well on an artificial dataset.
To our knowledge, it is the only publicly available
system for grammaticality prediction. It is very
11
We use the Nov. 12, 2013 version of the Stanford Parser.
12
We selected a threshold for binarization from a grid of
1001 points from 1 to 4 that maximized the accuracy of bina-
rized predictions from a model trained on the training set and
evaluated on the binarized development set. For evaluating
the three single-feature baselines discussed below, we used
the same approach except with grid ranging from the min-
imum development set feature value to the maximum plus
0.1% of the range.
13
The Post (2011) system is available at https://
github.com/mjpost/post2011judging.
177
Ordinal Task Binary Task
r Sig.
r
? % Acc. Sig.
%Acc.
?
our system 0.644 0.479 79.3 0.419
our system
logistic
0.616 * 0.484 80.7 0.428
Post 0.321 * 0.225 75.5 * 0.195
Post
logistic
0.259 * 0.181 74.4 * 0.181
complete link 0.386 * 0.335 74.8 * 0.302
gigaword avglogprob 0.414 * 0.290 76.7 * 0.280
num misspelled -0.462 * -0.370 74.8 * -0.335
Table 2: Human-machine agreement statistics for our system, the system from Post (2011), and simple
baselines, computed from the averages of human ratings in the testing set (?2). ?*? in a Sig. column
indicates a statistically significant difference from ?our system? (p < .05, see text for details). A majority
baseline for the binary task achieves 74.8% accuracy. The best results for each metric are in bold.
different from our system since it relies on par-
tial tree-substitution grammar derivations as fea-
tures. We use the feature computation components
of that system but replace its statistical model. The
system was designed for use with a dataset consist-
ing of 50% grammatical and 50% ungrammatical
sentences, rather than data with ordinal or continu-
ous labels. Additionally, its classifier implementa-
tion does not output scores or probabilities. There-
fore, we used the same learning algorithms as for
our system (i.e., ridge regression for the ordinal
task and logistic regression for the binary task).
14
To create further baselines for comparison,
we selected the following features that represent
ways one might approximate grammaticality if a
comprehensive model was unavailable: whether
the link parser can fully parse the sentence
(complete link), the Gigaword language
model score (gigaword avglogprob),
and the number of misspelled tokens
(num misspelled). Note that we expect
the number of misspelled tokens to be negatively
correlated with grammaticality. We flipped the
sign of the misspelling feature when computing
accuracy for the binary task.
To identify whether the differences in perfor-
mance for the ordinal task between our system and
each of the baselines are statistically significant,
we used the BC
a
Bootstrap (Efron and Tibshirani,
1993) with 10,000 replications to compute 95%
confidence intervals for the absolute value of r for
our system minus the absolute value of r for each
of the alternative methods. For the binary task, we
14
In preliminary experiments, we observed little difference
in performance between logistic regression and the original
support vector classifier used by the system from Post (2011).
used the sign test to test for significant differences
in accuracy. The results are in Table 2.
5 Discussion and Conclusions
In this paper, we developed a system for predict-
ing grammaticality on an ordinal scale and cre-
ated a labeled dataset that we have released pub-
licly (?2) to enable more realistic evaluations in
future research. Our system outperformed an ex-
isting state-of-the-art system (Post, 2011) in eval-
uations on binary and ordinal scales. This is the
most realistic evaluation of methods for predicting
sentence-level grammaticality to date.
Surprisingly, the system from Post (2011) per-
formed quite poorly on the GUG dataset. We spec-
ulate that this is due to the fact that the Post sys-
tem relies heavily on features extracted from au-
tomatic syntactic parses. While Post found that
such a system can effectively distinguish gram-
matical news text sentences from sentences gen-
erated by a language model, measuring the gram-
maticality of real sentences from language learn-
ers seems to require a wider variety of features,
including n-gram counts, language model scores,
etc. Of course, our findings do not indicate that
syntactic features such as those from Post (2011)
are without value. In future work, it may be pos-
sible to improve grammaticality measurement by
integrating such features into a larger system.
Acknowledgements
We thank Beata Beigman Klebanov, Yoko Futagi,
Su-Youn Yoon, and the anonymous reviewers for
their helpful comments. We also thank Jennifer
Foster for discussions about this work and Matt
Post for making his system publicly available.
178
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins,
Aoife Cahill, and Martin Chodorow. 2013.
TOEFL11: A Corpus of Non-Native English. Tech-
nical report, Educational Testing Service.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Alexander Clark, Gianluca Giorgolo, and Shalom Lap-
pin. 2013. Towards a statistical model of grammat-
icality. In Proceedings of the 35th Annual Confer-
ence of the Cognitive Science Society, pages 2064?
2069.
Ann Copestake and Dan Flickinger. 2000. An
open-source grammar development environment
and broad-coverage English grammar using HPSG.
In Proceedings of the 2nd International Confer-
ence on Language Resources and Evaluation (LREC
2000), Athens, Greece.
Deborah Coughlin. 2003. Correlating automated and
human assessments of machine translation quality.
In Proceedings of MT Summit IX, pages 63?70.
Matthew W. Crocker and Frank Keller. 2005. Prob-
abilistic grammars as models of gradience in lan-
guage processing. In Gradience in Grammar: Gen-
erative Perspectives. University Press.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Correcting
Semantic Collocation Errors with L1-induced Para-
phrases. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 107?117, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
LREC 2006, pages 449?454.
B. Efron and R. Tibshirani. 1993. An Introduction to
the Bootstrap. Chapman and Hall/CRC, Boca Ra-
ton, FL.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-level MT evaluation without refer-
ence translations: Beyond language modeling. In
Proceedings of EAMT, pages 103?111. Springer-
Verlag.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing, 12(2):115?129.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423?430, Sapporo,
Japan, July. Association for Computational Linguis-
tics.
John Lee, Ming Zhou, and Xiaohua Liu. 2007. De-
tection of Non-Native Sentences Using Machine-
Translated Training Data. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics; Companion Volume, Short Pa-
pers, pages 93?96, Rochester, New York, April. As-
sociation for Computational Linguistics.
Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-
tin Chodorow. 2011. E-rating machine translation.
In Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 108?115, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine Learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Matt Post. 2011. Judging Grammaticality with Tree
Substitution Grammar Derivations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 217?222, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In 7th International Con-
ference on Spoken Language Processing.
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,
Zhongyang Xiong, John Lee, and Chin-Yew Lin.
2007. Detecting Erroneous Sentences using Auto-
matically Mined Sequential Patterns. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 81?88, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Joel R. Tetreault and Martin Chodorow. 2008. The
Ups and Downs of Preposition Error Detection in
ESL Writing. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(Coling 2008), pages 865?872, Manchester, UK,
August. Coling 2008 Organizing Committee.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2007. A Comparative Evaluation of Deep
and Shallow Approaches to the Automatic Detec-
tion of Common Grammatical Errors. In Proceed-
ings of the 2007 Joint Conference on Empirical
179
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 112?121, Prague, Czech Republic,
June. Association for Computational Linguistics.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2009. Judging grammaticality: Experi-
ments in sentence classification. CALICO Journal,
26(3):474?490.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser
Features for Sentence Grammaticality Classifica-
tion. In Proceedings of the Australasian Language
Technology Association Workshop 2010, pages 67?
75, Melbourne, Australia, December.
180
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 529?535,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
ETS: Discriminative Edit Models for Paraphrase Scoring
Michael Heilman and Nitin Madnani
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541, USA
{mheilman,nmadnani}@ets.org
Abstract
Many problems in natural language process-
ing can be viewed as variations of the task of
measuring the semantic textual similarity be-
tween short texts. However, many systems
that address these tasks focus on a single task
and may or may not generalize well. In this
work, we extend an existing machine transla-
tion metric, TERp (Snover et al, 2009a), by
adding support for more detailed feature types
and by implementing a discriminative learning
algorithm. These additions facilitate applica-
tions of our system, called PERP, to similar-
ity tasks other than machine translation eval-
uation, such as paraphrase recognition. In
the SemEval 2012 Semantic Textual Similar-
ity task, PERP performed competitively, par-
ticularly at the two surprise subtasks revealed
shortly before the submission deadline.
1 Introduction
Techniques for measuring the similarity of two sen-
tences have various potential applications: auto-
mated short answer scoring (Nielsen et al, 2008;
Leacock and Chodorow, 2003), question answering
(Wang et al, 2007), machine translation evaluation
(Przybocki et al, 2009; Snover et al, 2009a), etc.
An important aspect of this problem is that sim-
ilarity is not binary. Sentences can be very seman-
tically similar, such that they might be called para-
phrases of each other. They might be completely
different. Or, they might be somewhere in between.
Indeed, it is arguable that all sentence pairs (except
exact duplicates) lie somewhere on a continuum of
similarity. Therefore, it is desirable to develop meth-
ods that model sentence pair similarity on a contin-
uous, or at least ordinal, scale.
In this paper, we describe a system for measuring
the semantic similarity of pairs of short texts. As a
starting point, we use the Translation Error Rate Plus
(Snover et al, 2009a), or TERp, system, which was
specifically developed for machine translation eval-
uation. TERp takes two sentences as input, finds a
set of weighted edits that convert one into the other
with low overall weight, and then produces a length-
normalized score. TERp also has a greedy, heuris-
tic learning algorithm for inducing weights from la-
beled sentence pairs in order to increase correlations
with human similarity scores.
Some features of the original TERp make adap-
tation to other semantic similarity tasks difficult, in-
cluding its largely one-to-one mapping of features
to edits and its heuristic, greedy learning algorithm.
For example, there is a single feature for lexical sub-
stitution, even though it is clear that different types
of substitutions have different effects on similarity
(e.g., substituting ?43.6? with ?17? versus substitut-
ing ?a? for ?an?). In addition, the heuristic learn-
ing algorithm, which involves perturbing the weight
vector by small amounts as in grid search, seems un-
scalable to larger sets of overlapping features.
Therefore, here, we use TERp?s inference algo-
rithms that find low cost edit sequences but use a dis-
criminative learning algorithm based on the Percep-
tron (Rosenblatt, 1958; Collins, 2002) to estimate
edit cost parameters, along with an expanded fea-
ture set for broader coverage of the phenomena that
are relevant to sentence-to-sentence similarity. We
529
refer to this new approach as Paraphrase Edit Rate
with the Perceptron (PERP).
In addition to describing PERP, we discuss how it
was applied for the SemEval 2012 Semantic Textual
Similarity (STS) task.
2 Problem Definition
In this work, our goal is to create a system that can
take as input two sentences (or short texts) x1 and x2
and produce as output a prediction y? for how simi-
lar they are. Here, we use the 0 to 5 ordinal scale
from the STS task, where increasing values indicate
greater semantic similarity.
The STS task data includes five subtasks with text
pairs from different sources: the Microsoft Research
Paraphrase Corpus (Dolan et al, 2004) (MSRpar),
The Microsoft Research Video corpus (Chen and
Dolan, 2011) (MSRvid), statistical machine transla-
tion output of parliament proceedings (Koehn, 2005)
(SMT-eur). For each of these sources, approxi-
mately 750 sentence pairs x1 and x2 and gold stan-
dard similarity values y were provided for training
and development.
In addition, there were two surprise data sources
revealed shortly before the submission deadline:
pairs of sentences from Ontonotes (Pradhan and
Xue, 2009) and Wordnet (Fellbaum, 1998) (OnWN),
and machine translations of sentences from news
conversations (SMT-news). For all five sources,
the held-out test set contained several hundred text
pairs. See the task description (Agirre et al, 2012)
for additional details.
3 TER, TERp, and PERP
In this section, we briefly describe the TER and
TERp machine translation metrics, and how the
PERP system extends them in order to better model
semantic textual similarity.
TER (Snover et al, 2006) uses a greedy search al-
gorithm to find a set of edits to convert one of the
paired input sentences into the other. We can view
this set of edits as an alignment a between the two
input sentences x1 and x2, and when two words in
x1 and x2, respectively, are part of an edit operation,
we say that those words are aligned.1 Unlike tradi-
1For machine translation evaluation with TERp and PERP,
x1 is a system?s hypothesis and x2 is a reference translation. For
tional edit distance measures, TER allow for shifts?
that is, edits that change the positions of words or
phrases in the input sentence x1. Essentially, TER
searches among a set of possible shifts of the phrases
in x1 to find a set of shifts that result in the least
cost alignment, using edits of other types, between
x2 and the shifted version of x1. TER allows one to
specify costs for different edit types, but it does not
include a method for learning those costs from data.
TERp (Snover et al, 2009b; Snover et al, 2009a)
extends TER in two key ways. First, TERp in-
cludes new types of edits, including edits for substi-
tution of synonyms, word stems, and phrasal para-
phrases extracted from a pivot-based paraphrase ta-
ble (?3.1). Second, it includes a heuristic learning
algorithm for inferring cost parameters from labeled
data. TERp includes 8 types of edits: match (M), in-
sertion (I), deletion (D), substitution (S), stemming
(T), synonymy (Y), shift (Sh), and phrase substitu-
tion (P). The edits are mutually exclusive, such that
synonymy edits do not count as substitutions, for ex-
ample. TERp has 11 total parameters, with a single
parameter for each edit except for phrase substition,
which has four.
PERP has a general framework similar to that
of TERp. It extends TERp, however, by includ-
ing additional edit parameters, and by using a dis-
criminative learning algorithm (see ?5) to learn pa-
rameters rather than the heuristic technique used by
TERp. Thus, PERP uses the same greedy algorithm
as TERp for finding the optimal sets of edits given
the cost parameters, but it allows the cost for an indi-
vidual edit to depend on multiple, overlapping fea-
tures of that edit. For example, costs for substitu-
tion edits depend on whether the aligned words are
pronouns, whether the aligned words represent num-
bers, the lengths of the aligned words, etc. See ?4 for
the full list of features in PERP.
An alignment from the MSRpar portion of the
STS training data is illustrated in Figure 1.
3.1 Phrasal Paraphrases
PERP uses probabilistic phrasal substitutions to
align phrases in the hypothesis with phrases in the
all STS subtasks, we assigned sentences in the first and second
columns of the input files to x2 and x1, respectively, so that
the hypotheses and references in the SMT-eur subtask would be
assigned appropriately.
530
the research firm earlier had forecast an increase of 4.9 percent .
the firm earlier had predicted increase this year a 4.9 percent .
the firm had predicted earlier this year a 4.9 percent increase .
synonymy
shift shift insert
delete delete delete
insertinsert
x1
x2
Figure 1: An example of a PERP alignment for a sentence pair from the Microsoft Research Paraphrase Corpus.
The search algorithm first performs shifts on x1 and then performs other edits on x2. The zero cost edits that match
individual words are not shown.
reference. It does so by looking up?in a pre-
computed phrase table?paraphrases of phrases in
the reference and using its associated edit cost as
the cost of performing a match against the hypoth-
esis. The paraphrase table used in PERP was iden-
tical to the one used by Snover et al (2009a). It
was extracted using the pivot-based method as de-
scribed by Bannard and Callison-Burch (2005) with
several additional filtering mechanisms to increase
the precision of the extracted pairs. The pivot-based
method utilizes the inherent monolingual semantic
knowledge from bilingual corpora: we first iden-
tify phrasal correspondences between English and a
given foreign language F , then map from English to
English by following translation units from English
to the other language and back. For example, if the
two English phrases e1 and e2 both correspond to
the same foreign phrase f , then they may be consid-
ered to be paraphrases of each other with the follow-
ing probability:
p(e1|e2) ? p(e1|f)p(f |e2)
If there are several pivot phrases that link the two
English phrases, then they are all used in computing
the probability:
p(e1|e2) ?
?
f ?
p(e1|f ?)p(f ?|e2)
We used the same phrasal paraphrase database as
in TERp (Snover et al, 2009a), which was extracted
from an Arabic-English newswire bitext containing
a million sentences. A few examples of the para-
phrase pairs used in the MSRpar portion of the STS
training data are shown below:
(commission? panel)
(the spying? espionage)
(suffered? underwent)
(room to? space for)
(per cent? percent)
4 Features
As discussed in ?3, PERP expands on TERp?s origi-
nal features in order to better model semantic textual
similarity.
PERP models a pair of sentences x1 and x2 us-
ing a feature function f(a) that extracts a vector of
real-valued features from an alignment a between
x1 and x2. This alignment is found with TERp?s
inference algorithm and consists of a set of edits
of various types along with information about the
words on which those edits operate. For example,
the alignment might contain an edit with the infor-
mation, ?The token ?the? in x1 was substituted for
the token ?an? in x2.? This edit would increment the
features in f(a) for the number of substitutions and
the number of substitutions of stopwords, along with
other relevant substitution features.
The set of features encoded in f(a) are described
in Table 1.2 It includes general features that always
fire for edits of a particular type (e.g., the ?Substi-
tution? feature) as well as specific features that fire
only in specific situations (e.g., the ?Sub-Pronoun-
Both? edit, which fires only when one pronoun is
substituted for another).
The function f(a) is normalized for sentence
2All words were converted to lower-case. Word frequen-
cies were calculated from the NYT stories in the fifth edition
of the English Gigaword corpus. The stories were tokenized
using NLTK and words occurring fewer than 100 times were
excluded. Words occurring at least 100 times constituted the vo-
cabulary used for computing the OOV features. The OOV and
frequency features only fired for words that consisted only of
letters, and the frequency features did not fire for OOV words.
The set of negation words including the following: ?no?, ?not?,
?never?, and ?n?t?. The stopword list contained 158 common
words and punctuation symbols.
531
Edits Feature Name Description
- Intercept Always 1 (and not normalized by text lengths)
T Stemming The number of times that two words with the same stem, according to the Porter
(1980) stemmer, were aligned.
Y Synonymy The number of times that a pair of synonyms, according to WordNet (Fellbaum,
1998), were aligned.
Sh Shift The number of shifts.
P Paraphrase1 The number of phrasal paraphrasing operations.
P Paraphrase2 The sum of q log10(p), where p is the probability in the pivot-based paraphrase table
for a paraphrase edit and q is the number of edits for that paraphrase edit. See Snover
et al (2009a) for further explanation.
P Paraphrase3 The sum of pq, where p and q are as above.
P Paraphrase4 The sum of q, where q is as above.
I Insertion The number of insertions.
D Deletion The number of deletions.
I, D Insert-Delete-
LogFreq
The sum of log10 freq(w) over all insertions and deletions, where w is the word
being inserted or deleted and freq(w) is the relative frequency of w.
I, D Insert-Delete-
LogWordLen
The sum of log10 length(w) over all insertions and deletions, where w is the word
being inserted or deleted.
I, D Insert-Delete-
X
The number of insertions and deletions of X in alignment, where X is: (a) punctu-
ation, (b) numbers, (c) personal pronouns, (d) negation words, (e) stop words, or (f)
out-of-vocabulary (OOV) words (6 features in all).
S Substitution The number of substitutions.
S Sub-X-Both The number of substitutions where both words are: (a) punctuation, (b) numbers, (c)
personal pronouns, (d) negation words, (e) stop words, or (f) OOV words (6 features
in all).
S Sub-X-1only The number of substitutions where only one word is: (a) punctuation, (b) a number,
(c) a personal pronoun, (d) a negation word, (e) a stop word, or (f) an OOV word (6
features in all).
S Sub-LogFreq-
Diff
The sum of | log10 freq(w1)? log10 freq(w2)| over all substitutions.
S Sub-Contain The number of substitutions where both words have more than 5 characters and one
is a proper substring of the other.
S Sub-Diff-By-
NonWord
The number of substitutions where the words differ only by non-alphanumeric char-
acters.
S Sub-Small-
LevDist
The number of substitutions where both words have more than 5 characters and the
Levenshtein distance between them is 1.
S Sub-Norm-
LevDist
The sum of the following over all substitutions: the Levenshtein distance between
the words normalized by the length of the longer word.
Table 1: The set of features in PERP. The first column lists which edits for which each feature is relevant.
lengths by dividing all the values in Table 1 by the
sum of the number of words in x1 and x2, except for
the intercept feature that models the base similarity
value in the training data and always has value 1.
There are 36 features and corresponding parame-
ters in all, compared to 11 for TERp.
It is worth pointing out that while the mutual ex-
clusivity between most of the original TERp edits
is preserved, PERP does have shared features be-
tween insert and delete edits (e.g., ?Insert-Delete-
Number?), and could in principle share features be-
tween substitution, stemming, and synonymy edits.
5 Learning
Given a training set consisting of paired sentences
x1 and x2 and gold standard semantic similarity rat-
ings y, PERP uses Algorithm 1 to induce a good set
532
Algorithm 1 learn(w, T , ?, x1,x2,y):
An Averaged Perceptron algorithm for learning edit
cost parameters. T is the number of iterations
through the dataset. ? is a learning rate. x1 and
x2 are paired lists of sentences, and y is a list of
similarities that correspond to those sentence pairs.
wsum = 0
for t = 1, 2, . . . , T do
x1,x2,y = shuffle(x1,x2,y)
for i = 1, 2, . . . , |y| do
a = TERpAlign(w, x1i, x2i)
y? = w ? f(a)
w = w + ?(yi ? y?)f(a)
w = applyShiftConstraint(w)
wsum = wsum + w
end for
end for
return wsumT |y|
of cost parameters for its various features.3 The al-
gorithm is a fairly straightforward application of the
Perceptron algorithm described by Collins (2002).4
The only notable difference is that the algorithm
constrains PERP?s shift parameter to be at least 0.01
in the step labeled ?applyShiftConstraint.? We found
that TERp?s inference algorithm would fail if the
shift cost reached zero.5 In our experiments, we ini-
tialized all weights to 0, except for the following: the
?Substitution,? ?Insertion,? and ?Deletion? weights
were initialized to 1.0, and the ?Shift? weight was
initialized to 0.1. Following Collins (2002), the al-
gorithm returns an averaged version of the weights,
though this did not appear to substantially impact
performance.
3The ?shuffle? step shuffles the lists of sentence pairs and
scores together such that their orderings are randomized but that
they stay aligned with each other.
4There are a few hyperparameters in the learning algorithms.
For our experiments, we set the number of iterations through
the training data T to 200. We set the learning rate ? to 0.01 to
avoid large oscillations in the parameters. We did not system-
atically tune the hyperparameters. Other values might lead to
better performance.
5With zero cost shifts, TERp would enter a loop and even-
tually exceed the amount of available memory. We also set the
same minimum cost of 0.01 for shifts in our experiments with
the original TERp.
6 Experiments
In this section, we report results for the STS shared
task. For a full description of the task, see Agirre et
al. (2012).
The task consisted of three known subtasks
(MSRpar, MSRvid, and SMT-eur) and two surprise
subtasks (On-WN, SMT-news). For the known sub-
tasks, we trained models with task-specific data
only. For the On-WN subtask, we used the model
trained for MSRpar. For SMT-news, we used the
model trained for SMT-eur.
Our submissions to the task included results from
two variations, one using the full system (PERP-
phrases) and one with the paraphrase substitution
edits disabled (PERP), in order to isolate the effect
of including phrasal paraphrases. In our original
submission, the PERPphrases system included a mi-
nor bug that affected the calculation of the phrasal
paraphrasing features. Here, we report both the orig-
inal results and a corrected version (?PERPphrases
(fix)?), though the correction only minimally af-
fected performance. We also tested two variations
of the original TERp system: one with the weights
set as reported by Snover et al (2009a) (?TERp
(default)?), and one tuned in the same task-specific
manner as PERP (?TERp (tuned)?). We multiplied
TERp?s predictions by ?1 since it produces costs
rather than similarities.
The results, in terms of Pearson correlations with
test set gold standard scores, are shown in Table 2.
In addition to correlations for each subtask, we in-
clude the three aggregated measures used for the
task. The ?ALL? measure is the Pearson correlations
on the concatenation of all the data for all five sub-
tasks. It was the original measured used to aggregate
the results for the different subtasks. The second ag-
gregated measure is the ?Allnrm? measure, which
we view as an oracle because it uses the gold stan-
dard similarity values from the test set to adjust sys-
tem predictions. The final aggregate measure is the
mean of the correlations for the subtasks, weighted
by the number of examples in each subtask?s test set
(?Mean?). See Agirre et al (2012) for a full descrip-
tion of the metrics.
For comparison, the table also includes the re-
sults from the top-ranked submission according to
the ?ALL? measure, the results for the word-overlap
533
Aggregated Measures Subtask Measures
ALL ALLnrm Mean MSRpar MSRvid SMT-eur On-WN SMT-news
UKP (top-ranked) .8239 .8579 .6773 .6830 .8739 .5280 .6641 .4937
PERPphrases (fix) ? .7837 ? .6405 .6410 .7209 .4852 .7127 .5312
PERPphrases .7834 .8089 .6399 .6397 .7200 .4850 .7124 .5312
PERP .7808 .8064 .6305 .6211 .7210 .4722 .7080 .5149
TERp (tuned) ? .5558 ? .5582 .5400 .6099 .4967 .5862 .5135
TERp (default) .4477 .7291 .5253 .5049 .5217 .4748 .6169 .4566
baseline .3110 .6732 .4356 .4334 .2996 .4542 .5864 .3908
mean of submissions .5864 .7773 .5286 .4894 .7049 .3958 .5557 .3731
Table 2: Pearson correlations between predictions about the test data and gold standard scores. ??? marks experiments
that were not parts of the official SemEval task 6 evaluation. The highest correlation in each column is given in bold.
ALLnrm results are not included for all runs because we did not have an implementation of that measure.
baseline from the organizers (Agirre et al, 2012),
and the means across all 88 submissions (not includ-
ing the baseline).
Table 3 shows the rankings in the official results
of the PERPphrases submission, for each subtask
and overall, along with Pearson correlations from
PERP and the best submission for each subtask.
Aggregated Measure Rank ? ?best
ALL 6 .7834 .8239
ALLnrm 27 .8089 .8635
Mean 7 .6399 .6773
Subtask Measure Rank ? ?best
MSRpar 8 .6397 .7343
MSRvid 52 .7200 .8803
SMT-eur 21 .4850 .5666
On-WN 2 .7124 .7273
SMT-news 4 .5312 .6085
Table 3: The ranking and correlation (?) obtained by
PERPphrases for each of the five datasets as well for all
datasets combined. The STS task had a total of 88 sub-
missions. ?best shows the correlation for the best submis-
sion, across all submissions, for each dataset.
7 Conclusion
From the results in ?6, PERP appears to be com-
petitive at measuring semantic textual similarity. It
performed particularly well on the surprise subtasks,
indicating that it generalizes well to new data. Fi-
nally, with the exception of the SMT-eur machine
translation evaluation subtask, PERP outperformed
the TERp system for all of the STS subtasks.
Acknowledgments
We would like to thank the organizers of SemEval
and the Semantic Textual Similarity task. We would
also like to thank Matt Snover for making the origi-
nal TERp code available.
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre.
2012. SemEval-2012 task 6: A pilot on semantic tex-
tual similarity. In Proc. of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), in con-
junction with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
C. Bannard and C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proc. of ACL, pages
597?604.
D. Chen and W. B. Dolan. 2011. Collecting highly par-
allel data for paraphrase evaluation. In Proc. of ACL,
pages 190?200.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
the perceptron algorithm. In Proc. of EMNLP.
W. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In Proc. of
COLING, pages 350?356, Geneva, Switzerland.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. Bradford Books.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In Proc. of Machine Transla-
tion Summit.
C. Leacock and M. Chodorow. 2003. c-rater: Scoring of
short-answer questions. Computers and the Humani-
ties, 37.
534
R. D. Nielsen, W. Ward, and J. H. Martin. 2008. Clas-
sification errors in a domain-independent assessment
system. In Proc. of the Third Workshop on Innova-
tive Use of Natural Language Processing for Building
Educational Applications.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130?137.
S. S. Pradhan and N. Xue. 2009. OntoNotes: The 90%
solution. In Proc. of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Companion Volume: Tutorial Abstracts, pages
11?12.
M. A. Przybocki, K. Peterson, S. Bronsart, and G. A.
Sanders. 2009. The NIST 2008 metrics for machine
translation challenge - overview, methodology, met-
rics, and results. Machine Translation, 23(2-3):71?
103.
F. Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of Translation Edit Rate
with targeted human annotation. In Proc. of the Con-
ference of the Association for Machine Translation in
the Americas (AMTA).
M. Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009a. Fluency, adequacy, or HTER? Exploring dif-
ferent human judgments with a tunable MT metric. In
Proc. of the Fourth Workshop on Statistical Machine
Translation at the 12th Meeting of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2009), March.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009b. TER-Plus: Paraphrase, semantic, and align-
ment enhancements to Translation Edit Rate. Machine
Translation, 23(2?3):117?127.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What is
the Jeopardy model? A quasi-synchronous grammar
for QA. In Proc. of of EMNLP.
535
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 96?102, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
HENRY-CORE: Domain Adaptation and Stacking for Text Similarity?
Michael Heilman and Nitin Madnani
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541, USA
{mheilman,nmadnani}@ets.org
Abstract
This paper describes a system for automat-
ically measuring the semantic similarity be-
tween two texts, which was the aim of the
2013 Semantic Textual Similarity (STS) task
(Agirre et al, 2013). For the 2012 STS task,
Heilman and Madnani (2012) submitted the
PERP system, which performed competitively
in relation to other submissions. However,
approaches including word and n-gram fea-
tures also performed well (Ba?r et al, 2012;
S?aric? et al, 2012), and the 2013 STS task fo-
cused more on predicting similarity for text
pairs from new domains. Therefore, for the
three variations of our system that we were al-
lowed to submit, we used stacking (Wolpert,
1992) to combine PERP with word and n-
gram features and applied the domain adapta-
tion approach outlined by Daume III (2007)
to facilitate generalization to new domains.
Our submissions performed well at most sub-
tasks, particularly at measuring the similarity
of news headlines, where one of our submis-
sions ranked 2nd among 89 from 34 teams, but
there is still room for improvement.
1 Introduction
We aim to develop an automatic measure of the se-
mantic similarity between two short texts (e.g., sen-
tences). Such a measure could be useful for vari-
ous applications, including automated short answer
scoring (Leacock and Chodorow, 2003; Nielsen et
al., 2008), question answering (Wang et al, 2007),
? System description papers for this task were required to
have a team ID and task ID (e.g., ?HENRY-CORE?) as a prefix.
and machine translation evaluation (Przybocki et al,
2009).
In this paper, we describe our submissions to the
2013 Semantic Textual Similarity (STS) task (Agirre
et al, 2013), which evaluated implementations of
text-to-text similarity measures. Submissions were
evaluated according to Pearson correlations between
gold standard similarity values acquired from hu-
man raters and machine-produced similarity val-
ues. Teams were allowed to submit up to three
submissions. For each submission, correlations
were calculated separately for four subtasks: mea-
suring similarity between news headlines (?head-
lines?), between machine translation outputs and hu-
man reference translations (?SMT?), between word
glosses from OntoNotes (Pradhan and Xue, 2009)
and WordNet (Fellbaum, 1998) (?OnWN?), and be-
tween frame descriptions from FrameNet (Fillmore
et al, 2003) and glosses from WordNet (?FNWN?).
A weighted mean of the correlations was also com-
puted as an overall evaluation metric (the OnWn and
FNWN datasets were smaller than the headlines and
SMT datasets).
The suggested training data for the 2013 STS
task was the data from the 2012 STS task (Agirre
et al, 2012), including both the training and test
sets for that year. The 2012 task was similar ex-
cept that the data were from a different set of sub-
tasks: measuring similarity between sentences from
the Microsoft Research Paraphrase corpus (Dolan
et al, 2004) (?MSRpar?), between sentences from
the Microsoft Research Video Description corpus
(Chen and Dolan, 2011) (?MSRvid?), and between
human and machine translations of parliamentary
96
proceedings (?SMTeuroparl?). The 2012 task pro-
vided training and test sets for those three subtasks
and also included two additional tasks with just test
sets: a similar OnWN task, and measuring similar-
ity between human and machine translations of news
broadcasts (?SMTnews?).
Heilman and Madnani (2012) described the PERP
system and submitted it to the 2012 STS task. PERP
measures the similarity of a sentence pair by find-
ing a sequence of edit operations (e.g., insertions,
deletions, substitutions, and shifts) that converts one
sentence to the other. It then uses various features
of the edits, with weights learned from labeled sen-
tence pairs, to assign a similarity score. PERP per-
formed well, ranking 7th out of 88 submissions from
35 teams according to the weighted mean correla-
tion. However, PERP lacked some of the useful
word and n-gram overlap features included in some
of the other top-performing submissions. In addi-
tion, domain adaptation seemed more relevant for
the STS 2013 task since in-domain data was avail-
able only for one (OnWN) of the four subtasks.
Therefore, in this work, we combine the PERP
system with various word and n-gram features.
We also apply the domain adaptation technique of
Daume III (2007) to support generalization beyond
the domains in the training data.
2 System Details
In this section, we describe the system we devel-
oped, and the variations of it that comprise our sub-
missions to the 2013 STS task.
Our system is a linear model estimated using
ridge regression, as implemented in the scikit-learn
toolkit (Pedregosa et al, 2011). The system uses
a 5-fold cross-validation grid search to tune the ?
penalty for ridge regression (with ? ? 2{?5,?4,...,4}).
During development, we evaluated its performance
on the full STS 2012 data (training and test) us-
ing 10-fold cross-validation, with the 5-fold cross-
validation being used to tune within each training
partition.
2.1 Features
Our full system uses the following features com-
puted from an input sentence pair (s1, s2).
The system standardizes feature values to zero
mean and unit variance by subtracting the feature?s
mean and dividing by its standard deviation. The
means and standard deviations are estimated from
the training set, or from each training partition dur-
ing cross-validation.
2.1.1 n-gram Overlap Features
The system computes Jaccard similarity (i.e., the
ratio of the sizes of the set intersection to the set
union) for the following overlap features:
? character n-gram overlap (n = 1 . . . 12). Note
that this is computed from the entire original
texts for a pair, including punctuation, whites-
pace, etc.
? word n-gram overlap (n = 2 . . . 8). We do not
include n = 1 here because it would be identi-
cal to the n = 1 version for the unordered word
n-gram feature described next.
? unordered word n-gram overlap features (n =
1 . . . 3). By unordered, we mean combina-
tions (in the mathematical sense of ?combi-
nations?) of word tokens, regardless of order.
Note that these features are similar to the word
n-gram overlap features except that the words
need not be contiguous to match. For example,
the text ?John saw Mary? would result in the
following unordered word n-grams: {john},
{mary}, {saw}, {john, saw}, {mary, saw},
{john, mary}, and {john, mary, saw}.
For the word and unordered n-gram overlap fea-
tures, we computed two variants: one based on all
tokens and one based on just content words, which
we define as words that are not punctuation and do
not appear in the NLTK (Bird et al, 2009) English
stopword list. We lowercase everything for the word
overlap measures but not for character overlap.
2.1.2 Length Features
The system includes various length-related fea-
tures, where Lmax = max(length(s1), length(s2)),
Lmin = min(length(s1), length(s2)), and length(x)
denotes the number of tokens in x. log denotes the
natural logarithm.
? log(LmaxLmin )
? Lmax?LminLmax
97
? log(Lmin)
? log(Lmax)
? log(|Lmax ? Lmin|+ 1)
2.1.3 Sentiment Features
The system includes various features based on the
proprietary sentiment lexicon described by Beigman
Klebanov et al (2012). Each word in this lexicon
is associated with a 3-tuple specifying a distribution
over three classes: positive, negative, and neutral.
These distributions were estimated via crowdsourc-
ing. If a word is not in the lexicon, we assume its
positivity and negativity are zero.
We define the set of sentiment words in a sen-
tence s as ?(s) = {w : positivity(w) > 0.5 ?
negativity(w) > 0.5}. We also define the pos-
itivity, negativity, and neutrality of a sentence as
the sum over the corresponding values of indi-
vidual words w. For example, positivity(s) =
?
w?s positivity(w).
The system includes the following features:
? ?(s1)??(s2)?(s1)??(s2) (i.e., the Jaccard similarity of the
sentiment words)
? The cosine distance between
(positivity(s1), negativity(s1)) and
(positivity(s2), negativity(s2))
? |positivity(s1)? positivity(s2)|
? |negativity(s1)? negativity(s2)|
? |neutrality(s1)? neutrality(s2)|
2.1.4 PERP with Stacking
The system also incorporates the PERP system
(Heilman and Madnani, 2012) (as briefly described
in ?1) as a feature in its model by using 10-fold
stacking (Wolpert, 1992). Stacking is a procedure
similar to k-fold cross-validation that allows one to
use the output of one model as the input to another
model, without requiring multiple training sets. A
PERP model is iteratively trained on nine folds and
then the PERP feature is computed for the tenth,
producing PERP features for the whole training set,
which are then used in the final regression model.
We trained PERP in a general manner using data
from all the STS 2012 subtasks rather than training
subtask-specific models. PERP was trained for 100
iterations.
We refer readers to Heilman and Madnani (2012)
for a full description of PERP. Next, we provide de-
tails about modifications made to PERP since STS
2012. Although these details are not necessary to
understand how the system works in general, we in-
clude them here for completeness.
? We extended PERP to model abbreviations as
zero cost edits, using a list of common abbrevi-
ations extracted from Wikipedia.1
? In a similar vein, we also extended PERP
to model multiword sequences with differing
punctuation (e.g., ?Built-In Test? ? ?Built In
Test?) as zero cost edits.
? We changed the stemming and synonymy edits
in the original PERP (Heilman and Madnani,
2012) to be substitution edits that activate addi-
tional stemming and synonymy indicator fea-
tures.
? We added an incentive to TERp?s (Snover et
al., 2009) original inference algorithm to pre-
fer matching words when searching for a good
edit sequence. We added this to avoid rare
cases where other edits would have a negative
costs, and then the same word in a sentence
pair would be, for example inserted and deleted
rather than matched.
? We fixed a minor bug in the inference algo-
rithm, which appeared to only affect results on
the MSRvid subtask in the STS 2012 task.
? We tweaked the learning algorithm by increas-
ing the learning rate and not performing weight
averaging.
2.2 Domain Adaptation
The system also uses the domain adaptation tech-
nique described by Daume III (2007) to facilitate
generalization to new domains. Instead of having
a single weight for each of the features described
above, the system maintains a generic and a subtask-
specific copy. For example, the content bigram over-
lap feature had six copies: a generic copy and one
for each of the five subtasks in the training data from
1http://en.wikipedia.org/wiki/List_of_
acronyms_and_initialisms, downloaded April 27,
2012
98
STS 2012 (i.e., OnWN, MSRpar, MSRvid, SMTeu-
roparl, SMTnews). And then for an instance from
MSRpar, only the generic and MSRpar-specific ver-
sions of the feature will be active. For an instance
from a new subtask (e.g., a test set instance), only
the generic feature will be active.
We also included a generic intercept feature and
intercept features for each subtask (these always had
a value of 1). These help the model capture, for
example, whether high or low similarities are more
frequent in general, without having to use the other
feature weights to do so.
2.3 Submissions
We submitted three variations of the system.
? Run 1: This run used all the features described
above. In addition, we mapped the test subtasks
to the training subtasks as follows so that the
specific features would be active for test data
from previously unseen but related subtasks:
headlines to MSRpar, SMT to SMTnews, and
FNWN to OnWN.
? Run 2: As in Run 1, this run used all the fea-
tures described above. However, we did not
map the STS 2013 subtasks to STS 2012 sub-
tasks. Thus, the specific copies of features were
only active for OnWN test set examples.
? Run 3: This run used all the features except for
the PERP and sentiment features. Like Run 2,
this run did not map subtasks.
3 Results
This section presents results on the STS 2012 data
(our development set) and results for our submis-
sions to STS 2013.
3.1 STS 2012 (development set)
Although we used cross-validation on the entire STS
2012 dataset during preliminary experiments (?2),
in this section, we train the system on the original
STS 2012 training set and report performance on the
original STS 2012 test set, in order to facilitate com-
parison to submissions to that task. It is important to
note that our system?s results here may be somewhat
optimistic since we had access to the STS 2012 test
data and were using it for development, whereas the
participants in the 2012 task only had access to the
training data.
Table 1 presents the results. We include the results
for our three submissions, the results for the top-
ranked submission according to the weighted mean
(?UKP?), the results for the best submission from
Heilman and Madnani (2012) (?PERPphrases?), and
the mean across all submissions. Note that while we
compare to the PERP submission from Heilman and
Madnani (2012), the results are not directly compa-
rable since the version of PERP is not the same and
since PERP was trained differently.
For Run 1 on the STS 2012 data, we mapped
OnWN to MSRpar, and SMTnews to SMTeuroparl,
similar to Heilman and Madnani (2012).
3.2 STS 2013 (unseen test set)
Table 2 presents results for our submissions to the
2013 STS task. We include results for our three sub-
missions, results for the top-ranked submission ac-
cording to the weighted mean, results for the base-
line provided by the task organizers, and the mean
across all submissions and the baseline from the or-
ganizers.2
Note that while our Run 2 submission outper-
formed the top-ranked UMBC submission on the
headlines subtask, as shown in 2, there was another
UMBC submission that performed better than Run 2
for the headlines subtask.
4 Discussion
The weighted mean correlation across tasks for our
submissions was relatively poor compared to the
top-ranked systems for STS 2013: our Run 1, Run 2,
and Run 3 submissions beat the baseline and ranked
41st, 26th, and 48th, respectively, out of 89 submis-
sions.
The primary reason for this result is that perfor-
mance of our submissions was poor for the OnWN
subtask, where, e.g., our Run 2 submission?s corre-
lation was r = .4631, compared to r = .8431 for
the top-ranked submission for that subtask (?deft-
baseline?). Upon investigation, we found that
OnWN training and test data were very different in
terms of their score distributions. The mean gold
2The STS 2013 results are from http://ixa2.si.
ehu.es/sts/.
99
Submission MSRpar MSRvid SMTeuroparl OnWN SMTnews W. Mean
Run 1 .6461 .8060 .5014 .7073 .4876 .6577
Run 2 .6461 .8060 .5014 .7274 .4744 .6609
Run 3 .6369 .7904 .5101 .7010 .4985 .6529
UKP (top-ranked) .6830 .8739 .5280 .6641 .4937 .6773
PERPphrases .6397 .7200 .4850 .7124 .5312 .6399
mean-2012 .4894 .7049 .3958 .5557 .3731 .5286
Table 1: Pearson correlations for STS 2012 data for each subtask and then the weighted mean across subtasks. ?UKP?
was submitted by Ba?r et al (2012), ?PERPphrases? was submitted by Heilman and Madnani (2012), and ?mean-2012?
is the mean of all submissions to STS 2012.
Submission headlines OnWN FNWN SMT W. Mean
Run 1 .7601 .4631 .3516 .2801 .4917
Run 2 .7645 .4631 .3905 .3593 .5229
Run 3 .7103 .3934 .3364 .3308 .4734
UMBC (top-ranked) .7642 .7529 .5818 .3804 .6181
baseline .5399 .2828 .2146 .2861 .3639
mean-2013 .6022 .5042 .2887 .2989 .4503
Table 2: Pearson correlations for STS 2013 data for each subtask and then the weighted mean across subtasks.
?UMBC? = ?UMBC EBIQUITY-ParingWords?, and ?mean-2013? is the mean of the submissions to STS 2013 and
the baseline.
standard similarity value for the STS 2012 OnWN
data was 3.87 (with a standard deviation of 1.02),
while the mean for the 2013 OnWN data was 2.31
(with a standard deviation of 1.76). We speculate
that our system performed relatively poorly because
it was expecting the OnWN data to include many
highly similar sentences (as in the 2012 data). We
hypothesize that incorporating more detailed Word-
Net information (only the PERP feature used Word-
Net, and only in a limited fashion, to check syn-
onymy) and task-specific features for comparing
definitions might have helped performance for the
OnWN subtask.
If we ignore the definition comparison subtasks,
and consider performance on just the headlines and
SMT subtasks, the system performed quite well.
Our Run 2 submission had a mean correlation of
r = .5619 for those two subtasks, which would rank
5th among all submissions.
We have not fully explored the effects on perfor-
mance of the domain adaptation approach used in
the system, but our approach of mapping tasks used
for our Run 1 submission did not seem to help. It
seems better to keep a general model, as in Runs 2
and 3.
Additionally, we observe that the performance of
Run 3, which did not use the PERP and sentiment
features, was relatively good compared to Runs 1
and 2, which used all the features. This indicates
that if speed and implementation simplicity are im-
portant concerns for an application, it may suffice to
use relatively simple overlap and length features to
measure semantic similarity.
The contribution of domain adaptation is not
clear. Mapping novel subtasks to tasks for which
training data is available (?2.3), in combination with
the domain adaptation technique we used, did not
generally improve performance. However, we leave
to future work a detailed analysis of whether the
domain adaptation approach (without mapping) is
better than simply training a separate system for
each subtask and using out-of-domain data when in-
domain data is unavailable.
5 Conclusion
In this paper, we described a system for predicting
the semantic similarity of two short texts. The sys-
tem uses stacking to combine a trained edit-based
similarity model (Heilman and Madnani, 2012) with
100
simple features such as word and n-gram overlap,
and it uses the technique described by Daume III
(2007) to support generalization to domains not rep-
resented in the training data. We also presented eval-
uation results, using data from the STS 2012 and
STS 2013 shared tasks, that indicate that the system
performs competitively relative to other approaches
for many tasks. In particular, we observed very
good performance on the news headline similarity
and MT evaluation subtasks of the STS 2013 shared
task.
Acknowledgments
We would like to thank the STS 2013 task organizers
for facilitating this research and Dan Blanchard for
helping with scikit-learn.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing semantic textual simi-
larity by combining multiple content similarity mea-
sures. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics ? Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
2012), pages 435?440, Montre?al, Canada, 7-8 June.
Association for Computational Linguistics.
Beata Beigman Klebanov, Jill Burstein, Nitin Madnani,
Adam Faulkner, and Joel Tetreault. 2012. Build-
ing sentiment lexicon(s) from scratch for essay data.
In Proceedings of the 13th International Conference
on Intelligent Text Processing and Computational Lin-
guistics (CICLing), New Delhi, India, March.
S. Bird, E. Klein, and E. Loper. 2009. Natural Language
Processing with Python. O?Reilly Media.
David Chen and William Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 190?200, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Hal Daume III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Association
for Computational Linguistics.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of Coling 2004, pages 350?356, Geneva,
Switzerland, Aug 23?Aug 27. COLING.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. Bradford Books.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to Framenet.
International Journal of Lexicography, 16(3):235?
250.
Michael Heilman and Nitin Madnani. 2012. ETS: Dis-
criminative edit models for paraphrase scoring. In
*SEM 2012: The First Joint Conference on Lexi-
cal and Computational Semantics ? Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 529?535, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
C. Leacock and M. Chodorow. 2003. c-rater: Scoring of
short-answer questions. Computers and the Humani-
ties, 37.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008. Classification errors in a domain-independent
assessment system. In Proceedings of the Third Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 10?18, Columbus, Ohio,
June. Association for Computational Linguistics.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825?
2830.
S. S. Pradhan and N. Xue. 2009. OntoNotes: The 90%
solution. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
101
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Tutorial Ab-
stracts, pages 11?12.
M. A. Przybocki, K. Peterson, S. Bronsart, and G. A.
Sanders. 2009. The NIST 2008 metrics for machine
translation challenge - overview, methodology, met-
rics, and results. Machine Translation, 23(2-3):71?
103.
Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. TER-Plus: Paraphrase,
semantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117?127,
September.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic?. 2012. TakeLab: Systems
for measuring semantic text similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 22?32,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241?259.
102
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 275?279, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
ETS: Domain Adaptation and Stacking for Short Answer Scoring?
Michael Heilman and Nitin Madnani
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541, USA
{mheilman,nmadnani}@ets.org
Abstract
Automatic scoring of short text responses to
educational assessment items is a challeng-
ing task, particularly because large amounts
of labeled data (i.e., human-scored responses)
may or may not be available due to the va-
riety of possible questions and topics. As
such, it seems desirable to integrate various
approaches, making use of model answers
from experts (e.g., to give higher scores to
responses that are similar), prescored student
responses (e.g., to learn direct associations
between particular phrases and scores), etc.
Here, we describe a system that uses stack-
ing (Wolpert, 1992) and domain adaptation
(Daume III, 2007) to achieve this aim, allow-
ing us to integrate item-specific n-gram fea-
tures and more general text similarity mea-
sures (Heilman and Madnani, 2012). We re-
port encouraging results from the Joint Stu-
dent Response Analysis and 8th Recognizing
Textual Entailment Challenge.
1 Introduction
In this paper, we address the problem of automati-
cally scoring short text responses to educational as-
sessment items for measuring content knowledge.
Many approaches can be and have been taken to
this problem?e.g., Leacock and Chodorow (2003),
Nielsen et al (2008), inter alia. The effectiveness
of any particular approach likely depends on the the
availability of data (among other factors). For exam-
ple, if thousands of prescored responses are avail-
?System description papers for SemEval 2013 are required
to have a team ID (e.g., ?ETS?) as a prefix.
able, then a simple classifier using n-gram features
may suffice. However, if only model answers (i.e.,
reference answers) or rubrics are available, more
general semantic similarity measures (or even rule-
based approaches) would be more effective.
It seems likely that, in many cases, there will
be model answers as well as a modest number of
prescored responses available, as was the case for
the Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment Challenge (?2). There-
fore, we desire to incorporate both task-specific fea-
tures, such as n-grams, as well as more general fea-
tures such as the semantic similarity of the response
to model answers.
We also observe that some features may them-
selves require machine learning or tuning on data
from the domain, in addition to any machine learn-
ing required for the overall system.
In this paper, we describe a machine learning ap-
proach to short answer scoring that allows us to in-
corporate both item-specific and general features by
using the domain adaptation technique of Daume III
(2007). In addition, the approach employs stacking
(Wolpert, 1992) to support the integration of com-
ponents that require tuning or machine learning.
2 Task Overview
In this section, we describe the task to which we ap-
plied our system: the Joint Student Response Anal-
ysis and 8th Recognizing Textual Entailment Chal-
lenge (Dzikovska et al, 2013), which was task 7 at
SemEval 2013.
The aim of the task is to classify student responses
to assessment items from two datasets represent-
275
ing different science domains: the Beetle dataset,
which pertains to basic electricity and electronics
(Dzikovska et al, 2010), and the Science Entail-
ments corpus (SciEntsBank) (Nielsen et al, 2008),
which covers a wider range of scientific topics.
Responses were organized into five categories:
correct, partially correct, contradictory, irrelevant,
and non-domain. The SciEntsBank responses were
converted to this format as described by Dzikovska
et al (2012).
The Beetle training data had about 4,000 student
answers to 47 questions. The SciEntsBank training
data had about 5,000 prescored student answers to
135 questions from 12 domains (different learning
modules). For each item, one or more model re-
sponses were provided by the task organizers.
There were three different evaluation scenarios:
?unseen answers?, for scoring new answers to items
represented in the training data; ?unseen questions?,
for scoring answers to new items from domains rep-
resented in the training data; and ?unseen domains?,
for scoring answers to items from new domains
(only for SciEntsBank since Beetle focused on a sin-
gle domain).
Performance was evaluated using accuracy,
macro-average F1 scores, and weighted average F1
scores.
For additional details, see the task description pa-
per (Dzikovska et al, 2013).
3 System Details
In this section, we describe the short answer scoring
system we developed, and the variations of it that
comprise our submissions to task 7. We begin by
describing our statistical modeling approach. There-
after, we describe the features used by the model
(?3.1), including the PERP feature that relies on
stacking (Wolpert, 1992), and then the domain adap-
tation technique we used (?3.2).
Our system is a logistic regression model with
`2 regularization. It uses the implementation of lo-
gistic regression from the scikit-learn toolkit (Pe-
dregosa et al, 2011).1 To tune the C hyperparame-
ter, it uses a 5-fold cross-validation grid search (with
1The scikit-learn toolkit uses a one-versus-all scheme, us-
ing multiple binary logistic regression classifiers, rather than a
single multiclass logistic regression classifier.
C ? 10{?3,?2,...,3}).
During development, we evaluated performance
using 10-fold cross-validation, with the 5-fold cross-
validation grid search still used for tuning within
each training partition (i.e., each set of 9 folds used
for training during cross-validation).
3.1 Features
Our full system includes the following features.
3.1.1 Baseline Features
It includes all of the baseline features generated
with the code provided by the task organizers.2
There are four types of lexically-driven text similar-
ity measures, and each is computed by comparing
the learner response to both the expected answer(s)
and the question, resulting in eight features in total.
They are described more fully by Dzikovska et al
(2012).
3.1.2 Intercept Feature
The system includes an intercept feature that is al-
ways equal to one, which, in combination with the
domain adaptation technique described in ?3.2, al-
lows the system to model the a priori distribution
over classes for each domain and item. Having these
explicit intercept features effectively saves the learn-
ing algorithm from having to use other features to
encode the distribution over classes.
3.1.3 Word and Character n-gram Features
The system includes binary indicator features for
the following types of n-grams:
? lowercased word n-grams in the response text
for n ? {1, 2, 3}.
? lowercased word n-grams in the response text
for n ? {4, 5, . . . , 11}, grouped into 10,000
bins by hashing and using a modulo operation
(i.e., the ?hashing trick?) (Weinberger et al,
2009).
? lowercased character n-grams in the response
text for n ? {5, 6, 7, 8}
2At the time of writing, the baseline code could
be downloaded at http://www.cs.york.ac.uk/
semeval-2013/task7/.
276
3.1.4 Text Similarity Features
The system includes the following text similarity
features that compare the student response either to
a) the reference answers for the appropriate item, or
b) the student answers in the training set that are la-
beled ?correct?.
? the maximum of the smoothed, uncased BLEU
(Papineni et al, 2002) scores obtained by com-
paring the student response to each correct
reference answer. We also include the word
n-gram precision and recall values for n ?
{1, 2, 3, 4} for the maximally similar reference
answer.
? the maximum of the smoothed, uncased BLEU
scores obtained by comparing the student re-
sponse to each correct training set student an-
swer. We also include the word n-gram preci-
sion and recall values for n ? {1, 2, 3, 4} for
the maximally similar student answer.
? the maximum PERP (Heilman and Madnani,
2012) score obtained by comparing the student
response to the correct reference answers.
? the maximum PERP score obtained by compar-
ing the student response to the correct student
answers.
PERP is an edit-based approach to text similar-
ity. It computes the similarity of sentence pairs by
finding sequences of edit operations (e.g., insertions,
deletions, substitutions, and shifts) that convert one
sentence in a pair to the other. Then, using various
features of the edits and weights for those features
learned from labeled sentence pairs, it assigns a sim-
ilarity score. Heilman and Madnani (2012) provide
a detailed description of the original PERP system.
In addition, Heilman and Madnani (To Appear) de-
scribe some minor modifications to PERP used in
this work.
To estimate weights for PERP?s edit features, we
need labeled sentence pairs. First, we describe how
these labeled sentence pairs are generated from the
task data, and then we describe the stacking ap-
proach used to avoid training PERP on the same data
it will compute features for.
For the reference answer PERP feature, we use
the Cartesian product of the set of correct reference
answers (?good? or ?best? for Beetle) and the set
of student answers, using 1 as the similarity score
(i.e., the label for training PERP) for pairs where the
student answer is labeled ?correct? and 0 for all oth-
ers. For the student answer PERP feature, we use
the Cartesian product of the set of correct student
answers and the set of all student answers, using 1
as the similarity score for pairs where both student
answers are labeled ?correct? and 0 for all others.3
We use 10 iterations for training PERP.
In order to avoid training PERP on the same re-
sponses it will compute features for, we use 10-fold
stacking (Wolpert, 1992). In this process, the train-
ing data are split up into ten folds. To compute the
PERP features for the instances in each fold, PERP
is trained on the other nine folds. After all 10 itera-
tions, there are PERP features for every example in
the training set. This process is similar to 10-fold
cross-validation.
3.2 Domain Adaptation
The system uses the domain adaptation technique
from Daume III (2007) to support generalization
across items and domains.
Instead of having a single weight for each feature,
following Daume III (2007), the system has multiple
copies with potentially different weights: a generic
copy, a domain-specific copy, and an item-specific
copy. For an answer to an unseen item (i.e., ques-
tion) from a new domain in the test set, only the
generic feature will be active. In contrast, for an an-
swer to an item represented in the training data, the
generic, domain-specific, and item-specific copies
of the feature would be active and contribute to the
score.
For our submissions, this feature copying ap-
proach was not used for the baseline features
(?3.1.1) or the BLEU and PERP text similarity fea-
tures (?3.1.4), which are less item-specific. Those
features had only general copies. We did not test
whether doing so would affect performance.
3The Cartesian product of the sets of correct student answers
and of all student answers will contain some pairs of identi-
cal correct answers. We decided to simply include these when
training PERP, since we felt it would be desirable for PERP to
learn that identical sentences should be considered similar.
277
Beetle SciEntsBank
Submission A Q A Q D
Run 1 .5520 .5470 .5350 .4870 .4470
Run 2 .7050 .6140 .6250 .3560 .4340
Run 3 .7000 .5860 .6400 .4110 .4140
maximum .7050 .6140 .6400 .4920 .4710
mean .5143 .3978 .4568 .3769 .3736
Table 1: Weighted average F1 scores for 5-way classification for our SemEval 2013 task 7 submissions, along with
the maximum and mean performance, for comparison. ?A? = unseen answers, ?Q? = unseen questions, ?D? = unseen
domains (see ?2 for details). Results that were the maximum score among submissions for part of the task are in bold.
3.3 Submissions
We submitted three variations of the system. For
each variation, a separate model was trained for Bee-
tle and for SciEntsBank.
? Run 1: This run included the baseline (?3.1.1),
intercept (?3.1.2), and the text-similarity fea-
tures (?3.1.4) that compare student responses to
reference answers (but not those that compare
to scored student responses in the training set).
? Run 2: This run included the baseline (?3.1.1),
intercept (?3.1.2), and n-gram features (?3.1.3).
? Run 3: This run included all features.
4 Results
Table 1 presents the weighted averages of F1 scores
across the five categories for the 5-way subtask, for
each dataset and scenario. The maximum and mean
scores of all the submissions are included for com-
parison. These results were provided to us by the
task organizers.
For conciseness, we do not include accuracy or
macro-average F1 scores here. We observed that, in
general, the results from different evaluation metrics
were very similar to each other. We refer the reader
to the task description paper (Dzikovska et al, 2013)
for a full report of the task results.
Interestingly, the differences in performance be-
tween the unseen answers task and the other tasks
was somewhat larger for the SciEntsBank dataset
than for the Beetle dataset. We speculate that this re-
sult is because the SciEntsBank data covered a more
diverse set of topics.
Note that Runs 1 and 2 use subsets of the features
from the full system (Run 3). While Runs 1 and 2
are not directly comparable to each other, Runs 1
and 3 can be compared to measure the effect of the
features based on other previously scored student re-
sponses (i.e., n-grams, and the PERP and BLEU fea-
tures based on student responses). Similarly, Runs 2
and 3 can be compared to measure the combined ef-
fect of all BLEU and PERP features.
It appears that features of the other student re-
sponses improve performance for the unseen an-
swers task. For example, the full system (Run 3)
performed better than Run 1, which did not include
features of other student responses, on the unseen
answers task for both Beetle and SciEntsBank.
However, it is not clear whether the PERP and
BLEU features improve performance. The full sys-
tem (Run 3) did not always outperform Run 2, which
did not include these features.
We leave to future work various additional ques-
tions, such as whether student response features or
reference answer similarity features are more use-
ful in general, and whether there are any systematic
differences between human-machine and human-
human disagreements.
5 Conclusion
We have presented an approach for short answer
scoring that uses stacking (Wolpert, 1992) and do-
main adaptation (Daume III, 2007) to support the
integration of various types of task-specific and gen-
eral features. Evaluation results from task 7 at Se-
mEval 2013 indicate that the system achieves rela-
tively high levels of agreement with human scores,
as compared to other systems submitted to the
shared task.
278
Acknowledgments
We would like to thank the task organizers for facil-
itating this research and Dan Blanchard for helping
with scikit-learn.
References
Hal Daume III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Association
for Computational Linguistics.
Myroslava O. Dzikovska, Diana Bental, Johanna D.
Moore, Natalie Steinhauser, Gwendolyn Campbell,
Elaine Farrow, and Charles B. Callaway. 2010. In-
telligent tutoring with natural language support in the
BEETLE II system. In Proceedings of Fifth European
Conference on Technology Enhanced Learning (EC-
TEL 2010).
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback
for explanation questions: A dataset and baselines.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 200?210, Montre?al, Canada, June. Association
for Computational Linguistics.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student re-
sponse analysis and 8th recognizing textual entailment
challenge. In *SEM 2013: The First Joint Conference
on Lexical and Computational Semantics, Atlanta,
Georgia, USA, 13-14 June. Association for Compu-
tational Linguistics.
Michael Heilman and Nitin Madnani. 2012. ETS: Dis-
criminative edit models for paraphrase scoring. In
*SEM 2012: The First Joint Conference on Lexi-
cal and Computational Semantics ? Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 529?535, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
Michael Heilman and Nitin Madnani. To Appear. Henry:
Domain adapation and stacking for text similarity. In
*SEM 2013: The Second Joint Conference on Lexical
and Computational Semantics. Association for Com-
putational Linguistics.
C. Leacock and M. Chodorow. 2003. c-rater: Scoring of
short-answer questions. Computers and the Humani-
ties, 37.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008. Classification errors in a domain-independent
assessment system. In Proceedings of the Third Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 10?18, Columbus, Ohio,
June. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825?
2830.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature hash-
ing for large scale multitask learning. In Proceedings
of the 26th Annual International Conference on Ma-
chine Learning, ICML ?09, pages 1113?1120, New
York, NY, USA. ACM.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241?259.
279
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 35?40,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Rating Computer-Generated Questions with Mechanical Turk
Michael Heilman
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
mheilman@cs.cmu.edu
Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We use Amazon Mechanical Turk to rate
computer-generated reading comprehension
questions about Wikipedia articles. Such
application-specific ratings can be used to
train statistical rankers to improve systems?
final output, or to evaluate technologies that
generate natural language. We discuss the
question rating scheme we developed, assess
the quality of the ratings that we gathered
through Amazon Mechanical Turk, and show
evidence that these ratings can be used to im-
prove question generation.
1 Introduction
This paper discusses the use of Amazon Mechani-
cal Turk (MTurk) to rate computer-generated read-
ing comprehension questions about Wikipedia arti-
cles.
We have developed a question generation sys-
tem (Heilman and Smith, 2009; Heilman and
Smith, 2010) that uses the overgenerate-and-rank
paradigm (Langkilde and Knight, 1998). In the
the overgenerate-and-rank approach, many system-
generated outputs are ranked in order to select higher
quality outputs. While the approach has had con-
siderable success in natural language generation
(Langkilde and Knight, 1998; Walker et al, 2001),
it often requires human labels on system output for
the purpose of learning to rank. We employ MTurk
to reduce the time and cost of acquiring these labels.
For many problems, large labeled datasets do not
exist. One alternative is to build rule-based sys-
tems, but it is often difficult and time-consuming
to accurately encode relevant linguistic knowledge
in rules. Another alternative, unsupervised or semi-
supervised learning, usually requires clever formu-
lations of bias that guide the learning process (Car-
roll and Charniak, 1992; Yarowsky, 1995); such
intuitions are not always available. Thus, small,
application-specific labeled datasets, which can be
cheaply constructed using MTurk, may provide con-
siderable benefits by enabling the use of supervised
learning.
In addition to using MTurk ratings to train a
learned ranking component, we could also use
MTurk ratings to evaluate the final top-ranked out-
put of our system. More generally, MTurk can be a
useful evaluation tool for systems that output natu-
ral language (e.g., systems for natural language gen-
eration, summarization, translation). For example,
Callison-Burch (2009) used MTurk to evaluate ma-
chine translations. MTurk facilitates the efficient
measurement and understanding of errors made by
such technologies, and could be used to complement
automatic evaluation metrics such as BLEU (Pap-
ineni et al, 2002) and ROUGE (Lin, 2004).
It is true that, for our task, MTurk workers
annotate computer-generated rather than human-
generated natural language. Thus, the data will
not be as generally useful as other types of anno-
tations, such as parse trees, which could be used to
build general purpose syntactic parsers. However,
for the reasons described above, we believe the use
of MTurk to rate computer-generated output can be
useful for the training, development, and evaluation
of language technologies.
The remainder of the paper is organized as fol-
lows: ?2 and ?3 briefly describe the question gener-
ation system and corpora used in our experiments.
?4 provides the details of our rating scheme. ?5 dis-
cusses the quantity, cost, speed, and quality of the
ratings we gathered. ?6 presents preliminary experi-
ments showing that theMTurk ratings improve ques-
tion ranking. Finally, in ?7, we conclude.
35
2 Question Generation System
We use MTurk to improve and evaluate a system
for automatic question generation (QG). In our QG
approach, hand-crafted rules transform declarative
sentences from an input text into a large set of ques-
tions (i.e., hundreds per page). This rule system is
complemented by a statistical ranker, which ranks
questions according to their quality. Currently, we
focus on basic linguistic issues and the goal of pro-
ducing acceptable questions?that is, questions that
are grammatical, make sense, and are not vague. We
believe an educator could select and revise output
from the system in order to produce a final set of
high-quality, challenging questions.
Our system is described by Heilman and Smith
(2010). In that work, we employed a differ-
ent scheme involving binary judgments of question
quality according to various factors such as gram-
maticality, vagueness, and others. We also employed
university students as novice annotators. For the
training dataset, only one human rated each ques-
tion. See Heilman and Smith (2009) for more de-
tails.1
3 Corpora
In our experiments, we generated questions from
60 articles sampled from the ?featured? articles in
the English Wikipedia2 that have between 250 and
2,000 word tokens. This collection provides expos-
itory texts written at an adult reading level from a
variety of domains, which roughly approximates the
prose that a secondary or post-secondary level stu-
dent would encounter. By choosing from the fea-
tured articles, we intended to select well-edited ar-
ticles about topics of general interest. We then ran-
domly selected 20 questions from each of 60 articles
for labeling with MTurk.3
1We also generated some questions using a technique that
replaces pronouns and underspecified noun phrases with an-
tecedent mentions identified by a coreference resolver. We will
not provide details about this component here because they are
not relevant to our use of MTurk to rate questions. A forthcom-
ing paper will describe these additions.
2The English Wikipedia data were downloaded on Decem-
ber 16, 2008 from http://en.wikipedia.org
3Five questions were later eliminated from this set due to
minor implementation changes, the details of which are unin-
teresting. The final set contained 1,195 questions.
Rating Details
1 Bad The question has major prob-
lems.
2 Unacceptable The question definitely has a
minor problem.
3 Borderline The question might have a
problem, but I?m not sure.
4 Acceptable The question does not have
problems.
5 Good The question is as good as one
that a human teacher might
write for a reading quiz.
Table 1: The five-point question rating scale.
4 Rating Scheme
This section describes the rating scheme we de-
veloped for evaluating the quality of computer-
generated questions on MTurk.
Questions were presented independently as sin-
gle human intelligence tasks (HITs). At the top of
the page, raters were given the instructions shown
in Figure 1 along with 7 examples of good and bad
questions with their appropriate ratings. Below the
instructions and examples was an excerpt from the
source text consisting of up to 5 sentences of con-
text, ending with the primary sentence that the ques-
tion was generated from. The question to be rated
then followed.
Below each question was the five-point rating
scale shown in Table 1. Workers were required to
select a single rating by clicking a radio button. At
the bottom of the page, the entire source article text
was given, in case the worker felt it was necessary
to refer back to more context.
We paid 5 cents per rating,4 and each question was
rated by five workers. With the 10% commission
charge by Amazon, each question cost 27.5 cents.
The final rating value was computed by taking
the arithmetic mean of the ratings. Table 2 provides
some examples of questions and their mean ratings.
4.1 Monitoring Turker Ratings
During some pilot tests, we found that it was par-
ticularly important to set some qualification criteria
for workers. Specifically, we only allowed workers
4Given the average time spent per HIT, the pay rate can be
extrapolated to $5?10 per hour.
36
Figure 1: A screenshot of the instructions given to workers.
who had completed at least 50 previously accepted
HITs. We also required that at least 95% of workers?
previous submissions had been accepted.
We also submitted HITs in batches of 100 to 500
so that we could more closely monitor the process.
In addition, we performed a limited amount of
semi-automated monitoring of the ratings, and re-
jected work from workers who were clearly ran-
domly clicking on answers or not following the rat-
ing scheme properly. We tried to err on the side of
accepting bad work. After all ratings for a batch
of questions were received, we calculated for each
worker the number of ratings submitted, the aver-
age time spent on each question, the average rating,
and the correlation of the worker?s rating with the
mean of the other 4 ratings. We used a combination
of these statistics to identify extremely bad workers
(e.g., ones who had negative correlations with other
workers and spent less than 10 seconds per ques-
tion). If some of the ratings for a question were
rejected, then the HIT was ?extended? in order to
receive 5 ratings.
5 Quantity, Cost, Speed, and Quality
This section discusses the quantity and quality of the
question ratings we received from MTurk.
5.1 Quantity and Cost of Ratings
We received 5 ratings each for 1,200 questions, cost-
ing a total of $330. 178 workers participated. Work-
ers submitted 33.9 ratings on average (s.d. = 58.0).
The distribution of ratings per worker was highly
skewed, such that a handful of workers submitted
100 or more ratings (max = 395). The ratings from
these who submitted more than 100 ratings seemed
to be slightly lower in quality but still acceptable.
The median number of ratings per worker was 11.
5.2 Speed of Ratings
Ratings were received very quickly once the HITs
were submitted. Figure 2 shows the cumulative
number of ratings received for a batch of questions,
37
Source Text Excerpt Question Rating
MD 36 serves as the main road through the Georges Creek
Valley, a region which is historically known for coal mining,
and has been designated by MDSHA as part of the Coal Her-
itage Scenic Byway.
Which part has MD 36 been desig-
nated by MDSHA as?
1.4
He worked further on the story with the Soviet author Isaac
Babel, but no material was ever published or released from
their collaboration, and the production of Bezhin Meadow
came to an end.
What did the production of Bezhin
Meadow come to?
2.0
The design was lethal, successful and much imitated, and
remains one of the definitive weapons of World War II.
Does the design remain one of the
definitive weapons of World War II?
2.8
Francium was discovered by Marguerite Perey in France
(from which the element takes its name) in 1939.
Where was Francium discovered by
Marguerite Perey in 1939?
3.8
Lazare Ponticelli was the longest-surviving officially recog-
nized veteran. . . Although he attempted to remain with his
French regiment, he eventually enlisted in. . .
Did Lazare Ponticelli attempt to re-
main with his French regiment?
4.4
Table 2: Example computer-generated questions, along with their mean ratings from Mechanical Turk.
1000150020002500
Cumulative # Ratings
05001000150020002500
010
203
040
506
070
Minut
es Elap
sed
05001000150020002500
010
203
040
506
070
Minut
es Elap
sed
Figure 2: The cumulative number of ratings submitted by
MTurk workers over time, for a batch of 497 questions
posted simultaneously (there are 5 ratings per question).
indicating that more than 1,000 ratings were re-
ceived per hour.
5.3 Quality of Ratings
We evaluated inter-rater agreement by having the
first author and an independent judge rate a random
sample of 40 questions from 4 articles. The indepen-
dent judge was a computational linguist. The Pear-
son correlation coefficient between the first author?s
ratings and the mean ratings from MTurk work-
ers was r = 0.79, which is fairly strong though
not ideal. The correlation between the independent
judge?s ratings and the MTurk workers was r =
0.74. These fairly strong positive correlations be-
tween the MTurk ratings and the two human judges
provide evidence that the rating scheme is consis-
tent and well-defined. The results also agree with
Snow et al (2008), who found that aggregating la-
bels from 3 to 7 workers often provides expert lev-
els of agreement. Interestingly, the agreement be-
tween the two human raters was somewhat lower
(r = 0.65), suggesting that aggregated labels from a
crowd of MTurk workers can be more reliable than
individual humans.5
6 Using Labeled Data to Improve Question
Ranking
In this section, we provide some preliminary results
to demonstrate that MTurk ratings can be used for
learning to rank QG output.
First, we briefly characterize the quality of un-
ranked output. Figure 3 shows a histogram of the
mean MTurk ratings for the 1,195 questions, show-
ing that only a relatively small fraction of the ques-
tions created by the overgenerating steps of our sys-
tem are acceptable: 12.9% when using 3.5 as the
threshold for acceptability.
However, ranking can lead to substantially higher
levels of quality in the top-ranked questions, which
5We also converted the ratings into binary values based on
whether they exceeded a threshold of 3.5. After this conversion
to a nominal scale, we computed a Cohen?s ? of 0.54, which
indicates ?moderate? agreement (Landis and Koch, 1977).
38
5%10%15%20%25%
Percent of Questions
0%5%10%15%20%25%
Mean
 
Ratin
g Ran
ge
Figure 3: The distribution of the 1,195 question ratings.
might be presented first in a user interface. There-
fore, we investigated how many MTurk-rated ques-
tions are needed to train an effective statistical ques-
tion ranker. Our ranking model is essentially the
same as the one used by Heilman and Smith (2010).
Rather than logistic regression, which we used pre-
viously, here we use a linear regression with `2 reg-
ularization to account for the ordinal scale of the av-
eraged question ratings. We set the regularization
parameter through cross-validation with the training
data.
The regression includes all of the features de-
scribed by Heilman and Smith (2010). It includes
features for sentence lengths, whether the question
includes various WH words, whether certain syntac-
tic transformations performed during QG, whether
negation words are present in questions, how many
times various parts of speech appeared, and others.
It also includes some additional coreference features
for parts of speech and lengths of noun phrase men-
tions and their antecedents.6 In all, the ranker in-
cludes 326 features.
For our experiments, we set aside a randomly cho-
sen 200 of the 1,195 rated questions as a test set.
We then trained statistical rankers on randomly sam-
pled subsets of the remaining questions, from size
N = 50 up to N = 995. For each value of N ,
we used the ranker trained on that amount of data
to rank the 200 test questions. We then computed
6Since these additional coreference features are not immedi-
ately relevant to this work, we will not describe them fully here.
A forthcoming paper will describe them in more detail.
0.40.50.6
Acceptability of 
Ranked Fifth
0.20.30.40.50.6 0
250
500
750
1000
Top-
Traini
ng Set 
Size
0.20.30.40.50.6 0
250
500
750
1000
Traini
ng Set 
Size
Figure 4: A graph of the acceptability of top-ranked ques-
tions when datasets of increasing size are used to train a
statistical question ranker. Error bars show 95% confi-
dence intervals computed from the 10 runs of the sam-
pling process.
the percentage of the top fifth of the ranked test set
questions with a mean rating above 3.5. For each
N less than 995, we repeated the entire sampling,
training, and ranking process 10 times and averaged
the results. (We used the same 200 question test set
throughout the process.)
Figure 4 presents the results, with the acceptabil-
ity of unranked questions (23%) included at N = 0
for comparison. We see that ranking more than dou-
bles the acceptability of the top-ranked questions,
consistent with findings from Heilman and Smith
(2010). It appears that ranking performance im-
proves as more training data are used. When 650 ex-
amples were used, 49% of the top-ranked questions
were acceptable. Ranking performance appears to
level off somewhat when more than 650 training ex-
amples are used. However, we speculate that if the
model included more fine-grained features, the value
of additional labeled data might increase.7
7 Conclusion
In this paper, we used MTurk to gather quality rat-
ings for computer-generated questions. We pre-
7To directly compare the ranker?s predictions to the correla-
tions presented in ?5.3, we computed a correlation coefficient
between the test set ratings from MTurk and the ratings pre-
dicted by the ranker when it was trained on all 995 training ex-
amples. The coefficient was r = 0.36, which is statistically sig-
nificant (p < .001) but suggests that there is substantial room
for improvement in the ranking model.
39
sented a question rating scheme, and found high lev-
els of inter-rater agreement (r ? 0.74) between rat-
ings from reliable humans and ratings from MTurk.
We also showed that ratings can be gathered from
MTurk quickly (more than 1,000 per hour) and
cheaply (less than 30 cents per question).
While ratings of computer-generated language are
not as generally useful as, for example, annotations
of the syntactic structure of human-generated lan-
guage, many research paradigms involving the auto-
matic generation of language may be able to benefit
from using MTurk to quickly and cheaply evaluate
ongoing work. Also, we demonstrated that such rat-
ings can be used in an overgenerate-and-rank strat-
egy to greatly improve the quality of a system?s top-
ranked output.
References
C. Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proc. of EMNLP.
G. Carroll and E. Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from cor-
pora. Technical report, Brown University.
M. Heilman and N. A. Smith. 2009. Question gener-
ation via overgenerating transformations and ranking.
Technical Report CMU-LTI-09-013, Language Tech-
nologies Institute, Carnegie Mellon University.
M. Heilman and N. A. Smith. 2010. Good question!
statistical ranking for question generation. In Proc. of
NAACL-HLT.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33.
I. Langkilde and Kevin Knight. 1998. Generation that
exploits corpus-based statistical knowledge. In Proc.
of ACL.
C. Lin. 2004. ROUGE: a package for automatic eval-
uation of summaries. In Proc. of Workshop on Text
Summarization.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
R. Snow, B. O?Connor, D. Jurafsky, and A. Ng. 2008.
Cheap and fast ? but is it good? evaluating non-expert
annotations for natural language tasks. In Proc. of
EMNLP.
M. A. Walker, O. Rambow, and M. Rogati. 2001. Spot:
a trainable sentence planner. In Proc. of NAACL.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. of
ACL.
40
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 233?241,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Precision Isn?t Everything:
A Hybrid Approach to Grammatical Error Detection
Michael Heilman and Aoife Cahill and Joel Tetreault
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541, USA
{mheilman,acahill,jtetreault}@ets.org
Abstract
Some grammatical error detection methods,
including the ones currently used by the Edu-
cational Testing Service?s e-rater system (At-
tali and Burstein, 2006), are tuned for pre-
cision because of the perceived high cost
of false positives (i.e., marking fluent En-
glish as ungrammatical). Precision, however,
is not optimal for all tasks, particularly the
HOO 2012 Shared Task on grammatical er-
rors, which uses F-score for evaluation. In this
paper, we extend e-rater?s preposition and de-
terminer error detection modules with a large-
scale n-gram method (Bergsma et al, 2009)
that complements the existing rule-based and
classifier-based methods. On the HOO 2012
Shared Task, the hybrid method performed
better than its component methods in terms of
F-score, and it was competitive with submis-
sions from other HOO 2012 participants.
1 Introduction
The detection of grammatical errors is a challenging
problem that, arguably, requires the use of both lin-
guistic knowledge (e.g., in the form of rules or com-
plex features) and large corpora for statistical learn-
ing. Additionally, grammatical error detection can
be applied in various scenarios (e.g., automated es-
say scoring, writing assistance, language learning),
many of which may benefit from task-specific adap-
tation or tuning. For example, one might want to
take a different approach when detecting errors for
the purpose of providing feedback than when de-
tecting errors to evaluate the quality of writing in
an essay. Thus, it seems desirable to take a flexible
approach to grammatical error detection that incor-
porates multiple, complementary techniques.
In this paper, we extend the preposition and de-
terminer error detection modules currently used in
the Educational Testing Service?s e-rater automated
essay scoring system (Attali and Burstein, 2006) for
the HOO 2012 Shared Task on grammatical errors
(?2). We refer to this set of modules from e-rater as
our ?base system? (?3). While the base system uses
statistical methods to learn models of grammatical
English, it also leverages substantial amounts of lin-
guistic knowledge in the form of various hand-coded
filters and complex syntactic features. The base sys-
tem is also tuned for high precision at the expense
of recall in order to avoid a high rate of potentially
costly false positives (i.e., frequent marking of cor-
rect English sentences as ungrammatical).
We apply the pre-existing base system without
modifications but complement it with a large-scale
n-gram method (?5) based on work by Bergsma et
al. (2009). The n-gram method employs very little
linguistic knowledge and instead relies almost ex-
clusively upon corpus statistics. We also tune the
resulting hybrid system with labeled training data
in order to maximize the primary evaluation met-
ric used in the HOO 2012 Shared Task: balanced
F-score, or F1 (?6). We find that the tuned hybrid
system improves upon the recall and F-score of the
base system. Also, in the HOO 2012 Shared Task,
the hybrid system achieved results that were com-
petitive with other submitted grammatical error de-
tection systems (?7).
233
2 Task Definition
In this section, we provide a brief overview of the
HOO 2012 Shared Task (Dale et al, 2012). The
task focuses on prepositions and determiners only,
distinguishing the following error types: preposition
selection errors (coded ?RT? in the data), extraneous
prepositions (?UT?), missing prepositions (?MT?),
determiner selection errors (?RD?), extraneous de-
terminers (?UD?), and missing determiners (?MD?).
For training and testing data, the shared task uses
short essays from an examination for speakers of En-
glish as a foreign language. The data includes gold
standard human annotations identifying preposition
and determiner errors. These errors are represented
as edits that transform an ungrammatical text into
a grammatical one. Edits consist of start and end
offsets into the original text and a correction string
that should replace the original text at the speci-
fied offsets. The offsets differ by error type: word
selection errors include just the word, extraneous
word errors include an extra space after the word so
that a blank will result in an appropriate amount of
whitespace, and missing word errors specify spans
of length zero.1
There are three subtasks: detection, recognition,
and correction. Each is evaluated according to pre-
cision, recall, and F-score according to a set of
gold standard edits produced by human annotation.
While the correction subtask requires both correct
character offsets and appropriate corrections, the de-
tection and recognition subtasks only consider the
offsets. Detection and recognition are essentially the
same, except that detection allows for loose match-
ing of offsets, which permits mismatches between
the extraneous use (e.g., UT) and word selection
(e.g., RT) error types. For our submission to the
shared task, we chose to tune for the detection sub-
task, and we also chose to avoid the correction task
entirely since the interface to the pre-existing base
system did not give us access to possible corrections.
1The offsets for extraneous word errors prior to punctuation,
a relatively rare occurrence, include a space before the word
rather than after it. Our script for converting our system?s output
into the HOO 2012 format did not account for this, which may
have decreased recognition performance slightly.
3 Base System
As our base system, we repurpose a complex sys-
tem designed to automatically score student essays
(both native and non-native and across a wide range
of competency levels). The system is also used to
give feedback to essay writers, so precision is fa-
vored over recall. There are three main modules in
the essay-scoring system whose purpose it is to de-
tect preposition and determiner errors (as they are
defined in that system). Many of the details have
been reported previously (Chodorow and Leacock,
2000; Han et al, 2004; Han et al, 2006; Chodorow
et al, 2007; Tetreault and Chodorow, 2008), so here
we will only give brief summaries of these modules.
It is important to note that this system was run
without modification. That is, no training of new
models or tuning was carried out specifically for the
shared task. In addition, for the two statistical mod-
ules, we only had access to the final, boolean deci-
sions about whether an error is present or not at a
particular location in text. That is, we did not have
access to confidence scores, and so task-specific tun-
ing for F-score was not an option.
3.1 Preposition Error Detection
The base system detects incorrect and extraneous
prepositions (Chodorow et al, 2007; Tetreault and
Chodorow, 2008). Tetreault and Chodorow (2008)
reports approximately 84% precision and 19% re-
call on both error types combined when evaluating
the system on manually annotated non-native text.
3.1.1 Incorrect Prepositions
The module to detect incorrectly used preposi-
tions consists of a multiclass logistic regression (i.e.,
?Maximum Entropy?) model of grammatical usage,
along with heuristic pre- and post- filters. The mod-
ule works by extracting a set of features from the
?context? around a preposition, generating a distri-
bution over possible prepositions using the model of
grammatical usage, and then flagging an error if the
difference in probability between the text?s original
preposition and an alternative preposition exceeds a
certain threshold. The probability for any correction
also needs to exceed another minimum threshold.
For this work, we used the pre-existing, manually-
set thresholds.
234
A pre-filter prevents any contexts that contain
spelling errors from being submitted to the logistic
regression model. The motivation for this is that the
NLP components that provide the features for the
model are unreliable on such data, and since the sys-
tems favors precision over recall, no attempt is made
to correct prepositions where the system cannot rely
on the accuracy of those features.
The logistic regression model of correct preposi-
tion usage is trained on approximately 82 million
words from the San Jose Mercury News2 and texts
for 11th to 12th grade reading levels from the Meta-
Metrics Lexile corpus, resulting in 7 million prepo-
sition contexts. The model uses 25 types of features:
words and part-of-speech tags around the existing
preposition, head verb (or noun) in the preceding
VP (or NP), head noun in the following NP, among
others. NPs and VPs were detected using chunking
rather than full parsing, as the performance of statis-
tical parsers on erroneous text was deemed to be too
poor.
A post-filter rules out certain candidates based on
the following heuristics: (1) if the suggested correc-
tion is an antonym of the original preposition (e.g.,
from vs to), it is discarded; (2) any correction of the
benefactive for is discarded when the head noun of
the following NP is human (detected as a WordNet
hyponym of person or group).
3.1.2 Extraneous Prepositions
Heuristics are applied to detect common occur-
rences of extraneous prepositions in two scenar-
ios: (1) accidentally repeated prepositions (e.g., with
with) and (2) insertion of unnecessary prepositions
in plural quantifier constructions (e.g., some of peo-
ple).
3.2 Determiner Error Detection
There are two separate components that detect er-
rors related to determiners. The first is a filter-based
model that detects determiner errors involving num-
ber and person agreement. The second is a statistical
system that supplements the rule-based system and
detects article errors.
2The San Jose Mercury News is available from the Linguis-
tic Data Consortium (catalog number LDC93T3A).
3.2.1 Filter-based system
The filter-based system combines unsupervised
detection of a set of possible errors (Chodorow and
Leacock, 2000) with hand-crafted filters designed
to reduce this set to the largest subset of correctly
flagged errors and the smallest possible number
of false positives. Chodorow and Leacock (2000)
found that low-frequency bigrams (sequences of two
lexical categories with a negative log-likelihood) are
quite reliable predictors of grammatical errors. Text
is tagged and chunked, and filters that detect likely
cases of NP-internal agreement violations are ap-
plied. These filters will mark, for example, a sin-
gular determiner followed by a plural noun head and
vice versa, or a number disagreement between a nu-
meral and the noun it modifies. This system has
the ability to take advantage of linguistic knowledge,
which contributes to its ability to detect errors with
high precision.
3.2.2 Statistical model
In addition to the hand-crafted filters described
above, there is a statistical component that detects
incorrect, missing and extraneous articles (Han et
al., 2004; Han et al, 2006). This component con-
sists of a multiclass logistic regression that selects
an appropriate article for every NP from a, an, the,
or . This model is trained on 31.5 million words
of diverse genres from the MetaMetrics Lexile cor-
pus (from 10th to 12th grade reading levels), or 8
million NP contexts. Again, NPs were determined
by chunking. The model includes various features:
words and POS tags around and within the NP, NP
head information including the countability of the
head noun (estimated automatically from large cor-
pora), etc.
In a cross-validation experiment, the model
achieved approximately 83% accuracy on well-
edited text. In an experiment evaluated on non-
native learner text, the model achieved approxi-
mately 85% agreement with human annotators.
4 Task-Specific Heuristic Filtering
There is not a one-to-one mapping between the def-
initions of determiner and preposition errors as used
in the HOO data set and the definitions used in our
base system. For example, our base system marks
235
errors involving every, many and other quantifiers as
determiner errors, while these are not marked in the
current HOO 2012 Shared Task data.
To ensure that our system was aligned with the
HOO 2012 Shared Task, we automatically extracted
lists of the most frequently occurring determiners
and prepositions in the HOO training data. Any RT,
UT, RD or UD edit predicted for a word not in those
lists is automatically discarded. In the training data,
this resulted in the removal of 4 of the 463 RT errors
and 98 of the 361 RD errors detected by the base
system.
5 Large-scale n-Gram Models
In order to complement the high-precision base sys-
tem and increase recall, we incorporate a large
scale n-gram model into our full system. Specifi-
cally, we adapt the SUMLM method from Bergsma
et al (2009). SUMLM creates confusion sets for
each preposition token in an input text and uses the
Google Web 1T 5-gram Corpus to score each item
in the confusion set.3 We extend SUMLM to sup-
port determiners, extraneous use errors, and missing
word errors.
Consider the case of preposition selection errors.
For a preposition token at position i in an input sen-
tence w, we compute the following score for each
possible alternative v, using Eq. 1.4
s(w, i, v) =
?
n=2...5
?
x?G(w,i,n,v)
log(count(x))
|G(w, i, n, v)|
(1)
The function G(w, i, n, v) returns the set of n-
grams in w that include the word at position i and
3The Google Web 1T 5-gram Corpus is available from the
Linguistic Data Consortium (catalog number LDC2006T13).
We plan to test other corpora for n-gram counts in future work.
4The n-gram approach considers all of the following words
to be prepositions: to, of, in, for, on, with, at, by, as, from, about,
up, over, into, down, between, off, during, under, through,
around, among, until, without, along, within, outside, toward,
inside, upon, except, onto, towards, besides, beside, and under-
neath. It considers all of the following words to be determiners:
a, an, and the. The sets of possible prepositions and determiners
for the base system are not exactly the same. Part of speech tags
are not used in the n-gram system except to identify insertion
points for missing prepositions and determiners.
replace that word, wi, with v. For example, if w =
Mary and John went at the store to buy milk, n = 4,
i = 4, and v = to, then G(w, i, n, v) returns the
following 4-grams:
? and John went to
? John went to the
? went to the store
? to the store to
The expression log(count(x)) is the natural loga-
rithm of the number of times the n-gram x occurred
in the corpus.5 |G(w, i, n, v)| is the number of n-
gram count lookups, used to normalize the scores.
Note that this normalization factor is not included in
the original SumLM. When v is an alternative prepo-
sition not near the beginning or end of a sentence,
|G(w, i, n, v)| = 14 since there are 14 n-gram count
lookups in the numerator. Or, for example, if i = 0,
indicating that the preposition occurs at the begin-
ning of the sentence, |G(w, i, n, v)| = 4.6
Next, we compute the ratio of the score of each
alternative to the score for the original, using Eq. 2.
r(w, i, v) =
s(w, i, v)
s(w, i, wi)
(2)
We then identify the best scoring alternative, re-
quiring that its score be higher than the original (i.e.,
r(w, i, v) > 1). The procedure is the same for deter-
miners, except, of course, that the set of alternatives
includes determiners rather than prepositions.
To extend the method from Bergsma et al (2009)
for extraneous prepositions and determiners, we
simply set v to be a blank and sum over j = 3 . . . 5
instead. |G(w, i, n, v)|will then be 12 instead of 14,
since bigrams from the original sentence, which be-
come unigrams when replacing wi with a blank, are
excluded.
To identify positions at which to flag selection or
extraneous use errors, we simply scan for words that
match an item in our sets of possible prepositions
and determiners. To extend the method for missing
5We use the TrendStream system (Flor, 2012) to retrieve n-
gram counts efficiently.
6Our n-gram counts do not include start- or end-of-sentence
symbols. Also, all n-grams are case-normalized with numbers
replaced by a special symbol.
236
Algorithm 1 tune(W, y, y? ?, ?min):
The hill-climbing algorithm for optimizing the n-
gram method?s penalty parameters q. W consists
of the training set texts. y? is a set of candidate edits.
y is a set of gold standard edits. ? is an initial step
size, and ?min is a minimum step size.
qallbest ? 0
scoreallbest ? eval(qallbest,W,y, y?)
while ? > ?min do
scorebest ? ??
qbest ? qallbest
for qtmp ? perturb(qbest, ?) do
scoretmp ? eval(qtmp,W,y, y?)
if scoretmp > scorebest then
qbest ? qtmp
scorebest ? scoretmp
end if
end for
if scorebest > scoreallbest then
qallbest ? qbest
scoreallbest ? scorebest
else
?? 0.5 ? ?
end if
end while
return qallbest
word errors, however, we apply a set of heuristics to
identify potential insertion points.7
6 Tuning
The n-gram approach in ?5 generates a large num-
ber of possible edits of different types. In this sec-
tion, we describe how we filter edits using their
scores and how we combine them with edits from
the base system (?3).
As described above, for an alternative v to be con-
sidered as a candidate edit, the value of r(w, i, v) in
Eq. 2 must be greater than a threshold of 1, indicat-
ing that the alternative scores higher than the origi-
nal word. However, we observed low precision dur-
ing development when including all candidate ed-
its and decided to penalize the ratios. Bergsma et
al. (2009) discuss raising the threshold, which has
7The heuristics are based on those used in Gamon (2010)
(personal communication).
a similar effect. Preliminary experiments indicated
that different edits (e.g., extraneous preposition edits
and preposition selection edits) should have differ-
ent penalties, and we also want to avoid edits with
overlapping spans. Thus, for each location with one
or more candidate edits, we select the best according
to Equation 3 and filter out the rest.
v? = argmax
v
r(w, i, v)? penalty(wi, v) (3)
penalty(wi, v) is a function that takes the current
word wi and the alternative v and returns one of 6
values: qRT for preposition selection, qUT for extra-
neous prepositions, qMT for missing prepositions,
qRD for determiner selection, qUD for extraneous
determiners, and qMD for missing determiners.
If the value for r(w, i, v?)?penalty(wi, v?) does
not exceed 1, we exclude it from the output.
We tune the vector q of all the penalties to op-
timize our objective function (F-score, see ?7) on
the training set using the hill-climbing approach de-
scribed in Algorithm 1. The algorithm initializes
the parameter vector to all zeros, and then itera-
tively evaluates candidate parameter vectors that re-
sult from taking positive and negative steps of size
? in each direction (steps with negative penalties
are skipped). The best step is taken if it improves
the current score, according to the eval function,
which returns the training set F-score after filtering
based on the current parameters.8 This process pro-
ceeds until there is no improvement. Then, the step
size ? is halved, and the whole process is repeated.
The algorithm proceeds as such until the step size
becomes lower than a specified minimum ?min.
When merging edits from the base system and the
n-gram approach, the hybrid system always prefers
edits from the base system if any edit spans overlap,
equivalent to including them in Eq. 3 and assigning
them a penalty of ??.9 Note that the set of pre-
dicted edits y passed as input to the tune algorithm
8Our implementation of the tuning algorithm uses the HOO
2012 Shared Task?s evalfrag.py module to evaluate the F-
score for the error detection subtask.
9If the base system produces overlapping edits, we keep
them all. If there are overlapping edits from the n-gram sys-
tem that have the same highest value for the penalized score in
Equation 3 and do not overlap with any base system edits, we
keep them all.
237
texts
edits
edits
edits
parameters
parameters
gold edits
base system -gram systemn
filteringtuning 
heuristic filtering
training testing
Figure 1: The architecture of the hybrid system. Different
steps are discussed in different parts of the paper: ?base
system? in ?3, ?n-gram system? in ?5, ?heuristic filter-
ing? in ?4, and ?tuning? and ?filtering? in ?6.
includes edits from both the base and n-gram meth-
ods.
Figure 1 illustrates the processes of training and
of producing test output from the hybrid system.
7 Results
Table 1 presents results for the HOO 2012 detec-
tion subtask, including errors of all types. The re-
sults here, reproduced from Dale et al (2012), are
prior to applying participant-suggested revisions to
the set of gold standard edits.10 We include four
variations of our approach: the base system (?3, la-
beled ?base?); the n-gram system (?5, labeled ?n-
gram?) by itself, tuned without edits from the base
system; the hybrid system, tuned with edits from the
base system (?hybrid?); and a variation of the hy-
10After submitting our predictions for the shared task, we
noticed a few minor implementation mistakes in our code re-
lated to the conversion of edits from the base system (?3) and
the task-specific heuristic filtering (?4). We corrected them and
retrained our system. The detection F-scores for the original
and corrected implementations were as follows: 26.45% (orig-
inal) versus 26.23% (corrected) for the base system, 30.70%
(original) versus 30.45% (corrected) for the n-gram system,
35.65% (original) versus 35.24% (corrected) for the hybrid sys-
tem, and 31.82% (original) versus 31.45% (corrected) for the
hybridindep system. Except for this footnote, all results in this
paper are for the original system.
run P R F
base 0 52.63 17.66 26.45
n-gram ? 25.87 37.75 30.70
hybrid 1 33.59 37.97 35.65
hybridindep 2 24.88 44.15 31.82
UI 8 37.22 43.71 40.20
Table 1: Precision, recall, and F-score for the combined
preposition and determiner error detection subtask for
various methods, before participant-suggested revisions
to the gold standard were applied. All values are percent-
ages. Official run numbers are shown in the ?run? col-
umn. The ?n-gram? run was not part of our official sub-
mission. For comparison, ?UI? is the submission, from
another team, that achieved the highest detection F-score
in the HOO 2012 Shared Task.
brid system (?hybridindep?) with the penalties tuned
independently, rather than jointly, to maximize F-
score for detection of each error type. For compari-
son, we also include the best performing run for the
detection subtask in terms of F-score (labeled ?UI?).
We observe that the base and n-gram systems ap-
pear to complement each other well for this task: the
base system achieved 26.45% F-score, and the n-
gram system achieved 30.70%, while the hybrid sys-
tem, with penalties tuned jointly, achieved 35.65%.
Table 2 shows further evidence that the two systems
have complementary performance. We calculate the
overlap between each system?s edits and the gold
standard. We see that only a small number of edits
are predicted by both systems (38 in total, 18 cor-
rect and 20 incorrect), and that the base system pre-
dicts 62 correct edits that the n-gram method does
not predict, and similarly the n-gram method pre-
dicts 92 correct edits that the base system does not
predict. The table also verifies that the base system
exhibits high precision (only 68 false positives in to-
tal) while the n-gram system is tuned for higher re-
call (286 false positives).
Not surprisingly, when the n-gram method?s
penalties were tuned independently (?hybridindep?)
rather than jointly, the overall score was lower, at
31.82% F-score. However, tuning independently
might be desirable if one were concerned with
performance on specific error types or if macro-
averaged F-score were the objective.
The hybrid system performed quite competitively
238
(1) All models had a UD very strange long shoes made from black skin . . .
(2) I think it is a great idea to organise this sort of festival because most of UT people enjoy it.
Figure 2: Examples of errors detected by the base system and missed by the n-gram models.
(3) We have to buy for UT some thing.
(4) I am  MD good diffender.
Figure 3: Examples of errors detected by the n-gram system and missed by the base model.
? gold /? gold
? base /? base ? base /? base
? n-gram 18 92 20 266
/? n-gram 62 276 48 ?
Table 2: The numbers of edits that overlap in the hybrid
system?s output and the gold standard for the test set. The
hybrid system?s output is broken down by whether edits
came from the base system (?3) or the n-gram method
(?5). The empty cell corresponds to hypothetical edits
that were in neither the gold standard or the system?s out-
put (e.g., edits missed by annotators), which we cannot
count.
compared to the other HOO 2012 submissions,
achieving the 3rd best results out of 14 teams for
the detection and recognition subtasks. The per-
formance of the ?UI? system was somewhat higher,
however, at 40.20% F-score compared to the hybrid
system?s 35.65%. We speculate that our hybrid sys-
tem?s performance could be improved somewhat if
we also tuned the base system for the task.
8 Error Analysis
It is illustrative to look at some examples of edits
that the base system correctly detects but the n-gram
model does not, and vice versa. Figure 2 shows ex-
amples of errors detected by the base system, but
missed by the n-gram system. Example (1) illus-
trates that the n-gram model has no concept of syn-
tactic structure. The base system, on the other hand,
carries out simple processing including POS tagging
and chunking, and is therefore aware of at least some
longer-distance dependencies (e.g., a . . . shoes). Ex-
ample (2) shows the effectiveness of the heuris-
tics based on quantifier constructions mentioned in
?3.1.2. These heuristics were developed by devel-
opers familiar with the kinds of errors that language
learners frequently make, and are therefore more tar-
geted than the general n-gram method.
Figure 3 shows examples of errors detected by the
n-gram system but missed by the base system. Ex-
ample (3) shows an example of where the base sys-
tem does not detect the extraneous preposition be-
cause it only searches for these in certain quantifier
constructions. Example (4) contains a spelling error,
which confuses the determiner error detection sys-
tem. It has not seen the misspelling often enough to
be able to reliably judge whether it needs an article
or not before it, and so errs on the side of caution.
When diffender is correctly spelled as defender, the
base system does detect that there is a missing article
in the sentence.
There were a small number of cases where dialect
caused a mismatch between our system?s error pre-
dictions and the gold standard. For example, an ho-
tel is not marked as an error in the gold standard
since it is correct in many dialects. However, it was
always corrected to a hotel by our system. Our sys-
tem also often corrected determiners before the noun
camp, since in American Standard English it is more
usual to talk about going to Summer Camp rather
than going to a/the Summer Camp.
Although the task was to detect preposition and
determiner errors in isolation, there was sometimes
interference from other errors in the sentence. This
impacted the task in two ways. Firstly, in a sentence
239
with multiple errors, it was sometimes possible to
correct it in multiple ways, not all of which involved
preposition or determiner errors. For example, you
could correct the phrase a women by either chang-
ing the a to the, deleting the a entirely or replacing
women with woman. The last change would not fall
under the category of determiner error, and so there
was sometimes a mismatch between the corrections
predicted by the system and the gold standard cor-
rections. Secondly, the presence of multiple errors
impacted the task when a gold standard correction
depended on another error in the same sentence be-
ing corrected in a particular way. For example, you
could correct I?m really excited to read the book. as
I?m really excited about reading the book., however
if you add the preposition about without correcting
to read this correction results in the sentence becom-
ing even more ungrammatical than the original.11
9 Conclusion
In this paper, we have described a hybrid system
for grammatical error detection that combines a pre-
existing base system, which leverages detailed lin-
guistic knowledge and produces high-precision out-
put, with a large-scale n-gram approach, which re-
lies almost exclusively on simple counting of n-
grams in a massive corpus. Though the base system
was not tuned at all for the HOO 2012 Shared Task,
it performed well in the official evaluation. The two
methods also complemented each other well: many
of the predictions from one did not appear in the out-
put of the other, and the F-score of the hybrid system
was considerably higher than the scores for the indi-
vidual methods.
Acknowledgments
We thank Martin Chodorow for discussions about
the base system, Daniel Blanchard for help with run-
ning the base system, Nitin Madnani for discussions
about the paper and for its title, and Michael Flor for
the TrendStream system.
11Many of these cases were addressed in the revised version
of the gold standard data, however we feel that the issue is a
more general one and deserves consideration in the design of
future tasks.
References
Yigal Attali and Jill Burstein. 2006. Automated Es-
say Scoring with e-rater V.2. Journal of Technol-
ogy, Learning, and Assessment, 4(3). Available from
http://www.jtla.org.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2009.
Web-Scale N-gram Models for Lexical Disambigua-
tion. In Proceedings of the 21st international joint
conference on Artifical intelligence, IJCAI?09, pages
1507?1512, Pasadena, California. Morgan Kaufmann
Publishers Inc.
Martin Chodorow and Claudia Leacock. 2000. An Un-
supervised Method for Detecting Grammatical Errors.
In Proceedings of the First Meeting of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL), pages 140?147, Seattle, Wash-
ington. Association for Computational Linguistics.
Martin Chodorow, Joel Tetreault, and Na-Rae Han. 2007.
Detection of Grammatical Errors Involving Preposi-
tions. In Proceedings of the Fourth ACL-SIGSEM
Workshop on Prepositions, pages 25?30, Prague,
Czech Republic. Association for Computational Lin-
guistics.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition and
Determiner Error Correction Shared Task. In Proceed-
ings of the Seventh Workshop on Innovative Use of
NLP for Building Educational Applications, Montreal,
Canada. Association for Computational Linguistics.
Michael Flor. 2012. A fast and flexible archi-
tecture for very large word n-gram datasets.
Natural Language Engineering, pages 1?33.
doi:10.1017/S135132491100034.
Michael Gamon. 2010. Using Mostly Native Data to
Correct Errors in Learners? Writing. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 163?171, Los An-
geles, California. Association for Computational Lin-
guistics.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2004. Detecting Errors in English Article Usage with
a Maximum Entropy Classifier Trained on a Large, Di-
verse Corpus. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC 2004), pages 1625?1628, Lisbon, Portugal.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12:115?129. doi:10.1017/S1351324906004190.
Joel R. Tetreault and Martin Chodorow. 2008. The
Ups and Downs of Preposition Error Detection in
240
ESL Writing. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 865?872, Manchester, UK. Coling
2008 Organizing Committee.
241
Proceedings of the First Workshop on Argumentation Mining, pages 69?78,
Baltimore, Maryland USA, June 26, 2014. c?2014 Association for Computational Linguistics
Applying Argumentation Schemes for Essay Scoring    Yi Song   Michael Heilman   Beata Beigman Klebanov   Paul Deane Educational Testing Service Princeton, NJ, USA  {ysong, mheilman, bbeigmanklebanov, pdeane}@ets.org    Abstract 
Under the framework of the argumentation scheme theory (Walton, 1996), we developed annotation protocols for an argumentative writing task to support identification and classification of the arguments being made in essays. Each annotation protocol defined ar-gumentation schemes (i.e., reasoning pat-terns) in a given writing prompt and listed questions to help evaluate an argument based on these schemes, to make the argument structure in a text explicit and classifiable. We report findings based on an annotation of 600 essays. Most annotation categories were applied reliably by human annotators, and some categories significantly contributed to essay score. An NLP system to identify sen-tences containing scheme-relevant critical questions was developed based on the human annotations.   1. Introduction In this paper, we analyze the structure of argu-ments as a first step in analyzing their quality.  Argument structure plays a critical role in identi-fying relevant arguments based on their content, so it seems reasonable to focus first on identify-ing characteristic patterns of argumentation and the ways in which such arguments are typically developed when they are explicitly stated. It is worthwhile to classify the arguments in a text and to identify their structure when they are ex-tended to include whole text segments (Walton, 1996; Walton, Reed, and Macagno, 2008), but it is not clear how far human annotation can go in analyzing argument structure.  An analysis of the effectiveness and full com-plexity of argument structure is different than the identification of generic elements that might compose an argument, such as claims (e.g., a thesis sentence), main reasons (e.g., supporting topic sentences), evidence (e.g., elaborating 
segments), and other components, such as the introduction and conclusion (Burstein, Kukich, Wolff, Lu, Chodorow, Braden-Harder, & Harris, 1998; Burstein, Marcu, and Knight, 2003; Pendar & Cotos, 2008). In contrast, here we focus on analyzing specific types of arguments, what the literature terms argumentation schemes (Walton, 1996). Argumentation schemes include schemat-ic content and take into account a pattern of pos-sible argumentation moves in a larger persuasive dialog. Understanding these argumentation schemes is important for understanding the logic behind an argument. Critical questions associat-ed with a particular argumentation scheme pro-vide a normative standard that can be used to evaluate the relevance of an argument?s justifica-tory structure (van Eemeren and Grootendorst, 1992; Walton, 1996; Walton et al., 2008).   We aimed to lay foundations for the automat-ed analysis of argumentation schemes, such as the identification and classification of the argu-ments in an essay. Specifically, we developed annotation protocols for writing prompts in an argument analysis task from a graduate school admissions test. The task was designed to assess how well a student analyzes someone else?s ar-gument, which is provided by the prompt.  The student must critically evaluate the logical soundness of the given argument. The annotation categories were designed to map student re-sponses to the scheme-relevant critical questions. We examined whether this approach provides a useful framework for describing argumentation and whether human annotators can apply it relia-bly and consistently. Furthermore, we have be-gun work on automating the annotation process by developing a system to predict whether sen-tences contain scheme-relevant critical questions. 2. Theoretical Framework As Nussbaum (2011) notes, there have been crit-ical advances in the study of informal argument, 
69
  
which takes place within a social context involv-ing dialog among people with different beliefs, most notably the development of theories that provide relatively rich schemata for classifying informal arguments, such as Walton (1996).  An argumentation scheme is defined as ?a more or less conventionalized way of represent-ing the relation between what is stated in the ar-gument and what is stated in the standpoint? (van Eemeren and Grootendorst, 1992, p. 96). It is a strategic pattern of argumentation linking prem-ises to a conclusion and illustrating how the con-clusion is derived from the premises. This ?in-ternal structure? of argumentation reflects justifi-catory standards that can be used to help evaluate the reasonableness of an argument (van Eemeren and Grootendorst, 2004). Argumentation schemes should be distinguished from the kinds of structures postulated in Mann and Thompson?s (1988) Rhetorical Structure Theory (RST) be-cause they focus on relations inherent in the meaning of the argument, regardless of whether they are explicitly realized in the discourse. Consider, for instance, argument from conse-quences, which applies when the primary claim argues for or against a proposed policy (i.e., course of action) by citing positive or negative consequences that would follow if the policy were adopted (Walton, 1996). Elaborations of an argument from consequences are designed to defend against possible objections. For instance, an opponent could claim that the claimed conse-quences are not probable; or that they are not desirable; or that they are less important than other, undesirable consequences. Thus a sophis-ticated writer, in elaborating an argument from consequences, may provide information to rein-force the idea that the argued consequences are probable, desirable, and more important than any possible undesired effects. These moves corre-spond to what the literature calls critical ques-tions, which function as a standard for evaluating the reasonableness of an argument based on its argumentation schemes (Walton, 1996). Walton and his colleagues (2008) analyzed over 60 argumentation schemes, and identified critical questions associated with certain schemes as the logical moves in argumentative discourse. The range of possible moves is quite large, espe-cially when people use multiple schemes. There have been several efforts to annotate corpora with argumentation scheme information to sup-port future machine learning efforts (Mochales and Ieven, 2009; Palau and Moens, 2009; Rienks, Heylen, and Van der Weijden, 2005; 
Verbree, Rienks, and Heylen, 2006), to support argument representation (Atkinson, Bench-Capon, and McBurney, 2006; Rahwan, Banihashemi, Reed, Walton, and Abdallah, 2010), and to teach argumentative writing (Fer-retti, Lewis, and Andrews-Weckerly, 2009; Nussbaum and Schraw, 2007; Nussbaum and Edwards, 2011; Song and Ferretti, 2013). In ad-dition, Feng and Hirsh (2011) used the argumen-tation schemes to reconstruct the implicit parts (i.e., unstated assumptions) of the argument structure. In many previous studies, the data sets on argumentation schemes were relatively small and the inter-rater agreement was not measured.  We are particularly interested in exploring the relationship between the use of scheme-relevant critical questions and essay quality, as measured by holistic essay scores. The difference between an expert and a novice is that the expert knows which critical questions should be asked when the dynamic of the argument requires them, while the novice misses the essential moves to ask critical questions that help evaluate if the argument is valid or reasonable. Often, students presume information and fail to ask questions that would reveal potential fallacies. For exam-ple, they might use quotations from books, ar-guments from TV programs, or opinions posted online without evaluating whether the infor-mation is adequately supported by evidence. Critically evaluating arguments is considered an important skill in college and graduate school. For example, a widely accepted graduate admis-sions test has a task to assess students? critical thinking and analytical writing skills. In this ar-gument analysis task, students should demon-strate skills in critiquing other people?s argu-ments, such as identifying unwarranted assump-tions or discussing what specific evidence is need to support the argument. They must com-municate their evaluation of the arguments clear-ly to the audience. To accomplish this task suc-cessfully, students need to evaluate the argu-ments against appropriate criteria. Therefore, their essays could be analyzed using an annota-tion approach based on the theory of argumenta-tion schemes and critical questions.  Our research questions were as follows:   1. Can this scheme-based annotation approach be applied consistently by annotators to a corpus of argumentative essays? 2. Do annotation categories based on the theo-ry of argumentation schemes contribute 
70
  
significantly to the prediction of essay scores? 3. Can we use NLP techniques to train an au-tomated classifier for distinguishing sen-tences that raise critical questions from sen-tences that contain no critical questions? 3  Development of Annotation Protocols Although Walton?s argumentation schemes pro-vided a good framework for analyzing argu-ments, it was challenging to apply them in some cases of argument essays because various inter-pretations could be made on some argument structures. For instance, people were often con-fused with argument from consequences, argu-ment from correlation to cause, and argument from cause to effect because all these three types of arguments indicate a causal relationship. While it is good that Walton tried to identify var-iations of a causal relationship, a side effect is that some schemes are not so distinguishable from each other, especially for someone who is not an expert in logic. This ambiguity makes it difficult to apply his theory directly to annota-tion. Thus, we modified Walton?s schemes and created new schemes when necessary to achieve exclusive annotation categories and capture the features in the argument analysis task. In this paper, we illustrate our annotation pro-tocols on a policy argument because over half of the argument analysis prompts for the assess-ment we are working with deal with policy is-sues (i.e., issues involve the possibility of putting a practice into place). Here, we use the ?Patriot Car? prompt as an example.   The following appeared in a memo-randum from the new president of the Patriot car manufacturing company.   "In the past, the body styles of Patriot cars have been old-fashioned, and our cars have not sold as well as have our competitors' cars. But now, since many regions in this country report rapid in-creases in the numbers of newly licensed drivers, we should be able to increase our share of the market by selling cars to this growing population. Thus, we should discontinue our oldest models and con-centrate instead on manufacturing sporty cars. We can also improve the success of our marketing campaigns by switching our advertising to the Youth Advertising 
agency, which has successfully promoted the country's leading soft drink."  Test takers are asked to analyze the reasoning in the argument, consider any assumptions, and discuss how well any evidence that is mentioned supports the conclusion. The prompt states that the new president of the Patriot car manufacturing company pointed out a problem that the body styles of Patriot cars have been old-fashioned and their cars have not sold as well as their competitors? cars. The president proposed a plan to discontinue their oldest mod-els and to concentrate on manufacturing sporty cars. He believed that this plan will lead to an increase in their market share (i.e., the goal). This is a policy issue because it involves whether the plan of discontinuing oldest car models and manufacturing sporty cars should be put into place. This prompt shows a typical pattern of many argument analysis prompts about policy issues: (1) a problem is stated; (2) a plan is pro-posed; and (3) a desirable goal will be achieved if the plan is implemented. Thus, we created a policy scheme that includes these three major components (i.e., problem, plan, and goal), and a causal relationship that bridges the plan to the goal in the policy scheme. Therefore, a causal scheme appears in a policy argument to represent the causal relationship from the proposed plan to the goal. This part is different from Walton?s analysis. He uses the argument from conse-quences scheme for policy arguments, but it cre-ated confusions when applying it to annotation, especially when students unconsciously use the word ?cause? to introduce a potential conse-quence that follows a policy. In addition, our causal scheme combines the argument from cor-relation to cause scheme and the argument from cause to effect scheme specified by Walton.  Accordingly, we revised or re-arranged some of the critical questions in Walton?s theory. For example, challenges to arguments that use a poli-cy scheme fall into the following six categories: (a) problem; (b) goal; (c) plan implementation; (d) plan definition; (e) side effect; and (f) alterna-tive plan. When someone writes that the presi-dent should re-evaluate whether this is really a problem, it matches the question in the ?prob-lem? category; when someone questions if there  is an alternative plan that could also help achieve the goal and is better than the plan proposed by the president, it should be categorized as a chal-lenge in ?alternative plan.? We call these ?specif-ic questions? because they are attached to a par-
71
  
ticular prompt. In other words, specific questions are content dependent. Each category also in-cludes one or more ?general questions? that can be asked for any argument using the same argu-mentation scheme, and in this case, it is the poli-cy scheme.  We have developed annotation protocols for various argumentation schemes. Table 1 includes part of the annotation protocols (i.e., scheme, category, and general critical questions) for three argumentation schemes: the policy argument scheme, the causal argument scheme, and the argument from a sample scheme. This study fo-cuses on these three argumentation schemes and 16 associated categories.  4  Application of the Annotation Ap-proach This section focuses on applying the annotation approach and the following research question: Can this scheme-based annotation approach be applied consistently by raters to a corpus of ar-gumentative essays?  4.1  Annotation Rules 
The first step of the annotation is reading the en-tire essay. It is important to understand the writ-er?s major arguments and the organization of the essay. Next, the annotator will identify and high-light any text segment (e.g., paragraph, sentence, or clause) that addresses a critical question. Usu-ally, the minimal text segment is at the sentence-level, but it could be the case that the selection is at the phrase-level when a sentence includes multiple points that match more than one critical question. Thirdly, for a highlighted unit, the an-notator will choose a topic, a category, and a se-cond topic, if applicable. Only one category label can be assigned to each selected text unit. ?Generic? information will not be selected or assigned an annotation label. Generic infor-mation includes restatements of the text in the prompt, general statements that do not address any specific questions, rhetoric attacks, and irrel-evant information. Note that this notion of gener-ic information is related to ?shell language,? as described by Madnani et al (2012).  However, our definition here focuses more closely on sen-tences that do not raise critical questions.  Sur-face errors (e.g., grammar and spelling) can be 
Scheme Category Critical Question 
Policy 
Problem Is this really a problem? Is the problem well-defined? Goal How desirable is this goal? Are there specific conflicting goals we do not wish to sacrifice? Plan Implementation Is it practically possible to carry out this plan?  Plan Definition Is the plan well defined? Side Effects Are there negative side effects that should be taken into account if we carry out our plan? Alternative plan Are there better alternatives that could achieve the goal? 
Causal 
Causal Mechanism Is there really a correlation? Is the correlation merely a coincidence (invalid causal relationship)? Are there alternative causal factors? Causal Efficacy Is the causal mechanism strong enough to produce the desired effects? Applicability Does this causal mechanism apply? Intervening Factors Are there intervening factors that could undermine the causal mechanism? 
Sample 
Significance Are the patterns we see in the sample clear-cut enough (and in the right direction) to support the desired inference? Representativeness Is there any reason to think that this sample might not be representative of the group about which we wish to make an inference? Stability Is there any reason to think this pattern will be stable across all the circumstances about which we wish to make an inference?  Sample Size Is there any reason to think that the sample may not be large enough and reliable enough to support the inference we wish to draw? Validity Is the sample measured in a way that will give valid information on the population attributes about which we wish to make inferences?  Alternatives Are there external considerations that could invalidate the claims? Table 1: Annotation protocols for three types of argumentation schemes 
72
  
ignored if they do not prevent people from un-derstanding the meaning of the essay. Here is an example of annotated text.  As stated by the president, there is a rap-id increase in the number of newly li-censed drivers which would be a market-able target.  [However, there was no con-crete evidence that these newly licensed drivers favored sporty cars over other model types.]Causal Applicability [On a similar note, there was no anecdotal evidence demonstrating that lack of sales was con-tributed to the old-fashion body styles of the Patriot cars.]Causal Mechanism [There could be numerous other factors contrib-uting to their lack of sales:  prices are not competitive, safety ratings are not as high, features are not as appealing.  The best way to tackle this problem is to send out researches and surveys to get the opinions of consumers.]Causal Mechanism 4.2  Annotation Tool The annotation interface includes the following elements: 1. the original writing prompt; 2. topics that the prompt addresses; 3. categories associated with critical questions relevant to that type of argument; 4. general critical questions that can be used across prompts that possess the same argu-mentation scheme; and 5. specific critical questions for this particular prompt.   The annotators highlight text segments to be an-notated and then clicked a button to choose a topic (e.g., body style versus advertising agency in the Patriot Car prompt) and a category to iden-tify which critical questions were addressed.  4.3  Data and Annotation Procedures In this section, we report our annotation on two selected argument analysis prompts in an as-sessment for graduate school admissions. The actual prompts are not included here because they may be used in future tests. Both prompts deal with policy issues and are involved in causal reasoning, but the second prompt also has a sam-ple scheme (see Table 1). For each prompt, we randomly selected 300 essays to annotate. These essays were written between 2008 and 2010.  
Four annotators with linguistics backgrounds who were not co-authors of the paper received training on the annotation approach. Training focused on the application to specific prompts because each prompt had a specific annotation protocol that covers the argumentation schemes and how they relate to the prompt?s topics. The first author delivered the training sessions, and helped resolve differences of opinion during practice annotation rounds. After training and practice, the annotators annotated 20 pilot essays for a selected prompt to test their agreement. This pilot stage gave us another chance to find and clarify any confusion about the annotation categories. After that, the annotators worked on the sampled set of 300 essays, and these annota-tions were then used for analyses. For each prompt, 40 essays were randomly selected, and all 4 annotators annotated these 40 essays to check the inter-annotator agreement.  For the experiments described later that involve the mul-tiply-annotated set, we used the annotations from the annotator who seemed most consistent. 4.4  Inter-Annotator Agreement To compute human-human agreement, we auto-matically split the essays into sentences.  For each sentence, we computed the annotations that overlapped with at least part of the tence.  Then, for each category, we computed human-human agreement across all sentences about whether that category should be marked or not.  We also created a ?Generic? label, as dis-cussed in section 4.1, for sentences that were not marked by any of the other labels. We computed two inter-annotator agreement statistics. Our primary statistic is Cohen?s kappa between pairs of raters. Four annotators generat-ed 6 pairs of kappa values, and in this report we only report the average kappa value for each an-notation category. As an alternative statistic, we computed Krippendorff?s alpha, a chance-corrected statistic for calculating the inter-annotator agreement between multiple coders (four annotators in our case), which is similar to multi kappa (Krippendorff, 1980). Table 2 shows the kappa and alpha values for each annotation category, excluding those that were rare. To identify rare categories, we aver-aged the numbers of sentences annotated under a category among four annotators, which indicated how many sentences were annotated under this category in 40 essays.  If the number was lower than 10, which means that no more than one sen-tence was annotated in every four essays, then 
73
  
the category was considered rare. Most rare cate-gories had low inter-rater agreement, which is not surprising.  It is not realistic to require anno-tators to always agree about rare categories. From Table 2, we can see that the kappa value and the alpha value on the same category were close. The inter-annotator agreement on the ?ge-neric? category varied little across the two prompts (kappa: 0.572-0.604; alpha: 0.571-0.603), which indicates that the annotators had a fairly good agreement on this category. The an-notators had good agreements on most of the commonly used categories (kappa ranged from 0.549 to 0.848, and alpha ranged from 0.537 to 0.843) except the ?plan definition? under the pol-icy scheme in prompt B (both kappa and alpha values were below 0.400). The major reason for this disagreement is that one annotator marked a significantly higher number of sentences (more than double) for this category than others did.  
 Table 2: Inter-annotator agreement 5  Essay Score and Annotation Features This section explores the second research ques-tion: Do annotation categories based on the theo-ry of argumentation schemes contribute signifi-cantly to the prediction of essay scores?  An-swering this question would tell us whether we capture an important construct of the argument analysis task by recognizing these argumentation features. Specifically, we tested whether these features add predictive value to a model based 
the state-of-the-art e-rater essay scoring system (Burstein, Tetreault, and Madnani, 2013). To explore the relationship between annota-tion categories and essay quality, we ran a multi-ple regression analysis for each prompt. Essay quality was the dependent variable and was measured by a final human score, on a scale from 0 to 6. The independent variables were nine high-level e-rater features and the annotation categories relevant to a prompt (Prompt A: 10 categories; Prompt B 16 categories). The e-rater features were designed to measure different as-pects of writing (grammar, mechanics, style, us-age, word choice, word length, sentence variety, development, and organization). We computed the percentage of sentences that were marked as belonging to each category (i.e., the number of sentences in a category divided by the total num-ber of sentences) to factor out essay length. Note that the generic category was negatively correlated with the essay score in both prompts, since it included responses judged irrelevant to the scheme-relevant critical questions. In other words, the generic responses are the parts of the text that do not present specific critical evalua-tions of the arguments in a given prompt. For the purposes of our evaluation, we used the inverse feature labeled ?all critical questions?: the pro-portion of the text that actually raises some criti-cal question (i.e., is not generic), regardless of scheme. We believe this formulation more trans-parently expresses the underlying mechanism relating the feature to essay quality. For each prompt, we split the 300 essays into two data sets: the training set and the testing set. The testing set had the 40 essays that were anno-tated by all four annotators, and the training set had the remaining 260. We trained three models with stepwise regression on the training set and evaluated them on the testing set:  1. A model that included only the e-rater fea-tures to examine how well the e-rater mod-el works (?baseline?) 2. A model with the baseline features and all the annotation category percentage varia-bles except for the "generic" category vari-able (?baseline + categories?) 3. A model with the baseline features and a feature corresponding to the inverse of the "generic" category (?baseline + all critical questions?).  Table 3 presents the Pearson correlation coef-ficient r values for comparing model predictions 
Prompt Category Kappa Alpha 
Prompt A     Generic 0.572 0.571  Policy : Problem 0.644 0.640  Policy : Side Effects 0.612 0.609  Policy : Alternative Plan 0.665 0.666  Causal : Causal Mechanism 0.680 0.676  Causal : Applicability 0.557 0.555 Prompt B     Generic 0.604 0.603  Policy : Problem 0.848 0.843  Policy : Plan Definition 0.346 0.327  Causal : Causal Mechanism 0.620 0.622  Causal : Applicability 0.767 0.769  Sample : Validity 0.549 0.537 
74
  
to human scores for each of the models. In prompt A, three annotation categories (causal mechanism, applicability, and alternative plan) were selected by the stepwise regression because they significantly contributed to the essay score above the nine e-rater features. This model showed higher test set correlations than the base-line model (? r = .014). The model with the gen-eral argument feature (?all critical questions?) showed a similar increase (? r = .014).   Training Set r Testing Set r Testing Set ? r Prompt A    baseline	 ? .838 .852 --- baseline + specific categories	 ? .852 .866 .014 baseline +  all critical questions	 ? .858 .866 .014  Prompt B    baseline	 ? .818 .761 --- baseline + specific categories	 ? .835 .817 .056 baseline +  all critical questions	 ? .845 .821 .060  Table 3: Performance of essay scoring models with and without argumentation features  Similar observations apply to prompt B. The causal mechanism category added prediction significantly above e-rater with an increase (? r = .056). The model containing the general argu-ment feature (?all critical questions?) performed slightly better (? r = .060). These results suggest that annotation catego-ries based on argumentation schemes contribute additional useful information about essay quality to a strong baseline essay scoring model.  In the next section, we report on preliminary experi-ments testing whether these annotations can be automated, which would almost certainly be nec-essary for practical applications. 6  Argumentation Schemes NLP System We developed an NLP system for automatically identifying the presence of scheme-relevant criti-cal questions in essays, and we evaluated this system with annotated data from the two selected argument prompts. This addresses the third re-search question: Can we use NLP techniques to train an automated classifier for distinguishing 
sentences that raise critical questions from sen-tences that contain no critical questions? 6.1  Modeling In this initial development of the NLP system, we focused on the task of predicting whether a sentence raises any critical questions or none (i.e., generic vs. nongeneric). As such, the task was binary classification at the level of the sen-tence. The system we developed uses the SKLL tool1 to fit L2-penalized logistic regression mod-els with the following features:  ? Word n-grams: Binary indicators for the presence of contiguous subsequences of n words in the sentence. The value of n ranged from 1 to 3. These features had value 1 if a particular n-gram was present in a sentence and 0 otherwise. ? word n-grams of the previous and next sen-tences: These are analogous to the word n-gram features for the current sentence. ? sentence length bins: Binary indicators for whether the sentence is longer than 2t word tokens, where t  ranges from 1 to 10. ? sentence position: The sentence number di-vided by the number of sentences in text. ? part of speech tags: Binary indicators for the presence of words with various parts of speech, as predicted by NLTK 2.0.4. ? prompt overlap: Three features based on lex-ical overlap between the sentence and the prompt for the essay: a) the Jaccard similari-ty between the sets of word n-grams in the sentence and prompt (n = 1, 2, 3), b) the Jac-card similarity between the sets of word uni-grams (i.e., just n = 1) in the sentence and prompt, and c) the Jaccard similarity be-tween the sets of ?content? word unigrams in the sentence and prompt (for this, content words were defined as word tokens that con-tained only numbers and letters and did not appear in NLTK?s English stopword list). 6.2  Experiments For these experiments, we used the training and testing sets described in Section 5. We trained models on the training data for each prompt in-dividually and on the combination of the training data for both prompts. To measure generalization across prompts, we tested these models on the testing data for each prompt and on the combina-                                                1 https://github.com/EducationalTestingService/skll 
75
  
tion of the testing data for the two prompts. We evaluated performance in terms of unweighted Cohen?s kappa. The results are in Table 4.  Training Testing Kappa combined combined .438 Prompt A  .350 Prompt B  .346 combined Prompt A .379 Prompt A  .410 Prompt B  .217 combined Prompt B .498 Prompt A  .285 Prompt B  .478  Table 4: Performance of the NLP Model  The model trained on data from both prompts performed relatively well compared to the other models.  For the testing data for prompt B, the combined model outperformed the model trained on just data from prompt B.  However, the prompt-specific model for prompt A slightly outperformed the combined model on the testing data for prompt A. Although the performance of models trained with data from one prompt and tested with data from another prompt did not perform as well, there is evidence of some generalization across prompts. The model trained on data from prompt B and tested on data from prompt A had kappa = 0.217; the model trained on data from prompt A and tested on data from prompt B had kappa = 0.285. Of course, these human-machine agree-ment values were somewhat lower than human-human agreement values (0.572 and 0.604, re-spectively), leaving substantial room for im-provement in future work. We also examined the most strongly weighted features in the combined model.  We observed that multiple hedge words (e.g., ?perhaps?, ?may?) had positive weights, which associated with the ?generic? class.  We also observed that words related to argumentation (e.g., ?conclu-sions?, ?questions?) had negative weights, which associated them with the nongeneric class, as one would expect.  One issue of concern is that some words related to the specific topics discussed in the prompts received high weights as well, which may limit generalizability.  
7  Conclusion Our research focused on identification and classi-fication of argumentation schemes in argumenta-tive text. We developed annotation protocols that capture various argumentation schemes. The an-notation categories corresponded to scheme-relevant critical questions, and for text segments that do not contain any critical questions, we as-signed a ?generic? category. In this paper, we reported the results based on an annotation of a large pool of student essays (both high-quality and low-quality essays). Results showed that most of the common annotation categories (e.g. causal mechanism, alternative plan) can be ap-plied reliably by the four annotators. However, the annotation work is labor-intensive. People need to receive sufficient train-ing to apply the approach consistently. They must not only identify meaningful chunks of tex-tual information but also assign the right annota-tion category label for the selected text. Despite these complexities, it is a worthwhile investiga-tion. Developing a systematic classification of argument structures not only plays a critical role in this project, but also has a potential contribu-tion to other assessments on argumentation skills aligned with the Common Core State Standards. This work would help improve the current auto-mated scoring techniques for argumentative es-says because this annotation approach takes into account the argument structure and its content.  We ran regression analyses and found that manual annotations grounded in the argumenta-tion schemes theory predict essay quality. Our data showed that features based on manual ar-gument scheme annotations significantly con-tributed to models of essay scores for both prompts. This is probably because our approach focused on the core of argumentation, rather than surface or word-level features (e.g., mechanics, grammar, usage, style, essay organization, and vocabulary) examined by the baseline model. Furthermore, we have implemented an auto-mated system for predicting the human annota-tions. This system focused only on predicting whether or not a sentence raises any critical questions (i.e., generic vs. nongeneric). In the future, we plan to test whether features based on automated annotations make contributions to essay scoring models that are similar to the con-tributions of manual annotations.  We also plan to work on detecting specific critical questions and adding additional features, such as features from Feng and Hirst (2011). 
76
  
Acknowledgements  We would like to thank Keelan Evanini, Jill Burstein, Aoife Cahill, and the anonymous re-viewers of this paper for their helpful comments.  We would also like to thank Michael Flor for helping set up the annotation interface, and Melissa Lopez, Matthew Mulholland, Patrick Houghton, and Laura Ridolfi for annotating the data. References Katie Atkinson, Trevor Bench-Capon, and Peter McBurney. 2006. Computational representation of practical argument. Synthese, 152: 157-206. Burstein, Jill, Karen Kukich, Susanne Wolff, Chi Lu, Martin Chodorow, Lisa Braden-Harder, and Mary Dee Harris. 1998. "Automated scoring using a hybrid feature identification technique." In Pro-ceedings of the 17th international conference on Computational linguistics-Volume 1, pp. 206-210. Association for Computational Linguistics. Jill Burstein, Daniel Marcu, and Kevin Knight. 2003. Finding the WRITE stuff: Automatic identification of discourse structure in student essays. IEEE Transactions on Intelligent Systems, 18(1): 32-39.  Jill Burstein, Joel Tetreault, and Nitin Madnani. 2013. The e-rater automated essay scoring system. In Sermis, M. D. and Burstein, J. (eds.), Handbook of Automated Essay Evaluation: Current Applications and New Directions (pp. 55-67). New York: Routledge.  Vanessa W. Feng and Graeme Hirst. 2011.  Classify-ing arguments by scheme.  Proceedings of the 49th Annual Meeting of the Association for Computa-tional Linguistics, Portland, OR.   Ralph P. Ferretti, William E. Lewis, and Scott An-drews-Weckerly. 2009. Do goals affect the struc-ture of students? argumentative writing strategies? Journal of Educational Psychology, 101: 577-589. Klaus Krippendorff. 1980. Content Analysis: An In-troduction to its Methodology. Beverly Hills, CA : Sage Publications.Mann, William C., and Sandra A. Thompson. 1988. "Rhetorical structure theory: Toward a functional theory of text organization." Text 8(3): 243-281. Nitin Madnani, Michael Heilman, Joel Tetreault, and Martin Chodorow.  2012.  Identifying High Level Organizational Elements in Argumentative Dis-course. Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.  (pp. 20-28).  Association for Com-putational Linguistics. 
Raquel Mochales and Asgje Ieven. 2009. Creating an argumentation corpus: do theories apply to real ar-guments?: a case study on the legal argumentation of the ECHR. In ICAIL ?09: Proceedings of the 12th International Conference on Artificial Intelli-gence and Law.  Michael Nussbaum. 2011. Argumentation, dialogue theory, and probability modeling: alternative frameworks for argumentation research in educa-tion. Educational Psychologist, 46: 84-106.  Nussbaum, E. M. and Edwards, O.V. (2011). Critical questions and argument stratagems: A framework for enhancing and analyzing students? reasoning practices. Journal of the Learning Sciences, 20, 443-488. Palau, R.M. and Moens, M. F. 2009. Automatic ar-gument detection and its role in law and the seman-tic web. In Proceedings of the 2009 conference on law, ontologies and the semantic web. IOS Press, Amsterdam, The Netherlands.Pendar, Nick, and Elena Cotos. 2008. "Automatic identification of discourse moves in scientific article introductions." In Proceedings of the Third Workshop on Innova-tive Use of NLP for Building Educational Applica-tions, pp. 62-70. Association for Computational Linguistics. Rahwan, I., Banihashemi, B., Reed, C. Walton, D., and Abdallah, S. (2010). Representing and classi-fying arguments on the semantic web. The Knowledge Engineering Review. Rienks, R., Heylen, D., and Van der Weijden, E. 2005. Argument diagramming of meeting conver-sations. In A. Vinciarelli, J. Odobez (Ed.), Pro-ceedings of Multimodal Multiparty Meeting Pro-cessing, Workshop at the 7th International Confer-ence on Multimodal Interfaces (pp. 85?92). Trento, Italy.  Yi Song and Ralph P. Ferretti. 2013. Teaching critical questions about argumentation through the revising process: Effects of strategy instruction on college students? argumentative essays. Reading and Writ-ing: An Interdisciplinary Journal, 26(1): 67-90. Stephen E. Toulmin. 1958. The uses of argument. Cambridge University Press, Cambridge, UK. Frans H. van Eemeren and Rob Grootendorst. 1992. Argumentation, communication, and fallacies: A pragma-dialectical perspective. Mahwah, NJ: Erl-baum. Frans H. van Eemeren and Rob Grootendorst. 2004. A systematic theory of argumentation: A pragma-dialectical approach. Cambridge, UK: Cambridge University Press. Verbree, D., Rienks, H., and Heylen, D. (2006). First Steps Towards the Automatic Construction of Ar-gument-Diagrams from Real Discussions. In Pro-
77
  
ceedings of the 2006 conference on Computational Models of Argument: Proceedings of COMMA 2006. IOS Press, Amsterdam, The Netherlands. Douglas N. Walton. 1996. Argumentation schemes for presumptive reasoning. Mahwah, NJ: Lawrence Erlbaum. Douglas N. Walton, Chris Reed, and Fabrizio Macagno. 2008. Argumentation schemes. New York, NY: Cambridge University Press. 
78
Proceedings of the Second Workshop on Metaphor in NLP, pages 11?17,
Baltimore, MD, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
Different Texts, Same Metaphors: Unigrams and Beyond
Beata Beigman Klebanov, Chee Wee Leong, Michael Heilman, Michael Flor
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541
{bbeigmanklebanov,cleong,mheilman,mflor}@ets.org
Abstract
Current approaches to supervised learning
of metaphor tend to use sophisticated fea-
tures and restrict their attention to con-
structions and contexts where these fea-
tures apply. In this paper, we describe the
development of a supervised learning sys-
tem to classify all content words in a run-
ning text as either being used metaphori-
cally or not. We start by examining the
performance of a simple unigram baseline
that achieves surprisingly good results for
some of the datasets. We then show how
the recall of the system can be improved
over this strong baseline.
1 Introduction
Current approaches to supervised learning of
metaphor tend to (a) use sophisticated features
based on theories of metaphor, (b) apply to cer-
tain selected constructions, like adj-noun or verb-
object pairs, and (c) concentrate on metaphors
of certain kind, such as metaphors about gover-
nance or about the mind. In this paper, we de-
scribe the development of a supervised machine
learning system to classify all content words in a
running text as either being used metaphorically
or not ? a task not yet addressed in the literature,
to our knowledge. This approach would enable,
for example, quantification of the extent to which
a given text uses metaphor, or the extent to which
two different texts use similar metaphors. Both of
these questions are important in our target appli-
cation ? scoring texts (in our case, essays written
for a test) for various aspects of effective use of
language, one of them being the use of metaphor.
We start by examining the performance of a
simple unigram baseline that achieves surprisingly
good results for some of the datasets. We then
show how the recall of the system can be improved
over this strong baseline.
2 Data
We use two datasets that feature full text anno-
tations of metaphors: A set of essays written for
a large-scale assessment of college graduates and
the VUAmsterdam corpus (Steen et al., 2010),
1
containing articles from four genres sampled from
the BNC. Table 1 shows the sizes of the six sets,
as well as the proportion of metaphors in them; the
following sections explain their composition.
Data #Texts #NVAR #metaphors
tokens (%)
News 49 18,519 3,405 (18%)
Fiction 11 17,836 2,497 (14%)
Academic 12 29,469 3,689 (13%)
Conversation 18 15,667 1,149 ( 7%)
Essay Set A 85 21,838 2,368 (11%)
Essay Set B 79 22,662 2,745 (12%)
Table 1: Datasets used in this study. NVAR =
Nouns, Verbs, Adjectives, Adverbs, as tagged by
the Stanford POS tagger (Toutanova et al., 2003).
2.1 VUAmsterdam Data
The dataset consists of 117 fragments sampled
across four genres: Academic, News, Conversa-
tion, and Fiction. Each genre is represented by ap-
proximately the same number of tokens, although
the number of texts differs greatly, where the news
archive has the largest number of texts.
We randomly sampled 23% of the texts from
each genre to set aside for a blind test to be carried
out at a later date with a more advanced system;
the current experiments are performed using cross-
validation on the remaining 90 fragments: 10-fold
on News, 9-fold on Conversation, 11 on Fiction,
and 12 on Academic. All instances from the same
text were always placed in the same fold.
1
http://www2.let.vu.nl/oz/metaphorlab/metcor/search/index.html
11
The data is annotated using MIP-VU proce-
dure. It is based on the MIP procedure (Prag-
glejaz, 2007), extending it to handle metaphori-
city through reference (such as marking did as a
metaphor in As the weather broke up, so did their
friendship) and allow for explicit coding of diffi-
cult cases where a group of annotators could not
arrive at a consensus. The tagset is rich and is
organized hierarchically, detecting various types
of metaphors, words that flag the presense of
metaphors, etc. In this paper, we consider only the
top-level partition, labeling all content words with
the tag ?function=mrw? (metaphor-related word)
as metaphors, while all other content words are la-
beled as non-metaphors.
2
2.2 Essay Data
The dataset consists of 224 essays written for a
high-stakes large-scale assessment of analytical
writing taken by college graduates aspiring to en-
ter a graduate school in the United States. Out of
these, 80 were set aside for future experiments and
not used for this paper. Of the remaining essays,
85 essays discuss the statement ?High-speed elec-
tronic communications media, such as electronic
mail and television, tend to prevent meaningful
and thoughtful communication? (Set A), and 79
discuss the statement ?In the age of television,
reading books is not as important as it once was.
People can learn as much by watching television
as they can by reading books.? (Set B). Multiple
essays on the same topic is a unique feature of this
dataset, allowing the examination of the effect of
topic on performance, by comparing performance
in within-topic and across-topic settings.
The essays were annotated using a protocol
that prefers a reader?s intuition over a formal de-
finition, and emphasizes the connection between
metaphor and the arguments that are put forward
by the writer. The protocol is presented in detail
in Beigman Klebanov and Flor (2013). All essays
were doubly annotated. The reliability is ? = 0.58
for Set A and ? = 0.56 for Set B. We merge the two
annotations (union), following the observation in
a previous study Beigman Klebanov et al. (2008)
that attention slips play a large role in accounting
for observed disagreements.
We will report results for 10-fold cross-
validation on each of sets A and B, as well as
2
We note that this top-level partition was used for many
of the analyses discussed in (Steen et al., 2010).
across prompts, where the machine learner would
be trained on Set A and tested on Set B and vice
versa.
3 Supervised Learning of Metaphor
For this study, we consider each content-word to-
ken in a text as an instance to be classified as a
metaphor or non-metaphor. We use the logistic
regression classifier in the SKLL package (Blan-
chard et al., 2013), which is based on scikit-learn
(Pedregosa et al., 2011), optimizing for F
1
score
(class ?metaphor?). We consider the following
features for metaphor detection.
? Unigrams (U): All content words from the
relevant training data are used as features,
without stemming or lemmatization.
? Part-of-Speech (P): We use Stanford POS
tagger 3.3.0 and the full Penn Treebank tagset
for content words (tags starting with A, N, V,
and J), removing the auxiliaries have, be, do.
? Concreteness (C): We use Brysbaert et al.
(2013) database of concreteness ratings for
about 40,000 English words. The mean ra-
tings, ranging 1-5, are binned in 0.25 incre-
ments; each bin is used as a binary feature.
? Topic models (T): We use Latent Dirich-
let Allocation (Blei et al., 2003) to derive
a 100-topic model from the NYT corpus
years 2003?2007 (Sandhaus, 2008) to rep-
resent common topics of public discussion.
The NYT data was lemmatized using NLTK
(Bird, 2006). We used the gensim toolkit
(
?
Reh?u?rek and Sojka, 2010) for building the
models, with default parameters. The score
assigned to an instance w on a topic t is
log
P (w|t)
P (w)
where P (w) were estimated from
the Gigaword corpus (Parker et al., 2009).
These features are based on the hypothesis
that certain topics are likelier to be used as
source domains for metaphors than others.
4 Results
For each dataset, we present the results for the
unigram model (baseline) and the results for the
full model containing all the features. For cross-
validation results, all words from the same text
were always placed in the same fold, to ensure that
we are evaluating generalization across texts.
12
M Unigram UPCT
Data F P R F P R F
Set A .20 .72 .43 .53 .70 .47 .56
Set B .22 .79 .54 .64 .76 .60 .67
B-A .20 .58 .45 .50 .56 .50 .53
A-B .22 .71 .28 .40 .72 .35 .47
News .31 .62 .38 .47 .61 .43 .51
Fiction .25 .54 .23 .32 .54 .24 .33
Acad. .23 .51 .20 .27 .50 .22 .28
Conv. .14 .39 .14 .21 .36 .15 .21
Table 2: Summary of performance, in terms of
precision, recall, and F
1
. Set A, B, and VUAm-
sterdam: cross-validation. B-A and A-B: Training
on B and testing on A, and vice versa, respectively.
Column M: F
1
of a pseudo-system that classifies
all words as metaphors.
4.1 Performance of the Baseline Model
First, we observe the strong performance of the
unigram baseline for the cross-validation within
sets A and B (rows 1 and 2 in Table 2). For a
new essay, about half its metaphors will have been
observed in a sample of a few dozen essays on the
same topic; these words are also consistently used
as metaphors, as precision is above 70%. Once the
same-topic assumption is relaxed down to related
topics, the sharing of metaphor is reduced (com-
pare rows 1 vs 3 and 2 vs 4), but still substantial.
Moving to VUAmsterdam data, we observe that
the performance of the unigram model on the
News partition is comparable to its performance in
the cross-prompt scenario in the essay data (com-
pare row 5 to rows 3-4 in Table 2), suggesting that
the News fragments tend to discuss a set of related
topics and exhibit substantial sharing of metaphors
across texts.
The performance of the unigram model is much
lower for the other VUAmsterdam partitions, al-
though it is still non-trivial, as evidenced by its
consistent improvement over a pseudo-baseline
that classifies all words as metaphor, attaining
100% recall (shown in column M in Table 2). The
weaker performance could be due to highly diver-
gent topics between texts in each of the partitions.
It is also possible that the number of different
texts in these partitions is insufficient for covering
the metaphors that are common in these kinds of
texts ? recall that these partitions have small num-
bers of long texts, whereas the News partition has
a larger number of short texts (see Table 1).
4.2 Beyond Baseline
The addition of topic model, POS, and concrete-
ness features produces a significant increase in
recall across all evaluations (p < 0.01), using
McNemar?s test of the significance of differ-
ences between correlated proportions (McNemar,
1947). Even for Conversations, where recall
improvement is the smallest and F
1
score does
not improve, the UPCT model recovers all 161
metaphors found by the unigrams plus 14 addi-
tional metaphors, yielding a significant result on
the correlated test.
We next investigate the relative contribution of
the different types of features in the UPCT model
by ablating each type and observing the effect on
performance. Table 3 shows ablation results for
essay and News data, where substantial improve-
ments over the unigram baseline were produced.
We observe, as expected, that the unigram fea-
tures contributed the most, as removing them re-
sults in the most dramatic drop in performance,
although the combination of concreteness, POS,
and topic models recovers about one-fourth of
metaphors with over 50% precision, showing non-
trivial performance on essay data.
The second most effective feature set for essay
data are the topic models ? they are responsible for
most of the recall gain obtained by the UPCT mo-
del. For example, one of the topics with a positive
weight in essays in set B deals with visual ima-
gery, its top 5 most likely words in the NYT being
picture, image, photograph, camera, photo. This
topic is often used metaphorically, with words
like superficial, picture, framed, reflective, mirror,
capture, vivid, distorted, exposure, scenes, face,
background that were all observed as metaphors in
Set B. In the News data, a topic that deals with hur-
ricane Katrina received a positive weight, as words
of suffering and recovery from distaster are often
used metaphorically when discussing other things:
starved, severed, awash, damaged, relief, victim,
distress, hits, swept, bounce, response, recovering,
suffering.
The part-of-speech features help improve recall
across all datasets in Table 3, while concreteness
features are effective only for some of the sets.
5 Discussion: Metaphor & Word Sense
The classical ?one sense per discourse? finding of
Gale et al. (1992) that words keep their senses
within the same text 98% of the time suggests that
13
Set A cross-val. Set B cross-val. Train B : Test A Train A : Test B News
P R F P R F P R F P R F P R F
M .11 1.0 .20 .12 1.0 .22 .11 1.0 .20 .12 1.0 .22 .18 1.0 .31
U .72 .43 .53 .79 .54 .64 .58 .45 .50 .71 .28 .40 .62 .38 .47
UPCT .70 .47 .56 .76 .60 .67 .56 .50 .53 .72 .35 .47 .61 .43 .51
? U .58 .21 .31 .63 .28 .38 .44 .21 .29 .59 .18 .27 .55 .23 .32
? P .71 .46 .56 .76 .58 .66 .57 .48 .52 .70 .33 .45 .61 .41 .49
? C .70 .46 .55 .77 .58 .66 .56 .50 .53 .71 .34 .46 .61 .43 .50
? T .71 .43 .53 .78 .55 .65 .57 .45 .51 .71 .29 .41 .62 .41 .49
Table 3: Ablation evaluations. Model M is a pseudo-system that classifies all instances as metaphors.
if a word is used as a metaphor once in a text, it is
very likely to be a metaphor if it is used again in
the same text. Indeed, this is the reason for putting
all words from the same text in the same fold in
cross-validations, as training and testing on diffe-
rent parts of the same text would produce inflated
estimates of metaphor classification performance.
Koeling et al. (2005) extend the notion of dis-
course beyond a single text to a domain, such as
articles on Finance, Sports, and a general BNC
domain. For a set of words that each have at
least one Finance and one Sports sense and not
more than 12 senses in total, guessing the pre-
dominant sense in Finance and Sports yielded 77%
and 76% precision, respectively. Our results with
the unigram model show that guessing ?metaphor?
based on a sufficient proportion of previously ob-
served metaphorical uses in the given domain
yields about 76% precision for essays on the same
topic. Thus, metaphoricity distinctions in same-
topic essays behave similarly to sense distinctions
for polysemous words with a predominant sense
in the Finance and Sports articles, keeping to their
domain-specific predominant sense
3
4
of the time.
Note that a domain-specific predominant sense
may or may not be the same as the most frequent
sense overall; similarly, a word?s tendency to be
used metaphorically might be domain specific or
general. The results for the BNC at large are likely
to reflect general rather than domain-specific sense
distributions. According to Koeling et al. (2005),
guessing the predominant sense in the BNC yields
51% precision; our finding for BNC News is 62%
precision for the unigram model. The difference
could be due to the mixing of the BNC genres in
Koeling et al. (2005), given the lower precision of
metaphoricity prediction in non-news (Table 2).
In all, our results suggest that the pattern of
metaphorical and non-metaphorical use is in line
with that of dominant word-sense for more and
less topically restricted domains.
6 Related Work
The extent to which different texts use similar
metaphors was addressed by Pasanek and Scul-
ley (2008) for corpora written by the same author.
They studied metaphors of mind in the oeuvre
of 7 authors, including John Milton and William
Shakespeare. They created a set of metaphori-
cal and non-metaphorical references to the mind
using excerpts from various texts written by these
authors. Using cross-validation with unigram
features for each of the authors separately, they
present very high accuracies (85%-94%), suggest-
ing that authors are highly self-consistent in the
metaphors of mind they select. They also find
good generalizations between some pairs of au-
thors, due to borrowing or literary allusion.
Studies using political texts, such as speeches
by politicians or news articles discussing politi-
cally important events, documented repeated use
of words from certain source domains, such as
rejuvenation in Tony Blair?s speeches (Charteris-
Black, 2005) or railroad metaphors in articles dis-
cussing political integration of Europe (Musolff,
2000). Our results regarding settings with substan-
tial topical consistency second these observations.
According to the Conceptual Metaphor theory
(Lakoff and Johnson, 1980), we expect certain ba-
sic metaphors to be highly ubiquitous in any cor-
pus of texts, such as TIME IS SPACE or UP IS
GOOD. To the extent that these metaphors are
realized through frequent content words, we ex-
pect some cross-text generalization power for a
unigram model. Perhaps the share of these basic
metaphors in all metaphors in a text is reflected
most faithfully in the peformance of the unigram
model on the non-News partitions of the VUAms-
14
terdam data, where topical sharing is minimal.
Approaches to metaphor detection are often ei-
ther rule-based or unsupervised (Martin, 1990;
Fass, 1991; Shutova et al., 2010; Shutova and
Sun, 2013; Li et al., 2013), although supervised
approaches have recently been attempted with the
advent of relatively large collections of metaphor-
annotated materials (Mohler et al., 2013; Hovy et
al., 2013; Pasanek and Sculley, 2008; Gedigan
et al., 2006). These approaches are difficult to
compare to our results, as these typically are not
whole texts but excerpts, and only certain kinds of
metaphors are annotated, such as metaphors about
governance or about the mind, or only words be-
longing to certain syntactic or semantic class are
annotated, such as verbs
3
or motion words only.
Concreteness as a predictor of metaphoricity
was discussed in Turney et al. (2011) in the context
of concrete adjectives modifying abstract nouns.
The POS features are inspired by the discussion
of the preference and aversion of various POS
towards metaphoricity in Goatly (1997). Heintz
et al. (2013) use LDA topics built on Wikipedia
along with manually constructed seed lists for po-
tential source and target topics in the broad tar-
get domain of governance, in order to identify
sentences using lexica from both source and tar-
get domains as potentially containing metaphors.
Bethard et al. (2009) use LDA topics built on BNC
as features for classifying metaphorical and non-
metaphorical uses of 9 words in 450 sentences that
use these words, modeling metaphorical vs non-
metaphorical contexts for these words. In both
cases, LDA is used to capture the topical compo-
sition of a sentence; in contrast, we use LDA to
capture the tendency of words belonging to a topic
to be used metaphorically in a given discourse.
Dunn (2013) compared algorithms based on
various theories of metaphor on VUAmsterdam
data. The evaluations were done at sentence level,
where a sentence is metaphorical if it contains at
least one metaphorically used word. In this ac-
counting, the distribution is almost a mirror-image
of our setting, as 84% of sentences in News were
labeled as metaphorical, whereas 18% of content
words are tagged as such. The News partition was
very difficult for the systems examined in Dunn
(2013) ? three of the four systems failed to pre-
dict any non-metaphorical sentences, and the one
system that did so suffered from a low recall of
3
as in Shutova and Teufel (2010)
metaphors, 20%. Dunn (2013) shows that the
different systems he compared had relatively low
agreement (? < 0.3); he interprets this finding as
suggesting that the different theories underlying
the models capture different aspects of metapho-
ricity and therefore detect different metaphors. It
is therefore likely that features derived from the
various models would fruitfully complement each
other in a supervised learning setting; our findings
suggest that the simplest building block ? that of
a unigram model ? should not be ignored in such
experiments.
7 Conclusions
We address supervised learning of metaphoricity
of words of any content part of speech in a running
text. To our knowledge, this task has not yet been
studied in the literature. We experimented with a
simple unigram model that was surprisingly suc-
cessful for some of the datasets, and showed how
its recall can be further improved using topic mo-
dels, POS, and concreteness features.
The generally solid performance of the unigram
features suggests that these features should not be
neglected when trying to predict metaphors in a
supervised learning paradigm. Inasmuch as me-
taphoricity classification is similar to a coarse-
grained word sense disambiguation, a unigram
model can be thought of as a crude predominant
sense model for WSD, and is the more effective
the more topically homogeneous the data.
By evaluating models with LDA-based topic
features in addition to unigrams, we showed that
topical homogeneity can be exploited beyond uni-
grams. In topically homogeneous data, certain
topics commonly discussed in the public sphere
might not be addressed, yet their general fa-
miliarity avails them as sources for metaphors.
For essays on communication, topics like sports
and architecture are unlikely to be discussed; yet
metaphors from these domains can be used, such
as leveling of the playing field through cheap and
fast communications or buildling bridges across
cultures through the internet.
In future work, we intend to add features that
capture the relationship between the current word
and its immediate context, as well as add essays
from additional prompts to build a more topically
diverse set for exploration of cross-topic generali-
zation of our models for essay data.
15
References
Beata Beigman Klebanov and Michael Flor. 2013.
Argumentation-relevant metaphors in test-taker es-
says. In Proceedings of the First Workshop on
Metaphor in NLP, pages 11?20, Atlanta, Georgia,
June. Association for Computational Linguistics.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing disagreements. In
COLING 2008 workshop on Human Judgments in
Computational Linguistics, pages 2?7, Manchester,
UK.
Steven Bethard, Vicky Tzuyin Lai, and James Martin.
2009. Topic model analysis of metaphor frequency
for psycholinguistic stimuli. In Proceedings of the
Workshop on Computational Approaches to Linguis-
tic Creativity, CALC ?09, pages 9?16, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Steven Bird. 2006. NLTK: The natural language
toolkit. In Proceedings of the ACL, Interactive Pre-
sentations, pages 69?72.
Daniel Blanchard, Michael Heilman,
and Nitin Madnani. 2013. SciKit-
Learn Laboratory. GitHub repository,
https://github.com/EducationalTestingService/skll.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Marc Brysbaert, Amy Beth Warriner, and Victor Ku-
perman. 2013. Concreteness ratings for 40 thou-
sand generally known english word lemmas. Behav-
ior Research Methods, pages 1?8.
Jonathan Charteris-Black. 2005. Politicians and
rhetoric: The persuasive power of metaphors. Pal-
grave MacMillan, Houndmills, UK and New York.
Jonathan Dunn. 2013. What metaphor identification
systems can tell us about metaphor-in-language. In
Proceedings of the First Workshop on Metaphor in
NLP, pages 1?10, Atlanta, Georgia, June. Associa-
tion for Computational Linguistics.
Dan Fass. 1991. Met*: A method for discriminating
metonymy and metaphor by computer. Computa-
tional Linguistics, 17(1):49?90.
William Gale, Kenneth Church, and David Yarowsky.
1992. One sense per discourse. In Proceedings of
the Speech and Natural Language Workshop, pages
233?237.
Matt Gedigan, John Bryant, Srini Narayanan, and Bra-
nimir Ciric. 2006. Catching metaphors. In Pro-
ceedings of the 3rd Workshop on Scalable Natural
Language Understanding, pages 41?48, New York.
Andrew Goatly. 1997. The Language of Metaphors.
Routledge, London.
Ilana Heintz, Ryan Gabbard, Mahesh Srivastava, Dave
Barner, Donald Black, Majorie Friedman, and Ralph
Weischedel. 2013. Automatic Extraction of Lin-
guistic Metaphors with LDA Topic Modeling. In
Proceedings of the First Workshop on Metaphor in
NLP, pages 58?66, Atlanta, Georgia, June. Associa-
tion for Computational Linguistics.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. In Pro-
ceedings of the First Workshop on Metaphor in NLP,
pages 52?57, Atlanta, GA. Association for Compu-
tational Linguistics.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of HLT-
EMNLP, pages 419?426, Vancouver, Canada. Asso-
ciation for Computational Linguistics.
George Lakoff and Mark Johnson. 1980. Metaphors
we live by. University of Chicago Press, Chicago.
Hongsong Li, Kenny Q. Zhu, and Haixun Wang. 2013.
Data-driven metaphor recognition and explanation.
Transactions of the ACL, 1:379?390.
James Martin. 1990. A computational model of
metaphor interpretation. Academic Press Profes-
sional, Inc., San Diego, CA, USA.
Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika, 12(2):153?157.
Michael Mohler, David Bracewell, Marc Tomlinson,
and David Hinote. 2013. Semantic signatures for
example-based linguistic metaphor detection. In
Proceedings of the First Workshop on Metaphor in
NLP, pages 27?35, Atlanta, GA. Association for
Computational Linguistics.
Andreas Musolff. 2000. Mirror images of Eu-
rope: Metaphors in the public debate about
Europe in Britain and Germany. Mu?nchen:
Iudicium. Annotated data is available at
http://www.dur.ac.uk/andreas.musolff/Arcindex.htm.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth
Edition LDC2009T13. Linguistic Data Consortium,
Philadelphia.
Bradley Pasanek and D. Sculley. 2008. Mining mil-
lions of metaphors. Literary and Linguistic Com-
puting, 23(3):345?360.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake VanderPlas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
16
Group Pragglejaz. 2007. MIP: A Method for Iden-
tifying Metaphorically Used Words in Discourse.
Metaphor and Symbol, 22(1):1?39.
Radim
?
Reh?u?rek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45?
50, Valletta, Malta, May. ELRA.
Evan Sandhaus. 2008. The New York Times Anno-
tated Corpus. LDC Catalog No: LDC2008T19.
Ekaterina Shutova and Lin Sun. 2013. Unsu-
pervised metaphor identification using hierarchical
graph factorization clustering. In Proceedings of
HLT-NAACL, pages 978?988.
Ekaterina Shutova and Simone Teufel. 2010.
Metaphor corpus annotated for source - target do-
main mappings. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike Ros-
ner, and Daniel Tapias, editors, Proceedings of the
Seventh International Conference on Language Re-
sources and Evaluation (LREC?10), pages 3255?
3261, Valletta, Malta, May. European Language Re-
sources Association (ELRA).
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proceedings of the 23rd International
Conference on Computational Linguistics (COL-
ING), pages 1002?1010.
Gerard Steen, Aletta Dorst, Berenike Herrmann, Anna
Kaal, Tina Krennmayr, and Trijntje Pasma. 2010. A
Method for Linguistic Metaphor Identification. Am-
sterdam: John Benjamins.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of NAACL, pages 252?259.
Peter Turney, Yair Neuman, Dan Assaf, and Yohai Co-
hen. 2011. Literal and metaphorical sense identi-
fication through concrete and abstract context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 680?
690, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
17
