Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 13?20,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Corpus Creation for New Genres:
A Crowdsourced Approach to PP Attachment
Mukund Jha, Jacob Andreas, Kapil Thadani, Sara Rosenthal and Kathleen McKeown
Department of Computer Science
Columbia University
New York, NY 10027, USA
{mj2472,jda2129}@columbia.edu, {kapil,sara,kathy}@cs.columbia.edu
Abstract
This paper explores the task of building an ac-
curate prepositional phrase attachment corpus
for new genres while avoiding a large invest-
ment in terms of time and money by crowd-
sourcing judgments. We develop and present
a system to extract prepositional phrases and
their potential attachments from ungrammati-
cal and informal sentences and pose the subse-
quent disambiguation tasks as multiple choice
questions to workers from Amazon?s Mechan-
ical Turk service. Our analysis shows that
this two-step approach is capable of producing
reliable annotations on informal and poten-
tially noisy blog text, and this semi-automated
strategy holds promise for similar annotation
projects in new genres.
1 Introduction
Recent decades have seen rapid development in nat-
ural language processing tools for parsing, semantic
role-labeling, machine translation, etc., and much of
this success can be attributed to the study of statisti-
cal techniques and the availability of large annotated
corpora for training. However, the performance of
these systems is heavily dependent on the domain
and genre of their training data, i.e. systems trained
on data from a particular domain tend to perform
poorly when applied to other domains and adap-
tation techniques are not always able to compen-
sate (Dredze et al, 2007). For this reason, achiev-
ing high performance on new domains and genres
frequently necessitates the collection of annotated
training data from those domains and genres, a time-
consuming and frequently expensive process.
This paper examines the problem of collecting
high-quality annotations for new genres with a focus
on time and cost efficiency. We explore the well-
studied but non-trivial task of prepositional phrase
(PP) attachment and describe a semi-automated sys-
tem for identifying accurate attachments in blog
data, which is frequently noisy and difficult to parse.
PP attachment disambiguation involves finding a
correct attachment for a prepositional phrase in a
sentence. For example, in the sentence ?We went to
John?s house on Saturday?, the phrase ?on Satur-
day? attaches to the verb ?went?. In another exam-
ple, ?We went to John?s house on 12th Street?, the
PP ?on 12th street? attaches to the noun ?John?s
house?. This sort of disambiguation requires se-
mantic knowledge about sentences that is difficult
to glean from their surface form, a problem which
is compounded by the informal nature and irregular
vocabulary of blog text.
In this work, we investigate whether crowd-
sourced human judgments are capable of distin-
guishing appropriate attachments. We present a sys-
tem that simplifies the attachment problem and rep-
resents it in a format that can be intuitively tackled
by humans.
Our approach to this task makes use of a heuristic-
based system built on a shallow parser that identi-
fies the likely words or phrases to which a PP can
attach. To subsequently select the correct attach-
ment, we leverage human judgments from multi-
ple untrained annotators (referred to here as work-
ers) through Amazon?s Mechanical Turk 1, an online
marketplace for work. This two-step approach of-
1http://www.mturk.amazon.com
13
fers distinct advantages: the automated system cuts
down the space of potential attachments effectively
with little error, and the disambiguation task can be
reduced to small multiple choice questions which
can be tackled quickly and aggregated reliably.
The remainder of this paper focuses on the PP at-
tachment task over blog text and our analysis of the
resulting aggregate annotations. We note, however,
that this type of semi-automated approach is poten-
tially applicable to any task which can be reliably
decomposed into independent judgments that un-
trained annotators can tackle (e.g., quantifier scop-
ing, conjunction scope). This work is intended as
an initial step towards the development of efficient
hybrid annotation tools that seamlessly incorporate
aggregate human wisdom alongside effective algo-
rithms.
2 Related Work
Identifying PP attachments is an essential task for
building syntactic parse trees. While this task has
been studied using fully-automated systems, many
of them rely on parse tree output for predicting po-
tential attachments (Ratnaparkhi et al, 1994; Yeh
and Vilain, 1998; Stetina and Nagao, 1997; Zavrel
et al, 1997). However, systems that rely on good
parses are unlikely to perform well on new genres
such as blogs and machine translated texts for which
parse tree training data is not readily available.
Furthermore, the predominant dataset for eval-
uating PP attachment is the RRR dataset (Ratna-
parkhi et al, 1994) which consists of PP attach-
ment cases from the Wall Street Journal portion of
the Penn Treebank. Instead of complete sentences,
this dataset consists of sets of the form {V,N1,P,N2}
where {P,N2} is the PP and {V,N1} are the poten-
tial attachments. This simplification of the PP at-
tachment task to a choice between two alternatives
is unrealistic when considering the potential long-
distance attachments encountered in real-world text.
While blogs and other web text, such as discus-
sion forums and emails, have been studied for a va-
riety of tasks such as information extraction (Hong
and Davison, 2009), social networking (Gruhl et
al., 2004), and sentiment analysis (Leshed and
Kaye, 2006), we are not aware of any previous ef-
forts to gather syntactic data (such as PP attach-
ments) in the genre. Syntactic methods such as
POS tagging, parsing and structural disambiguation
are commonly used when analyzing well-structured
text. Including the use of syntactic information
has yielded improvements in accuracy in speech
recognition (Chelba and Jelenik, 1998; Collins et
al., 2005) and machine translation (DeNeefe and
Knight, 2009; Carreras and Collins, 2009). We an-
ticipate that datasets such as ours could be useful for
such tasks as well.
Amazon?s Mechanical Turk (MTurk) has become
very popular for manual annotation tasks and has
been shown to perform equally well over labeling
tasks such as affect recognition, word similarity, rec-
ognizing textual entailment, event temporal order-
ing and word sense disambiguation, when compared
to annotations from experts (Snow et al, 2008).
While these tasks were small in scale and intended to
demonstrate the viability of annotation via MTurk,
it has also proved effective in large-scale tasks in-
cluding the collection of accurate speech transcrip-
tions (Gruenstein et al, 2009). In this paper we ex-
plore a method for corpus building on a large scale
in order to extend annotation into new domains and
genres.
We previously evaluated crowdsourced PP attach-
ment annotation by using MTurk workers to repro-
duce PP attachments from the Wall Street Journal
corpus (Rosenthal et al, 2010). The results demon-
strated that MTurk workers are capable of identi-
fying PP attachments in newswire text, but the ap-
proach used to generate attachment options is de-
pendent on the existing gold-standard parse trees
and cannot be used on corpora where parse trees are
not available. In this paper, we build on the semi-
automated annotation principle while avoiding the
dependency on parsers, allowing us to apply this
technique to the noisy and informal text found in
blogs.
3 System Description
Our system must both identify PPs and generate a
list of potential attachments for each PP in this sec-
tion. Figure 1 illustrates the structure of the system.
First, the system extracts sentences from scraped
blog data. Text is preprocessed by stripping HTML
tags, advertisements, non-Latin and non-printable
14
PPs
PPs
Question 
 Builder
PP Identifier
Chunker
+
Preprocessor
sentences
Chunked 
Chunked 
sentences
point predictor
Attachment
attachments
Potential
Mechanical
     Turk
Questions
forNew domain
data (Blogs)
Figure 1: Overview of question generation system
characters. Emoticon symbols are removed using a
standard list. 2
The cleaned data is then partitioned into sentences
using the NLTK sentence splitter. 3 In order to
compensate for the common occurrence of informal
punctuation and web-specific symbols in blog text,
we replace all punctuation symbols between quo-
tation marks and parentheses with placeholder tags
(e.g. ?QuestionMark?) during the sentence splitting
process and do the same for website names, time
markers and referring phrases (e.g. @John). Ad-
ditionally, we attempt to re-split sentences at ellipsis
boundaries if they are longer than 80 words and dis-
card them if this fails.
As parsers trained on news corpora tend to per-
form poorly on unstructured texts like blogs, we
rely on a chunker to partition sentences into phrases.
Choosing a good chunker is essential to this ap-
proach: around 35% of the cases in which the cor-
rect attachment is not predicted by the system are
due to chunker error. We experimented with differ-
ent chunkers over a random sample of 50 sentences
before selecting a CRF-based chunker (Phan, 2006)
for its robust performance.
The chunker output is initially processed by fus-
ing together chunks in order to ensure that a single
chunk represents a complete attachment point. Two
consecutive NP chunks are fused if the first contains
an element with a possessive part of speech tag (e.g.
John?s book), while particle chunks (PRT) are fused
with the VP chunks that precede them (e.g. pack
up). These chunked sentences are then processed
to identify PPs and potential attachment points for
them, which can then be used to generate questions
2http://www.astro.umd.edu/?marshall/
smileys.html
3http://www.nltk.org
for MTurk workers.
3.1 PP Extraction
PPs can be classified into two broad categories based
on the number of chunks they contain. A simple
PP consists of only two chunks: a preposition and
one noun phrase, while a compound PP has multi-
ple simple PPs attached to its primary noun phrase.
For example, in the sentence ?I just made some last-
minute changes to the latest issue of our newsletter?,
the PP with preposition ?to? can be considered to be
either the simple PP ?to the latest issue? or the com-
pound PP ?to the latest issue of our newsletter?.
We handle compound PPs by breaking them down
into multiple simple PPs; compound PPs can be re-
covered by identifying the attachments of their con-
stituent simple PPs. Our simple PP extraction al-
gorithm identifies PPs as a sequence of chunks that
consist of one or more prepositions terminating in a
noun phrase or gerund.
3.2 Attachment Point Prediction
A PP usually attaches to the noun or verb phrase pre-
ceding it or, in some cases, can modify a following
clause by attaching to the head verb. We build a set
of rules based on this intuition to pick out the poten-
tial attachments in the sentence; these rules are de-
scribed in Table 1. The rules are applied separately
for each PP in a sentence and in the same sequence
as mentioned in the table (except for rule 4, which
is applied while choosing a chunk using any of the
other rules).
15
Rule Example
1 Choose closest NP and VP preceding the PP. I made modifications to our newsletter.
2 Choose next closest VP preceding the PP if the VP selected in (1)
contains a VBG.
He snatched the disk flying away with one hand.
3 Choose first VP following the PP. On his desk he has a photograph.
4 All chunks inside parentheses are skipped, unless the PP falls within
parentheses.
Please refer to the new book (second edition) for
more notes.
5 Choose anything immediately preceding the PP that is not out of
chunk and has not already been picked.
She is full of excitement.
6 If a selected NP contains the word and, expand it into two options,
one with the full expression and one with only the terms following
and.
He is president and chairman of the board.
7 For PPs in chains of the form P-NP-P-NP (PP-PP), choose all the
NPs in the chain preceding the PP and apply all the above rules
considering the whole chain as a single PP.
They found my pictures of them from the concert.
8 If there are fewer than four options after applying the above rules,
also select the VP preceding the last VP selected, the NP preceding
the last NP selected, and the VP following the last VP picked.
Table 1: List of rules for attachment point predictor. In the examples, PPs are denoted by boldfaced text and potential
attachment options are underlined.
4 Experiments
An experimental study was undertaken to test our
hypothesis that we could obtain reliable annotations
on informal genres using MTurk workers. Here we
describe the dataset and our methods.
4.1 Dataset and Interface
We used a corpus of blog posts made on LiveJour-
nal 4 for system development and evaluation. Only
posts from English-speaking countries (i.e. USA,
Canada, UK, Australia and New Zealand) were con-
sidered for this study.
The interface provided to MTurk workers showed
the sentence on a plain background with the PP high-
lighted and a statement prompting them to pick the
phrase in the sentence that the given PP modified.
The question was followed by a list of options. In
addition, we provided MTurk workers the option to
indicate problems with the given PP or the listed op-
tions. Workers could write in the correct attachment
if they determined that it wasn?t present in the list of
options, or the correct PP if the one they were pre-
sented with was malformed. This allowed them to
correct errors made by the chunker and automated
attachment point predictor. In all cases, workers
were forced to pick the best answer among the op-
tions regardless of errors. We also supplied a num-
4http://www.livejournal.com
ber of examples covering both well-formed and er-
roneous cases to aid them in identifying appropriate
attachments.
4.2 Experimental Setup
For our experiment, we randomly selected 1000
questions from the output produced by the system
and provided each question to five different MTurk
workers, thereby obtaining five different judgments
for each PP attachment case. Workers were paid four
cents per question and the average completion time
per task was 48 seconds. In total $225 was spent
on the full study with $200 spent on the workers and
$25 on MTurk fees.The total time taken for the study
was approximately 16 hours.
A pilot study was carried out with 50 sentences
before the full study to test the annotation interface
and experiment with different ways of presenting the
PP and attachment options to workers. During this
study, we observed that while workers were will-
ing to suggest correct answers or PPs when faced
with erroneous questions, they often opted to not
pick any of the options provided unless the question
was well-formed. This was problematic because, in
many cases, expert annotators were able to identify
the most appropriate attachment option. Therefore,
in the final study we forced them to pick the most
suitable option from the given choices before indi-
cating errors and writing in alternatives.
16
Workers in agreement Number of questions Accuracy Coverage
5 (unanimity) 389 97.43% 41.33%
? 4 (majority) 689 94.63% 73.22%
? 3 (majority) 887 88.61% 94.26%
? 2 (plurality) 906 87.75% 96.28%
Total 941 84.48% 100%
Table 2: Accuracy and coverage over agreement thresholds
5 Evaluation corpus
In order to determine if the MTurk results were re-
liable, worker responses had to be validated by hav-
ing expert annotators perform the same task. For
this purpose, two of the authors annotated the 1000
questions used for the experiment independently and
compared their judgments. Disagreements were ob-
served in 127 cases; these were then resolved by a
pool of non-author annotators. If all three annota-
tors on a case disagreed with each other the question
was discarded; this situation occured 43 times. An
additional 16 questions were discarded because they
did not have a valid PP. For example, ?I am painting
with my blanket on today?. Here ?on today? is in-
correctly extracted as a PP because the particle ?on?
is tagged as a preposition. The rest of the analysis
presented in this section was performed on the re-
maining 941 sentences.
The annotators? judgments were compared to the
answers provided by the MTurk workers and, in
the case of disagreement between the experts and
the majority of workers, the sentences were man-
ually inspected to determine the reason. In five
cases, more than one valid attachment was possi-
ble; for example, in the sentence ?The video below is
of my favourite song on the album - A Real Woman?,
the PP ?of my favourite song? could attach to either
the noun phrase ?the video? or the verb ?is? and con-
veys the same meaning. In such cases, both the ex-
perts and the workers were considered to have cho-
sen the correct answer.
In 149 cases, the workers also augmented their
choices by providing corrections to incomplete an-
swers and badly constructed PPs. For example,
the PP ?of the Rings and Mikey? in the sentence
?Samwise from Lord of the Rings and Mikey from
The Goonies are the same actor ?? was corrected to
?of the Rings?. In 34/39 of the cases where the cor-
rect answer was not present in the options provided,
at least one worker indicated correct attachment for
the PP.
5.1 Attachment Prediction Evaluation
We measure the recall for our attachment point pre-
dictor as the number of questions for which the cor-
rect attachment appeared among the generated op-
tions divided by the total number of questions. The
system achieves a recall of 95.85% (902/941 ques-
tions). We observed that in many cases where the
correct attachment point was not predicted, it was
due to a chunker error. For example, in the following
sentence, ?Stop all the clocks , cut off the telephone
, Prevent the dog from barking with a juicy bone...?,
the PP ?from barking? attaches to the verb ?Pre-
vent?; however, due to an error in chunking ?Pre-
vent? is tagged as a noun phrase and hence is not
picked by our system. The correct attachment was
also occasionally missed when the attachment point
was too far from the PP. For example, in the sentence
?Fitting as many people as possible on one sofa and
under many many covers and getting intimate?, the
correct attachment for the PP ?under many many
covers? is the verb ?Fitting? but it is not picked by
our system.
Even though the correct attachment was not al-
ways given, the workers could still provide their own
correct answer. In the first example above, 3/5 work-
ers indicated that the correct attachment was not in
the list of options and wrote it in.
6 Results
Table 2 summarizes the results of the experiment.
We assess both the coverage and reliability of
worker predictions at various levels of worker agree-
ment. This serves as an indicator of the effective-
ness of the MTurk results: the accuracy can be taken
17
Figure 2: The number of questions in which exactly x
workers provided the correct answer
as a general confidence measure for worker predic-
tions; when five workers agree we can be 97.43%
confident in the correctness of their prediction, when
at least four workers agree we can be 94.63% con-
fident, etc. Unanimity indicates that all workers
agreed on an answer, majority indicates that more
than half of workers agreed on an answer, and plu-
rality indicates that two workers agreed on a single
answer, while the remaining three workers each se-
lected different answers. We observe that at high
levels of worker agreement, we get extremely high
accuracy but limited coverage of the data set; as
we decrease our standard for agreement, coverage
increases rapidly while accuracy remains relatively
high.
Figure 2 shows the number of workers providing
the correct answer on a per-question basis. This
illustrates the distribution of worker agreements
across questions. Note that in the majority of cases
(69.2%), at least four workers provided the correct
answer; in only 3.6% of cases were no workers able
to select the correct attachment.
Figure 3 shows the distribution of worker agree-
ments. Unlike Table 2, these figures are not cumu-
lative and include non-plurality two-worker agree-
ments. Note that the number of agreements dis-
cussed in this figure is greater than the 941 evaluated
because in some cases there were multiple agree-
ments on a single question. As an example, three
workers may choose one answer while the remain-
ing two workers choose another; this question then
produces both a three-worker agreement as well as a
two-worker agreement.
Figure 3: The number of cases in which exactly x work-
ers agreed on an answer
No. of options No. of cases Accuracy
< 4 179 86.59%
4 718 84.26%
> 4 44 79.55%
Table 3: Variation in worker performance with the num-
ber of attachment options presented
All questions on which there is agreement also
produce a majority vote, with one exception: the
2/2/1 agreement. Although the correct answer was
selected by one set of two workers in every case of
2/2/1 agreement, this is not particularly useful for
corpus-building as we have no way to identify a pri-
ori which set is correct. Fortunately, 2/2/1 agree-
ments were also quite rare and occurred in only 3%
of cases.
Figure 3 appears to indicate that instances of
agreement between two workers are unlikely to pro-
duce good attachments; they have a an average ac-
curacy of 37.2%. However, this is due in large part
to cases of 3/2 agreement, in which the two workers
in the minority are usually wrong, as well as cases of
2/2/1 agreement which contain at least one incorrect
instance of two-worker agreement. However, if we
only consider cases in which the two-worker agree-
ment forms a plurality (i.e. all other workers dis-
agree amongst themselves), we observe an average
accuracy of 64.3% which is similar to that of cases
of three-worker agreement (67.7%).
We also attempted to study the variation in worker
performance based on the complexity of the task;
specifically looking at how response accuracy var-
ied depending on the number of options that workers
were presented with. Although our system aimed to
18
Figure 4: Variation in accuracy with sentence length.
generate four attachment options per case, fewer op-
tions were produced for small sentences and opening
PPs while additional options were generated in sen-
tences containing PP-NP chains (see Table 1 for the
complete list of rules). Table 3 shows the variation in
accuracy with the number of options provided to the
workers. We might expect that an increased number
of options may be correlated with decreased accu-
racy and the data does indeed seem to suggest this
trend; however, we do not have enough datapoints
for the cases with fewer or more than four options to
verify whether this effect is significant.
We also analyzed the relationship between the
length of the sentence (in terms of number of words)
and the accuracy. Figure 4 indicates that as the
length of the sentence increases, the average accu-
racy decreases. This is not entirely unexpected as
lengthy sentences tend to be more complicated and
therefore harder for human readers to parse.
7 Conclusions and Future Work
We have shown that by working in conjunction
with automated attachment point prediction sys-
tems, MTurk workers are capable of annotating PP
attachment problems with high accuracy, even when
working with unstructured and informal blog text.
This work provides an immediate framework for the
building of PP attachment corpora for new genres
without a dependency on full parsing.
More broadly, the semi-automated framework
outlined in this paper is not limited to the task of
annotating PP attachments; indeed, it is suitable for
almost any syntactic or semantic annotation task
where untrained human workers can be presented
with a limited number of options for selection. By
dividing the desired annotation task into smaller
sub-tasks that can be tackled independently or in a
pipelined manner, we anticipate that more syntac-
tic information can be extracted from unstructured
text in new domains and genres without the sizable
investment of time and money normally associated
with hiring trained linguists to build new corpora.
To this end, we intend to further leverage the advent
of crowdsourcing resources in order to tackle more
sophisticated annotation tasks.
Acknowledgements
The authors would like to thank Kevin Lerman for
his help in formulating the original ideas for this
work. This material is based on research supported
in part by the U.S. National Science Foundation
(NSF) under IIS-05-34871. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the authors and do not nec-
essarily reflect the views of the NSF.
References
Xavier Carreras and Michael Collins. 2009. Non-
projective parsing for statistical machine translation.
In Proceedings of EMNLP, pages 200?209.
Ciprian Chelba and Frederick Jelenik. 1998. Structured
language modeling for speech recognition. In Pro-
ceedings of NLDB.
Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling for
speech recognition. In Proceedings of ACL, pages
507?514.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proceedings of
EMNLP, pages 727?736.
Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuz-
man Ganchev, Joa?o Graca, and Fernando Pereira.
2007. Frustratingly hard domain adaptation for depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL, pages 1051?1055,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Alex Gruenstein, Ian McGraw, and Andrew Sutherland.
2009. A self-transcribing speech corpus: collecting
continuous speech with an online educational game.
In Proceedings of the Speech and Language Technol-
ogy in Education (SLaTE) Workshop.
19
Figure 5: HIT Interface for PP attachment task
Daniel Gruhl, R. Guha, David Liben-Nowell, and An-
drew Tomkins. 2004. Information diffusion through
blogspace. In Proceedings of WWW, pages 491?501.
Liangjie Hong and Brian D. Davison. 2009. A
classification-based approach to question answering in
discussion boards. In Proceedings of SIGIR, pages
171?178.
Gilly Leshed and Joseph ?Jofish? Kaye. 2006. Under-
standing how bloggers feel: recognizing affect in blog
posts. In CHI ?06 extended abstracts on Human fac-
tors in computing systems, pages 1019?1024.
Xuan-Hieu Phan. 2006. CRFChunker: CRF
English phrase chunker. http://crfchunker.
sourceforge.net.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994. A maximum entropy model for prepositional
phrase attachment. In Proceedings of HLT, pages 250?
255.
Sara Rosenthal, William J. Lipovsky, Kathleen McKe-
own, Kapil Thadani, and Jacob Andreas. 2010. Semi-
automated annotation for prepositional phrase attach-
ment. In Proceedings of LREC, Valletta, Malta.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of EMNLP, pages 254?263.
Jiri Stetina and Makoto Nagao. 1997. Corpus based PP
attachment ambiguity resolution with a semantic dic-
tionary. In Proceedings of the Workshop on Very Large
Corpora, pages 66?80.
Alexander S. Yeh and Marc B. Vilain. 1998. Some prop-
erties of preposition and subordinate conjunction at-
tachments. In Proceedings of COLING, pages 1436?
1442.
Jakub Zavrel, Walter Daelemans, and Jorn Veenstra.
1997. Resolving PP attachment ambiguities with
memory-based learning. In Proceedings of the Work-
shop on Computational Language Learning (CoNLL),
pages 136?144.
Appendix A: Mechanical Turk Interface
Figure 5 shows a screenshot of the interface pro-
vided to the Mechanical Turk workers for the PP at-
tachment task. By default, examples and additional
options are hidden but can be viewed using the links
provided. The screenshot illustrates a case in which
a worker is confronted with an incorrect PP and uses
the additional options to correct it.
20
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 64?71,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Cancer Stage Prediction Based on Patient Online Discourse
Mukund Jha
Computer Science
Columbia University
New York, NY 10027
mj2472@columbia.edu
Noe?mie Elhadad
Biomedical Informatics
Columbia University
New York, NY 10032
noemie@dbmi.columbia.edu
Abstract
Forums and mailing lists dedicated to par-
ticular diseases are increasingly popular
online. Automatically inferring the health
status of a patient can be useful for both
forum users and health researchers who
study patients? online behaviors. In this
paper, we focus on breast cancer forums
and present a method to predict the stage
of patients? cancers from their online dis-
course. We show that what the patients
talk about (content-based features) and
whom they interact with (social network-
based features) provide complementary
cues to predicting cancer stage and can be
leveraged for better prediction. Our meth-
ods are extendable and can be applied to
other tasks of acquiring contextual infor-
mation about online health forum partici-
pants.
1 Introduction
In this paper we investigate an automated method
of inferring the stage of a patient?s breast cancer
from discourse in an online forum. Such informa-
tion can prove invaluable both for forummembers,
by enriching their use of this rapidly developing
and increasingly popular medium, and for health
researchers, by providing them with tools to quan-
tify and better understand patient populations and
how they behave online.
Patients with chronic diseases like diabetes or
life-threatening conditions like breast cancer get
a wealth of information from medical profession-
als about their diagnoses, test results, and treat-
ment options, but such information is not always
satisfactory or sufficient for patients. Much of
that is essential to their everyday lives and the
management of their condition escapes the clin-
ical realm. Furthermore, patients feel informed
and empowered by exchanging experiences and
emotional support with others in the same circum-
stances. Thus, it is not surprising that patient com-
munities have flourished on the Web over the past
decade, through active disease-specific discussion
forums and mailing lists.
For health professionals, this new medium
presents exciting research avenues related to the-
ories of psycho-social support and how patients
manage their conditions. Qualitative analyses of
forums and mailing list posts show that breast
cancer patients and survivors provide and seek
support to and from their peers and that support,
while also emotional, is largely informational in
nature (Civan and Pratt, 2007; Meier et al, 2007).
Emotional support may include words of encour-
agement and prayers. Examples of informational
support are providing personal experiences with a
treatment, discussing new research, explaining a
pathology report to a peer, as well as exchanging
information pertinent to patients? daily lives, such
as whether to shave one?s head once chemotherapy
starts.
Given the kinds of benefits that patients and sur-
vivors seek and provide in online forums, it seems
likely that they would be inclined to gravitate to-
ward others whose circumstances most closely re-
semble their own, beyond sharing the general di-
agnosis of breast cancer. In fact, focus groups
and surveys conducted with breast cancer patients
identified and emphasized the need for online can-
cer forum participants to identify other patients of
a particular age, stage of illness, or having opted
for similar treatment (Rozmovits and Ziebland,
2004; van Uden-Kraan et al, 2008).
The stage of a patient?s cancer, in particular, can
be a crucial proxy for finding those whose experi-
ences are likely similar and relevant to one?s own.
For breast cancer, there are five high-level stan-
dard stages (0 to IV). While they do not give the
whole picture about a particular cancer (the stages
64
themselves can be described with finer granular-
ity and they do no not encompass additional in-
formation like hormonal sensitivity), physicians
have traditionally relied on them for prognosis and
determining treatment options. For patients and
survivors, they are a useful way to communicate
to their peers their health status, as evidenced by
the members? signatures on forums and mailing
lists (Meier et al, 2007).
Although many forums provide pre-set profile
fields for users to populate with important back-
ground information, such as the stage of their can-
cer (e.g., the popular forum on breastcancer.
org), in practice, only a fraction of members have
a complete profile. Thus, an automated way of in-
ferring member profile information via the social
network created by a forum?s users would help fill
in the blanks.
Beyond identifying other patients in a forum in
similar circumstances, such a tool can have nu-
merous practical benefits for both forum users and
health researchers who study patients? online be-
havior. When a patient searches for a particu-
lar piece of information in a forum, incorporat-
ing contextual information about the user into the
search mechanism can improve search results. For
example, a search tool can rank higher the posts
that were authored by patients with the same stage.
For health researchers, questions which bring a
better understanding of forum usage (i.e., ?are pa-
tients with stage IV cancer more or less active in a
forum than patients with early stage cancer?) can
be answered accurately only if all members of the
forums are taken into account, not just the ones
who filled out their member profiles. Furthermore,
in the context of health communication, the more
information is available about an individual, the
more effective the message can be, from generic
to personalized to targetted to tailored (Kreuter et
al., 2000). Our research contributes an automated
method to acquiring contextual information about
forum participants. We focus on cancer stage as
an exmple of context information.
Our research question is whether it is possible
to predict the stage of individuals? cancer based on
their online discourse. By discourse we mean both
the information she conveys and whom she talks
to in a forum. Following ethical guidelines in pro-
cessing of patient data online, we focus on a pop-
ular breast cancer forum with a large number of
participants (Eysenbach and Till, 2001). We show
that the content of members? posts and the stage
of their interlocutors can provide complementary
clues to identifying cancer stages.
2 Related Work
Researchers have begun to explore the possibility
of diagnosing patients based on their speech pro-
ductions. Content analysis methods, which rely on
patient speech transcripts or texts authored by pa-
tients, have been leveraged for understanding can-
cer coping mechanisms (Graves et al, 2005; Ban-
tum and Owen, 2009), psychiatric diagnoses (Ox-
man et al, 1988; Elvevaag et al, 2010), and the
analysis of suicide notes (Pestian et al, 2008).
In all cases, results, while not fully accurate, are
promising and show that patient-generated con-
tent is a valuable clue to diagnosis in an automated
framework.
Our work departs from these experiments in that
we do not attempt to predict the psychological
state of a patient, but rather the status of a clinical
condition. Staging breast cancer provides a way to
summarize the status of the cancer based on clin-
ical characteristics (the size of the tumor, whether
the cancer is invasive or not, whether cancer cells
are present in the lymph nodes, and whether the
cancer has spread beyond the breast). There are
five high-level stages for breast cancer. Stage 0
describes a non-invasive cancer. Stage I represents
early stage of an invasive cancer, where the tumor
size is less than 2 centimeters and no lymph nodes
are involved (that is, the cancer has not spread out-
side of the breast). Stages II and III describe a can-
cer with larger tumor size and/or the cancer has
spread outside of the breast. Stage IV describes
a cancer that have metastasized to distant parts of
the body, such as lungs and bones.
In our work, we analyze naturally occurring
content, generated by patients talking to each other
online. As such, our sample population is much
larger than in earlier works (typically less than 100
subjects). Like the researchers who focus on con-
tent analysis, we rely on the content generated by
patients, but we also hypothesize that whom the
patients interact with can help the prediction of
cancer stage.
In particular, we build a social network based
on patients? interactions to boost text-based pre-
dictions. Graph-based methods are becoming
increasingly popular in the NLP community,
and similar approaches have been employed and
65
shown to perform well in other areas like ques-
tion answering (Jurczyk, 2007) (Harabagiu et al,
2006), word-sense disambiguation (Niu et al,
2005), and textual entailment (Haghighi, 2005).
3 Methods
Our methods to predict cancer stage operate in a
supervised framework. We cast the task of stage
prediction as a 4-way classification (Stage I to IV).
We hypothesize that the discourse of patients on-
line, as defined by the content of their posts in a
forum, can be leveraged to predict cancer stage.
Furthermore, we hypothesize that the social net-
work derived by whom patients interact with can
provide an additional clue for stage detection.
We experimented with three methods of predict-
ing cancer stage:
Text-based stage prediction A classifier is
trained given the post history of a patient.
Network-based stage prediction A social net-
work representing the interactions among fo-
rummembers is built, and a label propagation
algorithm is applied to infer the stage of indi-
vidual patients.
Combined prediction A classifier which com-
bines text-based and network-based features.
Next we describe each method in detail, along
with our dataset and our experimental setup.
3.1 Data Collection and Preprocessing
We collected posts from the publicly available dis-
cussion board from breastcancer.org. It is
a popular forum, with more than 60,000 regis-
tered members, and more than 50,000 threads dis-
cussed in 60 subforums. To collect our dataset,
we crawled the content of the most popular subfo-
rums.1
Collected posts were translated from HTML
into an XML format, keeping track of author id,
1There were 17 such subforums: ?Just Diagnosed,? ?Help
Me Get Through Treatment,? ?Surgery - Before, During, and
After,? ?Chemotherapy - Before, During and After,? ?Ra-
diation Therapy - Before, During and After,? ?Hormonal
Therapy - Before, During and After,? ?Alternative, Com-
plementary and Holistic Treatment,? ?Stage I and II Breast
Cancer,? ?Just Diagnosed with a Recurrence or Metastasis,?
?Stage III Breast Cancer,? ?Stage IV Breast Cancer Sur-
vivors,? ?HER2/neu Positive Breast Cancer,? ?Deperession,
Anxiety and Post Traumatic Stress Disorder,? ?Fitness and
Getting Back in Shape,? ?Healthy Recipes for Everyday Liv-
ing,? ?Recommend Your Resources,? ?Clinical Trials, Re-
search, News, and Study Results.?
Nb. of threads 26,160
Nb. of posts 524,247
Nb. of threads with < 20 posts 22,334
Nb. of users with profile Stage I 2,226
Nb. of users with profile Stage II 2,406
Nb. of users with profile Stage III 1,031
Nb. of users with profile Stage IV 749
Total Nb. of users with profile 6,412
Nb. of active users profiled Stage I 1,317
Nb. of active users profiled Stage II 1,400
Nb. of active users profiled Stage III 580
Nb. of active users profiled Stage IV 448
Total Nb. of active users with profile 3,745
Table 1: General statistics of the dataset.
thread id, position of the post in the thread, body of
the post, and signature of the author (which is kept
separated from the body of the post). The con-
tent of the posts was tokenized, lower-cased and
stemmed. Images, URLs, and stop words were re-
moved.
To post in breastcancer.org, users must
register. They have the option to enter a profile
with pre-set fields related to their breast cancer di-
agnosis; in particular cancer stage between stage
I and IV. We collected the list of members who
entered their stage information, thereby providing
us with an annotated set of patients with their cor-
responding cancer stage. Table 1 shows various
statistics for our dataset. Active users are defined
as members who have posted more than 50 words
overall in the forums. Note the low number of
user with profile information (approximately 10%
of the overall number of registered participants in
the forum).
3.2 Text-Based Stage Prediction
We trained a text-based classifier relying on the
full post history of each patient. The full post
history was concatenated. Signature information,
which is derived automatically from the patient?s
profile (and thus contains stage information) was
removed from the posts. The classifier relied on
unigrams and bigrams only. Table 2 shows statis-
tics about post history length, measured as number
of words authored by a forum member.
3.3 Network-Based Stage Prediction
We hypothesize that patients tend to interact in a
forum with patients with similar stage. To test this
66
Stages Min Max Average Median
I 4 609,608 8,429 3,123
II 2 353,731 8,142 3,112
III 8 211,655 9,297 3,189
IV 10 893,326 17,083 326
Table 2: Statistics about number of words in post
history.
?
2
12
2
1
5
IV
III
I
IV
3
Figure 1: Nodes in the social network of forum
member interaction.
hypothesis, we represent the interactions of the pa-
tients as a social network. The nodes in the net-
work represent patients, and an edge is present be-
tween two nodes if the patients interact with each
other, that is they are part of the same threads of-
ten. Weights on edges represent the degree of in-
teraction. Higher weight on an edge between two
forum members indicates they interact more often.
More precisely, we build an undirected, weighted
network, where the nodes representing training in-
stances are labeled with their provided stage infor-
mation and their labels are fixed. Figure 1 shows
an example of node and its immediate neighbors
in the network. Of his five neighbors, four repre-
sent training instances and have a fixed stage, and
one represents a user with an unknown stage.
A label propagation algorithm is applied to the
network, so that every node in the network is as-
signed a stage between I and IV (Raghavan et al,
2007). Given a node and its immediate neighbors,
it looks for the most frequent labels, taking into ac-
count the edge weights. In our example, the prop-
agated label for the central node will be stage IV.
This label, in turn, will be used to assign a label to
the other nodes. When building the social network
of interactions, we experimented with the follow-
ing parameters.
Nodes in the network. We experimented with
including all the forum members who participated
in a conversation thread. Thus, it includes all the
members, even the ones without a known cancer
stage. This resulted in a network of 15,035 forum
participants. This way, the network covers more
interactions among more users, but is very sparse
in its initial labeling (only the training instances
in the dataset of active members with a known la-
bel are labeled). The label propagation algorithm
assigns labels to all the nodes, but we test its ac-
curacy only on the test instances. We also ex-
perimented with including only the patients in the
training and testing sets, thereby reducing the size
of the network but also decreasing the sparsity of
the labeling. This resulted in a network of 3,305
nodes.2
Drawing edges in the network. An edge be-
tween two users indicate they are frequently in-
teracting. One crude way is to draw an edge be-
tween every user participating in the same thread,
this however does not provide an accurate picture
and hence does not yield good results. In our ap-
proach we draw an edge in two steps. First, since
threads are often long and can span over multiple
topics, we only draw an edge if the two individ-
uals? posts are within five posts of each other in
the thread. Second, we then look for any direct
references made by a user to another user in their
post. In forum threads, users usually make a di-
rect reference by either by explicitly referring to
each other using their real name or internet alases
or by quoting each other, i.e., repeating or stating
what the other user has mentioned in her post. For
example in ?Hey Dana, I went through the same
thing the first time I went to my doctor...?, the au-
thor of the post is referring to another user with
name ?Dana?. We rely on such explicit references
to build accurate graph.3 To find direct explicit ref-
erences, we search in every post of a thread for any
mention of names (real or aliases) of users partic-
ipating in the thread and if one is found we draw
an edge between them.
We observed that users refer to each other very
2This number of nodes is less than the numbers of over-
all active members in our gold standard because some active
members have either posted in threads with only one post or
with more than 20 posts.
3An alternative approach is to identify quotes in posts. In
our particular dataset, quotes did not occur often, and thus
were ignored when assessing the degree of interaction be-
tween two forum members.
67
frequently using their real names instead of inter-
net names (which are long and often arbitrary).
These are often hard to detect because no data is
present which link users? forum aliases to their
real name. We use following approach to extract
real names of the users.
Extracting real names. For every user, we ex-
tract the last ten words (signature) from every post
posted by the user and concatenate them after re-
moving all stop words and other common signa-
ture terms (like thanks, all the best, love, good luck
etc.) using a pre-compiled list. We then mine for
the most frequent name occurring in the concate-
nated text using standard list of names and extract-
ing capitalized words. We also experimented with
using Named Entity Recognizers, but our simple
rule based name extractor gave us better results
with higher precision. Finally, we map the ex-
tracted real name with the user?s alias and utilize
them to find direct references between posts.
Weights Computation. The weight of an edge
between two nodes represents the degree of inter-
action between two corresponding users (the more
often they communicate, the higher the weight).
Since the label propagation algorithm takes into
account the weighted frequency of neighboring
nodes, these weights are crucial. We compute
the weights in following manner: for each pair of
users with an existing edge (as determined above),
we iterate through their posts in common threads,
and add the cosine similarity score between the
two posts to the weight of the edge. For edges
made through direct references we add the high-
est cosine similarity score between any two pair of
posts in that particular thread. This way we weigh
higher the edges made through direct reference as
we are more confident about them.
The full network of all users (15,035 nodes)
had 480,051 edges, and the restricted network of
dataset users (3,305 nodes) had 28,152 edges.
3.4 Combining Text-Based and
Network-Based Predictions
To test the hypothesis that text-based and network-
based predictions model different aspects of pa-
tients and thus provide complementary cues to
stage prediction, we trained a classifier which in-
corporates text-based and network-based features.
The combined classifier contained the following
features: text-based predicted label, confidence
score of the text-based prediction, network-based
predicted label, percentage of immediate neigh-
bors in the network with a stage I label, stage II,
III and IV labels (neighbors in the network with
no labels do not contribute to the counts). For in-
stance, the central node in Figure 1 is assigned the
feature values 1/4, 0, 1/4 and 1/2 for the ratio of
stage I, II, III and IV neighbors.
3.5 Experimental Setup
Our dataset for the three models consisted of the
3,745 active members. For all the models, we fol-
low a five-fold stratified cross validation scheme.
The text-based classification was carried out with
BoosTexter (Schapire and Singer, 2000), trained
with 800 rounds of boosting. The label propaga-
tion on the social network was carried out in R.4
The final decision-tree classification was carried
out in Weka, relying on an SVM classifier with
default parameters (Hall et al, 2009).
4 Results
Table 3 shows the results of the text-based predic-
tion, the network-based prediction and the com-
bined prediction for each stage measured by Pre-
cision, Recall and F-measure. For comparison, we
report on the results of a baseline text-based pre-
diction. The baseline prediction assigns a stage
based on the explicit mention of stage in the post
history of a patient. In practice, it is a rule-
based prediction with matching against the pattern
?stage [IV|four|4]? for stage IV prediction,
and similarly for other stages. The text-based pre-
diction yields better results than the baseline, with
a marked improvement for each stage.
The network-based prediction performs only
slightly worse than the text-based predictions. The
hypothesis that whom the patient interacts with in
the forums helps predict stage holds. To verify this
point further, we computed for each stage the av-
erage ratio of neighbors per stage based on the so-
cial network of interactions, as shown in Figure 2.
For instance, stage IV patients interact mostly with
their peers (49% of their posts are shared with
other stage IV users), and to some extent with
other patients (18% of their posts with stage I pa-
tients, 20% with stage II patients, and 13% with
stage III patients). Except for stage III patients, all
other patients are mostly interacting with similarly
staged patients.
4www.r-project.org
68
Baseline Text Based
Stage Precision Recall F Stage Precision Recall F
I 76.2 26.4 39.3 I 54.9 63.9 59.1
II 79.4 18.7 30.3 II 51.6 55.0 53.2
III 76.6 35.0 48.0 III 52.7 30.3 38.5
IV 76.4 50.7 60.9 IV 82.5 71.2 76.4
Network Based Combined
Stage Precision Recall F Stage Precision Recall F
I 50.4 56.7 53.4 I 57.1 65.4 61.0
II 49.6 49.1 49.3 II 56.6 53.5 55.0
III 65.7 27.7 39.0 III 56.1 48.3 51.9
IV 59.3 83.7 69.4 IV 84.7 81.3 83.0
Table 3: Stage prediction results (Precision, Recall, and F-measure).
When combining the text-based and the
network-based predictions in an overall classifier
the prediction yields the best results. These results
confirm the potential in combining the two facets
of patient discourse, content and social interaction.
The results presented in the table correspond to
a network built with the full set of users, including
those without any profile information. When re-
stricting the network on the patients with stage la-
bels only, we obtained similar results (F-measures
of 56% for stage I, 52% for stage II, 43% for stage
III, and 79% for stage IV). This shows that it is
worth modeling the full set of interactions and the
full network structure, even when a large number
of nodes have missing labels.
Finally, we also experimented with building
networks with no weights or with weights with-
out the 5-post-apart restriction. In both cases, the
results of the network-based and combined predic-
tions are lower than those presented in Table 3. We
interpret this fact as a confirmation that our edge
weighting strategy models to a promising extent
the degree of interaction among patients.
5 Discussion
Text-based prediction. Results confirm that
cancer stage can be predicted by a patient?s on-
line discourse. When examining the unigrams and
bigrams picked up by the classifier as predictive
of stage, we can get a sense of the frequent top-
ics of discussion of patients. For instance, the
phrases ?tumor mm? (referring to tumor size in
millimeters) and ?breast radiation? were highly
predictive of stage I patients. The words ?hat? and
?hair? were highly predictive of stages II and III,
Figure 2: Distribution of stage-wise interactions.
while stage IV patients were predicted by the pres-
ence of the phrases ?bone met.? (which stands for
bone metastasis), ?met lung? ?liver,? and ?lym-
phedema? (which is a side effect of cancer treat-
ment linked to the removal of lymph nodes and
tumor).
Figure 3 shows the overall accuracy of the text-
based classifier, when tested against the amount of
text available for the classification. As expected,
the longer the post history, the more accurate the
classification.
Representing degree of interaction among pa-
tients. In our experiments, we observed that the
weigthing scheme of edges had a strong impact
on the overall accuracy of stage prediction. The
more interaction was modeled (through distance
in thread and identification of explicit references),
the better the results. This confirms the hypothesis
that dialogue is helpful in predicting cancer stage,
and emphasizes the need for accurate techniques
69
Figure 3: Overall text-based prediction accuracy
against post history length.
to model interaction among forum participants in
a social network.
Discourse of Stage IV patients. Both the text-
based and the network-based predictions provide
higher precision and recall for the stage IV pa-
tients. This is emphasized by Figure 2, where
we see that, in our dataset, stage IV patients talk
mostly to each other. These results suggest that
stage IV patients have particular discourse, which
separates them from other patients. This presents
interesting avenues for future investigation.
6 Future Work and Conclusion
In this paper, we investigated breast cancer stage
prediction based on the online discourse of pa-
tients participating in a breast cancer-specific fo-
rum. We show that relying on lexical features de-
rived from the content of the posts of a patient
provides promising classification results. Further-
more, even a simple social network representing
patient interactions on a forum, yields predictions
with comparable results. Combining the two ap-
proaches boosts results, as content and interaction
seem to model complementary aspects of patient
discourse.
Our experiments show that stage IV patients ap-
pear to exhibit specific textual and social patterns
in forums. This point can prove useful to health re-
searchers who want to quantify patient behaviors
online.
The strategy of combining two facets of dis-
course (content and interactions) introduces sev-
eral interesting research questions. In the future,
we plan to investigate some of them. In a first step,
we plan to better model the interactions of patients
online. For instance, we would like to analyze the
content of the posts to determine further if two pa-
tients are in direct communication, and the domain
of their exchange (e.g., clinical vs. day-to-day vs.
emotional). As we have observed that the way
edges in the network are weighted has an impact
on overall performance, we could then investigate
whether the domain(s) of interaction among users
(clinical matters vs. emotional and instrumental
matters for instance) has an impact on predicting
cancer stage by taking the different domains of in-
teraction in account in the weight computation.
Finally, this work relies on a single, yet highly
active and popular, forum. We would like to
test our results on different breast cancer forums,
but also on other disease-specific forums, where
patients can be separated in clinically relevant
groups.
Acknowledgments
We thank Phani Nivarthi for his help on data col-
lection. This work is supported in part by a Google
Research Award. Any opinions, findings, or con-
clusions are those of the authors, and do not neces-
sarily reflect the views of the funding organization.
References
Erin Bantum and Jason Owen. 2009. Evaluating the
validity of computerized content analysis programs
for identification of emotional expression in cancer
narratives. Psychological Assessment, 21(1):79?88.
Andrea Civan and Wanda Pratt. 2007. Threading to-
gether patient expertise. In Proceedings of the AMIA
Annual Symposium, pages 140?144.
Brita Elvevaag, Peter Foltz, Mark Rosenstein, and
Lynn DeLisi. 2010. An automated method to ana-
lyze language use in patients with schizophrenia and
their first degree-relatives. Journal of Neurolinguis-
tics, 23:270?284.
Gunther Eysenbach and James Till. 2001. Ethical is-
sues in qualitative research on internet communities.
BMJ, 323:1103?1105.
Kristi Graves, John Schmidt, Julie Bollmer, Michele
Fejfar, Shelby Langer, Lee Blonder, and Michael
Andrykowski. 2005. Emotional expression and
emotional recognition in breast cancer survivors:
A controlled comparison. Psychology and Health,
20(5):579?595.
70
Aria Haghighi. 2005. Robust textual inference via
graph matching. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP?05, pages 387?394.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1).
Sanda Harabagiu, Finley Lacatusu, and Andrew Hickl.
2006. Answering complex questions with random
walk models. In Proceedings of SIGIR Conference
(SIGIR?06), pages 220?227.
Pawel Jurczyk. 2007. Discovering authorities in ques-
tion answer communities using link analysis. In
Proceedings of the ACM Conference on Information
and Knowledge Management (CIKM?07).
Matthew Kreuter, David Farrell, Laura Olevitch, and
Laura Brennan. 2000. Tailoring health messages:
customizing communication using computer tech-
nology. Lawrence Erlbaum Associates.
Andrea Meier, Elizabeth Lyons, Gilles Frydman,
Michael Forlenza, and Barbara Rimer. 2007. How
cancer survivors provide support on cancer-related
internet mailing lists. Journal of Medical Internet
Research, 9(2):e12.
Zheng-Yu Niu, Dong-Hong Ji, and Chew Lim Tan.
2005. Word sense disambiguation using label prop-
agation based semi-supervised learning. In Pro-
ceedings of the ACL Conference (ACL?05), pages
395?402.
Thomas Oxman, Stanley Rosenberg, Paula Schnurr,
and Gary Tucker. 1988. Diagnostic classification
through content analysis of patient speech. Ameri-
can Joural of Psychatry, 145:464?468.
John Pestian, Pawel Matykiewicz, Jacqueline Grupp-
Phelan, Sarah Arszman Lavanier, Jennifer Combs,
and Robert Kowatch. 2008. Using natural language
processing to classify suicide notes. In Proceedings
of BioNLP?08, pages 96?97.
Usha Raghavan, Reka Albert, and Soundar Kumara.
2007. Near linear time algorithm to detect commu-
nity structures in large-scale networks. Physics Re-
view, page E 76 036106.
Linda Rozmovits and Sue Ziebland. 2004. What do
patients with prostate or breast cancer want from
an Internet site? a qualitative study of information
needs. Patient Education and Counseling, 53:57?
64.
Robert Schapire and Yoram Singer. 2000. BoosTex-
ter: A boosting-based system for text categorization.
Machine Learning, 39(2/3):135?168.
Cornelia van Uden-Kraan, Constance Drossaert, Erik
Tall, Bret Shaw, Erwin Seydel, and Mart van de
Laar. 2008. Empowering processes and outcomes
of participation in online support groups for patients
with breast cancer, arthritis, or fibromyalgia. Quali-
tative Health Research, 18(3):405?417.
71
