Rapid Development of Translation Tools: 
Appl ication to Persian and Turkish 
Jan W. Amtrup~ Karine Megerdoomian and Rdmi Zajac 
Comput ing  Research  Lab  
New Mex ico  S ta te  Un ivers i ty  
{j  amtrup ,  kar  ?ne,  za j  ac}Ocr l ,  nmsu. edu 
Abstract 
The Computing Research laboratory (CRL) is devel- 
oping a machine translation toolkit that allows rapid 
deployment of translation capabilities. This toolkit 
has been used to develop several machine translation 
systems, including a Persian-English and a Turkish- 
English system, which will be demonstrated. We 
present he architecture of these systems as well as 
the development methodology. 
1 Introduction 
At CRL, one of the major research topics is the de- 
velopment and deployment of machine translation 
systems for low-density languages in a short amount 
of time. As the availability of knowledge sources 
suitable for automatic processing in those languages 
(e.g. Persian, Turkish, Serbo-Croatian) is usually 
scarce, the systems developed have to assist the ac- 
quisition process in an incremental fashion, starting 
out fl'om low-level translation on a word-fo>word 
basis and gradually extending to the incorporation 
of syntactic and world knowledge. 
The tasks and requirements for a machine trans- 
lation enviromnent that supports linguists with the 
necessary tools to develop and debug increasingly 
complex knowledge about a specific language in- 
elude: 
? The development of a bilingual dictionary that 
is used for initial basic translation and can fur- 
ther be utilized in the more complex translation 
system stages. 
? Methods to describe and process morpholog- 
ically rich languages, either by integrating 
already existing processors or by developing 
a morphologicM processor within the system 
framework. 
? Glossing a text to ensure the correctness ofmor- 
phological analysis and the colnpleteness of the 
dictionary for a given corpus. 
? Processors and grammar development tools for 
the syntactic analysis of the source language. 
? In order to allow rapid development cycles, the 
translation system itself has to be reasonably 
fast and provide the user with a rich envi- 
ronment for debugging of linguistic data and 
knowledge. 
? The system used for development must be con- 
figurable for a large variety of tasks that emerge 
during the development process. 
We have developed a component-based transla- 
tion system that meets all of the criteria men- 
tioned above. In the next section, we will describe 
the m:chitecture of the system MEAT (Multilingual 
Environment for Advanced Trmlslations) , which is 
used to translate between a number of languages 
(Persian, Turkish, Arabic, Japanese, Korean, Rus- 
sian, Serbo-Croatian and Spanish) and English. In 
the following sections, we will describe the general 
development cycle for a new language, going into 
nmre detail for two such languages, Persian and 
Turkish. 
2 Genera l  a rch i tec ture  
MEAT is a publicly available nvironmeut I hat as- 
sists a linguist in rapidly developing a machine trans- 
lation system, in order to keep the overhead in- 
volved in learning and using the system as low as 
possible, the linguist uses use simple yet powerful ba- 
sic data and control structures. These structures are 
oriented towards contemporary linguistic and con> 
putationM linguistic theories. 
In MEAT, linguistic knowledge is entirely repre- 
sented using Typed Feature Structures (TFS) (Car- 
penter, 1992; Zajac, 1992), the most widely used rep- 
resentational formalism today. We developed a fast 
implementation of Typed Feature Structm:es with 
appropriateness, based on an abstract machine view 
(eft Carpenter and Qu (1995), Wintner and Francez 
(1995). Bilingual dictionary entries as well as all 
kinds of rules (morphology, syntax, transfer, gener- 
ation) are expressed as feature structures of specific 
types, so only one description language has to be 
mastered. This usually leads to a rapid familiarity 
with the system, yielding a high productivity almost 
from the start. 
i ht tp ://crl.nmsu. edu/~j aratrup/Meat 
982 
Runtime linguistic objects (words, syntactic struc- 
tures etc.) are stored in a central data struct.ure. 
We use an extension of the well-known concept of 
a chart (Kay, 1973) to hold all temporary and fi- 
nal results. As nmltiple components have to process 
different origins of data, the chart is equipped with 
several different layers, each of which denotes a spe- 
cific aspect of processing. Thus, at every point dur- 
ing the runtime of the system, the contents of the 
chart reflect what operations have been performed 
so far (Amtrup, 1999). The complete chart is avail- 
able to the user using a graphical interface. This 
chart browser can be used to exactly trace why a 
specific solution was produced, a significant aid in 
developing and debugging rammars. 
MEAT addresses the necessity of carrying out sev- 
eral different asks by providing a component-based 
architecture. The core of the system consists of the 
formalism and the chart data representation. All 
processing components arc implemented in the form 
of plug-ins, components that obey a small interface 
to comumnicate with the main application. The 
choice of which components to apply to an actual 
input, the order in which the components are pro- 
cessed, and individual parameters for components 
can be specified by the user, allowing for a highly 
flexible way of configuring a machine translation (or 
word-lookup, of glossing etc.) system (el. Amtrup 
et al (2000) for a more detailed description of the 
architecture). 
The MEAT system is completely implemented in
G++,  resulting in a relatively tast mode of op-- 
eration. The implenrentation of the TFS formal- 
ism supports between 3000 and 4500 unifications 
per second, depending on the application it is used 
in. Translating an average length sentence (20-25 
words) takes about 3.5 seconds on a Pentium PII400 
(in non-optimized ebug mode). The system sup.- 
ports Unix (tested on Solaris and Linux) and Win-- 
dows95/98/NT. We use Unicode to represent charac- 
ter data, as we face translations of several different, 
non-European languages with a variety of scripts. 
3 Development cycle 
One of the main requirement facilitating the deploy- 
ment of a new language with possibly scarce pre- 
existing resources is the ability to incrementally de- 
velop knowledge sources and translation capability 
(the incremental pproach to MT development is de- 
scribed in (Zajac, 1999)). In the case of translation 
sy,~tems at our laboratory, we mostly translate into 
English. Thus, a complete s t of English resources i
already available (dictionary, generation grammars 
and morphological generation) and does not need to 
be developed. 
The first step in bootstrapping a running system 
is to build a bilingual dictionary. The work on the 
dictionary usually continues throughout he devel.- 
opment process of higher level knowledge sources. 
We use dictionaries where entries are encoded as flat 
feature-value pairs, as shown in Figure 1. 
$Headword OJ lyb hftglnh 
$Category Noun 
$Number Plural 
$Regular False 
$English Seven Wonder<nc plur>; 
$$ 
Figure h A Persian--English dictionary entry. 
While this is already enough information to faciL 
irate a basic word-.for-word translation, in general 
a morphological analyzer for the source language is 
needed to translate real-world text. For MEAT, one 
can either import the results of an existing morpho- 
logical analyzer, or use the native description lan- 
guage, based on a finite-state transducer using char- 
acters as left projections and typed feature struc- 
tures as right projections (Zajac, 1998). After com- 
pleting the morphological nalysis of the source lan- 
guage and specifying the mapping of lexical features 
to English, glossing is available. The Glosser is an 
MEAT application consisting of morphological nal- 
ysis of source language words, tbllowed by dictionary 
lookup for single words and compounds, and the 
translation into English inflected word forms. An 
example of the interface \[br the glosser is shown in. 
l?igure 2. 
. . . . . . . . . . . . . . . . . . . . . .  
File Edit View G0 C0mlaun\[c~2ot Helo 
- :  - "7 -  . . . .  
~' ,~3~ t~l ?ndont ts ia ;  
J and; 
~3/z~}J_~ cont ract  j 
~ @rant; ~st i t~o;  p~.ttir~/; 
&~- to; atl in; on; for; 
)~  )~ East TJ2~O=; 
b .; 
L~I  mi~j~at~r~; approval.; slanatu~; 
u~ wine; 
~I~E dig; pull ofl; do; 
~ J / i  IndonQ=ia; 
J an~ 
3J~i today; t odag; 
&~a;(&~3; (Ve?\ ]  . . . . . . . . .  
diq; pu l l  o f f ;  
Pre~ent ;  Th/x~;  P lura l _  
~c~ive; S~o3~ctive 
1~ unstgned Javn ~pFl~t Whqd~v 
'c-A i . . . .  iAp~a taeat~p~ ;~ r,~,~ '.~ " -  -5 "J ~ -2 -  
Figure :2: The Glosser interface 
The next step in developing a nmdium-.quality, 
broad coverage translation system is to develop 
983 
.-,C _2227.7_Z:22_C-_~TfT2L27AL--Z:ZZT~:7:hfZ.._~\[_ 2 -,~Nv,~-:c~_ j~ ~-::27 _~ ~ T--_ZiLT.\[  Z.::__22 2 7J:.7-.-_L_2_..2Y Y_~:2-__-? . I . :  .4_ 
I 
Q~II% ; 
Vtn'Ilces 
I -dgeS 
161Z 
Sta~.  Ima 
i0 ...... 
N~k i 
~-~: : -2 -L77  ............ 7"-"---,A,/"-->..# / 
_--J - - .  " -" , , '> -~ / j~<-~2 '  
"-. / ,",  . ->%/" . / . .C , -< .~Z- - ,  ~ . .  / , ~ . . . . , . , .  _ . , .~ .~ 
\ // ' .  I /  ,'::;G ~ > "';-5-"..'~,~ .... 
~1/5 I/' /.'-'/2?" ~51"-'2-.~,-I-~.:--.<<." 
" II I "  t /  ':" - "  1/.-~2,./-~ / . /~ . . , .  7 
\ 1' ,/ //.$.-'L>:'7..~;v, .... IK,*; '  .'%-,~ - 
\ f/ ~?,~" ~,h'~ 2 ...... ~,?7:-.. \ 
by lqYy  Iz sh  mD l)y'~ 
~.~'~c'_. -~ .~_~ . . . .  :--. "-.. ;.q<*j -.. "-, %,.  ~.* 
....... ~" 7-Yb?:--'" ...... :-v~<~, .... "-3,.',q../" 
_ _~,~7<---7..2;.=~ . . . . . . . . .  ~ ~- . .~ .  ~_ .  , zv%,>,y .  x 'q . , , \  . . . . .  -~- - 
~ \ ,x~* .~ J~"  --.. ,, . . . . . .  ---.-?.. v:>:< "-~--'>. ";~.y" _ 
I f z ly "s  ^s l Iydy  y l f th  Is /  v 
I 
83 gg g9 ?3  ?7 6'~ fib g6 100 l i l t  I'" 
Cam lamt: Sentence \[ . . . . .  i- 
. . . . . . . . . . .  l : \ [aer . l~ .~M~lra~ ounaTZeel \[Ad\]ecti'3e" \[ I \[ \[Pre m~l ta0nA.  \ ]~ ' \ ]N '  \] ICPoIIIP \[ \[ \[ \[ \[~lu~er a l \]l'!u~' I Nu:'~ \] $"" : "Verh lUP lS  ", \[NounlN' INPo \[ \[ \[Noun\]If' I NP0l,'ct; ~ IPP IX IXP \[ \[ \[ \[M l' i 
~ :lmr.l:~le ..~e rdetg: e he~d VerbPhraae I
a~er . rml~.~nm,~ head f:nt~y( ~ "' 
form g,,~m I 
4:per.R111e. Se.te|i?~ ;~orph Ve ;.bllo i:pholo~-i \[ 
,~ ,~ ,  \[blle. ,~e,~ |~Ice lex Ve rha\]J, ox~call 
\[i:laa i*. FRIIIt. ~.!~, telll?~ ~ poa 9eeb. 
7:ger .Ru le .~e. te i *ce i  p~, : : )entS  te~. "y lh% 
regul.a," T~uel .  
I I :p~ r.  l'Illl~l. ~n  ll~llrA~ infl Ve~bal.lnf kect\].on\[ 
,~: NF.  ~t1111. ~\[~n ~t II g ~ voice Undeflned, 
t lhper.Rtl leo~len~m;e clztzc : C?.Jtic\[ 
function : Nulll, 
11:~.Rtde.8~It~I1C8 tense ' Part ic lp1~L 
IZ:per.RtdmS6~ten~s I causat lw  False, I 
1:J - -e l '  ~UlI~ ~lall|/eligli~ i , person  Unde~:tned. , \[ 
. 4' ? ' ~ood Un(~f~.ned. { 
14 : l l l~? .Rt~e 'Se"~t tce  \[ nu~erAg~eement  Undefined, 
nege, t lon  Fa lse \ ]  l ,  
, og~ Or rhography ! 
key : "ylftn", * 
!~ \[:--~ Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ~ .... 
............... . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . .  :.::: : : : : _ : : .  :: .:: : -= ._ :  ....... : . . . .  - :  =-  :_-.::_ .:_ :. : : _  ' : : - : : - _  _: .  _ _ :  :_;: . :_ ._  _ -_ . _=>-:_ -  2 : : J  
Figure 3: Viewing  a eomple :~c  aual&,;is 
knowledge sources  for the stt?uctural analysis of in- 
put sentences. Mt';AT supports the use of modm 
lar mfific.ation grammars, which facilitate,," develop. 
merit and debugging. E;ach grammar module ca,~. be 
developed and tested irJ isolation, the final system 
applying each gtammar in a linear fashion (Zajac 
and Amtrup, 2000). The main. component used is a 
bidirectional island-.parser (cf. Stock et al (1988)) 
for unification-based gra.mmars. The grammar ules 
are usually writtet~ in the style of context-free rules 
with  \ [~ssoc ia ted  unification constraints as shown i~ 
Figure 4. The rules allow for the specification of 
the right-hand side as a regular-.expression of feature 
structures. We plan to add more restricted types of 
grammars (e.g. based on finite-.state transducers) to 
give the linguist a richer choice of syntactic processes 
to choose from. 
For the time being, the transfer capabilities of the 
system are restricted to lexical transDr, as we have 
not finished the implementation f a complex trans- 
fer module. Thus, the grammar developer either 
needs to create structural descriptions that match 
the English generation, or the English generation 
grammar has to be modified for each language. 
At each point during the development of a trans-. 
Adj  ~ ::: ' i ;u :c , thdLc ;  \[ 
J im : i;u~:', g djBa~: \[ 
\ [ \ [c :x :}. !eAd : #Ld\ ] j  
g}pe{. : :  : '~adv\ ]  , 
#adv: : tl**. &dvg~Et;ry "Y" 
#ad. j ' : : :  i:.u:~.', gd jEnt : ! :y  
:>  
\ ] ;  
Figm'e 4: A Turldsh syntax rule 
lation syste.m, we consider it essential to be ahle 
not ouly to see the resuRs, but also to monitor 
the processing history of a result. Thus, th.e chart 
that leads 1;o the construction of all English o l t tp t l t  
can be viewed in order to examine all intermediate 
constructions. In the MEAT system, each module 
records various steps of computations in the chart 
which can be inspected statically after processing. A
unified data interface for all modules in the systein 
allows both the inspection of recorded internal data 
structures for each module (when it makes sen:-;e, 
984 
such aq ill a ch.art parser), aml tile im;pecticm of tit(.' 
input/ou?put of all module,';. The graphical i~terfa.ce 
used to view complex a.t~alyscs i.r; shown in Figure 3. 
d A.pp\] J .ca{; ions 
In this section, we give an overview of the capal)i!i. 
ti(~'; of MEA71' using ~wo (-l!ri{~.nt examples from work 
af, our laboratory. In the Shiraz pro jec t  ? (Amtrup 
ei; al., 2000), we developed a machine translatio~ 
sys tem from Farsi tO English, for which no previous 
knowledge sources were awdlable. We mainly target 
news material and the transl~tion of web pages. Tile 
Tm'kish~English ystem has been developed with the 
Expedition project a, an enterprise for the rapid de- 
velopment of MT systems for low-density languages. 
Botl /systems use a common user interface for ac~ 
cess to MEAT, which is shown in Figure 5. The 
MT systems are targeted t() the translation of newsy. 
paper text and other sources available online (e.g. 
web pages). The emphasis is therefore put ou exten+ 
sive coverage rather than very-+high quality transta+ 
tion+ Currently, we reach for bol;h systmns a level 
of quality that allows to assess in detail tile content 
of source texts, at the expense of some rmfelicitous 
English. 
r~ t a ~  
. . . . . . . . . . . . .  D vc~lv~ {ll;t,~c s:/Im~oa~tj am~ In Iplr,eanhltm-! C fl t g '~A.  t..R ~d~q~ ~f~l  ,+01. bl t 
. . . . . . . . . . . . . . .  i?i ~ZL2Z -2-~?2-?.27 ~-+ ~2Z7-~ -_._ _L_,7 
. . . . . . . . . . . . . . . .  T~ak~tma 
c~,~ :\[iT~;i-;;,a;;~Eh Tg~GiTg~'TggU~..,.-gJ~ g ;gTJE.TKg7 \[y 
T7:2~: 7 7 ~ 7 ~ 7  : I}IC~W: CriaX~ =true\]( ocon0~, 
Dm ( rL~Rhlh ) I I 
' '  ~ " ~t" j C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  \[ :  
~-~ ............................. s~,2~, ,~  . . . . .  
~ht~l~llt~a }lONer gx~cut in  9 llodu\[~ cll~rgS~v~r +~ 
. . . . . . . .  ~5:___.::~ . . . . . .  : \ [chartsauer  Size-ag, re~xde~t-S~, toCah,.O, C~U.,0 010a ' ? 
ht~l -~ IRt |~  Execut in~ Nodule su ' fce  I 
\[c+ Sueface0ener~nor  S tar t inq  t~ 1 +OZ L~t s 
t~tl-O3,lxt ": : S~tcfGcn:Ur f~ e0onor~orsiz~+0E ~=~tdeltt=~K'znL~h~d" to~cl-0,  ~pII-00{\]0s . J  
hl~*1-0/'bct I r }Tot~\]." g~ze=S2R, re~id~nt-48g, toeal~99800 C~J=2 3~0s (croat ~/  
\ ] Iu , - .+ ;  =~ , i ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
Figure 5: The MEAT user interface 
4,1 Persian..+Englisb MT 
The input for the Persian-English system is usu- 
ally taken from web pages (on-line news articles), 
although plain text can be handled as well. The im 
ternal encoding is Unicode, and various codeset con- 
w.'rters are available; we also developed an ASCII- 
based transliteration to facilitate the easy acquisi~ 
tion of dictionaries and grammars (see Figure 1). 
The dictionary consists of approximately 50,000 en+ 
tries, single words as well as multi-word compounds. 
Additionally, we utilize a multi-lingual onomasticon 
maintained locally to identify proper names. 
2http  : / / c r l .  ransu, edu/sh i raz  
3ht tp  : / / c r l .  nmsu. edu/exped i t  ion 
we developed a, morphological grammar for Per- 
siai~ using a unifies.lion-based formalism (Zajac, 
1998). A sample rule is showtt in Figure 6 (of. 
Megerdoomian (2000) for a more thorough descrip 
~ion of the Persian morphological analyzer). 
PresentStem = < 
Regu l  a rPx 'e  sent  S t  em 
< < <" In"  "y"?> 
per .  Verba l \ [ in f \ ] . . causat ive :  Trne\]> I
< per .Verba . \ ]o \ [ in f l . causat ive :  Fa lse\ ]> 
>>; 
Figu.re 6: A Persian morphological rule 
The knowledge sources for syntax were (manually) 
developed using a corpus of 3,000 tagged and brack- 
eted setrt, ences extracted h:om a 10MB corpus of Per.? 
sian news articles. We use three grammars, respon.- 
sible lor the attaetunent of auxiliaries to main verbs, 
the recognition and processing of light verb phenom-. 
ena, and phrasal and sentential syntax, respectively. 
The combined size is about 110 rules. The develop. 
ment of the Persian resources took several months, 
primarily due to the fact that the translation system 
was developed in parallel to the linguistic knowl.. 
edge. The Persian resources were developed by a 
team of one computational linguist (morphological 
and syntactic grammars, overall supervision ibr lan-- 
guage resources), and 3 lexicographers (dictionary 
and corpus annotation). 
4?2 'lti~urkish+-English MT 
Withii~ yet another project (Expedition), we devel- 
oped a machine translation system, for Turkish. This 
application functioned as a benchnrark on how much 
effort the building of a medium-quality system re-- 
quires, given that an appropriate framework is al-. 
ready available. 
For Turkish, we use a pre-existing morphologi- 
cal analyzer (Oflazer, 1994). Turkish shows a rich 
derivational and inflectional morphology, which ac- 
counts for most of the system development work 
that was necessary to build a wrapper for inte- 
grating the Turkish morphological analyzer in the 
system (approximatly 60 person-hours) 4. The de- 
velopment of the Turkish syntactic grammars took 
around 100 person-hours, resulting in 85 unification- 
based phrase structure rules describing the basics of 
Turkish syntax. The development of the bilingual 
Turkish-English dictionary had been going on for 
4The main problem dur ing the adaptat ion was the treat- 
ment  of derivational information, where the morphologically 
analyzed input does not conform exactly to the contents of 
the dictionary 
985 
some time prior to the application of MEAT, and 
currently contains approximately 43,000 headwords. 
5 Conc lus ion 
The development of automatic machine translation 
systems for several languages is a complicated task, 
even more so if it has to be done in a short amount 
of time. We have shown how the availability of a 
machine translation environment based on contem- 
porary computational linguistic theories and using 
a sound system design aids a linguist in building 
a complete system, starting out with relatively siln- 
ple tasks as word-for-word translation and increlnen- 
tally increasing the complexity and capabilities of 
the system. Using the current library of modules, it 
is possible to achieve a level of quality on par with 
the best transfer-based MT systems. Some of the 
strong points of the system are: (1) The system can 
be used by a linguist with reasonable knowledge of 
computational linguistics and does not require spe- 
cific programming skills. (2) It can be easily con- 
figured for building a variety of applications, includ- 
ing a complete MT system, by reusing a library of 
generic modular components. (3) It supports an in- 
crelnental develol)ment methodology which allows to 
develop a system in a step-wise fashion and enables 
to deliver rumfing systems early in the development 
cycle. (4) Based on the experiences in building the 
MT systems mentioned in the paper, we estimate 
that a team of one linguist and three lexicographers 
can build a basic transfer-based MT system with 
medium-size coverage (dictionary of 50,000 head- 
words, all most fi'equent syntactic onstructions in
the language) in less than a year. 
One major improve,nent of the system would be 
a nmre integrated test and debug cycle linked to a 
corpus and to a database of test items. Although ex- 
isting testing methodologies and tools could be used 
(l)auphin E., 1996; Judith Klein and Wegst, 1998), 
building test sets is a rather time-consulning task 
and some new approach to testing supporting rapid 
development of MT systems with an emphasis on 
wide coverage would be needed. 
References 
Jan W. Amtrup, Hmnid Mansouri Rad, Karine 
Megerdoomian, and Rdmi Zajac. 2000. Persian- 
English Machine Translation: An Overview of 
the Shiraz Project. Memoranda in Computer and 
Cognitive Science MCCS-00-319, Colnputing Re- 
search Laboratory, New Mexico State University, 
Las Cruces, NM, April. 
Jan W. Amtrup. 1999. Incrcmcntal Speech TraTtsla- 
tion. Nmnber 1735 in Lecture Notes in Artificial 
Intelligence. Springer Verlag, Berlin, Heidelberg, 
New York. 
Bob Carpenter and Yan Qu. 1995. An Abstract 
Machine for Attribute-Value Logics. In Proceed- 
ings of the ~th International Workshop on Pars- 
ing Technologies (II/VPT95), pages 59-70, Prague. 
Charles University. 
Bob Carpenter. 1992. The Logic of 2~ped Feature 
Structures. Tracts in Theoretical Computer Sci- 
ence. Cambridge University Press, Cambridge. 
Lux V. Dauphin E. 1996. Corpus-Based annotated 
Test Set for Machine Translation Evaluation by 
an Industrial User. In Proceedings of the 16th 
International Conference on Computational Lin- 
guistics, COLING-96, pages 1061-1065, Center 
for Sprogteknologi, Copenhagell, Demnark, Au- 
gust 5-9. 
Klaus Netter Judith Klein, Sabine Lehmann and 
Tillman Wegst. 1998. DIET in the context of MT 
evaluation. In Proceedings of KONVENS-98. 
Martin Kay. 1973. The MIND System. In. 
R. Rustin, editor, Natural Language Processing, 
pages 155-188. Algorithmic Press, New York. 
Karine Megerdoomian. 2000. Unification-Based 
Persian Morphology. In Proceedings of the CI- 
Cling 2000, Mexico City, Mexico, February. 
Kemal Oflazer. 1994. Two-level Description of 
Tnrkish MorI)hology. Literary and Linguistic 
Computing, 9(2). 
Oliviero Stock, Rino Falcone, and Patrizia Insin- 
namo. 1988. Island Parsing and Bidirectional 
Charts. In Proc. of the 12 t1~ COLING, pages 636- 
641, Budapest, Hungary, August. 
Shuly Wintner and Nissim Francez. 1995. Pars- 
ing with Typed Feature Structures. In Proceed- 
ings of the ~th International Workshop on Parsing 
Technologies (\]WPT95), pages 273 287, Prague, 
September. Charles University. 
Rdmi Zajac and Jan W. Amtrup. 2000. Modular 
Unification-Based Parsers. In Proc. Sizth lr~,tcrna- 
tional Workshop on Parsing Technologies, trento, 
Italy, February. 
Rdmi Zajac. 1992. Inheritance and Constraint- 
Based Grammar FormMislns. Computational Lin- 
guistics, 18(2):159-182. 
R&mi Zajac. 1998. Feature Structures, Unification 
and Finite-State Transducers. In FSMNLP'98, 
Intcrnational Workshop on Finite State Methods 
in Natural Language Processing, Ankara, ~lhlrkey, 
June. 
Remi Zajac. 1999. A Multilevel Frmnework for In- 
cremental Development of MT Systems. In Ma- 
chine Translation Summit VII, pages 131-157, 
University of Singapore, September 1.3-17. 
986 
 
	  	
Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 51?56,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Acquiring High Quality Non-Expert Knowledge from                         
On-demand Workforce 
 
Donghui Feng           Sveva Besana          Remi Zajac  
AT&T Interactive Research 
Glendale, CA, 91203 
{dfeng, sbesana, rzajac}@attinteractive.com 
 
  
 
Abstract 
Being expensive and time consuming, human 
knowledge acquisition has consistently been a 
major bottleneck for solving real problems. In 
this paper, we present a practical framework 
for acquiring high quality non-expert knowl-
edge from on-demand workforce using Ama-
zon Mechanical Turk (MTurk). We show how 
to apply this framework to collect large-scale 
human knowledge on AOL query classifica-
tion in a fast and efficient fashion. Based on 
extensive experiments and analysis, we dem-
onstrate how to detect low-quality labels from 
massive data sets and their impact on collect-
ing high-quality knowledge. Our experimental 
findings also provide insight into the best 
practices on balancing cost and data quality for 
using MTurk. 
1 Introduction 
Human knowledge acquisition is critical for 
training intelligent systems to solve real prob-
lems, both for industry applications and aca-
demic research. For example, many machine 
learning and natural language processing tasks 
require non-trivial human labeled data for super-
vised learning-based approaches. Traditionally 
this has been collected from domain experts, 
which we refer to as expert knowledge. 
However, acquiring in-house expert knowl-
edge is usually very expensive, time consuming, 
and has consistently been a major bottleneck for 
many research problems. For example, tremen-
dous efforts have been put into creating TREC 
corpora (Voorhees, 2003).  
As a result, several research projects spon-
sored by NSF and DARPA aim to construct 
valuable data resources via human labeling; these 
are exemplified by PennTree Bank (Marcus et 
al., 1993), FrameNet (Baker et al, 1998), and 
OntoNotes (Hovy et al, 2006).  
In addition, there are projects such as Open 
Mind Common Sense (OMCS) (Stork, 1999; 
Singh et al, 2002), ISI LEARNER (Chklovski, 
2003), and the Fact Entry Tool by Cycorp (Be-
lasco et al, 2002) where knowledge is gathered 
from volunteers. 
One interesting approach followed by von 
Ahn and Dabbish (2004), applied to image label-
ing on the Web, is to collect valuable input from 
entertained labelers. Turning label acquisition 
into a computer game addresses tediousness, 
which is one of the main reasons that it is hard to 
gather large quantities of data from volunteers.     
More recently researchers have begun to ex-
plore approaches for acquiring human knowl-
edge from an on-demand workforce such as 
Amazon Mechanical Turk1. MTurk is a market-
place for jobs that require human intelligence. 
There has been an increase in demand for 
crowdsourcing prompted by both the academic 
community and industry needs. For instance, 
Microsoft/Powerset uses MTurk for search rele-
vance evaluation and other companies are lever-
aging turkers to clean their data sources. 
However, while it is cheap and fast to obtain 
large-scale non-expert labels using MTurk, it is 
still unclear how to leverage its capability more 
efficiently and economically to obtain sufficient 
useful and high-quality data for solving real 
problems.  
In this paper, we present a practical frame-
work for acquiring high quality non-expert 
knowledge using MTurk. As a case study we 
have applied this framework to obtain human 
classifications on AOL queries (determining 
whether a query might be a local search or not). 
Based on extensive experiments and analysis, we 
show how to detect bad labelers/labels from 
massive data sets and how to build high-quality 
labeling sets. Our experiments also provide in-
                                                 
1 Amazon Mechanical Turk:  http://www.mturk.com/ 
51
sight into the best practices for balancing cost 
and data quality when using MTurk. 
The remainder of this paper is organized as 
follows: In Section 2, we review related work 
using MTurk. We describe our methodology in 
Section 3 and in Section 4 we present our ex-
perimental results and further analysis. In Sec-
tion 5 we draw conclusions and discuss our plans 
for future work. 
2 Related Work 
It is either infeasible or very time and cost con-
suming to acquire in-house expert human knowl-
edge. To obtain valuable human knowledge (e.g., 
in the format of labeled data), many research 
projects in the natural language community have 
been funded to create large-scale corpora and 
knowledge bases, such as PenTreeBank (Marcus 
et al, 1993), FrameNet (Baker et al, 1998), 
PropBank (Palmer et al, 2005), and OntoNotes 
(Hovy et al, 2006). 
MTurk has been attracting much attention 
within several research areas since its release. Su 
et al (2007) use MTurk to collect large-scale 
review data. Kaisser and Lowe (2008) report 
their work on generating research collections of 
question-answering pairs using MTurk. Sorokin 
and Forsyth (2008) outsource image-labeling 
tasks to MTurk. Kittur et al (2008) use MTurk 
as the paradigm for user studies. In the natural 
language community Snow et al (2008) report 
their work on collecting linguistic annotation for 
a variety of natural language tasks including 
word sense disambiguation, word similarity, and 
textual entailment recognition. 
However, most of the reported work focuses 
on how to apply data collected from MTurk to 
their applications. In our work, we concentrate 
on presenting a practical framework for using 
MTurk by separating the process into a valida-
tion phase and a large-scale submission phase.  
By analyzing workers? behavior and their data 
quality, we investigate how to detect low-quality 
labels and their impact on collected human 
knowledge; in addition, during the validation 
step we study how to best use MTurk to balance 
payments and data quality. Although our work is 
based on the submission of a classification task, 
the framework and approaches can be adapted 
for other types of tasks. 
In the next section, we will discuss in more 
detail our practical framework for using MTurk. 
3 Methodology 
3.1 Amazon Mechanical Turk 
Amazon launched their MTurk service in 2005. 
This service was initially used for internal pro-
jects and eventually fulfilled the demand for us-
ing human intelligence to perform various tasks 
that computers currently cannot do or do very 
well.  
MTurk users naturally fall into two roles: a re-
quester and a turker. As a requester, you can de-
fine your Human Intelligent Tasks (HITs), de-
sign suitable templates, and submit your tasks to 
be completed by turkers. A turker may choose 
from HITs that she is eligible to work on and get 
paid after the requester approves her work. The 
work presented in this paper is mostly from the 
perspective of a requester. 
3.2 Key Issues 
While it is quite easy to start using MTurk, re-
questers have to confront the following: how can 
we obtain sufficient useful and high-quality data 
for solving real problems efficiently and eco-
nomically?  
In practice, there are three key issues to con-
sider when answering this question. 
Key Issues Description 
Data    
Quality 
Is the labeled data good enough for 
practical use? 
Cost What is the sweet spot for payment? 
Scale How efficiently can MTurk be used 
when handling large-scale data sets? 
Can the submitted job be done in a 
timely manner?  
Table 1. Key issues for using MTurk. 
Requesters want to obtain high-quality data on 
a large scale without overpaying turkers. Our 
proposed framework will address these key is-
sues.  
3.3 Approaches 
Since not all tasks collecting non-expert knowl-
edge share the same characteristics and suitable 
applications, there is not a one-size-fits-all solu-
tion as the best practice when using MTurk.  
In our approach, we divide the process into 
two phases:  
? Validation Phase.  
? Large-scale Submission Phase.  
The first phase gives us information used to 
determine if MTurk is a valid approach for a 
given problem and what the optimal parameters 
for high quality and a short turn-around time are. 
52
We have to determine the right cost for the task 
and the optimal number of labels. We empiri-
cally determine these parameters with an MTurk 
submission using a small amount of data. These 
optimal parameters are then used for the large-
scale submission phase.  
Most data labeling tasks require subjective 
judgments. One cannot expect labeling results 
from different labelers to always be the same. 
The degree of agreement among turkers varies 
depending on the complexity and ambiguity of 
individual tasks. Typically we need to obtain 
multiple labels for each HIT by assigning multi-
ple turkers to the same task. 
Researchers mainly use the following two 
quantitative measures to assess inter-agreement: 
observed agreement and kappa statistics.  
P(A) is the observed agreement among anno-
tators. It represents the portion where annotators 
produce identical labels. This is very natural and 
straightforward. However, people argue this may 
not necessarily reflect the exact degree of agree-
ment due to chance agreement.  
P(E)  is the hypothetical probability of chance 
agreement. In other words, P(E)  represents the 
degree of agreement if both annotators conduct 
annotations randomly (according to their own 
prior probability).  
We can also use the kappa coefficient as a 
quantitative measure of inter-person agreement. 
It is a commonly used measure to remove the 
effect of chance agreement. It was first intro-
duced in statistics (Cohen, 1960) and has been 
widely used in the language technology commu-
nity, especially for corpus-driven approaches 
(Carletta, 1996; Krippendorf, 1980). Kappa is 
defined with the following equation:  
kappa = P(A) ? P(E)
1? P(E)  
Generally it is viewed more robust than ob-
served agreement P(A)  because it removes 
chance agreement P(E) . 
DetectOutlier( P) 
    for each turker p ? P  
collect the label set L  from p 
for each label l ? L  
    /* compared with others? majority voting */ 
    compute its agreement with others 
compute P(A) p  (or kappa p ) 
    analyze the distribution of P(A) 
    return outlier turkers 
Figure 1. Outlier detection algorithm. 
We use these measures to automatically detect 
outlier turkers producing low-quality results. 
Figure 1 shows our algorithm for automatically 
detecting outlier turkers.  
4 Experiments 
Based on our proposed framework and ap-
proaches, as a case study we conducted experi-
ments on a classification task using MTurk.  
The classification task requires the turker to 
determine whether a web query is a local search 
or not. For example, is the user typing this query 
looking for a local business or not? The labeled 
data set can be used to train a query classifier for 
a web search system. 
This capability will make search systems able 
to distinguish local search queries from other 
types of queries and to apply specific search al-
gorithms and data resources to better serve users? 
information needs.  
For example, if a person types ?largest biomed 
company in San Diego? and the web search sys-
tems can recognize this query as a local search 
query, it will apply local search algorithms on 
listing data instead of or as well as generating a 
general web search request.  
4.1 Validation Phase 
We downloaded the publicly available AOL 
query log2 and used this as our corpus. We first 
scanned all queries with geographic locations 
(including states, cities, and neighborhoods) and 
then randomly selected a set of queries for our 
experiments. 
For the validation phase, 700 queries were 
first labeled in-house by domain experts and we 
refer to this set as expert labels. To obtain the 
optimal parameters including the desired number 
of labels and payment price, we designed our 
HITs and experiments in the following way:  
We put 10 queries into one HIT, requested 15 
labels for each query/HIT, and varied payment 
for each HIT in four separate runs. Our payments 
include $0.01, $0.02, $0.05, and $0.10 per HIT. 
The goal is to have HITs completed in a timely 
fashion and have them yield high-quality data.  
We submitted our HITs to MTurk in four dif-
ferent runs with the following prices: $0.01, 
$0.02, $0.03, and $0.10. According to our pre-
defined evaluation measures and our outlier de-
tection algorithm, we investigated how to obtain 
the optimal parameters. Figure 2. shows the task 
completion statistics for the four different runs. 
                                                 
2 AOL Log Data: http://www.gregsadetsky.com/aol-data/ 
53
 
Figure 2. Task completion statistics. 
As shown in Figure 2, with the increase of 
payments, the average hourly rate increases from 
$0.72 to $9.73 and the total turn-around time 
dramatically decreases from more than 47 hours 
to about 1.5 hours. In the meantime, people tend 
to become more focused on the tasks and spend 
less time per HIT. 
In addition, as we increase payment, more 
people tend to stay with the task and take it more 
seriously as evidenced by the quality of the la-
beled data. This results in fewer numbers of 
workers overall as well as fewer outliers as 
shown in Figure 3.  
 
Figure 3. Total number of workers and outliers. 
We investigate two types of agreements, inter-
turker agreement and agreement between turkers 
and our in-house experts. For inter non-expert 
agreements, we compute each turker?s agreement 
with all others? majority voting results.  
Payment 
(USD) 0.01 0.02 0.05 0.10 
Median of 
inter-
turker 
agreement 0.8074 0.8583 0.9346 0.9028 
Table 2. Median of inter-turker agreements. 
As in our outlier detection algorithm, we ana-
lyzed the distribution of inter-turker agreements. 
Table 2 shows the median values of inter-turker 
agreement as we vary the payment prices. The 
median value keeps on increasing when the price 
increases from $0.01, to $0.02 and $0.05. How-
ever, it drops as the price increases from $0.05 to 
$0.10. This implies that turkers do not necessar-
ily improve their work quality as they get paid 
more. One of the possible explanations for this 
phenomenon is that when the reward is high 
people tend to work towards completing the task 
as fast as possible instead of focusing on submit-
ting high-quality data. This trend may be intrin-
sic to the task we have submitted and further ex-
periments will show if this turker behavior is 
task-independent.    
 
Figure 4. Agreement with experts. 
 
Figure 5. Inter non-expert agreement. 
We also analyzed agreement between non-
experts and experts. Figure 4 depicts the trend of 
the agreement scores with the increase of number 
of labels and payments. For example, given 
seven labels per query, in the experiment with 
the $0.05 payment, the majority voting of non-
expert labels has an agreement of 0.9465 with 
expert labeling. As explained earlier we do not 
necessarily obtain the best data qual-
ity/agreement with the $0.10 payment. Instead, 
we get the highest agreement with the $0.05 
payment. We have determined this rate to be the 
54
sweet spot in terms of cost. Also, seven labels 
per query produce a very high agreement with no 
further significant improvement when we in-
crease the number of labels.  
For inter non-expert agreements, we found 
similar trends in terms of different payments and 
number of labels as shown in Figure 5. 
As mentioned above, our algorithm is able to 
detect turkers producing low-quality data. One 
natural question is: how will their labels affect 
the overall data quality? 
We studied this problem in two different 
ways. We evaluated the data quality by removing 
either all polluted queries or only outliers? labels. 
Here polluted queries refer to those queries re-
ceiving at least one label from outliers. By re-
moving polluted queries, we only investigate the 
clean data set without any outlier labels. The 
other alternative is to only remove outliers? la-
bels for specific queries but others? labels for 
those queries will be kept. Both the agreement 
between experts and non-experts and inter-non-
experts agreement show similar trends: data 
quality without outliers? labels is slightly better 
since there is less noise. However, as outliers? 
labels may span a large number of queries, it 
may not be feasible to remove all polluted que-
ries. For example, in one of our experiments, 
outliers? labels pollute more than half of all the 
records. We cannot simply remove all the queries 
with outliers? labels due to consideration of cost.  
On the other hand, the effect of outliers? labels 
is not that significant if a certain number of re-
quested labels per query are collected. As shown 
in Figure 6, noisy data from outliers can be over-
ridden by assigning more labelers. 
 
Figure 6. Agreement with Experts (removing 
outliers? labels (payment = $0.05)).  
From the validation phase of the query classi-
fication task, we determine that the optimal pa-
rameters are paying $0.05 per HIT and request-
ing seven labels per query. Given this number of 
labels, the effect of outliers? labels can be over-
ridden for the final result.  
4.2 Large-scale Submission Phase 
Having obtained the optimal parameters from the 
validation phase, we are then ready to make a 
large-scale submission.  
For this phase, we paid $0.05 per HIT and re-
quested seven labels per query/HIT. Following 
similar filtering and sampling approaches as in 
the validation phase, we selected 22.5k queries 
from the AOL search log. Table 3 shows the de-
tected outliers for this large-scale submission.  
Total Number of Turkers 228 
Number of Outlier Turkers 23 
Outlier Ratio 10.09% 
Table 3. Number of turkers and outliers. 
Based on the distribution of inter-turker 
agreement, any turkers with agreement less than 
0.6501 are recognized as outliers. For a total 
number of 15,750 HITs, 228 turkers contributed 
to the labeling effort and 10.09% of them were 
recognized as outliers.  
Table 4 shows the number of labels from the 
outliers and the approval ratio of collected data. 
About 10.08% of labels are from outlier turkers 
and rejected.  
Total Number of Labels 157,500 
Number of Outlier Labels 15,870 
Approval Ratio 89.92% 
Table 4. Total number of labels. 
We have experimented using MTurk for a web 
query classification task. With learned optimal 
parameters from the validation phase, we col-
lected large-scale high-quality non-expert labels 
in a fast and economical way. These data will be 
used to train query classifiers to enhance web 
search systems handling local search queries. 
5 Conclusions and Future Work 
In this paper, we presented a practical framework 
for acquiring high quality non-expert knowledge 
from an on-demand and scalable workforce. Us-
ing Amazon Mechanical Turk, we collected 
large-scale human classification knowledge on 
web search queries.  
To learn the best practices when using MTurk, 
we presented a two-phase approach, a validation 
phase and a large-scale submission phase. We 
conducted extensive experiments to obtain the 
optimal parameters on the number of labelers 
and payments in the validation phase. We also 
presented an algorithm to automatically detect 
55
outlier turkers based on the agreement analysis 
and investigated the effect of removing an inac-
curately labeled set.  
Acquiring high-quality human knowledge will 
remain a major concern and a bottleneck for in-
dustry applications and academic problems. Un-
like traditional ways of collecting in-house hu-
man knowledge, MTurk provides an alternative 
way to acquire non-expert knowledge. As shown 
in our experiments, given appropriate quality 
control, we have been able to acquire high-
quality data in a very fast and efficient way. We 
believe MTurk will attract more attention and 
usage in broader areas. 
In the future, we are planning to investigate 
how this framework can be applied to different 
types of human knowledge acquisition tasks and 
how to leverage large-scale labeled data sets for 
solving natural language processing problems.  
References  
Baker, C.F., Fillmore, C.J., and Lowe, J.B. 1998. The 
Berkeley FrameNet Project. In Proc. of COLING-
ACL-1998.  
Belasco, A., Curtis, J., Kahlert, R., Klein, C., Mayans, 
C., and Reagan, P. 2002. Representing Knowledge 
Gaps Effectively. In Practical Aspects of Knowl-
edge Management, (PAKM).  
Carletta, J. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
guistics. 22(2):249?254. 
Chklovski, T. 2003. LEARNER: A System for Ac-
quiring Commonsense Knowledge by Analogy. In 
Proc. of Second International Conference on 
Knowledge Capture (KCAP 2003).  
Cohen, J. 1960. A coefficient of agreement for nomi-
nal scales. Educational and Psychological Meas-
urement. Vol.20, No.1, pp.37-46.  
Colowick, S.M. and Pool, J. 2007. Disambiguating for 
the web: a test of two methods. In Proc. of the 4th 
international Conference on Knowledge Capture 
(K-CAP 2007).  
Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., and 
Weischedel, R. 2006. OntoNotes: The 90% Solu-
tion. In Proc. of HLT-NAACL-2006.  
Kaisser, M. and Lowe, J.B. 2008. Creating a Research 
Collection of Question Answer Sentence Pairs with 
Amazon's Mechanical Turk. In Proc. of the Fifth 
International Conference on Language Resources 
and Evaluation (LREC-2008).  
Kittur, A., Chi, E. H., and Suh, B. 2008. Crowdsourc-
ing user studies with Mechanical Turk. In Proc. of 
the 26th Annual ACM Conference on Human Fac-
tors in Computing Systems (CHI-2008). 
Krippendorf, K. 1980. Content Analysis: An introduc-
tion to its methodology. Sage Publications.  
Marcus, M., Marcinkiewicz, M.A., and Santorini, B. 
1993. Building a large annotated corpus of English: 
the Penn Treebank. Computational Linguistics. 
19:2, June 1993.  
Nakov, P. 2008. Paraphrasing Verbs for Noun Com-
pound Interpretation. In Proc. of the Workshop on 
Multiword Expressions (MWE-2008).  
Palmer, M., Gildea, D., and Kingsbury, P. 2005. The 
Proposition Bank: A Corpus Annotated with Se-
mantic Roles. Computational Linguistics. 31:1.  
Sheng, V.S., Provost, F., and Ipeirotis, P.G. 2008. Get 
another label? improving data quality and data 
mining using multiple, noisy labelers. In Proc. of 
the 14th ACM SIGKDD international Conference 
on Knowledge Discovery and Data Mining (KDD-
2008).  
Singh, P., Lin, T., Mueller, E., Lim, G., Perkins, T., 
and Zhu, W. 2002. Open Mind Common Sense: 
Knowledge acquisition from the general public. In 
Meersman, R. and Tari, Z. (Eds.), LNCS: Vol. 
2519. On the Move to Meaningful Internet Sys-
tems: DOA/CoopIS/ODBASE (pp. 1223-1237). 
Springer-Verlag.  
Snow, R., O?Connor, B., Jurafsky, D., and Ng, A.Y. 
2008. Cheap and Fast ? But is it Good? Evaluating 
Non-Expert Annotations for Natural Language 
Tasks . In Proc. of EMNLP-2008.  
Sorokin, A. and Forsyth, D. 2008. Utility data annota-
tion with Amazon Mechanical Turk. In Proc. of the 
First IEEE Workshop on Internet Vision at CVPR-
2008.  
Stork, D.G. 1999. The Open Mind Initiative. IEEE 
Expert Systems and Their Applications. pp. 16-20, 
May/June 1999.  
Su, Q., Pavlov, D., Chow, J., and Baker, W.C. 2007. 
Internet-scale collection of human-reviewed data. 
In Proc. of the 16th international Conference on 
World Wide Web (WWW-2007).  
Von Ahn, L. and Dabbish, L. 2004. Labeling Images 
with a Computer Game. In Proc. of ACM Confer-
ence on Human Factors in Computing Systmes 
(CHI). pp. 319-326. 
Voorhees, E.M. 2003. Overview of TREC 2003. In 
Proc. of TREC-2003.  
56
SYSTRAN?s Chinese Word Segmentation 
Jin Yang Jean Senellart Remi Zajac 
SYSTRAN Software, Inc. 
9333 Genesee Ave. 
San Diego, CA 92121, USA 
jyang@systransoft.com 
SYSTRAN S.A. 
1, rue du Cimeti?re 
95230 Soisy-sous-Montmorency, France 
senellart@systran.fr 
 
SYSTRAN Software, Inc. 
9333 Genesee Ave. 
San Diego, CA 92121, USA 
zajac@systransoft.com 
Abstract 
SYSTRAN?s Chinese word segmentation 
is one important component of its 
Chinese-English machine translation 
system. The Chinese word segmentation 
module uses a rule-based approach, based 
on a large dictionary and fine-grained 
linguistic rules. It works on general-
purpose texts from different Chinese-
speaking regions, with comparable 
performance. SYSTRAN participated in 
the four open tracks in the First 
International Chinese Word Segmentation 
Bakeoff. This paper gives a general 
description of the segmentation module, 
as well as the results and analysis of its 
performance in the Bakeoff. 
1 Introduction 
Chinese word segmentation is one of the pre-
processing steps of the SYSTRAN Chinese-
English Machine Translation (MT) system. The 
development of the Chinese-English MT system 
began in August 1994, and this is where the 
Chinese word segmentation issue was first 
addressed. The algorithm of the early version of 
the segmentation module was borrowed from 
SYSTRAN?s Japanese segmentation module. The 
program ran on a large word list, which contained 
600,000 entries at the time1. The basic strategy was 
to list all possible matches for an entire linguistic 
unit, then solve the overlapping matches via 
linguistic rules. The development was focused on 
technical domains, and high accuracy was 
achieved after only three months of development. 
Since then, development has shifted to other areas 
of Chinese-English MT, including the enrichment 
of the bi-lingual word lists with part-of-speech, 
syntactic and semantic features. In 2001, the 
development of a prototype Chinese-Japanese MT 
system began. Although the project only lasted for 
three months, some important changes were made 
in the segmentation convention, regarding the 
distinction between words and phrases 2 . Along 
with new developments of the SYSTRAN MT 
engine, the segmentation engine has recently been 
re-implemented. The dictionary and the general 
approach remain unchanged, but dictionary lookup 
and rule matching were re-implemented using 
finite-state technology, and linguistic rules for the 
segmentation module are now expressed using a 
context-free-based formalism, improving 
maintainability. The re-implementation generates 
multiple segmentation results with associated 
probabilities. This will allow for disambiguation at 
a later stage of the MT process, and will widen the 
possibility of word segmentation for other 
applications. 
2 System Description 
2.1 Segmentation Standard 
Our definition of words and our segmentation 
conventions are based on available standards, 
modified for MT purposes. The PRC standard (Liu 
et al, 1993) was initially used. Sample differences 
are listed as follows: 
 
Type PRC SYSTRAN 
NP ???? 
??????? 
??   ?? 
??  ??  ??? 
CD 31? 31  ? 
CD + M ?? ??? ?  ?   ?  ?  ? 
DI4 + CD ?? ?  ? 
Name ?  ? ?  ?? ??   ??? 
 
Table 1. Segmentation Divergences with the PRC Guidelines 
2.2 Methodology 
The SYSTRAN Chinese word segmentation 
module uses a rule-based approach and a large 
dictionary. The dictionary is derived from the 
Chinese-English MT dictionary. It currently 
includes about 400,000 words. The basic 
segmentation strategy is to list all possible matches 
for a translation unit (typically, a sentence), then to 
solve overlapping matches via linguistic rules. The 
same segmentation module and the same 
dictionary are used to segment different types of 
text with comparable performance. 
All dictionary lookup and rule matching are 
performed using a low level Finite State 
Automaton library. The segmentation speed is 
3,500 characters per second using a Pentium 4 
2.4GHZ processor. 
Dictionary 
The Chinese-English MT dictionary currently 
contains 400,000 words (e.g., ??), and 200,000 
multi-word expressions (e.g., ??  ??  ??? ). 
Only words are used for the segmentation. 
Specialized linguistic rules are associated with the 
dictionary. The dictionary is general purpose, with 
good coverage on several domains. Domain-
specific dictionaries are also available, but were 
not used in the Bakeoff. 
The dictionary contains words from different 
Chinese-speaking regions, but the representation is 
mostly in simplified Chinese. The traditional 
characters are considered as ?variants?, and they 
are not physically stored in the dictionary. For 
example, ???  and ???  are stored in the 
dictionary, and ??? can also be found via the 
character matching ?? ?. 
The dictionary is encoded in Unicode (UTF8), 
and all internal operations manipulate UTF8 
strings. Major encoding conversions are supported, 
including GB2312-80, GB13000, BIG-5, BIG5-
HKSCS, etc. 
Training 
The segmentation module has been tested and fine-
tuned on general texts, and on texts in the technical 
and military domains (because of specific customer 
requirements for the MT system). Due to the wide 
availability of news texts, the news domain has 
also recently been used for training and testing. 
The training process is merely reduced to the 
customization of a SYSTRAN MT system. In the 
current version of the MT system, customization is 
achieved by building a User Dictionary (UD). A 
UD supplements the main dictionary: any word 
that is not found in the main MT system dictionary 
is added in a User Dictionary.  
Name-Entity Recognition and Unknown Words 
Name entity recognition is still under development. 
Recognition of Chinese persons? names is done via 
linguistic rules. Foreign name recognition is not 
yet implemented due to the difficulty of obtaining 
translations. 
Due to the unavailability of translations, even 
when an unknown word has been successfully 
recognized, we consider the unknown word 
recognition as part of the terminology extraction 
process. This feature was not integrated for the 
Bakeoff. 
2.3 Evaluation 
Our internal evaluation has been focused on the 
accuracy of segmentation using our own 
segmentation standard. Our evaluation process 
includes large-scale bilingual regression testing for 
the Chinese-English system, as well as regression 
testing of the segmenter itself using a test database 
of over 5MB of test items. Two criteria are used: 
1. Overlapping Ambiguity Strings (OAS): the 
reference segmentation and the segmenter 
segmentation overlap for some string, e.g., 
AB-C and A-BC. As shown below, this 
typically indicates an error from our 
segmenter. 
2. Covering Ambiguity Strings (CAS): the test 
strings that cover the reference strings 
(CAS-T: ABC and AB-C), and the reference 
strings that cover the test strings (CAS-R: 
AB-C and ABC). These cases arise mostly 
from a difference between equally valid 
segmentation standards. 
No evaluation with other standards had been done 
before the Bakeoff. 
 
Test Reference Type 
???  ?? ??  ??? OAS 
???? ? ? ? ? CAS-T 
???? ?? ? ? CAS-T 
?? ? ? CAS-T 
?? ?? ???? CAS-R 
1994  ? 1994 ? CAS-R 
? ? ?? CAS-R 
Table 2. Types of Segmentation Differences 
3 Discussion of the Bakeoff 
3.1 Results 
SYSTRAN participated in the four open tracks in 
the First International Chinese Word Segmentation 
Bakeoff http://www.sighan.org/bakeoff2003/. Each 
track corresponds to one corpus with its own word 
segementation standard. Each corpus had its own 
segmentation standard that was significantly 
different from the others. The training process 
included building a User Dictionary that contains 
words found in the training corpora, but not in the 
SYSTRAN dictionary.  Although each of these 
corpora was segmented according to its own 
standard, we made a single UD containing all the 
words gathered in all corpora.  
Although the ranking of the SYSTRAN 
segmenter is different in the four open tracks, 
SYSTRAN?s segmentation performance is quite 
comparable across the four corpora. This is to be 
compared to the scores obtained by other 
participants, where good performance was 
typically obtained on one corpus only. SYSTRAN 
scores for the 4 tracks are shown in Table 3 (Sproat 
and Emerson, 2003). 
 
Track R P F Roov Riv 
ASo 0.915 0.894 0.904 0.426 0.926 
CTBo 0.891 0.877 0.884 0.733 0.925 
HKo 0.898 0.860 0.879 0.616 0.920 
PKo 0.905 0.869 0.886 0.503 0.934 
Table 3. SYSTRAN?s Scores in the Bakeoff 
3.2 Discussions 
The segmentation differences between the 
reference corpora and SYSTRAN?s results are 
further analyzed. Table 4 shows the  partition of 
divergences between OAS, CAS-T, and CAS-R 
strings:3 
 
 Total Same OAS CAS-T CAS-R 
ASo 11,985 10,970 76 448 491 
CTBo 39,922 35,561 231 2,419 1,711 
HKo 34,959 31,397 217 1,436 1,909 
PKo 17,194 15,554 82 615 943 
Table 4. Count of OAS and CAS Divergence 
The majority of OAS divergences show incorrect 
segmentation from SYSTRAN. However, 
differences in CAS do not necessarily indicate 
incorrect segmentation results. The reasons can be 
categorized as follows: a) different segmentation 
standards, b) unknown word problem, c) name 
entity recognition problem, and d) miscellaneous4. 
The distributions of the differences are further 
analyzed in Table 5 and 6 for the ASo and PKo 
corpora, respectively. 
 
CAS-R: Unique Strings=334 (total=491) 
Type Count Percent Examples 
Different 
Standards 
184 55% ???  
??  
????  
???? 
Unknown  
Words 
116 35% ??  
??  
??  
?? 
Name  
Entity 
30 9% ??  
???  
?? 
Misc. 4 1% ???? 
CAS-T: Unique Strings=137 (total=448) 
Type Count Percent Examples 
Different  
Standard 
134 98% ??  
??? 
???? 
True 
Covering 
3 2% ?? 
?? 
Table 5. Distribution of Divergences in the ASo Track  
 
 
CAS-R: Unique Strings=508 (total=943) 
Type Count Percent Examples 
Different 
Standards 
294 58% ???? 
?? 
?? 
?? 
?? 
2001 ? 
Unknown  
Words 
90 18% ?? 
?? 
?? 
Name  
Entity 
61 12% ??? 
??? 
Misc. 63 12% 20% 
3.9? 
CAS-T: Unique Strings=197 (total=615) 
Type Count Percent Examples 
Different  
Standards 
194 98% ??? 
?? 
??? 
??? 
True 
Covering 
3 2% ?? 
?? 
Table 6. Distribution of Divergences in the PKo Track  
 
This analysis shows that the segmentation results 
are greatly impacted by the difference in the 
segmentation standards. Other problems include 
for example the encoding of numbers using single 
bytes instead of the standard double-byte encoding 
in the PKo corpus, which account for about 12% of 
differences in the PKo track scores.  
4 Conclusion 
For an open track segmentation competition like 
the Bakeoff, we need to achieve a balance between 
the following aspects:  
? Segmentation standards: differences between 
one?s own standard and the reference standard. 
? Adaptation to the other standards: whether one 
should adapt to other standards. 
? Dictionary coverage: the coverage of one?s 
own dictionary and the dictionary obtained by 
training. 
? Algorithm: combination of segmentation, 
unknown word identification, and name entity 
recognition. 
? Speed: the time needed to segment the corpora. 
? Training: time and manpower used for training 
each corpus and track 
Few systems participated in all open tracks: 
only SYSTRAN and one university participated in 
all four. We devoted about 2 person/week for this 
evaluation. We rank in the top three of three open 
tracks, and only the PKo track scores are lower, 
probably because of encoding problems for 
numbers for this corpus (we did not adjust our 
segmenter to cope with this corpus-specific 
problem). Our results are very consistent for all 
open tracks, indicating a very robust approach to 
Chinese segmentation.  
Analysis of results shows that SYSTRAN?s 
Chinese word segmentation excels in the area of 
dictionary coverage, robustness, and speed. The 
vast majority of divergences with the test corpora 
originate from differences in segmentation 
standards (over 55% for CAS-R and about 98% for 
CAS-T). True errors range between 0% and 2% 
only, the rest being assigned to either the lack of 
unknown word processing or the lack of a name 
entity recognizer. Although not integrated, the 
unknown word identification and name entity 
recognition are under development as part of a 
terminology extraction tool. 
For future Chinese word segmentation 
evaluations, some of the issues that arose in this 
Bakeoff would need to be addressed to obtain even 
more significant results, including word 
segmentation standards and encoding problems for 
example. We would also welcome the introduction 
of a surprise track, similar to the surprise track of 
the DARPA MT evaluations that would require 
participants to submit results within 24 hours on an 
unknown corpus. 
References 
Liu, Y, Tan Q. & Shen, X. 1993. Segmentation 
Standard for Modern Chinese Information Processing 
and Automatic Segmentation Methodology. 
Sproat, R., & Emerson T. 2003.  The First International 
Chinese Word Segmentation Bakeoff.  In the 
Proceedings of the Second SIGHAN Workshop on 
Chinese Language Processing. ACL03. 
 
                                                          
1  The word list only contained Chinese-English 
bilingual dictionary without any syntactic or semantic 
features. It also contained many compound nouns, e.g. 
????. 
2 Compound nouns are no longer considered as words. 
They were moved to the expression dictionary. For 
example, ???? has become ?? ??. 
3 The number of words in the reference strings is used 
when counting OAS and CAS divergences. For 
example, ?????s CAS count is three because the 
number of words in the reference string ?? ? ? is 
three. 
4 Word segmentation in SYSTRAN MT systems occurs 
after sentence identification and normalization. During 
word segmentation, Chinese numbers are converted into 
Arabic numbers. 
