Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 459?467,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Projecting Parameters for Multilingual Word Sense Disambiguation 
 
Mitesh M. Khapra Sapan Shah Piyush Kedia Pushpak Bhattacharyya 
Department of Computer Science and Engineering 
Indian Institute of Technology, Bombay 
Powai, Mumbai ? 400076, 
Maharashtra, India. 
{miteshk,sapan,charasi,pb}@cse.iitb.ac.in 
 
Abstract 
We report in this paper a way of doing Word 
Sense Disambiguation (WSD) that has its ori-
gin in multilingual MT and that is cognizant 
of the fact that parallel corpora, wordnets and 
sense annotated corpora are scarce re-
sources. With respect to these resources, lan-
guages show different levels of readiness; 
however a more resource fortunate language 
can help a less resource fortunate language. 
Our WSD method can be applied to a lan-
guage even when no sense tagged corpora for 
that language is available. This is achieved by 
projecting wordnet and corpus parameters 
from another language to the language in 
question. The approach is centered around a 
novel synset based multilingual dictionary and 
the empirical observation that within a domain 
the distribution of senses remains more or less 
invariant across languages. The effectiveness 
of our approach is verified by doing parameter 
projection and then running two different 
WSD algorithms. The accuracy values of ap-
proximately 75% (F1-score) for three lan-
guages in two different domains establish the 
fact that within a domain it is possible to cir-
cumvent the problem of scarcity of resources 
by projecting parameters like sense distribu-
tions, corpus-co-occurrences, conceptual dis-
tance, etc. from one language to another. 
1 Introduction 
Currently efforts are on in India to build large scale 
Machine Translation and Cross Lingual Search 
systems in consortia mode. These efforts are large, 
in the sense that 10-11 institutes and 6-7 languages 
spanning the length and breadth of the country are 
involved.  The approach taken for translation is 
transfer based which needs to tackle the problem of 
word sense disambiguation (WSD) (Sergei et. al., 
2003).  Since 90s machine learning based ap-
proaches to WSD using sense marked corpora have 
gained ground (Eneko Agirre & Philip Edmonds, 
2007). However, the creation of sense marked cor-
pora has always remained a costly proposition. 
Statistical MT has obviated the need for elaborate 
resources for WSD, because WSD in SMT hap-
pens implicitly through parallel corpora (Brown et. 
al., 1993). But parallel corpora too are a very cost-
ly resource.  
The above situation brings out the challenges 
involved in Indian language MT and CLIR. Lack 
of resources coupled with the multiplicity of Indian 
languages severely affects the performance of sev-
eral NLP tasks. In the light of this, we focus on the 
problem of developing methodologies that reuse 
resources. The idea is to do the annotation work 
for one language and find ways of using them for 
another language. 
Our work on WSD takes place in a multilingual 
setting involving Hindi (national language of India; 
500 million speaker base), Marathi (20 million 
speaker base), Bengali (185 million speaker base) 
and Tamil (74 million speaker base). The wordnet 
of Hindi and sense marked corpora of Hindi are 
used for all these languages. Our methodology 
rests on a novel multilingual dictionary organiza-
tion and on the idea of ?parameter projection? from 
Hindi to the other languages. Also the domains of 
interest are tourism and health. 
The roadmap of the paper is as follows. Section 
2 describes related work. In section 3 we introduce 
the parameters essential for domain-specific WSD. 
Section 4 builds the case for parameter projection. 
Section 5 introduces the Multilingual Dictionary 
Framework which plays a key role in parameter 
projection. Section 6 is the core of the work, where 
we present parameter projection from one language 
to another. Section 7 describes two WSD algo-
rithms which combine various parameters for do-
459
main-specific WSD. Experiments and results are 
presented in sections 8 and 9. Section 10 concludes 
the paper. 
2 Related work 
Knowledge based approaches to WSD such as 
Lesk?s algorithm (Michael Lesk, 1986), Walker?s 
algorithm (Walker D. & Amsler R., 1986), concep-
tual density (Agirre Eneko & German Rigau, 1996) 
and random walk algorithm (Mihalcea Rada, 2005) 
essentially do Machine Readable Dictionary loo-
kup. However, these are fundamentally overlap 
based algorithms which suffer from overlap sparsi-
ty, dictionary definitions being generally small in 
length.  
Supervised learning algorithms for WSD are 
mostly word specific classifiers, e.g., WSD using 
SVM (Lee et. al., 2004), Exemplar based WSD 
(Ng Hwee T. & Hian B. Lee, 1996) and decision 
list based algorithm (Yarowsky, 1994). The re-
quirement of a large training corpus renders these 
algorithms unsuitable for resource scarce languag-
es. 
Semi-supervised and unsupervised algorithms 
do not need large amount of annotated corpora, but 
are again word specific classifiers, e.g., semi-
supervised decision list algorithm (Yarowsky, 
1995) and Hyperlex (V?ronis Jean, 2004)). Hybrid 
approaches like WSD using Structural Semantic 
Interconnections (Roberto Navigli & Paolo Velar-
di, 2005) use combinations of more than one 
knowledge sources (wordnet as well as a small 
amount of tagged corpora). This allows them to 
capture important information encoded in wordnet 
(Fellbaum, 1998) as well as draw syntactic genera-
lizations from minimally tagged corpora.  
At this point we state that no single existing so-
lution to WSD completely meets our requirements 
of multilinguality, high domain accuracy and 
good performance in the face of not-so-large 
annotated corpora. 
3 Parameters for WSD  
We discuss a number of parameters that play a 
crucial role in WSD. To appreciate this, consider 
the following example: 
 
The river flows through this region to meet the sea. 
 
The word sea is ambiguous and has three senses as 
given in the Princeton Wordnet (PWN): 
S1: (n) sea (a division of an ocean or a large body 
of salt water partially enclosed by land) 
S2: (n) ocean, sea (anything apparently limitless in 
quantity or volume) 
S3: (n) sea (turbulent water with swells of consi-
derable size) "heavy seas" 
Our first parameter is obtained from Domain 
specific sense distributions. In the above example, 
the first sense is more frequent in the tourism do-
main (verified from manually sense marked tour-
ism corpora). Domain specific sense distribution 
information should be harnessed in the WSD task. 
The second parameter arises from the domin-
ance of senses in the domain. Senses are ex-
pressed by synsets, and we define a dominant 
sense as follows: 
 
A few dominant senses in the Tourism domain are 
{place, country, city, area}, {body of water}, {flo-
ra, fauna}, {mode of transport} and {fine arts}. In 
disambiguating a word, that sense which belongs 
to the sub-tree of a domain-specific dominant 
sense should be given a higher score than other 
senses. The value of this parameter (?) is decided 
as follows: 
? = 1; if the candidate synset is a dominant synset 
? = 0.5; if the candidate synset belongs to the sub-
tree of a dominant synset 
? = 0.001; if the candidate synset is neither a do-
minant synset nor belongs to the sub-tree of a do-
minant synset. 
Our third parameter comes from Corpus co-
occurrence. Co-occurring monosemous words as 
well as already disambiguated words in the con-
text help in disambiguation. For example, the word 
river appearing in the context of sea is a mono-
semous word. The frequency of co-occurrence of 
river with the ?water body? sense of sea is high in 
the tourism domain. Corpus co-occurrence is cal-
A synset node in the wordnet hypernymy 
hierarchy is called Dominant if the syn-
sets in the sub-tree below the synset are 
frequently occurring in the domain cor-
pora. 
460
culated by considering the senses which occur in a 
window of 10 words around a sense. 
Our fourth parameter is based on the semantic 
distance between any pair of synsets in terms of 
the shortest path length between two synsets in the 
wordnet graph. An edge in the shortest path can be 
any semantic relation from the wordnet relation 
repository (e.g., hypernymy, hyponymy, meronymy, 
holonymy, troponymy etc.). 
For nouns we do something additional over and 
above the semantic distance. We take advantage of 
the deeper hierarchy of noun senses in the wordnet 
structure. This gives rise to our fifth and final pa-
rameter which arises out of the conceptual dis-
tance between a pair of senses. Conceptual 
distance between two synsets S1 and S2 is calcu-
lated using Equation (1), motivated by Agirre Ene-
ko & German Rigau (1996). 
 
Concep-
tual 
Distance    
(S1, S2) 
 
 
 
= 
Length of the path between (S1, 
S2) in terms of hypernymy hie-
rarchy 
Height of the lowest common 
ancestor of S1 and S2 in the word-
net hierarchy 
 
 
 (1) 
The conceptual distance is proportional to the 
path length between the synsets, as it should be. 
The distance is also inversely proportional to the 
height of the common ancestor of two sense nodes, 
because as the common ancestor becomes more 
and more general the conceptual relatedness tends 
to get vacuous (e.g., two nodes being related 
through entity which is the common ancestor of 
EVERYTHING, does not really say anything 
about the relatedness). 
To summarize, our various parameters used for 
domain-specific WSD are: 
Wordnet-dependent parameters  
? belongingness-to-dominant-concept 
? conceptual-distance 
? semantic-distance 
Corpus-dependent parameters 
? sense distributions 
? corpus co-occurrence. 
In section 7 we show how these parameters are 
used to come up with a scoring function for WSD. 
4 Building a case for Parameter Projec-
tion   
Wordnet-dependent parameters depend on the 
graph based structure of Wordnet whereas the 
Corpus-dependent parameters depend on various 
statistics learnt from a sense marked corpora. Both 
the tasks of (a) constructing a wordnet from scratch 
and (b) collecting sense marked corpora for mul-
tiple languages are tedious and expensive. An im-
portant question being addressed in this paper is: 
whether the effort required in constructing seman-
tic graphs for multiple wordnets and collecting 
sense marked corpora can be avoided? Our find-
ings seem to suggest that by projecting relations 
from the wordnet of a language and by projecting 
corpus statistics from the sense marked corpora of 
the language we can achieve this end. Before we 
proceed to discuss the way to realize parameter 
projection, we present a novel dictionary which 
facilitates this task. 
5 Synset based multilingual dictionary  
Parameter projection as described in section 4 rests 
on a novel and effective method of storage and use 
of dictionary in a multilingual setting proposed by 
Mohanty et. al. (2008). For the purpose of current 
discussion, we will call this multilingual dictionary 
framework MultiDict. One important departure 
from traditional dictionary is that synsets are 
linked, and after that the words inside the syn-
sets are linked. The basic mapping is thus be-
tween synsets and thereafter between the words.  
 
Concepts L1 
(Eng-
lish) 
L2 (Hindi) L3 (Mara-
thi) 
04321: a 
youthful 
male per-
son 
{male
child, 
boy} 
{????? ladkaa, 
????  baalak,  
????? 
bachchaa}  
{?????  mulgaa , 
?????  porgaa , 
???  por } 
Table 1: Multilingual Dictionary Framework 
Table 1 shows the structure of MultiDict, with one 
example row standing for the concept of boy. The 
first column is the pivot describing a concept with 
a unique ID. The subsequent columns show the 
words expressing the concept in respective lan-
guages (in the example table above, English, Hindi 
and Marathi). Thus to express the concept ?04321: 
a youthful male person?, there are two lexical ele-
ments in English, which constitute a synset. Cor-
respondingly, the Hindi and Marathi synsets 
contain 3 words each. 
461
It may be noted that the central language whose 
synsets the synsets of other languages link to is 
Hindi. This way of linking synsets- more popularly 
known as the expansion approach- has several ad-
vantages as discussed in (Mohanty et. al., 2008). 
One advantage germane to the point of this paper 
is that the synsets in a particular column automati-
cally inherit the various semantic relations of the 
Hindi wordnet (Dipak Narayan et. al., 2000), 
which saves the effort involved in reconstructing 
these relations for multiple languages. 
After the synsets are linked, cross linkages are 
set up manually from the words of a synset to the 
words of a linked synset of the central language. 
The average number of such links per synset per 
language pair is approximately 3. These cross-
linkages actually solve the problem of lexical 
choice in translating from text of one language to 
another. 
Thus for the Marathi word ?????  {mulagaa} de-
noting ?a youthful male person?, the correct lexi-
cal substitute from the corresponding Hindi synset 
is ????? {ladakaa} (Figure 1). One might argue that 
any word within the synset could serve the purpose 
of translation. However, the exact lexical substitu-
tion has to respect native speaker acceptability.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: Cross linked synset members for the 
concept: a youthful male person 
We put these cross linkages to another use, as 
described later. 
Since it is the MultiDict which is at the heart of 
parameter projection, we would like to summarize 
the main points of this section. (1) By linking with 
the synsets of Hindi, the cost of building wordnets 
of other languages is partly reduced (semantic rela-
tions are inherited). The wordnet parameters of 
Hindi wordnet now become projectable to other 
languages. (2) By using the cross linked words in 
the synsets, corpus parameters become projectable 
(vide next section).  
6 Parameter projection using MultDict  
6.1 P(Sense|Word) parameter 
Suppose a word (say, W) in language L1 (say, Ma-
rathi) has k senses. For each of these k senses we 
are interested in finding the parameter P(Si|W)- 
which is the probability of sense Si given the word 
W expressed as: 
? ??  ?) =  
#(??  ,?)
 #(??  ,?)?  
 
where ?#? indicates ?count-of?. Consider the exam-
ple of two senses of the Marathi word ???? 
{saagar}, viz., sea and abundance and the corres-
ponding cross-linked words in Hindi (Figure 2 be-
low): 
     Marathi            Hindi 
 
 
 
 
 
 
Figure 2: Two senses of the Marathi word ???? 
(saagar), viz., {water body} and {abundance}, and 
the corresponding cross-linked words in Hindi1. 
The probability P({water body}|saagar) for Mara-
thi is  
#({????? ????}, ??????)
#({????? ????}, ??????) + #({?????????}, ??????)
 
 
We propose that this can be approximated by the 
counts from Hindi sense marked corpora by replac-
ing saagar with the cross linked Hindi words sa-
mudra and saagar, as per Figure 2: 
 
#({water body}, samudra)
#({water body}, samudra) + #({abundance}, saagar)
 
                                                          
1 Sense_8231 shows the same word saagar for both Marathi 
and Hindi. This is not uncommon, since Marathi and Hindi are 
sister languages. 
????? 
/MW1 
mulagaa,  
????? 
/MW2 
poragaa, 
??? /MW3 
pora  
 
  ????? 
/HW1 
ladakaa,  
???? 
/HW2 
baalak, 
????? /HW3 
bachcha, 
???? /HW4 
choraa  
 
 
 
male-child 
/HW1, 
 
boy 
/HW2  
 
 
 
 
Marathi Synset Hindi Synset   English Synset 
Sense_2650 
Sense_8231 
 
saagar (sea) 
{water body} 
saagar (sea) 
{abundance} 
samudra (sea) 
{water body} 
saagar (sea) 
{abundance} 
462
Thus, the following formula is used for calculat-
ing the sense distributions of Marathi words using 
the sense marked Hindi corpus from the same do-
main: 
? ?? ?) =  
#(??  , ?????_??????_?????_????)
 #(??  , ?????_??????_?????_????)?  
           (2) 
Note that we are not interested in the exact sense 
distribution of the words, but only in the relative 
sense distribution.  
To prove that the projected relative distribution 
is faithful to the actual relative distribution of 
senses, we obtained the sense distribution statistics 
of a set of Marathi words from a sense tagged Ma-
rathi corpus (we call the sense marked corpora of a 
language its self corpora). These sense distribu-
tion statistics were compared with the statistics for 
these same words obtained by projecting from a 
sense tagged Hindi corpus using Equation (2).  The 
results are summarized in Table 2. 
Sr. 
No 
Marathi 
Word 
Synset P(S|word) 
as learnt 
from 
sense 
tagged 
Marathi 
corpus 
P(S|word) as 
projected 
from sense 
tagged 
Hindi cor-
pus 
1 ???? ?? 
(kimat)  
{ worth } 0.684 0.714 
{ price }  0.315 0.285 
2 ????? 
(rasta)  
 
{ roadway } 0.164 0.209 
{road, 
route} 
0.835 0.770 
3 ????? 
(thikan) 
{ land site, 
place} 
0.962 0.878 
{ home } 0.037 0.12 
4 ???? 
(saagar) 
{water 
body} 
1.00 1.00 
{abun-
dance} 
0 0 
Table 2: Comparison of the sense distributions of 
some Marathi words learnt from Marathi sense 
tagged corpus with those projected from Hindi 
sense tagged corpus. 
The fourth row of Table 2 shows that whenever 
???? (saagar) (sea) appears in the Marathi tourism 
corpus there is a 100% chance that it will appear in 
the ?water body? sense and 0% chance that it will 
appear in the sense of ?abundance?. Column 5 
shows that the same probability values are ob-
tained using projections from Hindi tourism cor-
pus. Taking another example, the third row shows 
that whenever ????? (thikaan) (place, home) ap-
pears in the Marathi tourism corpus there is a much 
higher chance of it appearing in the sense of 
?place? (96.2%) then in the sense of ?home? 
(3.7%). Column 5 shows that the relative proba-
bilities of the two senses remain the same even 
when using projections from Hindi tourism corpus 
(i.e. by using the corresponding cross-linked words 
in Hindi). To quantify these observations, we cal-
culated the average KL divergence and Spearman?s 
correlation co-efficient between the two distribu-
tions. The KL divergence is 0.766 and Spearman?s 
correlation co-efficient is 0.299. Both these values 
indicate that there is a high degree of similarity 
between the distributions learnt using projection 
and those learnt from the self corpus. 
6.2 Co-occurrence parameter 
Similarly, within a domain, the statistics of co-
occurrence of senses remain the same across lan-
guages. For example, the co-occurrence of the Ma-
rathi synsets {???? (akash) (sky), ????? (ambar) 
(sky)} and {??? (megh) (cloud), ???? (abhra) 
(cloud)} in the Marathi corpus remains more or 
less same as (or proportional to) the co-occurrence 
between the corresponding Hindi synsets in the 
Hindi corpus.   
Sr. No Synset Co-
occurring 
Synset 
P(co-
occurrence) 
as learnt 
from sense 
tagged 
Marathi 
corpus 
P(co-
occurrence) 
as learnt 
from sense 
tagged 
Hindi 
corpus 
1 {???, ?????} 
{small bush} 
{???, ?????, 
?????, ?????, 
???, ????}  
{tree} 
0.125 0.125 
2 {???, ????} 
{cloud} 
{????, 
????, 
?????}  
{sky} 
0.167 0.154 
3 {?????, ?^???, 
?^???, 
??????}  
{geographical 
area} 
{????, 
???}  
{travel} 
0.0019 0.0017 
Table 3: Comparison of the corpus co-occurrence 
statistics learnt from Marathi and Hindi Tourism 
corpus. 
463
Table 3 shows a few examples depicting similarity 
between co-occurrence statistics learnt from Mara-
thi tourism corpus and Hindi tourism corpus. Note 
that we are talking about co-occurrence of synsets 
and not words. For example, the second row shows 
that the probability of co-occurrence of the synsets 
{cloud} and {sky} is almost same in the Marathi 
and Hindi corpus. 
7 Our algorithms for WSD 
We describe two algorithms to establish the use-
fulness of the idea of parameter projection. The 
first algorithm- called iterative WSD (IWSD-) is 
greedy, and the second based on PageRank algo-
rithm is exhaustive. Both use scoring functions that 
make use of the parameters detailed in the previous 
sections.  
7.1 Iterative WSD (IWSD) 
We have been motivated by the Energy expression 
in Hopfield network (Hopfield, 1982) in formulat-
ing a scoring function for ranking the senses. Hop-
field Network is a fully connected bidirectional 
symmetric network of bi-polar (0/1 or +1/-1) neu-
rons. We consider the asynchronous Hopfield 
Network. At any instant, a randomly chosen neu-
ron (a) examines the weighted sum of the input, (b) 
compares this value with a threshold and (c) gets to 
the state of 1 or 0, depending on whether the input 
is greater than or less than or equal to the thre-
shold. The assembly of 0/1 states of individual 
neurons defines a state of the whole network. Each 
state has associated with it an energy, E, given by 
the following expression 
 
? = ????? +  ???
?
?>?
?
?=1
????  
 
(3) 
 
where, N is the total number of neurons in the net-
work, ??   and ??  are the activations of neurons i and 
j respectively and ???  is the weight of the connec-
tion between neurons i and j.  Energy is a funda-
mental property of Hopfield networks, providing 
the necessary machinery for discussing conver-
gence, stability and such other considerations. 
The energy expression as given above cleanly 
separates the influence of self-activations of neu-
rons and that of interactions amongst neurons to 
the global macroscopic property of energy of the 
network.  This fact has been the primary insight for 
equation (4) which was proposed to score the most 
appropriate synset in the given context. The cor-
respondences are as follows:   
 
Neuron ? Synset 
Self-activation ? Corpus Sense Distribu-
tion 
Weight of connec-
tion between two 
neurons 
 
? 
Weight as a function of 
corpus co-occurrence 
and Wordnet distance 
measures between syn-
sets 
 
?? = argmax
?
  ?? ? ?? +  ??? ? ?? ? ??
?  ? J
   4  
?????, 
  J = ??? ?? ????????????? ?????            
         ?? = ?????????????????????????????? (??)
   ??  = ? ??  | ????                                                   
 
 ??? =  ??????????????????  ?? , ??                    
                 ?  1 ????????????????????(?? , ?? )           
                   ?  1 ???????????????????????(?? , ?? )    
 
The component ?? ? ??  of the energy due to the self 
activation of a neuron can be compared to the cor-
pus specific sense of a word in a domain. The other 
component ??? ?  ?? ? ??  coming from the interaction 
of activations can be compared to the score of a 
sense due to its interaction in the form of corpus 
co-occurrence, conceptual distance, and wordnet-
based semantic distance with the senses of other 
words in the sentence. The first component thus 
captures the rather static corpus sense, whereas the 
second expression brings in the sentential context.  
Algorithm 1: performIterativeWSD(sentence) 
1. Tag all monosemous words in the sentence. 
2. Iteratively disambiguate the remaining words in the 
sentence in increasing order of their degree of polyse-
my. 
3. At each stage select that sense for a word which max-
imizes the score given by Equation (4) 
Algorithm1: Iterative WSD  
IWSD is clearly a greedy algorithm. It bases its 
decisions on already disambiguated words, and 
ignores words with higher degree of polysemy. For 
example, while disambiguating bisemous words, 
the algorithm uses only the monosemous words. 
464
7.2 Modified PageRank algorithm 
Rada Mihalcea (2005) proposed the idea of using 
PageRank algorithm to find the best combination 
of senses in a sense graph. The nodes in a sense 
graph correspond to the senses of all the words in a 
sentence and the edges depict the strength of inte-
raction between senses. The score of each node in 
the graph is then calculated using the following 
recursive formula: 
????? ?? =                                                                 
 1? d + d ?  
Wij
 WjkSk?Out  Si 
? Score Sj 
S j?In S i 
 
Instead of calculating Wij  based on the overlap 
between the definition of senses Si and S  as pro-
posed by Rada Mihalcea (2005), we calculate the 
edge weights using the following formula: 
 ??? =  ??????????????????  ?? , ??                    
                   ?  1 ???????????????????? ?? , ??             
                   
?  1 ??????????????????????? ?? , ??    
?  ? ??  | ?????                                                   
?  ? ??  | ?????                                                   
  
? = ??????? ??????  ????????? 0.85     
 
This formula helps capture the edge weights in 
terms of the corpus bias as well as the interaction 
between the senses in the corpus and wordnet. It 
should be noted that this algorithm is not greedy. 
Unlike IWSD, this algorithm allows all the senses 
of all words to play a role in the disambiguation 
process.  
8 Experimental Setup: 
We tested our algorithm on tourism corpora in 3 
languages (viz., Marathi, Bengali and Tamil) and 
health corpora in 1 language (Marathi) using pro-
jections from Hindi. The corpora for both the do-
mains were manually sense tagged. A 4-fold cross 
validation was done for all the languages in both 
the domains. The size of the corpus for each lan-
guage is described in Table 4. 
Language # of polysemous words 
(tokens) 
Tourism 
Domain 
Health 
Domain 
Hindi 50890 29631 
Marathi 32694 8540 
Bengali 9435  - 
Tamil 17868 - 
Table 4: Size of manually sense tagged corpora for 
different languages. 
 
Table 5 shows the number of synsets in MultiDict 
for each language. 
Language # of synsets in 
MultiDict 
Hindi 29833 
Marathi 16600 
Bengali 10732 
Tamil 5727 
Table 5: Number of synsets for each language 
 
Algorithm Language 
Marathi Bengali 
P  % R % F % P  % R % F % 
IWSD (training on self corpora; no parameter pro-
jection) 81.29 80.42 80.85 81.62 78.75 79.94 
IWSD (training on Hindi and reusing parameters  
for another language) 73.45 70.33 71.86 79.83 79.65 79.79 
PageRank (training on self corpora; no parameter 
projection) 79.61 79.61 79.61 76.41 76.41 76.41 
PageRank (training on Hindi and reusing parame-
ters  for another language) 71.11 71.11 71.11 75.05 75.05 75.05 
Wordnet Baseline 58.07 58.07 58.07 52.25 52.25 52.25 
Table 6: Precision, Recall and F-scores of IWSD, PageRank and Wordnet Baseline. Values are re-
ported with and without parameter projection. 
 
465
9 Results and Discussions 
Table 6 shows the results of disambiguation (preci-
sion, recall and F-score). We give values for two 
algorithms in the tourism domain: IWSD and Pa-
geRank. In each case figures are given for both 
with and without parameter projection. The word-
net baseline figures too are presented for the sake 
of grounding the results.  
Note the lines of numbers in bold, and compare 
them with the numbers in the preceding line. This 
shows the fall in accuracy value when one tries the 
parameter projection approach in place of self cor-
pora. For example, consider the F-score as given 
by IWSD for Marathi. It degrades from about 81% 
to 72% in using parameter projection in place of 
self corpora.  Still, the value is much more than the 
baseline, viz., the wordnet first sense (a typically 
reported baseline). 
Coming to PageRank for Marathi, the fall in ac-
curacy is about 8%. Appendix A shows the corres-
ponding figure for Tamil with IWSD as 10%. 
Appendix B reports the fall to be 11% for a differ-
ent domain- Health- for Marathi (using IWSD).  
In all these cases, even after degradation the per-
formance is far above the wordnet baseline. This 
shows that one could trade accuracy with the cost 
of creating sense annotated corpora.  
10 Conclusion and Future Work: 
Based on our study for 3 languages and 2 domains, 
we conclude the following: 
(i) Domain specific sense distributions- if 
obtainable- can be exploited to advantage. 
(ii) Since sense distributions remain same across 
languages, it is possible to create a disambiguation 
engine that will work even in the absence of sense 
tagged corpus for some resource deprived 
language, provided (a) there are aligned and cross 
linked sense dictionaries for the language in 
question and another resource rich language, (b) 
the domain in which disambiguation needs to be 
performed for the resource deprived language is 
the same as the domain for which sense tagged 
corpora is available for the resource rich language.  
(iii) Provided the accuracy reduction is not drastic, 
it may make sense to trade high accuracy for the 
effort in collecting sense marked corpora.  
It would be interesting to test our algorithm on 
other domains and other languages to conclusively 
establish the effectiveness of parameter projection 
for multilingual WSD.  
It would also be interesting to analyze the con-
tribution of corpus and wordnet parameters inde-
pendently. 
References  
Agirre Eneko & German Rigau. 1996. Word sense dis-
ambiguation using conceptual density. In Proceed-
ings of the 16th International Conference on 
Computational Linguistics (COLING), Copenhagen, 
Denmark. 
Dipak Narayan, Debasri Chakrabarti, Prabhakar Pande 
and P. Bhattacharyya. 2002. An Experience in Build-
ing the Indo WordNet - a WordNet for Hindi. First 
International Conference on Global WordNet, My-
sore, India. 
Eneko Agirre & Philip Edmonds. 2007. Word Sense 
Disambiguation Algorithms and Applications. Sprin-
ger Publications. 
Fellbaum, C. 1998. WordNet: An Electronic Lexical 
Database. The MIT Press.  
Hindi Wordnet. 
http://www.cfilt.iitb.ac.in/wordnet/webhwn/ 
J. J. Hopfield. April 1982. "Neural networks and physi-
cal systems with emergent collective computational 
abilities", Proceedings of the National Academy of 
Sciences of the USA, vol. 79 no. 8 pp. 2554-2558. 
Lee Yoong K., Hwee T. Ng & Tee K. Chia. 2004. Su-
pervised word sense disambiguation with support 
vector machines and multiple knowledge sources. 
Proceedings of Senseval-3: Third International 
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, Barcelona, Spain, 137-140. 
Lin Dekang. 1997. Using syntactic dependency as local 
context to resolve word sense ambiguity. In Proceed-
ings of the 35th Annual Meeting of the Association 
for Computational Linguistics (ACL), Madrid, 64-71. 
Michael Lesk. 1986. Automatic sense disambiguation 
using machine readable dictionaries: how to tell a 
pine cone from an ice cream cone. In Proceedings of 
the 5th annual international conference on Systems 
documentation, Toronto, Ontario, Canada. 
Mihalcea Rada. 2005. Large vocabulary unsupervised 
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of 
the Joint Human Language Technology and Empiri-
466
cal Methods in Natural Language Processing Confe-
rence (HLT/EMNLP), Vancouver, Canada, 411-418. 
Ng Hwee T. & Hian B. Lee. 1996. Integrating multiple 
knowledge sources to disambiguate word senses: An 
exemplar-based approach. In Proceedings of the 34th 
Annual Meeting of the Association for Computation-
al Linguistics (ACL), Santa Cruz, U.S.A., 40-47. 
Peter F. Brown and Vincent J.Della Pietra and Stephen 
A. Della Pietra and Robert. L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics Vol 
19, 263-311. 
Rajat Mohanty, Pushpak Bhattacharyya, Prabhakar 
Pande, Shraddha Kalele, Mitesh Khapra and Aditya 
Sharma. 2008. Synset Based Multilingual Dictionary: 
Insights, Applications and Challenges. Global Word-
net Conference, Szeged, Hungary, January 22-25. 
Resnik Philip. 1997. Selectional preference and sense 
disambiguation. In Proceedings of ACL Workshop 
on Tagging Text with Lexical Semantics, Why, What 
and How? Washington, U.S.A., 52-57. 
Roberto Navigli, Paolo Velardi. 2005. Structural Se-
mantic Interconnections: A Knowledge-Based Ap-
proach to Word Sense Disambiguation. IEEE 
Transactions On Pattern Analysis and Machine Intel-
ligence. 
Sergei Nirenburg, Harold Somers, and Yorick Wilks. 
2003. Readings in Machine Translation. Cambridge, 
MA: MIT Press. 
V?ronis Jean. 2004. HyperLex: Lexical cartography for 
information retrieval. Computer Speech & Language, 
18(3):223-252. 
Walker D. and Amsler R. 1986. The Use of Machine 
Readable Dictionaries in Sublanguage Analysis. In 
Analyzing Language in Restricted Domains, Grish-
man and Kittredge (eds), LEA Press, pp. 69-83. 
Yarowsky David. 1994. Decision lists for lexical ambi-
guity resolution: Application to accent restoration in 
Spanish and French. In Proceedings of the 32nd An-
nual Meeting of the association for Computational 
Linguistics (ACL), Las Cruces, U.S.A., 88-95. 
Yarowsky David. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics (ACL), 
Cambridge, MA, 189-196. 
 
 
Appendix A: Results for Tamil (Tourism 
Domain) 
Algorithm P  % R  % F % 
IWSD (training on 
Tamil) 89.50 88.18 88.83 
IWSD (training on 
Hindi and reusing  for 
Tamil) 84.60 73.79 78.82 
Wordnet Baseline 65.62 65.62 65.62 
Table 7: Tamil Tourism corpus using parameters 
projected from Hindi 
Appendix B: Results for Marathi (Health 
Domain) 
Algorithm 
Words 
P  % R  % F % 
IWSD (training on Mara-
thi) 84.28 81.25 82.74 
IWSD (training on Hindi 
and reusing  for Marathi) 75.96 67.75 71.62 
Wordnet Baseline 60.32 60.32 60.32 
Table 8: Marathi Health corpus parameters pro-
jected from Hindi 
 
467
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 84?87,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Improving transliteration accuracy using word-origin detection and
lexicon lookup
Mitesh M. Khapra
IIT Bombay
miteshk@cse.iitb.ac.in
Pushpak Bhattacharyya
IIT Bombay
pb@cse.iitb.ac.in
Abstract
We propose a framework for translit-
eration which uses (i) a word-origin
detection engine (pre-processing) (ii) a
CRF based transliteration engine and (iii)
a re-ranking model based on lexicon-
lookup (post-processing). The results
obtained for English-Hindi and English-
Kannada transliteration show that the pre-
processing and post-processing modules
improve the top-1 accuracy by 7.1%.
1 Introduction
Machine transliteration is the method of automati-
cally converting Out-Of-Vocabulary (OOV) words
in one language to their phonetic equivalents in
another language. An attempt is made to retain
the original pronunciation of the source word to
as great an extent as allowed by the orthographic
and phonological rules of the target language. This
is not a great challenge for language pairs like
Hindi-Marathi which have very similar alphabetic
and phonetic sets. However, the problem becomes
non-trivial for language pairs like English-Hindi
and English-Kannada which have reasonably dif-
ferent alphabet sets and sound systems.
Machine transliteration find its application in
Cross-Lingual Information Retrieval (CLIR) and
Machine Translation (MT). In CLIR, machine
transliteration can help in translating the OOV
terms like proper names and technical terms which
frequently appear in the source language queries
(e.g. Jaipur in ?Jaipur palace?). Similarly it can
help improve the performance of MT by translat-
ing proper names and technical terms which are
not present in the translation dictionary.
Current models for transliteration can be clas-
sified as grapheme-based models, phoneme-based
models and hybrid models. Grapheme-based mod-
els like source channel model (Lee and Choi,
1998), Maximum Entropy Model (Goto et al,
2003), Conditional Random Fields (Veeravalli et
al., 2008) and Decision Trees (Kang and Choi,
2000) treat transliteration as an orthographic pro-
cess and try to map the source graphemes di-
rectly to the target graphemes. Phoneme based
models like the ones based on Weighted Finite
State Transducers (WFST) (Knight and Graehl,
1997) and extended Markov window (Jung et al,
2000) treat transliteration as a phonetic process
rather than an orthographic process. Under this
framework, transliteration is treated as a conver-
sion from source grapheme to source phoneme
followed by a conversion from source phoneme
to target grapheme. Hybrid models either use a
combination of a grapheme based model and a
phoneme based model (Stalls and Knight, 1998)
or capture the correspondence between source
graphemes and source phonemes to produce target
language graphemes (Oh and Choi, 2002).
Combining any of the above transliteration en-
gines with pre-processing modules like word-
origin detection (Oh and Choi, 2002) and/or
post-processing modules like re-ranking using
clues from monolingual resources (Al-Onaizan
and Knight, 2002) can enhance the performance of
the system. We propose such a framework which
uses (i) language model based word-origin detec-
tion (ii) CRF based transliteration engine and (iii)
a re-ranking model based on lexicon lookup on the
target language (Hindi and Kannada in our case).
The roadmap of the paper is as follows. In
section 2 we describe the 3 components of the
proposed framework. In section 3 we present
the results for English-Hindi and English-Kannada
transliteration on the datasets (Kumaran and
Kellner, 2007) released for NEWS 2009 Ma-
chine Transliteration Shared Task1(Haizhou et al,
2009). Section 4 concludes the paper.
1https://translit.i2r.a-star.edu.sg/news2009/
84
2 Proposed framework for
Transliteration
Figure 1: Proposed framework for transliteration.
2.1 Word Origin Detection
To emphasize the importance of Word Origin De-
tection we consider the example of letter ?d?.
When ?d? appears in a name of Western origin (e.g.
Daniel, Durban) and is not followed by the letter
?h?, it invariably gets transliterated as Hindi letter
X, whereas, if it appears in a name of Indic origin
(e.g. Indore, Jharkhand) then it is equally likely to
be transliterated as d or X. This shows that the de-
cision is influenced by the origin of the word. The
Indic dataset (Hindi, Kannada, and Tamil) for the
Shared Task consisted of a mix of Indic and West-
ern names. We therefore felt the need of train-
ing separate models for words of Indic origin and
words of Western origin.
For this we needed to separate the words in
the training data based on their origin. We first
manually classified 3000 words from the training
set into words of Indic origin and Western origin.
These words were used as seed input for the boot-
strapping algorithm described below:
1. Build two n-gram language models: one for
the already classified names of Indic origin
and another for the names of Western origin.
Here, by n-gram we mean n-character ob-
tained by splitting the words into a sequence
of characters.
2. Split each of the remaining words into a se-
quence of characters and find the probability
of this sequence using the two language mod-
els constructed in step 1.
3. If the probability of a word (i.e. a sequence
of characters) is higher in the Indic language
model than in the Western language model
then classify it as Indic word else classify it
as Western word.
4. Repeat steps 1-3 till all words have been clas-
sified.
Thus, we classified the entire training set into
words of Indic origin and words of Western origin.
The two language models (one for words of Indic
origin and another for words of Western origin)
thus obtained were then used to classify the test
data using steps 2 and 3 of the above algorithm.
Manual verification showed that this method was
able to determine the origin of the words in the test
data with an accuracy of 97%.
2.2 CRF based transliteration engine
Conditional Random Fields (Lafferty et al, 2001)
are undirected graphical models used for labeling
sequential data. Under this model, the conditional
probability distribution of the target word given
the source word is given by,
P (Y |X;?) = 1Z(X) ? e
PT
t=1
PK
k=1 ?kfk(Yt?1,Yt,X,t)
(1)
where,
X = source word (English)
Y = target word (Hindi,Kannada)
T = length of source word (English)
K = number of features
?k = feature weight
Z(X) = normalization constant
CRF++2 which is an open source implemen-
tation of CRF was used for training and decod-
ing. GIZA++ (Och and Ney, 2000), which is a
freely available implementation of the IBM align-
ment models (Brown et al, 1993) was used to get
character level alignments for English-Hindi word
pairs in the training data. Under this alignment,
each character in the English word is aligned to
zero or more characters in the corresponding Hindi
word. The following features are then generated
using this character-aligned data (here ei and hi
are the characters at position i of the source word
and target word respectively):
? hi and ej such that i ? 2 ? j ? i + 2
? hi and source character bigrams ( {ei?1, ei}
or {ei, ei+1})
? hi and source character trigrams ( {ei?2,
ei?1, ei} or {ei?1, ei, ei+1} or {ei, ei+1,
ei+2})
2http://crfpp.sourceforge.net/
85
? hi, hi?1 and ej such that i ? 2 ? j ? i + 2
? hi, hi?1 and source character bigrams
? hi, hi?1 and source character trigrams
Two separate models were trained: one for the
words of Indic origin and another for the words
of Western origin. At the time of testing, the
words were first classified as Indic origin words
and Western origin words using the classifier de-
scribed in section 2.1. The top-10 transliterations
for each word were then generated using the cor-
rect CRF model depending on the origin of the
word.
2.3 Re-ranking using lexicon lookup
Since the dataset for the Shared Task contains
words of Indic origin there is a possibility that the
correct transliteration of some of these words may
be found in a Hindi lexicon. Such a lexicon con-
taining 90677 unique words was constructed by
extracting words from the Hindi Wordnet3. If a
candidate transliteration generated by the CRF en-
gine is found in this lexicon then its rank is in-
creased and it is moved towards the top of the list.
If multiple outputs are found in the lexicon then all
such outputs are moved towards the top of the list
and the relative ranking of these outputs remains
the same as that assigned by the CRF engine. For
example, if the 4th and 6th candidate generated by
the CRF engine are found in the lexicon then these
two candidates will be moved to positions 1 and 2
respectively. We admit that this way of moving
candidates to the top of the list is adhoc. Ideally, if
the lexicon also stored the frequency of each word
then the candidates could be re-ranked using these
frequencies. But unfortunately the lexicon does
not store such frequency counts.
3 Results
The system was tested for English-Hindi and
English-Kannada transliteration using the dataset
(Kumaran and Kellner, 2007) released for NEWS
2009 Machine Transliteration Shared Task. We
submitted one standard run and one non-standard
run for the English-Hindi task and one standard
run for the English-Kannada task. The re-ranking
module was used only for the non-standard run as
it uses resources (lexicon) other than those pro-
vided for the task. We did not have a lexicon
3http://www.cfilt.iitb.ac.in/wordnet/webhwn
for Kannada so were not able to apply the re-
ranking module for English-Kannada task. The
performance of the system was evaluated us-
ing 6 measures, viz., Word Accuracy in Top-
1 (ACC), Fuzziness in Top-1 (Mean F-score),
Mean Reciprocal Rank (MRR), MAPref , MAP10
and MAPsys. Please refer to the white paper of
NEWS 2009 Machine Transliteration Shared Task
(Haizhou et al, 2009) for more details of these
measures.
Table 1 and Table 2 report the results4 for
English-Hindi and English-Kannada translitera-
tion respectively. For English-Hindi we report
3 results: (i) without any pre-processing (word-
origin detection) or post-processing (re-ranking)
(ii) with pre-processing but no post-processing and
(iii) with both pre-processing and post-processing.
The results clearly show that the addition of these
modules boosts the performance. The use of
word-origin detection boosts the top-1 accuracy by
around 0.9% and the use of lexicon lookup based
re-ranking boosts the accuracy by another 6.2%.
Thus, together these two modules give an incre-
ment of 7.1% in the accuracy. Corresponding im-
provements are also seen in the other 5 metrics.
4 Conclusion
We presented a framework for transliteration
which uses (i) a word-origin detection engine
(pre-processing) (ii) a CRF based transliteration
engine and (iii) a re-ranking model based on
lexicon-lookup (post-processing). The results
show that this kind of pre-processing and post-
processing helps to boost the performance of
the transliteration engine. The re-ranking using
lexicon lookup is slightly adhoc as ideally the
re-ranking should take into account the frequency
of the words in the lexicon. Since such frequency
counts are not available it would be useful to find
the web counts for each transliteration candidate
using a search engine and use these web counts to
re-rank the candidates.
4Please note that the results reported in this paper are bet-
ter than the results we submitted to the shared task. This im-
provement was due to the correction of an error in the tem-
plate file given as input to CRF++.
86
Method ACC Mean
F-score
MRR MAPref MAP10 MAPsys
CRF Engine
(no word origin detection, no re-
ranking)
0.408 0.878 0.534 0.403 0.188 0.188
CRF Engine +
Word-Origin detection
(no re-ranking)
Standard run
0.417 0.877 0.546 0.409 0.192 0.192
CRF Engine +
Word-Origin detection +
Re-ranking
Non-Standard run
0.479 0.884 0.588 0.475 0.208 0.208
Table 1: Results for English-Kannada transliteration.
Method Accuracy
(top1)
Mean
F-score
MRR MAPref MAP10 MAPsys
CRF Engine +
Word-Origin detection
(no re-ranking)
Standard run
0.335 0.859 0.453 0.327 0.154 0.154
Table 2: Results for English-Kannada transliteration.
References
B. J. Kang and K. S. Choi 2000. Automatic translitera-
tion and back-transliteration by decision tree learn-
ing. Proceedings of the 2nd International Confer-
ence on Language Resources and Evaluation, 1135-
1411.
Bonnie Glover Stalls and Kevin Knight 1998. Trans-
lating Names and Technical Terms in Arabic Text.
Proceedings of COLING/ACL Workshop on Com-
putational Approaches to Semitic Languages, 34-41.
Haizhou Li, A Kumaran, Min Zhang, Vladimir Pervou-
chine 2009. Whitepaper of NEWS 2009 Machine
Transliteration Shared Task. Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS
2009).
I. Goto and N. Kato and N. Uratani and T. Ehara
2003. Transliteration considering context informa-
tion based on the maximum entropy method. Pro-
ceedings of MT-Summit IX, 125132.
J. S. Lee and K. S. Choi. 1998. English to Korean
statistical transliteration for information retrieval.
Computer Processing of Oriental Languages, 17-37.
John Lafferty, Andrew McCallum, Fernando Pereira
2001. Conditional Random Fields: Probabilis-
tic Models for Segmenting and Labeling Sequence
Data. In Proceedings of the Eighteenth International
Conference on Machine Learning.
Jong-hoon Oh and Key-sun Choi 2002. An English-
Korean Transliteration Model Using Pronunciation
and Contextual Rules. Proceedings of the 19th Inter-
national Conference on Computational Linguistics
(COLING), 758-764.
Kevin Knight and Jonathan Graehl 1997. Machine
transliteration. Computational Linguistics, 128-
135.
Kumaran, A. and Kellner, Tobias 2007. A generic
framework for machine transliteration. SIGIR ?07:
Proceedings of the 30th annual international ACM
SIGIR conference on Research and development in
information retrieval, 721-722.
Och Franz Josef and Hermann Ney 2000. Improved
Statistical Alignment Models. Proc. of the 38th An-
nual Meeting of the Association for Computational
Linguistics, pp. 440-447
P. F. Brown, S. A. Della Pietra, and R. L. Mercer 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263-311.
Sung Young Jung and SungLim Hong and Eunok Paek
2000. An English to Korean transliteration model of
extended Markov window. Proceedings of the 18th
conference on Computational linguistics, 383-389.
Suryaganesh Veeravalli and Sreeharsha Yella and
Prasad Pingali and Vasudeva Varma 2008. Statisti-
cal Transliteration for Cross Language Information
Retrieval using HMM alignment model and CRF.
Proceedings of the 2nd workshop on Cross Lingual
Information Access (CLIA) Addressing the Infor-
mation Need of Multilingual Societies.
Yaser Al-Onaizan and Kevin Knight 2001. Translating
named entities using monolingual and bilingual re-
sources. ACL ?02: Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, 400-408.
87
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 555?563,
Beijing, August 2010
Value for Money: Balancing Annotation Effort, Lexicon Building and
Accuracy for Multilingual WSD
Mitesh M. Khapra Saurabh Sohoney Anup Kulkarni Pushpak Bhattacharyya
Department of Computer Science and Engineering,
Indian Institute of Technology Bombay
{miteshk,saurabhsohoney,anup,pb}@cse.iitb.ac.in
Abstract
Sense annotation and lexicon building are
costly affairs demanding prudent invest-
ment of resources. Recent work on mul-
tilingual WSD has shown that it is possi-
ble to leverage the annotation work done
for WSD of one language (SL) for another
(TL), by projecting Wordnet and sense
marked corpus parameters of SL to TL.
However, this work does not take into ac-
count the cost of manually cross-linking
the words within aligned synsets. Further,
it does not answer the question of ?Can
better accuracy be achieved if a user is
willing to pay additional money?? We
propose a measure for cost-benefit analy-
sis which measures the ?value for money?
earned in terms of accuracy by invest-
ing in annotation effort and lexicon build-
ing. Two key ideas explored in this pa-
per are (i) the use of probabilistic cross-
linking model to reduce manual cross-
linking effort and (ii) the use of selective
sampling to inject a few training examples
for hard-to-disambiguate words from the
target language to boost the accuracy.
1 Introduction
Word Sense Disambiguation (WSD) is one of
the most widely investigated problems of Natural
Language Processing (NLP). Previous works have
shown that supervised approaches to Word Sense
Disambiguation which rely on sense annotated
corpora (Ng and Lee, 1996; Lee et al, 2004) out-
perform unsupervised (Veronis, 2004) and knowl-
edge based approaches (Mihalcea, 2005). How-
ever, creation of sense marked corpora has always
remained a costly proposition, especially for some
of the resource deprived languages.
To circumvent this problem, Khapra et al
(2009) proposed a WSD method that can be ap-
plied to a language even when no sense tagged
corpus for that language is available. This is
achieved by projecting Wordnet and corpus pa-
rameters from another language to the language
in question. The approach is centered on a novel
synset based multilingual dictionary (Mohanty et
al., 2008) where the synsets of different languages
are aligned and thereafter the words within the
synsets are manually cross-linked. For example,
the word WL1 belonging to synset S of language
L1 will be manually cross-linked to the word WL2
of the corresponding synset in language L2 to in-
dicate that WL2 is the best substitute for WL1 ac-
cording to an experienced bilingual speaker?s in-
tuition.
We extend their work by addressing the follow-
ing question on the economics of annotation, lex-
icon building and performance:
? Is there an optimal point of balance between
the annotation effort and the lexicon build-
ing (i.e. manual cross-linking) effort at which
one can be assured of best value for money in
terms of accuracy?
To address the above question we first propose
a probabilistic cross linking model to eliminate
the effort of manually cross linking words within
the source and target language synsets and cali-
brate the resultant trade-off in accuracy. Next, we
show that by injecting examples for most frequent
hard-to-disambiguate words from the target do-
main one can achieve higher accuracies at optimal
555
cost of annotation. Finally, we propose a measure
for cost-benefit analysis which identifies the op-
timal point of balance between these three related
entities, viz., cross-linking, sense annotation and
accuracy of disambiguation.
The remainder of this paper is organized as fol-
lows. In section 2 we present related work. In sec-
tion 3 we describe the Synset based multilingual
dictionary which enables parameter projection. In
section 4 we discuss the work of Khapra et al
(2009) on parameter projection for multilingual
WSD. Section 5 is on the economics of multilin-
gual WSD. In section 6 we propose a probabilistic
model for representing the cross-linkage of words
within synsets. In section 7 we present a strat-
egy for injecting hard-to-disambiguate cases from
the target language using selective sampling. In
section 8 we introduce a measure for cost-benefit
analysis for calculating the value for money in
terms of accuracy, annotation effort and lexicon
building effort. In section 9 we describe the exper-
imental setup. In section 10 we present the results
followed by discussion in section 11. Section 12
concludes the paper.
2 Related Work
Knowledge based approaches to WSD such as
Lesk?s algorithm (Lesk, 1986), Walker?s algo-
rithm (Walker and Amsler, 1986), Conceptual
Density (Agirre and Rigau, 1996) and PageRank
(Mihalcea, 2005) are less demanding in terms of
resources but fail to deliver good results. Super-
vised approaches like SVM (Lee et al, 2004) and
k-NN (Ng and Lee, 1996), on the other hand, give
better accuracies, but the requirement of large an-
notated corpora renders them unsuitable for re-
source scarce languages.
Recent work by Khapra et al (2009) has shown
that it is possible to project the parameters learnt
from the annotation work of one language to an-
other language provided aligned Wordnets for two
languages are available. However, their work does
not address the question of further improving the
accuracy of WSD by using a small amount of
training data from the target language. Some sim-
ilar work has been done in the area of domain
adaptation where Chan et al (2007) showed that
adding just 30% of the target data to the source
data achieved the same performance as that ob-
tained by taking the entire source and target data.
Similarly, Agirre and de Lacalle (2009) reported a
22% error reduction when source and target data
were combined for training a classifier, compared
to the case when only the target data was used for
training the classifier. However, such combining
of training statistics has not been tried in cases
where the source data is in one language and the
target data is in another language.
To the best of our knowledge, no previous work
has attempted to perform resource conscious all-
words multilingual Word Sense Disambigua-
tion by finding a trade-off between the cost (in
terms of annotation effort and lexicon creation ef-
fort) and the quality in terms of F-score.
3 Synset based multilingual dictionary
A novel and effective method of storage and use
of dictionary in a multilingual setting was pro-
posed by Mohanty et al (2008). For the purpose
of current discussion, we will refer to this multi-
lingual dictionary framework as MultiDict. One
important departure in this framework from the
traditional dictionary is that synsets are linked,
and after that the words inside the synsets
are linked. The basic mapping is thus between
synsets and thereafter between the words.
Concepts L1 (English) L2 (Hindi) L3 (Marathi)
04321: a
youthful
male
person
{malechild,
boy}
{lw?A
(ladkaa),
bAl?
(baalak),
bQcA
(bachchaa)}
{m  lgA
(mulgaa),
porgA
(por-
gaa), por
(por)}
Table 1: Multilingual Dictionary Framework
Table 1 shows the structure of MultiDict, with one
example row standing for the concept of boy. The
first column is the pivot describing a concept with
a unique ID. The subsequent columns show the
words expressing the concept in respective lan-
guages (in the example table, English, Hindi and
Marathi). After the synsets are linked, cross link-
ages are set up manually from the words of a
synset to the words of a linked synset of the pivot
language. For example, for the Marathi word
m  lgA (mulgaa), ?a youthful male person?, the
556
correct lexical substitute from the corresponding
Hindi synset is lw?A (ladkaa). The average num-
ber of such links per synset per language pair is
approximately 3.
4 Parameter Projection
Khapra et al (2009) proposed that the various
parameters essential for domain-specific Word
Sense Disambiguation can be broadly classified
into two categories:
Wordnet-dependent parameters:
? belongingness-to-dominant-concept
? conceptual distance
? semantic distance
Corpus-dependent parameters:
? sense distributions
? corpus co-occurrence
They proposed a scoring function (Equation (1))
which combines these parameters to identify the
correct sense of a word in a context:
S? = argmax
i
(?iVi +
?
j?J
Wij ? Vi ? Vj) (1)
where,
i ? Candidate Synsets
J = Set of disambiguated words
?i = BelongingnessToDominantConcept(Si)
Vi = P (Si|word)
Wij = CorpusCooccurrence(Si, Sj)
? 1/WNConceptualDistance(Si, Sj)
? 1/WNSemanticGraphDistance(Si, Sj)
The first component ?iVi of Equation (1) captures
influence of the corpus specific sense of a word in
a domain. The other component Wij ?Vi ?Vj cap-
tures the influence of interaction of the candidate
sense with the senses of context words weighted
by factors of co-occurrence, conceptual distance
and semantic distance.
Wordnet-dependent parameters depend on the
structure of the Wordnet whereas the Corpus-
dependent parameters depend on various statis-
tics learnt from a sense marked corpora. Both the
tasks of (a) constructing a Wordnet from scratch
and (b) collecting sense marked corpora for mul-
tiple languages are tedious and expensive. Khapra
et al (2009) observed that by projecting relations
from the Wordnet of a language and by project-
ing corpus statistics from the sense marked cor-
pora of the language to those of the target lan-
guage, the effort required in constructing seman-
tic graphs for multiple Wordnets and collecting
sense marked corpora for multiple languages can
be avoided or reduced. At the heart of their work
lies the MultiDict described in previous section
which facilitates parameter projection in the fol-
lowing manner:
1. By linking with the synsets of a pivot re-
source rich language (Hindi, in our case), the cost
of building Wordnets of other languages is partly
reduced (semantic relations are inherited). The
Wordnet parameters of Hindi Wordnet now be-
come projectable to other languages.
2. For calculating corpus specific sense distri-
butions, P (Sense Si|Word W ), we need the
counts, #(Si,W ). By using cross linked words
in the synsets, these counts become projectable to
the target language (Marathi, in our case) as they
can be approximated by the counts of the cross
linked Hindi words calculated from the Hindi
sense marked corpus as follows:
P (Si|W ) = #(Si,marathi word)P
j #(Sj ,marathi word)
P (Si|W ) ? #(Si, cross linked hindi word)P
j #(Sj , cross linked hindi word)
The rationale behind the above approximation
is the observation that within a domain sense dis-
tributions remain the same across languages.
5 The Economics of Multilingual WSD
The problem of multilingual WSD using parame-
ter projection can be viewed as an economic sys-
tem consisting of three factors. The first factor is
the cost of manually cross-linking the words in a
synsets of the target language to the words in the
corresponding synset in the pivot language. The
second factor is the cost of sense annotated data
from the target language. The third factor is the
accuracy of WSD The first two factors in some
557
sense relate to the cost of purchasing a commod-
ity and the third factor relates to the commodity
itself.
The work of Khapra et al (2009) as described
above does not attempt to reach an optimal cost-
benefit point in this economic system. They place
their bets on manual cross-linking only and set-
tle for the accuracy achieved thereof. Specifi-
cally, they do not explore the inclusion of small
amount of annotated data from the target language
to boost the accuracy (as mentioned earlier, su-
pervised systems which use annotated data from
the target language are known to perform bet-
ter). Further, it is conceivable that with respect
to accuracy-cost trade-off, there obtains a case
for balancing one cost against the other, viz., the
cost of cross-linking and the cost of annotation.
In some cases bilingual lexicographers (needed
for manual cross-linking) may be more expensive
compared to monolingual annotators. There it
makes sense to place fewer bets on manual cross-
linking and more on collecting annotated corpora.
On the other hand if manual cross-linking is cheap
then a very small amount of annotated corpora
can be used in conjunction with full manual cross-
linking to boost the accuracy. Based on the above
discussion, if ka is the cost of sense annotating
one word, kc is the cost of manually cross-linking
a word and A is the accuracy desired then the
problem of multilingual WSD can be cast as an
optimization problem:
minimize wa ? ka +wc ? kc
s.t.
Accuracy ? A
where, wc and wa are the number of words to be
manually cross linked and annotated respectively.
Ours is thus a 3-factor economic model (cross-
linking, annotation and accuracy) as opposed to
the 2-factor model (cross-linking, accuracy) pro-
posed by Khapra et al (2009).
6 Optimal cross-linking
As mentioned earlier, in some cases where bilin-
gual lexicographers are expensive we might be in-
terested in reducing the effort of manual cross-
linking. For such situations, we propose that
only a small number of words, comprising of the
most frequently appearing ones should be manu-
ally cross linked and the rest of the words should
be cross-linked using a probabilistic model. The
rationale here is simple: invest money in words
which are bound to occur frequently in the test
data and achieve maximum impact on the accu-
racy. In the following paragraphs, we explain our
probabilistic cross linking model.
The model proposed by Khapra et al (2009) is
a deterministic model where the expected count
for (Sense S, Marathi Word W ), i.e., the num-
ber of times the word W appears in sense S is
approximated by the count for the correspond-
ing cross linked Hindi word. Such a model as-
sumes that each Marathi word links to appropri-
ate Hindi word(s) as identified manually by a lex-
icographer. Instead, we propose a probabilistic
model where a Marathi word can link to every
word in the corresponding Hindi synset with
some probability. The expected count for (S,W )
can then be estimated as:
E[#(S,W )] =
X
hi?cross links
P (hi|W,S) ? #(S, hi) (2)
where, P (hi|W,S) is the probability that the word
hi from the corresponding Hindi synset is the
correct cross-linked word for the given Marathi
word. For example, one of the senses of the
Marathi word maan is {neck} i.e. ?the body
part which connects the head to the rest of the
body?. The corresponding Hindi synset has 10
words {gardan, gala, greeva, halak, kandhar and
so on}. Thus, using Equation (2), the expected
count, E[C({neck},maan)], is calculated as:
E[#({neck}, maan)] =
P (gardan|maan,{neck}) ? #({neck}, gardan)
+ P (gala|maan, {neck}) ?#({neck}, gala)
+ P (greeva|maan,{neck}) ? #({neck}, greeva)
+ . . . so on for all words in the Hindi synset
Instead of using a uniform probability distribution
over the Hindi words we go by the empirical ob-
servation that some words in a synset are more
representative of that sense than other words, i.e.
some words are more preferred while expressing
that sense. For example, out of the 10 words in
558
the Hindi synset only 2 words {gardan, gala} ap-
peared in the corpus. We thus estimate the value
of P (hi|W,S) empirically from the Hindi sense
marked corpus by making the following indepen-
dence assumption:
P (hi|W,S) = P (hi|S)
The rationale behind the above independence as-
sumption becomes clear if we represent words and
synsets using the Bayesian network of Figure 1.
Here, the Hindi word hi and the Marathi word W
Figure 1: Bayesian network formed by a synset S
and the constituent Hindi and Marathi words
are considered to be derived from the same par-
ent concept S. In other words, they represent two
different manifestations- one in Hindi and one in
Marathi- of the same synset S. Given the above
representation, it is easy to see that given the par-
ent synset S, the Hindi word hi is independent of
the Marathi word W .
7 Optimal annotation using Selective
Sampling
In the previous section we dealt with the ques-
tion of optimal cross-linking. Now we take up
the other dimension of this economic system, viz.,
optimal use of annotated corpora for better accu-
racy. In other words, if an application demands
higher accuracy for WSD and is willing to pay for
some annotation then there should be a way of en-
suring best possible accuracy at lowest possible
cost. This can be done by including small amount
of sense annotated data from the target language.
The simplest strategy is to randomly annotate text
from the target language and use it as training
data. However, this strategy of random sampling
may not be the most optimum in terms of cost.
Instead, we propose a selective sampling strategy
where the aim is to identify hard-to-disambiguate
words from the target language and use them for
training.
The algorithm proceeds as follows:
1. First, using the probabilistic cross linking
model and aligned Wordnets we learn the param-
eters described in Section 4.
2. We then apply this scoring function on un-
tagged examples (development set) from the tar-
get language and identify hard-to-disambiguate
words i.e., the words which were disambiguated
with a very low confidence.
3. Training instances of these words are then in-
jected into the training data and the parameters
learnt from them are used instead of the projected
parameters learnt from the source language cor-
pus.
Thus, the selective sampling strategy ensures
that we get maximum value for money by spend-
ing it on annotating only those words which would
otherwise not have been disambiguated correctly.
A random selection strategy, in contrast, might
bring in words which were disambiguated cor-
rectly using only the projected parameters.
8 A measure for cost-benefit analysis
We need a measure for cost-benefit analysis based
on the three dimensions of our economic system,
viz., annotation effort, lexicon creation effort and
performance in terms of F-score. The first two di-
mensions can be fused into a single dimension by
expressing the annotation effort and lexicon cre-
ation effort in terms of cost incurred. For example,
we assume that the cost of annotating one word is
ka and the cost of cross-linking one word is kc ru-
pees. Further, we define a baseline and an upper
bound for the F-score. In this case, the baseline
would be the accuracy that can be obtained with-
out spending any money on cross-linking and an-
notation in the target language. An upper bound
could be the best F-score obtained using a large
amount of annotated corpus in the target domain.
Based on the above description, an ideal measure
for cost-benefit analysis would assign a
1. reward depending on the improvement over the
baseline performance.
2. penalty depending on the difference from the
upper bound on performance.
3. reward inversely proportional to the cost in-
559
curred in terms of annotation effort and/or manual
cross-linking.
Based on the above wish-list we propose a mea-
sure for cost-benefit analysis. Let,
MGB = Marginal Gain over Baseline (MGB)
= Performance(P )?Baseline(B)Cost(C)
MDU = Marginal Drop from Upperbound (MDU)
= UpperBound(U)? Performance(P )Cost(C)
then
CostBenefit(CB) = MGB ?MDU
9 Experimental Setup
We used Hindi as the source language (SL) and
trained a WSD engine using Hindi sense tagged
corpus. The parameters thus learnt were then pro-
jected using the MultiDict (refer section 3 and
4) to build a resource conscious Marathi (TL)
WSD engine. We used the same dataset as de-
scribed in Khapra et al (2009) for all our ex-
periments. The data was collected from two do-
mains, viz., Tourism and Health. The data for
Tourism domain was collected by manually trans-
lating English documents downloaded from In-
dian Tourism websites into Hindi and Marathi.
Similarly, English documents for Health domain
were obtained from two doctors and were manu-
ally translated into Hindi and Marathi. The Hindi
and Marathi documents thus created were manu-
ally sense annotated by two lexicographers adept
in Hindi and Marathi using the respective Word-
nets as sense repositories. Table 2 summarizes
some statistics about the corpora.
As for cross-linking, Hindi is used as the pivot
language and words in Marathi synset are linked
to the words in the corresponding Hindi synset.
The total number of cross-links that were man-
ually setup were 3600 for Tourism and 1800 for
Health. The cost of cross-linking as well as
sense annotating one word was taken to be 10 ru-
pees. These costs were estimated based on quo-
tations from lexicographers. However, these costs
need to be taken as representative values only and
may vary greatly depending on the availability of
skilled bilingual lexicographers and skilled mono-
lingual annotators.
Language #of polysemous
words
average degree of
polysemy
Tourism Health Tourism Health
Hindi 56845 30594 3.69 3.59
Marathi 34156 10337 3.41 3.60
Table 2: Number of polysemous words and aver-
age degree of polysemy.
10 Results
Tables 3 and 4 report the average 4-fold perfor-
mance on Marathi Tourism and Health data using
different proportions of available resources, i.e.,
annotated corpora and manual cross-links. In each
of these tables, along the rows, we increase the
amount of Marathi sense annotated corpora from
0K to 6K. Similarly, along the columns we show
the increase in the number of manual cross links
(MCL) used. For example, the second column of
Tables 3 and 4 reports the F-scores when proba-
bilistic cross-linking (PCL) was used for all words
(i.e., no manual cross-links) and varying amounts
of sense annotated corpora from Marathi were
used. Similarly, the first row represents the case
in which no sense annotated corpus from Marathi
was used and varying amounts of manual cross-
links were used.
We report three values in the tables, viz., F-
score (F), cost in terms of money (C) and the cost-
benefit (CB) obtained by using x amount of anno-
tated corpus and y amount of manual cross-links.
The cost was estimated using the values given in
section 9 (i.e., 10 rupees for cross-linking or sense
annotating one word). For calculating, the cost-
benefit baseline was taken as the F-score obtained
by using no cross-links and no annotated corpora
i.e. 68.21% for Tourism and 67.28% for Health
(see first F-score cell of Tables 3 and 4). Similarly
the upper bound (F-scores obtained by training on
entire Marathi sense marked corpus) for Tourism
and Health were 83.16% and 80.67% respectively
(see last row of Table 5).
Due to unavailability of large amount of tagged
Health corpus, the injection size was varied from
0-to-4K only. In the other dimension, we varied
the cross-links from 0 to 1/3rd to 2/3rd to full only
560
Selective Only PCL 1/3 MCL 2/3 MCL Full MCL
Sampling F C CB F C CB F C CB F C CB
0K 68.21 0 - 72.08 12 -0.601 73.31 24 -0.198 73.34 36 -0.130
1K 71.18 10 -0.901 74.96 22 -0.066 77.58 34 0.111 77.73 46 0.089
2K 74.35 20 -0.134 76.96 32 0.080 78.57 44 0.131 79.23 56 0.127
3K 75.21 30 -0.032 77.78 42 0.100 78.68 54 0.111 79.8 66 0.125
4K 76.40 40 0.036 78.66 52 0.114 79.18 64 0.110 80.36 76 0.123
5K 77.04 50 0.054 78.51 62 0.091 79.60 74 0.106 80.46 86 0.111
6K 78.58 60 0.097 79.75 72 0.113 80.8 84 0.122 80.44 96 0.099
Table 3: F-Score (F) in %, Cost (C) in thousand rupees and Cost Benefit (CB) values using different
amounts of sense annotated corpora and manual cross links in Tourism domain.
Selective Only PCL 1/3 MCL 2/3 MCL Full MCL
Sampling F C CB F C CB F C CB F C CB
0K 67.28 0 - 71.39 6 -0.862 73.06 12 -0.153 73.34 18 -0.071
1K 72.51 10 -0.293 75.57 16 0.199 77.41 22 0.312 78.16 28 0.299
2K 75.64 20 0.167 77.29 26 0.255 78.13 32 0.260 78.63 38 0.245
3K 76.78 30 0.187 79.35 36 0.299 79.79 42 0.277 79.88 48 0.246
4K 77.42 40 0.172 79.59 46 0.244 80.54 52 0.253 80.15 58 0.213
Table 4: F-Score (F) in %, Cost (C) in thousand rupees and Cost Benefit (CB) values using different
amounts of sense annotated corpora and manual cross links in Health domain.
Strategy Tourism Health
WFS 57.86 52.77
Only PCL 68.21 67.28
1/6 MCL 69.95 69.57
2/6 MCL 72.08 71.39
3/6 MCL 72.97 72.61
4/6 MCL 73.39 73.06
5/6 MCL 73.55 73.27
Full MCL 73.62 73.34
Upper Bound 83.16 80.67
Table 5: F-score (in %) obtained by using different amounts of manually cross linked words
Strategy Size of target side annotated corpus
0K 1K 2K 3K 4K 5K 6K
Random + PCL 68.21 70.62 71.79 73.03 73.61 76.42 77.52
Random + MCL 73.34 75.32 75.89 76.79 76.83 78.91 80.87
Selective Sampling + PCL 68.21 71.18 74.35 75.21 76.40 77.04 78.58
Selective Sampling + MCL 73.34 77.73 79.23 79.8 79.8 80.46 80.44
Table 6: Comparing F-scores obtained using random sampling and selective sampling (Tourism)
Strategy Size of target side annotated corpus
0K 1K 2K 3K 4K 5K 6K
Annotation + PCL 68.21 71.20 74.35 75.21 76.40 77.04 78.58
Only Annotation 57.86 62.32 64.84 66.86 68.89 69.64 71.82
Table 7: Comparing F-scores obtained using Only Annotation and Annotation + PCL(Tourism)
561
(refer to Tables 3 and 4). However, to give an
idea about the soundness of probabilistic cross-
linking we performed a separate set of experi-
ments by varying the number of cross-links and
using no sense annotated corpora. Table 5 sum-
marizes these results and compares them with the
baseline (Wordnet first sense) and skyline.
In Table 6 we compare our selective sampling
strategy with random sampling when fully proba-
bilistic cross-linking (PCL) is used and when fully
manual cross-linking (MCL) is used. Here again,
due to lack of space we report results only on
Tourism domain. However, we would like to men-
tion that similar experiments on Health domain
showed that the results were indeed consistent.
Finally, in Table 7 we compare the accuracies
obtained when certain amount of annotated corpus
from Marathi is used alone, with the case when the
same amount of annotated corpus is used in con-
junction with probabilistic cross-linking. While
calculating the results for the second row in Table
7, we found that the recall was very low due to the
small size of injections. Hence, to ensure a fair
comparison with our strategy (first row) we used
the Wordnet first sense (WFS) for these recall er-
rors (a typical practice in WSD literature).
11 Discussions
We make the following observations:
1. PCL v/s MCL: Table 5 shows that the proba-
bilistic cross-linking model performs much better
than the WFS (a typically reported baseline) and
it comes very close to the performance of manual
cross-linking. This establishes the soundness of
the probabilistic model and suggests that with a
little compromise in the accuracy, the model can
be used as an approximation to save the cost of
manual cross-linking. Further, in Table 7 we see
that when PCL is used in conjunction with cer-
tain amount of annotated corpus we get up to 9%
improvement in F-score as compared to the case
when the same amount of annotated corpus is used
alone. Thus, in the absence of skilled bilingual
lexicographers, PCL can still be used to boost the
accuracy obtained using annotated corpora.
2. Selective Sampling v/s Random Annotation:
Table 6 shows the benefit of selective sampling
over random annotation. This benefit is felt more
when the amount of training data injected from
Marathi is small. For example, when an annotated
corpus of size 2K is used, selective sampling gives
an advantage of 3% to 4% over random selection.
Thus the marginal gain (i.e., value for money) ob-
tained by using selective sampling is more than
that obtained by using random annotation.
3. Optimal cost-benefit: Finally, we address the
main message of our work, i.e., finding the best
cost benefit. By referring to Tables 3 and 4, we
see that the best value for money in Tourism do-
main is obtained by manually cross-linking 2/3rd
of all corpus words and sense annotating 2K tar-
get words and in the Health domain it is obtained
by manually cross-linking 2/3rd of all corpus
words but sense annotating only 1K words. This
suggests that striking a balance between cross-
linking and annotation gives the best value for
money. Further, we would like to highlight that
our 3-factor economic model is able to capture
these relations better than the 2-factor model of
Khapra et al (2010). As per their model the best
F-score achieved using manual cross-linking for
ALL words was 73.34% for both Tourism and
Health domain at a cost of 36K and 18K respec-
tively. On the other hand, using our model we ob-
tain higher accuracies of 76.96% in the Tourism
domain (using 1/3rd manual cross-links and 2K
injection) at a lower total cost (32K rupees) and
75.57% in the Health domain (using only 1/3rd
cross-linking and 1K injection) at a lower cost
(16K rupees).
12 Conclusion
We reported experiments on multilingual WSD
using different amounts of annotated corpora and
manual cross-links. We showed that there exists
some trade-off between the accuracy and balanc-
ing the cost of annotation and lexicon creation.
In the absence of skilled bilingual lexicographers
one can use a probabilistic cross-linking model
and still obtain good accuracies. Also, while sense
annotating a corpus, careful selection of words us-
ing selective sampling can give better marginal
gain as compared to random sampling.
562
References
Agirre, Eneko and Oier Lopez de Lacalle. 2009. Su-
pervised domain adaption for wsd. In EACL ?09:
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 42?50, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Agirre, Eneko and German Rigau. 1996. Word sense
disambiguation using conceptual density. In In Pro-
ceedings of the 16th International Conference on
Computational Linguistics (COLING).
Chan, Y.S., H. T. Ng, and D. Chiang. 2007. Word
sense disambiguation improves statistical machine
translation. In In Proc. of ACL.
Khapra, Mitesh M., Sapan Shah, Piyush Kedia, and
Pushpak Bhattacharyya. 2009. Projecting param-
eters for multilingual word sense disambiguation.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
459?467, Singapore, August. Association for Com-
putational Linguistics.
Khapra, Mitesh, Sapan Shah, Piyush Kedia, and Push-
pak Bhattacharyya. 2010. Domain-specific word
sense disambiguation combining corpus based and
wordnet based parameters. In 5th International
Conference on Global Wordnet (GWC2010).
Lee, Yoong Keok, Hwee Tou Ng, and Tee Kiah Chia.
2004. Supervised word sense disambiguation with
support vector machines and multiple knowledge
sources. In Proceedings of Senseval-3: Third In-
ternational Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, pages 137?140.
Lesk, Michael. 1986. Automatic sense disambigua-
tion using machine readable dictionaries: how to tell
a pine cone from an ice cream cone. In In Proceed-
ings of the 5th annual international conference on
Systems documentation.
Mihalcea, Rada. 2005. Large vocabulary unsuper-
vised word sense disambiguation with graph-based
algorithms for sequence data labeling. In In Pro-
ceedings of the Joint Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing Conference (HLT/EMNLP), pages 411?418.
Mohanty, Rajat, Pushpak Bhattacharyya, Prabhakar
Pande, Shraddha Kalele, Mitesh Khapra, and Aditya
Sharma. 2008. Synset based multilingual dic-
tionary: Insights, applications and challenges. In
Global Wordnet Conference.
Ng, Hwee Tou and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
senses: An exemplar-based approach. In In Pro-
ceedings of the 34th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
40?47.
Veronis, Jean. 2004. Hyperlex: Lexical cartography
for information retrieval. In Computer Speech and
Language, pages 18(3):223?252.
Walker, D. and R. Amsler. 1986. The use of machine
readable dictionaries in sublanguage analysis. In In
Analyzing Language in Restricted Domains, Grish-
man and Kittredge (eds), LEA Press, pages 69?83.
563
Coling 2010: Poster Volume, pages 347?355,
Beijing, August 2010
Verbs are where all the action lies: Experiences of Shallow Parsing of a
Morphologically Rich Language
Harshada Gune Mugdha Bapat Mitesh M. Khapra Pushpak Bhattacharyya
Department of Computer Science and Engineering,
Indian Institute of Technology Bombay
{harshadag,mbapat,miteshk,pb}@cse.iitb.ac.in
Abstract
Verb suffixes and verb complexes of mor-
phologically rich languages carry a lot of
information. We show that this infor-
mation if harnessed for the task of shal-
low parsing can lead to dramatic improve-
ments in accuracy for a morphologically
rich language- Marathi1. The crux of the
approach is to use a powerful morpholog-
ical analyzer backed by a high coverage
lexicon to generate rich features for a CRF
based sequence classifier. Accuracy fig-
ures of 94% for Part of Speech Tagging
and 97% for Chunking using a modestly
sized corpus (20K words) vindicate our
claim that for morphologically rich lan-
guages linguistic insight can obviate the
need for large amount of annotated cor-
pora.
1 Introduction
Shallow parsing which involves Part-of-Speech
(POS) tagging and Chunking is a fundamental
task of Natural Language Processing (NLP). It is
natural to view each of these sub-tasks as a se-
quence labeling task of assigning POS/chunk la-
bels to a given word sequence. For languages like
English where annotated corpora are available in
abundance these tasks can be performed with very
high accuracy using data-driven machine learning
techniques. Languages of the world show differ-
ent levels of readiness with respect to such anno-
tated resources and hence not all languages may
1Marathi is the official language of Maharashtra, a state in
Western India. The language has close to 20 million speakers
in the world.
provide a conducive platform for machine learn-
ing techniques.
In this scenario, morphologically rich lan-
guages from the Indian subcontinent present a
very interesting case. While these languages do
not enjoy the resource abundance of English, their
linguistic richness can be used to offset this re-
source deficit. Specifically, in such languages, the
suffixes carry a lot of information about the cate-
gory of a word which can be harnessed for shal-
low parsing. This is especially true in the case of
verbs where suffixes like Z {ne}, ZAr {naare} 2
clearly indicate the category of the word. Further,
the structure of verb groups in such languages is
relatively rigid and can be used to reduce the am-
biguity between main verbs and auxiliary verbs.
In the current work, we aim to reduce the data
requirement of machine learning techniques by
appropriate feature engineering based on the char-
acteristics of the language. Specifically, we tar-
get Marathi- a morphologically rich language-
and show that a powerful morphological analyzer
backed by a high coverage lexicon and a simple
but accurate Verb Group Identifier (VGI) can go a
long way in improving the accuracy of a state of
the art sequence classifier. Further, we show that
harnessing such features is the only way by which
one can hope to build a high-accuracy classifier
for such languages, and that simply throwing in a
large amount of annotated corpora does not serve
the purpose. Hence it makes more sense to invest
time and money in developing good morphologi-
cal analyzers for such languages than investing in
annotation. Accuracy figures of 94% for Part of
2These are the suffixes which derive infinitive and gerund
verb forms respectively.
347
Speech Tagging and 97% for Chunking using a
modestly sized corpus (20K words) vindicate our
claim that for morphologically rich languages lin-
guistic knowledge plays a very important role in
shallow parsing of these languages.
2 Related Work
Many POS taggers have been built for English
employing machine learning techniques ranging
from Decision Trees (Black et al, 1992) to Graph-
ical Models (Brants, 2000; Brill, 1995; Ratna-
parkhi, 1996; Lafferty et al, 2001). Even hy-
brid taggers such as CLAWS (Garside and Smith,
1997) which combine stochastic and rule based
approaches have been developed. However, most
of these techniques do not focus on harnessing the
morphology; instead they rely on the abundance
of data which is not a very suitable proposition
for some of the resource deprived languages of the
Indian sub-continent.
Morphological processing based taggers using
a combination of hand-crafted rules and anno-
tated corpora have been tried for Turkish (Oflazer
and Kuruo?z, 1994), Arabic (Tlili-Guiassa, 2006),
Hungarian (Megyesi, 1999) and Modern Greek
(Giorgos et al, 1999). The work on Hindi POS
tagging (Singh et al, 2006) comes closest to our
approach which showed that using a detailed lin-
guistic analysis of morphosyntactic phenomena,
followed by leveraging suffix information and ac-
curate verb group identification can help to build
a high-accuracy (93-94%) part of speech tagger
for Hindi. However, to the best of our knowledge,
there is no POS tagger and Chunker available for
Marathi and ours is the first attempt at building
one.
3 Motivating Examples
To explain the importance of suffix information
for shallow parsing we present two motivating ex-
amples. First, consider the following Marathi sen-
tence,
hA r-taA don gAvA\nA joXZArA aAh.
haa rasta don gavaannaa jodaNaaraa VM aahe.
this road two villages connecting is
this is the road connecting VM two villages.
The word joXZArA {jodaNaaraa} (connecting)
in the above sentence is a verb and can be cat-
egorized as such by simply looking at the suffix
ZArA {Naaraa} as this suffix does not appear with
any other POS category. When suffix informa-
tion is used as a feature a statistical POS tagger
is able to identify the correct POS tag of joXZArA
{jodaNaaraa} even when it does not appear in the
training data. Hence, using suffix information en-
sures that a classifier is able to learn meaningful
patterns even in the absence of large training data.
Next, we consider two examples for chunking.
? VGNN (Gerund Verb Chunk)
mAZsAn uXyAcA ?y? klA.
maaNasaane uDaNyaachaa B-VGNN3
prayatna kelaa.
man fly try do
man tried flying B-VGNN.
? VGINF (Infinitival Verb Chunk)
(yAn cAlAylA s  zvAta klF.
tyaane chaalaayalaa B-VGNF suruvaata
kelii.
he walk start did
he started to walk B-VGINF.
Here, we are dealing with the case of two specific
verb chunks, viz., VGNN (gerund verb chunk) and
VGINF (infinitival verb chunk). A chunk having
a gerund always gets annotated as VGNN and a
chunk having an infinitival verb always gets anno-
tated as VGINF. Thus, the correct identification of
these verb chunks boils down to the correct iden-
tification of gerunds and infinitival verb forms in
the sentence which in turn depend on the careful
analysis of suffix information. For example, in
Marathi, the attachment of the verbal suffix ?y-
AcA? {Nyaachaa} to a verb root always results in
a gerund. Similarly, the attachment of the verbal
suffix ?ylA? {yalaa} to a verb root always results
in an infinitival verb form. The use of such suffix
information as features can thus lead to better gen-
eralization for handling unseen words and thereby
reduce the need for additional training data. For
instance, in the first sentence, even when the word
?uXyAcA? {uDaNyaachaa} does not appear in
3Note that for all our experiments we used BI scheme for
chunking as opposed to the BIO scheme
348
the training data, a classifier which uses suffix in-
formation is able to label it correctly based on its
experience of previous words having suffix ?y-
AcA? {Nyaachaa} whereas a classifier which does
not use suffix information fails to classify it cor-
rectly.
4 Morphological Structure of Marathi
Marathi nouns inflect for number and case. They
may undergo derivation on the attachment of post-
positions. In the oblique case, first a stem is ob-
tained from the root by applying the rules of in-
flection. Then a postposition is attached to the
stem. Postpositions (including case markers and
the derivational suffixes) play a very important
role in Marathi morphology due to the complex
morphotactics.
Marathi adjectives can be classified into two
categories: ones that do not inflect and others that
inflect for gender, number and case where such an
inflection agrees with the gender and number of
the noun modified by them.
The verbs inflect for gender, number and
person of the subject and the direct object in a
sentence. They also inflect for tense and aspect
of the action as well as mood of the speaker in
an illocutionary act. They may even undergo
derivation to derive the nouns, adjectives or
postpositions. Verbal morphology in Marathi
is based on Aakhyaata theory for inflection and
Krudanta theory for derivation which are two
types of verb suffixes (Damale, 1970).
Aakhyaata Theory: Aakhyaata refers to tense,
aspect and mood. Aakhyaata form is realized
through an aakhyaata suffix which is a closing
suffix attached to verb root. For example, bslA
{basalaa} (sat) comes from basa + laa. There are
8 types of aakhyaatas named after the phonemic
shape of the aakhyaata suffix. Associated with ev-
ery aakhyaata are various aakhyaata-arthas which
indicate the features: tense, aspect and mood. An
aakhyaata may or may not agree with gender.
Krudanta Theory: Krudanta suffixes are at-
tached to the end of verbs to form non-infinitive
verb forms. For example, DAvAylA (DAv +
aAylA) {dhaavaayalaa} (to run). There are 8
types of krudantas defined in Marathi.
5 Design of Marathi Shallow Parser
Figure 1 and 2 show the overall architectures of
Marathi POS tagger and chunker. The proposed
system contains 3 important components. First,
a morphological analyzer which provides ambi-
guity schemes and suffix information for gener-
ating a rich set of features. Ambiguity Scheme
refers to the list of possible POS categories a word
can take. This can add valuable information to a
sequence classifier by restricting the set of pos-
sible POS categories for a word. For example,
the word jAta {jaat} meaning caste or go(caste-
noun, go- VM/VAUX) can appear as a noun or a
main verb or an auxiliary verb. Hence it falls in
the ambiguity scheme <NN-VM-VAUX>. This
information is stored in a lexicon. These features
are then fed to a CRF based engine which cou-
ples them with other elementary features (previ-
ous/next words and bigram tags) for training a se-
quence labeler. Finally, in the case of POS tagger,
we use a Verb Group Identifier (VGI) which acts
as an error correcting module for correcting the
output of the CRF based sequence labeler. Each
of these components is described in detail in the
following sub-sections.
5.1 Morphological Analyzer
The formation of polymorphemic words leads
to complexities which need to be handled dur-
ing the analysis process. For example, consider
the steps involved in the formation of the word
dvAsmorQyAn {devasamorchyane} (the one in
front of the God + ERGATIVE).
devaasamora = (deva ? devaa)
+ samora
devaasamorachaa = ( devaasamora ? devaasamora)
+ chaa
devaasamorachyaane = (devaasamorachaa ? devaasamorachyaa)
+ ne
In theory, the process can continue recursively for
the attachment of any number of suffixes. How-
ever, in practice, we have observed that a word in
Marathi contains at most 4 suffixes.
FSMs prove to be elegant and computationally
efficient tools for analyzing polymorphemic
349
Figure 1: Architecture of POS Tagger
words. However, the recursive process of word
formation in Marathi involves inflection at the
time of attachment of every new suffix. The FSM
needs to be modified to handle this. However,
during the i-th recursion only (i-1)-th morpheme
changes its form which can be handled by suit-
ably modifying the FSM. The formation of word
dvAsmorQyAn {devaasamorachyaane} can be
viewed as:
devaasamora = (deva ? devaa)
+ samora
devaasamorachaa = ( deva ? devaa)
+ ( samora ? samora)
+ chaa
devaasamorachyaane = (deva ? devaa)
+ (samora ? samora)
+ (chaa ? chyaa)
+ ne
In general,
Polymorphemic word = (inflected morpheme1)
+ (inflected morpheme2) + ...
Now, we can create an FSM which is aware of
these inflected forms of morphemes in addition to
the actual morphemes to handle the above recur-
sive process of word formation. These inflected
forms are generated using the paradigm-based4
system written in Java and then fed to the FSM
implemented using SFST5.
4A paradigm identifies the uninflected form of words
which share similar inflectional patterns.
5http://www.ims.uni-stuttgart.de/projekte/gramotron
Our lexicon contains 16448 nouns categorized
into 76 paradigms, 8516 adjectives classified
as inflecting and non-inflecting adjectives, 1160
verbs classified into 22 classes. It contains 142
postpositions, 80 aakhyaata and 8 krudanta suf-
fixes.
5.2 CRF
Conditional Random Fields (Lafferty et al, 2001)
are undirected graphical models used for labeling
sequential data. Under this model, the conditional
probability distribution of a tag given the observed
word sequence is given by,
P (Y |X;?) = 1Z(X) ? e
PT
t=1
PK
k=1 ?kfk(Yt?1,Yt,X,t)
(1)
where,
X = source word
Y = target word
T = length of sentence
K = number of features
?k = feature weight
Z(X) = normalization constant
We used CRF++6, an open source implementa-
tion of CRF, for training and further decoding the
tag sequence. We used the following features for
training the sequence labeler (here, wi is the i-th
word, ti is the i-th pos tag and ci is the i-th chunk
tag).
/SOFTWARE/SFST.html
6http://crfpp.sourceforge.net/
350
Figure 2: Architecture of Chunker
Features used for POS tagger training
Consider position of interest = i
? ti ti?1 and wj such that i? 3 < j < i+ 3
? ti ti?1 and suffix information of wi
? ti ti?1 and ambiguity scheme of wi
Here, the first features are weak features which
depend only on the previous/next words and bi-
gram tags. The next two are rich morphological
features which make use of the output of the
morphological analyzer.
Features used for Chunker training
Consider position of interest = i
? ci ci?1 and tj , wj such that i?3 < j < i+3
? ci ci?1 and suffix information of wi
where ci, ci?1 ? {B, I}. Here again, the first set
of features are weak features and the second set
of features are rich morphological features.
5.3 Verb Group Identification (VGI)
In Marathi, certain auxiliaries like asta {asate}
(be), aAh {aahe} (is) etc.. can also act as main
verbs in certain contexts. This ambiguity between
VM (main verbs) and VAUX (auxiliary verbs) can
lead to a large number of errors in POS tagging
if not handled correctly. However, the relatively
rigid structure of Marathi VG coupled with dis-
tinct suffix-affinity of auxiliary verbs allows us to
capture this ambiguity well using the following
simple regular expression:
MainVerbRoot (KrudantaSuffix AuxVerbRoot)*
AakhyaataSuffix
The above regular expression imposes some re-
striction on the occurrence of certain auxiliary
verbs after specific krudanta suffixes. This restric-
tion is captured with the help of a rule file contain-
ing krudanta suffix-auxiliary verb pairs. A sample
entry from this file is
Un , kAY [oon, kaaDh]
which suggests that the auxiliary verb kAY
{kaaDh} can appear after the suffix Un {oon}.
We created a rule file containing around 350 such
valid krudanta suffix-auxiliary verb pairs.
An important point which needs to be high-
lighted here is that a simple left to right scan ig-
noring suffix information and marking the first
verb constituent as main verb and every other
constituent as auxiliary verb does not work for
Marathi. For example, consider the following
verb sequence,
(yAlA ucl n aAZAv lAgl.
tyaalaa uchaluun aaNaave laagale
He carry bring need
It was needed to carry and bring him.
Here, a simple left to right scan of the verb se-
quence ignoring the suffix information would im-
ply that ucl n is a VM whereas aAZAv and
lAgl are VAUX. However, this is not the case
and can be identified correctly by considering the
suffix affinity of auxiliary verbs. Specifically, in
this case, the verb root aAZ cannot take the role
of an auxiliary verb when it appears after the kru-
danta suffix Un. This suggests that the verb
351
aAZAv does not belong to the same verb group
as ucl n and hence is not a VAUX. This shows
suffix and regular expression help in disambiguat-
ing VM-VAUX which is a challenge in all POS
taggers.
6 Experimental Setup
We used documents from the TOURISM and
NEWS domain for all our experiments 7. These
documents were hand annotated by two Marathi
lexicographers. The total size of the corpus was
kept large (106273 POS tagged words and 63033
chunks) to study the impact of the size of training
data versus the amount of linguistic information
used. The statistics about each POS tag and chunk
tag are summarized in Table 1 and Table 2.
POS
Tag
Frequency
in Corpus
POS
Tag
Frequency
in Corpus
NN 51047 RP 359
NST 578 CC 3735
PRP 8770 QW 630
DEM 3241 QF 1928
VM 17716 QC 2787
VAUX 6295 QO 277
JJ 7311 INTF 158
RB 1060 INJ 22
UT 97 RDP 39
PSP 69 NEG 154
Table 1: POS Tags in Training Data
Chunk
Tag
Frequency
in Corpus
Chunk
Tag
Frequency
in Corpus
NP 40254 JJP 2680
VGF 7425 VGNF 3553
VGNN 1105 VGINF 58
RBP 782 BLK 2337
CCP 4796 NEGP 43
Table 2: Chunk Tags in Training Data
7 Results
We report results in four different settings:
Weak Features (WF): Here we use the basic
7The data can be found at www.cfilt.iitb.ac.in/
CRF classifier with elementary word features (i.e.,
words appearing in a context window of 3) and bi-
gram tag features and POS tags in case of chunker.
Weak Morphological Features (Weak-MF): In
addition to the elementary features we use sub-
strings of length 1 to 7 appearing at the end of the
word as feature. The idea here is that such sub-
strings taken from the end of the word can provide
a good approximation of the actual suffix of the
word. Such substrings thus provide a statistical
approximation of the suffixes in the absence of a
full fledged morphological analyzer. This should
not be confused with weak features which mean
tags and word.
Rich Morphological Features (Rich-MF): In
addition to the elementary features we use the am-
biguity schemes and suffix information provided
by the morphological analyzer.
Reach Morphological Features + Verb Group
Identification (Rich-MF+VGI): This setting is
applicable only for POS tagging where we apply
an error correcting VGI module to correct the out-
put of the feature rich CRF tagger.
In each case we first divided the data into four
folds (75% for training and 25% for testing).
Next, we varied the training data in increments of
10K and calculated the accuracy of each of the
above models. The x-axis represents the size of
the training data and the y-axis represents the pre-
cision of the tagger/chunker. Figure 3 plots the
average precision of the POS tagger across all cat-
egories using WF, Weak-MF, Rich-MF and Rich-
MF VGI for varying sizes of the training data.
Figure 6 plots the average precision of the chun-
ker across all categories using WF, Weak-MF and
Rich-MF. Next, to show that the impact of mor-
phological analysis is felt more for verbs than
other POS categories we plot the accuracies of
verb pos tags (Figure 4) and verb chunk tags (Fig-
ure 7) using WF, Weak-MF, Rich-MF and Rich-
MF VGI for varying sizes of the training data.
8 Discussions
We made the following interesting observations
from the above graphs and tables.
1. Importance of linguistic knowledge: Fig-
ure 3 shows that using a large amount of anno-
tated corpus (91k), the best accuracy one can hope
352
 
50
 
60
 
70
 
80
 
90
 
100
 
10000
 
20000
 
30000
 
40000
 
50000
 
60000
 
70000
 
80000
 
90000
% Accuracy
No. of
 Word
s in Tr
aining
 Data
Accur
acy v/
s Data
 Size
WF
Weak
-MF Rich-M
F
Rich-M
F + V
GI
 
86
 
88
 
90
 
92
 
94
 
96
 
98
 
100
 
10000
 
20000
 
30000
 
40000
 
50000
 
60000
% Accuracy
No. of
 Word
s in Tr
aining
 Data
Accur
acy v/
s Data
 Size
WF
Weak
-MF Rich-M
F
Figure 3: Average Accuracy of all POS Tags Figure 6: Average Accuracy of all Chunk Tags
(Note: The graphs for Rich-MF and Rich-MF+VGI coincide)
 
30
 
40
 
50
 
60
 
70
 
80
 
90
 
100
 
10000
 
20000
 
30000
 
40000
 
50000
 
60000
 
70000
 
80000
 
90000
% Accuracy
No. of
 Word
s in Tr
aining
 Data
Accur
acy v/
s Data
 Size
WF
Weak
-MF Rich-M
F
Rich-M
F + V
GI
 
75
 
80
 
85
 
90
 
95
 
100
 
10000
 
20000
 
30000
 
40000
 
50000
 
60000
% Accuracy
No. of
 Word
s in Tr
aining
 Data
Accur
acy v/
s Data
 Size
WF
Weak
-MF Rich-M
F
Figure 4: Average Accuracy of Verb POS Tags Figure 7: Average Accuracy of Verb Chunks
(Note: The graphs for Rich-MF and Rich-MF+VGI almost coincide)
 
50
 
60
 
70
 
80
 
90
 
100
 
10000
 
20000
 
30000
 
40000
 
50000
 
60000
 
70000
 
80000
 
90000
% Accuracy
No. of
 Word
s in Tr
aining
 Data
Accur
acy v/
s Data
 Size
WF
Weak
-MF Rich-M
F
 
75
 
80
 
85
 
90
 
95
 
100
 
10000
 
20000
 
30000
 
40000
 
50000
 
60000
% Accuracy
No. of
 Word
s in Tr
aining
 Data
Accur
acy v/
s Data
 Size
WF
Weak
-MF Rich-M
F
Figure 5: Average Accuracy of Non Verb POS Tags Figure 8: Average Accuracy of Non Verb Chunks
(Note: All the graphs coincide.)
353
for is around 85% if morphological information is
not harnessed i.e., if only weak features are used.
Adding more data will definitely not be of much
use as the curve is already close to saturation. On
the other hand, if morphological information is
completely harnessed using a rich morphological
analyzer then an accuracy as high as 94% can be
obtained by using data as small as 20k words. Fig-
ure 6 tells a similar story. In the absence of mor-
phological features a large amount of annotated
corpus (62k words) is needed to reach an accu-
racy of 96%, whereas if suffix information is used
then the same accuracy can be reached using a
much smaller training corpus (20k words). This
clearly shows that while dealing with morpholog-
ically rich languages, time and effort should be
invested in building powerful morphological ana-
lyzer.
2. Weak morphological features vs rich mor-
phological analyzer: Figure 3 shows that in the
case of POS tagging using just weak morpholog-
ical features gives much better results than the
baseline (i.e. using only weak features). How-
ever, it does not do as well as the rich features
especially when the training size is small, thereby
suggesting that an approximation of the morpho-
logical suffixes may not work for a language hav-
ing rich and diverse morphology. On the other
hand, in the case of chunking, the weak morpho-
logical features do marginally better than the rich
morphological features suggesting that for a rela-
tively easier task (chunking as compared to POS
tagging) even a simple approximation of the ac-
tual suffixes may deliver the goods.
3. Specific case of verbs: Figure 4 shows that in
case of POS tagging using suffixes as features re-
sults in a significant increase in accuracy of verbs.
Specifically accuracy increases from 62% to 95%
using a very small amount of annotated corpus
(20K words). Comparing this with figure 5 we see
that while using morphological information defi-
nitely helps other POS categories, the impact is
not as high as that felt for verbs. Figures 7 and
8 for chunking show a similar pattern i.e., the ac-
curacy of verb chunks is affected more by mor-
phology as compared to other chunk tags. These
figures support our claim that ?verbs is where all
the action lies? and they indeed need special treat-
VM VAUX
VM 17078 347
VAUX 257 6025
Table 3: Confusion matrix for VM-VAUX using
Rich-MF
ment in terms of morphological analysis.
4. Effect of VGI: Figures 3 and 4 show that
the VGI module does not lead to any improve-
ment in the overall accuracy. A detailed analysis
showed that this is mainly because there was not
much VM-VAUX ambiguity left after applying
CRF model containing rich morphological fea-
tures. To further illustrate our point we present the
confusion matrix (see Table 3 ) for verb tags for
a POS tagger using Rich-MF. Table 3 shows that
there were only 347 VM tags which got wrongly
tagged as VAUX and 257 VAUX tags which got
wrongly tagged as VM. Thus the rich morpholog-
ical features were able to take care of most VM-
VAUX ambiguities in the data. However we feel
that if the data contains several VM-VAUX ambi-
guities such as the one illustrated in the example
in Section 5.3 then the VGI module would come
in play and help to boost the performance by re-
solving such ambiguities.
9 Conclusion
We presented here our work on shallow parsing of
a morphologically rich language- Marathi. Our re-
sults show that while dealing with such languages
one cannot ignore the importance of harnessing
morphological features. This is especially true for
verbs where improvements upto 50% in accuracy
can be obtained by adroit handling of suffixes and
accurate verb group identification. An important
conclusion that can be drawn from our work is
that while dealing with morphologically rich lan-
guages it makes sense to invest time and money
in developing powerful morphological analyzers
than placing all the bets on annotating data.
References
Black, Ezra, Fred Jelinek, John Lafferty, Robert Mer-
cer, and Salim Roukos. 1992. Decision tree mod-
els applied to the labeling of text with parts-of-
354
speech. In HLT ?91: Proceedings of the workshop
on Speech and Natural Language, pages 117?121,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Brants, Thorsten. 2000. TnT - A Statistical Part-
of-Speech Tagger. In 6th Applied Natural Lan-
guage Processing (ANLP ?00), April 29 - May 4,
pages 224?231. Association for Computational Lin-
gusitics.
Brill, Eric. 1995. Transformation-Based Error-Driven
Learning and Natural Language Processing: A Case
Study in Part-of-Speech Tagging. Computational
Linguistics, 21(4):543?565.
Damale, M K. 1970. Shastriya Marathi Vyaakarana.
Pune Deshmukh and Company.
Garside, Roger and Nicholas Smith. 1997. A Hybrid
Grammatical Tagger: CLAWS. In Garside, Roger,
Geoffrey Leech, and Tony McEnery, editors, Cor-
pus Annotation, pages 102?121. Longman, London.
Giorgos, Orphanos, Kalles Dimitris, Papagelis Thana-
sis, and Christodoulakis Dimitris. 1999. Decision
Trees and NLP: A case study in POS Tagging.
Lafferty, John, Andrew McCallum, and F. Pereira.
2001. Conditional Random Fields: Probabilis-
tic Models for Segmenting and Labeling Sequence
Data. In Proc. 18th International Conf. on Machine
Learning, pages 282?289. Morgan Kaufmann, San
Francisco, CA.
Megyesi, Beta. 1999. Improving Brill?s POS Tagger
For An Agglutinative Language, 02.
Oflazer, Kemal and Ilker Kuruo?z. 1994. Tagging and
Morphological Disambiguation of Turkish Text. In
ANLP, pages 144?149.
Ratnaparkhi, Adwait. 1996. A Maximum Entropy
Model for Part-of-Speech Tagging. In Brill, Eric
and Kenneth Church, editors, Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 133?142. Association for
Computational Linguistics, Somerset, New Jersey.
Singh, Smriti, Kuhoo Gupta, Manish Shrivastava, and
Pushpak Bhattacharyya. 2006. Morphological
Richness Offsets Resource Demand - Experiences
in Constructing a POS Tagger for Hindi. In Pro-
ceedings of ACL-2006.
Tlili-Guiassa, Yamina. 2006. Hybrid Method for Tag-
ging Arabic Text. Journal of Computer Science 2,
3:245?248.
355
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 6?9, Dublin, Ireland, August 23-29 2014.
Claims on demand ? an initial demonstration of a system for automatic 
detection and polarity identification of context dependent claims in 
massive corpora  
 
Ehud Aharoni Carlos Alzate Roy Bar-Haim Yonatan Bilu 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Dublin Research 
Lab, Ireland 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Haifa Research 
Lab, Haifa, Israel 
Lena Dankin Iris Eiron Daniel Hershcovich Shay Hummel 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Haifa Research 
Lab, Haifa, Israel 
Mitesh Khapra Tamar Lavee1 Ran Levy Paul Matchen 
IBM Bangalore Re-
search Lab, India 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM YKT Research 
Lab, US 
Anatoly Polnarov Vikas Raykar Ruty Rinott Amrita Saha 
Hebrew University, 
Jerusalem, Israel 
IBM Bangalore Re-
search Lab, India 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Bangalore Re-
search Lab, India 
Naama Zwerdling David Konopnicki Dan Gutfreund Noam Slonim2 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Haifa Research 
Lab, Haifa, Israel 
Abstract 
While discussing a concrete controversial topic, most humans will find it challenging to swiftly raise a 
diverse set of convincing and relevant claims that should set the basis of their arguments. Here, we dem-
onstrate the initial capabilities of a system that, given a controversial topic, can automatically pinpoint 
relevant claims in Wikipedia, determine their polarity with respect to the given topic, and articulate them 
per the user's request. 
1 Introduction 
The ability to argue in a persuasive manner is an important aspect of human interaction that naturally 
arises in various domains such as politics, marketing, law, and health-care. Furthermore, good decision 
making relies on the quality of the arguments being presented and the process by which they are re-
solved. Thus, it is not surprising that argumentation has long been a topic of interest in academic re-
search, and different models have been proposed to capture the notion of an argument (Freeley and 
Steinberg, 2008). 
A fundamental component which is common to all these models is the concept of claim (or conclu-
sion). Specifically, at the heart of every argument lies a single claim, which is the assertion the argu-
ment aims to prove. Given a concrete topic, or context, most humans will find it challenging to swiftly 
raise a diverse set of convincing and relevant claims that should set the basis of their arguments.  
                                                    
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0/ 
 
1 Present affiliation: Yahoo! 
2 Corresponding author, at noams@il.ibm.com 
 
6
In this work we demonstrate the initial capabilities of a system that, given a controversial topic, can 
automatically pinpoint relevant claims in Wikipedia, determine their polarity with respect to the given 
topic, and articulate them per the user's request. 
 
2 Basic concepts and associated challenges 
We define and rely on the following two concepts:  
Topic: Short, usually controversial statement that defines the subject of interest.  
Context Dependent Claim (CDC): General, and concise statement, that directly supports or contests 
the given Topic. 
Given these definitions, as well as a few more detailed criteria to reduce the variability in the manu-
ally labeled data, human labelers were asked to detect CDCs for a diverse set of Topics, in relevant 
Wikipedia articles.  The collected data were used to train and assess the performance of the statistical 
models that underlie our system. These data are freely available for academic research (Aharoni et al 
2014). 
The distinction between a CDC and other related texts can be quite subtle, as illustrated in Table 1.  
For example, automatically distinguishing a CDC like S1 from a statement that simply defines a rele-
vant concept like S4, from a claim which is not relevant enough to the given Topic like S5, from a 
statement like S6 that merely repeats the given Topic in different words, or from a statement that repre-
sents a relevant claim which is not general enough like S7, is clearly challenging. Further, CDCs can 
be of different flavors, ranging from factual assertions like S1 to statements that are more of a matter of 
opinion (Pang and Lee 2008) like S2, adding to the complexity of the task. Moreover, our data suggest 
that even if one focuses on Wikipedia articles that are highly relevant to the given Topic, only ?2% of 
their sentences include CDCs.  
Furthermore, since CDCs are by definition concise statements, they typically do not span entire 
Wikipedia sentences but rather sub-sentences. This is illustrated in Table 2. There are many optional 
boundaries to consider when trying to identify the exact boundaries of a CDC within a typical Wikipe-
dia sentence. This task further complicates the CDC detection problem. Thus, we are faced with a large 
number of candidate CDCs, of which only a tiny fraction represents positive examples that might be 
quite reminiscent of some of the negative examples. Finally, automatically determining the correct 
Pro/Con polarity of a candidate CDC with respect to the Topic poses additional unique challenges. 
Nonetheless, by breaking the problem into a set of modular tangible problems and by employing vari-
ous techniques - specifically designed to the problems at hand - we obtain promising results, demon-
strated by the capabilities of our system. 
 
Topic The sale of violent video games to minors should be banned 
(Pro) CDC S1: Violent video games can increase children?s aggression 
(Pro) CDC S2: Video game publishers unethically train children in the use of weapons 
Note, that a valid CDC is not necessarily factual.  
(Con) CDC S3: Violent games affect children positively 
Invalid 
CDC 1 
S4: Video game addiction is excessive or compulsive use of computer and video games 
that interferes with daily life. 
This statement defines a concept relevant to the Topic, not a relevant claim.  
Invalid 
CDC 2 
S5: Violent TV shows just mirror the violence that goes on in the real world.  
This statement is not relevant enough to the Topic. 
Invalid 
CDC 3 
S6: Violent video games should not be sold to children. 
This statement simply repeats the Topic, and thus is not considered a valid CDC.  
Invalid 
CDC 4 
S7: ?Doom? has been blamed for nationally covered school shooting. 
This statement is not general enough to represent a CDC, as it focuses on a specific 
single video game. 
Table 1. Examples of CDCs and invalid CDCs.  
 
 
7
  
 
Because violence in video games is interactive and not passive, critics such as Dave Grossman and 
Jack Thompson argue that violence in games  hardens children to unethical acts, calling first-person 
shooter games ``murder simulators'', although no conclusive evidence has supported this belief. 
Table 2. A CDC is often only a small part of a single Wikipedia sentence - e.g., the part marked in bold 
in this example. 
 
 
Figure 1. High level architecture of the demonstrated system. 
3 High Level Architecture 
The demonstrated system relies on a cascade of engines, depicted in Figure 1. In general, these engines 
rely on various IR, NLP and ML technologies, as well as different resources and lexicons like 
WordNet (Miller, 1995). Some engines are more mature than others, and, correspondingly, already 
employ a complex inner architecture, that will be discussed in more detail elsewhere. Given a Topic, 
the Topic Analysis engine starts with initial semantic analysis of the Topic, aiming to identify the 
main concepts mentioned in this Topic and the sentiment towards each concept. Next, the CDC 
Oriented Article Retrieval engine employs IR and opinion mining techniques in order to retrieve 
Wikipedia articles that with high probability contain CDCs. Next, the CDC Detection engine relies on 
a combination of NLP and ML techniques to zoom-in within the retrieved articles and detect candidate 
CDCs. A detailed description of this engine can be found in (Levy et al 2014). Next, the CDC 
Pro/Con engine aims to automatically determine the polarity of the candidate CDC with respect to the 
given Topic by analyzing and contrasting the sentiment towards key concepts mentioned in the Topic 
and within the candidate CDC. Next, the CDC Equivalence engine uses techniques reminiscent of 
automatic paraphrase detection to identify whether two candidate CDCs are semantically equivalent, so 
to avoid redundancy in the generated output. Finally, the CDC Refinement engine aims to improve 
the precision of the generated output, based on the results collected thus far; e.g., using a simple rule-
based approach, we remove candidate CDCs for which the predicted Pro/Con polarity has low 
confidence. The remaining predictions are sent to the Text To Speech engine that articulates the top 
CDC predictions at the user's request.    
4 Summary 
Given a Topic, the demonstrated system is currently focused on detecting and articulating relevant 
CDCs. Combining this system with technologies that could automatically detect evidence to support 
these CDCs, may give rise to a new generation of automatic argumentation systems. In principle, such 
systems may swiftly detect relevant CDCs in massive corpora, and support these CDCs with evidence 
detected within other articles, or even within entirely different corpora, ending up with automatically 
8
generated arguments that were never explicitly proposed before in this form by humans. The system 
described herein represents an important step in pursuing this vision. 
Acknowledgments 
We would like to thank our colleagues in the IBM debating technologies team who participate in 
generating more advanced versions of the system presented in this work, and further continue to 
contribute to essential aspects of this project. These include Priyanka Agrawal, Indrajit Bhattacharya, 
Feng Cao, Lea Deleris, Francesco Dinuzzo, Liat Ein-Dor, Ron Hoory, Hui Jia Zhu, Qiong Kai Xu, 
Abhishek Kumar, Ofer Lavi, Naftali Liberman, Yosi Mass, Yuan Ni, Asaf Rendel, Haggai Roitman, 
Bogdan Sacaleanu, Dafna Sheinwald, Eyal Shnarch, Mathieu Sinn, Orith Toledo-Ronen, Doron 
Veltzer and Yoav Katz. 
References 
Austin J. Freeley and David L. Steinberg. 2008. Argumentation and Debate. Wadsworth, Belmont, 
California. 
Ehud Aharoni, Anatoly Polnarov, Tamar Lavee, Daniel Hershcovich, Ran Levy, Ruty Rinott, Dan Gut-
freund, and Noam Slonim. 2014. "A Benchmark Dataset for Automatic Detection of Claims and 
Evidence in the Context of Controversial Topics", in Proceedings of the First Workshop on Argu-
mentation and Computation, ACL 2014, to appear. 
Bo Pang and Lillian Lee. 2008. "Opinion mining and sentiment analysis" in Foundations and Trends in 
Information Retrieval, Vol. 2, pp. 1-135. 
George A Miller. 1995. " WordNet: A Lexical Database for English" in Communications of the ACM, 
Vol. 38, pp. 29-41. 
Ran Levy, Yonatan Bilu, Daniel Hershcovich, Ehud Aharoni and Noam Slonim. 2014. ?Context De-
pendent Claim Detection.?  In Proceedings of the 25th International Conference on Computational 
Linguistics, COLING 2014, to appear.   
9
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 420?428,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Everybody loves a rich cousin: An empirical study of transliteration through
bridge languages
Mitesh M. Khapra
Indian Institute of Technology
Bombay,
Powai, Mumbai 400076,
India
miteshk@cse.iitb.ac.in
A Kumaran
Microsoft Research India,
Bangalore,
India
a.kumaran@microsoft.com
Pushpak Bhattacharyya
Indian Institute of Technology
Bombay,
Powai, Mumbai 400076,
India
pb@cse.iitb.ac.in
Abstract
Most state of the art approaches for machine
transliteration are data driven and require sig-
nificant parallel names corpora between lan-
guages. As a result, developing translitera-
tion functionality among n languages could
be a resource intensive task requiring paral-
lel names corpora in the order of nC2. In this
paper, we explore ways of reducing this high
resource requirement by leveraging the avail-
able parallel data between subsets of the n lan-
guages, transitively. We propose, and show
empirically, that reasonable quality transliter-
ation engines may be developed between two
languages, X and Y , even when no direct par-
allel names data exists between them, but only
transitively through language Z . Such sys-
tems alleviate the need for O(nC2) corpora,
significantly. In addition we show that the per-
formance of such transitive transliteration sys-
tems is in par with direct transliteration sys-
tems, in practical applications, such as CLIR
systems.
1 Introduction
Names and Out Of Vocabulary (OOV) terms appear
very frequently in written and spoken text and hence
play a very important role in several Natural Lan-
guage Processing applications. Several studies have
shown that handling names correctly across lan-
guages can significantly improve the performance of
CLIR Systems (Mandl and Womser-Hacker, 2004)
and the utility of machine translation systems. The
fact that translation lexicons or even statistical dic-
tionaries derived from parallel data do not provide a
good coverage of name and OOV translations, un-
derscores the need for good transliteration engines
to transform them between the language.
The importance of machine transliteration, in the
above context, is well realized by the research com-
munity and several approaches have been proposed
to solve the problem. However, most state of the art
approaches are data driven and require significant
parallel names corpora between languages. Such
data may not always be available between every pair
of languages, thereby limiting our ability to support
transliteration functionality between many language
pairs, and subsequently information access between
languages. For example, let us consider a practi-
cal scenario where we have six languages from four
different language families as shown in Figure 1.
The nodes in the graph represent languages and the
edges indicate the availability of data between that
language pair and thus the availability of a Machine
Transliteration system for that pair. It is easy to see
the underlying characteristics of the graph. Data is
available between a language pair due to one of the
following three reasons:
Politically related languages: Due to the political
dominance of English it is easy to obtain parallel
names data between English and most languages.
Genealogically related languages: Arabic and He-
brew share a common origin and there is a signifi-
cant overlap between their phoneme and grapheme
inventory. It is easy to obtain parallel names data
between these two languages.
Demographically related languages: Hindi and
Kannada are languages spoken in the Indian sub-
continent, though they are from different language
families. However, due to the shared culture and de-
mographics, it is easy to create parallel names data
between these two languages.
420
Figure 1: A connected graph of languages
On the other hand, for politically, demographi-
cally and genealogically unrelated languages such
as, say, Hindi and Hebrew, parallel data is not readily
available, either due to the unavailability of skilled
bilingual speakers. Even the argument of using
Wikipedia resources for such creation of such par-
allel data does not hold good, as the amount of inter-
linking may be very small to yield data. For exam-
ple, only 800 name pairs between Hindi and Hebrew
were mined using a state of the art mining algorithm
(Udupa et al, 2009), from Wikipedia interwiki links.
We propose a methodology to develop a practi-
cal Machine Transliteration system between any two
nodes of the above graph, provided a two-step path
exists between them. That is, even when no parallel
data exists between X & Y but sufficient data exists
between X & Z and Z & Y it is still possible to de-
velop transliteration functionality between X & Y
by combining a X ? Z system with a Z ? Y
system. For example, given the graph of Figure 1,
we explore the possibility of developing translitera-
tion functionality between Hindi and Russian even
though no direct data is available between these two
languages. Further, we show that in many cases the
bridge language can be suitably selected to ensure
optimal MT accuracy.
To establish the practicality and utility of our ap-
proach we integrated such a bridge transliteration
system with a standard CLIR system and compared
its performance with that of a direct transliteration
system. We observed that such a bridge system
performs well in practice and in specific instances
results in improvement in CLIR performance over
a baseline system further strengthening our claims
that such bridge systems are good practical solutions
for alleviating the resource scarcity problem.
To summarize, our main contributions in this pa-
per are:
1. Constructing bridge transliteration systems and
establishing empirically their quality.
2. Demonstrating their utility in providing prac-
tical transliteration functionality between two
languages X & Y with no direct parallel data
between them.
3. Demonstrating that in specific cases it is pos-
sible to select the bridge language so that op-
timal Machine Transliteration accuracy is en-
sured while stepping through the bridge lan-
guage.
1.1 Organization of the Paper
This paper is organized in the following manner. In
section 2 we present the related work and highlight
the lack of work on transliteration in resource scarce
scenarios. In section 3 we discuss the methodology
of bridge transliteration. Section 4 discusses the ex-
periments and datasets used. Section 4.3 discusses
the results and error analysis. Section 5 discusses or-
thographic characteristics to be considered while se-
lecting the bridge language. Section 6 demonstrates
the effectiveness of such bridge systems in a practi-
cal scenario, viz., Cross Language Information Re-
trieval. Section 7 concludes the paper, highlighting
future research issues.
2 Related Work
Current models for transliteration can be classi-
fied as grapheme-based, phoneme-based and hy-
brid models. Grapheme-based models, such as,
Source Channel Model (Lee and Choi, 1998), Max-
imum Entropy Model (Goto et al, 2003), Condi-
tional Random Fields (Veeravalli et al, 2008) and
Decision Trees (Kang and Choi, 2000) treat translit-
eration as an orthographic process and try to map
the source language graphemes directly to the tar-
get language graphemes. Phoneme based models,
such as, the ones based on Weighted Finite State
421
Transducers (WFST) (Knight and Graehl, 1997)
and extended Markov window (Jung et al, 2000)
treat transliteration as a phonetic process rather than
an orthographic process. Under such frameworks,
transliteration is treated as a conversion from source
grapheme to source phoneme followed by a conver-
sion from source phoneme to target grapheme. Hy-
brid models either use a combination of a grapheme
based model and a phoneme based model (Stalls
and Knight, 1998) or capture the correspondence be-
tween source graphemes and source phonemes to
produce target language graphemes (Oh and Choi,
2002).
A significant shortcoming of all the previous
works was that none of them addressed the issue of
performing transliteration in a resource scarce sce-
nario, as there was always an implicit assumption
of availability of data between a pair of languages.
In particular, none of the above approaches address
the problem of developing transliteration functional-
ity between a pair of languages when no direct data
exists between them but sufficient data is available
between each of these languages and an intermedi-
ate language. Some work on similar lines has been
done in Machine Translation (Wu and Wang, 2007)
wherein an intermediate bridge language (say, Z) is
used to fill the data void that exists between a given
language pair (say, X and Y ). In fact, recently it has
been shown that the accuracy of a X ? Z Machine
Translation system can be improved by using addi-
tional X ? Y data provided Z and Y share some
common vocabulary and cognates (Nakov and Ng,
2009). However, no such effort has been made in the
area of Machine Transliteration. To the best of our
knowledge, this work is the first attempt at providing
a practical solution to the problem of transliteration
in the face of resource scarcity.
3 Bridge Transliteration Systems
In this section, we explore the salient question ?Is
it possible to develop a practical machine transliter-
ation system between X and Y , by composing two
intermediate X ? Z and Z ? Y transliteration
systems?? We use a standard transliteration method-
ology based on orthography for all experiments (as
outlined in section 3.1), to ensure the applicability
of the methodology to a variety of languages.
3.1 CRF based transliteration engine
Conditional Random Fields ((Lafferty et al, 2001))
are undirected graphical models used for labeling
sequential data. Under this model, the conditional
probability distribution of the target word given the
source word is given by,
P (Y |X;?) = 1
N(X)
? e
PT
t=1
PK
k=1 ?kfk(Yt?1,Yt,X,t)
(1)
where,
X = source word
Y = target word
T = length of source word
K = number of features
?k = feature weight
N(X) = normalization constant
CRF++ 1, an open source implementation of CRF
was used for training and decoding (i.e. transliter-
ating the names). GIZA++ (Och and Ney, 2003),
a freely available implementation of the IBM align-
ment models (Brown et al, 1993) was used to get
character level alignments for the name pairs in the
parallel names training corpora. Under this align-
ment, each character in the source word is aligned to
zero or more characters in the corresponding target
word. The following features are then generated us-
ing this character-aligned data (here ei and hi form
the i-th pair of aligned characters in the source word
and target word respectively):
? hi and ej such that i? 2 ? j ? i + 2
? hi and source character bigrams ( {ei?1, ei} or
{ei, ei+1})
? hi and source character trigrams ( {ei?2, ei?1,
ei} or {ei?1, ei, ei+1} or {ei, ei+1, ei+2})
? hi, hi?1 and ej such that i? 2 ? j ? i + 2
? hi, hi?1 and source character bigrams
? hi, hi?1 and source character trigrams
1http://crfpp.sourceforge.net/
422
3.2 Bridge Transliteration Methodology
In this section, we outline our methodology for com-
posing transitive transliteration systems between X
and Y , using a bridge language Z , by chaining indi-
vidual direct transliteration systems. Our approach
of using bridge transliteration for finding the best
target string (Y ?), given the input string X can be
represented by the following probabilistic expres-
sion:
Y ? = arg max
Y
P (Y |X)
=
?
Z
P (Y,Z|X)
=
?
Z
P (Y |Z,X) ? P (Z|X) (2)
We simplify the above expression, by assuming that
Y is independent of X given Z; the linguistic intu-
ition behind this assumption is that the top-k outputs
of the X ? Z system corresponding to a string in
X, capture all the transliteration information neces-
sary for transliterating to Y . Subsequently, in sec-
tion 5 we discuss the characteristics of the effective
bridge languages to maximize the capture of neces-
sary information for the second stage of the translit-
eration, namely for generating correct strings of Z .
Thus,
Y ? =
?
Z
P (Y |Z) ? P (Z|X) (3)
The probabilities P (Y |Z) and P (Z|X) in Equation
(3) are derived from the two stages of the bridge sys-
tem. Specifically, we assume that the parallel names
corpora are available between the language pair, X
and Z , and the language pair, Z and Y . We train two
baseline CRF based transliteration systems (as out-
lined in Section 3.1), between the language X and
Z , and Z and Y . Each name in language X was
provided as an input into X ? Z transliteration sys-
tem, and the top-10 candidate strings in language Z
produced by this first stage system were given as an
input into the second stage system Z ? Y . The re-
sults were merged using Equation (2). Finally, the
top-10 outputs of this system were selected as the
output of the bridge system.
4 Experiments
It is a well known fact that transliteration is lossy,
and hence the transitive systems may be expected to
suffer from the accumulation of errors in each stage,
resulting in a system that is of much poorer quality
than a direct transliteration system. In this section,
we set out to quantify this expected loss in accuracy,
by a series of experiments in a set of languages us-
ing bridge transliteration systems and a baseline di-
rect systems. We conducted a comprehensive set of
experiments in a diverse set of languages, as shown
in Figure 1, that include English, Indic (Hindi and
Kannada), Slavic (Russian) and Semitic (Arabic and
Hebrew) languages. The datasets and results are de-
scribed in the following subsections.
4.1 Datasets
To be consistent, for training each of these systems,
we used approximately 15K name pairs corpora (as
this was the maximum data available for some lan-
guage pairs). While we used the NEWS 2009 train-
ing corpus (Li et al, 2009) as a part of our train-
ing data, we enhanced the data set to about 15K by
adding more data of similar characteristics (such as,
name origin, domain, length of the name strings,
etc.), taken from the same source as the original
NEWS 2009 data. For languages such as Arabic
and Hebrew which were not part of the NEWS 2009
shared task, the data was created along the same
lines. All results are reported on the standard NEWS
2009 test set, wherever applicable. The test set con-
sists of about 1,000 name pairs in languages X and
Y ; to avoid any bias, it was made sure that there is
no overlap between the test set with the training sets
of both the X ? Z and Z ? Y systems. To estab-
lish a baseline, the same CRF based transliteration
system (outlined in Section 3.1) was trained with a
15K name pairs corpora between the languages X
? Y . The same test set used for testing the transi-
tive systems was used for testing the direct system
as well. As before, to avoid any bias, we made sure
that there is no overlap between the test set and the
training set for the direct system as well.
4.2 Results
We produce top-10 outputs from the bridge system
as well from the direct system and compare their
performance. The performance is measured using
the following standard measures, viz., top-1 accu-
racy (ACC-1) and Mean F-score. These measures
are described in detail in (Li et al, 2009). Table 1
423
Language
Pair
ACC-1 Relative change in
ACC-1 Mean F-score
Relative change in
Mean F-score
Hin-Rus 0.507 0.903
Hin-Eng-Rus 0.466 -8.08% 0.886 -1.88%
Hin-Ara 0.458 0.897
Hin-Eng-Ara 0.420 -8.29% 0.876 -2.34%
Eng-Heb 0.544 0.917
Eng-Ara-Heb 0.544 0% 0.917 0%
Hin-Eng 0.422 0.884
Hin-Kan-Eng 0.382 -9.51% 0.871 -1.47%
Table 1: Stepping through an intermediate language
presents the performance measures, both for a di-
rect system (say, Hin-Rus), and a transitional sys-
tem (say, Hin-Eng-Rus), in 4 different transitional
systems, between English, Indic, Semitic and Slavic
languages. In each case, we observe that the transi-
tional systems have a slightly lower quality, with an
absolute drop in accuracy (ACC-1) of less than 0.05
(relative drop under 10%), and an absolute drop in
Mean F-Score of 0.02 (relative drop under 3%).
4.3 Analysis of Results
Intuitively, one would expect that the errors of the
two stages of the transitive transliteration system
(i.e., X ? Z , and Z ? Y ) to compound, leading
to a considerable loss in the overall performance of
the system. Given that the accuracies of the direct
transliteration systems are as given in Table 2, the
transitive systems are expected to have accuracies
close to the product of the accuracies of the individ-
ual stages, for independent systems.
Language Pair ACC-1 Mean F-Score
Hin-Eng 0.422 0.884
Eng-Rus 0.672 0.935
Eng-Ara 0.514 0.905
Ara-Heb 1.000 1.000
Hin-Kan 0.433 0.879
Kan-Eng 0.434 0.886
Table 2: Performance of Direct Transliteration Systems
However, as we observe in Table 1, the relative
drop in the accuracy (ACC-1) is less than 10% from
that of the direct system, which goes against our in-
tuition. To identify the reasons for the better than
expected performance, we performed a detailed er-
ror analysis of each stage of the bridge translitera-
tion systems, and the results are reported in Tables 3
? 5. We draw attention to two interesting facts which
account for the better than expected performance of
the bridge system:
Improved 2nd stage performance on correct
inputs: In each one of the cases, as expected, the
ACC-1 of the first stage is same as the ACC-1 of the
X ? Z system. However, we notice that the ACC-1
of the second stage on the correct strings output
in the first stage, is significantly better than the the
ACC-1 of the Z ? Y system! For example, the
ACC-1 of the Eng-Rus system is 67.2% (see Table
2), but, that of the 2nd stage Eng-Rus system is
77.8%, namely, on the strings that are transliterated
correctly by the first stage. Our analysis indicate
that there are two reasons for such improvement:
First, the strings that get transliterated correctly in
the first stage are typically shorter or less ambigu-
ous and hence have a better probability of correct
transliterations in the both stages. This phenomenon
could be verified empirically: Names like gopAl
{Gopal}, rm? {Ramesh}, rAm {Ram} are
shorter and in general have less ambiguity on target
orthography. Second, also significantly, the use of
top-10 outputs from the first stage as input to the
second stage provides a better opportunity for the
second stage to produce correct string in Z . Again,
this phenomenon is verified by providing increasing
number of top-n results to the 2nd stage.
424
Hi?En?Ru En ? Ru(Stage-2)
Stage-2
Acc.
Correct Error
Hi?En Correct 263 75 77.81%
(Stage-1) Error 119 362 24.74%
Table 3: Error Analysis for Hi?En?Ru
Hi?En?Ar En ? Ar(Stage-2)
Stage-2
Acc.
Correct Error
Hi?En Correct 221 127 63.50%
(Stage-1) Error 119 340 25.70%
Table 4: Error Analysis for Hi?En?Ar
2nd stage error correction on incorrect inputs:
The last rows in each of the above tables 3 ? 5 re-
port the performance of the second stage system on
strings that were transliterated incorrectly by the first
stage. While we expected the second row to pro-
duce incorrect transliterations nearly for all inputs
(as the input themselves were incorrect in Z), we
find to our surprise that upto 25% of the erroneous
strings in Z were getting transliterated correctly in
Y ! This provides credence to our hypothesis that
sufficient transliteration information is captured in
the 1st stage output (even when incorrect) that may
be exploited in the 2nd stage. Empirically, we veri-
fied that in most cases (nearly 60%) the errors were
due to the incorrectly transliterated vowels, and in
many cases, they get corrected in the second stage,
and re-ranked higher in the output. Figure 2 shows a
few examples of such error corrections in the second
stage.
Figure 2: Examples of error corrections
Hi?Ka?En Ka ? En(Stage-2)
Stage-2
Acc.
Correct Error
Hi?Ka Correct 225 196 53.44%
(Stage-1) Error 151 400 27.40%
Table 5: Error Analysis for Hi?Ka?En
5 Characteristics of the bridge language
An interesting question that we explore in this sec-
tion is ?how the choice of bridge language influence
the performance of the bridge system??. The under-
lying assumption in transitive transliteration systems
(as expressed in Equation 3), is that ?Y is indepen-
dent of X given Z?. In other words, we assume that
the representations in the language will Z ?capture
sufficient transliteration information from X to pro-
duce correct strings in Y ?. We hypothesize that two
parameters of the bridge language, namely, the or-
thography inventory and the phoneme-to-grapheme
entropy, that has most influence on the quality of the
transitional systems, and provide empirical evidence
for this hypothesis.
5.1 Richer Orthographic Inventory
In each of the successful bridge systems (that is,
those with a relative performance drop of less than
10%), presented in Table 1, namely, Hin-Eng-Ara,
Eng-Ara-Heb and Hin-Kan-Eng, the bridge lan-
guage has, in general, richer orthographic inven-
tory than the target language. Arabic has a reduced
set of vowels, and hence poorer orthographic inven-
tory compared with English. Similarly, between the
closely related Semitic languages Arabic-Hebrew,
there is a many-to-one mapping from Arabic to He-
brew, and between Kannada-English, Kannada has
nearly a superset of vowels and consonants as com-
pared to English or Hindi.
As an example for a poor choice of Z , we present
a transitional system, Hindi ? Arabic ? English, in
Table 6, in which the transitional language z (Ara-
bic) has smaller orthographic inventory than Y (En-
glish).
Arabic has a reduced set of vowels and, unlike En-
glish, in most contexts short vowels are optional. As
a result, when Arabic is used as the bridge language
the loss of information (in terms of vowels) is large
425
Language
Pair
ACC-1 Relative change in
ACC-1
Hin-Eng 0.422
Hin-Ara-Eng 0.155 -64.28%
Table 6: Incorrect choice of bridge language
and the second stage system has no possibility of re-
covering from such a loss. The performance of the
bridge system confirms such a drastic drop in ACC-
1 of nearly 64% compared with the direct system.
5.2 Higher Phoneme-Grapheme Entropy
We also find that the entropy in phoneme - grapheme
mapping of a language indicate a good correlation
with a good choice for a transition language. In
a good transitional system (say, Hin-Eng-Rus), En-
glish has a more ambiguous phoneme-to-grapheme
mapping than Russian; for example, in English the
phoneme ?s? as in Sam or Cecilia can be repre-
sented by the graphemes ?c? and ?s?, whereas Rus-
sian uses only a single character to represent this
phoneme. In such cases, the ambiguity introduced
by the bridge language helps in recovering from er-
rors in the X ? Z system. The relative loss of
ACC-1 for this transitional system is only about 8%.
The Table 7 shows another transitional system, in
which a poor choice was for the transitional lan-
guage was made.
Language
Pair
ACC-1 Relative change in
ACC-1
Hin-Eng 0.422
Hin-Tam-Eng 0.231 -45.26%
Table 7: Incorrect choice of bridge language
Tamil has a reduced set of consonants compared
with Hindi or English. For example, the Hindi con-
sonants (k, kh, g, gh) are represented by a sin-
gle character in Tamil. As a result, when Tamil is
used as the bridge language it looses information (in
terms of consonants) and results in a significant drop
in performance (nearly a 45% drop in ACC-1) for
the bridge system.
6 Effectiveness of Bridge Transliteration
on CLIR System
In this section, we demonstrate the effectiveness of
our bridge transliteration system on a downstream
application, namely, a Crosslingual Information Re-
trieval system. We used the standard document col-
lections from CLEF 2006 (Nardi and Peters, 2006),
CLEF 2007 (Nardi and Peters, 2007) and FIRE 2008
(FIRE, 2008). We used Hindi as the query language.
All the three fields (title, description and narration)
of the topics were used for the retrieval. Since the
collection and topics are from the previous years,
their relevance judgments were also available as a
reference for automatic evaluation.
6.1 Experimental Setup
We used primarily the statistical dictionaries gen-
erated by training statistical word alignment mod-
els on an existing Hindi-English parallel corpora.
As with any CLIR system that uses translation lex-
icon, we faced the problem of out-of-vocabulary
(OOV) query terms that need to be transliterated,
as they are typically proper names in the target lan-
guage. First, for comparison, we used the above
mentioned CLIR system with no transliteration en-
gine (Basic), and measured the crosslingual retrieval
performance. Clearly, the OOV terms would not be
converted into target language, and hence contribute
nothing to the retrieval performance. Second, we in-
tegrated a direct machine transliteration system be-
tween Hindi and English (D-HiEn), and calibrated
the improvement in performance. Third, we inte-
grate, instead of a direct system, a bridge transliter-
ation system between Hindi and English, transition-
ing through Kannada (B-HiKaEn). For both, direct
as well as bridge transliteration, we retained the top-
5 transliterations generated by the appropriate sys-
tem, for retrieval.
6.2 Results and Discussion
The results of the above experiments are given in
Table 7. The current focus of these experiments is
to answer the question of whether the bridge ma-
chine transliteration systems used to transliterate
the OOV words in Hindi queries to English (by step-
ping through Kannada) performs at par with a di-
rect transliteration system. As expected, enhancing
the CLIR system with a machine transliteration sys-
426
Collection CLIR System MAP Relative MAP change
from Basic
Recall Relative Recall change
from Basic
Basic 0.1463 - 0.4952 -
CLEF 2006 D-HiEn 0.1536 +4.98% 0.5151 +4.01%
B-HiKaEn 0.1529 +4.51% 0.5302 +7.06%
Basic 0.2521 - 0.7156 -
CLEF 2007 D-HiEn 0.2556 +1.38% 0.7170 + 0.19%
B-HiKaEn 0.2748 +9.00% 0.7174 + 0.25%
Basic 0.4361 - 0.8457 -
FIRE 2008 D-HiEn 0.4505 +3.30% 0.8506 +0.57%
B-HiKaEn 0.4573 +4.86% 0.8621 +1.93%
Table 8: CLIR Experiments with bridge transliteration systems
tem (D-HiEn) gives better results over a CLIR sys-
tem with no transliteration functionality (Basic). On
the standard test collections, the bridge translitera-
tion system performs in par or better than the di-
rect transliteration system in terms of MAP as well
as recall. Even though, the bridged system is of
slightly lesser quality in ACC-1 in Hi-Ka-En, com-
pared to Hi-En (see Table 1), the top-5 results had
captured the correct transliteration, as shown in our
analysis. A detailed analysis of the query transla-
tions produced by the above systems showed that in
some cases the bridge systems does produce a bet-
ter transliteration thereby leading to a better MAP.
As an illustration, consider the OOV terms vEV?n
{Vatican} and n-l {Nestle} and the corre-
sponding transliterations generated by the different
systems. The Direct-HiEn system was unable to
OOV term D-HiEn B-HiKaEn
vetican vetican
veticon vettican
vEV?n vettican vatican
(vatican) vetticon watican
wetican wetican
nesle nestle
nesly nesle
n-l nesley nesley
(nestle) nessle nestley
nesey nesly
Table 9: Sample output in direct and bridge systems
generate the correct transliteration in the top-5 re-
sults whereas the B-HiKaEn was able to produce the
correct transliteration in the top-5 results thereby re-
sulting in an improvement in MAP for these queries.
7 Conclusions
In this paper, we introduced the idea of bridge
transliteration systems that were developed employ-
ing well-studied orthographic approaches between
constituent languages. We empirically established
the quality of such bridge transliteration systems
and showed that quite contrary to our expectations,
the quality of such systems does not degrade dras-
tically as compared to the direct systems. Our er-
ror analysis showed that these better-than-expected
results can be attributed to (i) Better performance
(?10-12%) of the second stage system on the strings
transliterated correctly by the first stage system and
(ii) Significant (?25%) error correction in the sec-
ond stage. Next, we highlighted that the perfor-
mance of such bridge systems will be satisfactory as
long as the orthographic inventory of the bridge lan-
guage is either richer or more ambiguous as com-
pared to the target language. We showed that our
results are consistent with this hypothesis and pro-
vided two examples where there is a significant drop
in the accuracy when the bridge language violates
the above constraints. Finally, we showed that a
state of the art CLIR system integrated with a bridge
transliteration system performs in par with the same
CLIR system integrated with a direct translitera-
tion system, vindicating our claim that such bridge
transliteration systems can be use in real-world ap-
plications to alleviate the resource requirement of
nC2 parallel names corpora.
427
References
Peter E Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19:263?311.
FIRE. 2008. Forum for information retrieval evaluation.
Isao Goto, Naoto Kato, Noriyoshi Uratani, and Terumasa
Ehara. 2003. Transliteration considering context in-
formation based on the maximum entropy method. In
Proceedings of MT-Summit IX, pages 125?132.
Sung Young Jung, SungLim Hong, and Eunok Paek.
2000. An english to korean transliteration model of
extended markov window. In Proceedings of the 18th
conference on Computational linguistics, pages 383?
389.
Byung-Ju Kang and Key-Sun Choi. 2000. Automatic
transliteration and back-transliteration by decision tree
learning. In Proceedings of the 2nd International Con-
ference on Language Resources and Evaluation, pages
1135?1411.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Computational Linguistics, pages
128?135.
John D. Lafferty, Andrew Mccallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML ?01: Proceedings of the Eighteenth Interna-
tional Conference on Machine Learning, pages 282?
289, San Francisco, CA, USA.
Jae Sung Lee and Key-Sun Choi. 1998. English to ko-
rean statistical transliteration for information retrieval.
In Computer Processing of Oriental Languages, pages
17?37.
Haizhou Li, A Kumaran, , Min Zhang, and Vladimir Per-
vouvhine. 2009. Whitepaper of news 2009 machine
transliteration shared task. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Transliter-
ation (NEWS 2009), pages 19?26, Suntec, Singapore,
August. Association for Computational Linguistics.
Thomas Mandl and Christa Womser-Hacker. 2004. How
do named entities contribute to retrieval effectiveness?
In CLEF, pages 833?842.
Preslav Nakov and Hwee Tou Ng. 2009. Improved statis-
tical machine translation for resource-poor languages
using related resource-rich languages. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1358?1367, Singa-
pore, August. Association for Computational Linguis-
tics.
A Nardi and C Peters. 2006. Working notes for the clef
2006 workshop.
A Nardi and C Peters. 2007. Working notes for the clef
2007 workshop.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Jong-hoon Oh and Key-Sun Choi. 2002. An english-
korean transliteration model using pronunciation and
contextual rules. In Proceedings of the 19th In-
ternational Conference on Computational Linguistics
(COLING), pages 758?764.
Bonnie Glover Stalls and Kevin Knight. 1998. Trans-
lating names and technical terms in arabic text. In
Proceedings of COLING/ACL Workshop on Computa-
tional Approaches to Semitic Languages, pages 34?41.
Raghavendra Udupa, K Saravanan, Anton Bakalov, and
Abhijit Bhole. 2009. ?they are out there, if you know
where to look: Mining transliterations of oov query
terms for cross language information retrieval?. In
ECIR?09: Proceedings of the 31st European Confer-
ence on IR research on Advances in Information Re-
trieval, pages 437?448, Toulouse, France.
Suryaganesh Veeravalli, Sreeharsha Yella, Prasad Pin-
gali, and Vasudeva Varma. 2008. Statistical translit-
eration for cross language information retrieval using
hmm alignment model and crf. In Proceedings of the
2nd workshop on Cross Lingual Information Access
(CLIA) Addressing the Information Need of Multilin-
gual Societies.
Hua Wu and Haifeng Wang. 2007. Pivot language
approach for phrase-based statistical machine transla-
tion. Machine Translation, 21(3):165?181.
428
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 492?500,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving the Multilingual User Experience of Wikipedia Using
Cross-Language Name Search
Raghavendra Udupa
Microsoft Research India
Bangalore, India.
Mitesh Khapra ?
Indian Institute of Technology Bombay
Powai, India.
Abstract
Although Wikipedia has emerged as a power-
ful collaborative Encyclopedia on the Web, it
is only partially multilingual as most of the
content is in English and a small number of
other languages. In real-life scenarios, non-
English users in general and ESL/EFL 1 users
in particular, have a need to search for rele-
vant English Wikipedia articles as no relevant
articles are available in their language. The
multilingual experience of such users can be
significantly improved if they could express
their information need in their native language
while searching for English Wikipedia arti-
cles. In this paper, we propose a novel cross-
language name search algorithm and employ
it for searching English Wikipedia articles in
a diverse set of languages including Hebrew,
Hindi, Russian, Kannada, Bangla and Tamil.
Our empirical study shows that the multilin-
gual experience of users is significantly im-
proved by our approach.
1 Introduction
Since its inception in 2001, Wikipedia has emerged
as the most famous free, web-based, collaborative,
and multilingual encyclopedia with over 13 million
articles in over 270 languages. However, Wikipedia
exhibits severe asymmetry in the distribution of its
content in the languages of the world with only a
small number of languages dominating (see Table
?This work was done when the author was a summer intern
at Microsoft Research India.
1English as Second Language and English as Foreign Lan-
guage.
1). As a consequence, most users of the under-
represented languages of the world have no choice
but to consult foreign language Wikipedia articles
for satisfying their information needs.
Table 1: Linguistic asymmetry of Wikipedia
Language Speakers Contributors Articles
English 1500M 47.1% 3,072,373
Russian 278M 5.2% 441,860
Hebrew 10M 0.7% 97,987
Hindi 550M 0.06% 50,926
Bangla 230M 0.02% 20,342
Tamil 66M 0.04% 19,472
Kannada 47M 0.02% 7,185
Although consulting foreign language Wikipedia
is not a solution for the problem of linguistic asym-
metry, in the specific case of ESL/EFL users who
form a sizable fraction of Internet users of the world
2
, it is arguably the most practical option today. Typ-
ically, ESL/EFL users are reasonably good at read-
ing and extracting relevant information from English
content but not so good at expressing their infor-
mation needs in English. In particular, getting the
spellings of foreign names in English correctly is
very difficult for most ESL/EFL users due to the dif-
ferences in the way a foreign name is pronounced
in the native languages. For instance, Japanese
EFL speakers often break consonant clusters in for-
eign names using vowels (see Table 2) and Hindi
ESL speakers find it difficult to differentiate between
?an?, ?en?, and ?on? in English names (such as ?Clin-
2As per some estimates, there are about 1 Billion ESL and
EFL speakers in the world today and their number is growing.
492
ton?) and will most likely use ?an? (?Clintan?).
Table 2: Influence of native language on the English
spelling of names.
Wikipedia
Entity Hindi Japanese Kannada
Stephen
Hawking
Stefan
Hoking
Suchifun
Houkingu
Steephan
Haakimg
Paul Krug-
man
Pol Crugmun PooruKuruguman
Paal Kraga-
man
Haroun
al-Rashid
Haroon
al-Rashid
Haruun
aru-Rasheedo
Haroon
al-Rasheed
Subrahmaniya
Bharati
Subramaniya
Bharati
Suburaamaniya
Bahaarachi
Subrahmanya
Bharathi
In principle, English spell-checkers (Ahmad and
Kondrak, 2005) can handle the problem of incor-
rect spellings in the queries formed by ESL/EFL
users. But in practice, there are two difficulties.
Firstly, most English spell-checkers do not have a
good coverage of names which form the bulk of user
queries. Secondly, spelling correction of names is
difficult because spelling mistakes are markedly in-
fluenced by the native language of the user. Not
surprisingly, Wikipedia?s inbuilt spell-checker sug-
gests ?Suchin Housing? as the only alternative to the
query ?Suchifun Houkingu? instead of the correct
entity ?Stephen Hawking? (See Table 3 for more ex-
amples).
The inability of ESL/EFL speakers to express
their information needs correctly in English and the
poor performance of spell-checkers highlight the
need for a practical solution for the linguistic asym-
metry problem of Wikipedia. In this work, we argue
the multilingual user experience of ESL/EFL users
can be significantly improved by allowing them to
express their information need in their native lan-
guage. While it might seem that we would need
a fully functional cross-language retrieval system
that supports translation of non-English queries to
English, we note that a good number of the pages
in Wikipedia are on people. This empirical fact
allows us to improve the multilingual experience
of ESL/EFL Wikipedia users by means of cross-
language name search which is less resource de-
manding than a fully functional cross-language re-
trieval system.
There are several challenges that need to be ad-
dressed in order to enable cross-language name
Table 3: Spelling suggestions by Wikipedia.
User Input
Wikipedia?s
Suggestion
Correct Spelling
Suchifun Houkingu Suchin Housing Stephen Hawking
Stefan Hoking Stefan Ho king Stephen Hawking
Pol Crugman Poll Krugman Paul Krugman
Paal Kragaman Paul Krugman Paul Krugman
Suburaamaniya Ba-
haarachi
Subramaniya
Baracchi
Subrahmaniya
Bharati
search in Wikipedia.
? Firstly, name queries are expressed by
ESL/EFL users in the native languages using
the orthography of those languages. Translit-
erating the name into Latin script using a
Machine Transliteration system is an option
but state-of-the-art Machine Transliteration
technologies are still far away from producing
the correct transliteration. Further, as pointed
out by (Udupa et al, 2009a), it is not enough
if a Machine Transliteration system generates
a correct transliteration; it must produce the
transliteration that is present in the Wikipedia
title.
? Secondly, there are about 6 million titles (in-
cluding redirects) in English Wikipedia which
rules out the naive approach of comparing the
query with every one of the English Wikipedia
titles for transliteration equivalence as is done
typically in transliteration mining tasks. A
practical cross-language name search system
for Wikipedia must be able to search millions
of Wikipedia titles in a fraction of a second and
return the most relevant titles.
? Thirdly, names are typically multi-word and
as a consequence there might not be an ex-
act match between the query and English
Wikipedia titles. Any cross-language name
search system for Wikipedia must be able
to deal with multi-word names and partial
matches effectively.
? Fourthly, the cross-language name search sys-
493
tem must be tolerant to spelling variations in
the query as well as the Wikipedia titles.
In this work, we propose a novel approach to
cross-language name search in Wikipedia that ad-
dresses all the challenges described above. Fur-
ther, our approach does not depend on either spell-
checkers or Machine Transliteration. Rather we
transform the problem into a geometric search prob-
lem and employ a state-of-the-art geometric algo-
rithm for searching a very large database of names.
This enables us to accurately search the relevant
Wikipedia titles for a given user query in a fraction
of a second even on a single processor.
1.1 Our Contributions
Our contributions can be summarized as follows:
1. We introduce a language and orthography in-
dependent geometric representation for single-
word names (Section 3.1).
2. We model the problem of learning the geo-
metric representation of names as a multi-view
learning problem and employ the machinery
of Canonical Correlation Analysis (CCA) to
compute a low-dimensional Euclidean feature
space. We map both foreign single-word names
and English single-word names to points in the
common feature space and the similarity be-
tween two single-word names is an exponen-
tially decaying function of the squared geomet-
ric distance between the corresponding points
(Section 3).
3. We model the problem of searching a database
of names as a geometric nearest neighbor prob-
lem in low-dimensional Euclidean space and
employ the well-known ANN algorithm for
approximate nearest neighbors to search for
the equivalent of a query name in the English
Wikipedia titles (Arya et al, 1998) (Section
3.3).
4. We introduce a simple and efficient algorithm
for computing the similarity scores of multi-
word names from the single-word similarity
scores (Section 3.4).
5. We show experimentally that our approach sig-
nificantly improves the multilingual experience
of ESL/EFL users (Section 4).
2 Related Work
Although approximate similarity search is well-
studied, we are not aware of any non-trivial cross-
language name search algorithm in the litera-
ture. However, several techniques for mining name
transliterations from monolingual and comparable
corpora have been studied (Pasternack and Roth,
2009), (Goldwasser and Roth, 2008), (Klementiev
and Roth, 2006), (Sproat et al, 2006), (Udupa et al,
2009b). These techniques employ various translit-
eration similarity models. Character unigrams and
bigrams were used as features to learn a discrimi-
native transliteration model and time series similar-
ity was combined with the transliteration similarity
model (Klementiev and Roth, 2006). A generative
transliteration model was proposed and used along
with cross-language information retrieval to mine
named entity transliterations from large comparable
corpora (Udupa et al, 2009b). However, none of
these transliteration similarity models are applicable
for searching very large name databases as they rely
on brute-force search. Not surprisingly, (Pasternack
and Roth, 2009) report that ?.. testing [727 single
word English names] with fifty thousand [Russian]
candidates is a large computational hurdle (it takes
our model about seven hours)?.
Several algorithms for string similarity search
have been proposed and applied to various problems
(Jin et al, 2005). None of them are directly applica-
ble to cross-language name search as they are based
on the assumption that the query string shares the
same alphabet as the database strings.
Machine Transliteration has been studied exten-
sively in the context of Machine Translation and
Cross-Language Information Retrieval (Knight and
Graehl, 1998), (Virga and Khudanpur, 2003), (Kuo
et al, 2006), (Sherif and Kondrak, 2007), (Ravi and
Knight, 2009), (Li et al, 2009), (Khapra and Bhat-
tacharyya, 2009). However, Machine Transliteration
followed by string similarity search gives less-than-
satisfactory solution for the cross-language name
search problem as we will see later in Section 4.
CCA was introduced by Hotelling in 1936 and has
494
been applied to various problems including CLIR,
Text Clustering, and Image Retrieval (Hardoon et
al., 2004). Recently, CCA has gained importance
in the Machine Learning community as a technique
for multi-view learning. CCA computes a common
semantic feature space for two-view data and al-
lows users to query a database using either of the
two views. CCA has been used in bilingual lexi-
con extraction from comparable corpora (Gaussier
et al, 2004) and monolingual corpora (Haghighi et
al., 2008).
Nearest neighbor search is a fundamental prob-
lem where challenge is to preprocess a set of points
in some metric space into a geometric data struc-
ture so that given a query point, its k-nearest neigh-
bors in the set can be reported as fast as possi-
ble. It has applications in many areas including pat-
tern recognition and classification, machine learn-
ing, data compression, data mining, document re-
trieval and statistics. The brute-force search algo-
rithm can find the nearest neighbors in running time
proportional to the product of the number of points
and the dimension of the metric space. When the di-
mension of the metric space is small, there exist al-
gorithms which give better running time than brute-
force search. However, the search time grows expo-
nentially with the dimension and none of the algo-
rithms do significantly better than brute-force search
for high-dimensional data. Fortunately, efficient al-
gorithms exist if instead of exact nearest neighbors,
we ask for approximate nearest neighbors (Arya et
al., 1998).
3 Cross-Language Name Search as a
Geometric Search Problem
The key idea behind our approach is the following:
if we can embed names as points (or equivalently
as vectors) in a suitable geometric space, then the
problem of searching a very large database of names
can be casted as a geometric search problem, i.e. one
of finding the nearest neighbors of the query point in
the database.
As illustrative examples, consider the names
Stephen and Steven. A simple geometric represen-
tation for these names is the one induced by their
corresponding features: {St, te, ep, ph, he, en} and
{St, te, ev, ve, en} 3. In this representation, each
character bigram constitutes a dimension of the geo-
metric feature space whose coordinate value is the
number of times the bigram appears in the name.
It is possible to find a low-dimensional representa-
tion for the names by using Principal Components
Analysis or any other dimensionality reduction tech-
nique on the bigram feature vectors. However, the
key point to note is that once we have an appropri-
ate geometric representation for names, the similar-
ity between two names can be computed as
Kmono (name1, name2) = e?||?1??2||
2/22 (1)
where ?1 and ?2 are the feature vectors of the two
names and  is a constant. Armed with the geomet-
ric similarity measure, we can leverage geometric
search techniques for finding names similar to the
query.
In the case of cross-language name search, we
need a feature representation of names that is lan-
guage/script independent. Once we map names in
different languages/scripts to the same feature space,
we can essentially treat similarity search as a geo-
metric search problem.
3.1 Language/Script Independent Geometric
Representation of Names
To obtain language/script independent geometric
representation of names, we start by forming the lan-
guage/script specific feature vectors as described in
Section 3. Given two names, Stephen in Latin script
and -VFPn in Devanagari script, we form the corre-
sponding character bigram feature vectors ? (using
features {St, te, ep, ph, en}) and ? (using features
{-V, VF, FP, Pn}) respectively. We then map these
vectors to a common geometric feature space using
two linear transformations A and B:
?? AT? = ?s ? Rd (2)
? ? BT? = ?s ? Rd (3)
The vectors ?s and ?s can be viewed as lan-
guage/script independent representation of the
names Stephen and -VFPn.
3Here, we have employed character bigrams as features. In
principle, we can use any suitable set of features including pho-
netic features extracted from the strings.
495
3.1.1 Cross-Language Similarity of Names
In order to search a database of names in English
when the query is in a native language, say Hindi, we
need to be able to measure the similarity of a name in
Devangari script with names in Latin script. The lan-
guage/script independent representation gives a nat-
ural way to measure the similarity of names across
languages. By embedding the language/script spe-
cific feature vectors ? and ? in a common feature
space via the projections A and B, we can com-
pute the similarity of the corresponding names as
follows:
Kcross (name1, name2) = e?||?s??s||
2/22 (4)
It is easy to see from Equation 4 that the similarity
score of two names is small when the projections of
the names are negatively correlated.
3.2 Learning Common Feature Space using
CCA
Ideally, the transformations A and B should be such
that similar names in the two languages are mapped
to close-by points in the common geometric fea-
ture space. It is possible to learn such transforma-
tions from a training set of name transliterations in
the two languages using the well-known multi-view
learning framework of Canonical Correlation Anal-
ysis (Hardoon et al, 2004). By viewing the lan-
guage/script specific feature vectors as two represen-
tations/views of the same semantic object, the entity
whose name is written as Stephen in English and as
-VFPn in Hindi, we can employ the machinery of
CCA to find the transformations A and B.
Given a sample of multivariate data with two
views, CCA finds a linear transformation for each
view such that the correlation between the projec-
tions of the two views is maximized. Consider
a sample Z = {(xi, yi)}Ni=1 of multivariate data
where xi ? Rm and yi ? Rn are two views of the
object. Let X = {xi}Ni=1 and Y = {yi}Ni=1. As-
sume thatX and Y are centered4, i.e., they have zero
mean. Let a and b be two directions. We can project
X onto the direction a to get U = {ui}Ni=1 where
ui = aTxi. Similarly, we can project Y onto the di-
rection b to get the projections V = {vi}ni=1 where
4If X and Y are not centered, they can be centered by sub-
tracting the respective means.
vi = bT yi. The aim of CCA is to find a pair of di-
rections (a, b) such that the projections U and V are
maximally correlated. This is achieved by solving
the following optimization problem:
? = max(a,b)
< Xa,Xb >
||Xa||||Xb||
= max(a,b)
aTXY T b?
aTXXT a
?
bTY Y T b
The objective function of Equation 5 can be max-
imized by solving the following generalized eigen
value problem (Hardoon et al, 2004):
XY T
(
Y Y T
)?1
Y XTa = ?2XXTa
(
Y Y T
)?1
Y XTa = ?b
The subsequent basis vectors can be found
by adding the orthogonality of bases con-
straint to the objective function. Although
the number of basis vectors can be as high as
min{Rank(X), Rank(Y )}, in practice, only the
first few basis vectors are used since the correlation
of the projections is high for these vectors and small
for the remaining vectors.
Let A and B be the first d > 0 basis vectors com-
puted by CCA.
Figure 1: Projected names (English-Hindi).
3.2.1 Common Geometric Feature Space
As described in Section 3.1, we represent names
as points in the common geometric feature space de-
fined by the projection matrices A and B. Figure 1
496
shows a 2-dimensional common feature space com-
puted by CCA for English (Latin script) and Hindi
(Devanagari script) names. As can be seen from the
figure, names that are transliterations of each other
are mapped to near-by points in the common feature
space.
Figure 2 shows a 2-dimensional common feature
space for English (Latin script) and Russian (Cyrillic
script) names. As can be seen from the figure, names
that are transliterations of each other are mapped to
near-by points in the common feature space.
Figure 2: Projected names (English-Russian).
3.3 Querying the Name Database
Given a database D = {ei}Mi=1 of single-word
names in English, we first compute their lan-
guage/script specific feature vectors ?(i), i =
1, . . . ,M . We then compute the projections ?(i)s =
AT?(i). Thus, we transform the name database D
into a set of vectors {?(1)s , . . . , ?(M)s } in Rd.
Given a query name h in Hindi, we compute its
language/script specific feature vector ? and project
it on to the common feature space to get ?s =
BT? ? Rd. Names similar to h in the database D
can be found as solutions of the k-nearest neighbor
problem:
eik = argmaxei?D?{eij }k?1j=1 Kcross (ei, h)
= argmaxei?D?{eij }k?1j=1 e
?||?(i)s ??s||2/22
= argminei?D?{eij }k?1j=1 ||?
(i)
s ? ?s||
Unfortunately, computing exact k-nearest neigh-
bors in dimensions much higher than 8 is difficult
and the best-known methods are only marginally
better than brute-force search (Arya et al, 1998).
Fortunately, there exist very efficient algorithms for
computing approximate nearest neighbors and in
practice they do nearly as well as the exact near-
est neighbors algorithms (Arya et al, 1998). It is
also possible to control the tradeoff between accu-
racy and running time by specifiying a maximum
approximation error bound. We employ the well-
known Approximate Nearest Neighbors (aka ANN)
algorithm by Arya and Mount which is known to do
well in practice when d ? 100 (Arya et al, 1998).
3.4 Combining Single-Word Similarities
The approach described in the previous sections
works only for single-word names. We need to com-
bine the similarities at the level of individual words
into a similarity function for multi-word names. To-
wards this end, we form a weighted bipartite graph
from the two multi-word names as follows:
We first tokenize the Hindi query name into sin-
gle word tokens and find the nearest English neigh-
bors for each of these Hindi tokens using the method
outlined section 3.3. We then find out all the En-
glish Words which contain one or more of the En-
glish neighbors thus fetched. Let E = e1e2 . . . eI
be one such multi-word English name and H =
h1h2 . . . hJ be the multi-word Hindi query. We form
a weighted bipartite graph G = (S ? T,W ) with a
node si for the ith word ei in E and node tj for the
jth word hj in H . The weight of the edge (si, tj) is
set as wij = Kcross (ei, hj).
Let w be the weight of the maximum weighted
bipartite matching in the graph G. We define the
similarity between E and H as follows:
Kcross (E,H) =
w
|I ? J |+ 1 . (5)
The numerator of the right hand side of Equation
5 favors name pairs which have a good number of
high quality matches at the individual word level
whereas the denominator penalizes pairs that have
disproportionate lengths.
Note that, in practice, both I and J are small and
hence we can find the maximum weighted bipartite
matching very easily. Further, most edge weights in
497
Figure 3: Combining Single-Word Similarities.
the bipartite graph are negligibly small. Therefore,
even a greedy matching algorithm suffices in prac-
tice.
4 Experiments and Results
In the remainder of this section, we refer to our sys-
tem by GEOM-SEARCH.
4.1 Experimental Setup
We tested our cross language name search system
using six native languages, viz., Russian, Hebrew,
Hindi, Kannada, Tamil and Bangla. For each of
these languages, we created a test set consisting of
1000 multi-word name queries and found manually
the most relevant Wikipedia article for each query in
the test set. The Wikipedia articles thus found and
all the redirect titles that linked to them formed the
gold standard for evaluating the performance of our
system.
In order to compare the performance of GEOM-
SEARCH with a reasonable baseline, we imple-
mented the following baseline: We used a state-of-
the art Machine Transliteration system to generate
the best transliteration of each of the queries. We
used the edit distance between the transliteration and
the single-word English name as the similarity score.
We combined single word similarities using the ap-
proach described in Section 3.4. We refer to this
baseline by TRANS-SEARCH.
Note that several English Wikipedia names some-
times get the same score for a query. Therefore,
we used a tie-aware mean-reciprocal rank measure
to evaluate the performance (McSherry and Najork,
2008).
4.2 GEOM-SEARCH
The training and search procedure employed by
GEOM-SEARCH are described below.
4.2.1 CCA Training
We learnt the linear transformations A and B that
project the language/script specific feature vectors to
the common feature space using the approach dis-
cussed in Section 3.2. The learning algorithm re-
quires a training set consisting of pairs of single-
word names in English and the respective native lan-
guage. We used approximately 15, 000 name pairs
for each native language.
A key parameter in CCA training is the number of
dimensions of the common feature space. We found
the optimal number of dimensions using a tuning set
consisting of 1, 000 correct name pairs and 1, 000
incorrect name pairs for each native language. We
found that d = 50 is a very good choice for each
native language.
Another key aspect of training is the choice of
language/script specific features. For the six lan-
guages we experimented with and also for English,
we found that character bigrams formed a good set
of features. We note that for languages such as Chi-
nese, Japanese, and Korean, unigrams are the best
choice. Also, for these languages, it may help to
syllabify the English name.
4.2.2 Search
As a pre-processing step, we extracted a list of 1.3
million unique words from the Wikipedia titles. We
computed the language/script specific feature vector
for each word in this list and projected the vector to
the common feature space as described in Section
3.1. The low-dimensional embeddings thus com-
puted formed the input to the ANN algorithm.
We tokenized each query in the native language
into constituent words. For each constituent, we first
computed the language/script specific feature vector,
projected it to the common feature space, and found
the k-nearest neighbors using the ANN algorithm.
We used k=100 for all our experiments.
After finding the nearest neighbors and the corre-
sponding similarity scores, we combined the scores
using the approach described in Section 3.4.
4.3 TRANS-SEARCH
The training and search procedure employed by
TRANS-SEARCH are described below.
498
Figure 4: Top scoring English Wikipedia page retrieved by GEOM-SEARCH
4.3.1 Transliteration Training
We used a state-of-the-art CRF-based translitera-
tion technique for transliterating the native language
names (Khapra and Bhattacharyya, 2009). We used
CRF++, an open-source CRF training tool, to train
the transliteration system. We used exactly the
same features and parameter settings as described in
(Khapra and Bhattacharyya, 2009). As in the case of
CCA, we use around 15, 000 single word name pairs
in the training.
4.3.2 Search
The preprocessing step for TRANS-SEARCH is
the same as that for GEOM-SEARCH. We translit-
erated each constituent of the query into English and
find all single-word English names that are at an edit
distance of at most 3. We computed the similarity
score as described in Section 3.4.
4.4 Evaluation
We evaluated the performance of GEOM-SEARCH
and TRANS-SEARCH using a tie-aware mean re-
ciprocal rank (MRR). Table 4 compares the average
time per query and the MRR of the two systems.
GEOM-SEARCH performed significantly better
than the transliteration based baseline system for all
the six languages. On an average, the relevant En-
glish Wikipedia page was found in the top 2 re-
sults produced by GEOM-SEARCH for all the six
native languages. Clearly, this shows that GEOM-
SEARCH is highly effective as a cross-langauge
name search system. The good results also validate
our claim that cross-language name search can im-
Table 4: MRR and average time per query (in seconds)
for the two systems.
Language GEOM TRANS
Time MRR Time MRR
Hin 0.51 0.686 2.39 0.485
Tam 0.23 0.494 2.16 0.291
Kan 1.08 0.689 2.17 0.522
Ben 1.30 0.495 ? ?
Rus 0.15 0.563 1.65 0.476
Heb 0.65 0.723 ? ?
prove the multi-lingual user experience of ESL/EFL
users.
5 Conclusions
GEOM-SEARCH, a geometry-based cross-
language name search system for Wikipedia,
improves the multilingual experience of ESL/EFL
users of Wikipedia by allowing them to formulate
queries in their native languages. Further, it is easy
to integrate a Machine Translation system with
GEOM-SEARCH. Such a system would find the
relevant English Wikipedia page for a query using
GEOM-SEARCH and then translate the relevant
Wikipedia pages to the native language using the
Machine Translation system.
6 Acknowledgement
We thank Jagadeesh Jagarlamudi and Shaishav Ku-
mar for their help.
499
References
Farooq Ahmad and Grzegorz Kondrak. 2005. Learn-
ing a spelling error model from search query logs. In
HLT ?05: Proceedings of the conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing, pages 955?962, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Sunil Arya, David M. Mount, Nathan S. Netanyahu, Ruth
Silverman, and Angela Y. Wu. 1998. An optimal
algorithm for approximate nearest neighbor searching
fixed dimensions. J. ACM, 45(6):891?923.
?Eric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herve? De?jean. 2004. A geometric
view on bilingual lexicon extraction from comparable
corpora. In ACL, pages 526?533.
Dan Goldwasser and Dan Roth. 2008. Transliteration as
constrained optimization. In EMNLP, pages 353?362.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT, pages 771?779, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
David R. Hardoon, Sa?ndor Szedma?k, and John Shawe-
Taylor. 2004. Canonical correlation analysis: An
overview with application to learning methods. Neu-
ral Computation, 16(12):2639?2664.
Liang Jin, Nick Koudas, Chen Li, and Anthony K. H.
Tung. 2005. Indexing mixed types for approximate
retrieval. In VLDB, pages 793?804.
Mitesh Khapra and Pushpak Bhattacharyya. 2009. Im-
proving transliteration accuracy using word-origin de-
tection and lexicon lookup. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration (NEWS 2009). Association for Computational
Linguistics.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilingual
comparable corpora. In HLT-NAACL.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599?
612.
Jin-Shea Kuo, Haizhou Li, and Ying-Kuei Yang. 2006.
Learning transliteration lexicons from the web. In
ACL.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML, pages 282?289.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009. Report of news 2009 machine
transliteration shared task. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration (NEWS 2009). Association for Computational
Linguistics.
Frank McSherry and Marc Najork. 2008. Computing
information retrieval performance measures efficiently
in the presence of tied scores. In ECIR, pages 414?
421.
Jeff Pasternack and Dan Roth. 2009. Learning better
transliterations. In CIKM, pages 177?186.
Sujith Ravi and Kevin Knight. 2009. Learning phoneme
mappings for transliteration without parallel data. In
NAACL-HLT.
Hanan Samet. 2006. Foundations of Multidimensional
and Metric Data Structures (The Morgan Kaufmann
Series in Computer Graphics). Morgan Kaufmann,
August.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In ACL.
Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006.
Named entity transliteration with comparable corpora.
In ACL.
Raghavendra Udupa, K. Saravanan, Anton Bakalov, and
Abhijit Bhole. 2009a. ?they are out there, if you
know where to look?: Mining transliterations of oov
query terms for cross-language information retrieval.
In ECIR, pages 437?448.
Raghavendra Udupa, K. Saravanan, A. Kumaran, and Ja-
gadeesh Jagarlamudi. 2009b. Mint: A method for ef-
fective and scalable mining of named entity transliter-
ations from large comparable corpora. In EACL, pages
799?807.
Paola Virga and Sanjeev Khudanpur. 2003. Transliter-
ation of proper names in cross-language applications.
In SIGIR, pages 365?366.
500
Proceedings of NAACL-HLT 2013, pages 315?324,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Improving reordering performance using higher order and structural
features
Mitesh M. Khapra
IBM Research India
mikhapra@in.ibm.com
Ananthakrishnan Ramanathan
IBM Research India
anandr42@gmail.com
Karthik Visweswariah
IBM Research India
v-karthik@in.ibm.com
Abstract
Recent work has shown that word aligned data
can be used to learn a model for reordering
source sentences to match the target order.
This model learns the cost of putting a word
immediately before another word and finds the
best reordering by solving an instance of the
Traveling Salesman Problem (TSP). However,
for efficiently solving the TSP, the model is
restricted to pairwise features which examine
only a pair of words and their neighborhood.
In this work, we go beyond these pairwise fea-
tures and learn a model to rerank the n-best
reorderings produced by the TSP model us-
ing higher order and structural features which
help in capturing longer range dependencies.
In addition to using a more informative set
of source side features, we also capture target
side features indirectly by using the transla-
tion score assigned to a reordering. Our exper-
iments, involving Urdu-English, show that the
proposed approach outperforms a state-of-the-
art PBSMT system which uses the TSP model
for reordering by 1.3 BLEU points, and a pub-
licly available state-of-the-art MT system, Hi-
ero, by 3 BLEU points.
1 Introduction
Handling the differences in word orders between
pairs of languages is crucial in producing good ma-
chine translation. This is especially true for lan-
guage pairs such as Urdu-English which have sig-
nificantly different sentence structures. For exam-
ple, the typical word order in Urdu is Subject Object
Verb whereas the typical word order in English is
Subject Verb Object. Phrase based systems (Koehn
et al, 2003) rely on a lexicalized distortion model
(Al-Onaizan and Papineni, 2006; Tillman, 2004)
and the target language model to produce output
words in the correct order. This is known to be in-
adequate when the languages are very different in
terms of word order (refer to Table 3 in Section 3).
Pre-ordering source sentences while training and
testing has become a popular approach in overcom-
ing the word ordering challenge. Most techniques
for pre-ordering (Collins et al, 2005; Wang et al,
2007; Ramanathan et al, 2009) depend on a high
quality source language parser, which means these
methods work only if the source language has a
parser (this rules out many languages). Recent work
(Visweswariah et al, 2011) has shown that it is pos-
sible to learn a reordering model from a relatively
small number of hand aligned sentences . This elim-
inates the need of a source or target parser.
In this work, we build upon the work of
Visweswariah et al (2011) which solves the reorder-
ing problem by treating it as an instance of the
Traveling Salesman Problem (TSP). They learn a
model which assigns costs to all pairs of words in
a sentence, where the cost represents the penalty of
putting a word immediately preceding another word.
The best permutation is found via the chained Lin-
Kernighan heuristic for solving a TSP. Since this
model relies on solving a TSP efficiently, it cannot
capture features other than pairwise features that ex-
amine the words and neighborhood for each pair of
words in the source sentence. In the remainder of
this paper we refer to this model as the TSP model.
Our aim is to go beyond this limitation of the TSP
model and use a richer set of features instead of us-
ing pairwise features only. In particular, we are in-
terested in features that allow us to examine triples
of words/POS tags in the candidate reordering per-
315
mutation (this is akin to going from bigram to tri-
gram language models), and also structural features
that allow us to examine the properties of the seg-
mentation induced by the candidate permutation. To
go beyond the set of features incorporated by the
TSP model, we do not solve the search problem
which would be NP-hard. Instead, we restrict our-
selves to an n-best list produced by the base TSP
model and then search in that list. Using a richer
set of features, we learn a model to rerank these n-
best reorderings. The parameters of the model are
learned using the averaged perceptron algorithm. In
addition to using a richer set of source side features
we also indirectly capture target side features by in-
terpolating the score assigned by our model with the
score assigned by the decoder of a MT system.
To justify the use of these informative features,
we point to the example in Table 1. Here, the head
(driver) of the underlined English Noun Phrase (The
driver of the car) appears to the left of the Noun
Phrase whereas the head (chaalak {driver}) of the
corresponding Urdu Noun Phrase (gaadi {car} ka
{of} chaalak {driver}) appears to the right of the
Noun Phrase. To produce the correct reordering of
the source Urdu sentence the model has to make an
unusual choice of putting gaadi {car} before bola
{said}. We say this is an unusual choice because the
model examines only pairwise features and it is un-
likely that it would have seen sentences having the
bigram ?car said?. If the exact segmentation of the
source sentence was known, then the model could
have used the information that the word gaadi {car}
appears in a segment whose head is the noun chaalak
{driver} and hence its not unusual to put gaadi {car}
before bola {said} (because the construct ?NP said?
is not unusual). However, since the segmentation
of the source sentence is not known in advance, we
use a heuristic (explained later) to find the segmen-
tation induced by a reordering. We then extract
features (such as first word current segment,
end word current segment) to approximate these
long range dependencies.
Using this richer set of features with Urdu-
English as the source language pair, our approach
outperforms the following state of the art systems:
(i) a PBSMT system which uses TSP model for re-
ordering (by 1.3 BLEU points), (ii) a hierarchical
PBSMT system (by 3 BLEU points). The overall
Input Urdu: fir gaadi ka chaalak kuch bola
Gloss: then car of driver said something
English: Then the driver of the car said something.
Ref. reordering: fir chaalak ka gaadi bola kuch
Table 1: Example motivating the use of structural features
gain is 6.3 BLEU points when compared to a stan-
dard PBSMT system which uses a lexicalized distor-
tion model (Al-Onaizan and Papineni, 2006).
The rest of this paper is organized as follows. In
Section 2 we discuss our approach of re-ranking the
n-best reorderings produced by the TSP model. This
includes a discussion of the model used, the features
used and the algorithm used for learning the parame-
ters of the model. It also includes a discussion on the
modification to the Chained Lin-Kernighan heuris-
tic to produce n-best reorderings. Next, in Section
3 we describe our experimental setup and report the
results of our experiments. In Section 4 we present
some discussions based on our study. In section 5 we
briefly describe some prior related work. Finally, in
Section 6, we present some concluding remarks and
highlight possible directions for future work.
2 Re-ranking using higher order and
structural features
As mentioned earlier, the TSP model (Visweswariah
et al, 2011) looks only at local features for a word
pair (wi, wj). We believe that for better reorder-
ing it is essential to look at higher order and struc-
tural features (i.e., features which look at the overall
structure of a sentence). The primary reason why
Visweswariah et al (2011) consider only pairwise
bigram features is that with higher order features the
reordering problem can no longer be cast as a TSP
and hence cannot be solved using existing efficient
heuristic solvers. However, we do not have to deal
with an NP-Hard search problem because instead of
considering all possible reorderings we restrict our
search space to only the n-best reorderings produced
by the base TSP model. Formally, given a set of
reorderings, ? = [pi1, pi2, pi3, ...., pin], for a source
sentence s, we are interesting in assigning a score,
score(pi), to each of these reorderings and pick the
reordering which has the highest score. In this paper,
we parametrize this score as:
score(pi) = ?T?(pi) (1)
316
where, ? is the weight vector and ?(pi) is a vector
of features extracted from the reordering pi. The aim
then is to find,
pi? = arg max
pi??
score(pi) (2)
In the following sub-sections, we first briefly
describe our overall approach towards finding pi?.
Next, we describe our modification to the Lin-
Kernighan heuristic for producing n-best outputs
for TSP instead of the 1-best output used by
(Visweswariah et al, 2011). We then discuss the fea-
tures used for re-ranking these n-best outputs, fol-
lowed by a discussion on the learning algorithm used
for estimating the parameters of the model. Finally,
we describe how we interpolate the score assigned
by our model with the score assigned by the decoder
of a SMT engine to indirectly capture target side fea-
tures.
2.1 Overall approach
The training stage of our approach involves two
phases : (i) Training a TSP model which will be
used to generate n-best reorderings and (ii) Training
a re-ranking model using these n-best reorderings.
For training both the models we need a collection
of sentences where the desired reordering pi?(x) for
each input sentence x is known. These reference or-
derings are derived from word aligned source-target
sentence pairs (see first 4 rows of Figure 1). We first
divide this word aligned data into N parts and use
A?i to denote the alignments leaving out the i-th
part. We then train a TSP model M?i using refer-
ence reorderings derived from A?i as described in
(Visweswariah et al, 2011). Next, we produce n-
best reorderings for the source sentences using the
algorithm getNBestReorderings(sentence) de-
scribed later. Dividing the data into N parts is nec-
essary to ensure that the re-ranking model is trained
using a realistic n-best list rather than a very opti-
mistic n-best list (which would be the case if part i
is reordered using a model which has already seen
part i during training).
Each of the n-best reorderings is then repre-
sented as a feature vector comprising of higher
order and structural features. The weights
of these features are then estimated using the
averaged perceptron method. At test time,
getNBestReorderings(sentence) is used to gen-
erate the n-best reorderings for the test sentence us-
ing the trained TSP model. These reorderings are
then represented using higher order and structural
features and re-ranked using the weights learned ear-
lier. We now describe the different stages of our al-
gorithm.
2.2 Generating n-best reorderings for the TSP
model
The first stage of our approach is to train a TSP
model and generate n-best reorderings using it. The
decoder used by Visweswariah et al (2011) relies
on the Chained Lin-Kernighan heuristic (Lin and
Kernighan, 1973) to produce the 1-best permutation
for the TSP problem. Since our algorithm aims at
re-ranking an n-best list of permutations (reorder-
ings), we made a modification to the Chained Lin-
Kernighan heuristic to produce this n-best list as
shown in Algorithm 1 .
Algorithm 1 getNBestReorderings(sentence)
NbestSet = ?
pi? = Identity permutation
pi? = linkernighan(pi?)
insert(NbestSet, pi?)
for i = 1? nIter do
pi
?
= perturb(pi?)
pi
?
= linkernighan(pi
?
)
if C(pi
?
) < maxpi?NbestSetC(pi) then
InsertOrReplace(NbestSet, pi
?
)
end if
if C(pi
?
) < C(pi?) then
pi? = pi
?
end if
end for
In Algorithm 1 perturb() is a four-edge pertur-
bation described in (Applegate et al, 2003), and
linkernighan() is the Lin-Kernighan heuristic that
applies a sequence of flips that potentially returns
a lower cost permutation as described in (Lin and
Kernighan, 1973). The cost C(pi) is calculated us-
ing a trained TSP model.
2.3 Features
We represent each of the n-best reorderings obtained
above as a vector of features which can be divided
into two sets : (i) higher order features and (ii) struc-
317
Segmentation Based Features
(extracted for every segment in
the induced segmentation)
Features fired for the seg-
ment [mere(PRP) ghar(NN)]
in Figure1
end lex current segment ghar
end lex prev segment Shyam
end pos current segment NN
end pos prev segment NN
length of current segment 2
first lex current segment mere
first lex next segment aaye
first pos current segment PRP
first pos next segment V RB
Higher order features Features fired for the triplet
Shyam(NN) the(Vaux)
aaye(VRB) in Figure1
lex triplet jumps lex triplet = ?Shyam the
aaye? && jumps = [4,?1]
pos triplet jumps pos triplet = ?NN Vaux
VRB? && jumps = [4,?1]
Table 2: Features used in our model.
tural features. The higher order features are es-
sentially trigram lexical and pos features whereas
the structural features are derived from the sentence
structure induced by a reordering (explained later).
2.3.1 Higher Order Features
Since deriving a good reordering would essen-
tially require analyzing the syntactic structure of the
source sentence, the tasks of reordering and parsing
are often considered to be related. The main motiva-
tion for using higher order features thus comes from
a related work on parsing (Koo and Collins, 2010)
where the performance of a state of the art parser
was improved by considering higher order depen-
dencies. In our model we use trigram features (see
Table 2) of the following form:
?(rui, rui+1, rui+2, J(rui, rui+1), J(rui+1, rui+2))
where rui =word at position i in the reordered
source sentence and J(x, y) = difference between
the positions of x and y in the original source
sentence.
Figure 1 shows an example of jumps between dif-
ferent word pairs in an Urdu sentence. Since such
higher order features will typically be sparse, we
also use some back-off features. For example, in-
stead of using the absolute values of jumps we di-
vide the jumps into 3 buckets, viz., high, low and
medium and use these buckets in conjunction with
the triplets as back-off features.
Figure 1: Segmentation induced on the Urdu sentence
when it is reordered according to its English translation.
Note that the words Shyam and mere are adjacent to each
other in the original Urdu sentence but not in the re-
ordered Urdu sentence. Hence, the word mere marks the
beginning of a new segment.
2.3.2 Structural Features
The second set of features is based on the hy-
pothesis that any reordering of the source sentence
induces a segmentation on the sentence. This seg-
mentation is based on the following heuristic: if wi
and wi+1 appear next to each other in the original
sentence but do not appear next to each other in the
reordered sentence then wi marks the end of a seg-
ment and wi+1 marks the beginning of the next seg-
ment. To understand this better please refer to Fig-
ure 1 which shows the correct reordering of an Urdu
sentence based on its English translation and the cor-
responding segmentation induced on the Urdu sen-
tence. If the correct segmentation of a sentence is
known in advance then one could use a hierarchical
model where the goal would be to reorder segments
instead of reordering words individually (basically,
instead of words, treat segments as units of reorder-
ing. In principle, this is similar to what is done by
parser based reordering methods). Since the TSP
model does not explicitly use segmentation based
features it often produces wrong reorderings (refer
to the motivating example in Section 1).
Reordering such sentences correctly requires
some knowledge about the hierarchical structure of
the sentence. To capture such hierarchical informa-
tion, we use features which look at the elements
318
(words, pos tags) of a segment and its neighboring
segments. These features along with examples are
listed in Table 2. These features should help us in
selecting a reordering which induces a segmentation
which is closest to the correct segmentation induced
by the reference reordering. Note that every feature
listed in Table 2 is a binary feature which takes on
the value 1 if it fires for the given reordering and
value 0 if it does not fire for the given reordering. In
addition to the features listed in Table 2 we also use
the score assigned by the TSP model as a feature.
2.4 Estimating model parameters
We use perceptron as the learning algorithm for es-
timating the parameters of our model described in
Equation 1. To begin with, all parameters are ini-
tialized to 0 and the learning algorithm is run for N
iterations. During each iteration the parameters are
updated after every training instance is seen. For ex-
ample, during the i-th iteration, after seeing the j-th
training sentence, we update the k-th parameter ?k
using the following update rule:
?(i,j)k = ?
(i,j?1)
k + ?k(pi
gold
j )? ?k(pi
?
j ) (3)
where, ?(i,j)k = value of the k-th parameter after
seeing sentence j in iteration i
?k = k-th feature
pigoldj = gold reordering for the j-th sentence
pi?j = arg max
pi??j
?(i,j?1)
T
?(pi)
where ?j is the set of n-best reorderings for the j-
th sentence. pi?j is thus the highest-scoring reorder-
ing for the j-th sentence under the current parame-
ter vector. Since the averaged perceptron method is
known to perform better than the perceptron method,
we used the averaged values of the parameters at the
end of N iterations, calculated as:
?avgk =
1
N ? t
N?
i=1
t?
j=1
?(i,j)k (4)
where, N = Number of iterations
t = Number of training instances
We observed that in most cases the reference re-
ordering in not a part of the n-best list produced
by the TSP model. In such cases instead of using
?k(pi
gold
j ) for updating the weights in Equation 3 we
use ?k(pi
closest to gold
j ) as this is known to be a better
strategy for learning a re-ranking model (Arun and
Koehn, 2007). piclosest to goldj is given by:
arg max
piij??j
# of common bigram pairs in piij and pi
gold
j
len(pigoldj )
where, ?j = set of n-best reorderings for j
th sentence
piclosest to goldj is thus the reordering which has the
maximum overlap with pigoldj in terms of the number
of word pairs (wm, wn) where wn is put next to wm.
2.5 Interpolating with MT score
The approach described above aims at producing a
better reordering by extracting richer features from
the source sentence. Since the final aim is to im-
prove the performance of an MT system, it would
potentially be beneficial to interpolate the scores as-
signed by Equation 1 to a given reordering with the
score assigned by the decoder of an MT system to
the translation of the source sentence under this re-
ordering. Intuitively, the MT score would allow us
to capture features from the target sentence which
are obviously not available to our model. With this
motivation, we use the following interpolated score
(scoreI ) to select the best translation.
scoreI(ti) = ??score?(pii) + (1? ?) ? scoreMT (ti)
where, ti =translation produced under the i-th
reordering of the source sentence
score?(pii) =score assigned by our model to the
i-th reordering
scoreMT (ti) =score assigned by the MT system to ti
The weight ? is used to ensure that score?(pii) and
scoreMT (pii) are in the same range (it just serves as
a normalization constant). We acknowledge that the
above process is expensive because it requires the
MT system to decode n reorderings for every source
sentence. However, the aim of this work is to show
that interpolating with the MT score which implic-
itly captures features from the target sentence helps
in improving the performance. Ideally, this interpo-
lation should (and can) be done at decode time with-
out having to decode n reorderings for every source
319
sentence (for example by expressing the n reorder-
ings as a lattice), but, we leave this as future work.
3 Empirical evaluation
We evaluated our reordering approach on Urdu-
English. We use two types of evaluation, one in-
trinsic and one extrinsic. For intrinsic evaluation,
we compare the reordered source sentence in Urdu
with a reference reordering obtained from the hand
alignments using BLEU (referred to as monolingual
BLEU or mBLEU by (Visweswariah et al, 2011) ).
Additionally, we evaluate the effect of reordering on
MT performance using BLEU (extrinsic evaluation).
As mentioned earlier, our training process in-
volves two phases : (i) Generating n-best reorder-
ings for the training data and (ii) using these n-best
reorderings to train a perceptron model. We use the
same data for training the reordering model as well
as our perceptron model. This data contains 180K
words of manual alignments (part of the NIST MT-
08 training data) and 3.9M words of automatically
generated machine alignments (1.7M words from
the NIST MT-08 training data1 and 2.2M words ex-
tracted from sources on the web2). The machine
alignments were generated using a supervised maxi-
mum entropy model (Ittycheriah and Roukos, 2005)
and then corrected using an improved correction
model (McCarley et al, 2011). We first divide the
training data into 10 folds. The n-best reorder-
ings for each fold are then generated using a model
trained on the remaining 9 folds. This division into
10 folds is done for reasons explained earlier in Sec-
tion 2.1. These n-best reorderings are then used to
train the perceptron model as described in Section
2.4. Note that Visweswariah et al (2011) used only
manually aligned data for training the TSP model.
However, we use machine aligned data in addition
to manually aligned data for training the TSP model
as it leads to better performance. We used this im-
provised TSP model as the state of the art baseline
(rows 2 and 3 in Tables 3 and 4 respectively) for
comparing with our approach.
We observed that the perceptron algorithm con-
verges after 5 iterations beyond which there is very
little (<1%) improvement in the bigram precision on
1http://www.ldc.upenn.edu
2http://centralasiaonline.com
the training data itself (bigram precision is the frac-
tion of word pairs which are correctly put next to
each other). Hence, for all the numbers reported in
this paper, we used 5 iterations of perceptron train-
ing. Similarly, while generating the n-best reorder-
ings, we experimented with following values of n :
10, 25, 50, 100 and 200. We observed that, by re-
stricting the search space to the top-50 reorderings
we get the best reordering performance (mBLEU)
on a development set. Hence, we used n=50 for our
MT experiments.
For intrinsic evaluation we use a development set
of 8017 Urdu tokens reordered manually. Table 3
compares the performance of the top-1 reordering
output by our algorithm with the top-1 reordering
generated by the improved TSP model in terms of
mBLEU. We see a gain of 1.8 mBLEU points with
our approach.
Next, we see the impact of the better reorderings
produced by our system on the performance of
a state-of-the-art MT system. For this, we used
a standard phrase based system (Al-Onaizan and
Papineni, 2006) with a lexicalized distortion model
with a window size of +/-4 words (Tillmann and
Ney, 2003). As mentioned earlier, our training data
consisted of 3.9M words including the NIST MT-08
training data. We use HMM alignments along with
higher quality alignments from a supervised aligner
(McCarley et al, 2011). The Gigaword English
corpus was used for building the English language
model. We report results on the NIST MT-08
evaluation set, averaging BLEU scores from the
News and Web conditions to provide a single BLEU
score. Table 4 compares the MT performance
obtained by reordering the training and test data
using the following approaches:
1. No pre-ordering: A baseline system which
does not use any source side reordering as a pre-
processing step
2. HIERO : A state of the art hierarchical phrase
based translation system (Chiang, 2007)
3. TSP: A system which uses the 1-best reordering
produced by the TSP model
4. Higher order & structural features: A system
320
Approach mBLEU
Unreordered 31.2
TSP 56.6
Higher order & structural features 58.4
Table 3: mBLEU scores for Urdu to English reordering
using different models.
Approach BLEU
No pre-ordering 21.9
HIERO 25.2
TSP 26.9
Higher order & structural features 27.5
Interpolating with MT score 28.2
Table 4: MT performance for Urdu to English without re-
ordering and with reordering using different approaches.
which reranks n-best reorderings produced by TSP
using higher order and structural features
5. Interpolating with MT score : A system which
interpolates the score assigned to a reordering by
our model with the score assigned by a MT system
We used Joshua 4.0 (Ganitkevitch et al, 2012)
which provides an open source implementation of
HIERO. For training, tuning and testing HIERO
we used the same experimental setup as described
above. As seen in Table 4, we get an overall gain of
6.2 BLEU points with our approach as compared to
a baseline system which does not use any reordering.
More importantly, we outperform (i) a PBSMT sys-
tem which uses the TSP model by 1.3 BLEU points
and (ii) a state of the art hierarchical phrase based
translation system by 3 points.
4 Discussions
We now discuss some error corrections and ablation
tests.
4.1 Example of error correction
We first give an example where the proposed ap-
proach performed better than the TSP model. In the
example below, I = input sentence, E= gold English
translation, T = incorrect reordering produced by
TSP and O = correct reordering produced by our
approach. Note that the words roman catholic aur
protestant in the input sentence get translated as
Sentence length mBLEU
Unreordered TSP Our
approach
1-14 words (small) 29.7 58.7 57.8
15-22 words (med.) 28.2 56.8 59.2
23+ words (long) 33.4 55.8 58.2
All 31.2 56.6 58.4
Table 5: mBLEU improvements on sentences of different
lengths
a continuous phrase in English (Roman Catholic
and Protestant) and hence should be treated as a
single unit by the reordering model. The TSP model
fails to keep this segment intact whereas our model
(which uses segmentation based features) does so
and matches the reference reordering.
I: ab roman catholic aur protestant ke darmiyaan
ikhtilafat khatam ho chuke hai
E: The differences between Roman Catholics and
Protestants have now ended
T: ab roman ikhtilafat ke darmiyaan catholic aur
protestant hai khatam ho chuke
O: ab ikhtilafat ke darmiyaan roman catholic aur
protestant hai khatam ho chuke
4.2 Performance based on sentence length
We split the test data into roughly three equal parts
based on length, and calculated the mBLEU im-
provements on each of these parts as reported in
Table 5. These results show that the model works
much better for medium-to-long sentences. In fact,
we see a drop in performance for small sentences. A
possible reason for this could be that the structural
features that we use are derived through a heuristic
that is error-prone, and in shorter sentences, where
there would be fewer reordering problems, these er-
rors hurt more than they help. While this needs to be
analyzed further, we could meanwhile combine the
two models fruitfully by using the base TSP model
for small sentences and the new model for longer
sentences.
321
Disabled feature mBLEU
end lex current segment 57.6
end lex prev segment 57.6
end pos current segment 57.8
end pos prev segment 57.4
length 57.6
lex triplet jumps 58.0
pos triplet jumps 56.1
first lex current segment 58.2
first lex next segment 58.2
first pos current segment 57.6
first pos next segment 57.6
NONE 58.4
Table 6: Ablation test indicating the contribution of each
feature to the reordering performance.
4.3 Ablation test
To study the contribution of each feature to the
reordering performance, we did an ablation test
wherein we disabled one feature at a time and mea-
sured the change in the mBLEU scores. Table 6
summarizes the results of our ablation test. The
maximum drop in performance is obtained when the
pos triplet jumps feature is disabled. This obser-
vation supports our claim that higher order features
(more than bigrams) are essential for better reorder-
ing. The lex triplet jumps feature has the least
impact on the performance mainly because it is a
lexicalized feature and hence very sparse. Also note
that there is a high correlation between the perfor-
mances obtained by dropping one feature from each
of the following pairs :
i) first lex current segment, first lex next segment
ii) first pos current segment, first pos next segment
iii) end lex current segment, end lex next segment.
This is because these pairs of features are
highly dependent features. Note that similar to
the pos triplet jumps feature we also tried a
pos quadruplet jumps feature but it did not help
(mainly due to overfitting and sparsity).
5 Related Work
There are several studies which have shown that re-
ordering the source side sentence to match the target
side order leads to improvements in Machine Trans-
lation. These approaches can be broadly classified
into three types. First, approaches which reorder
source sentences by applying rules to the source side
parse; the rules are either hand-written (Collins et
al., 2005; Wang et al, 2007; Ramanathan et al,
2009) or learned from data (Xia and McCord, 2004;
Genzel, 2010; Visweswariah et al, 2010). These
approaches require a source side parser which is
not available for many languages. The second type
of approaches treat machine translation decoding
as a parsing problem by using source and/or tar-
get side syntax in a Context Free Grammar frame-
work. These include Hierarchical models (Chi-
ang, 2007) and syntax based models (Yamada and
Knight, 2002; Galley et al, 2006; Liu et al, 2006;
Zollmann and Venugopal, 2006). The third type of
approaches, avoid the use of a parser (as required
by syntax based models) and instead train a reorder-
ing model using reference reorderings derived from
aligned data. These approaches (Tromble and Eis-
ner, 2009; Visweswariah et al, 2011; DeNero and
Uszkoreit, 2011; Neubig et al, 2012) have a low de-
code time complexity as reordering is done as a pre-
processing step and not integrated with the decoder.
Our work falls under the third category, as it im-
proves upon the work of (Visweswariah et al, 2011)
which is closely related to the work of (Tromble
and Eisner, 2009) but performs better. The focus
of our work is to use higher order and structural
features (based on segmentation of the source sen-
tence) which are not captured by their model. Some
other works have used collocation based segmenta-
tion (Henr??quez Q. et al, 2010) and Multiword Ex-
pressions as segments (Bouamor et al, 2012) to im-
prove the performance of SMT but without much
success. The idea of improving performance by re-
ranking a n-best list of outputs has been used re-
cently for the related task of parsing (Katz-Brown et
al., 2011) using targeted self-training for improving
the performance of reordering. However, in contrast,
in our work we directly aim at improving the perfor-
mance of a reordering model.
6 Conclusion
In this work, we proposed a model for re-ranking
the n-best reorderings produced by a state of the
art reordering model (TSP model) which is limited
to pair wise features. Our model uses a more in-
formative set of features consisting of higher order
features, structural features and target side features
322
(captured indirectly using translation scores). The
problem of intractability is solved by restricting the
search space to the n-best reorderings produced by
the TSP model. A detailed ablation test shows that
of all the features used, the pos triplet features are
most informative for reordering. A gain of 1.3 and 3
BLEU points over a state of the art phrase based and
hierarchical machine translation system respectively
provides good extrinsic validation of our claim that
such long range features are useful.
As future work, we would like to evaluate our al-
gorithm on other language pairs. We also plan to
integrate the score assigned by our model into the
decoder to avoid having to do n decodings for ev-
ery source sentence. Also, it would be interesting
to model the segmentation explicitly, where the aim
would be to first segment the sentence and then use
a two level hierarchical reordering model which first
reorders these segments and then reorders the words
within the segment.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of ACL, ACL-44, pages 529?536, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
David Applegate, William Cook, and Andre Rohe. 2003.
Chained lin-kernighan for large traveling salesman
problems. In INFORMS Journal On Computing.
Abhishek Arun and Philipp Koehn. 2007. Online
learning methods for discriminative training of phrase
based statistical machine translation. In In Proceed-
ings of MT Summit.
Dhouha Bouamor, Nasredine Semmar, and Pierre
Zweigenbaum. 2012. Identifying bilingual multi-
word expressions for statistical machine translation.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Mehmet Uur Doan, Bente
Maegaard, Joseph Mariani, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey, may. European
Language Resources Association (ELRA).
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Comput. Linguist., 33(2):201?228, June.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Morristown, NJ, USA. Association for Computational
Linguistics.
John DeNero and Jakob Uszkoreit. 2011. Inducing sen-
tence structure from parallel corpora for reordering.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 193?203, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 961?968, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post,
and Chris Callison-Burch. 2012. Joshua 4.0: Pack-
ing, pro, and paraphrases. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
pages 283?291, Montre?al, Canada, June. Association
for Computational Linguistics.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, COLING ?10,
pages 376?384, Stroudsburg, PA, USA. Association
for Computational Linguistics.
A. Carlos Henr??quez Q., R. Marta Costa-jussa`, Vidas
Daudaravicius, E. Rafael Banchs, and B. Jose? Marin?o.
2010. Using collocation segmentation to augment the
phrase table. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, WMT ?10, pages 98?102, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT/EMNLP,
HLT ?05, pages 89?96, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Jason Katz-Brown, Slav Petrov, Ryan McDonald, Franz
Och, David Talbot, Hiroshi Ichikawa, Masakazu Seno,
and Hideto Kazawa. 2011. Training a parser for
machine translation reordering. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 183?192,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
323
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1?11, Stroudsburg, PA,
USA. Association for Computational Linguistics.
S. Lin and B. W. Kernighan. 1973. An effective heuristic
algorithm for the travelling-salesman problem. Oper-
ations Research, pages 498?516.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, ACL-44, pages 609?616, Stroudsburg,
PA, USA. Association for Computational Linguistics.
J. Scott McCarley, Abraham Ittycheriah, Salim Roukos,
Bing Xiang, and Jian-ming Xu. 2011. A correc-
tion model for word alignments. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 889?898,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 843?853, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: addressing the crux
of the fluency problem in English-Hindi smt. In
Proceedings of ACL-IJCNLP.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for statistical machine translation. Computa-
tional Linguistics, 29(1):97?133.
Roy Tromble and Jason Eisner. 2009. Learning linear or-
dering problems for better translation. In Proceedings
of EMNLP.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with automat-
ically derived rules for improved statistical machine
translation. In Proceedings of the 23rd International
Conference on Computational Linguistics.
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for
improved machine translation. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 486?496,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of EMNLP-CoNLL.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned rewrite
patterns. In COLING.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical mt. In Proceedings of ACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation.
324
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1532?1541,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
All Words Domain Adapted WSD: Finding a Middle Ground between
Supervision and Unsupervision
Mitesh M. Khapra Anup Kulkarni Saurabh Sohoney Pushpak Bhattacharyya
Indian Institute of Technology Bombay,
Mumbai - 400076, India.
{miteshk,anup,saurabhsohoney,pb}@cse.iitb.ac.in
Abstract
In spite of decades of research on word
sense disambiguation (WSD), all-words
general purpose WSD has remained a dis-
tant goal. Many supervised WSD systems
have been built, but the effort of creat-
ing the training corpus - annotated sense
marked corpora - has always been a matter
of concern. Therefore, attempts have been
made to develop unsupervised and knowl-
edge based techniques for WSD which do
not need sense marked corpora. However
such approaches have not proved effective,
since they typically do not better Word-
net first sense baseline accuracy. Our re-
search reported here proposes to stick to
the supervised approach, but with far less
demand on annotation. We show that if
we have ANY sense marked corpora, be it
from mixed domain or a specific domain, a
small amount of annotation in ANY other
domain can deliver the goods almost as
if exhaustive sense marking were avail-
able in that domain. We have tested our
approach across Tourism and Health do-
main corpora, using also the well known
mixed domain SemCor corpus. Accuracy
figures close to self domain training lend
credence to the viability of our approach.
Our contribution thus lies in finding a con-
venient middle ground between pure su-
pervised and pure unsupervised WSD. Fi-
nally, our approach is not restricted to any
specific set of target words, a departure
from a commonly observed practice in do-
main specific WSD.
1 Introduction
Amongst annotation tasks, sense marking surely
takes the cake, demanding as it does high level
of language competence, topic comprehension and
domain sensitivity. This makes supervised ap-
proaches to WSD a difficult proposition (Agirre
et al, 2009b; Agirre et al, 2009a; McCarthy et
al., 2007). Unsupervised and knowledge based ap-
proaches have been tried with the hope of creating
WSD systems with no need for sense marked cor-
pora (Koeling et al, 2005; McCarthy et al, 2007;
Agirre et al, 2009b). However, the accuracy fig-
ures of such systems are low.
Our work here is motivated by the desire to de-
velop annotation-lean all-words domain adapted
techniques for supervised WSD. It is a common
observation that domain specific WSD exhibits
high level of accuracy even for the all-words sce-
nario (Khapra et al, 2010) - provided training and
testing are on the same domain. Also domain
adaptation - in which training happens in one do-
main and testing in another - often is able to attain
good levels of performance, albeit on a specific set
of target words (Chan and Ng, 2007; Agirre and
de Lacalle, 2009). To the best of our knowledge
there does not exist a system that solves the com-
bined problem of all words domain adapted WSD.
We thus propose the following:
a. For any target domain, create a small amount
of sense annotated corpus.
b. Mix it with an existing sense annotated cor-
pus ? from a mixed domain or specific do-
main ? to train the WSD engine.
This procedure tested on four adaptation scenar-
ios, viz., (i) SemCor (Miller et al, 1993) to
Tourism, (ii) SemCor to Health, (iii) Tourism to
Health and (iv) Health to Tourism has consistently
yielded good performance (to be explained in sec-
tions 6 and 7).
The remainder of this paper is organized as fol-
lows. In section 2 we discuss previous work in the
area of domain adaptation for WSD. In section 3
1532
we discuss three state of art supervised, unsuper-
vised and knowledge based algorithms for WSD.
Section 4 discusses the injection strategy for do-
main adaptation. In section 5 we describe the
dataset used for our experiments. We then present
the results in section 6 followed by discussions in
section 7. Section 8 examines whether there is any
need for intelligent choice of injections. Section
9 concludes the paper highlighting possible future
directions.
2 Related Work
Domain specific WSD for selected target words
has been attempted by Ng and Lee (1996), Agirre
and de Lacalle (2009), Chan and Ng (2007), Koel-
ing et al (2005) and Agirre et al (2009b). They
report results on three publicly available lexical
sample datasets, viz., DSO corpus (Ng and Lee,
1996), MEDLINE corpus (Weeber et al, 2001)
and the corpus made available by Koeling et al
(2005). Each of these datasets contains a handful
of target words (41-191 words) which are sense
marked in the corpus.
Our main inspiration comes from the target-
word specific results reported by Chan and Ng
(2007) and Agirre and de Lacalle (2009). The
former showed that adding just 30% of the target
data to the source data achieved the same perfor-
mance as that obtained by taking the entire source
and target data. Agirre and de Lacalle (2009) re-
ported a 22% error reduction when source and
target data were combined for training a classi-
fier, as compared to the case when only the target
data was used for training the classifier. However,
both these works focused on target word specific
WSD and do not address all-words domain spe-
cific WSD.
In the unsupervised setting, McCarthy et al
(2007) showed that their predominant sense acqui-
sition method gives good results on the corpus of
Koeling et al (2005). In particular, they showed
that the performance of their method is compa-
rable to the most frequent sense obtained from a
tagged corpus, thereby making a strong case for
unsupervised methods for domain-specific WSD.
More recently, Agirre et al (2009b) showed that
knowledge based approaches which rely only on
the semantic relations captured by the Wordnet
graph outperform supervised approaches when ap-
plied to specific domains. The good results ob-
tained by McCarthy et al (2007) and Agirre et
al. (2009b) for unsupervised and knowledge based
approaches respectively have cast a doubt on the
viability of supervised approaches which rely on
sense tagged corpora. However, these conclusions
were drawn only from the performance on certain
target words, leaving open the question of their
utility in all words WSD.
We believe our work contributes to the WSD
research in the following way: (i) it shows that
there is promise in supervised approach to all-
word WSD, through the instrument of domain
adaptation; (ii) it places in perspective some very
recently reported unsupervised and knowledge
based techniques of WSD; (ii) it answers some
questions arising out of the debate between super-
vision and unsupervision in WSD; and finally (iv)
it explores a convenient middle ground between
unsupervised and supervised WSD ? the territory
of ?annotate-little and inject? paradigm.
3 WSD algorithms employed by us
In this section we describe the knowledge based,
unsupervised and supervised approaches used for
our experiments.
3.1 Knowledge Based Approach
Agirre et al (2009b) showed that a graph based
algorithm which uses only the relations between
concepts in a Lexical Knowledge Base (LKB) can
outperform supervised approaches when tested on
specific domains (for a set of chosen target words).
We employ their method which involves the fol-
lowing steps:
1. Represent Wordnet as a graph where the con-
cepts (i.e., synsets) act as nodes and the re-
lations between concepts define edges in the
graph.
2. Apply a context-dependent Personalized
PageRank algorithm on this graph by intro-
ducing the context words as nodes into the
graph and linking them with their respective
synsets.
3. These nodes corresponding to the context
words then inject probability mass into the
synsets they are linked to, thereby influencing
the final relevance of all nodes in the graph.
We used the publicly available implementation
of this algorithm1 for our experiments.
1http://ixa2.si.ehu.es/ukb/
1533
3.2 Unsupervised Approach
McCarthy et al (2007) used an untagged corpus to
construct a thesaurus of related words. They then
found the predominant sense (i.e., the most fre-
quent sense) of each target word using pair-wise
Wordnet based similarity measures by pairing the
target word with its top-k neighbors in the the-
saurus. Each target word is then disambiguated
by assigning it its predominant sense ? the moti-
vation being that the predominant sense is a pow-
erful, hard-to-beat baseline. We implemented their
method using the following steps:
1. Obtain a domain-specific untagged corpus (we
crawled a corpus of approximately 9M words
from the web).
2. Extract grammatical relations from this text us-
ing a dependency parser2 (Klein and Manning,
2003).
3. Use the grammatical relations thus extracted to
construct features for identifying the k nearest
neighbors for each word using the distributional
similarity score described in (Lin, 1998).
4. Rank the senses of each target word in the test
set using a weighted sum of the distributional
similarity scores of the neighbors. The weights
in the sum are based on Wordnet Similarity
scores (Patwardhan and Pedersen, 2003).
5. Each target word in the test set is then disam-
biguated by simply assigning it its predominant
sense obtained using the above method.
3.3 Supervised approach
Khapra et al (2010) proposed a supervised algo-
rithm for domain-specific WSD and showed that it
beats the most frequent corpus sense and performs
on par with other state of the art algorithms like
PageRank. We implemented their iterative algo-
rithm which involves the following steps:
1. Tag all monosemous words in the sentence.
2. Iteratively disambiguate the remaining words in
the sentence in increasing order of their degree
of polysemy.
3. At each stage rank the candidate senses of
a word using the scoring function of Equa-
tion (1) which combines corpus based param-
eters (such as, sense distributions and corpus
co-occurrence) and Wordnet based parameters
2We used the Stanford parser - http://nlp.
stanford.edu/software/lex-parser.shtml
(such as, semantic similarity, conceptual dis-
tance, etc.)
S? = arg max
i
(?iVi +
?
j?J
Wij ? Vi ? Vj)
(1)
where,
i ? Candidate Synsets
J = Set of disambiguated words
?i = BelongingnessToDominantConcept(Si)
Vi = P (Si|word)
Wij = CorpusCooccurrence(Si, Sj)
? 1/WNConceptualDistance(Si, Sj)
? 1/WNSemanticGraphDistance(Si, Sj)
4. Select the candidate synset with maximizes the
above score as the winner sense.
4 Injections for Supervised Adaptation
This section describes the main interest of our
work i.e. adaptation using injections. For su-
pervised adaptation, we use the supervised algo-
rithm described above (Khapra et al, 2010) in the
following 3 settings as proposed by Agirre et al
(2009a):
a. Source setting: We train the algorithm on a
mixed-domain corpus (SemCor) or a domain-
specific corpus (say, Tourism) and test it on a
different domain (say, Health). A good perfor-
mance in this setting would indicate robustness
to domain-shifts.
b. Target setting: We train and test the algorithm
using data from the same domain. This gives the
skyline performance, i.e., the best performance
that can be achieved if sense marked data from
the target domain were available.
c. Adaptation setting: This setting is the main fo-
cus of interest in the paper. We augment the
training data which could be from one domain
or mixed domain with a small amount of data
from the target domain. This combined data is
then used for training. The aim here is to reach
as close to the skyline performance using as lit-
tle data as possible. For injecting data from the
target domain we randomly select some sense
marked words from the target domain and add
1534
Polysemous words Monosemous words
Category Tourism Health Tourism Health
Noun 53133 15437 23665 6979
Verb 15528 7348 1027 356
Adjective 19732 5877 10569 2378
Adverb 6091 1977 4323 1694
All 94484 30639 39611 11407
Avg. no. of instances perpolysemous word
Category Health Tourism SemCor
Noun 7.06 12.56 10.98
Verb 7.47 9.76 11.95
Adjective 5.74 12.07 8.67
Adverb 9.11 19.78 25.44
All 6.94 12.17 11.25
Table 1: Polysemous and Monosemous words per
category in each domain
Table 2: Average number of instances per polyse-
mous word per category in the 3 domains
Avg. degree of Wordnet polysemy
for polysemous words
Category Health Tourism SemCor
Noun 5.24 4.95 5.60
Verb 10.60 10.10 9.89
Adjective 5.52 5.08 5.40
Adverb 3.64 4.16 3.90
All 6.49 5.77 6.43
Avg. degree of Corpus polysemy
for polysemous words
Category Health Tourism SemCor
Noun 1.92 2.60 3.41
Verb 3.41 4.55 4.73
Adjective 2.04 2.57 2.65
Adverb 2.16 2.82 3.09
All 2.31 2.93 3.56
Table 3: Average degree of Wordnet polysemy of
polysemous words per category in the 3 domains
Table 4: Average degree of Corpus polysemy of
polysemous words per category in the 3 domains
them to the training data. An obvious ques-
tion which arises at this point is ?Why were the
words selected at random?? or ?Can selection
of words using some active learning strategy
yield better results than a random selection??
We discuss this question in detail in Section 7
and show that a random set of injections per-
forms no worse than a craftily selected set of
injections.
5 DataSet Preparation
Due to the lack of any publicly available all-words
domain specific sense marked corpora we set upon
the task of collecting data from two domains, viz.,
Tourism and Health. The data for Tourism do-
main was downloaded from Indian Tourism web-
sites whereas the data for Health domain was ob-
tained from two doctors. This data was manu-
ally sense annotated by two lexicographers adept
in English. Princeton Wordnet 2.13 (Fellbaum,
1998) was used as the sense inventory. A total
of 1,34,095 words from the Tourism domain and
42,046 words from the Health domain were man-
ually sense marked. Some files were sense marked
by both the lexicographers and the Inter Tagger
Agreement (ITA) calculated from these files was
83% which is comparable to the 78% ITA reported
on the SemCor corpus considering the domain-
specific nature of the corpus.
We now present different statistics about the
corpora. Table 1 summarizes the number of poly-
semous and monosemous words in each category.
3http://wordnetweb.princeton.edu/perl/webwn
Note that we do not use the monosemous words
while calculating precision and recall of our algo-
rithms.
Table 2 shows the average number of instances
per polysemous word in the 3 corpora. We note
that the number of instances per word in the
Tourism domain is comparable to that in the Sem-
Cor corpus whereas the number of instances per
word in the Health corpus is smaller due to the
overall smaller size of the Health corpus.
Tables 3 and 4 summarize the average degree
of Wordnet polysemy and corpus polysemy of the
polysemous words in the corpus. Wordnet poly-
semy is the number of senses of a word as listed
in the Wordnet, whereas corpus polysemy is the
number of senses of a word actually appearing in
the corpus. As expected, the average degree of
corpus polysemy (Table 4) is much less than the
average degree of Wordnet polysemy (Table 3).
Further, the average degree of corpus polysemy
(Table 4) in the two domains is less than that in the
mixed-domain SemCor corpus, which is expected
due to the domain specific nature of the corpora.
Finally, Table 5 summarizes the number of unique
polysemous words per category in each domain.
No. of unique polysemous words
Category Health Tourism SemCor
Noun 2188 4229 5871
Verb 984 1591 2565
Adjective 1024 1635 2640
Adverb 217 308 463
All 4413 7763 11539
Table 5: Number of unique polysemous words per category
in each domain.
1535
The data is currently being enhanced by manu-
ally sense marking more words from each domain
and will be soon freely available4 for research pur-
poses.
6 Results
We tested the 3 algorithms described in section 4
using SemCor, Tourism and Health domain cor-
pora. We did a 2-fold cross validation for su-
pervised adaptation and report the average perfor-
mance over the two folds. Since the knowledge
based and unsupervised methods do not need any
training data we simply test it on the entire corpus
from the two domains.
6.1 Knowledge Based approach
The results obtained by applying the Personalized
PageRank (PPR) method to Tourism and Health
data are summarized in Table 6. We also report
the Wordnet first sense baseline (WFS).
Domain Algorithm P(%) R(%) F(%)
Tourism PPR 53.1 53.1 53.1
WFS 62.5 62.5 62.5
Health PPR 51.1 51.1 51.1
WFS 65.5 65.5 65.5
Table 6: Comparing the performance of Person-
alized PageRank (PPR) with Wordnet First Sense
Baseline (WFS)
6.2 Unsupervised approach
The predominant sense for each word in the two
domains was calculated using the method de-
scribed in section 4.2. McCarthy et al (2004)
reported that the best results were obtained us-
ing k = 50 neighbors and the Wordnet Similar-
ity jcn measure (Jiang and Conrath, 1997). Fol-
lowing them, we used k = 50 and observed that
the best results for nouns and verbs were obtained
using the jcn measure and the best results for ad-
jectives and adverbs were obtained using the lesk
measure (Banerjee and Pedersen, 2002). Accord-
ingly, we used jcn for nouns and verbs and lesk
for adjectives and adverbs. Each target word in
the test set is then disambiguated by simply as-
signing it its predominant sense obtained using
the above method. We tested this approach only
on Tourism domain due to unavailability of large
4http://www.cfilt.iitb.ac.in/wsd/annotated corpus
untagged Health corpus which is needed for con-
structing the thesaurus. The results are summa-
rized in Table 7.
Domain Algorithm P(%) R(%) F(%)
Tourism McCarthy 51.85 49.32 50.55
WFS 62.50 62.50 62.50
Table 7: Comparing the performance of unsuper-
vised approach with Wordnet First Sense Baseline
(WFS)
6.3 Supervised adaptation
We report results in the source setting, target set-
ting and adaptation setting as described earlier
using the following four combinations for source
and target data:
1. SemCor to Tourism (SC?T) where SemCor is
used as the source domain and Tourism as the
target (test) domain.
2. SemCor to Health (SC?H) where SemCor is
used as the source domain and Health as the tar-
get (test) domain.
3. Tourism to Health (T?H) where Tourism is
used as the source domain and Health as the tar-
get (test) domain.
4. Health to Tourism (H?T) where Health is
used as the source domain and Tourism as the
target (test) domain.
In each case, the target domain data was divided
into two folds. One fold was set aside for testing
and the other for injecting data in the adaptation
setting. We increased the size of the injected target
examples from 1000 to 14000 words in increments
of 1000. We then repeated the same experiment by
reversing the role of the two folds.
Figures 1, 2, 3 and 4 show the graphs of the av-
erage F-score over the 2-folds for SC?T, SC?H,
T?H and H?T respectively. The x-axis repre-
sents the amount of training data (in words) in-
jected from the target domain and the y-axis rep-
resents the F-score. The different curves in each
graph are as follows:
a. only random : This curve plots the perfor-
mance obtained using x randomly selected
sense tagged words from the target domain and
zero sense tagged words from the source do-
main (x was varied from 1000 to 14000 words
in increments of 1000).
1536
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 0  2000  4000  6000  8000  10000  12000  14000
F-
sc
or
e 
(%
)
Injection Size (words)
Injection Size v/s F-score
wfs
srcb
tsky
only_random
random+semcor
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 0  2000  4000  6000  8000  10000  12000  14000
F-
sc
or
e 
(%
)
Injection Size (words)
Injection Size v/s F-score
wfs
srcb
tsky
only_random
random+semcor
Figure 1: Supervised adaptation from
SemCor to Tourism using injections
Figure 2: Supervised adaptation from
SemCor to Health using injections
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 0  2000  4000  6000  8000  10000  12000  14000
F-
sc
or
e 
(%
)
Injection Size (words)
Injection Size v/s F-score
wfs
srcb
tsky
only_random
random+tourism
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 0  2000  4000  6000  8000  10000  12000  14000
F-
sc
or
e 
(%
)
Injection Size (words)
Injection Size v/s F-score
wfs
srcb
tsky
only_random
random+health
Figure 3: Supervised adaptation from
Tourism to Health using injections
Figure 4: Supervised adaptation from
Health to Tourism using injections
b. random+source : This curve plots the perfor-
mance obtained by mixing x randomly selected
sense tagged words from the target domain with
the entire training data from the source domain
(again x was varied from 1000 to 14000 words
in increments of 1000).
c. source baseline (srcb) : This represents the F-
score obtained by training on the source data
alone without mixing any examples from the
target domain.
d. wordnet first sense (wfs) : This represents the
F-score obtained by selecting the first sense
from Wordnet, a typically reported baseline.
e. target skyline (tsky) : This represents the av-
erage 2-fold F-score obtained by training on
one entire fold of the target data itself (Health:
15320 polysemous words; Tourism: 47242 pol-
ysemous words) and testing on the other fold.
These graphs along with other results are dis-
cussed in the next section.
7 Discussions
We discuss the performance of the three ap-
proaches.
7.1 Knowledge Based and Unsupervised
approaches
It is apparent from Tables 6 and 7 that knowl-
edge based and unsupervised approaches do not
perform well when compared to the Wordnet first
sense (which is freely available and hence can be
used for disambiguation). Further, we observe that
the performance of these approaches is even less
than the source baseline (i.e., the case when train-
ing data from a source domain is applied as it is
to a target domain - without using any injections).
These observations bring out the weaknesses of
these approaches when used in an all-words set-
ting and clearly indicate that they come nowhere
close to replacing a supervised system.
1537
7.2 Supervised adaptation
1. The F-score obtained by training on SemCor
(mixed-domain corpus) and testing on the two
target domains without using any injections
(srcb) ? F-score of 61.7% on Tourism and F-
score of 65.5% on Health ? is comparable to the
best result reported on the SEMEVAL datasets
(65.02%, where both training and testing hap-
pens on a mixed-domain corpus (Snyder and
Palmer, 2004)). This is in contrast to previ-
ous studies (Escudero et al, 2000; Agirre and
Martinez, 2004) which suggest that instead of
adapting from a generic/mixed domain to a spe-
cific domain, it is better to completely ignore
the generic examples and use hand-tagged data
from the target domain itself. The main rea-
son for the contrasting results is that the ear-
lier work focused only on a handful of target
words whereas we focus on all words appearing
in the corpus. So, while the behavior of a few
target words would change drastically when the
domain changes, a majority of the words will
exhibit the same behavior (i.e., same predomi-
nant sense) even when the domain changes. We
agree that the overall performance is still lower
than that obtained by training on the domain-
specific corpora. However, it is still better than
the performance of unsupervised and knowl-
edge based approaches which tilts the scale in
favor of supervised approaches even when only
mixed domain sense marked corpora is avail-
able.
2. Adding injections from the target domain im-
proves the performance. As the amount of in-
jection increases the performance approaches
the skyline, and in the case of SC?H and T?H
it even crosses the skyline performance showing
that combining the source and target data can
give better performance than using the target
data alone. This is consistent with the domain
adaptation results reported by Agirre and de La-
calle (2009) on a specific set of target words.
3. The performance of random+source is always
better than only random indicating that the data
from the source domain does help to improve
performance. A detailed analysis showed that
the gain obtained by using the source data is at-
tributable to reducing recall errors by increasing
the coverage of seen words.
4. Adapting from one specific domain (Tourism or
Health) to another specific domain (Health or
Tourism) gives the same performance as that ob-
tained by adapting from a mixed-domain (Sem-
Cor) to a specific domain (Tourism, Health).
This is an interesting observation as it suggests
that as long as data from one domain is avail-
able it is easy to build a WSD engine that works
for other domains by injecting a small amount
of data from these domains.
To verify that the results are consistent, we ran-
domly selected 5 different sets of injections from
fold-1 and tested the performance on fold-2. We
then repeated the same experiment by reversing
the roles of the two folds. The results were in-
deed consistent irrespective of the set of injections
used. Due to lack of space we have not included
the results for these 5 different sets of injections.
7.3 Quantifying the trade-off between
performance and corpus size
To correctly quantify the benefit of adding injec-
tions from the target domain, we calculated the
amount of target data (peak size) that is needed
to reach the skyline F-score (peak F) in the ab-
sence of any data from the source domain. The
peak size was found to be 35000 (Tourism) and
14000 (Health) corresponding to peak F values of
74.2% (Tourism) and 73.4% (Health). We then
plotted a graph (Figure 5) to capture the rela-
tion between the size of injections (expressed as
a percentage of the peak size) and the F-score (ex-
pressed as a percentage of the peak F).
 80
 85
 90
 95
 100
 105
 0  20  40  60  80  100
%
 p
ea
k_
F
% peak_size
Size v/s Performance
SC --> H
T --> H
SC --> T
H --> T
Figure 5: Trade-off between performance
and corpus size
We observe that by mixing only 20-40% of the
peak size with the source domain we can obtain up
to 95% of the performance obtained by using the
1538
entire target data (peak size). In absolute terms,
the size of the injections is only 7000-9000 poly-
semous words which is a very small price to pay
considering the performance benefits.
8 Does the choice of injections matter?
An obvious question which arises at this point is
?Why were the words selected at random?? or
?Can selection of words using some active learn-
ing strategy yield better results than a random
selection?? An answer to this question requires
a more thorough understanding of the sense-
behavior exhibited by words across domains. In
any scenario involving a shift from domain D1 to
domain D2, we will always encounter words be-
longing to the following 4 categories:
a. WD1 : This class includes words which are en-
countered only in the source domain D1 and do
not appear in the target domain D2. Since we
are interested in adapting to the target domain
and since these words do not appear in the tar-
get domain, it is quite obvious that they are not
important for the problem of domain adapta-
tion.
b. WD2 : This class includes words which are en-
countered only in the target domain D2 and do
not appear in the source domain D1. Again, it
is quite obvious that these words are important
for the problem of domain adaptation. They fall
in the category of unseen words and need han-
dling from that point of view.
c. WD1D2conformists : This class includes words
which are encountered in both the domains and
exhibit the same predominant sense in both the
domains. Correct identification of these words
is important so that we can use the predomi-
nant sense learned from D1 for disambiguating
instances of these words appearing in D2.
d. WD1D2non?conformists : This class includes
words which are encountered in both the do-
mains but their predominant sense in the tar-
get domain D2 does not conform to the pre-
dominant sense learned from the source domain
D1. Correct identification of these words is im-
portant so that we can ignore the predominant
senses learned from D1 while disambiguating
instances of these words appearing in D2.
Table 8 summarizes the percentage of words that
fall in each category in each of the three adapta-
tion scenarios. The fact that nearly 50-60% of the
words fall in the ?conformist? category once again
makes a strong case for reusing sense tagged data
from one domain to another domain.
Category SC?T SC?H T?H
WD2 7.14% 5.45% 13.61%
Conformists 49.54% 60.43% 54.31%
Non-Conformists 43.30% 34.11% 32.06%
Table 8: Percentage of Words belonging to each
category in the three settings.
The above characterization suggests that an ideal
domain adaptation strategy should focus on in-
jecting WD2 and WD1D2non?conformists as these
would yield maximum benefits if injected into the
training data. While it is easy to identify the
WD2 words, ?identifying non-conformists? is a
hard problem which itself requires some type of
WSD5. However, just to prove that a random in-
jection strategy does as good as an ideal strategy
we assume the presence of an oracle which iden-
tifies the WD1D2non?conformists. We then augment
the training data with 5-8 instances for WD2 and
WD1D2non?conformists words thus identified. We
observed that adding more than 5-8 instances per
word does not improve the performance. This is
due to the ?one sense per domain? phenomenon ?
seeing only a few instances of a word is sufficient
to identify the predominant sense of the word. Fur-
ther, to ensure a better overall performance, the
instances of the most frequent words are injected
first followed by less frequent words till we ex-
haust the total size of the injections (1000, 2000
and so on). We observed that there was a 75-
80% overlap between the words selected by ran-
dom strategy and oracle strategy. This is because
oracle selects the most frequent words which also
have a high chance of getting selected when a ran-
dom sampling is done.
Figures 6, 7, 8 and 9 compare the performance
of the two strategies. We see that the random strat-
egy does as well as the oracle strategy thereby sup-
porting our claim that if we have sense marked
corpus from one domain then simply injecting ANY
small amount of data from the target domain will
5Note that the unsupervised predominant sense acquisi-
tion method of McCarthy et al (2007) implicitly identifies
conformists and non-conformists
1539
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 0  2000  4000  6000  8000  10000  12000  14000
F-
sc
or
e 
(%
)
Injection Size (words)
Injection Size v/s F-score
wfs
srcb
tsky
random+semcor
oracle+semcor
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 0  2000  4000  6000  8000  10000  12000  14000
F-
sc
or
e 
(%
)
Injection Size (words)
Injection Size v/s F-score
wfs
srcb
tsky
random+semcor
oracle+semcor
Figure 6: Comparing random strategy
with oracle based ideal strategy for Sem-
Cor to Tourism adaptation
Figure 7: Comparing random strategy
with oracle based ideal strategy for Sem-
Cor to Health adaptation
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 0  2000  4000  6000  8000  10000  12000  14000
F-
sc
or
e 
(%
)
Injection Size (words)
Injection Size v/s F-score
wfs
srcb
tsky
random+tourism
oracle+tourism
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 0  2000  4000  6000  8000  10000  12000  14000
F-
sc
or
e 
(%
)
Injection Size (words)
Injection Size v/s F-score
wfs
srcb
tsky
random+health
oracle+health
Figure 8: Comparing random strat-
egy with oracle based ideal strategy for
Tourism to Health adaptation
Figure 9: Comparing random strat-
egy with oracle based ideal strategy for
Health to Tourism adaptation
do the job.
9 Conclusion and Future Work
Based on our study of WSD in 4 domain adap-
tation scenarios, we make the following conclu-
sions:
1. Supervised adaptation by mixing small amount
of data (7000-9000 words) from the target do-
main with the source domain gives nearly the
same performance (F-score of around 70% in
all the 4 adaptation scenarios) as that obtained
by training on the entire target domain data.
2. Unsupervised and knowledge based approaches
which use distributional similarity and Word-
net based similarity measures do not compare
well with the Wordnet first sense baseline per-
formance and do not come anywhere close to
the performance of supervised adaptation.
3. Supervised adaptation from a mixed domain to
a specific domain gives the same performance
as that from one specific domain (Tourism) to
another specific domain (Health).
4. Supervised adaptation is not sensitive to the
type of data being injected. This is an interest-
ing finding with the following implication: as
long as one has sense marked corpus - be it from
a mixed or specific domain - simply injecting
ANY small amount of data from the target do-
main suffices to beget good accuracy.
As future work, we would like to test our work on
the Environment domain data which was released
as part of the SEMEVAL 2010 shared task on ?All-
words Word Sense Disambiguation on a Specific
Domain?.
1540
References
Eneko Agirre and Oier Lopez de Lacalle. 2009. Su-
pervised domain adaption for wsd. In EACL ?09:
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 42?50, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Eneko Agirre and David Martinez. 2004. The effect of
bias on an automatically-built word sense corpus. In
Proceedings of the 4rd International Conference on
Languages Resources and Evaluations (LREC).
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Andrea Marchetti, Antonio Toral, and Piek
Vossen. 2009a. Semeval-2010 task 17: all-words
word sense disambiguation on a specific domain. In
DEW ?09: Proceedings of the Workshop on Seman-
tic Evaluations: Recent Achievements and Future
Directions, pages 123?128, Morristown, NJ, USA.
Association for Computational Linguistics.
Eneko Agirre, Oier Lopez De Lacalle, and Aitor Soroa.
2009b. Knowledge-based wsd on specific domains:
Performing better than generic supervised wsd. In
In Proceedings of IJCAI.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In CICLing ?02: Proceedings
of the Third International Conference on Compu-
tational Linguistics and Intelligent Text Processing,
pages 136?145, London, UK. Springer-Verlag.
Yee Seng Chan and Hwee Tou Ng. 2007. Do-
main adaptation with active learning for word sense
disambiguation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 49?56, Prague, Czech Republic,
June. Association for Computational Linguistics.
Gerard Escudero, Llu??s Ma`rquez, and German Rigau.
2000. An empirical study of the domain depen-
dence of supervised word sense disambiguation sys-
tems. In Proceedings of the 2000 Joint SIGDAT con-
ference on Empirical methods in natural language
processing and very large corpora, pages 172?180,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database.
J.J. Jiang and D.W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proc. of the Int?l. Conf. on Research in Computa-
tional Linguistics, pages 19?33.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Push-
pak Bhattacharyya. 2010. Domain-specific word
sense disambiguation combining corpus based and
wordnet based parameters. In 5th International
Conference on Global Wordnet (GWC2010).
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In IN PROCEEDINGS
OF THE 41ST ANNUAL MEETING OF THE ASSO-
CIATION FOR COMPUTATIONAL LINGUISTICS,
pages 423?430.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In HLT ?05: Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 419?426, Morristown, NJ, USA.
Association for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 17th
international conference on Computational linguis-
tics, pages 768?774, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In ACL ?04: Proceedings of the 42nd
Annual Meeting on Association for Computational
Linguistics, page 279, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of predom-
inant word senses. Comput. Linguist., 33(4):553?
590.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
HLT ?93: Proceedings of the workshop on Human
Language Technology, pages 303?308, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
sense: an exemplar-based approach. In Proceedings
of the 34th annual meeting on Association for Com-
putational Linguistics, pages 40?47, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Siddharth Patwardhan and Ted Pedersen. 2003.
The cpan wordnet::similarity package. http://search
.cpan.org/ sid/wordnet-similarity/.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Rada Mihalcea and Phil
Edmonds, editors, Senseval-3: Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, pages 41?43, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
Marc Weeber, James G. Mork, and Alan R. Aronson.
2001. Developing a test collection for biomedical
word sense disambiguation. In In Proceedings of
the AMAI Symposium, pages 746?750.
1541
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 561?569,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Together We Can: Bilingual Bootstrapping for WSD
Mitesh M. Khapra Salil Joshi Arindam Chatterjee Pushpak Bhattacharyya
Department Of Computer Science and Engineering,
IIT Bombay,
Powai,
Mumbai, 400076.
{miteshk,salilj,arindam,pb}@cse.iitb.ac.in
Abstract
Recent work on bilingual Word Sense Disam-
biguation (WSD) has shown that a resource
deprived language (L1) can benefit from the
annotation work done in a resource rich lan-
guage (L2) via parameter projection. How-
ever, this method assumes the presence of suf-
ficient annotated data in one resource rich lan-
guage which may not always be possible. In-
stead, we focus on the situation where there
are two resource deprived languages, both
having a very small amount of seed annotated
data and a large amount of untagged data. We
then use bilingual bootstrapping, wherein, a
model trained using the seed annotated data
of L1 is used to annotate the untagged data of
L2 and vice versa using parameter projection.
The untagged instances of L1 and L2 which
get annotated with high confidence are then
added to the seed data of the respective lan-
guages and the above process is repeated. Our
experiments show that such a bilingual boot-
strapping algorithm when evaluated on two
different domains with small seed sizes using
Hindi (L1) and Marathi (L2) as the language
pair performs better than monolingual boot-
strapping and significantly reduces annotation
cost.
1 Introduction
The high cost of collecting sense annotated data for
supervised approaches (Ng and Lee, 1996; Lee et
al., 2004) has always remained a matter of concern
for some of the resource deprived languages of the
world. The problem is even more hard-hitting for
multilingual regions (e.g., India which has more than
20 constitutionally recognized languages). To cir-
cumvent this problem, unsupervised and knowledge
based approaches (Lesk, 1986; Walker and Amsler,
1986; Agirre and Rigau, 1996; McCarthy et al,
2004; Mihalcea, 2005) have been proposed as an al-
ternative but they have failed to deliver good accura-
cies. Semi-supervised approaches (Yarowsky, 1995)
which use a small amount of annotated data and a
large amount of untagged data have shown promise
albeit for a limited set of target words. The above
situation highlights the need for high accuracy re-
source conscious approaches to all-words multilin-
gual WSD.
Recent work by Khapra et al (2010) in this di-
rection has shown that it is possible to perform cost
effective WSD in a target language (L2) without
compromising much on accuracy by leveraging on
the annotation work done in another language (L1).
This is achieved with the help of a novel synset-
aligned multilingual dictionary which facilitates the
projection of parameters learned from the Wordnet
and annotated corpus of L1 to L2. This approach
thus obviates the need for collecting large amounts
of annotated corpora in multiple languages by rely-
ing on sufficient annotated corpus in one resource
rich language. However, in many situations such a
pivot resource rich language itself may not be avail-
able. Instead, we might have two or more languages
having a small amount of annotated corpus and a
large amount of untagged corpus. Addressing such
situations is the main focus of this work. Specifi-
cally, we address the following question:
In the absence of a pivot resource rich lan-
guage is it possible for two resource de-
prived languages to mutually benefit from
each other?s annotated data?
While addressing the above question we assume that
561
even though it is hard to obtain large amounts of
annotated data in multiple languages, it should be
fairly easy to obtain a large amount of untagged data
in these languages. We leverage on such untagged
data by employing a bootstrapping strategy. The
idea is to train an initial model using a small amount
of annotated data in both the languages and itera-
tively expand this seed data by including untagged
instances which get tagged with a high confidence
in successive iterations. Instead of using monolin-
gual bootstrapping, we use bilingual bootstrapping
via parameter projection. In other words, the pa-
rameters learned from the annotated data of L1 (and
L2 respectively) are projected to L2 (and L1 respec-
tively) and the projected model is used to tag the un-
tagged instances of L2 (and L1 respectively).
Such a bilingual bootstrapping strategy when
tested on two domains, viz., Tourism and Health us-
ing Hindi (L1) and Marathi (L2) as the language
pair, consistently does better than a baseline strat-
egy which uses only seed data for training without
performing any bootstrapping. Further, it consis-
tently performs better than monolingual bootstrap-
ping. A simple and intuitive explanation for this is
as follows. In monolingual bootstrapping a language
can benefit only from its own seed data and hence
can tag only those instances with high confidence
which it has already seen. On the other hand, in
bilingual bootstrapping a language can benefit from
the seed data available in the other language which
was not previously seen in its self corpus. This is
very similar to the process of co-training (Blum and
Mitchell, 1998) wherein the annotated data in the
two languages can be seen as two different views of
the same data. Hence, the classifier trained on one
view can be improved by adding those untagged in-
stances which are tagged with a high confidence by
the classifier trained on the other view.
The remainder of this paper is organized as fol-
lows. In section 2 we present related work. Section
3 describes the Synset algned multilingual dictio-
nary which facilitates parameter projection. Section
4 discusses the work of Khapra et al (2009) on pa-
rameter projection. In section 5 we discuss bilin-
gual bootstrapping which is the main focus of our
work followed by a brief discussion on monolingual
bootstrapping. Section 6 describes the experimental
setup. In section 7 we present the results followed
by discussion in section 8. Section 9 concludes the
paper.
2 Related Work
Bootstrapping for Word Sense Disambiguation was
first discussed in (Yarowsky, 1995). Starting with a
very small number of seed collocations an initial de-
cision list is created. This decisions list is then ap-
plied to untagged data and the instances which get
tagged with a high confidence are added to the seed
data. This algorithm thus proceeds iteratively in-
creasing the seed size in successive iterations. This
monolingual bootstrapping method showed promise
when tested on a limited set of target words but was
not tried for all-words WSD.
The failure of monolingual approaches (Ng and
Lee, 1996; Lee et al, 2004; Lesk, 1986; Walker and
Amsler, 1986; Agirre and Rigau, 1996; McCarthy
et al, 2004; Mihalcea, 2005) to deliver high accura-
cies for all-words WSD at low costs created interest
in bilingual approaches which aim at reducing the
annotation effort. Recent work in this direction by
Khapra et al (2009) aims at reducing the annotation
effort in multiple languages by leveraging on exist-
ing resources in a pivot language. They showed that
it is possible to project the parameters learned from
the annotation work of one language to another lan-
guage provided aligned Wordnets for the two lan-
guages are available. However, they do not address
situations where two resource deprived languages
have aligned Wordnets but neither has sufficient an-
notated data. In such cases bilingual bootstrapping
can be used so that the two languages can mutually
benefit from each other?s small annotated data.
Li and Li (2004) proposed a bilingual bootstrap-
ping approach for the more specific task of Word
Translation Disambiguation (WTD) as opposed to
the more general task of WSD. This approach does
not need parallel corpora (just like our approach)
and relies only on in-domain corpora from two lan-
guages. However, their work was evaluated only on
a handful of target words (9 nouns) for WTD as op-
posed to the broader task of WSD. Our work instead
focuses on improving the performance of all words
WSD for two resource deprived languages using
bilingual bootstrapping. At the heart of our work lies
parameter projection facilitated by a synset algned
562
multilingual dictionary described in the next section.
3 Synset Aligned Multilingual Dictionary
A novel and effective method of storage and use of
dictionary in a multilingual setting was proposed by
Mohanty et al (2008). For the purpose of current
discussion, we will refer to this multilingual dictio-
nary framework as MultiDict. One important de-
parture in this framework from the traditional dic-
tionary is that synsets are linked, and after that
the words inside the synsets are linked. The ba-
sic mapping is thus between synsets and thereafter
between the words.
Concepts L1
(English)
L2
(Hindi)
L3
(Marathi)
04321:
a youth-
ful male
person
{male
child,
boy}
{lwkA
(ladkaa),
bAlk
(baalak),
bQcA
(bachchaa)}
{m  lgA
(mulgaa),
porgA
(porgaa),
por (por)}
Table 1: Multilingual Dictionary Framework
Table 1 shows the structure of MultiDict, with one
example row standing for the concept of boy. The
first column is the pivot describing a concept with a
unique ID. The subsequent columns show the words
expressing the concept in respective languages (in
the example table, English, Hindi and Marathi). Af-
ter the synsets are linked, cross linkages are set up
manually from the words of a synset to the words
of a linked synset of the pivot language. For exam-
ple, for the Marathi word m  lgA (mulgaa), ?a youth-
ful male person?, the correct lexical substitute from
the corresponding Hindi synset is lwkA (ladkaa).
The average number of such links per synset per lan-
guage pair is approximately 3. However, since our
work takes place in a semi-supervised setting, we
do not assume the presence of these manual cross
linkages between synset members. Instead, in the
above example, we assume that all the words in
the Hindi synset are equally probable translations
of every word in the corresponding Marathi synset.
Such cross-linkages between synset members facil-
itate parameter projection as explained in the next
section.
4 Parameter Projection
Khapra et al (2009) proposed that the various
parameters essential for domain-specific Word
Sense Disambiguation can be broadly classified into
two categories:
Wordnet-dependent parameters:
? belongingness-to-dominant-concept
? conceptual distance
? semantic distance
Corpus-dependent parameters:
? sense distributions
? corpus co-occurrence
They proposed a scoring function (Equation (1))
which combines these parameters to identify the cor-
rect sense of a word in a context:
S? = argmax
i
(?iVi +
?
j?J
Wij ? Vi ? Vj) (1)
where,
i ? Candidate Synsets
J = Set of disambiguated words
?i = BelongingnessToDominantConcept(Si)
Vi = P (Si|word)
Wij = CorpusCooccurrence(Si, Sj)
? 1/WNConceptualDistance(Si, Sj)
? 1/WNSemanticGraphDistance(Si, Sj)
The first component ?iVi of Equation (1) captures
influence of the corpus specific sense of a word in a
domain. The other component Wij ?Vi ?Vj captures
the influence of interaction of the candidate sense
with the senses of context words weighted by factors
of co-occurrence, conceptual distance and semantic
distance.
Wordnet-dependent parameters depend on the
structure of the Wordnet whereas the Corpus-
dependent parameters depend on various statistics
learned from a sense marked corpora. Both the
tasks of (a) constructing a Wordnet from scratch and
(b) collecting sense marked corpora for multiple
languages are tedious and expensive. Khapra et
563
al. (2009) observed that by projecting relations
from the Wordnet of a language and by projecting
corpus statistics from the sense marked corpora
of the language to those of the target language,
the effort required in constructing semantic graphs
for multiple Wordnets and collecting sense marked
corpora for multiple languages can be avoided
or reduced. At the heart of their work lies the
MultiDict described in previous section which
facilitates parameter projection in the following
manner:
1. By linking with the synsets of a pivot resource
rich language (Hindi, in our case), the cost of build-
ing Wordnets of other languages is partly reduced
(semantic relations are inherited). The Wordnet pa-
rameters of Hindi Wordnet now become projectable
to other languages.
2. For calculating corpus specific sense distribu-
tions, P (Sense Si|Word W ), we need the counts,
#(Si,W ). By using cross linked words in the
synsets, these counts become projectable to the tar-
get language (Marathi, in our case) as they can be
approximated by the counts of the cross linked Hindi
words calculated from the Hindi sense marked cor-
pus as follows:
P (Si|W ) =
#(Si,marathi word)
?
j #(Sj ,marathi word)
P (Si|W ) ?
#(Si, cross linked hindi word)
?
j #(Sj , cross linked hindi word)
The rationale behind the above approximation is the
observation that within a domain the counts of cross-
linked words will remain the same across languages.
This parameter projection strategy as explained
above lies at the heart of our work and allows us
to perform bilingual bootstrapping by projecting the
models learned from one language to another.
5 Bilingual Bootstrapping
We now come to the main contribution of our work,
i.e., bilingual bootstrapping. As shown in Algorithm
1, we start with a small amount of seed data (LD1
and LD2) in the two languages. Using this data we
learn the parameters described in the previous sec-
tion. We collectively refer to the parameters learned
Algorithm 1 Bilingual Bootstrapping
LD1 := Seed Labeled Data from L1
LD2 := Seed Labeled Data from L2
UD1 := Unlabeled Data from L1
UD2 := Unlabeled Data from L2
repeat
?1 := model trained using LD1
?2 := model trained using LD2
{Project models from L1/L2 to L2/L1}
??2 := project(?1, L2)
??1 := project(?2, L1)
for all u1 ? UD1 do
s := sense assigned by ??1 to u1
if confidence(s) >  then
LD1 := LD1 + u1
UD1 := UD1 - u1
end if
end for
for all u2 ? UD2 do
s := sense assigned by ??2 to u2
if confidence(s) >  then
LD2 := LD2 + u2
UD2 := UD2 - u2
end if
end for
until convergence
from the seed data as models ?1 and ?2 for L1 and L2
respectively. The parameter projection strategy de-
scribed in the previous section is then applied to ?1
and ?2 to obtain the projected models ??2 and ??1 re-
spectively. These projected models are then applied
to the untagged data of L1 and L2 and the instances
which get labeled with a high confidence are added
to the labeled data of the respective languages. This
process is repeated till we reach convergence, i.e.,
till it is no longer possible to move any data from
UD1 (and UD2) to LD1 (and LD2 respectively).
We compare our algorithm with monolingual
bootstrapping where the self models ?1 and ?2 are
directly used to annotate the unlabeled instances in
L1 and L2 respectively instead of using the projected
models ??1 and ??2. The process of monolingual boot-
564
Algorithm 2 Monolingual Bootstrapping
LD1 := Seed Labeled Data from L1
LD2 := Seed Labeled Data from L2
UD1 := Unlabeled Data from L1
UD2 := Unlabeled Data from L2
repeat
?1 := model trained using LD1
?2 := model trained using LD2
for all u1 ? UD1 do
s := sense assigned by ?1 to u1
if confidence(s) >  then
LD1 := LD1 + u1
UD1 := UD1 - u1
end if
end for
for all u2 ? UD2 do
s := sense assigned by ?2 to u2
if confidence(s) >  then
LD2 := LD2 + u2
UD2 := UD2 - u2
end if
end for
until convergence
strapping is shown in Algorithm 2.
6 Experimental Setup
We used the publicly available dataset1 described
in Khapra et al (2010) for all our experiments.
The data was collected from two domains, viz.,
Tourism and Health. The data for Tourism domain
was collected by manually translating English doc-
uments downloaded from Indian Tourism websites
into Hindi and Marathi. Similarly, English docu-
ments for Health domain were obtained from two
doctors and were manually translated into Hindi and
Marathi. The entire data was then manually an-
notated by three lexicographers adept in Hindi and
Marathi. The various statistics pertaining to the total
number of words, number of words per POS cate-
gory and average degree of polysemy are described
in Tables 2 to 5.
Although Tables 2 and 3 also report the num-
1http://www.cfilt.iitb.ac.in/wsd/annotated corpus
Polysemous words Monosemous words
Category Tourism Health Tourism Health
Noun 62336 24089 35811 18923
Verb 6386 1401 3667 5109
Adjective 18949 8773 28998 12138
Adverb 4860 2527 13699 7152
All 92531 36790 82175 43322
Table 2: Polysemous and Monosemous words per cate-
gory in each domain for Hindi
Polysemous words Monosemous words
Category Tourism Health Tourism Health
Noun 45589 17482 27386 11383
Verb 7879 3120 2672 1500
Adjective 13107 4788 16725 6032
Adverb 4036 1727 5023 1874
All 70611 27117 51806 20789
Table 3: Polysemous and Monosemous words per cate-
gory in each domain for Marathi
Avg. degree of Wordnet polysemy
for polysemous words
Category Tourism Health
Noun 3.02 3.17
Verb 5.05 6.58
Adjective 2.66 2.75
Adverb 2.52 2.57
All 3.09 3.23
Table 4: Average degree of Wordnet polysemy per cate-
gory in the 2 domains for Hindi
Avg. degree of Wordnet polysemy
for polysemous words
Category Tourism Health
Noun 3.06 3.18
Verb 4.96 5.18
Adjective 2.60 2.72
Adverb 2.44 2.45
All 3.14 3.29
Table 5: Average degree of Wordnet polysemy per cate-
gory in the 2 domains for Marathi
565
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  1000  2000  3000  4000  5000
F-
sc
or
e 
(%
)
Seed Size (words)
Seed Size v/s F-score
OnlySeed
WFS
BiBoot
MonoBoot
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  1000  2000  3000  4000  5000
F-
sc
or
e 
(%
)
Seed Size (words)
Seed Size v/s F-score
OnlySeed
WFS
BiBoot
MonoBoot
Figure 1: Comparison of BiBoot, Mono-
Boot, OnlySeed and WFS on Hindi Health
data
Figure 2: Comparison of BiBoot, Mono-
Boot, OnlySeed and WFS on Hindi
Tourism data
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  1000  2000  3000  4000  5000
F-
sc
or
e 
(%
)
Seed Size (words)
Seed Size v/s F-score
OnlySeed
WFS
BiBoot
MonoBoot
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  1000  2000  3000  4000  5000
F-
sc
or
e 
(%
)
Seed Size (words)
Seed Size v/s F-score
OnlySeed
WFS
BiBoot
MonoBoot
Figure 3: Comparison of BiBoot, Mono-
Boot, OnlySeed and WFS on Marathi
Health data
Figure 4: Comparison of BiBoot, Mono-
Boot, OnlySeed and WFS on Marathi
Tourism data
ber of monosemous words, we would like to clearly
state that we do not consider monosemous words
while evaluating the performance of our algorithms
(as monosemous words do not need any disambigua-
tion).
We did a 4-fold cross validation of our algorithm
using the above described corpora. Note that even
though the corpora were parallel we did not use this
property in any way in our experiments or algorithm.
In fact, the documents in the two languages were
randomly split into 4 folds without ensuring that the
parallel documents remain in the same folds for the
two languages. We experimented with different seed
sizes varying from 0 to 5000 in steps of 250. The
seed annotated data and untagged instances for boot-
strapping are extracted from 3 folds of the data and
the final evaluation is done on the held-out data in
the 4th fold.
We ran both the bootstrapping algorithms (i.e.,
monolingual bootstrapping and bilingual boot-
strapping) for 10 iterations but, we observed
that after 1-2 iterations the algorithms converge.
In each iteration only those words for which
P (assigned sense|word) > 0.6 get moved to the
labeled data. Ideally, this threshold (0.6) should
have been selected using a development set. How-
ever, since our work focuses on resource scarce lan-
guages we did not want to incur the additional cost
of using a development set. Hence, we used a fixed
threshold of 0.6 so that in each iteration only those
words get moved to the labeled data for which the
assigned sense is clearly a majority sense (P > 0.6).
566
Language-
Domain Algorithm F-score(%)
No. of tagged
words needed to
achieve this
F-score
% Reduction in annotation
cost
Hindi-Health Biboot 57.70 1250
(2250+2250)?(1250+1750)
(2250+2250) ? 100 = 33.33%
OnlySeed 57.99 2250
Marathi-Health Biboot 64.97 1750
OnlySeed 64.51 2250
Hindi-Tourism Biboot 60.67 1000
(2000+2000)?(1000+1250)
(2000+2000) ? 100 = 43.75%
OnlySeed 59.83 2000
Marathi-Tourism Biboot 61.90 1250
OnlySeed 61.68 2000
Table 6: Reduction in annotation cost achieved using Bilingual Bootstrapping
7 Results
The results of our experiments are summarized in
Figures 1 to 4. The x-axis represents the amount of
seed data used and the y-axis represents the F-scores
obtained. The different curves in each graph are as
follows:
a. BiBoot: This curve represents the F-score ob-
tained after 10 iterations by using bilingual boot-
strapping with different amounts of seed data.
b. MonoBoot: This curve represents the F-score ob-
tained after 10 iterations by using monolingual
bootstrapping with different amounts of seed data.
c. OnlySeed: This curve represents the F-score ob-
tained by training on the seed data alone without
using any bootstrapping.
d. WFS: This curve represents the F-score obtained
by simply selecting the first sense from Wordnet,
a typically reported baseline.
8 Discussions
In this section we discuss the important observations
made from Figures 1 to 4.
8.1 Performance of Bilingual bootstrapping
For small seed sizes, the F-score of bilingual boot-
strapping is consistently better than the F-score ob-
tained by training only on the seed data without us-
ing any bootstrapping. This is true for both the lan-
guages in both the domains. Further, bilingual boot-
strapping also does better than monolingual boot-
strapping for small seed sizes. As explained earlier,
this better performance can be attributed to the fact
that in monolingual bootstrapping the algorithm can
tag only those instances with high confidence which
it has already seen in the training data. Hence, in
successive iterations, very little new information be-
comes available to the algorithm. This is clearly
evident from the fact that the curve of monolin-
gual bootstrapping (MonoBoot) is always close to
the curve of OnlySeed.
8.2 Effect of seed size
The benefit of bilingual bootstrapping is clearly felt
for small seed sizes. However, as the seed size in-
creases the performance of the 3 algorithms, viz.,
MonoBoot, BiBoot and OnlySeed is more or less the
same. This is intuitive, because, as the seed size in-
creases the algorithm is able to see more and more
tagged instances in its self corpora and hence does
not need any assistance from the other language. In
other words, the annotated data in L1 is not able to
add any new information to the training process of
L2 and vice versa.
8.3 Bilingual bootstrapping reduces annotation
cost
The performance boost obtained at small seed sizes
suggests that bilingual bootstrapping helps to reduce
the overall annotation costs for both the languages.
To further illustrate this, we take some sample points
from the graph and compare the number of tagged
words needed by BiBoot and OnlySeed to reach the
same (or nearly the same) F-score. We present this
comparison in Table 6.
567
The rows for Hindi-Health and Marathi-Health in
Table 6 show that when BiBoot is employed we
need 1250 tagged words in Hindi and 1750 tagged
words in Marathi to attain F-scores of 57.70% and
64.97% respectively. On the other hand, in the ab-
sence of bilingual bootstrapping, (i.e., using Only-
Seed) we need 2250 tagged words each in Hindi and
Marathi to achieve similar F-scores. BiBoot thus
gives a reduction of 33.33% in the overall annota-
tion cost ( {1250 + 1750} v/s {2250 + 2250}) while
achieving similar F-scores. Similarly, the results for
Hindi-Tourism and Marathi-Tourism show that Bi-
Boot gives a reduction of 43.75% in the overall an-
notation cost while achieving similar F-scores. Fur-
ther, since the results of MonoBoot are almost the
same as OnlySeed, the above numbers indicate that
BiBoot provides a reduction in cost when compared
to MonoBoot also.
8.4 Contribution of monosemous words in the
performance of BiBoot
As mentioned earlier, monosemous words in the test
set are not considered while evaluating the perfor-
mance of our algorithm but, we add monosemous
words to the seed data. However, we do not count
monosemous words while calculating the seed size
as there is no manual annotation cost associated with
monosemous words (they can be tagged automati-
cally by fetching their singleton sense id from the
wordnet). We observed that the monosemous words
of L1 help in boosting the performance of L2 and
vice versa. This is because for a given monose-
mous word in L2 (or L1 respectively) the corre-
sponding cross-linked word in L1 (or L2 respec-
tively) need not necessarily be monosemous. In such
cases, the cross-linked polysemous word in L2 (or
L1 respectively) benefits from the projected statis-
tics of a monosemous word in L1 (or L2 respec-
tively). This explains why BiBoot gives an F-score
of 35-52% even at zero seed size even though the
F-score of OnlySeed is only 2-5% (see Figures 1 to
4).
9 Conclusion
We presented a bilingual bootstrapping algorithm
for Word Sense Disambiguation which allows two
resource deprived languages to mutually benefit
from each other?s data via parameter projection. The
algorithm consistently performs better than mono-
lingual bootstrapping. It also performs better than
using only monolingual seed data without using any
bootstrapping. The benefit of bilingual bootstrap-
ping is felt prominently when the seed size in the two
languages is very small thus highlighting the useful-
ness of this algorithm in highly resource constrained
scenarios.
Acknowledgments
We acknowledge the support of Microsoft Re-
search India in the form of an International Travel
Grant, which enabled one of the authors (Mitesh M.
Khapra) to attend this conference.
References
Eneko Agirre and German Rigau. 1996. Word sense dis-
ambiguation using conceptual density. In In Proceed-
ings of the 16th International Conference on Compu-
tational Linguistics (COLING).
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. pages 92?
100. Morgan Kaufmann Publishers.
Mitesh M. Khapra, Sapan Shah, Piyush Kedia, and Push-
pak Bhattacharyya. 2009. Projecting parameters for
multilingual word sense disambiguation. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 459?467, Singa-
pore, August. Association for Computational Linguis-
tics.
Mitesh Khapra, Saurabh Sohoney, Anup Kulkarni, and
Pushpak Bhattacharyya. 2010. Value for money: Bal-
ancing annotation effort, lexicon building and accu-
racy for multilingual wsd. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics.
Yoong Keok Lee, Hwee Tou Ng, and Tee Kiah Chia.
2004. Supervised word sense disambiguation with
support vector machines and multiple knowledge
sources. In Proceedings of Senseval-3: Third Inter-
national Workshop on the Evaluation of Systems for
the Semantic Analysis of Text, pages 137?140.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In In Proceedings of the
5th annual international conference on Systems docu-
mentation.
Hang Li and Cong Li. 2004. Word translation disam-
biguation using bilingual bootstrapping. Comput. Lin-
guist., 30:1?22, March.
568
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In ACL ?04: Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, page 279, Morristown, NJ, USA.
Association for Computational Linguistics.
Rada Mihalcea. 2005. Large vocabulary unsupervised
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In In Proceedings of
the Joint Human Language Technology and Empirical
Methods in Natural Language Processing Conference
(HLT/EMNLP), pages 411?418.
Rajat Mohanty, Pushpak Bhattacharyya, Prabhakar
Pande, Shraddha Kalele, Mitesh Khapra, and Aditya
Sharma. 2008. Synset based multilingual dictionary:
Insights, applications and challenges. In Global Word-
net Conference.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate word
senses: An exemplar-based approach. In In Proceed-
ings of the 34th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 40?47.
D. Walker and R. Amsler. 1986. The use of machine
readable dictionaries in sublanguage analysis. In In
Analyzing Language in Restricted Domains, Grish-
man and Kittredge (eds), LEA Press, pages 69?83.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd annual meeting on Association for
Computational Linguistics, pages 189?196, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
569
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1275?1284,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Cut the noise: Mutually reinforcing reordering and alignments for
improved machine translation
Karthik Visweswariah
IBM Research India
v-karthik@in.ibm.com
Mitesh M. Khapra
IBM Research India
mikhapra@in.ibm.com
Ananthakrishnan Ramanathan
IBM Research India
anandr42@gmail.com
Abstract
Preordering of a source language sentence
to match target word order has proved to
be useful for improving machine transla-
tion systems. Previous work has shown
that a reordering model can be learned
from high quality manual word alignments
to improve machine translation perfor-
mance. In this paper, we focus on further
improving the performance of the reorder-
ing model (and thereby machine transla-
tion) by using a larger corpus of sentence
aligned data for which manual word align-
ments are not available but automatic ma-
chine generated alignments are available.
The main challenge we tackle is to gen-
erate quality data for training the reorder-
ing model in spite of the machine align-
ments being noisy. To mitigate the effect
of noisy machine alignments, we propose
a novel approach that improves reorder-
ings produced given noisy alignments and
also improves word alignments using in-
formation from the reordering model. This
approach generates alignments that are 2.6
f-Measure points better than a baseline su-
pervised aligner. The data generated al-
lows us to train a reordering model that
gives an improvement of 1.8 BLEU points
on the NIST MT-08 Urdu-English eval-
uation set over a reordering model that
only uses manual word alignments, and a
gain of 5.2 BLEU points over a standard
phrase-based baseline.
1 Introduction
Dealing with word order differences between
source and target languages presents a significant
challenge for machine translation systems. Failing
to produce target words in the correct order results
in machine translation output that is not fluent and
is often very hard to understand. These problems
are particularly severe when translating between
languages which have very different structure.
Phrase based systems (Koehn et al, 2003) use
lexicalized distortion models (Al-Onaizan and Pa-
pineni, 2006; Tillman, 2004) and scores from the
target language model to produce words in the cor-
rect order in the target language. These systems
typically are only able to capture short range re-
orderings and the amount of data required to po-
tentially capture longer range reordering phenom-
ena is prohibitively large.
There has been a large body of work showing
the efficacy of preordering source sentences using
a source parser and applying hand written or auto-
matically learned rules (Collins et al, 2005; Wang
et al, 2007; Ramanathan et al, 2009; Xia and Mc-
Cord, 2004; Genzel, 2010; Visweswariah et al,
2010). Recently, approaches that address the prob-
lem of word order differences between the source
and target language without requiring a high qual-
ity source or target parser have been proposed
(DeNero and Uszkoreit, 2011; Visweswariah et
al., 2011; Neubig et al, 2012). These methods
use a small corpus of manual word alignments
(where the words in the source sentence are man-
ually aligned to the words in the target sentence)
to learn a model to preorder the source sentence to
match target order.
In this paper, we build upon the approach in
(Visweswariah et al, 2011) which uses manual
word alignments for learning a reordering model.
Specifically, we show that we can significantly
improve reordering performance by using a large
number of sentence pairs for which manual word
alignments are not available. The motivation for
going beyond manual word alignments is clear:
the reordering model can have millions of features
and estimating weights for the features on thou-
sands of sentences of manual word alignments is
1275
likely to be inadequate. One approach to deal with
this problem would be to use only part-of-speech
tags as features for all but the most frequent words.
This will cut down on the number of features and
perhaps the model would be learnable with a small
set of manual word alignments. Unfortunately, as
we will see in the experimental section, leaving
out lexical information from the models hurts per-
formance even with a relatively small set of man-
ual word alignments. Another option would be to
collect more manual word alignments but this is
undesirable because it is time consuming and ex-
pensive.
The challenge in going beyond manual word
alignments and using machine alignments is the
noise in the machine alignments which affects the
performance of the reordering model (see Section
5). We illustrate this with the help of a motivating
example. Consider the example English sentence
and its translation shown in Figure 1.
He went to the stadium to play
vaha khelne keliye stadium ko gaya
Figure 1: An example English sentence with
its Urdu translation with alignment links. Red
(dotted) links are incorrect links while the blue
(dashed) links are the corresponding correct links.
A standard word alignment algorithm that we
used (McCarley et al, 2011) made the mistake of
mis-aligning the Urdu ko and keliye (it switched
the two). Deriving reference reorderings from
these wrong alignments would give us an incor-
rect reordering. A reordering model trained on
such incorrect reorderings would obviously per-
form poorly. Our task is thus two-fold (i) im-
prove the quality of machine alignments (ii) use
these less noisy alignments to derive cleaner train-
ing data for a reordering model.
Before proceeding, we first point out that the
two tasks, viz., reordering and word alignment
are related: Having perfect reordering makes the
alignment task easier while having perfect align-
ments in turn makes the task of finding reorder-
ings trivial. Motivated by this fact, we introduce
models that allow us to connect the source/target
reordering and the word alignments and show
that these models help in mutually improving the
performance of word alignments and reordering.
Specifically, we build two models: the first scores
reorderings given the source sentence and noisy
alignments, the second scores alignments given
the noisy source and target reorderings and the
source and target sentences themselves. The sec-
ond model helps produce better alignments, while
we use the first model to help generate better ref-
erence reordering given noisy alignments. These
improved reference reorderings will then be used
to train a reordering model.
Our experiments show that reordering models
trained using these improved machine alignments
perform significantly better than models trained
only on manual word alignments. This results in
a 1.8 BLEU point gain in machine translation per-
formance on an Urdu-English machine translation
task over a preordering model trained using only
manual word alignments. In all, this increases
the gain in performance by using the preordering
model to 5.2 BLEU points over a standard phrase-
based system with no preordering.
The rest of this paper is structured as follows.
Section 2 describes the main reordering issues in
Urdu-English translation. Section 3 introduces the
reordering modeling framework that forms the ba-
sis for our work. Section 4 describes the two mod-
els we use to tie together reordering and align-
ments and how we use these models to generate
training data for training our reordering model.
Section 5 presents the experimental setup used for
evaluating the models proposed in this paper on
an Urdu-English machine translation task. Sec-
tion 6 presents the results of our experiments.
We describe related work in Section 7 and finally
present some concluding remarks and potential fu-
ture work in Section 8.
2 Reordering issues in Urdu-English
translation
In this section we describe the main sources of
word order differences between Urdu and English
since this is the language pair we experiment with
in this paper.
The typical word order in Urdu is Subject-
Object-Verb unlike English in which the order is
Subject-Verb-Object. Urdu has case markers that
sometimes (but not always) mark the subject and
the object of a sentence. This difference in the
placement of verbs can often lead to movements of
verbs over long distances (depending on the num-
ber of words in the object). Phrase based systems
do not capture such long distance movements well.
1276
Another difference is that Urdu uses post-
positions unlike English which uses prepositions.
This can also lead to long range movements de-
pending on the length of the noun phrase that the
post-position follows. The order of noun phrases
and prepositional phrases is also swapped in Urdu
as compared with English.
3 Reordering model
In this section we briefly describe the reordering
model (Visweswariah et al, 2011) that forms the
basis of our work. We also describe an approx-
imation we make in the training process that sig-
nificantly speeds up the training without much loss
of accuracy which enables training on much larger
data sets. Consider a source sentence w that we
would like to reorder to match the target order. Let
pi represent a candidate permutation of the source
sentence w. pii denotes the index of the word in the
source sentence that maps to position i in the can-
didate reordering, thus reordering with this candi-
date permutation pi we will reorder the sentence
w to wpi1 , wpi2 , ..., wpin . The reordering model we
use assigns costs to candidate permutations as:
C(pi|w) =
?
i
c(pii?1, pii).
The costs c(m,n) are pairwise costs of putting
wm immediately before wn in the reordering. We
reorder the sentence w according to the permu-
tation pi that minimizes the cost C(pi|w). We
find the minimal cost permutation by converting
the problem into a symmetric Travelling Salesman
Problem (TSP) and then using an implementation
of the chained Lin-Kernighan heuristic (Applegate
et al, 2003). The costs in the reordering model
c(m,n) are parameterized by a linear model:
c(m,n) = ?T?(w,m, n)
where ? is a learned vector of weights and ? is a
vector of binary feature functions that inspect the
words and POS tags of the source sentence at and
around positions m and n. We use the features
(?) described in Visweswariah et al (2011) that
were based on features used in dependency pars-
ing (McDonald et al, 2005a).
To learn the weight vector ? we require a cor-
pus of sentences w with their desired reorderings
pi?. Past work Visweswariah et al (2011) used
high quality manual word alignments to derive the
desired reorderings pi? as follows. Given word
aligned source and target sentences, we drop the
source words that are not aligned1. Let mi be the
mean of the target word positions that the source
word at index i is aligned to. We then sort the
source indices in increasing order ofmi (this order
defines pi?). If mi = mj (for example, because wi
and wj are aligned to the same set of words) we
keep them in the same order that they occurred in
the source sentence.
We used the single best Margin Infused Relaxed
Algorithm (MIRA) (McDonald et al (2005b),
Crammer and Singer (2003)) with online updates
to our parameters given by:
?i+1 = argmin
?
||? ? ?i||
s.t. C(pi?|w) < C(p?i|w) ? L(pi?, p?i).
In the equation above, p?i = argminpi C(pi|w) is
the best reordering based on the current parameter
value ?i and L is a loss function. We take L to be
the number of words for which the hypothesized
permutation p?i has a different preceding word as
compared with the reference permutation pi?.
In this paper we focus on the case where in ad-
dition to using a relatively small number of man-
ual word aligned sentences to derive the refer-
ence permutations pi? used to train our model,
we would like to use more abundant but nois-
ier machine aligned sentence pairs. To handle
the larger amount of training data we obtain from
machine alignments, we make an approximation
in training that we found empirically to not af-
fect performance but that makes training faster
by more than a factor of five. This allows us
to train the reordering model with roughly 150K
sentences in about two hours. The approximation
we make is that instead of using the chained Lin-
Kernighan heuristic to solve the TSP problem to
find p?i = argminpi C(pi|w), we select greedily
for each word the preceding word that has the low-
est cost2. Using ?i to denote argminj c(j, i) and
letting
C(?|w) =
?
i
c(?i, i),
1Note that the unaligned source words are dropped only at
the time of training. At the time of testing all source words are
retained as the alignment information is obviously not avail-
able at test time.
2It should be noted that this approximation was done only
at the time of training. At the time of testing we still use the
chained Lin-Kernighan heuristic to solve the TSP problem.
1277
we do the update according to:
?i+1 = argmin
?
||? ? ?i||
s.t. C(pi?|w) < C(?|w) ? L(pi?,?).
Again the loss L(pi?,?) is the number of positions
i for which pi?i?1 is different from ?i?1.
4 Generating reference reordering from
parallel sentences
The main aim of our work is to improve the re-
ordering model by using parallel sentences for
which manual word alignments are not avail-
able. In other words, we want to generate rel-
atively clean reference reorderings from parallel
sentences and use them for training a reordering
model. A straightforward approach for this is to
use a supervised aligner to align the words in the
sentences and then derive the reference reordering
as we do for manual word alignments. However,
as we will see in the experimental results, the qual-
ity of a reordering model trained from automatic
alignments is very sensitive to the quality of align-
ments. This motivated us to explore if we can fur-
ther improve our aligner and the method for gen-
erating reference reorderings given alignments.
We improve upon the above mentioned ba-
sic approach by coupling the tasks of reorder-
ing and word alignment. We do this by build-
ing a reordering model (C(pis|ws,wt,a)) that
scores reorderings pis given the source sentence
ws, target sentence wt and machine alignments
a. Complementing this model, we build an align-
ment model (P (a|ws,wt,pis,pit)) that scores
alignments a given the source and target sen-
tences and their predicted reorderings according to
source and target reordering models. The model
(C(pis|ws,wt,a)) helps to produce better refer-
ence reorderings for training our final reordering
model given fixed machine alignments and the
alignment model (P (a|ws,wt,pis,pit)) helps im-
prove the machine alignments taking into account
information from reordering models. In the fol-
lowing sections, we describe our overall approach
followed by a description of the two models.
4.1 Overall approach to generating training
data
We first describe our overall approach to gen-
erating training data for the reordering model
given a small corpus of sentences with manual
C(pis|ws) C(pit|wt)
Step 1: Train reordering models
using manual word alignments
P (a|ws,wt, pis, pit)
C(pis|ws,a) C(pit|wt,a)
Step 2: Feed predictions
of the reordering models
to the alignment model
Step 3: Feed predictions
of the alignment model
to the reordering models
Figure 2: Overall approach: Building a sequence
of reordering and alignment models.
word alignments (H) and a much larger corpus of
parallel sentences (U ) that are not word aligned.
The basic idea is to chain together the two models,
viz., reordering model and alignment model, as
illustrated in Figure 2. The steps involved are as
described below:
Step 1: First, we use manual word alignments
(H) to train source and target reordering models
as described in (Visweswariah et al, 2011).
Step 2: Next, we use the hand alignments to train
an alignment model P (a|ws,wt,pis,pit). In
addition to the original source and target sentence,
we also feed the predictions of the reordering
model trained in Step 1 to this alignment model
(see section 4.2 for details of the model itself).
Step 3: Finally, we use the predictions of the
alignment model trained in Step 2 to train reorder-
ing models C(pis|ws,wt,a) (see section 4.3 for
details on the reordering model itself).
After building the sequence of models shown in
Figure 2, we apply them in sequence on the un-
aligned parallel data U , starting with the reorder-
ing models C(pis|ws) and C(pit|wt). The re-
orderings obtained for the source side in U (after
applying the final model C(pis|ws,a)) are used
along with reference reorderings obtained from
the manual word alignments to train our reorder-
ing model. Note that, in theory, we could iterate
over steps 2 and 3 several times but, in practice
we did not see a benefit of going beyond one iter-
1278
ation in our experiments. Also, since we are inter-
ested only in the source side reorderings produced
by the model C(pis|ws,a), the target reordering
model C(pit|wt,a) is needed only if we iterate
over steps 2 and 3.
We now point to some practical considerations
of our approach. Consider the case when we are
training an alignment model conditioned on re-
orderings (P (a|ws,wt,pis,pit)). If the reorder-
ing model that generated these reorderings pis,pit
were trained on the same data that we are using
to train the alignment model, then the reorder-
ings would be much better than we would ex-
pect on unseen test data, and hence the align-
ment model (P (a|ws,wt,pis,pit)) may learn to
make the alignment overly consistent with the re-
orderings pis and pit. To counter this problem,
we divide the training data H into K parts and
at each stage we apply a model (reordering or
alignment) on part i that had not seen part i in
training. This ensures that the alignment model
does not see very optimistic reorderings and vice
versa. We now describe the individual models,
viz., P (a|ws,wt,pis,pit) and C(pis|ws,a).
4.2 Modeling alignments given reordering
In this section we describe how we fuse informa-
tion from source and target reordering models to
improve word alignments.
As a base model we use the correction model
for word alignments proposed by McCarley et
al. (2011). This model was significantly better
than the MaxEnt aligner (Ittycheriah and Roukos,
2005) and is also flexible in the sense that it allows
for arbitrary features to be introduced while still
keeping training and decoding tractable by using a
greedy decoding algorithm that explores potential
alignments in a small neighborhood of the current
alignment. The model thus needs a reasonably
good initial alignment to start with for which we
use the MaxEnt aligner (Ittycheriah and Roukos,
2005) as in McCarley et al (2011).
The correction model is a log-linear model:
P (a|ws,wt) = exp(?
T?(a,ws,wt))
Z(ws,wt) .
The ?s are trained using the LBFGS algorithm
(Liu et al, 1989) to maximize the log-likelihood
smoothed with L2 regularization. The feature
functions ? we start with are those used in Mc-
Carley et al (2011) and include features encoding
the Model 1 probabilities between pairs of words
linked in the alignment a, features that inspect
source and target POS tags and parses (if avail-
able) and features that inspect the alignments of
adjacent words in the source and target sentence.
To incorporate information from the reorder-
ing model, we add features that use the predicted
source pis and target permutations pit. We intro-
duce some notation to describe these features. Let
Sm and Sn be the set of indices of target words
thatwsm andwsn are aligned to respectively. We de-
fine the minimum signed distance (msd) between
these two sets as:
msd(Sm, Sn) = i? ? j?
where, (i?, j?) = arg min
(i,j)?Sm?Sn
|i? j|
We quantize and encode with binary features
the minimum signed distance between the sets of
the indices of the target words that source words
adjacent in the reordering pis (wspisi and wspisi+1) are
aligned to. We instantiate similar features with the
roles of source and target sentences reversed. With
this addition of features we use the same training
and testing procedure as in McCarley et al (2011).
If the reorderings pis were perfect we would learn
to only allow alignments where wspisi and w
s
pisi+1
were aligned to adjacent words in the target sen-
tence. Although the reordering model is not per-
fect, preferring alignments consistent with the re-
ordering models improves the aligner.
4.3 Modeling reordering given alignments
To model source permutations given source (ws)
and target (wt) sentences, and alignments (a) we
reuse the reordering model framework described
in Section 3 adding additional features capturing
the relation between a hypothesized permutation
pi and alignments a. To allow for searching via
the same TSP formulation we once again assign
costs to candidate permutations as:
C(pis|ws,wt,a) =
?
i
c(pii?1, pii|ws,a).
Note that we introduce a dependence on the target
sentence wt only through the alignment a. Once
again we parameterize the costs by a linear model:
c(m,n) = ?T?(ws,a,m, n).
For the feature functions ?, in addition to the
features that only depend on ws,m, n (that we
1279
use in our standard reordering model) we add
binary indicator features based on msd(Sm, Sn)
and msd(Sm, Sn) conjoined with POS(wsm) and
POS(wsn).
Here, Sm and Sn are the set of indices of tar-
get words that wsm and wsn are aligned to respec-
tively. We conjoin the msd (minimum signed dis-
tance) with the POS tags to allow the model to cap-
ture the fact that the alignment error rate maybe
higher for some POS tags than others (e.g., we
have observed verbs have a higher error rate in
Urdu-English alignments).
Given these features we train the parameters ?
using the MIRA algorithm as described in Sec-
tion 3. Using this model, we can find the low-
est cost permutation C(pis|ws,a) using the Lin-
Kernighan heuristic as described in Section 3.
This model allows us to combine features from
the original reordering model along with informa-
tion coming from the alignments to find source re-
orderings given a parallel corpus and alignments.
We will see in the experimental section that this
improves upon the simple heuristic for deriving re-
orderings described in Section 3.
5 Experimental setup
In this section we describe the experimental setup
that we used to evaluate the models proposed in
this paper. All experiments were done on Urdu-
English and we evaluate reordering in two ways:
Firstly, we evaluate reordering performance di-
rectly by comparing the reordered source sentence
in Urdu with a reference reordering obtained from
the manual word alignments using BLEU (Pap-
ineni et al, 2002) (we call this measure monolin-
gual BLEU or mBLEU). All mBLEU results are
reported on a small test set of about 400 sentences
set aside from our set of sentences with manual
word alignments. Additionally, we evaluate the ef-
fect of reordering on our final systems for machine
translation measured using BLEU.
We use about 10K sentences (180K words) of
manual word alignments which were created in
house using part of the NIST MT-08 training data3
to train our baseline reordering model and to train
our supervised machine aligners. We use a parallel
corpus of 3.9M words consisting of 1.7M words
from the NIST MT-08 training data set and 2.2M
words extracted from parallel news stories on the
3http://www.ldc.upenn.edu
web4. The parallel corpus is used for building our
phrased based machine translation system and to
add training data for our reordering model. For
our English language model, we use the Gigaword
English corpus in addition to the English side of
our parallel corpus. Our Part-of-Speech tagger is
a Maximum Entropy Markov model tagger trained
on roughly fifty thousand words from the CRULP
corpus (Hussain, 2008).
For our machine translation experiments, we
used a standard phrase based system (Al-Onaizan
and Papineni, 2006) with a lexicalized distortion
model with a window size of +/-4 words5. To
extract phrases we use HMM alignments along
with higher quality alignments from a supervised
aligner (McCarley et al, 2011). We report results
on the (four reference) NIST MT-08 evaluation set
in Table 4 for the News and Web conditions. The
News and Web conditions each contain roughly
20K words in the test set, with the Web condition
containing more informal text from the web.
6 Results and Discussions
We now discuss the results of our experiments.
Need for additional data: We first show the need
for additional data in Urdu-English reordering.
Column 2 of Table 1 shows mBLEU as a function
of the number of sentences with manual word
alignments that are used to train the reordering
model. We see a roughly 3 mBLEU points drop
in performance per halving of data indicating a
potential for improvement by adding more data.
Using fewer features: We compare the perfor-
mance of a model trained using lexical features
for all words (Column 2 of Table 1) with a model
trained using lexical features only for the 1000
most frequent words (Column 3 of Table 1). The
motivation for this is to explore if a good model
can be learned even from a small amount of data if
we restrict the number of features in a reasonable
manner. However, we see that even with only
2.4K sentences with manual word alignments our
model benefits from lexical identities of more
than the 1000 most frequent words.
Effect of quality of machine alignments: We
next look at the use of automatically generated
4http://centralasiaonline.com
5Note that the same window size of +/-4 words was used
for all the systems, i.e., the baseline system as well as the
systems using different preordering techniques.
1280
Data size All features Frequent lex only
10K 52.5 50.8
5K 49.6 49.0
2.5K 46.6 46.2
Table 1: mBLEU scores for Urdu to English re-
ordering using different number of sentences of
manually word aligned training data with all fea-
tures and with lexical features instantiated only for
the 1000 most frequent words.
machine alignments to train the reordering model
and see the effect of aligner quality on the re-
ordering model generated using this data. These
experiments also form the baseline for the mod-
els we propose in this paper to clean up align-
ments. We experimented with two different super-
vised aligners : a maximum entropy aligner (Itty-
cheriah and Roukos, 2005) and an improved cor-
rection model that corrects the maximum entropy
alignments (McCarley et al, 2011).
Aligner Train size mBLEU
Type f-Measure (words)
None - 35.5
Manual 180K 52.5
MaxEnt 70.0 3.9M 49.5
Correction model 78.1 3.9M 55.1
Table 2: mBLEU scores for Urdu to English re-
ordering using models trained on different data
sources and tested on a development set of 8017
Urdu tokens.
Table 2 shows mBLEU scores when the re-
ordering model is trained on reordering references
created from aligners with different quality. We
see that the quality of the alignments matter a
great deal to the reordering model; using MaxEnt
alignments cause a degradation in performance
over just using a small set of manual word align-
ments. The alignments obtained using the aligner
of McCarley et al (2011) are of much better
quality and hence give higher reordering perfor-
mance. Note that this reordering performance
is much better than that obtained using manual
word alignments because the size of machine
alignments is much larger (3.9M v/s 180K words).
Improvements in reordering performance us-
ing the proposed models: Table 3 shows im-
provements in the reordering model when using
the models proposed in this paper. We useH to re-
fer to the manually word aligned data and U to re-
fer to the additional sentence pairs for which man-
ual word alignments are not available. We report
the following numbers :
1. Base correction model: This is the baseline
where we use the correction model of McCar-
ley et al (2011) for generating word alignments.
The f-Measure of this aligner is 78.1% (see row
1, column 2). Corresponding to this, we also re-
port the baseline for our reordering experiments
in the third column. Here, we first generate word
alignments for U using the aligner of McCarley et
al. (2011) and then extract reference reorderings
from these alignments. We then combine these
reference reorderings with the reference reorder-
ings derived fromH and use this combined data to
train a reordering model which serves as the base-
line (mBLEU = 55.1).
2. Correction model, C(pi|a): Here, once again
we generate alignments for U using the correc-
tion model of McCarley et al (2011). However,
instead of using the basic approach of extracting
reference reorderings, we use our improved model
C(pi|a) to generate reference reorderings from U .
These reference reorderings are again combined
with the reference reorderings derived fromH and
used to train a reordering model (mBLEU = 56.4).
3. P (a|pi), C(pi|a): Here, we build the entire se-
quence of models shown in Figure 2. The align-
ment model P (a|pi) is first improved by using pre-
dictions from the reordering model. These im-
proved alignments are then used to extract better
reference reorderings from U using C(pi|a).
We see substantial improvements over simply
adding in the data from the machine alignments.
Improvements come roughly in equal parts from
the two techniques we proposed in this paper : (i)
using a model to generate reference reorderings
from noisy alignments and (ii) using reordering in-
formation to improve the aligner.
Method f-Measure mBLEU
Base Correction model 78.1 55.1
Correction model, C(pi|a) 78.1 56.4
P (a|pi), C(pi|a) 80.7 57.6
Table 3: mBLEU with different methods to gener-
ate reordering model training data from a machine
aligned parallel corpus in addition to manual word
alignments.
Improvements in MT performance using the
proposed models: We report results for a phrase
based system with different preordering tech-
niques. For results including a reordering model,
we simply reorder the source side Urdu data both
while training and at test time. In addition to
1281
phrase based systems with different preordering
methods, we also report on a hierarchical phrase
based system for which we used Joshua 4.0 (Gan-
itkevitch et al, 2012). We see a significant gain of
1.8 BLEU points in machine translation by going
beyond manual word alignments using the best re-
ordering model reported in Table 3. We also note a
gain of 2.0 BLEU points over a hierarchical phrase
based system.
System type MT-08 evalWeb News All
Baseline (no preordering) 18.4 25.6 22.2
Hierarchical phrase based 19.6 30.7 25.4
Reordering: Manual alignments 20.7 30.0 25.6
+ Machine alignments simple 21.3 30.9 26.4
+ machine alignments, model based 22.1 32.2 27.4
Table 4: MT performance without preordering
(phrase based and hierarchical phrase based),
and with reordering models using different data
sources (phrase based).
7 Related work
Dealing with the problem of handling word order
differences in machine translation has recently re-
ceived much attention. The approaches proposed
for solving this problem can be broadly divided
into 3 sets as discussed below.
The first set of approaches handle the reorder-
ing problem as part of the decoding process. Hier-
archical models (Chiang, 2007) and syntax based
models (Yamada and Knight, 2002; Galley et
al., 2006; Liu et al, 2006; Zollmann and Venu-
gopal, 2006) improve upon the simpler phrase
based models but with significant additional com-
putational cost (compared with phrase based sys-
tems) due to the inclusion of chart based parsing in
the decoding process. Syntax based models also
require a high quality source or target language
parser.
The second set of approaches rely on a source
language parser and treat reordering as a separate
process that is applied on the source language sen-
tence at training and test time before using a stan-
dard approach to machine translation. Preordering
the source data with hand written or automatically
learned rules is effective and efficient (Collins
et al, 2005; Wang et al, 2007; Ramanathan et
al., 2009; Xia and McCord, 2004; Genzel, 2010;
Visweswariah et al, 2010) but requires a source
language parser.
Recent approaches that avoid the need for a
source or target language parser and retain the ef-
ficiency of preordering models were proposed in
(Tromble and Eisner, 2009; DeNero and Uszko-
reit, 2011; Visweswariah et al, 2011; Neubig
et al, 2012). (DeNero and Uszkoreit, 2011;
Visweswariah et al, 2011; Neubig et al, 2012) fo-
cus on the use of manual word alignments to learn
preordering models and in both cases no benefit
was obtained by using the parallel corpus in ad-
dition to manual word alignments. Our work is
an extension of Visweswariah et al (2011) and
we focus on being able to incorporate relatively
noisy machine alignments to improve the reorder-
ing model.
In addition to being related to work in reorder-
ing, our work is also more broadly related to sev-
eral other efforts which we now outline. Seti-
awan et al (2010) proposed the use of function
word reordering to improve alignments. While
this work is similar to one of our models (model
of alignments given reordering) we differ in us-
ing a reordering model of all words (not just func-
tion words) and both source and target sentences
(not just the source sentence). The task of directly
learning a reordering model for language pairs that
are very different is closely related to the task of
parsing and hence work on semi-supervised pars-
ing (Koo et al, 2008; McClosky et al, 2006;
Suzuki et al, 2009) is broadly related to our work.
Our work coupling reordering and alignments is
also similar in spirit to approaches where parsing
and alignment are coupled (Wu, 1997).
8 Conclusion
In the paper we showed that a reordering model
can benefit from data beyond a relatively small
corpus of manual word alignments. We proposed
a model that scores reorderings given alignments
and the source sentence that we use to gener-
ate cleaner training data from noisy alignments.
We also proposed a model that scores alignments
given source and target sentence reorderings that
improves a supervised alignment model by 2.6
points in f-Measure. While the improvement in
alignment performance is modest, the improve-
ment does result in improved reordering models.
Cumulatively, we see a gain of 1.8 BLEU points
over a baseline reordering model that only uses
manual word alignments, a gain of 2.0 BLEU
points over a hierarchical phrase based system,
and a gain of 5.2 BLEU points over a phrase based
1282
system that uses no source preordering on a pub-
licly available Urdu-English test set.
As future work we would like to evaluate our
models on other language pairs. Another avenue
of future work we would like to explore is the use
of monolingual source and target data to further
assist the reordering model. We hope to be able to
learn lexical information such as how many argu-
ments a verb takes, what nouns are potential sub-
jects for a given verb by gathering statistics from
an English parser and projecting to the source lan-
guage via our word/phrase translation table.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of ACL, ACL-44, pages 529?536, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
David Applegate, William Cook, and Andre Rohe.
2003. Chained lin-kernighan for large traveling
salesman problems. In INFORMS Journal On Com-
puting.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201?228, June.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991, March.
John DeNero and Jakob Uszkoreit. 2011. Inducing
sentence structure from parallel corpora for reorder-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 193?203, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, ACL-44, pages 961?968, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt
Post, and Chris Callison-Burch. 2012. Joshua 4.0:
Packing, pro, and paraphrases. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, pages 283?291, Montre?al, Canada, June. As-
sociation for Computational Linguistics.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics.
Sarmad Hussain. 2008. Resources for Urdu language
processing. In Proceedings of the 6th Workshop on
Asian Language Resources, IJCNLP?08.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT/EMNLP,
HLT ?05, pages 89?96, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In ACL, pages 595?603.
Dong C. Liu, Jorge Nocedal, and Dong C. 1989. On
the limited memory bfgs method for large scale op-
timization. Mathematical Programming, 45:503?
528.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Com-
putational Linguistics, ACL-44, pages 609?616,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
J. Scott McCarley, Abraham Ittycheriah, Salim
Roukos, Bing Xiang, and Jian-ming Xu. 2011. A
correction model for word alignments. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?11, pages 889?
898, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
HLT-NAACL.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?05, pages 91?98, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
1283
Natural Language Learning, pages 843?853, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: addressing the crux
of the fluency problem in English-Hindi smt. In Pro-
ceedings of ACL-IJCNLP.
Hendra Setiawan, Chris Dyer, and Philip Resnik. 2010.
Discriminative word alignment with a function word
reordering model. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 534?544, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of semi-
supervised structured conditional models for depen-
dency parsing. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2 - Volume 2, EMNLP ?09,
pages 551?560, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Christoph Tillman. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of EMNLP.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with automat-
ically derived rules for improved statistical machine
translation. In Proceedings of the 23rd International
Conference on Computational Linguistics.
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for im-
proved machine translation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?11, pages 486?496,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of EMNLP-
CoNLL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comput. Linguist., 23(3):377?403, September.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In COLING.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical MT. In Proceedings of ACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation.
1284
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 138?141,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
OWNS: Cross-lingual Word Sense Disambiguation Using Weighted
Overlap Counts and Wordnet Based Similarity Measures
Lipta Mahapatra Meera Mohan
Dharmsinh Desai University
Nadiad, India
lipta.mahapatra89@gmail.com
mu.mohan@gmail.com
Mitesh M. Khapra Pushpak Bhattacharyya
Indian Institute of Technology Bombay
Powai, Mumbai 400076,India
miteshk@cse.iitb.ac.in
pb@cse.iitb.ac.in
Abstract
We report here our work on English
French Cross-lingual Word Sense Disam-
biguation where the task is to find the
best French translation for a target English
word depending on the context in which it
is used. Our approach relies on identifying
the nearest neighbors of the test sentence
from the training data using a pairwise
similarity measure. The proposed mea-
sure finds the affinity between two sen-
tences by calculating a weighted sum of
the word overlap and the semantic over-
lap between them. The semantic overlap
is calculated using standard Wordnet Sim-
ilarity measures. Once the nearest neigh-
bors have been identified, the best trans-
lation is found by taking a majority vote
over the French translations of the nearest
neighbors.
1 Introduction
Cross Language Word Sense Disambiguation
(CL-WSD) is the problem of finding the correct
target language translation of a word given the
context in which it appears in the source language.
In many cases a full disambiguation may not be
necessary as it is common for different meanings
of a word to have the same translation. This is es-
pecially true in cases where the sense distinction
is very fine and two or more senses of a word are
closely related. For example, the two senses of
the word letter, namely, ?formal document? and
?written/printed message? have the same French
translation ?lettre?. The problem is thus reduced
to distinguishing between the coarser senses of
a word and ignoring the finer sense distinctions
which is known to be a common cause of errors
in conventional WSD. CL-WSD can thus be seen
as a slightly relaxed version of the conventional
WSD problem. However, CL-WSD has its own
set of challenges as described below.
The translations learnt from a parallel corpus
may contain a lot of errors. Such errors are hard
to avoid due to the inherent noise associated with
statistical alignment models. This problem can be
overcome if good bilingual dictionaries are avail-
able between the source and target language. Eu-
roWordNet1 can be used to construct such a bilin-
gual dictionary between English and French but it
is not freely available. Instead, in this work, we
use a noisy statistical dictionary learnt from the
Europarl parallel corpus (Koehn, 2005) which is
freely downloadable.
Another challenge arises in the form of match-
ing the lexical choice of a native speaker. For ex-
ample, the word coach (as in, vehicle) may get
translated differently as autocar, autobus or bus
even when it appears in very similar contexts.
Such decisions depend on the native speaker?s in-
tuition and are very difficult for a machine to repli-
cate due to their inconsistent usage in a parallel
training corpus.
The above challenges are indeed hard to over-
come, especially in an unsupervised setting, as ev-
idenced by the lower accuracies reported by all
systems participating in the SEMEVAL Shared
Task on Cross-lingual Word Sense Disambigua-
tion (Lefever and Hoste, 2010). Our system
ranked second in the English French task (in the
out-of-five evaluation). Even though its average
performance was lower than the baseline by 3%
it performed better than the baseline for 12 out of
the 20 target nouns.
Our approach identifies the top-five translations
of a word by taking a majority vote over the trans-
lations appearing in the nearest neighbors of the
test sentence as found in the training data. We
use a pairwise similarity measure which finds the
affinity between two sentences by calculating a
1http://www.illc.uva.nl/EuroWordNet
138
weighted sum of the word overlap and the seman-
tic overlap between them. The semantic overlap is
calculated using standard Wordnet Similarity mea-
sures.
The remainder of this paper is organized as fol-
lows. In section 2 we describe related work on
WSD. In section 3 we describe our approach. In
Section 4 we present the results followed by con-
clusion in section 5.
2 Related Work
Knowledge based approaches to WSD such as
Lesk?s algorithm (Lesk, 1986), Walker?s algorithm
(Walker and Amsler, 1986), Conceptual Density
(Agirre and Rigau, 1996) and Random Walk Algo-
rithm (Mihalcea, 2005) are fundamentally overlap
based algorithms which suffer from data sparsity.
While these approaches do well in cases where
there is a surface match (i.e., exact word match)
between two occurrences of the target word (say,
training and test sentence) they fail in cases where
their is a semantic match between two occurrences
of the target word even though there is no surface
match between them. The main reason for this
failure is that these approaches do not take into
account semantic generalizations (e.g., train is-
a vehicle).
On the other hand, WSD approaches which use
Wordnet based semantic similarity measures (Pat-
wardhan et al, 2003) account for such seman-
tic generalizations and can be used in conjunc-
tion with overlap based approaches. We there-
fore propose a scoring function which combines
the strength of overlap based approaches ? fre-
quently co-occurring words indeed provide strong
clues ? with semantic generalizations using Word-
net based similarity measures. The disambigua-
tion is then done using k-NN (Ng and Lee, 1996)
where the k nearest neighbors of the test sentence
are identified using this scoring function. Once
the nearest neighbors have been identified, the best
translation is found by taking a majority vote over
the translations of these nearest neighbors.
3 Our approach
In this section we explain our approach for Cross
Language Word Sense Disambiguation. The main
emphasis is on disambiguation i.e. finding English
sentences from the training data which are closely
related to the test sentence.
3.1 Motivating Examples
To explain our approach we start with two moti-
vating examples. First, consider the following oc-
currences of the word coach:
? S
1
:...carriage of passengers by coach and
bus...
? S
2
:...occasional services by coach and bus
and the transit operations...
? S
3
:...the Gloucester coach saw the game...
In the first two cases, the word coach appears
in the sense of a vehicle and in both the cases the
word bus appears in the context. Hence, the sur-
face similarity (i.e., word-overlap count) of S
1
and
S
2
would be higher than that of S
1
and S
3
and
S
2
and S
3
. This highlights the strength of overlap
based approaches ? frequently co-occurring words
can provide strong clues for identifying similar us-
age patterns of a word.
Next, consider the following two occurrences of
the word coach:
? S
1
:...I boarded the last coach of the train...
? S
2
:...I alighted from the first coach of the
bus...
Here, the surface similarity (i.e., word-overlap
count) of S
1
and S
2
is zero even though in both
the cases the word coach appears in the sense of
vehicle. This problem can be overcome by us-
ing a suitable Wordnet based similarity measure
which can uncover the hidden semantic similarity
between these two sentences by identifying that
{bus, train} and {boarded, alighted} are closely
related words.
3.2 Scoring function
Based on the above motivating examples, we pro-
pose a scoring function for calculating the simi-
larity between two sentences containing the target
word. Let S
1
be the test sentence containing m
words and let S
2
be a training sentence containing
n words. Further, let w
1i
be the i-th word of S
1
and let w
2j
be the j-th word of S
2
. The similarity
between S
1
and S
2
is then given by,
Sim(S
1
, S
2
) = ? ?Overlap(S
1
, S
2
)
+ (1? ?) ? Semantic Sim(S
1
, S
2
)
(1)
where,
139
Overlap(S
1
, S
2
) =
1
m + n
m
?
i=1
n
?
j=1
freq(w
1i
) ? 1
{w
1i
=w
2j
}
and,
Semantic Sim(S
1
, S
2
) =
1
m
m
?
i=1
Best Sim(w
1i
, S
2
)
where,
Best Sim(w
1i
, S
2
) = max
w
2j
?S
2
lch(w
1i
, w
2j
)
We used the lch measure (Leacock and Chodorow,
1998) for calculating semantic similarity of two
words. The semantic similarity between S
1
and
S
2
is then calculated by simply summing over the
maximum semantic similarity of each constituent
word of S
1
over all words of S
2
. Also note that
the overlap count is weighted according to the fre-
quency of the overlapping words. This frequency
is calculated from all the sentences in the train-
ing data containing the target word. The ratio-
nal behind using a frequency-weighted sum is that
more frequently appearing co-occurring words are
better indicators of the sense of the target word
(of course, stop words and function words are not
considered). For example, the word bus appeared
very frequently with coach in the training data
and was a strong indicator of the vehicle sense
of coach. The values of Overlap(S
1
, S
2
) and
Semantic Sim(S
1
, S
2
) are appropriately nor-
malized before summing them in Equation (1). To
prevent the semantic similarity measure from in-
troducing noise by over-generalizing we chose a
very high value of ?. This effectively ensured
that the Semantic Sim(S
1
, S
2
) term in Equation
(1) became active only when the Overlap(S
1
, S
2
)
measure suffered data sparsity. In other words, we
placed a higher bet on Overlap(S
1
, S
2
) than on
Semantic Sim(S
1
, S
2
) as we found the former
to be more reliable.
3.3 Finding translations of the target word
We used GIZA++2 (Och and Ney, 2003), a freely
available implementation of the IBM alignment
models (Brown et al, 1993) to get word level
alignments for the sentences in the English-French
2http://sourceforge.net/projects/giza/
portion of the Europarl corpus. Under this align-
ment, each word in the source sentence is aligned
to zero or more words in the corresponding tar-
get sentence. Once the nearest neighbors for a test
sentence are identified using the similarity score
described earlier, we use the word alignment mod-
els to find the French translation of the target word
in the top-k nearest training sentences. These
translations are then ranked according to the num-
ber of times they appear in these top-k nearest
neighbors. The top-5 most frequent translations
are then returned as the output.
4 Results
We report results on the English-French Cross-
Lingual Word Sense Disambiguation task. The
test data contained 50 instances for 20 polysemous
nouns, namely, coach, education, execution, fig-
ure, job, letter, match, mission, mood, paper, post,
pot, range, rest, ring, scene, side, soil, strain and
test. We first extracted the sentences containing
these words from the English-French portion of
the Europarl corpus. These sentences served as the
training data to be compared with each test sen-
tence for identifying the nearest neighbors. The
appropriate translations for the target word in the
test sentence were then identified using the ap-
proach outlined in section 3.2 and 3.3. For the
best evaluation we submitted two runs: one con-
taining only the top-1 translation and another con-
taining top-2 translations. For the oof evaluation
we submitted one run containing the top-5 trans-
lations. The system was evaluated using Precision
and Recall measures as described in the task pa-
per (Lefever and Hoste, 2010). In the oof evalua-
tion our system gave the second best performance
among all the participants. However, the average
precision was 3% lower than the baseline calcu-
lated by simply identifying the five most frequent
translations of a word according to GIZA++ word
alignments. A detailed analysis showed that in the
oof evaluation we did better than the baseline for
12 out of the 20 nouns and in the best evaluation
we did better than the baseline for 5 out of the 20
nouns. Table 1 summarizes the performance of our
system in the best evaluation and Table 2 gives the
detailed performance of our system in the oof eval-
uation. In both the evaluations our system pro-
vided a translation for every word in the test data
and hence the precision was same as recall in all
cases. We refer to our system as OWNS (Overlap
140
and WordNet Similarity).
System Precision Recall
OWNS 16.05 16.05
Baseline 20.71 20.71
Table 1: Performance of our system in best evalu-
ation
Word OWNS Baseline
(Precision) (Precision)
coach 45.11 39.04
education 82.15 80.4
execution 59.22 39.63
figure 30.56 35.67
job 43.93 40.98
letter 46.01 42.34
match 31.01 15.73
mission 55.33 97.19
mood 35.22 64.81
paper 48.93 40.95
post 36.65 41.76
pot 26.8 65.23
range 16.28 17.02
rest 39.89 38.72
ring 39.74 33.74
scene 33.89 38.7
side 37.85 36.58
soil 67.79 59.9
strain 21.13 30.02
test 64.65 61.31
Average 43.11 45.99
Table 2: Performance of our system in oof evalua-
tion
5 Conclusion
We described our system for English French
Cross-Lingual Word Sense Disambiguation which
calculates the affinity between two sentences by
combining the weighted word overlap counts with
semantic similarity measures. This similarity
score is used to find the nearest neighbors of the
test sentence from the training data. Once the
nearest neighbors have been identified, the best
translation is found by taking a majority vote over
the translations of these nearest neighbors. Our
system gave the second best performance in the
oof evaluation among all the systems that partic-
ipated in the English French Cross-Lingual Word
Sense Disambiguation task. Even though the av-
erage performance of our system was less than the
baseline by around 3%, it outperformed the base-
line system for 12 out of the 20 nouns.
References
Eneko Agirre and German Rigau. 1996. Word sense
disambiguation using conceptual density. In In Pro-
ceedings of the 16th International Conference on
Computational Linguistics (COLING).
Peter E Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19:263?311.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In In Proceedings of the
MT Summit.
C. Leacock and M. Chodorow, 1998. Combining lo-
cal context and WordNet similarity for word sense
identification, pages 305?332. In C. Fellbaum (Ed.),
MIT Press.
Els Lefever and Veronique Hoste. 2010. Semeval-
2010 task 3: Cross-lingual word sense disambigua-
tion. In Proceedings of the 5th International Work-
shop on Semantic Evaluations (SemEval-2010), As-
sociation for Computational Linguistics.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In In Proceed-
ings of the 5th annual international conference on
Systems documentation.
Rada Mihalcea. 2005. Large vocabulary unsupervised
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In In Proceed-
ings of the Joint Human Language Technology and
Empirical Methods in Natural Language Processing
Conference (HLT/EMNLP), pages 411?418.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
senses: An exemplar-based approach. In In Pro-
ceedings of the 34th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
40?47.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2003. Using measures of semantic re-
latedness for word sense disambiguation. In In pro-
ceedings of the Fourth International Conference on
Intelligent Text Processing and Computation Lin-
guistics (CICLing.
D. Walker and R. Amsler. 1986. The use of machine
readable dictionaries in sublanguage analysis. In In
Analyzing Language in Restricted Domains, Grish-
man and Kittredge (eds), LEA Press, pages 69?83.
141
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 421?426,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
CFILT: Resource Conscious Approaches for All-Words Domain Specific
WSD
Anup Kulkarni Mitesh M. Khapra Saurabh Sohoney Pushpak Bhattacharyya
Department of Computer Science and Engineering,
Indian Institute of Technology Bombay,
Powai, Mumbai 400076,
India
{anup,miteshk,saurabhsohoney,pb}@cse.iitb.ac.in
Abstract
We describe two approaches for All-words
Word Sense Disambiguation on a Spe-
cific Domain. The first approach is a
knowledge based approach which extracts
domain-specific largest connected com-
ponents from the Wordnet graph by ex-
ploiting the semantic relations between all
candidate synsets appearing in a domain-
specific untagged corpus. Given a test
word, disambiguation is performed by
considering only those candidate synsets
that belong to the top-k largest connected
components.
The second approach is a weakly super-
vised approach which relies on the ?One
Sense Per Domain? heuristic and uses a
few hand labeled examples for the most
frequently appearing words in the target
domain. Once the most frequent words
have been disambiguated they can pro-
vide strong clues for disambiguating other
words in the sentence using an iterative
disambiguation algorithm. Our weakly
supervised system gave the best perfor-
mance across all systems that participated
in the task even when it used as few as 100
hand labeled examples from the target do-
main.
1 Introduction
Domain specific WSD exhibits high level of ac-
curacy even for the all-words scenario (Khapra et
al., 2010) - provided training and testing are on the
same domain. However, the effort of creating the
training corpus - annotated sense marked corpora
- for every domain of interest has always been a
matter of concern. Therefore, attempts have been
made to develop unsupervised (McCarthy et al,
2007; Koeling et al, 2005) and knowledge based
techniques (Agirre et al, 2009) for WSD which
do not need sense marked corpora. However, such
approaches have not proved effective, since they
typically do not perform better than the Wordnet
first sense baseline accuracy in the all-words sce-
nario.
Motivated by the desire to develop annotation-
lean all-words domain specific techniques for
WSD we propose two resource conscious ap-
proaches. The first approach is a knowledge based
approach which focuses on retaining only domain
specific synsets in the Wordnet using a two step
pruning process. In the first step, the Wordnet
graph is restricted to only those synsets which
contain words appearing in an untagged domain-
specific corpus. In the second step, the graph is
pruned further by retaining only the largest con-
nected components of the pruned graph. Each tar-
get word in a given sentence is then disambiguated
using an iterative disambiguation process by con-
sidering only those candidate synsets which ap-
pear in the top-k largest connected components.
Our knowledge based approach performed better
than current state of the art knowledge based ap-
proach (Agirre et al, 2009). Also, the precision
was better than the Wordnet first sense baseline
even though the F-score was slightly lower than
the baseline.
The second approach is a weakly supervised ap-
proach which uses a few hand labeled examples
for the most frequent words in the target domain
in addition to the publicly available mixed-domain
SemCor (Miller et al, 1993) corpus. The underly-
ing assumption is that words exhibit ?One Sense
Per Domain? phenomenon and hence even as few
as 5 training examples per word would be suffi-
cient to identify the predominant sense of the most
frequent words in the target domain. Further, once
the most frequent words have been disambiguated
using the predominant sense, they can provide
strong clues for disambiguating other words in the
421
sentence. Our weakly supervised system gave the
best performance across all systems that partici-
pated in the task even when it used as few as 100
hand labeled examples from the target domain.
The remainder of this paper is organized as fol-
lows. In section 2 we describe related work on
domain-specific WSD. In section 3 we discuss an
Iterative Word Sense Disambiguation algorithm
which lies at the heart of both our approaches. In
section 4 we describe our knowledge based ap-
proach. In section 5 we describe our weakly su-
pervised approach. In section 6 we present results
and discussions followed by conclusion in section
7.
2 Related Work
There are two important lines of work for do-
main specific WSD. The first focuses on target
word specific WSD where the results are reported
on a handful of target words (41-191 words) on
three lexical sample datasets, viz., DSO corpus
(Ng and Lee, 1996), MEDLINE corpus (Weeber et
al., 2001) and the corpus of Koeling et al (2005).
The second focuses on all-words domain specific
WSD where the results are reported on large anno-
tated corpora from two domains, viz., TOURISM
and HEALTH (Khapra et al, 2010).
In the target word setting, it has been shown that
unsupervised methods (McCarthy et al, 2007) and
knowledge based methods (Agirre et al, 2009)
can do better than wordnet first sense baseline and
in some cases can also outperform supervised ap-
proaches. However, since these systems have been
tested only for certain target words, the question of
their utility in all words WSD it still open .
In the all words setting, Khapra et al (2010)
have shown significant improvements over the
wordnet first sense baseline using a fully super-
vised approach. However, the need for sense anno-
tated corpus in the domain of interest is a matter of
concern and provides motivation for adapting their
approach to annotation scarce scenarios. Here, we
take inspiration from the target-word specific re-
sults reported by Chan and Ng (2007) where by
using just 30% of the target data they obtained the
same performance as that obtained by using the
entire target data.
We take the fully supervised approach of
(Khapra et al, 2010) and convert it to a weakly su-
pervised approach by using only a handful of hand
labeled examples for the most frequent words ap-
pearing in the target domain. For the remaining
words we use the sense distributions learnt from
SemCor (Miller et al, 1993) which is a publicly
available mixed domain corpus. Our approach is
thus based on the ?annotate-little from the target
domain? paradigm and does better than all the sys-
tems that participated in the shared task.
Even our knowledge based approach does better
than current state of the art knowledge based ap-
proaches (Agirre et al, 2009). Here, we use an un-
tagged corpus to prune the Wordnet graph thereby
reducing the number of candidate synsets for each
target word. To the best of our knowledge such an
approach has not been tried earlier.
3 Iterative Word Sense Disambiguation
The Iterative Word Sense Disambiguation (IWSD)
algorithm proposed by Khapra et al (2010) lies at
the heart of both our approaches. They use a scor-
ing function which combines corpus based param-
eters (such as, sense distributions and corpus co-
occurrence) and Wordnet based parameters (such
as, semantic similarity, conceptual distance, etc.)
for ranking the candidates synsets of a word. The
algorithm is iterative in nature and involves the
following steps:
? Tag all monosemous words in the sentence.
? Iteratively disambiguate the remaining words
in the sentence in increasing order of their de-
gree of polysemy.
? At each stage rank the candidate senses of a
word using the scoring function of Equation
(1).
S
?
= argmax
i
(?
i
V
i
+
?
j?J
W
ij
? V
i
? V
j
) (1)
where,
i ? Candidate Synsets
J = Set of disambiguated words
?
i
= BelongingnessToDominantConcept(S
i
)
V
i
= P (S
i
|word)
W
ij
= CorpusCooccurrence(S
i
, S
j
)
? 1/WNConceptualDistance(S
i
, S
j
)
? 1/WNSemanticGraphDistance(S
i
, S
j
)
The scoring function as given above cleanly
separates the self-merit of a synset (P (S
i
|word))
422
as learnt from a tagged corpus and its interaction-
merit in the form of corpus co-occurrence, con-
ceptual distance, and wordnet-based semantic dis-
tance with the senses of other words in the sen-
tence. The scoring function can thus be easily
adapted depending upon the amount of informa-
tion available. For example, in the weakly su-
pervised setting, P (S
i
|word) will be available for
some words for which either manually hand la-
beled training data from environment domain is
used or which appear in the SemCor corpus. For
such words, all the parameters in Equation (1) will
be used for scoring the candidate synsets and for
remaining words only the interaction parameters
will be used. Similarly, in the knowledge based
setting, P (S
i
|word) will never be available and
hence only the wordnet based interaction parame-
ters (i.e., WNConceptualDistance(S
i
, S
j
) and
WNSemanticGraphDistance(S
i
, S
j
)) will be
used for scoring the pruned list of candidate
synsets. Please refer to (Khapra et al, 2010) for
the details of how each parameter is calculated.
4 Knowledge-Based WSD using Graph
Pruning
Wordnet can be viewed as a graph where synsets
act as nodes and the semantic relations between
them act as edges. It should be easy to see
that given a domain-specific corpus, synsets from
some portions of this graph would be more likely
to occur than synsets from other portions. For
example, given a corpus from the HEALTH do-
main one might expect synsets belonging to the
sub-trees of ?doctor?, ?medicine?, ?disease? to
appear more frequently than the synsets belonging
to the sub-tree of ?politics?. Such dominance ex-
hibited by different components can be harnessed
for domain-specific WSD and is the motivation for
our work.
The crux of the approach is to identify such do-
main specific components using a two step prun-
ing process as described below:
Step 1: First, we use an untagged corpus from
the environment domain to identify the unique
words appearing in the domain. Note that, by
unique words we mean all content words which
appear at least once in the environment corpus
(these words may or may not appear in a gen-
eral mixed domain corpus). This untagged corpus
containing 15 documents (22K words) was down-
loaded from the websites of WWF1 and ECNC2
and contained articles on Climate Change, De-
forestation, Species Extinction, Marine Life and
Ecology. Once the unique words appearing in
this environment-specific corpus are identified, we
restrict the Wordnet graph to only those synsets
which contain one or more of these unique words
as members. This step thus eliminates all spurious
synsets which are not related to the environment
domain.
Step 2: In the second step, we perform a Breadth-
First-Search on the pruned graph to identify the
connected components of the graph. While
traversing the graph we consider only those edges
which correspond to the hypernymy-hyponymy re-
lation and ignore all other semantic relations as we
observed that such relations add noise to the com-
ponents. The top-5 largest components thus iden-
tified were considered to be environment-specific
components. A subset of synsets appearing in one
such sample component is listed in Table 1.
Each target word in a given sentence is then disam-
biguated using the IWSD algorithm described in
section 3. However, now the argmax of Equation
(1) is computed only over those candidate synsets
which belong to the top-5 largest components and
all other candidate synsets are ignored. The sug-
gested pruning technique is indeed very harsh and
as a result there are many words for which none
of their candidate synsets belong to these top-5
largest components. These are typically domain-
invariant words for which pruning does not make
sense as the synsets of such generic words do
not belong to domain-specific components of the
Wordnet graph. In such cases, we consider all the
candidate synsets of these words while computing
the argmax of Equation (1).
5 Weakly Supervised WSD
Words are known to exhibit ?One Sense Per Do-
main?. For example, in the HEALTH domain the
word cancer will invariably occur in the disease
sense and almost never in the sense of a zodiac
sign. This is especially true for the most frequently
appearing nouns in the domain as these are typi-
cally domain specific nouns. For example, nouns
such as farmer, species, population, conservation,
nature, etc. appear very frequently in the envi-
ronment domain and exhibit a clear predominant
1http://www.wwf.org
2http://www.ecnc.org
423
{ safety} - NOUN - the state of being certain that adverse effects will not be caused by some agent
under defined conditions; ?insure the safety of the children?; ?the reciprocal of safety is risk?
{preservation, saving} - NOUN - the activity of protecting something from loss or danger
{environment} - NOUN - the totality of surrounding conditions; ?he longed for the comfortable
environment of his living room?
{animation, life, living, aliveness} - NOUN - the condition of living or the state of being alive;
?while there?s life there?s hope?; ?life depends on many chemical and physical processes?
{renovation, restoration, refurbishment} - NOUN - the state of being restored to its former good
condition; ?the inn was a renovation of a Colonial house?
{ecology} - NOUN - the environment as it relates to living organisms; ?it changed the ecology of
the island?
{development} - NOUN - a state in which things are improving; the result of developing (as in the
early part of a game of chess); ?after he saw the latest development he changed his mind and be-
came a supporter?; ?in chess your should take care of your development before moving your queen?
{survival, endurance} - NOUN - a state of surviving; remaining alive
. . . . . . . . . . . .
. . . . . . . . . . . .
Table 1: Environment specific component identified after pruning
sense in the domain. As a result as few as 5 hand
labeled examples per noun are sufficient for find-
ing the predominant sense of these nouns. Further,
once these most frequently occurring nouns have
been disambiguated they can help in disambiguat-
ing other words in the sentence by contributing to
the interaction-merit of Equation (1) (note that in
Equation (1), J = Set of disambiguated words).
Based on the above intuition, we slightly mod-
ified the IWSD algorithm and converted it to a
weakly supervised algorithm. The original algo-
rithm as described in section 3 uses monosemous
words as seed input (refer to the first step of the al-
gorithm). Instead, we use the most frequently ap-
pearing nouns as the seed input. These nouns are
disambiguated using their pre-dominant sense as
calculated from the hand labeled examples. Our
weakly supervised IWSD algorithm can thus be
summarized as follows
? If a word w in a test sentence belongs to
the list of most frequently appearing domain-
specific nouns then disambiguate it first us-
ing its self-merit (i.e., P (S
i
|word)) as learnt
from the hand labeled examples.
? Iteratively disambiguate the remaining words
in the sentence in increasing order of their de-
gree of polysemy.
? While disambiguating the remaining words
rank the candidate senses of a word using
the self-merit learnt from SemCor and the
interaction-merit based on previously disam-
biguated words.
The most frequent words and the corresponding
examples to be hand labeled are extracted from the
same 15 documents (22K words) as described in
section 4.
6 Results
We report the performance of our systems in the
SEMEVAL task on All-words Word Sense Dis-
ambiguation on a Specific Domain (Agirre et al,
2010). The task involved sense tagging 1398
nouns and verbs from 3 documents extracted from
the environment domain. We submitted one run
for the knowledge based system and 2 runs for the
weakly supervised system. For the weakly super-
vised system, in one run we used 5 training ex-
amples each for the 80 most frequently appear-
ing nouns in the domain and in the second run we
424
used 5 training examples each for the 200 most
frequently appearing nouns. Both our submis-
sions in the weakly supervised setting performed
better than all other systems that participated in
the shared task. Post-submission we even exper-
imented with using 5 training examples each for
as few as 20 most frequent nouns and even in
this case we found that our weakly supervised sys-
tem performed better than all other systems that
participated in the shared task.
The precision of our knowledge based system
was slightly better than the most frequent sense
(MFS) baseline reported by the task organizers
but the recall was slightly lower than the baseline.
Also, our approach does better than the current
state of the art knowledge based approach (Person-
alized Page Rank approach of Agirre et al (2009)).
All results are summarized in Table 2. The fol-
lowing guide specifies the systems reported:
? WS-k: Weakly supervised approach using 5
training examples for the k most frequently
appearing nouns in the environment domain.
? KB: Knowledge based approach using graph
based pruning.
? PPR: Personalized PageRank approach of
Agirre et al (2009).
? MFS: Most Frequent Sense baseline pro-
vided by the task organizers.
? Random: Random baseline provided by the
task organizers.
System Precision Recall Rank in shared task
WS-200 0.570 0.555 1
WS-80 0.554 0.540 2
WS-20 0.548 0.535 3 (Post submission)
KB 0.512 0.495 7
PPR 0.373 0.368 24 (Post submission)
MFS 0.505 0.505 6
Random 0.23 0.23 30
Table 2: The performance of our systems in the
shared task
In Table 3 we provide the results of WS-200 for
each POS category. As expected, the results for
nouns are much better than those for verbs mainly
because nouns are more likely to stick to the ?One
sense per domain? property than verbs.
Category Precision Recall
Verbs 45.37 42.89
Nouns 59.64 59.01
Table 3: The performance of WS-200 on each
POS category
7 Conclusion
We presented two resource conscious approaches
for All-words Word Sense Disambiguation on a
Specific Domain. The first approach is a knowl-
edge based approach which retains only domain
specific synsets from the Wordnet by using a two
step pruning process. This approach does better
than the current state of the art knowledge based
approaches although its performance is slightly
lower than the Most Frequent Sense baseline. The
second approach which is a weakly supervised ap-
proach based on the ?annotate-little from the tar-
get domain? paradigm performed better than all
systems that participated in the task even when it
used as few as 100 hand labeled examples from
the target domain. This approach establishes the
veracity of the ?One sense per domain? phe-
nomenon by showing that even as few as five ex-
amples per word are sufficient for predicting the
predominant sense of a word.
Acknowledgments
We would like to thank Siva Reddy and Abhilash
Inumella (from IIIT Hyderabad, India) for provid-
ing us the results of Personalized PageRank (PPR)
for comparison.
References
Eneko Agirre, Oier Lopez De Lacalle, and Aitor Soroa.
2009. Knowledge-based wsd on specific domains:
Performing better than generic supervised wsd.
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Shu kai Hsieh, Maurizio Tesconi, Mon-
ica Monachini, Piek Vossen, and Roxanne Segers.
2010. Semeval-2010 task 17: All-words word sense
disambiguation on a specific domain.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense dis-
ambiguation. In In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 49?56.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Push-
pak Bhattacharyya. 2010. Domain-specific word
425
sense disambiguation combining corpus based and
wordnet based parameters. In 5th International
Conference on Global Wordnet (GWC2010).
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In HLT ?05: Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 419?426, Morristown, NJ, USA.
Association for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of predom-
inant word senses. Comput. Linguist., 33(4):553?
590.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
HLT ?93: Proceedings of the workshop on Human
Language Technology, pages 303?308, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
senses: An exemplar-based approach. In In Pro-
ceedings of the 34th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
40?47.
Marc Weeber, James G. Mork, and Alan R. Aronson.
2001. Developing a test collection for biomedical
word sense disambiguation. In In Proceedings of the
American Medical Informatics Association Annual
Symposium (AMIA 2001), pages 746?750.
426
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 21?28,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Report of NEWS 2010 Transliteration Mining Shared Task 
 
A Kumaran Mitesh M. Khapra Haizhou Li 
Microsoft Research India 
Bangalore, India 
Indian Institute of Technology Bombay 
Mumbai, India 
Institute for Infocomm  
Research, Singapore 
 
Abstract 
This report documents the details of the Trans-
literation Mining Shared Task that was run as 
a part of the Named Entities Workshop 
(NEWS 2010), an ACL 2010 workshop.  The 
shared task featured mining of name translite-
rations from the paired Wikipedia titles in 5 
different language pairs, specifically, between 
English and one of Arabic, Chinese, Hindi 
Russian and Tamil.  Totally 5 groups took part 
in this shared task, participating in multiple 
mining tasks in different languages pairs.  The 
methodology and the data sets used in this 
shared task are published in the Shared Task 
White Paper [Kumaran et al 2010]. We meas-
ure and report 3 metrics on the submitted re-
sults to calibrate the performance of individual 
systems on a commonly available Wikipedia 
dataset.  We believe that the significant contri-
bution of this shared task is in (i) assembling a 
diverse set of participants working in the area 
of transliteration mining, (ii) creating a base-
line performance of transliteration mining sys-
tems in a set of diverse languages using com-
monly available Wikipedia data, and (iii) pro-
viding a basis for meaningful comparison and 
analysis of trade-offs between various algo-
rithmic approaches used in mining.  We be-
lieve that this shared task would complement 
the NEWS 2010 transliteration generation 
shared task, in enabling development of prac-
tical systems with a small amount of seed data 
in a given pair of languages. 
1 Introduction  
Proper names play a significant role in Machine 
Translation (MT) and Information Retrieval (IR) 
systems.  When the systems involve multiple 
languages, The MT and IR system rely on Ma-
chine Transliteration systems, as the proper 
names are not usually available in standard trans-
lation lexicons. The quality of the Machine 
Transliteration systems plays a significant part in 
determining the overall quality of the system, 
and hence, they are critical for most multilingual 
application systems.  The importance of Machine 
Transliteration systems has been well understood 
by the community, as evidenced by significant 
publication in this important area. 
While research over the last two decades has 
shown that reasonably good quality Machine 
Transliteration systems may be developed easily, 
they critically rely on parallel names corpora for 
their development.  The Machine Transliteration 
Shared Task of the NEWS 2009 workshop 
(NEWS 2009) has shown that many interesting 
approaches exist for Machine Transliteration, 
and about 10-25K parallel names is sufficient for 
most state of the art systems to provide a practic-
al solution for the critical need.  The traditional 
source for crosslingual parallel data ? the bilin-
gual dictionaries ? offer only limited support as 
they do not include proper names (other than 
ones of historical importance).  The statistical 
dictionaries, though they contain parallel names, 
do not have sufficient coverage, as they depend 
on some threshold statistical evidence 1 . New 
names and many variations of them are intro-
duced to the vocabulary of a language every day 
that need to be captured for any good quality 
end-to-end system such as MT or CLIR.   So 
there is a perennial need for harvesting parallel 
names data, to support end-user applications and 
systems well and accurately. 
This is the specific focus of the Transliteration 
Mining Shared Task in NEWS 2010 workshop 
(an ACL 2010 Workshop): To mine accurately 
parallel names from a popular, ubiquitous source, 
the Wikipedia.  Wikipedia exists in more than 
250 languages, and every Wikipedia article has a 
link to an equivalent article in other languages2.  
We focused on this specific resource ? the Wiki-
pedia titles in multiple languages and the inter-
linking between them ? as the source of parallel 
names.  Any successful mining of parallel names 
from title would signal copious availability of 
parallel names data, enabling transliteration gen-
eration systems in many languages of the world. 
                                                 
1 In our experiments with Indian Express news corpo-
ra over 2 years shows that 80% of the names occur 
less than 5 times in the entire corpora. 
2 Note that the titles contain concepts, events, dates, 
etc., in addition to names.  Even when the titles are 
names, parts of them may not be transliterations. 
21
2 Transliteration Mining Shared Task 
In this section, we provide details of the shared 
task, and the datasets used for the task and results 
evaluation.  
2.1 Shared Task: Task Details 
The task featured in this shared task was to de-
velop a mining system for identifying single 
word transliteration pairs from the standard inter-
linked Wikipedia topics (aka, Wikipedia Inter-
Language Links, or WIL3) in one or more of the 
specified language pairs. The WIL?s link articles 
on the same topic in multiple languages, and are 
traditionally used as a parallel language resource 
for many natural language processing applica-
tions, such as Machine Translation, Crosslingual 
Search, etc.  Specific WIL?s of interest for our 
task were those that contained proper names ? 
either wholly or partly ? which can yield rich 
transliteration data.   
The task involved transliteration mining in the 
language pairs summarized in Table 1.  
 
Source 
Language 
Target Lan-
guage 
Track ID 
English  Chinese  WM-EnCn 
English  Hindi  WM-EnHi 
English  Tamil WM-EnTa 
English  Russian  WM-EnRu 
English Arabic WM-EnAr 
Table 1: Language Pairs in the shared task 
 
Each WIL consisted of a topic in the source 
and target language pair, and the task was to 
identify parts of the topic (in the respective lan-
guage titles) that are transliterations of each oth-
er. A seed data set (of about 1K transliteration 
pairs) was provided for each language pair, and 
was the only resource to be used for developing a 
mining system.  The participants were expected 
to produce a paired list of source-target single 
word named entities, for every WIL provided. At 
the evaluation time, a random subset of WIL?s 
(about 1K WIL?s) in each language pair were 
hand labeled, and used to test the results pro-
duced by the participants.  
Participants were allowed to use only the 1K 
seed data provided by the organizers to produce 
?standard? results; this restriction is imposed to 
provide a meaningful way of comparing the ef-
                                                 
3 Wikipedia?s Interlanguage Links: 
http://en.wikipedia.org/wiki/Help:Interlanguage_links
.  
fective methods and approaches.  However, 
?non-standard? runs were permitted where par-
ticipants were allowed to use more seed data or 
any language-specific resource available to them. 
2.2 Data Sets for the Task  
The following datasets were used for each lan-
guage pair, for this task.   
 
Training Data  Size Remarks 
Seed Data  
(Parallel 
names) 
~1K Paired names be-
tween source and 
target languages. 
To-be-mined 
Wikipedia In-
ter-Wiki-Link 
Data (Noisy) 
Vari-
able 
Paired named entities 
between source and 
target languages ob-
tained directly from 
Wikipedia 
Test Data 
 
~1K This was a subset of 
Wikipedia Inter-
Wiki-Link data, 
which was hand la-
beled for evaluation. 
Table 2: Datasets created for the shared task 
 
The first two sets were provided by the orga-
nizers to the participants, and the third was used 
for evaluation. 
 
Seed transliteration data:  In addition we pro-
vided approximately 1K parallel names in each 
language pair as seed data to develop any metho-
dology to identify transliterations.  For standard 
run results, only this seed data was to be used, 
though for non-standard runs, more data or other 
linguistics resources were allowed. 
 
English Names Hindi Names 
village ????? 
linden ?????? 
market ??????? 
mysore ????? 
Table 3: Sample English-Hindi seed data 
 
English Names Russian Names 
gregory ???????? 
hudson ?????? 
victor ?????? 
baranowski ??????????? 
Table 4: Sample English-Russian seed data 
 
To-Mine-Data WIL data:  All WIL?s were ex-
tracted from the Wikipedia around January 2010, 
22
and provided to the participants.  The extracted 
names were provided as-is, with no hand verifi-
cation about their correctness, completeness or 
consistency.  As sample of the WIL data for Eng-
lish-Hindi and English-Russian is shown in 
Tables 5 and 6 respectively.  Note that there are 
0, 1 or more single-word transliterations from 
each WIL. 
 
# English Wikipedia  
Title 
Hindi Wikipedia 
Title 
1 Indian National Congress ?????? ????????? ????????? 
2 University of Oxford ????????? ????????????? 
3 Indian Institute of Science 
?????? ??????? 
???????? 
4 Jawaharlal Nehru University 
???????? ????? 
?????????????  
Table 5: English-Hindi Wikipedia title pairs 
 
# English Wikipedia  
Title 
Russian Wikipedia 
Title 
1 Mikhail Gorbachev 
????????, ?????? 
????????? 
2 George Washington ?????????, ??????  
3 Treaty of Versailles ??????????? ??????? 
4 French Republic ??????? 
Table 6: English-Russian Wikipedia title pairs 
Test set:  We randomly selected ~1000 wikipe-
dia links (from the large noisy Inter-wiki-links) 
as test-set, and manually extracted the single 
word transliteration pairs associated with each of 
these WILs.  Please note that a given WIL can 
provide 0, 1 or more single-word transliteration 
pairs.  To keep the task simple, it was specified 
that only those transliterations would be consi-
dered correct that were clear transliterations 
word-per-word (morphological variations one or 
both sides are not considered transliterations) 
These 1K test set was be a subset of Wikipedia 
data provided to the user.  The gold dataset is as 
shown in Tables 7 and 8. 
 
WIL# English Names Hindi Names 
1 Congress ????????? 
2 Oxford ????????? 
3 <Null> <Null> 
4 Jawaharlal ???????? 
4 Nehru ????? 
  Table 7: Sample English-Hindi transliteration 
pairs mined from Wikipedia title pairs 
WIL# English Names Russian Names 
1 Mikhail ?????? 
1 Gorbachev ???????? 
2 George ?????? 
2 Washington ????????? 
3 Versailles ??????????? 
4 <Null> <Null> 
  Table 8: Sample English-Russian translitera-
tion pairs mined from Wikipedia title pairs 
2.3 Evaluation: 
The participants were expected to mine such sin-
gle-word transliteration data for every specific 
WIL, though the evaluation was done only 
against the randomly selected, hand-labeled test 
set.  A participant may submit a maximum of 10 
runs for a given language pair (including a min-
imum of one mandatory ?standard? run).  There 
could be more standard runs, without exceeding 
10 (including the non-standard runs). 
At evaluation time, the task organizers 
checked every WIL in test set from among the 
user-provided results, to evaluate the quality of 
the submission on the 3 metrics described later.  
3 Evaluation Metrics 
We measured the quality of the mining task us-
ing the following measures:  
1. PrecisionCorrectTransliterations(PTrans) 
2. RecallCorrectTransliteration  (RTrans) 
3. F-ScoreCorrectTransliteration (FTrans).   
Please refer to the following figures for the ex-
planations: 
 
A = True Positives (TP) = Pairs that were identi-
fied as "Correct Transliterations" by the partici-
pant and were indeed "Correct Transliterations" 
as per the gold standard 
B = False Positives (FP) = Pairs that were identi-
fied as "Correct Transliterations" by the partici-
pant but they were "Incorrect Transliterations" as 
per the gold standard. 
C = False Negatives (FN) = Pairs that were iden-
tified as "Incorrect Transliterations" by the par-
ticipant but were actually "Correct Translitera-
tions" as per the gold standard. 
D = True Negatives (TN) = Pairs that were iden-
tified as "Incorrect Transliterations" by the par-
ticipant and were indeed "Incorrect Translitera-
tions" as per the gold standard.  
 
23
Figure 1: Overview of the mining task and evaluation 
 
1. RecallCorrectTransliteration  (RTrans) 
The recall was computed using the sample as 
follows: 
?????? =
??
?? + ??
=  
?
? + ?
=  
?
?
 
 
2. PrecisionCorrectTransliteration  (PTrans) 
The precision was computed using the sample as 
follows: 
?????? =
??
?? + ??
=  
?
? + ?
 
 
3. F-Score (F) 
? =
2 ? ?????? ? ??????
?????? + ??????
 
4 Participants & Approaches 
The following 5 teams participated in the Trans-
literation Mining Task?: 
 
# Team Organization 
1   Alberta University of Alberta, Canada 
2   CMIC Cairo Microsoft Innovation  
Centre, Egypt 
3   Groningen University of Groningen,  
Netherlands 
4   IBM Egypt IBM Egypt, Cairo, Egypt 
5   MINT? Microsoft Research India, India 
                                                 
? Non-participating system, included for reference.  
  Table 9: Participants in the Shared Task  
The approaches used by the 4 participating 
groups can be broadly classified as discrimina-
tive and generation based approaches. Discri-
minative approaches treat the mining task as a 
binary classification problem where the goal is to 
build a classifier that identifies whether a given 
pair is a valid transliteration pair or not. Genera-
tion based approaches on the other hand generate 
transliterations for each word in the source title 
and measure their similarity with the candidate 
words in the target title. Below, we give a sum-
mary of the various participating systems. 
The CMIC team (Darwish et. al., 2010) used a 
generative transliteration model (HMM) to trans-
literate each word in the source title and com-
pared the transliterations with the words appear-
ing in the target title. For example, for a given 
word Ei in the source title if the model generates 
a transliteration Fj which appears in the target 
title then (Ei, Fj) are considered as transliteration 
pairs. The results are further improved by using 
phonetic conflation (PC) and iteratively training 
(IterT) the generative model using the mined 
transliteration pairs. For phonetic conflation a 
modified SOUNDEX scheme is used wherein 
vowels are discarded and phonetically similar 
characters are conflated. Both, phonetic confla-
tion and iterative training, led to an increase in 
24
recall which was better than the corresponding 
decline in precision. 
The Alberta team (Jiampojamarn et. al., 2010) 
fielded 5 different systems in the shared task. 
The first system uses a simple edit distance based 
method where a pair of strings is classified as a 
transliteration pair if the Normalized Edit Dis-
tance (NED) between them is above a certain 
threshold. To calculate the NED, the target lan-
guage string is first Romanized by replacing each 
target grapheme by the source grapheme having 
the highest conditional probability. These condi-
tional probabilities are obtained by aligning the 
seed set of transliteration pairs using an M2M-
aligner approach (Jiampojamarn et. al., 2007). 
The second system uses a SVM based discrimin-
ative classifier trained using an improved feature 
representation (BK 2007) (Bergsma and Kon-
drak, 2007). These features include all substring 
pairs up to a maximum length of three as ex-
tracted from the aligned word pairs. The transli-
teration pairs in the seed data provided for the 
shared task were used as positive examples. The 
negative examples were obtained by generating 
all possible source-target pairs in the seed data 
and taking those pairs which are not translitera-
tions but have a longest common subsequence 
ratio above a certain threshold. One drawback of 
this system is that longer substrings cannot be 
used due to the combinatorial explosion in the 
number of unique features as the substring length 
increases. To overcome this problem they pro-
pose a third system which uses a standard n-gram 
string kernel (StringKernel) that implicitly em-
beds a string in a feature space that has one co-
ordinate for each unique n-gram (Shawe-Taylor 
and Cristianini, 2004). The above 3 systems are 
essentially discriminative systems. In addition, 
they propose a generation based approach (DI-
RECTL+) which determines whether the gener-
ated transliteration pairs of a source word and 
target word are similar to a given candidate pair. 
They use a state-of-the-art online discriminative 
sequence prediction model based on many-to-
many alignments, further augmented by the in-
corporation of joint n-gram features (Jiampoja-
marn et. al., 2010). Apart from the four systems 
described above, they propose an additional sys-
tem for English Chinese, wherein they formulate 
the mining task as a matching problem (Match-
ing) and greedily extract the pairs with highest 
similarity. The similarity is calculated using the 
alignments obtained by training a generation 
model (Jiampojamarn et. al., 2007) using the 
seed data. 
The IBM Cairo team (Noemans et. al., 2010) 
proposed a generation based approach which 
takes inspiration from Phrase Based Statistical 
Machine Translation (PBSMT) and learns a cha-
racter-to-character alignment model between the 
source and target language using GIZA++. This 
alignment table is then represented using a finite 
state automaton (FSA) where the input is the 
source character and the output is the target cha-
racter. For a given word in the source title, can-
didate transliterations are generated using this 
FST and are compared with the words in the tar-
get title. In addition they also submitted a base-
line run which used phonetic edit distance. 
The Groningen (Nabende et. al., 2010) team 
used a generation based approach that uses pair 
HMMs (P-HMM) to find the similarity between 
a given pair of source and target strings. The 
proposed variant of pair HMM uses transition 
parameters that are distinct between each of the 
edit states and emission parameters that are also 
distinct. The three edits states are substitution 
state, deletion state and insertion state. The pa-
rameters of the pair HMM are estimated using 
the Baum-Welch Expectation Maximization al-
gorithm (Baum et. al. 1970).  
Finally, as a reference, results of a previously 
published system ? MINT (Udupa et. al., 2009) ? 
were also included in this report as a reference.  
MINT is a large scalable mining system for min-
ing transliterations from comparable corpora, 
essentially multilingual news articles in the same 
timeline.  While MINT takes a two step approach 
? first aligning documents based on content simi-
larity, and subsequently mining transliterations 
based on a name similarity model ? for this task, 
only the transliteration mining step is employed. 
For mining transliterations a logistic function 
based similarity model (LFS) trained discrimina-
tively with the seed parallel names data was em-
ployed.  It should be noted here that the MINT 
algorithm was used as-is for mining translitera-
tions from Wikipedia paired titles, with no fine-
tuning.  While the standard runs used only the 
data provided by the organizers, the non-standard 
runs used about 15K (Seed+) parallel names be-
tween the languages. 
5 Results & Analysis 
The results for EnAr, EnCh, EnHi, EnRu and 
EnTa are summarized in Tables 10, 11, 12, 13 
and 14 respectively. The results clearly indicate 
that there is no single approach which performs 
well across all languages. In fact, there is even 
25
no single genre (discriminative v/s generation 
based) which performs well across all languages. 
We, therefore, do a case by case analysis of the 
results and highlight some important observa-
tions. 
? The discriminative classifier using string 
kernels proposed by Jiampojamarn et. al. 
(2010) consistently performed well in all the 
4 languages that it was tested on. Specifical-
ly, it gave the best performance for EnHi and 
EnTa. 
? The simple discriminative approach based on 
Normalized Edit Distance (NED) gave the 
best result for EnRu. Further, the authors re-
port that the results of StringKernel and BK-
2007 were not significantly better than NED. 
? The use of phonetic conflation consistently 
performed better than the case when phonet-
ic conflation was not used.  
? The results for EnCh are significantly lower 
when compared to the results for other lana-
guge pairs. This shows that mining translite-
ration pairs between alphabetic languages 
(EnRu, EnAr, EnHi, EnTa) is relatively easi-
er as compared to the case when one of the 
languages is non-alphabetic (EnCh) 
6 Plans for the Future Editions 
This shared task was designed as a comple-
mentary shared task to the popular NEWS 
Shared Tasks on Transliteration Generation; suc-
cessful mining of transliteration pairs demon-
strated in this shared task would be a viable 
source for generating data for developing a state 
of the art transliteration generation system.    
We intend to extend the scope of the mining in 
3 different ways: (i) extend mining to more lan-
guage pairs, (ii) allow identification of near 
transliterations where there may be changes do to 
the morphology of the target (or the source) lan-
guages, and, (iii) demonstrate an end-to-end 
transliteration system that may be developed 
starting with a small seed corpora of, say, 1000 
paired names. 
 
 
References  
Baum, L., Petrie, T., Soules, G. and Weiss, N. 1970. A 
Maximization Technique Occurring in the Statis-
tical Analysis of Probabilistic Functions of Markov 
Chains. In The Annals of Mathematical Statistics, 
41 (1): 164-171. 
Bergsma, S. and Kondrak, G. 2007. Alignment Based 
Discriminative String Similarity. In Proceedings of 
the 45th Annual Meeting of the ACL, 2007.  
Darwish, K. 2010. Transliteration Mining with Pho-
netic Conflation and Iterative Training. Proceed-
ings of the 2010 Named Entities Workshop: Shared 
Task on Transliteration Mining, 2010.  
Jiampojamarn, S., Dwyer, K., Bergsma, S., Bhargava, 
A., Dou, Q., Kim, M. Y. and Kondrak, G. 2010. 
Transliteration generation and mining with limited 
training resources. Proceedings of the 2010 
Named Entities Workshop: Shared Task on Trans-
literation Mining, 2010. 
Shawe-Taylor, J and Cristianini, N. 2004. Kernel Me-
thods for Pattern Analysis. Cambridge University 
Press. 
Klementiev, A. and Roth, D. 2006. Weakly supervised 
named entity transliteration and discovery from 
multilingual comparable corpora. Proceedings of 
the 44th Annual Meeting of the ACL, 2006.  
Knight, K. and Graehl, J. 1998. Machine Translitera-
tion. Computational Linguistics.  
Kumaran, A., Khapra, M. and Li, Haizhou. 2010. 
Whitepaper on NEWS 2010 Shared Task on Trans-
literation Mining. Proceedings of the 2010 Named 
Entities Workshop: Shared Task on Transliteration 
Mining, 2010. 
Nabende, P. 2010. Mining Transliterations from Wi-
kipedia using Pair HMMs. Proceedings of the 2010 
Named Entities Workshop: Shared Task on Trans-
literation Mining, 2010. 
Noeman, S. and Madkour, A. 2010. Language inde-
pendent Transliteration mining system using Finite 
State Automata framework. Proceedings of the 
2010 Named Entities Workshop: Shared Task on 
Transliteration Mining, 2010.  
Udupa, R., Saravanan, K., Kumaran, A. and Jagarla-
mudi, J. 2009. MINT: A Method for Effective and 
Scalable Mining of Named Entity Transliterations 
from Large Comparable Corpora. Proceedings of 
the 12th Conference of the European Chapter of 
Association for Computational Linguistics, 2009.  
  
26
Participant Run Type Description Precision Recall F-Score 
IBM Egypt 
 
Standard 
FST, edit distance 2 with nor-
malized characters 0.887 0.945 0.915 
IBM Egypt 
 
Standard 
FST, edit distance 1 with nor-
malized characters 0.859 0.952 0.903 
IBM Egypt 
 
Standard 
Phonetic distance, with norma-
lized characters 0.923 0.830 0.874 
CMIC Standard HMM + IterT 0.886 0.817 0.850 
CMIC Standard HMM + PC 0.900 0.796 0.845 
CMIC Standard (HMM + ItertT) + PC 0.818 0.827 0.822 
Alberta Non- Standard  0.850 0.780 0.820 
Alberta Standard BK-2007 0.834 0.798 0.816 
Alberta Standard NED+ 0.818 0.783 0.800 
CMIC Standard (HMM + PC + ItertT) + PC 0.895 0.678 0.771 
Alberta Standard DirecTL+ 0.861 0.652 0.742 
CMIC Standard HMM 0.966 0.587 0.730 
CMIC Standard HMM + PC + IterT 0.952 0.588 0.727 
IBM Egypt 
 
Standard 
FST, edit distance 2 without 
normalized characters 0.701 0.747 0.723 
IBM Egypt 
 
Standard 
FST, edit distance 1 without 
normalized characters 0.681 0.755 0.716 
IBM Egypt 
 
Standard 
Phonetic distance, without 
normalized characters 0.741 0.666 0.702 
Table 10: Results of the English Arabic task 
 
Participant Run Type Description Precision Recall F-Score 
Alberta Standard Matching 0.698 0.427 0.530 
Alberta Non-Standard  0.700 0.430 0.530 
CMIC Standard (HMM + IterT) + PC 1 0.030 0.059 
CMIC Standard HMM + IterT 1 0.026 0.05 
CMIC Standard HMM + PC 1 0.024 0.047 
CMIC Standard (HMM + PC + IterT) + PC 1 0.022 0.044 
CMIC Standard HMM 1 0.016 0.032 
CMIC Standard HMM + PC + IterT 1 0.016 0.032 
Alberta Standard DirecTL+ 0.045 0.005 0.009 
Table 11: Results of the English Chinese task 
 
Participant Run Type Description Precision Recall F-Score 
MINT? Non-Standard LFS + Seed+ 0.967 0.923 0.944 
Alberta  Standard StringKernel 0.954 0.895 0.924 
Alberta Standard NED+ 0.875 0.941 0.907 
Alberta Standard DirecTL+ 0.945 0.866 0.904 
CMIC Standard (HMM + PC + IterT) + PC 0.953 0.855 0.902 
Alberta Standard BK-2007 0.883 0.880 0.882 
CMIC Standard (HMM + IterT) + PC  0.951 0.812 0.876 
CMIC Standard HMM + PC 0.959 0.786 0.864 
Alberta Non-Standard  0.890 0.820 0.860 
MINT? Standard LFS 0.943 0.780 0.854 
MINT? Standard LFS 0.946 0.773 0.851 
                                                 
? Non-participating system 
27
CMIC Standard HMM + PC + IterT 0.981 0.687 0.808 
CMIC Standard HMM + IterT 0.984 0.569 0.721 
CMIC Standard HMM 0.987 0.559 0.714 
Table 10: Results of the English Hindi task 
 
Participant Run Type Description Precision Recall F-Score 
Alberta Standard NED+ 0.880 0.869 0.875 
CMIC Standard HMM + PC 0.813 0.839 0.826 
MINT? Non-Standard LFS + Seed+ 0.797 0.853 0.824 
Groningen? Standard P-HMM 0.780 0.834 0.806 
Alberta Standard StringKernel 0.746 0.889 0.811 
CMIC Standard HMM 0.868 0.748 0.804 
CMIC Standard HMM + PC + IterT 0.843 0.747 0.792 
Alberta Non-Standard  0.730 0.870 0.790 
Alberta Standard DirecTL+ 0.778 0.795 0.786 
CMIC Standard HMM + IterT 0.716 0.868 0.785 
MINT? Standard LFS 0.822 0.752 0.785 
CMIC Standard (HMM + PC + IterT) + PC 0.771 0.794 0.782 
Alberta Standard BK-2007 0.684 0.902 0.778 
CMIC Standard (HMM + IterT) + PC 0.673 0.881 0.763 
Groningen Standard P-HMM 0.658 0.334 0.444 
Table 11: Results of the English Russian task 
 
Participant Run Type Description Precision Recall F-Score 
Alberta Standard StringKernel 0.923 0.906 0.914 
MINT? Non-Standard LFS + Seed+ 0.910 0.897 0.904 
MINT? Standard LFS 0.899 0.814 0.855 
MINT? Standard LFS 0.913 0.790 0.847 
Alberta Standard BK-2007 0.808 0.852 0.829 
CMIC Standard (HMM + IterT) + PC 0.939 0.741 0.828 
Alberta Non-Standard  0.820 0.820 0.820 
Alberta Standard DirectL+ 0.919 0.710 0.801 
Alberta Standard NED+ 0.916 0.696 0.791 
CMIC Standard HMM + IterT 0.952 0.668 0.785 
CMIC Standard HMM + PC 0.963 0.604 0.743 
CMIC Standard (HMM + PC + IterT) + PC 0.968 0.567 0.715 
CMIC Standard HMM + PC + IterT 0.975 0.446 0.612 
CMIC Standard HMM 0.976 0.407 0.575 
Table 12: Results of the English Tamil task 
 
                                                 
? Non-participating system 
? Post-deadline submission of the participating system 
28
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 29?38,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Whitepaper of NEWS 2010 Shared Task on  
Transliteration Mining 
A Kumaran Mitesh M. Khapra Haizhou Li 
Microsoft Research India 
Bangalore, India 
Indian Institute of Technology-Bombay 
Mumbai, India 
Institute for Infocomm  
Research, Singapore 
 
 
 
Abstract 
Transliteration is generally defined as phonetic 
translation of names across languages. Ma-
chine Transliteration is a critical technology in 
many domains, such as machine translation, 
cross-language information retriev-
al/extraction, etc. Recent research has shown 
that high quality machine transliteration sys-
tems may be developed in a language-neutral 
manner, using a reasonably sized good quality 
corpus (~15-25K parallel names) between a 
given pair of languages.  In this shared task, 
we focus on acquisition of such good quality 
names corpora in many languages, thus com-
plementing the machine transliteration shared 
task that is concurrently conducted in the same 
NEWS 2010 workshop.  Specifically, this task 
focuses on mining the Wikipedia paired enti-
ties data (aka, inter-wiki-links) to produce 
high-quality transliteration data that may be 
used for transliteration tasks. 
1 Task Description  
The task is to develop a system for mining single 
word transliteration pairs from the standard Wi-
kipedia paired topics (aka, Wikipedia Inter-
Language Links, or WIL1) in one or more of the 
specified language pairs. The WIL?s link articles 
on the same topic in multiple languages, and are 
traditionally used as a parallel language resource 
for many NLP applications, such as Machine 
Translation, Crosslingual Search, etc.  Specific 
WIL?s of interest for our task are those that con-
tain proper names ? either wholly or partly ? 
which can yield rich transliteration data.   
Each WIL consists of a topic in the source and 
the language pair, and the task is to identify parts 
of the topic (in the respective language titles) that 
are transliterations of each other. A seed data set 
(of about 1K transliteration pairs) would be pro-
vided for each language pair, and are the only 
resource to be used for developing a mining sys-
tem.  The participants are expected to produce a 
                                                 
1
 Wikipedia?s Interlanguage Links:  
http://en.wikipedia.org/wiki/Help:Interlanguage_links.  
paired list of source-target single word named 
entities, for every WIL provided. At the evalua-
tion time, a random subset of WIL?s (about 1K 
WIL?s) in each language pair that are hand la-
beled would be used to test the results produced 
by the participants.  
Participants may use only the 1K seed data 
provided by the organizers to produce ?standard? 
results; this restriction is imposed to provide a 
meaningful way of comparing the effective me-
thods and approaches.  However, ?non-standard? 
runs would be permitted where participants may 
use more seed data or any language-specific re-
source available to them. 
2 Important Dates  
SHARED TASK SCHEDULES 
Registration Opens  1-Feb-2010 
Registration Closes   13-Mar-2010 
Training Data Release  26 -Feb-2010 
Test Data Release  13-Mar-2010 
Results Submission Due  20-Mar-2010 
Evaluation Results An-
nouncement 27-Mar-2010 
Short Papers Due  5-Apr-2010 
Workshop Paper Sub-
mission Closes  5-Apr-2010 
Workshop & Task Pa-
pers Acceptance  6-May-2010 
CRC Due  15-May-2010 
Workshop Date   16-Jul-2010 
3 Participation 
1. Registration (1 Feb 2010) 
a. Prospective participants are to register to 
the NEWS-2010 Workshop homepage, for 
this specific task. 
2. Training Data Release (26 Feb 2010) 
a. Registered participants are to obtain seed 
and Wikipedia data from the Shared Task 
organizers. 
 
29
3. Evaluation Script (1 March 2010) 
a. A sample submission and an evaluation 
script will be released in due course. 
b. The participants must make sure that their 
output is produced in a way that the evalua-
tion script may run and produce the ex-
pected output. 
c. The same script (with held out test data and 
the user outputs) would be used for final 
evaluation. 
 
4. Testing data (13 March 2010) 
a. The test data would be a held out data of 
approximately 1K ?gold-standard? mined 
data. 
b. The submissions (up to 10) would be tested 
against the test data, and the results pub-
lished. 
 
5. Results (27 March 2010) 
a. On the results announcement date, the 
evaluation results would be published on 
the Workshop website. 
b. Note that only the scores (in respective me-
trics) of the participating systems on each 
language pairs would be published, but no 
explicit ranking of the participating sys-
tems.   
c. Note that this is a shared evaluation task 
and not a competition; the results are meant 
to be used to evaluate systems on common 
data set with common metrics, and not to 
rank the participating systems.  While the 
participants can cite the performance of 
their systems (scores on metrics) from the 
workshop report, they should not use any 
ranking information in their publications. 
d. Further, all participants should agree not to 
reveal identities of other participants in any 
of their publications unless you get permis-
sion from the other respective participants. 
If the participants want to remain anonym-
ous in published results, they should inform 
the organizers at the time of registration.  
Note that the results of their systems would 
still be published, but with the participant 
identities masked. As a result, in this case, 
your organization name will still appear in 
the web site as one of participants, but it is 
not linked explicitly with your results. 
 
6. Short Papers on Task (5 April 2010) 
a. Each submitting site is required to submit a 
4-page system paper (short paper) for its 
submissions, including their approach, data 
used and the results. 
b. All system short papers will be included in 
the proceedings. Selected short papers will 
be presented in the NEWS 2010 workshop.  
Acceptance of the system short-papers 
would be announced together with that of 
other papers. 
4 Languages Involved  
The task involves transliteration mining in the 
language pairs summarized in the following ta-
ble.   
   
Source Lan-
guage 
Target Lan-
guage 
Track ID 
English  Chinese  WM-EnCn 
English  Hindi  WM-EnHi 
English  Tamil WM-EnTa 
English  Russian  WM-EnRu 
English Arabic WM-EnAr 
Table 1: Language Pairs in the shared task 
5 Data Sets for the Task  
The following datasets are used for each lan-
guage pair, for this task.   
 
Training Data  Size Remarks 
Seed Data (Pa-
rallel) 
~1K Paired names be-
tween source and 
target languages. 
To-be-mined 
Wikipedia Inter-
Wiki-Link Data 
(Noisy) 
Vari-
able 
Paired named entities 
between source and 
target languages ob-
tained directly from 
Wikipedia 
Test Data 
 
~1K This is a subset of 
Wikipedia Inter-
Wiki-Link data, 
which will be hand 
labeled. 
Table 2: Datasets for the shared task 
The first two sets would be provided by the or-
ganizers to the participants, and the third will be 
used for evaluation. 
 
To-Mine-Data WIL data:  All WIL?s from an 
appropriate download from Wikipedia would be 
provided.  The WIL data might look like the 
samples shown in Tables 3 and 4, with the sin-
30
gle-word transliterations highlighted.  Note that 
there could be 0, 1 or more single-word translite-
rations from each WIL. 
 
# English Wikipedia  
Title 
Hindi Wikipedia 
Title 
1 Indian National Congress ?????? ????????? ????????? 
2 University of Oxford ????????? 
????????????? 
3 Indian Institute of Science ?????? ??????? 
???????? 
4 Jawaharlal Nehru Univer-
sity 
???????? ????? 
?????????????  
Table 3: Sample English-Hindi Wikipedia title 
pairs 
 
# English Wikipedia  
Title 
Russian Wikipedia 
Title 
1 Mikhail Gorbachev ????????, ?????? 
????????? 
2 George Washington ?????????, ??????  
3 Treaty of Versailles ??????????? ??????? 
4 French Republic ??????? 
Table 4: Sample English-Russian Wikipedia title 
pairs 
Seed transliteration data:  In addition we pro-
vide approximately 1K parallel names in each 
language pair as seed data to develop any metho-
dology to identify transliterations.  For standard 
run results, only this seed data could be used, 
though for non-standard runs, more data or other 
linguistics resources may be used. 
English Names Hindi Names 
Village ????? 
Linden ??????? 
Market ????? 
Mysore ????? 
Table 5: Sample English-Hindi seed data 
 
English Names Russian Names 
Gregory ???????? 
Hudson ?????? 
Victor ?????? 
baranowski ??????????? 
Table 6: Sample English-Russian seed data 
 
Test set:  We plan to randomly select ~1000 wi-
kipedia links (from the large noisy Inter-wiki-
links) as test-set, and manually extract the single 
word transliteration pairs associated with each of 
these WILs.  Please note that a given WIL can 
provide 0, 1 or more single-word transliteration 
pairs.  To keep the task simple, we consider as 
correct transliterations only those that are clear 
transliterations word-per-word (morphological 
variations one or both sides are not considered 
transliterations) These 1K test set will be a subset 
of Wikipedia data provided to the user.  The gold 
dataset might look like the following (assuming 
the items 1, 2, 3 and 4 in Tables 3 and 4 were 
among the randomly selected WIL?s from To-
Mine-Data).   
 
WIL# English Names Hindi Names 
1 Congress ????????? 
2 Oxford ????????? 
3 <Null> <Null> 
4 Jawaharlal ???????? 
4 Nehru ????? 
  Table 7: Sample English-Hindi transliteration 
pairs mined from Wikipedia title pairs 
 
WIL# English Names Russian Names 
1 Mikhail ?????? 
1 Gorbachev ???????? 
2 George ?????? 
2 Washington ????????? 
3 Versailles ??????????? 
4 <Null> <Null> 
  Table 8: Sample English-Russian translitera-
tion pairs mined from Wikipedia title pairs 
 
Evaluation: The participants are expected to 
mine such single-word transliteration data for 
every specific WIL, though the evaluation would 
be done only against the randomly selected, 
hand-labeled test set.  At evaluation time, the 
task organizers check every WIL in test set from 
among the user-provided results, to evaluate the 
quality of the submission on the 3 metrics de-
scribed later.  
Additional information on data use: 
1. Seed data may have ownership and appropri-
ate licenses may need to be procured for use.  
2. To-be-mined Wikipedia data is extracted 
from Wikipedia (in Jan/Feb 2010), and dis-
tributed as-is.  No assurances that they are 
correct, complete or consistent. 
 
31
Figure 1: Overview of the mining task and evaluation 
 
3. The hand-labeled test set is created by 
NEWS shared task organizers, and will be 
used for computing the metrics for a given 
submission. 
4. We expect that the participants to use only 
the seed data (parallel names) provided by 
the Shared Task for a standard run to ensure 
a fair evaluation and a meaningful compari-
son between the effectiveness of approaches 
taken by various systems.  At least one such 
run (using only the data provided by the 
shared task) is mandatory for all participants 
for a given task that they participate in.   
5. If more data (either parallel names data or 
monolingual data), or any language-specific 
modules were used, then all such runs using 
extra data or resources must be marked as 
?Non-standard?.  For such non-standard 
runs, it is required to disclose the size and 
characteristics of the data or the nature of 
languages resources used, in their paper. 
6. A participant may submit a maximum of 10 
runs for a given language pair (including one 
or more ?standard? run).  There could be 
more standard runs, without exceeding 10 
(including the non-standard runs). 
6 Paper Format 
All paper submissions to NEWS 2010 should 
follow the ACL 2010 paper submission policy 
(http://acl2010.org/papers.html), including paper 
format, blind review policy and title and author 
format convention. Shared task system short pa-
pers are also in two-column format without ex-
ceeding four (4) pages plus any extra page for 
references. However, there is no need for double-
blind requirements, as the users may refer to 
their runs and metrics in the published results.   
7 Evaluation Metrics 
We plan to measure the quality of the mining 
task using the following measures:  
 
1. PrecisionCorrectTransliterations (PTrans) 
2. RecallCorrectTransliteration (RTrans) 
3. F-ScoreCorrectTransliteration (FTrans).   
 
Please refer to the following figures for the ex-
planations: 
 
A = True Positives (TP) = Pairs that were identi-
fied as "Correct Transliterations" by the partici-
pant and were indeed "Correct Transliterations" 
as per the gold standard 
B = False Positives (FP) = Pairs that were identi-
fied as "Correct Transliterations" by the partici-
pant but they were "Incorrect Transliterations" as 
per the gold standard. 
C = False Negatives (FN) = Pairs that were iden-
tified as "Incorrect Transliterations" by the par-
ticipant but were actually "Correct Translitera-
tions" as per the gold standard. 
32
D = True Negatives (TN) = Pairs that were iden-
tified as "Incorrect Transliterations" by the par-
ticipant and were indeed "Incorrect Translitera-
tions" as per the gold standard. 
 
1. RecallCorrectTransliteration  (RTrans) 
The recall is going to be computed using the 
sample as follows: 
?????? =
??
?? + ??
=  
?
? + ?
=  
?
?
 
 
2. PrecisionCorrectTransliteration  (PTrans) 
The precision is going to be computed using the 
sample as follows: 
?????? =
??
?? + ??
=  
?
? + ?
 
3. F-Score (F) 
? =
2 ? ?????? ? ??????
?????? + ??????
 
8 Contact Us 
If you have any questions about this share task 
and the database, please contact one of the orga-
nizers below: 
 
Dr. A. Kumaran 
 Microsoft Research India 
Bangalore 560080 INDIA 
a.kumaran@microsoft.com 
 
Mitesh Khapra  
 Indian Institute of Technology-Bombay 
 Mumbai, INDIA 
MKhapra@cse.iitb.ac.in.  
 
Dr Haizhou Li 
 Institute for Infocomm Research 
 Singapore, SINGAPORE 138632 
hli@i2r.a-star.edu.sg.  
  
33
Appendix A: Seed Parallel Names Data  
 
? File Naming Conventions: 
o NEWS09_Seed_XXYY_1K.xml,  
? XX: Source Language 
? YY: Target Language 
? 1K: number of parallel names  
 
? File Formats:  
o All data would be made available in XML formats (Appendix A). 
 
? Data Encoding Formats:  
o The data would be in Unicode, in UTF-8 encoding.  The results are expected to be 
submitted in UTF-8 format only, and in the XML format specified. 
 
File: NEWS2009_Seed_EnHi_1000.xml 
 
<?xml version="1.0" encoding="UTF-8"?> 
 <SeedCorpus 
      CorpusID = "NEWS2009-Seed-EnHi-1K" 
     SourceLang = "English" 
     TargetLang = "Hindi" 
     CorpusType = "Seed" 
     CorpusSize = "1000" 
     CorpusFormat = "UTF8"> 
  <Name ID=?1?> 
   <SourceName>eeeeee1</SourceName> 
   <TargetName ID="1">hhhhhh1_1</TargetName> 
   <TargetName ID="2">hhhhhh1_2</TargetName> 
   ... 
   <TargetName ID="n">hhhhhh1_n</TargetName> 
  </Name> 
  <Name ID=?2?> 
   <SourceName>eeeeee2</SourceName> 
   <TargetName ID="1">hhhhhh2_1</TargetName> 
   <TargetName ID="2">hhhhhh2_2</TargetName> 
   ... 
   <TargetName ID="m">hhhhhh2_m</TargetName> 
  </Name> 
... 
  <!-- rest of the names to follow --> 
  ... 
 </SeedCorpus> 
 
 
Appendix B: Wikipedia InterwikiLinks Data  
 
? File Naming Conventions: 
o NEWS09_Wiki_XXYY_nnnn.xml,  
? XX: Source Language 
? YY: Target Language 
? nnnn: size of paired entities culled from Wikipedia (?25K?, ?10000?, etc.) 
? File Formats:  
o All data would be made available in XML formats (Appendix A). 
? Data Encoding Formats:  
o The data would be in Unicode, in UTF-8 encoding.  The results are expected to be 
submitted in UTF-8 format only, and in the XML format specified. 
 
 
34
File: NEWS2009_Wiki_EnHi_10K.xml 
<?xml version="1.0" encoding="UTF-8"?> 
 <WikipediaCorpus 
      CorpusID = "NEWS2009-Wiki-EnHi-10K" 
     SourceLang = "English" 
     TargetLang = "Hindi" 
     CorpusType = "Wiki" 
     CorpusSize = "10000" 
     CorpusFormat = "UTF8"> 
  <Title ID=?1?> 
   <SourceEntity>e1 e2 ? en</SourceEntity> 
   <TargetEntity>h1 h2 ? hm</TargetEntity> 
  </Title> 
  <Title ID=?2?> 
   <SourceEntity>e1 e2 ? ei</SourceEntity> 
   <TargetEntity>h1 h2 ? hj</TargetEntity> 
  </Title> 
... 
  <!-- rest of the titles to follow --> 
  ... 
 </ WikipediaCorpus> 
 
 
Appendix C: Results Submission - Format 
 
? File Naming Conventions: 
o NEWS09_Result_XXYY_gggg_nn_description.xml 
? XX: Source 
? YY: Target 
? gggg: Group ID 
? nn: run ID.  
? description: Description of the run 
? File Formats:  
o All results would be submitted in XML formats (Appendix B). 
? Data Encoding Formats:  
o The data would be in Unicode, in UTF-8 encoding.  The results are expected to be 
submitted in UTF-8 format only. 
Example: NEWS2009_EnHi_TUniv_01_HMMBased.xml 
 
<?xml version="1.0" encoding="UTF-8"?> 
 <WikipediaMiningTaskResults 
      SourceLang = "English" 
     TargetLang = "Hindi" 
     GroupID = "Trans University" 
     RunID = "1" 
     RunType = "Standard" 
    Comments = "SVD Run with params: alpha=xxx beta=yyy"> 
  <Title ID="1"> 
   <MinedPair ID="1"> 
<SourceName>e1</SourceName> 
    <TargetName>h1</TargetName> 
</MinedPair> 
   <MinedPair ID="2"> 
<SourceName>e2</SourceName> 
    <TargetName>h2</TargetName> 
</MinedPair> 
    <!?followed by other pairs mined from this title--> 
  </Title> 
  <Title ID="2"> 
   <MinedPair ID="1"> 
<SourceName>e1</SourceName> 
    <TargetName>h1</TargetName> 
</MinedPair> 
35
   <MinedPair ID="2"> 
<SourceName>e2</SourceName> 
    <TargetName>h2</TargetName> 
</MinedPair> 
   <!?followed by other pairs mined from this title--> 
  </Title> 
... 
  <!-- All titles in the culled data to follow --> 
  ... 
 </WikipediaMiningTaskResults> 
 
 
Appendix D: Sample Eng-Hindi Interwikilink Data 
 
<?xml version="1.0" encoding="UTF-8"?>  
<WikipediaCorpus CorpusID = "NEWS2009-Wiki-EnHi-Sample"  
SourceLang = "English"  
TargetLang = "Hindi"  
CorpusType = "Wiki" CorpusSize = "3" 
 CorpusFormat = "UTF8"> 
  <Title ID="1"> 
  <SourceEntity>Indian National Congress</SourceEntity> 
  <TargetEntity>?????? ????????? ?????????</TargetEntity> 
 </Title> 
<!-- {Congress, ?????????} should be identified by the paricipants--> 
 <Title ID="2"> 
  <SourceEntity>University of Oxford</SourceEntity> 
  <TargetEntity>????????? ?????????????</TargetEntity> 
 </Title> 
<!-- {Oxford, ?????????} should be identified by the paricipants--> 
 <Title ID="3"> 
  <SourceEntity>Jawaharlal Nehru University</SourceEntity> 
  <TargetEntity>???????? ????? ?????????????</TargetEntity> 
 </Title> 
<!-- {Jawaharlal, ????????} and {Nehru, ?????} should be  
identified by the paricipants--> 
 <Title ID="4"> 
  <SourceEntity>Indian Institute Of Science</SourceEntity> 
  <TargetEntity>?????? ??????? ????????</TargetEntity> 
 </Title> 
<!--There are no transliteration pairs here --> 
</WikipediaCorpus> 
 
 
Appendix E: Eng-Hindi Gold Mined Data (wrt the above WIL Data) 
 
<?xml version="1.0" encoding="UTF-8"?> 
<WikipediaMiningTaskResults 
 SourceLang = "English" 
 TargetLang = "Hindi" 
 GroupID = "Gold-Standard" 
 RunID = "" 
 RunType = "" 
Comments = ""> 
 <Title ID="1"> 
  <MinedPair ID="1"> 
   <SourceName>Congress</SourceName> 
   <TargetName> ?????????</TargetName> 
  </MinedPair> 
 </Title> 
 <Title ID="2"> 
  <MinedPair ID="1"> 
36
   <SourceName>Oxford</SourceName> 
   <TargetName> ?????????</TargetName> 
  </MinedPair> 
 </Title> 
 <Title ID="3"> 
  <MinedPair ID="1"> 
   <SourceName>Jawaharlal</SourceName> 
   <TargetName> ????????</TargetName> 
  </MinedPair> 
  <MinedPair ID="2"> 
   <SourceName>Nehru</SourceName> 
   <TargetName> ?????</TargetName> 
  </MinedPair> 
 </Title> 
 <Title ID="4"> 
 </Title> 
</WikipediaMiningTaskResults> 
 
 
Appendix F: English-Hindi Sample Submission and Evaluation 
 
<?xml version="1.0" encoding="UTF-8"?> 
<WikipediaMiningTaskResults 
 SourceLang = "English" 
 TargetLang = "Hindi" 
 GroupID = "Gold-Standard" 
 RunID = "" 
 RunType = "" 
 <Title ID="1"> 
  <MinedPair ID="1"> 
   <SourceName>Congress</SourceName> 
   <TargetName> ?????????</TargetName> 
  </MinedPair> 
The participant mined all correct transliteration pairs  
 </Title> 
 <Title ID="2"> 
  <MinedPair ID="1"> 
   <SourceName>Oxford</SourceName> 
   <TargetName> ?????????</TargetName> 
  </MinedPair> 
  <MinedPair ID="1"> 
   <SourceName>University</SourceName> 
   <TargetName>?????????????</TargetName> 
  </MinedPair> 
The participant mined an incorrect transliteration pair {University,?????????????} 
 </Title> 
 <Title ID="3"> 
  <MinedPair ID="1"> 
   <SourceName>Jawaharlal</SourceName> 
   <TargetName> ????????</TargetName> 
  </MinedPair> 
The participant missed the correct transliteration pair {Nehru, ?????} 
 </Title> 
 <Title ID="4"> 
  <MinedPair ID="1"> 
   <SourceName>Indian</SourceName> 
   <TargetName>??????</TargetName> 
  </MinedPair> 
The participant mined an incorrect transliteration pair {Indian, ??????} 
 </Title> 
</WikipediaMiningTaskResults> 
 
37
Sample Evaluation 
T = |{(Congress, ?????????), (Oxford, ?????????), (Jawaharlal, ????????),(Nehru, ?????)} | = 4 
A = TP = | {(Congress, ?????????), (Oxford, ?????????), (Jawaharlal, ????????)}| = 3 
B = FP = |{(Indian, ??????), (University, ?????????????) }| = 2 
C = FN = |{(Nehru, ?????)}| = 1 
 
?????? =
??
?? + ??
=  
?
? + ?
=  
?
?
=  
3
4
= 0.75 
 
?????? =
??
??+??
=  
?
?+?
=  
3
5
= 0.60 
 
? =
2 ? ?????? ? ??????
?????? + ??????
=  
2 ? 0.6 ? 0.75
0.6 + 0.75
=  0.67 
 
38
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 70?78,
Beijing, August 2010
More Languages, More MAP?: A Study of Multiple Assisting Languages
in Multilingual PRF
Vishal Vachhani Manoj K. Chinnakotla Mitesh M. Khapra Pushpak Bhattacharyya
Department of Computer Science and Engineering,
Indian Institute of Technology Bombay
{vishalv,manoj,miteshk,pb}@cse.iitb.ac.in
Abstract
Multilingual Pseudo-Relevance Feedback
(MultiPRF) is a framework to improve
the PRF of a source language by taking
the help of another language called as-
sisting language. In this paper, we ex-
tend the MultiPRF framework to include
multiple assisting languages. We consider
three different configurations to incorpo-
rate multiple assisting languages - a) Par-
allel - all assisting languages combined
simultaneously b) Serial - assisting lan-
guages combined in sequence one after
another and c) Selective - dynamically se-
lecting the best feedback model for each
query. We study their effect on MultiPRF
performance. Results using multiple as-
sisting languages are mixed and it helps in
boosting MultiPRF accuracy only in some
cases. We also observe that MultiPRF be-
comes more robust with increase in num-
ber of assisting languages.
1 Introduction
Pseudo-Relevance Feedback (PRF) (Buckley et
al., 1994; Xu and Croft, 2000; Mitra et al, 1998)
is known to be an effective technique to im-
prove the effectiveness of Information Retrieval
(IR) systems. In PRF, the top ?k? documents
from the ranked list retrieved using the initial key-
word query are assumed to be relevant. Later,
these documents are used to refine the user query
and the final ranked list is obtained using the
above refined query. Although PRF has been
shown to improve retrieval, it suffers from the
following drawbacks: (a) Lexical and Semantic
Non-Inclusion: the type of term associations ob-
tained for query expansion is restricted to only
co-occurrence based relationships in the feedback
documents and (b) Lack of Robustness: due to
the inherent assumption in PRF, i.e., relevance
of top k documents, performance is sensitive to
that of the initial retrieval algorithm and as a re-
sult is not robust. Typically, larger coverage en-
sures higher proportion of relevant documents in
the top k retrieval (Hawking et al, 1999). How-
ever, some resource-constrained languages do not
have adequate information coverage in their own
language. For example, languages like Hungarian
and Finnish have meager online content in their
own languages.
Multilingual Pseudo-Relevance Feedback
(MultiPRF) (Chinnakotla et al, 2010a) is a
novel framework for PRF to overcome the above
limitations of PRF. It does so by taking the help of
a different language called the assisting language.
Thus, the performance of a resource-constrained
language could be improved by harnessing the
good coverage of another language. MulitiPRF
showed significant improvements on standard
CLEF collections (Braschler and Peters, 2004)
over state-of-art PRF system. On the web, each
language has its own exclusive topical coverage
besides sharing a large number of common topics
with other languages. For example, information
about Saudi Arabia government policies and
regulations is more likely to be found in Arabic
language web and also information about a local
event in Spain is more likely to be covered in
Spanish web than in English. Hence, using
multiple languages in conjunction is more likely
to ensure satisfaction of the user information need
and hence will be more robust.
In this paper, we extend the MultiPRF frame-
work to multiple assisting languages. We study
70
the various possible ways of combining the mod-
els learned from multiple assisting languages. We
propose three different configurations for includ-
ing multiple assisting languages in MultiPRF - a)
Parallel b) Serial and c) Selective. In Parallel com-
bination, all the assisting languages are combined
simultaneously using interpolation. In Serial con-
figuration, the assisting languages are applied in
sequence one after another and finally, in Selec-
tive configuration, the best feedback model is dy-
namically chosen for each query. We experiment
with each of the above configurations and present
both quantitative and qualitative analysis of the re-
sults. Results using multiple assisting languages
are mixed and it helps in boosting MultiPRF ac-
curacy only in some cases. We also observe that
MultiPRF becomes more robust with increase in
number of assisting languages. Besides, we also
study the relation between number of assisting
languages, coverage and the MultiPRF accuracy.
The paper is organized as follows: Section 2,
explains the Language Modeling (LM) based PRF
approach. Section 3, describes the MultiPRF ap-
proach. Section 4 explains the various configu-
rations to extend MultiPRF for multiple assisting
languages. Section 6 presents the results and dis-
cussions. Finally, Section 7 concludes the paper.
2 PRF in the LM Framework
The Language Modeling (LM) Framework allows
PRF to be modeled in a principled manner. In the
LM approach, documents and queries are mod-
eled using multinomial distribution over words
called document language model P (w|D) and
query language model P (w|?Q) respectively. For
a given query, the document language models are
ranked based on their proximity to the query lan-
guage model, measured using KL-Divergence.
KL(?Q||D) =
?
w
P (w|?Q) ? log
P (w|?Q)
P (w|D)
Since the query length is short, it is difficult to es-
timate ?Q accurately using the query alone. In
PRF, the top k documents obtained through the
initial ranking algorithm are assumed to be rele-
vant and used as feedback for improving the es-
timation of ?Q. The feedback documents con-
tain both relevant and noisy terms from which
Symbol Description
?Q Query Language Model
?FL1 Feedback Language Model obtained from PRF in L1
?FL2 Feedback Language Model obtained from PRF in L2
?TransL1 Feedback Model Translated from L2 to L1
t(f |e) Probabilistic Bi-Lingual Dictionary from L2 to L1
?, ? Interpolation coefficients coefficients used in MultiPRF
Table 1: Glossary of Symbols used in explaining MultiPRF
the feedback language model is inferred based on
a Generative Mixture Model (Zhai and Lafferty,
2001).
Let DF = {d1, d2, . . . , dk} be the top k doc-
uments retrieved using the initial ranking algo-
rithm. Zhai and Lafferty (Zhai and Lafferty, 2001)
model the feedback document setDF as a mixture
of two distributions: (a) the feedback language
model and (b) the collection model P (w|C). The
feedback language model is inferred using the EM
Algorithm (Dempster et al, 1977), which itera-
tively accumulates probability mass on the most
distinguishing terms, i.e. terms which are more
frequent in the feedback document set than in the
entire collection. To maintain query focus the fi-
nal converged feedback model, ?F is interpolated
with the initial query model ?Q to obtain the final
query model ?Final.
?Final = (1? ?) ??Q + ? ??F
?Final is used to re-rank the corpus using the
KL-Divergence ranking function to obtain the fi-
nal ranked list of documents. Henceforth, we refer
to the above technique as Model Based Feedback
(MBF).
3 Multilingual Pseudo-Relevance
Feedback (MultiPRF)
Chinnakotla et al (Chinnakotla et al, 2010a;
Chinnakotla et al, 2010b) propose the MultiPRF
approach which overcomes the fundamental limi-
tations of PRF with the help of an assisting collec-
tion in a different language. Given a query Q in
the source language L1, it is automatically trans-
lated into the assisting language L2. The docu-
ments in the L2 collection are ranked using the
query likelihood ranking function (John Lafferty
and Chengxiang Zhai, 2003). Using the top k doc-
uments, they estimate the feedback model using
MBF as described in the previous section. Simi-
larly, they also estimate a feedback model using
71
the original query and the top k documents re-
trieved from the initial ranking in L1. Let the re-
sultant feedback models be ?FL2 and ?FL1 respec-tively. The feedback model estimated in the as-
sisting language ?FL2 is translated back into lan-guage L1 using a probabilistic bi-lingual dictio-
nary t(f |e) from L2 ? L1 as follows:
P (f |?TransL1 ) =
?
? e in L2
t(f |e) ? P (e|?FL2) (1)
The probabilistic bi-lingual dictionary t(f |e) is
learned from a parallel sentence-aligned corpora
in L1 ? L2 based on word level alignments. The
probabilistic bi-lingual dictionary acts as a rich
source of morphologically and semantically re-
lated feedback terms. Thus, the translation model
adds related terms in L1 which have their source
as the term from feedback model ?FL2 . The finalMultiPRF model is obtained by interpolating the
above translated feedback model with the original
query model and the feedback model of language
L1 as given below:
?MultiL1 = (1? ? ? ?) ??Q + ? ??FL1 + ? ??TransL1(2)
In order to retain the query focus during back
translation, the feedback model in L2 is interpo-
lated with the translated query before translation
of the L2 feedback model. The parameters ? and
? control the relative importance of the original
query model, feedback model of L1 and the trans-
lated feedback model obtained from L1 and are
tuned based on the choice of L1 and L2.
4 Extending MultiPRF to Multiple
Assisting Languages
In this section, we extend the MultiPRF model
described earlier to multiple assisting languages.
Since each language produces a different feed-
back model, there could be different ways of com-
bining these models as suggested below.
Parallel: One way is to include the new assist-
ing language model using one more interpo-
lation coefficient which gives the effect of us-
ing multiple assisting languages in parallel.
Serial: Alternately, we can have a serial combi-
nation wherein language L2 is first assisted
Initial Retrieval(LM Based Query Likelihood)
Top ?k?Results
PRF(Model Based Feedback)
L  Index
Final Ranked List Of Documents in L
Feedback ModelInterpolation
Relevance ModelTranslation 
KL-Divergence Ranking Function
Feedback Model  ?L1Feedback Model ?L
Query in L Translated Query to L1
ProbabilisticDictionaryL1? LQuery Model ?Q
Translated Query to LnInitial Retrieval(LM Based Query Likelihood)
Top ?k?Results
PRF(Model Based Feedback)
L1 Index
Relevance ModelTranslation 
Feedback Model  ?Ln
Initial Retrieval
Top ?k?Results
PRF(Model Based Feedback)
LnIndex
ProbabilisticDictionaryLn? L
Figure 1: Schematic of the Multilingual PRF Approach Us-
ing Parallel Assistance
Initial Retrieval Algorithm(LM Based Query Likelihood)
Top ?k?Results
PRF(Model Based Feedback)
FeedbackModel Interpolation
KL-Divergence Ranking Function
L Index
Initial Retrieval Algorithm(LM Based Query Likelihood)
Top ?k? Results
PRF(Model Based Feedback)
L2  Index
Relevance ModelTranslation 
L1 Index
Feedback Model    ?L1
Query in L1
Initial Retrieval Algorithm(LM Based Query Likelihood)
Top ?k? Results
PRF(Model Based Feedback)
FeedbackModel Interpolation
Feedback Model    ?L2
Query in L2
Top ?k? ResultsPRF(Model Based Feedback) KL Divergence Ranking
ProbabilisticDictionaryL2 ? L1
Relevance ModelTranslation 
ProbabilisticDictionaryL1? L
Feedback Model    ?LQuery Model ?Q
Figure 2: Schematic of the Multilingual PRF Approach Us-
ing Serial Assistance
by language L3 and then this MultiPRF sys-
tem is used to assist the source language L1.
Selective: Finally, we can have selective assis-
tance wherein we dynamically select which
assisting language to use based on the input
query.
Below we describe each of these systems in detail.
4.1 Parallel Combination
The MultiPRF model as explained in section 3 in-
terpolates the query model of L1 with the MBF
of L1 and the translated feedback model of the
assisting language L2. The most natural exten-
sion to this approach is to translate the query into
multiple languages instead of a single language
and collect the feedback terms from the initial re-
72
Language CLEF Collection Identifier Description
No. of 
Documents
No. of Unique 
Terms CLEF Topics (No. of Topics)
English EN-02+03 LA Times 94, Glasgow Herald 95 169477 234083 91-200 (67)
French FR-02+03 Le Monde 94, French SDA 94-95 129806 182214 91-200 (67)
German DE-02+03 Frankfurter Rundschau 94, Der Spiegel 94-95, German SDA 94-95 294809 867072 91-200 (67)
Finnish FI-02+03 Aamulehti 94-95 55344 531160 91-200 (67)
Dutch NL-02+03 NRC Handelsblad 94-95, Algemeen Dagblad 94-95 190604 575582 91-200 (67)
Spanish ES-02+03 EFE 94, EFE 95 454045 340250 91-200 (67)
Table 2: Details of the CLEF Datasets used for Evaluating the MultiPRF approach. The number shown in brackets of the final
column CLEF Topics indicate the actual number of topics used during evaluation.
trieval of each of these languages. The translated
feedback models resulting from each of these re-
trievals can then be interpolated to get the final
parallel MultiPRF model. Specifically, if L1 is the
source language and L2, L3, . . . Ln are assisting
languages then final parallel MultiPRF model can
be obtained by generalizing Equation 2 as shown
below:
?MultiAssistL1 = (1? ? ?
X
i
?i) ??Q + ? ??F +
X
i
?i ??TransLi
(3)
The schematic representation of parallel combina-
tion is shown in Figure 1.
4.2 Serial Combination
Let L1 be the source language and let L2 and L3
be two assisting languages. A serial combination
can then be achieved by cascading two MultiPRF
systems as described below:
1. Construct a MultiPRF system with L2 as
the source language and L3 as the assist-
ing language. We call this system as L2L3-
MultiPRF system.
2. Next, construct a MultiPRF system with L1
as the source language and L2L3-MultiPRF
as the assisting system.
As compared to a single assistance system where
only L2 is used as the assisting language for
L1, here the performance of language L2 is first
boosted using L3 as the assisting language. This
boosted system is then used for assisting L1. Also
note that unlike parallel assistance here we do
not introduce an extra interpolation co-efficient in
the original MultiPRF model given in Equation 2.
The schematic representation of serial combina-
tion is shown in Figure 2.
4.3 Selective Assistance
We motivate selective assistance by posing the
following question: ?Given a source language
L1 and two assisting languages L2 and L3, is
it possible that L2 is ideal for assisting some
queries whereas L3 is ideal for assisting some
other queries?? For example, suppose L2 has a
rich collection of TOURISM documents whereas
L3 has a rich collection of HEALTH documents.
Now, given a query pertaining to TOURISM do-
main one might expect L2 to serve as a better as-
sisting language whereas given a query pertaining
to the HEALTH domain one might expect L3 to
serve as a better assisting language. This intuition
can be captured by suitably changing the interpo-
lation model as shown below:
?BestL = SelectBestModel(?
F
L ,?
Trans
L1 ,?
Trans
L2 ,?
Trans
L12 )
?MultiL1 = (1? ?) ??Q + ? ??
Best
L (4)
where, SelectBestModel() gives the best
model for a particular query using the algorithm
mentioned below which is based on minimizing
the query drift as described in (?):
1. Obtain the four feedback models, viz.,
?FL ,?TransL1 ,?
Trans
L2 ,?
Trans
L12
2. Build a language model (say, LM ) using
queryQ and top-100 documents of initial re-
trieval in language L.
3. Find the KL-Divergence between LM and
the four models obtained during step 1.
4. Select the model which has minimum KL-
Divergence score from LM . Call this model
?BestL .
5. Get the final model by interpolating the
query model, ?Q, with ?BestL .
73
5 Experimental Setup
We evaluate the performance of our system us-
ing the standard CLEF evaluation data in six lan-
guages, widely varying in their familial relation-
ships - Dutch, German, English, French, Spanish
and Finnish. The details of the collections and
their corresponding topics used for MultiPRF are
given in Table 2. Note that, in each experiment,
we choose assisting collections such that the top-
ics in the source language are covered in the as-
sisting collection so as to get meaningful feedback
terms. In all the topics, we only use the title field.
We ignore the topics which have no relevant docu-
ments as the true performance on those topics can-
not be evaluated.
We use the Terrier IR platform (Ounis et al,
2005) for indexing the documents. We perform
standard tokenization, stop word removal and
stemming. We use the Porter Stemmer for English
and the stemmers available through the Snowball
package for other languages. Other than these,
we do not perform any language-specific process-
ing on the languages. In case of French, since
some function words like l?, d? etc., occur as pre-
fixes to a word, we strip them off during index-
ing and query processing, since it significantly im-
proves the baseline performance. We use standard
evaluation measures like MAP, P@5 and P@10
for evaluation. Additionally, for assessing robust-
ness, we use the Geometric Mean Average Preci-
sion (GMAP) metric (Robertson, 2006) which is
also used in the TREC Robust Track (Voorhees,
2006). The probabilistic bi-lingual dictionary
used in MultiPRF was learnt automatically by run-
ning GIZA++: a word alignment tool (Och and
Ney, 2003) on a parallel sentence aligned corpora.
For all the above language pairs we used the Eu-
roparl Corpus (Philipp, 2005). We use Google
Translate as the query translation system as it has
been shown to perform well for the task (Wu et
al., 2008). We use two-stage Dirichlet smooth-
ing with the optimal parameters tuned based on
the collection (Zhai and Lafferty, 2004). We tune
the parameters of MBF, specifically ? and ?, and
choose the values which give the optimal perfor-
mance on a given collection. We observe that the
optimal parameters ? and ? are uniform across
collections and vary in the range 0.4-0.48. We
Source
Langs
Assist.
Langs
MBF MultiPRF
(L1)
MultiPRF
(L2)
MultiPRF
(L1,L2)
EN
DE-NL
MAP 0.4495 0.4464 0.4471 0.4885(4.8)?
P@5 0.4955 0.4925 0.5045 0.5164(2.4)
P@10 0.4328 0.4343 0.4373 0.4463(2.1)
DE-FI
MAP 0.4495 0.4464 0.4545 0.4713(3.7)?
P@5 0.4955 0.4925 0.5194 0.5224(1.2)
P@10 0.4328 0.4343 0.4373 0.4507(3.1)
NL-ES
MAP 0.4495 0.4471 0.4566 0.4757(4.2)?
P@5 0.4955 0.5045 0.5164 0.5224(0.6)
P@10 0.4328 0.4373 0.4537 0.4448(2.4)
ES-FR
MAP 0.4495 0.4566 0.4563 0.48(5.1)?
P@5 0.4955 0.5164 0.5075 0.5224(1.2)
P@10 0.4328 0.4537 0.4343 0.4388(-3.3)
ES-FI
MAP 0.4495 0.4566 0.4545 0.48(5.1)?
P@5 0.4955 0.5164 0.5194 0.5254(1.7)
P@10 0.4328 0.4537 0.4373 0.4403(-3.0)
FR-FI
MAP 0.4495 0.4563 0.4545 0.4774(4.6)
P@5 0.4955 0.5075 0.5194 0.5284(4.1)?
P@10 0.4328 0.4343 0.4373 0.4373(0.7)
FI
EN-FR
MAP 0.3578 0.3411 0.3553 0.3688(3.8)
P@5 0.3821 0.394 0.397 0.4149(4.5)?
P@10 0.3105 0.3463 0.3433 0.3433(0.1)
NL-DE
MAP 0.3578 0.3722 0.3796 0.3929(3.5)
P@5 0.3821 0.406 0.403 0.4149(3.0)
P@10 0.3105 0.3478 0.3582 0.3597(0.4)
ES-DE
MAP 0.3578 0.369 0.3796 0.4058(6.9)?
P@5 0.3821 0.4119 0.403 0.4239(5.2)
P@10 0.3105 0.3448 0.3582 0.3612(0.8)
FR-DE
MAP 0.3578 0.3553 0.3796 0.3988(5.1)?
P@5 0.3821 0.397 0.403 0.406(0.7)
P@10 0.3105 0.3433 0.3582 0.3507(-2.1)
NL-ES
MAP 0.3578 0.3722 0.369 0.3875(4.1)?
P@5 0.3821 0.406 0.4119 0.4060.0)
P@10 0.3105 0.3478 0.3448 0.3537(1.7)
NL-FR
MAP 0.3578 0.3722 0.3553 0.3875(4.1)?
P@5 0.3821 0.406 0.397 0.409(0.7)
P@10 0.3105 0.3478 0.3433 0.3463(-0.4)
ES-FR
MAP 0.3578 0.369 0.3553 0.3823(3.6)
P@5 0.3821 0.4119 0.397 0.4119(0.0)
P@10 0.3105 0.3448 0.3433 0.3418(-0.9)
FR EN-ES
MAP 0.4356 0.4658 0.4634 0.4803(3.1)
P@5 0.4776 0.4925 0.4925 0.4985(1.2)
P@10 0.4194 0.4358 0.4388 0.4493(3.1)?
Table 3: Comparison of MultiPRF Multiple Assisting Lan-
guages using parallel assistance framework with MultiPRF
with single assisting language. Only language pairs where
positive improvements were obtained are reported here. Re-
sults marked as ? indicate that the improvement was sta-
tistically significant over baseline (Maximum of MultiPRF
with single assisting language) at 90% confidence level (? =
0.01) when tested using a paired two-tailed t-test.
uniformly choose the top ten documents for feed-
back.
6 Results and Discussion
Tables ?? and ?? present the results for Multi-
PRF with two assisting languages using paral-
lel assistance and selective assistance framework.
Out of the total 60 possible combinations, in Ta-
ble ??, we only report the combinations where
we have obtained positive improvements greater
than 3%. We observe most improvements in En-
glish, Finnish and French. We did not observe any
improvements using the serial assistance frame-
work over MultiPRF with single assisting lan-
74
Source
Langs
Assist.
Langs
Parallel Model Selective Model
EN DE-NL
MAP 0.4651 0.4848
P@5 0.5254 0.5224
P@10 0.4493 0.4522
NL-FI
MAP 0.4387 0.4502
P@5 0.5015 0.5164
P@10 0.4284 0.4358
DE
EN-FR
MAP 0.4097 0.4302
P@5 0.594 0.5851
P@10 0.5149 0.5179
FR-ES
MAP 0.4215 0.4333
P@5 0.591 0.591
P@10 0.5239 0.5209
FR-NL
MAP 0.4139 0.4236
P@5 0.5701 0.5701
P@10 0.5075 0.5134
FR-FI
MAP 0.3925 0.4055
P@5 0.5101 0.5642
P@10 0.4851 0.5
NL-FI
MAP 0.3974 0.4192
P@5 0.5731 0.5612
P@10 0.497 0.503
ES EN-FI
MAP 0.4436 0.4501
P@5 0.6179 0.6269
P@10 0.5567 0.5657
DE-FI
MAP 0.4542 0.465
P@5 0.6269 0.6179
P@10 0.5627 0.5582
NL-FI
MAP 0.4531 0.4611
P@5 0.6269 0.6299
P@10 0.5627 0.5627
Table 4: Results showing the positive improvements of Mul-
tiPRF with selective assistance framework over MultiPRF
with parallel assistance framework.
guage. Hence, we do not report their results as
the results were almost equivalent to single as-
sisting language. As shown in Table ??, selec-
tive assistance does give decent improvements in
some language pairs. An interesting point to note
in selective assistance is that it helps languages
like Spanish whose monolingual performance and
document coverage are both high.
6.1 Qualitative Comparison of Feedback
Terms using Multiple Languages
In this section, we qualitatively compare the re-
sults of MultiPRF with two assisting languages
with that of MultiPRF with single assisting lan-
guage, based on the top feedback terms obtained
by each model. Specifically, in Table 5 we com-
pare the terms obtained by MultiPRF using (i)
Only L1 as assisting language, (ii) Only L2 as as-
sisting language and (iii) Both L1 and L2 as as-
sisting languages in a parallel combination. For
example, the first row in the above table shows
the terms obtained by each model for the En-
glish query ?Golden Globes 1994?. Here, L1 is
French and L2 is Spanish. Terms like ?Gold?
and ?Prize? appearing in the translated feedback
model of L1 cause a drift in the topic towards
?Gold Prize? resulting in a lower MAP score
(0.33). Similarly, the terms like ?forrest? and
?spielberg? appearing in the translated feedback
model of L2 cause a drift in topic towards For-
rest Gump and Spielberg Oscars resulting in a
MAP score (0.5). However, when the models
from two languages are combined, terms which
cause a topic drift get ranked lower and as a result
the focus of the query is wrenched back. A sim-
ilar observation was made for the English query
?Damages in Ozone Layer? using French (L1)
and Spanish (L2) as assisting languages. Here,
terms from the translated feedback model of L1
cause a drift in topic towards ?militri bacteria?
whereas the terms from the translated feedback
model of L2 cause a drift in topic towards ?iraq
war?. However, in the combined model these
terms get lower rank there by bringing back the
focus of the query. For the Finnish query ?Lasten
oikeudet? (Children?s Rights), in German (L1),
the topic drift is introduced by terms like ?las,
gram, yhteis?. In case of Dutch (L2), the query
drift is caused by ?mandy, richard, slovakia? (L2)
and in the case of combined model, these terms
get less weightage and the relevant terms like
?laps, oikeuks, vanhemp? which are common in
both models, receive higher weightage causing an
improvement in query performance.
Next, we look at a few negative examples where
the parallel combination actually performs poorer
than the individual models. This happens when
some drift-terms (i.e., terms which can cause
topic drift) get mutually reinforced by both the
models. For example, for the German query
?Konkurs der Baring-Bank? (Bankruptcy of Bar-
ing Bank) the term ?share market? which was ac-
tually ranked lower in the individual models gets
boosted in the combined model resulting in a drift
in topic. Similarly, for the German query ?Ehren-
Oscar fu?r italienische Regisseure? (Honorary Os-
car for Italian directors) the term ?head office?
which was actually ranked lower in the individual
models gets ranked higher in the combined model
due to mutual reinforcement resulting in a topic
drift.
75
TOPIC NO.
QUERIES
(Meaning in 
Eng.)
TRANSLATED ENGLISH 
QUERIES 
(Assisting Lang.)
L1 
M AP
L2
M AP
L1 - L2
M AP
Representative Terms with L1 as
Single Assisting Language (With 
M eaning)
Representative Terms with L2 as
Single Assisting Language (With 
Meaning)
Representative Terms with L1& L2 as 
Assisting Langs. (With Meaning)
English ?03 
TOPIC 165 Globes 1994
Golden Globes 1994 (FR)
Globos de Oro 1994 (ES) 0.33 0.5 1
Gold, prize, oscar, nomin, best award, 
hollywood , actor, director ,actress, world, 
won ,list, winner, televi , foreign ,year, press 
world, nomin, film, award, delici, planet, 
earth, actress, list, drama, director, actor, 
spielberg, music, movie, forrest, hank 
oscar, nomin, best, award, hollywood actor, 
director, cinema, televi , music, actress, 
drama, role, hank, foreign, gold
Finnish '03
TOPIC 152
Lasten oikeudet
(Children?s
Rights)
Rechte des Kindes (DE)
Kinderrechten (NL) 0.2 0.25 0.37
laps (child), oikeuks (rights), oikeud (rights),
kind, oikeus (right), is? (father), oikeut
(justify ), vanhemp (parent), vanhem
(parents), las, gram, yhteis , unicef, sunt,
? iti(mother), yleissopimnks (conventions)
oikeuks (rights), laps (child), oikeud (right),
mandy , richard, slovakia , t?h?nast (to date),
tuomar (judge), tyto , kid, , nuor (young
people), nuort (young ), sano(said) , 
perustam(establishing)
laps (child), oikeuks (rights), oikeud (rights),
oikeus (right), is? (father, parent), vanhemp
(parent), vanhem (parents), oikeut (justify),
las, mandy , nuort (young ), richard, nuor
(young people), slovakia , t?h?nast (to date),
English ?03
TOPIC 148
Damages in 
Ozone Layer
Dommages ? la couche 
d'ozone (FR)
Destrucci?n de la capa de 
ozono (ES)
0.08 0.07 0.2 damag, militri, uv , layer, condition, chemic, bacteria, ban, radiat, ultraviolet
damag, weather, atmospher, earth, problem, 
report, research, harm, iraq , war, scandal, 
illigel, latin, hair
damag, uv , layer,weather , atmospher, earth, 
problem, report, research , utraviolet , chemic
German '03
TOPIC 180
Konkurs der
Baring -Bank
(Bankruptcy of 
Baring Bank )
Bankruptcy of Barings (EN)
Baringsin
Konkurssi (FI) 0.55 0.51 0.33
zentralbank(central bank),bankrott(bank 
cruptcy), investitionsbank, sigapur, london , 
britisch, index, tokio, england, 
werbung(advertising), japan
fall, konkurs, bankrott(Bankruptcy), 
warnsignal(warning), ignoriert, 
zusammenbruch (collepse), london, singapur, 
britisch(british), dollar, tokio, druck(pressur), 
handel(trade) 
aktienmarkt(share market), investitionsbank , 
bankrott, zentralbank (central bank), federal, 
singapur, london, britisch, index, tokio, dollar, 
druck, england, dokument(document)
German '03
TOPIC 198
Ehren-Oscar f?r
italienische
Regisseure
(Honorary Oscar 
for Italian 
directors)
Honorary Oscar for Italian 
Directors (EN)
Kunnia -Oscar italialaisille
elokuvaohjaajille (FI)
0.5 0.35 0.2
Direktor(director), film, regierungschef(prime) 
, best antonionis, antonionins, lieb, 
geschicht(history) , paris, preis, berlin, 
monitor, kamera
Generaldirektion(General director), film, 
ehrenmitglied, regisseur, direktor, verleih , 
itali, oscar, award, antonionins
generaldirektion(head office), 
ehrenmitglied(honorable member), 
regierungschef(prime), regisseur(director 
),oscar, genossenschaftsbank (corporate 
bank)
Table 5: Qualitative Comparison of MultiPRF Results using two assisting languages with single assisting language.
6.2 Effect of Coverage on MultiPRF
Accuracy
A study of the results obtained for MultiPRF using
single assisting language and multiple assisting
languages with different source languages showed
that certain languages are more suited to be ben-
efited by assisting languages. In particular, lan-
guages having smaller collections are more likely
to be benefited if assisted by a language having a
larger collection size. For example, Finnish which
has the smallest collection (55344 documents)
showed maximum improvement when supported
by assisting language(s). Based on this observa-
tion, we plotted a graph of the collection size of a
source language v/s the average improvement ob-
tained by using two assisting languages to see if
their exists a correlation between these two fac-
tors. As shown in Figure 3, there indeed exists a
high correlation between these two entities. At
one extreme, we have a language like Spanish
which has the largest collection (454045 docu-
ments) and is not benefited much by assisting lan-
guages. On the other extreme, we have Finnish
which has the smallest collection size and is ben-
efited most by assisting languages.
454.045 (Spanish)
294.809 (German)
190.604 (Dutch) 169.477 (English)
129.806 (French)
55.344 (Finnish)
0
50
100
150
200
250
300
350
400
450
500
0 1 2 3 4 5 6 7
Coverage(No.of Docs in Thousands)
Avg. Improvement in MAP of MultiPRF using two Assisting Languages (%)
Figure 3: Effect of Coverage on Average MultiPRF MAP
using Two Assisting Languages.
6.3 Effect of Number of Assisting Languages
on MultiPRF Accuracy
Another interesting question which needs to be
addressed is ?Whether it helps to use more than
two assisting languages?? and if so ?Is there an
optimum number of assisting languages beyond
which there will be no improvement??. To an-
swer these questions, we performed experiments
using 1-4 assisting languages with each source
language. As seen in Figure 4, in general as the
number of assisting languages increases the per-
formance saturates (typically after 3 languages).
Thus, for 5 out of the 6 source languages, the per-
formance saturates after 3 languages which is in
line with what we would intuitively expect. How-
ever, in the case of German, on an average, the
76
0. 35
0. 37
0. 39
0. 41
0. 43
0. 45
0. 47
0. 49
0 2 4 6
MAP
No. of. Assisting Langs.
English
Avg. MAP
MBF
0. 35
0. 37
0. 39
0. 41
0. 43
0. 45
0. 47
0 2 4 6
MAP
No. of Assisting Langs.
French
Avg. MAP
MBF
0. 35
0. 37
0. 39
0. 41
0. 43
0. 45
0 2 4 6
MAP
No. of Assisting Langs.
Finnish
Avg. MAP
MBF
0. 35
0. 37
0. 39
0. 41
0. 43
0. 45
0 2 4 6
MAP
No. of Assisting Langs.
German
Avg. MAP
MBF
0. 35
0. 37
0. 39
0. 41
0. 43
0. 45
0. 47
0 2 4 6
MAP
No. of Assisting Langs.
Dutch
Avg. MAP
MBF
0. 35
0. 37
0. 39
0. 41
0. 43
0. 45
0. 47
0. 49
0 2 4 6
MAP
No. of Assisting Langs.
Spanish
Avg. MAP
MBF
Figure 4: Effect of Number of Assisting Languages on Avg. MultiPRF Performance with Multiple Assistance.
0
0. 05
0. 1
0. 15
0. 2
0. 25
0. 3
0. 35
0. 4
English French German Spanish Dutch Finnish
Avg
. G
MA
P
Source Language
MBF
1
2
3
4
Figure 5: Effect of Number of Assisting Languages on Ro-
bustness measured through GMAP.
performance drops as the number of assisting lan-
guages is increased. This drop is counter intuitive
and needs further investigation.
6.4 Effect of Number of Assisting Languages
on Robustness
One of the primary motivations for including mul-
tiple assisting languages in MultiPRF was to in-
crease the robustness of retrieval through better
coverage. We varied the number of assisting lan-
guages for each source and studied the average
GMAP. The results are shown in Figure 5. We
observe that in almost all the source languages,
the GMAP value increases with number of assist-
ing languages and then reaches a saturation after
reaching three languages.
7 Conclusion
In this paper, we extended the MultiPRF frame-
work to multiple assisting languages. We pre-
sented three different configurations for including
multiple assisting languages - a) Parallel b) Serial
and c) Selective. We observe that the results are
mixed with parallel and selective assistance show-
ing improvements in some cases. We also observe
that the robustness of MultiPRF increases with
number of assisting languages. We analyzed the
influence of coverage of MultiPRF accuracy and
observed that it is inversely correlated. Finally,
increasing the number of assisting languages in-
creases the MultiPRF accuracy to some extent and
then it saturates beyond that limit. Many of the
above results (negative results of serial, selective
configurations etc.) require deeper investigation
which we plan to take up in future.
References
Braschler, Martin and Carol Peters. 2004. Cross-
language evaluation forum: Objectives, results,
achievements. Inf. Retr., 7(1-2):7?31.
Buckley, Chris, Gerald Salton, James Allan, and Amit
Singhal. 1994. Automatic query expansion using
smart : Trec 3. In Proceedings of The Third Text
REtrieval Conference (TREC-3, pages 69?80.
Chinnakotla, Manoj K., Karthik Raman, and Push-
pak Bhattacharyya. 2010a. Multilingual pseudo-
77
relevance feedback: English lends a helping hand.
In ACM SIGIR 2010, Geneva, Switzerland, July.
ACM.
Chinnakotla, Manoj K., Karthik Raman, and Push-
pak Bhattacharyya. 2010b. Multilingual pseudo-
relevance feedback: Performance study of assisting
languages. In ACL 2010, Uppsala, Sweeden, July.
ACL.
Dempster, A., N. Laird, and D. Rubin. 1977. Maxi-
mum Likelihood from Incomplete Data via the EM
Algorithm. Journal of the Royal Statistical Society,
39:1?38.
Hawking, David, Paul Thistlewaite, and Donna Har-
man. 1999. Scaling up the trec collection. Inf. Retr.,
1(1-2):115?137.
John Lafferty and Chengxiang Zhai. 2003. Proba-
bilistic Relevance Models Based on Document and
Query Generation. In Language Modeling for Infor-
mation Retrieval, volume 13, pages 1?10. Kluwer
International Series on IR.
Mitra, Mandar, Amit Singhal, and Chris Buckley.
1998. Improving automatic query expansion. In
SIGIR ?98: Proceedings of the 21st annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 206?214,
New York, NY, USA. ACM.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Ounis, I., G. Amati, Plachouras V., B. He, C. Macdon-
ald, and Johnson. 2005. Terrier Information Re-
trieval Platform. In Proceedings of the 27th Euro-
pean Conference on IR Research (ECIR 2005), vol-
ume 3408 of Lecture Notes in Computer Science,
pages 517?519. Springer.
Philipp, Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Robertson, Stephen. 2006. On gmap: and other trans-
formations. In CIKM ?06: Proceedings of the 15th
ACM international conference on Information and
knowledge management, pages 78?83, New York,
NY, USA. ACM.
Voorhees, Ellen. 2006. Overview of the trec 2005
robust retrieval track. In E. M. Voorhees and L.
P. Buckland, editors, The Fourteenth Text REtrieval
Conference, TREC 2005, Gaithersburg, MD. NIST.
Wu, Dan, Daqing He, Heng Ji, and Ralph Grishman.
2008. A study of using an out-of-box commercial
mt system for query translation in clir. In iNEWS
?08: Proceeding of the 2nd ACM workshop on Im-
proving non english web searching, pages 71?76,
New York, NY, USA. ACM.
Xu, Jinxi and W. Bruce Croft. 2000. Improving the ef-
fectiveness of information retrieval with local con-
text analysis. ACM Trans. Inf. Syst., 18(1):79?112.
Zhai, Chengxiang and John Lafferty. 2001. Model-
based Feedback in the Language Modeling ap-
proach to Information Retrieval. In CIKM ?01: Pro-
ceedings of the tenth international conference on In-
formation and knowledge management, pages 403?
410, New York, NY, USA. ACM Press.
Zhai, Chengxiang and John Lafferty. 2004. A Study of
Smoothing Methods for Language Models applied
to Information Retrieval. ACM Transactions on In-
formation Systems, 22(2):179?214.
78
